[{"id": "1506.00019", "submitter": "Zachary Lipton", "authors": "Zachary C. Lipton, John Berkowitz, Charles Elkan", "title": "A Critical Review of Recurrent Neural Networks for Sequence Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Countless learning tasks require dealing with sequential data. Image\ncaptioning, speech synthesis, and music generation all require that a model\nproduce outputs that are sequences. In other domains, such as time series\nprediction, video analysis, and musical information retrieval, a model must\nlearn from inputs that are sequences. Interactive tasks, such as translating\nnatural language, engaging in dialogue, and controlling a robot, often demand\nboth capabilities. Recurrent neural networks (RNNs) are connectionist models\nthat capture the dynamics of sequences via cycles in the network of nodes.\nUnlike standard feedforward neural networks, recurrent networks retain a state\nthat can represent information from an arbitrarily long context window.\nAlthough recurrent neural networks have traditionally been difficult to train,\nand often contain millions of parameters, recent advances in network\narchitectures, optimization techniques, and parallel computation have enabled\nsuccessful large-scale learning with them. In recent years, systems based on\nlong short-term memory (LSTM) and bidirectional (BRNN) architectures have\ndemonstrated ground-breaking performance on tasks as varied as image\ncaptioning, language translation, and handwriting recognition. In this survey,\nwe review and synthesize the research that over the past three decades first\nyielded and then made practical these powerful learning models. When\nappropriate, we reconcile conflicting notation and nomenclature. Our goal is to\nprovide a self-contained explication of the state of the art together with a\nhistorical perspective and references to primary research.\n", "versions": [{"version": "v1", "created": "Fri, 29 May 2015 20:16:51 GMT"}, {"version": "v2", "created": "Mon, 29 Jun 2015 20:01:00 GMT"}, {"version": "v3", "created": "Wed, 23 Sep 2015 04:59:24 GMT"}, {"version": "v4", "created": "Sat, 17 Oct 2015 05:06:11 GMT"}], "update_date": "2015-10-20", "authors_parsed": [["Lipton", "Zachary C.", ""], ["Berkowitz", "John", ""], ["Elkan", "Charles", ""]]}, {"id": "1506.00102", "submitter": "Pau Bellot", "authors": "Pau Bellot, Patrick E. Meyer", "title": "Efficient combination of pairswise feature networks", "comments": "JMLR: Workshop and Conference Proceedings, 2014 Connectomics (ECML\n  2014)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a novel method for the reconstruction of a neural network\nconnectivity using calcium fluorescence data. We introduce a fast unsupervised\nmethod to integrate different networks that reconstructs structural\nconnectivity from neuron activity. Our method improves the state-of-the-art\nreconstruction method General Transfer Entropy (GTE). We are able to better\neliminate indirect links, improving therefore the quality of the network via a\nnormalization and ensemble process of GTE and three new informative features.\nThe approach is based on a simple combination of networks, which is remarkably\nfast. The performance of our approach is benchmarked on simulated time series\nprovided at the connectomics challenge and also submitted at the public\ncompetition.\n", "versions": [{"version": "v1", "created": "Sat, 30 May 2015 10:31:31 GMT"}], "update_date": "2015-06-02", "authors_parsed": [["Bellot", "Pau", ""], ["Meyer", "Patrick E.", ""]]}, {"id": "1506.00165", "submitter": "Shay Moran", "authors": "Shay Moran and Manfred K. Warmuth", "title": "Labeled compression schemes for extremal classes", "comments": "21 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DM math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is a long-standing open problem whether there always exists a compression\nscheme whose size is of the order of the Vapnik-Chervonienkis (VC) dimension\n$d$. Recently compression schemes of size exponential in $d$ have been found\nfor any concept class of VC dimension $d$. Previously, compression schemes of\nsize $d$ have been given for maximum classes, which are special concept classes\nwhose size equals an upper bound due to Sauer-Shelah. We consider a\ngeneralization of maximum classes called extremal classes. Their definition is\nbased on a powerful generalization of the Sauer-Shelah bound called the\nSandwich Theorem, which has been studied in several areas of combinatorics and\ncomputer science. The key result of the paper is a construction of a sample\ncompression scheme for extremal classes of size equal to their VC dimension. We\nalso give a number of open problems concerning the combinatorial structure of\nextremal classes and the existence of unlabeled compression schemes for them.\n", "versions": [{"version": "v1", "created": "Sat, 30 May 2015 21:01:11 GMT"}, {"version": "v2", "created": "Tue, 2 Jun 2015 06:59:52 GMT"}, {"version": "v3", "created": "Fri, 22 Jul 2016 12:53:56 GMT"}], "update_date": "2016-07-25", "authors_parsed": [["Moran", "Shay", ""], ["Warmuth", "Manfred K.", ""]]}, {"id": "1506.00195", "submitter": "Kaisheng Yao", "authors": "Baolin Peng and Kaisheng Yao", "title": "Recurrent Neural Networks with External Memory for Language\n  Understanding", "comments": "submitted to Interspeech 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recurrent Neural Networks (RNNs) have become increasingly popular for the\ntask of language understanding. In this task, a semantic tagger is deployed to\nassociate a semantic label to each word in an input sequence. The success of\nRNN may be attributed to its ability to memorize long-term dependence that\nrelates the current-time semantic label prediction to the observations many\ntime instances away. However, the memory capacity of simple RNNs is limited\nbecause of the gradient vanishing and exploding problem. We propose to use an\nexternal memory to improve memorization capability of RNNs. We conducted\nexperiments on the ATIS dataset, and observed that the proposed model was able\nto achieve the state-of-the-art results. We compare our proposed model with\nalternative models and report analysis results that may provide insights for\nfuture research.\n", "versions": [{"version": "v1", "created": "Sun, 31 May 2015 05:10:03 GMT"}], "update_date": "2015-06-02", "authors_parsed": [["Peng", "Baolin", ""], ["Yao", "Kaisheng", ""]]}, {"id": "1506.00227", "submitter": "YaJun Cui", "authors": "Yajun Cui, Yang Zhao, Kafei Xiao, Chenglong Zhang, Lei Wang", "title": "Parallel Spectral Clustering Algorithm Based on Hadoop", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DS cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Spectral clustering and cloud computing is emerging branch of computer\nscience or related discipline. It overcome the shortcomings of some traditional\nclustering algorithm and guarantee the convergence to the optimal solution,\nthus have to the widespread attention. This article first introduced the\nparallel spectral clustering algorithm research background and significance,\nand then to Hadoop the cloud computing Framework has carried on the detailed\nintroduction, then has carried on the related to spectral clustering is\nintroduced, then introduces the spectral clustering arithmetic Method of\nparallel and relevant steps, finally made the related experiments, and the\nexperiment are summarized.\n", "versions": [{"version": "v1", "created": "Sun, 31 May 2015 13:39:41 GMT"}], "update_date": "2015-06-02", "authors_parsed": [["Cui", "Yajun", ""], ["Zhao", "Yang", ""], ["Xiao", "Kafei", ""], ["Zhang", "Chenglong", ""], ["Wang", "Lei", ""]]}, {"id": "1506.00312", "submitter": "Masrour Zoghi", "authors": "Masrour Zoghi, Zohar Karnin, Shimon Whiteson and Maarten de Rijke", "title": "Copeland Dueling Bandits", "comments": "33 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A version of the dueling bandit problem is addressed in which a Condorcet\nwinner may not exist. Two algorithms are proposed that instead seek to minimize\nregret with respect to the Copeland winner, which, unlike the Condorcet winner,\nis guaranteed to exist. The first, Copeland Confidence Bound (CCB), is designed\nfor small numbers of arms, while the second, Scalable Copeland Bandits (SCB),\nworks better for large-scale problems. We provide theoretical results bounding\nthe regret accumulated by CCB and SCB, both substantially improving existing\nresults. Such existing results either offer bounds of the form $O(K \\log T)$\nbut require restrictive assumptions, or offer bounds of the form $O(K^2 \\log\nT)$ without requiring such assumptions. Our results offer the best of both\nworlds: $O(K \\log T)$ bounds without restrictive assumptions.\n", "versions": [{"version": "v1", "created": "Mon, 1 Jun 2015 00:44:37 GMT"}], "update_date": "2015-06-02", "authors_parsed": [["Zoghi", "Masrour", ""], ["Karnin", "Zohar", ""], ["Whiteson", "Shimon", ""], ["de Rijke", "Maarten", ""]]}, {"id": "1506.00323", "submitter": "Anastasia Podosinnikova", "authors": "Anastasia Podosinnikova, Simon Setzer, and Matthias Hein", "title": "Robust PCA: Optimization of the Robust Reconstruction Error over the\n  Stiefel Manifold", "comments": "long version of GCPR 2014 paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is well known that Principal Component Analysis (PCA) is strongly affected\nby outliers and a lot of effort has been put into robustification of PCA. In\nthis paper we present a new algorithm for robust PCA minimizing the trimmed\nreconstruction error. By directly minimizing over the Stiefel manifold, we\navoid deflation as often used by projection pursuit methods. In distinction to\nother methods for robust PCA, our method has no free parameter and is\ncomputationally very efficient. We illustrate the performance on various\ndatasets including an application to background modeling and subtraction. Our\nmethod performs better or similar to current state-of-the-art methods while\nbeing faster.\n", "versions": [{"version": "v1", "created": "Mon, 1 Jun 2015 01:57:15 GMT"}], "update_date": "2015-06-02", "authors_parsed": [["Podosinnikova", "Anastasia", ""], ["Setzer", "Simon", ""], ["Hein", "Matthias", ""]]}, {"id": "1506.00327", "submitter": "Zhiguang Wang", "authors": "Zhiguang Wang and Tim Oates", "title": "Imaging Time-Series to Improve Classification and Imputation", "comments": "Accepted by IJCAI-2015 ML track", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE stat.ML", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  Inspired by recent successes of deep learning in computer vision, we propose\na novel framework for encoding time series as different types of images,\nnamely, Gramian Angular Summation/Difference Fields (GASF/GADF) and Markov\nTransition Fields (MTF). This enables the use of techniques from computer\nvision for time series classification and imputation. We used Tiled\nConvolutional Neural Networks (tiled CNNs) on 20 standard datasets to learn\nhigh-level features from the individual and compound GASF-GADF-MTF images. Our\napproaches achieve highly competitive results when compared to nine of the\ncurrent best time series classification approaches. Inspired by the bijection\nproperty of GASF on 0/1 rescaled data, we train Denoised Auto-encoders (DA) on\nthe GASF images of four standard and one synthesized compound dataset. The\nimputation MSE on test data is reduced by 12.18%-48.02% when compared to using\nthe raw data. An analysis of the features and weights learned via tiled CNNs\nand DAs explains why the approaches work.\n", "versions": [{"version": "v1", "created": "Mon, 1 Jun 2015 02:17:06 GMT"}], "update_date": "2015-06-02", "authors_parsed": [["Wang", "Zhiguang", ""], ["Oates", "Tim", ""]]}, {"id": "1506.00333", "submitter": "Lin Ma", "authors": "Lin Ma, Zhengdong Lu, Hang Li", "title": "Learning to Answer Questions From Image Using Convolutional Neural\n  Network", "comments": "7 pages, 4 figures. Accepted by AAAI 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.CV cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose to employ the convolutional neural network (CNN)\nfor the image question answering (QA). Our proposed CNN provides an end-to-end\nframework with convolutional architectures for learning not only the image and\nquestion representations, but also their inter-modal interactions to produce\nthe answer. More specifically, our model consists of three CNNs: one image CNN\nto encode the image content, one sentence CNN to compose the words of the\nquestion, and one multimodal convolution layer to learn their joint\nrepresentation for the classification in the space of candidate answer words.\nWe demonstrate the efficacy of our proposed model on the DAQUAR and COCO-QA\ndatasets, which are two benchmark datasets for the image QA, with the\nperformances significantly outperforming the state-of-the-art.\n", "versions": [{"version": "v1", "created": "Mon, 1 Jun 2015 03:09:49 GMT"}, {"version": "v2", "created": "Fri, 13 Nov 2015 09:54:59 GMT"}], "update_date": "2015-11-16", "authors_parsed": [["Ma", "Lin", ""], ["Lu", "Zhengdong", ""], ["Li", "Hang", ""]]}, {"id": "1506.00354", "submitter": "Yasser Roudi", "authors": "Yasser Roudi and Graham Taylor", "title": "Learning with hidden variables", "comments": "revised version accepted in Current Opinion in Neurobiology", "journal-ref": "Current Opinion in Neurobiology (2015), 35: 110-118", "doi": "10.1016/j.conb.2015.07.006", "report-no": null, "categories": "q-bio.NC cond-mat.dis-nn cs.LG cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning and inferring features that generate sensory input is a task\ncontinuously performed by cortex. In recent years, novel algorithms and\nlearning rules have been proposed that allow neural network models to learn\nsuch features from natural images, written text, audio signals, etc. These\nnetworks usually involve deep architectures with many layers of hidden neurons.\nHere we review recent advancements in this area emphasizing, amongst other\nthings, the processing of dynamical inputs by networks with hidden nodes and\nthe role of single neuron models. These points and the questions they arise can\nprovide conceptual advancements in understanding of learning in the cortex and\nthe relationship between machine learning approaches to learning with hidden\nnodes and those in cortical circuits.\n", "versions": [{"version": "v1", "created": "Mon, 1 Jun 2015 05:36:19 GMT"}, {"version": "v2", "created": "Fri, 24 Jul 2015 20:37:48 GMT"}], "update_date": "2021-04-13", "authors_parsed": [["Roudi", "Yasser", ""], ["Taylor", "Graham", ""]]}, {"id": "1506.00438", "submitter": "Aravind Rajeswaran", "authors": "Aravind Rajeswaran and Shankar Narasimhan", "title": "Network Topology Identification using PCA and its Graph Theoretic\n  Interpretations", "comments": "Structure of paper is changed to improve presentation. Methods and\n  results are unchanged. A more detailed literature survey has been added", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DM cs.SY stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We solve the problem of identifying (reconstructing) network topology from\nsteady state network measurements. Concretely, given only a data matrix\n$\\mathbf{X}$ where the $X_{ij}$ entry corresponds to flow in edge $i$ in\nconfiguration (steady-state) $j$, we wish to find a network structure for which\nflow conservation is obeyed at all the nodes. This models many network problems\ninvolving conserved quantities like water, power, and metabolic networks. We\nshow that identification is equivalent to learning a model $\\mathbf{A_n}$ which\ncaptures the approximate linear relationships between the different variables\ncomprising $\\mathbf{X}$ (i.e. of the form $\\mathbf{A_n X \\approx 0}$) such that\n$\\mathbf{A_n}$ is full rank (highest possible) and consistent with a network\nnode-edge incidence structure. The problem is solved through a sequence of\nsteps like estimating approximate linear relationships using Principal\nComponent Analysis, obtaining f-cut-sets from these approximate relationships,\nand graph realization from f-cut-sets (or equivalently f-circuits). Each step\nand the overall process is polynomial time. The method is illustrated by\nidentifying topology of a water distribution network. We also study the extent\nof identifiability from steady-state data.\n", "versions": [{"version": "v1", "created": "Mon, 1 Jun 2015 10:57:00 GMT"}, {"version": "v2", "created": "Thu, 21 Jan 2016 18:31:42 GMT"}], "update_date": "2016-01-22", "authors_parsed": [["Rajeswaran", "Aravind", ""], ["Narasimhan", "Shankar", ""]]}, {"id": "1506.00468", "submitter": "Michal Lukasik", "authors": "Michal Lukasik and Trevor Cohn and Kalina Bontcheva", "title": "Classifying Tweet Level Judgements of Rumours in Social Media", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Social media is a rich source of rumours and corresponding community\nreactions. Rumours reflect different characteristics, some shared and some\nindividual. We formulate the problem of classifying tweet level judgements of\nrumours as a supervised learning task. Both supervised and unsupervised domain\nadaptation are considered, in which tweets from a rumour are classified on the\nbasis of other annotated rumours. We demonstrate how multi-task learning helps\nachieve good results on rumours from the 2011 England riots.\n", "versions": [{"version": "v1", "created": "Mon, 1 Jun 2015 12:20:21 GMT"}, {"version": "v2", "created": "Thu, 10 Sep 2015 18:25:55 GMT"}], "update_date": "2015-09-11", "authors_parsed": [["Lukasik", "Michal", ""], ["Cohn", "Trevor", ""], ["Bontcheva", "Kalina", ""]]}, {"id": "1506.00511", "submitter": "Jimmy Ba", "authors": "Jimmy Ba, Kevin Swersky, Sanja Fidler and Ruslan Salakhutdinov", "title": "Predicting Deep Zero-Shot Convolutional Neural Networks using Textual\n  Descriptions", "comments": "Correct the typos in table 1 regarding [5]. To appear in ICCV 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One of the main challenges in Zero-Shot Learning of visual categories is\ngathering semantic attributes to accompany images. Recent work has shown that\nlearning from textual descriptions, such as Wikipedia articles, avoids the\nproblem of having to explicitly define these attributes. We present a new model\nthat can classify unseen categories from their textual description.\nSpecifically, we use text features to predict the output weights of both the\nconvolutional and the fully connected layers in a deep convolutional neural\nnetwork (CNN). We take advantage of the architecture of CNNs and learn features\nat different layers, rather than just learning an embedding space for both\nmodalities, as is common with existing approaches. The proposed model also\nallows us to automatically generate a list of pseudo- attributes for each\nvisual category consisting of words from Wikipedia articles. We train our\nmodels end-to-end us- ing the Caltech-UCSD bird and flower datasets and\nevaluate both ROC and Precision-Recall curves. Our empirical results show that\nthe proposed model significantly outperforms previous methods.\n", "versions": [{"version": "v1", "created": "Mon, 1 Jun 2015 14:37:06 GMT"}, {"version": "v2", "created": "Fri, 25 Sep 2015 16:20:44 GMT"}], "update_date": "2015-09-28", "authors_parsed": [["Ba", "Jimmy", ""], ["Swersky", "Kevin", ""], ["Fidler", "Sanja", ""], ["Salakhutdinov", "Ruslan", ""]]}, {"id": "1506.00552", "submitter": "Julie Nutini", "authors": "Julie Nutini, Mark Schmidt, Issam H. Laradji, Michael Friedlander,\n  Hoyt Koepke", "title": "Coordinate Descent Converges Faster with the Gauss-Southwell Rule Than\n  Random Selection", "comments": "ICML 2015. v2: Updated the Gauss-Southwell-q result in Section 8 and\n  Appendix H, to remove the part depending on mu_1 (the proof had an error).\n  Added Section 8.1, which discusses conditions under which a rate depending on\n  mu_1 does hold", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.LG stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There has been significant recent work on the theory and application of\nrandomized coordinate descent algorithms, beginning with the work of Nesterov\n[SIAM J. Optim., 22(2), 2012], who showed that a random-coordinate selection\nrule achieves the same convergence rate as the Gauss-Southwell selection rule.\nThis result suggests that we should never use the Gauss-Southwell rule, as it\nis typically much more expensive than random selection. However, the empirical\nbehaviours of these algorithms contradict this theoretical result: in\napplications where the computational costs of the selection rules are\ncomparable, the Gauss-Southwell selection rule tends to perform substantially\nbetter than random coordinate selection. We give a simple analysis of the\nGauss-Southwell rule showing that---except in extreme cases---its convergence\nrate is faster than choosing random coordinates. Further, in this work we (i)\nshow that exact coordinate optimization improves the convergence rate for\ncertain sparse problems, (ii) propose a Gauss-Southwell-Lipschitz rule that\ngives an even faster convergence rate given knowledge of the Lipschitz\nconstants of the partial derivatives, (iii) analyze the effect of approximate\nGauss-Southwell rules, and (iv) analyze proximal-gradient variants of the\nGauss-Southwell rule.\n", "versions": [{"version": "v1", "created": "Mon, 1 Jun 2015 16:04:37 GMT"}, {"version": "v2", "created": "Sun, 28 Oct 2018 17:11:00 GMT"}], "update_date": "2018-10-30", "authors_parsed": [["Nutini", "Julie", ""], ["Schmidt", "Mark", ""], ["Laradji", "Issam H.", ""], ["Friedlander", "Michael", ""], ["Koepke", "Hoyt", ""]]}, {"id": "1506.00619", "submitter": "Bart van Merri\\\"enboer", "authors": "Bart van Merri\\\"enboer, Dzmitry Bahdanau, Vincent Dumoulin, Dmitriy\n  Serdyuk, David Warde-Farley, Jan Chorowski, Yoshua Bengio", "title": "Blocks and Fuel: Frameworks for deep learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce two Python frameworks to train neural networks on large\ndatasets: Blocks and Fuel. Blocks is based on Theano, a linear algebra compiler\nwith CUDA-support. It facilitates the training of complex neural network models\nby providing parametrized Theano operations, attaching metadata to Theano's\nsymbolic computational graph, and providing an extensive set of utilities to\nassist training the networks, e.g. training algorithms, logging, monitoring,\nvisualization, and serialization. Fuel provides a standard format for machine\nlearning datasets. It allows the user to easily iterate over large datasets,\nperforming many types of pre-processing on the fly.\n", "versions": [{"version": "v1", "created": "Mon, 1 Jun 2015 19:28:27 GMT"}], "update_date": "2015-06-02", "authors_parsed": [["van Merri\u00ebnboer", "Bart", ""], ["Bahdanau", "Dzmitry", ""], ["Dumoulin", "Vincent", ""], ["Serdyuk", "Dmitriy", ""], ["Warde-Farley", "David", ""], ["Chorowski", "Jan", ""], ["Bengio", "Yoshua", ""]]}, {"id": "1506.00671", "submitter": "Ludwig Schmidt", "authors": "Jayadev Acharya, Ilias Diakonikolas, Jerry Li, Ludwig Schmidt", "title": "Sample-Optimal Density Estimation in Nearly-Linear Time", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.IT cs.LG math.IT math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We design a new, fast algorithm for agnostically learning univariate\nprobability distributions whose densities are well approximated by piecewise\npolynomial functions. Let $f$ be the density function of an arbitrary\nunivariate distribution, and suppose that $f$ is $\\mathrm{OPT}$-close in\n$L_1$-distance to an unknown piecewise polynomial function with $t$ interval\npieces and degree $d$. Our algorithm draws $n = O(t(d+1)/\\epsilon^2)$ samples\nfrom $f$, runs in time $\\tilde{O}(n \\cdot \\mathrm{poly}(d))$, and with\nprobability at least $9/10$ outputs an $O(t)$-piecewise degree-$d$ hypothesis\n$h$ that is $4 \\cdot \\mathrm{OPT} +\\epsilon$ close to $f$.\n  Our general algorithm yields (nearly) sample-optimal and nearly-linear time\nestimators for a wide range of structured distribution families over both\ncontinuous and discrete domains in a unified way. For most of our applications,\nthese are the first sample-optimal and nearly-linear time estimators in the\nliterature. As a consequence, our work resolves the sample and computational\ncomplexities of a broad class of inference tasks via a single \"meta-algorithm\".\nMoreover, we experimentally demonstrate that our algorithm performs very well\nin practice.\n  Our algorithm consists of three \"levels\": (i) At the top level, we employ an\niterative greedy algorithm for finding a good partition of the real line into\nthe pieces of a piecewise polynomial. (ii) For each piece, we show that the\nsub-problem of finding a good polynomial fit on the current interval can be\nsolved efficiently with a separation oracle method. (iii) We reduce the task of\nfinding a separating hyperplane to a combinatorial problem and give an\nefficient algorithm for this problem. Combining these three procedures gives a\ndensity estimation algorithm with the claimed guarantees.\n", "versions": [{"version": "v1", "created": "Mon, 1 Jun 2015 20:44:22 GMT"}], "update_date": "2015-06-03", "authors_parsed": [["Acharya", "Jayadev", ""], ["Diakonikolas", "Ilias", ""], ["Li", "Jerry", ""], ["Schmidt", "Ludwig", ""]]}, {"id": "1506.00745", "submitter": "Paul Wiggins Dr", "authors": "Colin H. LaMont and Paul A. Wiggins", "title": "An objective prior that unifies objective Bayes and information-based\n  inference", "comments": "7 pages, 1 figure (+ minor corrections)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG physics.data-an", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There are three principle paradigms of statistical inference: (i) Bayesian,\n(ii) information-based and (iii) frequentist inference. We describe an\nobjective prior (the weighting or $w$-prior) which unifies objective Bayes and\ninformation-based inference. The $w$-prior is chosen to make the marginal\nprobability an unbiased estimator of the predictive performance of the model.\nThis definition has several other natural interpretations. From the perspective\nof the information content of the prior, the $w$-prior is both uniformly and\nmaximally uninformative. The $w$-prior can also be understood to result in a\nuniform density of distinguishable models in parameter space. Finally we\ndemonstrate the the $w$-prior is equivalent to the Akaike Information Criterion\n(AIC) for regular models in the asymptotic limit. The $w$-prior appears to be\ngenerically applicable to statistical inference and is free of {\\it ad hoc}\nregularization. The mechanism for suppressing complexity is analogous to AIC:\nmodel complexity reduces model predictivity. We expect this new objective-Bayes\napproach to inference to be widely-applicable to machine-learning problems\nincluding singular models.\n", "versions": [{"version": "v1", "created": "Tue, 2 Jun 2015 04:35:12 GMT"}, {"version": "v2", "created": "Thu, 25 Jun 2015 03:03:51 GMT"}], "update_date": "2015-06-26", "authors_parsed": [["LaMont", "Colin H.", ""], ["Wiggins", "Paul A.", ""]]}, {"id": "1506.00779", "submitter": "Junpei Komiyama", "authors": "Junpei Komiyama, Junya Honda, Hiroshi Nakagawa", "title": "Optimal Regret Analysis of Thompson Sampling in Stochastic Multi-armed\n  Bandit Problem with Multiple Plays", "comments": "Appeared in ICML2015. Fixed the evaluation of term (B) in Lemma 3.\n  Replaced \\tilde{\\mu}->\\theta", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We discuss a multiple-play multi-armed bandit (MAB) problem in which several\narms are selected at each round. Recently, Thompson sampling (TS), a randomized\nalgorithm with a Bayesian spirit, has attracted much attention for its\nempirically excellent performance, and it is revealed to have an optimal regret\nbound in the standard single-play MAB problem. In this paper, we propose the\nmultiple-play Thompson sampling (MP-TS) algorithm, an extension of TS to the\nmultiple-play MAB problem, and discuss its regret analysis. We prove that MP-TS\nfor binary rewards has the optimal regret upper bound that matches the regret\nlower bound provided by Anantharam et al. (1987). Therefore, MP-TS is the first\ncomputationally efficient algorithm with optimal regret. A set of computer\nsimulations was also conducted, which compared MP-TS with state-of-the-art\nalgorithms. We also propose a modification of MP-TS, which is shown to have\nbetter empirical performance.\n", "versions": [{"version": "v1", "created": "Tue, 2 Jun 2015 07:42:16 GMT"}, {"version": "v2", "created": "Tue, 24 May 2016 12:21:19 GMT"}, {"version": "v3", "created": "Wed, 20 Mar 2019 18:10:22 GMT"}], "update_date": "2019-03-22", "authors_parsed": [["Komiyama", "Junpei", ""], ["Honda", "Junya", ""], ["Nakagawa", "Hiroshi", ""]]}, {"id": "1506.00799", "submitter": "Xiangyu Zeng", "authors": "Xiangyu Zeng and Shi Yin and Dong Wang", "title": "Learning Speech Rate in Speech Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A significant performance reduction is often observed in speech recognition\nwhen the rate of speech (ROS) is too low or too high. Most of present\napproaches to addressing the ROS variation focus on the change of speech\nsignals in dynamic properties caused by ROS, and accordingly modify the dynamic\nmodel, e.g., the transition probabilities of the hidden Markov model (HMM).\nHowever, an abnormal ROS changes not only the dynamic but also the static\nproperty of speech signals, and thus can not be compensated for purely by\nmodifying the dynamic model. This paper proposes an ROS learning approach based\non deep neural networks (DNN), which involves an ROS feature as the input of\nthe DNN model and so the spectrum distortion caused by ROS can be learned and\ncompensated for. The experimental results show that this approach can deliver\nbetter performance for too slow and too fast utterances, demonstrating our\nconjecture that ROS impacts both the dynamic and the static property of speech.\nIn addition, the proposed approach can be combined with the conventional HMM\ntransition adaptation method, offering additional performance gains.\n", "versions": [{"version": "v1", "created": "Tue, 2 Jun 2015 08:59:47 GMT"}], "update_date": "2015-06-03", "authors_parsed": [["Zeng", "Xiangyu", ""], ["Yin", "Shi", ""], ["Wang", "Dong", ""]]}, {"id": "1506.00821", "submitter": "Hung Hoang", "authors": "Hung Gia Hoang and Ba-Tuong Vo and Ba-Ngu Vo", "title": "A Generalized Labeled Multi-Bernoulli Filter Implementation using Gibbs\n  Sampling", "comments": "11 pages, 8 figures. Part of the paper has been accepted for\n  presentation at the 18th international conference on Information Fusion\n  (FUSION 15)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes an efficient implementation of the generalized labeled\nmulti-Bernoulli (GLMB) filter by combining the prediction and update into a\nsingle step. In contrast to the original approach which involves separate\ntruncations in the prediction and update steps, the proposed implementation\nrequires only one single truncation for each iteration, which can be performed\nusing a standard ranked optimal assignment algorithm. Furthermore, we propose a\nnew truncation technique based on Markov Chain Monte Carlo methods such as\nGibbs sampling, which drastically reduces the complexity of the filter. The\nsuperior performance of the proposed approach is demonstrated through extensive\nnumerical studies.\n", "versions": [{"version": "v1", "created": "Tue, 2 Jun 2015 09:59:34 GMT"}, {"version": "v2", "created": "Wed, 17 Jun 2015 11:18:15 GMT"}, {"version": "v3", "created": "Fri, 3 Jul 2015 06:17:28 GMT"}], "update_date": "2015-07-06", "authors_parsed": [["Hoang", "Hung Gia", ""], ["Vo", "Ba-Tuong", ""], ["Vo", "Ba-Ngu", ""]]}, {"id": "1506.00852", "submitter": "Ulrike von Luxburg", "authors": "Mehdi S. M. Sajjadi, Morteza Alamgir, Ulrike von Luxburg", "title": "Peer Grading in a Course on Algorithms and Data Structures: Machine\n  Learning Algorithms do not Improve over Simple Baselines", "comments": "Published at the Third Annual ACM Conference on Learning at Scale L@S", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Peer grading is the process of students reviewing each others' work, such as\nhomework submissions, and has lately become a popular mechanism used in massive\nopen online courses (MOOCs). Intrigued by this idea, we used it in a course on\nalgorithms and data structures at the University of Hamburg. Throughout the\nwhole semester, students repeatedly handed in submissions to exercises, which\nwere then evaluated both by teaching assistants and by a peer grading\nmechanism, yielding a large dataset of teacher and peer grades. We applied\ndifferent statistical and machine learning methods to aggregate the peer grades\nin order to come up with accurate final grades for the submissions (supervised\nand unsupervised, methods based on numeric scores and ordinal rankings).\nSurprisingly, none of them improves over the baseline of using the mean peer\ngrade as the final grade. We discuss a number of possible explanations for\nthese results and present a thorough analysis of the generated dataset.\n", "versions": [{"version": "v1", "created": "Tue, 2 Jun 2015 12:03:30 GMT"}, {"version": "v2", "created": "Wed, 10 Feb 2016 14:49:19 GMT"}], "update_date": "2016-02-11", "authors_parsed": [["Sajjadi", "Mehdi S. M.", ""], ["Alamgir", "Morteza", ""], ["von Luxburg", "Ulrike", ""]]}, {"id": "1506.00935", "submitter": "Hastagiri Prakash Vanchinathan", "authors": "Hastagiri P. Vanchinathan, Andreas Marfurt, Charles-Antoine Robelin,\n  Donald Kossmann, Andreas Krause", "title": "Discovering Valuable Items from Massive Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.IT math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Suppose there is a large collection of items, each with an associated cost\nand an inherent utility that is revealed only once we commit to selecting it.\nGiven a budget on the cumulative cost of the selected items, how can we pick a\nsubset of maximal value? This task generalizes several important problems such\nas multi-arm bandits, active search and the knapsack problem. We present an\nalgorithm, GP-Select, which utilizes prior knowledge about similarity be- tween\nitems, expressed as a kernel function. GP-Select uses Gaussian process\nprediction to balance exploration (estimating the unknown value of items) and\nexploitation (selecting items of high value). We extend GP-Select to be able to\ndiscover sets that simultaneously have high utility and are diverse. Our\npreference for diversity can be specified as an arbitrary monotone submodular\nfunction that quantifies the diminishing returns obtained when selecting\nsimilar items. Furthermore, we exploit the structure of the model updates to\nachieve an order of magnitude (up to 40X) speedup in our experiments without\nresorting to approximations. We provide strong guarantees on the performance of\nGP-Select and apply it to three real-world case studies of industrial\nrelevance: (1) Refreshing a repository of prices in a Global Distribution\nSystem for the travel industry, (2) Identifying diverse, binding-affine\npeptides in a vaccine de- sign task and (3) Maximizing clicks in a web-scale\nrecommender system by recommending items to users.\n", "versions": [{"version": "v1", "created": "Tue, 2 Jun 2015 16:01:46 GMT"}], "update_date": "2015-06-08", "authors_parsed": [["Vanchinathan", "Hastagiri P.", ""], ["Marfurt", "Andreas", ""], ["Robelin", "Charles-Antoine", ""], ["Kossmann", "Donald", ""], ["Krause", "Andreas", ""]]}, {"id": "1506.00976", "submitter": "Gautier Marti", "authors": "Gautier Marti, Philippe Very and Philippe Donnat", "title": "Toward a generic representation of random variables for machine learning", "comments": "submitted to Pattern Recognition Letters", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a pre-processing and a distance which improve the\nperformance of machine learning algorithms working on independent and\nidentically distributed stochastic processes. We introduce a novel\nnon-parametric approach to represent random variables which splits apart\ndependency and distribution without losing any information. We also propound an\nassociated metric leveraging this representation and its statistical estimate.\nBesides experiments on synthetic datasets, the benefits of our contribution is\nillustrated through the example of clustering financial time series, for\ninstance prices from the credit default swaps market. Results are available on\nthe website www.datagrapple.com and an IPython Notebook tutorial is available\nat www.datagrapple.com/Tech for reproducible research.\n", "versions": [{"version": "v1", "created": "Tue, 2 Jun 2015 17:58:48 GMT"}, {"version": "v2", "created": "Thu, 3 Sep 2015 19:23:30 GMT"}], "update_date": "2015-09-04", "authors_parsed": [["Marti", "Gautier", ""], ["Very", "Philippe", ""], ["Donnat", "Philippe", ""]]}, {"id": "1506.00990", "submitter": "Yao Lu", "authors": "Yao Lu", "title": "Unsupervised Learning on Neural Network Outputs: with Application in\n  Zero-shot Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The outputs of a trained neural network contain much richer information than\njust an one-hot classifier. For example, a neural network might give an image\nof a dog the probability of one in a million of being a cat but it is still\nmuch larger than the probability of being a car. To reveal the hidden structure\nin them, we apply two unsupervised learning algorithms, PCA and ICA, to the\noutputs of a deep Convolutional Neural Network trained on the ImageNet of 1000\nclasses. The PCA/ICA embedding of the object classes reveals their visual\nsimilarity and the PCA/ICA components can be interpreted as common visual\nfeatures shared by similar object classes. For an application, we proposed a\nnew zero-shot learning method, in which the visual features learned by PCA/ICA\nare employed. Our zero-shot learning method achieves the state-of-the-art\nresults on the ImageNet of over 20000 classes.\n", "versions": [{"version": "v1", "created": "Tue, 2 Jun 2015 19:12:00 GMT"}, {"version": "v10", "created": "Mon, 23 May 2016 08:42:10 GMT"}, {"version": "v2", "created": "Wed, 3 Jun 2015 13:40:09 GMT"}, {"version": "v3", "created": "Thu, 4 Jun 2015 10:59:03 GMT"}, {"version": "v4", "created": "Tue, 7 Jul 2015 07:30:00 GMT"}, {"version": "v5", "created": "Wed, 7 Oct 2015 11:55:56 GMT"}, {"version": "v6", "created": "Sun, 25 Oct 2015 23:25:57 GMT"}, {"version": "v7", "created": "Mon, 2 Nov 2015 08:20:34 GMT"}, {"version": "v8", "created": "Thu, 5 Nov 2015 12:34:31 GMT"}, {"version": "v9", "created": "Thu, 28 Jan 2016 18:44:26 GMT"}], "update_date": "2016-05-24", "authors_parsed": [["Lu", "Yao", ""]]}, {"id": "1506.00999", "submitter": "Antoine Bordes", "authors": "Alberto Garcia-Duran, Antoine Bordes, Nicolas Usunier, Yves Grandvalet", "title": "Combining Two And Three-Way Embeddings Models for Link Prediction in\n  Knowledge Bases", "comments": "26 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper tackles the problem of endogenous link prediction for Knowledge\nBase completion. Knowledge Bases can be represented as directed graphs whose\nnodes correspond to entities and edges to relationships. Previous attempts\neither consist of powerful systems with high capacity to model complex\nconnectivity patterns, which unfortunately usually end up overfitting on rare\nrelationships, or in approaches that trade capacity for simplicity in order to\nfairly model all relationships, frequent or not. In this paper, we propose\nTatec a happy medium obtained by complementing a high-capacity model with a\nsimpler one, both pre-trained separately and then combined. We present several\nvariants of this model with different kinds of regularization and combination\nstrategies and show that this approach outperforms existing methods on\ndifferent types of relationships by achieving state-of-the-art results on four\nbenchmarks of the literature.\n", "versions": [{"version": "v1", "created": "Tue, 2 Jun 2015 19:34:19 GMT"}], "update_date": "2015-06-03", "authors_parsed": [["Garcia-Duran", "Alberto", ""], ["Bordes", "Antoine", ""], ["Usunier", "Nicolas", ""], ["Grandvalet", "Yves", ""]]}, {"id": "1506.01060", "submitter": "Nan Zhou", "authors": "Nan Zhou, Yangyang Xu, Hong Cheng, Jun Fang, Witold Pedrycz", "title": "Global and Local Structure Preserving Sparse Subspace Learning: An\n  Iterative Approach to Unsupervised Feature Selection", "comments": "32 page, 6 figures and 60 references", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As we aim at alleviating the curse of high-dimensionality, subspace learning\nis becoming more popular. Existing approaches use either information about\nglobal or local structure of the data, and few studies simultaneously focus on\nglobal and local structures as the both of them contain important information.\nIn this paper, we propose a global and local structure preserving sparse\nsubspace learning (GLoSS) model for unsupervised feature selection. The model\ncan simultaneously realize feature selection and subspace learning. In\naddition, we develop a greedy algorithm to establish a generic combinatorial\nmodel, and an iterative strategy based on an accelerated block coordinate\ndescent is used to solve the GLoSS problem. We also provide whole iterate\nsequence convergence analysis of the proposed iterative algorithm. Extensive\nexperiments are conducted on real-world datasets to show the superiority of the\nproposed approach over several state-of-the-art unsupervised feature selection\napproaches.\n", "versions": [{"version": "v1", "created": "Tue, 2 Jun 2015 21:02:16 GMT"}, {"version": "v2", "created": "Mon, 19 Oct 2015 18:13:24 GMT"}], "update_date": "2015-10-20", "authors_parsed": [["Zhou", "Nan", ""], ["Xu", "Yangyang", ""], ["Cheng", "Hong", ""], ["Fang", "Jun", ""], ["Pedrycz", "Witold", ""]]}, {"id": "1506.01077", "submitter": "Saullo Haniell Galv\\~ao De Oliveira", "authors": "Saullo Haniell Galv\\~ao de Oliveira, Rosana Veroneze, Fernando Jos\\'e\n  Von Zuben", "title": "On bicluster aggregation and its benefits for enumerative solutions", "comments": "15 pages, will be published by Springer Verlag in the LNAI Series in\n  the book Advances in Data Mining", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  Biclustering involves the simultaneous clustering of objects and their\nattributes, thus defining local two-way clustering models. Recently, efficient\nalgorithms were conceived to enumerate all biclusters in real-valued datasets.\nIn this case, the solution composes a complete set of maximal and non-redundant\nbiclusters. However, the ability to enumerate biclusters revealed a challenging\nscenario: in noisy datasets, each true bicluster may become highly fragmented\nand with a high degree of overlapping. It prevents a direct analysis of the\nobtained results. To revert the fragmentation, we propose here two approaches\nfor properly aggregating the whole set of enumerated biclusters: one based on\nsingle linkage and the other directly exploring the rate of overlapping. Both\nproposals were compared with each other and with the actual state-of-the-art in\nseveral experiments, and they not only significantly reduced the number of\nbiclusters but also consistently increased the quality of the solution.\n", "versions": [{"version": "v1", "created": "Tue, 2 Jun 2015 22:26:42 GMT"}], "update_date": "2015-06-04", "authors_parsed": [["de Oliveira", "Saullo Haniell Galv\u00e3o", ""], ["Veroneze", "Rosana", ""], ["Von Zuben", "Fernando Jos\u00e9", ""]]}, {"id": "1506.01110", "submitter": "Bokai Cao", "authors": "Bokai Cao, Hucheng Zhou, Guoqiang Li and Philip S. Yu", "title": "Multi-View Factorization Machines", "comments": "WSDM 2016", "journal-ref": null, "doi": "10.1145/2835776.2835777", "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For a learning task, data can usually be collected from different sources or\nbe represented from multiple views. For example, laboratory results from\ndifferent medical examinations are available for disease diagnosis, and each of\nthem can only reflect the health state of a person from a particular\naspect/view. Therefore, different views provide complementary information for\nlearning tasks. An effective integration of the multi-view information is\nexpected to facilitate the learning performance. In this paper, we propose a\ngeneral predictor, named multi-view machines (MVMs), that can effectively\ninclude all the possible interactions between features from multiple views. A\njoint factorization is embedded for the full-order interaction parameters which\nallows parameter estimation under sparsity. Moreover, MVMs can work in\nconjunction with different loss functions for a variety of machine learning\ntasks. A stochastic gradient descent method is presented to learn the MVM\nmodel. We further illustrate the advantages of MVMs through comparison with\nother methods for multi-view classification, including support vector machines\n(SVMs), support tensor machines (STMs) and factorization machines (FMs).\n", "versions": [{"version": "v1", "created": "Wed, 3 Jun 2015 03:06:42 GMT"}, {"version": "v2", "created": "Fri, 23 Mar 2018 21:18:28 GMT"}], "update_date": "2018-03-28", "authors_parsed": [["Cao", "Bokai", ""], ["Zhou", "Hucheng", ""], ["Li", "Guoqiang", ""], ["Yu", "Philip S.", ""]]}, {"id": "1506.01113", "submitter": "Conrado Miranda", "authors": "Conrado Silva Miranda, Fernando Jos\\'e Von Zuben", "title": "Multi-Objective Optimization for Self-Adjusting Weighted Gradient in\n  Machine Learning Tasks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Much of the focus in machine learning research is placed in creating new\narchitectures and optimization methods, but the overall loss function is seldom\nquestioned. This paper interprets machine learning from a multi-objective\noptimization perspective, showing the limitations of the default linear\ncombination of loss functions over a data set and introducing the hypervolume\nindicator as an alternative. It is shown that the gradient of the hypervolume\nis defined by a self-adjusting weighted mean of the individual loss gradients,\nmaking it similar to the gradient of a weighted mean loss but without requiring\nthe weights to be defined a priori. This enables an inner boosting-like\nbehavior, where the current model is used to automatically place higher weights\non samples with higher losses but without requiring the use of multiple models.\nResults on a denoising autoencoder show that the new formulation is able to\nachieve better mean loss than the direct optimization of the mean loss,\nproviding evidence to the conjecture that self-adjusting the weights creates a\nsmoother loss surface.\n", "versions": [{"version": "v1", "created": "Wed, 3 Jun 2015 03:53:18 GMT"}, {"version": "v2", "created": "Tue, 21 Jul 2015 01:54:13 GMT"}], "update_date": "2015-07-22", "authors_parsed": [["Miranda", "Conrado Silva", ""], ["Von Zuben", "Fernando Jos\u00e9", ""]]}, {"id": "1506.01163", "submitter": "Yi-Hsiu Liao", "authors": "Yi-Hsiu Liao, Hung-Yi Lee, Lin-shan Lee", "title": "Towards Structured Deep Neural Network for Automatic Speech Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we propose the Structured Deep Neural Network (Structured DNN)\nas a structured and deep learning algorithm, learning to find the best\nstructured object (such as a label sequence) given a structured input (such as\na vector sequence) by globally considering the mapping relationships between\nthe structure rather than item by item.\n  When automatic speech recognition is viewed as a special case of such a\nstructured learning problem, where we have the acoustic vector sequence as the\ninput and the phoneme label sequence as the output, it becomes possible to\ncomprehensively learned utterance by utterance as a whole, rather than frame by\nframe.\n  Structured Support Vector Machine (structured SVM) was proposed to perform\nASR with structured learning previously, but limited by the linear nature of\nSVM. Here we propose structured DNN to use nonlinear transformations in\nmulti-layers as a structured and deep learning algorithm. It was shown to beat\nstructured SVM in preliminary experiments on TIMIT.\n", "versions": [{"version": "v1", "created": "Wed, 3 Jun 2015 08:41:05 GMT"}], "update_date": "2015-06-04", "authors_parsed": [["Liao", "Yi-Hsiu", ""], ["Lee", "Hung-Yi", ""], ["Lee", "Lin-shan", ""]]}, {"id": "1506.01186", "submitter": "Leslie Smith", "authors": "Leslie N. Smith", "title": "Cyclical Learning Rates for Training Neural Networks", "comments": "Presented at WACV 2017; see https://github.com/bckenstler/CLR for\n  instructions to implement CLR in Keras", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is known that the learning rate is the most important hyper-parameter to\ntune for training deep neural networks. This paper describes a new method for\nsetting the learning rate, named cyclical learning rates, which practically\neliminates the need to experimentally find the best values and schedule for the\nglobal learning rates. Instead of monotonically decreasing the learning rate,\nthis method lets the learning rate cyclically vary between reasonable boundary\nvalues. Training with cyclical learning rates instead of fixed values achieves\nimproved classification accuracy without a need to tune and often in fewer\niterations. This paper also describes a simple way to estimate \"reasonable\nbounds\" -- linearly increasing the learning rate of the network for a few\nepochs. In addition, cyclical learning rates are demonstrated on the CIFAR-10\nand CIFAR-100 datasets with ResNets, Stochastic Depth networks, and DenseNets,\nand the ImageNet dataset with the AlexNet and GoogLeNet architectures. These\nare practical tools for everyone who trains neural networks.\n", "versions": [{"version": "v1", "created": "Wed, 3 Jun 2015 09:54:31 GMT"}, {"version": "v2", "created": "Fri, 5 Jun 2015 20:40:18 GMT"}, {"version": "v3", "created": "Wed, 26 Oct 2016 19:07:58 GMT"}, {"version": "v4", "created": "Thu, 29 Dec 2016 15:20:01 GMT"}, {"version": "v5", "created": "Thu, 23 Mar 2017 11:38:19 GMT"}, {"version": "v6", "created": "Tue, 4 Apr 2017 11:34:46 GMT"}], "update_date": "2017-04-05", "authors_parsed": [["Smith", "Leslie N.", ""]]}, {"id": "1506.01192", "submitter": "Bo-Hsiang Tseng", "authors": "Bo-Hsiang Tseng, Hung-Yi Lee, and Lin-Shan Lee", "title": "Personalizing Universal Recurrent Neural Network Language Model with\n  User Characteristic Features by Social Network Crowdsouring", "comments": "IEEE Automatic Speech Recognition and Understanding Workshop (ASRU\n  2015), 13-17 Dec 2015, Scottsdale, Arizona, USA", "journal-ref": null, "doi": "10.1109/ASRU.2015.7404778", "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the popularity of mobile devices, personalized speech recognizer becomes\nmore realizable today and highly attractive. Each mobile device is primarily\nused by a single user, so it's possible to have a personalized recognizer well\nmatching to the characteristics of individual user. Although acoustic model\npersonalization has been investigated for decades, much less work have been\nreported on personalizing language model, probably because of the difficulties\nin collecting enough personalized corpora. Previous work used the corpora\ncollected from social networks to solve the problem, but constructing a\npersonalized model for each user is troublesome. In this paper, we propose a\nuniversal recurrent neural network language model with user characteristic\nfeatures, so all users share the same model, except each with different user\ncharacteristic features. These user characteristic features can be obtained by\ncrowdsouring over social networks, which include huge quantity of texts posted\nby users with known friend relationships, who may share some subject topics and\nwording patterns. The preliminary experiments on Facebook corpus showed that\nthis proposed approach not only drastically reduced the model perplexity, but\noffered very good improvement in recognition accuracy in n-best rescoring\ntests. This approach also mitigated the data sparseness problem for\npersonalized language models.\n", "versions": [{"version": "v1", "created": "Wed, 3 Jun 2015 10:14:21 GMT"}, {"version": "v2", "created": "Tue, 23 Aug 2016 03:40:47 GMT"}, {"version": "v3", "created": "Tue, 22 Nov 2016 10:12:12 GMT"}], "update_date": "2016-11-23", "authors_parsed": [["Tseng", "Bo-Hsiang", ""], ["Lee", "Hung-Yi", ""], ["Lee", "Lin-Shan", ""]]}, {"id": "1506.01195", "submitter": "Tianyi Liu", "authors": "Tianyi Liu, Shuangsang Fang, Yuehui Zhao, Peng Wang, Jun Zhang", "title": "Implementation of Training Convolutional Neural Networks", "comments": "10 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning refers to the shining branch of machine learning that is based\non learning levels of representations. Convolutional Neural Networks (CNN) is\none kind of deep neural network. It can study concurrently. In this article, we\ngave a detailed analysis of the process of CNN algorithm both the forward\nprocess and back propagation. Then we applied the particular convolutional\nneural network to implement the typical face recognition problem by java. Then,\na parallel strategy was proposed in section4. In addition, by measuring the\nactual time of forward and backward computing, we analysed the maximal speed up\nand parallel efficiency theoretically.\n", "versions": [{"version": "v1", "created": "Wed, 3 Jun 2015 10:18:49 GMT"}, {"version": "v2", "created": "Thu, 4 Jun 2015 02:10:39 GMT"}], "update_date": "2015-06-05", "authors_parsed": [["Liu", "Tianyi", ""], ["Fang", "Shuangsang", ""], ["Zhao", "Yuehui", ""], ["Wang", "Peng", ""], ["Zhang", "Jun", ""]]}, {"id": "1506.01326", "submitter": "Philipp Hennig PhD", "authors": "Philipp Hennig and Michael A Osborne and Mark Girolami", "title": "Probabilistic Numerics and Uncertainty in Computations", "comments": "Author Generated Postprint. 17 pages, 4 Figures, 1 Table", "journal-ref": null, "doi": "10.1098/rspa.2015.0142", "report-no": null, "categories": "math.NA cs.AI cs.LG stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We deliver a call to arms for probabilistic numerical methods: algorithms for\nnumerical tasks, including linear algebra, integration, optimization and\nsolving differential equations, that return uncertainties in their\ncalculations. Such uncertainties, arising from the loss of precision induced by\nnumerical calculation with limited time or hardware, are important for much\ncontemporary science and industry. Within applications such as climate science\nand astrophysics, the need to make decisions on the basis of computations with\nlarge and complex data has led to a renewed focus on the management of\nnumerical uncertainty. We describe how several seminal classic numerical\nmethods can be interpreted naturally as probabilistic inference. We then show\nthat the probabilistic view suggests new algorithms that can flexibly be\nadapted to suit application specifics, while delivering improved empirical\nperformance. We provide concrete illustrations of the benefits of probabilistic\nnumeric algorithms on real scientific problems from astrometry and astronomical\nimaging, while highlighting open problems with these new algorithms. Finally,\nwe describe how probabilistic numerical methods provide a coherent framework\nfor identifying the uncertainty in calculations performed with a combination of\nnumerical algorithms (e.g. both numerical optimisers and differential equation\nsolvers), potentially allowing the diagnosis (and control) of error sources in\ncomputations.\n", "versions": [{"version": "v1", "created": "Wed, 3 Jun 2015 17:45:01 GMT"}], "update_date": "2016-02-17", "authors_parsed": [["Hennig", "Philipp", ""], ["Osborne", "Michael A", ""], ["Girolami", "Mark", ""]]}, {"id": "1506.01330", "submitter": "Xiaojun Chang", "authors": "Sen Wang, Feiping Nie, Xiaojun Chang, Lina Yao, Xue Li, Quan Z. Sheng", "title": "Unsupervised Feature Analysis with Class Margin Optimization", "comments": "Accepted by European Conference on Machine Learning and Principles\n  and Practice of Knowledge Discovery in Databases, ECML/PKDD 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Unsupervised feature selection has been always attracting research attention\nin the communities of machine learning and data mining for decades. In this\npaper, we propose an unsupervised feature selection method seeking a feature\ncoefficient matrix to select the most distinctive features. Specifically, our\nproposed algorithm integrates the Maximum Margin Criterion with a\nsparsity-based model into a joint framework, where the class margin and feature\ncorrelation are taken into account at the same time. To maximize the total data\nseparability while preserving minimized within-class scatter simultaneously, we\npropose to embed Kmeans into the framework generating pseudo class label\ninformation in a scenario of unsupervised feature selection. Meanwhile, a\nsparsity-based model, ` 2 ,p-norm, is imposed to the regularization term to\neffectively discover the sparse structures of the feature coefficient matrix.\nIn this way, noisy and irrelevant features are removed by ruling out those\nfeatures whose corresponding coefficients are zeros. To alleviate the local\noptimum problem that is caused by random initializations of K-means, a\nconvergence guaranteed algorithm with an updating strategy for the clustering\nindicator matrix, is proposed to iteractively chase the optimal solution.\nPerformance evaluation is extensively conducted over six benchmark data sets.\nFrom plenty of experimental results, it is demonstrated that our method has\nsuperior performance against all other compared approaches.\n", "versions": [{"version": "v1", "created": "Wed, 3 Jun 2015 17:49:52 GMT"}], "update_date": "2015-06-04", "authors_parsed": [["Wang", "Sen", ""], ["Nie", "Feiping", ""], ["Chang", "Xiaojun", ""], ["Yao", "Lina", ""], ["Li", "Xue", ""], ["Sheng", "Quan Z.", ""]]}, {"id": "1506.01338", "submitter": "Hossein Keshavarz", "authors": "Hossein Keshavarz, Clayton Scott, XuanLong Nguyen", "title": "Optimal change point detection in Gaussian processes", "comments": "42 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.IT cs.LG math.IT stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of detecting a change in the mean of one-dimensional\nGaussian process data. This problem is investigated in the setting of\nincreasing domain (customarily employed in time series analysis) and in the\nsetting of fixed domain (typically arising in spatial data analysis). We\npropose a detection method based on the generalized likelihood ratio test\n(GLRT), and show that our method achieves nearly asymptotically optimal rate in\nthe minimax sense, in both settings. The salient feature of the proposed method\nis that it exploits in an efficient way the data dependence captured by the\nGaussian process covariance structure. When the covariance is not known, we\npropose the plug-in GLRT method and derive conditions under which the method\nremains asymptotically near optimal. By contrast, the standard CUSUM method,\nwhich does not account for the covariance structure, is shown to be\nasymptotically optimal only in the increasing domain. Our algorithms and\naccompanying theory are applicable to a wide variety of covariance structures,\nincluding the Matern class, the powered exponential class, and others. The\nplug-in GLRT method is shown to perform well for maximum likelihood estimators\nwith a dense covariance matrix.\n", "versions": [{"version": "v1", "created": "Wed, 3 Jun 2015 18:05:30 GMT"}, {"version": "v2", "created": "Sat, 8 Apr 2017 00:07:03 GMT"}], "update_date": "2017-04-11", "authors_parsed": [["Keshavarz", "Hossein", ""], ["Scott", "Clayton", ""], ["Nguyen", "XuanLong", ""]]}, {"id": "1506.01339", "submitter": "Jacob Whitehill", "authors": "Jacob Whitehill", "title": "Exploiting an Oracle that Reports AUC Scores in Machine Learning\n  Contests", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In machine learning contests such as the ImageNet Large Scale Visual\nRecognition Challenge and the KDD Cup, contestants can submit candidate\nsolutions and receive from an oracle (typically the organizers of the\ncompetition) the accuracy of their guesses compared to the ground-truth labels.\nOne of the most commonly used accuracy metrics for binary classification tasks\nis the Area Under the Receiver Operating Characteristics Curve (AUC). In this\npaper we provide proofs-of-concept of how knowledge of the AUC of a set of\nguesses can be used, in two different kinds of attacks, to improve the accuracy\nof those guesses. On the other hand, we also demonstrate the intractability of\none kind of AUC exploit by proving that the number of possible binary labelings\nof $n$ examples for which a candidate solution obtains a AUC score of $c$ grows\nexponentially in $n$, for every $c\\in (0,1)$.\n", "versions": [{"version": "v1", "created": "Wed, 3 Jun 2015 18:06:49 GMT"}, {"version": "v2", "created": "Fri, 13 Nov 2015 15:02:42 GMT"}], "update_date": "2015-11-16", "authors_parsed": [["Whitehill", "Jacob", ""]]}, {"id": "1506.01367", "submitter": "Ludwig Schmidt", "authors": "Jerry Li, Ludwig Schmidt", "title": "A Nearly Optimal and Agnostic Algorithm for Properly Learning a Mixture\n  of k Gaussians, for any Constant k", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.IT cs.LG math.IT math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning a Gaussian mixture model (GMM) is a fundamental problem in machine\nlearning, learning theory, and statistics. One notion of learning a GMM is\nproper learning: here, the goal is to find a mixture of $k$ Gaussians\n$\\mathcal{M}$ that is close to the density $f$ of the unknown distribution from\nwhich we draw samples. The distance between $\\mathcal{M}$ and $f$ is typically\nmeasured in the total variation or $L_1$-norm.\n  We give an algorithm for learning a mixture of $k$ univariate Gaussians that\nis nearly optimal for any fixed $k$. The sample complexity of our algorithm is\n$\\tilde{O}(\\frac{k}{\\epsilon^2})$ and the running time is $(k \\cdot\n\\log\\frac{1}{\\epsilon})^{O(k^4)} + \\tilde{O}(\\frac{k}{\\epsilon^2})$. It is\nwell-known that this sample complexity is optimal (up to logarithmic factors),\nand it was already achieved by prior work. However, the best known time\ncomplexity for proper learning a $k$-GMM was\n$\\tilde{O}(\\frac{1}{\\epsilon^{3k-1}})$. In particular, the dependence between\n$\\frac{1}{\\epsilon}$ and $k$ was exponential. We significantly improve this\ndependence by replacing the $\\frac{1}{\\epsilon}$ term with a $\\log\n\\frac{1}{\\epsilon}$ while only increasing the exponent moderately. Hence, for\nany fixed $k$, the $\\tilde{O} (\\frac{k}{\\epsilon^2})$ term dominates our\nrunning time, and thus our algorithm runs in time which is nearly-linear in the\nnumber of samples drawn. Achieving a running time of $\\textrm{poly}(k,\n\\frac{1}{\\epsilon})$ for proper learning of $k$-GMMs has recently been stated\nas an open problem by multiple researchers, and we make progress on this\nquestion.\n  Moreover, our approach offers an agnostic learning guarantee: our algorithm\nreturns a good GMM even if the distribution we are sampling from is not a\nmixture of Gaussians. To the best of our knowledge, our algorithm is the first\nagnostic proper learning algorithm for GMMs.\n", "versions": [{"version": "v1", "created": "Wed, 3 Jun 2015 19:55:10 GMT"}], "update_date": "2015-06-04", "authors_parsed": [["Li", "Jerry", ""], ["Schmidt", "Ludwig", ""]]}, {"id": "1506.01490", "submitter": "Chun-Liang Li", "authors": "Chun-Liang Li, Hsuan-Tien Lin, Chi-Jen Lu", "title": "Rivalry of Two Families of Algorithms for Memory-Restricted Streaming\n  PCA", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of recovering the subspace spanned by the first $k$\nprincipal components of $d$-dimensional data under the streaming setting, with\na memory bound of $O(kd)$. Two families of algorithms are known for this\nproblem. The first family is based on the framework of stochastic gradient\ndescent. Nevertheless, the convergence rate of the family can be seriously\naffected by the learning rate of the descent steps and deserves more serious\nstudy. The second family is based on the power method over blocks of data, but\nsetting the block size for its existing algorithms is not an easy task. In this\npaper, we analyze the convergence rate of a representative algorithm with\ndecayed learning rate (Oja and Karhunen, 1985) in the first family for the\ngeneral $k>1$ case. Moreover, we propose a novel algorithm for the second\nfamily that sets the block sizes automatically and dynamically with faster\nconvergence rate. We then conduct empirical studies that fairly compare the two\nfamilies on real-world data. The studies reveal the advantages and\ndisadvantages of these two families.\n", "versions": [{"version": "v1", "created": "Thu, 4 Jun 2015 07:36:57 GMT"}, {"version": "v2", "created": "Mon, 12 Oct 2015 02:19:30 GMT"}], "update_date": "2015-10-13", "authors_parsed": [["Li", "Chun-Liang", ""], ["Lin", "Hsuan-Tien", ""], ["Lu", "Chi-Jen", ""]]}, {"id": "1506.01520", "submitter": "Brendan van Rooyen", "authors": "Brendan van Rooyen, Aditya Krishna Menon, Robert C. Williamson", "title": "An Average Classification Algorithm", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many classification algorithms produce a classifier that is a weighted\naverage of kernel evaluations. When working with a high or infinite dimensional\nkernel, it is imperative for speed of evaluation and storage issues that as few\ntraining samples as possible are used in the kernel expansion. Popular existing\napproaches focus on altering standard learning algorithms, such as the Support\nVector Machine, to induce sparsity, as well as post-hoc procedures for sparse\napproximations. Here we adopt the latter approach. We begin with a very simple\nclassifier, given by the kernel mean $$ f(x) = \\frac{1}{n}\n\\sum\\limits_{i=i}^{n} y_i K(x_i,x) $$ We then find a sparse approximation to\nthis kernel mean via herding. The result is an accurate, easily parallelized\nalgorithm for learning classifiers.\n", "versions": [{"version": "v1", "created": "Thu, 4 Jun 2015 09:38:23 GMT"}, {"version": "v2", "created": "Fri, 5 Jun 2015 22:29:36 GMT"}, {"version": "v3", "created": "Tue, 15 Dec 2015 22:47:53 GMT"}], "update_date": "2015-12-17", "authors_parsed": [["van Rooyen", "Brendan", ""], ["Menon", "Aditya Krishna", ""], ["Williamson", "Robert C.", ""]]}, {"id": "1506.01709", "submitter": "H\\'ector P. Mart\\'inez", "authors": "Vincent E. Farrugia, H\\'ector P. Mart\\'inez, Georgios N. Yannakakis", "title": "The Preference Learning Toolbox", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Preference learning (PL) is a core area of machine learning that handles\ndatasets with ordinal relations. As the number of generated data of ordinal\nnature is increasing, the importance and role of the PL field becomes central\nwithin machine learning research and practice. This paper introduces an open\nsource, scalable, efficient and accessible preference learning toolbox that\nsupports the key phases of the data training process incorporating various\npopular data preprocessing, feature selection and preference learning methods.\n", "versions": [{"version": "v1", "created": "Thu, 4 Jun 2015 19:58:56 GMT"}], "update_date": "2015-06-05", "authors_parsed": [["Farrugia", "Vincent E.", ""], ["Mart\u00ednez", "H\u00e9ctor P.", ""], ["Yannakakis", "Georgios N.", ""]]}, {"id": "1506.01744", "submitter": "Kevin Chen", "authors": "Chicheng Zhang, Jimin Song, Kevin C Chen, Kamalika Chaudhuri", "title": "Spectral Learning of Large Structured HMMs for Comparative Epigenomics", "comments": "27 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.ST q-bio.GN stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop a latent variable model and an efficient spectral algorithm\nmotivated by the recent emergence of very large data sets of chromatin marks\nfrom multiple human cell types. A natural model for chromatin data in one cell\ntype is a Hidden Markov Model (HMM); we model the relationship between multiple\ncell types by connecting their hidden states by a fixed tree of known\nstructure. The main challenge with learning parameters of such models is that\niterative methods such as EM are very slow, while naive spectral methods result\nin time and space complexity exponential in the number of cell types. We\nexploit properties of the tree structure of the hidden states to provide\nspectral algorithms that are more computationally efficient for current\nbiological datasets. We provide sample complexity bounds for our algorithm and\nevaluate it experimentally on biological data from nine human cell types.\nFinally, we show that beyond our specific model, some of our algorithmic ideas\ncan be applied to other graphical models.\n", "versions": [{"version": "v1", "created": "Thu, 4 Jun 2015 22:57:28 GMT"}], "update_date": "2015-06-09", "authors_parsed": [["Zhang", "Chicheng", ""], ["Song", "Jimin", ""], ["Chen", "Kevin C", ""], ["Chaudhuri", "Kamalika", ""]]}, {"id": "1506.01829", "submitter": "Remi Lajugie", "authors": "R\\'emi Lajugie (SIERRA, DI-ENS), Piotr Bojanowski (WILLOW, DI-ENS),\n  Sylvain Arlot (SIERRA, DI-ENS), Francis Bach (SIERRA, DI-ENS)", "title": "Semidefinite and Spectral Relaxations for Multi-Label Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we address the problem of multi-label classification. We\nconsider linear classifiers and propose to learn a prior over the space of\nlabels to directly leverage the performance of such methods. This prior takes\nthe form of a quadratic function of the labels and permits to encode both\nattractive and repulsive relations between labels. We cast this problem as a\nstructured prediction one aiming at optimizing either the accuracies of the\npredictors or the F 1-score. This leads to an optimization problem closely\nrelated to the max-cut problem, which naturally leads to semidefinite and\nspectral relaxations. We show on standard datasets how such a general prior can\nimprove the performances of multi-label techniques.\n", "versions": [{"version": "v1", "created": "Fri, 5 Jun 2015 09:19:01 GMT"}], "update_date": "2015-06-08", "authors_parsed": [["Lajugie", "R\u00e9mi", "", "SIERRA, DI-ENS"], ["Bojanowski", "Piotr", "", "WILLOW, DI-ENS"], ["Arlot", "Sylvain", "", "SIERRA, DI-ENS"], ["Bach", "Francis", "", "SIERRA, DI-ENS"]]}, {"id": "1506.01900", "submitter": "Yossi Arjevani", "authors": "Yossi Arjevani and Ohad Shamir", "title": "Communication Complexity of Distributed Convex Learning and Optimization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the fundamental limits to communication-efficient distributed\nmethods for convex learning and optimization, under different assumptions on\nthe information available to individual machines, and the types of functions\nconsidered. We identify cases where existing algorithms are already worst-case\noptimal, as well as cases where room for further improvement is still possible.\nAmong other things, our results indicate that without similarity between the\nlocal objective functions (due to statistical data similarity or otherwise)\nmany communication rounds may be required, even if the machines have unbounded\ncomputational power.\n", "versions": [{"version": "v1", "created": "Fri, 5 Jun 2015 13:24:17 GMT"}, {"version": "v2", "created": "Wed, 28 Oct 2015 19:02:22 GMT"}], "update_date": "2015-10-29", "authors_parsed": [["Arjevani", "Yossi", ""], ["Shamir", "Ohad", ""]]}, {"id": "1506.01911", "submitter": "Lionel Pigou", "authors": "Lionel Pigou, A\\\"aron van den Oord, Sander Dieleman, Mieke Van\n  Herreweghe, Joni Dambre", "title": "Beyond Temporal Pooling: Recurrence and Temporal Convolutions for\n  Gesture Recognition in Video", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent studies have demonstrated the power of recurrent neural networks for\nmachine translation, image captioning and speech recognition. For the task of\ncapturing temporal structure in video, however, there still remain numerous\nopen research questions. Current research suggests using a simple temporal\nfeature pooling strategy to take into account the temporal aspect of video. We\ndemonstrate that this method is not sufficient for gesture recognition, where\ntemporal information is more discriminative compared to general video\nclassification tasks. We explore deep architectures for gesture recognition in\nvideo and propose a new end-to-end trainable neural network architecture\nincorporating temporal convolutions and bidirectional recurrence. Our main\ncontributions are twofold; first, we show that recurrence is crucial for this\ntask; second, we show that adding temporal convolutions leads to significant\nimprovements. We evaluate the different approaches on the Montalbano gesture\nrecognition dataset, where we achieve state-of-the-art results.\n", "versions": [{"version": "v1", "created": "Fri, 5 Jun 2015 13:43:01 GMT"}, {"version": "v2", "created": "Mon, 9 Nov 2015 16:20:26 GMT"}, {"version": "v3", "created": "Wed, 10 Feb 2016 16:50:29 GMT"}], "update_date": "2016-02-11", "authors_parsed": [["Pigou", "Lionel", ""], ["Oord", "A\u00e4ron van den", ""], ["Dieleman", "Sander", ""], ["Van Herreweghe", "Mieke", ""], ["Dambre", "Joni", ""]]}, {"id": "1506.01972", "submitter": "Zeyuan Allen-Zhu", "authors": "Zeyuan Allen-Zhu, Yang Yuan", "title": "Improved SVRG for Non-Strongly-Convex or Sum-of-Non-Convex Objectives", "comments": "improved writing and included more experiments in this version", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many classical algorithms are found until several years later to outlive the\nconfines in which they were conceived, and continue to be relevant in\nunforeseen settings. In this paper, we show that SVRG is one such method: being\noriginally designed for strongly convex objectives, it is also very robust in\nnon-strongly convex or sum-of-non-convex settings.\n  More precisely, we provide new analysis to improve the state-of-the-art\nrunning times in both settings by either applying SVRG or its novel variant.\nSince non-strongly convex objectives include important examples such as Lasso\nor logistic regression, and sum-of-non-convex objectives include famous\nexamples such as stochastic PCA and is even believed to be related to training\ndeep neural nets, our results also imply better performances in these\napplications.\n", "versions": [{"version": "v1", "created": "Fri, 5 Jun 2015 17:00:43 GMT"}, {"version": "v2", "created": "Fri, 5 Feb 2016 20:55:39 GMT"}, {"version": "v3", "created": "Fri, 27 May 2016 19:14:20 GMT"}], "update_date": "2016-05-30", "authors_parsed": [["Allen-Zhu", "Zeyuan", ""], ["Yuan", "Yang", ""]]}, {"id": "1506.02075", "submitter": "Antoine Bordes", "authors": "Antoine Bordes, Nicolas Usunier, Sumit Chopra, Jason Weston", "title": "Large-scale Simple Question Answering with Memory Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Training large-scale question answering systems is complicated because\ntraining sources usually cover a small portion of the range of possible\nquestions. This paper studies the impact of multitask and transfer learning for\nsimple question answering; a setting for which the reasoning required to answer\nis quite easy, as long as one can retrieve the correct evidence given a\nquestion, which can be difficult in large-scale conditions. To this end, we\nintroduce a new dataset of 100k questions that we use in conjunction with\nexisting benchmarks. We conduct our study within the framework of Memory\nNetworks (Weston et al., 2015) because this perspective allows us to eventually\nscale up to more complex reasoning, and show that Memory Networks can be\nsuccessfully trained to achieve excellent performance.\n", "versions": [{"version": "v1", "created": "Fri, 5 Jun 2015 21:48:39 GMT"}], "update_date": "2015-06-09", "authors_parsed": [["Bordes", "Antoine", ""], ["Usunier", "Nicolas", ""], ["Chopra", "Sumit", ""], ["Weston", "Jason", ""]]}, {"id": "1506.02078", "submitter": "Andrej Karpathy", "authors": "Andrej Karpathy, Justin Johnson, Li Fei-Fei", "title": "Visualizing and Understanding Recurrent Networks", "comments": "changing style, adding references, minor changes to text", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recurrent Neural Networks (RNNs), and specifically a variant with Long\nShort-Term Memory (LSTM), are enjoying renewed interest as a result of\nsuccessful applications in a wide range of machine learning problems that\ninvolve sequential data. However, while LSTMs provide exceptional results in\npractice, the source of their performance and their limitations remain rather\npoorly understood. Using character-level language models as an interpretable\ntestbed, we aim to bridge this gap by providing an analysis of their\nrepresentations, predictions and error types. In particular, our experiments\nreveal the existence of interpretable cells that keep track of long-range\ndependencies such as line lengths, quotes and brackets. Moreover, our\ncomparative analysis with finite horizon n-gram models traces the source of the\nLSTM improvements to long-range structural dependencies. Finally, we provide\nanalysis of the remaining errors and suggests areas for further study.\n", "versions": [{"version": "v1", "created": "Fri, 5 Jun 2015 22:33:04 GMT"}, {"version": "v2", "created": "Tue, 17 Nov 2015 02:42:24 GMT"}], "update_date": "2015-11-18", "authors_parsed": [["Karpathy", "Andrej", ""], ["Johnson", "Justin", ""], ["Fei-Fei", "Li", ""]]}, {"id": "1506.02080", "submitter": "Ruben Martinez-Cantin", "authors": "Ruben Martinez-Cantin", "title": "Local Nonstationarity for Efficient Bayesian Optimization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bayesian optimization has shown to be a fundamental global optimization\nalgorithm in many applications: ranging from automatic machine learning,\nrobotics, reinforcement learning, experimental design, simulations, etc. The\nmost popular and effective Bayesian optimization relies on a surrogate model in\nthe form of a Gaussian process due to its flexibility to represent a prior over\nfunction. However, many algorithms and setups relies on the stationarity\nassumption of the Gaussian process. In this paper, we present a novel\nnonstationary strategy for Bayesian optimization that is able to outperform the\nstate of the art in Bayesian optimization both in stationary and nonstationary\nproblems.\n", "versions": [{"version": "v1", "created": "Fri, 5 Jun 2015 22:36:15 GMT"}], "update_date": "2015-06-09", "authors_parsed": [["Martinez-Cantin", "Ruben", ""]]}, {"id": "1506.02085", "submitter": "Min Xu", "authors": "Min Xu, Rudy Setiono", "title": "Gene selection for cancer classification using a hybrid of univariate\n  and multivariate feature selection methods", "comments": null, "journal-ref": "Applied Genomics and Proteomics. 2003:2(2)79-91", "doi": null, "report-no": null, "categories": "q-bio.QM cs.CE cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Various approaches to gene selection for cancer classification based on\nmicroarray data can be found in the literature and they may be grouped into two\ncategories: univariate methods and multivariate methods. Univariate methods\nlook at each gene in the data in isolation from others. They measure the\ncontribution of a particular gene to the classification without considering the\npresence of the other genes. In contrast, multivariate methods measure the\nrelative contribution of a gene to the classification by taking the other genes\nin the data into consideration. Multivariate methods select fewer genes in\ngeneral. However, the selection process of multivariate methods may be\nsensitive to the presence of irrelevant genes, noises in the expression and\noutliers in the training data. At the same time, the computational cost of\nmultivariate methods is high. To overcome the disadvantages of the two types of\napproaches, we propose a hybrid method to obtain gene sets that are small and\nhighly discriminative.\n  We devise our hybrid method from the univariate Maximum Likelihood method\n(LIK) and the multivariate Recursive Feature Elimination method (RFE). We\nanalyze the properties of these methods and systematically test the\neffectiveness of our proposed method on two cancer microarray datasets. Our\nexperiments on a leukemia dataset and a small, round blue cell tumors dataset\ndemonstrate the effectiveness of our hybrid method. It is able to discover sets\nconsisting of fewer genes than those reported in the literature and at the same\ntime achieve the same or better prediction accuracy.\n", "versions": [{"version": "v1", "created": "Fri, 5 Jun 2015 23:29:06 GMT"}], "update_date": "2015-06-18", "authors_parsed": [["Xu", "Min", ""], ["Setiono", "Rudy", ""]]}, {"id": "1506.02087", "submitter": "Min Xu", "authors": "Min Xu", "title": "Global Gene Expression Analysis Using Machine Learning Methods", "comments": "Author's master thesis (National University of Singapore, May 2003).\n  Adviser: Rudy Setiono", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.QM cs.CE cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Microarray is a technology to quantitatively monitor the expression of large\nnumber of genes in parallel. It has become one of the main tools for global\ngene expression analysis in molecular biology research in recent years. The\nlarge amount of expression data generated by this technology makes the study of\ncertain complex biological problems possible and machine learning methods are\nplaying a crucial role in the analysis process. At present, many machine\nlearning methods have been or have the potential to be applied to major areas\nof gene expression analysis. These areas include clustering, classification,\ndynamic modeling and reverse engineering.\n  In this thesis, we focus our work on using machine learning methods to solve\nthe classification problems arising from microarray data. We first identify the\nmajor types of the classification problems; then apply several machine learning\nmethods to solve the problems and perform systematic tests on real and\nartificial datasets. We propose improvement to existing methods. Specifically,\nwe develop a multivariate and a hybrid feature selection method to obtain high\nclassification performance for high dimension classification problems. Using\nthe hybrid feature selection method, we are able to identify small sets of\nfeatures that give predictive accuracy that is as good as that from other\nmethods which require many more features.\n", "versions": [{"version": "v1", "created": "Fri, 5 Jun 2015 23:37:20 GMT"}], "update_date": "2015-06-18", "authors_parsed": [["Xu", "Min", ""]]}, {"id": "1506.02107", "submitter": "Jie Ding", "authors": "Jie Ding, Mohammad Noshad, and Vahid Tarokh", "title": "Data-Driven Learning of the Number of States in Multi-State\n  Autoregressive Models", "comments": "This paper will appear in the Proceedings of 53rd Annual Allerton\n  Conference on Communication, Control, and Computing, 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we consider the class of multi-state autoregressive processes\nthat can be used to model non-stationary time-series of interest. In order to\ncapture different autoregressive (AR) states underlying an observed time\nseries, it is crucial to select the appropriate number of states. We propose a\nnew model selection technique based on the Gap statistics, which uses a null\nreference distribution on the stable AR filters to check whether adding a new\nAR state significantly improves the performance of the model. To that end, we\ndefine a new distance measure between AR filters based on mean squared\nprediction error (MSPE), and propose an efficient method to generate random\nstable filters that are uniformly distributed in the coefficient space.\nNumerical results are provided to evaluate the performance of the proposed\napproach.\n", "versions": [{"version": "v1", "created": "Sat, 6 Jun 2015 02:47:24 GMT"}, {"version": "v2", "created": "Wed, 16 Sep 2015 15:54:13 GMT"}, {"version": "v3", "created": "Sat, 10 Oct 2015 00:25:31 GMT"}], "update_date": "2015-10-13", "authors_parsed": [["Ding", "Jie", ""], ["Noshad", "Mohammad", ""], ["Tarokh", "Vahid", ""]]}, {"id": "1506.02108", "submitter": "Chunhua Shen", "authors": "Guosheng Lin, Chunhua Shen, Ian Reid, Anton van den Hengel", "title": "Deeply Learning the Messages in Message Passing Inference", "comments": "11 pages. Appearing in Proc. The Twenty-ninth Annual Conference on\n  Neural Information Processing Systems (NIPS), 2015, Montreal, Canada", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep structured output learning shows great promise in tasks like semantic\nimage segmentation. We proffer a new, efficient deep structured model learning\nscheme, in which we show how deep Convolutional Neural Networks (CNNs) can be\nused to estimate the messages in message passing inference for structured\nprediction with Conditional Random Fields (CRFs). With such CNN message\nestimators, we obviate the need to learn or evaluate potential functions for\nmessage calculation. This confers significant efficiency for learning, since\notherwise when performing structured learning for a CRF with CNN potentials it\nis necessary to undertake expensive inference for every stochastic gradient\niteration. The network output dimension for message estimation is the same as\nthe number of classes, in contrast to the network output for general CNN\npotential functions in CRFs, which is exponential in the order of the\npotentials. Hence CNN message learning has fewer network parameters and is more\nscalable for cases that a large number of classes are involved. We apply our\nmethod to semantic image segmentation on the PASCAL VOC 2012 dataset. We\nachieve an intersection-over-union score of 73.4 on its test set, which is the\nbest reported result for methods using the VOC training images alone. This\nimpressive performance demonstrates the effectiveness and usefulness of our CNN\nmessage learning method.\n", "versions": [{"version": "v1", "created": "Sat, 6 Jun 2015 02:52:38 GMT"}, {"version": "v2", "created": "Wed, 10 Jun 2015 06:49:06 GMT"}, {"version": "v3", "created": "Tue, 8 Sep 2015 04:29:45 GMT"}], "update_date": "2015-09-09", "authors_parsed": [["Lin", "Guosheng", ""], ["Shen", "Chunhua", ""], ["Reid", "Ian", ""], ["Hengel", "Anton van den", ""]]}, {"id": "1506.02113", "submitter": "David Chickering", "authors": "David Maxwell Chickering and Christopher Meek", "title": "Selective Greedy Equivalence Search: Finding Optimal Bayesian Networks\n  Using a Polynomial Number of Score Evaluations", "comments": "Full version of UAI paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce Selective Greedy Equivalence Search (SGES), a restricted version\nof Greedy Equivalence Search (GES). SGES retains the asymptotic correctness of\nGES but, unlike GES, has polynomial performance guarantees. In particular, we\nshow that when data are sampled independently from a distribution that is\nperfect with respect to a DAG ${\\cal G}$ defined over the observable variables\nthen, in the limit of large data, SGES will identify ${\\cal G}$'s equivalence\nclass after a number of score evaluations that is (1) polynomial in the number\nof nodes and (2) exponential in various complexity measures including\nmaximum-number-of-parents, maximum-clique-size, and a new measure called {\\em\nv-width} that is at least as small as---and potentially much smaller than---the\nother two. More generally, we show that for any hereditary and\nequivalence-invariant property $\\Pi$ known to hold in ${\\cal G}$, we retain the\nlarge-sample optimality guarantees of GES even if we ignore any GES deletion\noperator during the backward phase that results in a state for which $\\Pi$ does\nnot hold in the common-descendants subgraph.\n", "versions": [{"version": "v1", "created": "Sat, 6 Jun 2015 03:56:44 GMT"}], "update_date": "2015-06-09", "authors_parsed": [["Chickering", "David Maxwell", ""], ["Meek", "Christopher", ""]]}, {"id": "1506.02117", "submitter": "Mingsheng Long", "authors": "Mingsheng Long, Zhangjie Cao, Jianmin Wang, Philip S. Yu", "title": "Learning Multiple Tasks with Multilinear Relationship Networks", "comments": "NIPS 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep networks trained on large-scale data can learn transferable features to\npromote learning multiple tasks. Since deep features eventually transition from\ngeneral to specific along deep networks, a fundamental problem of multi-task\nlearning is how to exploit the task relatedness underlying parameter tensors\nand improve feature transferability in the multiple task-specific layers. This\npaper presents Multilinear Relationship Networks (MRN) that discover the task\nrelationships based on novel tensor normal priors over parameter tensors of\nmultiple task-specific layers in deep convolutional networks. By jointly\nlearning transferable features and multilinear relationships of tasks and\nfeatures, MRN is able to alleviate the dilemma of negative-transfer in the\nfeature layers and under-transfer in the classifier layer. Experiments show\nthat MRN yields state-of-the-art results on three multi-task learning datasets.\n", "versions": [{"version": "v1", "created": "Sat, 6 Jun 2015 04:38:48 GMT"}, {"version": "v2", "created": "Thu, 16 Feb 2017 07:46:11 GMT"}, {"version": "v3", "created": "Fri, 26 May 2017 00:27:14 GMT"}, {"version": "v4", "created": "Mon, 6 Nov 2017 14:56:12 GMT"}], "update_date": "2017-11-07", "authors_parsed": [["Long", "Mingsheng", ""], ["Cao", "Zhangjie", ""], ["Wang", "Jianmin", ""], ["Yu", "Philip S.", ""]]}, {"id": "1506.02142", "submitter": "Yarin Gal", "authors": "Yarin Gal, Zoubin Ghahramani", "title": "Dropout as a Bayesian Approximation: Representing Model Uncertainty in\n  Deep Learning", "comments": "12 pages, 6 figures; fixed a mistake with standard error and added a\n  new table with updated results (marked \"Update [October 2016]\"); Published in\n  ICML 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning tools have gained tremendous attention in applied machine\nlearning. However such tools for regression and classification do not capture\nmodel uncertainty. In comparison, Bayesian models offer a mathematically\ngrounded framework to reason about model uncertainty, but usually come with a\nprohibitive computational cost. In this paper we develop a new theoretical\nframework casting dropout training in deep neural networks (NNs) as approximate\nBayesian inference in deep Gaussian processes. A direct result of this theory\ngives us tools to model uncertainty with dropout NNs -- extracting information\nfrom existing models that has been thrown away so far. This mitigates the\nproblem of representing uncertainty in deep learning without sacrificing either\ncomputational complexity or test accuracy. We perform an extensive study of the\nproperties of dropout's uncertainty. Various network architectures and\nnon-linearities are assessed on tasks of regression and classification, using\nMNIST as an example. We show a considerable improvement in predictive\nlog-likelihood and RMSE compared to existing state-of-the-art methods, and\nfinish by using dropout's uncertainty in deep reinforcement learning.\n", "versions": [{"version": "v1", "created": "Sat, 6 Jun 2015 12:30:43 GMT"}, {"version": "v2", "created": "Thu, 27 Aug 2015 13:39:15 GMT"}, {"version": "v3", "created": "Sun, 27 Sep 2015 15:15:31 GMT"}, {"version": "v4", "created": "Sat, 31 Oct 2015 19:45:05 GMT"}, {"version": "v5", "created": "Wed, 25 May 2016 18:48:52 GMT"}, {"version": "v6", "created": "Tue, 4 Oct 2016 16:50:26 GMT"}], "update_date": "2016-10-05", "authors_parsed": [["Gal", "Yarin", ""], ["Ghahramani", "Zoubin", ""]]}, {"id": "1506.02155", "submitter": "Zoltan Szabo", "authors": "Bharath K. Sriperumbudur and Zoltan Szabo", "title": "Optimal Rates for Random Fourier Features", "comments": "To appear at NIPS-2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.LG math.FA stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Kernel methods represent one of the most powerful tools in machine learning\nto tackle problems expressed in terms of function values and derivatives due to\ntheir capability to represent and model complex relations. While these methods\nshow good versatility, they are computationally intensive and have poor\nscalability to large data as they require operations on Gram matrices. In order\nto mitigate this serious computational limitation, recently randomized\nconstructions have been proposed in the literature, which allow the application\nof fast linear algorithms. Random Fourier features (RFF) are among the most\npopular and widely applied constructions: they provide an easily computable,\nlow-dimensional feature representation for shift-invariant kernels. Despite the\npopularity of RFFs, very little is understood theoretically about their\napproximation quality. In this paper, we provide a detailed finite-sample\ntheoretical analysis about the approximation quality of RFFs by (i)\nestablishing optimal (in terms of the RFF dimension, and growing set size)\nperformance guarantees in uniform norm, and (ii) presenting guarantees in $L^r$\n($1\\le r<\\infty$) norms. We also propose an RFF approximation to derivatives of\na kernel with a theoretical study on its approximation quality.\n", "versions": [{"version": "v1", "created": "Sat, 6 Jun 2015 14:37:01 GMT"}, {"version": "v2", "created": "Wed, 4 Nov 2015 22:58:57 GMT"}], "update_date": "2015-11-06", "authors_parsed": [["Sriperumbudur", "Bharath K.", ""], ["Szabo", "Zoltan", ""]]}, {"id": "1506.02158", "submitter": "Yarin Gal", "authors": "Yarin Gal, Zoubin Ghahramani", "title": "Bayesian Convolutional Neural Networks with Bernoulli Approximate\n  Variational Inference", "comments": "12 pages, 3 figures, ICLR format, updated with reviewer comments", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolutional neural networks (CNNs) work well on large datasets. But\nlabelled data is hard to collect, and in some applications larger amounts of\ndata are not available. The problem then is how to use CNNs with small data --\nas CNNs overfit quickly. We present an efficient Bayesian CNN, offering better\nrobustness to over-fitting on small data than traditional approaches. This is\nby placing a probability distribution over the CNN's kernels. We approximate\nour model's intractable posterior with Bernoulli variational distributions,\nrequiring no additional model parameters.\n  On the theoretical side, we cast dropout network training as approximate\ninference in Bayesian neural networks. This allows us to implement our model\nusing existing tools in deep learning with no increase in time complexity,\nwhile highlighting a negative result in the field. We show a considerable\nimprovement in classification accuracy compared to standard techniques and\nimprove on published state-of-the-art results for CIFAR-10.\n", "versions": [{"version": "v1", "created": "Sat, 6 Jun 2015 14:43:40 GMT"}, {"version": "v2", "created": "Thu, 27 Aug 2015 13:30:17 GMT"}, {"version": "v3", "created": "Sun, 27 Sep 2015 13:34:58 GMT"}, {"version": "v4", "created": "Mon, 2 Nov 2015 14:33:59 GMT"}, {"version": "v5", "created": "Mon, 30 Nov 2015 21:22:15 GMT"}, {"version": "v6", "created": "Mon, 18 Jan 2016 20:42:07 GMT"}], "update_date": "2016-01-19", "authors_parsed": [["Gal", "Yarin", ""], ["Ghahramani", "Zoubin", ""]]}, {"id": "1506.02159", "submitter": "Bamdev Mishra", "authors": "Hiroyuki Kasai and Bamdev Mishra", "title": "Riemannian preconditioning for tensor completion", "comments": "Supplementary material included in the paper. An extension of the\n  paper is in arXiv:1605.08257", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NA cs.LG math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel Riemannian preconditioning approach for the tensor\ncompletion problem with rank constraint. A Riemannian metric or inner product\nis proposed that exploits the least-squares structure of the cost function and\ntakes into account the structured symmetry in Tucker decomposition. The\nspecific metric allows to use the versatile framework of Riemannian\noptimization on quotient manifolds to develop a preconditioned nonlinear\nconjugate gradient algorithm for the problem. To this end, concrete matrix\nrepresentations of various optimization-related ingredients are listed.\nNumerical comparisons suggest that our proposed algorithm robustly outperforms\nstate-of-the-art algorithms across different problem instances encompassing\nvarious synthetic and real-world datasets.\n", "versions": [{"version": "v1", "created": "Sat, 6 Jun 2015 14:52:13 GMT"}, {"version": "v2", "created": "Fri, 27 May 2016 17:28:32 GMT"}], "update_date": "2016-05-30", "authors_parsed": [["Kasai", "Hiroyuki", ""], ["Mishra", "Bamdev", ""]]}, {"id": "1506.02162", "submitter": "Shahin Jabbari", "authors": "Shahin Jabbari, Ryan Rogers, Aaron Roth, Zhiwei Steven Wu", "title": "Learning from Rational Behavior: Predicting Solutions to Unknown Linear\n  Programs", "comments": "The short version of this paper appears in the proceedings of NIPS-16", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.GT cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We define and study the problem of predicting the solution to a linear\nprogram (LP) given only partial information about its objective and\nconstraints. This generalizes the problem of learning to predict the purchasing\nbehavior of a rational agent who has an unknown objective function, that has\nbeen studied under the name \"Learning from Revealed Preferences\". We give\nmistake bound learning algorithms in two settings: in the first, the objective\nof the LP is known to the learner but there is an arbitrary, fixed set of\nconstraints which are unknown. Each example is defined by an additional known\nconstraint and the goal of the learner is to predict the optimal solution of\nthe LP given the union of the known and unknown constraints. This models the\nproblem of predicting the behavior of a rational agent whose goals are known,\nbut whose resources are unknown. In the second setting, the objective of the LP\nis unknown, and changing in a controlled way. The constraints of the LP may\nalso change every day, but are known. An example is given by a set of\nconstraints and partial information about the objective, and the task of the\nlearner is again to predict the optimal solution of the partially known LP.\n", "versions": [{"version": "v1", "created": "Sat, 6 Jun 2015 15:10:25 GMT"}, {"version": "v2", "created": "Sun, 8 May 2016 20:07:43 GMT"}, {"version": "v3", "created": "Wed, 26 Oct 2016 16:41:01 GMT"}], "update_date": "2016-10-27", "authors_parsed": [["Jabbari", "Shahin", ""], ["Rogers", "Ryan", ""], ["Roth", "Aaron", ""], ["Wu", "Zhiwei Steven", ""]]}, {"id": "1506.02190", "submitter": "Lei Tang", "authors": "Lei Tang", "title": "Thresholding for Top-k Recommendation with Temporal Dynamics", "comments": "10 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work focuses on top-k recommendation in domains where underlying data\ndistribution shifts overtime. We propose to learn a time-dependent bias for\neach item over whatever existing recommendation engine. Such a bias learning\nprocess alleviates data sparsity in constructing the engine, and at the same\ntime captures recent trend shift observed in data. We present an alternating\noptimization framework to resolve the bias learning problem, and develop\nmethods to handle a variety of commonly used recommendation evaluation\ncriteria, as well as large number of items and users in practice. The proposed\nalgorithm is examined, both offline and online, using real world data sets\ncollected from the largest retailer worldwide. Empirical results demonstrate\nthat the bias learning can almost always boost recommendation performance. We\nencourage other practitioners to adopt it as a standard component in\nrecommender systems where temporal dynamics is a norm.\n", "versions": [{"version": "v1", "created": "Sat, 6 Jun 2015 20:13:28 GMT"}, {"version": "v2", "created": "Mon, 9 Nov 2015 05:37:07 GMT"}], "update_date": "2015-11-10", "authors_parsed": [["Tang", "Lei", ""]]}, {"id": "1506.02216", "submitter": "Junyoung Chung", "authors": "Junyoung Chung, Kyle Kastner, Laurent Dinh, Kratarth Goel, Aaron\n  Courville, Yoshua Bengio", "title": "A Recurrent Latent Variable Model for Sequential Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we explore the inclusion of latent random variables into the\ndynamic hidden state of a recurrent neural network (RNN) by combining elements\nof the variational autoencoder. We argue that through the use of high-level\nlatent random variables, the variational RNN (VRNN)1 can model the kind of\nvariability observed in highly structured sequential data such as natural\nspeech. We empirically evaluate the proposed model against related sequential\nmodels on four speech datasets and one handwriting dataset. Our results show\nthe important roles that latent random variables can play in the RNN dynamic\nhidden state.\n", "versions": [{"version": "v1", "created": "Sun, 7 Jun 2015 04:23:50 GMT"}, {"version": "v2", "created": "Thu, 18 Jun 2015 02:25:53 GMT"}, {"version": "v3", "created": "Fri, 19 Jun 2015 04:57:00 GMT"}, {"version": "v4", "created": "Thu, 15 Oct 2015 18:10:41 GMT"}, {"version": "v5", "created": "Mon, 2 Nov 2015 18:56:13 GMT"}, {"version": "v6", "created": "Wed, 6 Apr 2016 20:52:32 GMT"}], "update_date": "2016-04-08", "authors_parsed": [["Chung", "Junyoung", ""], ["Kastner", "Kyle", ""], ["Dinh", "Laurent", ""], ["Goel", "Kratarth", ""], ["Courville", "Aaron", ""], ["Bengio", "Yoshua", ""]]}, {"id": "1506.02222", "submitter": "Xiangyu Wang", "authors": "Xiangyu Wang, David Dunson and Chenlei Leng", "title": "No penalty no tears: Least squares in high-dimensional linear models", "comments": "Added results for non-sparse models; Added results for elliptical\n  distribution; Added simulations for adaptive lasso", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.LG math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Ordinary least squares (OLS) is the default method for fitting linear models,\nbut is not applicable for problems with dimensionality larger than the sample\nsize. For these problems, we advocate the use of a generalized version of OLS\nmotivated by ridge regression, and propose two novel three-step algorithms\ninvolving least squares fitting and hard thresholding. The algorithms are\nmethodologically simple to understand intuitively, computationally easy to\nimplement efficiently, and theoretically appealing for choosing models\nconsistently. Numerical exercises comparing our methods with penalization-based\napproaches in simulations and data analyses illustrate the great potential of\nthe proposed algorithms.\n", "versions": [{"version": "v1", "created": "Sun, 7 Jun 2015 05:45:24 GMT"}, {"version": "v2", "created": "Wed, 10 Jun 2015 03:31:06 GMT"}, {"version": "v3", "created": "Fri, 9 Oct 2015 21:30:39 GMT"}, {"version": "v4", "created": "Mon, 23 Nov 2015 09:21:37 GMT"}, {"version": "v5", "created": "Thu, 16 Jun 2016 07:13:40 GMT"}], "update_date": "2016-06-17", "authors_parsed": [["Wang", "Xiangyu", ""], ["Dunson", "David", ""], ["Leng", "Chenlei", ""]]}, {"id": "1506.02227", "submitter": "Peter Richtarik", "authors": "Dominik Csiba and Peter Richt\\'arik", "title": "Primal Method for ERM with Flexible Mini-batching Schemes and Non-convex\n  Losses", "comments": "13 pages, 3 figures, 2 algorithms", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.DS cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work we develop a new algorithm for regularized empirical risk\nminimization. Our method extends recent techniques of Shalev-Shwartz [02/2015],\nwhich enable a dual-free analysis of SDCA, to arbitrary mini-batching schemes.\nMoreover, our method is able to better utilize the information in the data\ndefining the ERM problem. For convex loss functions, our complexity results\nmatch those of QUARTZ, which is a primal-dual method also allowing for\narbitrary mini-batching schemes. The advantage of a dual-free analysis comes\nfrom the fact that it guarantees convergence even for non-convex loss\nfunctions, as long as the average loss is convex. We illustrate through\nexperiments the utility of being able to design arbitrary mini-batching\nschemes.\n", "versions": [{"version": "v1", "created": "Sun, 7 Jun 2015 06:45:55 GMT"}], "update_date": "2015-06-09", "authors_parsed": [["Csiba", "Dominik", ""], ["Richt\u00e1rik", "Peter", ""]]}, {"id": "1506.02256", "submitter": "Zhiyuan Tang", "authors": "Zhiyuan Tang, Dong Wang, Yiqiao Pan, Zhiyong Zhang", "title": "Knowledge Transfer Pre-training", "comments": "arXiv admin note: text overlap with arXiv:1505.04630", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Pre-training is crucial for learning deep neural networks. Most of existing\npre-training methods train simple models (e.g., restricted Boltzmann machines)\nand then stack them layer by layer to form the deep structure. This layer-wise\npre-training has found strong theoretical foundation and broad empirical\nsupport. However, it is not easy to employ such method to pre-train models\nwithout a clear multi-layer structure,e.g., recurrent neural networks (RNNs).\nThis paper presents a new pre-training approach based on knowledge transfer\nlearning. In contrast to the layer-wise approach which trains model components\nincrementally, the new approach trains the entire model as a whole but with an\neasier objective function. This is achieved by utilizing soft targets produced\nby a prior trained model (teacher model). Compared to the conventional\nlayer-wise methods, this new method does not care about the model structure, so\ncan be used to pre-train very complex models. Experiments on a speech\nrecognition task demonstrated that with this approach, complex RNNs can be well\ntrained with a weaker deep neural network (DNN) model. Furthermore, the new\nmethod can be combined with conventional layer-wise pre-training to deliver\nadditional gains.\n", "versions": [{"version": "v1", "created": "Sun, 7 Jun 2015 11:55:33 GMT"}], "update_date": "2015-06-09", "authors_parsed": [["Tang", "Zhiyuan", ""], ["Wang", "Dong", ""], ["Pan", "Yiqiao", ""], ["Zhang", "Zhiyong", ""]]}, {"id": "1506.02264", "submitter": "Shmuel Peleg", "authors": "Yedid Hoshen, Shmuel Peleg", "title": "Visual Learning of Arithmetic Operations", "comments": "To appear in AAAI 2016", "journal-ref": "Proc. AAAI'16, Phoenix, Feb. 2016, pp. 3733-3739", "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A simple Neural Network model is presented for end-to-end visual learning of\narithmetic operations from pictures of numbers. The input consists of two\npictures, each showing a 7-digit number. The output, also a picture, displays\nthe number showing the result of an arithmetic operation (e.g., addition or\nsubtraction) on the two input numbers. The concepts of a number, or of an\noperator, are not explicitly introduced. This indicates that addition is a\nsimple cognitive task, which can be learned visually using a very small number\nof neurons.\n  Other operations, e.g., multiplication, were not learnable using this\narchitecture. Some tasks were not learnable end-to-end (e.g., addition with\nRoman numerals), but were easily learnable once broken into two separate\nsub-tasks: a perceptual \\textit{Character Recognition} and cognitive\n\\textit{Arithmetic} sub-tasks. This indicates that while some tasks may be\neasily learnable end-to-end, other may need to be broken into sub-tasks.\n", "versions": [{"version": "v1", "created": "Sun, 7 Jun 2015 13:44:15 GMT"}, {"version": "v2", "created": "Fri, 27 Nov 2015 12:18:48 GMT"}], "update_date": "2017-01-06", "authors_parsed": [["Hoshen", "Yedid", ""], ["Peleg", "Shmuel", ""]]}, {"id": "1506.02312", "submitter": "Renato Pereira", "authors": "Renato de Pontes Pereira and Paulo Martins Engel", "title": "A Framework for Constrained and Adaptive Behavior-Based Agents", "comments": "2015; 15 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG cs.RO cs.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Behavior Trees are commonly used to model agents for robotics and games,\nwhere constrained behaviors must be designed by human experts in order to\nguarantee that these agents will execute a specific chain of actions given a\nspecific set of perceptions. In such application areas, learning is a desirable\nfeature to provide agents with the ability to adapt and improve interactions\nwith humans and environment, but often discarded due to its unreliability. In\nthis paper, we propose a framework that uses Reinforcement Learning nodes as\npart of Behavior Trees to address the problem of adding learning capabilities\nin constrained agents. We show how this framework relates to Options in\nHierarchical Reinforcement Learning, ensuring convergence of nested learning\nnodes, and we empirically show that the learning nodes do not affect the\nexecution of other nodes in the tree.\n", "versions": [{"version": "v1", "created": "Sun, 7 Jun 2015 20:52:31 GMT"}], "update_date": "2015-06-09", "authors_parsed": [["Pereira", "Renato de Pontes", ""], ["Engel", "Paulo Martins", ""]]}, {"id": "1506.02327", "submitter": "Cheng-Tao Chung", "authors": "Cheng-Tao Chung, Cheng-Yu Tsai, Hsiang-Hung Lu, Yuan-ming Liou,\n  Yen-Chen Wu, Yen-Ju Lu, Hung-yi Lee and Lin-shan Lee", "title": "A Multi-layered Acoustic Tokenizing Deep Neural Network (MAT-DNN) for\n  Unsupervised Discovery of Linguistic Units and Generation of High Quality\n  Features", "comments": "submitted to Interspeech 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper summarizes the work done by the authors for the Zero Resource\nSpeech Challenge organized in the technical program of Interspeech 2015. The\ngoal of the challenge is to discover linguistic units directly from unlabeled\nspeech data. The Multi-layered Acoustic Tokenizer (MAT) proposed in this work\nautomatically discovers multiple sets of acoustic tokens from the given corpus.\nEach acoustic token set is specified by a set of hyperparameters that describe\nthe model configuration. These sets of acoustic tokens carry different\ncharacteristics of the given corpus and the language behind thus can be\nmutually reinforced. The multiple sets of token labels are then used as the\ntargets of a Multi-target DNN (MDNN) trained on low-level acoustic features.\nBottleneck features extracted from the MDNN are used as feedback for the MAT\nand the MDNN itself. We call this iterative system the Multi-layered Acoustic\nTokenizing Deep Neural Network (MAT-DNN) which generates high quality features\nfor track 1 of the challenge and acoustic tokens for track 2 of the challenge.\n", "versions": [{"version": "v1", "created": "Sun, 7 Jun 2015 23:52:54 GMT"}], "update_date": "2015-06-09", "authors_parsed": [["Chung", "Cheng-Tao", ""], ["Tsai", "Cheng-Yu", ""], ["Lu", "Hsiang-Hung", ""], ["Liou", "Yuan-ming", ""], ["Wu", "Yen-Chen", ""], ["Lu", "Yen-Ju", ""], ["Lee", "Hung-yi", ""], ["Lee", "Lin-shan", ""]]}, {"id": "1506.02344", "submitter": "Anastasios Kyrillidis", "authors": "Megasthenis Asteris, Anastasios Kyrillidis, Alexandros G. Dimakis,\n  Han-Gyol Yi and, Bharath Chandrasekaran", "title": "Stay on path: PCA along graph paths", "comments": "12 pages, 5 figures, In Proceedings of International Conference on\n  Machine Learning (ICML) 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.IT cs.LG math.IT math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a variant of (sparse) PCA in which the set of feasible support\nsets is determined by a graph. In particular, we consider the following\nsetting: given a directed acyclic graph $G$ on $p$ vertices corresponding to\nvariables, the non-zero entries of the extracted principal component must\ncoincide with vertices lying along a path in $G$.\n  From a statistical perspective, information on the underlying network may\npotentially reduce the number of observations required to recover the\npopulation principal component. We consider the canonical estimator which\noptimally exploits the prior knowledge by solving a non-convex quadratic\nmaximization on the empirical covariance. We introduce a simple network and\nanalyze the estimator under the spiked covariance model. We show that side\ninformation potentially improves the statistical complexity.\n  We propose two algorithms to approximate the solution of the constrained\nquadratic maximization, and recover a component with the desired properties. We\nempirically evaluate our schemes on synthetic and real datasets.\n", "versions": [{"version": "v1", "created": "Mon, 8 Jun 2015 03:37:36 GMT"}, {"version": "v2", "created": "Fri, 19 Jun 2015 02:27:49 GMT"}], "update_date": "2015-06-22", "authors_parsed": [["Asteris", "Megasthenis", ""], ["Kyrillidis", "Anastasios", ""], ["Dimakis", "Alexandros G.", ""], ["and", "Han-Gyol Yi", ""], ["Chandrasekaran", "Bharath", ""]]}, {"id": "1506.02348", "submitter": "Kamalika Chaudhuri", "authors": "Kamalika Chaudhuri and Sham Kakade and Praneeth Netrapalli and Sujay\n  Sanghavi", "title": "Convergence Rates of Active Learning for Maximum Likelihood Estimation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An active learner is given a class of models, a large set of unlabeled\nexamples, and the ability to interactively query labels of a subset of these\nexamples; the goal of the learner is to learn a model in the class that fits\nthe data well.\n  Previous theoretical work has rigorously characterized label complexity of\nactive learning, but most of this work has focused on the PAC or the agnostic\nPAC model. In this paper, we shift our attention to a more general setting --\nmaximum likelihood estimation. Provided certain conditions hold on the model\nclass, we provide a two-stage active learning algorithm for this problem. The\nconditions we require are fairly general, and cover the widely popular class of\nGeneralized Linear Models, which in turn, include models for binary and\nmulti-class classification, regression, and conditional random fields.\n  We provide an upper bound on the label requirement of our algorithm, and a\nlower bound that matches it up to lower order terms. Our analysis shows that\nunlike binary classification in the realizable case, just a single extra round\nof interaction is sufficient to achieve near-optimal performance in maximum\nlikelihood estimation. On the empirical side, the recent work in\n~\\cite{Zhang12} and~\\cite{Zhang14} (on active linear and logistic regression)\nshows the promise of this approach.\n", "versions": [{"version": "v1", "created": "Mon, 8 Jun 2015 04:05:43 GMT"}], "update_date": "2015-06-09", "authors_parsed": [["Chaudhuri", "Kamalika", ""], ["Kakade", "Sham", ""], ["Netrapalli", "Praneeth", ""], ["Sanghavi", "Sujay", ""]]}, {"id": "1506.02351", "submitter": "Junbo Zhao", "authors": "Junbo Zhao, Michael Mathieu, Ross Goroshin, Yann LeCun", "title": "Stacked What-Where Auto-encoders", "comments": "Workshop track - ICLR 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a novel architecture, the \"stacked what-where auto-encoders\"\n(SWWAE), which integrates discriminative and generative pathways and provides a\nunified approach to supervised, semi-supervised and unsupervised learning\nwithout relying on sampling during training. An instantiation of SWWAE uses a\nconvolutional net (Convnet) (LeCun et al. (1998)) to encode the input, and\nemploys a deconvolutional net (Deconvnet) (Zeiler et al. (2010)) to produce the\nreconstruction. The objective function includes reconstruction terms that\ninduce the hidden states in the Deconvnet to be similar to those of the\nConvnet. Each pooling layer produces two sets of variables: the \"what\" which\nare fed to the next layer, and its complementary variable \"where\" that are fed\nto the corresponding layer in the generative decoder.\n", "versions": [{"version": "v1", "created": "Mon, 8 Jun 2015 04:45:33 GMT"}, {"version": "v2", "created": "Fri, 12 Jun 2015 02:38:59 GMT"}, {"version": "v3", "created": "Tue, 23 Jun 2015 02:59:39 GMT"}, {"version": "v4", "created": "Sat, 4 Jul 2015 23:36:39 GMT"}, {"version": "v5", "created": "Wed, 11 Nov 2015 04:06:00 GMT"}, {"version": "v6", "created": "Sun, 15 Nov 2015 00:23:14 GMT"}, {"version": "v7", "created": "Tue, 17 Nov 2015 20:36:18 GMT"}, {"version": "v8", "created": "Sun, 14 Feb 2016 21:09:22 GMT"}], "update_date": "2016-02-16", "authors_parsed": [["Zhao", "Junbo", ""], ["Mathieu", "Michael", ""], ["Goroshin", "Ross", ""], ["LeCun", "Yann", ""]]}, {"id": "1506.02428", "submitter": "Purushottam Kar", "authors": "Kush Bhatia and Prateek Jain and Purushottam Kar", "title": "Robust Regression via Hard Thresholding", "comments": "24 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of Robust Least Squares Regression (RLSR) where several\nresponse variables can be adversarially corrupted. More specifically, for a\ndata matrix X \\in R^{p x n} and an underlying model w*, the response vector is\ngenerated as y = X'w* + b where b \\in R^n is the corruption vector supported\nover at most C.n coordinates. Existing exact recovery results for RLSR focus\nsolely on L1-penalty based convex formulations and impose relatively strict\nmodel assumptions such as requiring the corruptions b to be selected\nindependently of X.\n  In this work, we study a simple hard-thresholding algorithm called TORRENT\nwhich, under mild conditions on X, can recover w* exactly even if b corrupts\nthe response variables in an adversarial manner, i.e. both the support and\nentries of b are selected adversarially after observing X and w*. Our results\nhold under deterministic assumptions which are satisfied if X is sampled from\nany sub-Gaussian distribution. Finally unlike existing results that apply only\nto a fixed w*, generated independently of X, our results are universal and hold\nfor any w* \\in R^p.\n  Next, we propose gradient descent-based extensions of TORRENT that can scale\nefficiently to large scale problems, such as high dimensional sparse recovery\nand prove similar recovery guarantees for these extensions. Empirically we find\nTORRENT, and more so its extensions, offering significantly faster recovery\nthan the state-of-the-art L1 solvers. For instance, even on moderate-sized\ndatasets (with p = 50K) with around 40% corrupted responses, a variant of our\nproposed method called TORRENT-HYB is more than 20x faster than the best L1\nsolver.\n", "versions": [{"version": "v1", "created": "Mon, 8 Jun 2015 10:13:53 GMT"}], "update_date": "2015-06-09", "authors_parsed": [["Bhatia", "Kush", ""], ["Jain", "Prateek", ""], ["Kar", "Purushottam", ""]]}, {"id": "1506.02438", "submitter": "John Schulman", "authors": "John Schulman, Philipp Moritz, Sergey Levine, Michael Jordan, Pieter\n  Abbeel", "title": "High-Dimensional Continuous Control Using Generalized Advantage\n  Estimation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.RO cs.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Policy gradient methods are an appealing approach in reinforcement learning\nbecause they directly optimize the cumulative reward and can straightforwardly\nbe used with nonlinear function approximators such as neural networks. The two\nmain challenges are the large number of samples typically required, and the\ndifficulty of obtaining stable and steady improvement despite the\nnonstationarity of the incoming data. We address the first challenge by using\nvalue functions to substantially reduce the variance of policy gradient\nestimates at the cost of some bias, with an exponentially-weighted estimator of\nthe advantage function that is analogous to TD(lambda). We address the second\nchallenge by using trust region optimization procedure for both the policy and\nthe value function, which are represented by neural networks.\n  Our approach yields strong empirical results on highly challenging 3D\nlocomotion tasks, learning running gaits for bipedal and quadrupedal simulated\nrobots, and learning a policy for getting the biped to stand up from starting\nout lying on the ground. In contrast to a body of prior work that uses\nhand-crafted policy representations, our neural network policies map directly\nfrom raw kinematics to joint torques. Our algorithm is fully model-free, and\nthe amount of simulated experience required for the learning tasks on 3D bipeds\ncorresponds to 1-2 weeks of real time.\n", "versions": [{"version": "v1", "created": "Mon, 8 Jun 2015 11:12:48 GMT"}, {"version": "v2", "created": "Thu, 26 Nov 2015 20:07:11 GMT"}, {"version": "v3", "created": "Tue, 8 Dec 2015 20:16:20 GMT"}, {"version": "v4", "created": "Thu, 21 Jan 2016 19:44:56 GMT"}, {"version": "v5", "created": "Fri, 9 Sep 2016 17:54:07 GMT"}, {"version": "v6", "created": "Sat, 20 Oct 2018 18:55:07 GMT"}], "update_date": "2018-10-23", "authors_parsed": [["Schulman", "John", ""], ["Moritz", "Philipp", ""], ["Levine", "Sergey", ""], ["Jordan", "Michael", ""], ["Abbeel", "Pieter", ""]]}, {"id": "1506.02465", "submitter": "Marius Lindauer", "authors": "Bernd Bischl, Pascal Kerschke, Lars Kotthoff, Marius Lindauer, Yuri\n  Malitsky, Alexandre Frechette, Holger Hoos, Frank Hutter, Kevin Leyton-Brown,\n  Kevin Tierney, Joaquin Vanschoren", "title": "ASlib: A Benchmark Library for Algorithm Selection", "comments": "Accepted to be published in Artificial Intelligence Journal", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The task of algorithm selection involves choosing an algorithm from a set of\nalgorithms on a per-instance basis in order to exploit the varying performance\nof algorithms over a set of instances. The algorithm selection problem is\nattracting increasing attention from researchers and practitioners in AI. Years\nof fruitful applications in a number of domains have resulted in a large amount\nof data, but the community lacks a standard format or repository for this data.\nThis situation makes it difficult to share and compare different approaches\neffectively, as is done in other, more established fields. It also\nunnecessarily hinders new researchers who want to work in this area. To address\nthis problem, we introduce a standardized format for representing algorithm\nselection scenarios and a repository that contains a growing number of data\nsets from the literature. Our format has been designed to be able to express a\nwide variety of different scenarios. Demonstrating the breadth and power of our\nplatform, we describe a set of example experiments that build and evaluate\nalgorithm selection models through a common interface. The results display the\npotential of algorithm selection to achieve significant performance\nimprovements across a broad range of problems and algorithms.\n", "versions": [{"version": "v1", "created": "Mon, 8 Jun 2015 12:35:04 GMT"}, {"version": "v2", "created": "Wed, 24 Feb 2016 14:38:52 GMT"}, {"version": "v3", "created": "Wed, 6 Apr 2016 13:20:23 GMT"}], "update_date": "2016-04-07", "authors_parsed": [["Bischl", "Bernd", ""], ["Kerschke", "Pascal", ""], ["Kotthoff", "Lars", ""], ["Lindauer", "Marius", ""], ["Malitsky", "Yuri", ""], ["Frechette", "Alexandre", ""], ["Hoos", "Holger", ""], ["Hutter", "Frank", ""], ["Leyton-Brown", "Kevin", ""], ["Tierney", "Kevin", ""], ["Vanschoren", "Joaquin", ""]]}, {"id": "1506.02509", "submitter": "Lei Zhang", "authors": "Lei Zhang and David Zhang", "title": "SVM and ELM: Who Wins? Object Recognition with Deep Convolutional\n  Features from ImageNet", "comments": "7 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning with a convolutional neural network (CNN) has been proved to be\nvery effective in feature extraction and representation of images. For image\nclassification problems, this work aim at finding which classifier is more\ncompetitive based on high-level deep features of images. In this report, we\nhave discussed the nearest neighbor, support vector machines and extreme\nlearning machines for image classification under deep convolutional activation\nfeature representation. Specifically, we adopt the benchmark object recognition\ndataset from multiple sources with domain bias for evaluating different\nclassifiers. The deep features of the object dataset are obtained by a\nwell-trained CNN with five convolutional layers and three fully-connected\nlayers on the challenging ImageNet. Experiments demonstrate that the ELMs\noutperform SVMs in cross-domain recognition tasks. In particular,\nstate-of-the-art results are obtained by kernel ELM which outperforms SVMs with\nabout 4% of the average accuracy. The features and codes are available in\nhttp://www.escience.cn/people/lei/index.html\n", "versions": [{"version": "v1", "created": "Mon, 8 Jun 2015 13:58:01 GMT"}], "update_date": "2015-06-09", "authors_parsed": [["Zhang", "Lei", ""], ["Zhang", "David", ""]]}, {"id": "1506.02510", "submitter": "Onur Dikmen", "authors": "Onur Dikmen", "title": "Learning Mixtures of Ising Models using Pseudolikelihood", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Maximum pseudolikelihood method has been among the most important methods for\nlearning parameters of statistical physics models, such as Ising models. In\nthis paper, we study how pseudolikelihood can be derived for learning\nparameters of a mixture of Ising models. The performance of the proposed\napproach is demonstrated for Ising and Potts models on both synthetic and real\ndata.\n", "versions": [{"version": "v1", "created": "Mon, 8 Jun 2015 14:00:32 GMT"}], "update_date": "2015-06-09", "authors_parsed": [["Dikmen", "Onur", ""]]}, {"id": "1506.02516", "submitter": "Edward Grefenstette", "authors": "Edward Grefenstette, Karl Moritz Hermann, Mustafa Suleyman, Phil\n  Blunsom", "title": "Learning to Transduce with Unbounded Memory", "comments": "14 pages, 4 figures, NIPS 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, strong results have been demonstrated by Deep Recurrent Neural\nNetworks on natural language transduction problems. In this paper we explore\nthe representational power of these models using synthetic grammars designed to\nexhibit phenomena similar to those found in real transduction problems such as\nmachine translation. These experiments lead us to propose new memory-based\nrecurrent networks that implement continuously differentiable analogues of\ntraditional data structures such as Stacks, Queues, and DeQues. We show that\nthese architectures exhibit superior generalisation performance to Deep RNNs\nand are often able to learn the underlying generating algorithms in our\ntransduction experiments.\n", "versions": [{"version": "v1", "created": "Mon, 8 Jun 2015 14:23:30 GMT"}, {"version": "v2", "created": "Fri, 30 Oct 2015 16:24:40 GMT"}, {"version": "v3", "created": "Tue, 3 Nov 2015 14:07:29 GMT"}], "update_date": "2015-11-04", "authors_parsed": [["Grefenstette", "Edward", ""], ["Hermann", "Karl Moritz", ""], ["Suleyman", "Mustafa", ""], ["Blunsom", "Phil", ""]]}, {"id": "1506.02530", "submitter": "Martin Tak\\'a\\v{c}", "authors": "Chenxin Ma, Rachael Tappenden, Martin Tak\\'a\\v{c}", "title": "Linear Convergence of the Randomized Feasible Descent Method Under the\n  Weak Strong Convexity Assumption", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we generalize the framework of the feasible descent method\n(FDM) to a randomized (R-FDM) and a coordinate-wise random feasible descent\nmethod (RC-FDM) framework. We show that the famous SDCA algorithm for\noptimizing the SVM dual problem, or the stochastic coordinate descent method\nfor the LASSO problem, fits into the framework of RC-FDM. We prove linear\nconvergence for both R-FDM and RC-FDM under the weak strong convexity\nassumption. Moreover, we show that the duality gap converges linearly for\nRC-FDM, which implies that the duality gap also converges linearly for SDCA\napplied to the SVM dual problem.\n", "versions": [{"version": "v1", "created": "Mon, 8 Jun 2015 14:54:08 GMT"}], "update_date": "2015-06-09", "authors_parsed": [["Ma", "Chenxin", ""], ["Tappenden", "Rachael", ""], ["Tak\u00e1\u010d", "Martin", ""]]}, {"id": "1506.02535", "submitter": "Mario Marchand", "authors": "Louis Fortier-Dubois, Fran\\c{c}ois Laviolette, Mario Marchand,\n  Louis-Emile Robitaille, Jean-Francis Roy", "title": "Efficient Learning of Ensembles with QuadBoost", "comments": "9 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We first present a general risk bound for ensembles that depends on the Lp\nnorm of the weighted combination of voters which can be selected from a\ncontinuous set. We then propose a boosting method, called QuadBoost, which is\nstrongly supported by the general risk bound and has very simple rules for\nassigning the voters' weights. Moreover, QuadBoost exhibits a rate of decrease\nof its empirical error which is slightly faster than the one achieved by\nAdaBoost. The experimental results confirm the expectation of the theory that\nQuadBoost is a very efficient method for learning ensembles.\n", "versions": [{"version": "v1", "created": "Mon, 8 Jun 2015 15:10:56 GMT"}, {"version": "v2", "created": "Fri, 19 Jun 2015 14:14:29 GMT"}, {"version": "v3", "created": "Fri, 10 Jul 2015 20:15:09 GMT"}, {"version": "v4", "created": "Tue, 21 Jul 2015 17:37:12 GMT"}, {"version": "v5", "created": "Fri, 20 Nov 2015 19:33:34 GMT"}], "update_date": "2015-11-23", "authors_parsed": [["Fortier-Dubois", "Louis", ""], ["Laviolette", "Fran\u00e7ois", ""], ["Marchand", "Mario", ""], ["Robitaille", "Louis-Emile", ""], ["Roy", "Jean-Francis", ""]]}, {"id": "1506.02544", "submitter": "Youssef  Mroueh", "authors": "Youssef Mroueh, Stephen Voinea, Tomaso Poggio", "title": "Learning with Group Invariant Features: A Kernel Perspective", "comments": "NIPS 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We analyze in this paper a random feature map based on a theory of invariance\nI-theory introduced recently. More specifically, a group invariant signal\nsignature is obtained through cumulative distributions of group transformed\nrandom projections. Our analysis bridges invariant feature learning with kernel\nmethods, as we show that this feature map defines an expected Haar integration\nkernel that is invariant to the specified group action. We show how this\nnon-linear random feature map approximates this group invariant kernel\nuniformly on a set of $N$ points. Moreover, we show that it defines a function\nspace that is dense in the equivalent Invariant Reproducing Kernel Hilbert\nSpace. Finally, we quantify error rates of the convergence of the empirical\nrisk minimization, as well as the reduction in the sample complexity of a\nlearning algorithm using such an invariant representation for signal\nclassification, in a classical supervised learning setting.\n", "versions": [{"version": "v1", "created": "Mon, 8 Jun 2015 15:19:30 GMT"}, {"version": "v2", "created": "Fri, 4 Dec 2015 20:49:25 GMT"}], "update_date": "2015-12-07", "authors_parsed": [["Mroueh", "Youssef", ""], ["Voinea", "Stephen", ""], ["Poggio", "Tomaso", ""]]}, {"id": "1506.02550", "submitter": "Junpei Komiyama", "authors": "Junpei Komiyama, Junya Honda, Hisashi Kashima, Hiroshi Nakagawa", "title": "Regret Lower Bound and Optimal Algorithm in Dueling Bandit Problem", "comments": "26 pages, 10 figures, to appear in COLT2015 (ver.3: revised related\n  work (RUCB))", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the $K$-armed dueling bandit problem, a variation of the standard\nstochastic bandit problem where the feedback is limited to relative comparisons\nof a pair of arms. We introduce a tight asymptotic regret lower bound that is\nbased on the information divergence. An algorithm that is inspired by the\nDeterministic Minimum Empirical Divergence algorithm (Honda and Takemura, 2010)\nis proposed, and its regret is analyzed. The proposed algorithm is found to be\nthe first one with a regret upper bound that matches the lower bound.\nExperimental comparisons of dueling bandit algorithms show that the proposed\nalgorithm significantly outperforms existing ones.\n", "versions": [{"version": "v1", "created": "Mon, 8 Jun 2015 15:28:39 GMT"}, {"version": "v2", "created": "Tue, 16 Jun 2015 10:19:53 GMT"}, {"version": "v3", "created": "Mon, 29 Jun 2015 12:35:58 GMT"}], "update_date": "2015-06-30", "authors_parsed": [["Komiyama", "Junpei", ""], ["Honda", "Junya", ""], ["Kashima", "Hisashi", ""], ["Nakagawa", "Hiroshi", ""]]}, {"id": "1506.02554", "submitter": "Christina Heinze", "authors": "Christina Heinze, Brian McWilliams, Nicolai Meinshausen", "title": "DUAL-LOCO: Distributing Statistical Estimation Using Random Projections", "comments": "13 pages", "journal-ref": "Proceedings of the 19th International Conference on Artificial\n  Intelligence and Statistics, 51, 2016, 12 pages", "doi": null, "report-no": null, "categories": "stat.ML cs.DC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present DUAL-LOCO, a communication-efficient algorithm for distributed\nstatistical estimation. DUAL-LOCO assumes that the data is distributed\naccording to the features rather than the samples. It requires only a single\nround of communication where low-dimensional random projections are used to\napproximate the dependences between features available to different workers. We\nshow that DUAL-LOCO has bounded approximation error which only depends weakly\non the number of workers. We compare DUAL-LOCO against a state-of-the-art\ndistributed optimization method on a variety of real world datasets and show\nthat it obtains better speedups while retaining good accuracy.\n", "versions": [{"version": "v1", "created": "Mon, 8 Jun 2015 15:35:24 GMT"}, {"version": "v2", "created": "Fri, 8 Jan 2016 16:44:27 GMT"}], "update_date": "2016-08-04", "authors_parsed": [["Heinze", "Christina", ""], ["McWilliams", "Brian", ""], ["Meinshausen", "Nicolai", ""]]}, {"id": "1506.02557", "submitter": "Diederik P Kingma M.Sc.", "authors": "Diederik P. Kingma, Tim Salimans, Max Welling", "title": "Variational Dropout and the Local Reparameterization Trick", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate a local reparameterizaton technique for greatly reducing the\nvariance of stochastic gradients for variational Bayesian inference (SGVB) of a\nposterior over model parameters, while retaining parallelizability. This local\nreparameterization translates uncertainty about global parameters into local\nnoise that is independent across datapoints in the minibatch. Such\nparameterizations can be trivially parallelized and have variance that is\ninversely proportional to the minibatch size, generally leading to much faster\nconvergence. Additionally, we explore a connection with dropout: Gaussian\ndropout objectives correspond to SGVB with local reparameterization, a\nscale-invariant prior and proportionally fixed posterior variance. Our method\nallows inference of more flexibly parameterized posteriors; specifically, we\npropose variational dropout, a generalization of Gaussian dropout where the\ndropout rates are learned, often leading to better models. The method is\ndemonstrated through several experiments.\n", "versions": [{"version": "v1", "created": "Mon, 8 Jun 2015 15:37:56 GMT"}, {"version": "v2", "created": "Sun, 20 Dec 2015 16:07:38 GMT"}], "update_date": "2015-12-22", "authors_parsed": [["Kingma", "Diederik P.", ""], ["Salimans", "Tim", ""], ["Welling", "Max", ""]]}, {"id": "1506.02565", "submitter": "Seungjin Choi", "authors": "Yong-Deok Kim, Taewoong Jang, Bohyung Han, and Seungjin Choi", "title": "Learning to Select Pre-Trained Deep Representations with Bayesian\n  Evidence Framework", "comments": "Appearing in CVPR-2016 (oral presentation)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a Bayesian evidence framework to facilitate transfer learning from\npre-trained deep convolutional neural networks (CNNs). Our framework is\nformulated on top of a least squares SVM (LS-SVM) classifier, which is simple\nand fast in both training and testing, and achieves competitive performance in\npractice. The regularization parameters in LS-SVM is estimated automatically\nwithout grid search and cross-validation by maximizing evidence, which is a\nuseful measure to select the best performing CNN out of multiple candidates for\ntransfer learning; the evidence is optimized efficiently by employing Aitken's\ndelta-squared process, which accelerates convergence of fixed point update. The\nproposed Bayesian evidence framework also provides a good solution to identify\nthe best ensemble of heterogeneous CNNs through a greedy algorithm. Our\nBayesian evidence framework for transfer learning is tested on 12 visual\nrecognition datasets and illustrates the state-of-the-art performance\nconsistently in terms of prediction accuracy and modeling efficiency.\n", "versions": [{"version": "v1", "created": "Mon, 8 Jun 2015 15:56:26 GMT"}, {"version": "v2", "created": "Tue, 9 Jun 2015 18:57:35 GMT"}, {"version": "v3", "created": "Thu, 24 Dec 2015 03:40:28 GMT"}, {"version": "v4", "created": "Mon, 25 Apr 2016 01:35:31 GMT"}], "update_date": "2016-04-26", "authors_parsed": [["Kim", "Yong-Deok", ""], ["Jang", "Taewoong", ""], ["Han", "Bohyung", ""], ["Choi", "Seungjin", ""]]}, {"id": "1506.02582", "submitter": "Huizhen Yu", "authors": "Huizhen Yu", "title": "On Convergence of Emphatic Temporal-Difference Learning", "comments": "A minor correction is made (see page 1 for details). 45 pages. A\n  shorter 28-page article based on the first version appeared at the 28th\n  Annual Conference on Learning Theory (COLT), 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider emphatic temporal-difference learning algorithms for policy\nevaluation in discounted Markov decision processes with finite spaces. Such\nalgorithms were recently proposed by Sutton, Mahmood, and White (2015) as an\nimproved solution to the problem of divergence of off-policy\ntemporal-difference learning with linear function approximation. We present in\nthis paper the first convergence proofs for two emphatic algorithms,\nETD($\\lambda$) and ELSTD($\\lambda$). We prove, under general off-policy\nconditions, the convergence in $L^1$ for ELSTD($\\lambda$) iterates, and the\nalmost sure convergence of the approximate value functions calculated by both\nalgorithms using a single infinitely long trajectory. Our analysis involves new\ntechniques with applications beyond emphatic algorithms leading, for example,\nto the first proof that standard TD($\\lambda$) also converges under off-policy\ntraining for $\\lambda$ sufficiently large.\n", "versions": [{"version": "v1", "created": "Mon, 8 Jun 2015 16:42:10 GMT"}, {"version": "v2", "created": "Wed, 12 Aug 2015 23:06:08 GMT"}, {"version": "v3", "created": "Thu, 28 Dec 2017 17:36:48 GMT"}], "update_date": "2017-12-29", "authors_parsed": [["Yu", "Huizhen", ""]]}, {"id": "1506.02585", "submitter": "Zhimin Peng", "authors": "Zhimin Peng, Prudhvi Gurram, Heesung Kwon, Wotao Yin", "title": "Optimal Sparse Kernel Learning for Hyperspectral Anomaly Detection", "comments": "4 pages, 1 figure, 5th workshop on Hyperspectral image and signal\n  processing: evolution in remote sensing", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, a novel framework of sparse kernel learning for Support Vector\nData Description (SVDD) based anomaly detection is presented. In this work,\noptimal sparse feature selection for anomaly detection is first modeled as a\nMixed Integer Programming (MIP) problem. Due to the prohibitively high\ncomputational complexity of the MIP, it is relaxed into a Quadratically\nConstrained Linear Programming (QCLP) problem. The QCLP problem can then be\npractically solved by using an iterative optimization method, in which multiple\nsubsets of features are iteratively found as opposed to a single subset. The\nQCLP-based iterative optimization problem is solved in a finite space called\nthe \\emph{Empirical Kernel Feature Space} (EKFS) instead of in the input space\nor \\emph{Reproducing Kernel Hilbert Space} (RKHS). This is possible because of\nthe fact that the geometrical properties of the EKFS and the corresponding RKHS\nremain the same. Now, an explicit nonlinear exploitation of the data in a\nfinite EKFS is achievable, which results in optimal feature ranking.\nExperimental results based on a hyperspectral image show that the proposed\nmethod can provide improved performance over the current state-of-the-art\ntechniques.\n", "versions": [{"version": "v1", "created": "Mon, 8 Jun 2015 16:51:40 GMT"}], "update_date": "2015-06-09", "authors_parsed": [["Peng", "Zhimin", ""], ["Gurram", "Prudhvi", ""], ["Kwon", "Heesung", ""], ["Yin", "Wotao", ""]]}, {"id": "1506.02617", "submitter": "Behnam Neyshabur", "authors": "Behnam Neyshabur, Ruslan Salakhutdinov, Nathan Srebro", "title": "Path-SGD: Path-Normalized Optimization in Deep Neural Networks", "comments": "12 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We revisit the choice of SGD for training deep neural networks by\nreconsidering the appropriate geometry in which to optimize the weights. We\nargue for a geometry invariant to rescaling of weights that does not affect the\noutput of the network, and suggest Path-SGD, which is an approximate steepest\ndescent method with respect to a path-wise regularizer related to max-norm\nregularization. Path-SGD is easy and efficient to implement and leads to\nempirical gains over SGD and AdaGrad.\n", "versions": [{"version": "v1", "created": "Mon, 8 Jun 2015 19:01:33 GMT"}], "update_date": "2015-06-09", "authors_parsed": [["Neyshabur", "Behnam", ""], ["Salakhutdinov", "Ruslan", ""], ["Srebro", "Nathan", ""]]}, {"id": "1506.02620", "submitter": "Ching-pei Lee", "authors": "Ching-pei Lee, Kai-Wei Chang, Shyam Upadhyay, Dan Roth", "title": "Distributed Training of Structured SVM", "comments": "NIPS Workshop on Optimization for Machine Learning, 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.DC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Training structured prediction models is time-consuming. However, most\nexisting approaches only use a single machine, thus, the advantage of computing\npower and the capacity for larger data sets of multiple machines have not been\nexploited. In this work, we propose an efficient algorithm for distributedly\ntraining structured support vector machines based on a distributed\nblock-coordinate descent method. Both theoretical and experimental results\nindicate that our method is efficient.\n", "versions": [{"version": "v1", "created": "Mon, 8 Jun 2015 19:12:24 GMT"}, {"version": "v2", "created": "Sun, 14 Feb 2016 12:15:45 GMT"}], "update_date": "2016-02-16", "authors_parsed": [["Lee", "Ching-pei", ""], ["Chang", "Kai-Wei", ""], ["Upadhyay", "Shyam", ""], ["Roth", "Dan", ""]]}, {"id": "1506.02626", "submitter": "Song Han", "authors": "Song Han, Jeff Pool, John Tran, William J. Dally", "title": "Learning both Weights and Connections for Efficient Neural Networks", "comments": "Published as a conference paper at NIPS 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural networks are both computationally intensive and memory intensive,\nmaking them difficult to deploy on embedded systems. Also, conventional\nnetworks fix the architecture before training starts; as a result, training\ncannot improve the architecture. To address these limitations, we describe a\nmethod to reduce the storage and computation required by neural networks by an\norder of magnitude without affecting their accuracy by learning only the\nimportant connections. Our method prunes redundant connections using a\nthree-step method. First, we train the network to learn which connections are\nimportant. Next, we prune the unimportant connections. Finally, we retrain the\nnetwork to fine tune the weights of the remaining connections. On the ImageNet\ndataset, our method reduced the number of parameters of AlexNet by a factor of\n9x, from 61 million to 6.7 million, without incurring accuracy loss. Similar\nexperiments with VGG-16 found that the number of parameters can be reduced by\n13x, from 138 million to 10.3 million, again with no loss of accuracy.\n", "versions": [{"version": "v1", "created": "Mon, 8 Jun 2015 19:28:43 GMT"}, {"version": "v2", "created": "Wed, 29 Jul 2015 22:27:31 GMT"}, {"version": "v3", "created": "Fri, 30 Oct 2015 23:29:27 GMT"}], "update_date": "2015-11-03", "authors_parsed": [["Han", "Song", ""], ["Pool", "Jeff", ""], ["Tran", "John", ""], ["Dally", "William J.", ""]]}, {"id": "1506.02629", "submitter": "Vitaly Feldman", "authors": "Cynthia Dwork, Vitaly Feldman, Moritz Hardt, Toniann Pitassi, Omer\n  Reingold, Aaron Roth", "title": "Generalization in Adaptive Data Analysis and Holdout Reuse", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Overfitting is the bane of data analysts, even when data are plentiful.\nFormal approaches to understanding this problem focus on statistical inference\nand generalization of individual analysis procedures. Yet the practice of data\nanalysis is an inherently interactive and adaptive process: new analyses and\nhypotheses are proposed after seeing the results of previous ones, parameters\nare tuned on the basis of obtained results, and datasets are shared and reused.\nAn investigation of this gap has recently been initiated by the authors in\n(Dwork et al., 2014), where we focused on the problem of estimating\nexpectations of adaptively chosen functions.\n  In this paper, we give a simple and practical method for reusing a holdout\n(or testing) set to validate the accuracy of hypotheses produced by a learning\nalgorithm operating on a training set. Reusing a holdout set adaptively\nmultiple times can easily lead to overfitting to the holdout set itself. We\ngive an algorithm that enables the validation of a large number of adaptively\nchosen hypotheses, while provably avoiding overfitting. We illustrate the\nadvantages of our algorithm over the standard use of the holdout set via a\nsimple synthetic experiment.\n  We also formalize and address the general problem of data reuse in adaptive\ndata analysis. We show how the differential-privacy based approach given in\n(Dwork et al., 2014) is applicable much more broadly to adaptive data analysis.\nWe then show that a simple approach based on description length can also be\nused to give guarantees of statistical validity in adaptive settings. Finally,\nwe demonstrate that these incomparable approaches can be unified via the notion\nof approximate max-information that we introduce.\n", "versions": [{"version": "v1", "created": "Mon, 8 Jun 2015 19:34:29 GMT"}, {"version": "v2", "created": "Fri, 25 Sep 2015 19:04:32 GMT"}], "update_date": "2015-09-28", "authors_parsed": [["Dwork", "Cynthia", ""], ["Feldman", "Vitaly", ""], ["Hardt", "Moritz", ""], ["Pitassi", "Toniann", ""], ["Reingold", "Omer", ""], ["Roth", "Aaron", ""]]}, {"id": "1506.02632", "submitter": "L.A. Prashanth", "authors": "Prashanth L.A., Cheng Jie, Michael Fu, Steve Marcus and Csaba\n  Szepesv\\'ari", "title": "Cumulative Prospect Theory Meets Reinforcement Learning: Prediction and\n  Control", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cumulative prospect theory (CPT) is known to model human decisions well, with\nsubstantial empirical evidence supporting this claim. CPT works by distorting\nprobabilities and is more general than the classic expected utility and\ncoherent risk measures. We bring this idea to a risk-sensitive reinforcement\nlearning (RL) setting and design algorithms for both estimation and control.\nThe RL setting presents two particular challenges when CPT is applied:\nestimating the CPT objective requires estimations of the entire distribution of\nthe value function and finding a randomized optimal policy. The estimation\nscheme that we propose uses the empirical distribution to estimate the\nCPT-value of a random variable. We then use this scheme in the inner loop of a\nCPT-value optimization procedure that is based on the well-known simulation\noptimization idea of simultaneous perturbation stochastic approximation (SPSA).\nWe provide theoretical convergence guarantees for all the proposed algorithms\nand also illustrate the usefulness of CPT-based criteria in a traffic signal\ncontrol application.\n", "versions": [{"version": "v1", "created": "Mon, 8 Jun 2015 19:37:55 GMT"}, {"version": "v2", "created": "Sun, 20 Sep 2015 04:19:53 GMT"}, {"version": "v3", "created": "Fri, 26 Feb 2016 21:30:04 GMT"}], "update_date": "2016-03-01", "authors_parsed": [["A.", "Prashanth L.", ""], ["Jie", "Cheng", ""], ["Fu", "Michael", ""], ["Marcus", "Steve", ""], ["Szepesv\u00e1ri", "Csaba", ""]]}, {"id": "1506.02633", "submitter": "Antonio Rieser", "authors": "Antonio Rieser", "title": "A Topological Approach to Spectral Clustering", "comments": "21 Pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  We propose two related unsupervised clustering algorithms which, for input,\ntake data assumed to be sampled from a uniform distribution supported on a\nmetric space $X$, and output a clustering of the data based on the selection of\na topological model for the connected components of $X$. Both algorithms work\nby selecting a graph on the samples from a natural one-parameter family of\ngraphs, using a geometric criterion in the first case and an information\ntheoretic criterion in the second. The estimated connected components of $X$\nare identified with the kernel of the associated graph Laplacian, which allows\nthe algorithm to work without requiring the number of expected clusters or\nother auxiliary data as input.\n", "versions": [{"version": "v1", "created": "Mon, 8 Jun 2015 19:39:37 GMT"}, {"version": "v2", "created": "Sat, 25 Jul 2020 16:37:29 GMT"}], "update_date": "2020-07-28", "authors_parsed": [["Rieser", "Antonio", ""]]}, {"id": "1506.02649", "submitter": "Alon Gonen", "authors": "Alon Gonen, Shai Shalev-Shwartz", "title": "Faster SGD Using Sketched Conditioning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NA cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel method for speeding up stochastic optimization algorithms\nvia sketching methods, which recently became a powerful tool for accelerating\nalgorithms for numerical linear algebra. We revisit the method of conditioning\nfor accelerating first-order methods and suggest the use of sketching methods\nfor constructing a cheap conditioner that attains a significant speedup with\nrespect to the Stochastic Gradient Descent (SGD) algorithm. While our\ntheoretical guarantees assume convexity, we discuss the applicability of our\nmethod to deep neural networks, and experimentally demonstrate its merits.\n", "versions": [{"version": "v1", "created": "Mon, 8 Jun 2015 15:08:37 GMT"}], "update_date": "2015-06-10", "authors_parsed": [["Gonen", "Alon", ""], ["Shalev-Shwartz", "Shai", ""]]}, {"id": "1506.02686", "submitter": "George Monta\\~nez", "authors": "George D. Montanez, Cosma Rohilla Shalizi", "title": "The LICORS Cabinet: Nonparametric Algorithms for Spatio-temporal\n  Prediction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Spatio-temporal data is intrinsically high dimensional, so unsupervised\nmodeling is only feasible if we can exploit structure in the process. When the\ndynamics are local in both space and time, this structure can be exploited by\nsplitting the global field into many lower-dimensional \"light cones\". We review\nlight cone decompositions for predictive state reconstruction, introducing\nthree simple light cone algorithms. These methods allow for tractable inference\nof spatio-temporal data, such as full-frame video. The algorithms make few\nassumptions on the underlying process yet have good predictive performance and\ncan provide distributions over spatio-temporal data, enabling sophisticated\nprobabilistic inference.\n", "versions": [{"version": "v1", "created": "Mon, 8 Jun 2015 20:26:08 GMT"}, {"version": "v2", "created": "Wed, 14 Sep 2016 14:20:11 GMT"}], "update_date": "2016-09-15", "authors_parsed": [["Montanez", "George D.", ""], ["Shalizi", "Cosma Rohilla", ""]]}, {"id": "1506.02690", "submitter": "Zhiguang Wang", "authors": "Zhiguang Wang, Tim Oates, James Lo", "title": "Adaptive Normalized Risk-Averting Training For Deep Neural Networks", "comments": "AAAI 2016, 0.39%~0.4% ER on MNIST with single 32-32-256-10 ConvNets,\n  code available at https://github.com/cauchyturing/ANRAE", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a set of new error criteria and learning approaches,\nAdaptive Normalized Risk-Averting Training (ANRAT), to attack the non-convex\noptimization problem in training deep neural networks (DNNs). Theoretically, we\ndemonstrate its effectiveness on global and local convexity lower-bounded by\nthe standard $L_p$-norm error. By analyzing the gradient on the convexity index\n$\\lambda$, we explain the reason why to learn $\\lambda$ adaptively using\ngradient descent works. In practice, we show how this method improves training\nof deep neural networks to solve visual recognition tasks on the MNIST and\nCIFAR-10 datasets. Without using pretraining or other tricks, we obtain results\ncomparable or superior to those reported in recent literature on the same tasks\nusing standard ConvNets + MSE/cross entropy. Performance on deep/shallow\nmultilayer perceptrons and Denoised Auto-encoders is also explored. ANRAT can\nbe combined with other quasi-Newton training methods, innovative network\nvariants, regularization techniques and other specific tricks in DNNs. Other\nthan unsupervised pretraining, it provides a new perspective to address the\nnon-convex optimization problem in DNNs.\n", "versions": [{"version": "v1", "created": "Mon, 8 Jun 2015 20:42:12 GMT"}, {"version": "v2", "created": "Fri, 7 Aug 2015 14:53:46 GMT"}, {"version": "v3", "created": "Thu, 9 Jun 2016 04:10:22 GMT"}], "update_date": "2016-06-10", "authors_parsed": [["Wang", "Zhiguang", ""], ["Oates", "Tim", ""], ["Lo", "James", ""]]}, {"id": "1506.02717", "submitter": "Paul Kirchner", "authors": "Paul Kirchner and Pierre-Alain Fouque", "title": "An Improved BKW Algorithm for LWE with Applications to Cryptography and\n  Lattices", "comments": "CRYPTO 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.DS cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we study the Learning With Errors problem and its binary\nvariant, where secrets and errors are binary or taken in a small interval. We\nintroduce a new variant of the Blum, Kalai and Wasserman algorithm, relying on\na quantization step that generalizes and fine-tunes modulus switching. In\ngeneral this new technique yields a significant gain in the constant in front\nof the exponent in the overall complexity. We illustrate this by solving p\nwithin half a day a LWE instance with dimension n = 128, modulus $q = n^2$,\nGaussian noise $\\alpha = 1/(\\sqrt{n/\\pi} \\log^2 n)$ and binary secret, using\n$2^{28}$ samples, while the previous best result based on BKW claims a time\ncomplexity of $2^{74}$ with $2^{60}$ samples for the same parameters. We then\nintroduce variants of BDD, GapSVP and UniqueSVP, where the target point is\nrequired to lie in the fundamental parallelepiped, and show how the previous\nalgorithm is able to solve these variants in subexponential time. Moreover, we\nalso show how the previous algorithm can be used to solve the BinaryLWE problem\nwith n samples in subexponential time $2^{(\\ln 2/2+o(1))n/\\log \\log n}$. This\nanalysis does not require any heuristic assumption, contrary to other algebraic\napproaches; instead, it uses a variant of an idea by Lyubashevsky to generate\nmany samples from a small number of samples. This makes it possible to\nasymptotically and heuristically break the NTRU cryptosystem in subexponential\ntime (without contradicting its security assumption). We are also able to solve\nsubset sum problems in subexponential time for density $o(1)$, which is of\nindependent interest: for such density, the previous best algorithm requires\nexponential time. As a direct application, we can solve in subexponential time\nthe parameters of a cryptosystem based on this problem proposed at TCC 2010.\n", "versions": [{"version": "v1", "created": "Mon, 8 Jun 2015 22:22:25 GMT"}, {"version": "v2", "created": "Mon, 15 Jun 2015 19:53:27 GMT"}, {"version": "v3", "created": "Mon, 22 Jun 2015 22:16:33 GMT"}, {"version": "v4", "created": "Mon, 29 Jun 2015 23:24:59 GMT"}], "update_date": "2015-07-01", "authors_parsed": [["Kirchner", "Paul", ""], ["Fouque", "Pierre-Alain", ""]]}, {"id": "1506.02719", "submitter": "Andres Munoz", "authors": "Mehryar Mohri and Andres Munoz Medina", "title": "Non-parametric Revenue Optimization for Generalized Second Price\n  Auctions", "comments": "To be published in Proceedings of UAI 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.GT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an extensive analysis of the key problem of learning optimal\nreserve prices for generalized second price auctions. We describe two\nalgorithms for this task: one based on density estimation, and a novel\nalgorithm benefiting from solid theoretical guarantees and with a very\nfavorable running-time complexity of $O(n S \\log (n S))$, where $n$ is the\nsample size and $S$ the number of slots. Our theoretical guarantees are more\nfavorable than those previously presented in the literature. Additionally, we\nshow that even if bidders do not play at an equilibrium, our second algorithm\nis still well defined and minimizes a quantity of interest. To our knowledge,\nthis is the first attempt to apply learning algorithms to the problem of\nreserve price optimization in GSP auctions. Finally, we present the first\nconvergence analysis of empirical equilibrium bidding functions to the unique\nsymmetric Bayesian-Nash equilibrium of a GSP.\n", "versions": [{"version": "v1", "created": "Mon, 8 Jun 2015 22:45:30 GMT"}], "update_date": "2015-06-10", "authors_parsed": [["Mohri", "Mehryar", ""], ["Medina", "Andres Munoz", ""]]}, {"id": "1506.02732", "submitter": "Zhiguang Wang", "authors": "Wei Song, Zhiguang Wang, Yangdong Ye, Ming Fan", "title": "Empirical Studies on Symbolic Aggregation Approximation Under\n  Statistical Perspectives for Knowledge Discovery in Time Series", "comments": "7 pages, 6 figures. Accepted by FSKD 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.IT math.IT", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  Symbolic Aggregation approXimation (SAX) has been the de facto standard\nrepresentation methods for knowledge discovery in time series on a number of\ntasks and applications. So far, very little work has been done in empirically\ninvestigating the intrinsic properties and statistical mechanics in SAX words.\nIn this paper, we applied several statistical measurements and proposed a new\nstatistical measurement, i.e. information embedding cost (IEC) to analyze the\nstatistical behaviors of the symbolic dynamics. Our experiments on the\nbenchmark datasets and the clinical signals demonstrate that SAX can always\nreduce the complexity while preserving the core information embedded in the\noriginal time series with significant embedding efficiency. Our proposed IEC\nscore provide a priori to determine if SAX is adequate for specific dataset,\nwhich can be generalized to evaluate other symbolic representations. Our work\nprovides an analytical framework with several statistical tools to analyze,\nevaluate and further improve the symbolic dynamics for knowledge discovery in\ntime series.\n", "versions": [{"version": "v1", "created": "Mon, 8 Jun 2015 23:52:04 GMT"}], "update_date": "2015-06-10", "authors_parsed": [["Song", "Wei", ""], ["Wang", "Zhiguang", ""], ["Ye", "Yangdong", ""], ["Fan", "Ming", ""]]}, {"id": "1506.02753", "submitter": "Alexey Dosovitskiy", "authors": "Alexey Dosovitskiy and Thomas Brox", "title": "Inverting Visual Representations with Convolutional Networks", "comments": "Version 4 - final version to appear in CVPR-2016. Visually better\n  results obtained with feature similarity and adversarial training are in a\n  different paper - arXiv:1602.02644", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Feature representations, both hand-designed and learned ones, are often hard\nto analyze and interpret, even when they are extracted from visual data. We\npropose a new approach to study image representations by inverting them with an\nup-convolutional neural network. We apply the method to shallow representations\n(HOG, SIFT, LBP), as well as to deep networks. For shallow representations our\napproach provides significantly better reconstructions than existing methods,\nrevealing that there is surprisingly rich information contained in these\nfeatures. Inverting a deep network trained on ImageNet provides several\ninsights into the properties of the feature representation learned by the\nnetwork. Most strikingly, the colors and the rough contours of an image can be\nreconstructed from activations in higher network layers and even from the\npredicted class probabilities.\n", "versions": [{"version": "v1", "created": "Tue, 9 Jun 2015 02:31:40 GMT"}, {"version": "v2", "created": "Thu, 19 Nov 2015 16:35:56 GMT"}, {"version": "v3", "created": "Thu, 3 Dec 2015 18:18:57 GMT"}, {"version": "v4", "created": "Tue, 26 Apr 2016 23:30:11 GMT"}], "update_date": "2016-04-28", "authors_parsed": [["Dosovitskiy", "Alexey", ""], ["Brox", "Thomas", ""]]}, {"id": "1506.02761", "submitter": "Shihao Ji", "authors": "Shihao Ji, Hyokun Yun, Pinar Yanardag, Shin Matsushima, and S. V. N.\n  Vishwanathan", "title": "WordRank: Learning Word Embeddings via Robust Ranking", "comments": "Conference on Empirical Methods in Natural Language Processing\n  (EMNLP), November 1-5, 2016, Austin, Texas, USA", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Embedding words in a vector space has gained a lot of attention in recent\nyears. While state-of-the-art methods provide efficient computation of word\nsimilarities via a low-dimensional matrix embedding, their motivation is often\nleft unclear. In this paper, we argue that word embedding can be naturally\nviewed as a ranking problem due to the ranking nature of the evaluation\nmetrics. Then, based on this insight, we propose a novel framework WordRank\nthat efficiently estimates word representations via robust ranking, in which\nthe attention mechanism and robustness to noise are readily achieved via the\nDCG-like ranking losses. The performance of WordRank is measured in word\nsimilarity and word analogy benchmarks, and the results are compared to the\nstate-of-the-art word embedding techniques. Our algorithm is very competitive\nto the state-of-the- arts on large corpora, while outperforms them by a\nsignificant margin when the training set is limited (i.e., sparse and noisy).\nWith 17 million tokens, WordRank performs almost as well as existing methods\nusing 7.2 billion tokens on a popular word similarity benchmark. Our multi-node\ndistributed implementation of WordRank is publicly available for general usage.\n", "versions": [{"version": "v1", "created": "Tue, 9 Jun 2015 03:08:06 GMT"}, {"version": "v2", "created": "Sat, 6 Feb 2016 06:02:56 GMT"}, {"version": "v3", "created": "Tue, 23 Feb 2016 06:40:14 GMT"}, {"version": "v4", "created": "Tue, 27 Sep 2016 21:11:56 GMT"}], "update_date": "2016-09-29", "authors_parsed": [["Ji", "Shihao", ""], ["Yun", "Hyokun", ""], ["Yanardag", "Pinar", ""], ["Matsushima", "Shin", ""], ["Vishwanathan", "S. V. N.", ""]]}, {"id": "1506.02784", "submitter": "Song Liu Dr.", "authors": "Song Liu, Kenji Fukumizu", "title": "Estimating Posterior Ratio for Classification: Transfer Learning from\n  Probabilistic Perspective", "comments": "Revision Comments: The proofs were corrected from a few mistakes. The\n  title and the introduction was changed. We have also re-run a few experiments", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Transfer learning assumes classifiers of similar tasks share certain\nparameter structures. Unfortunately, modern classifiers uses sophisticated\nfeature representations with huge parameter spaces which lead to costly\ntransfer. Under the impression that changes from one classifier to another\nshould be ``simple'', an efficient transfer learning criteria that only learns\nthe ``differences'' is proposed in this paper. We train a \\emph{posterior\nratio} which turns out to minimizes the upper-bound of the target learning\nrisk. The model of posterior ratio does not have to share the same parameter\nspace with the source classifier at all so it can be easily modelled and\nefficiently trained. The resulting classifier therefore is obtained by simply\nmultiplying the existing probabilistic-classifier with the learned posterior\nratio.\n", "versions": [{"version": "v1", "created": "Tue, 9 Jun 2015 05:38:17 GMT"}, {"version": "v2", "created": "Wed, 10 Jun 2015 03:17:57 GMT"}, {"version": "v3", "created": "Mon, 19 Oct 2015 05:16:55 GMT"}], "update_date": "2015-10-20", "authors_parsed": [["Liu", "Song", ""], ["Fukumizu", "Kenji", ""]]}, {"id": "1506.02785", "submitter": "Danica J. Sutherland", "authors": "Danica J. Sutherland and Jeff Schneider", "title": "On the Error of Random Fourier Features", "comments": "Published at UAI 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Kernel methods give powerful, flexible, and theoretically grounded approaches\nto solving many problems in machine learning. The standard approach, however,\nrequires pairwise evaluations of a kernel function, which can lead to\nscalability issues for very large datasets. Rahimi and Recht (2007) suggested a\npopular approach to handling this problem, known as random Fourier features.\nThe quality of this approximation, however, is not well understood. We improve\nthe uniform error bound of that paper, as well as giving novel understandings\nof the embedding's variance, approximation error, and use in some machine\nlearning methods. We also point out that surprisingly, of the two main variants\nof those features, the more widely used is strictly higher-variance for the\nGaussian kernel and has worse bounds.\n", "versions": [{"version": "v1", "created": "Tue, 9 Jun 2015 05:39:02 GMT"}], "update_date": "2021-04-08", "authors_parsed": [["Sutherland", "Danica J.", ""], ["Schneider", "Jeff", ""]]}, {"id": "1506.02903", "submitter": "Daniel Hsu", "authors": "Daniel Hsu, Aryeh Kontorovich, Csaba Szepesv\\'ari", "title": "Mixing Time Estimation in Reversible Markov Chains from a Single Sample\n  Path", "comments": "28 pages; minor clarification in Appendix A concerning lower bounds", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article provides the first procedure for computing a fully\ndata-dependent interval that traps the mixing time $t_{\\text{mix}}$ of a finite\nreversible ergodic Markov chain at a prescribed confidence level. The interval\nis computed from a single finite-length sample path from the Markov chain, and\ndoes not require the knowledge of any parameters of the chain. This stands in\ncontrast to previous approaches, which either only provide point estimates, or\nrequire a reset mechanism, or additional prior knowledge. The interval is\nconstructed around the relaxation time $t_{\\text{relax}}$, which is strongly\nrelated to the mixing time, and the width of the interval converges to zero\nroughly at a $\\sqrt{n}$ rate, where $n$ is the length of the sample path. Upper\nand lower bounds are given on the number of samples required to achieve\nconstant-factor multiplicative accuracy. The lower bounds indicate that, unless\nfurther restrictions are placed on the chain, no procedure can achieve this\naccuracy level before seeing each state at least $\\Omega(t_{\\text{relax}})$\ntimes on the average. Finally, future directions of research are identified.\n", "versions": [{"version": "v1", "created": "Tue, 9 Jun 2015 13:30:13 GMT"}, {"version": "v2", "created": "Sun, 1 Nov 2015 00:09:24 GMT"}, {"version": "v3", "created": "Tue, 3 Nov 2015 04:27:46 GMT"}], "update_date": "2015-11-04", "authors_parsed": [["Hsu", "Daniel", ""], ["Kontorovich", "Aryeh", ""], ["Szepesv\u00e1ri", "Csaba", ""]]}, {"id": "1506.02914", "submitter": "Eric Tramel", "authors": "Marylou Gabri\\'e and Eric W. Tramel and Florent Krzakala", "title": "Training Restricted Boltzmann Machines via the Thouless-Anderson-Palmer\n  Free Energy", "comments": "8 pages, 7 figures, demo online at\n  http://www.lps.ens.fr/~krzakala/WASP.html", "journal-ref": "Advances in Neural Information Processing Systems (NIPS 2015) 28,\n  pages 640--648", "doi": null, "report-no": null, "categories": "cond-mat.dis-nn cs.LG cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Restricted Boltzmann machines are undirected neural networks which have been\nshown to be effective in many applications, including serving as\ninitializations for training deep multi-layer neural networks. One of the main\nreasons for their success is the existence of efficient and practical\nstochastic algorithms, such as contrastive divergence, for unsupervised\ntraining. We propose an alternative deterministic iterative procedure based on\nan improved mean field method from statistical physics known as the\nThouless-Anderson-Palmer approach. We demonstrate that our algorithm provides\nperformance equal to, and sometimes superior to, persistent contrastive\ndivergence, while also providing a clear and easy to evaluate objective\nfunction. We believe that this strategy can be easily generalized to other\nmodels as well as to more accurate higher-order approximations, paving the way\nfor systematic improvements in training Boltzmann machines with hidden units.\n", "versions": [{"version": "v1", "created": "Tue, 9 Jun 2015 14:02:02 GMT"}, {"version": "v2", "created": "Mon, 15 Jun 2015 08:30:06 GMT"}], "update_date": "2016-02-11", "authors_parsed": [["Gabri\u00e9", "Marylou", ""], ["Tramel", "Eric W.", ""], ["Krzakala", "Florent", ""]]}, {"id": "1506.02975", "submitter": "Vincent Zhao", "authors": "Vincent Zhao, Steven W. Zucker", "title": "Stagewise Learning for Sparse Clustering of Discretely-Valued Data", "comments": "9 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG q-bio.QM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The performance of EM in learning mixtures of product distributions often\ndepends on the initialization. This can be problematic in crowdsourcing and\nother applications, e.g. when a small number of 'experts' are diluted by a\nlarge number of noisy, unreliable participants. We develop a new EM algorithm\nthat is driven by these experts. In a manner that differs from other\napproaches, we start from a single mixture class. The algorithm then develops\nthe set of 'experts' in a stagewise fashion based on a mutual information\ncriterion. At each stage EM operates on this subset of the players, effectively\nregularizing the E rather than the M step. Experiments show that stagewise EM\noutperforms other initialization techniques for crowdsourcing and neurosciences\napplications, and can guide a full EM to results comparable to those obtained\nknowing the exact distribution.\n", "versions": [{"version": "v1", "created": "Tue, 9 Jun 2015 16:00:21 GMT"}, {"version": "v2", "created": "Sat, 28 May 2016 02:38:42 GMT"}], "update_date": "2016-05-31", "authors_parsed": [["Zhao", "Vincent", ""], ["Zucker", "Steven W.", ""]]}, {"id": "1506.03016", "submitter": "Atsushi Nitanda", "authors": "Atsushi Nitanda", "title": "Accelerated Stochastic Gradient Descent for Minimizing Finite Sums", "comments": "[v2] corrected citation to proxSVRG, corrected typos in Figure\n  1(option2) and 3(R4 -> R3)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose an optimization method for minimizing the finite sums of smooth\nconvex functions. Our method incorporates an accelerated gradient descent (AGD)\nand a stochastic variance reduction gradient (SVRG) in a mini-batch setting.\nUnlike SVRG, our method can be directly applied to non-strongly and strongly\nconvex problems. We show that our method achieves a lower overall complexity\nthan the recently proposed methods that supports non-strongly convex problems.\nMoreover, this method has a fast rate of convergence for strongly convex\nproblems. Our experiments show the effectiveness of our method.\n", "versions": [{"version": "v1", "created": "Tue, 9 Jun 2015 17:38:32 GMT"}, {"version": "v2", "created": "Wed, 10 Jun 2015 16:25:39 GMT"}], "update_date": "2015-06-11", "authors_parsed": [["Nitanda", "Atsushi", ""]]}, {"id": "1506.03018", "submitter": "Yihan Gao", "authors": "Yihan Gao, Aditya Parameswaran, Jian Peng", "title": "On the Interpretability of Conditional Probability Estimates in the\n  Agnostic Setting", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the interpretability of conditional probability estimates for binary\nclassification under the agnostic setting or scenario. Under the agnostic\nsetting, conditional probability estimates do not necessarily reflect the true\nconditional probabilities. Instead, they have a certain calibration property:\namong all data points that the classifier has predicted P(Y = 1|X) = p, p\nportion of them actually have label Y = 1. For cost-sensitive decision\nproblems, this calibration property provides adequate support for us to use\nBayes Decision Theory. In this paper, we define a novel measure for the\ncalibration property together with its empirical counterpart, and prove an\nuniform convergence result between them. This new measure enables us to\nformally justify the calibration property of conditional probability\nestimations, and provides new insights on the problem of estimating and\ncalibrating conditional probabilities.\n", "versions": [{"version": "v1", "created": "Tue, 9 Jun 2015 17:41:48 GMT"}, {"version": "v2", "created": "Tue, 28 Feb 2017 18:21:57 GMT"}], "update_date": "2017-03-01", "authors_parsed": [["Gao", "Yihan", ""], ["Parameswaran", "Aditya", ""], ["Peng", "Jian", ""]]}, {"id": "1506.03039", "submitter": "Jack Gorham", "authors": "Jackson Gorham and Lester Mackey", "title": "Measuring Sample Quality with Stein's Method", "comments": "17 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.PR stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To improve the efficiency of Monte Carlo estimation, practitioners are\nturning to biased Markov chain Monte Carlo procedures that trade off asymptotic\nexactness for computational speed. The reasoning is sound: a reduction in\nvariance due to more rapid sampling can outweigh the bias introduced. However,\nthe inexactness creates new challenges for sampler and parameter selection,\nsince standard measures of sample quality like effective sample size do not\naccount for asymptotic bias. To address these challenges, we introduce a new\ncomputable quality measure based on Stein's method that quantifies the maximum\ndiscrepancy between sample and target expectations over a large class of test\nfunctions. We use our tool to compare exact, biased, and deterministic sample\nsequences and illustrate applications to hyperparameter selection, convergence\nrate assessment, and quantifying bias-variance tradeoffs in posterior\ninference.\n", "versions": [{"version": "v1", "created": "Tue, 9 Jun 2015 18:48:58 GMT"}, {"version": "v2", "created": "Fri, 19 Jun 2015 05:15:18 GMT"}, {"version": "v3", "created": "Sat, 12 Sep 2015 23:31:21 GMT"}, {"version": "v4", "created": "Mon, 11 Jan 2016 03:47:27 GMT"}, {"version": "v5", "created": "Mon, 6 Mar 2017 18:59:16 GMT"}, {"version": "v6", "created": "Tue, 1 Jan 2019 03:07:44 GMT"}], "update_date": "2019-01-03", "authors_parsed": [["Gorham", "Jackson", ""], ["Mackey", "Lester", ""]]}, {"id": "1506.03059", "submitter": "Nadav Cohen", "authors": "Nadav Cohen, Or Sharir and Amnon Shashua", "title": "Deep SimNets", "comments": null, "journal-ref": "The IEEE Conference on Computer Vision and Pattern Recognition\n  (CVPR), 2016, pp. 4782-4791", "doi": null, "report-no": null, "categories": "cs.NE cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a deep layered architecture that generalizes convolutional neural\nnetworks (ConvNets). The architecture, called SimNets, is driven by two\noperators: (i) a similarity function that generalizes inner-product, and (ii) a\nlog-mean-exp function called MEX that generalizes maximum and average. The two\noperators applied in succession give rise to a standard neuron but in \"feature\nspace\". The feature spaces realized by SimNets depend on the choice of the\nsimilarity operator. The simplest setting, which corresponds to a convolution,\nrealizes the feature space of the Exponential kernel, while other settings\nrealize feature spaces of more powerful kernels (Generalized Gaussian, which\nincludes as special cases RBF and Laplacian), or even dynamically learned\nfeature spaces (Generalized Multiple Kernel Learning). As a result, the SimNet\ncontains a higher abstraction level compared to a traditional ConvNet. We argue\nthat enhanced expressiveness is important when the networks are small due to\nrun-time constraints (such as those imposed by mobile applications). Empirical\nevaluation validates the superior expressiveness of SimNets, showing a\nsignificant gain in accuracy over ConvNets when computational resources at\nrun-time are limited. We also show that in large-scale settings, where\ncomputational complexity is less of a concern, the additional capacity of\nSimNets can be controlled with proper regularization, yielding accuracies\ncomparable to state of the art ConvNets.\n", "versions": [{"version": "v1", "created": "Tue, 9 Jun 2015 19:40:05 GMT"}, {"version": "v2", "created": "Tue, 29 Mar 2016 14:30:45 GMT"}], "update_date": "2016-10-18", "authors_parsed": [["Cohen", "Nadav", ""], ["Sharir", "Or", ""], ["Shashua", "Amnon", ""]]}, {"id": "1506.03072", "submitter": "Vijay Kumar", "authors": "Vijay Kumar and Dan Levy", "title": "Clustering by transitive propagation", "comments": "13 pages + 2 appendices, figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cond-mat.stat-mech stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a global optimization algorithm for clustering data given the\nratio of likelihoods that each pair of data points is in the same cluster or in\ndifferent clusters. To define a clustering solution in terms of pairwise\nrelationships, a necessary and sufficient condition is that belonging to the\nsame cluster satisfies transitivity. We define a global objective function\nbased on pairwise likelihood ratios and a transitivity constraint over all\ntriples, assigning an equal prior probability to all clustering solutions. We\nmaximize the objective function by implementing max-sum message passing on the\ncorresponding factor graph to arrive at an O(N^3) algorithm. Lastly, we\ndemonstrate an application inspired by mutational sequencing for decoding\nrandom binary words transmitted through a noisy channel.\n", "versions": [{"version": "v1", "created": "Tue, 9 Jun 2015 20:00:26 GMT"}], "update_date": "2015-06-11", "authors_parsed": [["Kumar", "Vijay", ""], ["Levy", "Dan", ""]]}, {"id": "1506.03099", "submitter": "Samy Bengio", "authors": "Samy Bengio, Oriol Vinyals, Navdeep Jaitly, Noam Shazeer", "title": "Scheduled Sampling for Sequence Prediction with Recurrent Neural\n  Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recurrent Neural Networks can be trained to produce sequences of tokens given\nsome input, as exemplified by recent results in machine translation and image\ncaptioning. The current approach to training them consists of maximizing the\nlikelihood of each token in the sequence given the current (recurrent) state\nand the previous token. At inference, the unknown previous token is then\nreplaced by a token generated by the model itself. This discrepancy between\ntraining and inference can yield errors that can accumulate quickly along the\ngenerated sequence. We propose a curriculum learning strategy to gently change\nthe training process from a fully guided scheme using the true previous token,\ntowards a less guided scheme which mostly uses the generated token instead.\nExperiments on several sequence prediction tasks show that this approach yields\nsignificant improvements. Moreover, it was used successfully in our winning\nentry to the MSCOCO image captioning challenge, 2015.\n", "versions": [{"version": "v1", "created": "Tue, 9 Jun 2015 20:33:47 GMT"}, {"version": "v2", "created": "Mon, 15 Jun 2015 15:29:22 GMT"}, {"version": "v3", "created": "Wed, 23 Sep 2015 16:35:42 GMT"}], "update_date": "2015-09-24", "authors_parsed": [["Bengio", "Samy", ""], ["Vinyals", "Oriol", ""], ["Jaitly", "Navdeep", ""], ["Shazeer", "Noam", ""]]}, {"id": "1506.03101", "submitter": "Bo Dai", "authors": "Bo Dai, Niao He, Hanjun Dai, Le Song", "title": "Provable Bayesian Inference via Particle Mirror Descent", "comments": "38 pages, 26 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bayesian methods are appealing in their flexibility in modeling complex data\nand ability in capturing uncertainty in parameters. However, when Bayes' rule\ndoes not result in tractable closed-form, most approximate inference algorithms\nlack either scalability or rigorous guarantees. To tackle this challenge, we\npropose a simple yet provable algorithm, \\emph{Particle Mirror Descent} (PMD),\nto iteratively approximate the posterior density. PMD is inspired by stochastic\nfunctional mirror descent where one descends in the density space using a small\nbatch of data points at each iteration, and by particle filtering where one\nuses samples to approximate a function. We prove result of the first kind that,\nwith $m$ particles, PMD provides a posterior density estimator that converges\nin terms of $KL$-divergence to the true posterior in rate $O(1/\\sqrt{m})$. We\ndemonstrate competitive empirical performances of PMD compared to several\napproximate inference algorithms in mixture models, logistic regression, sparse\nGaussian processes and latent Dirichlet allocation on large scale datasets.\n", "versions": [{"version": "v1", "created": "Tue, 9 Jun 2015 20:57:37 GMT"}, {"version": "v2", "created": "Tue, 3 May 2016 19:06:18 GMT"}, {"version": "v3", "created": "Thu, 5 May 2016 22:56:13 GMT"}], "update_date": "2016-05-09", "authors_parsed": [["Dai", "Bo", ""], ["He", "Niao", ""], ["Dai", "Hanjun", ""], ["Song", "Le", ""]]}, {"id": "1506.03134", "submitter": "Oriol Vinyals", "authors": "Oriol Vinyals, Meire Fortunato, Navdeep Jaitly", "title": "Pointer Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CG cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a new neural architecture to learn the conditional probability\nof an output sequence with elements that are discrete tokens corresponding to\npositions in an input sequence. Such problems cannot be trivially addressed by\nexistent approaches such as sequence-to-sequence and Neural Turing Machines,\nbecause the number of target classes in each step of the output depends on the\nlength of the input, which is variable. Problems such as sorting variable sized\nsequences, and various combinatorial optimization problems belong to this\nclass. Our model solves the problem of variable size output dictionaries using\na recently proposed mechanism of neural attention. It differs from the previous\nattention attempts in that, instead of using attention to blend hidden units of\nan encoder to a context vector at each decoder step, it uses attention as a\npointer to select a member of the input sequence as the output. We call this\narchitecture a Pointer Net (Ptr-Net). We show Ptr-Nets can be used to learn\napproximate solutions to three challenging geometric problems -- finding planar\nconvex hulls, computing Delaunay triangulations, and the planar Travelling\nSalesman Problem -- using training examples alone. Ptr-Nets not only improve\nover sequence-to-sequence with input attention, but also allow us to generalize\nto variable size output dictionaries. We show that the learnt models generalize\nbeyond the maximum lengths they were trained on. We hope our results on these\ntasks will encourage a broader exploration of neural learning for discrete\nproblems.\n", "versions": [{"version": "v1", "created": "Tue, 9 Jun 2015 23:38:16 GMT"}, {"version": "v2", "created": "Mon, 2 Jan 2017 10:25:29 GMT"}], "update_date": "2017-01-03", "authors_parsed": [["Vinyals", "Oriol", ""], ["Fortunato", "Meire", ""], ["Jaitly", "Navdeep", ""]]}, {"id": "1506.03137", "submitter": "Tselil Schramm", "authors": "Tselil Schramm and Benjamin Weitz", "title": "Symmetric Tensor Completion from Multilinear Entries and Learning\n  Product Mixtures over the Hypercube", "comments": "Removed adversarial matrix completion algorithm after discovering\n  that our matrix completion results can be derived from prior work", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We give an algorithm for completing an order-$m$ symmetric low-rank tensor\nfrom its multilinear entries in time roughly proportional to the number of\ntensor entries. We apply our tensor completion algorithm to the problem of\nlearning mixtures of product distributions over the hypercube, obtaining new\nalgorithmic results. If the centers of the product distribution are linearly\nindependent, then we recover distributions with as many as $\\Omega(n)$ centers\nin polynomial time and sample complexity. In the general case, we recover\ndistributions with as many as $\\tilde\\Omega(n)$ centers in quasi-polynomial\ntime, answering an open problem of Feldman et al. (SIAM J. Comp.) for the\nspecial case of distributions with incoherent bias vectors.\n  Our main algorithmic tool is the iterated application of a low-rank matrix\ncompletion algorithm for matrices with adversarially missing entries.\n", "versions": [{"version": "v1", "created": "Tue, 9 Jun 2015 23:53:42 GMT"}, {"version": "v2", "created": "Thu, 9 Jul 2015 22:54:20 GMT"}, {"version": "v3", "created": "Mon, 23 Nov 2015 21:32:41 GMT"}], "update_date": "2015-11-25", "authors_parsed": [["Schramm", "Tselil", ""], ["Weitz", "Benjamin", ""]]}, {"id": "1506.03159", "submitter": "Dustin Tran", "authors": "Dustin Tran, David M. Blei, Edoardo M. Airoldi", "title": "Copula variational inference", "comments": "Appears in Neural Information Processing Systems, 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop a general variational inference method that preserves dependency\namong the latent variables. Our method uses copulas to augment the families of\ndistributions used in mean-field and structured approximations. Copulas model\nthe dependency that is not captured by the original variational distribution,\nand thus the augmented variational family guarantees better approximations to\nthe posterior. With stochastic optimization, inference on the augmented\ndistribution is scalable. Furthermore, our strategy is generic: it can be\napplied to any inference procedure that currently uses the mean-field or\nstructured approach. Copula variational inference has many advantages: it\nreduces bias; it is less sensitive to local optima; it is less sensitive to\nhyperparameters; and it helps characterize and interpret the dependency among\nthe latent variables.\n", "versions": [{"version": "v1", "created": "Wed, 10 Jun 2015 04:14:22 GMT"}, {"version": "v2", "created": "Sat, 31 Oct 2015 06:52:07 GMT"}], "update_date": "2015-11-03", "authors_parsed": [["Tran", "Dustin", ""], ["Blei", "David M.", ""], ["Airoldi", "Edoardo M.", ""]]}, {"id": "1506.03163", "submitter": "Leonid Boytsov", "authors": "Bilegsaikhan Naidan, Leonid Boytsov, Eric Nyberg", "title": "Permutation Search Methods are Efficient, Yet Faster Search is Possible", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DB cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We survey permutation-based methods for approximate k-nearest neighbor\nsearch. In these methods, every data point is represented by a ranked list of\npivots sorted by the distance to this point. Such ranked lists are called\npermutations. The underpinning assumption is that, for both metric and\nnon-metric spaces, the distance between permutations is a good proxy for the\ndistance between original points. Thus, it should be possible to efficiently\nretrieve most true nearest neighbors by examining only a tiny subset of data\npoints whose permutations are similar to the permutation of a query. We further\ntest this assumption by carrying out an extensive experimental evaluation where\npermutation methods are pitted against state-of-the art benchmarks (the\nmulti-probe LSH, the VP-tree, and proximity-graph based retrieval) on a variety\nof realistically large data set from the image and textual domain. The focus is\non the high-accuracy retrieval methods for generic spaces. Additionally, we\nassume that both data and indices are stored in main memory. We find\npermutation methods to be reasonably efficient and describe a setup where these\nmethods are most useful. To ease reproducibility, we make our software and data\nsets publicly available.\n", "versions": [{"version": "v1", "created": "Wed, 10 Jun 2015 04:50:29 GMT"}, {"version": "v2", "created": "Sun, 14 Jun 2015 10:21:06 GMT"}, {"version": "v3", "created": "Sun, 21 Jun 2015 20:35:03 GMT"}, {"version": "v4", "created": "Mon, 31 Oct 2016 18:50:48 GMT"}], "update_date": "2016-11-01", "authors_parsed": [["Naidan", "Bilegsaikhan", ""], ["Boytsov", "Leonid", ""], ["Nyberg", "Eric", ""]]}, {"id": "1506.03271", "submitter": "Gergely Neu", "authors": "Gergely Neu", "title": "Explore no more: Improved high-probability regret bounds for\n  non-stochastic bandits", "comments": "To appear at NIPS 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work addresses the problem of regret minimization in non-stochastic\nmulti-armed bandit problems, focusing on performance guarantees that hold with\nhigh probability. Such results are rather scarce in the literature since\nproving them requires a large deal of technical effort and significant\nmodifications to the standard, more intuitive algorithms that come only with\nguarantees that hold on expectation. One of these modifications is forcing the\nlearner to sample arms from the uniform distribution at least\n$\\Omega(\\sqrt{T})$ times over $T$ rounds, which can adversely affect\nperformance if many of the arms are suboptimal. While it is widely conjectured\nthat this property is essential for proving high-probability regret bounds, we\nshow in this paper that it is possible to achieve such strong results without\nthis undesirable exploration component. Our result relies on a simple and\nintuitive loss-estimation strategy called Implicit eXploration (IX) that allows\na remarkably clean analysis. To demonstrate the flexibility of our technique,\nwe derive several improved high-probability bounds for various extensions of\nthe standard multi-armed bandit framework. Finally, we conduct a simple\nexperiment that illustrates the robustness of our implicit exploration\ntechnique.\n", "versions": [{"version": "v1", "created": "Wed, 10 Jun 2015 12:19:21 GMT"}, {"version": "v2", "created": "Thu, 16 Jul 2015 12:59:46 GMT"}, {"version": "v3", "created": "Tue, 3 Nov 2015 08:42:39 GMT"}], "update_date": "2015-11-04", "authors_parsed": [["Neu", "Gergely", ""]]}, {"id": "1506.03338", "submitter": "Shixiang Gu", "authors": "Shixiang Gu and Zoubin Ghahramani and Richard E. Turner", "title": "Neural Adaptive Sequential Monte Carlo", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sequential Monte Carlo (SMC), or particle filtering, is a popular class of\nmethods for sampling from an intractable target distribution using a sequence\nof simpler intermediate distributions. Like other importance sampling-based\nmethods, performance is critically dependent on the proposal distribution: a\nbad proposal can lead to arbitrarily inaccurate estimates of the target\ndistribution. This paper presents a new method for automatically adapting the\nproposal using an approximation of the Kullback-Leibler divergence between the\ntrue posterior and the proposal distribution. The method is very flexible,\napplicable to any parameterized proposal distribution and it supports online\nand batch variants. We use the new framework to adapt powerful proposal\ndistributions with rich parameterizations based upon neural networks leading to\nNeural Adaptive Sequential Monte Carlo (NASMC). Experiments indicate that NASMC\nsignificantly improves inference in a non-linear state space model\noutperforming adaptive proposal methods including the Extended Kalman and\nUnscented Particle Filters. Experiments also indicate that improved inference\ntranslates into improved parameter learning when NASMC is used as a subroutine\nof Particle Marginal Metropolis Hastings. Finally we show that NASMC is able to\ntrain a latent variable recurrent neural network (LV-RNN) achieving results\nthat compete with the state-of-the-art for polymorphic music modelling. NASMC\ncan be seen as bridging the gap between adaptive SMC methods and the recent\nwork in scalable, black-box variational inference.\n", "versions": [{"version": "v1", "created": "Wed, 10 Jun 2015 14:52:09 GMT"}, {"version": "v2", "created": "Thu, 11 Jun 2015 07:11:56 GMT"}, {"version": "v3", "created": "Mon, 16 Nov 2015 21:28:48 GMT"}], "update_date": "2015-11-18", "authors_parsed": [["Gu", "Shixiang", ""], ["Ghahramani", "Zoubin", ""], ["Turner", "Richard E.", ""]]}, {"id": "1506.03374", "submitter": "Shipra Agrawal", "authors": "Shipra Agrawal and Nikhil R. Devanur and Lihong Li", "title": "An efficient algorithm for contextual bandits with knapsacks, and an\n  extension to concave objectives", "comments": "Extended abstract appeared in COLT 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider a contextual version of multi-armed bandit problem with global\nknapsack constraints. In each round, the outcome of pulling an arm is a scalar\nreward and a resource consumption vector, both dependent on the context, and\nthe global knapsack constraints require the total consumption for each resource\nto be below some pre-fixed budget. The learning agent competes with an\narbitrary set of context-dependent policies. This problem was introduced by\nBadanidiyuru et al. (2014), who gave a computationally inefficient algorithm\nwith near-optimal regret bounds for it. We give a computationally efficient\nalgorithm for this problem with slightly better regret bounds, by generalizing\nthe approach of Agarwal et al. (2014) for the non-constrained version of the\nproblem. The computational time of our algorithm scales logarithmically in the\nsize of the policy space. This answers the main open question of Badanidiyuru\net al. (2014). We also extend our results to a variant where there are no\nknapsack constraints but the objective is an arbitrary Lipschitz concave\nfunction of the sum of outcome vectors.\n", "versions": [{"version": "v1", "created": "Wed, 10 Jun 2015 16:14:19 GMT"}, {"version": "v2", "created": "Sat, 9 Jul 2016 05:46:06 GMT"}], "update_date": "2016-07-12", "authors_parsed": [["Agrawal", "Shipra", ""], ["Devanur", "Nikhil R.", ""], ["Li", "Lihong", ""]]}, {"id": "1506.03378", "submitter": "Lihong Li", "authors": "Che-Yu Liu and Lihong Li", "title": "On the Prior Sensitivity of Thompson Sampling", "comments": "Appears in the 27th International Conference on Algorithmic Learning\n  Theory (ALT), 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The empirically successful Thompson Sampling algorithm for stochastic bandits\nhas drawn much interest in understanding its theoretical properties. One\nimportant benefit of the algorithm is that it allows domain knowledge to be\nconveniently encoded as a prior distribution to balance exploration and\nexploitation more effectively. While it is generally believed that the\nalgorithm's regret is low (high) when the prior is good (bad), little is known\nabout the exact dependence. In this paper, we fully characterize the\nalgorithm's worst-case dependence of regret on the choice of prior, focusing on\na special yet representative case. These results also provide insights into the\ngeneral sensitivity of the algorithm to the choice of priors. In particular,\nwith $p$ being the prior probability mass of the true reward-generating model,\nwe prove $O(\\sqrt{T/p})$ and $O(\\sqrt{(1-p)T})$ regret upper bounds for the\nbad- and good-prior cases, respectively, as well as \\emph{matching} lower\nbounds. Our proofs rely on the discovery of a fundamental property of Thompson\nSampling and make heavy use of martingale theory, both of which appear novel in\nthe literature, to the best of our knowledge.\n", "versions": [{"version": "v1", "created": "Wed, 10 Jun 2015 16:22:26 GMT"}, {"version": "v2", "created": "Thu, 21 Jul 2016 01:43:09 GMT"}], "update_date": "2016-07-22", "authors_parsed": [["Liu", "Che-Yu", ""], ["Li", "Lihong", ""]]}, {"id": "1506.03379", "submitter": "Lihong Li", "authors": "Emma Brunskill and Lihong Li", "title": "The Online Coupon-Collector Problem and Its Application to Lifelong\n  Reinforcement Learning", "comments": "13 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Transferring knowledge across a sequence of related tasks is an important\nchallenge in reinforcement learning (RL). Despite much encouraging empirical\nevidence, there has been little theoretical analysis. In this paper, we study a\nclass of lifelong RL problems: the agent solves a sequence of tasks modeled as\nfinite Markov decision processes (MDPs), each of which is from a finite set of\nMDPs with the same state/action sets and different transition/reward functions.\nMotivated by the need for cross-task exploration in lifelong learning, we\nformulate a novel online coupon-collector problem and give an optimal\nalgorithm. This allows us to develop a new lifelong RL algorithm, whose overall\nsample complexity in a sequence of tasks is much smaller than single-task\nlearning, even if the sequence of tasks is generated by an adversary. Benefits\nof the algorithm are demonstrated in simulated problems, including a recently\nintroduced human-robot interaction problem.\n", "versions": [{"version": "v1", "created": "Wed, 10 Jun 2015 16:23:29 GMT"}, {"version": "v2", "created": "Mon, 21 Sep 2015 22:55:59 GMT"}], "update_date": "2015-09-23", "authors_parsed": [["Brunskill", "Emma", ""], ["Li", "Lihong", ""]]}, {"id": "1506.03410", "submitter": "Benjamin Falk", "authors": "Tyler M. Tomita, James Browne, Cencheng Shen, Jaewon Chung, Jesse L.\n  Patsolic, Benjamin Falk, Jason Yim, Carey E. Priebe, Randal Burns, Mauro\n  Maggioni, Joshua T. Vogelstein", "title": "Sparse Projection Oblique Randomer Forests", "comments": "31 pages; submitted to Journal of Machine Learning Research for\n  review", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Decision forests, including Random Forests and Gradient Boosting Trees, have\nrecently demonstrated state-of-the-art performance in a variety of machine\nlearning settings. Decision forests are typically ensembles of axis-aligned\ndecision trees; that is, trees that split only along feature dimensions. In\ncontrast, many recent extensions to decision forests are based on axis-oblique\nsplits. Unfortunately, these extensions forfeit one or more of the favorable\nproperties of decision forests based on axis-aligned splits, such as robustness\nto many noise dimensions, interpretability, or computational efficiency. We\nintroduce yet another decision forest, called \"Sparse Projection Oblique\nRandomer Forests\" (SPORF). SPORF uses very sparse random projections, i.e.,\nlinear combinations of a small subset of features. SPORF significantly improves\naccuracy over existing state-of-the-art algorithms on a standard benchmark\nsuite for classification with >100 problems of varying dimension, sample size,\nand number of classes. To illustrate how SPORF addresses the limitations of\nboth axis-aligned and existing oblique decision forest methods, we conduct\nextensive simulated experiments. SPORF typically yields improved performance\nover existing decision forests, while mitigating computational efficiency and\nscalability and maintaining interpretability. SPORF can easily be incorporated\ninto other ensemble methods such as boosting to obtain potentially similar\ngains.\n", "versions": [{"version": "v1", "created": "Wed, 10 Jun 2015 17:55:51 GMT"}, {"version": "v2", "created": "Wed, 4 May 2016 18:14:39 GMT"}, {"version": "v3", "created": "Mon, 19 Mar 2018 21:51:16 GMT"}, {"version": "v4", "created": "Wed, 10 Oct 2018 00:36:04 GMT"}, {"version": "v5", "created": "Mon, 30 Sep 2019 21:35:19 GMT"}, {"version": "v6", "created": "Thu, 3 Oct 2019 14:04:32 GMT"}], "update_date": "2019-10-04", "authors_parsed": [["Tomita", "Tyler M.", ""], ["Browne", "James", ""], ["Shen", "Cencheng", ""], ["Chung", "Jaewon", ""], ["Patsolic", "Jesse L.", ""], ["Falk", "Benjamin", ""], ["Yim", "Jason", ""], ["Priebe", "Carey E.", ""], ["Burns", "Randal", ""], ["Maggioni", "Mauro", ""], ["Vogelstein", "Joshua T.", ""]]}, {"id": "1506.03412", "submitter": "Vamsi Ithapu", "authors": "Vamsi K. Ithapu, Sathya Ravi, Vikas Singh", "title": "Convergence rates for pretraining and dropout: Guiding learning\n  parameters using network structure", "comments": "This manuscript is now superseded by arXiv:1511.05297 and the\n  corresponding accepted paper in 54th Allerton Conference on Communication,\n  Control and Computing (2017)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.NE math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Unsupervised pretraining and dropout have been well studied, especially with\nrespect to regularization and output consistency. However, our understanding\nabout the explicit convergence rates of the parameter estimates, and their\ndependence on the learning (like denoising and dropout rate) and structural\n(like depth and layer lengths) aspects of the network is less mature. An\ninteresting question in this context is to ask if the network structure could\n\"guide\" the choices of such learning parameters. In this work, we explore these\ngaps between network structure, the learning mechanisms and their interaction\nwith parameter convergence rates. We present a way to address these issues\nbased on the backpropagation convergence rates for general nonconvex objectives\nusing first-order information. We then incorporate two learning mechanisms into\nthis general framework -- denoising autoencoder and dropout, and subsequently\nderive the convergence rates of deep networks. Building upon these bounds, we\nprovide insights into the choices of learning parameters and network sizes that\nachieve certain levels of convergence accuracy. The results derived here\nsupport existing empirical observations, and we also conduct a set of\nexperiments to evaluate them.\n", "versions": [{"version": "v1", "created": "Wed, 10 Jun 2015 17:59:57 GMT"}, {"version": "v2", "created": "Tue, 17 Nov 2015 04:52:53 GMT"}, {"version": "v3", "created": "Wed, 22 Feb 2017 17:32:07 GMT"}], "update_date": "2017-02-23", "authors_parsed": [["Ithapu", "Vamsi K.", ""], ["Ravi", "Sathya", ""], ["Singh", "Vikas", ""]]}, {"id": "1506.03425", "submitter": "Krzysztof Choromanski", "authors": "Krzysztof Choromanski and Sanjiv Kumar and Xiaofeng Liu", "title": "Fast Online Clustering with Randomized Skeleton Sets", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a new fast online clustering algorithm that reliably recovers\narbitrary-shaped data clusters in high throughout data streams. Unlike the\nexisting state-of-the-art online clustering methods based on k-means or\nk-medoid, it does not make any restrictive generative assumptions. In addition,\nin contrast to existing nonparametric clustering techniques such as DBScan or\nDenStream, it gives provable theoretical guarantees. To achieve fast\nclustering, we propose to represent each cluster by a skeleton set which is\nupdated continuously as new data is seen. A skeleton set consists of weighted\nsamples from the data where weights encode local densities. The size of each\nskeleton set is adapted according to the cluster geometry. The proposed\ntechnique automatically detects the number of clusters and is robust to\noutliers. The algorithm works for the infinite data stream where more than one\npass over the data is not feasible. We provide theoretical guarantees on the\nquality of the clustering and also demonstrate its advantage over the existing\nstate-of-the-art on several datasets.\n", "versions": [{"version": "v1", "created": "Wed, 10 Jun 2015 18:41:55 GMT"}], "update_date": "2015-06-11", "authors_parsed": [["Choromanski", "Krzysztof", ""], ["Kumar", "Sanjiv", ""], ["Liu", "Xiaofeng", ""]]}, {"id": "1506.03478", "submitter": "Lucas Theis", "authors": "Lucas Theis and Matthias Bethge", "title": "Generative Image Modeling Using Spatial LSTMs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modeling the distribution of natural images is challenging, partly because of\nstrong statistical dependencies which can extend over hundreds of pixels.\nRecurrent neural networks have been successful in capturing long-range\ndependencies in a number of problems but only recently have found their way\ninto generative image models. We here introduce a recurrent image model based\non multi-dimensional long short-term memory units which are particularly suited\nfor image modeling due to their spatial structure. Our model scales to images\nof arbitrary size and its likelihood is computationally tractable. We find that\nit outperforms the state of the art in quantitative comparisons on several\nimage datasets and produces promising results when used for texture synthesis\nand inpainting.\n", "versions": [{"version": "v1", "created": "Wed, 10 Jun 2015 20:56:14 GMT"}, {"version": "v2", "created": "Fri, 18 Sep 2015 08:06:06 GMT"}], "update_date": "2015-09-21", "authors_parsed": [["Theis", "Lucas", ""], ["Bethge", "Matthias", ""]]}, {"id": "1506.03486", "submitter": "Akshay Balsubramani", "authors": "Akshay Balsubramani, Aaditya Ramdas", "title": "Sequential Nonparametric Testing with the Law of the Iterated Logarithm", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new algorithmic framework for sequential hypothesis testing with\ni.i.d. data, which includes A/B testing, nonparametric two-sample testing, and\nindependence testing as special cases. It is novel in several ways: (a) it\ntakes linear time and constant space to compute on the fly, (b) it has the same\npower guarantee as a non-sequential version of the test with the same\ncomputational constraints up to a small factor, and (c) it accesses only as\nmany samples as are required - its stopping time adapts to the unknown\ndifficulty of the problem. All our test statistics are constructed to be\nzero-mean martingales under the null hypothesis, and the rejection threshold is\ngoverned by a uniform non-asymptotic law of the iterated logarithm (LIL). For\nthe case of nonparametric two-sample mean testing, we also provide a finite\nsample power analysis, and the first non-asymptotic stopping time calculations\nfor this class of problems. We verify our predictions for type I and II errors\nand stopping times using simulations.\n", "versions": [{"version": "v1", "created": "Wed, 10 Jun 2015 21:28:38 GMT"}, {"version": "v2", "created": "Wed, 2 Mar 2016 02:11:19 GMT"}], "update_date": "2016-03-03", "authors_parsed": [["Balsubramani", "Akshay", ""], ["Ramdas", "Aaditya", ""]]}, {"id": "1506.03493", "submitter": "Aaron Schein", "authors": "Aaron Schein, John Paisley, David M. Blei, Hanna Wallach", "title": "Bayesian Poisson Tensor Factorization for Inferring Multilateral\n  Relations from Sparse Dyadic Event Counts", "comments": "To appear in Proceedings of the 21st ACM SIGKDD Conference of\n  Knowledge Discovery and Data Mining (KDD 2015)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.LG cs.SI stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a Bayesian tensor factorization model for inferring latent group\nstructures from dynamic pairwise interaction patterns. For decades, political\nscientists have collected and analyzed records of the form \"country $i$ took\naction $a$ toward country $j$ at time $t$\"---known as dyadic events---in order\nto form and test theories of international relations. We represent these event\ndata as a tensor of counts and develop Bayesian Poisson tensor factorization to\ninfer a low-dimensional, interpretable representation of their salient\npatterns. We demonstrate that our model's predictive performance is better than\nthat of standard non-negative tensor factorization methods. We also provide a\ncomparison of our variational updates to their maximum likelihood counterparts.\nIn doing so, we identify a better way to form point estimates of the latent\nfactors than that typically used in Bayesian Poisson matrix factorization.\nFinally, we showcase our model as an exploratory analysis tool for political\nscientists. We show that the inferred latent factor matrices capture\ninterpretable multilateral relations that both conform to and inform our\nknowledge of international affairs.\n", "versions": [{"version": "v1", "created": "Wed, 10 Jun 2015 21:49:31 GMT"}], "update_date": "2015-06-12", "authors_parsed": [["Schein", "Aaron", ""], ["Paisley", "John", ""], ["Blei", "David M.", ""], ["Wallach", "Hanna", ""]]}, {"id": "1506.03498", "submitter": "Alaa Saade", "authors": "Alaa Saade, Florent Krzakala and Lenka Zdeborov\\'a", "title": "Matrix Completion from Fewer Entries: Spectral Detectability and Rank\n  Estimation", "comments": "NIPS Conference 2015", "journal-ref": "Advances in Neural Information Processing Systems (NIPS 2015) 28,\n  pages 1261--1269", "doi": null, "report-no": null, "categories": "cond-mat.dis-nn cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The completion of low rank matrices from few entries is a task with many\npractical applications. We consider here two aspects of this problem:\ndetectability, i.e. the ability to estimate the rank $r$ reliably from the\nfewest possible random entries, and performance in achieving small\nreconstruction error. We propose a spectral algorithm for these two tasks\ncalled MaCBetH (for Matrix Completion with the Bethe Hessian). The rank is\nestimated as the number of negative eigenvalues of the Bethe Hessian matrix,\nand the corresponding eigenvectors are used as initial condition for the\nminimization of the discrepancy between the estimated matrix and the revealed\nentries. We analyze the performance in a random matrix setting using results\nfrom the statistical mechanics of the Hopfield neural network, and show in\nparticular that MaCBetH efficiently detects the rank $r$ of a large $n\\times m$\nmatrix from $C(r)r\\sqrt{nm}$ entries, where $C(r)$ is a constant close to $1$.\nWe also evaluate the corresponding root-mean-square error empirically and show\nthat MaCBetH compares favorably to other existing approaches.\n", "versions": [{"version": "v1", "created": "Wed, 10 Jun 2015 22:46:02 GMT"}, {"version": "v2", "created": "Tue, 30 Jun 2015 17:15:13 GMT"}, {"version": "v3", "created": "Thu, 28 Jan 2016 10:16:56 GMT"}], "update_date": "2016-02-11", "authors_parsed": [["Saade", "Alaa", ""], ["Krzakala", "Florent", ""], ["Zdeborov\u00e1", "Lenka", ""]]}, {"id": "1506.03504", "submitter": "Philip Bachman", "authors": "Philip Bachman and Doina Precup", "title": "Data Generation as Sequential Decision Making", "comments": "Accepted for publication at Advances in Neural Information Processing\n  Systems (NIPS) 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We connect a broad class of generative models through their shared reliance\non sequential decision making. Motivated by this view, we develop extensions to\nan existing model, and then explore the idea further in the context of data\nimputation -- perhaps the simplest setting in which to investigate the relation\nbetween unconditional and conditional generative modelling. We formulate data\nimputation as an MDP and develop models capable of representing effective\npolicies for it. We construct the models using neural networks and train them\nusing a form of guided policy search. Our models generate predictions through\nan iterative process of feedback and refinement. We show that this approach can\nlearn effective policies for imputation problems of varying difficulty and\nacross multiple datasets.\n", "versions": [{"version": "v1", "created": "Wed, 10 Jun 2015 23:17:24 GMT"}, {"version": "v2", "created": "Sun, 1 Nov 2015 00:31:11 GMT"}, {"version": "v3", "created": "Tue, 3 Nov 2015 01:16:31 GMT"}], "update_date": "2015-11-04", "authors_parsed": [["Bachman", "Philip", ""], ["Precup", "Doina", ""]]}, {"id": "1506.03509", "submitter": "Furong Huang", "authors": "Furong Huang, Animashree Anandkumar", "title": "Convolutional Dictionary Learning through Tensor Factorization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Tensor methods have emerged as a powerful paradigm for consistent learning of\nmany latent variable models such as topic models, independent component\nanalysis and dictionary learning. Model parameters are estimated via CP\ndecomposition of the observed higher order input moments. However, in many\ndomains, additional invariances such as shift invariances exist, enforced via\nmodels such as convolutional dictionary learning. In this paper, we develop\nnovel tensor decomposition algorithms for parameter estimation of convolutional\nmodels. Our algorithm is based on the popular alternating least squares method,\nbut with efficient projections onto the space of stacked circulant matrices.\nOur method is embarrassingly parallel and consists of simple operations such as\nfast Fourier transforms and matrix multiplications. Our algorithm converges to\nthe dictionary much faster and more accurately compared to the alternating\nminimization over filters and activation maps.\n", "versions": [{"version": "v1", "created": "Wed, 10 Jun 2015 23:48:18 GMT"}, {"version": "v2", "created": "Fri, 12 Jun 2015 22:46:18 GMT"}, {"version": "v3", "created": "Thu, 18 Jun 2015 21:04:28 GMT"}], "update_date": "2015-06-22", "authors_parsed": [["Huang", "Furong", ""], ["Anandkumar", "Animashree", ""]]}, {"id": "1506.03623", "submitter": "Han Xiao Bookman", "authors": "Han Xiao, Xiaoyan Zhu", "title": "Max-Entropy Feed-Forward Clustering Neural Network", "comments": "This paper has been published in ICANN 2015", "journal-ref": "ICANN 2015: International Conference on Artificial Neural\n  Networks, Amsterdam, The Netherlands, (May 14-15, 2015)", "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The outputs of non-linear feed-forward neural network are positive, which\ncould be treated as probability when they are normalized to one. If we take\nEntropy-Based Principle into consideration, the outputs for each sample could\nbe represented as the distribution of this sample for different clusters.\nEntropy-Based Principle is the principle with which we could estimate the\nunknown distribution under some limited conditions. As this paper defines two\nprocesses in Feed-Forward Neural Network, our limited condition is the\nabstracted features of samples which are worked out in the abstraction process.\nAnd the final outputs are the probability distribution for different clusters\nin the clustering process. As Entropy-Based Principle is considered into the\nfeed-forward neural network, a clustering method is born. We have conducted\nsome experiments on six open UCI datasets, comparing with a few baselines and\napplied purity as the measurement . The results illustrate that our method\noutperforms all the other baselines that are most popular clustering methods.\n", "versions": [{"version": "v1", "created": "Thu, 11 Jun 2015 11:01:40 GMT"}], "update_date": "2015-06-12", "authors_parsed": [["Xiao", "Han", ""], ["Zhu", "Xiaoyan", ""]]}, {"id": "1506.03626", "submitter": "Han Xiao Bookman", "authors": "Han Xiao, Xiaoyan Zhu", "title": "Margin-Based Feed-Forward Neural Network Classifiers", "comments": "This paper has been published in ICANN 2015: International Conference\n  on Artificial Neural Networks, Amsterdam, The Netherlands, (May 14-15, 2015)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Margin-Based Principle has been proposed for a long time, it has been proved\nthat this principle could reduce the structural risk and improve the\nperformance in both theoretical and practical aspects. Meanwhile, feed-forward\nneural network is a traditional classifier, which is very hot at present with a\ndeeper architecture. However, the training algorithm of feed-forward neural\nnetwork is developed and generated from Widrow-Hoff Principle that means to\nminimize the squared error. In this paper, we propose a new training algorithm\nfor feed-forward neural networks based on Margin-Based Principle, which could\neffectively promote the accuracy and generalization ability of neural network\nclassifiers with less labelled samples and flexible network. We have conducted\nexperiments on four UCI open datasets and achieved good results as expected. In\nconclusion, our model could handle more sparse labelled and more high-dimension\ndataset in a high accuracy while modification from old ANN method to our method\nis easy and almost free of work.\n", "versions": [{"version": "v1", "created": "Thu, 11 Jun 2015 11:10:25 GMT"}], "update_date": "2015-06-12", "authors_parsed": [["Xiao", "Han", ""], ["Zhu", "Xiaoyan", ""]]}, {"id": "1506.03648", "submitter": "Deepak Pathak", "authors": "Deepak Pathak, Philipp Kr\\\"ahenb\\\"uhl and Trevor Darrell", "title": "Constrained Convolutional Neural Networks for Weakly Supervised\n  Segmentation", "comments": "12 pages, ICCV 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an approach to learn a dense pixel-wise labeling from image-level\ntags. Each image-level tag imposes constraints on the output labeling of a\nConvolutional Neural Network (CNN) classifier. We propose Constrained CNN\n(CCNN), a method which uses a novel loss function to optimize for any set of\nlinear constraints on the output space (i.e. predicted label distribution) of a\nCNN. Our loss formulation is easy to optimize and can be incorporated directly\ninto standard stochastic gradient descent optimization. The key idea is to\nphrase the training objective as a biconvex optimization for linear models,\nwhich we then relax to nonlinear deep networks. Extensive experiments\ndemonstrate the generality of our new learning framework. The constrained loss\nyields state-of-the-art results on weakly supervised semantic image\nsegmentation. We further demonstrate that adding slightly more supervision can\ngreatly improve the performance of the learning algorithm.\n", "versions": [{"version": "v1", "created": "Thu, 11 Jun 2015 12:30:17 GMT"}, {"version": "v2", "created": "Sun, 18 Oct 2015 23:51:40 GMT"}], "update_date": "2015-10-20", "authors_parsed": [["Pathak", "Deepak", ""], ["Kr\u00e4henb\u00fchl", "Philipp", ""], ["Darrell", "Trevor", ""]]}, {"id": "1506.03662", "submitter": "Simon Lacoste-Julien", "authors": "Thomas Hofmann, Aurelien Lucchi, Simon Lacoste-Julien, Brian\n  McWilliams", "title": "Variance Reduced Stochastic Gradient Descent with Neighbors", "comments": "Appears in: Advances in Neural Information Processing Systems 28\n  (NIPS 2015). 13 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Stochastic Gradient Descent (SGD) is a workhorse in machine learning, yet its\nslow convergence can be a computational bottleneck. Variance reduction\ntechniques such as SAG, SVRG and SAGA have been proposed to overcome this\nweakness, achieving linear convergence. However, these methods are either based\non computations of full gradients at pivot points, or on keeping per data point\ncorrections in memory. Therefore speed-ups relative to SGD may need a minimal\nnumber of epochs in order to materialize. This paper investigates algorithms\nthat can exploit neighborhood structure in the training data to share and\nre-use information about past stochastic gradients across data points, which\noffers advantages in the transient optimization phase. As a side-product we\nprovide a unified convergence analysis for a family of variance reduction\nalgorithms, which we call memorization algorithms. We provide experimental\nresults supporting our theory.\n", "versions": [{"version": "v1", "created": "Thu, 11 Jun 2015 13:14:33 GMT"}, {"version": "v2", "created": "Thu, 5 Nov 2015 12:30:49 GMT"}, {"version": "v3", "created": "Tue, 17 Nov 2015 22:00:11 GMT"}, {"version": "v4", "created": "Fri, 26 Feb 2016 19:55:56 GMT"}], "update_date": "2016-02-29", "authors_parsed": [["Hofmann", "Thomas", ""], ["Lucchi", "Aurelien", ""], ["Lacoste-Julien", "Simon", ""], ["McWilliams", "Brian", ""]]}, {"id": "1506.03693", "submitter": "Edward Meeds", "authors": "Edward Meeds and Max Welling", "title": "Optimization Monte Carlo: Efficient and Embarrassingly Parallel\n  Likelihood-Free Inference", "comments": "NIPS 2015 camera ready", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe an embarrassingly parallel, anytime Monte Carlo method for\nlikelihood-free models. The algorithm starts with the view that the\nstochasticity of the pseudo-samples generated by the simulator can be\ncontrolled externally by a vector of random numbers u, in such a way that the\noutcome, knowing u, is deterministic. For each instantiation of u we run an\noptimization procedure to minimize the distance between summary statistics of\nthe simulator and the data. After reweighing these samples using the prior and\nthe Jacobian (accounting for the change of volume in transforming from the\nspace of summary statistics to the space of parameters) we show that this\nweighted ensemble represents a Monte Carlo estimate of the posterior\ndistribution. The procedure can be run embarrassingly parallel (each node\nhandling one sample) and anytime (by allocating resources to the worst\nperforming sample). The procedure is validated on six experiments.\n", "versions": [{"version": "v1", "created": "Thu, 11 Jun 2015 14:45:30 GMT"}, {"version": "v2", "created": "Wed, 2 Dec 2015 18:58:09 GMT"}], "update_date": "2015-12-03", "authors_parsed": [["Meeds", "Edward", ""], ["Welling", "Max", ""]]}, {"id": "1506.03705", "submitter": "Youssef  Mroueh", "authors": "Youssef Mroueh, Steven Rennie, Vaibhava Goel", "title": "Random Maxout Features", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose and study random maxout features, which are\nconstructed by first projecting the input data onto sets of randomly generated\nvectors with Gaussian elements, and then outputing the maximum projection value\nfor each set. We show that the resulting random feature map, when used in\nconjunction with linear models, allows for the locally linear estimation of the\nfunction of interest in classification tasks, and for the locally linear\nembedding of points when used for dimensionality reduction or data\nvisualization. We derive generalization bounds for learning that assess the\nerror in approximating locally linear functions by linear functions in the\nmaxout feature space, and empirically evaluate the efficacy of the approach on\nthe MNIST and TIMIT classification tasks.\n", "versions": [{"version": "v1", "created": "Thu, 11 Jun 2015 15:19:47 GMT"}, {"version": "v2", "created": "Fri, 12 Jun 2015 19:18:28 GMT"}], "update_date": "2015-06-15", "authors_parsed": [["Mroueh", "Youssef", ""], ["Rennie", "Steven", ""], ["Goel", "Vaibhava", ""]]}, {"id": "1506.03729", "submitter": "Emmanuel Abbe A", "authors": "Emmanuel Abbe and Colin Sandon", "title": "Recovering communities in the general stochastic block model without\n  knowing the parameters", "comments": "arXiv admin note: substantial text overlap with arXiv:1503.00609", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR cs.IT cs.LG cs.SI math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most recent developments on the stochastic block model (SBM) rely on the\nknowledge of the model parameters, or at least on the number of communities.\nThis paper introduces efficient algorithms that do not require such knowledge\nand yet achieve the optimal information-theoretic tradeoffs identified in\n[AS15] for linear size communities. The results are three-fold: (i) in the\nconstant degree regime, an algorithm is developed that requires only a\nlower-bound on the relative sizes of the communities and detects communities\nwith an optimal accuracy scaling for large degrees; (ii) in the regime where\ndegrees are scaled by $\\omega(1)$ (diverging degrees), this is enhanced into a\nfully agnostic algorithm that only takes the graph in question and\nsimultaneously learns the model parameters (including the number of\ncommunities) and detects communities with accuracy $1-o(1)$, with an overall\nquasi-linear complexity; (iii) in the logarithmic degree regime, an agnostic\nalgorithm is developed that learns the parameters and achieves the optimal\nCH-limit for exact recovery, in quasi-linear time. These provide the first\nalgorithms affording efficiency, universality and information-theoretic\noptimality for strong and weak consistency in the general SBM with linear size\ncommunities.\n", "versions": [{"version": "v1", "created": "Thu, 11 Jun 2015 16:09:28 GMT"}], "update_date": "2015-06-12", "authors_parsed": [["Abbe", "Emmanuel", ""], ["Sandon", "Colin", ""]]}, {"id": "1506.03736", "submitter": "Joseph  Salmon", "authors": "Eugene Ndiaye, Olivier Fercoq, Alexandre Gramfort, Joseph Salmon", "title": "GAP Safe screening rules for sparse multi-task and multi-class models", "comments": "in Proceedings of the 29-th Conference on Neural Information\n  Processing Systems (NIPS), 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.OC stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  High dimensional regression benefits from sparsity promoting regularizations.\nScreening rules leverage the known sparsity of the solution by ignoring some\nvariables in the optimization, hence speeding up solvers. When the procedure is\nproven not to discard features wrongly the rules are said to be \\emph{safe}. In\nthis paper we derive new safe rules for generalized linear models regularized\nwith $\\ell_1$ and $\\ell_1/\\ell_2$ norms. The rules are based on duality gap\ncomputations and spherical safe regions whose diameters converge to zero. This\nallows to discard safely more variables, in particular for low regularization\nparameters. The GAP Safe rule can cope with any iterative solver and we\nillustrate its performance on coordinate descent for multi-task Lasso, binary\nand multinomial logistic regression, demonstrating significant speed ups on all\ntested datasets with respect to previous safe rules.\n", "versions": [{"version": "v1", "created": "Thu, 11 Jun 2015 16:25:36 GMT"}, {"version": "v2", "created": "Wed, 18 Nov 2015 10:07:20 GMT"}], "update_date": "2015-11-19", "authors_parsed": [["Ndiaye", "Eugene", ""], ["Fercoq", "Olivier", ""], ["Gramfort", "Alexandre", ""], ["Salmon", "Joseph", ""]]}, {"id": "1506.03767", "submitter": "Oren Rippel", "authors": "Oren Rippel, Jasper Snoek and Ryan P. Adams", "title": "Spectral Representations for Convolutional Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Discrete Fourier transforms provide a significant speedup in the computation\nof convolutions in deep learning. In this work, we demonstrate that, beyond its\nadvantages for efficient computation, the spectral domain also provides a\npowerful representation in which to model and train convolutional neural\nnetworks (CNNs).\n  We employ spectral representations to introduce a number of innovations to\nCNN design. First, we propose spectral pooling, which performs dimensionality\nreduction by truncating the representation in the frequency domain. This\napproach preserves considerably more information per parameter than other\npooling strategies and enables flexibility in the choice of pooling output\ndimensionality. This representation also enables a new form of stochastic\nregularization by randomized modification of resolution. We show that these\nmethods achieve competitive results on classification and approximation tasks,\nwithout using any dropout or max-pooling.\n  Finally, we demonstrate the effectiveness of complex-coefficient spectral\nparameterization of convolutional filters. While this leaves the underlying\nmodel unchanged, it results in a representation that greatly facilitates\noptimization. We observe on a variety of popular CNN configurations that this\nleads to significantly faster convergence during training.\n", "versions": [{"version": "v1", "created": "Thu, 11 Jun 2015 18:23:18 GMT"}], "update_date": "2015-06-12", "authors_parsed": [["Rippel", "Oren", ""], ["Snoek", "Jasper", ""], ["Adams", "Ryan P.", ""]]}, {"id": "1506.03805", "submitter": "Balaji Lakshminarayanan", "authors": "Balaji Lakshminarayanan, Daniel M. Roy and Yee Whye Teh", "title": "Mondrian Forests for Large-Scale Regression when Uncertainty Matters", "comments": "Proceedings of the 19th International Conference on Artificial\n  Intelligence and Statistics (AISTATS) 2016, Cadiz, Spain. JMLR: W&CP volume\n  51", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many real-world regression problems demand a measure of the uncertainty\nassociated with each prediction. Standard decision forests deliver efficient\nstate-of-the-art predictive performance, but high-quality uncertainty estimates\nare lacking. Gaussian processes (GPs) deliver uncertainty estimates, but\nscaling GPs to large-scale data sets comes at the cost of approximating the\nuncertainty estimates. We extend Mondrian forests, first proposed by\nLakshminarayanan et al. (2014) for classification problems, to the large-scale\nnon-parametric regression setting. Using a novel hierarchical Gaussian prior\nthat dovetails with the Mondrian forest framework, we obtain principled\nuncertainty estimates, while still retaining the computational advantages of\ndecision forests. Through a combination of illustrative examples, real-world\nlarge-scale datasets, and Bayesian optimization benchmarks, we demonstrate that\nMondrian forests outperform approximate GPs on large-scale regression tasks and\ndeliver better-calibrated uncertainty assessments than decision-forest-based\nmethods.\n", "versions": [{"version": "v1", "created": "Thu, 11 Jun 2015 19:55:02 GMT"}, {"version": "v2", "created": "Thu, 15 Oct 2015 18:10:07 GMT"}, {"version": "v3", "created": "Wed, 20 Apr 2016 11:43:13 GMT"}, {"version": "v4", "created": "Fri, 27 May 2016 11:15:55 GMT"}], "update_date": "2016-05-30", "authors_parsed": [["Lakshminarayanan", "Balaji", ""], ["Roy", "Daniel M.", ""], ["Teh", "Yee Whye", ""]]}, {"id": "1506.03877", "submitter": "J\\\"org Bornschein", "authors": "Jorg Bornschein and Samira Shabanian and Asja Fischer and Yoshua\n  Bengio", "title": "Bidirectional Helmholtz Machines", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Efficient unsupervised training and inference in deep generative models\nremains a challenging problem. One basic approach, called Helmholtz machine,\ninvolves training a top-down directed generative model together with a\nbottom-up auxiliary model used for approximate inference. Recent results\nindicate that better generative models can be obtained with better approximate\ninference procedures. Instead of improving the inference procedure, we here\npropose a new model which guarantees that the top-down and bottom-up\ndistributions can efficiently invert each other. We achieve this by\ninterpreting both the top-down and the bottom-up directed models as approximate\ninference distributions and by defining the model distribution to be the\ngeometric mean of these two. We present a lower-bound for the likelihood of\nthis model and we show that optimizing this bound regularizes the model so that\nthe Bhattacharyya distance between the bottom-up and top-down approximate\ndistributions is minimized. This approach results in state of the art\ngenerative models which prefer significantly deeper architectures while it\nallows for orders of magnitude more efficient approximate inference.\n", "versions": [{"version": "v1", "created": "Fri, 12 Jun 2015 00:08:20 GMT"}, {"version": "v2", "created": "Tue, 7 Jul 2015 19:08:07 GMT"}, {"version": "v3", "created": "Fri, 20 Nov 2015 18:48:00 GMT"}, {"version": "v4", "created": "Mon, 4 Jan 2016 00:07:47 GMT"}, {"version": "v5", "created": "Wed, 25 May 2016 02:54:26 GMT"}], "update_date": "2016-05-26", "authors_parsed": [["Bornschein", "Jorg", ""], ["Shabanian", "Samira", ""], ["Fischer", "Asja", ""], ["Bengio", "Yoshua", ""]]}, {"id": "1506.03899", "submitter": "Yiyi Liao", "authors": "Yiyi Liao, Sarath Kodagoda, Yue Wang, Lei Shi, Yong Liu", "title": "Place classification with a graph regularized deep neural network model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Place classification is a fundamental ability that a robot should possess to\ncarry out effective human-robot interactions. It is a nontrivial classification\nproblem which has attracted many research. In recent years, there is a high\nexploitation of Artificial Intelligent algorithms in robotics applications.\nInspired by the recent successes of deep learning methods, we propose an\nend-to-end learning approach for the place classification problem. With the\ndeep architectures, this methodology automatically discovers features and\ncontributes in general to higher classification accuracies. The pipeline of our\napproach is composed of three parts. Firstly, we construct multiple layers of\nlaser range data to represent the environment information in different levels\nof granularity. Secondly, each layer of data is fed into a deep neural network\nmodel for classification, where a graph regularization is imposed to the deep\narchitecture for keeping local consistency between adjacent samples. Finally,\nthe predicted labels obtained from all the layers are fused based on confidence\ntrees to maximize the overall confidence. Experimental results validate the\neffective- ness of our end-to-end place classification framework in which both\nthe multi-layer structure and the graph regularization promote the\nclassification performance. Furthermore, results show that the features\nautomatically learned from the raw input range data can achieve competitive\nresults to the features constructed based on statistical and geometrical\ninformation.\n", "versions": [{"version": "v1", "created": "Fri, 12 Jun 2015 05:45:36 GMT"}], "update_date": "2015-06-15", "authors_parsed": [["Liao", "Yiyi", ""], ["Kodagoda", "Sarath", ""], ["Wang", "Yue", ""], ["Shi", "Lei", ""], ["Liu", "Yong", ""]]}, {"id": "1506.03942", "submitter": "Longfei Lu", "authors": "Longfei Lu", "title": "Optimal $\\gamma$ and $C$ for $\\epsilon$-Support Vector Regression with\n  RBF Kernels", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The objective of this study is to investigate the efficient determination of\n$C$ and $\\gamma$ for Support Vector Regression with RBF or mahalanobis kernel\nbased on numerical and statistician considerations, which indicates the\nconnection between $C$ and kernels and demonstrates that the deviation of\ngeometric distance of neighbour observation in mapped space effects the predict\naccuracy of $\\epsilon$-SVR. We determinate the arrange of $\\gamma$ & $C$ and\npropose our method to choose their best values.\n", "versions": [{"version": "v1", "created": "Fri, 12 Jun 2015 09:03:50 GMT"}], "update_date": "2015-06-15", "authors_parsed": [["Lu", "Longfei", ""]]}, {"id": "1506.04002", "submitter": "Mahshid Majd", "authors": "Farzaneh Shoeleh, Mahshid Majd, Ali Hamzeh, Sattar Hashemi", "title": "Knowledge Representation in Learning Classifier Systems: A Review", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Knowledge representation is a key component to the success of all rule based\nsystems including learning classifier systems (LCSs). This component brings\ninsight into how to partition the problem space what in turn seeks prominent\nrole in generalization capacity of the system as a whole. Recently, knowledge\nrepresentation component has received great deal of attention within data\nmining communities due to its impacts on rule based systems in terms of\nefficiency and efficacy. The current work is an attempt to find a comprehensive\nand yet elaborate view into the existing knowledge representation techniques in\nLCS domain in general and XCS in specific. To achieve the objectives, knowledge\nrepresentation techniques are grouped into different categories based on the\nclassification approach in which they are incorporated. In each category, the\nunderlying rule representation schema and the format of classifier condition to\nsupport the corresponding representation are presented. Furthermore, a precise\nexplanation on the way that each technique partitions the problem space along\nwith the extensive experimental results is provided. To have an elaborated view\non the functionality of each technique, a comparative analysis of existing\ntechniques on some conventional problems is provided. We expect this survey to\nbe of interest to the LCS researchers and practitioners since it provides a\nguideline for choosing a proper knowledge representation technique for a given\nproblem and also opens up new streams of research on this topic.\n", "versions": [{"version": "v1", "created": "Fri, 12 Jun 2015 12:29:31 GMT"}], "update_date": "2015-06-15", "authors_parsed": [["Shoeleh", "Farzaneh", ""], ["Majd", "Mahshid", ""], ["Hamzeh", "Ali", ""], ["Hashemi", "Sattar", ""]]}, {"id": "1506.04089", "submitter": "Hongyuan Mei", "authors": "Hongyuan Mei, Mohit Bansal, Matthew R. Walter", "title": "Listen, Attend, and Walk: Neural Mapping of Navigational Instructions to\n  Action Sequences", "comments": "To appear at AAAI 2016 (and an extended version of a NIPS 2015\n  Multimodal Machine Learning workshop paper)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG cs.NE cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a neural sequence-to-sequence model for direction following, a\ntask that is essential to realizing effective autonomous agents. Our\nalignment-based encoder-decoder model with long short-term memory recurrent\nneural networks (LSTM-RNN) translates natural language instructions to action\nsequences based upon a representation of the observable world state. We\nintroduce a multi-level aligner that empowers our model to focus on sentence\n\"regions\" salient to the current world state by using multiple abstractions of\nthe input sentence. In contrast to existing methods, our model uses no\nspecialized linguistic resources (e.g., parsers) or task-specific annotations\n(e.g., seed lexicons). It is therefore generalizable, yet still achieves the\nbest results reported to-date on a benchmark single-sentence dataset and\ncompetitive results for the limited-training multi-sentence setting. We analyze\nour model through a series of ablations that elucidate the contributions of the\nprimary components of our model.\n", "versions": [{"version": "v1", "created": "Fri, 12 Jun 2015 18:05:00 GMT"}, {"version": "v2", "created": "Thu, 2 Jul 2015 19:22:33 GMT"}, {"version": "v3", "created": "Wed, 2 Dec 2015 20:46:09 GMT"}, {"version": "v4", "created": "Thu, 17 Dec 2015 17:57:42 GMT"}], "update_date": "2015-12-18", "authors_parsed": [["Mei", "Hongyuan", ""], ["Bansal", "Mohit", ""], ["Walter", "Matthew R.", ""]]}, {"id": "1506.04093", "submitter": "Zhanxing Zhu", "authors": "Zhanxing Zhu and Amos J. Storkey", "title": "Adaptive Stochastic Primal-Dual Coordinate Descent for Separable Saddle\n  Point Problems", "comments": "Accepted by ECML/PKDD2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider a generic convex-concave saddle point problem with separable\nstructure, a form that covers a wide-ranged machine learning applications.\nUnder this problem structure, we follow the framework of primal-dual updates\nfor saddle point problems, and incorporate stochastic block coordinate descent\nwith adaptive stepsize into this framework. We theoretically show that our\nproposal of adaptive stepsize potentially achieves a sharper linear convergence\nrate compared with the existing methods. Additionally, since we can select\n\"mini-batch\" of block coordinates to update, our method is also amenable to\nparallel processing for large-scale data. We apply the proposed method to\nregularized empirical risk minimization and show that it performs comparably\nor, more often, better than state-of-the-art methods on both synthetic and\nreal-world data sets.\n", "versions": [{"version": "v1", "created": "Fri, 12 Jun 2015 18:08:37 GMT"}], "update_date": "2015-06-15", "authors_parsed": [["Zhu", "Zhanxing", ""], ["Storkey", "Amos J.", ""]]}, {"id": "1506.04132", "submitter": "Yingzhen Li", "authors": "Yingzhen Li, Jose Miguel Hernandez-Lobato, Richard E. Turner", "title": "Stochastic Expectation Propagation", "comments": "Published at NIPS 2015. 18 pages including supplementary", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Expectation propagation (EP) is a deterministic approximation algorithm that\nis often used to perform approximate Bayesian parameter learning. EP\napproximates the full intractable posterior distribution through a set of local\napproximations that are iteratively refined for each datapoint. EP can offer\nanalytic and computational advantages over other approximations, such as\nVariational Inference (VI), and is the method of choice for a number of models.\nThe local nature of EP appears to make it an ideal candidate for performing\nBayesian learning on large models in large-scale dataset settings. However, EP\nhas a crucial limitation in this context: the number of approximating factors\nneeds to increase with the number of data-points, N, which often entails a\nprohibitively large memory overhead. This paper presents an extension to EP,\ncalled stochastic expectation propagation (SEP), that maintains a global\nposterior approximation (like VI) but updates it in a local way (like EP).\nExperiments on a number of canonical learning problems using synthetic and\nreal-world datasets indicate that SEP performs almost as well as full EP, but\nreduces the memory consumption by a factor of $N$. SEP is therefore ideally\nsuited to performing approximate Bayesian learning in the large model, large\ndataset setting.\n", "versions": [{"version": "v1", "created": "Fri, 12 Jun 2015 19:51:06 GMT"}, {"version": "v2", "created": "Wed, 18 Nov 2015 10:52:17 GMT"}], "update_date": "2015-11-19", "authors_parsed": [["Li", "Yingzhen", ""], ["Hernandez-Lobato", "Jose Miguel", ""], ["Turner", "Richard E.", ""]]}, {"id": "1506.04135", "submitter": "Fabrice Rossi", "authors": "Arnaud De Myttenaere (SAMM, Viadeo), Boris Golden (Viadeo),\n  B\\'en\\'edicte Le Grand (CRI), Fabrice Rossi (SAMM)", "title": "Reducing offline evaluation bias of collaborative filtering algorithms", "comments": "European Symposium on Artificial Neural Networks, Computational\n  Intelligence and Machine Learning (ESANN), Apr 2015, Bruges, Belgium.\n  pp.137-142, 2015, Proceedings of the 23-th European Symposium on Artificial\n  Neural Networks, Computational Intelligence and Machine Learning (ESANN 2015)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recommendation systems have been integrated into the majority of large online\nsystems to filter and rank information according to user profiles. It thus\ninfluences the way users interact with the system and, as a consequence, bias\nthe evaluation of the performance of a recommendation algorithm computed using\nhistorical data (via offline evaluation). This paper presents a new application\nof a weighted offline evaluation to reduce this bias for collaborative\nfiltering algorithms.\n", "versions": [{"version": "v1", "created": "Fri, 12 Jun 2015 19:57:27 GMT"}], "update_date": "2015-06-15", "authors_parsed": [["De Myttenaere", "Arnaud", "", "SAMM, Viadeo"], ["Golden", "Boris", "", "Viadeo"], ["Grand", "B\u00e9n\u00e9dicte Le", "", "CRI"], ["Rossi", "Fabrice", "", "SAMM"]]}, {"id": "1506.04147", "submitter": "Jacob Andreas", "authors": "Jacob Andreas, Maxim Rabinovich, Dan Klein, Michael I. Jordan", "title": "On the accuracy of self-normalized log-linear models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CL cs.LG stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Calculation of the log-normalizer is a major computational obstacle in\napplications of log-linear models with large output spaces. The problem of fast\nnormalizer computation has therefore attracted significant attention in the\ntheoretical and applied machine learning literature. In this paper, we analyze\na recently proposed technique known as \"self-normalization\", which introduces a\nregularization term in training to penalize log normalizers for deviating from\nzero. This makes it possible to use unnormalized model scores as approximate\nprobabilities. Empirical evidence suggests that self-normalization is extremely\neffective, but a theoretical understanding of why it should work, and how\ngenerally it can be applied, is largely lacking. We prove generalization bounds\non the estimated variance of normalizers and upper bounds on the loss in\naccuracy due to self-normalization, describe classes of input distributions\nthat self-normalize easily, and construct explicit examples of high-variance\ninput distributions. Our theoretical results make predictions about the\ndifficulty of fitting self-normalized models to several classes of\ndistributions, and we conclude with empirical validation of these predictions.\n", "versions": [{"version": "v1", "created": "Fri, 12 Jun 2015 20:00:29 GMT"}, {"version": "v2", "created": "Thu, 18 Jun 2015 15:22:50 GMT"}], "update_date": "2015-06-19", "authors_parsed": [["Andreas", "Jacob", ""], ["Rabinovich", "Maxim", ""], ["Klein", "Dan", ""], ["Jordan", "Michael I.", ""]]}, {"id": "1506.04176", "submitter": "Fabrice Rossi", "authors": "Arnaud De Myttenaere (SAMM), Boris Golden (Viadeo), B\\'en\\'edicte Le\n  Grand (CRI), Fabrice Rossi (SAMM)", "title": "Using the Mean Absolute Percentage Error for Regression Models", "comments": "European Symposium on Artificial Neural Networks, Computational\n  Intelligence and Machine Learning (ESANN), Apr 2015, Bruges, Belgium. 2015,\n  Proceedings of the 23-th European Symposium on Artificial Neural Networks,\n  Computational Intelligence and Machine Learning (ESANN 2015)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study in this paper the consequences of using the Mean Absolute Percentage\nError (MAPE) as a measure of quality for regression models. We show that\nfinding the best model under the MAPE is equivalent to doing weighted Mean\nAbsolute Error (MAE) regression. We show that universal consistency of\nEmpirical Risk Minimization remains possible using the MAPE instead of the MAE.\n", "versions": [{"version": "v1", "created": "Fri, 12 Jun 2015 20:38:57 GMT"}], "update_date": "2015-06-16", "authors_parsed": [["De Myttenaere", "Arnaud", "", "SAMM"], ["Golden", "Boris", "", "Viadeo"], ["Grand", "B\u00e9n\u00e9dicte Le", "", "CRI"], ["Rossi", "Fabrice", "", "SAMM"]]}, {"id": "1506.04177", "submitter": "Fabrice Rossi", "authors": "Tsirizo Rabenoro (SAMM), J\\'er\\^ome Lacaille, Marie Cottrell (SAMM),\n  Fabrice Rossi (SAMM)", "title": "Search Strategies for Binary Feature Selection for a Naive Bayes\n  Classifier", "comments": null, "journal-ref": "European Symposium on Artificial Neural Networks, Computational\n  Intelligence and Machine Learning (ESANN), Apr 2015, Bruges, Belgium.\n  pp.291-296, 2015, Proceedings of the 23-th European Symposium on Artificial\n  Neural Networks, Computational Intelligence and Machine Learning (ESANN 2015)", "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We compare in this paper several feature selection methods for the Naive\nBayes Classifier (NBC) when the data under study are described by a large\nnumber of redundant binary indicators. Wrapper approaches guided by the NBC\nestimation of the classification error probability out-perform filter\napproaches while retaining a reasonable computational cost.\n", "versions": [{"version": "v1", "created": "Fri, 12 Jun 2015 20:39:31 GMT"}], "update_date": "2015-06-16", "authors_parsed": [["Rabenoro", "Tsirizo", "", "SAMM"], ["Lacaille", "J\u00e9r\u00f4me", "", "SAMM"], ["Cottrell", "Marie", "", "SAMM"], ["Rossi", "Fabrice", "", "SAMM"]]}, {"id": "1506.04209", "submitter": "Kejun Huang", "authors": "Kejun Huang, Nicholas D. Sidiropoulos, Athanasios P. Liavas", "title": "A Flexible and Efficient Algorithmic Framework for Constrained Matrix\n  and Tensor Factorization", "comments": null, "journal-ref": null, "doi": "10.1109/TSP.2016.2576427", "report-no": null, "categories": "stat.ML cs.LG math.OC stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a general algorithmic framework for constrained matrix and tensor\nfactorization, which is widely used in signal processing and machine learning.\nThe new framework is a hybrid between alternating optimization (AO) and the\nalternating direction method of multipliers (ADMM): each matrix factor is\nupdated in turn, using ADMM, hence the name AO-ADMM. This combination can\nnaturally accommodate a great variety of constraints on the factor matrices,\nand almost all possible loss measures for the fitting. Computation caching and\nwarm start strategies are used to ensure that each update is evaluated\nefficiently, while the outer AO framework exploits recent developments in block\ncoordinate descent (BCD)-type methods which help ensure that every limit point\nis a stationary point, as well as faster and more robust convergence in\npractice. Three special cases are studied in detail: non-negative matrix/tensor\nfactorization, constrained matrix/tensor completion, and dictionary learning.\nExtensive simulations and experiments with real data are used to showcase the\neffectiveness and broad applicability of the proposed framework.\n", "versions": [{"version": "v1", "created": "Sat, 13 Jun 2015 01:42:05 GMT"}, {"version": "v2", "created": "Wed, 28 Oct 2015 16:24:52 GMT"}], "update_date": "2016-08-24", "authors_parsed": [["Huang", "Kejun", ""], ["Sidiropoulos", "Nicholas D.", ""], ["Liavas", "Athanasios P.", ""]]}, {"id": "1506.04217", "submitter": "Ching-pei Lee", "authors": "Ching-pei Lee", "title": "On the Equivalence of CoCoA+ and DisDCA", "comments": "This article is withdrawn by the author because this is actually a\n  known fact", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  In this document, we show that the algorithm CoCoA+ (Ma et al., ICML, 2015)\nunder the setting used in their experiments, which is also the best setting\nsuggested by the authors that proposed this algorithm, is equivalent to the\npractical variant of DisDCA (Yang, NIPS, 2013).\n", "versions": [{"version": "v1", "created": "Sat, 13 Jun 2015 03:52:44 GMT"}, {"version": "v2", "created": "Mon, 22 Jun 2015 03:28:45 GMT"}], "update_date": "2015-06-23", "authors_parsed": [["Lee", "Ching-pei", ""]]}, {"id": "1506.04257", "submitter": "Matthew Malloy", "authors": "Matthew L. Malloy, Scott Alfeld, Paul Barford", "title": "Contamination Estimation via Convex Relaxations", "comments": "To appear, ISIT 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.LG math.IT math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Identifying anomalies and contamination in datasets is important in a wide\nvariety of settings. In this paper, we describe a new technique for estimating\ncontamination in large, discrete valued datasets. Our approach considers the\nnormal condition of the data to be specified by a model consisting of a set of\ndistributions. Our key contribution is in our approach to contamination\nestimation. Specifically, we develop a technique that identifies the minimum\nnumber of data points that must be discarded (i.e., the level of contamination)\nfrom an empirical data set in order to match the model to within a specified\ngoodness-of-fit, controlled by a p-value. Appealing to results from large\ndeviations theory, we show a lower bound on the level of contamination is\nobtained by solving a series of convex programs. Theoretical results guarantee\nthe bound converges at a rate of $O(\\sqrt{\\log(p)/p})$, where p is the size of\nthe empirical data set.\n", "versions": [{"version": "v1", "created": "Sat, 13 Jun 2015 11:51:52 GMT"}], "update_date": "2015-06-16", "authors_parsed": [["Malloy", "Matthew L.", ""], ["Alfeld", "Scott", ""], ["Barford", "Paul", ""]]}, {"id": "1506.04359", "submitter": "Yunwen Lei", "authors": "Yunwen Lei and \\\"Ur\\\"un Dogan and Alexander Binder and Marius Kloft", "title": "Multi-class SVMs: From Tighter Data-Dependent Generalization Bounds to\n  Novel Algorithms", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper studies the generalization performance of multi-class\nclassification algorithms, for which we obtain, for the first time, a\ndata-dependent generalization error bound with a logarithmic dependence on the\nclass size, substantially improving the state-of-the-art linear dependence in\nthe existing data-dependent generalization analysis. The theoretical analysis\nmotivates us to introduce a new multi-class classification machine based on\n$\\ell_p$-norm regularization, where the parameter $p$ controls the complexity\nof the corresponding bounds. We derive an efficient optimization algorithm\nbased on Fenchel duality theory. Benchmarks on several real-world datasets show\nthat the proposed algorithm can achieve significant accuracy gains over the\nstate of the art.\n", "versions": [{"version": "v1", "created": "Sun, 14 Jun 2015 08:07:23 GMT"}], "update_date": "2015-06-16", "authors_parsed": [["Lei", "Yunwen", ""], ["Dogan", "\u00dcr\u00fcn", ""], ["Binder", "Alexander", ""], ["Kloft", "Marius", ""]]}, {"id": "1506.04364", "submitter": "Yunwen Lei", "authors": "Yunwen Lei and Alexander Binder and \\\"Ur\\\"un Dogan and Marius Kloft", "title": "Localized Multiple Kernel Learning---A Convex Approach", "comments": "to appear in ACML 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a localized approach to multiple kernel learning that can be\nformulated as a convex optimization problem over a given cluster structure. For\nwhich we obtain generalization error guarantees and derive an optimization\nalgorithm based on the Fenchel dual representation. Experiments on real-world\ndatasets from the application domains of computational biology and computer\nvision show that convex localized multiple kernel learning can achieve higher\nprediction accuracies than its global and non-convex local counterparts.\n", "versions": [{"version": "v1", "created": "Sun, 14 Jun 2015 09:11:13 GMT"}, {"version": "v2", "created": "Thu, 13 Oct 2016 00:54:24 GMT"}], "update_date": "2016-10-14", "authors_parsed": [["Lei", "Yunwen", ""], ["Binder", "Alexander", ""], ["Dogan", "\u00dcr\u00fcn", ""], ["Kloft", "Marius", ""]]}, {"id": "1506.04416", "submitter": "Anoop Korattikara", "authors": "Anoop Korattikara, Vivek Rathod, Kevin Murphy, Max Welling", "title": "Bayesian Dark Knowledge", "comments": "final version submitted to NIPS 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of Bayesian parameter estimation for deep neural\nnetworks, which is important in problem settings where we may have little data,\nand/ or where we need accurate posterior predictive densities, e.g., for\napplications involving bandits or active learning. One simple approach to this\nis to use online Monte Carlo methods, such as SGLD (stochastic gradient\nLangevin dynamics). Unfortunately, such a method needs to store many copies of\nthe parameters (which wastes memory), and needs to make predictions using many\nversions of the model (which wastes time).\n  We describe a method for \"distilling\" a Monte Carlo approximation to the\nposterior predictive density into a more compact form, namely a single deep\nneural network. We compare to two very recent approaches to Bayesian neural\nnetworks, namely an approach based on expectation propagation [Hernandez-Lobato\nand Adams, 2015] and an approach based on variational Bayes [Blundell et al.,\n2015]. Our method performs better than both of these, is much simpler to\nimplement, and uses less computation at test time.\n", "versions": [{"version": "v1", "created": "Sun, 14 Jun 2015 16:22:16 GMT"}, {"version": "v2", "created": "Tue, 7 Jul 2015 18:38:47 GMT"}, {"version": "v3", "created": "Fri, 6 Nov 2015 23:51:30 GMT"}], "update_date": "2015-11-10", "authors_parsed": [["Korattikara", "Anoop", ""], ["Rathod", "Vivek", ""], ["Murphy", "Kevin", ""], ["Welling", "Max", ""]]}, {"id": "1506.04422", "submitter": "Rafael Pinto", "authors": "Rafael Pinto and Paulo Engel", "title": "A Fast Incremental Gaussian Mixture Model", "comments": "10 pages, no figures, draft submission to Plos One", "journal-ref": null, "doi": "10.1371/journal.pone.0139931", "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work builds upon previous efforts in online incremental learning, namely\nthe Incremental Gaussian Mixture Network (IGMN). The IGMN is capable of\nlearning from data streams in a single-pass by improving its model after\nanalyzing each data point and discarding it thereafter. Nevertheless, it\nsuffers from the scalability point-of-view, due to its asymptotic time\ncomplexity of $\\operatorname{O}\\bigl(NKD^3\\bigr)$ for $N$ data points, $K$\nGaussian components and $D$ dimensions, rendering it inadequate for\nhigh-dimensional data. In this paper, we manage to reduce this complexity to\n$\\operatorname{O}\\bigl(NKD^2\\bigr)$ by deriving formulas for working directly\nwith precision matrices instead of covariance matrices. The final result is a\nmuch faster and scalable algorithm which can be applied to high dimensional\ntasks. This is confirmed by applying the modified algorithm to high-dimensional\nclassification datasets.\n", "versions": [{"version": "v1", "created": "Sun, 14 Jun 2015 17:02:49 GMT"}, {"version": "v2", "created": "Thu, 18 Jun 2015 17:04:01 GMT"}], "update_date": "2017-02-08", "authors_parsed": [["Pinto", "Rafael", ""], ["Engel", "Paulo", ""]]}, {"id": "1506.04448", "submitter": "Yining Wang", "authors": "Yining Wang, Hsiao-Yu Tung, Alexander Smola and Animashree Anandkumar", "title": "Fast and Guaranteed Tensor Decomposition via Sketching", "comments": "29 pages. Appeared in Proceedings of Advances in Neural Information\n  Processing Systems (NIPS), held at Montreal, Canada in 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Tensor CANDECOMP/PARAFAC (CP) decomposition has wide applications in\nstatistical learning of latent variable models and in data mining. In this\npaper, we propose fast and randomized tensor CP decomposition algorithms based\non sketching. We build on the idea of count sketches, but introduce many novel\nideas which are unique to tensors. We develop novel methods for randomized\ncomputation of tensor contractions via FFTs, without explicitly forming the\ntensors. Such tensor contractions are encountered in decomposition methods such\nas tensor power iterations and alternating least squares. We also design novel\ncolliding hashes for symmetric tensors to further save time in computing the\nsketches. We then combine these sketching ideas with existing whitening and\ntensor power iterative techniques to obtain the fastest algorithm on both\nsparse and dense tensors. The quality of approximation under our method does\nnot depend on properties such as sparsity, uniformity of elements, etc. We\napply the method for topic modeling and obtain competitive results.\n", "versions": [{"version": "v1", "created": "Sun, 14 Jun 2015 23:07:38 GMT"}, {"version": "v2", "created": "Tue, 20 Oct 2015 14:45:41 GMT"}], "update_date": "2015-10-21", "authors_parsed": [["Wang", "Yining", ""], ["Tung", "Hsiao-Yu", ""], ["Smola", "Alexander", ""], ["Anandkumar", "Animashree", ""]]}, {"id": "1506.04449", "submitter": "Wenlin Chen", "authors": "Wenlin Chen, James T. Wilson, Stephen Tyree, Kilian Q. Weinberger,\n  Yixin Chen", "title": "Compressing Convolutional Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolutional neural networks (CNN) are increasingly used in many areas of\ncomputer vision. They are particularly attractive because of their ability to\n\"absorb\" great quantities of labeled data through millions of parameters.\nHowever, as model sizes increase, so do the storage and memory requirements of\nthe classifiers. We present a novel network architecture, Frequency-Sensitive\nHashed Nets (FreshNets), which exploits inherent redundancy in both\nconvolutional layers and fully-connected layers of a deep learning model,\nleading to dramatic savings in memory and storage consumption. Based on the key\nobservation that the weights of learned convolutional filters are typically\nsmooth and low-frequency, we first convert filter weights to the frequency\ndomain with a discrete cosine transform (DCT) and use a low-cost hash function\nto randomly group frequency parameters into hash buckets. All parameters\nassigned the same hash bucket share a single value learned with standard\nback-propagation. To further reduce model size we allocate fewer hash buckets\nto high-frequency components, which are generally less important. We evaluate\nFreshNets on eight data sets, and show that it leads to drastically better\ncompressed performance than several relevant baselines.\n", "versions": [{"version": "v1", "created": "Sun, 14 Jun 2015 23:16:40 GMT"}], "update_date": "2015-06-16", "authors_parsed": [["Chen", "Wenlin", ""], ["Wilson", "James T.", ""], ["Tyree", "Stephen", ""], ["Weinberger", "Kilian Q.", ""], ["Chen", "Yixin", ""]]}, {"id": "1506.04477", "submitter": "Sang-Woo Lee", "authors": "Sang-Woo Lee, Min-Oh Heo, Jiwon Kim, Jeonghee Kim, Byoung-Tak Zhang", "title": "Dual Memory Architectures for Fast Deep Learning of Stream Data via an\n  Online-Incremental-Transfer Strategy", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The online learning of deep neural networks is an interesting problem of\nmachine learning because, for example, major IT companies want to manage the\ninformation of the massive data uploaded on the web daily, and this technology\ncan contribute to the next generation of lifelong learning. We aim to train\ndeep models from new data that consists of new classes, distributions, and\ntasks at minimal computational cost, which we call online deep learning.\nUnfortunately, deep neural network learning through classical online and\nincremental methods does not work well in both theory and practice. In this\npaper, we introduce dual memory architectures for online incremental deep\nlearning. The proposed architecture consists of deep representation learners\nand fast learnable shallow kernel networks, both of which synergize to track\nthe information of new data. During the training phase, we use various online,\nincremental ensemble, and transfer learning techniques in order to achieve\nlower error of the architecture. On the MNIST, CIFAR-10, and ImageNet image\nrecognition tasks, the proposed dual memory architectures performs much better\nthan the classical online and incremental ensemble algorithm, and their\naccuracies are similar to that of the batch learner.\n", "versions": [{"version": "v1", "created": "Mon, 15 Jun 2015 04:44:38 GMT"}], "update_date": "2015-06-16", "authors_parsed": [["Lee", "Sang-Woo", ""], ["Heo", "Min-Oh", ""], ["Kim", "Jiwon", ""], ["Kim", "Jeonghee", ""], ["Zhang", "Byoung-Tak", ""]]}, {"id": "1506.04488", "submitter": "Lili Mou", "authors": "Lili Mou, Ran Jia, Yan Xu, Ge Li, Lu Zhang, Zhi Jin", "title": "Distilling Word Embeddings: An Encoding Approach", "comments": "Accepted by CIKM-16 as a short paper, and by the Representation\n  Learning for Natural Language Processing (RL4NLP) Workshop @ACL-16 for\n  presentation", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Distilling knowledge from a well-trained cumbersome network to a small one\nhas recently become a new research topic, as lightweight neural networks with\nhigh performance are particularly in need in various resource-restricted\nsystems. This paper addresses the problem of distilling word embeddings for NLP\ntasks. We propose an encoding approach to distill task-specific knowledge from\na set of high-dimensional embeddings, which can reduce model complexity by a\nlarge margin as well as retain high accuracy, showing a good compromise between\nefficiency and performance. Experiments in two tasks reveal the phenomenon that\ndistilling knowledge from cumbersome embeddings is better than directly\ntraining neural networks with small embeddings.\n", "versions": [{"version": "v1", "created": "Mon, 15 Jun 2015 06:30:36 GMT"}, {"version": "v2", "created": "Sun, 24 Jul 2016 16:22:09 GMT"}], "update_date": "2016-07-26", "authors_parsed": [["Mou", "Lili", ""], ["Jia", "Ran", ""], ["Xu", "Yan", ""], ["Li", "Ge", ""], ["Zhang", "Lu", ""], ["Jin", "Zhi", ""]]}, {"id": "1506.04513", "submitter": "Matus Telgarsky", "authors": "Matus Telgarsky and Miroslav Dud\\'ik and Robert Schapire", "title": "Convex Risk Minimization and Conditional Probability Estimation", "comments": "To appear, COLT 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proves, in very general settings, that convex risk minimization is\na procedure to select a unique conditional probability model determined by the\nclassification problem. Unlike most previous work, we give results that are\ngeneral enough to include cases in which no minimum exists, as occurs\ntypically, for instance, with standard boosting algorithms. Concretely, we\nfirst show that any sequence of predictors minimizing convex risk over the\nsource distribution will converge to this unique model when the class of\npredictors is linear (but potentially of infinite dimension). Secondly, we show\nthe same result holds for \\emph{empirical} risk minimization whenever this\nclass of predictors is finite dimensional, where the essential technical\ncontribution is a norm-free generalization bound.\n", "versions": [{"version": "v1", "created": "Mon, 15 Jun 2015 08:41:39 GMT"}], "update_date": "2015-06-16", "authors_parsed": [["Telgarsky", "Matus", ""], ["Dud\u00edk", "Miroslav", ""], ["Schapire", "Robert", ""]]}, {"id": "1506.04557", "submitter": "Chao Du", "authors": "Chao Du, Jun Zhu and Bo Zhang", "title": "Learning Deep Generative Models with Doubly Stochastic MCMC", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present doubly stochastic gradient MCMC, a simple and generic method for\n(approximate) Bayesian inference of deep generative models (DGMs) in a\ncollapsed continuous parameter space. At each MCMC sampling step, the algorithm\nrandomly draws a mini-batch of data samples to estimate the gradient of\nlog-posterior and further estimates the intractable expectation over hidden\nvariables via a neural adaptive importance sampler, where the proposal\ndistribution is parameterized by a deep neural network and learnt jointly. We\ndemonstrate the effectiveness on learning various DGMs in a wide range of\ntasks, including density estimation, data generation and missing data\nimputation. Our method outperforms many state-of-the-art competitors.\n", "versions": [{"version": "v1", "created": "Mon, 15 Jun 2015 11:37:09 GMT"}, {"version": "v2", "created": "Sun, 11 Oct 2015 08:29:24 GMT"}, {"version": "v3", "created": "Wed, 14 Oct 2015 12:28:24 GMT"}, {"version": "v4", "created": "Mon, 7 Mar 2016 14:14:00 GMT"}], "update_date": "2016-03-08", "authors_parsed": [["Du", "Chao", ""], ["Zhu", "Jun", ""], ["Zhang", "Bo", ""]]}, {"id": "1506.04573", "submitter": "Emilie Morvant", "authors": "Pascal Germain (SIERRA), Amaury Habrard (LaHC), Fran\\c{c}ois\n  Laviolette, Emilie Morvant (LaHC)", "title": "A New PAC-Bayesian Perspective on Domain Adaptation", "comments": "Published at ICML 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the issue of PAC-Bayesian domain adaptation: We want to learn, from\na source domain, a majority vote model dedicated to a target one. Our\ntheoretical contribution brings a new perspective by deriving an upper-bound on\nthe target risk where the distributions' divergence---expressed as a\nratio---controls the trade-off between a source error measure and the target\nvoters' disagreement. Our bound suggests that one has to focus on regions where\nthe source data is informative.From this result, we derive a PAC-Bayesian\ngeneralization bound, and specialize it to linear classifiers. Then, we infer a\nlearning algorithmand perform experiments on real data.\n", "versions": [{"version": "v1", "created": "Mon, 15 Jun 2015 12:46:45 GMT"}, {"version": "v2", "created": "Mon, 21 Sep 2015 10:49:00 GMT"}, {"version": "v3", "created": "Mon, 14 Mar 2016 19:44:22 GMT"}, {"version": "v4", "created": "Tue, 26 Jul 2016 10:29:33 GMT"}], "update_date": "2016-07-27", "authors_parsed": [["Germain", "Pascal", "", "SIERRA"], ["Habrard", "Amaury", "", "LaHC"], ["Laviolette", "Fran\u00e7ois", "", "LaHC"], ["Morvant", "Emilie", "", "LaHC"]]}, {"id": "1506.04584", "submitter": "Lin Xu", "authors": "Zhihai Yang, Lin Xu, Zhongmin Cai", "title": "Re-scale AdaBoost for Attack Detection in Collaborative Filtering\n  Recommender Systems", "comments": "30 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Collaborative filtering recommender systems (CFRSs) are the key components of\nsuccessful e-commerce systems. Actually, CFRSs are highly vulnerable to attacks\nsince its openness. However, since attack size is far smaller than that of\ngenuine users, conventional supervised learning based detection methods could\nbe too \"dull\" to handle such imbalanced classification. In this paper, we\nimprove detection performance from following two aspects. First, we extract\nwell-designed features from user profiles based on the statistical properties\nof the diverse attack models, making hard classification task becomes easier to\nperform. Then, refer to the general idea of re-scale Boosting (RBoosting) and\nAdaBoost, we apply a variant of AdaBoost, called the re-scale AdaBoost\n(RAdaBoost) as our detection method based on extracted features. RAdaBoost is\ncomparable to the optimal Boosting-type algorithm and can effectively improve\nthe performance in some hard scenarios. Finally, a series of experiments on the\nMovieLens-100K data set are conducted to demonstrate the outperformance of\nRAdaBoost comparing with some classical techniques such as SVM, kNN and\nAdaBoost.\n", "versions": [{"version": "v1", "created": "Mon, 15 Jun 2015 13:07:52 GMT"}], "update_date": "2015-06-16", "authors_parsed": [["Yang", "Zhihai", ""], ["Xu", "Lin", ""], ["Cai", "Zhongmin", ""]]}, {"id": "1506.04720", "submitter": "Siqi Nie", "authors": "Siqi Nie, Qiang Ji", "title": "Latent Regression Bayesian Network for Data Representation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep directed generative models have attracted much attention recently due to\ntheir expressive representation power and the ability of ancestral sampling.\nOne major difficulty of learning directed models with many latent variables is\nthe intractable inference. To address this problem, most existing algorithms\nmake assumptions to render the latent variables independent of each other,\neither by designing specific priors, or by approximating the true posterior\nusing a factorized distribution. We believe the correlations among latent\nvariables are crucial for faithful data representation. Driven by this idea, we\npropose an inference method based on the conditional pseudo-likelihood that\npreserves the dependencies among the latent variables. For learning, we propose\nto employ the hard Expectation Maximization (EM) algorithm, which avoids the\nintractability of the traditional EM by max-out instead of sum-out to compute\nthe data likelihood. Qualitative and quantitative evaluations of our model\nagainst state of the art deep models on benchmark datasets demonstrate the\neffectiveness of the proposed algorithm in data representation and\nreconstruction.\n", "versions": [{"version": "v1", "created": "Mon, 15 Jun 2015 19:34:59 GMT"}], "update_date": "2015-06-16", "authors_parsed": [["Nie", "Siqi", ""], ["Ji", "Qiang", ""]]}, {"id": "1506.04776", "submitter": "Jeff Heaton", "authors": "Jeff Heaton", "title": "Encog: Library of Interchangeable Machine Learning Models for Java and\n  C#", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MS cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces the Encog library for Java and C#, a scalable,\nadaptable, multiplatform machine learning framework that was 1st released in\n2008. Encog allows a variety of machine learning models to be applied to\ndatasets using regression, classification, and clustering. Various supported\nmachine learning models can be used interchangeably with minimal recoding.\nEncog uses efficient multithreaded code to reduce training time by exploiting\nmodern multicore processors. The current version of Encog can be downloaded\nfrom http://www.encog.org.\n", "versions": [{"version": "v1", "created": "Mon, 15 Jun 2015 21:20:06 GMT"}], "update_date": "2015-06-17", "authors_parsed": [["Heaton", "Jeff", ""]]}, {"id": "1506.04782", "submitter": "Manjesh Kumar Hanawal", "authors": "Manjesh Kumar Hanawal and Venkatesh Saligrama and Michal Valko and R\\'\n  emi Munos", "title": "Cheap Bandits", "comments": "To be presented at ICML 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider stochastic sequential learning problems where the learner can\nobserve the \\textit{average reward of several actions}. Such a setting is\ninteresting in many applications involving monitoring and surveillance, where\nthe set of the actions to observe represent some (geographical) area. The\nimportance of this setting is that in these applications, it is actually\n\\textit{cheaper} to observe average reward of a group of actions rather than\nthe reward of a single action. We show that when the reward is \\textit{smooth}\nover a given graph representing the neighboring actions, we can maximize the\ncumulative reward of learning while \\textit{minimizing the sensing cost}. In\nthis paper we propose CheapUCB, an algorithm that matches the regret guarantees\nof the known algorithms for this setting and at the same time guarantees a\nlinear cost again over them. As a by-product of our analysis, we establish a\n$\\Omega(\\sqrt{dT})$ lower bound on the cumulative regret of spectral bandits\nfor a class of graphs with effective dimension $d$.\n", "versions": [{"version": "v1", "created": "Mon, 15 Jun 2015 21:42:45 GMT"}, {"version": "v2", "created": "Thu, 18 Jun 2015 22:40:51 GMT"}], "update_date": "2015-06-22", "authors_parsed": [["Hanawal", "Manjesh Kumar", ""], ["Saligrama", "Venkatesh", ""], ["Valko", "Michal", ""], ["Munos", "R\\' emi", ""]]}, {"id": "1506.04820", "submitter": "Haipeng Luo", "authors": "Alina Beygelzimer, Elad Hazan, Satyen Kale and Haipeng Luo", "title": "Online Gradient Boosting", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We extend the theory of boosting for regression problems to the online\nlearning setting. Generalizing from the batch setting for boosting, the notion\nof a weak learning algorithm is modeled as an online learning algorithm with\nlinear loss functions that competes with a base class of regression functions,\nwhile a strong learning algorithm is an online learning algorithm with convex\nloss functions that competes with a larger class of regression functions. Our\nmain result is an online gradient boosting algorithm which converts a weak\nonline learning algorithm into a strong one where the larger class of functions\nis the linear span of the base class. We also give a simpler boosting algorithm\nthat converts a weak online learning algorithm into a strong one where the\nlarger class of functions is the convex hull of the base class, and prove its\noptimality.\n", "versions": [{"version": "v1", "created": "Tue, 16 Jun 2015 02:20:32 GMT"}, {"version": "v2", "created": "Fri, 30 Oct 2015 20:04:31 GMT"}], "update_date": "2015-11-03", "authors_parsed": [["Beygelzimer", "Alina", ""], ["Hazan", "Elad", ""], ["Kale", "Satyen", ""], ["Luo", "Haipeng", ""]]}, {"id": "1506.04834", "submitter": "Samuel Bowman", "authors": "Samuel R. Bowman, Christopher D. Manning, and Christopher Potts", "title": "Tree-structured composition in neural networks without tree-structured\n  architectures", "comments": "To appear in the proceedings of the 2015 NIPS Workshop on Cognitive\n  Computation: Integrating Neural and Symbolic Approaches", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Tree-structured neural networks encode a particular tree geometry for a\nsentence in the network design. However, these models have at best only\nslightly outperformed simpler sequence-based models. We hypothesize that neural\nsequence models like LSTMs are in fact able to discover and implicitly use\nrecursive compositional structure, at least for tasks with clear cues to that\nstructure in the data. We demonstrate this possibility using an artificial data\ntask for which recursive compositional structure is crucial, and find an\nLSTM-based sequence model can indeed learn to exploit the underlying tree\nstructure. However, its performance consistently lags behind that of tree\nmodels, even on large training sets, suggesting that tree-structured models are\nmore effective at exploiting recursive structure.\n", "versions": [{"version": "v1", "created": "Tue, 16 Jun 2015 05:12:52 GMT"}, {"version": "v2", "created": "Tue, 18 Aug 2015 16:08:26 GMT"}, {"version": "v3", "created": "Mon, 9 Nov 2015 19:45:09 GMT"}], "update_date": "2015-11-10", "authors_parsed": [["Bowman", "Samuel R.", ""], ["Manning", "Christopher D.", ""], ["Potts", "Christopher", ""]]}, {"id": "1506.04838", "submitter": "Zeyuan Allen-Zhu", "authors": "Zeyuan Allen-Zhu and Zhenyu Liao and Lorenzo Orecchia", "title": "Spectral Sparsification and Regret Minimization Beyond Matrix\n  Multiplicative Updates", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we provide a novel construction of the linear-sized spectral\nsparsifiers of Batson, Spielman and Srivastava [BSS14]. While previous\nconstructions required $\\Omega(n^4)$ running time [BSS14, Zou12], our\nsparsification routine can be implemented in almost-quadratic running time\n$O(n^{2+\\varepsilon})$.\n  The fundamental conceptual novelty of our work is the leveraging of a strong\nconnection between sparsification and a regret minimization problem over\ndensity matrices. This connection was known to provide an interpretation of the\nrandomized sparsifiers of Spielman and Srivastava [SS11] via the application of\nmatrix multiplicative weight updates (MWU) [CHS11, Vis14]. In this paper, we\nexplain how matrix MWU naturally arises as an instance of the\nFollow-the-Regularized-Leader framework and generalize this approach to yield a\nlarger class of updates. This new class allows us to accelerate the\nconstruction of linear-sized spectral sparsifiers, and give novel insights on\nthe motivation behind Batson, Spielman and Srivastava [BSS14].\n", "versions": [{"version": "v1", "created": "Tue, 16 Jun 2015 05:31:57 GMT"}], "update_date": "2015-06-17", "authors_parsed": [["Allen-Zhu", "Zeyuan", ""], ["Liao", "Zhenyu", ""], ["Orecchia", "Lorenzo", ""]]}, {"id": "1506.04855", "submitter": "Wojciech Kot{\\l}owski", "authors": "Wojciech Kot{\\l}owski, Manfred K. Warmuth", "title": "PCA with Gaussian perturbations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most of machine learning deals with vector parameters. Ideally we would like\nto take higher order information into account and make use of matrix or even\ntensor parameters. However the resulting algorithms are usually inefficient.\nHere we address on-line learning with matrix parameters. It is often easy to\nobtain online algorithm with good generalization performance if you\neigendecompose the current parameter matrix in each trial (at a cost of\n$O(n^3)$ per trial). Ideally we want to avoid the decompositions and spend\n$O(n^2)$ per trial, i.e. linear time in the size of the matrix data. There is a\ncore trade-off between the running time and the generalization performance,\nhere measured by the regret of the on-line algorithm (total gain of the best\noff-line predictor minus the total gain of the on-line algorithm). We focus on\nthe key matrix problem of rank $k$ Principal Component Analysis in\n$\\mathbb{R}^n$ where $k \\ll n$. There are $O(n^3)$ algorithms that achieve the\noptimum regret but require eigendecompositions. We develop a simple algorithm\nthat needs $O(kn^2)$ per trial whose regret is off by a small factor of\n$O(n^{1/4})$. The algorithm is based on the Follow the Perturbed Leader\nparadigm. It replaces full eigendecompositions at each trial by the problem\nfinding $k$ principal components of the current covariance matrix that is\nperturbed by Gaussian noise.\n", "versions": [{"version": "v1", "created": "Tue, 16 Jun 2015 07:04:19 GMT"}, {"version": "v2", "created": "Thu, 23 Jul 2015 20:22:27 GMT"}], "update_date": "2015-07-27", "authors_parsed": [["Kot\u0142owski", "Wojciech", ""], ["Warmuth", "Manfred K.", ""]]}, {"id": "1506.04891", "submitter": "Douglas Bagnall", "authors": "Douglas Bagnall", "title": "Author Identification using Multi-headed Recurrent Neural Networks", "comments": "8 pages, 3 figures Version 1 was a notebook for the PAN@CLEF Author\n  Identification challenge. Version 2 is expanded to be a full paper for\n  CLEF2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recurrent neural networks (RNNs) are very good at modelling the flow of text,\nbut typically need to be trained on a far larger corpus than is available for\nthe PAN 2015 Author Identification task. This paper describes a novel approach\nwhere the output layer of a character-level RNN language model is split into\nseveral independent predictive sub-models, each representing an author, while\nthe recurrent layer is shared by all. This allows the recurrent layer to model\nthe language as a whole without over-fitting, while the outputs select aspects\nof the underlying model that reflect their author's style. The method proves\ncompetitive, ranking first in two of the four languages.\n", "versions": [{"version": "v1", "created": "Tue, 16 Jun 2015 09:41:55 GMT"}, {"version": "v2", "created": "Tue, 16 Aug 2016 05:04:57 GMT"}], "update_date": "2016-08-17", "authors_parsed": [["Bagnall", "Douglas", ""]]}, {"id": "1506.04908", "submitter": "Alexandre d'Aspremont", "authors": "Vincent Roulet, Fajwel Fogel, Alexandre d'Aspremont, Francis Bach", "title": "Learning with Clustering Structure", "comments": "Completely rewritten. New convergence proofs in the clustered and\n  sparse clustered case. New projection algorithm on sparse clustered vectors", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study supervised learning problems using clustering constraints to impose\nstructure on either features or samples, seeking to help both prediction and\ninterpretation. The problem of clustering features arises naturally in text\nclassification for instance, to reduce dimensionality by grouping words\ntogether and identify synonyms. The sample clustering problem on the other\nhand, applies to multiclass problems where we are allowed to make multiple\npredictions and the performance of the best answer is recorded. We derive a\nunified optimization formulation highlighting the common structure of these\nproblems and produce algorithms whose core iteration complexity amounts to a\nk-means clustering step, which can be approximated efficiently. We extend these\nresults to combine sparsity and clustering constraints, and develop a new\nprojection algorithm on the set of clustered sparse vectors. We prove\nconvergence of our algorithms on random instances, based on a union of\nsubspaces interpretation of the clustering structure. Finally, we test the\nrobustness of our methods on artificial data sets as well as real data\nextracted from movie reviews.\n", "versions": [{"version": "v1", "created": "Tue, 16 Jun 2015 10:44:30 GMT"}, {"version": "v2", "created": "Fri, 11 Mar 2016 09:22:47 GMT"}, {"version": "v3", "created": "Mon, 19 Sep 2016 08:37:40 GMT"}], "update_date": "2016-09-20", "authors_parsed": [["Roulet", "Vincent", ""], ["Fogel", "Fajwel", ""], ["d'Aspremont", "Alexandre", ""], ["Bach", "Francis", ""]]}, {"id": "1506.05011", "submitter": "Theofanis Karaletsos", "authors": "Theofanis Karaletsos, Serge Belongie, Gunnar R\\\"atsch", "title": "Bayesian representation learning with oracle constraints", "comments": "16 pages, publishes in ICLR 16", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Representation learning systems typically rely on massive amounts of labeled\ndata in order to be trained to high accuracy. Recently, high-dimensional\nparametric models like neural networks have succeeded in building rich\nrepresentations using either compressive, reconstructive or supervised\ncriteria. However, the semantic structure inherent in observations is\noftentimes lost in the process. Human perception excels at understanding\nsemantics but cannot always be expressed in terms of labels. Thus,\n\\emph{oracles} or \\emph{human-in-the-loop systems}, for example crowdsourcing,\nare often employed to generate similarity constraints using an implicit\nsimilarity function encoded in human perception. In this work we propose to\ncombine \\emph{generative unsupervised feature learning} with a\n\\emph{probabilistic treatment of oracle information like triplets} in order to\ntransfer implicit privileged oracle knowledge into explicit nonlinear Bayesian\nlatent factor models of the observations. We use a fast variational algorithm\nto learn the joint model and demonstrate applicability to a well-known image\ndataset. We show how implicit triplet information can provide rich information\nto learn representations that outperform previous metric learning approaches as\nwell as generative models without this side-information in a variety of\npredictive tasks. In addition, we illustrate that the proposed approach\ncompartmentalizes the latent spaces semantically which allows interpretation of\nthe latent variables.\n", "versions": [{"version": "v1", "created": "Tue, 16 Jun 2015 15:54:59 GMT"}, {"version": "v2", "created": "Sat, 21 Nov 2015 05:24:01 GMT"}, {"version": "v3", "created": "Fri, 8 Jan 2016 04:47:21 GMT"}, {"version": "v4", "created": "Tue, 1 Mar 2016 23:36:04 GMT"}], "update_date": "2016-03-03", "authors_parsed": [["Karaletsos", "Theofanis", ""], ["Belongie", "Serge", ""], ["R\u00e4tsch", "Gunnar", ""]]}, {"id": "1506.05055", "submitter": "Manfred Jaeger", "authors": "Jiuchuan Jiang and Manfred Jaeger", "title": "Numeric Input Relations for Relational Learning with Applications to\n  Community Structure Analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most work in the area of statistical relational learning (SRL) is focussed on\ndiscrete data, even though a few approaches for hybrid SRL models have been\nproposed that combine numerical and discrete variables. In this paper we\ndistinguish numerical random variables for which a probability distribution is\ndefined by the model from numerical input variables that are only used for\nconditioning the distribution of discrete response variables. We show how\nnumerical input relations can very easily be used in the Relational Bayesian\nNetwork framework, and that existing inference and learning methods need only\nminor adjustments to be applied in this generalized setting. The resulting\nframework provides natural relational extensions of classical probabilistic\nmodels for categorical data. We demonstrate the usefulness of RBN models with\nnumeric input relations by several examples.\n  In particular, we use the augmented RBN framework to define probabilistic\nmodels for multi-relational (social) networks in which the probability of a\nlink between two nodes depends on numeric latent feature vectors associated\nwith the nodes. A generic learning procedure can be used to obtain a\nmaximum-likelihood fit of model parameters and latent feature values for a\nvariety of models that can be expressed in the high-level RBN representation.\nSpecifically, we propose a model that allows us to interpret learned latent\nfeature values as community centrality degrees by which we can identify nodes\nthat are central for one community, that are hubs between communities, or that\nare isolated nodes. In a multi-relational setting, the model also provides a\ncharacterization of how different relations are associated with each community.\n", "versions": [{"version": "v1", "created": "Tue, 16 Jun 2015 18:18:06 GMT"}], "update_date": "2015-06-17", "authors_parsed": [["Jiang", "Jiuchuan", ""], ["Jaeger", "Manfred", ""]]}, {"id": "1506.05070", "submitter": "Soumi Chaki", "authors": "Soumi Chaki", "title": "Reservoir Characterization: A Machine Learning Approach", "comments": "Supervisors: Prof. Aurobinda Routray and Prof. William K. Mohanty", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CE cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Reservoir Characterization (RC) can be defined as the act of building a\nreservoir model that incorporates all the characteristics of the reservoir that\nare pertinent to its ability to store hydrocarbons and also to produce them.It\nis a difficult problem due to non-linear and heterogeneous subsurface\nproperties and associated with a number of complex tasks such as data fusion,\ndata mining, formulation of the knowledge base, and handling of the\nuncertainty.This present work describes the development of algorithms to obtain\nthe functional relationships between predictor seismic attributes and target\nlithological properties. Seismic attributes are available over a study area\nwith lower vertical resolution. Conversely, well logs and lithological\nproperties are available only at specific well locations in a study area with\nhigh vertical resolution.Sand fraction, which represents per unit sand volume\nwithin the rock, has a balanced distribution between zero to unity.The thesis\naddresses the issues of handling the information content mismatch between\npredictor and target variables and proposes regularization of target property\nprior to building a prediction model.In this thesis, two Artificial Neural\nNetwork (ANN) based frameworks are proposed to model sand fraction from\nmultiple seismic attributes without and with well tops information\nrespectively. The performances of the frameworks are quantified in terms of\nCorrelation Coefficient, Root Mean Square Error, Absolute Error Mean, etc.\n", "versions": [{"version": "v1", "created": "Mon, 15 Jun 2015 16:20:23 GMT"}, {"version": "v2", "created": "Wed, 22 Jul 2015 16:09:33 GMT"}], "update_date": "2015-07-23", "authors_parsed": [["Chaki", "Soumi", ""]]}, {"id": "1506.05085", "submitter": "Wenjie Pei", "authors": "Wenjie Pei, Hamdi Dibeklio\\u{g}lu, David M.J. Tax, Laurens van der\n  Maaten", "title": "Time Series Classification using the Hidden-Unit Logistic Model", "comments": "17 pages, 4 figures, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a new model for time series classification, called the hidden-unit\nlogistic model, that uses binary stochastic hidden units to model latent\nstructure in the data. The hidden units are connected in a chain structure that\nmodels temporal dependencies in the data. Compared to the prior models for time\nseries classification such as the hidden conditional random field, our model\ncan model very complex decision boundaries because the number of latent states\ngrows exponentially with the number of hidden units. We demonstrate the strong\nperformance of our model in experiments on a variety of (computer vision)\ntasks, including handwritten character recognition, speech recognition, facial\nexpression, and action recognition. We also present a state-of-the-art system\nfor facial action unit detection based on the hidden-unit logistic model.\n", "versions": [{"version": "v1", "created": "Tue, 16 Jun 2015 19:20:00 GMT"}, {"version": "v2", "created": "Tue, 19 Jan 2016 13:33:52 GMT"}], "update_date": "2016-01-20", "authors_parsed": [["Pei", "Wenjie", ""], ["Dibeklio\u011flu", "Hamdi", ""], ["Tax", "David M. J.", ""], ["van der Maaten", "Laurens", ""]]}, {"id": "1506.05101", "submitter": "Dhruba Bhattacharyya", "authors": "Hirak Kashyap, Hasin Afzal Ahmed, Nazrul Hoque, Swarup Roy and Dhruba\n  Kumar Bhattacharyya", "title": "Big Data Analytics in Bioinformatics: A Machine Learning Perspective", "comments": "20 pages survey paper on Big data analytics in Bioinformatics", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CE cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bioinformatics research is characterized by voluminous and incremental\ndatasets and complex data analytics methods. The machine learning methods used\nin bioinformatics are iterative and parallel. These methods can be scaled to\nhandle big data using the distributed and parallel computing technologies.\n  Usually big data tools perform computation in batch-mode and are not\noptimized for iterative processing and high data dependency among operations.\nIn the recent years, parallel, incremental, and multi-view machine learning\nalgorithms have been proposed. Similarly, graph-based architectures and\nin-memory big data tools have been developed to minimize I/O cost and optimize\niterative processing.\n  However, there lack standard big data architectures and tools for many\nimportant bioinformatics problems, such as fast construction of co-expression\nand regulatory networks and salient module identification, detection of\ncomplexes over growing protein-protein interaction data, fast analysis of\nmassive DNA, RNA, and protein sequence data, and fast querying on incremental\nand heterogeneous disease networks. This paper addresses the issues and\nchallenges posed by several big data problems in bioinformatics, and gives an\noverview of the state of the art and the future research opportunities.\n", "versions": [{"version": "v1", "created": "Mon, 15 Jun 2015 11:32:00 GMT"}], "update_date": "2015-06-17", "authors_parsed": [["Kashyap", "Hirak", ""], ["Ahmed", "Hasin Afzal", ""], ["Hoque", "Nazrul", ""], ["Roy", "Swarup", ""], ["Bhattacharyya", "Dhruba Kumar", ""]]}, {"id": "1506.05163", "submitter": "Mikael Henaff", "authors": "Mikael Henaff, Joan Bruna, Yann LeCun", "title": "Deep Convolutional Networks on Graph-Structured Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep Learning's recent successes have mostly relied on Convolutional\nNetworks, which exploit fundamental statistical properties of images, sounds\nand video data: the local stationarity and multi-scale compositional structure,\nthat allows expressing long range interactions in terms of shorter, localized\ninteractions. However, there exist other important examples, such as text\ndocuments or bioinformatic data, that may lack some or all of these strong\nstatistical regularities.\n  In this paper we consider the general question of how to construct deep\narchitectures with small learning complexity on general non-Euclidean domains,\nwhich are typically unknown and need to be estimated from the data. In\nparticular, we develop an extension of Spectral Networks which incorporates a\nGraph Estimation procedure, that we test on large-scale classification\nproblems, matching or improving over Dropout Networks with far less parameters\nto estimate.\n", "versions": [{"version": "v1", "created": "Tue, 16 Jun 2015 22:31:09 GMT"}], "update_date": "2015-06-18", "authors_parsed": [["Henaff", "Mikael", ""], ["Bruna", "Joan", ""], ["LeCun", "Yann", ""]]}, {"id": "1506.05173", "submitter": "Saurabh Paul", "authors": "Saurabh Paul, Petros Drineas", "title": "Feature Selection for Ridge Regression with Provable Guarantees", "comments": "To appear in Neural Computation. A shorter version of this paper\n  appeared at ECML-PKDD 2014 under the title \"Deterministic Feature Selection\n  for Regularized Least Squares Classification.\"", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.IT cs.LG math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce single-set spectral sparsification as a deterministic sampling\nbased feature selection technique for regularized least squares classification,\nwhich is the classification analogue to ridge regression. The method is\nunsupervised and gives worst-case guarantees of the generalization power of the\nclassification function after feature selection with respect to the\nclassification function obtained using all features. We also introduce\nleverage-score sampling as an unsupervised randomized feature selection method\nfor ridge regression. We provide risk bounds for both single-set spectral\nsparsification and leverage-score sampling on ridge regression in the fixed\ndesign setting and show that the risk in the sampled space is comparable to the\nrisk in the full-feature space. We perform experiments on synthetic and\nreal-world datasets, namely a subset of TechTC-300 datasets, to support our\ntheory. Experimental results indicate that the proposed methods perform better\nthan the existing feature selection methods.\n", "versions": [{"version": "v1", "created": "Wed, 17 Jun 2015 00:05:04 GMT"}, {"version": "v2", "created": "Sat, 5 Dec 2015 18:27:38 GMT"}], "update_date": "2015-12-08", "authors_parsed": [["Paul", "Saurabh", ""], ["Drineas", "Petros", ""]]}, {"id": "1506.05232", "submitter": "Shizhao Sun", "authors": "Shizhao Sun, Wei Chen, Liwei Wang, Xiaoguang Liu, Tie-Yan Liu", "title": "On the Depth of Deep Neural Networks: A Theoretical View", "comments": "AAAI 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  People believe that depth plays an important role in success of deep neural\nnetworks (DNN). However, this belief lacks solid theoretical justifications as\nfar as we know. We investigate role of depth from perspective of margin bound.\nIn margin bound, expected error is upper bounded by empirical margin error plus\nRademacher Average (RA) based capacity term. First, we derive an upper bound\nfor RA of DNN, and show that it increases with increasing depth. This indicates\nnegative impact of depth on test performance. Second, we show that deeper\nnetworks tend to have larger representation power (measured by Betti numbers\nbased complexity) than shallower networks in multi-class setting, and thus can\nlead to smaller empirical margin error. This implies positive impact of depth.\nThe combination of these two results shows that for DNN with restricted number\nof hidden units, increasing depth is not always good since there is a tradeoff\nbetween positive and negative impacts. These results inspire us to seek\nalternative ways to achieve positive impact of depth, e.g., imposing\nmargin-based penalty terms to cross entropy loss so as to reduce empirical\nmargin error without increasing depth. Our experiments show that in this way,\nwe achieve significantly better test performance.\n", "versions": [{"version": "v1", "created": "Wed, 17 Jun 2015 07:51:42 GMT"}, {"version": "v2", "created": "Sat, 28 Nov 2015 14:21:41 GMT"}], "update_date": "2015-12-01", "authors_parsed": [["Sun", "Shizhao", ""], ["Chen", "Wei", ""], ["Wang", "Liwei", ""], ["Liu", "Xiaoguang", ""], ["Liu", "Tie-Yan", ""]]}, {"id": "1506.05254", "submitter": "John Schulman", "authors": "John Schulman, Nicolas Heess, Theophane Weber, Pieter Abbeel", "title": "Gradient Estimation Using Stochastic Computation Graphs", "comments": "Advances in Neural Information Processing Systems 28 (NIPS 2015)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In a variety of problems originating in supervised, unsupervised, and\nreinforcement learning, the loss function is defined by an expectation over a\ncollection of random variables, which might be part of a probabilistic model or\nthe external world. Estimating the gradient of this loss function, using\nsamples, lies at the core of gradient-based learning algorithms for these\nproblems. We introduce the formalism of stochastic computation\ngraphs---directed acyclic graphs that include both deterministic functions and\nconditional probability distributions---and describe how to easily and\nautomatically derive an unbiased estimator of the loss function's gradient. The\nresulting algorithm for computing the gradient estimator is a simple\nmodification of the standard backpropagation algorithm. The generic scheme we\npropose unifies estimators derived in variety of prior work, along with\nvariance-reduction techniques therein. It could assist researchers in\ndeveloping intricate models involving a combination of stochastic and\ndeterministic operations, enabling, for example, attention, memory, and control\nactions.\n", "versions": [{"version": "v1", "created": "Wed, 17 Jun 2015 09:32:31 GMT"}, {"version": "v2", "created": "Fri, 13 Nov 2015 03:19:18 GMT"}, {"version": "v3", "created": "Tue, 5 Jan 2016 19:56:22 GMT"}], "update_date": "2016-01-06", "authors_parsed": [["Schulman", "John", ""], ["Heess", "Nicolas", ""], ["Weber", "Theophane", ""], ["Abbeel", "Pieter", ""]]}, {"id": "1506.05268", "submitter": "Zhenzhou Wu", "authors": "Zhenzhou Wu, Shinji Takaki, Junichi Yamagishi", "title": "Deep Denoising Auto-encoder for Statistical Speech Synthesis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a deep denoising auto-encoder technique to extract better\nacoustic features for speech synthesis. The technique allows us to\nautomatically extract low-dimensional features from high dimensional spectral\nfeatures in a non-linear, data-driven, unsupervised way. We compared the new\nstochastic feature extractor with conventional mel-cepstral analysis in\nanalysis-by-synthesis and text-to-speech experiments. Our results confirm that\nthe proposed method increases the quality of synthetic speech in both\nexperiments.\n", "versions": [{"version": "v1", "created": "Wed, 17 Jun 2015 10:17:59 GMT"}], "update_date": "2015-06-18", "authors_parsed": [["Wu", "Zhenzhou", ""], ["Takaki", "Shinji", ""], ["Yamagishi", "Junichi", ""]]}, {"id": "1506.05439", "submitter": "Charlie Frogner", "authors": "Charlie Frogner, Chiyuan Zhang, Hossein Mobahi, Mauricio Araya-Polo,\n  Tomaso Poggio", "title": "Learning with a Wasserstein Loss", "comments": "NIPS 2015; v3 updates Algorithm 1 and Equations 6, 8", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning to predict multi-label outputs is challenging, but in many problems\nthere is a natural metric on the outputs that can be used to improve\npredictions. In this paper we develop a loss function for multi-label learning,\nbased on the Wasserstein distance. The Wasserstein distance provides a natural\nnotion of dissimilarity for probability measures. Although optimizing with\nrespect to the exact Wasserstein distance is costly, recent work has described\na regularized approximation that is efficiently computed. We describe an\nefficient learning algorithm based on this regularization, as well as a novel\nextension of the Wasserstein distance from probability measures to unnormalized\nmeasures. We also describe a statistical learning bound for the loss. The\nWasserstein loss can encourage smoothness of the predictions with respect to a\nchosen metric on the output space. We demonstrate this property on a real-data\ntag prediction problem, using the Yahoo Flickr Creative Commons dataset,\noutperforming a baseline that doesn't use the metric.\n", "versions": [{"version": "v1", "created": "Wed, 17 Jun 2015 19:36:41 GMT"}, {"version": "v2", "created": "Fri, 6 Nov 2015 03:46:05 GMT"}, {"version": "v3", "created": "Wed, 30 Dec 2015 01:08:11 GMT"}], "update_date": "2015-12-31", "authors_parsed": [["Frogner", "Charlie", ""], ["Zhang", "Chiyuan", ""], ["Mobahi", "Hossein", ""], ["Araya-Polo", "Mauricio", ""], ["Poggio", "Tomaso", ""]]}, {"id": "1506.05514", "submitter": "Ubai Sandouk", "authors": "Ubai Sandouk, Ke Chen", "title": "Learning Contextualized Semantics from Co-occurring Terms via a Siamese\n  Architecture", "comments": null, "journal-ref": null, "doi": null, "report-no": "2015-06-18", "categories": "cs.IR cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One of the biggest challenges in Multimedia information retrieval and\nunderstanding is to bridge the semantic gap by properly modeling concept\nsemantics in context. The presence of out of vocabulary (OOV) concepts\nexacerbates this difficulty. To address the semantic gap issues, we formulate a\nproblem on learning contextualized semantics from descriptive terms and propose\na novel Siamese architecture to model the contextualized semantics from\ndescriptive terms. By means of pattern aggregation and probabilistic topic\nmodels, our Siamese architecture captures contextualized semantics from the\nco-occurring descriptive terms via unsupervised learning, which leads to a\nconcept embedding space of the terms in context. Furthermore, the co-occurring\nOOV concepts can be easily represented in the learnt concept embedding space.\nThe main properties of the concept embedding space are demonstrated via\nvisualization. Using various settings in semantic priming, we have carried out\na thorough evaluation by comparing our approach to a number of state-of-the-art\nmethods on six annotation corpora in different domains, i.e., MagTag5K, CAL500\nand Million Song Dataset in the music domain as well as Corel5K, LabelMe and\nSUNDatabase in the image domain. Experimental results on semantic priming\nsuggest that our approach outperforms those state-of-the-art methods\nconsiderably in various aspects.\n", "versions": [{"version": "v1", "created": "Wed, 17 Jun 2015 23:03:43 GMT"}], "update_date": "2015-06-19", "authors_parsed": [["Sandouk", "Ubai", ""], ["Chen", "Ke", ""]]}, {"id": "1506.05600", "submitter": "Ridho Rahmadi", "authors": "Ridho Rahmadi, Perry Groot, Marianne Heins, Hans Knoop, Tom Heskes\n  (The OPTIMISTIC consortium)", "title": "Causality on Cross-Sectional Data: Stable Specification Search in\n  Constrained Structural Equation Modeling", "comments": null, "journal-ref": "Applied.Soft.Comp. 52 (2017) 687-698", "doi": "10.1016/j.asoc.2016.10.003", "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Causal modeling has long been an attractive topic for many researchers and in\nrecent decades there has seen a surge in theoretical development and discovery\nalgorithms. Generally discovery algorithms can be divided into two approaches:\nconstraint-based and score-based. The constraint-based approach is able to\ndetect common causes of the observed variables but the use of independence\ntests makes it less reliable. The score-based approach produces a result that\nis easier to interpret as it also measures the reliability of the inferred\ncausal relationships, but it is unable to detect common confounders of the\nobserved variables. A drawback of both score-based and constrained-based\napproaches is the inherent instability in structure estimation. With finite\nsamples small changes in the data can lead to completely different optimal\nstructures. The present work introduces a new hypothesis-free score-based\ncausal discovery algorithm, called stable specification search, that is robust\nfor finite samples based on recent advances in stability selection using\nsubsampling and selection algorithms. Structure search is performed over\nStructural Equation Models. Our approach uses exploratory search but allows\nincorporation of prior background knowledge. We validated our approach on one\nsimulated data set, which we compare to the known ground truth, and two\nreal-world data sets for Chronic Fatigue Syndrome and Attention Deficit\nHyperactivity Disorder, which we compare to earlier medical studies. The\nresults on the simulated data set show significant improvement over alternative\napproaches and the results on the real-word data sets show consistency with the\nhypothesis driven models constructed by medical experts.\n", "versions": [{"version": "v1", "created": "Thu, 18 Jun 2015 09:33:10 GMT"}, {"version": "v2", "created": "Wed, 18 May 2016 09:42:37 GMT"}, {"version": "v3", "created": "Thu, 14 Jul 2016 14:30:55 GMT"}], "update_date": "2017-02-06", "authors_parsed": [["Rahmadi", "Ridho", "", "The OPTIMISTIC consortium"], ["Groot", "Perry", "", "The OPTIMISTIC consortium"], ["Heins", "Marianne", "", "The OPTIMISTIC consortium"], ["Knoop", "Hans", "", "The OPTIMISTIC consortium"], ["Heskes", "Tom", "", "The OPTIMISTIC consortium"]]}, {"id": "1506.05692", "submitter": "Maxime Gasse", "authors": "Maxime Gasse (DM2L), Alex Aussem (DM2L), Haytham Elghazel (DM2L)", "title": "A hybrid algorithm for Bayesian network structure learning with\n  application to multi-label learning", "comments": "arXiv admin note: text overlap with arXiv:1101.5184 by other authors", "journal-ref": "Expert Systems with Applications, Elsevier, 2014, 41 (15),\n  pp.6755-6772", "doi": "10.1016/j.eswa.2014.04.032", "report-no": null, "categories": "stat.ML cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a novel hybrid algorithm for Bayesian network structure learning,\ncalled H2PC. It first reconstructs the skeleton of a Bayesian network and then\nperforms a Bayesian-scoring greedy hill-climbing search to orient the edges.\nThe algorithm is based on divide-and-conquer constraint-based subroutines to\nlearn the local structure around a target variable. We conduct two series of\nexperimental comparisons of H2PC against Max-Min Hill-Climbing (MMHC), which is\ncurrently the most powerful state-of-the-art algorithm for Bayesian network\nstructure learning. First, we use eight well-known Bayesian network benchmarks\nwith various data sizes to assess the quality of the learned structure returned\nby the algorithms. Our extensive experiments show that H2PC outperforms MMHC in\nterms of goodness of fit to new data and quality of the network structure with\nrespect to the true dependence structure of the data. Second, we investigate\nH2PC's ability to solve the multi-label learning problem. We provide\ntheoretical results to characterize and identify graphically the so-called\nminimal label powersets that appear as irreducible factors in the joint\ndistribution under the faithfulness condition. The multi-label learning problem\nis then decomposed into a series of multi-class classification problems, where\neach multi-class variable encodes a label powerset. H2PC is shown to compare\nfavorably to MMHC in terms of global classification accuracy over ten\nmulti-label data sets covering different application domains. Overall, our\nexperiments support the conclusions that local structural learning with H2PC in\nthe form of local neighborhood induction is a theoretically well-motivated and\nempirically effective learning framework that is well suited to multi-label\nlearning. The source code (in R) of H2PC as well as all data sets used for the\nempirical tests are publicly available.\n", "versions": [{"version": "v1", "created": "Thu, 18 Jun 2015 14:24:19 GMT"}], "update_date": "2015-06-19", "authors_parsed": [["Gasse", "Maxime", "", "DM2L"], ["Aussem", "Alex", "", "DM2L"], ["Elghazel", "Haytham", "", "DM2L"]]}, {"id": "1506.05790", "submitter": "Akshay Balsubramani", "authors": "Akshay Balsubramani, Yoav Freund", "title": "Scalable Semi-Supervised Aggregation of Classifiers", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present and empirically evaluate an efficient algorithm that learns to\naggregate the predictions of an ensemble of binary classifiers. The algorithm\nuses the structure of the ensemble predictions on unlabeled data to yield\nsignificant performance improvements. It does this without making assumptions\non the structure or origin of the ensemble, without parameters, and as scalably\nas linear learning. We empirically demonstrate these performance gains with\nrandom forests.\n", "versions": [{"version": "v1", "created": "Thu, 18 Jun 2015 19:53:12 GMT"}, {"version": "v2", "created": "Wed, 11 Nov 2015 01:06:07 GMT"}], "update_date": "2015-11-12", "authors_parsed": [["Balsubramani", "Akshay", ""], ["Freund", "Yoav", ""]]}, {"id": "1506.05849", "submitter": "Xundong Wu", "authors": "Xundong Wu", "title": "An Iterative Convolutional Neural Network Algorithm Improves Electron\n  Microscopy Image Segmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.LG", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  To build the connectomics map of the brain, we developed a new algorithm that\ncan automatically refine the Membrane Detection Probability Maps (MDPM)\ngenerated to perform automatic segmentation of electron microscopy (EM) images.\nTo achieve this, we executed supervised training of a convolutional neural\nnetwork to recover the removed center pixel label of patches sampled from a\nMDPM. MDPM can be generated from other machine learning based algorithms\nrecognizing whether a pixel in an image corresponds to the cell membrane. By\niteratively applying this network over MDPM for multiple rounds, we were able\nto significantly improve membrane segmentation results.\n", "versions": [{"version": "v1", "created": "Thu, 18 Jun 2015 23:11:40 GMT"}], "update_date": "2015-06-22", "authors_parsed": [["Wu", "Xundong", ""]]}, {"id": "1506.05855", "submitter": "Paul Wiggins Dr", "authors": "Colin H. LaMont and Paul A. Wiggins", "title": "Information-based inference for singular models and finite sample sizes:\n  A frequentist information criterion", "comments": "30 Pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG physics.data-an", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the information-based paradigm of inference, model selection is performed\nby selecting the candidate model with the best estimated predictive\nperformance. The success of this approach depends on the accuracy of the\nestimate of the predictive complexity. In the large-sample-size limit of a\nregular model, the predictive performance is well estimated by the Akaike\nInformation Criterion (AIC). However, this approximation can either\nsignificantly under or over-estimating the complexity in a wide range of\nimportant applications where models are either non-regular or\nfinite-sample-size corrections are significant. We introduce an improved\napproximation for the complexity that is used to define a new information\ncriterion: the Frequentist Information Criterion (QIC). QIC extends the\napplicability of information-based inference to the finite-sample-size regime\nof regular models and to singular models. We demonstrate the power and the\ncomparative advantage of QIC in a number of example analyses.\n", "versions": [{"version": "v1", "created": "Fri, 19 Jun 2015 00:53:40 GMT"}, {"version": "v2", "created": "Fri, 8 Jan 2016 22:13:52 GMT"}, {"version": "v3", "created": "Thu, 24 Mar 2016 21:42:30 GMT"}, {"version": "v4", "created": "Wed, 16 Aug 2017 20:36:48 GMT"}, {"version": "v5", "created": "Fri, 8 Jun 2018 16:22:44 GMT"}], "update_date": "2018-06-11", "authors_parsed": [["LaMont", "Colin H.", ""], ["Wiggins", "Paul A.", ""]]}, {"id": "1506.05860", "submitter": "Shaobo Han", "authors": "Shaobo Han, Xuejun Liao, David B. Dunson, Lawrence Carin", "title": "Variational Gaussian Copula Inference", "comments": "Appearing in Proceedings of the 19th International Conference on\n  Artificial Intelligence and Statistics (AISTATS) 2016, Cadiz, Spain. JMLR:\n  W&CP volume 51", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We utilize copulas to constitute a unified framework for constructing and\noptimizing variational proposals in hierarchical Bayesian models. For models\nwith continuous and non-Gaussian hidden variables, we propose a semiparametric\nand automated variational Gaussian copula approach, in which the parametric\nGaussian copula family is able to preserve multivariate posterior dependence,\nand the nonparametric transformations based on Bernstein polynomials provide\nample flexibility in characterizing the univariate marginal posteriors.\n", "versions": [{"version": "v1", "created": "Fri, 19 Jun 2015 01:49:46 GMT"}, {"version": "v2", "created": "Wed, 16 Mar 2016 01:26:51 GMT"}, {"version": "v3", "created": "Wed, 18 May 2016 15:16:28 GMT"}], "update_date": "2016-05-19", "authors_parsed": [["Han", "Shaobo", ""], ["Liao", "Xuejun", ""], ["Dunson", "David B.", ""], ["Carin", "Lawrence", ""]]}, {"id": "1506.05865", "submitter": "Baotian Hu", "authors": "Baotian Hu, Qingcai Chen, Fangze Zhu", "title": "LCSTS: A Large Scale Chinese Short Text Summarization Dataset", "comments": "Recently, we received feedbacks from Yuya Taguchi from NAIST in Japan\n  and Qian Chen from USTC of China, that the results in the EMNLP2015 version\n  seem to be underrated. So we carefully checked our results and find out that\n  we made a mistake while using the standard ROUGE. Then we re-evaluate all\n  methods in the paper and get corrected results listed in Table 2 of this\n  version", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automatic text summarization is widely regarded as the highly difficult\nproblem, partially because of the lack of large text summarization data set.\nDue to the great challenge of constructing the large scale summaries for full\ntext, in this paper, we introduce a large corpus of Chinese short text\nsummarization dataset constructed from the Chinese microblogging website Sina\nWeibo, which is released to the public\n{http://icrc.hitsz.edu.cn/Article/show/139.html}. This corpus consists of over\n2 million real Chinese short texts with short summaries given by the author of\neach text. We also manually tagged the relevance of 10,666 short summaries with\ntheir corresponding short texts. Based on the corpus, we introduce recurrent\nneural network for the summary generation and achieve promising results, which\nnot only shows the usefulness of the proposed corpus for short text\nsummarization research, but also provides a baseline for further research on\nthis topic.\n", "versions": [{"version": "v1", "created": "Fri, 19 Jun 2015 02:40:42 GMT"}, {"version": "v2", "created": "Mon, 22 Jun 2015 14:33:39 GMT"}, {"version": "v3", "created": "Mon, 17 Aug 2015 02:43:38 GMT"}, {"version": "v4", "created": "Fri, 19 Feb 2016 16:35:35 GMT"}], "update_date": "2016-02-22", "authors_parsed": [["Hu", "Baotian", ""], ["Chen", "Qingcai", ""], ["Zhu", "Fangze", ""]]}, {"id": "1506.05900", "submitter": "Hassan Ashtiani", "authors": "Hassan Ashtiani, Shai Ben-David", "title": "Representation Learning for Clustering: A Statistical Framework", "comments": "To be published in Proceedings of UAI 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We address the problem of communicating domain knowledge from a user to the\ndesigner of a clustering algorithm. We propose a protocol in which the user\nprovides a clustering of a relatively small random sample of a data set. The\nalgorithm designer then uses that sample to come up with a data representation\nunder which $k$-means clustering results in a clustering (of the full data set)\nthat is aligned with the user's clustering. We provide a formal statistical\nmodel for analyzing the sample complexity of learning a clustering\nrepresentation with this paradigm. We then introduce a notion of capacity of a\nclass of possible representations, in the spirit of the VC-dimension, showing\nthat classes of representations that have finite such dimension can be\nsuccessfully learned with sample size error bounds, and end our discussion with\nan analysis of that dimension for classes of representations induced by linear\nembeddings.\n", "versions": [{"version": "v1", "created": "Fri, 19 Jun 2015 08:18:59 GMT"}], "update_date": "2015-06-22", "authors_parsed": [["Ashtiani", "Hassan", ""], ["Ben-David", "Shai", ""]]}, {"id": "1506.05908", "submitter": "Chris Piech", "authors": "Chris Piech, Jonathan Spencer, Jonathan Huang, Surya Ganguli, Mehran\n  Sahami, Leonidas Guibas, Jascha Sohl-Dickstein", "title": "Deep Knowledge Tracing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CY cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  Knowledge tracing---where a machine models the knowledge of a student as they\ninteract with coursework---is a well established problem in computer supported\neducation. Though effectively modeling student knowledge would have high\neducational impact, the task has many inherent challenges. In this paper we\nexplore the utility of using Recurrent Neural Networks (RNNs) to model student\nlearning. The RNN family of models have important advantages over previous\nmethods in that they do not require the explicit encoding of human domain\nknowledge, and can capture more complex representations of student knowledge.\nUsing neural networks results in substantial improvements in prediction\nperformance on a range of knowledge tracing datasets. Moreover the learned\nmodel can be used for intelligent curriculum design and allows straightforward\ninterpretation and discovery of structure in student tasks. These results\nsuggest a promising new line of research for knowledge tracing and an exemplary\napplication task for RNNs.\n", "versions": [{"version": "v1", "created": "Fri, 19 Jun 2015 08:29:00 GMT"}], "update_date": "2015-06-22", "authors_parsed": [["Piech", "Chris", ""], ["Spencer", "Jonathan", ""], ["Huang", "Jonathan", ""], ["Ganguli", "Surya", ""], ["Sahami", "Mehran", ""], ["Guibas", "Leonidas", ""], ["Sohl-Dickstein", "Jascha", ""]]}, {"id": "1506.05950", "submitter": "Tapio Pahikkala", "authors": "Tapio Pahikkala, Markus Viljanen, Antti Airola, Willem Waegeman", "title": "Spectral Analysis of Symmetric and Anti-Symmetric Pairwise Kernels", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of learning regression functions from pairwise data\nwhen there exists prior knowledge that the relation to be learned is symmetric\nor anti-symmetric. Such prior knowledge is commonly enforced by symmetrizing or\nanti-symmetrizing pairwise kernel functions. Through spectral analysis, we show\nthat these transformations reduce the kernel's effective dimension. Further, we\nprovide an analysis of the approximation properties of the resulting kernels,\nand bound the regularization bias of the kernels in terms of the corresponding\nbias of the original kernel.\n", "versions": [{"version": "v1", "created": "Fri, 19 Jun 2015 10:24:01 GMT"}], "update_date": "2015-06-22", "authors_parsed": [["Pahikkala", "Tapio", ""], ["Viljanen", "Markus", ""], ["Airola", "Antti", ""], ["Waegeman", "Willem", ""]]}, {"id": "1506.05985", "submitter": "Xavier Bresson", "authors": "Xavier Bresson and Thomas Laurent and James von Brecht", "title": "Enhanced Lasso Recovery on Graph", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work aims at recovering signals that are sparse on graphs. Compressed\nsensing offers techniques for signal recovery from a few linear measurements\nand graph Fourier analysis provides a signal representation on graph. In this\npaper, we leverage these two frameworks to introduce a new Lasso recovery\nalgorithm on graphs. More precisely, we present a non-convex, non-smooth\nalgorithm that outperforms the standard convex Lasso technique. We carry out\nnumerical experiments on three benchmark graph datasets.\n", "versions": [{"version": "v1", "created": "Fri, 19 Jun 2015 12:59:18 GMT"}], "update_date": "2015-06-22", "authors_parsed": [["Bresson", "Xavier", ""], ["Laurent", "Thomas", ""], ["von Brecht", "James", ""]]}, {"id": "1506.06021", "submitter": "Emilio Ferrara", "authors": "Emilio Ferrara, Zeyao Yang", "title": "Measuring Emotional Contagion in Social Media", "comments": "10 pages, 5 figures", "journal-ref": "PloS one, 10(11), e0142390. 2015", "doi": "10.1371/journal.pone.0142390", "report-no": null, "categories": "cs.SI cs.LG physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Social media are used as main discussion channels by millions of individuals\nevery day. The content individuals produce in daily social-media-based\nmicro-communications, and the emotions therein expressed, may impact the\nemotional states of others. A recent experiment performed on Facebook\nhypothesized that emotions spread online, even in absence of non-verbal cues\ntypical of in-person interactions, and that individuals are more likely to\nadopt positive or negative emotions if these are over-expressed in their social\nnetwork. Experiments of this type, however, raise ethical concerns, as they\nrequire massive-scale content manipulation with unknown consequences for the\nindividuals therein involved. Here, we study the dynamics of emotional\ncontagion using Twitter. Rather than manipulating content, we devise a null\nmodel that discounts some confounding factors (including the effect of\nemotional contagion). We measure the emotional valence of content the users are\nexposed to before posting their own tweets. We determine that on average a\nnegative post follows an over-exposure to 4.34% more negative content than\nbaseline, while positive posts occur after an average over-exposure to 4.50%\nmore positive contents. We highlight the presence of a linear relationship\nbetween the average emotional valence of the stimuli users are exposed to, and\nthat of the responses they produce. We also identify two different classes of\nindividuals: highly and scarcely susceptible to emotional contagion. Highly\nsusceptible users are significantly less inclined to adopt negative emotions\nthan the scarcely susceptible ones, but equally likely to adopt positive\nemotions. In general, the likelihood of adopting positive emotions is much\ngreater than that of negative emotions.\n", "versions": [{"version": "v1", "created": "Fri, 19 Jun 2015 14:29:24 GMT"}], "update_date": "2017-03-07", "authors_parsed": [["Ferrara", "Emilio", ""], ["Yang", "Zeyao", ""]]}, {"id": "1506.06068", "submitter": "Teng Qiu", "authors": "Teng Qiu, Yongjie Li", "title": "A general framework for the IT-based clustering methods", "comments": "17 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  Previously, we proposed a physically inspired rule to organize the data\npoints in a sparse yet effective structure, called the in-tree (IT) graph,\nwhich is able to capture a wide class of underlying cluster structures in the\ndatasets, especially for the density-based datasets. Although there are some\nredundant edges or lines between clusters requiring to be removed by computer,\nthis IT graph has a big advantage compared with the k-nearest-neighborhood\n(k-NN) or the minimal spanning tree (MST) graph, in that the redundant edges in\nthe IT graph are much more distinguishable and thus can be easily determined by\nseveral methods previously proposed by us.\n  In this paper, we propose a general framework to re-construct the IT graph,\nbased on an initial neighborhood graph, such as the k-NN or MST, etc, and the\ncorresponding graph distances. For this general framework, our previous way of\nconstructing the IT graph turns out to be a special case of it. This general\nframework 1) can make the IT graph capture a wider class of underlying cluster\nstructures in the datasets, especially for the manifolds, and 2) should be more\neffective to cluster the sparse or graph-based datasets.\n", "versions": [{"version": "v1", "created": "Fri, 19 Jun 2015 16:03:31 GMT"}], "update_date": "2015-06-22", "authors_parsed": [["Qiu", "Teng", ""], ["Li", "Yongjie", ""]]}, {"id": "1506.06072", "submitter": "Emilio Ferrara", "authors": "Emilio Ferrara, Zeyao Yang", "title": "Quantifying the Effect of Sentiment on Information Diffusion in Social\n  Media", "comments": "10 pages, 5 figures", "journal-ref": "PeerJ Computer Science, 1, e26. 2015", "doi": "10.7717/peerj-cs.26", "report-no": null, "categories": "cs.SI cs.LG physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Social media have become the main vehicle of information production and\nconsumption online. Millions of users every day log on their Facebook or\nTwitter accounts to get updates and news, read about their topics of interest,\nand become exposed to new opportunities and interactions. Although recent\nstudies suggest that the contents users produce will affect the emotions of\ntheir readers, we still lack a rigorous understanding of the role and effects\nof contents sentiment on the dynamics of information diffusion. This work aims\nat quantifying the effect of sentiment on information diffusion, to understand:\n(i) whether positive conversations spread faster and/or broader than negative\nones (or vice-versa); (ii) what kind of emotions are more typical of popular\nconversations on social media; and, (iii) what type of sentiment is expressed\nin conversations characterized by different temporal dynamics. Our findings\nshow that, at the level of contents, negative messages spread faster than\npositive ones, but positive ones reach larger audiences, suggesting that people\nare more inclined to share and favorite positive contents, the so-called\npositive bias. As for the entire conversations, we highlight how different\ntemporal dynamics exhibit different sentiment patterns: for example, positive\nsentiment builds up for highly-anticipated events, while unexpected events are\nmainly characterized by negative sentiment. Our contribution is a milestone to\nunderstand how the emotions expressed in short texts affect their spreading in\nonline social ecosystems, and may help to craft effective policies and\nstrategies for content generation and diffusion.\n", "versions": [{"version": "v1", "created": "Fri, 19 Jun 2015 16:12:19 GMT"}], "update_date": "2017-03-07", "authors_parsed": [["Ferrara", "Emilio", ""], ["Yang", "Zeyao", ""]]}, {"id": "1506.06081", "submitter": "Qinqing Zheng", "authors": "Qinqing Zheng, John Lafferty", "title": "A Convergent Gradient Descent Algorithm for Rank Minimization and\n  Semidefinite Programming from Random Linear Measurements", "comments": "Fix a minor error in Appendix E", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a simple, scalable, and fast gradient descent algorithm to\noptimize a nonconvex objective for the rank minimization problem and a closely\nrelated family of semidefinite programs. With $O(r^3 \\kappa^2 n \\log n)$ random\nmeasurements of a positive semidefinite $n \\times n$ matrix of rank $r$ and\ncondition number $\\kappa$, our method is guaranteed to converge linearly to the\nglobal optimum.\n", "versions": [{"version": "v1", "created": "Fri, 19 Jun 2015 16:41:08 GMT"}, {"version": "v2", "created": "Thu, 8 Oct 2015 22:46:11 GMT"}, {"version": "v3", "created": "Thu, 24 Mar 2016 16:27:51 GMT"}], "update_date": "2016-03-25", "authors_parsed": [["Zheng", "Qinqing", ""], ["Lafferty", "John", ""]]}, {"id": "1506.06100", "submitter": "Guillaume Bouchard", "authors": "Guillaume Bouchard, Balaji Lakshminarayanan", "title": "Approximate Inference with the Variational Holder Bound", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.FA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce the Variational Holder (VH) bound as an alternative to\nVariational Bayes (VB) for approximate Bayesian inference. Unlike VB which\ntypically involves maximization of a non-convex lower bound with respect to the\nvariational parameters, the VH bound involves minimization of a convex upper\nbound to the intractable integral with respect to the variational parameters.\nMinimization of the VH bound is a convex optimization problem; hence the VH\nmethod can be applied using off-the-shelf convex optimization algorithms and\nthe approximation error of the VH bound can also be analyzed using tools from\nconvex optimization literature. We present experiments on the task of\nintegrating a truncated multivariate Gaussian distribution and compare our\nmethod to VB, EP and a state-of-the-art numerical integration method for this\nproblem.\n", "versions": [{"version": "v1", "created": "Fri, 19 Jun 2015 18:00:40 GMT"}], "update_date": "2015-06-22", "authors_parsed": [["Bouchard", "Guillaume", ""], ["Lakshminarayanan", "Balaji", ""]]}, {"id": "1506.06112", "submitter": "Ethan Rudd", "authors": "Ethan M. Rudd, Lalit P. Jain, Walter J. Scheirer, Terrance E. Boult", "title": "The Extreme Value Machine", "comments": "Pre-print of a manuscript accepted to the IEEE Transactions on\n  Pattern Analysis and Machine Intelligence (T-PAMI) journal", "journal-ref": null, "doi": "10.1109/TPAMI.2017.2707495", "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is often desirable to be able to recognize when inputs to a recognition\nfunction learned in a supervised manner correspond to classes unseen at\ntraining time. With this ability, new class labels could be assigned to these\ninputs by a human operator, allowing them to be incorporated into the\nrecognition function --- ideally under an efficient incremental update\nmechanism. While good algorithms that assume inputs from a fixed set of classes\nexist, e.g., artificial neural networks and kernel machines, it is not\nimmediately obvious how to extend them to perform incremental learning in the\npresence of unknown query classes. Existing algorithms take little to no\ndistributional information into account when learning recognition functions and\nlack a strong theoretical foundation. We address this gap by formulating a\nnovel, theoretically sound classifier --- the Extreme Value Machine (EVM). The\nEVM has a well-grounded interpretation derived from statistical Extreme Value\nTheory (EVT), and is the first classifier to be able to perform nonlinear\nkernel-free variable bandwidth incremental learning. Compared to other\nclassifiers in the same deep network derived feature space, the EVM is accurate\nand efficient on an established benchmark partition of the ImageNet dataset.\n", "versions": [{"version": "v1", "created": "Fri, 19 Jun 2015 19:04:54 GMT"}, {"version": "v2", "created": "Tue, 12 Jan 2016 00:21:24 GMT"}, {"version": "v3", "created": "Wed, 18 May 2016 00:57:06 GMT"}, {"version": "v4", "created": "Sun, 21 May 2017 01:47:04 GMT"}], "update_date": "2017-05-23", "authors_parsed": [["Rudd", "Ethan M.", ""], ["Jain", "Lalit P.", ""], ["Scheirer", "Walter J.", ""], ["Boult", "Terrance E.", ""]]}, {"id": "1506.06129", "submitter": "Paul Wiggins Dr", "authors": "Paul A. Wiggins", "title": "A simple application of FIC to model selection", "comments": "7 Pages, 1 figure, & Appendix. arXiv admin note: text overlap with\n  arXiv:1506.05855", "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.data-an cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We have recently proposed a new information-based approach to model\nselection, the Frequentist Information Criterion (FIC), that reconciles\ninformation-based and frequentist inference. The purpose of this current paper\nis to provide a simple example of the application of this criterion and a\ndemonstration of the natural emergence of model complexities with both AIC-like\n($N^0$) and BIC-like ($\\log N$) scaling with observation number $N$. The\napplication developed is deliberately simplified to make the analysis\nanalytically tractable.\n", "versions": [{"version": "v1", "created": "Fri, 19 Jun 2015 00:39:43 GMT"}], "update_date": "2015-06-23", "authors_parsed": [["Wiggins", "Paul A.", ""]]}, {"id": "1506.06155", "submitter": "Mohammad Norouzi", "authors": "Mohammad Norouzi, Maxwell D. Collins, David J. Fleet, Pushmeet Kohli", "title": "CO2 Forest: Improved Random Forest by Continuous Optimization of Oblique\n  Splits", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel algorithm for optimizing multivariate linear threshold\nfunctions as split functions of decision trees to create improved Random Forest\nclassifiers. Standard tree induction methods resort to sampling and exhaustive\nsearch to find good univariate split functions. In contrast, our method\ncomputes a linear combination of the features at each node, and optimizes the\nparameters of the linear combination (oblique) split functions by adopting a\nvariant of latent variable SVM formulation. We develop a convex-concave upper\nbound on the classification loss for a one-level decision tree, and optimize\nthe bound by stochastic gradient descent at each internal node of the tree.\nForests of up to 1000 Continuously Optimized Oblique (CO2) decision trees are\ncreated, which significantly outperform Random Forest with univariate splits\nand previous techniques for constructing oblique trees. Experimental results\nare reported on multi-class classification benchmarks and on Labeled Faces in\nthe Wild (LFW) dataset.\n", "versions": [{"version": "v1", "created": "Fri, 19 Jun 2015 20:42:47 GMT"}, {"version": "v2", "created": "Wed, 24 Jun 2015 21:23:43 GMT"}], "update_date": "2015-06-26", "authors_parsed": [["Norouzi", "Mohammad", ""], ["Collins", "Maxwell D.", ""], ["Fleet", "David J.", ""], ["Kohli", "Pushmeet", ""]]}, {"id": "1506.06179", "submitter": "Aaron Clauset", "authors": "Amir Ghasemian and Pan Zhang and Aaron Clauset and Cristopher Moore\n  and Leto Peel", "title": "Detectability thresholds and optimal algorithms for community structure\n  in dynamic networks", "comments": "9 pages, 3 figures", "journal-ref": "Phys. Rev. X 6, 031005 (2016)", "doi": "10.1103/PhysRevX.6.031005", "report-no": null, "categories": "stat.ML cond-mat.dis-nn cs.LG cs.SI physics.data-an", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the fundamental limits on learning latent community structure in\ndynamic networks. Specifically, we study dynamic stochastic block models where\nnodes change their community membership over time, but where edges are\ngenerated independently at each time step. In this setting (which is a special\ncase of several existing models), we are able to derive the detectability\nthreshold exactly, as a function of the rate of change and the strength of the\ncommunities. Below this threshold, we claim that no algorithm can identify the\ncommunities better than chance. We then give two algorithms that are optimal in\nthe sense that they succeed all the way down to this limit. The first uses\nbelief propagation (BP), which gives asymptotically optimal accuracy, and the\nsecond is a fast spectral clustering algorithm, based on linearizing the BP\nequations. We verify our analytic and algorithmic results via numerical\nsimulation, and close with a brief discussion of extensions and open questions.\n", "versions": [{"version": "v1", "created": "Fri, 19 Jun 2015 23:07:51 GMT"}], "update_date": "2016-07-20", "authors_parsed": [["Ghasemian", "Amir", ""], ["Zhang", "Pan", ""], ["Clauset", "Aaron", ""], ["Moore", "Cristopher", ""], ["Peel", "Leto", ""]]}, {"id": "1506.06256", "submitter": "Grigori Fursin", "authors": "Grigori Fursin and Abdul Memon and Christophe Guillon and Anton\n  Lokhmotov", "title": "Collective Mind, Part II: Towards Performance- and Cost-Aware Software\n  Engineering as a Natural Science", "comments": "Presented at the 18th International Workshop on Compilers for\n  Parallel Computing (CPC'15), London, UK", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SE cs.LG cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Nowadays, engineers have to develop software often without even knowing which\nhardware it will eventually run on in numerous mobile phones, tablets,\ndesktops, laptops, data centers, supercomputers and cloud services.\nUnfortunately, optimizing compilers are not keeping pace with ever increasing\ncomplexity of computer systems anymore and may produce severely underperforming\nexecutable codes while wasting expensive resources and energy.\n  We present our practical and collaborative solution to this problem via\nlight-weight wrappers around any software piece when more than one\nimplementation or optimization choice available. These wrappers are connected\nwith a public Collective Mind autotuning infrastructure and repository of\nknowledge (c-mind.org/repo) to continuously monitor various important\ncharacteristics of these pieces (computational species) across numerous\nexisting hardware configurations together with randomly selected optimizations.\nSimilar to natural sciences, we can now continuously track winning solutions\n(optimizations for a given hardware) that minimize all costs of a computation\n(execution time, energy spent, code size, failures, memory and storage\nfootprint, optimization time, faults, contentions, inaccuracy and so on) of a\ngiven species on a Pareto frontier along with any unexpected behavior. The\ncommunity can then collaboratively classify solutions, prune redundant ones,\nand correlate them with various features of software, its inputs (data sets)\nand used hardware either manually or using powerful predictive analytics\ntechniques. Our approach can then help create a large, realistic, diverse,\nrepresentative, and continuously evolving benchmark with related optimization\nknowledge while gradually covering all possible software and hardware to be\nable to predict best optimizations and improve compilers and hardware depending\non usage scenarios and requirements.\n", "versions": [{"version": "v1", "created": "Sat, 20 Jun 2015 15:34:39 GMT"}], "update_date": "2015-06-23", "authors_parsed": [["Fursin", "Grigori", ""], ["Memon", "Abdul", ""], ["Guillon", "Christophe", ""], ["Lokhmotov", "Anton", ""]]}, {"id": "1506.06272", "submitter": "Fei Sha", "authors": "Junqi Jin, Kun Fu, Runpeng Cui, Fei Sha and Changshui Zhang", "title": "Aligning where to see and what to tell: image caption with region-based\n  attention and scene factorization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent progress on automatic generation of image captions has shown that it\nis possible to describe the most salient information conveyed by images with\naccurate and meaningful sentences. In this paper, we propose an image caption\nsystem that exploits the parallel structures between images and sentences. In\nour model, the process of generating the next word, given the previously\ngenerated ones, is aligned with the visual perception experience where the\nattention shifting among the visual regions imposes a thread of visual\nordering. This alignment characterizes the flow of \"abstract meaning\", encoding\nwhat is semantically shared by both the visual scene and the text description.\nOur system also makes another novel modeling contribution by introducing\nscene-specific contexts that capture higher-level semantic information encoded\nin an image. The contexts adapt language models for word generation to specific\nscene types. We benchmark our system and contrast to published results on\nseveral popular datasets. We show that using either region-based attention or\nscene-specific contexts improves systems without those components. Furthermore,\ncombining these two modeling ingredients attains the state-of-the-art\nperformance.\n", "versions": [{"version": "v1", "created": "Sat, 20 Jun 2015 17:25:38 GMT"}], "update_date": "2015-06-23", "authors_parsed": [["Jin", "Junqi", ""], ["Fu", "Kun", ""], ["Cui", "Runpeng", ""], ["Sha", "Fei", ""], ["Zhang", "Changshui", ""]]}, {"id": "1506.06274", "submitter": "Chuiwen Ma", "authors": "Chuiwen Ma, Hao Su, Liang Shi", "title": "Pose Estimation Based on 3D Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we proposed a pose estimation system based on rendered image\ntraining set, which predicts the pose of objects in real image, with knowledge\nof object category and tight bounding box. We developed a patch-based\nmulti-class classification algorithm, and an iterative approach to improve the\naccuracy. We achieved state-of-the-art performance on pose estimation task.\n", "versions": [{"version": "v1", "created": "Sat, 20 Jun 2015 17:55:49 GMT"}], "update_date": "2015-06-23", "authors_parsed": [["Ma", "Chuiwen", ""], ["Su", "Hao", ""], ["Shi", "Liang", ""]]}, {"id": "1506.06318", "submitter": "Shang-Tse Chen", "authors": "Shang-Tse Chen, Maria-Florina Balcan, Duen Horng Chau", "title": "Communication Efficient Distributed Agnostic Boosting", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of learning from distributed data in the agnostic\nsetting, i.e., in the presence of arbitrary forms of noise. Our main\ncontribution is a general distributed boosting-based procedure for learning an\narbitrary concept space, that is simultaneously noise tolerant, communication\nefficient, and computationally efficient. This improves significantly over\nprior works that were either communication efficient only in noise-free\nscenarios or computationally prohibitive. Empirical results on large synthetic\nand real-world datasets demonstrate the effectiveness and scalability of the\nproposed approach.\n", "versions": [{"version": "v1", "created": "Sun, 21 Jun 2015 04:35:42 GMT"}, {"version": "v2", "created": "Fri, 18 Nov 2016 16:55:03 GMT"}], "update_date": "2016-11-21", "authors_parsed": [["Chen", "Shang-Tse", ""], ["Balcan", "Maria-Florina", ""], ["Chau", "Duen Horng", ""]]}, {"id": "1506.06438", "submitter": "Christopher De Sa", "authors": "Christopher De Sa, Ce Zhang, Kunle Olukotun, Christopher R\\'e", "title": "Taming the Wild: A Unified Analysis of Hogwild!-Style Algorithms", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Stochastic gradient descent (SGD) is a ubiquitous algorithm for a variety of\nmachine learning problems. Researchers and industry have developed several\ntechniques to optimize SGD's runtime performance, including asynchronous\nexecution and reduced precision. Our main result is a martingale-based analysis\nthat enables us to capture the rich noise models that may arise from such\ntechniques. Specifically, we use our new analysis in three ways: (1) we derive\nconvergence rates for the convex case (Hogwild!) with relaxed assumptions on\nthe sparsity of the problem; (2) we analyze asynchronous SGD algorithms for\nnon-convex matrix problems including matrix completion; and (3) we design and\nanalyze an asynchronous SGD algorithm, called Buckwild!, that uses\nlower-precision arithmetic. We show experimentally that our algorithms run\nefficiently for a variety of problems on modern hardware.\n", "versions": [{"version": "v1", "created": "Mon, 22 Jun 2015 01:48:39 GMT"}, {"version": "v2", "created": "Fri, 2 Oct 2015 22:46:57 GMT"}], "update_date": "2015-10-06", "authors_parsed": [["De Sa", "Christopher", ""], ["Zhang", "Ce", ""], ["Olukotun", "Kunle", ""], ["R\u00e9", "Christopher", ""]]}, {"id": "1506.06442", "submitter": "Fandong Meng", "authors": "Fandong Meng, Zhengdong Lu, Zhaopeng Tu, Hang Li, and Qun Liu", "title": "A Deep Memory-based Architecture for Sequence-to-Sequence Learning", "comments": "13 pages, Under review as a conference paper at ICLR 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose DEEPMEMORY, a novel deep architecture for sequence-to-sequence\nlearning, which performs the task through a series of nonlinear transformations\nfrom the representation of the input sequence (e.g., a Chinese sentence) to the\nfinal output sequence (e.g., translation to English). Inspired by the recently\nproposed Neural Turing Machine (Graves et al., 2014), we store the intermediate\nrepresentations in stacked layers of memories, and use read-write operations on\nthe memories to realize the nonlinear transformations between the\nrepresentations. The types of transformations are designed in advance but the\nparameters are learned from data. Through layer-by-layer transformations,\nDEEPMEMORY can model complicated relations between sequences necessary for\napplications such as machine translation between distant languages. The\narchitecture can be trained with normal back-propagation on sequenceto-sequence\ndata, and the learning can be easily scaled up to a large corpus. DEEPMEMORY is\nbroad enough to subsume the state-of-the-art neural translation model in\n(Bahdanau et al., 2015) as its special case, while significantly improving upon\nthe model with its deeper architecture. Remarkably, DEEPMEMORY, being purely\nneural network-based, can achieve performance comparable to the traditional\nphrase-based machine translation system Moses with a small vocabulary and a\nmodest parameter size.\n", "versions": [{"version": "v1", "created": "Mon, 22 Jun 2015 02:12:54 GMT"}, {"version": "v2", "created": "Wed, 18 Nov 2015 13:55:44 GMT"}, {"version": "v3", "created": "Thu, 19 Nov 2015 14:23:34 GMT"}, {"version": "v4", "created": "Thu, 7 Jan 2016 08:14:08 GMT"}], "update_date": "2016-01-08", "authors_parsed": [["Meng", "Fandong", ""], ["Lu", "Zhengdong", ""], ["Tu", "Zhaopeng", ""], ["Li", "Hang", ""], ["Liu", "Qun", ""]]}, {"id": "1506.06472", "submitter": "Peter Sadowski", "authors": "Pierre Baldi and Peter Sadowski", "title": "A Theory of Local Learning, the Learning Channel, and the Optimality of\n  Backpropagation", "comments": null, "journal-ref": "Neural Networks, vol. 83, pp. 51-74, Nov. 2016", "doi": "10.1016/j.neunet.2016.07.006", "report-no": null, "categories": "cs.LG cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In a physical neural system, where storage and processing are intimately\nintertwined, the rules for adjusting the synaptic weights can only depend on\nvariables that are available locally, such as the activity of the pre- and\npost-synaptic neurons, resulting in local learning rules. A systematic\nframework for studying the space of local learning rules is obtained by first\nspecifying the nature of the local variables, and then the functional form that\nties them together into each learning rule. Such a framework enables also the\nsystematic discovery of new learning rules and exploration of relationships\nbetween learning rules and group symmetries. We study polynomial local learning\nrules stratified by their degree and analyze their behavior and capabilities in\nboth linear and non-linear units and networks. Stacking local learning rules in\ndeep feedforward networks leads to deep local learning. While deep local\nlearning can learn interesting representations, it cannot learn complex\ninput-output functions, even when targets are available for the top layer.\nLearning complex input-output functions requires local deep learning where\ntarget information is communicated to the deep layers through a backward\nlearning channel. The nature of the communicated information about the targets\nand the structure of the learning channel partition the space of learning\nalgorithms. We estimate the learning channel capacity associated with several\nalgorithms and show that backpropagation outperforms them by simultaneously\nmaximizing the information rate and minimizing the computational cost, even in\nrecurrent networks. The theory clarifies the concept of Hebbian learning,\nestablishes the power and limitations of local learning rules, introduces the\nlearning channel which enables a formal analysis of the optimality of\nbackpropagation, and explains the sparsity of the space of learning rules\ndiscovered so far.\n", "versions": [{"version": "v1", "created": "Mon, 22 Jun 2015 05:16:57 GMT"}, {"version": "v2", "created": "Fri, 21 Oct 2016 23:24:25 GMT"}], "update_date": "2016-10-25", "authors_parsed": [["Baldi", "Pierre", ""], ["Sadowski", "Peter", ""]]}, {"id": "1506.06490", "submitter": "Baotian Hu", "authors": "Xiaoqiang Zhou, Baotian Hu, Qingcai Chen, Buzhou Tang, Xiaolong Wang", "title": "Answer Sequence Learning with Neural Networks for Answer Selection in\n  Community Question Answering", "comments": "6 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, the answer selection problem in community question answering\n(CQA) is regarded as an answer sequence labeling task, and a novel approach is\nproposed based on the recurrent architecture for this problem. Our approach\napplies convolution neural networks (CNNs) to learning the joint representation\nof question-answer pair firstly, and then uses the joint representation as\ninput of the long short-term memory (LSTM) to learn the answer sequence of a\nquestion for labeling the matching quality of each answer. Experiments\nconducted on the SemEval 2015 CQA dataset shows the effectiveness of our\napproach.\n", "versions": [{"version": "v1", "created": "Mon, 22 Jun 2015 07:26:51 GMT"}], "update_date": "2015-06-23", "authors_parsed": [["Zhou", "Xiaoqiang", ""], ["Hu", "Baotian", ""], ["Chen", "Qingcai", ""], ["Tang", "Buzhou", ""], ["Wang", "Xiaolong", ""]]}, {"id": "1506.06573", "submitter": "Akshay Balsubramani", "authors": "Akshay Balsubramani", "title": "PAC-Bayes Iterated Logarithm Bounds for Martingale Mixtures", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.PR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We give tight concentration bounds for mixtures of martingales that are\nsimultaneously uniform over (a) mixture distributions, in a PAC-Bayes sense;\nand (b) all finite times. These bounds are proved in terms of the martingale\nvariance, extending classical Bernstein inequalities, and sharpening and\nsimplifying prior work.\n", "versions": [{"version": "v1", "created": "Mon, 22 Jun 2015 12:47:07 GMT"}], "update_date": "2015-06-23", "authors_parsed": [["Balsubramani", "Akshay", ""]]}, {"id": "1506.06579", "submitter": "Jason Yosinski", "authors": "Jason Yosinski and Jeff Clune and Anh Nguyen and Thomas Fuchs and Hod\n  Lipson", "title": "Understanding Neural Networks Through Deep Visualization", "comments": "12 pages. To appear at ICML Deep Learning Workshop 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.NE", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  Recent years have produced great advances in training large, deep neural\nnetworks (DNNs), including notable successes in training convolutional neural\nnetworks (convnets) to recognize natural images. However, our understanding of\nhow these models work, especially what computations they perform at\nintermediate layers, has lagged behind. Progress in the field will be further\naccelerated by the development of better tools for visualizing and interpreting\nneural nets. We introduce two such tools here. The first is a tool that\nvisualizes the activations produced on each layer of a trained convnet as it\nprocesses an image or video (e.g. a live webcam stream). We have found that\nlooking at live activations that change in response to user input helps build\nvaluable intuitions about how convnets work. The second tool enables\nvisualizing features at each layer of a DNN via regularized optimization in\nimage space. Because previous versions of this idea produced less recognizable\nimages, here we introduce several new regularization methods that combine to\nproduce qualitatively clearer, more interpretable visualizations. Both tools\nare open source and work on a pre-trained convnet with minimal setup.\n", "versions": [{"version": "v1", "created": "Mon, 22 Jun 2015 12:57:15 GMT"}], "update_date": "2015-06-23", "authors_parsed": [["Yosinski", "Jason", ""], ["Clune", "Jeff", ""], ["Nguyen", "Anh", ""], ["Fuchs", "Thomas", ""], ["Lipson", "Hod", ""]]}, {"id": "1506.06628", "submitter": "Yunchao Wei", "authors": "Yunchao Wei, Yao Zhao, Zhenfeng Zhu, Shikui Wei, Yanhui Xiao, Jiashi\n  Feng and Shuicheng Yan", "title": "Modality-dependent Cross-media Retrieval", "comments": "in ACM Transactions on Intelligent Systems and Technology", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we investigate the cross-media retrieval between images and\ntext, i.e., using image to search text (I2T) and using text to search images\n(T2I). Existing cross-media retrieval methods usually learn one couple of\nprojections, by which the original features of images and text can be projected\ninto a common latent space to measure the content similarity. However, using\nthe same projections for the two different retrieval tasks (I2T and T2I) may\nlead to a tradeoff between their respective performances, rather than their\nbest performances. Different from previous works, we propose a\nmodality-dependent cross-media retrieval (MDCR) model, where two couples of\nprojections are learned for different cross-media retrieval tasks instead of\none couple of projections. Specifically, by jointly optimizing the correlation\nbetween images and text and the linear regression from one modal space (image\nor text) to the semantic space, two couples of mappings are learned to project\nimages and text from their original feature spaces into two common latent\nsubspaces (one for I2T and the other for T2I). Extensive experiments show the\nsuperiority of the proposed MDCR compared with other methods. In particular,\nbased the 4,096 dimensional convolutional neural network (CNN) visual feature\nand 100 dimensional LDA textual feature, the mAP of the proposed method\nachieves 41.5\\%, which is a new state-of-the-art performance on the Wikipedia\ndataset.\n", "versions": [{"version": "v1", "created": "Mon, 22 Jun 2015 14:33:39 GMT"}, {"version": "v2", "created": "Tue, 23 Jun 2015 01:34:01 GMT"}], "update_date": "2015-06-24", "authors_parsed": [["Wei", "Yunchao", ""], ["Zhao", "Yao", ""], ["Zhu", "Zhenfeng", ""], ["Wei", "Shikui", ""], ["Xiao", "Yanhui", ""], ["Feng", "Jiashi", ""], ["Yan", "Shuicheng", ""]]}, {"id": "1506.06646", "submitter": "Tadahiro Taniguchi", "authors": "Tadahiro Taniguchi, Ryo Nakashima, and Shogo Nagasaka", "title": "Nonparametric Bayesian Double Articulation Analyzer for Direct Language\n  Acquisition from Continuous Speech Signals", "comments": "15 pages, 7 figures, Draft submitted to IEEE Transactions on\n  Autonomous Mental Development (TAMD)", "journal-ref": null, "doi": "10.1109/TCDS.2016.2550591", "report-no": null, "categories": "cs.AI cs.CL cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Human infants can discover words directly from unsegmented speech signals\nwithout any explicitly labeled data. In this paper, we develop a novel machine\nlearning method called nonparametric Bayesian double articulation analyzer\n(NPB-DAA) that can directly acquire language and acoustic models from observed\ncontinuous speech signals. For this purpose, we propose an integrative\ngenerative model that combines a language model and an acoustic model into a\nsingle generative model called the \"hierarchical Dirichlet process hidden\nlanguage model\" (HDP-HLM). The HDP-HLM is obtained by extending the\nhierarchical Dirichlet process hidden semi-Markov model (HDP-HSMM) proposed by\nJohnson et al. An inference procedure for the HDP-HLM is derived using the\nblocked Gibbs sampler originally proposed for the HDP-HSMM. This procedure\nenables the simultaneous and direct inference of language and acoustic models\nfrom continuous speech signals. Based on the HDP-HLM and its inference\nprocedure, we developed a novel double articulation analyzer. By assuming\nHDP-HLM as a generative model of observed time series data, and by inferring\nlatent variables of the model, the method can analyze latent double\narticulation structure, i.e., hierarchically organized latent words and\nphonemes, of the data in an unsupervised manner. The novel unsupervised double\narticulation analyzer is called NPB-DAA.\n  The NPB-DAA can automatically estimate double articulation structure embedded\nin speech signals. We also carried out two evaluation experiments using\nsynthetic data and actual human continuous speech signals representing Japanese\nvowel sequences. In the word acquisition and phoneme categorization tasks, the\nNPB-DAA outperformed a conventional double articulation analyzer (DAA) and\nbaseline automatic speech recognition system whose acoustic model was trained\nin a supervised manner.\n", "versions": [{"version": "v1", "created": "Mon, 22 Jun 2015 15:21:57 GMT"}, {"version": "v2", "created": "Wed, 9 Mar 2016 15:59:07 GMT"}], "update_date": "2016-11-17", "authors_parsed": [["Taniguchi", "Tadahiro", ""], ["Nakashima", "Ryo", ""], ["Nagasaka", "Shogo", ""]]}, {"id": "1506.06707", "submitter": "Faicel Chamroukhi", "authors": "Faicel Chamroukhi", "title": "Non-Normal Mixtures of Experts", "comments": "61 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mixture of Experts (MoE) is a popular framework for modeling heterogeneity in\ndata for regression, classification and clustering. For continuous data which\nwe consider here in the context of regression and cluster analysis, MoE usually\nuse normal experts, that is, expert components following the Gaussian\ndistribution. However, for a set of data containing a group or groups of\nobservations with asymmetric behavior, heavy tails or atypical observations,\nthe use of normal experts may be unsuitable and can unduly affect the fit of\nthe MoE model. In this paper, we introduce new non-normal mixture of experts\n(NNMoE) which can deal with these issues regarding possibly skewed,\nheavy-tailed data and with outliers. The proposed models are the skew-normal\nMoE and the robust $t$ MoE and skew $t$ MoE, respectively named SNMoE, TMoE and\nSTMoE. We develop dedicated expectation-maximization (EM) and expectation\nconditional maximization (ECM) algorithms to estimate the parameters of the\nproposed models by monotonically maximizing the observed data log-likelihood.\nWe describe how the presented models can be used in prediction and in\nmodel-based clustering of regression data. Numerical experiments carried out on\nsimulated data show the effectiveness and the robustness of the proposed models\nin terms modeling non-linear regression functions as well as in model-based\nclustering. Then, to show their usefulness for practical applications, the\nproposed models are applied to the real-world data of tone perception for\nmusical data analysis, and the one of temperature anomalies for the analysis of\nclimate change data.\n", "versions": [{"version": "v1", "created": "Mon, 22 Jun 2015 18:12:36 GMT"}, {"version": "v2", "created": "Sun, 28 Jun 2015 14:18:03 GMT"}], "update_date": "2015-06-30", "authors_parsed": [["Chamroukhi", "Faicel", ""]]}, {"id": "1506.06714", "submitter": "Michel Galley", "authors": "Alessandro Sordoni, Michel Galley, Michael Auli, Chris Brockett,\n  Yangfeng Ji, Margaret Mitchell, Jian-Yun Nie, Jianfeng Gao, Bill Dolan", "title": "A Neural Network Approach to Context-Sensitive Generation of\n  Conversational Responses", "comments": "A. Sordoni, M. Galley, M. Auli, C. Brockett, Y. Ji, M. Mitchell,\n  J.-Y. Nie, J. Gao, B. Dolan. 2015. A Neural Network Approach to\n  Context-Sensitive Generation of Conversational Responses. In Proc. of\n  NAACL-HLT. Pages 196-205", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a novel response generation system that can be trained end to end\non large quantities of unstructured Twitter conversations. A neural network\narchitecture is used to address sparsity issues that arise when integrating\ncontextual information into classic statistical models, allowing the system to\ntake into account previous dialog utterances. Our dynamic-context generative\nmodels show consistent gains over both context-sensitive and\nnon-context-sensitive Machine Translation and Information Retrieval baselines.\n", "versions": [{"version": "v1", "created": "Mon, 22 Jun 2015 18:29:03 GMT"}], "update_date": "2015-06-23", "authors_parsed": [["Sordoni", "Alessandro", ""], ["Galley", "Michel", ""], ["Auli", "Michael", ""], ["Brockett", "Chris", ""], ["Ji", "Yangfeng", ""], ["Mitchell", "Margaret", ""], ["Nie", "Jian-Yun", ""], ["Gao", "Jianfeng", ""], ["Dolan", "Bill", ""]]}, {"id": "1506.06726", "submitter": "Ryan Kiros", "authors": "Ryan Kiros, Yukun Zhu, Ruslan Salakhutdinov, Richard S. Zemel, Antonio\n  Torralba, Raquel Urtasun, Sanja Fidler", "title": "Skip-Thought Vectors", "comments": "11 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe an approach for unsupervised learning of a generic, distributed\nsentence encoder. Using the continuity of text from books, we train an\nencoder-decoder model that tries to reconstruct the surrounding sentences of an\nencoded passage. Sentences that share semantic and syntactic properties are\nthus mapped to similar vector representations. We next introduce a simple\nvocabulary expansion method to encode words that were not seen as part of\ntraining, allowing us to expand our vocabulary to a million words. After\ntraining our model, we extract and evaluate our vectors with linear models on 8\ntasks: semantic relatedness, paraphrase detection, image-sentence ranking,\nquestion-type classification and 4 benchmark sentiment and subjectivity\ndatasets. The end result is an off-the-shelf encoder that can produce highly\ngeneric sentence representations that are robust and perform well in practice.\nWe will make our encoder publicly available.\n", "versions": [{"version": "v1", "created": "Mon, 22 Jun 2015 19:33:40 GMT"}], "update_date": "2015-06-23", "authors_parsed": [["Kiros", "Ryan", ""], ["Zhu", "Yukun", ""], ["Salakhutdinov", "Ruslan", ""], ["Zemel", "Richard S.", ""], ["Torralba", "Antonio", ""], ["Urtasun", "Raquel", ""], ["Fidler", "Sanja", ""]]}, {"id": "1506.06840", "submitter": "Sashank J. Reddi", "authors": "Sashank J. Reddi, Ahmed Hefny, Suvrit Sra, Barnab\\'as P\\'oczos, Alex\n  Smola", "title": "On Variance Reduction in Stochastic Gradient Descent and its\n  Asynchronous Variants", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study optimization algorithms based on variance reduction for stochastic\ngradient descent (SGD). Remarkable recent progress has been made in this\ndirection through development of algorithms like SAG, SVRG, SAGA. These\nalgorithms have been shown to outperform SGD, both theoretically and\nempirically. However, asynchronous versions of these algorithms---a crucial\nrequirement for modern large-scale applications---have not been studied. We\nbridge this gap by presenting a unifying framework for many variance reduction\ntechniques. Subsequently, we propose an asynchronous algorithm grounded in our\nframework, and prove its fast convergence. An important consequence of our\ngeneral approach is that it yields asynchronous versions of variance reduction\nalgorithms such as SVRG and SAGA as a byproduct. Our method achieves near\nlinear speedup in sparse settings common to machine learning. We demonstrate\nthe empirical performance of our method through a concrete realization of\nasynchronous SVRG.\n", "versions": [{"version": "v1", "created": "Tue, 23 Jun 2015 01:57:19 GMT"}, {"version": "v2", "created": "Mon, 25 Jan 2016 01:12:06 GMT"}], "update_date": "2016-01-26", "authors_parsed": [["Reddi", "Sashank J.", ""], ["Hefny", "Ahmed", ""], ["Sra", "Suvrit", ""], ["P\u00f3czos", "Barnab\u00e1s", ""], ["Smola", "Alex", ""]]}, {"id": "1506.06868", "submitter": "Luping Zhou", "authors": "Luping Zhou, Lei Wang, Lingqiao Liu, Philip Ogunbona, Dinggang Shen", "title": "Learning Discriminative Bayesian Networks from High-dimensional\n  Continuous Neuroimaging Data", "comments": "16 pages and 5 figures for the article (excluding appendix)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Due to its causal semantics, Bayesian networks (BN) have been widely employed\nto discover the underlying data relationship in exploratory studies, such as\nbrain research. Despite its success in modeling the probability distribution of\nvariables, BN is naturally a generative model, which is not necessarily\ndiscriminative. This may cause the ignorance of subtle but critical network\nchanges that are of investigation values across populations. In this paper, we\npropose to improve the discriminative power of BN models for continuous\nvariables from two different perspectives. This brings two general\ndiscriminative learning frameworks for Gaussian Bayesian networks (GBN). In the\nfirst framework, we employ Fisher kernel to bridge the generative models of GBN\nand the discriminative classifiers of SVMs, and convert the GBN parameter\nlearning to Fisher kernel learning via minimizing a generalization error bound\nof SVMs. In the second framework, we employ the max-margin criterion and build\nit directly upon GBN models to explicitly optimize the classification\nperformance of the GBNs. The advantages and disadvantages of the two frameworks\nare discussed and experimentally compared. Both of them demonstrate strong\npower in learning discriminative parameters of GBNs for neuroimaging based\nbrain network analysis, as well as maintaining reasonable representation\ncapacity. The contributions of this paper also include a new Directed Acyclic\nGraph (DAG) constraint with theoretical guarantee to ensure the graph validity\nof GBN.\n", "versions": [{"version": "v1", "created": "Tue, 23 Jun 2015 05:39:34 GMT"}], "update_date": "2015-06-24", "authors_parsed": [["Zhou", "Luping", ""], ["Wang", "Lei", ""], ["Liu", "Lingqiao", ""], ["Ogunbona", "Philip", ""], ["Shen", "Dinggang", ""]]}, {"id": "1506.06962", "submitter": "Fabrice Rossi", "authors": "Pierre Latouche (SAMM), Fabrice Rossi (SAMM)", "title": "Graphs in machine learning: an introduction", "comments": null, "journal-ref": "European Symposium on Artificial Neural Networks, Computational\n  Intelligence and Machine Learning (ESANN), Apr 2015, Bruges, Belgium.\n  pp.207-218, 2015, Proceedings of the 23-th European Symposium on Artificial\n  Neural Networks, Computational Intelligence and Machine Learning (ESANN 2015)", "doi": null, "report-no": null, "categories": "stat.ML cs.LG cs.SI physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Graphs are commonly used to characterise interactions between objects of\ninterest. Because they are based on a straightforward formalism, they are used\nin many scientific fields from computer science to historical sciences. In this\npaper, we give an introduction to some methods relying on graphs for learning.\nThis includes both unsupervised and supervised methods. Unsupervised learning\nalgorithms usually aim at visualising graphs in latent spaces and/or clustering\nthe nodes. Both focus on extracting knowledge from graph topologies. While most\nexisting techniques are only applicable to static graphs, where edges do not\nevolve through time, recent developments have shown that they could be extended\nto deal with evolving networks. In a supervised context, one generally aims at\ninferring labels or numerical values attached to nodes using both the graph\nand, when they are available, node characteristics. Balancing the two sources\nof information can be challenging, especially as they can disagree locally or\nglobally. In both contexts, supervised and un-supervised, data can be\nrelational (augmented with one or several global graphs) as described above, or\ngraph valued. In this latter case, each object of interest is given as a full\ngraph (possibly completed by other characteristics). In this context, natural\ntasks include graph clustering (as in producing clusters of graphs rather than\nclusters of nodes in a single graph), graph classification, etc. 1 Real\nnetworks One of the first practical studies on graphs can be dated back to the\noriginal work of Moreno [51] in the 30s. Since then, there has been a growing\ninterest in graph analysis associated with strong developments in the modelling\nand the processing of these data. Graphs are now used in many scientific\nfields. In Biology [54, 2, 7], for instance, metabolic networks can describe\npathways of biochemical reactions [41], while in social sciences networks are\nused to represent relation ties between actors [66, 56, 36, 34]. Other examples\ninclude powergrids [71] and the web [75]. Recently, networks have also been\nconsidered in other areas such as geography [22] and history [59, 39]. In\nmachine learning, networks are seen as powerful tools to model problems in\norder to extract information from data and for prediction purposes. This is the\nobject of this paper. For more complete surveys, we refer to [28, 62, 49, 45].\nIn this section, we introduce notations and highlight properties shared by most\nreal networks. In Section 2, we then consider methods aiming at extracting\ninformation from a unique network. We will particularly focus on clustering\nmethods where the goal is to find clusters of vertices. Finally, in Section 3,\ntechniques that take a series of networks into account, where each network is\n", "versions": [{"version": "v1", "created": "Tue, 23 Jun 2015 12:12:45 GMT"}], "update_date": "2015-06-24", "authors_parsed": [["Latouche", "Pierre", "", "SAMM"], ["Rossi", "Fabrice", "", "SAMM"]]}, {"id": "1506.06972", "submitter": "Gergo Barta", "authors": "Gergo Barta, Gyula Borbely, Gabor Nagy, Sandor Kazi, Tamas Henk", "title": "GEFCOM 2014 - Probabilistic Electricity Price Forecasting", "comments": "10 pages, 5 figures, KES-IDT 2015 conference. The final publication\n  is available at Springer via http://dx.doi.org/10.1007/978-3-319-19857-6_7", "journal-ref": null, "doi": "10.1007/978-3-319-19857-6_7", "report-no": null, "categories": "stat.ML cs.CE cs.LG stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Energy price forecasting is a relevant yet hard task in the field of\nmulti-step time series forecasting. In this paper we compare a well-known and\nestablished method, ARMA with exogenous variables with a relatively new\ntechnique Gradient Boosting Regression. The method was tested on data from\nGlobal Energy Forecasting Competition 2014 with a year long rolling window\nforecast. The results from the experiment reveal that a multi-model approach is\nsignificantly better performing in terms of error metrics. Gradient Boosting\ncan deal with seasonality and auto-correlation out-of-the box and achieve lower\nrate of normalized mean absolute error on real-world data.\n", "versions": [{"version": "v1", "created": "Tue, 23 Jun 2015 12:27:50 GMT"}], "update_date": "2015-06-24", "authors_parsed": [["Barta", "Gergo", ""], ["Borbely", "Gyula", ""], ["Nagy", "Gabor", ""], ["Kazi", "Sandor", ""], ["Henk", "Tamas", ""]]}, {"id": "1506.06980", "submitter": "Mary Wootters", "authors": "Moritz Hardt, Nimrod Megiddo, Christos Papadimitriou, Mary Wootters", "title": "Strategic Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Machine learning relies on the assumption that unseen test instances of a\nclassification problem follow the same distribution as observed training data.\nHowever, this principle can break down when machine learning is used to make\nimportant decisions about the welfare (employment, education, health) of\nstrategic individuals. Knowing information about the classifier, such\nindividuals may manipulate their attributes in order to obtain a better\nclassification outcome. As a result of this behavior---often referred to as\ngaming---the performance of the classifier may deteriorate sharply. Indeed,\ngaming is a well-known obstacle for using machine learning methods in practice;\nin financial policy-making, the problem is widely known as Goodhart's law. In\nthis paper, we formalize the problem, and pursue algorithms for learning\nclassifiers that are robust to gaming.\n  We model classification as a sequential game between a player named \"Jury\"\nand a player named \"Contestant.\" Jury designs a classifier, and Contestant\nreceives an input to the classifier, which he may change at some cost. Jury's\ngoal is to achieve high classification accuracy with respect to Contestant's\noriginal input and some underlying target classification function. Contestant's\ngoal is to achieve a favorable classification outcome while taking into account\nthe cost of achieving it.\n  For a natural class of cost functions, we obtain computationally efficient\nlearning algorithms which are near-optimal. Surprisingly, our algorithms are\nefficient even on concept classes that are computationally hard to learn. For\ngeneral cost functions, designing an approximately optimal strategy-proof\nclassifier, for inverse-polynomial approximation, is NP-hard.\n", "versions": [{"version": "v1", "created": "Tue, 23 Jun 2015 13:22:10 GMT"}, {"version": "v2", "created": "Sun, 22 Nov 2015 18:33:34 GMT"}], "update_date": "2015-11-24", "authors_parsed": [["Hardt", "Moritz", ""], ["Megiddo", "Nimrod", ""], ["Papadimitriou", "Christos", ""], ["Wootters", "Mary", ""]]}, {"id": "1506.07190", "submitter": "Nikola Mrksic", "authors": "Nikola Mrk\\v{s}i\\'c, Diarmuid \\'O S\\'eaghdha, Blaise Thomson, Milica\n  Ga\\v{s}i\\'c, Pei-Hao Su, David Vandyke, Tsung-Hsien Wen and Steve Young", "title": "Multi-domain Dialog State Tracking using Recurrent Neural Networks", "comments": "Accepted as a short paper in the 53rd Annual Meeting of the\n  Association for Computational Linguistics (ACL 2015)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Dialog state tracking is a key component of many modern dialog systems, most\nof which are designed with a single, well-defined domain in mind. This paper\nshows that dialog data drawn from different dialog domains can be used to train\na general belief tracking model which can operate across all of these domains,\nexhibiting superior performance to each of the domain-specific models. We\npropose a training procedure which uses out-of-domain data to initialise belief\ntracking models for entirely new domains. This procedure leads to improvements\nin belief tracking performance regardless of the amount of in-domain data\navailable for training the model.\n", "versions": [{"version": "v1", "created": "Tue, 23 Jun 2015 20:16:06 GMT"}], "update_date": "2015-06-25", "authors_parsed": [["Mrk\u0161i\u0107", "Nikola", ""], ["S\u00e9aghdha", "Diarmuid \u00d3", ""], ["Thomson", "Blaise", ""], ["Ga\u0161i\u0107", "Milica", ""], ["Su", "Pei-Hao", ""], ["Vandyke", "David", ""], ["Wen", "Tsung-Hsien", ""], ["Young", "Steve", ""]]}, {"id": "1506.07212", "submitter": "Rafael Frongillo", "authors": "Rafael Frongillo, Ian A. Kash", "title": "Elicitation Complexity of Statistical Properties", "comments": "This version fixes an error in the condition needed for the main\n  lower bound and adds an application to Range Value at Risk, along with a\n  substantial reorganization of the paper and numerous smaller changes. A\n  previous version appeared in Neural Information Processing Systems 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.OC math.ST q-fin.MF stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A property, or statistical functional, is said to be elicitable if it\nminimizes expected loss for some loss function. The study of which properties\nare elicitable sheds light on the capabilities and limitations of point\nestimation and empirical risk minimization. While recent work asks which\nproperties are elicitable, we instead advocate for a more nuanced question: how\nmany dimensions are required to indirectly elicit a given property? This number\nis called the elicitation complexity of the property. We lay the foundation for\na general theory of elicitation complexity, including several basic results\nabout how elicitation complexity behaves, and the complexity of standard\nproperties of interest. Building on this foundation, our main result gives\ntight complexity bounds for the broad class of Bayes risks. We apply these\nresults to several properties of interest, including variance, entropy, norms,\nand several classes of financial risk measures. We conclude with discussion and\nopen directions.\n", "versions": [{"version": "v1", "created": "Tue, 23 Jun 2015 23:22:05 GMT"}, {"version": "v2", "created": "Mon, 8 Oct 2018 00:45:54 GMT"}, {"version": "v3", "created": "Thu, 27 Aug 2020 20:42:55 GMT"}], "update_date": "2020-08-31", "authors_parsed": [["Frongillo", "Rafael", ""], ["Kash", "Ian A.", ""]]}, {"id": "1506.07216", "submitter": "Tengyu Ma", "authors": "Mark Braverman, Ankit Garg, Tengyu Ma, Huy L. Nguyen, and David P.\n  Woodruff", "title": "Communication Lower Bounds for Statistical Estimation Problems via a\n  Distributed Data Processing Inequality", "comments": "To appear at STOC 2016. Fixed typos in theorem 4.5 and incorporated\n  reviewers' suggestions", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CC cs.IT math.IT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the tradeoff between the statistical error and communication cost of\ndistributed statistical estimation problems in high dimensions. In the\ndistributed sparse Gaussian mean estimation problem, each of the $m$ machines\nreceives $n$ data points from a $d$-dimensional Gaussian distribution with\nunknown mean $\\theta$ which is promised to be $k$-sparse. The machines\ncommunicate by message passing and aim to estimate the mean $\\theta$. We\nprovide a tight (up to logarithmic factors) tradeoff between the estimation\nerror and the number of bits communicated between the machines. This directly\nleads to a lower bound for the distributed \\textit{sparse linear regression}\nproblem: to achieve the statistical minimax error, the total communication is\nat least $\\Omega(\\min\\{n,d\\}m)$, where $n$ is the number of observations that\neach machine receives and $d$ is the ambient dimension. These lower results\nimprove upon [Sha14,SD'14] by allowing multi-round iterative communication\nmodel. We also give the first optimal simultaneous protocol in the dense case\nfor mean estimation.\n  As our main technique, we prove a \\textit{distributed data processing\ninequality}, as a generalization of usual data processing inequalities, which\nmight be of independent interest and useful for other problems.\n", "versions": [{"version": "v1", "created": "Wed, 24 Jun 2015 01:01:41 GMT"}, {"version": "v2", "created": "Sun, 22 Nov 2015 23:37:03 GMT"}, {"version": "v3", "created": "Tue, 10 May 2016 00:58:29 GMT"}], "update_date": "2016-05-11", "authors_parsed": [["Braverman", "Mark", ""], ["Garg", "Ankit", ""], ["Ma", "Tengyu", ""], ["Nguyen", "Huy L.", ""], ["Woodruff", "David P.", ""]]}, {"id": "1506.07251", "submitter": "Kevin Vervier", "authors": "K\\'evin Vervier (CBIO), Pierre Mah\\'e, Jean-Baptiste Veyrieras,\n  Jean-Philippe Vert (CBIO)", "title": "Benchmark of structured machine learning methods for microbial\n  identification from mass-spectrometry data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG q-bio.QM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Microbial identification is a central issue in microbiology, in particular in\nthe fields of infectious diseases diagnosis and industrial quality control. The\nconcept of species is tightly linked to the concept of biological and clinical\nclassification where the proximity between species is generally measured in\nterms of evolutionary distances and/or clinical phenotypes. Surprisingly, the\ninformation provided by this well-known hierarchical structure is rarely used\nby machine learning-based automatic microbial identification systems.\nStructured machine learning methods were recently proposed for taking into\naccount the structure embedded in a hierarchy and using it as additional a\npriori information, and could therefore allow to improve microbial\nidentification systems. We test and compare several state-of-the-art machine\nlearning methods for microbial identification on a new Matrix-Assisted Laser\nDesorption/Ionization Time-of-Flight mass spectrometry (MALDI-TOF MS) dataset.\nWe include in the benchmark standard and structured methods, that leverage the\nknowledge of the underlying hierarchical structure in the learning process. Our\nresults show that although some methods perform better than others, structured\nmethods do not consistently perform better than their \"flat\" counterparts. We\npostulate that this is partly due to the fact that standard methods already\nreach a high level of accuracy in this context, and that they mainly confuse\nspecies close to each other in the tree, a case where using the known hierarchy\nis not helpful.\n", "versions": [{"version": "v1", "created": "Wed, 24 Jun 2015 06:13:15 GMT"}], "update_date": "2015-06-25", "authors_parsed": [["Vervier", "K\u00e9vin", "", "CBIO"], ["Mah\u00e9", "Pierre", "", "CBIO"], ["Veyrieras", "Jean-Baptiste", "", "CBIO"], ["Vert", "Jean-Philippe", "", "CBIO"]]}, {"id": "1506.07254", "submitter": "Ugo Louche", "authors": "Ugo Louche, Liva Ralaivola", "title": "Unconfused ultraconservative multiclass algorithms", "comments": null, "journal-ref": "Machine Learning, Springer Verlag (Germany), 2015, Machine\n  learning, 99 (2), pp.351", "doi": "10.1007/s10994-015-5490-3", "report-no": "MLJ-2015", "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We tackle the problem of learning linear classifiers from noisy datasets in a\nmulticlass setting. The two-class version of this problem was studied a few\nyears ago where the proposed approaches to combat the noise revolve around a\nPer-ceptron learning scheme fed with peculiar examples computed through a\nweighted average of points from the noisy training set. We propose to build\nupon these approaches and we introduce a new algorithm called UMA (for\nUnconfused Multiclass additive Algorithm) which may be seen as a generalization\nto the multiclass setting of the previous approaches. In order to characterize\nthe noise we use the confusion matrix as a multiclass extension of the\nclassification noise studied in the aforemen-tioned literature. Theoretically\nwell-founded, UMA furthermore displays very good empirical noise robustness, as\nevidenced by numerical simulations conducted on both synthetic and real data.\n", "versions": [{"version": "v1", "created": "Wed, 24 Jun 2015 06:31:21 GMT"}], "update_date": "2015-06-25", "authors_parsed": [["Louche", "Ugo", ""], ["Ralaivola", "Liva", ""]]}, {"id": "1506.07285", "submitter": "Richard Socher", "authors": "Ankit Kumar and Ozan Irsoy and Peter Ondruska and Mohit Iyyer and\n  James Bradbury and Ishaan Gulrajani and Victor Zhong and Romain Paulus and\n  Richard Socher", "title": "Ask Me Anything: Dynamic Memory Networks for Natural Language Processing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most tasks in natural language processing can be cast into question answering\n(QA) problems over language input. We introduce the dynamic memory network\n(DMN), a neural network architecture which processes input sequences and\nquestions, forms episodic memories, and generates relevant answers. Questions\ntrigger an iterative attention process which allows the model to condition its\nattention on the inputs and the result of previous iterations. These results\nare then reasoned over in a hierarchical recurrent sequence model to generate\nanswers. The DMN can be trained end-to-end and obtains state-of-the-art results\non several types of tasks and datasets: question answering (Facebook's bAbI\ndataset), text classification for sentiment analysis (Stanford Sentiment\nTreebank) and sequence modeling for part-of-speech tagging (WSJ-PTB). The\ntraining for these different tasks relies exclusively on trained word vector\nrepresentations and input-question-answer triplets.\n", "versions": [{"version": "v1", "created": "Wed, 24 Jun 2015 08:27:02 GMT"}, {"version": "v2", "created": "Fri, 24 Jul 2015 22:21:29 GMT"}, {"version": "v3", "created": "Tue, 29 Sep 2015 05:02:29 GMT"}, {"version": "v4", "created": "Tue, 9 Feb 2016 08:19:30 GMT"}, {"version": "v5", "created": "Sat, 5 Mar 2016 20:18:55 GMT"}], "update_date": "2016-03-08", "authors_parsed": [["Kumar", "Ankit", ""], ["Irsoy", "Ozan", ""], ["Ondruska", "Peter", ""], ["Iyyer", "Mohit", ""], ["Bradbury", "James", ""], ["Gulrajani", "Ishaan", ""], ["Zhong", "Victor", ""], ["Paulus", "Romain", ""], ["Socher", "Richard", ""]]}, {"id": "1506.07300", "submitter": "Luc Le Magoarou", "authors": "Luc Le Magoarou and R\\'emi Gribonval", "title": "Flexible Multi-layer Sparse Approximations of Matrices and Applications", "comments": null, "journal-ref": null, "doi": "10.1109/JSTSP.2016.2543461", "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The computational cost of many signal processing and machine learning\ntechniques is often dominated by the cost of applying certain linear operators\nto high-dimensional vectors. This paper introduces an algorithm aimed at\nreducing the complexity of applying linear operators in high dimension by\napproximately factorizing the corresponding matrix into few sparse factors. The\napproach relies on recent advances in non-convex optimization. It is first\nexplained and analyzed in details and then demonstrated experimentally on\nvarious problems including dictionary learning for image denoising, and the\napproximation of large matrices arising in inverse problems.\n", "versions": [{"version": "v1", "created": "Wed, 24 Jun 2015 10:02:13 GMT"}, {"version": "v2", "created": "Tue, 29 Mar 2016 07:56:08 GMT"}], "update_date": "2016-03-30", "authors_parsed": [["Magoarou", "Luc Le", ""], ["Gribonval", "R\u00e9mi", ""]]}, {"id": "1506.07365", "submitter": "Manuel Watter", "authors": "Manuel Watter, Jost Tobias Springenberg, Joschka Boedecker, Martin\n  Riedmiller", "title": "Embed to Control: A Locally Linear Latent Dynamics Model for Control\n  from Raw Images", "comments": "Final NIPS version", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce Embed to Control (E2C), a method for model learning and control\nof non-linear dynamical systems from raw pixel images. E2C consists of a deep\ngenerative model, belonging to the family of variational autoencoders, that\nlearns to generate image trajectories from a latent space in which the dynamics\nis constrained to be locally linear. Our model is derived directly from an\noptimal control formulation in latent space, supports long-term prediction of\nimage sequences and exhibits strong performance on a variety of complex control\nproblems.\n", "versions": [{"version": "v1", "created": "Wed, 24 Jun 2015 13:48:51 GMT"}, {"version": "v2", "created": "Thu, 25 Jun 2015 21:08:02 GMT"}, {"version": "v3", "created": "Fri, 20 Nov 2015 14:49:18 GMT"}], "update_date": "2015-11-23", "authors_parsed": [["Watter", "Manuel", ""], ["Springenberg", "Jost Tobias", ""], ["Boedecker", "Joschka", ""], ["Riedmiller", "Martin", ""]]}, {"id": "1506.07452", "submitter": "Wonmin Byeon", "authors": "Marijn F. Stollenga, Wonmin Byeon, Marcus Liwicki, Juergen Schmidhuber", "title": "Parallel Multi-Dimensional LSTM, With Application to Fast Biomedical\n  Volumetric Image Segmentation", "comments": "Marijn F. Stollenga and Wonmin Byeon are the shared first authors,\n  both authors contributed equally to this work", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolutional Neural Networks (CNNs) can be shifted across 2D images or 3D\nvideos to segment them. They have a fixed input size and typically perceive\nonly small local contexts of the pixels to be classified as foreground or\nbackground. In contrast, Multi-Dimensional Recurrent NNs (MD-RNNs) can perceive\nthe entire spatio-temporal context of each pixel in a few sweeps through all\npixels, especially when the RNN is a Long Short-Term Memory (LSTM). Despite\nthese theoretical advantages, however, unlike CNNs, previous MD-LSTM variants\nwere hard to parallelize on GPUs. Here we re-arrange the traditional cuboid\norder of computations in MD-LSTM in pyramidal fashion. The resulting\nPyraMiD-LSTM is easy to parallelize, especially for 3D data such as stacks of\nbrain slice images. PyraMiD-LSTM achieved best known pixel-wise brain image\nsegmentation results on MRBrainS13 (and competitive results on EM-ISBI12).\n", "versions": [{"version": "v1", "created": "Wed, 24 Jun 2015 16:26:51 GMT"}], "update_date": "2015-07-13", "authors_parsed": [["Stollenga", "Marijn F.", ""], ["Byeon", "Wonmin", ""], ["Liwicki", "Marcus", ""], ["Schmidhuber", "Juergen", ""]]}, {"id": "1506.07477", "submitter": "Jiatao Gu", "authors": "Jiatao Gu and Victor O.K. Li", "title": "Efficient Learning for Undirected Topic Models", "comments": "Accepted by ACL-IJCNLP 2015 short paper. 6 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL cs.IR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Replicated Softmax model, a well-known undirected topic model, is powerful in\nextracting semantic representations of documents. Traditional learning\nstrategies such as Contrastive Divergence are very inefficient. This paper\nprovides a novel estimator to speed up the learning based on Noise Contrastive\nEstimate, extended for documents of variant lengths and weighted inputs.\nExperiments on two benchmarks show that the new estimator achieves great\nlearning efficiency and high accuracy on document retrieval and classification.\n", "versions": [{"version": "v1", "created": "Wed, 24 Jun 2015 17:27:28 GMT"}], "update_date": "2015-06-25", "authors_parsed": [["Gu", "Jiatao", ""], ["Li", "Victor O. K.", ""]]}, {"id": "1506.07503", "submitter": "Jan Chorowski", "authors": "Jan Chorowski, Dzmitry Bahdanau, Dmitriy Serdyuk, Kyunghyun Cho,\n  Yoshua Bengio", "title": "Attention-Based Models for Speech Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recurrent sequence generators conditioned on input data through an attention\nmechanism have recently shown very good performance on a range of tasks in-\ncluding machine translation, handwriting synthesis and image caption gen-\neration. We extend the attention-mechanism with features needed for speech\nrecognition. We show that while an adaptation of the model used for machine\ntranslation in reaches a competitive 18.7% phoneme error rate (PER) on the\nTIMIT phoneme recognition task, it can only be applied to utterances which are\nroughly as long as the ones it was trained on. We offer a qualitative\nexplanation of this failure and propose a novel and generic method of adding\nlocation-awareness to the attention mechanism to alleviate this issue. The new\nmethod yields a model that is robust to long inputs and achieves 18% PER in\nsingle utterances and 20% in 10-times longer (repeated) utterances. Finally, we\npropose a change to the at- tention mechanism that prevents it from\nconcentrating too much on single frames, which further reduces PER to 17.6%\nlevel.\n", "versions": [{"version": "v1", "created": "Wed, 24 Jun 2015 19:10:33 GMT"}], "update_date": "2015-06-25", "authors_parsed": [["Chorowski", "Jan", ""], ["Bahdanau", "Dzmitry", ""], ["Serdyuk", "Dmitriy", ""], ["Cho", "Kyunghyun", ""], ["Bengio", "Yoshua", ""]]}, {"id": "1506.07504", "submitter": "Maja Rudolph", "authors": "Maja R. Rudolph, Joseph G. Ellis, and David M. Blei", "title": "Objective Variables for Probabilistic Revenue Maximization in\n  Second-Price Auctions with Reserve", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.GT cs.LG stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many online companies sell advertisement space in second-price auctions with\nreserve. In this paper, we develop a probabilistic method to learn a profitable\nstrategy to set the reserve price. We use historical auction data with features\nto fit a predictor of the best reserve price. This problem is delicate - the\nstructure of the auction is such that a reserve price set too high is much\nworse than a reserve price set too low. To address this we develop objective\nvariables, a new framework for combining probabilistic modeling with optimal\ndecision-making. Objective variables are \"hallucinated observations\" that\ntransform the revenue maximization task into a regularized maximum likelihood\nestimation problem, which we solve with an EM algorithm. This framework enables\na variety of prediction mechanisms to set the reserve price. As examples, we\nstudy objective variable methods with regression, kernelized regression, and\nneural networks on simulated and real data. Our methods outperform previous\napproaches both in terms of scalability and profit.\n", "versions": [{"version": "v1", "created": "Wed, 24 Jun 2015 19:20:18 GMT"}], "update_date": "2015-06-25", "authors_parsed": [["Rudolph", "Maja R.", ""], ["Ellis", "Joseph G.", ""], ["Blei", "David M.", ""]]}, {"id": "1506.07512", "submitter": "Roy Frostig", "authors": "Roy Frostig, Rong Ge, Sham M. Kakade, Aaron Sidford", "title": "Un-regularizing: approximate proximal point and faster stochastic\n  algorithms for empirical risk minimization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.DS cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop a family of accelerated stochastic algorithms that minimize sums\nof convex functions. Our algorithms improve upon the fastest running time for\nempirical risk minimization (ERM), and in particular linear least-squares\nregression, across a wide range of problem settings. To achieve this, we\nestablish a framework based on the classical proximal point algorithm. Namely,\nwe provide several algorithms that reduce the minimization of a strongly convex\nfunction to approximate minimizations of regularizations of the function. Using\nthese results, we accelerate recent fast stochastic algorithms in a black-box\nfashion. Empirically, we demonstrate that the resulting algorithms exhibit\nnotions of stability that are advantageous in practice. Both in theory and in\npractice, the provided algorithms reap the computational benefits of adding a\nlarge strongly convex regularization term, without incurring a corresponding\nbias to the original problem.\n", "versions": [{"version": "v1", "created": "Wed, 24 Jun 2015 19:53:45 GMT"}], "update_date": "2015-06-25", "authors_parsed": [["Frostig", "Roy", ""], ["Ge", "Rong", ""], ["Kakade", "Sham M.", ""], ["Sidford", "Aaron", ""]]}, {"id": "1506.07540", "submitter": "Benjamin Haeffele", "authors": "Benjamin D. Haeffele and Rene Vidal", "title": "Global Optimality in Tensor Factorization, Deep Learning, and Beyond", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NA cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Techniques involving factorization are found in a wide range of applications\nand have enjoyed significant empirical success in many fields. However, common\nto a vast majority of these problems is the significant disadvantage that the\nassociated optimization problems are typically non-convex due to a multilinear\nform or other convexity destroying transformation. Here we build on ideas from\nconvex relaxations of matrix factorizations and present a very general\nframework which allows for the analysis of a wide range of non-convex\nfactorization problems - including matrix factorization, tensor factorization,\nand deep neural network training formulations. We derive sufficient conditions\nto guarantee that a local minimum of the non-convex optimization problem is a\nglobal minimum and show that if the size of the factorized variables is large\nenough then from any initialization it is possible to find a global minimizer\nusing a purely local descent algorithm. Our framework also provides a partial\ntheoretical justification for the increasingly common use of Rectified Linear\nUnits (ReLUs) in deep neural networks and offers guidance on deep network\narchitectures and regularization strategies to facilitate efficient\noptimization.\n", "versions": [{"version": "v1", "created": "Wed, 24 Jun 2015 20:08:47 GMT"}], "update_date": "2015-06-26", "authors_parsed": [["Haeffele", "Benjamin D.", ""], ["Vidal", "Rene", ""]]}, {"id": "1506.07552", "submitter": "Yuchen Zhang", "authors": "Yuchen Zhang and Michael I. Jordan", "title": "Splash: User-friendly Programming Interface for Parallelizing Stochastic\n  Algorithms", "comments": "redo experiments to learn bigger models; compare Splash with\n  state-of-the-art implementations on Spark", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Stochastic algorithms are efficient approaches to solving machine learning\nand optimization problems. In this paper, we propose a general framework called\nSplash for parallelizing stochastic algorithms on multi-node distributed\nsystems. Splash consists of a programming interface and an execution engine.\nUsing the programming interface, the user develops sequential stochastic\nalgorithms without concerning any detail about distributed computing. The\nalgorithm is then automatically parallelized by a communication-efficient\nexecution engine. We provide theoretical justifications on the optimal rate of\nconvergence for parallelizing stochastic gradient descent. Splash is built on\ntop of Apache Spark. The real-data experiments on logistic regression,\ncollaborative filtering and topic modeling verify that Splash yields\norder-of-magnitude speedup over single-thread stochastic algorithms and over\nstate-of-the-art implementations on Spark.\n", "versions": [{"version": "v1", "created": "Wed, 24 Jun 2015 20:39:54 GMT"}, {"version": "v2", "created": "Wed, 23 Sep 2015 01:11:22 GMT"}], "update_date": "2015-09-24", "authors_parsed": [["Zhang", "Yuchen", ""], ["Jordan", "Michael I.", ""]]}, {"id": "1506.07609", "submitter": "Vikas Garg", "authors": "Vikas K. Garg, Cynthia Rudin, and Tommi Jaakkola", "title": "CRAFT: ClusteR-specific Assorted Feature selecTion", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a framework for clustering with cluster-specific feature\nselection. The framework, CRAFT, is derived from asymptotic log posterior\nformulations of nonparametric MAP-based clustering models. CRAFT handles\nassorted data, i.e., both numeric and categorical data, and the underlying\nobjective functions are intuitively appealing. The resulting algorithm is\nsimple to implement and scales nicely, requires minimal parameter tuning,\nobviates the need to specify the number of clusters a priori, and compares\nfavorably with other methods on real datasets.\n", "versions": [{"version": "v1", "created": "Thu, 25 Jun 2015 04:14:49 GMT"}], "update_date": "2015-06-26", "authors_parsed": [["Garg", "Vikas K.", ""], ["Rudin", "Cynthia", ""], ["Jaakkola", "Tommi", ""]]}, {"id": "1506.07613", "submitter": "Sobhan Naderi Parizi", "authors": "Sobhan Naderi Parizi, Kun He, Reza Aghajani, Stan Sclaroff, Pedro\n  Felzenszwalb", "title": "Generalized Majorization-Minimization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.IT cs.LG math.IT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Non-convex optimization is ubiquitous in machine learning.\nMajorization-Minimization (MM) is a powerful iterative procedure for optimizing\nnon-convex functions that works by optimizing a sequence of bounds on the\nfunction. In MM, the bound at each iteration is required to \\emph{touch} the\nobjective function at the optimizer of the previous bound. We show that this\ntouching constraint is unnecessary and overly restrictive. We generalize MM by\nrelaxing this constraint, and propose a new optimization framework, named\nGeneralized Majorization-Minimization (G-MM), that is more flexible. For\ninstance, G-MM can incorporate application-specific biases into the\noptimization procedure without changing the objective function. We derive G-MM\nalgorithms for several latent variable models and show empirically that they\nconsistently outperform their MM counterparts in optimizing non-convex\nobjectives. In particular, G-MM algorithms appear to be less sensitive to\ninitialization.\n", "versions": [{"version": "v1", "created": "Thu, 25 Jun 2015 04:56:50 GMT"}, {"version": "v2", "created": "Thu, 28 Jul 2016 04:47:13 GMT"}, {"version": "v3", "created": "Fri, 17 May 2019 17:13:53 GMT"}], "update_date": "2019-05-20", "authors_parsed": [["Parizi", "Sobhan Naderi", ""], ["He", "Kun", ""], ["Aghajani", "Reza", ""], ["Sclaroff", "Stan", ""], ["Felzenszwalb", "Pedro", ""]]}, {"id": "1506.07615", "submitter": "Hongyang Zhang", "authors": "Hongyang Zhang, Zhouchen Lin, Chao Zhang", "title": "Completing Low-Rank Matrices with Corrupted Samples from Few\n  Coefficients in General Basis", "comments": "To appear in IEEE Transactions on Information Theory", "journal-ref": null, "doi": "10.1109/TIT.2016.2573311", "report-no": null, "categories": "cs.IT cs.LG cs.NA math.IT math.NA stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Subspace recovery from corrupted and missing data is crucial for various\napplications in signal processing and information theory. To complete missing\nvalues and detect column corruptions, existing robust Matrix Completion (MC)\nmethods mostly concentrate on recovering a low-rank matrix from few corrupted\ncoefficients w.r.t. standard basis, which, however, does not apply to more\ngeneral basis, e.g., Fourier basis. In this paper, we prove that the range\nspace of an $m\\times n$ matrix with rank $r$ can be exactly recovered from few\ncoefficients w.r.t. general basis, though $r$ and the number of corrupted\nsamples are both as high as $O(\\min\\{m,n\\}/\\log^3 (m+n))$. Our model covers\nprevious ones as special cases, and robust MC can recover the intrinsic matrix\nwith a higher rank. Moreover, we suggest a universal choice of the\nregularization parameter, which is $\\lambda=1/\\sqrt{\\log n}$. By our\n$\\ell_{2,1}$ filtering algorithm, which has theoretical guarantees, we can\nfurther reduce the computational cost of our model. As an application, we also\nfind that the solutions to extended robust Low-Rank Representation and to our\nextended robust MC are mutually expressible, so both our theory and algorithm\ncan be applied to the subspace clustering problem with missing values under\ncertain conditions. Experiments verify our theories.\n", "versions": [{"version": "v1", "created": "Thu, 25 Jun 2015 05:11:44 GMT"}, {"version": "v2", "created": "Mon, 23 May 2016 17:59:24 GMT"}], "update_date": "2016-11-18", "authors_parsed": [["Zhang", "Hongyang", ""], ["Lin", "Zhouchen", ""], ["Zhang", "Chao", ""]]}, {"id": "1506.07643", "submitter": "Daniel Jiwoong  Im", "authors": "Daniel Jiwoong Im, Mohamed Ishmael Diwan Belghazi, Roland Memisevic", "title": "Conservativeness of untied auto-encoders", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We discuss necessary and sufficient conditions for an auto-encoder to define\na conservative vector field, in which case it is associated with an energy\nfunction akin to the unnormalized log-probability of the data. We show that the\nconditions for conservativeness are more general than for encoder and decoder\nweights to be the same (\"tied weights\"), and that they also depend on the form\nof the hidden unit activation function, but that contractive training criteria,\nsuch as denoising, will enforce these conditions locally. Based on these\nobservations, we show how we can use auto-encoders to extract the conservative\ncomponent of a vector field.\n", "versions": [{"version": "v1", "created": "Thu, 25 Jun 2015 07:23:56 GMT"}, {"version": "v2", "created": "Tue, 30 Jun 2015 14:18:50 GMT"}, {"version": "v3", "created": "Mon, 21 Sep 2015 22:11:41 GMT"}], "update_date": "2015-09-23", "authors_parsed": [["Im", "Daniel Jiwoong", ""], ["Belghazi", "Mohamed Ishmael Diwan", ""], ["Memisevic", "Roland", ""]]}, {"id": "1506.07650", "submitter": "Kun Xu", "authors": "Kun Xu, Yansong Feng, Songfang Huang, Dongyan Zhao", "title": "Semantic Relation Classification via Convolutional Neural Networks with\n  Simple Negative Sampling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Syntactic features play an essential role in identifying relationship in a\nsentence. Previous neural network models often suffer from irrelevant\ninformation introduced when subjects and objects are in a long distance. In\nthis paper, we propose to learn more robust relation representations from the\nshortest dependency path through a convolution neural network. We further\npropose a straightforward negative sampling strategy to improve the assignment\nof subjects and objects. Experimental results show that our method outperforms\nthe state-of-the-art methods on the SemEval-2010 Task 8 dataset.\n", "versions": [{"version": "v1", "created": "Thu, 25 Jun 2015 07:51:55 GMT"}], "update_date": "2015-06-26", "authors_parsed": [["Xu", "Kun", ""], ["Feng", "Yansong", ""], ["Huang", "Songfang", ""], ["Zhao", "Dongyan", ""]]}, {"id": "1506.07677", "submitter": "Suvrit Sra", "authors": "Reshad Hosseini and Suvrit Sra", "title": "Manifold Optimization for Gaussian Mixture Models", "comments": "19 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We take a new look at parameter estimation for Gaussian Mixture Models\n(GMMs). In particular, we propose using \\emph{Riemannian manifold optimization}\nas a powerful counterpart to Expectation Maximization (EM). An out-of-the-box\ninvocation of manifold optimization, however, fails spectacularly: it converges\nto the same solution but vastly slower. Driven by intuition from manifold\nconvexity, we then propose a reparamerization that has remarkable empirical\nconsequences. It makes manifold optimization not only match EM---a highly\nencouraging result in itself given the poor record nonlinear programming\nmethods have had against EM so far---but also outperform EM in many practical\nsettings, while displaying much less variability in running times. We further\nhighlight the strengths of manifold optimization by developing a somewhat tuned\nmanifold LBFGS method that proves even more competitive and reliable than\nexisting manifold optimization tools. We hope that our results encourage a\nwider consideration of manifold optimization for parameter estimation problems.\n", "versions": [{"version": "v1", "created": "Thu, 25 Jun 2015 09:40:51 GMT"}], "update_date": "2015-06-26", "authors_parsed": [["Hosseini", "Reshad", ""], ["Sra", "Suvrit", ""]]}, {"id": "1506.07704", "submitter": "Donggeun Yoo", "authors": "Donggeun Yoo, Sunggyun Park, Joon-Young Lee, Anthony S. Paek, In So\n  Kweon", "title": "AttentionNet: Aggregating Weak Directions for Accurate Object Detection", "comments": "To appear in ICCV 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a novel detection method using a deep convolutional neural network\n(CNN), named AttentionNet. We cast an object detection problem as an iterative\nclassification problem, which is the most suitable form of a CNN. AttentionNet\nprovides quantized weak directions pointing a target object and the ensemble of\niterative predictions from AttentionNet converges to an accurate object\nboundary box. Since AttentionNet is a unified network for object detection, it\ndetects objects without any separated models from the object proposal to the\npost bounding-box regression. We evaluate AttentionNet by a human detection\ntask and achieve the state-of-the-art performance of 65% (AP) on PASCAL VOC\n2007/2012 with an 8-layered architecture only.\n", "versions": [{"version": "v1", "created": "Thu, 25 Jun 2015 11:21:04 GMT"}, {"version": "v2", "created": "Sat, 26 Sep 2015 08:35:51 GMT"}], "update_date": "2015-09-29", "authors_parsed": [["Yoo", "Donggeun", ""], ["Park", "Sunggyun", ""], ["Lee", "Joon-Young", ""], ["Paek", "Anthony S.", ""], ["Kweon", "In So", ""]]}, {"id": "1506.07721", "submitter": "Kazuto Fukuchi", "authors": "Kazuto Fukuchi and Jun Sakuma", "title": "Fairness-Aware Learning with Restriction of Universal Dependency using\n  f-Divergences", "comments": "15 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fairness-aware learning is a novel framework for classification tasks. Like\nregular empirical risk minimization (ERM), it aims to learn a classifier with a\nlow error rate, and at the same time, for the predictions of the classifier to\nbe independent of sensitive features, such as gender, religion, race, and\nethnicity. Existing methods can achieve low dependencies on given samples, but\nthis is not guaranteed on unseen samples. The existing fairness-aware learning\nalgorithms employ different dependency measures, and each algorithm is\nspecifically designed for a particular one. Such diversity makes it difficult\nto theoretically analyze and compare them. In this paper, we propose a general\nframework for fairness-aware learning that uses f-divergences and that covers\nmost of the dependency measures employed in the existing methods. We introduce\na way to estimate the f-divergences that allows us to give a unified analysis\nfor the upper bound of the estimation error; this bound is tighter than that of\nthe existing convergence rate analysis of the divergence estimation. With our\ndivergence estimate, we propose a fairness-aware learning algorithm, and\nperform a theoretical analysis of its generalization error. Our analysis\nreveals that, under mild assumptions and even with enforcement of fairness, the\ngeneralization error of our method is $O(\\sqrt{1/n})$, which is the same as\nthat of the regular ERM. In addition, and more importantly, we show that, for\nany f-divergence, the upper bound of the estimation error of the divergence is\n$O(\\sqrt{1/n})$. This indicates that our fairness-aware learning algorithm\nguarantees low dependencies on unseen samples for any dependency measure\nrepresented by an f-divergence.\n", "versions": [{"version": "v1", "created": "Thu, 25 Jun 2015 12:24:43 GMT"}], "update_date": "2015-06-26", "authors_parsed": [["Fukuchi", "Kazuto", ""], ["Sakuma", "Jun", ""]]}, {"id": "1506.07840", "submitter": "Gal Mishne", "authors": "Gal Mishne, Uri Shaham, Alexander Cloninger and Israel Cohen", "title": "Diffusion Nets", "comments": "24 pages, 12 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.CA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Non-linear manifold learning enables high-dimensional data analysis, but\nrequires out-of-sample-extension methods to process new data points. In this\npaper, we propose a manifold learning algorithm based on deep learning to\ncreate an encoder, which maps a high-dimensional dataset and its\nlow-dimensional embedding, and a decoder, which takes the embedded data back to\nthe high-dimensional space. Stacking the encoder and decoder together\nconstructs an autoencoder, which we term a diffusion net, that performs\nout-of-sample-extension as well as outlier detection. We introduce new neural\nnet constraints for the encoder, which preserves the local geometry of the\npoints, and we prove rates of convergence for the encoder. Also, our approach\nis efficient in both computational complexity and memory requirements, as\nopposed to previous methods that require storage of all training points in both\nthe high-dimensional and the low-dimensional spaces to calculate the\nout-of-sample-extension and the pre-image.\n", "versions": [{"version": "v1", "created": "Thu, 25 Jun 2015 18:13:49 GMT"}], "update_date": "2015-06-26", "authors_parsed": [["Mishne", "Gal", ""], ["Shaham", "Uri", ""], ["Cloninger", "Alexander", ""], ["Cohen", "Israel", ""]]}, {"id": "1506.07924", "submitter": "Serdar Y\\\"uksel", "authors": "G\\\"urdal Arslan and Serdar Y\\\"uksel", "title": "Decentralized Q-Learning for Stochastic Teams and Games", "comments": "To appear in IEEE Trans. Automatic Control", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.GT cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There are only a few learning algorithms applicable to stochastic dynamic\nteams and games which generalize Markov decision processes to decentralized\nstochastic control problems involving possibly self-interested decision makers.\nLearning in games is generally difficult because of the non-stationary\nenvironment in which each decision maker aims to learn its optimal decisions\nwith minimal information in the presence of the other decision makers who are\nalso learning. In stochastic dynamic games, learning is more challenging\nbecause, while learning, the decision makers alter the state of the system and\nhence the future cost. In this paper, we present decentralized Q-learning\nalgorithms for stochastic games, and study their convergence for the weakly\nacyclic case which includes team problems as an important special case. The\nalgorithm is decentralized in that each decision maker has access to only its\nlocal information, the state information, and the local cost realizations;\nfurthermore, it is completely oblivious to the presence of other decision\nmakers. We show that these algorithms converge to equilibrium policies almost\nsurely in large classes of stochastic games.\n", "versions": [{"version": "v1", "created": "Thu, 25 Jun 2015 23:49:01 GMT"}, {"version": "v2", "created": "Mon, 2 May 2016 18:20:31 GMT"}], "update_date": "2016-05-03", "authors_parsed": [["Arslan", "G\u00fcrdal", ""], ["Y\u00fcksel", "Serdar", ""]]}, {"id": "1506.07947", "submitter": "Sewoong Oh", "authors": "Sewoong Oh, Kiran K. Thekumparampil, and Jiaming Xu", "title": "Collaboratively Learning Preferences from Ordinal Data", "comments": "38 pages 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.IT math.IT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In applications such as recommendation systems and revenue management, it is\nimportant to predict preferences on items that have not been seen by a user or\npredict outcomes of comparisons among those that have never been compared. A\npopular discrete choice model of multinomial logit model captures the structure\nof the hidden preferences with a low-rank matrix. In order to predict the\npreferences, we want to learn the underlying model from noisy observations of\nthe low-rank matrix, collected as revealed preferences in various forms of\nordinal data. A natural approach to learn such a model is to solve a convex\nrelaxation of nuclear norm minimization. We present the convex relaxation\napproach in two contexts of interest: collaborative ranking and bundled choice\nmodeling. In both cases, we show that the convex relaxation is minimax optimal.\nWe prove an upper bound on the resulting error with finite samples, and provide\na matching information-theoretic lower bound.\n", "versions": [{"version": "v1", "created": "Fri, 26 Jun 2015 03:06:27 GMT"}], "update_date": "2015-06-29", "authors_parsed": [["Oh", "Sewoong", ""], ["Thekumparampil", "Kiran K.", ""], ["Xu", "Jiaming", ""]]}, {"id": "1506.08009", "submitter": "Francois Petitjean Ph.D.", "authors": "Francois Petitjean, Tao Li, Nikolaj Tatti, Geoffrey I. Webb", "title": "Skopus: Mining top-k sequential patterns under leverage", "comments": null, "journal-ref": "Data Mining and Knowledge Discovery, September 2016, Volume 30,\n  Issue 5, pp 1086-1111", "doi": "10.1007/s10618-016-0467-9", "report-no": null, "categories": "cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a framework for exact discovery of the top-k sequential\npatterns under Leverage. It combines (1) a novel definition of the expected\nsupport for a sequential pattern - a concept on which most interestingness\nmeasures directly rely - with (2) SkOPUS: a new branch-and-bound algorithm for\nthe exact discovery of top-k sequential patterns under a given measure of\ninterest. Our interestingness measure employs the partition approach. A pattern\nis interesting to the extent that it is more frequent than can be explained by\nassuming independence between any of the pairs of patterns from which it can be\ncomposed. The larger the support compared to the expectation under\nindependence, the more interesting is the pattern. We build on these two\nelements to exactly extract the k sequential patterns with highest leverage,\nconsistent with our definition of expected support. We conduct experiments on\nboth synthetic data with known patterns and real-world datasets; both\nexperiments confirm the consistency and relevance of our approach with regard\nto the state of the art. This article was published in Data Mining and\nKnowledge Discovery and is accessible at\nhttp://dx.doi.org/10.1007/s10618-016-0467-9.\n", "versions": [{"version": "v1", "created": "Fri, 26 Jun 2015 09:36:10 GMT"}, {"version": "v2", "created": "Wed, 6 Jan 2016 04:48:08 GMT"}, {"version": "v3", "created": "Mon, 8 May 2017 23:59:16 GMT"}, {"version": "v4", "created": "Mon, 5 Feb 2018 01:26:34 GMT"}], "update_date": "2018-02-06", "authors_parsed": [["Petitjean", "Francois", ""], ["Li", "Tao", ""], ["Tatti", "Nikolaj", ""], ["Webb", "Geoffrey I.", ""]]}, {"id": "1506.08105", "submitter": "Parthan Kasarapu Mr", "authors": "Parthan Kasarapu", "title": "Modelling of directional data using Kent distributions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The modelling of data on a spherical surface requires the consideration of\ndirectional probability distributions. To model asymmetrically distributed data\non a three-dimensional sphere, Kent distributions are often used. The moment\nestimates of the parameters are typically used in modelling tasks involving\nKent distributions. However, these lack a rigorous statistical treatment. The\nfocus of the paper is to introduce a Bayesian estimation of the parameters of\nthe Kent distribution which has not been carried out in the literature, partly\nbecause of its complex mathematical form. We employ the Bayesian\ninformation-theoretic paradigm of Minimum Message Length (MML) to bridge this\ngap and derive reliable estimators. The inferred parameters are subsequently\nused in mixture modelling of Kent distributions. The problem of inferring the\nsuitable number of mixture components is also addressed using the MML\ncriterion. We demonstrate the superior performance of the derived MML-based\nparameter estimates against the traditional estimators. We apply the MML\nprinciple to infer mixtures of Kent distributions to model empirical data\ncorresponding to protein conformations. We demonstrate the effectiveness of\nKent models to act as improved descriptors of protein structural data as\ncompared to commonly used von Mises-Fisher distributions.\n", "versions": [{"version": "v1", "created": "Fri, 26 Jun 2015 15:03:33 GMT"}], "update_date": "2015-06-29", "authors_parsed": [["Kasarapu", "Parthan", ""]]}, {"id": "1506.08180", "submitter": "Amar Shah", "authors": "Amar Shah and David A. Knowles and Zoubin Ghahramani", "title": "An Empirical Study of Stochastic Variational Algorithms for the Beta\n  Bernoulli Process", "comments": "ICML, 12 pages. Volume 37: Proceedings of The 32nd International\n  Conference on Machine Learning, 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.AP stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Stochastic variational inference (SVI) is emerging as the most promising\ncandidate for scaling inference in Bayesian probabilistic models to large\ndatasets. However, the performance of these methods has been assessed primarily\nin the context of Bayesian topic models, particularly latent Dirichlet\nallocation (LDA). Deriving several new algorithms, and using synthetic, image\nand genomic datasets, we investigate whether the understanding gleaned from LDA\napplies in the setting of sparse latent factor models, specifically beta\nprocess factor analysis (BPFA). We demonstrate that the big picture is\nconsistent: using Gibbs sampling within SVI to maintain certain posterior\ndependencies is extremely effective. However, we find that different posterior\ndependencies are important in BPFA relative to LDA. Particularly,\napproximations able to model intra-local variable dependence perform best.\n", "versions": [{"version": "v1", "created": "Fri, 26 Jun 2015 18:55:11 GMT"}], "update_date": "2015-06-29", "authors_parsed": [["Shah", "Amar", ""], ["Knowles", "David A.", ""], ["Ghahramani", "Zoubin", ""]]}, {"id": "1506.08187", "submitter": "Yin Tat Lee", "authors": "S\\'ebastien Bubeck, Yin Tat Lee, Mohit Singh", "title": "A geometric alternative to Nesterov's accelerated gradient descent", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.DS cs.LG cs.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new method for unconstrained optimization of a smooth and\nstrongly convex function, which attains the optimal rate of convergence of\nNesterov's accelerated gradient descent. The new algorithm has a simple\ngeometric interpretation, loosely inspired by the ellipsoid method. We provide\nsome numerical evidence that the new method can be superior to Nesterov's\naccelerated gradient descent.\n", "versions": [{"version": "v1", "created": "Fri, 26 Jun 2015 19:39:50 GMT"}], "update_date": "2015-06-30", "authors_parsed": [["Bubeck", "S\u00e9bastien", ""], ["Lee", "Yin Tat", ""], ["Singh", "Mohit", ""]]}, {"id": "1506.08189", "submitter": "Gregory Puleo", "authors": "Gregory J. Puleo, Olgica Milenkovic", "title": "Correlation Clustering and Biclustering with Locally Bounded Errors", "comments": "20 pages, reorganized paper to emphasize the key properties of the\n  rounding algorithm and the broader class of possible objective functions", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider a generalized version of the correlation clustering problem,\ndefined as follows. Given a complete graph $G$ whose edges are labeled with $+$\nor $-$, we wish to partition the graph into clusters while trying to avoid\nerrors: $+$ edges between clusters or $-$ edges within clusters. Classically,\none seeks to minimize the total number of such errors. We introduce a new\nframework that allows the objective to be a more general function of the number\nof errors at each vertex (for example, we may wish to minimize the number of\nerrors at the worst vertex) and provide a rounding algorithm which converts\n\"fractional clusterings\" into discrete clusterings while causing only a\nconstant-factor blowup in the number of errors at each vertex. This rounding\nalgorithm yields constant-factor approximation algorithms for the discrete\nproblem under a wide variety of objective functions.\n", "versions": [{"version": "v1", "created": "Fri, 26 Jun 2015 19:46:41 GMT"}, {"version": "v2", "created": "Wed, 9 Dec 2015 22:39:19 GMT"}, {"version": "v3", "created": "Tue, 24 May 2016 19:29:33 GMT"}], "update_date": "2016-05-25", "authors_parsed": [["Puleo", "Gregory J.", ""], ["Milenkovic", "Olgica", ""]]}, {"id": "1506.08230", "submitter": "Mark Tygert", "authors": "Mark Tygert, Arthur Szlam, Soumith Chintala, Marc'Aurelio Ranzato,\n  Yuandong Tian, and Wojciech Zaremba", "title": "Convolutional networks and learning invariant to homogeneous\n  multiplicative scalings", "comments": "12 pages, 6 figures, 4 tables", "journal-ref": "Appl. Comput. Harmon. Anal., 42 (1): 154-166, 2017", "doi": null, "report-no": null, "categories": "cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The conventional classification schemes -- notably multinomial logistic\nregression -- used in conjunction with convolutional networks (convnets) are\nclassical in statistics, designed without consideration for the usual coupling\nwith convnets, stochastic gradient descent, and backpropagation. In the\nspecific application to supervised learning for convnets, a simple\nscale-invariant classification stage turns out to be more robust than\nmultinomial logistic regression, appears to result in slightly lower errors on\nseveral standard test sets, has similar computational costs, and features\nprecise control over the actual rate of learning. \"Scale-invariant\" means that\nmultiplying the input values by any nonzero scalar leaves the output unchanged.\n", "versions": [{"version": "v1", "created": "Fri, 26 Jun 2015 22:22:21 GMT"}, {"version": "v2", "created": "Sun, 13 Sep 2015 18:10:04 GMT"}, {"version": "v3", "created": "Thu, 24 Dec 2015 20:20:48 GMT"}, {"version": "v4", "created": "Tue, 16 Feb 2016 22:55:43 GMT"}], "update_date": "2017-01-02", "authors_parsed": [["Tygert", "Mark", ""], ["Szlam", "Arthur", ""], ["Chintala", "Soumith", ""], ["Ranzato", "Marc'Aurelio", ""], ["Tian", "Yuandong", ""], ["Zaremba", "Wojciech", ""]]}, {"id": "1506.08251", "submitter": "Szymon Jozef Sidor", "authors": "Jonathan Raiman and Szymon Sidor", "title": "Occam's Gates", "comments": "In review at NIPS", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We present a complimentary objective for training recurrent neural networks\n(RNN) with gating units that helps with regularization and interpretability of\nthe trained model. Attention-based RNN models have shown success in many\ndifficult sequence to sequence classification problems with long and short term\ndependencies, however these models are prone to overfitting. In this paper, we\ndescribe how to regularize these models through an L1 penalty on the activation\nof the gating units, and show that this technique reduces overfitting on a\nvariety of tasks while also providing to us a human-interpretable visualization\nof the inputs used by the network. These tasks include sentiment analysis,\nparaphrase recognition, and question answering.\n", "versions": [{"version": "v1", "created": "Sat, 27 Jun 2015 03:03:10 GMT"}], "update_date": "2015-06-30", "authors_parsed": [["Raiman", "Jonathan", ""], ["Sidor", "Szymon", ""]]}, {"id": "1506.08301", "submitter": "Yilun Wang", "authors": "Yilun Wang, Zhiqiang Li, Yifeng Wang, Xiaona Wang, Junjie Zheng,\n  Xujuan Duan, Huafu Chen", "title": "A Novel Approach for Stable Selection of Informative Redundant Features\n  from High Dimensional fMRI Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Feature selection is among the most important components because it not only\nhelps enhance the classification accuracy, but also or even more important\nprovides potential biomarker discovery. However, traditional multivariate\nmethods is likely to obtain unstable and unreliable results in case of an\nextremely high dimensional feature space and very limited training samples,\nwhere the features are often correlated or redundant. In order to improve the\nstability, generalization and interpretations of the discovered potential\nbiomarker and enhance the robustness of the resultant classifier, the redundant\nbut informative features need to be also selected. Therefore we introduced a\nnovel feature selection method which combines a recent implementation of the\nstability selection approach and the elastic net approach. The advantage in\nterms of better control of false discoveries and missed discoveries of our\napproach, and the resulted better interpretability of the obtained potential\nbiomarker is verified in both synthetic and real fMRI experiments. In addition,\nwe are among the first to demonstrate the robustness of feature selection\nbenefiting from the incorporation of stability selection and also among the\nfirst to demonstrate the possible unrobustness of the classical univariate\ntwo-sample t-test method. Specifically, we show the robustness of our feature\nselection results in existence of noisy (wrong) training labels, as well as the\nrobustness of the resulted classifier based on our feature selection results in\nthe existence of data variation, demonstrated by a multi-center\nattention-deficit/hyperactivity disorder (ADHD) fMRI data.\n", "versions": [{"version": "v1", "created": "Sat, 27 Jun 2015 15:28:31 GMT"}, {"version": "v2", "created": "Wed, 25 May 2016 02:37:39 GMT"}], "update_date": "2016-05-26", "authors_parsed": [["Wang", "Yilun", ""], ["Li", "Zhiqiang", ""], ["Wang", "Yifeng", ""], ["Wang", "Xiaona", ""], ["Zheng", "Junjie", ""], ["Duan", "Xujuan", ""], ["Chen", "Huafu", ""]]}, {"id": "1506.08349", "submitter": "Lantian Li Mr.", "authors": "Lantian Li and Yiye Lin and Zhiyong Zhang and Dong Wang", "title": "Improved Deep Speaker Feature Learning for Text-Dependent Speaker\n  Recognition", "comments": "arXiv admin note: substantial text overlap with arXiv:1505.06427", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A deep learning approach has been proposed recently to derive speaker\nidentifies (d-vector) by a deep neural network (DNN). This approach has been\napplied to text-dependent speaker recognition tasks and shows reasonable\nperformance gains when combined with the conventional i-vector approach.\nAlthough promising, the existing d-vector implementation still can not compete\nwith the i-vector baseline. This paper presents two improvements for the deep\nlearning approach: a phonedependent DNN structure to normalize phone variation,\nand a new scoring approach based on dynamic time warping (DTW). Experiments on\na text-dependent speaker recognition task demonstrated that the proposed\nmethods can provide considerable performance improvement over the existing\nd-vector implementation.\n", "versions": [{"version": "v1", "created": "Sun, 28 Jun 2015 03:32:02 GMT"}], "update_date": "2015-06-30", "authors_parsed": [["Li", "Lantian", ""], ["Lin", "Yiye", ""], ["Zhang", "Zhiyong", ""], ["Wang", "Dong", ""]]}, {"id": "1506.08350", "submitter": "Yadong Mu", "authors": "Yadong Mu and Wei Liu and Wei Fan", "title": "Stochastic Gradient Made Stable: A Manifold Propagation Approach for\n  Large-Scale Optimization", "comments": "14 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Stochastic gradient descent (SGD) holds as a classical method to build large\nscale machine learning models over big data. A stochastic gradient is typically\ncalculated from a limited number of samples (known as mini-batch), so it\npotentially incurs a high variance and causes the estimated parameters bounce\naround the optimal solution. To improve the stability of stochastic gradient,\nrecent years have witnessed the proposal of several semi-stochastic gradient\ndescent algorithms, which distinguish themselves from standard SGD by\nincorporating global information into gradient computation. In this paper we\ncontribute a novel stratified semi-stochastic gradient descent (S3GD) algorithm\nto this nascent research area, accelerating the optimization of a large family\nof composite convex functions. Though theoretically converging faster, prior\nsemi-stochastic algorithms are found to suffer from high iteration complexity,\nwhich makes them even slower than SGD in practice on many datasets. In our\nproposed S3GD, the semi-stochastic gradient is calculated based on efficient\nmanifold propagation, which can be numerically accomplished by sparse matrix\nmultiplications. This way S3GD is able to generate a highly-accurate estimate\nof the exact gradient from each mini-batch with largely-reduced computational\ncomplexity. Theoretic analysis reveals that the proposed S3GD elegantly\nbalances the geometric algorithmic convergence rate against the space and time\ncomplexities during the optimization. The efficacy of S3GD is also\nexperimentally corroborated on several large-scale benchmark datasets.\n", "versions": [{"version": "v1", "created": "Sun, 28 Jun 2015 03:33:38 GMT"}, {"version": "v2", "created": "Tue, 12 Jan 2016 21:30:08 GMT"}], "update_date": "2016-01-14", "authors_parsed": [["Mu", "Yadong", ""], ["Liu", "Wei", ""], ["Fan", "Wei", ""]]}, {"id": "1506.08422", "submitter": "Li-Qiang Niu", "authors": "Li-Qiang Niu and Xin-Yu Dai", "title": "Topic2Vec: Learning Distributed Representations of Topics", "comments": "6 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Latent Dirichlet Allocation (LDA) mining thematic structure of documents\nplays an important role in nature language processing and machine learning\nareas. However, the probability distribution from LDA only describes the\nstatistical relationship of occurrences in the corpus and usually in practice,\nprobability is not the best choice for feature representations. Recently,\nembedding methods have been proposed to represent words and documents by\nlearning essential concepts and representations, such as Word2Vec and Doc2Vec.\nThe embedded representations have shown more effectiveness than LDA-style\nrepresentations in many tasks. In this paper, we propose the Topic2Vec approach\nwhich can learn topic representations in the same semantic vector space with\nwords, as an alternative to probability. The experimental results show that\nTopic2Vec achieves interesting and meaningful results.\n", "versions": [{"version": "v1", "created": "Sun, 28 Jun 2015 16:17:40 GMT"}], "update_date": "2015-06-30", "authors_parsed": [["Niu", "Li-Qiang", ""], ["Dai", "Xin-Yu", ""]]}, {"id": "1506.08448", "submitter": "Dennis Forster", "authors": "Dennis Forster, Abdul-Saboor Sheikh and J\\\"org L\\\"ucke", "title": "Neural Simpletrons - Minimalistic Directed Generative Networks for\n  Learning with Few Labels", "comments": null, "journal-ref": "Neural Computation Volume 30, Issue 8, August 2018, p.2113-2174", "doi": "10.1162/neco_a_01100", "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Classifiers for the semi-supervised setting often combine strong supervised\nmodels with additional learning objectives to make use of unlabeled data. This\nresults in powerful though very complex models that are hard to train and that\ndemand additional labels for optimal parameter tuning, which are often not\ngiven when labeled data is very sparse. We here study a minimalistic\nmulti-layer generative neural network for semi-supervised learning in a form\nand setting as similar to standard discriminative networks as possible. Based\non normalized Poisson mixtures, we derive compact and local learning and neural\nactivation rules. Learning and inference in the network can be scaled using\nstandard deep learning tools for parallelized GPU implementation. With the\nsingle objective of likelihood optimization, both labeled and unlabeled data\nare naturally incorporated into learning. Empirical evaluations on standard\nbenchmarks show, that for datasets with few labels the derived minimalistic\nnetwork improves on all classical deep learning approaches and is competitive\nwith their recent variants without the need of additional labels for parameter\ntuning. Furthermore, we find that the studied network is the best performing\nmonolithic (`non-hybrid') system for few labels, and that it can be applied in\nthe limit of very few labels, where no other system has been reported to\noperate so far.\n", "versions": [{"version": "v1", "created": "Sun, 28 Jun 2015 20:25:15 GMT"}, {"version": "v2", "created": "Fri, 23 Oct 2015 12:35:00 GMT"}, {"version": "v3", "created": "Tue, 3 May 2016 13:52:20 GMT"}, {"version": "v4", "created": "Fri, 18 Nov 2016 15:08:51 GMT"}], "update_date": "2019-05-01", "authors_parsed": [["Forster", "Dennis", ""], ["Sheikh", "Abdul-Saboor", ""], ["L\u00fccke", "J\u00f6rg", ""]]}, {"id": "1506.08473", "submitter": "Majid Janzamin", "authors": "Majid Janzamin and Hanie Sedghi and Anima Anandkumar", "title": "Beating the Perils of Non-Convexity: Guaranteed Training of Neural\n  Networks using Tensor Methods", "comments": "The tensor decomposition analysis is expanded, and the analysis of\n  ridge regression is added for recovering the parameters of last layer of\n  neural network", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Training neural networks is a challenging non-convex optimization problem,\nand backpropagation or gradient descent can get stuck in spurious local optima.\nWe propose a novel algorithm based on tensor decomposition for guaranteed\ntraining of two-layer neural networks. We provide risk bounds for our proposed\nmethod, with a polynomial sample complexity in the relevant parameters, such as\ninput dimension and number of neurons. While learning arbitrary target\nfunctions is NP-hard, we provide transparent conditions on the function and the\ninput for learnability. Our training method is based on tensor decomposition,\nwhich provably converges to the global optimum, under a set of mild\nnon-degeneracy conditions. It consists of simple embarrassingly parallel linear\nand multi-linear operations, and is competitive with standard stochastic\ngradient descent (SGD), in terms of computational complexity. Thus, we propose\na computationally efficient method with guaranteed risk bounds for training\nneural networks with one hidden layer.\n", "versions": [{"version": "v1", "created": "Sun, 28 Jun 2015 23:19:49 GMT"}, {"version": "v2", "created": "Mon, 24 Aug 2015 19:31:24 GMT"}, {"version": "v3", "created": "Tue, 12 Jan 2016 03:19:42 GMT"}], "update_date": "2016-01-13", "authors_parsed": [["Janzamin", "Majid", ""], ["Sedghi", "Hanie", ""], ["Anandkumar", "Anima", ""]]}, {"id": "1506.08536", "submitter": "Luca Citi", "authors": "Luca Citi", "title": "A simple yet efficient algorithm for multiple kernel learning under\n  elastic-net constraints", "comments": "18 pages, one figure, updated version of the paper draft", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This papers introduces an algorithm for the solution of multiple kernel\nlearning (MKL) problems with elastic-net constraints on the kernel weights. The\nalgorithm compares very favourably in terms of time and space complexity to\nexisting approaches and can be implemented with simple code that does not rely\non external libraries (except a conventional SVM solver).\n", "versions": [{"version": "v1", "created": "Mon, 29 Jun 2015 08:11:52 GMT"}, {"version": "v2", "created": "Wed, 13 Jan 2016 23:01:42 GMT"}, {"version": "v3", "created": "Fri, 5 Apr 2019 09:37:11 GMT"}], "update_date": "2019-04-08", "authors_parsed": [["Citi", "Luca", ""]]}, {"id": "1506.08544", "submitter": "Stephane Robin", "authors": "Nathalie Peyrard and Marie-Jos\\'ee Cros and Simon de Givry and Alain\n  Franc and St\\'ephane Robin and R\\'egis Sabbadin and Thomas Schiex and\n  Matthieu Vignes", "title": "Exact and approximate inference in graphical models: variable\n  elimination and beyond", "comments": "47 pages, 3 tables, 12 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Probabilistic graphical models offer a powerful framework to account for the\ndependence structure between variables, which is represented as a graph.\nHowever, the dependence between variables may render inference tasks\nintractable. In this paper we review techniques exploiting the graph structure\nfor exact inference, borrowed from optimisation and computer science. They are\nbuilt on the principle of variable elimination whose complexity is dictated in\nan intricate way by the order in which variables are eliminated. The so-called\ntreewidth of the graph characterises this algorithmic complexity: low-treewidth\ngraphs can be processed efficiently. The first message that we illustrate is\ntherefore the idea that for inference in graphical model, the number of\nvariables is not the limiting factor, and it is worth checking for the\ntreewidth before turning to approximate methods. We show how algorithms\nproviding an upper bound of the treewidth can be exploited to derive a 'good'\nelimination order enabling to perform exact inference. The second message is\nthat when the treewidth is too large, algorithms for approximate inference\nlinked to the principle of variable elimination, such as loopy belief\npropagation and variational approaches, can lead to accurate results while\nbeing much less time consuming than Monte-Carlo approaches. We illustrate the\ntechniques reviewed in this article on benchmarks of inference problems in\ngenetic linkage analysis and computer vision, as well as on hidden variables\nrestoration in coupled Hidden Markov Models.\n", "versions": [{"version": "v1", "created": "Mon, 29 Jun 2015 08:45:11 GMT"}, {"version": "v2", "created": "Mon, 12 Mar 2018 07:45:08 GMT"}], "update_date": "2018-03-13", "authors_parsed": [["Peyrard", "Nathalie", ""], ["Cros", "Marie-Jos\u00e9e", ""], ["de Givry", "Simon", ""], ["Franc", "Alain", ""], ["Robin", "St\u00e9phane", ""], ["Sabbadin", "R\u00e9gis", ""], ["Schiex", "Thomas", ""], ["Vignes", "Matthieu", ""]]}, {"id": "1506.08581", "submitter": "Konstantinos Makantasis", "authors": "Konstantinos Makantasis, Anastasios Doulamis, Nikolaos Doulamis", "title": "Variational Inference for Background Subtraction in Infrared Imagery", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a Gaussian mixture model for background subtraction in infrared\nimagery. Following a Bayesian approach, our method automatically estimates the\nnumber of Gaussian components as well as their parameters, while simultaneously\nit avoids over/under fitting. The equations for estimating model parameters are\nanalytically derived and thus our method does not require any sampling\nalgorithm that is computationally and memory inefficient. The pixel density\nestimate is followed by an efficient and highly accurate updating mechanism,\nwhich permits our system to be automatically adapted to dynamically changing\noperation conditions. Experimental results and comparisons with other methods\nshow that our method outperforms, in terms of precision and recall, while at\nthe same time it keeps computational cost suitable for real-time applications.\n", "versions": [{"version": "v1", "created": "Mon, 29 Jun 2015 11:16:41 GMT"}], "update_date": "2015-06-30", "authors_parsed": [["Makantasis", "Konstantinos", ""], ["Doulamis", "Anastasios", ""], ["Doulamis", "Nikolaos", ""]]}, {"id": "1506.08621", "submitter": "Lennart Gulikers", "authors": "Lennart Gulikers, Marc Lelarge, Laurent Massouli\\'e", "title": "A spectral method for community detection in moderately-sparse\n  degree-corrected stochastic block models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR cs.LG cs.SI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider community detection in Degree-Corrected Stochastic Block Models\n(DC-SBM). We propose a spectral clustering algorithm based on a suitably\nnormalized adjacency matrix. We show that this algorithm consistently recovers\nthe block-membership of all but a vanishing fraction of nodes, in the regime\nwhere the lowest degree is of order log$(n)$ or higher. Recovery succeeds even\nfor very heterogeneous degree-distributions. The used algorithm does not rely\non parameters as input. In particular, it does not need to know the number of\ncommunities.\n", "versions": [{"version": "v1", "created": "Mon, 29 Jun 2015 13:44:54 GMT"}, {"version": "v2", "created": "Wed, 13 Apr 2016 14:50:53 GMT"}, {"version": "v3", "created": "Tue, 7 Feb 2017 12:31:41 GMT"}], "update_date": "2017-02-09", "authors_parsed": [["Gulikers", "Lennart", ""], ["Lelarge", "Marc", ""], ["Massouli\u00e9", "Laurent", ""]]}, {"id": "1506.08669", "submitter": "Tzu-Kuo Huang", "authors": "Tzu-Kuo Huang, Alekh Agarwal, Daniel J. Hsu, John Langford, Robert E.\n  Schapire", "title": "Efficient and Parsimonious Agnostic Active Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop a new active learning algorithm for the streaming setting\nsatisfying three important properties: 1) It provably works for any classifier\nrepresentation and classification problem including those with severe noise. 2)\nIt is efficiently implementable with an ERM oracle. 3) It is more aggressive\nthan all previous approaches satisfying 1 and 2. To do this we create an\nalgorithm based on a newly defined optimization problem and analyze it. We also\nconduct the first experimental analysis of all efficient agnostic active\nlearning algorithms, evaluating their strengths and weaknesses in different\nsettings.\n", "versions": [{"version": "v1", "created": "Mon, 29 Jun 2015 15:02:55 GMT"}, {"version": "v2", "created": "Wed, 6 Jan 2016 02:49:21 GMT"}, {"version": "v3", "created": "Thu, 7 Jan 2016 18:27:33 GMT"}], "update_date": "2016-01-08", "authors_parsed": [["Huang", "Tzu-Kuo", ""], ["Agarwal", "Alekh", ""], ["Hsu", "Daniel J.", ""], ["Langford", "John", ""], ["Schapire", "Robert E.", ""]]}, {"id": "1506.08690", "submitter": "Gergo Barta", "authors": "Gabor Nagy and Gergo Barta and Tamas Henk", "title": "Portfolio optimization using local linear regression ensembles in\n  RapidMiner", "comments": "RCOMM 2012: Rapidminer Community Meeting and Conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.PM cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we implement a Local Linear Regression Ensemble Committee\n(LOLREC) to predict 1-day-ahead returns of 453 assets form the S&P500. The\nestimates and the historical returns of the committees are used to compute the\nweights of the portfolio from the 453 stock. The proposed method outperforms\nbenchmark portfolio selection strategies that optimize the growth rate of the\ncapital. We investigate the effect of algorithm parameter m: the number of\nselected stocks on achieved average annual yields. Results suggest the\nalgorithm's practical usefulness in everyday trading.\n", "versions": [{"version": "v1", "created": "Mon, 29 Jun 2015 15:42:39 GMT"}], "update_date": "2015-06-30", "authors_parsed": [["Nagy", "Gabor", ""], ["Barta", "Gergo", ""], ["Henk", "Tamas", ""]]}, {"id": "1506.08700", "submitter": "Xavier Bouthillier", "authors": "Xavier Bouthillier, Kishore Konda, Pascal Vincent, Roland Memisevic", "title": "Dropout as data augmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Dropout is typically interpreted as bagging a large number of models sharing\nparameters. We show that using dropout in a network can also be interpreted as\na kind of data augmentation in the input space without domain knowledge. We\npresent an approach to projecting the dropout noise within a network back into\nthe input space, thereby generating augmented versions of the training data,\nand we show that training a deterministic network on the augmented samples\nyields similar results. Finally, we propose a new dropout noise scheme based on\nour observations and show that it improves dropout results without adding\nsignificant computational cost.\n", "versions": [{"version": "v1", "created": "Mon, 29 Jun 2015 15:55:45 GMT"}, {"version": "v2", "created": "Thu, 19 Nov 2015 21:48:20 GMT"}, {"version": "v3", "created": "Thu, 26 Nov 2015 18:29:08 GMT"}, {"version": "v4", "created": "Fri, 8 Jan 2016 02:56:04 GMT"}], "update_date": "2016-01-11", "authors_parsed": [["Bouthillier", "Xavier", ""], ["Konda", "Kishore", ""], ["Vincent", "Pascal", ""], ["Memisevic", "Roland", ""]]}, {"id": "1506.08760", "submitter": "Gautam Dasarathy", "authors": "Gautam Dasarathy, Robert Nowak, Xiaojin Zhu", "title": "S2: An Efficient Graph Based Active Learning Algorithm with Application\n  to Nonparametric Classification", "comments": "A version of this paper appears in the Conference on Learning Theory\n  (COLT) 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper investigates the problem of active learning for binary label\nprediction on a graph. We introduce a simple and label-efficient algorithm\ncalled S2 for this task. At each step, S2 selects the vertex to be labeled\nbased on the structure of the graph and all previously gathered labels.\nSpecifically, S2 queries for the label of the vertex that bisects the *shortest\nshortest* path between any pair of oppositely labeled vertices. We present a\ntheoretical estimate of the number of queries S2 needs in terms of a novel\nparametrization of the complexity of binary functions on graphs. We also\npresent experimental results demonstrating the performance of S2 on both real\nand synthetic data. While other graph-based active learning algorithms have\nshown promise in practice, our algorithm is the first with both good\nperformance and theoretical guarantees. Finally, we demonstrate the\nimplications of the S2 algorithm to the theory of nonparametric active\nlearning. In particular, we show that S2 achieves near minimax optimal excess\nrisk for an important class of nonparametric classification problems.\n", "versions": [{"version": "v1", "created": "Mon, 29 Jun 2015 18:03:25 GMT"}], "update_date": "2015-06-30", "authors_parsed": [["Dasarathy", "Gautam", ""], ["Nowak", "Robert", ""], ["Zhu", "Xiaojin", ""]]}, {"id": "1506.08909", "submitter": "Ryan Lowe T.", "authors": "Ryan Lowe, Nissan Pow, Iulian Serban, Joelle Pineau", "title": "The Ubuntu Dialogue Corpus: A Large Dataset for Research in Unstructured\n  Multi-Turn Dialogue Systems", "comments": "SIGDIAL 2015. 10 pages, 5 figures. Update includes link to new\n  version of the dataset, with some added features and bug fixes. See:\n  https://github.com/rkadlec/ubuntu-ranking-dataset-creator", "journal-ref": null, "doi": null, "report-no": "Proc. SIGDIAL 16 (2015) pp. 285-294", "categories": "cs.CL cs.AI cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces the Ubuntu Dialogue Corpus, a dataset containing almost\n1 million multi-turn dialogues, with a total of over 7 million utterances and\n100 million words. This provides a unique resource for research into building\ndialogue managers based on neural language models that can make use of large\namounts of unlabeled data. The dataset has both the multi-turn property of\nconversations in the Dialog State Tracking Challenge datasets, and the\nunstructured nature of interactions from microblog services such as Twitter. We\nalso describe two neural learning architectures suitable for analyzing this\ndataset, and provide benchmark performance on the task of selecting the best\nnext response.\n", "versions": [{"version": "v1", "created": "Tue, 30 Jun 2015 00:37:09 GMT"}, {"version": "v2", "created": "Tue, 21 Jul 2015 16:11:29 GMT"}, {"version": "v3", "created": "Thu, 4 Feb 2016 01:21:35 GMT"}], "update_date": "2016-07-26", "authors_parsed": [["Lowe", "Ryan", ""], ["Pow", "Nissan", ""], ["Serban", "Iulian", ""], ["Pineau", "Joelle", ""]]}, {"id": "1506.08910", "submitter": "Ravi Ganti", "authors": "Ravi Ganti, Nikhil Rao, Rebecca M. Willett and Robert Nowak", "title": "Learning Single Index Models in High Dimensions", "comments": "16 pages, 2 figures, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Single Index Models (SIMs) are simple yet flexible semi-parametric models for\nclassification and regression. Response variables are modeled as a nonlinear,\nmonotonic function of a linear combination of features. Estimation in this\ncontext requires learning both the feature weights, and the nonlinear function.\nWhile methods have been described to learn SIMs in the low dimensional regime,\na method that can efficiently learn SIMs in high dimensions has not been\nforthcoming. We propose three variants of a computationally and statistically\nefficient algorithm for SIM inference in high dimensions. We establish excess\nrisk bounds for the proposed algorithms and experimentally validate the\nadvantages that our SIM learning methods provide relative to Generalized Linear\nModel (GLM) and low dimensional SIM based learning methods.\n", "versions": [{"version": "v1", "created": "Tue, 30 Jun 2015 00:45:25 GMT"}], "update_date": "2015-07-01", "authors_parsed": [["Ganti", "Ravi", ""], ["Rao", "Nikhil", ""], ["Willett", "Rebecca M.", ""], ["Nowak", "Robert", ""]]}, {"id": "1506.08928", "submitter": "Sejong Yoon", "authors": "Changkyu Song and Sejong Yoon and Vladimir Pavlovic", "title": "Fast ADMM Algorithm for Distributed Optimization with Adaptive Penalty", "comments": "8 pages manuscript, 2 pages appendix, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose new methods to speed up convergence of the Alternating Direction\nMethod of Multipliers (ADMM), a common optimization tool in the context of\nlarge scale and distributed learning. The proposed method accelerates the speed\nof convergence by automatically deciding the constraint penalty needed for\nparameter consensus in each iteration. In addition, we also propose an\nextension of the method that adaptively determines the maximum number of\niterations to update the penalty. We show that this approach effectively leads\nto an adaptive, dynamic network topology underlying the distributed\noptimization. The utility of the new penalty update schemes is demonstrated on\nboth synthetic and real data, including a computer vision application of\ndistributed structure from motion.\n", "versions": [{"version": "v1", "created": "Tue, 30 Jun 2015 03:37:07 GMT"}], "update_date": "2016-04-05", "authors_parsed": [["Song", "Changkyu", ""], ["Yoon", "Sejong", ""], ["Pavlovic", "Vladimir", ""]]}, {"id": "1506.09016", "submitter": "Th\\'eo Trouillon", "authors": "Guillaume Bouchard, Th\\'eo Trouillon, Julien Perez, Adrien Gaidon", "title": "Online Learning to Sample", "comments": "Update: removed convergence theorem and proof as there is an error.\n  Submitted to UAI 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.NA math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Stochastic Gradient Descent (SGD) is one of the most widely used techniques\nfor online optimization in machine learning. In this work, we accelerate SGD by\nadaptively learning how to sample the most useful training examples at each\ntime step. First, we show that SGD can be used to learn the best possible\nsampling distribution of an importance sampling estimator. Second, we show that\nthe sampling distribution of an SGD algorithm can be estimated online by\nincrementally minimizing the variance of the gradient. The resulting algorithm\n- called Adaptive Weighted SGD (AW-SGD) - maintains a set of parameters to\noptimize, as well as a set of parameters to sample learning examples. We show\nthat AWSGD yields faster convergence in three different applications: (i) image\nclassification with deep features, where the sampling of images depends on\ntheir labels, (ii) matrix factorization, where rows and columns are not sampled\nuniformly, and (iii) reinforcement learning, where the optimized and\nexploration policies are estimated at the same time, where our approach\ncorresponds to an off-policy gradient algorithm.\n", "versions": [{"version": "v1", "created": "Tue, 30 Jun 2015 10:08:35 GMT"}, {"version": "v2", "created": "Tue, 15 Mar 2016 16:08:56 GMT"}], "update_date": "2016-03-16", "authors_parsed": [["Bouchard", "Guillaume", ""], ["Trouillon", "Th\u00e9o", ""], ["Perez", "Julien", ""], ["Gaidon", "Adrien", ""]]}, {"id": "1506.09039", "submitter": "Yutian Chen", "authors": "Yutian Chen, Zoubin Ghahramani", "title": "Scalable Discrete Sampling as a Multi-Armed Bandit Problem", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Drawing a sample from a discrete distribution is one of the building\ncomponents for Monte Carlo methods. Like other sampling algorithms, discrete\nsampling suffers from the high computational burden in large-scale inference\nproblems. We study the problem of sampling a discrete random variable with a\nhigh degree of dependency that is typical in large-scale Bayesian inference and\ngraphical models, and propose an efficient approximate solution with a\nsubsampling approach. We make a novel connection between the discrete sampling\nand Multi-Armed Bandits problems with a finite reward population and provide\nthree algorithms with theoretical guarantees. Empirical evaluations show the\nrobustness and efficiency of the approximate algorithms in both synthetic and\nreal-world large-scale problems.\n", "versions": [{"version": "v1", "created": "Tue, 30 Jun 2015 11:20:45 GMT"}, {"version": "v2", "created": "Tue, 9 Feb 2016 14:21:05 GMT"}, {"version": "v3", "created": "Wed, 27 Apr 2016 21:09:43 GMT"}], "update_date": "2016-04-29", "authors_parsed": [["Chen", "Yutian", ""], ["Ghahramani", "Zoubin", ""]]}, {"id": "1506.09153", "submitter": "Gunnar R\\\"atsch", "authors": "Christian Widmer, Marius Kloft, Vipin T Sreedharan, Gunnar R\\\"atsch", "title": "Framework for Multi-task Multiple Kernel Learning and Applications in\n  Genome Analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CE cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a general regularization-based framework for Multi-task learning\n(MTL), in which the similarity between tasks can be learned or refined using\n$\\ell_p$-norm Multiple Kernel learning (MKL). Based on this very general\nformulation (including a general loss function), we derive the corresponding\ndual formulation using Fenchel duality applied to Hermitian matrices. We show\nthat numerous established MTL methods can be derived as special cases from\nboth, the primal and dual of our formulation. Furthermore, we derive a modern\ndual-coordinate descend optimization strategy for the hinge-loss variant of our\nformulation and provide convergence bounds for our algorithm. As a special\ncase, we implement in C++ a fast LibLinear-style solver for $\\ell_p$-norm MKL.\nIn the experimental section, we analyze various aspects of our algorithm such\nas predictive performance and ability to reconstruct task relationships on\nbiologically inspired synthetic data, where we have full control over the\nunderlying ground truth. We also experiment on a new dataset from the domain of\ncomputational biology that we collected for the purpose of this paper. It\nconcerns the prediction of transcription start sites (TSS) over nine organisms,\nwhich is a crucial task in gene finding. Our solvers including all discussed\nspecial cases are made available as open-source software as part of the SHOGUN\nmachine learning toolbox (available at \\url{http://shogun.ml}).\n", "versions": [{"version": "v1", "created": "Tue, 30 Jun 2015 16:52:27 GMT"}], "update_date": "2015-07-01", "authors_parsed": [["Widmer", "Christian", ""], ["Kloft", "Marius", ""], ["Sreedharan", "Vipin T", ""], ["R\u00e4tsch", "Gunnar", ""]]}, {"id": "1506.09215", "submitter": "Simon Lacoste-Julien", "authors": "Jean-Baptiste Alayrac, Piotr Bojanowski, Nishant Agrawal, Josef Sivic,\n  Ivan Laptev, Simon Lacoste-Julien", "title": "Unsupervised Learning from Narrated Instruction Videos", "comments": "Appears in: 2016 IEEE Conference on Computer Vision and Pattern\n  Recognition (CVPR 2016). 21 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We address the problem of automatically learning the main steps to complete a\ncertain task, such as changing a car tire, from a set of narrated instruction\nvideos. The contributions of this paper are three-fold. First, we develop a new\nunsupervised learning approach that takes advantage of the complementary nature\nof the input video and the associated narration. The method solves two\nclustering problems, one in text and one in video, applied one after each other\nand linked by joint constraints to obtain a single coherent sequence of steps\nin both modalities. Second, we collect and annotate a new challenging dataset\nof real-world instruction videos from the Internet. The dataset contains about\n800,000 frames for five different tasks that include complex interactions\nbetween people and objects, and are captured in a variety of indoor and outdoor\nsettings. Third, we experimentally demonstrate that the proposed method can\nautomatically discover, in an unsupervised manner, the main steps to achieve\nthe task and locate the steps in the input videos.\n", "versions": [{"version": "v1", "created": "Tue, 30 Jun 2015 19:55:37 GMT"}, {"version": "v2", "created": "Thu, 2 Jul 2015 16:43:36 GMT"}, {"version": "v3", "created": "Mon, 30 Nov 2015 18:10:53 GMT"}, {"version": "v4", "created": "Tue, 28 Jun 2016 18:43:37 GMT"}], "update_date": "2016-06-29", "authors_parsed": [["Alayrac", "Jean-Baptiste", ""], ["Bojanowski", "Piotr", ""], ["Agrawal", "Nishant", ""], ["Sivic", "Josef", ""], ["Laptev", "Ivan", ""], ["Lacoste-Julien", "Simon", ""]]}]