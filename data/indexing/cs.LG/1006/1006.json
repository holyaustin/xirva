[{"id": "1006.0375", "submitter": "Joachim Buhmann M", "authors": "Joachim M. Buhmann", "title": "Information theoretic model validation for clustering", "comments": "9 pages, 2 figures, International Symposium on Information Theory\n  2010 (ISIT10 E-Mo-4.2), June 13-18 in Austin, TX}", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.LG math.IT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Model selection in clustering requires (i) to specify a suitable clustering\nprinciple and (ii) to control the model order complexity by choosing an\nappropriate number of clusters depending on the noise level in the data. We\nadvocate an information theoretic perspective where the uncertainty in the\nmeasurements quantizes the set of data partitionings and, thereby, induces\nuncertainty in the solution space of clusterings. A clustering model, which can\ntolerate a higher level of fluctuations in the measurements than alternative\nmodels, is considered to be superior provided that the clustering solution is\nequally informative. This tradeoff between \\emph{informativeness} and\n\\emph{robustness} is used as a model selection criterion. The requirement that\ndata partitionings should generalize from one data set to an equally probable\nsecond data set gives rise to a new notion of structure induced information.\n", "versions": [{"version": "v1", "created": "Wed, 2 Jun 2010 13:47:12 GMT"}], "update_date": "2010-06-03", "authors_parsed": [["Buhmann", "Joachim M.", ""]]}, {"id": "1006.0475", "submitter": "Alexey Chernov", "authors": "Alexey Chernov and Vladimir Vovk", "title": "Prediction with Advice of Unknown Number of Experts", "comments": "22 pages; draft version", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the framework of prediction with expert advice, we consider a recently\nintroduced kind of regret bounds: the bounds that depend on the effective\ninstead of nominal number of experts. In contrast to the NormalHedge bound,\nwhich mainly depends on the effective number of experts and also weakly depends\non the nominal one, we obtain a bound that does not contain the nominal number\nof experts at all. We use the defensive forecasting method and introduce an\napplication of defensive forecasting to multivalued supermartingales.\n", "versions": [{"version": "v1", "created": "Wed, 2 Jun 2010 19:41:27 GMT"}], "update_date": "2015-03-17", "authors_parsed": [["Chernov", "Alexey", ""], ["Vovk", "Vladimir", ""]]}, {"id": "1006.1129", "submitter": "Vladimir Pestov", "authors": "Vladimir Pestov", "title": "Predictive PAC learnability: a paradigm for learning from exchangeable\n  input data", "comments": "5 pages, latex, a postprint correcting a typo in the main definition\n  4.1", "journal-ref": "Proc. 2010 IEEE International Conference on Granular Computing\n  (GrC 2010), San Jose, CA, August 14-16, 2010, IEEE Computer Society, Los\n  Alamitos, 2010, pp. 387-391, Symposium on Foundations and Practice of Data\n  Mining", "doi": "10.1109/GrC.2010.102", "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Exchangeable random variables form an important and well-studied\ngeneralization of i.i.d. variables, however simple examples show that no\nnontrivial concept or function classes are PAC learnable under general\nexchangeable data inputs $X_1,X_2,\\ldots$. Inspired by the work of Berti and\nRigo on a Glivenko--Cantelli theorem for exchangeable inputs, we propose a new\nparadigm, adequate for learning from exchangeable data: predictive PAC\nlearnability. A learning rule $\\mathcal L$ for a function class $\\mathscr F$ is\npredictive PAC if for every $\\e,\\delta>0$ and each function $f\\in {\\mathscr\nF}$, whenever $\\abs{\\sigma}\\geq s(\\delta,\\e)$, we have with confidence\n$1-\\delta$ that the expected difference between $f(X_{n+1})$ and the image of\n$f\\vert\\sigma$ under $\\mathcal L$ does not exceed $\\e$ conditionally on\n$X_1,X_2,\\ldots,X_n$. Thus, instead of learning the function $f$ as such, we\nare learning to a given accuracy $\\e$ the predictive behaviour of $f$ at the\nfuture points $X_i(\\omega)$, $i>n$ of the sample path. Using de Finetti's\ntheorem, we show that if a universally separable function class $\\mathscr F$ is\ndistribution-free PAC learnable under i.i.d. inputs, then it is\ndistribution-free predictive PAC learnable under exchangeable inputs, with a\nslightly worse sample complexity.\n", "versions": [{"version": "v1", "created": "Sun, 6 Jun 2010 18:21:06 GMT"}, {"version": "v2", "created": "Sun, 22 Aug 2010 23:26:20 GMT"}], "update_date": "2016-11-18", "authors_parsed": [["Pestov", "Vladimir", ""]]}, {"id": "1006.1138", "submitter": "Alexander Rakhlin", "authors": "Alexander Rakhlin, Karthik Sridharan, Ambuj Tewari", "title": "Online Learning via Sequential Complexities", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of sequential prediction and provide tools to study\nthe minimax value of the associated game. Classical statistical learning theory\nprovides several useful complexity measures to study learning with i.i.d. data.\nOur proposed sequential complexities can be seen as extensions of these\nmeasures to the sequential setting. The developed theory is shown to yield\nprecise learning guarantees for the problem of sequential prediction. In\nparticular, we show necessary and sufficient conditions for online learnability\nin the setting of supervised learning. Several examples show the utility of our\nframework: we can establish learnability without having to exhibit an explicit\nonline learning algorithm.\n", "versions": [{"version": "v1", "created": "Sun, 6 Jun 2010 21:05:27 GMT"}, {"version": "v2", "created": "Thu, 24 Mar 2011 15:41:11 GMT"}, {"version": "v3", "created": "Tue, 12 Aug 2014 16:44:00 GMT"}], "update_date": "2014-08-13", "authors_parsed": [["Rakhlin", "Alexander", ""], ["Sridharan", "Karthik", ""], ["Tewari", "Ambuj", ""]]}, {"id": "1006.1288", "submitter": "Gilles Meyer", "authors": "Gilles Meyer, Silvere Bonnabel, Rodolphe Sepulchre", "title": "Regression on fixed-rank positive semidefinite matrices: a Riemannian\n  approach", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The paper addresses the problem of learning a regression model parameterized\nby a fixed-rank positive semidefinite matrix. The focus is on the nonlinear\nnature of the search space and on scalability to high-dimensional problems. The\nmathematical developments rely on the theory of gradient descent algorithms\nadapted to the Riemannian geometry that underlies the set of fixed-rank\npositive semidefinite matrices. In contrast with previous contributions in the\nliterature, no restrictions are imposed on the range space of the learned\nmatrix. The resulting algorithms maintain a linear complexity in the problem\nsize and enjoy important invariance properties. We apply the proposed\nalgorithms to the problem of learning a distance function parameterized by a\npositive semidefinite matrix. Good performance is observed on classical\nbenchmarks.\n", "versions": [{"version": "v1", "created": "Mon, 7 Jun 2010 16:20:02 GMT"}, {"version": "v2", "created": "Mon, 31 Jan 2011 09:59:44 GMT"}], "update_date": "2011-02-01", "authors_parsed": [["Meyer", "Gilles", ""], ["Bonnabel", "Silvere", ""], ["Sepulchre", "Rodolphe", ""]]}, {"id": "1006.1328", "submitter": "Jonathan Huang", "authors": "Jonathan Huang and Carlos Guestrin", "title": "Uncovering the Riffled Independence Structure of Rankings", "comments": "65 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Representing distributions over permutations can be a daunting task due to\nthe fact that the number of permutations of $n$ objects scales factorially in\n$n$. One recent way that has been used to reduce storage complexity has been to\nexploit probabilistic independence, but as we argue, full independence\nassumptions impose strong sparsity constraints on distributions and are\nunsuitable for modeling rankings. We identify a novel class of independence\nstructures, called \\emph{riffled independence}, encompassing a more expressive\nfamily of distributions while retaining many of the properties necessary for\nperforming efficient inference and reducing sample complexity. In riffled\nindependence, one draws two permutations independently, then performs the\n\\emph{riffle shuffle}, common in card games, to combine the two permutations to\nform a single permutation. Within the context of ranking, riffled independence\ncorresponds to ranking disjoint sets of objects independently, then\ninterleaving those rankings. In this paper, we provide a formal introduction to\nriffled independence and present algorithms for using riffled independence\nwithin Fourier-theoretic frameworks which have been explored by a number of\nrecent papers. Additionally, we propose an automated method for discovering\nsets of items which are riffle independent from a training set of rankings. We\nshow that our clustering-like algorithms can be used to discover meaningful\nlatent coalitions from real preference ranking datasets and to learn the\nstructure of hierarchically decomposable models based on riffled independence.\n", "versions": [{"version": "v1", "created": "Mon, 7 Jun 2010 18:45:46 GMT"}], "update_date": "2010-06-08", "authors_parsed": [["Huang", "Jonathan", ""], ["Guestrin", "Carlos", ""]]}, {"id": "1006.1746", "submitter": "Vianney Perchet", "authors": "Vianney Perchet (EC)", "title": "Calibration and Internal no-Regret with Partial Monitoring", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GT cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Calibrated strategies can be obtained by performing strategies that have no\ninternal regret in some auxiliary game. Such strategies can be constructed\nexplicitly with the use of Blackwell's approachability theorem, in an other\nauxiliary game. We establish the converse: a strategy that approaches a convex\n$B$-set can be derived from the construction of a calibrated strategy. We\ndevelop these tools in the framework of a game with partial monitoring, where\nplayers do not observe the actions of their opponents but receive random\nsignals, to define a notion of internal regret and construct strategies that\nhave no such regret.\n", "versions": [{"version": "v1", "created": "Wed, 9 Jun 2010 09:02:44 GMT"}], "update_date": "2010-07-28", "authors_parsed": [["Perchet", "Vianney", "", "EC"]]}, {"id": "1006.2156", "submitter": "Aditya Menon", "authors": "Aditya Krishna Menon and Charles Elkan", "title": "Dyadic Prediction Using a Latent Feature Log-Linear Model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In dyadic prediction, labels must be predicted for pairs (dyads) whose\nmembers possess unique identifiers and, sometimes, additional features called\nside-information. Special cases of this problem include collaborative filtering\nand link prediction. We present the first model for dyadic prediction that\nsatisfies several important desiderata: (i) labels may be ordinal or nominal,\n(ii) side-information can be easily exploited if present, (iii) with or without\nside-information, latent features are inferred for dyad members, (iv) it is\nresistant to sample-selection bias, (v) it can learn well-calibrated\nprobabilities, and (vi) it can scale to very large datasets. To our knowledge,\nno existing method satisfies all the above criteria. In particular, many\nmethods assume that the labels are ordinal and ignore side-information when it\nis present. Experimental results show that the new method is competitive with\nstate-of-the-art methods for the special cases of collaborative filtering and\nlink prediction, and that it makes accurate predictions on nominal data.\n", "versions": [{"version": "v1", "created": "Thu, 10 Jun 2010 21:19:28 GMT"}], "update_date": "2010-06-14", "authors_parsed": [["Menon", "Aditya Krishna", ""], ["Elkan", "Charles", ""]]}, {"id": "1006.2513", "submitter": "Rad Niazadeh", "authors": "Rad Niazadeh, Masoud Babaie-Zadeh and Christian Jutten", "title": "On the Achievability of Cram\\'er-Rao Bound In Noisy Compressed Sensing", "comments": null, "journal-ref": "IEEE Transactions on Signal Processing, Volume 60, Issue 1, Pages\n  518- 526, January 2012", "doi": "10.1109/TSP.2011.2171953", "report-no": null, "categories": "cs.IT cs.LG math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, it has been proved in Babadi et al. that in noisy compressed\nsensing, a joint typical estimator can asymptotically achieve the Cramer-Rao\nlower bound of the problem.To prove this result, this paper used a lemma,which\nis provided in Akcakaya et al,that comprises the main building block of the\nproof. This lemma is based on the assumption of Gaussianity of the measurement\nmatrix and its randomness in the domain of noise. In this correspondence, we\ngeneralize the results obtained in Babadi et al by dropping the Gaussianity\nassumption on the measurement matrix. In fact, by considering the measurement\nmatrix as a deterministic matrix in our analysis, we find a theorem similar to\nthe main theorem of Babadi et al for a family of randomly generated (but\ndeterministic in the noise domain) measurement matrices that satisfy a\ngeneralized condition known as The Concentration of Measures Inequality. By\nthis, we finally show that under our generalized assumptions, the Cramer-Rao\nbound of the estimation is achievable by using the typical estimator introduced\nin Babadi et al.\n", "versions": [{"version": "v1", "created": "Sun, 13 Jun 2010 06:07:09 GMT"}, {"version": "v2", "created": "Fri, 2 Jul 2010 03:49:15 GMT"}, {"version": "v3", "created": "Sun, 25 Aug 2013 17:12:28 GMT"}], "update_date": "2013-08-27", "authors_parsed": [["Niazadeh", "Rad", ""], ["Babaie-Zadeh", "Masoud", ""], ["Jutten", "Christian", ""]]}, {"id": "1006.2588", "submitter": "Daniel Hsu", "authors": "Alina Beygelzimer, Daniel Hsu, John Langford, Tong Zhang", "title": "Agnostic Active Learning Without Constraints", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present and analyze an agnostic active learning algorithm that works\nwithout keeping a version space. This is unlike all previous approaches where a\nrestricted set of candidate hypotheses is maintained throughout learning, and\nonly hypotheses from this set are ever returned. By avoiding this version space\napproach, our algorithm sheds the computational burden and brittleness\nassociated with maintaining version spaces, yet still allows for substantial\nimprovements over supervised learning for classification.\n", "versions": [{"version": "v1", "created": "Mon, 14 Jun 2010 02:03:12 GMT"}], "update_date": "2010-06-15", "authors_parsed": [["Beygelzimer", "Alina", ""], ["Hsu", "Daniel", ""], ["Langford", "John", ""], ["Zhang", "Tong", ""]]}, {"id": "1006.2592", "submitter": "Yiyuan She", "authors": "Yiyuan She and Art B. Owen", "title": "Outlier Detection Using Nonconvex Penalized Regression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.LG stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper studies the outlier detection problem from the point of view of\npenalized regressions. Our regression model adds one mean shift parameter for\neach of the $n$ data points. We then apply a regularization favoring a sparse\nvector of mean shift parameters. The usual $L_1$ penalty yields a convex\ncriterion, but we find that it fails to deliver a robust estimator. The $L_1$\npenalty corresponds to soft thresholding. We introduce a thresholding (denoted\nby $\\Theta$) based iterative procedure for outlier detection ($\\Theta$-IPOD). A\nversion based on hard thresholding correctly identifies outliers on some hard\ntest problems. We find that $\\Theta$-IPOD is much faster than iteratively\nreweighted least squares for large data because each iteration costs at most\n$O(np)$ (and sometimes much less) avoiding an $O(np^2)$ least squares estimate.\nWe describe the connection between $\\Theta$-IPOD and $M$-estimators. Our\nproposed method has one tuning parameter with which to both identify outliers\nand estimate regression coefficients. A data-dependent choice can be made based\non BIC. The tuned $\\Theta$-IPOD shows outstanding performance in identifying\noutliers in various situations in comparison to other existing approaches. This\nmethodology extends to high-dimensional modeling with $p\\gg n$, if both the\ncoefficient vector and the outlier pattern are sparse.\n", "versions": [{"version": "v1", "created": "Mon, 14 Jun 2010 02:51:41 GMT"}, {"version": "v2", "created": "Thu, 30 Sep 2010 19:04:02 GMT"}, {"version": "v3", "created": "Mon, 17 Oct 2011 02:23:15 GMT"}], "update_date": "2011-10-18", "authors_parsed": [["She", "Yiyuan", ""], ["Owen", "Art B.", ""]]}, {"id": "1006.2899", "submitter": "Tamir Hazan", "authors": "Tamir Hazan, Raquel Urtasun", "title": "Approximated Structured Prediction for Learning Large Scale Graphical\n  Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This manuscripts contains the proofs for \"A Primal-Dual Message-Passing\nAlgorithm for Approximated Large Scale Structured Prediction\".\n", "versions": [{"version": "v1", "created": "Tue, 15 Jun 2010 06:55:03 GMT"}, {"version": "v2", "created": "Mon, 9 Jul 2012 18:22:27 GMT"}], "update_date": "2012-07-10", "authors_parsed": [["Hazan", "Tamir", ""], ["Urtasun", "Raquel", ""]]}, {"id": "1006.3033", "submitter": "Pantelis Bouboulis", "authors": "Pantelis Bouboulis and Sergios Theodoridis", "title": "Extension of Wirtinger's Calculus to Reproducing Kernel Hilbert Spaces\n  and the Complex Kernel LMS", "comments": "15 pages (double column), preprint of article accepted in IEEE Trans.\n  Sig. Proc", "journal-ref": null, "doi": "10.1109/TSP.2010.2096420", "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Over the last decade, kernel methods for nonlinear processing have\nsuccessfully been used in the machine learning community. The primary\nmathematical tool employed in these methods is the notion of the Reproducing\nKernel Hilbert Space. However, so far, the emphasis has been on batch\ntechniques. It is only recently, that online techniques have been considered in\nthe context of adaptive signal processing tasks. Moreover, these efforts have\nonly been focussed on real valued data sequences. To the best of our knowledge,\nno adaptive kernel-based strategy has been developed, so far, for complex\nvalued signals. Furthermore, although the real reproducing kernels are used in\nan increasing number of machine learning problems, complex kernels have not,\nyet, been used, in spite of their potential interest in applications that deal\nwith complex signals, with Communications being a typical example. In this\npaper, we present a general framework to attack the problem of adaptive\nfiltering of complex signals, using either real reproducing kernels, taking\nadvantage of a technique called \\textit{complexification} of real RKHSs, or\ncomplex reproducing kernels, highlighting the use of the complex gaussian\nkernel. In order to derive gradients of operators that need to be defined on\nthe associated complex RKHSs, we employ the powerful tool of Wirtinger's\nCalculus, which has recently attracted attention in the signal processing\ncommunity. To this end, in this paper, the notion of Wirtinger's calculus is\nextended, for the first time, to include complex RKHSs and use it to derive\nseveral realizations of the Complex Kernel Least-Mean-Square (CKLMS) algorithm.\nExperiments verify that the CKLMS offers significant performance improvements\nover several linear and nonlinear algorithms, when dealing with nonlinearities.\n", "versions": [{"version": "v1", "created": "Tue, 15 Jun 2010 17:09:01 GMT"}, {"version": "v2", "created": "Thu, 30 Sep 2010 15:21:53 GMT"}, {"version": "v3", "created": "Sat, 27 Nov 2010 09:06:13 GMT"}], "update_date": "2017-04-26", "authors_parsed": [["Bouboulis", "Pantelis", ""], ["Theodoridis", "Sergios", ""]]}, {"id": "1006.3417", "submitter": "Kien Nguyen", "authors": "Kien C. Nguyen, Tansu Alpcan, Tamer Ba\\c{s}ar", "title": "Fictitious Play with Time-Invariant Frequency Update for Network\n  Security", "comments": "Proceedings of the 2010 IEEE Multi-Conference on Systems and Control\n  (MSC10), September 2010, Yokohama, Japan", "journal-ref": null, "doi": "10.1109/CCA.2010.5611248", "report-no": null, "categories": "cs.GT cs.CR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study two-player security games which can be viewed as sequences of\nnonzero-sum matrix games played by an Attacker and a Defender. The evolution of\nthe game is based on a stochastic fictitious play process, where players do not\nhave access to each other's payoff matrix. Each has to observe the other's\nactions up to present and plays the action generated based on the best response\nto these observations. In a regular fictitious play process, each player makes\na maximum likelihood estimate of her opponent's mixed strategy, which results\nin a time-varying update based on the previous estimate and current action. In\nthis paper, we explore an alternative scheme for frequency update, whose mean\ndynamic is instead time-invariant. We examine convergence properties of the\nmean dynamic of the fictitious play process with such an update scheme, and\nestablish local stability of the equilibrium point when both players are\nrestricted to two actions. We also propose an adaptive algorithm based on this\ntime-invariant frequency update.\n", "versions": [{"version": "v1", "created": "Thu, 17 Jun 2010 10:13:22 GMT"}], "update_date": "2016-11-17", "authors_parsed": [["Nguyen", "Kien C.", ""], ["Alpcan", "Tansu", ""], ["Ba\u015far", "Tamer", ""]]}, {"id": "1006.3679", "submitter": "Hossein Mobahi", "authors": "Hossein Mobahi, Shankar R. Rao, Allen Y. Yang, Shankar S. Sastry and\n  Yi Ma", "title": "Segmentation of Natural Images by Texture and Boundary Compression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.IT cs.LG math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a novel algorithm for segmentation of natural images that\nharnesses the principle of minimum description length (MDL). Our method is\nbased on observations that a homogeneously textured region of a natural image\ncan be well modeled by a Gaussian distribution and the region boundary can be\neffectively coded by an adaptive chain code. The optimal segmentation of an\nimage is the one that gives the shortest coding length for encoding all\ntextures and boundaries in the image, and is obtained via an agglomerative\nclustering process applied to a hierarchy of decreasing window sizes as\nmulti-scale texture features. The optimal segmentation also provides an\naccurate estimate of the overall coding length and hence the true entropy of\nthe image. We test our algorithm on the publicly available Berkeley\nSegmentation Dataset. It achieves state-of-the-art segmentation results\ncompared to other existing methods.\n", "versions": [{"version": "v1", "created": "Fri, 18 Jun 2010 12:37:28 GMT"}], "update_date": "2010-06-21", "authors_parsed": [["Mobahi", "Hossein", ""], ["Rao", "Shankar R.", ""], ["Yang", "Allen Y.", ""], ["Sastry", "Shankar S.", ""], ["Ma", "Yi", ""]]}, {"id": "1006.3780", "submitter": "Antony Joseph", "authors": "Andrew R. Barron, Antony Joseph", "title": "Least Squares Superposition Codes of Moderate Dictionary Size, Reliable\n  at Rates up to Capacity", "comments": "17 pages, 4 figures, journal submission", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.LG math.IT math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For the additive white Gaussian noise channel with average codeword power\nconstraint, new coding methods are devised in which the codewords are sparse\nsuperpositions, that is, linear combinations of subsets of vectors from a given\ndesign, with the possible messages indexed by the choice of subset. Decoding is\nby least squares, tailored to the assumed form of linear combination.\nCommunication is shown to be reliable with error probability exponentially\nsmall for all rates up to the Shannon capacity.\n", "versions": [{"version": "v1", "created": "Fri, 18 Jun 2010 19:35:52 GMT"}], "update_date": "2010-06-21", "authors_parsed": [["Barron", "Andrew R.", ""], ["Joseph", "Antony", ""]]}, {"id": "1006.3870", "submitter": "Antony Joseph", "authors": "Andrew R Barron, Antony Joseph", "title": "Toward Fast Reliable Communication at Rates Near Capacity with Gaussian\n  Noise", "comments": "5 pages, 4 figures, conference submission", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.LG math.IT math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For the additive Gaussian noise channel with average codeword power\nconstraint, sparse superposition codes and adaptive successive decoding is\ndeveloped. Codewords are linear combinations of subsets of vectors, with the\nmessage indexed by the choice of subset. A feasible decoding algorithm is\npresented. Communication is reliable with error probability exponentially small\nfor all rates below the Shannon capacity.\n", "versions": [{"version": "v1", "created": "Sat, 19 Jun 2010 13:51:27 GMT"}], "update_date": "2010-06-22", "authors_parsed": [["Barron", "Andrew R", ""], ["Joseph", "Antony", ""]]}, {"id": "1006.4039", "submitter": "Feng Yan", "authors": "Feng Yan, Shreyas Sundaram, S. V. N. Vishwanathan, Yuan Qi", "title": "Distributed Autonomous Online Learning: Regrets and Intrinsic\n  Privacy-Preserving Properties", "comments": "25 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Online learning has become increasingly popular on handling massive data. The\nsequential nature of online learning, however, requires a centralized learner\nto store data and update parameters. In this paper, we consider online learning\nwith {\\em distributed} data sources. The autonomous learners update local\nparameters based on local data sources and periodically exchange information\nwith a small subset of neighbors in a communication network. We derive the\nregret bound for strongly convex functions that generalizes the work by Ram et\nal. (2010) for convex functions. Most importantly, we show that our algorithm\nhas \\emph{intrinsic} privacy-preserving properties, and we prove the sufficient\nand necessary conditions for privacy preservation in the network. These\nconditions imply that for networks with greater-than-one connectivity, a\nmalicious learner cannot reconstruct the subgradients (and sensitive raw data)\nof other learners, which makes our algorithm appealing in privacy sensitive\napplications.\n", "versions": [{"version": "v1", "created": "Mon, 21 Jun 2010 11:30:06 GMT"}, {"version": "v2", "created": "Tue, 1 Feb 2011 03:16:12 GMT"}, {"version": "v3", "created": "Fri, 4 Feb 2011 16:06:35 GMT"}], "update_date": "2011-02-07", "authors_parsed": [["Yan", "Feng", ""], ["Sundaram", "Shreyas", ""], ["Vishwanathan", "S. V. N.", ""], ["Qi", "Yuan", ""]]}, {"id": "1006.4442", "submitter": "Angelika Kimmig", "authors": "Angelika Kimmig, Bart Demoen, Luc De Raedt, V\\'itor Santos Costa and\n  Ricardo Rocha", "title": "On the Implementation of the Probabilistic Logic Programming Language\n  ProbLog", "comments": "28 pages; To appear in Theory and Practice of Logic Programming\n  (TPLP)", "journal-ref": "Theory and Practice of Logic Programming, 11, 235-262, 2011", "doi": "10.1017/S1471068410000566", "report-no": null, "categories": "cs.PL cs.LG cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The past few years have seen a surge of interest in the field of\nprobabilistic logic learning and statistical relational learning. In this\nendeavor, many probabilistic logics have been developed. ProbLog is a recent\nprobabilistic extension of Prolog motivated by the mining of large biological\nnetworks. In ProbLog, facts can be labeled with probabilities. These facts are\ntreated as mutually independent random variables that indicate whether these\nfacts belong to a randomly sampled program. Different kinds of queries can be\nposed to ProbLog programs. We introduce algorithms that allow the efficient\nexecution of these queries, discuss their implementation on top of the\nYAP-Prolog system, and evaluate their performance in the context of large\nnetworks of biological entities.\n", "versions": [{"version": "v1", "created": "Wed, 23 Jun 2010 08:05:34 GMT"}], "update_date": "2011-03-04", "authors_parsed": [["Kimmig", "Angelika", ""], ["Demoen", "Bart", ""], ["De Raedt", "Luc", ""], ["Costa", "V\u00edtor Santos", ""], ["Rocha", "Ricardo", ""]]}, {"id": "1006.4540", "submitter": "William Jackson", "authors": "N. Suguna and K. Thanushkodi", "title": "A Novel Rough Set Reduct Algorithm for Medical Domain Based on Bee\n  Colony Optimization", "comments": "IEEE Publication Format,\n  https://sites.google.com/site/journalofcomputing/", "journal-ref": "Journal of Computing, Vol. 2, No. 6, June 2010, NY, USA, ISSN\n  2151-9617", "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Feature selection refers to the problem of selecting relevant features which\nproduce the most predictive outcome. In particular, feature selection task is\ninvolved in datasets containing huge number of features. Rough set theory has\nbeen one of the most successful methods used for feature selection. However,\nthis method is still not able to find optimal subsets. This paper proposes a\nnew feature selection method based on Rough set theory hybrid with Bee Colony\nOptimization (BCO) in an attempt to combat this. This proposed work is applied\nin the medical domain to find the minimal reducts and experimentally compared\nwith the Quick Reduct, Entropy Based Reduct, and other hybrid Rough Set methods\nsuch as Genetic Algorithm (GA), Ant Colony Optimization (ACO) and Particle\nSwarm Optimization (PSO).\n", "versions": [{"version": "v1", "created": "Wed, 23 Jun 2010 14:53:33 GMT"}], "update_date": "2010-06-24", "authors_parsed": [["Suguna", "N.", ""], ["Thanushkodi", "K.", ""]]}, {"id": "1006.4832", "submitter": "Kristiaan Pelckmans", "authors": "Kristiaan Pelckmans", "title": "MINLIP for the Identification of Monotone Wiener Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper studies the MINLIP estimator for the identification of Wiener\nsystems consisting of a sequence of a linear FIR dynamical model, and a\nmonotonically increasing (or decreasing) static function. Given $T$\nobservations, this algorithm boils down to solving a convex quadratic program\nwith $O(T)$ variables and inequality constraints, implementing an inference\ntechnique which is based entirely on model complexity control. The resulting\nestimates of the linear submodel are found to be almost consistent when no\nnoise is present in the data, under a condition of smoothness of the true\nnonlinearity and local Persistency of Excitation (local PE) of the data. This\nresult is novel as it does not rely on classical tools as a 'linearization'\nusing a Taylor decomposition, nor exploits stochastic properties of the data.\nIt is indicated how to extend the method to cope with noisy data, and empirical\nevidence contrasts performance of the estimator against other recently proposed\ntechniques.\n", "versions": [{"version": "v1", "created": "Thu, 24 Jun 2010 16:42:38 GMT"}], "update_date": "2010-06-25", "authors_parsed": [["Pelckmans", "Kristiaan", ""]]}, {"id": "1006.4990", "submitter": "Danny Bickson", "authors": "Yucheng Low and Joseph Gonzalez and Aapo Kyrola and Danny Bickson and\n  Carlos Guestrin and Joseph M. Hellerstein", "title": "GraphLab: A New Framework for Parallel Machine Learning", "comments": null, "journal-ref": "The 26th Conference on Uncertainty in Artificial Intelligence (UAI\n  2010), Catalina Island, California, July 8-11, 2010", "doi": null, "report-no": null, "categories": "cs.LG cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Designing and implementing efficient, provably correct parallel machine\nlearning (ML) algorithms is challenging. Existing high-level parallel\nabstractions like MapReduce are insufficiently expressive while low-level tools\nlike MPI and Pthreads leave ML experts repeatedly solving the same design\nchallenges. By targeting common patterns in ML, we developed GraphLab, which\nimproves upon abstractions like MapReduce by compactly expressing asynchronous\niterative algorithms with sparse computational dependencies while ensuring data\nconsistency and achieving a high degree of parallel performance. We demonstrate\nthe expressiveness of the GraphLab framework by designing and implementing\nparallel versions of belief propagation, Gibbs sampling, Co-EM, Lasso and\nCompressed Sensing. We show that using GraphLab we can achieve excellent\nparallel performance on large scale real-world problems.\n", "versions": [{"version": "v1", "created": "Fri, 25 Jun 2010 13:23:48 GMT"}], "update_date": "2010-06-28", "authors_parsed": [["Low", "Yucheng", ""], ["Gonzalez", "Joseph", ""], ["Kyrola", "Aapo", ""], ["Bickson", "Danny", ""], ["Guestrin", "Carlos", ""], ["Hellerstein", "Joseph M.", ""]]}, {"id": "1006.5051", "submitter": "Ping Li", "authors": "Ping Li", "title": "Fast ABC-Boost for Multi-Class Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Abc-boost is a new line of boosting algorithms for multi-class\nclassification, by utilizing the commonly used sum-to-zero constraint. To\nimplement abc-boost, a base class must be identified at each boosting step.\nPrior studies used a very expensive procedure based on exhaustive search for\ndetermining the base class at each boosting step. Good testing performances of\nabc-boost (implemented as abc-mart and abc-logitboost) on a variety of datasets\nwere reported.\n  For large datasets, however, the exhaustive search strategy adopted in prior\nabc-boost algorithms can be too prohibitive. To overcome this serious\nlimitation, this paper suggests a heuristic by introducing Gaps when computing\nthe base class during training. That is, we update the choice of the base class\nonly for every $G$ boosting steps (i.e., G=1 in prior studies). We test this\nidea on large datasets (Covertype and Poker) as well as datasets of moderate\nsizes. Our preliminary results are very encouraging. On the large datasets,\neven with G=100 (or larger), there is essentially no loss of test accuracy. On\nthe moderate datasets, no obvious loss of test accuracy is observed when G<=\n20~50. Therefore, aided by this heuristic, it is promising that abc-boost will\nbe a practical tool for accurate multi-class classification.\n", "versions": [{"version": "v1", "created": "Fri, 25 Jun 2010 19:48:50 GMT"}], "update_date": "2010-06-28", "authors_parsed": [["Li", "Ping", ""]]}, {"id": "1006.5060", "submitter": "Xiaohui Xie", "authors": "Gui-Bo Ye and Xiaohui Xie", "title": "Learning sparse gradients for variable selection and dimension reduction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Variable selection and dimension reduction are two commonly adopted\napproaches for high-dimensional data analysis, but have traditionally been\ntreated separately. Here we propose an integrated approach, called sparse\ngradient learning (SGL), for variable selection and dimension reduction via\nlearning the gradients of the prediction function directly from samples. By\nimposing a sparsity constraint on the gradients, variable selection is achieved\nby selecting variables corresponding to non-zero partial derivatives, and\neffective dimensions are extracted based on the eigenvectors of the derived\nsparse empirical gradient covariance matrix. An error analysis is given for the\nconvergence of the estimated gradients to the true ones in both the Euclidean\nand the manifold setting. We also develop an efficient forward-backward\nsplitting algorithm to solve the SGL problem, making the framework practically\nscalable for medium or large datasets. The utility of SGL for variable\nselection and feature extraction is explicitly given and illustrated on\nartificial data as well as real-world examples. The main advantages of our\nmethod include variable selection for both linear and nonlinear predictions,\neffective dimension reduction with sparse loadings, and an efficient algorithm\nfor large p, small n problems.\n", "versions": [{"version": "v1", "created": "Fri, 25 Jun 2010 20:27:00 GMT"}, {"version": "v2", "created": "Thu, 1 Jul 2010 05:06:43 GMT"}], "update_date": "2010-07-02", "authors_parsed": [["Ye", "Gui-Bo", ""], ["Xie", "Xiaohui", ""]]}, {"id": "1006.5086", "submitter": "Xiaohui Xie", "authors": "Gui-Bo Ye and Xiaohui Xie", "title": "Split Bregman method for large scale fused Lasso", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO cs.LG math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  rdering of regression or classification coefficients occurs in many\nreal-world applications. Fused Lasso exploits this ordering by explicitly\nregularizing the differences between neighboring coefficients through an\n$\\ell_1$ norm regularizer. However, due to nonseparability and nonsmoothness of\nthe regularization term, solving the fused Lasso problem is computationally\ndemanding. Existing solvers can only deal with problems of small or medium\nsize, or a special case of the fused Lasso problem in which the predictor\nmatrix is identity matrix. In this paper, we propose an iterative algorithm\nbased on split Bregman method to solve a class of large-scale fused Lasso\nproblems, including a generalized fused Lasso and a fused Lasso support vector\nclassifier. We derive our algorithm using augmented Lagrangian method and prove\nits convergence properties. The performance of our method is tested on both\nartificial data and real-world applications including proteomic data from mass\nspectrometry and genomic data from array CGH. We demonstrate that our method is\nmany times faster than the existing solvers, and show that it is especially\nefficient for large p, small n problems.\n", "versions": [{"version": "v1", "created": "Sat, 26 Jun 2010 00:17:32 GMT"}], "update_date": "2010-06-29", "authors_parsed": [["Ye", "Gui-Bo", ""], ["Xie", "Xiaohui", ""]]}, {"id": "1006.5090", "submitter": "Vladimir Pestov", "authors": "Vladimir Pestov", "title": "PAC learnability of a concept class under non-atomic measures: a problem\n  by Vidyasagar", "comments": "14 pages, 1 figure, latex 2e with Springer macros", "journal-ref": "Lect. Notes in Artificial Intelligence 6331, Springer, 2010, pp.\n  134-147", "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In response to a 1997 problem of M. Vidyasagar, we state a necessary and\nsufficient condition for distribution-free PAC learnability of a concept class\n$\\mathscr C$ under the family of all non-atomic (diffuse) measures on the\ndomain $\\Omega$. Clearly, finiteness of the classical Vapnik-Chervonenkis\ndimension of $\\mathscr C$ is a sufficient, but no longer necessary, condition.\nBesides, learnability of $\\mathscr C$ under non-atomic measures does not imply\nthe uniform Glivenko-Cantelli property with regard to non-atomic measures. Our\nlearnability criterion is stated in terms of a combinatorial parameter\n$\\VC({\\mathscr C}\\,{\\mathrm{mod}}\\,\\omega_1)$ which we call the VC dimension of\n$\\mathscr C$ modulo countable sets. The new parameter is obtained by\n``thickening up'' single points in the definition of VC dimension to\nuncountable ``clusters''. Equivalently, $\\VC(\\mathscr C\\modd\\omega_1)\\leq d$ if\nand only if every countable subclass of $\\mathscr C$ has VC dimension $\\leq d$\noutside a countable subset of $\\Omega$. The new parameter can be also expressed\nas the classical VC dimension of $\\mathscr C$ calculated on a suitable subset\nof a compactification of $\\Omega$. We do not make any measurability assumptions\non $\\mathscr C$, assuming instead the validity of Martin's Axiom (MA).\n", "versions": [{"version": "v1", "created": "Sat, 26 Jun 2010 01:44:57 GMT"}], "update_date": "2010-11-08", "authors_parsed": [["Pestov", "Vladimir", ""]]}, {"id": "1006.5188", "submitter": "Nicola Di Mauro", "authors": "Nicola Di Mauro and Teresa M.A. Basile and Stefano Ferilli and\n  Floriana Esposito", "title": "Feature Construction for Relational Sequence Learning", "comments": "15 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We tackle the problem of multi-class relational sequence learning using\nrelevant patterns discovered from a set of labelled sequences. To deal with\nthis problem, firstly each relational sequence is mapped into a feature vector\nusing the result of a feature construction method. Since, the efficacy of\nsequence learning algorithms strongly depends on the features used to represent\nthe sequences, the second step is to find an optimal subset of the constructed\nfeatures leading to high classification accuracy. This feature selection task\nhas been solved adopting a wrapper approach that uses a stochastic local search\nalgorithm embedding a naive Bayes classifier. The performance of the proposed\nmethod applied to a real-world dataset shows an improvement when compared to\nother established methods, such as hidden Markov models, Fisher kernels and\nconditional random fields for relational sequences.\n", "versions": [{"version": "v1", "created": "Sun, 27 Jun 2010 08:56:11 GMT"}], "update_date": "2010-06-29", "authors_parsed": [["Di Mauro", "Nicola", ""], ["Basile", "Teresa M. A.", ""], ["Ferilli", "Stefano", ""], ["Esposito", "Floriana", ""]]}, {"id": "1006.5261", "submitter": "Madjid Khalilian", "authors": "Madjid Khalilian, Norwati Mustapha", "title": "Data Stream Clustering: Challenges and Issues", "comments": "IMECS2010", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Very large databases are required to store massive amounts of data that are\ncontinuously inserted and queried. Analyzing huge data sets and extracting\nvaluable pattern in many applications are interesting for researchers. We can\nidentify two main groups of techniques for huge data bases mining. One group\nrefers to streaming data and applies mining techniques whereas second group\nattempts to solve this problem directly with efficient algorithms. Recently\nmany researchers have focused on data stream as an efficient strategy against\nhuge data base mining instead of mining on entire data base. The main problem\nin data stream mining means evolving data is more difficult to detect in this\ntechniques therefore unsupervised methods should be applied. However,\nclustering techniques can lead us to discover hidden information. In this\nsurvey, we try to clarify: first, the different problem definitions related to\ndata stream clustering in general; second, the specific difficulties\nencountered in this field of research; third, the varying assumptions,\nheuristics, and intuitions forming the basis of different approaches; and how\nseveral prominent solutions tackle different problems. Index Terms- Data\nStream, Clustering, K-Means, Concept drift\n", "versions": [{"version": "v1", "created": "Mon, 28 Jun 2010 04:02:17 GMT"}], "update_date": "2010-06-29", "authors_parsed": [["Khalilian", "Madjid", ""], ["Mustapha", "Norwati", ""]]}, {"id": "1006.5278", "submitter": "William Nzoukou", "authors": "Dhoha Almazro and Ghadeer Shahatah and Lamia Albdulkarim and Mona\n  Kherees and Romy Martinez and William Nzoukou", "title": "A Survey Paper on Recommender Systems", "comments": "This paper has some typos in it", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recommender systems apply data mining techniques and prediction algorithms to\npredict users' interest on information, products and services among the\ntremendous amount of available items. The vast growth of information on the\nInternet as well as number of visitors to websites add some key challenges to\nrecommender systems. These are: producing accurate recommendation, handling\nmany recommendations efficiently and coping with the vast growth of number of\nparticipants in the system. Therefore, new recommender system technologies are\nneeded that can quickly produce high quality recommendations even for huge data\nsets.\n  To address these issues we have explored several collaborative filtering\ntechniques such as the item based approach, which identify relationship between\nitems and indirectly compute recommendations for users based on these\nrelationships. The user based approach was also studied, it identifies\nrelationships between users of similar tastes and computes recommendations\nbased on these relationships.\n  In this paper, we introduce the topic of recommender system. It provides ways\nto evaluate efficiency, scalability and accuracy of recommender system. The\npaper also analyzes different algorithms of user based and item based\ntechniques for recommendation generation. Moreover, a simple experiment was\nconducted using a data mining application -Weka- to apply data mining\nalgorithms to recommender system. We conclude by proposing our approach that\nmight enhance the quality of recommender systems.\n", "versions": [{"version": "v1", "created": "Mon, 28 Jun 2010 07:20:28 GMT"}, {"version": "v2", "created": "Wed, 30 Jun 2010 07:37:17 GMT"}, {"version": "v3", "created": "Mon, 13 Sep 2010 04:16:30 GMT"}, {"version": "v4", "created": "Fri, 24 Dec 2010 07:22:48 GMT"}], "update_date": "2016-11-25", "authors_parsed": [["Almazro", "Dhoha", ""], ["Shahatah", "Ghadeer", ""], ["Albdulkarim", "Lamia", ""], ["Kherees", "Mona", ""], ["Martinez", "Romy", ""], ["Nzoukou", "William", ""]]}, {"id": "1006.5367", "submitter": "J\\'er\\^ome Kunegis", "authors": "J\\'er\\^ome Kunegis and Ernesto W. De Luca and Sahin Albayrak", "title": "The Link Prediction Problem in Bipartite Networks", "comments": "10 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We define and study the link prediction problem in bipartite networks,\nspecializing general link prediction algorithms to the bipartite case. In a\ngraph, a link prediction function of two vertices denotes the similarity or\nproximity of the vertices. Common link prediction functions for general graphs\nare defined using paths of length two between two nodes. Since in a bipartite\ngraph adjacency vertices can only be connected by paths of odd lengths, these\nfunctions do not apply to bipartite graphs. Instead, a certain class of graph\nkernels (spectral transformation kernels) can be generalized to bipartite\ngraphs when the positive-semidefinite kernel constraint is relaxed. This\ngeneralization is realized by the odd component of the underlying spectral\ntransformation. This construction leads to several new link prediction\npseudokernels such as the matrix hyperbolic sine, which we examine for rating\ngraphs, authorship graphs, folksonomies, document--feature networks and other\ntypes of bipartite networks.\n", "versions": [{"version": "v1", "created": "Mon, 28 Jun 2010 14:40:37 GMT"}], "update_date": "2010-07-27", "authors_parsed": [["Kunegis", "J\u00e9r\u00f4me", ""], ["De Luca", "Ernesto W.", ""], ["Albayrak", "Sahin", ""]]}]