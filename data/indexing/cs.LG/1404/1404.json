[{"id": "1404.0086", "submitter": "EPTCS", "authors": "Mario Benevides (Federal University of Rio de Janeiro), Isaque Lima\n  (Federal University of Rio de Janeiro), Rafael Nader (Federal University of\n  Rio de Janeiro), Pedro Rougemont (Federal University of Rio de Janeiro)", "title": "Using HMM in Strategic Games", "comments": "In Proceedings DCM 2013, arXiv:1403.7685", "journal-ref": "EPTCS 144, 2014, pp. 73-84", "doi": "10.4204/EPTCS.144.6", "report-no": null, "categories": "cs.GT cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we describe an approach to resolve strategic games in which\nplayers can assume different types along the game. Our goal is to infer which\ntype the opponent is adopting at each moment so that we can increase the\nplayer's odds. To achieve that we use Markov games combined with hidden Markov\nmodel. We discuss a hypothetical example of a tennis game whose solution can be\napplied to any game with similar characteristics.\n", "versions": [{"version": "v1", "created": "Tue, 1 Apr 2014 00:39:19 GMT"}], "update_date": "2014-04-02", "authors_parsed": [["Benevides", "Mario", "", "Federal University of Rio de Janeiro"], ["Lima", "Isaque", "", "Federal University of Rio de Janeiro"], ["Nader", "Rafael", "", "Federal University of\n  Rio de Janeiro"], ["Rougemont", "Pedro", "", "Federal University of Rio de Janeiro"]]}, {"id": "1404.0138", "submitter": "Shusen Wang", "authors": "Shusen Wang, Zhihua Zhang", "title": "Efficient Algorithms and Error Analysis for the Modified Nystrom Method", "comments": "9-page paper plus appendix. In Proceedings of the 17th International\n  Conference on Artificial Intelligence and Statistics (AISTATS) 2014,\n  Reykjavik, Iceland. JMLR: W&CP volume 33", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many kernel methods suffer from high time and space complexities and are thus\nprohibitive in big-data applications. To tackle the computational challenge,\nthe Nystr\\\"om method has been extensively used to reduce time and space\ncomplexities by sacrificing some accuracy. The Nystr\\\"om method speedups\ncomputation by constructing an approximation of the kernel matrix using only a\nfew columns of the matrix. Recently, a variant of the Nystr\\\"om method called\nthe modified Nystr\\\"om method has demonstrated significant improvement over the\nstandard Nystr\\\"om method in approximation accuracy, both theoretically and\nempirically.\n  In this paper, we propose two algorithms that make the modified Nystr\\\"om\nmethod practical. First, we devise a simple column selection algorithm with a\nprovable error bound. Our algorithm is more efficient and easier to implement\nthan and nearly as accurate as the state-of-the-art algorithm. Second, with the\nselected columns at hand, we propose an algorithm that computes the\napproximation in lower time complexity than the approach in the previous work.\nFurthermore, we prove that the modified Nystr\\\"om method is exact under certain\nconditions, and we establish a lower error bound for the modified Nystr\\\"om\nmethod.\n", "versions": [{"version": "v1", "created": "Tue, 1 Apr 2014 06:26:55 GMT"}], "update_date": "2014-04-02", "authors_parsed": [["Wang", "Shusen", ""], ["Zhang", "Zhihua", ""]]}, {"id": "1404.0200", "submitter": "Andreas Veit", "authors": "Andreas Veit, Christoph Goebel, Rohit Tidke, Christoph Doblander and\n  Hans-Arno Jacobsen", "title": "Household Electricity Demand Forecasting -- Benchmarking\n  State-of-the-Art Methods", "comments": "Technical Report", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The increasing use of renewable energy sources with variable output, such as\nsolar photovoltaic and wind power generation, calls for Smart Grids that\neffectively manage flexible loads and energy storage. The ability to forecast\nconsumption at different locations in distribution systems will be a key\ncapability of Smart Grids. The goal of this paper is to benchmark\nstate-of-the-art methods for forecasting electricity demand on the household\nlevel across different granularities and time scales in an explorative way,\nthereby revealing potential shortcomings and find promising directions for\nfuture research in this area. We apply a number of forecasting methods\nincluding ARIMA, neural networks, and exponential smoothening using several\nstrategies for training data selection, in particular day type and sliding\nwindow based strategies. We consider forecasting horizons ranging between 15\nminutes and 24 hours. Our evaluation is based on two data sets containing the\npower usage of individual appliances at second time granularity collected over\nthe course of several months. The results indicate that forecasting accuracy\nvaries significantly depending on the choice of forecasting methods/strategy\nand the parameter configuration. Measured by the Mean Absolute Percentage Error\n(MAPE), the considered state-of-the-art forecasting methods rarely beat\ncorresponding persistence forecasts. Overall, we observed MAPEs in the range\nbetween 5 and >100%. The average MAPE for the first data set was ~30%, while it\nwas ~85% for the other data set. These results show big room for improvement.\nBased on the identified trends and experiences from our experiments, we\ncontribute a detailed discussion of promising future research.\n", "versions": [{"version": "v1", "created": "Tue, 1 Apr 2014 11:32:53 GMT"}], "update_date": "2014-04-02", "authors_parsed": [["Veit", "Andreas", ""], ["Goebel", "Christoph", ""], ["Tidke", "Rohit", ""], ["Doblander", "Christoph", ""], ["Jacobsen", "Hans-Arno", ""]]}, {"id": "1404.0334", "submitter": "Menglong Zhu", "authors": "Menglong Zhu, Nikolay Atanasov, George J. Pappas, Kostas Daniilidis", "title": "Active Deformable Part Models", "comments": "9 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents an active approach for part-based object detection, which\noptimizes the order of part filter evaluations and the time at which to stop\nand make a prediction. Statistics, describing the part responses, are learned\nfrom training data and are used to formalize the part scheduling problem as an\noffline optimization. Dynamic programming is applied to obtain a policy, which\nbalances the number of part evaluations with the classification accuracy.\nDuring inference, the policy is used as a look-up table to choose the part\norder and the stopping time based on the observed filter responses. The method\nis faster than cascade detection with deformable part models (which does not\noptimize the part order) with negligible loss in accuracy when evaluated on the\nPASCAL VOC 2007 and 2010 datasets.\n", "versions": [{"version": "v1", "created": "Tue, 1 Apr 2014 18:07:58 GMT"}, {"version": "v2", "created": "Wed, 2 Apr 2014 19:00:29 GMT"}], "update_date": "2014-04-03", "authors_parsed": [["Zhu", "Menglong", ""], ["Atanasov", "Nikolay", ""], ["Pappas", "George J.", ""], ["Daniilidis", "Kostas", ""]]}, {"id": "1404.0400", "submitter": "Georgios Evangelopoulos", "authors": "Chiyuan Zhang, Georgios Evangelopoulos, Stephen Voinea, Lorenzo\n  Rosasco, Tomaso Poggio", "title": "A Deep Representation for Invariance And Music Classification", "comments": "5 pages, CBMM Memo No. 002, (to appear) IEEE 2014 International\n  Conference on Acoustics, Speech, and Signal Processing (ICASSP 2014)", "journal-ref": null, "doi": "10.1109/ICASSP.2014.6854954", "report-no": "CBMM Memo No. 002", "categories": "cs.SD cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Representations in the auditory cortex might be based on mechanisms similar\nto the visual ventral stream; modules for building invariance to\ntransformations and multiple layers for compositionality and selectivity. In\nthis paper we propose the use of such computational modules for extracting\ninvariant and discriminative audio representations. Building on a theory of\ninvariance in hierarchical architectures, we propose a novel, mid-level\nrepresentation for acoustical signals, using the empirical distributions of\nprojections on a set of templates and their transformations. Under the\nassumption that, by construction, this dictionary of templates is composed from\nsimilar classes, and samples the orbit of variance-inducing signal\ntransformations (such as shift and scale), the resulting signature is\ntheoretically guaranteed to be unique, invariant to transformations and stable\nto deformations. Modules of projection and pooling can then constitute layers\nof deep networks, for learning composite representations. We present the main\ntheoretical and computational aspects of a framework for unsupervised learning\nof invariant audio representations, empirically evaluated on music genre\nclassification.\n", "versions": [{"version": "v1", "created": "Tue, 1 Apr 2014 21:15:32 GMT"}], "update_date": "2016-11-17", "authors_parsed": [["Zhang", "Chiyuan", ""], ["Evangelopoulos", "Georgios", ""], ["Voinea", "Stephen", ""], ["Rosasco", "Lorenzo", ""], ["Poggio", "Tomaso", ""]]}, {"id": "1404.0427", "submitter": "Peter Banda", "authors": "Peter Banda, Christof Teuscher", "title": "Learning Two-input Linear and Nonlinear Analog Functions with a Simple\n  Chemical System", "comments": "13 pages, 4 figures, 2 tables", "journal-ref": "Lecture Notes in Computer Science, 8553, 14-26 (2014)", "doi": "10.1007/978-3-319-08123-6_2", "report-no": null, "categories": "q-bio.MN cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The current biochemical information processing systems behave in a\npredetermined manner because all features are defined during the design phase.\nTo make such unconventional computing systems reusable and programmable for\nbiomedical applications, adaptation, learning, and self-modification based on\nexternal stimuli would be highly desirable. However, so far, it has been too\nchallenging to implement these in wet chemistries. In this paper we extend the\nchemical perceptron, a model previously proposed by the authors, to function as\nan analog instead of a binary system. The new analog asymmetric signal\nperceptron learns through feedback and supports Michaelis-Menten kinetics. The\nresults show that our perceptron is able to learn linear and nonlinear\n(quadratic) functions of two inputs. To the best of our knowledge, it is the\nfirst simulated chemical system capable of doing so. The small number of\nspecies and reactions and their simplicity allows for a mapping to an actual\nwet implementation using DNA-strand displacement or deoxyribozymes. Our results\nare an important step toward actual biochemical systems that can learn and\nadapt.\n", "versions": [{"version": "v1", "created": "Wed, 2 Apr 2014 01:00:48 GMT"}, {"version": "v2", "created": "Sun, 13 Jul 2014 04:18:00 GMT"}], "update_date": "2014-07-15", "authors_parsed": [["Banda", "Peter", ""], ["Teuscher", "Christof", ""]]}, {"id": "1404.0453", "submitter": "Kiran Sree Pokkuluri Prof", "authors": "Pokkuluri Kiran Sree, Inampudi Ramesh Babu, SSSN Usha Devi N", "title": "Cellular Automata and Its Applications in Bioinformatics: A Review", "comments": null, "journal-ref": "Global Perspectives on Artificial Intelligence (GPAI) Volume 2\n  Issue 2, Pages 16-22 April 2014", "doi": null, "report-no": null, "categories": "cs.CE cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper aims at providing a survey on the problems that can be easily\naddressed by cellular automata in bioinformatics. Some of the authors have\nproposed algorithms for addressing some problems in bioinformatics but the\napplication of cellular automata in bioinformatics is a virgin field in\nresearch. None of the researchers has tried to relate the major problems in\nbioinformatics and find a common solution. Extensive literature surveys were\nconducted. We have considered some papers in various journals and conferences\nfor conduct of our research. This paper provides intuition towards relating\nvarious problems in bioinformatics logically and tries to attain a common frame\nwork for addressing the same.\n", "versions": [{"version": "v1", "created": "Wed, 2 Apr 2014 04:18:06 GMT"}], "update_date": "2014-04-03", "authors_parsed": [["Sree", "Pokkuluri Kiran", ""], ["Babu", "Inampudi Ramesh", ""], ["N", "SSSN Usha Devi", ""]]}, {"id": "1404.0466", "submitter": "Da Kuang", "authors": "Da Kuang, Alex Gittens, Raffay Hamid", "title": "piCholesky: Polynomial Interpolation of Multiple Cholesky Factors for\n  Efficient Approximate Cross-Validation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The dominant cost in solving least-square problems using Newton's method is\noften that of factorizing the Hessian matrix over multiple values of the\nregularization parameter ($\\lambda$). We propose an efficient way to\ninterpolate the Cholesky factors of the Hessian matrix computed over a small\nset of $\\lambda$ values. This approximation enables us to optimally minimize\nthe hold-out error while incurring only a fraction of the cost compared to\nexact cross-validation. We provide a formal error bound for our approximation\nscheme and present solutions to a set of key implementation challenges that\nallow our approach to maximally exploit the compute power of modern\narchitectures. We present a thorough empirical analysis over multiple datasets\nto show the effectiveness of our approach.\n", "versions": [{"version": "v1", "created": "Wed, 2 Apr 2014 05:33:41 GMT"}, {"version": "v2", "created": "Wed, 10 Jun 2015 18:20:16 GMT"}], "update_date": "2015-06-11", "authors_parsed": [["Kuang", "Da", ""], ["Gittens", "Alex", ""], ["Hamid", "Raffay", ""]]}, {"id": "1404.0649", "submitter": "Rafael Villanueva", "authors": "Juan-Carlos Cort\\'es, Francisco-J. Santonja, Ana-C. Tarazona,\n  Rafael-J. Villanueva, Javier Villanueva-Oller", "title": "A probabilistic estimation and prediction technique for dynamic\n  continuous social science models: The evolution of the attitude of the Basque\n  Country population towards ETA as a case study", "comments": "13 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present a computational technique to deal with uncertainty\nin dynamic continuous models in Social Sciences. Considering data from surveys,\nthe method consists of determining the probability distribution of the survey\noutput and this allows to sample data and fit the model to the sampled data\nusing a goodness-of-fit criterion based on the chi-square-test. Taking the\nfitted parameters non-rejected by the chi-square-test, substituting them into\nthe model and computing their outputs, we build 95% confidence intervals in\neach time instant capturing uncertainty of the survey data (probabilistic\nestimation). Using the same set of obtained model parameters, we also provide a\nprediction over the next few years with 95% confidence intervals (probabilistic\nprediction). This technique is applied to a dynamic social model describing the\nevolution of the attitude of the Basque Country population towards the\nrevolutionary organization ETA.\n", "versions": [{"version": "v1", "created": "Sun, 30 Mar 2014 20:49:33 GMT"}], "update_date": "2014-04-03", "authors_parsed": [["Cort\u00e9s", "Juan-Carlos", ""], ["Santonja", "Francisco-J.", ""], ["Tarazona", "Ana-C.", ""], ["Villanueva", "Rafael-J.", ""], ["Villanueva-Oller", "Javier", ""]]}, {"id": "1404.0736", "submitter": "Emily Denton", "authors": "Emily Denton, Wojciech Zaremba, Joan Bruna, Yann LeCun, Rob Fergus", "title": "Exploiting Linear Structure Within Convolutional Networks for Efficient\n  Evaluation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present techniques for speeding up the test-time evaluation of large\nconvolutional networks, designed for object recognition tasks. These models\ndeliver impressive accuracy but each image evaluation requires millions of\nfloating point operations, making their deployment on smartphones and\nInternet-scale clusters problematic. The computation is dominated by the\nconvolution operations in the lower layers of the model. We exploit the linear\nstructure present within the convolutional filters to derive approximations\nthat significantly reduce the required computation. Using large\nstate-of-the-art models, we demonstrate we demonstrate speedups of\nconvolutional layers on both CPU and GPU by a factor of 2x, while keeping the\naccuracy within 1% of the original model.\n", "versions": [{"version": "v1", "created": "Wed, 2 Apr 2014 23:31:12 GMT"}, {"version": "v2", "created": "Mon, 9 Jun 2014 15:53:55 GMT"}], "update_date": "2014-06-10", "authors_parsed": [["Denton", "Emily", ""], ["Zaremba", "Wojciech", ""], ["Bruna", "Joan", ""], ["LeCun", "Yann", ""], ["Fergus", "Rob", ""]]}, {"id": "1404.0751", "submitter": "Akshay Krishnamurthy", "authors": "Akshay Krishnamurthy, Martin Azizyan, Aarti Singh", "title": "Subspace Learning from Extremely Compressed Measurements", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider learning the principal subspace of a large set of vectors from an\nextremely small number of compressive measurements of each vector. Our\ntheoretical results show that even a constant number of measurements per column\nsuffices to approximate the principal subspace to arbitrary precision, provided\nthat the number of vectors is large. This result is achieved by a simple\nalgorithm that computes the eigenvectors of an estimate of the covariance\nmatrix. The main insight is to exploit an averaging effect that arises from\napplying a different random projection to each vector. We provide a number of\nsimulations confirming our theoretical results.\n", "versions": [{"version": "v1", "created": "Thu, 3 Apr 2014 02:58:37 GMT"}, {"version": "v2", "created": "Mon, 12 Dec 2016 15:36:21 GMT"}], "update_date": "2016-12-13", "authors_parsed": [["Krishnamurthy", "Akshay", ""], ["Azizyan", "Martin", ""], ["Singh", "Aarti", ""]]}, {"id": "1404.0789", "submitter": "Oscar Stiffelman", "authors": "Oscar Stiffelman", "title": "The Least Wrong Model Is Not in the Data", "comments": "added citations and acknowledgements, and replaced the ideal model\n  section with a more intuitive argument", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The true process that generated data cannot be determined when multiple\nexplanations are possible. Prediction requires a model of the probability that\na process, chosen randomly from the set of candidate explanations, generates\nsome future observation. The best model includes all of the information\ncontained in the minimal description of the data that is not contained in the\ndata. It is closely related to the Halting Problem and is logarithmic in the\nsize of the data. Prediction is difficult because the ideal model is not\ncomputable, and the best computable model is not \"findable.\" However, the error\nfrom any approximation can be bounded by the size of the description using the\nmodel.\n", "versions": [{"version": "v1", "created": "Thu, 3 Apr 2014 07:41:46 GMT"}, {"version": "v2", "created": "Fri, 4 Apr 2014 08:58:30 GMT"}, {"version": "v3", "created": "Thu, 17 Apr 2014 19:53:46 GMT"}], "update_date": "2014-04-18", "authors_parsed": [["Stiffelman", "Oscar", ""]]}, {"id": "1404.0933", "submitter": "Vikram Kumar allakonda", "authors": "Vikramkumar (B092633), Vijaykumar B (B091956), Trilochan (B092654)", "title": "Bayes and Naive Bayes Classifier", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  The Bayesian Classification represents a supervised learning method as well\nas a statistical method for classification. Assumes an underlying probabilistic\nmodel and it allows us to capture uncertainty about the model in a principled\nway by determining probabilities of the outcomes. This Classification is named\nafter Thomas Bayes (1702-1761), who proposed the Bayes Theorem. Bayesian\nclassification provides practical learning algorithms and prior knowledge and\nobserved data can be combined. Bayesian Classification provides a useful\nperspective for understanding and evaluating many learning algorithms. It\ncalculates explicit probabilities for hypothesis and it is robust to noise in\ninput data. In statistical classification the Bayes classifier minimises the\nprobability of misclassification. That was a visual intuition for a simple case\nof the Bayes classifier, also called: 1)Idiot Bayes 2)Naive Bayes 3)Simple\nBayes\n", "versions": [{"version": "v1", "created": "Thu, 3 Apr 2014 14:34:47 GMT"}], "update_date": "2014-04-04", "authors_parsed": [["Vikramkumar", "", "", "B092633"], ["B", "Vijaykumar", "", "B091956"], ["Trilochan", "", "", "B092654"]]}, {"id": "1404.0979", "submitter": "Martin Kasparick", "authors": "Martin Kasparick, Renato L. G. Cavalcante, Stefan Valentin, Slawomir\n  Stanczak, Masahiro Yukawa", "title": "Kernel-Based Adaptive Online Reconstruction of Coverage Maps With Side\n  Information", "comments": "IEEE Transactions on Vehicular Technology; revised and extended\n  version with new simulation scenario", "journal-ref": null, "doi": "10.1109/TVT.2015.2453391", "report-no": null, "categories": "cs.NI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we address the problem of reconstructing coverage maps from\npath-loss measurements in cellular networks. We propose and evaluate two\nkernel-based adaptive online algorithms as an alternative to typical offline\nmethods. The proposed algorithms are application-tailored extensions of\npowerful iterative methods such as the adaptive projected subgradient method\nand a state-of-the-art adaptive multikernel method. Assuming that the moving\ntrajectories of users are available, it is shown how side information can be\nincorporated in the algorithms to improve their convergence performance and the\nquality of the estimation. The complexity is significantly reduced by imposing\nsparsity-awareness in the sense that the algorithms exploit the compressibility\nof the measurement data to reduce the amount of data which is saved and\nprocessed. Finally, we present extensive simulations based on realistic data to\nshow that our algorithms provide fast, robust estimates of coverage maps in\nreal-world scenarios. Envisioned applications include path-loss prediction\nalong trajectories of mobile users as a building block for anticipatory\nbuffering or traffic offloading.\n", "versions": [{"version": "v1", "created": "Thu, 3 Apr 2014 15:46:54 GMT"}, {"version": "v2", "created": "Thu, 28 Aug 2014 16:13:59 GMT"}, {"version": "v3", "created": "Wed, 20 May 2015 07:50:01 GMT"}, {"version": "v4", "created": "Thu, 10 Oct 2019 17:56:35 GMT"}], "update_date": "2019-10-11", "authors_parsed": [["Kasparick", "Martin", ""], ["Cavalcante", "Renato L. G.", ""], ["Valentin", "Stefan", ""], ["Stanczak", "Slawomir", ""], ["Yukawa", "Masahiro", ""]]}, {"id": "1404.1066", "submitter": "Stephen Tyree", "authors": "Stephen Tyree, Jacob R. Gardner, Kilian Q. Weinberger, Kunal Agrawal,\n  John Tran", "title": "Parallel Support Vector Machines in Practice", "comments": "10 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we evaluate the performance of various parallel optimization\nmethods for Kernel Support Vector Machines on multicore CPUs and GPUs. In\nparticular, we provide the first comparison of algorithms with explicit and\nimplicit parallelization. Most existing parallel implementations for multi-core\nor GPU architectures are based on explicit parallelization of Sequential\nMinimal Optimization (SMO)---the programmers identified parallelizable\ncomponents and hand-parallelized them, specifically tuned for a particular\narchitecture. We compare these approaches with each other and with implicitly\nparallelized algorithms---where the algorithm is expressed such that most of\nthe work is done within few iterations with large dense linear algebra\noperations. These can be computed with highly-optimized libraries, that are\ncarefully parallelized for a large variety of parallel platforms. We highlight\nthe advantages and disadvantages of both approaches and compare them on various\nbenchmark data sets. We find an approximate implicitly parallel algorithm which\nis surprisingly efficient, permits a much simpler implementation, and leads to\nunprecedented speedups in SVM training.\n", "versions": [{"version": "v1", "created": "Thu, 3 Apr 2014 19:49:57 GMT"}], "update_date": "2014-04-04", "authors_parsed": [["Tyree", "Stephen", ""], ["Gardner", "Jacob R.", ""], ["Weinberger", "Kilian Q.", ""], ["Agrawal", "Kunal", ""], ["Tran", "John", ""]]}, {"id": "1404.1100", "submitter": "Jonathon Shlens", "authors": "Jonathon Shlens", "title": "A Tutorial on Principal Component Analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Principal component analysis (PCA) is a mainstay of modern data analysis - a\nblack box that is widely used but (sometimes) poorly understood. The goal of\nthis paper is to dispel the magic behind this black box. This manuscript\nfocuses on building a solid intuition for how and why principal component\nanalysis works. This manuscript crystallizes this knowledge by deriving from\nsimple intuitions, the mathematics behind PCA. This tutorial does not shy away\nfrom explaining the ideas informally, nor does it shy away from the\nmathematics. The hope is that by addressing both aspects, readers of all levels\nwill be able to gain a better understanding of PCA as well as the when, the how\nand the why of applying this technique.\n", "versions": [{"version": "v1", "created": "Thu, 3 Apr 2014 21:16:49 GMT"}], "update_date": "2014-04-07", "authors_parsed": [["Shlens", "Jonathon", ""]]}, {"id": "1404.1140", "submitter": "Christopher Amato", "authors": "Christopher Amato, Frans A. Oliehoek", "title": "Scalable Planning and Learning for Multiagent POMDPs: Extended Version", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Online, sample-based planning algorithms for POMDPs have shown great promise\nin scaling to problems with large state spaces, but they become intractable for\nlarge action and observation spaces. This is particularly problematic in\nmultiagent POMDPs where the action and observation space grows exponentially\nwith the number of agents. To combat this intractability, we propose a novel\nscalable approach based on sample-based planning and factored value functions\nthat exploits structure present in many multiagent settings. This approach\napplies not only in the planning case, but also in the Bayesian reinforcement\nlearning setting. Experimental results show that we are able to provide high\nquality solutions to large multiagent planning and learning problems.\n", "versions": [{"version": "v1", "created": "Fri, 4 Apr 2014 03:02:44 GMT"}, {"version": "v2", "created": "Sat, 20 Dec 2014 03:28:34 GMT"}], "update_date": "2014-12-23", "authors_parsed": [["Amato", "Christopher", ""], ["Oliehoek", "Frans A.", ""]]}, {"id": "1404.1144", "submitter": "Kiran Sree Pokkuluri Prof", "authors": "Pokkuluri Kiran Sree, Inampudi Ramesh Babu, SSSN Usha Devi N", "title": "AIS-MACA- Z: MACA based Clonal Classifier for Splicing Site, Protein\n  Coding and Promoter Region Identification in Eukaryotes", "comments": "6,1-6 Pages, Journal of Artificial Intelligence Research & Advances\n  Volume 1, Issue 1,2014. arXiv admin note: text overlap with arXiv:1403.5933,\n  arXiv:1404.0453", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CE cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bioinformatics incorporates information regarding biological data storage,\naccessing mechanisms and presentation of characteristics within this data. Most\nof the problems in bioinformatics and be addressed efficiently by computer\ntechniques. This paper aims at building a classifier based on Multiple\nAttractor Cellular Automata (MACA) which uses fuzzy logic with version Z to\npredict splicing site, protein coding and promoter region identification in\neukaryotes. It is strengthened with an artificial immune system technique\n(AIS), Clonal algorithm for choosing rules of best fitness. The proposed\nclassifier can handle DNA sequences of lengths 54,108,162,252,354. This\nclassifier gives the exact boundaries of both protein and promoter regions with\nan average accuracy of 90.6%. This classifier can predict the splicing site\nwith 97% accuracy. This classifier was tested with 1, 97,000 data components\nwhich were taken from Fickett & Toung , EPDnew, and other sequences from a\nrenowned medical university.\n", "versions": [{"version": "v1", "created": "Fri, 4 Apr 2014 03:31:29 GMT"}], "update_date": "2014-04-07", "authors_parsed": [["Sree", "Pokkuluri Kiran", ""], ["Babu", "Inampudi Ramesh", ""], ["N", "SSSN Usha Devi", ""]]}, {"id": "1404.1282", "submitter": "Dongwoo Kim", "authors": "Dongwoo Kim, Alice Oh", "title": "Hierarchical Dirichlet Scaling Process", "comments": null, "journal-ref": null, "doi": "10.1007/s10994-016-5621-5", "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present the \\textit{hierarchical Dirichlet scaling process} (HDSP), a\nBayesian nonparametric mixed membership model. The HDSP generalizes the\nhierarchical Dirichlet process (HDP) to model the correlation structure between\nmetadata in the corpus and mixture components. We construct the HDSP based on\nthe normalized gamma representation of the Dirichlet process, and this\nconstruction allows incorporating a scaling function that controls the\nmembership probabilities of the mixture components. We develop two scaling\nmethods to demonstrate that different modeling assumptions can be expressed in\nthe HDSP. We also derive the corresponding approximate posterior inference\nalgorithms using variational Bayes. Through experiments on datasets of\nnewswire, medical journal articles, conference proceedings, and product\nreviews, we show that the HDSP results in a better predictive performance than\nlabeled LDA, partially labeled LDA, and author topic model and a better\nnegative review classification performance than the supervised topic model and\nSVM.\n", "versions": [{"version": "v1", "created": "Sat, 22 Mar 2014 06:25:51 GMT"}, {"version": "v2", "created": "Mon, 12 May 2014 02:59:57 GMT"}, {"version": "v3", "created": "Wed, 11 Feb 2015 05:17:27 GMT"}], "update_date": "2017-07-10", "authors_parsed": [["Kim", "Dongwoo", ""], ["Oh", "Alice", ""]]}, {"id": "1404.1333", "submitter": "Li Li", "authors": "Li Li, John C. Snyder, Isabelle M. Pelaschier, Jessica Huang,\n  Uma-Naresh Niranjan, Paul Duncan, Matthias Rupp, Klaus-Robert M\\\"uller,\n  Kieron Burke", "title": "Understanding Machine-learned Density Functionals", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.chem-ph cs.LG physics.comp-ph stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Kernel ridge regression is used to approximate the kinetic energy of\nnon-interacting fermions in a one-dimensional box as a functional of their\ndensity. The properties of different kernels and methods of cross-validation\nare explored, and highly accurate energies are achieved. Accurate {\\em\nconstrained optimal densities} are found via a modified Euler-Lagrange\nconstrained minimization of the total energy. A projected gradient descent\nalgorithm is derived using local principal component analysis. Additionally, a\nsparse grid representation of the density can be used without degrading the\nperformance of the methods. The implications for machine-learned density\nfunctional approximations are discussed.\n", "versions": [{"version": "v1", "created": "Fri, 4 Apr 2014 18:20:23 GMT"}, {"version": "v2", "created": "Tue, 27 May 2014 01:23:13 GMT"}], "update_date": "2014-05-28", "authors_parsed": [["Li", "Li", ""], ["Snyder", "John C.", ""], ["Pelaschier", "Isabelle M.", ""], ["Huang", "Jessica", ""], ["Niranjan", "Uma-Naresh", ""], ["Duncan", "Paul", ""], ["Rupp", "Matthias", ""], ["M\u00fcller", "Klaus-Robert", ""], ["Burke", "Kieron", ""]]}, {"id": "1404.1356", "submitter": "Olivier Wintenberger", "authors": "Olivier Wintenberger (LSTA)", "title": "Optimal learning with Bernstein Online Aggregation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a new recursive aggregation procedure called Bernstein Online\nAggregation (BOA). The exponential weights include an accuracy term and a\nsecond order term that is a proxy of the quadratic variation as in Hazan and\nKale (2010). This second term stabilizes the procedure that is optimal in\ndifferent senses. We first obtain optimal regret bounds in the deterministic\ncontext. Then, an adaptive version is the first exponential weights algorithm\nthat exhibits a second order bound with excess losses that appears first in\nGaillard et al. (2014). The second order bounds in the deterministic context\nare extended to a general stochastic context using the cumulative predictive\nrisk. Such conversion provides the main result of the paper, an inequality of a\nnovel type comparing the procedure with any deterministic aggregation procedure\nfor an integrated criteria. Then we obtain an observable estimate of the excess\nof risk of the BOA procedure. To assert the optimality, we consider finally the\niid case for strongly convex and Lipschitz continuous losses and we prove that\nthe optimal rate of aggregation of Tsybakov (2003) is achieved. The batch\nversion of the BOA procedure is then the first adaptive explicit algorithm that\nsatisfies an optimal oracle inequality with high probability.\n", "versions": [{"version": "v1", "created": "Fri, 4 Apr 2014 19:33:55 GMT"}, {"version": "v2", "created": "Sun, 20 Apr 2014 19:26:23 GMT"}, {"version": "v3", "created": "Wed, 4 Feb 2015 17:58:04 GMT"}, {"version": "v4", "created": "Tue, 30 Aug 2016 06:44:14 GMT"}, {"version": "v5", "created": "Tue, 13 Sep 2016 14:23:48 GMT"}], "update_date": "2016-09-14", "authors_parsed": [["Wintenberger", "Olivier", "", "LSTA"]]}, {"id": "1404.1377", "submitter": "Zheng Wang", "authors": "Zheng Wang, Ming-Jun Lai, Zhaosong Lu, Wei Fan, Hasan Davulcu and\n  Jieping Ye", "title": "Orthogonal Rank-One Matrix Pursuit for Low Rank Matrix Completion", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.NA stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose an efficient and scalable low rank matrix\ncompletion algorithm. The key idea is to extend orthogonal matching pursuit\nmethod from the vector case to the matrix case. We further propose an economic\nversion of our algorithm by introducing a novel weight updating rule to reduce\nthe time and storage complexity. Both versions are computationally inexpensive\nfor each matrix pursuit iteration, and find satisfactory results in a few\niterations. Another advantage of our proposed algorithm is that it has only one\ntunable parameter, which is the rank. It is easy to understand and to use by\nthe user. This becomes especially important in large-scale learning problems.\nIn addition, we rigorously show that both versions achieve a linear convergence\nrate, which is significantly better than the previous known results. We also\nempirically compare the proposed algorithms with several state-of-the-art\nmatrix completion algorithms on many real-world datasets, including the\nlarge-scale recommendation dataset Netflix as well as the MovieLens datasets.\nNumerical results show that our proposed algorithm is more efficient than\ncompeting algorithms while achieving similar or better prediction performance.\n", "versions": [{"version": "v1", "created": "Fri, 4 Apr 2014 20:00:30 GMT"}, {"version": "v2", "created": "Wed, 16 Apr 2014 19:09:09 GMT"}], "update_date": "2014-04-17", "authors_parsed": [["Wang", "Zheng", ""], ["Lai", "Ming-Jun", ""], ["Lu", "Zhaosong", ""], ["Fan", "Wei", ""], ["Davulcu", "Hasan", ""], ["Ye", "Jieping", ""]]}, {"id": "1404.1491", "submitter": "Jayita Mitra", "authors": "Jayita Mitra and Diganta Saha", "title": "An Efficient Feature Selection in Classification of Audio Files", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we have focused on an efficient feature selection method in\nclassification of audio files. The main objective is feature selection and\nextraction. We have selected a set of features for further analysis, which\nrepresents the elements in feature vector. By extraction method we can compute\na numerical representation that can be used to characterize the audio using the\nexisting toolbox. In this study Gain Ratio (GR) is used as a feature selection\nmeasure. GR is used to select splitting attribute which will separate the\ntuples into different classes. The pulse clarity is considered as a subjective\nmeasure and it is used to calculate the gain of features of audio files. The\nsplitting criterion is employed in the application to identify the class or the\nmusic genre of a specific audio file from testing database. Experimental\nresults indicate that by using GR the application can produce a satisfactory\nresult for music genre classification. After dimensionality reduction best\nthree features have been selected out of various features of audio file and in\nthis technique we will get more than 90% successful classification result.\n", "versions": [{"version": "v1", "created": "Mon, 24 Mar 2014 16:05:26 GMT"}], "update_date": "2014-04-08", "authors_parsed": [["Mitra", "Jayita", ""], ["Saha", "Diganta", ""]]}, {"id": "1404.1492", "submitter": "James Brofos", "authors": "James Brofos", "title": "Ensemble Committees for Stock Return Classification and Prediction", "comments": "15 pages, 4 figures, Neukom Institute Computational Undergraduate\n  Research prize - second place", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper considers a portfolio trading strategy formulated by algorithms in\nthe field of machine learning. The profitability of the strategy is measured by\nthe algorithm's capability to consistently and accurately identify stock\nindices with positive or negative returns, and to generate a preferred\nportfolio allocation on the basis of a learned model. Stocks are characterized\nby time series data sets consisting of technical variables that reflect market\nconditions in a previous time interval, which are utilized produce binary\nclassification decisions in subsequent intervals. The learned model is\nconstructed as a committee of random forest classifiers, a non-linear support\nvector machine classifier, a relevance vector machine classifier, and a\nconstituent ensemble of k-nearest neighbors classifiers. The Global Industry\nClassification Standard (GICS) is used to explore the ensemble model's efficacy\nwithin the context of various fields of investment including Energy, Materials,\nFinancials, and Information Technology. Data from 2006 to 2012, inclusive, are\nconsidered, which are chosen for providing a range of market circumstances for\nevaluating the model. The model is observed to achieve an accuracy of\napproximately 70% when predicting stock price returns three months in advance.\n", "versions": [{"version": "v1", "created": "Sat, 5 Apr 2014 17:09:05 GMT"}], "update_date": "2014-04-08", "authors_parsed": [["Brofos", "James", ""]]}, {"id": "1404.1504", "submitter": "Steve Hanneke", "authors": "Yair Wiener, Steve Hanneke, Ran El-Yaniv", "title": "A Compression Technique for Analyzing Disagreement-Based Active Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a new and improved characterization of the label complexity of\ndisagreement-based active learning, in which the leading quantity is the\nversion space compression set size. This quantity is defined as the size of the\nsmallest subset of the training data that induces the same version space. We\nshow various applications of the new characterization, including a tight\nanalysis of CAL and refined label complexity bounds for linear separators under\nmixtures of Gaussians and axis-aligned rectangles under product densities. The\nversion space compression set size, as well as the new characterization of the\nlabel complexity, can be naturally extended to agnostic learning problems, for\nwhich we show new speedup results for two well known active learning\nalgorithms.\n", "versions": [{"version": "v1", "created": "Sat, 5 Apr 2014 18:58:12 GMT"}], "update_date": "2014-04-08", "authors_parsed": [["Wiener", "Yair", ""], ["Hanneke", "Steve", ""], ["El-Yaniv", "Ran", ""]]}, {"id": "1404.1521", "submitter": "Vivek Kulkarni", "authors": "Vivek Kulkarni, Rami Al-Rfou', Bryan Perozzi, Steven Skiena", "title": "Exploring the power of GPU's for training Polyglot language models", "comments": "version 2 (just corrected citation)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One of the major research trends currently is the evolution of heterogeneous\nparallel computing. GP-GPU computing is being widely used and several\napplications have been designed to exploit the massive parallelism that\nGP-GPU's have to offer. While GPU's have always been widely used in areas of\ncomputer vision for image processing, little has been done to investigate\nwhether the massive parallelism provided by GP-GPU's can be utilized\neffectively for Natural Language Processing(NLP) tasks. In this work, we\ninvestigate and explore the power of GP-GPU's in the task of learning language\nmodels. More specifically, we investigate the performance of training Polyglot\nlanguage models using deep belief neural networks. We evaluate the performance\nof training the model on the GPU and present optimizations that boost the\nperformance on the GPU.One of the key optimizations, we propose increases the\nperformance of a function involved in calculating and updating the gradient by\napproximately 50 times on the GPU for sufficiently large batch sizes. We show\nthat with the above optimizations, the GP-GPU's performance on the task\nincreases by factor of approximately 3-4. The optimizations we made are generic\nTheano optimizations and hence potentially boost the performance of other\nmodels which rely on these operations.We also show that these optimizations\nresult in the GPU's performance at this task being now comparable to that on\nthe CPU. We conclude by presenting a thorough evaluation of the applicability\nof GP-GPU's for this task and highlight the factors limiting the performance of\ntraining a Polyglot model on the GPU.\n", "versions": [{"version": "v1", "created": "Sat, 5 Apr 2014 21:25:54 GMT"}, {"version": "v2", "created": "Mon, 14 Apr 2014 16:32:22 GMT"}, {"version": "v3", "created": "Tue, 15 Apr 2014 13:18:37 GMT"}], "update_date": "2014-04-16", "authors_parsed": [["Kulkarni", "Vivek", ""], ["Al-Rfou'", "Rami", ""], ["Perozzi", "Bryan", ""], ["Skiena", "Steven", ""]]}, {"id": "1404.1559", "submitter": "Jai  Priyankka", "authors": "R. Vidya, Dr.G.M.Nasira, R. P. Jaia Priyankka", "title": "Sparse Coding: A Deep Learning using Unlabeled Data for High - Level\n  Representation", "comments": "4 Pages, 3 Figures, 2014 World Congress on Computing and\n  Communication Technologies (WCCCT)", "journal-ref": "Vidya R, Dr. Naisra G.M, Priyankka R.P. Jaia, \"Sparse Coding: A\n  Deep Learning using Unlabeled Data for High - Level Representation\" IEEE\n  Xplore 2014", "doi": "10.1109/WCCCT.2014.69", "report-no": null, "categories": "cs.LG cs.NE", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  Sparse coding algorithm is an learning algorithm mainly for unsupervised\nfeature for finding succinct, a little above high - level Representation of\ninputs, and it has successfully given a way for Deep learning. Our objective is\nto use High - Level Representation data in form of unlabeled category to help\nunsupervised learning task. when compared with labeled data, unlabeled data is\neasier to acquire because, unlike labeled data it does not follow some\nparticular class labels. This really makes the Deep learning wider and\napplicable to practical problems and learning. The main problem with sparse\ncoding is it uses Quadratic loss function and Gaussian noise mode. So, its\nperforms is very poor when binary or integer value or other Non- Gaussian type\ndata is applied. Thus first we propose an algorithm for solving the L1 -\nregularized convex optimization algorithm for the problem to allow High - Level\nRepresentation of unlabeled data. Through this we derive a optimal solution for\ndescribing an approach to Deep learning algorithm by using sparse code.\n", "versions": [{"version": "v1", "created": "Sun, 6 Apr 2014 09:50:45 GMT"}], "update_date": "2014-04-08", "authors_parsed": [["Vidya", "R.", ""], ["Nasira", "Dr. G. M.", ""], ["Priyankka", "R. P. Jaia", ""]]}, {"id": "1404.1561", "submitter": "Chunhua Shen", "authors": "Guosheng Lin, Chunhua Shen, Qinfeng Shi, Anton van den Hengel, David\n  Suter", "title": "Fast Supervised Hashing with Decision Trees for High-Dimensional Data", "comments": "Appearing in Proc. IEEE Conf. Computer Vision and Pattern\n  Recognition, 2014, Ohio, USA", "journal-ref": null, "doi": "10.1109/CVPR.2014.253", "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Supervised hashing aims to map the original features to compact binary codes\nthat are able to preserve label based similarity in the Hamming space.\nNon-linear hash functions have demonstrated the advantage over linear ones due\nto their powerful generalization capability. In the literature, kernel\nfunctions are typically used to achieve non-linearity in hashing, which achieve\nencouraging retrieval performance at the price of slow evaluation and training\ntime. Here we propose to use boosted decision trees for achieving non-linearity\nin hashing, which are fast to train and evaluate, hence more suitable for\nhashing with high dimensional data. In our approach, we first propose\nsub-modular formulations for the hashing binary code inference problem and an\nefficient GraphCut based block search method for solving large-scale inference.\nThen we learn hash functions by training boosted decision trees to fit the\nbinary codes. Experiments demonstrate that our proposed method significantly\noutperforms most state-of-the-art methods in retrieval precision and training\ntime. Especially for high-dimensional data, our method is orders of magnitude\nfaster than many methods in terms of training time.\n", "versions": [{"version": "v1", "created": "Sun, 6 Apr 2014 10:42:36 GMT"}, {"version": "v2", "created": "Wed, 28 May 2014 00:25:43 GMT"}], "update_date": "2016-11-17", "authors_parsed": [["Lin", "Guosheng", ""], ["Shen", "Chunhua", ""], ["Shi", "Qinfeng", ""], ["Hengel", "Anton van den", ""], ["Suter", "David", ""]]}, {"id": "1404.1592", "submitter": "Longbo Huang", "authors": "Longbo Huang, Xin Liu, Xiaohong Hao", "title": "The Power of Online Learning in Stochastic Network Optimization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.LG cs.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we investigate the power of online learning in stochastic\nnetwork optimization with unknown system statistics {\\it a priori}. We are\ninterested in understanding how information and learning can be efficiently\nincorporated into system control techniques, and what are the fundamental\nbenefits of doing so. We propose two \\emph{Online Learning-Aided Control}\ntechniques, $\\mathtt{OLAC}$ and $\\mathtt{OLAC2}$, that explicitly utilize the\npast system information in current system control via a learning procedure\ncalled \\emph{dual learning}. We prove strong performance guarantees of the\nproposed algorithms: $\\mathtt{OLAC}$ and $\\mathtt{OLAC2}$ achieve the\nnear-optimal $[O(\\epsilon), O([\\log(1/\\epsilon)]^2)]$ utility-delay tradeoff\nand $\\mathtt{OLAC2}$ possesses an $O(\\epsilon^{-2/3})$ convergence time.\n$\\mathtt{OLAC}$ and $\\mathtt{OLAC2}$ are probably the first algorithms that\nsimultaneously possess explicit near-optimal delay guarantee and sub-linear\nconvergence time. Simulation results also confirm the superior performance of\nthe proposed algorithms in practice. To the best of our knowledge, our attempt\nis the first to explicitly incorporate online learning into stochastic network\noptimization and to demonstrate its power in both theory and practice.\n", "versions": [{"version": "v1", "created": "Sun, 6 Apr 2014 15:59:05 GMT"}, {"version": "v2", "created": "Tue, 29 Jul 2014 15:48:24 GMT"}], "update_date": "2014-07-30", "authors_parsed": [["Huang", "Longbo", ""], ["Liu", "Xin", ""], ["Hao", "Xiaohong", ""]]}, {"id": "1404.1614", "submitter": "Alexander Churchill", "authors": "Alexander W. Churchill and Siddharth Sigtia and Chrisantha Fernando", "title": "A Denoising Autoencoder that Guides Stochastic Search", "comments": "Submitted to Parallel Problem Solving from Nature 2014", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An algorithm is described that adaptively learns a non-linear mutation\ndistribution. It works by training a denoising autoencoder (DA) online at each\ngeneration of a genetic algorithm to reconstruct a slowly decaying memory of\nthe best genotypes so far. A compressed hidden layer forces the autoencoder to\nlearn hidden features in the training set that can be used to accelerate search\non novel problems with similar structure. Its output neurons define a\nprobability distribution that we sample from to produce offspring solutions.\nThe algorithm outperforms a canonical genetic algorithm on several\ncombinatorial optimisation problems, e.g. multidimensional 0/1 knapsack\nproblem, MAXSAT, HIFF, and on parameter optimisation problems, e.g. Rastrigin\nand Rosenbrock functions.\n", "versions": [{"version": "v1", "created": "Sun, 6 Apr 2014 20:10:37 GMT"}], "update_date": "2014-04-08", "authors_parsed": [["Churchill", "Alexander W.", ""], ["Sigtia", "Siddharth", ""], ["Fernando", "Chrisantha", ""]]}, {"id": "1404.1999", "submitter": "Jonathon Shlens", "authors": "Jonathon Shlens", "title": "Notes on Generalized Linear Models of Neurons", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.LG q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Experimental neuroscience increasingly requires tractable models for\nanalyzing and predicting the behavior of neurons and networks. The generalized\nlinear model (GLM) is an increasingly popular statistical framework for\nanalyzing neural data that is flexible, exhibits rich dynamic behavior and is\ncomputationally tractable (Paninski, 2004; Pillow et al., 2008; Truccolo et\nal., 2005). What follows is a brief summary of the primary equations governing\nthe application of GLM's to spike trains with a few sentences linking this work\nto the larger statistical literature. Latter sections include extensions of a\nbasic GLM to model spatio-temporal receptive fields as well as network activity\nin an arbitrary numbers of neurons.\n", "versions": [{"version": "v1", "created": "Tue, 8 Apr 2014 03:41:50 GMT"}], "update_date": "2014-04-09", "authors_parsed": [["Shlens", "Jonathon", ""]]}, {"id": "1404.2078", "submitter": "Joost Broekens", "authors": "Joost Broekens and Tim Baarslag", "title": "Optimistic Risk Perception in the Temporal Difference error Explains the\n  Relation between Risk-taking, Gambling, Sensation-seeking and Low Fear", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Understanding the affective, cognitive and behavioural processes involved in\nrisk taking is essential for treatment and for setting environmental conditions\nto limit damage. Using Temporal Difference Reinforcement Learning (TDRL) we\ncomputationally investigated the effect of optimism in risk perception in a\nvariety of goal-oriented tasks. Optimism in risk perception was studied by\nvarying the calculation of the Temporal Difference error, i.e., delta, in three\nways: realistic (stochastically correct), optimistic (assuming action control),\nand overly optimistic (assuming outcome control). We show that for the gambling\ntask individuals with 'healthy' perception of control, i.e., action optimism,\ndo not develop gambling behaviour while individuals with 'unhealthy' perception\nof control, i.e., outcome optimism, do. We show that high intensity of\nsensations and low levels of fear co-occur due to optimistic risk perception.\nWe found that overly optimistic risk perception (outcome optimism) results in\nrisk taking and in persistent gambling behaviour in addition to high intensity\nof sensations. We discuss how our results replicate risk-taking related\nphenomena.\n", "versions": [{"version": "v1", "created": "Tue, 8 Apr 2014 10:26:27 GMT"}, {"version": "v2", "created": "Tue, 3 Feb 2015 14:09:51 GMT"}], "update_date": "2015-02-04", "authors_parsed": [["Broekens", "Joost", ""], ["Baarslag", "Tim", ""]]}, {"id": "1404.2083", "submitter": "Vladimir Vovk", "authors": "Evgeny Burnaev and Vladimir Vovk", "title": "Efficiency of conformalized ridge regression", "comments": "22 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Conformal prediction is a method of producing prediction sets that can be\napplied on top of a wide range of prediction algorithms. The method has a\nguaranteed coverage probability under the standard IID assumption regardless of\nwhether the assumptions (often considerably more restrictive) of the underlying\nalgorithm are satisfied. However, for the method to be really useful it is\ndesirable that in the case where the assumptions of the underlying algorithm\nare satisfied, the conformal predictor loses little in efficiency as compared\nwith the underlying algorithm (whereas being a conformal predictor, it has the\nstronger guarantee of validity). In this paper we explore the degree to which\nthis additional requirement of efficiency is satisfied in the case of Bayesian\nridge regression; we find that asymptotically conformal prediction sets differ\nlittle from ridge regression prediction intervals when the standard Bayesian\nassumptions are satisfied.\n", "versions": [{"version": "v1", "created": "Tue, 8 Apr 2014 10:49:08 GMT"}], "update_date": "2014-04-09", "authors_parsed": [["Burnaev", "Evgeny", ""], ["Vovk", "Vladimir", ""]]}, {"id": "1404.2229", "submitter": "Kerstin Eder", "authors": "Kerstin Eder, Chris Harper, Ute Leonards", "title": "Towards the Safety of Human-in-the-Loop Robotics: Challenges and\n  Opportunities for Safety Assurance of Robotic Co-Workers", "comments": null, "journal-ref": null, "doi": "10.1109/ROMAN.2014.6926328", "report-no": null, "categories": "cs.RO cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The success of the human-robot co-worker team in a flexible manufacturing\nenvironment where robots learn from demonstration heavily relies on the correct\nand safe operation of the robot. How this can be achieved is a challenge that\nrequires addressing both technical as well as human-centric research questions.\nIn this paper we discuss the state of the art in safety assurance, existing as\nwell as emerging standards in this area, and the need for new approaches to\nsafety assurance in the context of learning machines. We then focus on robotic\nlearning from demonstration, the challenges these techniques pose to safety\nassurance and indicate opportunities to integrate safety considerations into\nalgorithms \"by design\". Finally, from a human-centric perspective, we stipulate\nthat, to achieve high levels of safety and ultimately trust, the robotic\nco-worker must meet the innate expectations of the humans it works with. It is\nour aim to stimulate a discussion focused on the safety aspects of\nhuman-in-the-loop robotics, and to foster multidisciplinary collaboration to\naddress the research challenges identified.\n", "versions": [{"version": "v1", "created": "Tue, 8 Apr 2014 17:44:40 GMT"}, {"version": "v2", "created": "Sat, 31 May 2014 02:03:09 GMT"}, {"version": "v3", "created": "Wed, 4 Jun 2014 23:59:27 GMT"}], "update_date": "2021-04-13", "authors_parsed": [["Eder", "Kerstin", ""], ["Harper", "Chris", ""], ["Leonards", "Ute", ""]]}, {"id": "1404.2353", "submitter": "Denis Sidorov", "authors": "Victor Kurbatsky, Nikita Tomin, Vadim Spiryaev, Paul Leahy, Denis\n  Sidorov and Alexei Zhukov", "title": "Power System Parameters Forecasting Using Hilbert-Huang Transform and\n  Machine Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A novel hybrid data-driven approach is developed for forecasting power system\nparameters with the goal of increasing the efficiency of short-term forecasting\nstudies for non-stationary time-series. The proposed approach is based on mode\ndecomposition and a feature analysis of initial retrospective data using the\nHilbert-Huang transform and machine learning algorithms. The random forests and\ngradient boosting trees learning techniques were examined. The decision tree\ntechniques were used to rank the importance of variables employed in the\nforecasting models. The Mean Decrease Gini index is employed as an impurity\nfunction. The resulting hybrid forecasting models employ the radial basis\nfunction neural network and support vector regression. Apart from introduction\nand references the paper is organized as follows. The section 2 presents the\nbackground and the review of several approaches for short-term forecasting of\npower system parameters. In the third section a hybrid machine learning-based\nalgorithm using Hilbert-Huang transform is developed for short-term forecasting\nof power system parameters. Fourth section describes the decision tree learning\nalgorithms used for the issue of variables importance. Finally in section six\nthe experimental results in the following electric power problems are\npresented: active power flow forecasting, electricity price forecasting and for\nthe wind speed and direction forecasting.\n", "versions": [{"version": "v1", "created": "Wed, 9 Apr 2014 02:11:17 GMT"}], "update_date": "2014-04-10", "authors_parsed": [["Kurbatsky", "Victor", ""], ["Tomin", "Nikita", ""], ["Spiryaev", "Vadim", ""], ["Leahy", "Paul", ""], ["Sidorov", "Denis", ""], ["Zhukov", "Alexei", ""]]}, {"id": "1404.2644", "submitter": "Aur\\'elien Bellet", "authors": "Aur\\'elien Bellet, Yingyu Liang, Alireza Bagheri Garakani,\n  Maria-Florina Balcan, Fei Sha", "title": "A Distributed Frank-Wolfe Algorithm for Communication-Efficient Sparse\n  Learning", "comments": "Extended version of the SIAM Data Mining 2015 paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning sparse combinations is a frequent theme in machine learning. In this\npaper, we study its associated optimization problem in the distributed setting\nwhere the elements to be combined are not centrally located but spread over a\nnetwork. We address the key challenges of balancing communication costs and\noptimization errors. To this end, we propose a distributed Frank-Wolfe (dFW)\nalgorithm. We obtain theoretical guarantees on the optimization error\n$\\epsilon$ and communication cost that do not depend on the total number of\ncombining elements. We further show that the communication cost of dFW is\noptimal by deriving a lower-bound on the communication cost required to\nconstruct an $\\epsilon$-approximate solution. We validate our theoretical\nanalysis with empirical studies on synthetic and real-world data, which\ndemonstrate that dFW outperforms both baselines and competing methods. We also\nstudy the performance of dFW when the conditions of our analysis are relaxed,\nand show that dFW is fairly robust.\n", "versions": [{"version": "v1", "created": "Wed, 9 Apr 2014 22:16:39 GMT"}, {"version": "v2", "created": "Thu, 12 Jun 2014 04:08:51 GMT"}, {"version": "v3", "created": "Mon, 12 Jan 2015 15:14:19 GMT"}], "update_date": "2019-01-25", "authors_parsed": [["Bellet", "Aur\u00e9lien", ""], ["Liang", "Yingyu", ""], ["Garakani", "Alireza Bagheri", ""], ["Balcan", "Maria-Florina", ""], ["Sha", "Fei", ""]]}, {"id": "1404.2655", "submitter": "Afonso S. Bandeira", "authors": "Afonso S. Bandeira and Yuehaw Khoo and Amit Singer", "title": "Open problem: Tightness of maximum likelihood semidefinite relaxations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We have observed an interesting, yet unexplained, phenomenon: Semidefinite\nprogramming (SDP) based relaxations of maximum likelihood estimators (MLE) tend\nto be tight in recovery problems with noisy data, even when MLE cannot exactly\nrecover the ground truth. Several results establish tightness of SDP based\nrelaxations in the regime where exact recovery from MLE is possible. However,\nto the best of our knowledge, their tightness is not understood beyond this\nregime. As an illustrative example, we focus on the generalized Procrustes\nproblem.\n", "versions": [{"version": "v1", "created": "Thu, 10 Apr 2014 00:19:17 GMT"}], "update_date": "2014-04-11", "authors_parsed": [["Bandeira", "Afonso S.", ""], ["Khoo", "Yuehaw", ""], ["Singer", "Amit", ""]]}, {"id": "1404.2772", "submitter": "Ravi Ranjan", "authors": "Ravi Ranjan and G. Sahoo", "title": "A New Clustering Approach for Anomaly Intrusion Detection", "comments": "10 pages with 3 figures,2 Tables This paper explains about clustering\n  methodology used in Data Mining field for Intrusion Detection in the area of\n  Network Security", "journal-ref": "International Journal of Data Mining & Knowledge Management\n  Process (IJDKP),ISSN:2230-9608[Online],2231-007X[Print] Vol.4, No.2, March\n  2014, page(s): 29-38", "doi": "10.5121/ijdkp.2014.4203", "report-no": null, "categories": "cs.DC cs.CR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent advances in technology have made our work easier compare to earlier\ntimes. Computer network is growing day by day but while discussing about the\nsecurity of computers and networks it has always been a major concerns for\norganizations varying from smaller to larger enterprises. It is true that\norganizations are aware of the possible threats and attacks so they always\nprepare for the safer side but due to some loopholes attackers are able to make\nattacks. Intrusion detection is one of the major fields of research and\nresearchers are trying to find new algorithms for detecting intrusions.\nClustering techniques of data mining is an interested area of research for\ndetecting possible intrusions and attacks. This paper presents a new clustering\napproach for anomaly intrusion detection by using the approach of K-medoids\nmethod of clustering and its certain modifications. The proposed algorithm is\nable to achieve high detection rate and overcomes the disadvantages of K-means\nalgorithm.\n", "versions": [{"version": "v1", "created": "Thu, 10 Apr 2014 11:22:17 GMT"}], "update_date": "2014-04-11", "authors_parsed": [["Ranjan", "Ravi", ""], ["Sahoo", "G.", ""]]}, {"id": "1404.2885", "submitter": "Tian-Shun Jiang", "authors": "Tian-Shun Jiang, Zachary Polizzi, Christopher Yuan", "title": "A Networks and Machine Learning Approach to Determine the Best College\n  Coaches of the 20th-21st Centuries", "comments": "18 pages, Submitted to the 2014 Mathematical Contest in Modeling\n  (MCM)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP cs.LG cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Our objective is to find the five best college sports coaches of past century\nfor three different sports. We decided to look at men's basketball, football,\nand baseball. We wanted to use an approach that could definitively determine\nteam skill from the games played, and then use a machine-learning algorithm to\ncalculate the correct coach skills for each team in a given year. We created a\nnetworks-based model to calculate team skill from historical game data. A\ndigraph was created for each year in each sport. Nodes represented teams, and\nedges represented a game played between two teams. The arrowhead pointed\ntowards the losing team. We calculated the team skill of each graph using a\nright-hand eigenvector centrality measure. This way, teams that beat good teams\nwill be ranked higher than teams that beat mediocre teams. The eigenvector\ncentrality rankings for most years were well correlated with tournament\nperformance and poll-based rankings. We assumed that the relationship between\ncoach skill $C_s$, player skill $P_s$, and team skill $T_s$ was $C_s \\cdot P_s\n= T_s$. We then created a function to describe the probability that a given\nscore difference would occur based on player skill and coach skill. We\nmultiplied the probabilities of all edges in the network together to find the\nprobability that the correct network would occur with any given player skill\nand coach skill matrix. We was able to determine player skill as a function of\nteam skill and coach skill, eliminating the need to optimize two unknown\nmatrices. The top five coaches in each year were noted, and the top coach of\nall time was calculated by dividing the number of times that coach ranked in\nthe yearly top five by the years said coach had been active.\n", "versions": [{"version": "v1", "created": "Tue, 8 Apr 2014 22:04:53 GMT"}], "update_date": "2014-04-11", "authors_parsed": [["Jiang", "Tian-Shun", ""], ["Polizzi", "Zachary", ""], ["Yuan", "Christopher", ""]]}, {"id": "1404.2903", "submitter": "Marius Leordeanu", "authors": "Marius Leordeanu and Rahul Sukthankar", "title": "Thoughts on a Recursive Classifier Graph: a Multiclass Network for Deep\n  Object Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a general multi-class visual recognition model, termed the\nClassifier Graph, which aims to generalize and integrate ideas from many of\ntoday's successful hierarchical recognition approaches. Our graph-based model\nhas the advantage of enabling rich interactions between classes from different\nlevels of interpretation and abstraction. The proposed multi-class system is\nefficiently learned using step by step updates. The structure consists of\nsimple logistic linear layers with inputs from features that are automatically\nselected from a large pool. Each newly learned classifier becomes a potential\nnew feature. Thus, our feature pool can consist both of initial manually\ndesigned features as well as learned classifiers from previous steps (graph\nnodes), each copied many times at different scales and locations. In this\nmanner we can learn and grow both a deep, complex graph of classifiers and a\nrich pool of features at different levels of abstraction and interpretation.\nOur proposed graph of classifiers becomes a multi-class system with a recursive\nstructure, suitable for deep detection and recognition of several classes\nsimultaneously.\n", "versions": [{"version": "v1", "created": "Wed, 2 Apr 2014 11:38:35 GMT"}], "update_date": "2014-04-11", "authors_parsed": [["Leordeanu", "Marius", ""], ["Sukthankar", "Rahul", ""]]}, {"id": "1404.2948", "submitter": "Anna Goldenberg", "authors": "Bo Wang and Anna Goldenberg", "title": "Gradient-based Laplacian Feature Selection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Analysis of high dimensional noisy data is of essence across a variety of\nresearch fields. Feature selection techniques are designed to find the relevant\nfeature subset that can facilitate classification or pattern detection.\nTraditional (supervised) feature selection methods utilize label information to\nguide the identification of relevant feature subsets. In this paper, however,\nwe consider the unsupervised feature selection problem. Without the label\ninformation, it is particularly difficult to identify a small set of relevant\nfeatures due to the noisy nature of real-world data which corrupts the\nintrinsic structure of the data. Our Gradient-based Laplacian Feature Selection\n(GLFS) selects important features by minimizing the variance of the Laplacian\nregularized least squares regression model. With $\\ell_1$ relaxation, GLFS can\nfind a sparse subset of features that is relevant to the Laplacian manifolds.\nExtensive experiments on simulated, three real-world object recognition and two\ncomputational biology datasets, have illustrated the power and superior\nperformance of our approach over multiple state-of-the-art unsupervised feature\nselection methods. Additionally, we show that GLFS selects a sparser set of\nmore relevant features in a supervised setting outperforming the popular\nelastic net methodology.\n", "versions": [{"version": "v1", "created": "Thu, 10 Apr 2014 20:49:35 GMT"}], "update_date": "2014-04-14", "authors_parsed": [["Wang", "Bo", ""], ["Goldenberg", "Anna", ""]]}, {"id": "1404.2986", "submitter": "Jonathon Shlens", "authors": "Jonathon Shlens", "title": "A Tutorial on Independent Component Analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Independent component analysis (ICA) has become a standard data analysis\ntechnique applied to an array of problems in signal processing and machine\nlearning. This tutorial provides an introduction to ICA based on linear algebra\nformulating an intuition for ICA from first principles. The goal of this\ntutorial is to provide a solid foundation on this advanced topic so that one\nmight learn the motivation behind ICA, learn why and when to apply this\ntechnique and in the process gain an introduction to this exciting field of\nactive research.\n", "versions": [{"version": "v1", "created": "Fri, 11 Apr 2014 02:37:11 GMT"}], "update_date": "2014-04-14", "authors_parsed": [["Shlens", "Jonathon", ""]]}, {"id": "1404.3012", "submitter": "Shun Kataoka", "authors": "Kazuyuki Tanaka, Shun Kataoka, Muneki Yasuda, Yuji Waizumi and\n  Chiou-Ting Hsu", "title": "Bayesian image segmentations by Potts prior and loopy belief propagation", "comments": "24 pages, 9 figures", "journal-ref": "Journal of the Physical Society of Japan 83 (2014) 124002", "doi": "10.7566/JPSJ.83.124002", "report-no": null, "categories": "cs.CV cond-mat.dis-nn cond-mat.stat-mech cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a Bayesian image segmentation model based on Potts prior\nand loopy belief propagation. The proposed Bayesian model involves several\nterms, including the pairwise interactions of Potts models, and the average\nvectors and covariant matrices of Gauss distributions in color image modeling.\nThese terms are often referred to as hyperparameters in statistical machine\nlearning theory. In order to determine these hyperparameters, we propose a new\nscheme for hyperparameter estimation based on conditional maximization of\nentropy in the Potts prior. The algorithm is given based on loopy belief\npropagation. In addition, we compare our conditional maximum entropy framework\nwith the conventional maximum likelihood framework, and also clarify how the\nfirst order phase transitions in LBP's for Potts models influence our\nhyperparameter estimation procedures.\n", "versions": [{"version": "v1", "created": "Fri, 11 Apr 2014 06:31:03 GMT"}, {"version": "v2", "created": "Wed, 30 Apr 2014 10:26:46 GMT"}, {"version": "v3", "created": "Wed, 4 Jun 2014 11:18:44 GMT"}, {"version": "v4", "created": "Fri, 15 Aug 2014 00:58:40 GMT"}, {"version": "v5", "created": "Mon, 18 Aug 2014 04:45:26 GMT"}], "update_date": "2014-11-19", "authors_parsed": [["Tanaka", "Kazuyuki", ""], ["Kataoka", "Shun", ""], ["Yasuda", "Muneki", ""], ["Waizumi", "Yuji", ""], ["Hsu", "Chiou-Ting", ""]]}, {"id": "1404.3026", "submitter": "Todd Bodnar", "authors": "Todd Bodnar, Victoria C Barclay, Nilam Ram, Conrad S Tucker, Marcel\n  Salath\\'e", "title": "On the Ground Validation of Online Diagnosis with Twitter and Medical\n  Records", "comments": "Presented at of WWW2014. WWW'14 Companion, April 7-11, 2014, Seoul,\n  Korea", "journal-ref": null, "doi": "10.1145/2567948.2579272", "report-no": null, "categories": "cs.SI cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Social media has been considered as a data source for tracking disease.\nHowever, most analyses are based on models that prioritize strong correlation\nwith population-level disease rates over determining whether or not specific\nindividual users are actually sick. Taking a different approach, we develop a\nnovel system for social-media based disease detection at the individual level\nusing a sample of professionally diagnosed individuals. Specifically, we\ndevelop a system for making an accurate influenza diagnosis based on an\nindividual's publicly available Twitter data. We find that about half (17/35 =\n48.57%) of the users in our sample that were sick explicitly discuss their\ndisease on Twitter. By developing a meta classifier that combines text\nanalysis, anomaly detection, and social network analysis, we are able to\ndiagnose an individual with greater than 99% accuracy even if she does not\ndiscuss her health.\n", "versions": [{"version": "v1", "created": "Fri, 11 Apr 2014 07:55:51 GMT"}], "update_date": "2014-04-14", "authors_parsed": [["Bodnar", "Todd", ""], ["Barclay", "Victoria C", ""], ["Ram", "Nilam", ""], ["Tucker", "Conrad S", ""], ["Salath\u00e9", "Marcel", ""]]}, {"id": "1404.3184", "submitter": "Xiangrong Zeng", "authors": "Xiangrong Zeng and M\\'ario A. T. Figueiredo", "title": "Decreasing Weighted Sorted $\\ell_1$ Regularization", "comments": "5 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.IT cs.LG math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider a new family of regularizers, termed {\\it weighted sorted\n$\\ell_1$ norms} (WSL1), which generalizes the recently introduced {\\it\noctagonal shrinkage and clustering algorithm for regression} (OSCAR) and also\ncontains the $\\ell_1$ and $\\ell_{\\infty}$ norms as particular instances. We\nfocus on a special case of the WSL1, the {\\sl decreasing WSL1} (DWSL1), where\nthe elements of the argument vector are sorted in non-increasing order and the\nweights are also non-increasing. In this paper, after showing that the DWSL1 is\nindeed a norm, we derive two key tools for its use as a regularizer: the dual\nnorm and the Moreau proximity operator.\n", "versions": [{"version": "v1", "created": "Fri, 11 Apr 2014 18:50:34 GMT"}], "update_date": "2014-04-14", "authors_parsed": [["Zeng", "Xiangrong", ""], ["Figueiredo", "M\u00e1rio A. T.", ""]]}, {"id": "1404.3190", "submitter": "Cong Li", "authors": "Cong Li, Michael Georgiopoulos, Georgios C. Anagnostopoulos", "title": "Pareto-Path Multi-Task Multiple Kernel Learning", "comments": "Accepted by IEEE Transactions on Neural Networks and Learning Systems", "journal-ref": null, "doi": "10.1109/TNNLS.2014.2309939", "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A traditional and intuitively appealing Multi-Task Multiple Kernel Learning\n(MT-MKL) method is to optimize the sum (thus, the average) of objective\nfunctions with (partially) shared kernel function, which allows information\nsharing amongst tasks. We point out that the obtained solution corresponds to a\nsingle point on the Pareto Front (PF) of a Multi-Objective Optimization (MOO)\nproblem, which considers the concurrent optimization of all task objectives\ninvolved in the Multi-Task Learning (MTL) problem. Motivated by this last\nobservation and arguing that the former approach is heuristic, we propose a\nnovel Support Vector Machine (SVM) MT-MKL framework, that considers an\nimplicitly-defined set of conic combinations of task objectives. We show that\nsolving our framework produces solutions along a path on the aforementioned PF\nand that it subsumes the optimization of the average of objective functions as\na special case. Using algorithms we derived, we demonstrate through a series of\nexperimental results that the framework is capable of achieving better\nclassification performance, when compared to other similar MTL approaches.\n", "versions": [{"version": "v1", "created": "Fri, 11 Apr 2014 19:15:22 GMT"}], "update_date": "2014-04-14", "authors_parsed": [["Li", "Cong", ""], ["Georgiopoulos", "Michael", ""], ["Anagnostopoulos", "Georgios C.", ""]]}, {"id": "1404.3203", "submitter": "Afonso S. Bandeira", "authors": "Afonso S. Bandeira and Dustin G. Mixon and Benjamin Recht", "title": "Compressive classification and the rare eclipse problem", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.IT math.IT math.ST stat.TH", "license": "http://creativecommons.org/licenses/publicdomain/", "abstract": "  This paper addresses the fundamental question of when convex sets remain\ndisjoint after random projection. We provide an analysis using ideas from\nhigh-dimensional convex geometry. For ellipsoids, we provide a bound in terms\nof the distance between these ellipsoids and simple functions of their\npolynomial coefficients. As an application, this theorem provides bounds for\ncompressive classification of convex sets. Rather than assuming that the data\nto be classified is sparse, our results show that the data can be acquired via\nvery few measurements yet will remain linearly separable. We demonstrate the\nfeasibility of this approach in the context of hyperspectral imaging.\n", "versions": [{"version": "v1", "created": "Fri, 11 Apr 2014 19:49:05 GMT"}], "update_date": "2014-04-14", "authors_parsed": [["Bandeira", "Afonso S.", ""], ["Mixon", "Dustin G.", ""], ["Recht", "Benjamin", ""]]}, {"id": "1404.3291", "submitter": "Michael Wilber", "authors": "Michael J. Wilber and Iljung S. Kwak and Serge J. Belongie", "title": "Cost-Effective HITs for Relative Similarity Comparisons", "comments": "7 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Similarity comparisons of the form \"Is object a more similar to b than to c?\"\nare useful for computer vision and machine learning applications.\nUnfortunately, an embedding of $n$ points is specified by $n^3$ triplets,\nmaking collecting every triplet an expensive task. In noticing this difficulty,\nother researchers have investigated more intelligent triplet sampling\ntechniques, but they do not study their effectiveness or their potential\ndrawbacks. Although it is important to reduce the number of collected triplets,\nit is also important to understand how best to display a triplet collection\ntask to a user. In this work we explore an alternative display for collecting\ntriplets and analyze the monetary cost and speed of the display. We propose\nbest practices for creating cost effective human intelligence tasks for\ncollecting triplets. We show that rather than changing the sampling algorithm,\nsimple changes to the crowdsourcing UI can lead to much higher quality\nembeddings. We also provide a dataset as well as the labels collected from\ncrowd workers.\n", "versions": [{"version": "v1", "created": "Sat, 12 Apr 2014 14:33:18 GMT"}], "update_date": "2014-04-15", "authors_parsed": [["Wilber", "Michael J.", ""], ["Kwak", "Iljung S.", ""], ["Belongie", "Serge J.", ""]]}, {"id": "1404.3368", "submitter": "Aryeh Kontorovich", "authors": "Lee-Ad Gottlieb and Aryeh Kontorovich and Pinhas Nisnevitch", "title": "Near-optimal sample compression for nearest neighbors", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present the first sample compression algorithm for nearest neighbors with\nnon-trivial performance guarantees. We complement these guarantees by\ndemonstrating almost matching hardness lower bounds, which show that our bound\nis nearly optimal. Our result yields new insight into margin-based nearest\nneighbor classification in metric spaces and allows us to significantly sharpen\nand simplify existing bounds. Some encouraging empirical results are also\npresented.\n", "versions": [{"version": "v1", "created": "Sun, 13 Apr 2014 11:13:02 GMT"}, {"version": "v2", "created": "Thu, 4 Dec 2014 15:23:49 GMT"}, {"version": "v3", "created": "Fri, 5 Dec 2014 10:38:21 GMT"}, {"version": "v4", "created": "Mon, 26 Mar 2018 08:54:17 GMT"}], "update_date": "2018-03-28", "authors_parsed": [["Gottlieb", "Lee-Ad", ""], ["Kontorovich", "Aryeh", ""], ["Nisnevitch", "Pinhas", ""]]}, {"id": "1404.3378", "submitter": "Amit Daniely", "authors": "Amit Daniely and Shai Shalev-Shwatz", "title": "Complexity theoretic limitations on learning DNF's", "comments": "arXiv admin note: substantial text overlap with arXiv:1311.2272", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Using the recently developed framework of [Daniely et al, 2014], we show that\nunder a natural assumption on the complexity of refuting random K-SAT formulas,\nlearning DNF formulas is hard. Furthermore, the same assumption implies the\nhardness of learning intersections of $\\omega(\\log(n))$ halfspaces,\nagnostically learning conjunctions, as well as virtually all (distribution\nfree) learning problems that were previously shown hard (under complexity\nassumptions).\n", "versions": [{"version": "v1", "created": "Sun, 13 Apr 2014 12:42:10 GMT"}, {"version": "v2", "created": "Tue, 4 Nov 2014 18:28:50 GMT"}], "update_date": "2014-11-05", "authors_parsed": [["Daniely", "Amit", ""], ["Shalev-Shwatz", "Shai", ""]]}, {"id": "1404.3415", "submitter": "Evgeniy Abramov G.", "authors": "E. G. Abramov, A. B. Komissarov, D. A. Kornyakov", "title": "Generalized version of the support vector machine for binary\n  classification problems: supporting hyperplane machine", "comments": "22 pages with 3 figures, 1 Octave script", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper there is proposed a generalized version of the SVM for binary\nclassification problems in the case of using an arbitrary transformation x ->\ny. An approach similar to the classic SVM method is used. The problem is widely\nexplained. Various formulations of primal and dual problems are proposed. For\none of the most important cases the formulae are derived in detail. A simple\ncomputational example is demonstrated. The algorithm and its implementation is\npresented in Octave language.\n", "versions": [{"version": "v1", "created": "Sun, 13 Apr 2014 18:57:30 GMT"}, {"version": "v2", "created": "Tue, 15 Apr 2014 17:37:11 GMT"}], "update_date": "2014-04-16", "authors_parsed": [["Abramov", "E. G.", ""], ["Komissarov", "A. B.", ""], ["Kornyakov", "D. A.", ""]]}, {"id": "1404.3439", "submitter": "Omur  Arslan", "authors": "Omur Arslan and Daniel E. Koditschek", "title": "Anytime Hierarchical Clustering", "comments": "13 pages, 6 figures, 5 tables, in preparation for submission to a\n  conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new anytime hierarchical clustering method that iteratively\ntransforms an arbitrary initial hierarchy on the configuration of measurements\nalong a sequence of trees we prove for a fixed data set must terminate in a\nchain of nested partitions that satisfies a natural homogeneity requirement.\nEach recursive step re-edits the tree so as to improve a local measure of\ncluster homogeneity that is compatible with a number of commonly used (e.g.,\nsingle, average, complete) linkage functions. As an alternative to the standard\nbatch algorithms, we present numerical evidence to suggest that appropriate\nadaptations of this method can yield decentralized, scalable algorithms\nsuitable for distributed/parallel computation of clustering hierarchies and\nonline tracking of clustering trees applicable to large, dynamically changing\ndatabases and anomaly detection.\n", "versions": [{"version": "v1", "created": "Sun, 13 Apr 2014 23:07:20 GMT"}], "update_date": "2014-04-15", "authors_parsed": [["Arslan", "Omur", ""], ["Koditschek", "Daniel E.", ""]]}, {"id": "1404.3581", "submitter": "Arnaud Joly", "authors": "Arnaud Joly, Pierre Geurts, Louis Wehenkel", "title": "Random forests with random projections of the output space for high\n  dimensional multi-label classification", "comments": null, "journal-ref": "Machine Learning and Knowledge Discovery in Databases, 2014, Part\n  I, pp 607-622", "doi": "10.1007/978-3-662-44848-9_39", "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We adapt the idea of random projections applied to the output space, so as to\nenhance tree-based ensemble methods in the context of multi-label\nclassification. We show how learning time complexity can be reduced without\naffecting computational complexity and accuracy of predictions. We also show\nthat random output space projections may be used in order to reach different\nbias-variance tradeoffs, over a broad panel of benchmark problems, and that\nthis may lead to improved accuracy while reducing significantly the\ncomputational burden of the learning stage.\n", "versions": [{"version": "v1", "created": "Mon, 14 Apr 2014 13:52:29 GMT"}, {"version": "v2", "created": "Wed, 16 Apr 2014 10:27:00 GMT"}, {"version": "v3", "created": "Thu, 18 Sep 2014 15:29:39 GMT"}, {"version": "v4", "created": "Mon, 29 Sep 2014 16:01:50 GMT"}], "update_date": "2021-07-02", "authors_parsed": [["Joly", "Arnaud", ""], ["Geurts", "Pierre", ""], ["Wehenkel", "Louis", ""]]}, {"id": "1404.3591", "submitter": "Andreas Argyriou", "authors": "Andreas Argyriou and Marco Signoretto and Johan Suykens", "title": "Hybrid Conditional Gradient - Smoothing Algorithms with Applications to\n  Sparse and Low Rank Regularization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study a hybrid conditional gradient - smoothing algorithm (HCGS) for\nsolving composite convex optimization problems which contain several terms over\na bounded set. Examples of these include regularization problems with several\nnorms as penalties and a norm constraint. HCGS extends conditional gradient\nmethods to cases with multiple nonsmooth terms, in which standard conditional\ngradient methods may be difficult to apply. The HCGS algorithm borrows\ntechniques from smoothing proximal methods and requires first-order\ncomputations (subgradients and proximity operations). Unlike proximal methods,\nHCGS benefits from the advantages of conditional gradient methods, which render\nit more efficient on certain large scale optimization problems. We demonstrate\nthese advantages with simulations on two matrix optimization problems:\nregularization of matrices with combined $\\ell_1$ and trace norm penalties; and\na convex relaxation of sparse PCA.\n", "versions": [{"version": "v1", "created": "Mon, 14 Apr 2014 14:09:43 GMT"}, {"version": "v2", "created": "Tue, 15 Apr 2014 19:29:31 GMT"}], "update_date": "2014-04-16", "authors_parsed": [["Argyriou", "Andreas", ""], ["Signoretto", "Marco", ""], ["Suykens", "Johan", ""]]}, {"id": "1404.3606", "submitter": "Tsung-Han  Chan", "authors": "Tsung-Han Chan, Kui Jia, Shenghua Gao, Jiwen Lu, Zinan Zeng and Yi Ma", "title": "PCANet: A Simple Deep Learning Baseline for Image Classification?", "comments": null, "journal-ref": null, "doi": "10.1109/TIP.2015.2475625", "report-no": null, "categories": "cs.CV cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we propose a very simple deep learning network for image\nclassification which comprises only the very basic data processing components:\ncascaded principal component analysis (PCA), binary hashing, and block-wise\nhistograms. In the proposed architecture, PCA is employed to learn multistage\nfilter banks. It is followed by simple binary hashing and block histograms for\nindexing and pooling. This architecture is thus named as a PCA network (PCANet)\nand can be designed and learned extremely easily and efficiently. For\ncomparison and better understanding, we also introduce and study two simple\nvariations to the PCANet, namely the RandNet and LDANet. They share the same\ntopology of PCANet but their cascaded filters are either selected randomly or\nlearned from LDA. We have tested these basic networks extensively on many\nbenchmark visual datasets for different tasks, such as LFW for face\nverification, MultiPIE, Extended Yale B, AR, FERET datasets for face\nrecognition, as well as MNIST for hand-written digits recognition.\nSurprisingly, for all tasks, such a seemingly naive PCANet model is on par with\nthe state of the art features, either prefixed, highly hand-crafted or\ncarefully learned (by DNNs). Even more surprisingly, it sets new records for\nmany classification tasks in Extended Yale B, AR, FERET datasets, and MNIST\nvariations. Additional experiments on other public datasets also demonstrate\nthe potential of the PCANet serving as a simple but highly competitive baseline\nfor texture classification and object recognition.\n", "versions": [{"version": "v1", "created": "Mon, 14 Apr 2014 15:02:17 GMT"}, {"version": "v2", "created": "Thu, 28 Aug 2014 15:20:44 GMT"}], "update_date": "2015-10-28", "authors_parsed": [["Chan", "Tsung-Han", ""], ["Jia", "Kui", ""], ["Gao", "Shenghua", ""], ["Lu", "Jiwen", ""], ["Zeng", "Zinan", ""], ["Ma", "Yi", ""]]}, {"id": "1404.3656", "submitter": "Karthik Raman", "authors": "Karthik Raman and Thorsten Joachims", "title": "Methods for Ordinal Peer Grading", "comments": "Submitted to KDD 2014", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  MOOCs have the potential to revolutionize higher education with their wide\noutreach and accessibility, but they require instructors to come up with\nscalable alternates to traditional student evaluation. Peer grading -- having\nstudents assess each other -- is a promising approach to tackling the problem\nof evaluation at scale, since the number of \"graders\" naturally scales with the\nnumber of students. However, students are not trained in grading, which means\nthat one cannot expect the same level of grading skills as in traditional\nsettings. Drawing on broad evidence that ordinal feedback is easier to provide\nand more reliable than cardinal feedback, it is therefore desirable to allow\npeer graders to make ordinal statements (e.g. \"project X is better than project\nY\") and not require them to make cardinal statements (e.g. \"project X is a\nB-\"). Thus, in this paper we study the problem of automatically inferring\nstudent grades from ordinal peer feedback, as opposed to existing methods that\nrequire cardinal peer feedback. We formulate the ordinal peer grading problem\nas a type of rank aggregation problem, and explore several probabilistic models\nunder which to estimate student grades and grader reliability. We study the\napplicability of these methods using peer grading data collected from a real\nclass -- with instructor and TA grades as a baseline -- and demonstrate the\nefficacy of ordinal feedback techniques in comparison to existing cardinal peer\ngrading methods. Finally, we compare these peer-grading techniques to\ntraditional evaluation techniques.\n", "versions": [{"version": "v1", "created": "Mon, 14 Apr 2014 17:09:32 GMT"}], "update_date": "2014-04-15", "authors_parsed": [["Raman", "Karthik", ""], ["Joachims", "Thorsten", ""]]}, {"id": "1404.3840", "submitter": "Chaochao Lu", "authors": "Chaochao Lu, Xiaoou Tang", "title": "Surpassing Human-Level Face Verification Performance on LFW with\n  GaussianFace", "comments": "Appearing in Proceedings of the 29th AAAI Conference on Artificial\n  Intelligence (AAAI-15), Oral Presentation", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Face verification remains a challenging problem in very complex conditions\nwith large variations such as pose, illumination, expression, and occlusions.\nThis problem is exacerbated when we rely unrealistically on a single training\ndata source, which is often insufficient to cover the intrinsically complex\nface variations. This paper proposes a principled multi-task learning approach\nbased on Discriminative Gaussian Process Latent Variable Model, named\nGaussianFace, to enrich the diversity of training data. In comparison to\nexisting methods, our model exploits additional data from multiple\nsource-domains to improve the generalization performance of face verification\nin an unknown target-domain. Importantly, our model can adapt automatically to\ncomplex data distributions, and therefore can well capture complex face\nvariations inherent in multiple sources. Extensive experiments demonstrate the\neffectiveness of the proposed model in learning from diverse data sources and\ngeneralize to unseen domain. Specifically, the accuracy of our algorithm\nachieves an impressive accuracy rate of 98.52% on the well-known and\nchallenging Labeled Faces in the Wild (LFW) benchmark. For the first time, the\nhuman-level performance in face verification (97.53%) on LFW is surpassed.\n", "versions": [{"version": "v1", "created": "Tue, 15 Apr 2014 07:51:23 GMT"}, {"version": "v2", "created": "Mon, 16 Jun 2014 14:37:38 GMT"}, {"version": "v3", "created": "Sat, 20 Dec 2014 03:37:36 GMT"}], "update_date": "2014-12-23", "authors_parsed": [["Lu", "Chaochao", ""], ["Tang", "Xiaoou", ""]]}, {"id": "1404.3862", "submitter": "Aviv Tamar", "authors": "Aviv Tamar, Yonatan Glassner, Shie Mannor", "title": "Optimizing the CVaR via Sampling", "comments": "To appear in AAAI 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Conditional Value at Risk (CVaR) is a prominent risk measure that is being\nused extensively in various domains. We develop a new formula for the gradient\nof the CVaR in the form of a conditional expectation. Based on this formula, we\npropose a novel sampling-based estimator for the CVaR gradient, in the spirit\nof the likelihood-ratio method. We analyze the bias of the estimator, and prove\nthe convergence of a corresponding stochastic gradient descent algorithm to a\nlocal CVaR optimum. Our method allows to consider CVaR optimization in new\ndomains. As an example, we consider a reinforcement learning application, and\nlearn a risk-sensitive controller for the game of Tetris.\n", "versions": [{"version": "v1", "created": "Tue, 15 Apr 2014 10:32:05 GMT"}, {"version": "v2", "created": "Sun, 29 Jun 2014 15:35:36 GMT"}, {"version": "v3", "created": "Tue, 16 Sep 2014 15:32:48 GMT"}, {"version": "v4", "created": "Sat, 22 Nov 2014 14:44:54 GMT"}], "update_date": "2014-11-25", "authors_parsed": [["Tamar", "Aviv", ""], ["Glassner", "Yonatan", ""], ["Mannor", "Shie", ""]]}, {"id": "1404.4032", "submitter": "Ping Li", "authors": "Guangcan Liu and Ping Li", "title": "Recovery of Coherent Data via Low-Rank Dictionary Pursuit", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.IT cs.LG math.IT math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The recently established RPCA method provides us a convenient way to restore\nlow-rank matrices from grossly corrupted observations. While elegant in theory\nand powerful in reality, RPCA may be not an ultimate solution to the low-rank\nmatrix recovery problem. Indeed, its performance may not be perfect even when\ndata are strictly low-rank. This is because conventional RPCA ignores the\nclustering structures of the data which are ubiquitous in modern applications.\nAs the number of cluster grows, the coherence of data keeps increasing, and\naccordingly, the recovery performance of RPCA degrades. We show that the\nchallenges raised by coherent data (i.e., the data with high coherence) could\nbe alleviated by Low-Rank Representation (LRR), provided that the dictionary in\nLRR is configured appropriately. More precisely, we mathematically prove that\nif the dictionary itself is low-rank then LRR is immune to the coherence\nparameter which increases with the underlying cluster number. This provides an\nelementary principle for dealing with coherent data. Subsequently, we devise a\npractical algorithm to obtain proper dictionaries in unsupervised environments.\nOur extensive experiments on randomly generated matrices verify our claims.\n", "versions": [{"version": "v1", "created": "Tue, 15 Apr 2014 19:35:15 GMT"}, {"version": "v2", "created": "Wed, 16 Jul 2014 17:57:02 GMT"}], "update_date": "2014-07-17", "authors_parsed": [["Liu", "Guangcan", ""], ["Li", "Ping", ""]]}, {"id": "1404.4038", "submitter": "Grigorios Tsoumakas", "authors": "Christina Papagiannopoulou, Grigorios Tsoumakas, Ioannis Tsamardinos", "title": "Discovering and Exploiting Entailment Relationships in Multi-Label\n  Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work presents a sound probabilistic method for enforcing adherence of\nthe marginal probabilities of a multi-label model to automatically discovered\ndeterministic relationships among labels. In particular we focus on discovering\ntwo kinds of relationships among the labels. The first one concerns pairwise\npositive entailement: pairs of labels, where the presence of one implies the\npresence of the other in all instances of a dataset. The second concerns\nexclusion: sets of labels that do not coexist in the same instances of the\ndataset. These relationships are represented with a Bayesian network. Marginal\nprobabilities are entered as soft evidence in the network and adjusted through\nprobabilistic inference. Our approach offers robust improvements in mean\naverage precision compared to the standard binary relavance approach across all\n12 datasets involved in our experiments. The discovery process helps\ninteresting implicit knowledge to emerge, which could be useful in itself.\n", "versions": [{"version": "v1", "created": "Tue, 15 Apr 2014 19:47:15 GMT"}, {"version": "v2", "created": "Thu, 17 Apr 2014 16:05:57 GMT"}], "update_date": "2014-04-18", "authors_parsed": [["Papagiannopoulou", "Christina", ""], ["Tsoumakas", "Grigorios", ""], ["Tsamardinos", "Ioannis", ""]]}, {"id": "1404.4088", "submitter": "Sumaira Tasnim", "authors": "Akhlaqur Rahman, Sumaira Tasnim", "title": "Ensemble Classifiers and Their Applications: A Review", "comments": "Published with International Journal of Computer Trends and\n  Technology (IJCTT)", "journal-ref": "International Journal of Computer Trends and Technology (IJCTT)\n  10(1):31-35, April 2014", "doi": "10.14445/22312803/IJCTT-V10P107", "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Ensemble classifier refers to a group of individual classifiers that are\ncooperatively trained on data set in a supervised classification problem. In\nthis paper we present a review of commonly used ensemble classifiers in the\nliterature. Some ensemble classifiers are also developed targeting specific\napplications. We also present some application driven ensemble classifiers in\nthis paper.\n", "versions": [{"version": "v1", "created": "Tue, 15 Apr 2014 21:35:48 GMT"}], "update_date": "2014-04-17", "authors_parsed": [["Rahman", "Akhlaqur", ""], ["Tasnim", "Sumaira", ""]]}, {"id": "1404.4095", "submitter": "Peter Mills", "authors": "Peter Mills", "title": "Multi-borders classification", "comments": "Corrected error in equations: second and third equations were not\n  linearly independent. Corrected figure to match. \"Hierarchical\" scheme is a\n  decision tree", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The number of possible methods of generalizing binary classification to\nmulti-class classification increases exponentially with the number of class\nlabels. Often, the best method of doing so will be highly problem dependent.\nHere we present classification software in which the partitioning of\nmulti-class classification problems into binary classification problems is\nspecified using a recursive control language.\n", "versions": [{"version": "v1", "created": "Tue, 15 Apr 2014 22:06:35 GMT"}, {"version": "v2", "created": "Thu, 17 Apr 2014 05:19:49 GMT"}, {"version": "v3", "created": "Mon, 19 May 2014 03:43:42 GMT"}], "update_date": "2014-05-20", "authors_parsed": [["Mills", "Peter", ""]]}, {"id": "1404.4104", "submitter": "Jianing Shi", "authors": "Jianing V. Shi, Yangyang Xu, and Richard G. Baraniuk", "title": "Sparse Bilinear Logistic Regression", "comments": "27 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we introduce the concept of sparse bilinear logistic\nregression for decision problems involving explanatory variables that are\ntwo-dimensional matrices. Such problems are common in computer vision,\nbrain-computer interfaces, style/content factorization, and parallel factor\nanalysis. The underlying optimization problem is bi-convex; we study its\nsolution and develop an efficient algorithm based on block coordinate descent.\nWe provide a theoretical guarantee for global convergence and estimate the\nasymptotical convergence rate using the Kurdyka-{\\L}ojasiewicz inequality. A\nrange of experiments with simulated and real data demonstrate that sparse\nbilinear logistic regression outperforms current techniques in several\nimportant applications.\n", "versions": [{"version": "v1", "created": "Tue, 15 Apr 2014 22:54:21 GMT"}], "update_date": "2014-04-17", "authors_parsed": [["Shi", "Jianing V.", ""], ["Xu", "Yangyang", ""], ["Baraniuk", "Richard G.", ""]]}, {"id": "1404.4105", "submitter": "Aur\\'elien Bellet", "authors": "Yuan Shi and Aur\\'elien Bellet and Fei Sha", "title": "Sparse Compositional Metric Learning", "comments": "18 pages. To be published in Proceedings of the 27th AAAI Conference\n  on Artificial Intelligence (AAAI 2014)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new approach for metric learning by framing it as learning a\nsparse combination of locally discriminative metrics that are inexpensive to\ngenerate from the training data. This flexible framework allows us to naturally\nderive formulations for global, multi-task and local metric learning. The\nresulting algorithms have several advantages over existing methods in the\nliterature: a much smaller number of parameters to be estimated and a\nprincipled way to generalize learned metrics to new testing data points. To\nanalyze the approach theoretically, we derive a generalization bound that\njustifies the sparse combination. Empirically, we evaluate our algorithms on\nseveral datasets against state-of-the-art metric learning methods. The results\nare consistent with our theoretical findings and demonstrate the superiority of\nour approach in terms of classification performance and scalability.\n", "versions": [{"version": "v1", "created": "Tue, 15 Apr 2014 22:55:53 GMT"}], "update_date": "2019-01-25", "authors_parsed": [["Shi", "Yuan", ""], ["Bellet", "Aur\u00e9lien", ""], ["Sha", "Fei", ""]]}, {"id": "1404.4108", "submitter": "Ouais Alsharif", "authors": "Ouais Alsharif, Philip Bachman, Joelle Pineau", "title": "Representation as a Service", "comments": "8 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Consider a Machine Learning Service Provider (MLSP) designed to rapidly\ncreate highly accurate learners for a never-ending stream of new tasks. The\nchallenge is to produce task-specific learners that can be trained from few\nlabeled samples, even if tasks are not uniquely identified, and the number of\ntasks and input dimensionality are large. In this paper, we argue that the MLSP\nshould exploit knowledge from previous tasks to build a good representation of\nthe environment it is in, and more precisely, that useful representations for\nsuch a service are ones that minimize generalization error for a new hypothesis\ntrained on a new task. We formalize this intuition with a novel method that\nminimizes an empirical proxy of the intra-task small-sample generalization\nerror. We present several empirical results showing state-of-the art\nperformance on single-task transfer, multitask learning, and the full lifelong\nlearning problem.\n", "versions": [{"version": "v1", "created": "Mon, 24 Feb 2014 15:17:39 GMT"}, {"version": "v2", "created": "Wed, 9 Jul 2014 07:17:54 GMT"}], "update_date": "2014-07-10", "authors_parsed": [["Alsharif", "Ouais", ""], ["Bachman", "Philip", ""], ["Pineau", "Joelle", ""]]}, {"id": "1404.4114", "submitter": "Matthew D. Hoffman", "authors": "Matthew D. Hoffman and David M. Blei", "title": "Structured Stochastic Variational Inference", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Stochastic variational inference makes it possible to approximate posterior\ndistributions induced by large datasets quickly using stochastic optimization.\nThe algorithm relies on the use of fully factorized variational distributions.\nHowever, this \"mean-field\" independence approximation limits the fidelity of\nthe posterior approximation, and introduces local optima. We show how to relax\nthe mean-field approximation to allow arbitrary dependencies between global\nparameters and local hidden variables, producing better parameter estimates by\nreducing bias, sensitivity to local optima, and sensitivity to hyperparameters.\n", "versions": [{"version": "v1", "created": "Wed, 16 Apr 2014 00:12:03 GMT"}, {"version": "v2", "created": "Tue, 25 Nov 2014 18:56:38 GMT"}, {"version": "v3", "created": "Wed, 26 Nov 2014 04:14:16 GMT"}], "update_date": "2014-11-27", "authors_parsed": [["Hoffman", "Matthew D.", ""], ["Blei", "David M.", ""]]}, {"id": "1404.4171", "submitter": "Ning Chen", "authors": "Ning Chen, Jun Zhu, Jianfei Chen, Bo Zhang", "title": "Dropout Training for Support Vector Machines", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Dropout and other feature noising schemes have shown promising results in\ncontrolling over-fitting by artificially corrupting the training data. Though\nextensive theoretical and empirical studies have been performed for generalized\nlinear models, little work has been done for support vector machines (SVMs),\none of the most successful approaches for supervised learning. This paper\npresents dropout training for linear SVMs. To deal with the intractable\nexpectation of the non-smooth hinge loss under corrupting distributions, we\ndevelop an iteratively re-weighted least square (IRLS) algorithm by exploring\ndata augmentation techniques. Our algorithm iteratively minimizes the\nexpectation of a re-weighted least square problem, where the re-weights have\nclosed-form solutions. The similar ideas are applied to develop a new IRLS\nalgorithm for the expected logistic loss under corrupting distributions. Our\nalgorithms offer insights on the connection and difference between the hinge\nloss and logistic loss in dropout training. Empirical results on several real\ndatasets demonstrate the effectiveness of dropout training on significantly\nboosting the classification accuracy of linear SVMs.\n", "versions": [{"version": "v1", "created": "Wed, 16 Apr 2014 08:54:01 GMT"}], "update_date": "2014-04-17", "authors_parsed": [["Chen", "Ning", ""], ["Zhu", "Jun", ""], ["Chen", "Jianfei", ""], ["Zhang", "Bo", ""]]}, {"id": "1404.4175", "submitter": "Emanuele Olivetti", "authors": "Emanuele Olivetti, Seyed Mostafa Kia, Paolo Avesani", "title": "MEG Decoding Across Subjects", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Brain decoding is a data analysis paradigm for neuroimaging experiments that\nis based on predicting the stimulus presented to the subject from the\nconcurrent brain activity. In order to make inference at the group level, a\nstraightforward but sometimes unsuccessful approach is to train a classifier on\nthe trials of a group of subjects and then to test it on unseen trials from new\nsubjects. The extreme difficulty is related to the structural and functional\nvariability across the subjects. We call this approach \"decoding across\nsubjects\". In this work, we address the problem of decoding across subjects for\nmagnetoencephalographic (MEG) experiments and we provide the following\ncontributions: first, we formally describe the problem and show that it belongs\nto a machine learning sub-field called transductive transfer learning (TTL).\nSecond, we propose to use a simple TTL technique that accounts for the\ndifferences between train data and test data. Third, we propose the use of\nensemble learning, and specifically of stacked generalization, to address the\nvariability across subjects within train data, with the aim of producing more\nstable classifiers. On a face vs. scramble task MEG dataset of 16 subjects, we\ncompare the standard approach of not modelling the differences across subjects,\nto the proposed one of combining TTL and ensemble learning. We show that the\nproposed approach is consistently more accurate than the standard one.\n", "versions": [{"version": "v1", "created": "Wed, 16 Apr 2014 09:21:26 GMT"}], "update_date": "2014-04-17", "authors_parsed": [["Olivetti", "Emanuele", ""], ["Kia", "Seyed Mostafa", ""], ["Avesani", "Paolo", ""]]}, {"id": "1404.4326", "submitter": "Jason  Weston", "authors": "Antoine Bordes, Jason Weston and Nicolas Usunier", "title": "Open Question Answering with Weakly Supervised Embedding Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Building computers able to answer questions on any subject is a long standing\ngoal of artificial intelligence. Promising progress has recently been achieved\nby methods that learn to map questions to logical forms or database queries.\nSuch approaches can be effective but at the cost of either large amounts of\nhuman-labeled data or by defining lexicons and grammars tailored by\npractitioners. In this paper, we instead take the radical approach of learning\nto map questions to vectorial feature representations. By mapping answers into\nthe same space one can query any knowledge base independent of its schema,\nwithout requiring any grammar or lexicon. Our method is trained with a new\noptimization procedure combining stochastic gradient descent followed by a\nfine-tuning step using the weak supervision provided by blending automatically\nand collaboratively generated resources. We empirically demonstrate that our\nmodel can capture meaningful signals from its noisy supervision leading to\nmajor improvements over paralex, the only existing method able to be trained on\nsimilar weakly labeled data.\n", "versions": [{"version": "v1", "created": "Wed, 16 Apr 2014 17:57:01 GMT"}], "update_date": "2014-04-17", "authors_parsed": [["Bordes", "Antoine", ""], ["Weston", "Jason", ""], ["Usunier", "Nicolas", ""]]}, {"id": "1404.4351", "submitter": "Navodit Misra", "authors": "Navodit Misra and Ercan E. Kuruoglu", "title": "Stable Graphical Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Stable random variables are motivated by the central limit theorem for\ndensities with (potentially) unbounded variance and can be thought of as\nnatural generalizations of the Gaussian distribution to skewed and heavy-tailed\nphenomenon. In this paper, we introduce stable graphical (SG) models, a class\nof multivariate stable densities that can also be represented as Bayesian\nnetworks whose edges encode linear dependencies between random variables. One\nmajor hurdle to the extensive use of stable distributions is the lack of a\nclosed-form analytical expression for their densities. This makes penalized\nmaximum-likelihood based learning computationally demanding. We establish\ntheoretically that the Bayesian information criterion (BIC) can asymptotically\nbe reduced to the computationally more tractable minimum dispersion criterion\n(MDC) and develop StabLe, a structure learning algorithm based on MDC. We use\nsimulated datasets for five benchmark network topologies to empirically\ndemonstrate how StabLe improves upon ordinary least squares (OLS) regression.\nWe also apply StabLe to microarray gene expression data for lymphoblastoid\ncells from 727 individuals belonging to eight global population groups. We\nestablish that StabLe improves test set performance relative to OLS via\nten-fold cross-validation. Finally, we develop SGEX, a method for quantifying\ndifferential expression of genes between different population groups.\n", "versions": [{"version": "v1", "created": "Wed, 16 Apr 2014 19:12:47 GMT"}], "update_date": "2014-04-17", "authors_parsed": [["Misra", "Navodit", ""], ["Kuruoglu", "Ercan E.", ""]]}, {"id": "1404.4412", "submitter": "Guoxu Zhou", "authors": "Guoxu Zhou and Andrzej Cichocki and Qibin Zhao and Shengli Xie", "title": "Efficient Nonnegative Tucker Decompositions: Algorithms and Uniqueness", "comments": "appears in IEEE Transactions on Image Processing, 2015", "journal-ref": null, "doi": "10.1109/TIP.2015.2478396", "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Nonnegative Tucker decomposition (NTD) is a powerful tool for the extraction\nof nonnegative parts-based and physically meaningful latent components from\nhigh-dimensional tensor data while preserving the natural multilinear structure\nof data. However, as the data tensor often has multiple modes and is\nlarge-scale, existing NTD algorithms suffer from a very high computational\ncomplexity in terms of both storage and computation time, which has been one\nmajor obstacle for practical applications of NTD. To overcome these\ndisadvantages, we show how low (multilinear) rank approximation (LRA) of\ntensors is able to significantly simplify the computation of the gradients of\nthe cost function, upon which a family of efficient first-order NTD algorithms\nare developed. Besides dramatically reducing the storage complexity and running\ntime, the new algorithms are quite flexible and robust to noise because any\nwell-established LRA approaches can be applied. We also show how nonnegativity\nincorporating sparsity substantially improves the uniqueness property and\npartially alleviates the curse of dimensionality of the Tucker decompositions.\nSimulation results on synthetic and real-world data justify the validity and\nhigh efficiency of the proposed NTD algorithms.\n", "versions": [{"version": "v1", "created": "Thu, 17 Apr 2014 01:52:09 GMT"}, {"version": "v2", "created": "Wed, 16 Sep 2015 08:58:14 GMT"}], "update_date": "2015-09-17", "authors_parsed": [["Zhou", "Guoxu", ""], ["Cichocki", "Andrzej", ""], ["Zhao", "Qibin", ""], ["Xie", "Shengli", ""]]}, {"id": "1404.4606", "submitter": "Derek Greene", "authors": "Derek Greene, Derek O'Callaghan, P\\'adraig Cunningham", "title": "How Many Topics? Stability Analysis for Topic Models", "comments": "Improve readability of plots. Add minor clarifications", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Topic modeling refers to the task of discovering the underlying thematic\nstructure in a text corpus, where the output is commonly presented as a report\nof the top terms appearing in each topic. Despite the diversity of topic\nmodeling algorithms that have been proposed, a common challenge in successfully\napplying these techniques is the selection of an appropriate number of topics\nfor a given corpus. Choosing too few topics will produce results that are\noverly broad, while choosing too many will result in the \"over-clustering\" of a\ncorpus into many small, highly-similar topics. In this paper, we propose a\nterm-centric stability analysis strategy to address this issue, the idea being\nthat a model with an appropriate number of topics will be more robust to\nperturbations in the data. Using a topic modeling approach based on matrix\nfactorization, evaluations performed on a range of corpora show that this\nstrategy can successfully guide the model selection process.\n", "versions": [{"version": "v1", "created": "Wed, 16 Apr 2014 12:59:29 GMT"}, {"version": "v2", "created": "Tue, 20 May 2014 12:58:28 GMT"}, {"version": "v3", "created": "Thu, 19 Jun 2014 12:58:13 GMT"}], "update_date": "2014-06-20", "authors_parsed": [["Greene", "Derek", ""], ["O'Callaghan", "Derek", ""], ["Cunningham", "P\u00e1draig", ""]]}, {"id": "1404.4644", "submitter": "Ping Li", "authors": "Anshumali Shrivastava and Ping Li", "title": "A New Space for Comparing Graphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.IR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Finding a new mathematical representations for graph, which allows direct\ncomparison between different graph structures, is an open-ended research\ndirection. Having such a representation is the first prerequisite for a variety\nof machine learning algorithms like classification, clustering, etc., over\ngraph datasets. In this paper, we propose a symmetric positive semidefinite\nmatrix with the $(i,j)$-{th} entry equal to the covariance between normalized\nvectors $A^ie$ and $A^je$ ($e$ being vector of all ones) as a representation\nfor graph with adjacency matrix $A$. We show that the proposed matrix\nrepresentation encodes the spectrum of the underlying adjacency matrix and it\nalso contains information about the counts of small sub-structures present in\nthe graph such as triangles and small paths. In addition, we show that this\nmatrix is a \\emph{\"graph invariant\"}. All these properties make the proposed\nmatrix a suitable object for representing graphs.\n  The representation, being a covariance matrix in a fixed dimensional metric\nspace, gives a mathematical embedding for graphs. This naturally leads to a\nmeasure of similarity on graph objects. We define similarity between two given\ngraphs as a Bhattacharya similarity measure between their corresponding\ncovariance matrix representations. As shown in our experimental study on the\ntask of social network classification, such a similarity measure outperforms\nother widely used state-of-the-art methodologies. Our proposed method is also\ncomputationally efficient. The computation of both the matrix representation\nand the similarity value can be performed in operations linear in the number of\nedges. This makes our method scalable in practice.\n  We believe our theoretical and empirical results provide evidence for\nstudying truncated power iterations, of the adjacency matrix, to characterize\nsocial networks.\n", "versions": [{"version": "v1", "created": "Thu, 17 Apr 2014 20:39:24 GMT"}], "update_date": "2014-04-21", "authors_parsed": [["Shrivastava", "Anshumali", ""], ["Li", "Ping", ""]]}, {"id": "1404.4646", "submitter": "Ping Li", "authors": "Guangcan Liu and Ping Li", "title": "Advancing Matrix Completion by Modeling Extra Structures beyond\n  Low-Rankness", "comments": "arXiv admin note: text overlap with arXiv:1404.4032", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.IT cs.LG math.IT math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A well-known method for completing low-rank matrices based on convex\noptimization has been established by Cand{\\`e}s and Recht. Although\ntheoretically complete, the method may not entirely solve the low-rank matrix\ncompletion problem. This is because the method captures only the low-rankness\nproperty which gives merely a rough constraint that the data points locate on\nsome low-dimensional subspace, but generally ignores the extra structures which\nspecify in more detail how the data points locate on the subspace. Whenever the\ngeometric distribution of the data points is not uniform, the coherence\nparameters of data might be large and, accordingly, the method might fail even\nif the latent matrix we want to recover is fairly low-rank. To better handle\nnon-uniform data, in this paper we propose a method termed Low-Rank Factor\nDecomposition (LRFD), which imposes an additional restriction that the data\npoints must be represented as linear combinations of the bases in a dictionary\nconstructed or learnt in advance. We show that LRFD can well handle non-uniform\ndata, provided that the dictionary is configured properly: We mathematically\nprove that if the dictionary itself is low-rank then LRFD is immune to the\ncoherence parameters which might be large on non-uniform data. This provides an\nelementary principle for learning the dictionary in LRFD and, naturally, leads\nto a practical algorithm for advancing matrix completion. Extensive experiments\non randomly generated matrices and motion datasets show encouraging results.\n", "versions": [{"version": "v1", "created": "Thu, 17 Apr 2014 20:50:26 GMT"}, {"version": "v2", "created": "Wed, 16 Jul 2014 18:04:35 GMT"}], "update_date": "2014-07-17", "authors_parsed": [["Liu", "Guangcan", ""], ["Li", "Ping", ""]]}, {"id": "1404.4655", "submitter": "Santiago Segarra", "authors": "Gunnar Carlsson, Facundo M\\'emoli, Alejandro Ribeiro, Santiago Segarra", "title": "Hierarchical Quasi-Clustering Methods for Asymmetric Networks", "comments": "Accepted to the 31st International Conference on Machine Learning\n  (ICML), Beijing, China, 2014", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces hierarchical quasi-clustering methods, a generalization\nof hierarchical clustering for asymmetric networks where the output structure\npreserves the asymmetry of the input data. We show that this output structure\nis equivalent to a finite quasi-ultrametric space and study admissibility with\nrespect to two desirable properties. We prove that a modified version of single\nlinkage is the only admissible quasi-clustering method. Moreover, we show\nstability of the proposed method and we establish invariance properties\nfulfilled by it. Algorithms are further developed and the value of\nquasi-clustering analysis is illustrated with a study of internal migration\nwithin United States.\n", "versions": [{"version": "v1", "created": "Thu, 17 Apr 2014 21:16:13 GMT"}], "update_date": "2014-04-21", "authors_parsed": [["Carlsson", "Gunnar", ""], ["M\u00e9moli", "Facundo", ""], ["Ribeiro", "Alejandro", ""], ["Segarra", "Santiago", ""]]}, {"id": "1404.4667", "submitter": "Morteza Mardani", "authors": "Morteza Mardani, Gonzalo Mateos, and Georgios B. Giannakis", "title": "Subspace Learning and Imputation for Streaming Big Data Matrices and\n  Tensors", "comments": null, "journal-ref": null, "doi": "10.1109/TSP.2015.2417491", "report-no": null, "categories": "stat.ML cs.IT cs.LG math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Extracting latent low-dimensional structure from high-dimensional data is of\nparamount importance in timely inference tasks encountered with `Big Data'\nanalytics. However, increasingly noisy, heterogeneous, and incomplete datasets\nas well as the need for {\\em real-time} processing of streaming data pose major\nchallenges to this end. In this context, the present paper permeates benefits\nfrom rank minimization to scalable imputation of missing data, via tracking\nlow-dimensional subspaces and unraveling latent (possibly multi-way) structure\nfrom \\emph{incomplete streaming} data. For low-rank matrix data, a subspace\nestimator is proposed based on an exponentially-weighted least-squares\ncriterion regularized with the nuclear norm. After recasting the non-separable\nnuclear norm into a form amenable to online optimization, real-time algorithms\nwith complementary strengths are developed and their convergence is established\nunder simplifying technical assumptions. In a stationary setting, the\nasymptotic estimates obtained offer the well-documented performance guarantees\nof the {\\em batch} nuclear-norm regularized estimator. Under the same unifying\nframework, a novel online (adaptive) algorithm is developed to obtain multi-way\ndecompositions of \\emph{low-rank tensors} with missing entries, and perform\nimputation as a byproduct. Simulated tests with both synthetic as well as real\nInternet and cardiac magnetic resonance imagery (MRI) data confirm the efficacy\nof the proposed algorithms, and their superior performance relative to\nstate-of-the-art alternatives.\n", "versions": [{"version": "v1", "created": "Thu, 17 Apr 2014 22:55:08 GMT"}], "update_date": "2015-06-19", "authors_parsed": [["Mardani", "Morteza", ""], ["Mateos", "Gonzalo", ""], ["Giannakis", "Georgios B.", ""]]}, {"id": "1404.4702", "submitter": "Vitaly Feldman", "authors": "Vitaly Feldman, Pravesh Kothari and Jan Vondr\\'ak", "title": "Tight Bounds on $\\ell_1$ Approximation and Learning of Self-Bounding\n  Functions", "comments": "Fixed minor mistakes and typos", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the complexity of learning and approximation of self-bounding\nfunctions over the uniform distribution on the Boolean hypercube ${0,1}^n$.\nInformally, a function $f:{0,1}^n \\rightarrow \\mathbb{R}$ is self-bounding if\nfor every $x \\in {0,1}^n$, $f(x)$ upper bounds the sum of all the $n$ marginal\ndecreases in the value of the function at $x$. Self-bounding functions include\nsuch well-known classes of functions as submodular and fractionally-subadditive\n(XOS) functions. They were introduced by Boucheron et al. (2000) in the context\nof concentration of measure inequalities. Our main result is a nearly tight\n$\\ell_1$-approximation of self-bounding functions by low-degree juntas.\nSpecifically, all self-bounding functions can be $\\epsilon$-approximated in\n$\\ell_1$ by a polynomial of degree $\\tilde{O}(1/\\epsilon)$ over\n$2^{\\tilde{O}(1/\\epsilon)}$ variables. We show that both the degree and\njunta-size are optimal up to logarithmic terms. Previous techniques considered\nstronger $\\ell_2$ approximation and proved nearly tight bounds of\n$\\Theta(1/\\epsilon^{2})$ on the degree and $2^{\\Theta(1/\\epsilon^2)}$ on the\nnumber of variables. Our bounds rely on the analysis of noise stability of\nself-bounding functions together with a stronger connection between noise\nstability and $\\ell_1$ approximation by low-degree polynomials. This technique\ncan also be used to get tighter bounds on $\\ell_1$ approximation by low-degree\npolynomials and faster learning algorithm for halfspaces.\n  These results lead to improved and in several cases almost tight bounds for\nPAC and agnostic learning of self-bounding functions relative to the uniform\ndistribution. In particular, assuming hardness of learning juntas, we show that\nPAC and agnostic learning of self-bounding functions have complexity of\n$n^{\\tilde{\\Theta}(1/\\epsilon)}$.\n", "versions": [{"version": "v1", "created": "Fri, 18 Apr 2014 06:49:49 GMT"}, {"version": "v2", "created": "Fri, 22 Sep 2017 07:38:04 GMT"}, {"version": "v3", "created": "Sat, 1 Jun 2019 20:01:23 GMT"}], "update_date": "2019-06-04", "authors_parsed": [["Feldman", "Vitaly", ""], ["Kothari", "Pravesh", ""], ["Vondr\u00e1k", "Jan", ""]]}, {"id": "1404.4888", "submitter": "Isadora Nun Ms", "authors": "Isadora Nun, Karim Pichara, Pavlos Protopapas, Dae-Won Kim", "title": "Supervised detection of anomalous light-curves in massive astronomical\n  catalogs", "comments": "16 pages, 18 figures, published in The Astrophysical Journal", "journal-ref": "2014, ApJ, 793, 23", "doi": "10.1088/0004-637X/793/1/23", "report-no": null, "categories": "cs.CE astro-ph.IM cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The development of synoptic sky surveys has led to a massive amount of data\nfor which resources needed for analysis are beyond human capabilities. To\nprocess this information and to extract all possible knowledge, machine\nlearning techniques become necessary. Here we present a new method to\nautomatically discover unknown variable objects in large astronomical catalogs.\nWith the aim of taking full advantage of all the information we have about\nknown objects, our method is based on a supervised algorithm. In particular, we\ntrain a random forest classifier using known variability classes of objects and\nobtain votes for each of the objects in the training set. We then model this\nvoting distribution with a Bayesian network and obtain the joint voting\ndistribution among the training objects. Consequently, an unknown object is\nconsidered as an outlier insofar it has a low joint probability. Our method is\nsuitable for exploring massive datasets given that the training process is\nperformed offline. We tested our algorithm on 20 millions light-curves from the\nMACHO catalog and generated a list of anomalous candidates. We divided the\ncandidates into two main classes of outliers: artifacts and intrinsic outliers.\nArtifacts were principally due to air mass variation, seasonal variation, bad\ncalibration or instrumental errors and were consequently removed from our\noutlier list and added to the training set. After retraining, we selected about\n4000 objects, which we passed to a post analysis stage by perfoming a\ncross-match with all publicly available catalogs. Within these candidates we\nidentified certain known but rare objects such as eclipsing Cepheids, blue\nvariables, cataclysmic variables and X-ray sources. For some outliers there\nwere no additional information. Among them we identified three unknown\nvariability types and few individual outliers that will be followed up for a\ndeeper analysis.\n", "versions": [{"version": "v1", "created": "Fri, 18 Apr 2014 21:12:13 GMT"}, {"version": "v2", "created": "Wed, 3 Sep 2014 15:50:49 GMT"}, {"version": "v3", "created": "Wed, 27 May 2015 21:27:11 GMT"}], "update_date": "2015-05-29", "authors_parsed": [["Nun", "Isadora", ""], ["Pichara", "Karim", ""], ["Protopapas", "Pavlos", ""], ["Kim", "Dae-Won", ""]]}, {"id": "1404.4893", "submitter": "Daniele Codecasa", "authors": "Daniele Codecasa and Fabio Stella", "title": "CTBNCToolkit: Continuous Time Bayesian Network Classifier Toolkit", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG cs.MS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Continuous time Bayesian network classifiers are designed for temporal\nclassification of multivariate streaming data when time duration of events\nmatters and the class does not change over time. This paper introduces the\nCTBNCToolkit: an open source Java toolkit which provides a stand-alone\napplication for temporal classification and a library for continuous time\nBayesian network classifiers. CTBNCToolkit implements the inference algorithm,\nthe parameter learning algorithm, and the structural learning algorithm for\ncontinuous time Bayesian network classifiers. The structural learning algorithm\nis based on scoring functions: the marginal log-likelihood score and the\nconditional log-likelihood score are provided. CTBNCToolkit provides also an\nimplementation of the expectation maximization algorithm for clustering\npurpose. The paper introduces continuous time Bayesian network classifiers. How\nto use the CTBNToolkit from the command line is described in a specific\nsection. Tutorial examples are included to facilitate users to understand how\nthe toolkit must be used. A section dedicate to the Java library is proposed to\nhelp further code extensions.\n", "versions": [{"version": "v1", "created": "Fri, 18 Apr 2014 21:48:34 GMT"}], "update_date": "2014-04-22", "authors_parsed": [["Codecasa", "Daniele", ""], ["Stella", "Fabio", ""]]}, {"id": "1404.4960", "submitter": "Fei Tian", "authors": "Fei Tian, Haifang Li, Wei Chen, Tao Qin, Enhong Chen, Tie-Yan Liu", "title": "Agent Behavior Prediction and Its Generalization Analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Machine learning algorithms have been applied to predict agent behaviors in\nreal-world dynamic systems, such as advertiser behaviors in sponsored search\nand worker behaviors in crowdsourcing. The behavior data in these systems are\ngenerated by live agents: once the systems change due to the adoption of the\nprediction models learnt from the behavior data, agents will observe and\nrespond to these changes by changing their own behaviors accordingly. As a\nresult, the behavior data will evolve and will not be identically and\nindependently distributed, posing great challenges to the theoretical analysis\non the machine learning algorithms for behavior prediction. To tackle this\nchallenge, in this paper, we propose to use Markov Chain in Random Environments\n(MCRE) to describe the behavior data, and perform generalization analysis of\nthe machine learning algorithms on its basis. Since the one-step transition\nprobability matrix of MCRE depends on both previous states and the random\nenvironment, conventional techniques for generalization analysis cannot be\ndirectly applied. To address this issue, we propose a novel technique that\ntransforms the original MCRE into a higher-dimensional time-homogeneous Markov\nchain. The new Markov chain involves more variables but is more regular, and\nthus easier to deal with. We prove the convergence of the new Markov chain when\ntime approaches infinity. Then we prove a generalization bound for the machine\nlearning algorithms on the behavior data generated by the new Markov chain,\nwhich depends on both the Markovian parameters and the covering number of the\nfunction class compounded by the loss function for behavior prediction and the\nbehavior prediction model. To the best of our knowledge, this is the first work\nthat performs the generalization analysis on data generated by complex\nprocesses in real-world dynamic systems.\n", "versions": [{"version": "v1", "created": "Sat, 19 Apr 2014 14:57:54 GMT"}, {"version": "v2", "created": "Fri, 11 Jul 2014 06:30:18 GMT"}], "update_date": "2014-07-14", "authors_parsed": [["Tian", "Fei", ""], ["Li", "Haifang", ""], ["Chen", "Wei", ""], ["Qin", "Tao", ""], ["Chen", "Enhong", ""], ["Liu", "Tie-Yan", ""]]}, {"id": "1404.4997", "submitter": "Eric Price", "authors": "Moritz Hardt and Eric Price", "title": "Tight bounds for learning a mixture of two gaussians", "comments": "STOC 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  We consider the problem of identifying the parameters of an unknown mixture\nof two arbitrary $d$-dimensional gaussians from a sequence of independent\nrandom samples. Our main results are upper and lower bounds giving a\ncomputationally efficient moment-based estimator with an optimal convergence\nrate, thus resolving a problem introduced by Pearson (1894). Denoting by\n$\\sigma^2$ the variance of the unknown mixture, we prove that\n$\\Theta(\\sigma^{12})$ samples are necessary and sufficient to estimate each\nparameter up to constant additive error when $d=1.$ Our upper bound extends to\narbitrary dimension $d>1$ up to a (provably necessary) logarithmic loss in $d$\nusing a novel---yet simple---dimensionality reduction technique. We further\nidentify several interesting special cases where the sample complexity is\nnotably smaller than our optimal worst-case bound. For instance, if the means\nof the two components are separated by $\\Omega(\\sigma)$ the sample complexity\nreduces to $O(\\sigma^2)$ and this is again optimal.\n  Our results also apply to learning each component of the mixture up to small\nerror in total variation distance, where our algorithm gives strong\nimprovements in sample complexity over previous work. We also extend our lower\nbound to mixtures of $k$ Gaussians, showing that $\\Omega(\\sigma^{6k-2})$\nsamples are necessary to estimate each parameter up to constant additive error.\n", "versions": [{"version": "v1", "created": "Sat, 19 Apr 2014 23:59:35 GMT"}, {"version": "v2", "created": "Mon, 8 Dec 2014 22:15:35 GMT"}, {"version": "v3", "created": "Sun, 17 May 2015 04:47:58 GMT"}], "update_date": "2015-05-19", "authors_parsed": [["Hardt", "Moritz", ""], ["Price", "Eric", ""]]}, {"id": "1404.5009", "submitter": "Chunhua Shen", "authors": "Peng Wang, Chunhua Shen, Anton van den Hengel, Philip Torr", "title": "Efficient Semidefinite Branch-and-Cut for MAP-MRF Inference", "comments": "21 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a Branch-and-Cut (B&C) method for solving general MAP-MRF\ninference problems. The core of our method is a very efficient bounding\nprocedure, which combines scalable semidefinite programming (SDP) and a\ncutting-plane method for seeking violated constraints. In order to further\nspeed up the computation, several strategies have been exploited, including\nmodel reduction, warm start and removal of inactive constraints.\n  We analyze the performance of the proposed method under different settings,\nand demonstrate that our method either outperforms or performs on par with\nstate-of-the-art approaches. Especially when the connectivities are dense or\nwhen the relative magnitudes of the unary costs are low, we achieve the best\nreported results. Experiments show that the proposed algorithm achieves better\napproximation than the state-of-the-art methods within a variety of time\nbudgets on challenging non-submodular MAP-MRF inference problems.\n", "versions": [{"version": "v1", "created": "Sun, 20 Apr 2014 04:47:04 GMT"}, {"version": "v2", "created": "Tue, 16 Dec 2014 03:43:41 GMT"}, {"version": "v3", "created": "Thu, 9 Jul 2015 08:23:09 GMT"}, {"version": "v4", "created": "Wed, 9 Sep 2015 04:35:30 GMT"}], "update_date": "2015-09-10", "authors_parsed": [["Wang", "Peng", ""], ["Shen", "Chunhua", ""], ["Hengel", "Anton van den", ""], ["Torr", "Philip", ""]]}, {"id": "1404.5065", "submitter": "Eleftherios Spyromitros-Xioufis", "authors": "Grigorios Tsoumakas, Eleftherios Spyromitros-Xioufis, Aikaterini\n  Vrekou, Ioannis Vlahavas", "title": "Multi-Target Regression via Random Linear Target Combinations", "comments": null, "journal-ref": "ECML PKDD Proceedings, Part III (2014) 225-240", "doi": "10.1007/978-3-662-44845-8_15", "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multi-target regression is concerned with the simultaneous prediction of\nmultiple continuous target variables based on the same set of input variables.\nIt arises in several interesting industrial and environmental application\ndomains, such as ecological modelling and energy forecasting. This paper\npresents an ensemble method for multi-target regression that constructs new\ntarget variables via random linear combinations of existing targets. We discuss\nthe connection of our approach with multi-label classification algorithms, in\nparticular RA$k$EL, which originally inspired this work, and a family of recent\nmulti-label classification algorithms that involve output coding. Experimental\nresults on 12 multi-target datasets show that it performs significantly better\nthan a strong baseline that learns a single model for each target using\ngradient boosting and compares favourably to multi-objective random forest\napproach, which is a state-of-the-art approach. The experiments further show\nthat our approach improves more when stronger unconditional dependencies exist\namong the targets.\n", "versions": [{"version": "v1", "created": "Sun, 20 Apr 2014 19:17:23 GMT"}], "update_date": "2015-05-05", "authors_parsed": [["Tsoumakas", "Grigorios", ""], ["Spyromitros-Xioufis", "Eleftherios", ""], ["Vrekou", "Aikaterini", ""], ["Vlahavas", "Ioannis", ""]]}, {"id": "1404.5122", "submitter": "Zhilin Zhang", "authors": "Zhilin Zhang, Tzyy-Ping Jung, Scott Makeig, Zhouyue Pi, Bhaskar D. Rao", "title": "Spatiotemporal Sparse Bayesian Learning with Applications to Compressed\n  Sensing of Multichannel Physiological Signals", "comments": "Codes are available at:\n  https://sites.google.com/site/researchbyzhang/stsbl", "journal-ref": "IEEE Transactions On Neural Systems And Rehabilitation\n  Engineering, Vol. 22, No. 6, pp. 1186-1197, November 2014", "doi": "10.1109/TNSRE.2014.2319334", "report-no": null, "categories": "cs.IT cs.LG math.IT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Energy consumption is an important issue in continuous wireless\ntelemonitoring of physiological signals. Compressed sensing (CS) is a promising\nframework to address it, due to its energy-efficient data compression\nprocedure. However, most CS algorithms have difficulty in data recovery due to\nnon-sparsity characteristic of many physiological signals. Block sparse\nBayesian learning (BSBL) is an effective approach to recover such signals with\nsatisfactory recovery quality. However, it is time-consuming in recovering\nmultichannel signals, since its computational load almost linearly increases\nwith the number of channels.\n  This work proposes a spatiotemporal sparse Bayesian learning algorithm to\nrecover multichannel signals simultaneously. It not only exploits temporal\ncorrelation within each channel signal, but also exploits inter-channel\ncorrelation among different channel signals. Furthermore, its computational\nload is not significantly affected by the number of channels. The proposed\nalgorithm was applied to brain computer interface (BCI) and EEG-based driver's\ndrowsiness estimation. Results showed that the algorithm had both better\nrecovery performance and much higher speed than BSBL. Particularly, the\nproposed algorithm ensured that the BCI classification and the drowsiness\nestimation had little degradation even when data were compressed by 80%, making\nit very suitable for continuous wireless telemonitoring of multichannel\nsignals.\n", "versions": [{"version": "v1", "created": "Mon, 21 Apr 2014 06:35:57 GMT"}, {"version": "v2", "created": "Sat, 15 Nov 2014 01:53:29 GMT"}], "update_date": "2014-11-18", "authors_parsed": [["Zhang", "Zhilin", ""], ["Jung", "Tzyy-Ping", ""], ["Makeig", "Scott", ""], ["Pi", "Zhouyue", ""], ["Rao", "Bhaskar D.", ""]]}, {"id": "1404.5165", "submitter": "Kian Hsiang Low", "authors": "Nuo Xu, Kian Hsiang Low, Jie Chen, Keng Kiat Lim, Etkin Baris Ozgul", "title": "GP-Localize: Persistent Mobile Robot Localization using Online Sparse\n  Gaussian Process Observation Model", "comments": "28th AAAI Conference on Artificial Intelligence (AAAI 2014), Extended\n  version with proofs, 10 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Central to robot exploration and mapping is the task of persistent\nlocalization in environmental fields characterized by spatially correlated\nmeasurements. This paper presents a Gaussian process localization (GP-Localize)\nalgorithm that, in contrast to existing works, can exploit the spatially\ncorrelated field measurements taken during a robot's exploration (instead of\nrelying on prior training data) for efficiently and scalably learning the GP\nobservation model online through our proposed novel online sparse GP. As a\nresult, GP-Localize is capable of achieving constant time and memory (i.e.,\nindependent of the size of the data) per filtering step, which demonstrates the\npractical feasibility of using GPs for persistent robot localization and\nautonomy. Empirical evaluation via simulated experiments with real-world\ndatasets and a real robot experiment shows that GP-Localize outperforms\nexisting GP localization algorithms.\n", "versions": [{"version": "v1", "created": "Mon, 21 Apr 2014 10:28:00 GMT"}, {"version": "v2", "created": "Tue, 22 Apr 2014 08:03:33 GMT"}], "update_date": "2014-04-23", "authors_parsed": [["Xu", "Nuo", ""], ["Low", "Kian Hsiang", ""], ["Chen", "Jie", ""], ["Lim", "Keng Kiat", ""], ["Ozgul", "Etkin Baris", ""]]}, {"id": "1404.5214", "submitter": "Ping Li", "authors": "Anshumali Shrivastava and Ping Li", "title": "Graph Kernels via Functional Embedding", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a representation of graph as a functional object derived from the\npower iteration of the underlying adjacency matrix. The proposed functional\nrepresentation is a graph invariant, i.e., the functional remains unchanged\nunder any reordering of the vertices. This property eliminates the difficulty\nof handling exponentially many isomorphic forms. Bhattacharyya kernel\nconstructed between these functionals significantly outperforms the\nstate-of-the-art graph kernels on 3 out of the 4 standard benchmark graph\nclassification datasets, demonstrating the superiority of our approach. The\nproposed methodology is simple and runs in time linear in the number of edges,\nwhich makes our kernel more efficient and scalable compared to many widely\nadopted graph kernels with running time cubic in the number of vertices.\n", "versions": [{"version": "v1", "created": "Mon, 21 Apr 2014 14:56:17 GMT"}], "update_date": "2014-04-22", "authors_parsed": [["Shrivastava", "Anshumali", ""], ["Li", "Ping", ""]]}, {"id": "1404.5236", "submitter": "Boaz Barak", "authors": "Boaz Barak and David Steurer", "title": "Sum-of-squares proofs and the quest toward optimal algorithms", "comments": "Survey. To appear in proceedings of ICM 2014", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC cs.LG math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In order to obtain the best-known guarantees, algorithms are traditionally\ntailored to the particular problem we want to solve. Two recent developments,\nthe Unique Games Conjecture (UGC) and the Sum-of-Squares (SOS) method,\nsurprisingly suggest that this tailoring is not necessary and that a single\nefficient algorithm could achieve best possible guarantees for a wide range of\ndifferent problems.\n  The Unique Games Conjecture (UGC) is a tantalizing conjecture in\ncomputational complexity, which, if true, will shed light on the complexity of\na great many problems. In particular this conjecture predicts that a single\nconcrete algorithm provides optimal guarantees among all efficient algorithms\nfor a large class of computational problems.\n  The Sum-of-Squares (SOS) method is a general approach for solving systems of\npolynomial constraints. This approach is studied in several scientific\ndisciplines, including real algebraic geometry, proof complexity, control\ntheory, and mathematical programming, and has found applications in fields as\ndiverse as quantum information theory, formal verification, game theory and\nmany others.\n  We survey some connections that were recently uncovered between the Unique\nGames Conjecture and the Sum-of-Squares method. In particular, we discuss new\ntools to rigorously bound the running time of the SOS method for obtaining\napproximate solutions to hard optimization problems, and how these tools give\nthe potential for the sum-of-squares method to provide new guarantees for many\nproblems of interest, and possibly to even refute the UGC.\n", "versions": [{"version": "v1", "created": "Mon, 21 Apr 2014 16:24:13 GMT"}, {"version": "v2", "created": "Tue, 27 May 2014 17:52:52 GMT"}], "update_date": "2014-05-28", "authors_parsed": [["Barak", "Boaz", ""], ["Steurer", "David", ""]]}, {"id": "1404.5421", "submitter": "Orly Avner", "authors": "Orly Avner and Shie Mannor", "title": "Concurrent bandits and cognitive radio networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of multiple users targeting the arms of a single\nmulti-armed stochastic bandit. The motivation for this problem comes from\ncognitive radio networks, where selfish users need to coexist without any side\ncommunication between them, implicit cooperation or common control. Even the\nnumber of users may be unknown and can vary as users join or leave the network.\nWe propose an algorithm that combines an $\\epsilon$-greedy learning rule with a\ncollision avoidance mechanism. We analyze its regret with respect to the\nsystem-wide optimum and show that sub-linear regret can be obtained in this\nsetting. Experiments show dramatic improvement compared to other algorithms for\nthis setting.\n", "versions": [{"version": "v1", "created": "Tue, 22 Apr 2014 08:30:56 GMT"}], "update_date": "2014-04-23", "authors_parsed": [["Avner", "Orly", ""], ["Mannor", "Shie", ""]]}, {"id": "1404.5475", "submitter": "Rustem Takhanov", "authors": "Rustem Takhanov and Vladimir Kolmogorov", "title": "Combining pattern-based CRFs and weighted context-free grammars", "comments": "11 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.FL cs.DS cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider two models for the sequence labeling (tagging) problem. The first\none is a {\\em Pattern-Based Conditional Random Field }(\\PB), in which the\nenergy of a string (chain labeling) $x=x_1\\ldots x_n\\in D^n$ is a sum of terms\nover intervals $[i,j]$ where each term is non-zero only if the substring\n$x_i\\ldots x_j$ equals a prespecified word $w\\in \\Lambda$. The second model is\na {\\em Weighted Context-Free Grammar }(\\WCFG) frequently used for natural\nlanguage processing. \\PB and \\WCFG encode local and non-local interactions\nrespectively, and thus can be viewed as complementary.\n  We propose a {\\em Grammatical Pattern-Based CRF model }(\\GPB) that combines\nthe two in a natural way. We argue that it has certain advantages over existing\napproaches such as the {\\em Hybrid model} of Bened{\\'i} and Sanchez that\ncombines {\\em $\\mbox{$N$-grams}$} and \\WCFGs. The focus of this paper is to\nanalyze the complexity of inference tasks in a \\GPB such as computing MAP. We\npresent a polynomial-time algorithm for general \\GPBs and a faster version for\na special case that we call {\\em Interaction Grammars}.\n", "versions": [{"version": "v1", "created": "Tue, 22 Apr 2014 12:44:42 GMT"}, {"version": "v2", "created": "Sat, 1 Nov 2014 13:29:52 GMT"}], "update_date": "2014-11-04", "authors_parsed": [["Takhanov", "Rustem", ""], ["Kolmogorov", "Vladimir", ""]]}, {"id": "1404.5511", "submitter": "Robby Goetschalckx", "authors": "Robby Goetschalckx, Alan Fern, Prasad Tadepalli", "title": "Coactive Learning for Locally Optimal Problem Solving", "comments": "AAAI 2014 paper, including appendices", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Coactive learning is an online problem solving setting where the solutions\nprovided by a solver are interactively improved by a domain expert, which in\nturn drives learning. In this paper we extend the study of coactive learning to\nproblems where obtaining a globally optimal or near-optimal solution may be\nintractable or where an expert can only be expected to make small, local\nimprovements to a candidate solution. The goal of learning in this new setting\nis to minimize the cost as measured by the expert effort over time. We first\nestablish theoretical bounds on the average cost of the existing coactive\nPerceptron algorithm. In addition, we consider new online algorithms that use\ncost-sensitive and Passive-Aggressive (PA) updates, showing similar or improved\ntheoretical bounds. We provide an empirical evaluation of the learners in\nvarious domains, which show that the Perceptron based algorithms are quite\neffective and that unlike the case for online classification, the PA algorithms\ndo not yield significant performance gains.\n", "versions": [{"version": "v1", "created": "Fri, 18 Apr 2014 21:17:04 GMT"}], "update_date": "2014-04-23", "authors_parsed": [["Goetschalckx", "Robby", ""], ["Fern", "Alan", ""], ["Tadepalli", "Prasad", ""]]}, {"id": "1404.5521", "submitter": "Tanmay Sinha", "authors": "Tanmay Sinha", "title": "Together we stand, Together we fall, Together we win: Dynamic Team\n  Formation in Massive Open Online Courses", "comments": "In Proceedings of 5th IEEE International Conference on Application of\n  Digital Information & Web Technologies (ICADIWT), India, February 2014 (6\n  pages, 3 figures)", "journal-ref": null, "doi": "10.1109/ICADIWT.2014.6814694", "report-no": null, "categories": "cs.SI cs.CY cs.LG cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Massive Open Online Courses (MOOCs) offer a new scalable paradigm for\ne-learning by providing students with global exposure and opportunities for\nconnecting and interacting with millions of people all around the world. Very\noften, students work as teams to effectively accomplish course related tasks.\nHowever, due to lack of face to face interaction, it becomes difficult for MOOC\nstudents to collaborate. Additionally, the instructor also faces challenges in\nmanually organizing students into teams because students flock to these MOOCs\nin huge numbers. Thus, the proposed research is aimed at developing a robust\nmethodology for dynamic team formation in MOOCs, the theoretical framework for\nwhich is grounded at the confluence of organizational team theory, social\nnetwork analysis and machine learning. A prerequisite for such an undertaking\nis that we understand the fact that, each and every informal tie established\namong students offers the opportunities to influence and be influenced.\nTherefore, we aim to extract value from the inherent connectedness of students\nin the MOOC. These connections carry with them radical implications for the way\nstudents understand each other in the networked learning community. Our\napproach will enable course instructors to automatically group students in\nteams that have fairly balanced social connections with their peers, well\ndefined in terms of appropriately selected qualitative and quantitative network\nmetrics.\n", "versions": [{"version": "v1", "created": "Tue, 22 Apr 2014 15:12:17 GMT"}], "update_date": "2014-07-29", "authors_parsed": [["Sinha", "Tanmay", ""]]}, {"id": "1404.5692", "submitter": "Nikhil Rao", "authors": "Nikhil Rao, Parikshit Shah, Stephen Wright", "title": "Forward - Backward Greedy Algorithms for Atomic Norm Regularization", "comments": "To appear in IEEE Transactions on Signal Processing", "journal-ref": null, "doi": "10.1109/TSP.2015.2461515", "report-no": null, "categories": "cs.DS cs.LG math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In many signal processing applications, the aim is to reconstruct a signal\nthat has a simple representation with respect to a certain basis or frame.\nFundamental elements of the basis known as \"atoms\" allow us to define \"atomic\nnorms\" that can be used to formulate convex regularizations for the\nreconstruction problem. Efficient algorithms are available to solve these\nformulations in certain special cases, but an approach that works well for\ngeneral atomic norms, both in terms of speed and reconstruction accuracy,\nremains to be found. This paper describes an optimization algorithm called\nCoGEnT that produces solutions with succinct atomic representations for\nreconstruction problems, generally formulated with atomic-norm constraints.\nCoGEnT combines a greedy selection scheme based on the conditional gradient\napproach with a backward (or \"truncation\") step that exploits the quadratic\nnature of the objective to reduce the basis size. We establish convergence\nproperties and validate the algorithm via extensive numerical experiments on a\nsuite of signal processing applications. Our algorithm and analysis also allow\nfor inexact forward steps and for occasional enhancements of the current\nrepresentation to be performed. CoGEnT can outperform the basic conditional\ngradient method, and indeed many methods that are tailored to specific\napplications, when the enhancement and truncation steps are defined\nappropriately. We also introduce several novel applications that are enabled by\nthe atomic-norm framework, including tensor completion, moment problems in\nsignal processing, and graph deconvolution.\n", "versions": [{"version": "v1", "created": "Wed, 23 Apr 2014 03:31:45 GMT"}, {"version": "v2", "created": "Thu, 23 Jul 2015 01:33:16 GMT"}], "update_date": "2015-10-28", "authors_parsed": [["Rao", "Nikhil", ""], ["Shah", "Parikshit", ""], ["Wright", "Stephen", ""]]}, {"id": "1404.5772", "submitter": "Yuyu Zhang", "authors": "Yuyu Zhang, Hanjun Dai, Chang Xu, Jun Feng, Taifeng Wang, Jiang Bian,\n  Bin Wang and Tie-Yan Liu", "title": "Sequential Click Prediction for Sponsored Search with Recurrent Neural\n  Networks", "comments": "Accepted by AAAI 2014", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Click prediction is one of the fundamental problems in sponsored search. Most\nof existing studies took advantage of machine learning approaches to predict ad\nclick for each event of ad view independently. However, as observed in the\nreal-world sponsored search system, user's behaviors on ads yield high\ndependency on how the user behaved along with the past time, especially in\nterms of what queries she submitted, what ads she clicked or ignored, and how\nlong she spent on the landing pages of clicked ads, etc. Inspired by these\nobservations, we introduce a novel framework based on Recurrent Neural Networks\n(RNN). Compared to traditional methods, this framework directly models the\ndependency on user's sequential behaviors into the click prediction process\nthrough the recurrent structure in RNN. Large scale evaluations on the\nclick-through logs from a commercial search engine demonstrate that our\napproach can significantly improve the click prediction accuracy, compared to\nsequence-independent approaches.\n", "versions": [{"version": "v1", "created": "Wed, 23 Apr 2014 10:14:41 GMT"}, {"version": "v2", "created": "Mon, 28 Apr 2014 05:56:18 GMT"}, {"version": "v3", "created": "Mon, 28 Jul 2014 13:59:03 GMT"}], "update_date": "2014-07-29", "authors_parsed": [["Zhang", "Yuyu", ""], ["Dai", "Hanjun", ""], ["Xu", "Chang", ""], ["Feng", "Jun", ""], ["Wang", "Taifeng", ""], ["Bian", "Jiang", ""], ["Wang", "Bin", ""], ["Liu", "Tie-Yan", ""]]}, {"id": "1404.5899", "submitter": "Ran Zhao", "authors": "Ran Zhao, Deanna Needell, Christopher Johansen, Jerry L. Grenard", "title": "A Comparison of Clustering and Missing Data Methods for Health Sciences", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.NA cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we compare and analyze clustering methods with missing data in\nhealth behavior research. In particular, we propose and analyze the use of\ncompressive sensing's matrix completion along with spectral clustering to\ncluster health related data. The empirical tests and real data results show\nthat these methods can outperform standard methods like LPA and FIML, in terms\nof lower misclassification rates in clustering and better matrix completion\nperformance in missing data problems. According to our examination, a possible\nexplanation of these improvements is that spectral clustering takes advantage\nof high data dimension and compressive sensing methods utilize the\nnear-to-low-rank property of health data.\n", "versions": [{"version": "v1", "created": "Tue, 22 Apr 2014 05:04:00 GMT"}], "update_date": "2014-04-24", "authors_parsed": [["Zhao", "Ran", ""], ["Needell", "Deanna", ""], ["Johansen", "Christopher", ""], ["Grenard", "Jerry L.", ""]]}, {"id": "1404.5903", "submitter": "Che-Yu Liu Mr.", "authors": "Che-Yu Liu, S\\'ebastien Bubeck", "title": "Most Correlated Arms Identification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of finding the most mutually correlated arms among many\narms. We show that adaptive arms sampling strategies can have significant\nadvantages over the non-adaptive uniform sampling strategy. Our proposed\nalgorithms rely on a novel correlation estimator. The use of this accurate\nestimator allows us to get improved results for a wide range of problem\ninstances.\n", "versions": [{"version": "v1", "created": "Wed, 23 Apr 2014 17:25:02 GMT"}], "update_date": "2014-04-24", "authors_parsed": [["Liu", "Che-Yu", ""], ["Bubeck", "S\u00e9bastien", ""]]}, {"id": "1404.5997", "submitter": "Alex Krizhevsky", "authors": "Alex Krizhevsky", "title": "One weird trick for parallelizing convolutional neural networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.DC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  I present a new way to parallelize the training of convolutional neural\nnetworks across multiple GPUs. The method scales significantly better than all\nalternatives when applied to modern convolutional neural networks.\n", "versions": [{"version": "v1", "created": "Wed, 23 Apr 2014 22:37:56 GMT"}, {"version": "v2", "created": "Sat, 26 Apr 2014 23:10:51 GMT"}], "update_date": "2014-04-29", "authors_parsed": [["Krizhevsky", "Alex", ""]]}, {"id": "1404.6074", "submitter": "Marie Schrynemackers", "authors": "Marie Schrynemackers, Louis Wehenkel, M. Madan Babu and Pierre Geurts", "title": "Classifying pairs with trees for supervised biological network inference", "comments": "22 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Networks are ubiquitous in biology and computational approaches have been\nlargely investigated for their inference. In particular, supervised machine\nlearning methods can be used to complete a partially known network by\nintegrating various measurements. Two main supervised frameworks have been\nproposed: the local approach, which trains a separate model for each network\nnode, and the global approach, which trains a single model over pairs of nodes.\nHere, we systematically investigate, theoretically and empirically, the\nexploitation of tree-based ensemble methods in the context of these two\napproaches for biological network inference. We first formalize the problem of\nnetwork inference as classification of pairs, unifying in the process\nhomogeneous and bipartite graphs and discussing two main sampling schemes. We\nthen present the global and the local approaches, extending the later for the\nprediction of interactions between two unseen network nodes, and discuss their\nspecializations to tree-based ensemble methods, highlighting their\ninterpretability and drawing links with clustering techniques. Extensive\ncomputational experiments are carried out with these methods on various\nbiological networks that clearly highlight that these methods are competitive\nwith existing methods.\n", "versions": [{"version": "v1", "created": "Thu, 24 Apr 2014 10:22:33 GMT"}], "update_date": "2014-04-25", "authors_parsed": [["Schrynemackers", "Marie", ""], ["Wehenkel", "Louis", ""], ["Babu", "M. Madan", ""], ["Geurts", "Pierre", ""]]}, {"id": "1404.6163", "submitter": "Cedric Archambeau", "authors": "Behrouz Behmardi, Cedric Archambeau, Guillaume Bouchard", "title": "Overlapping Trace Norms in Multi-View Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multi-view learning leverages correlations between different sources of data\nto make predictions in one view based on observations in another view. A\npopular approach is to assume that, both, the correlations between the views\nand the view-specific covariances have a low-rank structure, leading to\ninter-battery factor analysis, a model closely related to canonical correlation\nanalysis. We propose a convex relaxation of this model using structured norm\nregularization. Further, we extend the convex formulation to a robust version\nby adding an l1-penalized matrix to our estimator, similarly to convex robust\nPCA. We develop and compare scalable algorithms for several convex multi-view\nmodels. We show experimentally that the view-specific correlations are\nimproving data imputation performances, as well as labeling accuracy in\nreal-world multi-label prediction tasks.\n", "versions": [{"version": "v1", "created": "Thu, 24 Apr 2014 15:50:02 GMT"}, {"version": "v2", "created": "Sun, 27 Apr 2014 14:59:53 GMT"}], "update_date": "2014-04-29", "authors_parsed": [["Behmardi", "Behrouz", ""], ["Archambeau", "Cedric", ""], ["Bouchard", "Guillaume", ""]]}, {"id": "1404.6216", "submitter": "Ping Li", "authors": "Ping Li", "title": "CoRE Kernels", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.DS cs.LG stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The term \"CoRE kernel\" stands for correlation-resemblance kernel. In many\napplications (e.g., vision), the data are often high-dimensional, sparse, and\nnon-binary. We propose two types of (nonlinear) CoRE kernels for non-binary\nsparse data and demonstrate the effectiveness of the new kernels through a\nclassification experiment. CoRE kernels are simple with no tuning parameters.\nHowever, training nonlinear kernel SVM can be (very) costly in time and memory\nand may not be suitable for truly large-scale industrial applications (e.g.\nsearch). In order to make the proposed CoRE kernels more practical, we develop\nbasic probabilistic hashing algorithms which transform nonlinear kernels into\nlinear kernels.\n", "versions": [{"version": "v1", "created": "Thu, 24 Apr 2014 18:35:37 GMT"}], "update_date": "2014-04-25", "authors_parsed": [["Li", "Ping", ""]]}, {"id": "1404.6272", "submitter": "Zhaowen Wang", "authors": "Zhaowen Wang, Jianchao Yang, Zhe Lin, Jonathan Brandt, Shiyu Chang,\n  Thomas Huang", "title": "Scalable Similarity Learning using Large Margin Neighborhood Embedding", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Classifying large-scale image data into object categories is an important\nproblem that has received increasing research attention. Given the huge amount\nof data, non-parametric approaches such as nearest neighbor classifiers have\nshown promising results, especially when they are underpinned by a learned\ndistance or similarity measurement. Although metric learning has been well\nstudied in the past decades, most existing algorithms are impractical to handle\nlarge-scale data sets. In this paper, we present an image similarity learning\nmethod that can scale well in both the number of images and the dimensionality\nof image descriptors. To this end, similarity comparison is restricted to each\nsample's local neighbors and a discriminative similarity measure is induced\nfrom large margin neighborhood embedding. We also exploit the ensemble of\nprojections so that high-dimensional features can be processed in a set of\nlower-dimensional subspaces in parallel without much performance compromise.\nThe similarity function is learned online using a stochastic gradient descent\nalgorithm in which the triplet sampling strategy is customized for quick\nconvergence of classification performance. The effectiveness of our proposed\nmodel is validated on several data sets with scales varying from tens of\nthousands to one million images. Recognition accuracies competitive with the\nstate-of-the-art performance are achieved with much higher efficiency and\nscalability.\n", "versions": [{"version": "v1", "created": "Thu, 24 Apr 2014 21:23:41 GMT"}], "update_date": "2014-04-28", "authors_parsed": [["Wang", "Zhaowen", ""], ["Yang", "Jianchao", ""], ["Lin", "Zhe", ""], ["Brandt", "Jonathan", ""], ["Chang", "Shiyu", ""], ["Huang", "Thomas", ""]]}, {"id": "1404.6369", "submitter": "Matthew England Dr", "authors": "Zongyan Huang, Matthew England, David Wilson, James H. Davenport,\n  Lawrence C. Paulson and James Bridge", "title": "Applying machine learning to the problem of choosing a heuristic to\n  select the variable ordering for cylindrical algebraic decomposition", "comments": "16 pages", "journal-ref": "Intelligent Computer Mathematics, pp. 92-107. (Lecture Notes in\n  Artificial Intelligence, 8543). Springer Berlin Heidelberg, 2014", "doi": "10.1007/978-3-319-08434-3_8", "report-no": null, "categories": "cs.SC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cylindrical algebraic decomposition(CAD) is a key tool in computational\nalgebraic geometry, particularly for quantifier elimination over real-closed\nfields. When using CAD, there is often a choice for the ordering placed on the\nvariables. This can be important, with some problems infeasible with one\nvariable ordering but easy with another. Machine learning is the process of\nfitting a computer model to a complex function based on properties learned from\nmeasured data. In this paper we use machine learning (specifically a support\nvector machine) to select between heuristics for choosing a variable ordering,\noutperforming each of the separate heuristics.\n", "versions": [{"version": "v1", "created": "Fri, 25 Apr 2014 09:43:05 GMT"}], "update_date": "2014-07-15", "authors_parsed": [["Huang", "Zongyan", ""], ["England", "Matthew", ""], ["Wilson", "David", ""], ["Davenport", "James H.", ""], ["Paulson", "Lawrence C.", ""], ["Bridge", "James", ""]]}, {"id": "1404.6580", "submitter": "Arvind Agarwal", "authors": "Arvind Agarwal, Saurabh Kataria", "title": "Multitask Learning for Sequence Labeling Tasks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present a learning method for sequence labeling tasks in\nwhich each example sequence has multiple label sequences. Our method learns\nmultiple models, one model for each label sequence. Each model computes the\njoint probability of all label sequences given the example sequence. Although\neach model considers all label sequences, its primary focus is only one label\nsequence, and therefore, each model becomes a task-specific model, for the task\nbelonging to that primary label. Such multiple models are learned {\\it\nsimultaneously} by facilitating the learning transfer among models through {\\it\nexplicit parameter sharing}. We experiment the proposed method on two\napplications and show that our method significantly outperforms the\nstate-of-the-art method.\n", "versions": [{"version": "v1", "created": "Fri, 25 Apr 2014 22:59:34 GMT"}, {"version": "v2", "created": "Mon, 9 May 2016 14:35:51 GMT"}], "update_date": "2016-05-10", "authors_parsed": [["Agarwal", "Arvind", ""], ["Kataria", "Saurabh", ""]]}, {"id": "1404.6674", "submitter": "Wei Yu", "authors": "Yu Wei and Pock Thomas", "title": "A Comparison of First-order Algorithms for Machine Learning", "comments": "Part of the OAGM 2014 proceedings (arXiv:1404.3538)", "journal-ref": null, "doi": null, "report-no": "OAGM/2014/16", "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Using an optimization algorithm to solve a machine learning problem is one of\nmainstreams in the field of science. In this work, we demonstrate a\ncomprehensive comparison of some state-of-the-art first-order optimization\nalgorithms for convex optimization problems in machine learning. We concentrate\non several smooth and non-smooth machine learning problems with a loss function\nplus a regularizer. The overall experimental results show the superiority of\nprimal-dual algorithms in solving a machine learning problem from the\nperspectives of the ease to construct, running time and accuracy.\n", "versions": [{"version": "v1", "created": "Sat, 26 Apr 2014 19:24:24 GMT"}], "update_date": "2014-04-29", "authors_parsed": [["Wei", "Yu", ""], ["Thomas", "Pock", ""]]}, {"id": "1404.6702", "submitter": "Oluwasanmi Koyejo", "authors": "Oluwasanmi Koyejo, Cheng Lee, Joydeep Ghosh", "title": "A Constrained Matrix-Variate Gaussian Process for Transposable Data", "comments": "23 pages, Preliminary version, Accepted for publication in Machine\n  Learning", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Transposable data represents interactions among two sets of entities, and are\ntypically represented as a matrix containing the known interaction values.\nAdditional side information may consist of feature vectors specific to entities\ncorresponding to the rows and/or columns of such a matrix. Further information\nmay also be available in the form of interactions or hierarchies among entities\nalong the same mode (axis). We propose a novel approach for modeling\ntransposable data with missing interactions given additional side information.\nThe interactions are modeled as noisy observations from a latent noise free\nmatrix generated from a matrix-variate Gaussian process. The construction of\nrow and column covariances using side information provides a flexible mechanism\nfor specifying a-priori knowledge of the row and column correlations in the\ndata. Further, the use of such a prior combined with the side information\nenables predictions for new rows and columns not observed in the training data.\nIn this work, we combine the matrix-variate Gaussian process model with low\nrank constraints. The constrained Gaussian process approach is applied to the\nprediction of hidden associations between genes and diseases using a small set\nof observed associations as well as prior covariances induced by gene-gene\ninteraction networks and disease ontologies. The proposed approach is also\napplied to recommender systems data which involves predicting the item ratings\nof users using known associations as well as prior covariances induced by\nsocial networks. We present experimental results that highlight the performance\nof constrained matrix-variate Gaussian process as compared to state of the art\napproaches in each domain.\n", "versions": [{"version": "v1", "created": "Sun, 27 Apr 2014 01:46:49 GMT"}], "update_date": "2014-04-29", "authors_parsed": [["Koyejo", "Oluwasanmi", ""], ["Lee", "Cheng", ""], ["Ghosh", "Joydeep", ""]]}, {"id": "1404.6876", "submitter": "Voot Tangkaratt", "authors": "Voot Tangkaratt, Ning Xie, and Masashi Sugiyama", "title": "Conditional Density Estimation with Dimensionality Reduction via\n  Squared-Loss Conditional Entropy Minimization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Regression aims at estimating the conditional mean of output given input.\nHowever, regression is not informative enough if the conditional density is\nmultimodal, heteroscedastic, and asymmetric. In such a case, estimating the\nconditional density itself is preferable, but conditional density estimation\n(CDE) is challenging in high-dimensional space. A naive approach to coping with\nhigh-dimensionality is to first perform dimensionality reduction (DR) and then\nexecute CDE. However, such a two-step process does not perform well in practice\nbecause the error incurred in the first DR step can be magnified in the second\nCDE step. In this paper, we propose a novel single-shot procedure that performs\nCDE and DR simultaneously in an integrated way. Our key idea is to formulate DR\nas the problem of minimizing a squared-loss variant of conditional entropy, and\nthis is solved via CDE. Thus, an additional CDE step is not needed after DR. We\ndemonstrate the usefulness of the proposed method through extensive experiments\non various datasets including humanoid robot transition and computer art.\n", "versions": [{"version": "v1", "created": "Mon, 28 Apr 2014 06:30:39 GMT"}], "update_date": "2014-04-29", "authors_parsed": [["Tangkaratt", "Voot", ""], ["Xie", "Ning", ""], ["Sugiyama", "Masashi", ""]]}, {"id": "1404.6955", "submitter": "Kenric Nelson", "authors": "Kenric P. Nelson, Madalina Barbu, Brian J. Scannell", "title": "Probabilistic graphs using coupled random variables", "comments": "Submitted for presentation at the Machine Intelligence and\n  Bio-inspired Computation: Theory and Applications Conference, SPIE Sensing\n  Technology and Applications, Baltimore, MD, May 8, 2014", "journal-ref": null, "doi": "10.1117/12.2050759", "report-no": null, "categories": "cs.LG cs.IT cs.NE math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural network design has utilized flexible nonlinear processes which can\nmimic biological systems, but has suffered from a lack of traceability in the\nresulting network. Graphical probabilistic models ground network design in\nprobabilistic reasoning, but the restrictions reduce the expressive capability\nof each node making network designs complex. The ability to model coupled\nrandom variables using the calculus of nonextensive statistical mechanics\nprovides a neural node design incorporating nonlinear coupling between input\nstates while maintaining the rigor of probabilistic reasoning. A generalization\nof Bayes rule using the coupled product enables a single node to model\ncorrelation between hundreds of random variables. A coupled Markov random field\nis designed for the inferencing and classification of UCI's MLR 'Multiple\nFeatures Data Set' such that thousands of linear correlation parameters can be\nreplaced with a single coupling parameter with just a (3%, 4%) percent\nreduction in (classification, inference) performance.\n", "versions": [{"version": "v1", "created": "Wed, 23 Apr 2014 19:25:48 GMT"}], "update_date": "2015-06-19", "authors_parsed": [["Nelson", "Kenric P.", ""], ["Barbu", "Madalina", ""], ["Scannell", "Brian J.", ""]]}, {"id": "1404.7048", "submitter": "Xiaowen Dong", "authors": "Xiaowen Dong, Dimitrios Mavroeidis, Francesco Calabrese, Pascal\n  Frossard", "title": "Multiscale Event Detection in Social Media", "comments": null, "journal-ref": "Data Mining and Knowledge Discovery, vol. 29, no. 5, pp.\n  1374-1405, September 2015", "doi": "10.1007/s10618-015-0421-2", "report-no": null, "categories": "cs.SI cs.LG physics.soc-ph stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Event detection has been one of the most important research topics in social\nmedia analysis. Most of the traditional approaches detect events based on fixed\ntemporal and spatial resolutions, while in reality events of different scales\nusually occur simultaneously, namely, they span different intervals in time and\nspace. In this paper, we propose a novel approach towards multiscale event\ndetection using social media data, which takes into account different temporal\nand spatial scales of events in the data. Specifically, we explore the\nproperties of the wavelet transform, which is a well-developed multiscale\ntransform in signal processing, to enable automatic handling of the interaction\nbetween temporal and spatial scales. We then propose a novel algorithm to\ncompute a data similarity graph at appropriate scales and detect events of\ndifferent scales simultaneously by a single graph-based clustering process.\nFurthermore, we present spatiotemporal statistical analysis of the noisy\ninformation present in the data stream, which allows us to define a novel\nterm-filtering procedure for the proposed event detection algorithm and helps\nus study its behavior using simulated noisy data. Experimental results on both\nsynthetically generated data and real world data collected from Twitter\ndemonstrate the meaningfulness and effectiveness of the proposed approach. Our\nframework further extends to numerous application domains that involve\nmultiscale and multiresolution data analysis.\n", "versions": [{"version": "v1", "created": "Fri, 25 Apr 2014 13:28:37 GMT"}, {"version": "v2", "created": "Fri, 6 Feb 2015 00:15:42 GMT"}], "update_date": "2015-08-31", "authors_parsed": [["Dong", "Xiaowen", ""], ["Mavroeidis", "Dimitrios", ""], ["Calabrese", "Francesco", ""], ["Frossard", "Pascal", ""]]}, {"id": "1404.7073", "submitter": "Jie Fu", "authors": "Jie Fu and Ufuk Topcu", "title": "Probably Approximately Correct MDP Learning and Control With Temporal\n  Logic Constraints", "comments": "9 pages, 5 figures, Accepted by 2014 Robotics: Science and Systems\n  (RSS)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SY cs.LG cs.LO cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider synthesis of control policies that maximize the probability of\nsatisfying given temporal logic specifications in unknown, stochastic\nenvironments. We model the interaction between the system and its environment\nas a Markov decision process (MDP) with initially unknown transition\nprobabilities. The solution we develop builds on the so-called model-based\nprobably approximately correct Markov decision process (PAC-MDP) methodology.\nThe algorithm attains an $\\varepsilon$-approximately optimal policy with\nprobability $1-\\delta$ using samples (i.e. observations), time and space that\ngrow polynomially with the size of the MDP, the size of the automaton\nexpressing the temporal logic specification, $\\frac{1}{\\varepsilon}$,\n$\\frac{1}{\\delta}$ and a finite time horizon. In this approach, the system\nmaintains a model of the initially unknown MDP, and constructs a product MDP\nbased on its learned model and the specification automaton that expresses the\ntemporal logic constraints. During execution, the policy is iteratively updated\nusing observation of the transitions taken by the system. The iteration\nterminates in finitely many steps. With high probability, the resulting policy\nis such that, for any state, the difference between the probability of\nsatisfying the specification under this policy and the optimal one is within a\npredefined bound.\n", "versions": [{"version": "v1", "created": "Mon, 28 Apr 2014 17:57:48 GMT"}, {"version": "v2", "created": "Wed, 30 Apr 2014 17:20:57 GMT"}], "update_date": "2014-05-01", "authors_parsed": [["Fu", "Jie", ""], ["Topcu", "Ufuk", ""]]}, {"id": "1404.7195", "submitter": "Michael Mathieu", "authors": "Michael Mathieu and Yann LeCun", "title": "Fast Approximation of Rotations and Hessians matrices", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A new method to represent and approximate rotation matrices is introduced.\nThe method represents approximations of a rotation matrix $Q$ with linearithmic\ncomplexity, i.e. with $\\frac{1}{2}n\\lg(n)$ rotations over pairs of coordinates,\narranged in an FFT-like fashion. The approximation is \"learned\" using gradient\ndescent. It allows to represent symmetric matrices $H$ as $QDQ^T$ where $D$ is\na diagonal matrix. It can be used to approximate covariance matrix of Gaussian\nmodels in order to speed up inference, or to estimate and track the inverse\nHessian of an objective function by relating changes in parameters to changes\nin gradient along the trajectory followed by the optimization procedure.\nExperiments were conducted to approximate synthetic matrices, covariance\nmatrices of real data, and Hessian matrices of objective functions involved in\nmachine learning problems.\n", "versions": [{"version": "v1", "created": "Tue, 29 Apr 2014 00:08:15 GMT"}], "update_date": "2014-04-30", "authors_parsed": [["Mathieu", "Michael", ""], ["LeCun", "Yann", ""]]}, {"id": "1404.7255", "submitter": "Cyril Voyant", "authors": "Cyril Voyant (SPE), Marie Laure Nivet (SPE), Christophe Paoli (SPE),\n  Marc Muselli (SPE), Gilles Notton (SPE)", "title": "Meteorological time series forecasting based on MLP modelling using\n  heterogeneous transfer functions", "comments": null, "journal-ref": "International Conference on Mathematical Modeling in Physical\n  Sciences 2014, Madrid : Spain (2014)", "doi": "10.1088/1742-6596/574/1/012064", "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose to study four meteorological and seasonal time\nseries coupled with a multi-layer perceptron (MLP) modeling. We chose to\ncombine two transfer functions for the nodes of the hidden layer, and to use a\ntemporal indicator (time index as input) in order to take into account the\nseasonal aspect of the studied time series. The results of the prediction\nconcern two years of measurements and the learning step, eight independent\nyears. We show that this methodology can improve the accuracy of meteorological\ndata estimation compared to a classical MLP modelling with a homogenous\ntransfer function.\n", "versions": [{"version": "v1", "created": "Tue, 29 Apr 2014 06:43:19 GMT"}], "update_date": "2015-06-19", "authors_parsed": [["Voyant", "Cyril", "", "SPE"], ["Nivet", "Marie Laure", "", "SPE"], ["Paoli", "Christophe", "", "SPE"], ["Muselli", "Marc", "", "SPE"], ["Notton", "Gilles", "", "SPE"]]}, {"id": "1404.7306", "submitter": "Canyi Lu", "authors": "Canyi Lu, Jinhui Tang, Shuicheng Yan, Zhouchen Lin", "title": "Generalized Nonconvex Nonsmooth Low-Rank Minimization", "comments": "IEEE International Conference on Computer Vision and Pattern\n  Recognition, 2014", "journal-ref": null, "doi": "10.1109/CVPR.2014.526", "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As surrogate functions of $L_0$-norm, many nonconvex penalty functions have\nbeen proposed to enhance the sparse vector recovery. It is easy to extend these\nnonconvex penalty functions on singular values of a matrix to enhance low-rank\nmatrix recovery. However, different from convex optimization, solving the\nnonconvex low-rank minimization problem is much more challenging than the\nnonconvex sparse minimization problem. We observe that all the existing\nnonconvex penalty functions are concave and monotonically increasing on\n$[0,\\infty)$. Thus their gradients are decreasing functions. Based on this\nproperty, we propose an Iteratively Reweighted Nuclear Norm (IRNN) algorithm to\nsolve the nonconvex nonsmooth low-rank minimization problem. IRNN iteratively\nsolves a Weighted Singular Value Thresholding (WSVT) problem. By setting the\nweight vector as the gradient of the concave penalty function, the WSVT problem\nhas a closed form solution. In theory, we prove that IRNN decreases the\nobjective function value monotonically, and any limit point is a stationary\npoint. Extensive experiments on both synthetic data and real images demonstrate\nthat IRNN enhances the low-rank matrix recovery compared with state-of-the-art\nconvex algorithms.\n", "versions": [{"version": "v1", "created": "Tue, 29 Apr 2014 10:45:22 GMT"}], "update_date": "2016-11-17", "authors_parsed": [["Lu", "Canyi", ""], ["Tang", "Jinhui", ""], ["Yan", "Shuicheng", ""], ["Lin", "Zhouchen", ""]]}, {"id": "1404.7456", "submitter": "Atilim Gunes Baydin", "authors": "Atilim Gunes Baydin, Barak A. Pearlmutter", "title": "Automatic Differentiation of Algorithms for Machine Learning", "comments": "7 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.SC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automatic differentiation---the mechanical transformation of numeric computer\nprograms to calculate derivatives efficiently and accurately---dates to the\norigin of the computer age. Reverse mode automatic differentiation both\nantedates and generalizes the method of backwards propagation of errors used in\nmachine learning. Despite this, practitioners in a variety of fields, including\nmachine learning, have been little influenced by automatic differentiation, and\nmake scant use of available tools. Here we review the technique of automatic\ndifferentiation, describe its two main modes, and explain how it can benefit\nmachine learning practitioners. To reach the widest possible audience our\ntreatment assumes only elementary differential calculus, and does not assume\nany knowledge of linear algebra.\n", "versions": [{"version": "v1", "created": "Mon, 28 Apr 2014 17:19:25 GMT"}], "update_date": "2014-04-30", "authors_parsed": [["Baydin", "Atilim Gunes", ""], ["Pearlmutter", "Barak A.", ""]]}, {"id": "1404.7472", "submitter": "Carl Mattfeld", "authors": "Carl Mattfeld", "title": "Implementing spectral methods for hidden Markov models with real-valued\n  emissions", "comments": "57 pages (including front and back matter), 7 figures. Master thesis\n  towards the Master's Degree in Physics, ETH Zurich, Switzerland", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hidden Markov models (HMMs) are widely used statistical models for modeling\nsequential data. The parameter estimation for HMMs from time series data is an\nimportant learning problem. The predominant methods for parameter estimation\nare based on local search heuristics, most notably the expectation-maximization\n(EM) algorithm. These methods are prone to local optima and oftentimes suffer\nfrom high computational and sample complexity. Recent years saw the emergence\nof spectral methods for the parameter estimation of HMMs, based on a method of\nmoments approach. Two spectral learning algorithms as proposed by Hsu, Kakade\nand Zhang 2012 (arXiv:0811.4413) and Anandkumar, Hsu and Kakade 2012\n(arXiv:1203.0683) are assessed in this work. Using experiments with synthetic\ndata, the algorithms are compared with each other. Furthermore, the spectral\nmethods are compared to the Baum-Welch algorithm, a well-established method\napplying the EM algorithm to HMMs. The spectral algorithms are found to have a\nmuch more favorable computational and sample complexity. Even though the\nalgorithms readily handle high dimensional observation spaces, instability\nissues are encountered in this regime. In view of learning from real-world\nexperimental data, the representation of real-valued observations for the use\nin spectral methods is discussed, presenting possible methods to represent data\nfor the use in the learning algorithms.\n", "versions": [{"version": "v1", "created": "Tue, 29 Apr 2014 19:28:09 GMT"}], "update_date": "2014-04-30", "authors_parsed": [["Mattfeld", "Carl", ""]]}, {"id": "1404.7527", "submitter": "Raphaela Palenta", "authors": "Timo K\\\"otzing and Raphaela Palenta", "title": "A Map of Update Constraints in Inductive Inference", "comments": "fixed a mistake in Theorem 21, result is the same", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate how different learning restrictions reduce learning power and\nhow the different restrictions relate to one another. We give a complete map\nfor nine different restrictions both for the cases of complete information\nlearning and set-driven learning. This completes the picture for these\nwell-studied \\emph{delayable} learning restrictions. A further insight is\ngained by different characterizations of \\emph{conservative} learning in terms\nof variants of \\emph{cautious} learning.\n  Our analyses greatly benefit from general theorems we give, for example\nshowing that learners with exclusively delayable restrictions can always be\nassumed total.\n", "versions": [{"version": "v1", "created": "Tue, 29 Apr 2014 20:54:40 GMT"}, {"version": "v2", "created": "Thu, 3 Jul 2014 21:46:43 GMT"}], "update_date": "2014-07-07", "authors_parsed": [["K\u00f6tzing", "Timo", ""], ["Palenta", "Raphaela", ""]]}, {"id": "1404.7796", "submitter": "Emilie Morvant", "authors": "Emilie Morvant (IST Austria), Amaury Habrard (LHC), St\\'ephane Ayache\n  (LIF)", "title": "Majority Vote of Diverse Classifiers for Late Fusion", "comments": "IAPR Joint International Workshops on Statistical Techniques in\n  Pattern Recognition and Structural and Syntactic Pattern Recignition, Joensuu\n  : Finland (2014)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the past few years, a lot of attention has been devoted to multimedia\nindexing by fusing multimodal informations. Two kinds of fusion schemes are\ngenerally considered: The early fusion and the late fusion. We focus on late\nclassifier fusion, where one combines the scores of each modality at the\ndecision level. To tackle this problem, we investigate a recent and elegant\nwell-founded quadratic program named MinCq coming from the machine learning\nPAC-Bayesian theory. MinCq looks for the weighted combination, over a set of\nreal-valued functions seen as voters, leading to the lowest misclassification\nrate, while maximizing the voters' diversity. We propose an extension of MinCq\ntailored to multimedia indexing. Our method is based on an order-preserving\npairwise loss adapted to ranking that allows us to improve Mean Averaged\nPrecision measure while taking into account the diversity of the voters that we\nwant to fuse. We provide evidence that this method is naturally adapted to late\nfusion procedures and confirm the good behavior of our approach on the\nchallenging PASCAL VOC'07 benchmark.\n", "versions": [{"version": "v1", "created": "Wed, 30 Apr 2014 16:55:00 GMT"}, {"version": "v2", "created": "Thu, 19 Jun 2014 08:06:24 GMT"}], "update_date": "2014-06-20", "authors_parsed": [["Morvant", "Emilie", "", "IST Austria"], ["Habrard", "Amaury", "", "LHC"], ["Ayache", "St\u00e9phane", "", "LIF"]]}, {"id": "1404.7828", "submitter": "Juergen Schmidhuber", "authors": "Juergen Schmidhuber", "title": "Deep Learning in Neural Networks: An Overview", "comments": "88 pages, 888 references", "journal-ref": "Neural Networks, Vol 61, pp 85-117, Jan 2015", "doi": "10.1016/j.neunet.2014.09.003", "report-no": "Technical Report IDSIA-03-14", "categories": "cs.NE cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, deep artificial neural networks (including recurrent ones)\nhave won numerous contests in pattern recognition and machine learning. This\nhistorical survey compactly summarises relevant work, much of it from the\nprevious millennium. Shallow and deep learners are distinguished by the depth\nof their credit assignment paths, which are chains of possibly learnable,\ncausal links between actions and effects. I review deep supervised learning\n(also recapitulating the history of backpropagation), unsupervised learning,\nreinforcement learning & evolutionary computation, and indirect search for\nshort programs encoding deep and large networks.\n", "versions": [{"version": "v1", "created": "Wed, 30 Apr 2014 18:39:00 GMT"}, {"version": "v2", "created": "Wed, 28 May 2014 15:33:51 GMT"}, {"version": "v3", "created": "Wed, 2 Jul 2014 16:05:33 GMT"}, {"version": "v4", "created": "Wed, 8 Oct 2014 10:00:38 GMT"}], "update_date": "2014-11-20", "authors_parsed": [["Schmidhuber", "Juergen", ""]]}]