[{"id": "1401.0044", "submitter": "Adrian Weller", "authors": "Adrian Weller, Tony Jebara", "title": "Approximating the Bethe partition function", "comments": null, "journal-ref": null, "doi": null, "report-no": "cucs-031-13", "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When belief propagation (BP) converges, it does so to a stationary point of\nthe Bethe free energy $F$, and is often strikingly accurate. However, it may\nconverge only to a local optimum or may not converge at all. An algorithm was\nrecently introduced for attractive binary pairwise MRFs which is guaranteed to\nreturn an $\\epsilon$-approximation to the global minimum of $F$ in polynomial\ntime provided the maximum degree $\\Delta=O(\\log n)$, where $n$ is the number of\nvariables. Here we significantly improve this algorithm and derive several\nresults including a new approach based on analyzing first derivatives of $F$,\nwhich leads to performance that is typically far superior and yields a fully\npolynomial-time approximation scheme (FPTAS) for attractive models without any\ndegree restriction. Further, the method applies to general (non-attractive)\nmodels, though with no polynomial time guarantee in this case, leading to the\nimportant result that approximating $\\log$ of the Bethe partition function,\n$\\log Z_B=-\\min F$, for a general model to additive $\\epsilon$-accuracy may be\nreduced to a discrete MAP inference problem. We explore an application to\npredicting equipment failure on an urban power network and demonstrate that the\nBethe approximation can perform well even when BP fails to converge.\n", "versions": [{"version": "v1", "created": "Mon, 30 Dec 2013 22:40:50 GMT"}], "update_date": "2014-01-03", "authors_parsed": [["Weller", "Adrian", ""], ["Jebara", "Tony", ""]]}, {"id": "1401.0104", "submitter": "Tao Xiong", "authors": "Yukun Bao, Tao Xiong, Zhongyi Hu", "title": "PSO-MISMO Modeling Strategy for Multi-Step-Ahead Time Series Prediction", "comments": "14 pages. IEEE Transactions on Cybernetics. 2013", "journal-ref": null, "doi": "10.1109/TCYB.2013.2265084", "report-no": null, "categories": "cs.AI cs.LG cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multi-step-ahead time series prediction is one of the most challenging\nresearch topics in the field of time series modeling and prediction, and is\ncontinually under research. Recently, the multiple-input several\nmultiple-outputs (MISMO) modeling strategy has been proposed as a promising\nalternative for multi-step-ahead time series prediction, exhibiting advantages\ncompared with the two currently dominating strategies, the iterated and the\ndirect strategies. Built on the established MISMO strategy, this study proposes\na particle swarm optimization (PSO)-based MISMO modeling strategy, which is\ncapable of determining the number of sub-models in a self-adaptive mode, with\nvarying prediction horizons. Rather than deriving crisp divides with equal-size\ns prediction horizons from the established MISMO, the proposed PSO-MISMO\nstrategy, implemented with neural networks, employs a heuristic to create\nflexible divides with varying sizes of prediction horizons and to generate\ncorresponding sub-models, providing considerable flexibility in model\nconstruction, which has been validated with simulated and real datasets.\n", "versions": [{"version": "v1", "created": "Tue, 31 Dec 2013 07:09:02 GMT"}], "update_date": "2014-01-03", "authors_parsed": [["Bao", "Yukun", ""], ["Xiong", "Tao", ""], ["Hu", "Zhongyi", ""]]}, {"id": "1401.0116", "submitter": "Dinesh Govindaraj", "authors": "Dinesh Govindaraj, Raman Sankaran, Sreedal Menon, Chiranjib\n  Bhattacharyya", "title": "Controlled Sparsity Kernel Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multiple Kernel Learning(MKL) on Support Vector Machines(SVMs) has been a\npopular front of research in recent times due to its success in application\nproblems like Object Categorization. This success is due to the fact that MKL\nhas the ability to choose from a variety of feature kernels to identify the\noptimal kernel combination. But the initial formulation of MKL was only able to\nselect the best of the features and misses out many other informative kernels\npresented. To overcome this, the Lp norm based formulation was proposed by\nKloft et. al. This formulation is capable of choosing a non-sparse set of\nkernels through a control parameter p. Unfortunately, the parameter p does not\nhave a direct meaning to the number of kernels selected. We have observed that\nstricter control over the number of kernels selected gives us an edge over\nthese techniques in terms of accuracy of classification and also helps us to\nfine tune the algorithms to the time requirements at hand. In this work, we\npropose a Controlled Sparsity Kernel Learning (CSKL) formulation that can\nstrictly control the number of kernels which we wish to select. The CSKL\nformulation introduces a parameter t which directly corresponds to the number\nof kernels selected. It is important to note that a search in t space is finite\nand fast as compared to p. We have also provided an efficient Reduced Gradient\nDescent based algorithm to solve the CSKL formulation, which is proven to\nconverge. Through our experiments on the Caltech101 Object Categorization\ndataset, we have also shown that one can achieve better accuracies than the\nprevious formulations through the right choice of t.\n", "versions": [{"version": "v1", "created": "Tue, 31 Dec 2013 09:13:09 GMT"}], "update_date": "2014-01-03", "authors_parsed": [["Govindaraj", "Dinesh", ""], ["Sankaran", "Raman", ""], ["Menon", "Sreedal", ""], ["Bhattacharyya", "Chiranjib", ""]]}, {"id": "1401.0118", "submitter": "Rajesh Ranganath", "authors": "Rajesh Ranganath and Sean Gerrish and David M. Blei", "title": "Black Box Variational Inference", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Variational inference has become a widely used method to approximate\nposteriors in complex latent variables models. However, deriving a variational\ninference algorithm generally requires significant model-specific analysis, and\nthese efforts can hinder and deter us from quickly developing and exploring a\nvariety of models for a problem at hand. In this paper, we present a \"black\nbox\" variational inference algorithm, one that can be quickly applied to many\nmodels with little additional derivation. Our method is based on a stochastic\noptimization of the variational objective where the noisy gradient is computed\nfrom Monte Carlo samples from the variational distribution. We develop a number\nof methods to reduce the variance of the gradient, always maintaining the\ncriterion that we want to avoid difficult model-based derivations. We evaluate\nour method against the corresponding black box sampling based methods. We find\nthat our method reaches better predictive likelihoods much faster than sampling\nmethods. Finally, we demonstrate that Black Box Variational Inference lets us\neasily explore a wide space of models by quickly constructing and evaluating\nseveral models of longitudinal healthcare data.\n", "versions": [{"version": "v1", "created": "Tue, 31 Dec 2013 09:32:43 GMT"}], "update_date": "2014-01-03", "authors_parsed": [["Ranganath", "Rajesh", ""], ["Gerrish", "Sean", ""], ["Blei", "David M.", ""]]}, {"id": "1401.0159", "submitter": "Michael Zibulevsky", "authors": "Michael Zibulevsky", "title": "Speeding-Up Convergence via Sequential Subspace Optimization: Current\n  State and Future Directions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NA cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This is an overview paper written in style of research proposal. In recent\nyears we introduced a general framework for large-scale unconstrained\noptimization -- Sequential Subspace Optimization (SESOP) and demonstrated its\nusefulness for sparsity-based signal/image denoising, deconvolution,\ncompressive sensing, computed tomography, diffraction imaging, support vector\nmachines. We explored its combination with Parallel Coordinate Descent and\nSeparable Surrogate Function methods, obtaining state of the art results in\nabove-mentioned areas. There are several methods, that are faster than plain\nSESOP under specific conditions: Trust region Newton method - for problems with\neasily invertible Hessian matrix; Truncated Newton method - when fast\nmultiplication by Hessian is available; Stochastic optimization methods - for\nproblems with large stochastic-type data; Multigrid methods - for problems with\nnested multilevel structure. Each of these methods can be further improved by\nmerge with SESOP. One can also accelerate Augmented Lagrangian method for\nconstrained optimization problems and Alternating Direction Method of\nMultipliers for problems with separable objective function and non-separable\nconstraints.\n", "versions": [{"version": "v1", "created": "Tue, 31 Dec 2013 15:25:50 GMT"}], "update_date": "2014-01-03", "authors_parsed": [["Zibulevsky", "Michael", ""]]}, {"id": "1401.0201", "submitter": "Ping Li", "authors": "Ping Li, Cun-Hui Zhang, Tong Zhang", "title": "Sparse Recovery with Very Sparse Compressed Counting", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.DS cs.IT cs.LG math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Compressed sensing (sparse signal recovery) often encounters nonnegative data\n(e.g., images). Recently we developed the methodology of using (dense)\nCompressed Counting for recovering nonnegative K-sparse signals. In this paper,\nwe adopt very sparse Compressed Counting for nonnegative signal recovery. Our\ndesign matrix is sampled from a maximally-skewed p-stable distribution (0<p<1),\nand we sparsify the design matrix so that on average (1-g)-fraction of the\nentries become zero. The idea is related to very sparse stable random\nprojections (Li et al 2006 and Li 2007), the prior work for estimating summary\nstatistics of the data.\n  In our theoretical analysis, we show that, when p->0, it suffices to use M=\nK/(1-exp(-gK) log N measurements, so that all coordinates can be recovered in\none scan of the coordinates. If g = 1 (i.e., dense design), then M = K log N.\nIf g= 1/K or 2/K (i.e., very sparse design), then M = 1.58K log N or M = 1.16K\nlog N. This means the design matrix can be indeed very sparse at only a minor\ninflation of the sample complexity.\n  Interestingly, as p->1, the required number of measurements is essentially M\n= 2.7K log N, provided g= 1/K. It turns out that this result is a general\nworst-case bound.\n", "versions": [{"version": "v1", "created": "Tue, 31 Dec 2013 18:17:09 GMT"}], "update_date": "2014-01-03", "authors_parsed": [["Li", "Ping", ""], ["Zhang", "Cun-Hui", ""], ["Zhang", "Tong", ""]]}, {"id": "1401.0247", "submitter": "Yingyu Liang", "authors": "Maria-Florina Balcan, Yingyu Liang, Pramod Gupta", "title": "Robust Hierarchical Clustering", "comments": "37 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One of the most widely used techniques for data clustering is agglomerative\nclustering. Such algorithms have been long used across many different fields\nranging from computational biology to social sciences to computer vision in\npart because their output is easy to interpret. Unfortunately, it is well\nknown, however, that many of the classic agglomerative clustering algorithms\nare not robust to noise. In this paper we propose and analyze a new robust\nalgorithm for bottom-up agglomerative clustering. We show that our algorithm\ncan be used to cluster accurately in cases where the data satisfies a number of\nnatural properties and where the traditional agglomerative algorithms fail. We\nalso show how to adapt our algorithm to the inductive setting where our given\ndata is only a small random sample of the entire data set. Experimental\nevaluations on synthetic and real world data sets show that our algorithm\nachieves better performance than other hierarchical algorithms in the presence\nof noise.\n", "versions": [{"version": "v1", "created": "Wed, 1 Jan 2014 04:16:21 GMT"}, {"version": "v2", "created": "Sun, 13 Jul 2014 01:51:05 GMT"}], "update_date": "2014-07-15", "authors_parsed": [["Balcan", "Maria-Florina", ""], ["Liang", "Yingyu", ""], ["Gupta", "Pramod", ""]]}, {"id": "1401.0255", "submitter": "Dinesh Govindaraj", "authors": "Dinesh Govindaraj, Tao Wang, S.V.N. Vishwanathan", "title": "Modeling Attractiveness and Multiple Clicks in Sponsored Search Results", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Click models are an important tool for leveraging user feedback, and are used\nby commercial search engines for surfacing relevant search results. However,\nexisting click models are lacking in two aspects. First, they do not share\ninformation across search results when computing attractiveness. Second, they\nassume that users interact with the search results sequentially. Based on our\nanalysis of the click logs of a commercial search engine, we observe that the\nsequential scan assumption does not always hold, especially for sponsored\nsearch results. To overcome the above two limitations, we propose a new click\nmodel. Our key insight is that sharing information across search results helps\nin identifying important words or key-phrases which can then be used to\naccurately compute attractiveness of a search result. Furthermore, we argue\nthat the click probability of a position as well as its attractiveness changes\nduring a user session and depends on the user's past click experience. Our\nmodel seamlessly incorporates the effect of externalities (quality of other\nsearch results displayed in response to a user query), user fatigue, as well as\npre and post-click relevance of a sponsored search result. We propose an\nefficient one-pass inference scheme and empirically evaluate the performance of\nour model via extensive experiments using the click logs of a large commercial\nsearch engine.\n", "versions": [{"version": "v1", "created": "Wed, 1 Jan 2014 06:45:58 GMT"}], "update_date": "2014-01-03", "authors_parsed": [["Govindaraj", "Dinesh", ""], ["Wang", "Tao", ""], ["Vishwanathan", "S. V. N.", ""]]}, {"id": "1401.0304", "submitter": "Shahar Mendelson", "authors": "Shahar Mendelson", "title": "Learning without Concentration", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We obtain sharp bounds on the performance of Empirical Risk Minimization\nperformed in a convex class and with respect to the squared loss, without\nassuming that class members and the target are bounded functions or have\nrapidly decaying tails.\n  Rather than resorting to a concentration-based argument, the method used here\nrelies on a `small-ball' assumption and thus holds for classes consisting of\nheavy-tailed functions and for heavy-tailed targets.\n  The resulting estimates scale correctly with the `noise level' of the\nproblem, and when applied to the classical, bounded scenario, always improve\nthe known bounds.\n", "versions": [{"version": "v1", "created": "Wed, 1 Jan 2014 16:28:19 GMT"}, {"version": "v2", "created": "Wed, 22 Oct 2014 17:59:50 GMT"}], "update_date": "2014-10-23", "authors_parsed": [["Mendelson", "Shahar", ""]]}, {"id": "1401.0362", "submitter": "Hao Peng", "authors": "Hao Peng and Yuan Qi", "title": "EigenGP: Gaussian Process Models with Adaptive Eigenfunctions", "comments": "Accepted by IJCAI 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Gaussian processes (GPs) provide a nonparametric representation of functions.\nHowever, classical GP inference suffers from high computational cost for big\ndata. In this paper, we propose a new Bayesian approach, EigenGP, that learns\nboth basis dictionary elements--eigenfunctions of a GP prior--and prior\nprecisions in a sparse finite model. It is well known that, among all\northogonal basis functions, eigenfunctions can provide the most compact\nrepresentation. Unlike other sparse Bayesian finite models where the basis\nfunction has a fixed form, our eigenfunctions live in a reproducing kernel\nHilbert space as a finite linear combination of kernel functions. We learn the\ndictionary elements--eigenfunctions--and the prior precisions over these\nelements as well as all the other hyperparameters from data by maximizing the\nmodel marginal likelihood. We explore computational linear algebra to simplify\nthe gradient computation significantly. Our experimental results demonstrate\nimproved predictive performance of EigenGP over alternative sparse GP methods\nas well as relevance vector machine.\n", "versions": [{"version": "v1", "created": "Thu, 2 Jan 2014 03:12:28 GMT"}, {"version": "v2", "created": "Tue, 25 Mar 2014 03:50:33 GMT"}, {"version": "v3", "created": "Mon, 13 Jul 2015 06:11:18 GMT"}], "update_date": "2015-07-14", "authors_parsed": [["Peng", "Hao", ""], ["Qi", "Yuan", ""]]}, {"id": "1401.0376", "submitter": "Chao Zhang", "authors": "Chao Zhang, Lei Zhang, Wei Fan, Jieping Ye", "title": "Generalization Bounds for Representative Domain Adaptation", "comments": "arXiv admin note: substantial text overlap with arXiv:1304.1574", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a novel framework to analyze the theoretical\nproperties of the learning process for a representative type of domain\nadaptation, which combines data from multiple sources and one target (or\nbriefly called representative domain adaptation). In particular, we use the\nintegral probability metric to measure the difference between the distributions\nof two domains and meanwhile compare it with the H-divergence and the\ndiscrepancy distance. We develop the Hoeffding-type, the Bennett-type and the\nMcDiarmid-type deviation inequalities for multiple domains respectively, and\nthen present the symmetrization inequality for representative domain\nadaptation. Next, we use the derived inequalities to obtain the Hoeffding-type\nand the Bennett-type generalization bounds respectively, both of which are\nbased on the uniform entropy number. Moreover, we present the generalization\nbounds based on the Rademacher complexity. Finally, we analyze the asymptotic\nconvergence and the rate of convergence of the learning process for\nrepresentative domain adaptation. We discuss the factors that affect the\nasymptotic behavior of the learning process and the numerical experiments\nsupport our theoretical findings as well. Meanwhile, we give a comparison with\nthe existing results of domain adaptation and the classical results under the\nsame-distribution assumption.\n", "versions": [{"version": "v1", "created": "Thu, 2 Jan 2014 07:32:01 GMT"}], "update_date": "2014-01-03", "authors_parsed": [["Zhang", "Chao", ""], ["Zhang", "Lei", ""], ["Fan", "Wei", ""], ["Ye", "Jieping", ""]]}, {"id": "1401.0509", "submitter": "Yann Dauphin", "authors": "Yann N. Dauphin, Gokhan Tur, Dilek Hakkani-Tur, Larry Heck", "title": "Zero-Shot Learning for Semantic Utterance Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  We propose a novel zero-shot learning method for semantic utterance\nclassification (SUC). It learns a classifier $f: X \\to Y$ for problems where\nnone of the semantic categories $Y$ are present in the training set. The\nframework uncovers the link between categories and utterances using a semantic\nspace. We show that this semantic space can be learned by deep neural networks\ntrained on large amounts of search engine query log data. More precisely, we\npropose a novel method that can learn discriminative semantic features without\nsupervision. It uses the zero-shot learning framework to guide the learning of\nthe semantic features. We demonstrate the effectiveness of the zero-shot\nsemantic learning algorithm on the SUC dataset collected by (Tur, 2012).\nFurthermore, we achieve state-of-the-art results by combining the semantic\nfeatures with a supervised method.\n", "versions": [{"version": "v1", "created": "Fri, 20 Dec 2013 17:08:26 GMT"}, {"version": "v2", "created": "Tue, 18 Feb 2014 20:34:08 GMT"}, {"version": "v3", "created": "Fri, 7 Mar 2014 23:31:02 GMT"}], "update_date": "2014-03-11", "authors_parsed": [["Dauphin", "Yann N.", ""], ["Tur", "Gokhan", ""], ["Hakkani-Tur", "Dilek", ""], ["Heck", "Larry", ""]]}, {"id": "1401.0514", "submitter": "Chris J. Maddison", "authors": "Chris J. Maddison and Daniel Tarlow", "title": "Structured Generative Models of Natural Source Code", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of building generative models of natural source code\n(NSC); that is, source code written and understood by humans. Our primary\ncontribution is to describe a family of generative models for NSC that have\nthree key properties: First, they incorporate both sequential and hierarchical\nstructure. Second, we learn a distributed representation of source code\nelements. Finally, they integrate closely with a compiler, which allows\nleveraging compiler logic and abstractions when building structure into the\nmodel. We also develop an extension that includes more complex structure,\nrefining how the model generates identifier tokens based on what variables are\ncurrently in scope. Our models can be learned efficiently, and we show\nempirically that including appropriate structure greatly improves the models,\nmeasured by the probability of generating test programs.\n", "versions": [{"version": "v1", "created": "Thu, 2 Jan 2014 19:35:31 GMT"}, {"version": "v2", "created": "Fri, 20 Jun 2014 08:12:20 GMT"}], "update_date": "2014-06-23", "authors_parsed": [["Maddison", "Chris J.", ""], ["Tarlow", "Daniel", ""]]}, {"id": "1401.0579", "submitter": "Tengyu Ma", "authors": "Sanjeev Arora, Aditya Bhaskara, Rong Ge, Tengyu Ma", "title": "More Algorithms for Provable Dictionary Learning", "comments": "23 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In dictionary learning, also known as sparse coding, the algorithm is given\nsamples of the form $y = Ax$ where $x\\in \\mathbb{R}^m$ is an unknown random\nsparse vector and $A$ is an unknown dictionary matrix in $\\mathbb{R}^{n\\times\nm}$ (usually $m > n$, which is the overcomplete case). The goal is to learn $A$\nand $x$. This problem has been studied in neuroscience, machine learning,\nvisions, and image processing. In practice it is solved by heuristic algorithms\nand provable algorithms seemed hard to find. Recently, provable algorithms were\nfound that work if the unknown feature vector $x$ is $\\sqrt{n}$-sparse or even\nsparser. Spielman et al. \\cite{DBLP:journals/jmlr/SpielmanWW12} did this for\ndictionaries where $m=n$; Arora et al. \\cite{AGM} gave an algorithm for\novercomplete ($m >n$) and incoherent matrices $A$; and Agarwal et al.\n\\cite{DBLP:journals/corr/AgarwalAN13} handled a similar case but with weaker\nguarantees.\n  This raised the problem of designing provable algorithms that allow sparsity\n$\\gg \\sqrt{n}$ in the hidden vector $x$. The current paper designs algorithms\nthat allow sparsity up to $n/poly(\\log n)$. It works for a class of matrices\nwhere features are individually recoverable, a new notion identified in this\npaper that may motivate further work.\n  The algorithm runs in quasipolynomial time because they use limited\nenumeration.\n", "versions": [{"version": "v1", "created": "Fri, 3 Jan 2014 02:52:17 GMT"}], "update_date": "2014-01-06", "authors_parsed": [["Arora", "Sanjeev", ""], ["Bhaskara", "Aditya", ""], ["Ge", "Rong", ""], ["Ma", "Tengyu", ""]]}, {"id": "1401.0711", "submitter": "Ishanu Chattopadhyay", "authors": "Ishanu Chattopadhyay and Hod Lipson", "title": "Computing Entropy Rate Of Symbol Sources & A Distribution-free Limit\n  Theorem", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.LG math.IT math.PR stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Entropy rate of sequential data-streams naturally quantifies the complexity\nof the generative process. Thus entropy rate fluctuations could be used as a\ntool to recognize dynamical perturbations in signal sources, and could\npotentially be carried out without explicit background noise characterization.\nHowever, state of the art algorithms to estimate the entropy rate have markedly\nslow convergence; making such entropic approaches non-viable in practice. We\npresent here a fundamentally new approach to estimate entropy rates, which is\ndemonstrated to converge significantly faster in terms of input data lengths,\nand is shown to be effective in diverse applications ranging from the\nestimation of the entropy rate of English texts to the estimation of complexity\nof chaotic dynamical systems. Additionally, the convergence rate of entropy\nestimates do not follow from any standard limit theorem, and reported\nalgorithms fail to provide any confidence bounds on the computed values.\nExploiting a connection to the theory of probabilistic automata, we establish a\nconvergence rate of $O(\\log \\vert s \\vert/\\sqrt[3]{\\vert s \\vert})$ as a\nfunction of the input length $\\vert s \\vert$, which then yields explicit\nuncertainty estimates, as well as required data lengths to satisfy\npre-specified confidence bounds.\n", "versions": [{"version": "v1", "created": "Fri, 3 Jan 2014 20:30:01 GMT"}, {"version": "v2", "created": "Fri, 21 Mar 2014 07:29:34 GMT"}], "update_date": "2014-03-24", "authors_parsed": [["Chattopadhyay", "Ishanu", ""], ["Lipson", "Hod", ""]]}, {"id": "1401.0742", "submitter": "Ishanu Chattopadhyay", "authors": "Ishanu Chattopadhyay and Hod Lipson", "title": "Data Smashing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CE cs.IT math.IT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Investigation of the underlying physics or biology from empirical data\nrequires a quantifiable notion of similarity - when do two observed data sets\nindicate nearly identical generating processes, and when they do not. The\ndiscriminating characteristics to look for in data is often determined by\nheuristics designed by experts, $e.g.$, distinct shapes of \"folded\" lightcurves\nmay be used as \"features\" to classify variable stars, while determination of\npathological brain states might require a Fourier analysis of brainwave\nactivity. Finding good features is non-trivial. Here, we propose a universal\nsolution to this problem: we delineate a principle for quantifying similarity\nbetween sources of arbitrary data streams, without a priori knowledge, features\nor training. We uncover an algebraic structure on a space of symbolic models\nfor quantized data, and show that such stochastic generators may be added and\nuniquely inverted; and that a model and its inverse always sum to the generator\nof flat white noise. Therefore, every data stream has an anti-stream: data\ngenerated by the inverse model. Similarity between two streams, then, is the\ndegree to which one, when summed to the other's anti-stream, mutually\nannihilates all statistical structure to noise. We call this data smashing. We\npresent diverse applications, including disambiguation of brainwaves pertaining\nto epileptic seizures, detection of anomalous cardiac rhythms, and\nclassification of astronomical objects from raw photometry. In our examples,\nthe data smashing principle, without access to any domain knowledge, meets or\nexceeds the performance of specialized algorithms tuned by domain experts.\n", "versions": [{"version": "v1", "created": "Fri, 3 Jan 2014 22:15:17 GMT"}], "update_date": "2014-01-07", "authors_parsed": [["Chattopadhyay", "Ishanu", ""], ["Lipson", "Hod", ""]]}, {"id": "1401.0764", "submitter": "Chunhua Shen", "authors": "Xi Li, Weiming Hu, Chunhua Shen, Anthony Dick, Zhongfei Zhang", "title": "Context-Aware Hypergraph Construction for Robust Spectral Clustering", "comments": "10 pages. Appearing in IEEE TRANSACTIONS ON KNOWLEDGE AND DATA\n  ENGINEERING: http://doi.ieeecomputersociety.org/10.1109/TKDE.2013.126", "journal-ref": null, "doi": "10.1109/TKDE.2013.126", "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Spectral clustering is a powerful tool for unsupervised data analysis. In\nthis paper, we propose a context-aware hypergraph similarity measure (CAHSM),\nwhich leads to robust spectral clustering in the case of noisy data. We\nconstruct three types of hypergraph---the pairwise hypergraph, the\nk-nearest-neighbor (kNN) hypergraph, and the high-order over-clustering\nhypergraph. The pairwise hypergraph captures the pairwise similarity of data\npoints; the kNN hypergraph captures the neighborhood of each point; and the\nclustering hypergraph encodes high-order contexts within the dataset. By\ncombining the affinity information from these three hypergraphs, the CAHSM\nalgorithm is able to explore the intrinsic topological information of the\ndataset. Therefore, data clustering using CAHSM tends to be more robust.\nConsidering the intra-cluster compactness and the inter-cluster separability of\nvertices, we further design a discriminative hypergraph partitioning criterion\n(DHPC). Using both CAHSM and DHPC, a robust spectral clustering algorithm is\ndeveloped. Theoretical analysis and experimental evaluation demonstrate the\neffectiveness and robustness of the proposed algorithm.\n", "versions": [{"version": "v1", "created": "Sat, 4 Jan 2014 02:05:35 GMT"}], "update_date": "2016-11-15", "authors_parsed": [["Li", "Xi", ""], ["Hu", "Weiming", ""], ["Shen", "Chunhua", ""], ["Dick", "Anthony", ""], ["Zhang", "Zhongfei", ""]]}, {"id": "1401.0767", "submitter": "Chunhua Shen", "authors": "Chunhua Shen, Fayao Liu", "title": "From Kernel Machines to Ensemble Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Ensemble methods such as boosting combine multiple learners to obtain better\nprediction than could be obtained from any individual learner. Here we propose\na principled framework for directly constructing ensemble learning methods from\nkernel methods. Unlike previous studies showing the equivalence between\nboosting and support vector machines (SVMs), which needs a translation\nprocedure, we show that it is possible to design boosting-like procedure to\nsolve the SVM optimization problems.\n  In other words, it is possible to design ensemble methods directly from SVM\nwithout any middle procedure.\n  This finding not only enables us to design new ensemble learning methods\ndirectly from kernel methods, but also makes it possible to take advantage of\nthose highly-optimized fast linear SVM solvers for ensemble learning.\n  We exemplify this framework for designing binary ensemble learning as well as\na new multi-class ensemble learning methods.\n  Experimental results demonstrate the flexibility and usefulness of the\nproposed framework.\n", "versions": [{"version": "v1", "created": "Sat, 4 Jan 2014 02:28:48 GMT"}], "update_date": "2014-01-07", "authors_parsed": [["Shen", "Chunhua", ""], ["Liu", "Fayao", ""]]}, {"id": "1401.0843", "submitter": "Warren Powell", "authors": "Warren R. Scott, Warren B. Powell, Somayeh Moazehi", "title": "Least Squares Policy Iteration with Instrumental Variables vs. Direct\n  Policy Search: Comparison Against Optimal Benchmarks Using Energy Storage", "comments": "37 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper studies approximate policy iteration (API) methods which use\nleast-squares Bellman error minimization for policy evaluation. We address\nseveral of its enhancements, namely, Bellman error minimization using\ninstrumental variables, least-squares projected Bellman error minimization, and\nprojected Bellman error minimization using instrumental variables. We prove\nthat for a general discrete-time stochastic control problem, Bellman error\nminimization using instrumental variables is equivalent to both variants of\nprojected Bellman error minimization. An alternative to these API methods is\ndirect policy search based on knowledge gradient. The practical performance of\nthese three approximate dynamic programming methods are then investigated in\nthe context of an application in energy storage, integrated with an\nintermittent wind energy supply to fully serve a stochastic time-varying\nelectricity demand. We create a library of test problems using real-world data\nand apply value iteration to find their optimal policies. These benchmarks are\nthen used to compare the developed policies. Our analysis indicates that API\nwith instrumental variables Bellman error minimization prominently outperforms\nAPI with least-squares Bellman error minimization. However, these approaches\nunderperform our direct policy search implementation.\n", "versions": [{"version": "v1", "created": "Sat, 4 Jan 2014 19:57:26 GMT"}], "update_date": "2014-01-07", "authors_parsed": [["Scott", "Warren R.", ""], ["Powell", "Warren B.", ""], ["Moazehi", "Somayeh", ""]]}, {"id": "1401.0852", "submitter": "Qing Zhou", "authors": "Bryon Aragam and Qing Zhou", "title": "Concave Penalized Estimation of Sparse Gaussian Bayesian Networks", "comments": "57 pages", "journal-ref": "Journal of Machine Learning Research 16(Nov):2273-2328, 2015", "doi": null, "report-no": null, "categories": "stat.ME cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop a penalized likelihood estimation framework to estimate the\nstructure of Gaussian Bayesian networks from observational data. In contrast to\nrecent methods which accelerate the learning problem by restricting the search\nspace, our main contribution is a fast algorithm for score-based structure\nlearning which does not restrict the search space in any way and works on\nhigh-dimensional datasets with thousands of variables. Our use of concave\nregularization, as opposed to the more popular $\\ell_0$ (e.g. BIC) penalty, is\nnew. Moreover, we provide theoretical guarantees which generalize existing\nasymptotic results when the underlying distribution is Gaussian. Most notably,\nour framework does not require the existence of a so-called faithful DAG\nrepresentation, and as a result the theory must handle the inherent\nnonidentifiability of the estimation problem in a novel way. Finally, as a\nmatter of independent interest, we provide a comprehensive comparison of our\napproach to several standard structure learning methods using open-source\npackages developed for the R language. Based on these experiments, we show that\nour algorithm is significantly faster than other competing methods while\nobtaining higher sensitivity with comparable false discovery rates for\nhigh-dimensional data. In particular, the total runtime for our method to\ngenerate a solution path of 20 estimates for DAGs with 8000 nodes is around one\nhour.\n", "versions": [{"version": "v1", "created": "Sat, 4 Jan 2014 23:27:48 GMT"}, {"version": "v2", "created": "Sun, 4 Jan 2015 23:34:01 GMT"}], "update_date": "2015-12-24", "authors_parsed": [["Aragam", "Bryon", ""], ["Zhou", "Qing", ""]]}, {"id": "1401.0869", "submitter": "Zhaosong Lu", "authors": "Zhaosong Lu and Yong Zhang", "title": "Schatten-$p$ Quasi-Norm Regularized Matrix Optimization via Iterative\n  Reweighted Singular Value Minimization", "comments": "This paper has been withdrawn by the author due to major revision and\n  corrections", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.LG math.NA stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we study general Schatten-$p$ quasi-norm (SPQN) regularized\nmatrix minimization problems. In particular, we first introduce a class of\nfirst-order stationary points for them, and show that the first-order\nstationary points introduced in [11] for an SPQN regularized $vector$\nminimization problem are equivalent to those of an SPQN regularized $matrix$\nminimization reformulation. We also show that any local minimizer of the SPQN\nregularized matrix minimization problems must be a first-order stationary\npoint. Moreover, we derive lower bounds for nonzero singular values of the\nfirst-order stationary points and hence also of the local minimizers of the\nSPQN regularized matrix minimization problems. The iterative reweighted\nsingular value minimization (IRSVM) methods are then proposed to solve these\nproblems, whose subproblems are shown to have a closed-form solution. In\ncontrast to the analogous methods for the SPQN regularized $vector$\nminimization problems, the convergence analysis of these methods is\nsignificantly more challenging. We develop a novel approach to establishing the\nconvergence of these methods, which makes use of the expression of a specific\nsolution of their subproblems and avoids the intricate issue of finding the\nexplicit expression for the Clarke subdifferential of the objective of their\nsubproblems. In particular, we show that any accumulation point of the sequence\ngenerated by the IRSVM methods is a first-order stationary point of the\nproblems. Our computational results demonstrate that the IRSVM methods\ngenerally outperform some recently developed state-of-the-art methods in terms\nof solution quality and/or speed.\n", "versions": [{"version": "v1", "created": "Sun, 5 Jan 2014 06:37:50 GMT"}, {"version": "v2", "created": "Tue, 24 Nov 2015 18:41:44 GMT"}, {"version": "v3", "created": "Mon, 31 Oct 2016 17:30:58 GMT"}], "update_date": "2016-11-01", "authors_parsed": [["Lu", "Zhaosong", ""], ["Zhang", "Yong", ""]]}, {"id": "1401.0887", "submitter": "Dorina Thanou", "authors": "Dorina Thanou, David I Shuman, Pascal Frossard", "title": "Learning parametric dictionaries for graph signals", "comments": null, "journal-ref": null, "doi": "10.1109/TSP.2014.2332441", "report-no": null, "categories": "cs.LG cs.SI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In sparse signal representation, the choice of a dictionary often involves a\ntradeoff between two desirable properties -- the ability to adapt to specific\nsignal data and a fast implementation of the dictionary. To sparsely represent\nsignals residing on weighted graphs, an additional design challenge is to\nincorporate the intrinsic geometric structure of the irregular data domain into\nthe atoms of the dictionary. In this work, we propose a parametric dictionary\nlearning algorithm to design data-adapted, structured dictionaries that\nsparsely represent graph signals. In particular, we model graph signals as\ncombinations of overlapping local patterns. We impose the constraint that each\ndictionary is a concatenation of subdictionaries, with each subdictionary being\na polynomial of the graph Laplacian matrix, representing a single pattern\ntranslated to different areas of the graph. The learning algorithm adapts the\npatterns to a training set of graph signals. Experimental results on both\nsynthetic and real datasets demonstrate that the dictionaries learned by the\nproposed algorithm are competitive with and often better than unstructured\ndictionaries learned by state-of-the-art numerical learning algorithms in terms\nof sparse approximation of graph signals. In contrast to the unstructured\ndictionaries, however, the dictionaries learned by the proposed algorithm\nfeature localized atoms and can be implemented in a computationally efficient\nmanner in signal processing tasks such as compression, denoising, and\nclassification.\n", "versions": [{"version": "v1", "created": "Sun, 5 Jan 2014 12:17:51 GMT"}], "update_date": "2015-06-18", "authors_parsed": [["Thanou", "Dorina", ""], ["Shuman", "David I", ""], ["Frossard", "Pascal", ""]]}, {"id": "1401.0898", "submitter": "Vijendra Singh", "authors": "Vijendra Singh and Shivani Pathak", "title": "Feature Selection Using Classifier in High Dimensional Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Feature selection is frequently used as a pre-processing step to machine\nlearning. It is a process of choosing a subset of original features so that the\nfeature space is optimally reduced according to a certain evaluation criterion.\nThe central objective of this paper is to reduce the dimension of the data by\nfinding a small set of important features which can give good classification\nperformance. We have applied filter and wrapper approach with different\nclassifiers QDA and LDA respectively. A widely-used filter method is used for\nbioinformatics data i.e. a univariate criterion separately on each feature,\nassuming that there is no interaction between features and then applied\nSequential Feature Selection method. Experimental results show that filter\napproach gives better performance in respect of Misclassification Error Rate.\n", "versions": [{"version": "v1", "created": "Sun, 5 Jan 2014 14:52:27 GMT"}], "update_date": "2014-01-07", "authors_parsed": [["Singh", "Vijendra", ""], ["Pathak", "Shivani", ""]]}, {"id": "1401.1123", "submitter": "Nicolas Galichet", "authors": "Nicolas Galichet (LRI, INRIA Saclay - Ile de France), Mich\\`ele Sebag\n  (LRI, INRIA Saclay - Ile de France), Olivier Teytaud (LRI, INRIA Saclay - Ile\n  de France)", "title": "Exploration vs Exploitation vs Safety: Risk-averse Multi-Armed Bandits", "comments": "16 pages", "journal-ref": "Asian Conference on Machine Learning 2013, Canberra : Australia\n  (2013)", "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motivated by applications in energy management, this paper presents the\nMulti-Armed Risk-Aware Bandit (MARAB) algorithm. With the goal of limiting the\nexploration of risky arms, MARAB takes as arm quality its conditional value at\nrisk. When the user-supplied risk level goes to 0, the arm quality tends toward\nthe essential infimum of the arm distribution density, and MARAB tends toward\nthe MIN multi-armed bandit algorithm, aimed at the arm with maximal minimal\nvalue. As a first contribution, this paper presents a theoretical analysis of\nthe MIN algorithm under mild assumptions, establishing its robustness\ncomparatively to UCB. The analysis is supported by extensive experimental\nvalidation of MIN and MARAB compared to UCB and state-of-art risk-aware MAB\nalgorithms on artificial and real-world problems.\n", "versions": [{"version": "v1", "created": "Mon, 6 Jan 2014 15:53:25 GMT"}], "update_date": "2014-01-07", "authors_parsed": [["Galichet", "Nicolas", "", "LRI, INRIA Saclay - Ile de France"], ["Sebag", "Mich\u00e8le", "", "LRI, INRIA Saclay - Ile de France"], ["Teytaud", "Olivier", "", "LRI, INRIA Saclay - Ile\n  de France"]]}, {"id": "1401.1465", "submitter": "David Balduzzi", "authors": "David Balduzzi", "title": "Cortical prediction markets", "comments": "To appear, AAMAS 2014", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.GT cs.LG cs.MA q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate cortical learning from the perspective of mechanism design.\nFirst, we show that discretizing standard models of neurons and synaptic\nplasticity leads to rational agents maximizing simple scoring rules. Second,\nour main result is that the scoring rules are proper, implying that neurons\nfaithfully encode expected utilities in their synaptic weights and encode\nhigh-scoring outcomes in their spikes. Third, with this foundation in hand, we\npropose a biologically plausible mechanism whereby neurons backpropagate\nincentives which allows them to optimize their usefulness to the rest of\ncortex. Finally, experiments show that networks that backpropagate incentives\ncan learn simple tasks.\n", "versions": [{"version": "v1", "created": "Tue, 7 Jan 2014 18:28:20 GMT"}], "update_date": "2014-01-08", "authors_parsed": [["Balduzzi", "David", ""]]}, {"id": "1401.1489", "submitter": "Romain H\\'erault", "authors": "John Komar and Romain H\\'erault and Ludovic Seifert", "title": "Key point selection and clustering of swimmer coordination through\n  Sparse Fisher-EM", "comments": "Presented at ECML/PKDD 2013 Workshop on Machine Learning and Data\n  Mining for Sports Analytics (MLSA2013)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CV cs.LG physics.data-an stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To answer the existence of optimal swimmer learning/teaching strategies, this\nwork introduces a two-level clustering in order to analyze temporal dynamics of\nmotor learning in breaststroke swimming. Each level have been performed through\nSparse Fisher-EM, a unsupervised framework which can be applied efficiently on\nlarge and correlated datasets. The induced sparsity selects key points of the\ncoordination phase without any prior knowledge.\n", "versions": [{"version": "v1", "created": "Tue, 7 Jan 2014 20:16:05 GMT"}], "update_date": "2014-01-08", "authors_parsed": [["Komar", "John", ""], ["H\u00e9rault", "Romain", ""], ["Seifert", "Ludovic", ""]]}, {"id": "1401.1549", "submitter": "Zheng Wen", "authors": "Zheng Wen, Daniel O'Neill and Hamid Reza Maei", "title": "Optimal Demand Response Using Device Based Reinforcement Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Demand response (DR) for residential and small commercial buildings is\nestimated to account for as much as 65% of the total energy savings potential\nof DR, and previous work shows that a fully automated Energy Management System\n(EMS) is a necessary prerequisite to DR in these areas. In this paper, we\npropose a novel EMS formulation for DR problems in these sectors. Specifically,\nwe formulate a fully automated EMS's rescheduling problem as a reinforcement\nlearning (RL) problem, and argue that this RL problem can be approximately\nsolved by decomposing it over device clusters. Compared with existing\nformulations, our new formulation (1) does not require explicitly modeling the\nuser's dissatisfaction on job rescheduling, (2) enables the EMS to\nself-initiate jobs, (3) allows the user to initiate more flexible requests and\n(4) has a computational complexity linear in the number of devices. We also\ndemonstrate the simulation results of applying Q-learning, one of the most\npopular and classical RL algorithms, to a representative example.\n", "versions": [{"version": "v1", "created": "Wed, 8 Jan 2014 00:49:01 GMT"}, {"version": "v2", "created": "Sat, 28 Jun 2014 04:24:47 GMT"}], "update_date": "2014-10-07", "authors_parsed": [["Wen", "Zheng", ""], ["O'Neill", "Daniel", ""], ["Maei", "Hamid Reza", ""]]}, {"id": "1401.1560", "submitter": "Tao Xiong", "authors": "Tao Xiong, Yukun Bao, Zhongyi Hu", "title": "Beyond One-Step-Ahead Forecasting: Evaluation of Alternative\n  Multi-Step-Ahead Forecasting Models for Crude Oil Prices", "comments": "32 pages", "journal-ref": "Energy Economics. 40, 2013: 405-415", "doi": "10.1016/j.eneco.2013.07.028", "report-no": null, "categories": "cs.LG cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An accurate prediction of crude oil prices over long future horizons is\nchallenging and of great interest to governments, enterprises, and investors.\nThis paper proposes a revised hybrid model built upon empirical mode\ndecomposition (EMD) based on the feed-forward neural network (FNN) modeling\nframework incorporating the slope-based method (SBM), which is capable of\ncapturing the complex dynamic of crude oil prices. Three commonly used\nmulti-step-ahead prediction strategies proposed in the literature, including\niterated strategy, direct strategy, and MIMO (multiple-input multiple-output)\nstrategy, are examined and compared, and practical considerations for the\nselection of a prediction strategy for multi-step-ahead forecasting relating to\ncrude oil prices are identified. The weekly data from the WTI (West Texas\nIntermediate) crude oil spot price are used to compare the performance of the\nalternative models under the EMD-SBM-FNN modeling framework with selected\ncounterparts. The quantitative and comprehensive assessments are performed on\nthe basis of prediction accuracy and computational cost. The results obtained\nin this study indicate that the proposed EMD-SBM-FNN model using the MIMO\nstrategy is the best in terms of prediction accuracy with accredited\ncomputational load.\n", "versions": [{"version": "v1", "created": "Wed, 8 Jan 2014 01:59:53 GMT"}], "update_date": "2014-01-10", "authors_parsed": [["Xiong", "Tao", ""], ["Bao", "Yukun", ""], ["Hu", "Zhongyi", ""]]}, {"id": "1401.1605", "submitter": "James Hensman", "authors": "James Hensman and Magnus Rattray and Neil D. Lawrence", "title": "Fast nonparametric clustering of structured time-series", "comments": "Accepted for publication in special edition of TPAMI on Bayesian\n  Nonparametrics", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this publication, we combine two Bayesian non-parametric models: the\nGaussian Process (GP) and the Dirichlet Process (DP). Our innovation in the GP\nmodel is to introduce a variation on the GP prior which enables us to model\nstructured time-series data, i.e. data containing groups where we wish to model\ninter- and intra-group variability. Our innovation in the DP model is an\nimplementation of a new fast collapsed variational inference procedure which\nenables us to optimize our variationala pproximation significantly faster than\nstandard VB approaches. In a biological time series application we show how our\nmodel better captures salient features of the data, leading to better\nconsistency with existing biological classifications, while the associated\ninference algorithm provides a twofold speed-up over EM-based variational\ninference.\n", "versions": [{"version": "v1", "created": "Wed, 8 Jan 2014 08:47:44 GMT"}, {"version": "v2", "created": "Mon, 14 Apr 2014 08:04:46 GMT"}], "update_date": "2014-04-15", "authors_parsed": [["Hensman", "James", ""], ["Rattray", "Magnus", ""], ["Lawrence", "Neil D.", ""]]}, {"id": "1401.1803", "submitter": "Stanislas Lauly", "authors": "Stanislas Lauly, Alex Boulanger, Hugo Larochelle", "title": "Learning Multilingual Word Representations using a Bag-of-Words\n  Autoencoder", "comments": "This workshop paper was accepted on Octoble 30 2013 at the NIPS 2013\n  workshop on deep learning\n  (https://sites.google.com/site/deeplearningworkshopnips2013/accepted-papers)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent work on learning multilingual word representations usually relies on\nthe use of word-level alignements (e.g. infered with the help of GIZA++)\nbetween translated sentences, in order to align the word embeddings in\ndifferent languages. In this workshop paper, we investigate an autoencoder\nmodel for learning multilingual word representations that does without such\nword-level alignements. The autoencoder is trained to reconstruct the\nbag-of-word representation of given sentence from an encoded representation\nextracted from its translation. We evaluate our approach on a multilingual\ndocument classification task, where labeled data is available only for one\nlanguage (e.g. English) while classification must be performed in a different\nlanguage (e.g. French). In our experiments, we observe that our method compares\nfavorably with a previously proposed method that exploits word-level alignments\nto learn word representations.\n", "versions": [{"version": "v1", "created": "Wed, 8 Jan 2014 20:36:57 GMT"}], "update_date": "2014-01-09", "authors_parsed": [["Lauly", "Stanislas", ""], ["Boulanger", "Alex", ""], ["Larochelle", "Hugo", ""]]}, {"id": "1401.1842", "submitter": "Shuchin Aeron", "authors": "Jason Gejie Liu and Shuchin Aeron", "title": "Robust Large Scale Non-negative Matrix Factorization using Proximal\n  Point Algorithm", "comments": "Appeared in IEEE GlobalSIP, 2013, TX, Austin", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.IT cs.LG cs.NA math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A robust algorithm for non-negative matrix factorization (NMF) is presented\nin this paper with the purpose of dealing with large-scale data, where the\nseparability assumption is satisfied. In particular, we modify the Linear\nProgramming (LP) algorithm of [9] by introducing a reduced set of constraints\nfor exact NMF. In contrast to the previous approaches, the proposed algorithm\ndoes not require the knowledge of factorization rank (extreme rays [3] or\ntopics [7]). Furthermore, motivated by a similar problem arising in the context\nof metabolic network analysis [13], we consider an entirely different regime\nwhere the number of extreme rays or topics can be much larger than the\ndimension of the data vectors. The performance of the algorithm for different\nsynthetic data sets are provided.\n", "versions": [{"version": "v1", "created": "Wed, 8 Jan 2014 21:39:03 GMT"}], "update_date": "2014-01-10", "authors_parsed": [["Liu", "Jason Gejie", ""], ["Aeron", "Shuchin", ""]]}, {"id": "1401.1880", "submitter": "Elad Liebman", "authors": "Elad Liebman, Maytal Saar-Tsechansky and Peter Stone", "title": "DJ-MC: A Reinforcement-Learning Agent for Music Playlist Recommendation", "comments": "-Updated to the most recent and completed version (to be presented at\n  AAMAS 2015) -Updated author list. in Autonomous Agents and Multiagent Systems\n  (AAMAS) 2015, Istanbul, Turkey, May 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, there has been growing focus on the study of automated\nrecommender systems. Music recommendation systems serve as a prominent domain\nfor such works, both from an academic and a commercial perspective. A\nfundamental aspect of music perception is that music is experienced in temporal\ncontext and in sequence. In this work we present DJ-MC, a novel\nreinforcement-learning framework for music recommendation that does not\nrecommend songs individually but rather song sequences, or playlists, based on\na model of preferences for both songs and song transitions. The model is\nlearned online and is uniquely adapted for each listener. To reduce exploration\ntime, DJ-MC exploits user feedback to initialize a model, which it subsequently\nupdates by reinforcement. We evaluate our framework with human participants\nusing both real song and playlist data. Our results indicate that DJ-MC's\nability to recommend sequences of songs provides a significant improvement over\nmore straightforward approaches, which do not take transitions into account.\n", "versions": [{"version": "v1", "created": "Thu, 9 Jan 2014 01:50:09 GMT"}, {"version": "v2", "created": "Wed, 25 Mar 2015 18:40:46 GMT"}], "update_date": "2015-03-26", "authors_parsed": [["Liebman", "Elad", ""], ["Saar-Tsechansky", "Maytal", ""], ["Stone", "Peter", ""]]}, {"id": "1401.1895", "submitter": "Mahdi Shahbaba PhD", "authors": "Mahdi Shahbaba and Soosan Beheshti", "title": "Efficient unimodality test in clustering by signature testing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper provides a new unimodality test with application in hierarchical\nclustering methods. The proposed method denoted by signature test (Sigtest),\ntransforms the data based on its statistics. The transformed data has much\nsmaller variation compared to the original data and can be evaluated in a\nsimple proposed unimodality test. Compared with the existing unimodality tests,\nSigtest is more accurate in detecting the overlapped clusters and has a much\nless computational complexity. Simulation results demonstrate the efficiency of\nthis statistic test for both real and synthetic data sets.\n", "versions": [{"version": "v1", "created": "Thu, 9 Jan 2014 05:16:35 GMT"}], "update_date": "2014-01-10", "authors_parsed": [["Shahbaba", "Mahdi", ""], ["Beheshti", "Soosan", ""]]}, {"id": "1401.1916", "submitter": "Tao Xiong", "authors": "Tao Xiong, Yukun Bao, Zhongyi Hu", "title": "Multiple-output support vector regression with a firefly algorithm for\n  interval-valued stock price index forecasting", "comments": "33 pages", "journal-ref": "Knowledge-based Systems. 55, 2013:87-100", "doi": "10.1016/j.knosys.2013.10.012", "report-no": null, "categories": "cs.CE cs.LG q-fin.ST", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Highly accurate interval forecasting of a stock price index is fundamental to\nsuccessfully making a profit when making investment decisions, by providing a\nrange of values rather than a point estimate. In this study, we investigate the\npossibility of forecasting an interval-valued stock price index series over\nshort and long horizons using multi-output support vector regression (MSVR).\nFurthermore, this study proposes a firefly algorithm (FA)-based approach, built\non the established MSVR, for determining the parameters of MSVR (abbreviated as\nFA-MSVR). Three globally traded broad market indices are used to compare the\nperformance of the proposed FA-MSVR method with selected counterparts. The\nquantitative and comprehensive assessments are performed on the basis of\nstatistical criteria, economic criteria, and computational cost. In terms of\nstatistical criteria, we compare the out-of-sample forecasting using\ngoodness-of-forecast measures and testing approaches. In terms of economic\ncriteria, we assess the relative forecast performance with a simple trading\nstrategy. The results obtained in this study indicate that the proposed FA-MSVR\nmethod is a promising alternative for forecasting interval-valued financial\ntime series.\n", "versions": [{"version": "v1", "created": "Thu, 9 Jan 2014 07:58:06 GMT"}], "update_date": "2014-01-13", "authors_parsed": [["Xiong", "Tao", ""], ["Bao", "Yukun", ""], ["Hu", "Zhongyi", ""]]}, {"id": "1401.1926", "submitter": "Zhongyi Hu", "authors": "Yukun Bao, Zhongyi Hu, Tao Xiong", "title": "A PSO and Pattern Search based Memetic Algorithm for SVMs Parameters\n  Optimization", "comments": "27 pages. Neurocomputing, 2013", "journal-ref": null, "doi": "10.1016/j.neucom.2013.01.027", "report-no": null, "categories": "cs.LG cs.AI cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Addressing the issue of SVMs parameters optimization, this study proposes an\nefficient memetic algorithm based on Particle Swarm Optimization algorithm\n(PSO) and Pattern Search (PS). In the proposed memetic algorithm, PSO is\nresponsible for exploration of the search space and the detection of the\npotential regions with optimum solutions, while pattern search (PS) is used to\nproduce an effective exploitation on the potential regions obtained by PSO.\nMoreover, a novel probabilistic selection strategy is proposed to select the\nappropriate individuals among the current population to undergo local\nrefinement, keeping a well balance between exploration and exploitation.\nExperimental results confirm that the local refinement with PS and our proposed\nselection strategy are effective, and finally demonstrate effectiveness and\nrobustness of the proposed PSO-PS based MA for SVMs parameters optimization.\n", "versions": [{"version": "v1", "created": "Thu, 9 Jan 2014 08:41:55 GMT"}], "update_date": "2014-01-10", "authors_parsed": [["Bao", "Yukun", ""], ["Hu", "Zhongyi", ""], ["Xiong", "Tao", ""]]}, {"id": "1401.1974", "submitter": "Vu Nguyen", "authors": "Vu Nguyen, Dinh Phung, XuanLong Nguyen, Svetha Venkatesh, Hung Hai Bui", "title": "Bayesian Nonparametric Multilevel Clustering with Group-Level Contexts", "comments": "Full version of ICML 2014", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a Bayesian nonparametric framework for multilevel clustering which\nutilizes group-level context information to simultaneously discover\nlow-dimensional structures of the group contents and partitions groups into\nclusters. Using the Dirichlet process as the building block, our model\nconstructs a product base-measure with a nested structure to accommodate\ncontent and context observations at multiple levels. The proposed model\npossesses properties that link the nested Dirichlet processes (nDP) and the\nDirichlet process mixture models (DPM) in an interesting way: integrating out\nall contents results in the DPM over contexts, whereas integrating out\ngroup-specific contexts results in the nDP mixture over content variables. We\nprovide a Polya-urn view of the model and an efficient collapsed Gibbs\ninference procedure. Extensive experiments on real-world datasets demonstrate\nthe advantage of utilizing context information via our model in both text and\nimage domains.\n", "versions": [{"version": "v1", "created": "Thu, 9 Jan 2014 12:08:07 GMT"}, {"version": "v2", "created": "Mon, 13 Jan 2014 06:28:03 GMT"}, {"version": "v3", "created": "Mon, 27 Jan 2014 08:13:58 GMT"}, {"version": "v4", "created": "Wed, 29 Jan 2014 01:54:57 GMT"}], "update_date": "2014-01-30", "authors_parsed": [["Nguyen", "Vu", ""], ["Phung", "Dinh", ""], ["Nguyen", "XuanLong", ""], ["Venkatesh", "Svetha", ""], ["Bui", "Hung Hai", ""]]}, {"id": "1401.2086", "submitter": "L.A. Prashanth", "authors": "H.L Prasad, L.A.Prashanth and Shalabh Bhatnagar", "title": "Actor-Critic Algorithms for Learning Nash Equilibria in N-player\n  General-Sum Games", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GT cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of finding stationary Nash equilibria (NE) in a\nfinite discounted general-sum stochastic game. We first generalize a non-linear\noptimization problem from Filar and Vrieze [2004] to a $N$-player setting and\nbreak down this problem into simpler sub-problems that ensure there is no\nBellman error for a given state and an agent. We then provide a\ncharacterization of solution points of these sub-problems that correspond to\nNash equilibria of the underlying game and for this purpose, we derive a set of\nnecessary and sufficient SG-SP (Stochastic Game - Sub-Problem) conditions.\nUsing these conditions, we develop two actor-critic algorithms: OFF-SGSP\n(model-based) and ON-SGSP (model-free). Both algorithms use a critic that\nestimates the value function for a fixed policy and an actor that performs\ndescent in the policy space using a descent direction that avoids local minima.\nWe establish that both algorithms converge, in self-play, to the equilibria of\na certain ordinary differential equation (ODE), whose stable limit points\ncoincide with stationary NE of the underlying general-sum stochastic game. On a\nsingle state non-generic game (see Hart and Mas-Colell [2005]) as well as on a\nsynthetic two-player game setup with $810,000$ states, we establish that\nON-SGSP consistently outperforms NashQ ([Hu and Wellman, 2003] and FFQ\n[Littman, 2001] algorithms.\n", "versions": [{"version": "v1", "created": "Wed, 8 Jan 2014 12:47:15 GMT"}, {"version": "v2", "created": "Thu, 2 Jul 2015 20:09:17 GMT"}], "update_date": "2015-07-06", "authors_parsed": [["Prasad", "H. L", ""], ["Prashanth", "L. A.", ""], ["Bhatnagar", "Shalabh", ""]]}, {"id": "1401.2224", "submitter": "Alireza Goudarzi", "authors": "Alireza Goudarzi, Peter Banda, Matthew R. Lakin, Christof Teuscher,\n  Darko Stefanovic", "title": "A Comparative Study of Reservoir Computing for Temporal Signal\n  Processing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Reservoir computing (RC) is a novel approach to time series prediction using\nrecurrent neural networks. In RC, an input signal perturbs the intrinsic\ndynamics of a medium called a reservoir. A readout layer is then trained to\nreconstruct a target output from the reservoir's state. The multitude of RC\narchitectures and evaluation metrics poses a challenge to both practitioners\nand theorists who study the task-solving performance and computational power of\nRC. In addition, in contrast to traditional computation models, the reservoir\nis a dynamical system in which computation and memory are inseparable, and\ntherefore hard to analyze. Here, we compare echo state networks (ESN), a\npopular RC architecture, with tapped-delay lines (DL) and nonlinear\nautoregressive exogenous (NARX) networks, which we use to model systems with\nlimited computation and limited memory respectively. We compare the performance\nof the three systems while computing three common benchmark time series:\nH{\\'e}non Map, NARMA10, and NARMA20. We find that the role of the reservoir in\nthe reservoir computing paradigm goes beyond providing a memory of the past\ninputs. The DL and the NARX network have higher memorization capability, but\nfall short of the generalization power of the ESN.\n", "versions": [{"version": "v1", "created": "Fri, 10 Jan 2014 03:39:28 GMT"}], "update_date": "2014-01-13", "authors_parsed": [["Goudarzi", "Alireza", ""], ["Banda", "Peter", ""], ["Lakin", "Matthew R.", ""], ["Teuscher", "Christof", ""], ["Stefanovic", "Darko", ""]]}, {"id": "1401.2288", "submitter": "Hemant Kumar Aggarwal", "authors": "Hemant Kumar Aggarwal and Angshul Majumdar", "title": "Extension of Sparse Randomized Kaczmarz Algorithm for Multiple\n  Measurement Vectors", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NA cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Kaczmarz algorithm is popular for iteratively solving an overdetermined\nsystem of linear equations. The traditional Kaczmarz algorithm can approximate\nthe solution in few sweeps through the equations but a randomized version of\nthe Kaczmarz algorithm was shown to converge exponentially and independent of\nnumber of equations. Recently an algorithm for finding sparse solution to a\nlinear system of equations has been proposed based on weighted randomized\nKaczmarz algorithm. These algorithms solves single measurement vector problem;\nhowever there are applications were multiple-measurements are available. In\nthis work, the objective is to solve a multiple measurement vector problem with\ncommon sparse support by modifying the randomized Kaczmarz algorithm. We have\nalso modeled the problem of face recognition from video as the multiple\nmeasurement vector problem and solved using our proposed technique. We have\ncompared the proposed algorithm with state-of-art spectral projected gradient\nalgorithm for multiple measurement vectors on both real and synthetic datasets.\nThe Monte Carlo simulations confirms that our proposed algorithm have better\nrecovery and convergence rate than the MMV version of spectral projected\ngradient algorithm under fairness constraints.\n", "versions": [{"version": "v1", "created": "Fri, 10 Jan 2014 11:24:35 GMT"}, {"version": "v2", "created": "Sun, 26 Jan 2014 10:05:15 GMT"}, {"version": "v3", "created": "Sun, 2 Feb 2014 08:13:58 GMT"}], "update_date": "2014-02-04", "authors_parsed": [["Aggarwal", "Hemant Kumar", ""], ["Majumdar", "Angshul", ""]]}, {"id": "1401.2304", "submitter": "Stefan Hummelsheim", "authors": "Stefan Hummelsheim", "title": "Lasso and equivalent quadratic penalized models", "comments": "7 pages with 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The least absolute shrinkage and selection operator (lasso) and ridge\nregression produce usually different estimates although input, loss function\nand parameterization of the penalty are identical. In this paper we look for\nridge and lasso models with identical solution set.\n  It turns out, that the lasso model with shrink vector $\\lambda$ and a\nquadratic penalized model with shrink matrix as outer product of $\\lambda$ with\nitself are equivalent, in the sense that they have equal solutions. To achieve\nthis, we have to restrict the estimates to be positive. This doesn't limit the\narea of application since we can easily decompose every estimate in a positive\nand negative part. The resulting problem can be solved with a non negative\nleast square algorithm.\n  Beside this quadratic penalized model, an augmented regression model with\npositive bounded estimates is developed which is also equivalent to the lasso\nmodel, but is probably faster to solve.\n", "versions": [{"version": "v1", "created": "Fri, 10 Jan 2014 12:23:47 GMT"}], "update_date": "2014-01-13", "authors_parsed": [["Hummelsheim", "Stefan", ""]]}, {"id": "1401.2411", "submitter": "L. Thorne McCarty", "authors": "L. Thorne McCarty", "title": "Clustering, Coding, and the Concept of Similarity", "comments": "Revised and expanded in response to referee reports. Current version:\n  65 pages, 18 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper develops a theory of clustering and coding which combines a\ngeometric model with a probabilistic model in a principled way. The geometric\nmodel is a Riemannian manifold with a Riemannian metric, ${g}_{ij}({\\bf x})$,\nwhich we interpret as a measure of dissimilarity. The probabilistic model\nconsists of a stochastic process with an invariant probability measure which\nmatches the density of the sample input data. The link between the two models\nis a potential function, $U({\\bf x})$, and its gradient, $\\nabla U({\\bf x})$.\nWe use the gradient to define the dissimilarity metric, which guarantees that\nour measure of dissimilarity will depend on the probability measure. Finally,\nwe use the dissimilarity metric to define a coordinate system on the embedded\nRiemannian manifold, which gives us a low-dimensional encoding of our original\ndata.\n", "versions": [{"version": "v1", "created": "Fri, 10 Jan 2014 17:36:23 GMT"}, {"version": "v2", "created": "Wed, 16 May 2018 13:16:05 GMT"}], "update_date": "2018-05-17", "authors_parsed": [["McCarty", "L. Thorne", ""]]}, {"id": "1401.2490", "submitter": "Sinan Yildirim", "authors": "Sinan Yildirim, A. Taylan Cemgil, Sumeetpal S. Singh", "title": "An Online Expectation-Maximisation Algorithm for Nonnegative Matrix\n  Factorisation Models", "comments": "6 pages, 3 figures", "journal-ref": "16th IFAC Symposium on System Identification, 2012, Volume 16,\n  Part 1,", "doi": "10.3182/20120711-3-BE-2027.00312", "report-no": null, "categories": "cs.LG stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we formulate the nonnegative matrix factorisation (NMF) problem\nas a maximum likelihood estimation problem for hidden Markov models and propose\nonline expectation-maximisation (EM) algorithms to estimate the NMF and the\nother unknown static parameters. We also propose a sequential Monte Carlo\napproximation of our online EM algorithm. We show the performance of the\nproposed method with two numerical examples.\n", "versions": [{"version": "v1", "created": "Sat, 11 Jan 2014 00:54:27 GMT"}], "update_date": "2014-01-14", "authors_parsed": [["Yildirim", "Sinan", ""], ["Cemgil", "A. Taylan", ""], ["Singh", "Sumeetpal S.", ""]]}, {"id": "1401.2504", "submitter": "Tao Xiong", "authors": "Yukun Bao, Tao Xiong, Zhongyi Hu", "title": "Multi-Step-Ahead Time Series Prediction using Multiple-Output Support\n  Vector Regression", "comments": "26 pages", "journal-ref": null, "doi": "10.1016/j.neucom.2013.09.010", "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Accurate time series prediction over long future horizons is challenging and\nof great interest to both practitioners and academics. As a well-known\nintelligent algorithm, the standard formulation of Support Vector Regression\n(SVR) could be taken for multi-step-ahead time series prediction, only relying\neither on iterated strategy or direct strategy. This study proposes a novel\nmultiple-step-ahead time series prediction approach which employs\nmultiple-output support vector regression (M-SVR) with multiple-input\nmultiple-output (MIMO) prediction strategy. In addition, the rank of three\nleading prediction strategies with SVR is comparatively examined, providing\npractical implications on the selection of the prediction strategy for\nmulti-step-ahead forecasting while taking SVR as modeling technique. The\nproposed approach is validated with the simulated and real datasets. The\nquantitative and comprehensive assessments are performed on the basis of the\nprediction accuracy and computational cost. The results indicate that: 1) the\nM-SVR using MIMO strategy achieves the best accurate forecasts with accredited\ncomputational load, 2) the standard SVR using direct strategy achieves the\nsecond best accurate forecasts, but with the most expensive computational cost,\nand 3) the standard SVR using iterated strategy is the worst in terms of\nprediction accuracy, but with the least computational cost.\n", "versions": [{"version": "v1", "created": "Sat, 11 Jan 2014 06:14:53 GMT"}], "update_date": "2014-01-14", "authors_parsed": [["Bao", "Yukun", ""], ["Xiong", "Tao", ""], ["Hu", "Zhongyi", ""]]}, {"id": "1401.2668", "submitter": "Jinbo Xu", "authors": "Jianzhu Ma, Sheng Wang, Zhiyong Wang and Jinbo Xu", "title": "MRFalign: Protein Homology Detection through Alignment of Markov Random\n  Fields", "comments": "Accepted by both RECOMB 2014 and PLOS Computational Biology", "journal-ref": null, "doi": "10.1371/journal.pcbi.1003500", "report-no": null, "categories": "q-bio.QM cs.CE cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sequence-based protein homology detection has been extensively studied and so\nfar the most sensitive method is based upon comparison of protein sequence\nprofiles, which are derived from multiple sequence alignment (MSA) of sequence\nhomologs in a protein family. A sequence profile is usually represented as a\nposition-specific scoring matrix (PSSM) or an HMM (Hidden Markov Model) and\naccordingly PSSM-PSSM or HMM-HMM comparison is used for homolog detection. This\npaper presents a new homology detection method MRFalign, consisting of three\nkey components: 1) a Markov Random Fields (MRF) representation of a protein\nfamily; 2) a scoring function measuring similarity of two MRFs; and 3) an\nefficient ADMM (Alternating Direction Method of Multipliers) algorithm aligning\ntwo MRFs. Compared to HMM that can only model very short-range residue\ncorrelation, MRFs can model long-range residue interaction pattern and thus,\nencode information for the global 3D structure of a protein family.\nConsequently, MRF-MRF comparison for remote homology detection shall be much\nmore sensitive than HMM-HMM or PSSM-PSSM comparison. Experiments confirm that\nMRFalign outperforms several popular HMM or PSSM-based methods in terms of both\nalignment accuracy and remote homology detection and that MRFalign works\nparticularly well for mainly beta proteins. For example, tested on the\nbenchmark SCOP40 (8353 proteins) for homology detection, PSSM-PSSM and HMM-HMM\nsucceed on 48% and 52% of proteins, respectively, at superfamily level, and on\n15% and 27% of proteins, respectively, at fold level. In contrast, MRFalign\nsucceeds on 57.3% and 42.5% of proteins at superfamily and fold level,\nrespectively. This study implies that long-range residue interaction patterns\nare very helpful for sequence-based homology detection. The software is\navailable for download at http://raptorx.uchicago.edu/download/.\n", "versions": [{"version": "v1", "created": "Sun, 12 Jan 2014 20:41:08 GMT"}, {"version": "v2", "created": "Wed, 15 Jan 2014 01:55:17 GMT"}], "update_date": "2015-06-18", "authors_parsed": [["Ma", "Jianzhu", ""], ["Wang", "Sheng", ""], ["Wang", "Zhiyong", ""], ["Xu", "Jinbo", ""]]}, {"id": "1401.2688", "submitter": "Kiran Sree Pokkuluri Prof", "authors": "Pokkuluri Kiran Sree, Inamupudi Ramesh Babu, SSSN Usha Devi N", "title": "PSMACA: An Automated Protein Structure Prediction Using MACA (Multiple\n  Attractor Cellular Automata)", "comments": "6 pages. arXiv admin note: substantial text overlap with\n  arXiv:1310.4342, arXiv:1310.4495", "journal-ref": "Journal of Bioinformatics and Intelligent Control Vol 2, pp\n  211--215, 2013", "doi": "10.1166/jbic.2013.1052", "report-no": null, "categories": "cs.CE cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Protein Structure Predication from sequences of amino acid has gained a\nremarkable attention in recent years. Even though there are some prediction\ntechniques addressing this problem, the approximate accuracy in predicting the\nprotein structure is closely 75%. An automated procedure was evolved with MACA\n(Multiple Attractor Cellular Automata) for predicting the structure of the\nprotein. Most of the existing approaches are sequential which will classify the\ninput into four major classes and these are designed for similar sequences.\nPSMACA is designed to identify ten classes from the sequences that share\ntwilight zone similarity and identity with the training sequences. This method\nalso predicts three states (helix, strand, and coil) for the structure. Our\ncomprehensive design considers 10 feature selection methods and 4 classifiers\nto develop MACA (Multiple Attractor Cellular Automata) based classifiers that\nare build for each of the ten classes. We have tested the proposed classifier\nwith twilight-zone and 1-high-similarity benchmark datasets with over three\ndozens of modern competing predictors shows that PSMACA provides the best\noverall accuracy that ranges between 77% and 88.7% depending on the dataset.\n", "versions": [{"version": "v1", "created": "Mon, 13 Jan 2014 00:38:52 GMT"}], "update_date": "2014-01-14", "authors_parsed": [["Sree", "Pokkuluri Kiran", ""], ["Babu", "Inamupudi Ramesh", ""], ["N", "SSSN Usha Devi", ""]]}, {"id": "1401.2753", "submitter": "Peilin Zhao", "authors": "Peilin Zhao, Tong Zhang", "title": "Stochastic Optimization with Importance Sampling", "comments": "29 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Uniform sampling of training data has been commonly used in traditional\nstochastic optimization algorithms such as Proximal Stochastic Gradient Descent\n(prox-SGD) and Proximal Stochastic Dual Coordinate Ascent (prox-SDCA). Although\nuniform sampling can guarantee that the sampled stochastic quantity is an\nunbiased estimate of the corresponding true quantity, the resulting estimator\nmay have a rather high variance, which negatively affects the convergence of\nthe underlying optimization procedure. In this paper we study stochastic\noptimization with importance sampling, which improves the convergence rate by\nreducing the stochastic variance. Specifically, we study prox-SGD (actually,\nstochastic mirror descent) with importance sampling and prox-SDCA with\nimportance sampling. For prox-SGD, instead of adopting uniform sampling\nthroughout the training process, the proposed algorithm employs importance\nsampling to minimize the variance of the stochastic gradient. For prox-SDCA,\nthe proposed importance sampling scheme aims to achieve higher expected dual\nvalue at each dual coordinate ascent step. We provide extensive theoretical\nanalysis to show that the convergence rates with the proposed importance\nsampling methods can be significantly improved under suitable conditions both\nfor prox-SGD and for prox-SDCA. Experiments are provided to verify the\ntheoretical analysis.\n", "versions": [{"version": "v1", "created": "Mon, 13 Jan 2014 08:47:44 GMT"}, {"version": "v2", "created": "Fri, 2 Jan 2015 09:17:48 GMT"}], "update_date": "2015-01-05", "authors_parsed": [["Zhao", "Peilin", ""], ["Zhang", "Tong", ""]]}, {"id": "1401.2838", "submitter": "Edward Meeds", "authors": "Edward Meeds and Max Welling", "title": "GPS-ABC: Gaussian Process Surrogate Approximate Bayesian Computation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG q-bio.QM stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Scientists often express their understanding of the world through a\ncomputationally demanding simulation program. Analyzing the posterior\ndistribution of the parameters given observations (the inverse problem) can be\nextremely challenging. The Approximate Bayesian Computation (ABC) framework is\nthe standard statistical tool to handle these likelihood free problems, but\nthey require a very large number of simulations. In this work we develop two\nnew ABC sampling algorithms that significantly reduce the number of simulations\nnecessary for posterior inference. Both algorithms use confidence estimates for\nthe accept probability in the Metropolis Hastings step to adaptively choose the\nnumber of necessary simulations. Our GPS-ABC algorithm stores the information\nobtained from every simulation in a Gaussian process which acts as a surrogate\nfunction for the simulated statistics. Experiments on a challenging realistic\nbiological problem illustrate the potential of these algorithms.\n", "versions": [{"version": "v1", "created": "Mon, 13 Jan 2014 14:02:37 GMT"}], "update_date": "2014-01-14", "authors_parsed": [["Meeds", "Edward", ""], ["Welling", "Max", ""]]}, {"id": "1401.2949", "submitter": "Larry Bull", "authors": "Larry Bull", "title": "Exploiting generalisation symmetries in accuracy-based learning\n  classifier systems: An initial study", "comments": "6 pages, 13 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modern learning classifier systems typically exploit a niched genetic\nalgorithm to facilitate rule discovery. When used for reinforcement learning,\nsuch rules represent generalisations over the state-action-reward space. Whilst\nencouraging maximal generality, the niching can potentially hinder the\nformation of generalisations in the state space which are symmetrical, or very\nsimilar, over different actions. This paper introduces the use of rules which\ncontain multiple actions, maintaining accuracy and reward metrics for each\naction. It is shown that problem symmetries can be exploited, improving\nperformance, whilst not degrading performance when symmetries are reduced.\n", "versions": [{"version": "v1", "created": "Fri, 10 Jan 2014 12:46:56 GMT"}], "update_date": "2014-01-14", "authors_parsed": [["Bull", "Larry", ""]]}, {"id": "1401.2955", "submitter": "Mahdi Pakdaman Naeini", "authors": "Mahdi Pakdaman Naeini, Gregory F. Cooper, Milos Hauskrecht", "title": "Binary Classifier Calibration: Bayesian Non-Parametric Approach", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A set of probabilistic predictions is well calibrated if the events that are\npredicted to occur with probability p do in fact occur about p fraction of the\ntime. Well calibrated predictions are particularly important when machine\nlearning models are used in decision analysis. This paper presents two new\nnon-parametric methods for calibrating outputs of binary classification models:\na method based on the Bayes optimal selection and a method based on the\nBayesian model averaging. The advantage of these methods is that they are\nindependent of the algorithm used to learn a predictive model, and they can be\napplied in a post-processing step, after the model is learned. This makes them\napplicable to a wide variety of machine learning models and methods. These\ncalibration methods, as well as other methods, are tested on a variety of\ndatasets in terms of both discrimination and calibration performance. The\nresults show the methods either outperform or are comparable in performance to\nthe state-of-the-art calibration methods.\n", "versions": [{"version": "v1", "created": "Mon, 13 Jan 2014 19:04:13 GMT"}], "update_date": "2014-01-14", "authors_parsed": [["Naeini", "Mahdi Pakdaman", ""], ["Cooper", "Gregory F.", ""], ["Hauskrecht", "Milos", ""]]}, {"id": "1401.3069", "submitter": "Shashank Mouli Satapathy Mr.", "authors": "Shashank Mouli Satapathy, Santanu Kumar Rath", "title": "Use Case Point Approach Based Software Effort Estimation using Various\n  Support Vector Regression Kernel Methods", "comments": "13 pages, 6 figures, 11 Tables, International Journal of Information\n  Processing (IJIP)", "journal-ref": "International Journal of Information Processing,7(4),2013,87-101", "doi": null, "report-no": null, "categories": "cs.SE cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The job of software effort estimation is a critical one in the early stages\nof the software development life cycle when the details of requirements are\nusually not clearly identified. Various optimization techniques help in\nimproving the accuracy of effort estimation. The Support Vector Regression\n(SVR) is one of several different soft-computing techniques that help in\ngetting optimal estimated values. The idea of SVR is based upon the computation\nof a linear regression function in a high dimensional feature space where the\ninput data are mapped via a nonlinear function. Further, the SVR kernel methods\ncan be applied in transforming the input data and then based on these\ntransformations, an optimal boundary between the possible outputs can be\nobtained. The main objective of the research work carried out in this paper is\nto estimate the software effort using use case point approach. The use case\npoint approach relies on the use case diagram to estimate the size and effort\nof software projects. Then, an attempt has been made to optimize the results\nobtained from use case point analysis using various SVR kernel methods to\nachieve better prediction accuracy.\n", "versions": [{"version": "v1", "created": "Tue, 14 Jan 2014 05:01:58 GMT"}, {"version": "v2", "created": "Wed, 15 Jan 2014 18:02:47 GMT"}], "update_date": "2014-01-16", "authors_parsed": [["Satapathy", "Shashank Mouli", ""], ["Rath", "Santanu Kumar", ""]]}, {"id": "1401.3148", "submitter": "Rodrigo de Lamare", "authors": "S. Xu, R. C. de Lamare and H. V. Poor", "title": "Dynamic Topology Adaptation and Distributed Estimation for Smart Grids", "comments": "4 figures, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.LG math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents new dynamic topology adaptation strategies for\ndistributed estimation in smart grids systems. We propose a dynamic exhaustive\nsearch--based topology adaptation algorithm and a dynamic sparsity--inspired\ntopology adaptation algorithm, which can exploit the topology of smart grids\nwith poor--quality links and obtain performance gains. We incorporate an\noptimized combining rule, named Hastings rule into our proposed dynamic\ntopology adaptation algorithms. Compared with the existing works in the\nliterature on distributed estimation, the proposed algorithms have a better\nconvergence rate and significantly improve the system performance. The\nperformance of the proposed algorithms is compared with that of existing\nalgorithms in the IEEE 14--bus system.\n", "versions": [{"version": "v1", "created": "Tue, 14 Jan 2014 11:35:19 GMT"}], "update_date": "2014-01-16", "authors_parsed": [["Xu", "S.", ""], ["de Lamare", "R. C.", ""], ["Poor", "H. V.", ""]]}, {"id": "1401.3198", "submitter": "Maxim Raginsky", "authors": "Peng Guan and Maxim Raginsky and Rebecca Willett", "title": "Online Markov decision processes with Kullback-Leibler control cost", "comments": "to appear in IEEE Transactions on Automatic Control", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.LG cs.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper considers an online (real-time) control problem that involves an\nagent performing a discrete-time random walk over a finite state space. The\nagent's action at each time step is to specify the probability distribution for\nthe next state given the current state. Following the set-up of Todorov, the\nstate-action cost at each time step is a sum of a state cost and a control cost\ngiven by the Kullback-Leibler (KL) divergence between the agent's next-state\ndistribution and that determined by some fixed passive dynamics. The online\naspect of the problem is due to the fact that the state cost functions are\ngenerated by a dynamic environment, and the agent learns the current state cost\nonly after selecting an action. An explicit construction of a computationally\nefficient strategy with small regret (i.e., expected difference between its\nactual total cost and the smallest cost attainable using noncausal knowledge of\nthe state costs) under mild regularity conditions is presented, along with a\ndemonstration of the performance of the proposed strategy on a simulated target\ntracking problem. A number of new results on Markov decision processes with KL\ncontrol cost are also obtained.\n", "versions": [{"version": "v1", "created": "Tue, 14 Jan 2014 14:40:29 GMT"}], "update_date": "2014-01-15", "authors_parsed": [["Guan", "Peng", ""], ["Raginsky", "Maxim", ""], ["Willett", "Rebecca", ""]]}, {"id": "1401.3258", "submitter": "Jeremy Kun", "authors": "Rajmonda Caceres, Kevin Carter, Jeremy Kun", "title": "A Boosting Approach to Learning Graph Representations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.SI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning the right graph representation from noisy, multisource data has\ngarnered significant interest in recent years. A central tenet of this problem\nis relational learning. Here the objective is to incorporate the partial\ninformation each data source gives us in a way that captures the true\nunderlying relationships. To address this challenge, we present a general,\nboosting-inspired framework for combining weak evidence of entity associations\ninto a robust similarity metric. We explore the extent to which different\nquality measurements yield graph representations that are suitable for\ncommunity detection. We then present empirical results on both synthetic and\nreal datasets demonstrating the utility of this framework. Our framework leads\nto suitable global graph representations from quality measurements local to\neach edge. Finally, we discuss future extensions and theoretical considerations\nof learning useful graph representations from weak feedback in general\napplication settings.\n", "versions": [{"version": "v1", "created": "Tue, 14 Jan 2014 17:07:01 GMT"}], "update_date": "2014-01-15", "authors_parsed": [["Caceres", "Rajmonda", ""], ["Carter", "Kevin", ""], ["Kun", "Jeremy", ""]]}, {"id": "1401.3322", "submitter": "Zoran Cvetkovic", "authors": "Jibran Yousafzai and Zoran Cvetkovic and Peter Sollich and Matthew\n  Ager", "title": "A Subband-Based SVM Front-End for Robust ASR", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG cs.SD", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work proposes a novel support vector machine (SVM) based robust\nautomatic speech recognition (ASR) front-end that operates on an ensemble of\nthe subband components of high-dimensional acoustic waveforms. The key issues\nof selecting the appropriate SVM kernels for classification in frequency\nsubbands and the combination of individual subband classifiers using ensemble\nmethods are addressed. The proposed front-end is compared with state-of-the-art\nASR front-ends in terms of robustness to additive noise and linear filtering.\nExperiments performed on the TIMIT phoneme classification task demonstrate the\nbenefits of the proposed subband based SVM front-end: it outperforms the\nstandard cepstral front-end in the presence of noise and linear filtering for\nsignal-to-noise ratio (SNR) below 12-dB. A combination of the proposed\nfront-end with a conventional front-end such as MFCC yields further\nimprovements over the individual front ends across the full range of noise\nlevels.\n", "versions": [{"version": "v1", "created": "Tue, 24 Dec 2013 08:45:07 GMT"}], "update_date": "2014-01-15", "authors_parsed": [["Yousafzai", "Jibran", ""], ["Cvetkovic", "Zoran", ""], ["Sollich", "Peter", ""], ["Ager", "Matthew", ""]]}, {"id": "1401.3372", "submitter": "Linas Vepstas PhD", "authors": "Linas Vepstas and Ben Goertzel", "title": "Learning Language from a Large (Unannotated) Corpus", "comments": "29 pages, 5 figures, research proposal", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A novel approach to the fully automated, unsupervised extraction of\ndependency grammars and associated syntax-to-semantic-relationship mappings\nfrom large text corpora is described. The suggested approach builds on the\nauthors' prior work with the Link Grammar, RelEx and OpenCog systems, as well\nas on a number of prior papers and approaches from the statistical language\nlearning literature. If successful, this approach would enable the mining of\nall the information needed to power a natural language comprehension and\ngeneration system, directly from a large, unannotated corpus.\n", "versions": [{"version": "v1", "created": "Tue, 14 Jan 2014 22:10:30 GMT"}], "update_date": "2014-01-16", "authors_parsed": [["Vepstas", "Linas", ""], ["Goertzel", "Ben", ""]]}, {"id": "1401.3390", "submitter": "Mahdi Pakdaman Naeini", "authors": "Mahdi Pakdaman Naeini, Gregory F. Cooper, Milos Hauskrecht", "title": "Binary Classifier Calibration: Non-parametric approach", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Accurate calibration of probabilistic predictive models learned is critical\nfor many practical prediction and decision-making tasks. There are two main\ncategories of methods for building calibrated classifiers. One approach is to\ndevelop methods for learning probabilistic models that are well-calibrated, ab\ninitio. The other approach is to use some post-processing methods for\ntransforming the output of a classifier to be well calibrated, as for example\nhistogram binning, Platt scaling, and isotonic regression. One advantage of the\npost-processing approach is that it can be applied to any existing\nprobabilistic classification model that was constructed using any\nmachine-learning method.\n  In this paper, we first introduce two measures for evaluating how well a\nclassifier is calibrated. We prove three theorems showing that using a simple\nhistogram binning post-processing method, it is possible to make a classifier\nbe well calibrated while retaining its discrimination capability. Also, by\ncasting the histogram binning method as a density-based non-parametric binary\nclassifier, we can extend it using two simple non-parametric density estimation\nmethods. We demonstrate the performance of the proposed calibration methods on\nsynthetic and real datasets. Experimental results show that the proposed\nmethods either outperform or are comparable to existing calibration methods.\n", "versions": [{"version": "v1", "created": "Tue, 14 Jan 2014 23:52:16 GMT"}], "update_date": "2014-01-16", "authors_parsed": [["Naeini", "Mahdi Pakdaman", ""], ["Cooper", "Gregory F.", ""], ["Hauskrecht", "Milos", ""]]}, {"id": "1401.3409", "submitter": "Xiaowei Zhou", "authors": "Xiaowei Zhou, Can Yang, Hongyu Zhao, Weichuan Yu", "title": "Low-Rank Modeling and Its Applications in Image Analysis", "comments": "To appear in ACM Computing Surveys", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Low-rank modeling generally refers to a class of methods that solve problems\nby representing variables of interest as low-rank matrices. It has achieved\ngreat success in various fields including computer vision, data mining, signal\nprocessing and bioinformatics. Recently, much progress has been made in\ntheories, algorithms and applications of low-rank modeling, such as exact\nlow-rank matrix recovery via convex programming and matrix completion applied\nto collaborative filtering. These advances have brought more and more\nattentions to this topic. In this paper, we review the recent advance of\nlow-rank modeling, the state-of-the-art algorithms, and related applications in\nimage analysis. We first give an overview to the concept of low-rank modeling\nand challenging problems in this area. Then, we summarize the models and\nalgorithms for low-rank matrix recovery and illustrate their advantages and\nlimitations with numerical experiments. Next, we introduce a few applications\nof low-rank modeling in the context of image analysis. Finally, we conclude\nthis paper with some discussions.\n", "versions": [{"version": "v1", "created": "Wed, 15 Jan 2014 02:17:33 GMT"}, {"version": "v2", "created": "Tue, 10 Jun 2014 03:40:29 GMT"}, {"version": "v3", "created": "Thu, 23 Oct 2014 02:05:18 GMT"}], "update_date": "2014-10-24", "authors_parsed": [["Zhou", "Xiaowei", ""], ["Yang", "Can", ""], ["Zhao", "Hongyu", ""], ["Yu", "Weichuan", ""]]}, {"id": "1401.3413", "submitter": "Avneesh Saluja", "authors": "Avneesh Saluja, Mahdi Pakdaman, Dongzhen Piao, Ankur P. Parikh", "title": "Infinite Mixed Membership Matrix Factorization", "comments": "For ICDM 2013 Workshop Proceedings", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Rating and recommendation systems have become a popular application area for\napplying a suite of machine learning techniques. Current approaches rely\nprimarily on probabilistic interpretations and extensions of matrix\nfactorization, which factorizes a user-item ratings matrix into latent user and\nitem vectors. Most of these methods fail to model significant variations in\nitem ratings from otherwise similar users, a phenomenon known as the \"Napoleon\nDynamite\" effect. Recent efforts have addressed this problem by adding a\ncontextual bias term to the rating, which captures the mood under which a user\nrates an item or the context in which an item is rated by a user. In this work,\nwe extend this model in a nonparametric sense by learning the optimal number of\nmoods or contexts from the data, and derive Gibbs sampling inference procedures\nfor our model. We evaluate our approach on the MovieLens 1M dataset, and show\nsignificant improvements over the optimal parametric baseline, more than twice\nthe improvements previously encountered for this task. We also extract and\nevaluate a DBLP dataset, wherein we predict the number of papers co-authored by\ntwo authors, and present improvements over the parametric baseline on this\nalternative domain as well.\n", "versions": [{"version": "v1", "created": "Wed, 15 Jan 2014 02:39:15 GMT"}], "update_date": "2014-01-16", "authors_parsed": [["Saluja", "Avneesh", ""], ["Pakdaman", "Mahdi", ""], ["Piao", "Dongzhen", ""], ["Parikh", "Ankur P.", ""]]}, {"id": "1401.3427", "submitter": "Laurent Miclet", "authors": "Laurent Miclet, Sabri Bayoudh, Arnaud Delhay", "title": "Analogical Dissimilarity: Definition, Algorithms and Two Experiments in\n  Machine Learning", "comments": null, "journal-ref": "Journal Of Artificial Intelligence Research, Volume 32, pages\n  793-824, 2008", "doi": "10.1613/jair.2519", "report-no": null, "categories": "cs.LG cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper defines the notion of analogical dissimilarity between four\nobjects, with a special focus on objects structured as sequences. Firstly, it\nstudies the case where the four objects have a null analogical dissimilarity,\ni.e. are in analogical proportion. Secondly, when one of these objects is\nunknown, it gives algorithms to compute it. Thirdly, it tackles the problem of\ndefining analogical dissimilarity, which is a measure of how far four objects\nare from being in analogical proportion. In particular, when objects are\nsequences, it gives a definition and an algorithm based on an optimal alignment\nof the four sequences. It gives also learning algorithms, i.e. methods to find\nthe triple of objects in a learning sample which has the least analogical\ndissimilarity with a given object. Two practical experiments are described: the\nfirst is a classification problem on benchmarks of binary and nominal data, the\nsecond shows how the generation of sequences by solving analogical equations\nenables a handwritten character recognition system to rapidly be adapted to a\nnew writer.\n", "versions": [{"version": "v1", "created": "Wed, 15 Jan 2014 04:42:13 GMT"}], "update_date": "2014-01-16", "authors_parsed": [["Miclet", "Laurent", ""], ["Bayoudh", "Sabri", ""], ["Delhay", "Arnaud", ""]]}, {"id": "1401.3429", "submitter": "Yi Wang", "authors": "Yi Wang, Nevin L. Zhang, Tao Chen", "title": "Latent Tree Models and Approximate Inference in Bayesian Networks", "comments": null, "journal-ref": "Journal Of Artificial Intelligence Research, Volume 32, pages\n  879-900, 2008", "doi": "10.1613/jair.2530", "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel method for approximate inference in Bayesian networks\n(BNs). The idea is to sample data from a BN, learn a latent tree model (LTM)\nfrom the data offline, and when online, make inference with the LTM instead of\nthe original BN. Because LTMs are tree-structured, inference takes linear time.\nIn the meantime, they can represent complex relationship among leaf nodes and\nhence the approximation accuracy is often good. Empirical evidence shows that\nour method can achieve good approximation accuracy at low online computational\ncost.\n", "versions": [{"version": "v1", "created": "Wed, 15 Jan 2014 04:46:37 GMT"}], "update_date": "2014-01-16", "authors_parsed": [["Wang", "Yi", ""], ["Zhang", "Nevin L.", ""], ["Chen", "Tao", ""]]}, {"id": "1401.3432", "submitter": "Tinne De Laet", "authors": "Tinne De Laet, Joris De Schutter, Herman Bruyninckx", "title": "A Rigorously Bayesian Beam Model and an Adaptive Full Scan Model for\n  Range Finders in Dynamic Environments", "comments": null, "journal-ref": "Journal Of Artificial Intelligence Research, Volume 33, pages\n  179-222, 2008", "doi": "10.1613/jair.2540", "report-no": null, "categories": "cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes and experimentally validates a Bayesian network model of\na range finder adapted to dynamic environments. All modeling assumptions are\nrigorously explained, and all model parameters have a physical interpretation.\nThis approach results in a transparent and intuitive model. With respect to the\nstate of the art beam model this paper: (i) proposes a different functional\nform for the probability of range measurements caused by unmodeled objects,\n(ii) intuitively explains the discontinuity encountered in te state of the art\nbeam model, and (iii) reduces the number of model parameters, while maintaining\nthe same representational power for experimental data. The proposed beam model\nis called RBBM, short for Rigorously Bayesian Beam Model. A maximum likelihood\nand a variational Bayesian estimator (both based on expectation-maximization)\nare proposed to learn the model parameters.\n  Furthermore, the RBBM is extended to a full scan model in two steps: first,\nto a full scan model for static environments and next, to a full scan model for\ngeneral, dynamic environments. The full scan model accounts for the dependency\nbetween beams and adapts to the local sample density when using a particle\nfilter. In contrast to Gaussian-based state of the art models, the proposed\nfull scan model uses a sample-based approximation. This sample-based\napproximation enables handling dynamic environments and capturing\nmulti-modality, which occurs even in simple static environments.\n", "versions": [{"version": "v1", "created": "Wed, 15 Jan 2014 04:49:23 GMT"}], "update_date": "2014-01-16", "authors_parsed": [["De Laet", "Tinne", ""], ["De Schutter", "Joris", ""], ["Bruyninckx", "Herman", ""]]}, {"id": "1401.3434", "submitter": "Bal\\'azs Csan\\'ad Cs\\'aji", "authors": "Bal\\'azs Csan\\'ad Cs\\'aji, L\\'aszl\\'o Monostori", "title": "Adaptive Stochastic Resource Control: A Machine Learning Approach", "comments": null, "journal-ref": "Journal Of Artificial Intelligence Research, Volume 32, pages\n  453-486, 2008", "doi": "10.1613/jair.2548", "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The paper investigates stochastic resource allocation problems with scarce,\nreusable resources and non-preemtive, time-dependent, interconnected tasks.\nThis approach is a natural generalization of several standard resource\nmanagement problems, such as scheduling and transportation problems. First,\nreactive solutions are considered and defined as control policies of suitably\nreformulated Markov decision processes (MDPs). We argue that this reformulation\nhas several favorable properties, such as it has finite state and action\nspaces, it is aperiodic, hence all policies are proper and the space of control\npolicies can be safely restricted. Next, approximate dynamic programming (ADP)\nmethods, such as fitted Q-learning, are suggested for computing an efficient\ncontrol policy. In order to compactly maintain the cost-to-go function, two\nrepresentations are studied: hash tables and support vector regression (SVR),\nparticularly, nu-SVRs. Several additional improvements, such as the application\nof limited-lookahead rollout algorithms in the initial phases, action space\ndecomposition, task clustering and distributed sampling are investigated, too.\nFinally, experimental results on both benchmark and industry-related data are\npresented.\n", "versions": [{"version": "v1", "created": "Wed, 15 Jan 2014 04:50:50 GMT"}], "update_date": "2014-01-16", "authors_parsed": [["Cs\u00e1ji", "Bal\u00e1zs Csan\u00e1d", ""], ["Monostori", "L\u00e1szl\u00f3", ""]]}, {"id": "1401.3441", "submitter": "Ran El-Yaniv", "authors": "Ran El-Yaniv, Dmitry Pechyony", "title": "Transductive Rademacher Complexity and its Applications", "comments": null, "journal-ref": "Journal Of Artificial Intelligence Research, Volume 35, pages\n  193-234, 2009", "doi": "10.1613/jair.2587", "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop a technique for deriving data-dependent error bounds for\ntransductive learning algorithms based on transductive Rademacher complexity.\nOur technique is based on a novel general error bound for transduction in terms\nof transductive Rademacher complexity, together with a novel bounding technique\nfor Rademacher averages for particular algorithms, in terms of their\n\"unlabeled-labeled\" representation. This technique is relevant to many advanced\ngraph-based transductive algorithms and we demonstrate its effectiveness by\nderiving error bounds to three well known algorithms. Finally, we present a new\nPAC-Bayesian bound for mixtures of transductive algorithms based on our\nRademacher bounds.\n", "versions": [{"version": "v1", "created": "Wed, 15 Jan 2014 04:54:14 GMT"}], "update_date": "2014-01-16", "authors_parsed": [["El-Yaniv", "Ran", ""], ["Pechyony", "Dmitry", ""]]}, {"id": "1401.3447", "submitter": "Saher Esmeir", "authors": "Saher Esmeir, Shaul Markovitch", "title": "Anytime Induction of Low-cost, Low-error Classifiers: a Sampling-based\n  Approach", "comments": null, "journal-ref": "Journal Of Artificial Intelligence Research, Volume 33, pages\n  1-31, 2008", "doi": "10.1613/jair.2602", "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Machine learning techniques are gaining prevalence in the production of a\nwide range of classifiers for complex real-world applications with nonuniform\ntesting and misclassification costs. The increasing complexity of these\napplications poses a real challenge to resource management during learning and\nclassification. In this work we introduce ACT (anytime cost-sensitive tree\nlearner), a novel framework for operating in such complex environments. ACT is\nan anytime algorithm that allows learning time to be increased in return for\nlower classification costs. It builds a tree top-down and exploits additional\ntime resources to obtain better estimations for the utility of the different\ncandidate splits. Using sampling techniques, ACT approximates the cost of the\nsubtree under each candidate split and favors the one with a minimal cost. As a\nstochastic algorithm, ACT is expected to be able to escape local minima, into\nwhich greedy methods may be trapped. Experiments with a variety of datasets\nwere conducted to compare ACT to the state-of-the-art cost-sensitive tree\nlearners. The results show that for the majority of domains ACT produces\nsignificantly less costly trees. ACT also exhibits good anytime behavior with\ndiminishing returns.\n", "versions": [{"version": "v1", "created": "Wed, 15 Jan 2014 05:09:07 GMT"}], "update_date": "2014-01-16", "authors_parsed": [["Esmeir", "Saher", ""], ["Markovitch", "Shaul", ""]]}, {"id": "1401.3454", "submitter": "Sherief  Abdallah", "authors": "Sherief Abdallah, Victor Lesser", "title": "A Multiagent Reinforcement Learning Algorithm with Non-linear Dynamics", "comments": null, "journal-ref": "Journal Of Artificial Intelligence Research, Volume 33, pages\n  521-549, 2008", "doi": "10.1613/jair.2628", "report-no": null, "categories": "cs.LG cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Several multiagent reinforcement learning (MARL) algorithms have been\nproposed to optimize agents decisions. Due to the complexity of the problem,\nthe majority of the previously developed MARL algorithms assumed agents either\nhad some knowledge of the underlying game (such as Nash equilibria) and/or\nobserved other agents actions and the rewards they received.\n  We introduce a new MARL algorithm called the Weighted Policy Learner (WPL),\nwhich allows agents to reach a Nash Equilibrium (NE) in benchmark\n2-player-2-action games with minimum knowledge. Using WPL, the only feedback an\nagent needs is its own local reward (the agent does not observe other agents\nactions or rewards). Furthermore, WPL does not assume that agents know the\nunderlying game or the corresponding Nash Equilibrium a priori. We\nexperimentally show that our algorithm converges in benchmark\ntwo-player-two-action games. We also show that our algorithm converges in the\nchallenging Shapleys game where previous MARL algorithms failed to converge\nwithout knowing the underlying game or the NE. Furthermore, we show that WPL\noutperforms the state-of-the-art algorithms in a more realistic setting of 100\nagents interacting and learning concurrently.\n  An important aspect of understanding the behavior of a MARL algorithm is\nanalyzing the dynamics of the algorithm: how the policies of multiple learning\nagents evolve over time as agents interact with one another. Such an analysis\nnot only verifies whether agents using a given MARL algorithm will eventually\nconverge, but also reveals the behavior of the MARL algorithm prior to\nconvergence. We analyze our algorithm in two-player-two-action games and show\nthat symbolically proving WPLs convergence is difficult, because of the\nnon-linear nature of WPLs dynamics, unlike previous MARL algorithms that had\neither linear or piece-wise-linear dynamics. Instead, we numerically solve WPLs\ndynamics differential equations and compare the solution to the dynamics of\nprevious MARL algorithms.\n", "versions": [{"version": "v1", "created": "Wed, 15 Jan 2014 05:13:47 GMT"}], "update_date": "2014-01-16", "authors_parsed": [["Abdallah", "Sherief", ""], ["Lesser", "Victor", ""]]}, {"id": "1401.3464", "submitter": "R\\'on\\'an Daly", "authors": "R\\'on\\'an Daly, Qiang Shen", "title": "Learning Bayesian Network Equivalence Classes with Ant Colony\n  Optimization", "comments": null, "journal-ref": "Journal Of Artificial Intelligence Research, Volume 35, pages\n  391-447, 2009", "doi": "10.1613/jair.2681", "report-no": null, "categories": "cs.NE cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bayesian networks are a useful tool in the representation of uncertain\nknowledge. This paper proposes a new algorithm called ACO-E, to learn the\nstructure of a Bayesian network. It does this by conducting a search through\nthe space of equivalence classes of Bayesian networks using Ant Colony\nOptimization (ACO). To this end, two novel extensions of traditional ACO\ntechniques are proposed and implemented. Firstly, multiple types of moves are\nallowed. Secondly, moves can be given in terms of indices that are not based on\nconstruction graph nodes. The results of testing show that ACO-E performs\nbetter than a greedy search and other state-of-the-art and metaheuristic\nalgorithms whilst searching in the space of equivalence classes.\n", "versions": [{"version": "v1", "created": "Wed, 15 Jan 2014 05:22:48 GMT"}], "update_date": "2014-01-16", "authors_parsed": [["Daly", "R\u00f3n\u00e1n", ""], ["Shen", "Qiang", ""]]}, {"id": "1401.3478", "submitter": "Facundo Bromberg", "authors": "Facundo Bromberg, Dimitris Margaritis, Vasant Honavar", "title": "Efficient Markov Network Structure Discovery Using Independence Tests", "comments": null, "journal-ref": "Journal Of Artificial Intelligence Research, Volume 35, pages\n  449-484, 2009", "doi": "10.1613/jair.2773", "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present two algorithms for learning the structure of a Markov network from\ndata: GSMN* and GSIMN. Both algorithms use statistical independence tests to\ninfer the structure by successively constraining the set of structures\nconsistent with the results of these tests. Until very recently, algorithms for\nstructure learning were based on maximum likelihood estimation, which has been\nproved to be NP-hard for Markov networks due to the difficulty of estimating\nthe parameters of the network, needed for the computation of the data\nlikelihood. The independence-based approach does not require the computation of\nthe likelihood, and thus both GSMN* and GSIMN can compute the structure\nefficiently (as shown in our experiments). GSMN* is an adaptation of the\nGrow-Shrink algorithm of Margaritis and Thrun for learning the structure of\nBayesian networks. GSIMN extends GSMN* by additionally exploiting Pearls\nwell-known properties of the conditional independence relation to infer novel\nindependences from known ones, thus avoiding the performance of statistical\ntests to estimate them. To accomplish this efficiently GSIMN uses the Triangle\ntheorem, also introduced in this work, which is a simplified version of the set\nof Markov axioms. Experimental comparisons on artificial and real-world data\nsets show GSIMN can yield significant savings with respect to GSMN*, while\ngenerating a Markov network with comparable or in some cases improved quality.\nWe also compare GSIMN to a forward-chaining implementation, called GSIMN-FCH,\nthat produces all possible conditional independences resulting from repeatedly\napplying Pearls theorems on the known conditional independence tests. The\nresults of this comparison show that GSIMN, by the sole use of the Triangle\ntheorem, is nearly optimal in terms of the set of independences tests that it\ninfers.\n", "versions": [{"version": "v1", "created": "Wed, 15 Jan 2014 05:33:29 GMT"}], "update_date": "2014-01-16", "authors_parsed": [["Bromberg", "Facundo", ""], ["Margaritis", "Dimitris", ""], ["Honavar", "Vasant", ""]]}, {"id": "1401.3479", "submitter": "Yllias  Chali", "authors": "Yllias Chali, Shafiq Rayhan Joty, Sadid A. Hasan", "title": "Complex Question Answering: Unsupervised Learning Approaches and\n  Experiments", "comments": null, "journal-ref": "Journal Of Artificial Intelligence Research, Volume 35, pages\n  1-47, 2009", "doi": "10.1613/jair.2784", "report-no": null, "categories": "cs.CL cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Complex questions that require inferencing and synthesizing information from\nmultiple documents can be seen as a kind of topic-oriented, informative\nmulti-document summarization where the goal is to produce a single text as a\ncompressed version of a set of documents with a minimum loss of relevant\ninformation. In this paper, we experiment with one empirical method and two\nunsupervised statistical machine learning techniques: K-means and Expectation\nMaximization (EM), for computing relative importance of the sentences. We\ncompare the results of these approaches. Our experiments show that the\nempirical approach outperforms the other two techniques and EM performs better\nthan K-means. However, the performance of these approaches depends entirely on\nthe feature set used and the weighting of these features. In order to measure\nthe importance and relevance to the user query we extract different kinds of\nfeatures (i.e. lexical, lexical semantic, cosine similarity, basic element,\ntree kernel based syntactic and shallow-semantic) for each of the document\nsentences. We use a local search technique to learn the weights of the\nfeatures. To the best of our knowledge, no study has used tree kernel functions\nto encode syntactic/semantic information for more complex tasks such as\ncomputing the relatedness between the query sentences and the document\nsentences in order to generate query-focused summaries (or answers to complex\nquestions). For each of our methods of generating summaries (i.e. empirical,\nK-means and EM) we show the effects of syntactic and shallow-semantic features\nover the bag-of-words (BOW) features.\n", "versions": [{"version": "v1", "created": "Wed, 15 Jan 2014 05:33:57 GMT"}], "update_date": "2014-01-16", "authors_parsed": [["Chali", "Yllias", ""], ["Joty", "Shafiq Rayhan", ""], ["Hasan", "Sadid A.", ""]]}, {"id": "1401.3488", "submitter": "Harr Chen", "authors": "Harr Chen, S.R.K. Branavan, Regina Barzilay, David R. Karger", "title": "Content Modeling Using Latent Permutations", "comments": null, "journal-ref": "Journal Of Artificial Intelligence Research, Volume 36, pages\n  129-163, 2009", "doi": "10.1613/jair.2830", "report-no": null, "categories": "cs.IR cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a novel Bayesian topic model for learning discourse-level document\nstructure. Our model leverages insights from discourse theory to constrain\nlatent topic assignments in a way that reflects the underlying organization of\ndocument topics. We propose a global model in which both topic selection and\nordering are biased to be similar across a collection of related documents. We\nshow that this space of orderings can be effectively represented using a\ndistribution over permutations called the Generalized Mallows Model. We apply\nour method to three complementary discourse-level tasks: cross-document\nalignment, document segmentation, and information ordering. Our experiments\nshow that incorporating our permutation-based model in these applications\nyields substantial improvements in performance over previously proposed\nmethods.\n", "versions": [{"version": "v1", "created": "Wed, 15 Jan 2014 05:38:17 GMT"}], "update_date": "2014-01-16", "authors_parsed": [["Chen", "Harr", ""], ["Branavan", "S. R. K.", ""], ["Barzilay", "Regina", ""], ["Karger", "David R.", ""]]}, {"id": "1401.3531", "submitter": "Ben Fulcher", "authors": "Ben D. Fulcher and Nick S. Jones", "title": "Highly comparative feature-based time-series classification", "comments": null, "journal-ref": "IEEE Trans. Knowl. Data Eng. 26, 3026 (2014)", "doi": "10.1109/TKDE.2014.2316504", "report-no": null, "categories": "cs.LG cs.AI cs.DB physics.data-an q-bio.QM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A highly comparative, feature-based approach to time series classification is\nintroduced that uses an extensive database of algorithms to extract thousands\nof interpretable features from time series. These features are derived from\nacross the scientific time-series analysis literature, and include summaries of\ntime series in terms of their correlation structure, distribution, entropy,\nstationarity, scaling properties, and fits to a range of time-series models.\nAfter computing thousands of features for each time series in a training set,\nthose that are most informative of the class structure are selected using\ngreedy forward feature selection with a linear classifier. The resulting\nfeature-based classifiers automatically learn the differences between classes\nusing a reduced number of time-series properties, and circumvent the need to\ncalculate distances between time series. Representing time series in this way\nresults in orders of magnitude of dimensionality reduction, allowing the method\nto perform well on very large datasets containing long time series or time\nseries of different lengths. For many of the datasets studied, classification\nperformance exceeded that of conventional instance-based classifiers, including\none nearest neighbor classifiers using Euclidean distances and dynamic time\nwarping and, most importantly, the features selected provide an understanding\nof the properties of the dataset, insight that can guide further scientific\ninvestigation.\n", "versions": [{"version": "v1", "created": "Wed, 15 Jan 2014 09:41:50 GMT"}, {"version": "v2", "created": "Fri, 9 May 2014 00:05:57 GMT"}], "update_date": "2017-11-10", "authors_parsed": [["Fulcher", "Ben D.", ""], ["Jones", "Nick S.", ""]]}, {"id": "1401.3579", "submitter": "Keyvan Yahya", "authors": "Keyvan Yahya", "title": "A Supervised Goal Directed Algorithm in Economical Choice Behaviour: An\n  Actor-Critic Approach", "comments": "Must be more flourished and fostered", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GT cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper aims to find an algorithmic structure that affords to predict and\nexplain economical choice behaviour particularly under uncertainty(random\npolicies) by manipulating the prevalent Actor-Critic learning method to comply\nwith the requirements we have been entrusted ever since the field of\nneuroeconomics dawned on us. Whilst skimming some basics of neuroeconomics that\nseem relevant to our discussion, we will try to outline some of the important\nworks which have so far been done to simulate choice making processes.\nConcerning neurological findings that suggest the existence of two specific\nfunctions that are executed through Basal Ganglia all the way up to sub-\ncortical areas, namely 'rewards' and 'beliefs', we will offer a modified\nversion of actor/critic algorithm to shed a light on the relation between these\nfunctions and most importantly resolve what is referred to as a challenge for\nactor-critic algorithms, that is, the lack of inheritance or hierarchy which\navoids the system being evolved in continuous time tasks whence the convergence\nmight not be emerged.\n", "versions": [{"version": "v1", "created": "Fri, 20 Dec 2013 05:54:58 GMT"}, {"version": "v2", "created": "Sat, 22 Feb 2014 19:15:53 GMT"}, {"version": "v3", "created": "Mon, 4 May 2020 01:47:30 GMT"}], "update_date": "2020-05-05", "authors_parsed": [["Yahya", "Keyvan", ""]]}, {"id": "1401.3607", "submitter": "Larry Bull", "authors": "Larry Bull", "title": "A Brief History of Learning Classifier Systems: From CS-1 to XCS", "comments": "37 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modern Learning Classifier Systems can be characterized by their use of rule\naccuracy as the utility metric for the search algorithm(s) discovering useful\nrules. Such searching typically takes place within the restricted space of\nco-active rules for efficiency. This paper gives an historical overview of the\nevolution of such systems up to XCS, and then some of the subsequent\ndevelopments of XCS to different types of learning.\n", "versions": [{"version": "v1", "created": "Wed, 15 Jan 2014 14:37:48 GMT"}, {"version": "v2", "created": "Fri, 7 Feb 2014 11:55:12 GMT"}], "update_date": "2014-02-10", "authors_parsed": [["Bull", "Larry", ""]]}, {"id": "1401.3632", "submitter": "Shaan Qamar", "authors": "Shaan Qamar, Rajarshi Guhaniyogi, David B. Dunson", "title": "Bayesian Conditional Density Filtering", "comments": "41 pages, 7 figures, 12 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a Conditional Density Filtering (C-DF) algorithm for efficient\nonline Bayesian inference. C-DF adapts MCMC sampling to the online setting,\nsampling from approximations to conditional posterior distributions obtained by\npropagating surrogate conditional sufficient statistics (a function of data and\nparameter estimates) as new data arrive. These quantities eliminate the need to\nstore or process the entire dataset simultaneously and offer a number of\ndesirable features. Often, these include a reduction in memory requirements and\nruntime and improved mixing, along with state-of-the-art parameter inference\nand prediction. These improvements are demonstrated through several\nillustrative examples including an application to high dimensional compressed\nregression. Finally, we show that C-DF samples converge to the target posterior\ndistribution asymptotically as sampling proceeds and more data arrives.\n", "versions": [{"version": "v1", "created": "Wed, 15 Jan 2014 15:40:40 GMT"}, {"version": "v2", "created": "Thu, 16 Oct 2014 21:47:00 GMT"}, {"version": "v3", "created": "Tue, 22 Sep 2015 07:41:00 GMT"}], "update_date": "2015-09-23", "authors_parsed": [["Qamar", "Shaan", ""], ["Guhaniyogi", "Rajarshi", ""], ["Dunson", "David B.", ""]]}, {"id": "1401.3737", "submitter": "Tobias Glasmachers", "authors": "Tobias Glasmachers and \\\"Ur\\\"un Dogan", "title": "Coordinate Descent with Online Adaptation of Coordinate Frequencies", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Coordinate descent (CD) algorithms have become the method of choice for\nsolving a number of optimization problems in machine learning. They are\nparticularly popular for training linear models, including linear support\nvector machine classification, LASSO regression, and logistic regression.\n  We consider general CD with non-uniform selection of coordinates. Instead of\nfixing selection frequencies beforehand we propose an online adaptation\nmechanism for this important parameter, called the adaptive coordinate\nfrequencies (ACF) method. This mechanism removes the need to estimate optimal\ncoordinate frequencies beforehand, and it automatically reacts to changing\nrequirements during an optimization run.\n  We demonstrate the usefulness of our ACF-CD approach for a variety of\noptimization problems arising in machine learning contexts. Our algorithm\noffers significant speed-ups over state-of-the-art training methods.\n", "versions": [{"version": "v1", "created": "Wed, 15 Jan 2014 20:50:00 GMT"}], "update_date": "2014-01-16", "authors_parsed": [["Glasmachers", "Tobias", ""], ["Dogan", "\u00dcr\u00fcn", ""]]}, {"id": "1401.3818", "submitter": "Xiaoxia Sun", "authors": "Xiaoxia Sun, Qing Qu, Nasser M. Nasrabadi, Trac D. Tran", "title": "Structured Priors for Sparse-Representation-Based Hyperspectral Image\n  Classification", "comments": "IEEE Geoscience and Remote Sensing Letter", "journal-ref": null, "doi": "10.1109/LGRS.2013.2290531", "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  Pixel-wise classification, where each pixel is assigned to a predefined\nclass, is one of the most important procedures in hyperspectral image (HSI)\nanalysis. By representing a test pixel as a linear combination of a small\nsubset of labeled pixels, a sparse representation classifier (SRC) gives rather\nplausible results compared with that of traditional classifiers such as the\nsupport vector machine (SVM). Recently, by incorporating additional structured\nsparsity priors, the second generation SRCs have appeared in the literature and\nare reported to further improve the performance of HSI. These priors are based\non exploiting the spatial dependencies between the neighboring pixels, the\ninherent structure of the dictionary, or both. In this paper, we review and\ncompare several structured priors for sparse-representation-based HSI\nclassification. We also propose a new structured prior called the low rank\ngroup prior, which can be considered as a modification of the low rank prior.\nFurthermore, we will investigate how different structured priors improve the\nresult for the HSI classification.\n", "versions": [{"version": "v1", "created": "Thu, 16 Jan 2014 03:21:26 GMT"}], "update_date": "2014-01-17", "authors_parsed": [["Sun", "Xiaoxia", ""], ["Qu", "Qing", ""], ["Nasrabadi", "Nasser M.", ""], ["Tran", "Trac D.", ""]]}, {"id": "1401.3829", "submitter": "Amy Greenwald", "authors": "Amy Greenwald, Seong Jae Lee, Victor Naroditskiy", "title": "RoxyBot-06: Stochastic Prediction and Optimization in TAC Travel", "comments": null, "journal-ref": "Journal Of Artificial Intelligence Research, Volume 36, pages\n  513-546, 2009", "doi": "10.1613/jair.2904", "report-no": null, "categories": "cs.GT cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we describe our autonomous bidding agent, RoxyBot, who emerged\nvictorious in the travel division of the 2006 Trading Agent Competition in a\nphoto finish. At a high level, the design of many successful trading agents can\nbe summarized as follows: (i) price prediction: build a model of market prices;\nand (ii) optimization: solve for an approximately optimal set of bids, given\nthis model. To predict, RoxyBot builds a stochastic model of market prices by\nsimulating simultaneous ascending auctions. To optimize, RoxyBot relies on the\nsample average approximation method, a stochastic optimization technique.\n", "versions": [{"version": "v1", "created": "Thu, 16 Jan 2014 04:47:45 GMT"}], "update_date": "2014-01-17", "authors_parsed": [["Greenwald", "Amy", ""], ["Lee", "Seong Jae", ""], ["Naroditskiy", "Victor", ""]]}, {"id": "1401.3836", "submitter": "Liyue Zhao", "authors": "Liyue Zhao, Yu Zhang and Gita Sukthankar", "title": "An Active Learning Approach for Jointly Estimating Worker Performance\n  and Annotation Reliability with Crowdsourced Data", "comments": "10 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Crowdsourcing platforms offer a practical solution to the problem of\naffordably annotating large datasets for training supervised classifiers.\nUnfortunately, poor worker performance frequently threatens to compromise\nannotation reliability, and requesting multiple labels for every instance can\nlead to large cost increases without guaranteeing good results. Minimizing the\nrequired training samples using an active learning selection procedure reduces\nthe labeling requirement but can jeopardize classifier training by focusing on\nerroneous annotations. This paper presents an active learning approach in which\nworker performance, task difficulty, and annotation reliability are jointly\nestimated and used to compute the risk function guiding the sample selection\nprocedure. We demonstrate that the proposed approach, which employs active\nlearning with Bayesian networks, significantly improves training accuracy and\ncorrectly ranks the expertise of unknown labelers in the presence of annotation\nnoise.\n", "versions": [{"version": "v1", "created": "Thu, 16 Jan 2014 04:51:19 GMT"}], "update_date": "2014-01-17", "authors_parsed": [["Zhao", "Liyue", ""], ["Zhang", "Yu", ""], ["Sukthankar", "Gita", ""]]}, {"id": "1401.3870", "submitter": "Erik Talvitie", "authors": "Erik Talvitie, Satinder Singh", "title": "Learning to Make Predictions In Partially Observable Environments\n  Without a Generative Model", "comments": null, "journal-ref": "Journal Of Artificial Intelligence Research, Volume 42, pages\n  353-392, 2011", "doi": "10.1613/jair.3396", "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When faced with the problem of learning a model of a high-dimensional\nenvironment, a common approach is to limit the model to make only a restricted\nset of predictions, thereby simplifying the learning problem. These partial\nmodels may be directly useful for making decisions or may be combined together\nto form a more complete, structured model. However, in partially observable\n(non-Markov) environments, standard model-learning methods learn generative\nmodels, i.e. models that provide a probability distribution over all possible\nfutures (such as POMDPs). It is not straightforward to restrict such models to\nmake only certain predictions, and doing so does not always simplify the\nlearning problem. In this paper we present prediction profile models:\nnon-generative partial models for partially observable systems that make only a\ngiven set of predictions, and are therefore far simpler than generative models\nin some cases. We formalize the problem of learning a prediction profile model\nas a transformation of the original model-learning problem, and show\nempirically that one can learn prediction profile models that make a small set\nof important predictions even in systems that are too complex for standard\ngenerative models.\n", "versions": [{"version": "v1", "created": "Thu, 16 Jan 2014 05:08:29 GMT"}], "update_date": "2014-01-17", "authors_parsed": [["Talvitie", "Erik", ""], ["Singh", "Satinder", ""]]}, {"id": "1401.3871", "submitter": "Mahdi Milani Fard", "authors": "Mahdi Milani Fard, Joelle Pineau", "title": "Non-Deterministic Policies in Markovian Decision Processes", "comments": null, "journal-ref": "Journal Of Artificial Intelligence Research, Volume 40, pages\n  1-24, 2011", "doi": "10.1613/jair.3175", "report-no": null, "categories": "cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Markovian processes have long been used to model stochastic environments.\nReinforcement learning has emerged as a framework to solve sequential planning\nand decision-making problems in such environments. In recent years, attempts\nwere made to apply methods from reinforcement learning to construct decision\nsupport systems for action selection in Markovian environments. Although\nconventional methods in reinforcement learning have proved to be useful in\nproblems concerning sequential decision-making, they cannot be applied in their\ncurrent form to decision support systems, such as those in medical domains, as\nthey suggest policies that are often highly prescriptive and leave little room\nfor the users input. Without the ability to provide flexible guidelines, it is\nunlikely that these methods can gain ground with users of such systems. This\npaper introduces the new concept of non-deterministic policies to allow more\nflexibility in the users decision-making process, while constraining decisions\nto remain near optimal solutions. We provide two algorithms to compute\nnon-deterministic policies in discrete domains. We study the output and running\ntime of these method on a set of synthetic and real-world problems. In an\nexperiment with human subjects, we show that humans assisted by hints based on\nnon-deterministic policies outperform both human-only and computer-only agents\nin a web navigation task.\n", "versions": [{"version": "v1", "created": "Thu, 16 Jan 2014 05:09:10 GMT"}], "update_date": "2014-01-17", "authors_parsed": [["Fard", "Mahdi Milani", ""], ["Pineau", "Joelle", ""]]}, {"id": "1401.3877", "submitter": "Botond Cseke", "authors": "Botond Cseke, Tom Heskes", "title": "Properties of Bethe Free Energies and Message Passing in Gaussian Models", "comments": null, "journal-ref": "Journal Of Artificial Intelligence Research, Volume 41, pages\n  1-24, 2011", "doi": "10.1613/jair.3195", "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We address the problem of computing approximate marginals in Gaussian\nprobabilistic models by using mean field and fractional Bethe approximations.\nWe define the Gaussian fractional Bethe free energy in terms of the moment\nparameters of the approximate marginals, derive a lower and an upper bound on\nthe fractional Bethe free energy and establish a necessary condition for the\nlower bound to be bounded from below. It turns out that the condition is\nidentical to the pairwise normalizability condition, which is known to be a\nsufficient condition for the convergence of the message passing algorithm. We\nshow that stable fixed points of the Gaussian message passing algorithm are\nlocal minima of the Gaussian Bethe free energy. By a counterexample, we\ndisprove the conjecture stating that the unboundedness of the free energy\nimplies the divergence of the message passing algorithm.\n", "versions": [{"version": "v1", "created": "Thu, 16 Jan 2014 05:11:12 GMT"}], "update_date": "2014-01-17", "authors_parsed": [["Cseke", "Botond", ""], ["Heskes", "Tom", ""]]}, {"id": "1401.3880", "submitter": "Harris Papadopoulos", "authors": "Harris Papadopoulos, Vladimir Vovk, Alex Gammerman", "title": "Regression Conformal Prediction with Nearest Neighbours", "comments": null, "journal-ref": "Journal Of Artificial Intelligence Research, Volume 40, pages\n  815-840, 2011", "doi": "10.1613/jair.3198", "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we apply Conformal Prediction (CP) to the k-Nearest Neighbours\nRegression (k-NNR) algorithm and propose ways of extending the typical\nnonconformity measure used for regression so far. Unlike traditional regression\nmethods which produce point predictions, Conformal Predictors output predictive\nregions that satisfy a given confidence level. The regions produced by any\nConformal Predictor are automatically valid, however their tightness and\ntherefore usefulness depends on the nonconformity measure used by each CP. In\neffect a nonconformity measure evaluates how strange a given example is\ncompared to a set of other examples based on some traditional machine learning\nalgorithm. We define six novel nonconformity measures based on the k-Nearest\nNeighbours Regression algorithm and develop the corresponding CPs following\nboth the original (transductive) and the inductive CP approaches. A comparison\nof the predictive regions produced by our measures with those of the typical\nregression measure suggests that a major improvement in terms of predictive\nregion tightness is achieved by the new measures.\n", "versions": [{"version": "v1", "created": "Thu, 16 Jan 2014 05:12:21 GMT"}], "update_date": "2014-01-17", "authors_parsed": [["Papadopoulos", "Harris", ""], ["Vovk", "Vladimir", ""], ["Gammerman", "Alex", ""]]}, {"id": "1401.3894", "submitter": "Andr\\'as Gy\\\"orgy", "authors": "Andr\\'as Gy\\\"orgy, Levente Kocsis", "title": "Efficient Multi-Start Strategies for Local Search Algorithms", "comments": null, "journal-ref": "Journal Of Artificial Intelligence Research, Volume 41, pages\n  407-444, 2011", "doi": "10.1613/jair.3313", "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Local search algorithms applied to optimization problems often suffer from\ngetting trapped in a local optimum. The common solution for this deficiency is\nto restart the algorithm when no progress is observed. Alternatively, one can\nstart multiple instances of a local search algorithm, and allocate\ncomputational resources (in particular, processing time) to the instances\ndepending on their behavior. Hence, a multi-start strategy has to decide\n(dynamically) when to allocate additional resources to a particular instance\nand when to start new instances. In this paper we propose multi-start\nstrategies motivated by works on multi-armed bandit problems and Lipschitz\noptimization with an unknown constant. The strategies continuously estimate the\npotential performance of each algorithm instance by supposing a convergence\nrate of the local search algorithm up to an unknown constant, and in every\nphase allocate resources to those instances that could converge to the optimum\nfor a particular range of the constant. Asymptotic bounds are given on the\nperformance of the strategies. In particular, we prove that at most a quadratic\nincrease in the number of times the target function is evaluated is needed to\nachieve the performance of a local search algorithm started from the attraction\nregion of the optimum. Experiments are provided using SPSA (Simultaneous\nPerturbation Stochastic Approximation) and k-means as local search algorithms,\nand the results indicate that the proposed strategies work well in practice,\nand, in all cases studied, need only logarithmically more evaluations of the\ntarget function as opposed to the theoretically suggested quadratic increase.\n", "versions": [{"version": "v1", "created": "Thu, 16 Jan 2014 05:17:32 GMT"}], "update_date": "2014-01-17", "authors_parsed": [["Gy\u00f6rgy", "Andr\u00e1s", ""], ["Kocsis", "Levente", ""]]}, {"id": "1401.3907", "submitter": "Xiaosong Lu", "authors": "Xiaosong Lu, Howard M. Schwartz, Sidney N. Givigi Jr", "title": "Policy Invariance under Reward Transformations for General-Sum\n  Stochastic Games", "comments": null, "journal-ref": "Journal Of Artificial Intelligence Research, Volume 41, pages\n  397-406, 2011", "doi": "10.1613/jair.3384", "report-no": null, "categories": "cs.GT cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We extend the potential-based shaping method from Markov decision processes\nto multi-player general-sum stochastic games. We prove that the Nash equilibria\nin a stochastic game remains unchanged after potential-based shaping is applied\nto the environment. The property of policy invariance provides a possible way\nof speeding convergence when learning to play a stochastic game.\n", "versions": [{"version": "v1", "created": "Thu, 16 Jan 2014 05:22:56 GMT"}], "update_date": "2014-01-17", "authors_parsed": [["Lu", "Xiaosong", ""], ["Schwartz", "Howard M.", ""], ["Givigi", "Sidney N.", "Jr"]]}, {"id": "1401.3973", "submitter": "Joan Serr\\`a", "authors": "Joan Serr\\`a and Josep Lluis Arcos", "title": "An Empirical Evaluation of Similarity Measures for Time Series\n  Classification", "comments": "28 pages, 5 figures, 3 tables", "journal-ref": "Knowledge-Based Systems 67: 305-314, 2014", "doi": "10.1016/j.knosys.2014.04.035", "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Time series are ubiquitous, and a measure to assess their similarity is a\ncore part of many computational systems. In particular, the similarity measure\nis the most essential ingredient of time series clustering and classification\nsystems. Because of this importance, countless approaches to estimate time\nseries similarity have been proposed. However, there is a lack of comparative\nstudies using empirical, rigorous, quantitative, and large-scale assessment\nstrategies. In this article, we provide an extensive evaluation of similarity\nmeasures for time series classification following the aforementioned\nprinciples. We consider 7 different measures coming from alternative measure\n`families', and 45 publicly-available time series data sets coming from a wide\nvariety of scientific domains. We focus on out-of-sample classification\naccuracy, but in-sample accuracies and parameter choices are also discussed.\nOur work is based on rigorous evaluation methodologies and includes the use of\npowerful statistical significance tests to derive meaningful conclusions. The\nobtained results show the equivalence, in terms of accuracy, of a number of\nmeasures, but with one single candidate outperforming the rest. Such findings,\ntogether with the followed methodology, invite researchers on the field to\nadopt a more consistent evaluation criteria and a more informed decision\nregarding the baseline measures to which new developments should be compared.\n", "versions": [{"version": "v1", "created": "Thu, 16 Jan 2014 10:21:44 GMT"}], "update_date": "2016-05-18", "authors_parsed": [["Serr\u00e0", "Joan", ""], ["Arcos", "Josep Lluis", ""]]}, {"id": "1401.4082", "submitter": "Shakir Mohamed", "authors": "Danilo Jimenez Rezende, Shakir Mohamed, Daan Wierstra", "title": "Stochastic Backpropagation and Approximate Inference in Deep Generative\n  Models", "comments": "Appears In Proceedings of the 31st International Conference on\n  Machine Learning (ICML), JMLR: W\\&CP volume 32, 2014", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.LG stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We marry ideas from deep neural networks and approximate Bayesian inference\nto derive a generalised class of deep, directed generative models, endowed with\na new algorithm for scalable inference and learning. Our algorithm introduces a\nrecognition model to represent approximate posterior distributions, and that\nacts as a stochastic encoder of the data. We develop stochastic\nback-propagation -- rules for back-propagation through stochastic variables --\nand use this to develop an algorithm that allows for joint optimisation of the\nparameters of both the generative and recognition model. We demonstrate on\nseveral real-world data sets that the model generates realistic samples,\nprovides accurate imputations of missing data and is a useful tool for\nhigh-dimensional data visualisation.\n", "versions": [{"version": "v1", "created": "Thu, 16 Jan 2014 16:33:23 GMT"}, {"version": "v2", "created": "Fri, 9 May 2014 12:53:17 GMT"}, {"version": "v3", "created": "Fri, 30 May 2014 10:00:36 GMT"}], "update_date": "2014-06-02", "authors_parsed": [["Rezende", "Danilo Jimenez", ""], ["Mohamed", "Shakir", ""], ["Wierstra", "Daan", ""]]}, {"id": "1401.4128", "submitter": "Charles-Henri Cappelaere", "authors": "Charles-Henri Cappelaere, R. Dubois, P. Roussel, G. Dreyfus", "title": "Towards the selection of patients requiring ICD implantation by\n  automatic classification from Holter monitoring indices", "comments": "Computing in Cardiology, Saragosse : Espagne (2013)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The purpose of this study is to optimize the selection of prophylactic\ncardioverter defibrillator implantation candidates. Currently, the main\ncriterion for implantation is a low Left Ventricular Ejection Fraction (LVEF)\nwhose specificity is relatively poor. We designed two classifiers aimed to\npredict, from long term ECG recordings (Holter), whether a low-LVEF patient is\nlikely or not to undergo ventricular arrhythmia in the next six months. One\nclassifier is a single hidden layer neural network whose variables are the most\nrelevant features extracted from Holter recordings, and the other classifier\nhas a structure that capitalizes on the physiological decomposition of the\narrhythmogenic factors into three disjoint groups: the myocardial substrate,\nthe triggers and the autonomic nervous system (ANS). In this ad hoc network,\nthe features were assigned to each group; one neural network classifier per\ngroup was designed and its complexity was optimized. The outputs of the\nclassifiers were fed to a single neuron that provided the required probability\nestimate. The latter was thresholded for final discrimination A dataset\ncomposed of 186 pre-implantation 30-mn Holter recordings of patients equipped\nwith an implantable cardioverter defibrillator (ICD) in primary prevention was\nused in order to design and test this classifier. 44 out of 186 patients\nunderwent at least one treated ventricular arrhythmia during the six-month\nfollow-up period. Performances of the designed classifier were evaluated using\na cross-test strategy that consists in splitting the database into several\ncombinations of a training set and a test set. The average arrhythmia\nprediction performances of the ad-hoc classifier are NPV = 77% $\\pm$ 13% and\nPPV = 31% $\\pm$ 19% (Negative Predictive Value $\\pm$ std, Positive Predictive\nValue $\\pm$ std). According to our study, improving prophylactic\nICD-implantation candidate selection by automatic classification from ECG\nfeatures may be possible, but the availability of a sizable dataset appears to\nbe essential to decrease the number of False Negatives.\n", "versions": [{"version": "v1", "created": "Thu, 16 Jan 2014 18:54:43 GMT"}], "update_date": "2014-01-17", "authors_parsed": [["Cappelaere", "Charles-Henri", ""], ["Dubois", "R.", ""], ["Roussel", "P.", ""], ["Dreyfus", "G.", ""]]}, {"id": "1401.4143", "submitter": "Sunho Park", "authors": "Sunho Park, TaeHyun Hwang, Seungjin Choi", "title": "Convex Optimization for Binary Classifier Aggregation in Multiclass\n  Problems", "comments": "Appeared in Proceedings of the 2014 SIAM International Conference on\n  Data Mining (SDM 2014)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multiclass problems are often decomposed into multiple binary problems that\nare solved by individual binary classifiers whose results are integrated into a\nfinal answer. Various methods, including all-pairs (APs), one-versus-all (OVA),\nand error correcting output code (ECOC), have been studied, to decompose\nmulticlass problems into binary problems. However, little study has been made\nto optimally aggregate binary problems to determine a final answer to the\nmulticlass problem. In this paper we present a convex optimization method for\nan optimal aggregation of binary classifiers to estimate class membership\nprobabilities in multiclass problems. We model the class membership probability\nas a softmax function which takes a conic combination of discrepancies induced\nby individual binary classifiers, as an input. With this model, we formulate\nthe regularized maximum likelihood estimation as a convex optimization problem,\nwhich is solved by the primal-dual interior point method. Connections of our\nmethod to large margin classifiers are presented, showing that the large margin\nformulation can be considered as a limiting case of our convex formulation.\nNumerical experiments on synthetic and real-world data sets demonstrate that\nour method outperforms existing aggregation methods as well as direct methods,\nin terms of the classification accuracy and the quality of class membership\nprobability estimates.\n", "versions": [{"version": "v1", "created": "Thu, 16 Jan 2014 19:49:02 GMT"}], "update_date": "2014-01-17", "authors_parsed": [["Park", "Sunho", ""], ["Hwang", "TaeHyun", ""], ["Choi", "Seungjin", ""]]}, {"id": "1401.4436", "submitter": "Muhammad Arshad Ul Abedin", "authors": "Muhammad Arshad Ul Abedin, Vincent Ng, Latifur Khan", "title": "Cause Identification from Aviation Safety Incident Reports via Weakly\n  Supervised Semantic Lexicon Construction", "comments": null, "journal-ref": "Journal Of Artificial Intelligence Research, Volume 38, pages\n  569-631, 2010", "doi": "10.1613/jair.2986", "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Aviation Safety Reporting System collects voluntarily submitted reports\non aviation safety incidents to facilitate research work aiming to reduce such\nincidents. To effectively reduce these incidents, it is vital to accurately\nidentify why these incidents occurred. More precisely, given a set of possible\ncauses, or shaping factors, this task of cause identification involves\nidentifying all and only those shaping factors that are responsible for the\nincidents described in a report. We investigate two approaches to cause\nidentification. Both approaches exploit information provided by a semantic\nlexicon, which is automatically constructed via Thelen and Riloffs Basilisk\nframework augmented with our linguistic and algorithmic modifications. The\nfirst approach labels a report using a simple heuristic, which looks for the\nwords and phrases acquired during the semantic lexicon learning process in the\nreport. The second approach recasts cause identification as a text\nclassification problem, employing supervised and transductive text\nclassification algorithms to learn models from incident reports labeled with\nshaping factors and using the models to label unseen reports. Our experiments\nshow that both the heuristic-based approach and the learning-based approach\n(when given sufficient training data) outperform the baseline system\nsignificantly.\n", "versions": [{"version": "v1", "created": "Thu, 16 Jan 2014 04:53:44 GMT"}], "update_date": "2014-01-20", "authors_parsed": [["Abedin", "Muhammad Arshad Ul", ""], ["Ng", "Vincent", ""], ["Khan", "Latifur", ""]]}, {"id": "1401.4489", "submitter": "Devansh Arpit", "authors": "Devansh Arpit, Ifeoma Nwogu, Gaurav Srivastava, Venu Govindaraju", "title": "An Analysis of Random Projections in Cancelable Biometrics", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With increasing concerns about security, the need for highly secure physical\nbiometrics-based authentication systems utilizing \\emph{cancelable biometric}\ntechnologies is on the rise. Because the problem of cancelable template\ngeneration deals with the trade-off between template security and matching\nperformance, many state-of-the-art algorithms successful in generating high\nquality cancelable biometrics all have random projection as one of their early\nprocessing steps. This paper therefore presents a formal analysis of why random\nprojections is an essential step in cancelable biometrics. By formally defining\nthe notion of an \\textit{Independent Subspace Structure} for datasets, it can\nbe shown that random projection preserves the subspace structure of data\nvectors generated from a union of independent linear subspaces. The bound on\nthe minimum number of random vectors required for this to hold is also derived\nand is shown to depend logarithmically on the number of data samples, not only\nin independent subspaces but in disjoint subspace settings as well. The\ntheoretical analysis presented is supported in detail with empirical results on\nreal-world face recognition datasets.\n", "versions": [{"version": "v1", "created": "Fri, 17 Jan 2014 23:21:56 GMT"}, {"version": "v2", "created": "Wed, 5 Feb 2014 02:57:25 GMT"}, {"version": "v3", "created": "Fri, 14 Nov 2014 02:38:09 GMT"}], "update_date": "2015-05-22", "authors_parsed": [["Arpit", "Devansh", ""], ["Nwogu", "Ifeoma", ""], ["Srivastava", "Gaurav", ""], ["Govindaraju", "Venu", ""]]}, {"id": "1401.4529", "submitter": "Bal\\'azs Hidasi", "authors": "Bal\\'azs Hidasi, Domonkos Tikk", "title": "General factorization framework for context-aware recommendations", "comments": "The final publication is available at Springer via\n  http://dx.doi.org/10.1007/s10618-015-0417-y. Data Mining and Knowledge\n  Discovery, 2015", "journal-ref": null, "doi": "10.1007/s10618-015-0417-y", "report-no": null, "categories": "cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Context-aware recommendation algorithms focus on refining recommendations by\nconsidering additional information, available to the system. This topic has\ngained a lot of attention recently. Among others, several factorization methods\nwere proposed to solve the problem, although most of them assume explicit\nfeedback which strongly limits their real-world applicability. While these\nalgorithms apply various loss functions and optimization strategies, the\npreference modeling under context is less explored due to the lack of tools\nallowing for easy experimentation with various models. As context dimensions\nare introduced beyond users and items, the space of possible preference models\nand the importance of proper modeling largely increases.\n  In this paper we propose a General Factorization Framework (GFF), a single\nflexible algorithm that takes the preference model as an input and computes\nlatent feature matrices for the input dimensions. GFF allows us to easily\nexperiment with various linear models on any context-aware recommendation task,\nbe it explicit or implicit feedback based. The scaling properties makes it\nusable under real life circumstances as well.\n  We demonstrate the framework's potential by exploring various preference\nmodels on a 4-dimensional context-aware problem with contexts that are\navailable for almost any real life datasets. We show in our experiments --\nperformed on five real life, implicit feedback datasets -- that proper\npreference modelling significantly increases recommendation accuracy, and\npreviously unused models outperform the traditional ones. Novel models in GFF\nalso outperform state-of-the-art factorization algorithms.\n  We also extend the method to be fully compliant to the Multidimensional\nDataspace Model, one of the most extensive data models of context-enriched\ndata. Extended GFF allows the seamless incorporation of information into the\nfac[truncated]\n", "versions": [{"version": "v1", "created": "Sat, 18 Jan 2014 11:13:26 GMT"}, {"version": "v2", "created": "Tue, 19 May 2015 11:50:22 GMT"}], "update_date": "2015-05-20", "authors_parsed": [["Hidasi", "Bal\u00e1zs", ""], ["Tikk", "Domonkos", ""]]}, {"id": "1401.4566", "submitter": "Mehrdad Mahdavi", "authors": "Mehrdad Mahdavi and Rong Jin", "title": "Excess Risk Bounds for Exponentially Concave Losses", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The overarching goal of this paper is to derive excess risk bounds for\nlearning from exp-concave loss functions in passive and sequential learning\nsettings. Exp-concave loss functions encompass several fundamental problems in\nmachine learning such as squared loss in linear regression, logistic loss in\nclassification, and negative logarithm loss in portfolio management. In batch\nsetting, we obtain sharp bounds on the performance of empirical risk\nminimization performed in a linear hypothesis space and with respect to the\nexp-concave loss functions. We also extend the results to the online setting\nwhere the learner receives the training examples in a sequential manner. We\npropose an online learning algorithm that is a properly modified version of\nonline Newton method to obtain sharp risk bounds. Under an additional mild\nassumption on the loss function, we show that in both settings we are able to\nachieve an excess risk bound of $O(d\\log n/n)$ that holds with a high\nprobability.\n", "versions": [{"version": "v1", "created": "Sat, 18 Jan 2014 17:07:38 GMT"}, {"version": "v2", "created": "Sat, 8 Feb 2014 05:02:49 GMT"}], "update_date": "2014-02-11", "authors_parsed": [["Mahdavi", "Mehrdad", ""], ["Jin", "Rong", ""]]}, {"id": "1401.4589", "submitter": "Rania Ibrahim", "authors": "Rania Ibrahim, Noha A. Yousri, Mohamed A. Ismail, Nagwa M. El-Makky", "title": "miRNA and Gene Expression based Cancer Classification using Self-\n  Learning and Co-Training Approaches", "comments": "8 pages, 4 figures, 10 tables, conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CE cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  miRNA and gene expression profiles have been proved useful for classifying\ncancer samples. Efficient classifiers have been recently sought and developed.\nA number of attempts to classify cancer samples using miRNA/gene expression\nprofiles are known in literature. However, the use of semi-supervised learning\nmodels have been used recently in bioinformatics, to exploit the huge corpuses\nof publicly available sets. Using both labeled and unlabeled sets to train\nsample classifiers, have not been previously considered when gene and miRNA\nexpression sets are used. Moreover, there is a motivation to integrate both\nmiRNA and gene expression for a semi-supervised cancer classification as that\nprovides more information on the characteristics of cancer samples. In this\npaper, two semi-supervised machine learning approaches, namely self-learning\nand co-training, are adapted to enhance the quality of cancer sample\nclassification. These approaches exploit the huge public corpuses to enrich the\ntraining data. In self-learning, miRNA and gene based classifiers are enhanced\nindependently. While in co-training, both miRNA and gene expression profiles\nare used simultaneously to provide different views of cancer samples. To our\nknowledge, it is the first attempt to apply these learning approaches to cancer\nclassification. The approaches were evaluated using breast cancer,\nhepatocellular carcinoma (HCC) and lung cancer expression sets. Results show up\nto 20% improvement in F1-measure over Random Forests and SVM classifiers.\nCo-Training also outperforms Low Density Separation (LDS) approach by around\n25% improvement in F1-measure in breast cancer.\n", "versions": [{"version": "v1", "created": "Sat, 18 Jan 2014 21:02:32 GMT"}], "update_date": "2014-01-21", "authors_parsed": [["Ibrahim", "Rania", ""], ["Yousri", "Noha A.", ""], ["Ismail", "Mohamed A.", ""], ["El-Makky", "Nagwa M.", ""]]}, {"id": "1401.4590", "submitter": "Enrique Amig\\'o", "authors": "Enrique Amig\\'o, Julio Gonzalo, Javier Artiles, Felisa Verdejo", "title": "Combining Evaluation Metrics via the Unanimous Improvement Ratio and its\n  Application to Clustering Tasks", "comments": null, "journal-ref": "Journal Of Artificial Intelligence Research, Volume 42, pages\n  689-718, 2011", "doi": "10.1613/jair.3401", "report-no": null, "categories": "cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many Artificial Intelligence tasks cannot be evaluated with a single quality\ncriterion and some sort of weighted combination is needed to provide system\nrankings. A problem of weighted combination measures is that slight changes in\nthe relative weights may produce substantial changes in the system rankings.\nThis paper introduces the Unanimous Improvement Ratio (UIR), a measure that\ncomplements standard metric combination criteria (such as van Rijsbergen's\nF-measure) and indicates how robust the measured differences are to changes in\nthe relative weights of the individual metrics. UIR is meant to elucidate\nwhether a perceived difference between two systems is an artifact of how\nindividual metrics are weighted.\n  Besides discussing the theoretical foundations of UIR, this paper presents\nempirical results that confirm the validity and usefulness of the metric for\nthe Text Clustering problem, where there is a tradeoff between precision and\nrecall based metrics and results are particularly sensitive to the weighting\nscheme used to combine them. Remarkably, our experiments show that UIR can be\nused as a predictor of how well differences between systems measured on a given\ntest bed will also hold in a different test bed.\n", "versions": [{"version": "v1", "created": "Sat, 18 Jan 2014 21:03:23 GMT"}], "update_date": "2014-01-21", "authors_parsed": [["Amig\u00f3", "Enrique", ""], ["Gonzalo", "Julio", ""], ["Artiles", "Javier", ""], ["Verdejo", "Felisa", ""]]}, {"id": "1401.4872", "submitter": "Ayman Bahaa-Eldin", "authors": "Hany Nashat Gabra, Ayman Mohammad Bahaa-Eldin, Huda Korashy", "title": "Classification of IDS Alerts with Data Mining Techniques", "comments": "2012 International Conference on Internet Study (NETs2012), Bangkok,\n  Thailand", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.DB cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A data mining technique to reduce the amount of false alerts within an IDS\nsystem is proposed. The new technique achieves an accuracy of 99% compared to\n97% by the current systems.\n", "versions": [{"version": "v1", "created": "Mon, 20 Jan 2014 11:58:23 GMT"}], "update_date": "2014-01-21", "authors_parsed": [["Gabra", "Hany Nashat", ""], ["Bahaa-Eldin", "Ayman Mohammad", ""], ["Korashy", "Huda", ""]]}, {"id": "1401.5136", "submitter": "Cong Li", "authors": "Cong Li, Michael Georgiopoulos, Georgios C. Anagnostopoulos", "title": "A Unifying Framework for Typical Multi-Task Multiple Kernel Learning\n  Problems", "comments": "17 pages, 1 figure. Accepted by IEEE Transactions on Neural Networks\n  and Learning Systems; currently published as Early Access Article", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Over the past few years, Multi-Kernel Learning (MKL) has received significant\nattention among data-driven feature selection techniques in the context of\nkernel-based learning. MKL formulations have been devised and solved for a\nbroad spectrum of machine learning problems, including Multi-Task Learning\n(MTL). Solving different MKL formulations usually involves designing algorithms\nthat are tailored to the problem at hand, which is, typically, a non-trivial\naccomplishment.\n  In this paper we present a general Multi-Task Multi-Kernel Learning\n(Multi-Task MKL) framework that subsumes well-known Multi-Task MKL\nformulations, as well as several important MKL approaches on single-task\nproblems. We then derive a simple algorithm that can solve the unifying\nframework. To demonstrate the flexibility of the proposed framework, we\nformulate a new learning problem, namely Partially-Shared Common Space (PSCS)\nMulti-Task MKL, and demonstrate its merits through experimentation.\n", "versions": [{"version": "v1", "created": "Tue, 21 Jan 2014 01:16:44 GMT"}], "update_date": "2014-01-22", "authors_parsed": [["Li", "Cong", ""], ["Georgiopoulos", "Michael", ""], ["Anagnostopoulos", "Georgios C.", ""]]}, {"id": "1401.5226", "submitter": "Nicolas Gillis", "authors": "Nicolas Gillis", "title": "The Why and How of Nonnegative Matrix Factorization", "comments": "25 pages, 5 figures. Some typos and errors corrected, Section 3.2\n  reorganized", "journal-ref": null, "doi": null, "report-no": "In: \"Regularization, Optimization, Kernels, and Support Vector\n  Machines\", J.A.K. Suykens, M. Signoretto and A. Argyriou (eds), Chapman &\n  Hall/CRC, Machine Learning and Pattern Recognition Series, pp. 257-291, 2014", "categories": "stat.ML cs.IR cs.LG math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Nonnegative matrix factorization (NMF) has become a widely used tool for the\nanalysis of high-dimensional data as it automatically extracts sparse and\nmeaningful features from a set of nonnegative data vectors. We first illustrate\nthis property of NMF on three applications, in image processing, text mining\nand hyperspectral imaging --this is the why. Then we address the problem of\nsolving NMF, which is NP-hard in general. We review some standard NMF\nalgorithms, and also present a recent subclass of NMF problems, referred to as\nnear-separable NMF, that can be solved efficiently (that is, in polynomial\ntime), even in the presence of noise --this is the how. Finally, we briefly\ndescribe some problems in mathematics and computer science closely related to\nNMF via the nonnegative rank.\n", "versions": [{"version": "v1", "created": "Tue, 21 Jan 2014 09:03:12 GMT"}, {"version": "v2", "created": "Fri, 7 Mar 2014 10:32:43 GMT"}], "update_date": "2014-12-10", "authors_parsed": [["Gillis", "Nicolas", ""]]}, {"id": "1401.5364", "submitter": "Kiran Sree Pokkuluri Prof", "authors": "Pokkuluri Kiran Sree, Inampudi Ramesh Babu, SSSN Usha Devi N", "title": "HMACA: Towards Proposing a Cellular Automata Based Tool for Protein\n  Coding, Promoter Region Identification and Protein Structure Prediction", "comments": null, "journal-ref": "International Journal of Research in Computer Applications &\n  Information Technology, Volume 1, Issue 1, July-September, 2013, pp. 26-31", "doi": null, "report-no": null, "categories": "cs.CE cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Human body consists of lot of cells, each cell consist of DeOxaRibo Nucleic\nAcid (DNA). Identifying the genes from the DNA sequences is a very difficult\ntask. But identifying the coding regions is more complex task compared to the\nformer. Identifying the protein which occupy little place in genes is a really\nchallenging issue. For understating the genes coding region analysis plays an\nimportant role. Proteins are molecules with macro structure that are\nresponsible for a wide range of vital biochemical functions, which includes\nacting as oxygen, cell signaling, antibody production, nutrient transport and\nbuilding up muscle fibers. Promoter region identification and protein structure\nprediction has gained a remarkable attention in recent years. Even though there\nare some identification techniques addressing this problem, the approximate\naccuracy in identifying the promoter region is closely 68% to 72%. We have\ndeveloped a Cellular Automata based tool build with hybrid multiple attractor\ncellular automata (HMACA) classifier for protein coding region, promoter region\nidentification and protein structure prediction which predicts the protein and\npromoter regions with an accuracy of 76%. This tool also predicts the structure\nof protein with an accuracy of 80%.\n", "versions": [{"version": "v1", "created": "Tue, 21 Jan 2014 16:15:29 GMT"}], "update_date": "2014-01-22", "authors_parsed": [["Sree", "Pokkuluri Kiran", ""], ["Babu", "Inampudi Ramesh", ""], ["N", "SSSN Usha Devi", ""]]}, {"id": "1401.5389", "submitter": "Sajib Dasgupta", "authors": "Sajib Dasgupta, Vincent Ng", "title": "Which Clustering Do You Want? Inducing Your Ideal Clustering with\n  Minimal Feedback", "comments": null, "journal-ref": "Journal Of Artificial Intelligence Research, Volume 39, pages\n  581-632, 2010", "doi": "10.1613/jair.3003", "report-no": null, "categories": "cs.IR cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While traditional research on text clustering has largely focused on grouping\ndocuments by topic, it is conceivable that a user may want to cluster documents\nalong other dimensions, such as the authors mood, gender, age, or sentiment.\nWithout knowing the users intention, a clustering algorithm will only group\ndocuments along the most prominent dimension, which may not be the one the user\ndesires. To address the problem of clustering documents along the user-desired\ndimension, previous work has focused on learning a similarity metric from data\nmanually annotated with the users intention or having a human construct a\nfeature space in an interactive manner during the clustering process. With the\ngoal of reducing reliance on human knowledge for fine-tuning the similarity\nfunction or selecting the relevant features required by these approaches, we\npropose a novel active clustering algorithm, which allows a user to easily\nselect the dimension along which she wants to cluster the documents by\ninspecting only a small number of words. We demonstrate the viability of our\nalgorithm on a variety of commonly-used sentiment datasets.\n", "versions": [{"version": "v1", "created": "Thu, 16 Jan 2014 04:56:03 GMT"}], "update_date": "2014-01-22", "authors_parsed": [["Dasgupta", "Sajib", ""], ["Ng", "Vincent", ""]]}, {"id": "1401.5390", "submitter": "S.R.K. Branavan", "authors": "S.R.K. Branavan, David Silver, Regina Barzilay", "title": "Learning to Win by Reading Manuals in a Monte-Carlo Framework", "comments": null, "journal-ref": "Journal Of Artificial Intelligence Research, Volume 43, pages\n  661-704, 2012", "doi": "10.1613/jair.3484", "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Domain knowledge is crucial for effective performance in autonomous control\nsystems. Typically, human effort is required to encode this knowledge into a\ncontrol algorithm. In this paper, we present an approach to language grounding\nwhich automatically interprets text in the context of a complex control\napplication, such as a game, and uses domain knowledge extracted from the text\nto improve control performance. Both text analysis and control strategies are\nlearned jointly using only a feedback signal inherent to the application. To\neffectively leverage textual information, our method automatically extracts the\ntext segment most relevant to the current game state, and labels it with a\ntask-centric predicate structure. This labeled text is then used to bias an\naction selection policy for the game, guiding it towards promising regions of\nthe action space. We encode our model for text analysis and game playing in a\nmulti-layer neural network, representing linguistic decisions via latent\nvariables in the hidden layers, and game action quality via the output layer.\nOperating within the Monte-Carlo Search framework, we estimate model parameters\nusing feedback from simulated games. We apply our approach to the complex\nstrategy game Civilization II using the official game manual as the text guide.\nOur results show that a linguistically-informed game-playing agent\nsignificantly outperforms its language-unaware counterpart, yielding a 34%\nabsolute improvement and winning over 65% of games when playing against the\nbuilt-in AI of Civilization.\n", "versions": [{"version": "v1", "created": "Sat, 18 Jan 2014 21:10:57 GMT"}], "update_date": "2014-01-22", "authors_parsed": [["Branavan", "S. R. K.", ""], ["Silver", "David", ""], ["Barzilay", "Regina", ""]]}, {"id": "1401.5535", "submitter": "Shu Kong", "authors": "Shu Kong, Zhuolin Jiang, Qiang Yang", "title": "Learning Mid-Level Features and Modeling Neuron Selectivity for Image\n  Classification", "comments": "19 pages, 14 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.NE cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We now know that mid-level features can greatly enhance the performance of\nimage learning, but how to automatically learn the image features efficiently\nand in an unsupervised manner is still an open question. In this paper, we\npresent a very efficient mid-level feature learning approach (MidFea), which\nonly involves simple operations such as $k$-means clustering, convolution,\npooling, vector quantization and random projection. We explain why this simple\nmethod generates the desired features, and argue that there is no need to spend\nmuch time in learning low-level feature extractors. Furthermore, to boost the\nperformance, we propose to model the neuron selectivity (NS) principle by\nbuilding an additional layer over the mid-level features before feeding the\nfeatures into the classifier. We show that the NS-layer learns\ncategory-specific neurons with both bottom-up inference and top-down analysis,\nand thus supports fast inference for a query image. We run extensive\nexperiments on several public databases to demonstrate that our approach can\nachieve state-of-the-art performances for face recognition, gender\nclassification, age estimation and object categorization. In particular, we\ndemonstrate that our approach is more than an order of magnitude faster than\nsome recently proposed sparse coding based methods.\n", "versions": [{"version": "v1", "created": "Wed, 22 Jan 2014 01:35:59 GMT"}, {"version": "v2", "created": "Thu, 23 Jan 2014 09:19:10 GMT"}], "update_date": "2014-01-24", "authors_parsed": [["Kong", "Shu", ""], ["Jiang", "Zhuolin", ""], ["Yang", "Qiang", ""]]}, {"id": "1401.5636", "submitter": "Shohei Shimizu", "authors": "Takanori Inazumi, Takashi Washio, Shohei Shimizu, Joe Suzuki, Akihiro\n  Yamamoto, Yoshinobu Kawahara", "title": "Causal Discovery in a Binary Exclusive-or Skew Acyclic Model: BExSAM", "comments": "10 pages. A longer version of our UAI2011 paper (Inazumi et al.,\n  2011). arXiv admin note: text overlap with arXiv:1202.3736", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Discovering causal relations among observed variables in a given data set is\na major objective in studies of statistics and artificial intelligence.\nRecently, some techniques to discover a unique causal model have been explored\nbased on non-Gaussianity of the observed data distribution. However, most of\nthese are limited to continuous data. In this paper, we present a novel causal\nmodel for binary data and propose an efficient new approach to deriving the\nunique causal model governing a given binary data set under skew distributions\nof external binary noises. Experimental evaluation shows excellent performance\nfor both artificial and real world data sets.\n", "versions": [{"version": "v1", "created": "Wed, 22 Jan 2014 11:58:27 GMT"}], "update_date": "2014-01-23", "authors_parsed": [["Inazumi", "Takanori", ""], ["Washio", "Takashi", ""], ["Shimizu", "Shohei", ""], ["Suzuki", "Joe", ""], ["Yamamoto", "Akihiro", ""], ["Kawahara", "Yoshinobu", ""]]}, {"id": "1401.5888", "submitter": "Changxing Shang", "authors": "Changxing Shang and Shengzhong Feng and Zhongying Zhao and Jianping\n  Fan", "title": "Efficiently Detecting Overlapping Communities through Seeding and\n  Semi-Supervised Learning", "comments": null, "journal-ref": null, "doi": "10.1007/s13042-015-0338-5", "report-no": null, "categories": "cs.SI cs.LG physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Seeding then expanding is a commonly used scheme to discover overlapping\ncommunities in a network. Most seeding methods are either too complex to scale\nto large networks or too simple to select high-quality seeds, and the\nnon-principled functions used by most expanding methods lead to poor\nperformance when applied to diverse networks. This paper proposes a new method\nthat transforms a network into a corpus where each edge is treated as a\ndocument, and all nodes of the network are treated as terms of the corpus. An\neffective seeding method is also proposed that selects seeds as a training set,\nthen a principled expanding method based on semi-supervised learning is applied\nto classify edges. We compare our new algorithm with four other community\ndetection algorithms on a wide range of synthetic and empirical networks.\nExperimental results show that the new algorithm can significantly improve\nclustering performance in most cases. Furthermore, the time complexity of the\nnew algorithm is linear to the number of edges, and this low complexity makes\nthe new algorithm scalable to large networks.\n", "versions": [{"version": "v1", "created": "Thu, 23 Jan 2014 07:51:54 GMT"}, {"version": "v2", "created": "Mon, 9 Jun 2014 08:14:48 GMT"}, {"version": "v3", "created": "Tue, 17 Jun 2014 01:08:34 GMT"}, {"version": "v4", "created": "Wed, 17 Sep 2014 09:24:17 GMT"}], "update_date": "2015-02-27", "authors_parsed": [["Shang", "Changxing", ""], ["Feng", "Shengzhong", ""], ["Zhao", "Zhongying", ""], ["Fan", "Jianping", ""]]}, {"id": "1401.5899", "submitter": "Badong Chen", "authors": "Badong Chen, Junli Liang, Nanning Zheng, Jose C. Principe", "title": "Kernel Least Mean Square with Adaptive Kernel Size", "comments": "25 pages, 9 figures, and 4 tables", "journal-ref": "Neurocomputing,2016", "doi": "10.1016/j.neucom.2016.01.004", "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Kernel adaptive filters (KAF) are a class of powerful nonlinear filters\ndeveloped in Reproducing Kernel Hilbert Space (RKHS). The Gaussian kernel is\nusually the default kernel in KAF algorithms, but selecting the proper kernel\nsize (bandwidth) is still an open important issue especially for learning with\nsmall sample sizes. In previous research, the kernel size was set manually or\nestimated in advance by Silvermans rule based on the sample distribution. This\nstudy aims to develop an online technique for optimizing the kernel size of the\nkernel least mean square (KLMS) algorithm. A sequential optimization strategy\nis proposed, and a new algorithm is developed, in which the filter weights and\nthe kernel size are both sequentially updated by stochastic gradient algorithms\nthat minimize the mean square error (MSE). Theoretical results on convergence\nare also presented. The excellent performance of the new algorithm is confirmed\nby simulations on static function estimation and short term chaotic time series\nprediction.\n", "versions": [{"version": "v1", "created": "Thu, 23 Jan 2014 09:19:27 GMT"}, {"version": "v2", "created": "Fri, 24 Jan 2014 08:52:30 GMT"}, {"version": "v3", "created": "Tue, 11 Feb 2014 08:48:35 GMT"}], "update_date": "2016-05-10", "authors_parsed": [["Chen", "Badong", ""], ["Liang", "Junli", ""], ["Zheng", "Nanning", ""], ["Principe", "Jose C.", ""]]}, {"id": "1401.5900", "submitter": "Nan Wang", "authors": "Nan Wang and Jan Melchior and Laurenz Wiskott", "title": "Gaussian-binary Restricted Boltzmann Machines on Modeling Natural Image\n  Statistics", "comments": "Current version is only an early manuscript and is subject to further\n  change", "journal-ref": "PLoS ONE 12(2): e0171015 (2017)", "doi": "10.1371/journal.pone.0171015", "report-no": null, "categories": "cs.NE cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a theoretical analysis of Gaussian-binary restricted Boltzmann\nmachines (GRBMs) from the perspective of density models. The key aspect of this\nanalysis is to show that GRBMs can be formulated as a constrained mixture of\nGaussians, which gives a much better insight into the model's capabilities and\nlimitations. We show that GRBMs are capable of learning meaningful features\nboth in a two-dimensional blind source separation task and in modeling natural\nimages. Further, we show that reported difficulties in training GRBMs are due\nto the failure of the training algorithm rather than the model itself. Based on\nour analysis we are able to propose several training recipes, which allowed\nsuccessful and fast training in our experiments. Finally, we discuss the\nrelationship of GRBMs to several modifications that have been proposed to\nimprove the model.\n", "versions": [{"version": "v1", "created": "Thu, 23 Jan 2014 09:21:15 GMT"}], "update_date": "2017-02-06", "authors_parsed": [["Wang", "Nan", ""], ["Melchior", "Jan", ""], ["Wiskott", "Laurenz", ""]]}, {"id": "1401.6002", "submitter": "Cyril Voyant", "authors": "Cyril Voyant (SPE), Gilles Notton (SPE), Christophe Paoli (SPE), Marie\n  Laure Nivet (SPE), Marc Muselli (SPE), Kahina Dahmani (LRIA)", "title": "Numerical weather prediction or stochastic modeling: an objective\n  criterion of choice for the global radiation forecasting", "comments": "International Journal of Energy Technology and Policy (2014)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Numerous methods exist and were developed for global radiation forecasting.\nThe two most popular types are the numerical weather predictions (NWP) and the\npredictions using stochastic approaches. We propose to compute a parameter\nnoted constructed in part from the mutual information which is a quantity that\nmeasures the mutual dependence of two variables. Both of these are calculated\nwith the objective to establish the more relevant method between NWP and\nstochastic models concerning the current problem.\n", "versions": [{"version": "v1", "created": "Wed, 22 Jan 2014 19:33:20 GMT"}], "update_date": "2014-01-24", "authors_parsed": [["Voyant", "Cyril", "", "SPE"], ["Notton", "Gilles", "", "SPE"], ["Paoli", "Christophe", "", "SPE"], ["Nivet", "Marie Laure", "", "SPE"], ["Muselli", "Marc", "", "SPE"], ["Dahmani", "Kahina", "", "LRIA"]]}, {"id": "1401.6024", "submitter": "Martin Slawski", "authors": "Martin Slawski, Matthias Hein, Pavlo Lutsik", "title": "Matrix factorization with Binary Components", "comments": "appeared in NIPS 2013", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motivated by an application in computational biology, we consider low-rank\nmatrix factorization with $\\{0,1\\}$-constraints on one of the factors and\noptionally convex constraints on the second one. In addition to the\nnon-convexity shared with other matrix factorization schemes, our problem is\nfurther complicated by a combinatorial constraint set of size $2^{m \\cdot r}$,\nwhere $m$ is the dimension of the data points and $r$ the rank of the\nfactorization. Despite apparent intractability, we provide - in the line of\nrecent work on non-negative matrix factorization by Arora et al. (2012) - an\nalgorithm that provably recovers the underlying factorization in the exact case\nwith $O(m r 2^r + mnr + r^2 n)$ operations for $n$ datapoints. To obtain this\nresult, we use theory around the Littlewood-Offord lemma from combinatorics.\n", "versions": [{"version": "v1", "created": "Thu, 23 Jan 2014 16:02:19 GMT"}], "update_date": "2014-01-24", "authors_parsed": [["Slawski", "Martin", ""], ["Hein", "Matthias", ""], ["Lutsik", "Pavlo", ""]]}, {"id": "1401.6118", "submitter": "Smita Nirkhi", "authors": "Smita Nirkhi, R.V. Dharaskar", "title": "Comparative study of Authorship Identification Techniques for Cyber\n  Forensics Analysis", "comments": null, "journal-ref": "published 2013", "doi": null, "report-no": null, "categories": "cs.CY cs.CR cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Authorship Identification techniques are used to identify the most\nappropriate author from group of potential suspects of online messages and find\nevidences to support the conclusion. Cybercriminals make misuse of online\ncommunication for sending blackmail or a spam email and then attempt to hide\ntheir true identities to void detection.Authorship Identification of online\nmessages is the contemporary research issue for identity tracing in cyber\nforensics. This is highly interdisciplinary area as it takes advantage of\nmachine learning, information retrieval, and natural language processing. In\nthis paper, a study of recent techniques and automated approaches to\nattributing authorship of online messages is presented. The focus of this\nreview study is to summarize all existing authorship identification techniques\nused in literature to identify authors of online messages. Also it discusses\nevaluation criteria and parameters for authorship attribution studies and list\nopen questions that will attract future work in this area.\n", "versions": [{"version": "v1", "created": "Tue, 24 Dec 2013 10:40:00 GMT"}], "update_date": "2014-02-20", "authors_parsed": [["Nirkhi", "Smita", ""], ["Dharaskar", "R. V.", ""]]}, {"id": "1401.6124", "submitter": "Fabricio de Franca Olivetti", "authors": "Fabricio Olivetti de Franca", "title": "Iterative Universal Hash Function Generator for Minhashing", "comments": "6 pages, 4 tables, 1 algorithm", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Minhashing is a technique used to estimate the Jaccard Index between two sets\nby exploiting the probability of collision in a random permutation. In order to\nspeed up the computation, a random permutation can be approximated by using an\nuniversal hash function such as the $h_{a,b}$ function proposed by Carter and\nWegman. A better estimate of the Jaccard Index can be achieved by using many of\nthese hash functions, created at random. In this paper a new iterative\nprocedure to generate a set of $h_{a,b}$ functions is devised that eliminates\nthe need for a list of random values and avoid the multiplication operation\nduring the calculation. The properties of the generated hash functions remains\nthat of an universal hash function family. This is possible due to the random\nnature of features occurrence on sparse datasets. Results show that the\nuniformity of hashing the features is maintaned while obtaining a speed up of\nup to $1.38$ compared to the traditional approach.\n", "versions": [{"version": "v1", "created": "Thu, 23 Jan 2014 19:03:38 GMT"}], "update_date": "2014-01-25", "authors_parsed": [["de Franca", "Fabricio Olivetti", ""]]}, {"id": "1401.6131", "submitter": "Jo\\~ao V. Gra\\c{c}a", "authors": "Jo\\~ao V. Gra\\c{c}a, Kuzman Ganchev, Luisa Coheur, Fernando Pereira,\n  Ben Taskar", "title": "Controlling Complexity in Part-of-Speech Induction", "comments": null, "journal-ref": "Journal Of Artificial Intelligence Research, Volume 41, pages\n  527-551, 2011", "doi": "10.1613/jair.3348", "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of fully unsupervised learning of grammatical\n(part-of-speech) categories from unlabeled text. The standard\nmaximum-likelihood hidden Markov model for this task performs poorly, because\nof its weak inductive bias and large model capacity. We address this problem by\nrefining the model and modifying the learning objective to control its capacity\nvia para- metric and non-parametric constraints. Our approach enforces\nword-category association sparsity, adds morphological and orthographic\nfeatures, and eliminates hard-to-estimate parameters for rare words. We develop\nan efficient learning algorithm that is not much more computationally intensive\nthan standard training. We also provide an open-source implementation of the\nalgorithm. Our experiments on five diverse languages (Bulgarian, Danish,\nEnglish, Portuguese, Spanish) achieve significant improvements compared with\nprevious methods for the same task.\n", "versions": [{"version": "v1", "created": "Thu, 16 Jan 2014 05:20:08 GMT"}], "update_date": "2014-01-24", "authors_parsed": [["Gra\u00e7a", "Jo\u00e3o V.", ""], ["Ganchev", "Kuzman", ""], ["Coheur", "Luisa", ""], ["Pereira", "Fernando", ""], ["Taskar", "Ben", ""]]}, {"id": "1401.6169", "submitter": "Hossein Soleimani", "authors": "Hossein Soleimani, David J. Miller", "title": "Parsimonious Topic Models with Salient Word Discovery", "comments": null, "journal-ref": "IEEE Transaction on Knowledge and Data Engineering, 27 (2015)\n  824-837", "doi": "10.1109/TKDE.2014.2345378", "report-no": null, "categories": "cs.LG cs.CL cs.IR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a parsimonious topic model for text corpora. In related models\nsuch as Latent Dirichlet Allocation (LDA), all words are modeled\ntopic-specifically, even though many words occur with similar frequencies\nacross different topics. Our modeling determines salient words for each topic,\nwhich have topic-specific probabilities, with the rest explained by a universal\nshared model. Further, in LDA all topics are in principle present in every\ndocument. By contrast our model gives sparse topic representation, determining\nthe (small) subset of relevant topics for each document. We derive a Bayesian\nInformation Criterion (BIC), balancing model complexity and goodness of fit.\nHere, interestingly, we identify an effective sample size and corresponding\npenalty specific to each parameter type in our model. We minimize BIC to\njointly determine our entire model -- the topic-specific words,\ndocument-specific topics, all model parameter values, {\\it and} the total\nnumber of topics -- in a wholly unsupervised fashion. Results on three text\ncorpora and an image dataset show that our model achieves higher test set\nlikelihood and better agreement with ground-truth class labels, compared to LDA\nand to a model designed to incorporate sparsity.\n", "versions": [{"version": "v1", "created": "Wed, 22 Jan 2014 21:47:48 GMT"}, {"version": "v2", "created": "Thu, 11 Sep 2014 20:24:41 GMT"}], "update_date": "2016-05-16", "authors_parsed": [["Soleimani", "Hossein", ""], ["Miller", "David J.", ""]]}, {"id": "1401.6240", "submitter": "Shaobo Lin", "authors": "Shaobo Lin, Xia Liu, Jian Fang and Zongben Xu", "title": "Is Extreme Learning Machine Feasible? A Theoretical Assessment (Part II)", "comments": "13 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  An extreme learning machine (ELM) can be regarded as a two stage feed-forward\nneural network (FNN) learning system which randomly assigns the connections\nwith and within hidden neurons in the first stage and tunes the connections\nwith output neurons in the second stage. Therefore, ELM training is essentially\na linear learning problem, which significantly reduces the computational\nburden. Numerous applications show that such a computation burden reduction\ndoes not degrade the generalization capability. It has, however, been open that\nwhether this is true in theory. The aim of our work is to study the theoretical\nfeasibility of ELM by analyzing the pros and cons of ELM. In the previous part\non this topic, we pointed out that via appropriate selection of the activation\nfunction, ELM does not degrade the generalization capability in the expectation\nsense. In this paper, we launch the study in a different direction and show\nthat the randomness of ELM also leads to certain negative consequences. On one\nhand, we find that the randomness causes an additional uncertainty problem of\nELM, both in approximation and learning. On the other hand, we theoretically\njustify that there also exists an activation function such that the\ncorresponding ELM degrades the generalization capability. In particular, we\nprove that the generalization capability of ELM with Gaussian kernel is\nessentially worse than that of FNN with Gaussian kernel. To facilitate the use\nof ELM, we also provide a remedy to such a degradation. We find that the\nwell-developed coefficient regularization technique can essentially improve the\ngeneralization capability. The obtained results reveal the essential\ncharacteristic of ELM and give theoretical guidance concerning how to use ELM.\n", "versions": [{"version": "v1", "created": "Fri, 24 Jan 2014 01:57:42 GMT"}], "update_date": "2014-01-27", "authors_parsed": [["Lin", "Shaobo", ""], ["Liu", "Xia", ""], ["Fang", "Jian", ""], ["Xu", "Zongben", ""]]}, {"id": "1401.6333", "submitter": "Yang Yu", "authors": "Yang Yu and Hong Qian", "title": "The Sampling-and-Learning Framework: A Statistical View of Evolutionary\n  Algorithms", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Evolutionary algorithms (EAs), a large class of general purpose optimization\nalgorithms inspired from the natural phenomena, are widely used in various\nindustrial optimizations and often show excellent performance. This paper\npresents an attempt towards revealing their general power from a statistical\nview of EAs. By summarizing a large range of EAs into the sampling-and-learning\nframework, we show that the framework directly admits a general analysis on the\nprobable-absolute-approximate (PAA) query complexity. We particularly focus on\nthe framework with the learning subroutine being restricted as a binary\nclassification, which results in the sampling-and-classification (SAC)\nalgorithms. With the help of the learning theory, we obtain a general upper\nbound on the PAA query complexity of SAC algorithms. We further compare SAC\nalgorithms with the uniform search in different situations. Under the\nerror-target independence condition, we show that SAC algorithms can achieve\npolynomial speedup to the uniform search, but not super-polynomial speedup.\nUnder the one-side-error condition, we show that super-polynomial speedup can\nbe achieved. This work only touches the surface of the framework. Its power\nunder other conditions is still open.\n", "versions": [{"version": "v1", "created": "Fri, 24 Jan 2014 13:10:11 GMT"}, {"version": "v2", "created": "Fri, 11 Apr 2014 14:29:27 GMT"}], "update_date": "2014-04-14", "authors_parsed": [["Yu", "Yang", ""], ["Qian", "Hong", ""]]}, {"id": "1401.6376", "submitter": "Jie Chen", "authors": "Jie Chen and Jos\\'e Carlos M. Bermudez and C\\'edric Richard", "title": "Steady-state performance of non-negative least-mean-square algorithm and\n  its variants", "comments": "10 pages, 4 figures", "journal-ref": null, "doi": "10.1109/LSP.2014.2320944", "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Non-negative least-mean-square (NNLMS) algorithm and its variants have been\nproposed for online estimation under non-negativity constraints. The transient\nbehavior of the NNLMS, Normalized NNLMS, Exponential NNLMS and Sign-Sign NNLMS\nalgorithms have been studied in our previous work. In this technical report, we\nderive closed-form expressions for the steady-state excess mean-square error\n(EMSE) for the four algorithms. Simulations results illustrate the accuracy of\nthe theoretical results. This is a complementary material to our previous work.\n", "versions": [{"version": "v1", "created": "Fri, 24 Jan 2014 15:36:09 GMT"}], "update_date": "2015-06-18", "authors_parsed": [["Chen", "Jie", ""], ["Bermudez", "Jos\u00e9 Carlos M.", ""], ["Richard", "C\u00e9dric", ""]]}, {"id": "1401.6413", "submitter": "Nuri Denizcan Vanli", "authors": "N. Denizcan Vanli, Muhammed O. Sayin, Suleyman S. Kozat", "title": "Predicting Nearly As Well As the Optimal Twice Differentiable Regressor", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study nonlinear regression of real valued data in an individual sequence\nmanner, where we provide results that are guaranteed to hold without any\nstatistical assumptions. We address the convergence and undertraining issues of\nconventional nonlinear regression methods and introduce an algorithm that\nelegantly mitigates these issues via an incremental hierarchical structure,\n(i.e., via an incremental decision tree). Particularly, we present a piecewise\nlinear (or nonlinear) regression algorithm that partitions the regressor space\nin a data driven manner and learns a linear model at each region. Unlike the\nconventional approaches, our algorithm gradually increases the number of\ndisjoint partitions on the regressor space in a sequential manner according to\nthe observed data. Through this data driven approach, our algorithm\nsequentially and asymptotically achieves the performance of the optimal twice\ndifferentiable regression function for any data sequence with an unknown and\narbitrary length. The computational complexity of the introduced algorithm is\nonly logarithmic in the data length under certain regularity conditions. We\nprovide the explicit description of the algorithm and demonstrate the\nsignificant gains for the well-known benchmark real data sets and chaotic\nsignals.\n", "versions": [{"version": "v1", "created": "Thu, 23 Jan 2014 15:51:36 GMT"}, {"version": "v2", "created": "Mon, 6 Oct 2014 23:32:10 GMT"}], "update_date": "2014-10-08", "authors_parsed": [["Vanli", "N. Denizcan", ""], ["Sayin", "Muhammed O.", ""], ["Kozat", "Suleyman S.", ""]]}, {"id": "1401.6421", "submitter": "Jonathan Huang", "authors": "Jonathan Huang, Ashish Kapoor, Carlos Guestrin", "title": "Riffled Independence for Efficient Inference with Partial Rankings", "comments": "arXiv admin note: text overlap with arXiv:1202.3734", "journal-ref": "Journal Of Artificial Intelligence Research, Volume 44, pages\n  491-532, 2012", "doi": "10.1613/jair.3543", "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Distributions over rankings are used to model data in a multitude of real\nworld settings such as preference analysis and political elections. Modeling\nsuch distributions presents several computational challenges, however, due to\nthe factorial size of the set of rankings over an item set. Some of these\nchallenges are quite familiar to the artificial intelligence community, such as\nhow to compactly represent a distribution over a combinatorially large space,\nand how to efficiently perform probabilistic inference with these\nrepresentations. With respect to ranking, however, there is the additional\nchallenge of what we refer to as human task complexity users are rarely willing\nto provide a full ranking over a long list of candidates, instead often\npreferring to provide partial ranking information. Simultaneously addressing\nall of these challenges i.e., designing a compactly representable model which\nis amenable to efficient inference and can be learned using partial ranking\ndata is a difficult task, but is necessary if we would like to scale to\nproblems with nontrivial size. In this paper, we show that the recently\nproposed riffled independence assumptions cleanly and efficiently address each\nof the above challenges. In particular, we establish a tight mathematical\nconnection between the concepts of riffled independence and of partial\nrankings. This correspondence not only allows us to then develop efficient and\nexact algorithms for performing inference tasks using riffled independence\nbased represen- tations with partial rankings, but somewhat surprisingly, also\nshows that efficient inference is not possible for riffle independent models\n(in a certain sense) with observations which do not take the form of partial\nrankings. Finally, using our inference algorithm, we introduce the first method\nfor learning riffled independence based models from partially ranked data.\n", "versions": [{"version": "v1", "created": "Thu, 23 Jan 2014 02:42:39 GMT"}], "update_date": "2014-01-27", "authors_parsed": [["Huang", "Jonathan", ""], ["Kapoor", "Ashish", ""], ["Guestrin", "Carlos", ""]]}, {"id": "1401.6424", "submitter": "Nico Goernitz", "authors": "Nico Goernitz, Marius Micha Kloft, Konrad Rieck, Ulf Brefeld", "title": "Toward Supervised Anomaly Detection", "comments": null, "journal-ref": "Journal Of Artificial Intelligence Research, Volume 46, pages\n  235-262, 2013", "doi": "10.1613/jair.3623", "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Anomaly detection is being regarded as an unsupervised learning task as\nanomalies stem from adversarial or unlikely events with unknown distributions.\nHowever, the predictive performance of purely unsupervised anomaly detection\noften fails to match the required detection rates in many tasks and there\nexists a need for labeled data to guide the model generation. Our first\ncontribution shows that classical semi-supervised approaches, originating from\na supervised classifier, are inappropriate and hardly detect new and unknown\nanomalies. We argue that semi-supervised anomaly detection needs to ground on\nthe unsupervised learning paradigm and devise a novel algorithm that meets this\nrequirement. Although being intrinsically non-convex, we further show that the\noptimization problem has a convex equivalent under relatively mild assumptions.\nAdditionally, we propose an active learning strategy to automatically filter\ncandidates for labeling. In an empirical study on network intrusion detection\ndata, we observe that the proposed learning methodology requires much less\nlabeled data than the state-of-the-art, while achieving higher detection\naccuracies.\n", "versions": [{"version": "v1", "created": "Thu, 23 Jan 2014 02:46:53 GMT"}], "update_date": "2014-01-27", "authors_parsed": [["Goernitz", "Nico", ""], ["Kloft", "Marius Micha", ""], ["Rieck", "Konrad", ""], ["Brefeld", "Ulf", ""]]}, {"id": "1401.6427", "submitter": "Seyed Abolghasem Mirroshandel", "authors": "Seyed Abolghasem Mirroshandel, Gholamreza Ghassem-Sani", "title": "Towards Unsupervised Learning of Temporal Relations between Events", "comments": null, "journal-ref": "Journal Of Artificial Intelligence Research, Volume 45, pages\n  125-163, 2012", "doi": "10.1613/jair.3693", "report-no": null, "categories": "cs.LG cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automatic extraction of temporal relations between event pairs is an\nimportant task for several natural language processing applications such as\nQuestion Answering, Information Extraction, and Summarization. Since most\nexisting methods are supervised and require large corpora, which for many\nlanguages do not exist, we have concentrated our efforts to reduce the need for\nannotated data as much as possible. This paper presents two different\nalgorithms towards this goal. The first algorithm is a weakly supervised\nmachine learning approach for classification of temporal relations between\nevents. In the first stage, the algorithm learns a general classifier from an\nannotated corpus. Then, inspired by the hypothesis of \"one type of temporal\nrelation per discourse, it extracts useful information from a cluster of\ntopically related documents. We show that by combining the global information\nof such a cluster with local decisions of a general classifier, a bootstrapping\ncross-document classifier can be built to extract temporal relations between\nevents. Our experiments show that without any additional annotated data, the\naccuracy of the proposed algorithm is higher than that of several previous\nsuccessful systems. The second proposed method for temporal relation extraction\nis based on the expectation maximization (EM) algorithm. Within EM, we used\ndifferent techniques such as a greedy best-first search and integer linear\nprogramming for temporal inconsistency removal. We think that the experimental\nresults of our EM based algorithm, as a first step toward a fully unsupervised\ntemporal relation extraction method, is encouraging.\n", "versions": [{"version": "v1", "created": "Thu, 23 Jan 2014 02:50:50 GMT"}], "update_date": "2014-01-27", "authors_parsed": [["Mirroshandel", "Seyed Abolghasem", ""], ["Ghassem-Sani", "Gholamreza", ""]]}, {"id": "1401.6484", "submitter": "Kiran Sree Pokkuluri Prof", "authors": "Pokkuluri Kiran Sree, Inampudi Ramesh Babu", "title": "Identification of Protein Coding Regions in Genomic DNA Using\n  Unsupervised FMACA Based Pattern Classifier", "comments": "arXiv admin note: text overlap with arXiv:1312.2642", "journal-ref": "IJCSNS International Journal of Computer Science and Network\n  Security, VOL.8 No.1, January 2008,305-310", "doi": null, "report-no": null, "categories": "cs.CE cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Genes carry the instructions for making proteins that are found in a cell as\na specific sequence of nucleotides that are found in DNA molecules. But, the\nregions of these genes that code for proteins may occupy only a small region of\nthe sequence. Identifying the coding regions play a vital role in understanding\nthese genes. In this paper we propose a unsupervised Fuzzy Multiple Attractor\nCellular Automata (FMCA) based pattern classifier to identify the coding region\nof a DNA sequence. We propose a distinct K-Means algorithm for designing FMACA\nclassifier which is simple, efficient and produces more accurate classifier\nthan that has previously been obtained for a range of different sequence\nlengths. Experimental results confirm the scalability of the proposed\nUnsupervised FCA based classifier to handle large volume of datasets\nirrespective of the number of classes, tuples and attributes. Good\nclassification accuracy has been established.\n", "versions": [{"version": "v1", "created": "Sat, 25 Jan 2014 01:48:14 GMT"}], "update_date": "2014-01-28", "authors_parsed": [["Sree", "Pokkuluri Kiran", ""], ["Babu", "Inampudi Ramesh", ""]]}, {"id": "1401.6497", "submitter": "Qibin Zhao Dr", "authors": "Qibin Zhao, Liqing Zhang, and Andrzej Cichocki", "title": "Bayesian CP Factorization of Incomplete Tensors with Automatic Rank\n  Determination", "comments": null, "journal-ref": null, "doi": "10.1109/TPAMI.2015.2392756", "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  CANDECOMP/PARAFAC (CP) tensor factorization of incomplete data is a powerful\ntechnique for tensor completion through explicitly capturing the multilinear\nlatent factors. The existing CP algorithms require the tensor rank to be\nmanually specified, however, the determination of tensor rank remains a\nchallenging problem especially for CP rank. In addition, existing approaches do\nnot take into account uncertainty information of latent factors, as well as\nmissing entries. To address these issues, we formulate CP factorization using a\nhierarchical probabilistic model and employ a fully Bayesian treatment by\nincorporating a sparsity-inducing prior over multiple latent factors and the\nappropriate hyperpriors over all hyperparameters, resulting in automatic rank\ndetermination. To learn the model, we develop an efficient deterministic\nBayesian inference algorithm, which scales linearly with data size. Our method\nis characterized as a tuning parameter-free approach, which can effectively\ninfer underlying multilinear factors with a low-rank constraint, while also\nproviding predictive distributions over missing entries. Extensive simulations\non synthetic data illustrate the intrinsic capability of our method to recover\nthe ground-truth of CP rank and prevent the overfitting problem, even when a\nlarge amount of entries are missing. Moreover, the results from real-world\napplications, including image inpainting and facial image synthesis,\ndemonstrate that our method outperforms state-of-the-art approaches for both\ntensor factorization and tensor completion in terms of predictive performance.\n", "versions": [{"version": "v1", "created": "Sat, 25 Jan 2014 05:08:33 GMT"}, {"version": "v2", "created": "Thu, 9 Oct 2014 09:48:37 GMT"}], "update_date": "2015-01-22", "authors_parsed": [["Zhao", "Qibin", ""], ["Zhang", "Liqing", ""], ["Cichocki", "Andrzej", ""]]}, {"id": "1401.6567", "submitter": "Kamal Sarkar", "authors": "Vivekananda Gayen, Kamal Sarkar", "title": "A Machine Learning Approach for the Identification of Bengali Noun-Noun\n  Compound Multiword Expressions", "comments": null, "journal-ref": "In Proceedings of ICON-2013: 10th International Conference on\n  Natural Language Processing, pp 290-296", "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a machine learning approach for identification of Bengali\nmultiword expressions (MWE) which are bigram nominal compounds. Our proposed\napproach has two steps: (1) candidate extraction using chunk information and\nvarious heuristic rules and (2) training the machine learning algorithm called\nRandom Forest to classify the candidates into two groups: bigram nominal\ncompound MWE or not bigram nominal compound MWE. A variety of association\nmeasures, syntactic and linguistic clues and a set of WordNet-based similarity\nfeatures have been used for our MWE identification task. The approach presented\nin this paper can be used to identify bigram nominal compound MWE in Bengali\nrunning text.\n", "versions": [{"version": "v1", "created": "Sat, 25 Jan 2014 18:45:29 GMT"}], "update_date": "2014-01-28", "authors_parsed": [["Gayen", "Vivekananda", ""], ["Sarkar", "Kamal", ""]]}, {"id": "1401.6597", "submitter": "Sadi Seker E", "authors": "Sadi Evren Seker, Y. Unal, Z. Erdem, and H. Erdinc Kocer", "title": "Ensembled Correlation Between Liver Analysis Outputs", "comments": null, "journal-ref": "International Journal of Biology and Biomedical Engineering, ISSN:\n  1998-4510, Volume 8, pp. 1-5, 2014", "doi": null, "report-no": null, "categories": "stat.ML cs.CE cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data mining techniques on the biological analysis are spreading for most of\nthe areas including the health care and medical information. We have applied\nthe data mining techniques, such as KNN, SVM, MLP or decision trees over a\nunique dataset, which is collected from 16,380 analysis results for a year.\nFurthermore we have also used meta-classifiers to question the increased\ncorrelation rate between the liver disorder and the liver analysis outputs. The\nresults show that there is a correlation among ALT, AST, Billirubin Direct and\nBillirubin Total down to 15% of error rate. Also the correlation coefficient is\nup to 94%. This makes possible to predict the analysis results from each other\nor disease patterns can be applied over the linear correlation of the\nparameters.\n", "versions": [{"version": "v1", "created": "Sat, 25 Jan 2014 23:52:37 GMT"}], "update_date": "2014-01-28", "authors_parsed": [["Seker", "Sadi Evren", ""], ["Unal", "Y.", ""], ["Erdem", "Z.", ""], ["Kocer", "H. Erdinc", ""]]}, {"id": "1401.6638", "submitter": "Tong Wu", "authors": "Tong Wu, Gungor Polatkan, David Steel, William Brown, Ingrid\n  Daubechies and Robert Calderbank", "title": "Painting Analysis Using Wavelets and Probabilistic Topic Models", "comments": "5 pages, 4 figures, ICIP 2013", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, computer-based techniques for stylistic analysis of paintings\nare applied to the five panels of the 14th century Peruzzi Altarpiece by Giotto\ndi Bondone. Features are extracted by combining a dual-tree complex wavelet\ntransform with a hidden Markov tree (HMT) model. Hierarchical clustering is\nused to identify stylistic keywords in image patches, and keyword frequencies\nare calculated for sub-images that each contains many patches. A generative\nhierarchical Bayesian model learns stylistic patterns of keywords; these\npatterns are then used to characterize the styles of the sub-images; this in\nturn, permits to discriminate between paintings. Results suggest that such\nunsupervised probabilistic topic models can be useful to distill characteristic\nelements of style.\n", "versions": [{"version": "v1", "created": "Sun, 26 Jan 2014 11:00:46 GMT"}], "update_date": "2014-01-28", "authors_parsed": [["Wu", "Tong", ""], ["Polatkan", "Gungor", ""], ["Steel", "David", ""], ["Brown", "William", ""], ["Daubechies", "Ingrid", ""], ["Calderbank", "Robert", ""]]}, {"id": "1401.6956", "submitter": "Joon Kwon", "authors": "Joon Kwon and Panayotis Mertikopoulos", "title": "A continuous-time approach to online optimization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider a family of learning strategies for online optimization problems\nthat evolve in continuous time and we show that they lead to no regret. From a\nmore traditional, discrete-time viewpoint, this continuous-time approach allows\nus to derive the no-regret properties of a large class of discrete-time\nalgorithms including as special cases the exponential weight algorithm, online\nmirror descent, smooth fictitious play and vanishingly smooth fictitious play.\nIn so doing, we obtain a unified view of many classical regret bounds, and we\nshow that they can be decomposed into a term stemming from continuous-time\nconsiderations and a term which measures the disparity between discrete and\ncontinuous time. As a result, we obtain a general class of infinite horizon\nlearning strategies that guarantee an $\\mathcal{O}(n^{-1/2})$ regret bound\nwithout having to resort to a doubling trick.\n", "versions": [{"version": "v1", "created": "Mon, 27 Jan 2014 18:45:54 GMT"}, {"version": "v2", "created": "Thu, 27 Feb 2014 14:36:01 GMT"}], "update_date": "2014-02-28", "authors_parsed": [["Kwon", "Joon", ""], ["Mertikopoulos", "Panayotis", ""]]}, {"id": "1401.6984", "submitter": "Yajie Miao", "authors": "Yajie Miao", "title": "Kaldi+PDNN: Building DNN-based ASR Systems with Kaldi and PDNN", "comments": "unpublished manuscript", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  The Kaldi toolkit is becoming popular for constructing automated speech\nrecognition (ASR) systems. Meanwhile, in recent years, deep neural networks\n(DNNs) have shown state-of-the-art performance on various ASR tasks. This\ndocument describes our open-source recipes to implement fully-fledged DNN\nacoustic modeling using Kaldi and PDNN. PDNN is a lightweight deep learning\ntoolkit developed under the Theano environment. Using these recipes, we can\nbuild up multiple systems including DNN hybrid systems, convolutional neural\nnetwork (CNN) systems and bottleneck feature systems. These recipes are\ndirectly based on the Kaldi Switchboard 110-hour setup. However, adapting them\nto new datasets is easy to achieve.\n", "versions": [{"version": "v1", "created": "Mon, 27 Jan 2014 19:55:34 GMT"}], "update_date": "2014-01-28", "authors_parsed": [["Miao", "Yajie", ""]]}, {"id": "1401.7020", "submitter": "Samantha Hansen", "authors": "R.H. Byrd, S.L. Hansen, J. Nocedal, Y.Singer", "title": "A Stochastic Quasi-Newton Method for Large-Scale Optimization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The question of how to incorporate curvature information in stochastic\napproximation methods is challenging. The direct application of classical\nquasi- Newton updating techniques for deterministic optimization leads to noisy\ncurvature estimates that have harmful effects on the robustness of the\niteration. In this paper, we propose a stochastic quasi-Newton method that is\nefficient, robust and scalable. It employs the classical BFGS update formula in\nits limited memory form, and is based on the observation that it is beneficial\nto collect curvature information pointwise, and at regular intervals, through\n(sub-sampled) Hessian-vector products. This technique differs from the\nclassical approach that would compute differences of gradients, and where\ncontrolling the quality of the curvature estimates can be difficult. We present\nnumerical results on problems arising in machine learning that suggest that the\nproposed method shows much promise.\n", "versions": [{"version": "v1", "created": "Mon, 27 Jan 2014 21:01:33 GMT"}, {"version": "v2", "created": "Wed, 18 Feb 2015 12:01:35 GMT"}], "update_date": "2015-02-19", "authors_parsed": [["Byrd", "R. H.", ""], ["Hansen", "S. L.", ""], ["Nocedal", "J.", ""], ["Singer", "Y.", ""]]}, {"id": "1401.7116", "submitter": "Teemu Roos", "authors": "Andrew Barron, Teemu Roos and Kazuho Watanabe", "title": "Bayesian Properties of Normalized Maximum Likelihood and its Fast\n  Computation", "comments": "Submitted to ISIT-2004 conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.LG math.IT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The normalized maximized likelihood (NML) provides the minimax regret\nsolution in universal data compression, gambling, and prediction, and it plays\nan essential role in the minimum description length (MDL) method of statistical\nmodeling and estimation. Here we show that the normalized maximum likelihood\nhas a Bayes-like representation as a mixture of the component models, even in\nfinite samples, though the weights of linear combination may be both positive\nand negative. This representation addresses in part the relationship between\nMDL and Bayes modeling. This representation has the advantage of speeding the\ncalculation of marginals and conditionals required for coding and prediction\napplications.\n", "versions": [{"version": "v1", "created": "Tue, 28 Jan 2014 09:06:30 GMT"}], "update_date": "2014-01-29", "authors_parsed": [["Barron", "Andrew", ""], ["Roos", "Teemu", ""], ["Watanabe", "Kazuho", ""]]}, {"id": "1401.7388", "submitter": "Benjamin Rubinstein", "authors": "J. Hyam Rubinstein and Benjamin I. P. Rubinstein and Peter L. Bartlett", "title": "Bounding Embeddings of VC Classes into Maximum Classes", "comments": "22 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One of the earliest conjectures in computational learning theory-the Sample\nCompression conjecture-asserts that concept classes (equivalently set systems)\nadmit compression schemes of size linear in their VC dimension. To-date this\nstatement is known to be true for maximum classes---those that possess maximum\ncardinality for their VC dimension. The most promising approach to positively\nresolving the conjecture is by embedding general VC classes into maximum\nclasses without super-linear increase to their VC dimensions, as such\nembeddings would extend the known compression schemes to all VC classes. We\nshow that maximum classes can be characterised by a local-connectivity property\nof the graph obtained by viewing the class as a cubical complex. This geometric\ncharacterisation of maximum VC classes is applied to prove a negative embedding\nresult which demonstrates VC-d classes that cannot be embedded in any maximum\nclass of VC dimension lower than 2d. On the other hand, we show that every VC-d\nclass C embeds in a VC-(d+D) maximum class where D is the deficiency of C,\ni.e., the difference between the cardinalities of a maximum VC-d class and of\nC. For VC-2 classes in binary n-cubes for 4 <= n <= 6, we give best possible\nresults on embedding into maximum classes. For some special classes of Boolean\nfunctions, relationships with maximum classes are investigated. Finally we give\na general recursive procedure for embedding VC-d classes into VC-(d+k) maximum\nclasses for smallest k.\n", "versions": [{"version": "v1", "created": "Wed, 29 Jan 2014 02:09:10 GMT"}], "update_date": "2014-02-04", "authors_parsed": [["Rubinstein", "J. Hyam", ""], ["Rubinstein", "Benjamin I. P.", ""], ["Bartlett", "Peter L.", ""]]}, {"id": "1401.7413", "submitter": "Canyi Lu", "authors": "Canyi Lu, Zhouchen Lin, Shuicheng Yan", "title": "Smoothed Low Rank and Sparse Matrix Recovery by Iteratively Reweighted\n  Least Squares Minimization", "comments": "IEEE Transactions on Image Processing 2015", "journal-ref": null, "doi": "10.1109/TIP.2014.2380155", "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work presents a general framework for solving the low rank and/or sparse\nmatrix minimization problems, which may involve multiple non-smooth terms. The\nIteratively Reweighted Least Squares (IRLS) method is a fast solver, which\nsmooths the objective function and minimizes it by alternately updating the\nvariables and their weights. However, the traditional IRLS can only solve a\nsparse only or low rank only minimization problem with squared loss or an\naffine constraint. This work generalizes IRLS to solve joint/mixed low rank and\nsparse minimization problems, which are essential formulations for many tasks.\nAs a concrete example, we solve the Schatten-$p$ norm and $\\ell_{2,q}$-norm\nregularized Low-Rank Representation (LRR) problem by IRLS, and theoretically\nprove that the derived solution is a stationary point (globally optimal if\n$p,q\\geq1$). Our convergence proof of IRLS is more general than previous one\nwhich depends on the special properties of the Schatten-$p$ norm and\n$\\ell_{2,q}$-norm. Extensive experiments on both synthetic and real data sets\ndemonstrate that our IRLS is much more efficient.\n", "versions": [{"version": "v1", "created": "Wed, 29 Jan 2014 05:16:52 GMT"}, {"version": "v2", "created": "Sat, 6 Dec 2014 14:20:42 GMT"}], "update_date": "2015-06-18", "authors_parsed": [["Lu", "Canyi", ""], ["Lin", "Zhouchen", ""], ["Yan", "Shuicheng", ""]]}, {"id": "1401.7620", "submitter": "Francisco Ruiz", "authors": "Francisco J. R. Ruiz, Isabel Valera, Carlos Blanco, Fernando\n  Perez-Cruz", "title": "Bayesian nonparametric comorbidity analysis of psychiatric disorders", "comments": "Submitted to Journal of Machine Learning Research", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The analysis of comorbidity is an open and complex research field in the\nbranch of psychiatry, where clinical experience and several studies suggest\nthat the relation among the psychiatric disorders may have etiological and\ntreatment implications. In this paper, we are interested in applying latent\nfeature modeling to find the latent structure behind the psychiatric disorders\nthat can help to examine and explain the relationships among them. To this end,\nwe use the large amount of information collected in the National Epidemiologic\nSurvey on Alcohol and Related Conditions (NESARC) database and propose to model\nthese data using a nonparametric latent model based on the Indian Buffet\nProcess (IBP). Due to the discrete nature of the data, we first need to adapt\nthe observation model for discrete random variables. We propose a generative\nmodel in which the observations are drawn from a multinomial-logit distribution\ngiven the IBP matrix. The implementation of an efficient Gibbs sampler is\naccomplished using the Laplace approximation, which allows integrating out the\nweighting factors of the multinomial-logit likelihood model. We also provide a\nvariational inference algorithm for this model, which provides a complementary\n(and less expensive in terms of computational complexity) alternative to the\nGibbs sampler allowing us to deal with a larger number of data. Finally, we use\nthe model to analyze comorbidity among the psychiatric disorders diagnosed by\nexperts from the NESARC database.\n", "versions": [{"version": "v1", "created": "Wed, 29 Jan 2014 18:46:38 GMT"}], "update_date": "2014-01-30", "authors_parsed": [["Ruiz", "Francisco J. R.", ""], ["Valera", "Isabel", ""], ["Blanco", "Carlos", ""], ["Perez-Cruz", "Fernando", ""]]}, {"id": "1401.7625", "submitter": "Aryan Mokhtari", "authors": "Aryan Mokhtari and Alejandro Ribeiro", "title": "RES: Regularized Stochastic BFGS Algorithm", "comments": "13 pages", "journal-ref": null, "doi": "10.1109/TSP.2014.2357775", "report-no": null, "categories": "cs.LG math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  RES, a regularized stochastic version of the Broyden-Fletcher-Goldfarb-Shanno\n(BFGS) quasi-Newton method is proposed to solve convex optimization problems\nwith stochastic objectives. The use of stochastic gradient descent algorithms\nis widespread, but the number of iterations required to approximate optimal\narguments can be prohibitive in high dimensional problems. Application of\nsecond order methods, on the other hand, is impracticable because computation\nof objective function Hessian inverses incurs excessive computational cost.\nBFGS modifies gradient descent by introducing a Hessian approximation matrix\ncomputed from finite gradient differences. RES utilizes stochastic gradients in\nlieu of deterministic gradients for both, the determination of descent\ndirections and the approximation of the objective function's curvature. Since\nstochastic gradients can be computed at manageable computational cost RES is\nrealizable and retains the convergence rate advantages of its deterministic\ncounterparts. Convergence results show that lower and upper bounds on the\nHessian egeinvalues of the sample functions are sufficient to guarantee\nconvergence to optimal arguments. Numerical experiments showcase reductions in\nconvergence time relative to stochastic gradient descent algorithms and\nnon-regularized stochastic versions of BFGS. An application of RES to the\nimplementation of support vector machines is developed.\n", "versions": [{"version": "v1", "created": "Wed, 29 Jan 2014 19:10:23 GMT"}], "update_date": "2015-06-18", "authors_parsed": [["Mokhtari", "Aryan", ""], ["Ribeiro", "Alejandro", ""]]}, {"id": "1401.7709", "submitter": "Deepayan Chakrabarti", "authors": "Deepayan Chakrabarti, Stanislav Funiak, Jonathan Chang, Sofus A.\n  Macskassy", "title": "Joint Inference of Multiple Label Types in Large Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.SI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We tackle the problem of inferring node labels in a partially labeled graph\nwhere each node in the graph has multiple label types and each label type has a\nlarge number of possible labels. Our primary example, and the focus of this\npaper, is the joint inference of label types such as hometown, current city,\nand employers, for users connected by a social network. Standard label\npropagation fails to consider the properties of the label types and the\ninteractions between them. Our proposed method, called EdgeExplain, explicitly\nmodels these, while still enabling scalable inference under a distributed\nmessage-passing architecture. On a billion-node subset of the Facebook social\nnetwork, EdgeExplain significantly outperforms label propagation for several\nlabel types, with lifts of up to 120% for recall@1 and 60% for recall@3.\n", "versions": [{"version": "v1", "created": "Thu, 30 Jan 2014 00:38:53 GMT"}], "update_date": "2014-01-31", "authors_parsed": [["Chakrabarti", "Deepayan", ""], ["Funiak", "Stanislav", ""], ["Chang", "Jonathan", ""], ["Macskassy", "Sofus A.", ""]]}, {"id": "1401.7727", "submitter": "Benjamin Rubinstein", "authors": "Battista Biggio and Igino Corona and Blaine Nelson and Benjamin I. P.\n  Rubinstein and Davide Maiorca and Giorgio Fumera and Giorgio Giacinto and and\n  Fabio Roli", "title": "Security Evaluation of Support Vector Machines in Adversarial\n  Environments", "comments": "47 pages, 9 figures; chapter accepted into book 'Support Vector\n  Machine Applications'", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Support Vector Machines (SVMs) are among the most popular classification\ntechniques adopted in security applications like malware detection, intrusion\ndetection, and spam filtering. However, if SVMs are to be incorporated in\nreal-world security systems, they must be able to cope with attack patterns\nthat can either mislead the learning algorithm (poisoning), evade detection\n(evasion), or gain information about their internal parameters (privacy\nbreaches). The main contributions of this chapter are twofold. First, we\nintroduce a formal general framework for the empirical evaluation of the\nsecurity of machine-learning systems. Second, according to our framework, we\ndemonstrate the feasibility of evasion, poisoning and privacy attacks against\nSVMs in real-world security problems. For each attack technique, we evaluate\nits impact and discuss whether (and how) it can be countered through an\nadversary-aware design of SVMs. Our experiments are easily reproducible thanks\nto open-source code that we have made available, together with all the employed\ndatasets, on a public repository.\n", "versions": [{"version": "v1", "created": "Thu, 30 Jan 2014 03:37:18 GMT"}], "update_date": "2014-01-31", "authors_parsed": [["Biggio", "Battista", ""], ["Corona", "Igino", ""], ["Nelson", "Blaine", ""], ["Rubinstein", "Benjamin I. P.", ""], ["Maiorca", "Davide", ""], ["Fumera", "Giorgio", ""], ["Giacinto", "Giorgio", ""], ["Roli", "and Fabio", ""]]}, {"id": "1401.7898", "submitter": "Aryeh Kontorovich", "authors": "Aryeh Kontorovich and Roi Weiss", "title": "Maximum Margin Multiclass Nearest Neighbors", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop a general framework for margin-based multicategory classification\nin metric spaces. The basic work-horse is a margin-regularized version of the\nnearest-neighbor classifier. We prove generalization bounds that match the\nstate of the art in sample size $n$ and significantly improve the dependence on\nthe number of classes $k$. Our point of departure is a nearly Bayes-optimal\nfinite-sample risk bound independent of $k$. Although $k$-free, this bound is\nunregularized and non-adaptive, which motivates our main result: Rademacher and\nscale-sensitive margin bounds with a logarithmic dependence on $k$. As the best\nprevious risk estimates in this setting were of order $\\sqrt k$, our bound is\nexponentially sharper. From the algorithmic standpoint, in doubling metric\nspaces our classifier may be trained on $n$ examples in $O(n^2\\log n)$ time and\nevaluated on new points in $O(\\log n)$ time.\n", "versions": [{"version": "v1", "created": "Thu, 30 Jan 2014 16:00:43 GMT"}], "update_date": "2014-01-31", "authors_parsed": [["Kontorovich", "Aryeh", ""], ["Weiss", "Roi", ""]]}, {"id": "1401.8008", "submitter": "David Venuto", "authors": "David Venuto, Toby Dylan Hocking, Lakjaree Sphanurattana, Masashi\n  Sugiyama", "title": "Support vector comparison machines", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In ranking problems, the goal is to learn a ranking function from labeled\npairs of input points. In this paper, we consider the related comparison\nproblem, where the label indicates which element of the pair is better, or if\nthere is no significant difference. We cast the learning problem as a margin\nmaximization, and show that it can be solved by converting it to a standard\nSVM. We use simulated nonlinear patterns, a real learning to rank sushi data\nset, and a chess data set to show that our proposed SVMcompare algorithm\noutperforms SVMrank when there are equality pairs.\n", "versions": [{"version": "v1", "created": "Thu, 30 Jan 2014 21:49:16 GMT"}, {"version": "v2", "created": "Wed, 20 Dec 2017 21:44:21 GMT"}, {"version": "v3", "created": "Thu, 23 Jul 2020 23:55:11 GMT"}], "update_date": "2020-07-27", "authors_parsed": [["Venuto", "David", ""], ["Hocking", "Toby Dylan", ""], ["Sphanurattana", "Lakjaree", ""], ["Sugiyama", "Masashi", ""]]}, {"id": "1401.8074", "submitter": "Kevin Leyton-Brown", "authors": "Erik Zawadzki, Asher Lipson, Kevin Leyton-Brown", "title": "Empirically Evaluating Multiagent Learning Algorithms", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GT cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There exist many algorithms for learning how to play repeated bimatrix games.\nMost of these algorithms are justified in terms of some sort of theoretical\nguarantee. On the other hand, little is known about the empirical performance\nof these algorithms. Most such claims in the literature are based on small\nexperiments, which has hampered understanding as well as the development of new\nmultiagent learning (MAL) algorithms. We have developed a new suite of tools\nfor running multiagent experiments: the MultiAgent Learning Testbed (MALT).\nThese tools are designed to facilitate larger and more comprehensive\nexperiments by removing the need to build one-off experimental code. MALT also\nprovides baseline implementations of many MAL algorithms, hopefully eliminating\nor reducing differences between algorithm implementations and increasing the\nreproducibility of results. Using this test suite, we ran an experiment\nunprecedented in size. We analyzed the results according to a variety of\nperformance metrics including reward, maxmin distance, regret, and several\nnotions of equilibrium convergence. We confirmed several pieces of conventional\nwisdom, but also discovered some surprising results. For example, we found that\nsingle-agent $Q$-learning outperformed many more complicated and more modern\nMAL algorithms.\n", "versions": [{"version": "v1", "created": "Fri, 31 Jan 2014 07:02:58 GMT"}], "update_date": "2014-02-03", "authors_parsed": [["Zawadzki", "Erik", ""], ["Lipson", "Asher", ""], ["Leyton-Brown", "Kevin", ""]]}, {"id": "1401.8126", "submitter": "Chunhua Shen", "authors": "Mehrtash Harandi, Richard Hartley, Chunhua Shen, Brian Lovell, Conrad\n  Sanderson", "title": "Extrinsic Methods for Coding and Dictionary Learning on Grassmann\n  Manifolds", "comments": "Appearing in International Journal of Computer Vision", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sparsity-based representations have recently led to notable results in\nvarious visual recognition tasks. In a separate line of research, Riemannian\nmanifolds have been shown useful for dealing with features and models that do\nnot lie in Euclidean spaces. With the aim of building a bridge between the two\nrealms, we address the problem of sparse coding and dictionary learning over\nthe space of linear subspaces, which form Riemannian structures known as\nGrassmann manifolds. To this end, we propose to embed Grassmann manifolds into\nthe space of symmetric matrices by an isometric mapping. This in turn enables\nus to extend two sparse coding schemes to Grassmann manifolds. Furthermore, we\npropose closed-form solutions for learning a Grassmann dictionary, atom by\natom. Lastly, to handle non-linearity in data, we extend the proposed Grassmann\nsparse coding and dictionary learning algorithms through embedding into Hilbert\nspaces.\n  Experiments on several classification tasks (gender recognition, gesture\nclassification, scene analysis, face recognition, action recognition and\ndynamic texture classification) show that the proposed approaches achieve\nconsiderable improvements in discrimination accuracy, in comparison to\nstate-of-the-art methods such as kernelized Affine Hull Method and\ngraph-embedding Grassmann discriminant analysis.\n", "versions": [{"version": "v1", "created": "Fri, 31 Jan 2014 10:59:38 GMT"}, {"version": "v2", "created": "Wed, 20 May 2015 00:12:44 GMT"}], "update_date": "2015-05-21", "authors_parsed": [["Harandi", "Mehrtash", ""], ["Hartley", "Richard", ""], ["Shen", "Chunhua", ""], ["Lovell", "Brian", ""], ["Sanderson", "Conrad", ""]]}, {"id": "1401.8212", "submitter": "Amin Rasekh", "authors": "Amin Rasekh, Chien-An Chen, Yan Lu", "title": "Human Activity Recognition using Smartphone", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.HC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Human activity recognition has wide applications in medical research and\nhuman survey system. In this project, we design a robust activity recognition\nsystem based on a smartphone. The system uses a 3-dimentional smartphone\naccelerometer as the only sensor to collect time series signals, from which 31\nfeatures are generated in both time and frequency domain. Activities are\nclassified using 4 different passive learning methods, i.e., quadratic\nclassifier, k-nearest neighbor algorithm, support vector machine, and\nartificial neural networks. Dimensionality reduction is performed through both\nfeature extraction and subset selection. Besides passive learning, we also\napply active learning algorithms to reduce data labeling expense. Experiment\nresults show that the classification rate of passive learning reaches 84.4% and\nit is robust to common positions and poses of cellphone. The results of active\nlearning on real data demonstrate a reduction of labeling labor to achieve\ncomparable performance with passive learning.\n", "versions": [{"version": "v1", "created": "Thu, 30 Jan 2014 17:28:10 GMT"}], "update_date": "2014-02-03", "authors_parsed": [["Rasekh", "Amin", ""], ["Chen", "Chien-An", ""], ["Lu", "Yan", ""]]}, {"id": "1401.8257", "submitter": "Shuai Li", "authors": "Claudio Gentile, Shuai Li, Giovanni Zappella", "title": "Online Clustering of Bandits", "comments": "In E. Xing and T. Jebara (Eds.), Proceedings of 31st International\n  Conference on Machine Learning, Journal of Machine Learning Research Workshop\n  and Conference Proceedings, Vol.32 (JMLR W&CP-32), Beijing, China, Jun.\n  21-26, 2014 (ICML 2014), Submitted by Shuai Li\n  (https://sites.google.com/site/shuailidotsli)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a novel algorithmic approach to content recommendation based on\nadaptive clustering of exploration-exploitation (\"bandit\") strategies. We\nprovide a sharp regret analysis of this algorithm in a standard stochastic\nnoise setting, demonstrate its scalability properties, and prove its\neffectiveness on a number of artificial and real-world datasets. Our\nexperiments show a significant increase in prediction performance over\nstate-of-the-art methods for bandit problems.\n", "versions": [{"version": "v1", "created": "Fri, 31 Jan 2014 18:49:42 GMT"}, {"version": "v2", "created": "Tue, 13 May 2014 07:13:06 GMT"}, {"version": "v3", "created": "Fri, 6 Jun 2014 13:59:04 GMT"}], "update_date": "2014-06-09", "authors_parsed": [["Gentile", "Claudio", ""], ["Li", "Shuai", ""], ["Zappella", "Giovanni", ""]]}, {"id": "1401.8269", "submitter": "Peter Turney", "authors": "Peter D. Turney and Saif M. Mohammad", "title": "Experiments with Three Approaches to Recognizing Lexical Entailment", "comments": "to appear in Natural Language Engineering", "journal-ref": "Natural Language Engineering, 21 (3), (2015), 437-476", "doi": "10.1017/S1351324913000387", "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Inference in natural language often involves recognizing lexical entailment\n(RLE); that is, identifying whether one word entails another. For example,\n\"buy\" entails \"own\". Two general strategies for RLE have been proposed: One\nstrategy is to manually construct an asymmetric similarity measure for context\nvectors (directional similarity) and another is to treat RLE as a problem of\nlearning to recognize semantic relations using supervised machine learning\ntechniques (relation classification). In this paper, we experiment with two\nrecent state-of-the-art representatives of the two general strategies. The\nfirst approach is an asymmetric similarity measure (an instance of the\ndirectional similarity strategy), designed to capture the degree to which the\ncontexts of a word, a, form a subset of the contexts of another word, b. The\nsecond approach (an instance of the relation classification strategy)\nrepresents a word pair, a:b, with a feature vector that is the concatenation of\nthe context vectors of a and b, and then applies supervised learning to a\ntraining set of labeled feature vectors. Additionally, we introduce a third\napproach that is a new instance of the relation classification strategy. The\nthird approach represents a word pair, a:b, with a feature vector in which the\nfeatures are the differences in the similarities of a and b to a set of\nreference words. All three approaches use vector space models (VSMs) of\nsemantics, based on word-context matrices. We perform an extensive evaluation\nof the three approaches using three different datasets. The proposed new\napproach (similarity differences) performs significantly better than the other\ntwo approaches on some datasets and there is no dataset for which it is\nsignificantly worse. Our results suggest it is beneficial to make connections\nbetween the research in lexical entailment and the research in semantic\nrelation classification.\n", "versions": [{"version": "v1", "created": "Fri, 31 Jan 2014 19:42:19 GMT"}], "update_date": "2015-06-02", "authors_parsed": [["Turney", "Peter D.", ""], ["Mohammad", "Saif M.", ""]]}]