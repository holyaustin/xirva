[{"id": "1007.0296", "submitter": "Marcus Hutter", "authors": "Wray Buntine and Marcus Hutter", "title": "A Bayesian View of the Poisson-Dirichlet Process", "comments": "50 LaTeX pages, 10 figures, 3 tables, 1 algorithm", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.LG math.PR stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The two parameter Poisson-Dirichlet Process (PDP), a generalisation of the\nDirichlet Process, is increasingly being used for probabilistic modelling in\ndiscrete areas such as language technology, bioinformatics, and image analysis.\nThere is a rich literature about the PDP and its derivative distributions such\nas the Chinese Restaurant Process (CRP). This article reviews some of the basic\ntheory and then the major results needed for Bayesian modelling of discrete\nproblems including details of priors, posteriors and computation.\n  The PDP allows one to build distributions over countable partitions. The PDP\nhas two other remarkable properties: first it is partially conjugate to itself,\nwhich allows one to build hierarchies of PDPs, and second using a marginalised\nrelative the CRP, one gets fragmentation and clustering properties that lets\none layer partitions to build trees. This article presents the basic theory for\nunderstanding the notion of partitions and distributions over them, the PDP and\nthe CRP, and the important properties of conjugacy, fragmentation and\nclustering, as well as some key related properties such as consistency and\nconvergence. This article also presents a Bayesian interpretation of the\nPoisson-Dirichlet process based on an improper and infinite dimensional\nDirichlet distribution. This means we can understand the process as just\nanother Dirichlet and thus all its sampling properties emerge naturally.\n  The theory of PDPs is usually presented for continuous distributions (more\ngenerally referred to as non-atomic distributions), however, when applied to\ndiscrete distributions its remarkable conjugacy property emerges. This context\nand basic results are also presented, as well as techniques for computing the\nsecond order Stirling numbers that occur in the posteriors for discrete\ndistributions.\n", "versions": [{"version": "v1", "created": "Fri, 2 Jul 2010 05:10:49 GMT"}, {"version": "v2", "created": "Wed, 15 Feb 2012 21:56:08 GMT"}], "update_date": "2012-02-17", "authors_parsed": [["Buntine", "Wray", ""], ["Hutter", "Marcus", ""]]}, {"id": "1007.0380", "submitter": "Mithun Das Gupta", "authors": "Mithun Das Gupta", "title": "Additive Non-negative Matrix Factorization for Missing Data", "comments": "General extension of the NMF framework", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NA cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Non-negative matrix factorization (NMF) has previously been shown to be a\nuseful decomposition for multivariate data. We interpret the factorization in a\nnew way and use it to generate missing attributes from test data. We provide a\njoint optimization scheme for the missing attributes as well as the NMF\nfactors. We prove the monotonic convergence of our algorithms. We present\nclassification results for cases with missing attributes.\n", "versions": [{"version": "v1", "created": "Thu, 1 Jul 2010 17:40:01 GMT"}], "update_date": "2010-07-05", "authors_parsed": [["Gupta", "Mithun Das", ""]]}, {"id": "1007.0481", "submitter": "Byung-Hak Kim", "authors": "Byung-Hak Kim, Arvind Yedla, and Henry D. Pfister", "title": "IMP: A Message-Passing Algorithmfor Matrix Completion", "comments": "To appear in Proc. 6th International Symposium on Turbo Codes and\n  Iterative Information Processing, Brest, France, September 6-10, 2010", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.LG math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A new message-passing (MP) method is considered for the matrix completion\nproblem associated with recommender systems. We attack the problem using a\n(generative) factor graph model that is related to a probabilistic low-rank\nmatrix factorization. Based on the model, we propose a new algorithm, termed\nIMP, for the recovery of a data matrix from incomplete observations. The\nalgorithm is based on a clustering followed by inference via MP (IMP). The\nalgorithm is compared with a number of other matrix completion algorithms on\nreal collaborative filtering (e.g., Netflix) data matrices. Our results show\nthat, while many methods perform similarly with a large number of revealed\nentries, the IMP algorithm outperforms all others when the fraction of observed\nentries is small. This is helpful because it reduces the well-known cold-start\nproblem associated with collaborative filtering (CF) systems in practice.\n", "versions": [{"version": "v1", "created": "Sat, 3 Jul 2010 08:36:57 GMT"}], "update_date": "2010-07-06", "authors_parsed": [["Kim", "Byung-Hak", ""], ["Yedla", "Arvind", ""], ["Pfister", "Henry D.", ""]]}, {"id": "1007.0484", "submitter": "Blaine Nelson", "authors": "Blaine Nelson and Benjamin I. P. Rubinstein and Ling Huang and Anthony\n  D. Joseph and Steven J. Lee and Satish Rao and J. D. Tygar", "title": "Query Strategies for Evading Convex-Inducing Classifiers", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR cs.GT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Classifiers are often used to detect miscreant activities. We study how an\nadversary can systematically query a classifier to elicit information that\nallows the adversary to evade detection while incurring a near-minimal cost of\nmodifying their intended malfeasance. We generalize the theory of Lowd and Meek\n(2005) to the family of convex-inducing classifiers that partition input space\ninto two sets one of which is convex. We present query algorithms for this\nfamily that construct undetected instances of approximately minimal cost using\nonly polynomially-many queries in the dimension of the space and in the level\nof approximation. Our results demonstrate that near-optimal evasion can be\naccomplished without reverse-engineering the classifier's decision boundary. We\nalso consider general lp costs and show that near-optimal evasion on the family\nof convex-inducing classifiers is generally efficient for both positive and\nnegative convexity for all levels of approximation if p=1.\n", "versions": [{"version": "v1", "created": "Sat, 3 Jul 2010 09:04:44 GMT"}], "update_date": "2010-07-06", "authors_parsed": [["Nelson", "Blaine", ""], ["Rubinstein", "Benjamin I. P.", ""], ["Huang", "Ling", ""], ["Joseph", "Anthony D.", ""], ["Lee", "Steven J.", ""], ["Rao", "Satish", ""], ["Tygar", "J. D.", ""]]}, {"id": "1007.0546", "submitter": "Pouyan  Rafiei Fard", "authors": "Keyvan Yahya, Pouyan Rafiei Fard", "title": "Computational Model of Music Sight Reading: A Reinforcement Learning\n  Approach", "comments": "This paper is withdrawn by author due to incomplete justification of\n  the model", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG cs.NE math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Although the Music Sight Reading process has been studied from the cognitive\npsychology view points, but the computational learning methods like the\nReinforcement Learning have not yet been used to modeling of such processes. In\nthis paper, with regards to essential properties of our specific problem, we\nconsider the value function concept and will indicate that the optimum policy\ncan be obtained by the method we offer without to be getting involved with\ncomputing of the complex value functions. Also, we will offer a normative\nbehavioral model for the interaction of the agent with the musical pitch\nenvironment and by using a slightly different version of Partially observable\nMarkov decision processes we will show that our method helps for faster\nlearning of state-action pairs in our implemented agents.\n", "versions": [{"version": "v1", "created": "Sun, 4 Jul 2010 12:18:56 GMT"}, {"version": "v2", "created": "Thu, 26 Aug 2010 19:32:16 GMT"}, {"version": "v3", "created": "Sat, 28 Aug 2010 10:57:19 GMT"}, {"version": "v4", "created": "Sat, 13 Jul 2013 22:59:26 GMT"}], "update_date": "2013-07-16", "authors_parsed": [["Yahya", "Keyvan", ""], ["Fard", "Pouyan Rafiei", ""]]}, {"id": "1007.0548", "submitter": "Pouyan  Rafiei Fard", "authors": "Keyvan Yahya, Pouyan Rafiei Fard", "title": "A Reinforcement Learning Model Using Neural Networks for Music Sight\n  Reading Learning Problem", "comments": "This paper is withdrawn because of its lack of novelty", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Music Sight Reading is a complex process in which when it is occurred in the\nbrain some learning attributes would be emerged. Besides giving a model based\non actor-critic method in the Reinforcement Learning, the agent is considered\nto have a neural network structure. We studied on where the sight reading\nprocess is happened and also a serious problem which is how the synaptic\nweights would be adjusted through the learning process. The model we offer here\nis a computational model on which an updated weights equation to fix the\nweights is accompanied too.\n", "versions": [{"version": "v1", "created": "Sun, 4 Jul 2010 12:37:13 GMT"}, {"version": "v2", "created": "Mon, 23 Aug 2010 22:57:42 GMT"}, {"version": "v3", "created": "Fri, 18 Nov 2011 20:11:34 GMT"}], "update_date": "2011-11-21", "authors_parsed": [["Yahya", "Keyvan", ""], ["Fard", "Pouyan Rafiei", ""]]}, {"id": "1007.0549", "submitter": "Larry Wasserman", "authors": "Christopher Genovese, Marco Perone-Pacifico, Isabella Verdinelli and\n  Larry Wasserman", "title": "Minimax Manifold Estimation", "comments": "journal submission, revision with some errors corrected", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We find the minimax rate of convergence in Hausdorff distance for estimating\na manifold M of dimension d embedded in R^D given a noisy sample from the\nmanifold. We assume that the manifold satisfies a smoothness condition and that\nthe noise distribution has compact support. We show that the optimal rate of\nconvergence is n^{-2/(2+d)}. Thus, the minimax rate depends only on the\ndimension of the manifold, not on the dimension of the space in which M is\nembedded.\n", "versions": [{"version": "v1", "created": "Sun, 4 Jul 2010 13:11:40 GMT"}, {"version": "v2", "created": "Tue, 23 Nov 2010 17:21:02 GMT"}, {"version": "v3", "created": "Wed, 28 Sep 2011 18:14:13 GMT"}], "update_date": "2011-09-29", "authors_parsed": [["Genovese", "Christopher", ""], ["Perone-Pacifico", "Marco", ""], ["Verdinelli", "Isabella", ""], ["Wasserman", "Larry", ""]]}, {"id": "1007.0660", "submitter": "Gabi Pragier", "authors": "Amnon Shashua, Gabi Pragier", "title": "The Latent Bernoulli-Gauss Model for Data Analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a new latent-variable model employing a Gaussian mixture\nintegrated with a feature selection procedure (the Bernoulli part of the model)\nwhich together form a \"Latent Bernoulli-Gauss\" distribution. The model is\napplied to MAP estimation, clustering, feature selection and collaborative\nfiltering and fares favorably with the state-of-the-art latent-variable models.\n", "versions": [{"version": "v1", "created": "Mon, 5 Jul 2010 11:46:35 GMT"}], "update_date": "2010-07-06", "authors_parsed": [["Shashua", "Amnon", ""], ["Pragier", "Gabi", ""]]}, {"id": "1007.0824", "submitter": "Remi Flamary", "authors": "R\\'emi Flamary (LITIS), Benjamin Labb\\'e (LITIS), Alain Rakotomamonjy\n  (LITIS)", "title": "Filtrage vaste marge pour l'\\'etiquetage s\\'equentiel \\`a noyaux de\n  signaux", "comments": null, "journal-ref": "Conf\\'erence Francophone sur l'Apprentissage Automatique, Clermont\n  Ferrand : France (2010)", "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We address in this paper the problem of multi-channel signal sequence\nlabeling. In particular, we consider the problem where the signals are\ncontaminated by noise or may present some dephasing with respect to their\nlabels. For that, we propose to jointly learn a SVM sample classifier with a\ntemporal filtering of the channels. This will lead to a large margin filtering\nthat is adapted to the specificity of each channel (noise and time-lag). We\nderive algorithms to solve the optimization problem and we discuss different\nfilter regularizations for automated scaling or selection of channels. Our\napproach is tested on a non-linear toy example and on a BCI dataset. Results\nshow that the classification performance on these problems can be improved by\nlearning a large margin filtering.\n", "versions": [{"version": "v1", "created": "Tue, 6 Jul 2010 07:47:00 GMT"}], "update_date": "2010-07-26", "authors_parsed": [["Flamary", "R\u00e9mi", "", "LITIS"], ["Labb\u00e9", "Benjamin", "", "LITIS"], ["Rakotomamonjy", "Alain", "", "LITIS"]]}, {"id": "1007.1282", "submitter": "Vladimir Pestov", "authors": "Vladimir Pestov", "title": "A note on sample complexity of learning binary output neural networks\n  under fixed input distributions", "comments": "6 pages, latex in IEEE conference proceedings format", "journal-ref": "Proc. 2010 Eleventh Brazilian Symposium on Neural Networks (S\\~ao\n  Bernardo do Campo, SP, Brazil, 23-28 October 2010), IEEE Computer Society,\n  2010, pp. 7-12", "doi": "10.1109/SBRN.2010.10", "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We show that the learning sample complexity of a sigmoidal neural network\nconstructed by Sontag (1992) required to achieve a given misclassification\nerror under a fixed purely atomic distribution can grow arbitrarily fast: for\nany prescribed rate of growth there is an input distribution having this rate\nas the sample complexity, and the bound is asymptotically tight. The rate can\nbe superexponential, a non-recursive function, etc. We further observe that\nSontag's ANN is not Glivenko-Cantelli under any input distribution having a\nnon-atomic part.\n", "versions": [{"version": "v1", "created": "Thu, 8 Jul 2010 03:58:25 GMT"}], "update_date": "2016-11-17", "authors_parsed": [["Pestov", "Vladimir", ""]]}, {"id": "1007.2049", "submitter": "Marcus Hutter", "authors": "Joel Veness, Kee Siong Ng, Marcus Hutter and David Silver", "title": "Reinforcement Learning via AIXI Approximation", "comments": "8 LaTeX pages, 1 figure", "journal-ref": "Proc. 24th AAAI Conference on Artificial Intelligence (AAAI 2010)\n  pages 605-611", "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces a principled approach for the design of a scalable\ngeneral reinforcement learning agent. This approach is based on a direct\napproximation of AIXI, a Bayesian optimality notion for general reinforcement\nlearning agents. Previously, it has been unclear whether the theory of AIXI\ncould motivate the design of practical algorithms. We answer this hitherto open\nquestion in the affirmative, by providing the first computationally feasible\napproximation to the AIXI agent. To develop our approximation, we introduce a\nMonte Carlo Tree Search algorithm along with an agent-specific extension of the\nContext Tree Weighting algorithm. Empirically, we present a set of encouraging\nresults on a number of stochastic, unknown, and partially observable domains.\n", "versions": [{"version": "v1", "created": "Tue, 13 Jul 2010 08:48:18 GMT"}], "update_date": "2010-10-04", "authors_parsed": [["Veness", "Joel", ""], ["Ng", "Kee Siong", ""], ["Hutter", "Marcus", ""], ["Silver", "David", ""]]}, {"id": "1007.2075", "submitter": "Marcus Hutter", "authors": "Peter Sunehag and Marcus Hutter", "title": "Consistency of Feature Markov Processes", "comments": "16 LaTeX pages", "journal-ref": "Proc. 21st International Conf. on Algorithmic Learning Theory\n  (ALT-2010) pages 360-374", "doi": null, "report-no": null, "categories": "cs.LG cs.IT math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We are studying long term sequence prediction (forecasting). We approach this\nby investigating criteria for choosing a compact useful state representation.\nThe state is supposed to summarize useful information from the history. We want\na method that is asymptotically consistent in the sense it will provably\neventually only choose between alternatives that satisfy an optimality property\nrelated to the used criterion. We extend our work to the case where there is\nside information that one can take advantage of and, furthermore, we briefly\ndiscuss the active setting where an agent takes actions to achieve desirable\noutcomes.\n", "versions": [{"version": "v1", "created": "Tue, 13 Jul 2010 10:54:14 GMT"}], "update_date": "2012-02-10", "authors_parsed": [["Sunehag", "Peter", ""], ["Hutter", "Marcus", ""]]}, {"id": "1007.2238", "submitter": "Cem Tekin", "authors": "Cem Tekin, Mingyan Liu", "title": "Online Algorithms for the Multi-Armed Bandit Problem with Markovian\n  Rewards", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the classical multi-armed bandit problem with Markovian rewards.\nWhen played an arm changes its state in a Markovian fashion while it remains\nfrozen when not played. The player receives a state-dependent reward each time\nit plays an arm. The number of states and the state transition probabilities of\nan arm are unknown to the player. The player's objective is to maximize its\nlong-term total reward by learning the best arm over time. We show that under\ncertain conditions on the state transition probabilities of the arms, a sample\nmean based index policy achieves logarithmic regret uniformly over the total\nnumber of trials. The result shows that sample mean based index policies can be\napplied to learning problems under the rested Markovian bandit model without\nloss of optimality in the order. Moreover, comparision between Anantharam's\nindex policy and UCB shows that by choosing a small exploration parameter UCB\ncan have a smaller regret than Anantharam's index policy.\n", "versions": [{"version": "v1", "created": "Wed, 14 Jul 2010 02:26:00 GMT"}, {"version": "v2", "created": "Mon, 26 Jul 2010 05:03:06 GMT"}], "update_date": "2010-07-27", "authors_parsed": [["Tekin", "Cem", ""], ["Liu", "Mingyan", ""]]}, {"id": "1007.2449", "submitter": "Kamran Karimi", "authors": "Kamran Karimi", "title": "A Brief Introduction to Temporality and Causality", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Causality is a non-obvious concept that is often considered to be related to\ntemporality. In this paper we present a number of past and present approaches\nto the definition of temporality and causality from philosophical, physical,\nand computational points of view. We note that time is an important ingredient\nin many relationships and phenomena. The topic is then divided into the two\nmain areas of temporal discovery, which is concerned with finding relations\nthat are stretched over time, and causal discovery, where a claim is made as to\nthe causal influence of certain events on others. We present a number of\ncomputational tools used for attempting to automatically discover temporal and\ncausal relations in data.\n", "versions": [{"version": "v1", "created": "Wed, 14 Jul 2010 22:41:30 GMT"}], "update_date": "2010-07-16", "authors_parsed": [["Karimi", "Kamran", ""]]}, {"id": "1007.2958", "submitter": "Hoang Trinh", "authors": "Hoang Trinh", "title": "A Machine Learning Approach to Recovery of Scene Geometry from Images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recovering the 3D structure of the scene from images yields useful\ninformation for tasks such as shape and scene recognition, object detection, or\nmotion planning and object grasping in robotics. In this thesis, we introduce a\ngeneral machine learning approach called unsupervised CRF learning based on\nmaximizing the conditional likelihood. We apply our approach to computer vision\nsystems that recover the 3-D scene geometry from images. We focus on recovering\n3D geometry from single images, stereo pairs and video sequences. Building\nthese systems requires algorithms for doing inference as well as learning the\nparameters of conditional Markov random fields (MRF). Our system is trained\nunsupervisedly without using ground-truth labeled data. We employ a\nslanted-plane stereo vision model in which we use a fixed over-segmentation to\nsegment the left image into coherent regions called superpixels, then assign a\ndisparity plane for each superpixel. Plane parameters are estimated by solving\nan MRF labelling problem, through minimizing an energy fuction. We demonstrate\nthe use of our unsupervised CRF learning algorithm for a parameterized\nslanted-plane stereo vision model involving shape from texture cues. Our stereo\nmodel with texture cues, only by unsupervised training, outperforms the results\nin related work on the same stereo dataset. In this thesis, we also formulate\nstructure and motion estimation as an energy minimization problem, in which the\nmodel is an extension of our slanted-plane stereo vision model that also\nhandles surface velocity. Velocity estimation is achieved by solving an MRF\nlabeling problem using Loopy BP. Performance analysis is done using our novel\nevaluation metrics based on the notion of view prediction error. Experiments on\nroad-driving stereo sequences show encouraging results.\n", "versions": [{"version": "v1", "created": "Sat, 17 Jul 2010 19:59:11 GMT"}], "update_date": "2010-07-20", "authors_parsed": [["Trinh", "Hoang", ""]]}, {"id": "1007.3564", "submitter": "Dacheng Tao", "authors": "Tianyi Zhou, Dacheng Tao, Xindong Wu", "title": "Manifold Elastic Net: A Unified Framework for Sparse Dimension Reduction", "comments": "33 pages, 12 figures", "journal-ref": "Journal of Data Mining and Knowledge Discovery, 2010", "doi": "10.1007/s10618-010-0182-x", "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is difficult to find the optimal sparse solution of a manifold learning\nbased dimensionality reduction algorithm. The lasso or the elastic net\npenalized manifold learning based dimensionality reduction is not directly a\nlasso penalized least square problem and thus the least angle regression (LARS)\n(Efron et al. \\cite{LARS}), one of the most popular algorithms in sparse\nlearning, cannot be applied. Therefore, most current approaches take indirect\nways or have strict settings, which can be inconvenient for applications. In\nthis paper, we proposed the manifold elastic net or MEN for short. MEN\nincorporates the merits of both the manifold learning based dimensionality\nreduction and the sparse learning based dimensionality reduction. By using a\nseries of equivalent transformations, we show MEN is equivalent to the lasso\npenalized least square problem and thus LARS is adopted to obtain the optimal\nsparse solution of MEN. In particular, MEN has the following advantages for\nsubsequent classification: 1) the local geometry of samples is well preserved\nfor low dimensional data representation, 2) both the margin maximization and\nthe classification error minimization are considered for sparse projection\ncalculation, 3) the projection matrix of MEN improves the parsimony in\ncomputation, 4) the elastic net penalty reduces the over-fitting problem, and\n5) the projection matrix of MEN can be interpreted psychologically and\nphysiologically. Experimental evidence on face recognition over various popular\ndatasets suggests that MEN is superior to top level dimensionality reduction\nalgorithms.\n", "versions": [{"version": "v1", "created": "Wed, 21 Jul 2010 05:50:47 GMT"}, {"version": "v2", "created": "Sat, 24 Jul 2010 03:48:30 GMT"}, {"version": "v3", "created": "Tue, 27 Jul 2010 03:01:09 GMT"}], "update_date": "2010-07-28", "authors_parsed": [["Zhou", "Tianyi", ""], ["Tao", "Dacheng", ""], ["Wu", "Xindong", ""]]}, {"id": "1007.3622", "submitter": "Alexey Koloydenko", "authors": "J\\\"uri Lember and Alexey A. Koloydenko", "title": "A generalized risk approach to path inference based on hidden Markov\n  models", "comments": "Section 5: corrected denominators of the scaled beta variables (pp.\n  27-30), => corrections in claims 1, 3, Prop. 12, bottom of Table 1. Decoder\n  (49), Corol. 14 are generalized to handle 0 probabilities. Notation is more\n  closely aligned with (Bishop, 2006). Details are inserted in eqn-s (43); the\n  positivity assumption in Prop. 11 is explicit. Fixed typing errors in\n  equation (41), Example 2", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motivated by the unceasing interest in hidden Markov models (HMMs), this\npaper re-examines hidden path inference in these models, using primarily a\nrisk-based framework. While the most common maximum a posteriori (MAP), or\nViterbi, path estimator and the minimum error, or Posterior Decoder (PD), have\nlong been around, other path estimators, or decoders, have been either only\nhinted at or applied more recently and in dedicated applications generally\nunfamiliar to the statistical learning community. Over a decade ago, however, a\nfamily of algorithmically defined decoders aiming to hybridize the two standard\nones was proposed (Brushe et al., 1998). The present paper gives a careful\nanalysis of this hybridization approach, identifies several problems and issues\nwith it and other previously proposed approaches, and proposes practical\nresolutions of those. Furthermore, simple modifications of the classical\ncriteria for hidden path recognition are shown to lead to a new class of\ndecoders. Dynamic programming algorithms to compute these decoders in the usual\nforward-backward manner are presented. A particularly interesting subclass of\nsuch estimators can be also viewed as hybrids of the MAP and PD estimators.\nSimilar to previously proposed MAP-PD hybrids, the new class is parameterized\nby a small number of tunable parameters. Unlike their algorithmic predecessors,\nthe new risk-based decoders are more clearly interpretable, and, most\nimportantly, work \"out of the box\" in practice, which is demonstrated on some\nreal bioinformatics tasks and data. Some further generalizations and\napplications are discussed in conclusion.\n", "versions": [{"version": "v1", "created": "Wed, 21 Jul 2010 11:44:30 GMT"}, {"version": "v2", "created": "Fri, 20 Jul 2012 16:31:20 GMT"}, {"version": "v3", "created": "Wed, 14 Nov 2012 16:16:43 GMT"}, {"version": "v4", "created": "Tue, 16 Apr 2013 11:58:17 GMT"}], "update_date": "2013-04-17", "authors_parsed": [["Lember", "J\u00fcri", ""], ["Koloydenko", "Alexey A.", ""]]}, {"id": "1007.3799", "submitter": "Aleksandrs Slivkins", "authors": "Umar Syed and Aleksandrs Slivkins and Nina Mishra", "title": "Adapting to the Shifting Intent of Search Queries", "comments": "This is the full version of the paper in NIPS'09", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Search engines today present results that are often oblivious to abrupt\nshifts in intent. For example, the query `independence day' usually refers to a\nUS holiday, but the intent of this query abruptly changed during the release of\na major film by that name. While no studies exactly quantify the magnitude of\nintent-shifting traffic, studies suggest that news events, seasonal topics, pop\nculture, etc account for 50% of all search queries. This paper shows that the\nsignals a search engine receives can be used to both determine that a shift in\nintent has happened, as well as find a result that is now more relevant. We\npresent a meta-algorithm that marries a classifier with a bandit algorithm to\nachieve regret that depends logarithmically on the number of query impressions,\nunder certain assumptions. We provide strong evidence that this regret is close\nto the best achievable. Finally, via a series of experiments, we demonstrate\nthat our algorithm outperforms prior approaches, particularly as the amount of\nintent-shifting traffic increases.\n", "versions": [{"version": "v1", "created": "Thu, 22 Jul 2010 04:58:24 GMT"}], "update_date": "2010-07-23", "authors_parsed": [["Syed", "Umar", ""], ["Slivkins", "Aleksandrs", ""], ["Mishra", "Nina", ""]]}, {"id": "1007.3858", "submitter": "Jon Sneyers", "authors": "Jon Sneyers, Wannes Meert, Joost Vennekens, Yoshitaka Kameya and\n  Taisuke Sato", "title": "CHR(PRISM)-based Probabilistic Logic Learning", "comments": null, "journal-ref": "Theory and Practice of Logic Programming, 10(4-6), 433-447, 2010", "doi": "10.1017/S1471068410000207", "report-no": null, "categories": "cs.PL cs.AI cs.LG cs.LO", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  PRISM is an extension of Prolog with probabilistic predicates and built-in\nsupport for expectation-maximization learning. Constraint Handling Rules (CHR)\nis a high-level programming language based on multi-headed multiset rewrite\nrules.\n  In this paper, we introduce a new probabilistic logic formalism, called\nCHRiSM, based on a combination of CHR and PRISM. It can be used for high-level\nrapid prototyping of complex statistical models by means of \"chance rules\". The\nunderlying PRISM system can then be used for several probabilistic inference\ntasks, including probability computation and parameter learning. We define the\nCHRiSM language in terms of syntax and operational semantics, and illustrate it\nwith examples. We define the notion of ambiguous programs and define a\ndistribution semantics for unambiguous programs. Next, we describe an\nimplementation of CHRiSM, based on CHR(PRISM). We discuss the relation between\nCHRiSM and other probabilistic logic programming languages, in particular PCHR.\nFinally we identify potential application domains.\n", "versions": [{"version": "v1", "created": "Thu, 22 Jul 2010 11:32:21 GMT"}], "update_date": "2010-07-23", "authors_parsed": [["Sneyers", "Jon", ""], ["Meert", "Wannes", ""], ["Vennekens", "Joost", ""], ["Kameya", "Yoshitaka", ""], ["Sato", "Taisuke", ""]]}, {"id": "1007.5133", "submitter": "Secretary  Ijaia", "authors": "Ming-Chang Lee (1) and Chang To (2) ((1) Fooyin University, Taiwan and\n  (2) Shu-Te University, Taiwan)", "title": "Comparison of Support Vector Machine and Back Propagation Neural Network\n  in Evaluating the Enterprise Financial Distress", "comments": "13 pages, 1 figure", "journal-ref": "International Journal of Artificial Intelligence & Applications\n  1.3 (2010) 31-43", "doi": "10.5121/ijaia.2010.1303", "report-no": null, "categories": "cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  Recently, applying the novel data mining techniques for evaluating enterprise\nfinancial distress has received much research alternation. Support Vector\nMachine (SVM) and back propagation neural (BPN) network has been applied\nsuccessfully in many areas with excellent generalization results, such as rule\nextraction, classification and evaluation. In this paper, a model based on SVM\nwith Gaussian RBF kernel is proposed here for enterprise financial distress\nevaluation. BPN network is considered one of the simplest and are most general\nmethods used for supervised training of multilayered neural network. The\ncomparative results show that through the difference between the performance\nmeasures is marginal; SVM gives higher precision and lower error rates.\n", "versions": [{"version": "v1", "created": "Thu, 29 Jul 2010 07:36:49 GMT"}], "update_date": "2010-07-30", "authors_parsed": [["Lee", "Ming-Chang", ""], ["To", "Chang", ""]]}]