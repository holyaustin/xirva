[{"id": "1302.0082", "submitter": "Barnabas Poczos", "authors": "Barnabas Poczos, Alessandro Rinaldo, Aarti Singh, Larry Wasserman", "title": "Distribution-Free Distribution Regression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  `Distribution regression' refers to the situation where a response Y depends\non a covariate P where P is a probability distribution. The model is Y=f(P) +\nmu where f is an unknown regression function and mu is a random error.\nTypically, we do not observe P directly, but rather, we observe a sample from\nP. In this paper we develop theory and methods for distribution-free versions\nof distribution regression. This means that we do not make distributional\nassumptions about the error term mu and covariate P. We prove that when the\neffective dimension is small enough (as measured by the doubling dimension),\nthen the excess prediction risk converges to zero with a polynomial rate.\n", "versions": [{"version": "v1", "created": "Fri, 1 Feb 2013 05:35:48 GMT"}], "update_date": "2013-02-04", "authors_parsed": [["Poczos", "Barnabas", ""], ["Rinaldo", "Alessandro", ""], ["Singh", "Aarti", ""], ["Wasserman", "Larry", ""]]}, {"id": "1302.0315", "submitter": "Mehrdad Mahdavi", "authors": "Rong Jin, Tianbao Yang, Mehrdad Mahdavi", "title": "Sparse Multiple Kernel Learning with Geometric Convergence Rate", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we study the problem of sparse multiple kernel learning (MKL),\nwhere the goal is to efficiently learn a combination of a fixed small number of\nkernels from a large pool that could lead to a kernel classifier with a small\nprediction error. We develop an efficient algorithm based on the greedy\ncoordinate descent algorithm, that is able to achieve a geometric convergence\nrate under appropriate conditions. The convergence rate is achieved by\nmeasuring the size of functional gradients by an empirical $\\ell_2$ norm that\ndepends on the empirical data distribution. This is in contrast to previous\nalgorithms that use a functional norm to measure the size of gradients, which\nis independent from the data samples. We also establish a generalization error\nbound of the learned sparse kernel classifier using the technique of local\nRademacher complexity.\n", "versions": [{"version": "v1", "created": "Fri, 1 Feb 2013 23:28:43 GMT"}], "update_date": "2013-02-05", "authors_parsed": [["Jin", "Rong", ""], ["Yang", "Tianbao", ""], ["Mahdavi", "Mehrdad", ""]]}, {"id": "1302.0386", "submitter": "Jean-Baptiste Mouret", "authors": "Sylvain Koos, Antoine Cully, Jean-Baptiste Mouret", "title": "Fast Damage Recovery in Robotics with the T-Resilience Algorithm", "comments": null, "journal-ref": null, "doi": "10.1177/0278364913499192", "report-no": null, "categories": "cs.RO cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Damage recovery is critical for autonomous robots that need to operate for a\nlong time without assistance. Most current methods are complex and costly\nbecause they require anticipating each potential damage in order to have a\ncontingency plan ready. As an alternative, we introduce the T-resilience\nalgorithm, a new algorithm that allows robots to quickly and autonomously\ndiscover compensatory behaviors in unanticipated situations. This algorithm\nequips the robot with a self-model and discovers new behaviors by learning to\navoid those that perform differently in the self-model and in reality. Our\nalgorithm thus does not identify the damaged parts but it implicitly searches\nfor efficient behaviors that do not use them. We evaluate the T-Resilience\nalgorithm on a hexapod robot that needs to adapt to leg removal, broken legs\nand motor failures; we compare it to stochastic local search, policy gradient\nand the self-modeling algorithm proposed by Bongard et al. The behavior of the\nrobot is assessed on-board thanks to a RGB-D sensor and a SLAM algorithm. Using\nonly 25 tests on the robot and an overall running time of 20 minutes,\nT-Resilience consistently leads to substantially better results than the other\napproaches.\n", "versions": [{"version": "v1", "created": "Sat, 2 Feb 2013 14:53:05 GMT"}], "update_date": "2014-03-26", "authors_parsed": [["Koos", "Sylvain", ""], ["Cully", "Antoine", ""], ["Mouret", "Jean-Baptiste", ""]]}, {"id": "1302.0406", "submitter": "Purushottam Kar", "authors": "Purushottam Kar", "title": "Generalization Guarantees for a Binary Classification Framework for\n  Two-Stage Multiple Kernel Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present generalization bounds for the TS-MKL framework for two stage\nmultiple kernel learning. We also present bounds for sparse kernel learning\nformulations within the TS-MKL framework.\n", "versions": [{"version": "v1", "created": "Sat, 2 Feb 2013 17:20:47 GMT"}], "update_date": "2013-02-05", "authors_parsed": [["Kar", "Purushottam", ""]]}, {"id": "1302.0435", "submitter": "Yu Zhang", "authors": "Yu Zhang, James Z. Wang and Jia Li", "title": "Parallel D2-Clustering: Large-Scale Clustering of Discrete Distributions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The discrete distribution clustering algorithm, namely D2-clustering, has\ndemonstrated its usefulness in image classification and annotation where each\nobject is represented by a bag of weighed vectors. The high computational\ncomplexity of the algorithm, however, limits its applications to large-scale\nproblems. We present a parallel D2-clustering algorithm with substantially\nimproved scalability. A hierarchical structure for parallel computing is\ndevised to achieve a balance between the individual-node computation and the\nintegration process of the algorithm. Additionally, it is shown that even with\na single CPU, the hierarchical structure results in significant speed-up.\nExperiments on real-world large-scale image data, Youtube video data, and\nprotein sequence data demonstrate the efficiency and wide applicability of the\nparallel D2-clustering algorithm. The loss in clustering accuracy is minor in\ncomparison with the original sequential algorithm.\n", "versions": [{"version": "v1", "created": "Sat, 2 Feb 2013 22:56:26 GMT"}, {"version": "v2", "created": "Wed, 6 Feb 2013 15:55:39 GMT"}], "update_date": "2013-02-07", "authors_parsed": [["Zhang", "Yu", ""], ["Wang", "James Z.", ""], ["Li", "Jia", ""]]}, {"id": "1302.0540", "submitter": "Harris Georgiou", "authors": "Harris V. Georgiou, Michael E. Mavroforakis", "title": "A game-theoretic framework for classifier ensembles using weighted\n  majority voting with local accuracy estimates", "comments": "21 pages, 9 tables, 1 figure, 68 references", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  In this paper, a novel approach for the optimal combination of binary\nclassifiers is proposed. The classifier combination problem is approached from\na Game Theory perspective. The proposed framework of adapted weighted majority\nrules (WMR) is tested against common rank-based, Bayesian and simple majority\nmodels, as well as two soft-output averaging rules. Experiments with ensembles\nof Support Vector Machines (SVM), Ordinary Binary Tree Classifiers (OBTC) and\nweighted k-nearest-neighbor (w/k-NN) models on benchmark datasets indicate that\nthis new adaptive WMR model, employing local accuracy estimators and the\nanalytically computed optimal weights outperform all the other simple\ncombination rules.\n", "versions": [{"version": "v1", "created": "Sun, 3 Feb 2013 22:12:52 GMT"}], "update_date": "2013-02-05", "authors_parsed": [["Georgiou", "Harris V.", ""], ["Mavroforakis", "Michael E.", ""]]}, {"id": "1302.0723", "submitter": "Kian Hsiang Low", "authors": "Nannan Cao, Kian Hsiang Low, John M. Dolan", "title": "Multi-Robot Informative Path Planning for Active Sensing of\n  Environmental Phenomena: A Tale of Two Algorithms", "comments": "12th International Conference on Autonomous Agents and Multiagent\n  Systems (AAMAS 2013), Extended version with proofs, 15 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.MA cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A key problem of robotic environmental sensing and monitoring is that of\nactive sensing: How can a team of robots plan the most informative observation\npaths to minimize the uncertainty in modeling and predicting an environmental\nphenomenon? This paper presents two principled approaches to efficient\ninformation-theoretic path planning based on entropy and mutual information\ncriteria for in situ active sensing of an important broad class of\nwidely-occurring environmental phenomena called anisotropic fields. Our\nproposed algorithms are novel in addressing a trade-off between active sensing\nperformance and time efficiency. An important practical consequence is that our\nalgorithms can exploit the spatial correlation structure of Gaussian\nprocess-based anisotropic fields to improve time efficiency while preserving\nnear-optimal active sensing performance. We analyze the time complexity of our\nalgorithms and prove analytically that they scale better than state-of-the-art\nalgorithms with increasing planning horizon length. We provide theoretical\nguarantees on the active sensing performance of our algorithms for a class of\nexploration tasks called transect sampling, which, in particular, can be\nimproved with longer planning time and/or lower spatial correlation along the\ntransect. Empirical evaluation on real-world anisotropic field data shows that\nour algorithms can perform better or at least as well as the state-of-the-art\nalgorithms while often incurring a few orders of magnitude less computational\ntime, even when the field conditions are less favorable.\n", "versions": [{"version": "v1", "created": "Mon, 4 Feb 2013 15:34:12 GMT"}, {"version": "v2", "created": "Tue, 5 Feb 2013 05:50:14 GMT"}], "update_date": "2013-02-06", "authors_parsed": [["Cao", "Nannan", ""], ["Low", "Kian Hsiang", ""], ["Dolan", "John M.", ""]]}, {"id": "1302.0895", "submitter": "Ping Li", "authors": "Ping Li and Cun-Hui Zhang", "title": "Exact Sparse Recovery with L0 Projections", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.IT cs.LG math.IT math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many applications concern sparse signals, for example, detecting anomalies\nfrom the differences between consecutive images taken by surveillance cameras.\nThis paper focuses on the problem of recovering a K-sparse signal x in N\ndimensions. In the mainstream framework of compressed sensing (CS), the vector\nx is recovered from M non-adaptive linear measurements y = xS, where S (of size\nN x M) is typically a Gaussian (or Gaussian-like) design matrix, through some\noptimization procedure such as linear programming (LP).\n  In our proposed method, the design matrix S is generated from an\n$\\alpha$-stable distribution with $\\alpha\\approx 0$. Our decoding algorithm\nmainly requires one linear scan of the coordinates, followed by a few\niterations on a small number of coordinates which are \"undetermined\" in the\nprevious iteration. Comparisons with two strong baselines, linear programming\n(LP) and orthogonal matching pursuit (OMP), demonstrate that our algorithm can\nbe significantly faster in decoding speed and more accurate in recovery\nquality, for the task of exact spare recovery. Our procedure is robust against\nmeasurement noise. Even when there are no sufficient measurements, our\nalgorithm can still reliably recover a significant portion of the nonzero\ncoordinates.\n  To provide the intuition for understanding our method, we also analyze the\nprocedure by assuming an idealistic setting. Interestingly, when K=2, the\n\"idealized\" algorithm achieves exact recovery with merely 3 measurements,\nregardless of N. For general K, the required sample size of the \"idealized\"\nalgorithm is about 5K.\n", "versions": [{"version": "v1", "created": "Mon, 4 Feb 2013 22:51:56 GMT"}], "update_date": "2013-02-06", "authors_parsed": [["Li", "Ping", ""], ["Zhang", "Cun-Hui", ""]]}, {"id": "1302.0962", "submitter": "Veenu  Mangat", "authors": "Savinderjit Kaur (Department of Information Technology, UIET, PU,\n  Chandigarh, India), Veenu Mangat (Department of Information Technology, UIET,\n  PU, Chandigarh, India)", "title": "Improved Accuracy of PSO and DE using Normalization: an Application to\n  Stock Price Prediction", "comments": null, "journal-ref": "(IJACSA) International Journal of Advanced Computer Science and\n  Applications, Vol. 3, No. 9, 2012", "doi": null, "report-no": null, "categories": "cs.NE cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data Mining is being actively applied to stock market since 1980s. It has\nbeen used to predict stock prices, stock indexes, for portfolio management,\ntrend detection and for developing recommender systems. The various algorithms\nwhich have been used for the same include ANN, SVM, ARIMA, GARCH etc. Different\nhybrid models have been developed by combining these algorithms with other\nalgorithms like roughest, fuzzy logic, GA, PSO, DE, ACO etc. to improve the\nefficiency. This paper proposes DE-SVM model (Differential EvolutionSupport\nvector Machine) for stock price prediction. DE has been used to select best\nfree parameters combination for SVM to improve results. The paper also compares\nthe results of prediction with the outputs of SVM alone and PSO-SVM model\n(Particle Swarm Optimization). The effect of normalization of data on the\naccuracy of prediction has also been studied.\n", "versions": [{"version": "v1", "created": "Tue, 5 Feb 2013 09:01:13 GMT"}], "update_date": "2013-02-06", "authors_parsed": [["Kaur", "Savinderjit", "", "Department of Information Technology, UIET, PU,\n  Chandigarh, India"], ["Mangat", "Veenu", "", "Department of Information Technology, UIET,\n  PU, Chandigarh, India"]]}, {"id": "1302.0963", "submitter": "Chunhua Shen", "authors": "Sakrapee Paisitkriangkrai, Chunhua Shen, Qinfeng Shi, Anton van den\n  Hengel", "title": "RandomBoost: Simplified Multi-class Boosting through Randomization", "comments": "15 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel boosting approach to multi-class classification problems,\nin which multiple classes are distinguished by a set of random projection\nmatrices in essence. The approach uses random projections to alleviate the\nproliferation of binary classifiers typically required to perform multi-class\nclassification. The result is a multi-class classifier with a single\nvector-valued parameter, irrespective of the number of classes involved. Two\nvariants of this approach are proposed. The first method randomly projects the\noriginal data into new spaces, while the second method randomly projects the\noutputs of learned weak classifiers. These methods are not only conceptually\nsimple but also effective and easy to implement. A series of experiments on\nsynthetic, machine learning and visual recognition data sets demonstrate that\nour proposed methods compare favorably to existing multi-class boosting\nalgorithms in terms of both the convergence rate and classification accuracy.\n", "versions": [{"version": "v1", "created": "Tue, 5 Feb 2013 09:04:25 GMT"}], "update_date": "2013-02-06", "authors_parsed": [["Paisitkriangkrai", "Sakrapee", ""], ["Shen", "Chunhua", ""], ["Shi", "Qinfeng", ""], ["Hengel", "Anton van den", ""]]}, {"id": "1302.0974", "submitter": "Jan Rupnik", "authors": "Jan Rupnik, Primoz Skraba, John Shawe-Taylor, Sabrina Guettes", "title": "A Comparison of Relaxations of Multiset Cannonical Correlation Analysis\n  and Applications", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Canonical correlation analysis is a statistical technique that is used to\nfind relations between two sets of variables. An important extension in pattern\nanalysis is to consider more than two sets of variables. This problem can be\nexpressed as a quadratically constrained quadratic program (QCQP), commonly\nreferred to Multi-set Canonical Correlation Analysis (MCCA). This is a\nnon-convex problem and so greedy algorithms converge to local optima without\nany guarantees on global optimality. In this paper, we show that despite being\nhighly structured, finding the optimal solution is NP-Hard. This motivates our\nrelaxation of the QCQP to a semidefinite program (SDP). The SDP is convex, can\nbe solved reasonably efficiently and comes with both absolute and\noutput-sensitive approximation quality. In addition to theoretical guarantees,\nwe do an extensive comparison of the QCQP method and the SDP relaxation on a\nvariety of synthetic and real world data. Finally, we present two useful\nextensions: we incorporate kernel methods and computing multiple sets of\ncanonical vectors.\n", "versions": [{"version": "v1", "created": "Tue, 5 Feb 2013 09:45:21 GMT"}], "update_date": "2013-02-06", "authors_parsed": [["Rupnik", "Jan", ""], ["Skraba", "Primoz", ""], ["Shawe-Taylor", "John", ""], ["Guettes", "Sabrina", ""]]}, {"id": "1302.1043", "submitter": "Amit Daniely", "authors": "Amit Daniely and Tom Helbertal", "title": "The price of bandit information in multiclass online classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider two scenarios of multiclass online learning of a hypothesis class\n$H\\subseteq Y^X$. In the {\\em full information} scenario, the learner is\nexposed to instances together with their labels. In the {\\em bandit} scenario,\nthe true label is not exposed, but rather an indication whether the learner's\nprediction is correct or not. We show that the ratio between the error rates in\nthe two scenarios is at most $8\\cdot|Y|\\cdot \\log(|Y|)$ in the realizable case,\nand $\\tilde{O}(\\sqrt{|Y|})$ in the agnostic case. The results are tight up to a\nlogarithmic factor and essentially answer an open question from (Daniely et.\nal. - Multiclass learnability and the erm principle).\n  We apply these results to the class of $\\gamma$-margin multiclass linear\nclassifiers in $\\reals^d$. We show that the bandit error rate of this class is\n$\\tilde{\\Theta}(\\frac{|Y|}{\\gamma^2})$ in the realizable case and\n$\\tilde{\\Theta}(\\frac{1}{\\gamma}\\sqrt{|Y|T})$ in the agnostic case. This\nresolves an open question from (Kakade et. al. - Efficient bandit algorithms\nfor online multiclass prediction).\n", "versions": [{"version": "v1", "created": "Tue, 5 Feb 2013 14:31:51 GMT"}, {"version": "v2", "created": "Tue, 9 Jul 2013 08:04:02 GMT"}], "update_date": "2013-07-10", "authors_parsed": [["Daniely", "Amit", ""], ["Helbertal", "Tom", ""]]}, {"id": "1302.1232", "submitter": "N. Raj Rao", "authors": "Raj Rao Nadakuditi", "title": "When are the most informative components for inference also the\n  principal components?", "comments": "Submitted to IEEE Transactions on Information Theory", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.DS cs.IT cs.LG math.IT math.PR stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Which components of the singular value decomposition of a signal-plus-noise\ndata matrix are most informative for the inferential task of detecting or\nestimating an embedded low-rank signal matrix? Principal component analysis\nascribes greater importance to the components that capture the greatest\nvariation, i.e., the singular vectors associated with the largest singular\nvalues. This choice is often justified by invoking the Eckart-Young theorem\neven though that work addresses the problem of how to best represent a\nsignal-plus-noise matrix using a low-rank approximation and not how to\nbest_infer_ the underlying low-rank signal component.\n  Here we take a first-principles approach in which we start with a\nsignal-plus-noise data matrix and show how the spectrum of the noise-only\ncomponent governs whether the principal or the middle components of the\nsingular value decomposition of the data matrix will be the informative\ncomponents for inference. Simply put, if the noise spectrum is supported on a\nconnected interval, in a sense we make precise, then the use of the principal\ncomponents is justified. When the noise spectrum is supported on multiple\nintervals, then the middle components might be more informative than the\nprincipal components.\n  The end result is a proper justification of the use of principal components\nin the setting where the noise matrix is i.i.d. Gaussian and the identification\nof scenarios, generically involving heterogeneous noise models such as mixtures\nof Gaussians, where the middle components might be more informative than the\nprincipal components so that they may be exploited to extract additional\nprocessing gain. Our results show how the blind use of principal components can\nlead to suboptimal or even faulty inference because of phase transitions that\nseparate a regime where the principal components are informative from a regime\nwhere they are uninformative.\n", "versions": [{"version": "v1", "created": "Tue, 5 Feb 2013 23:20:45 GMT"}], "update_date": "2013-02-07", "authors_parsed": [["Nadakuditi", "Raj Rao", ""]]}, {"id": "1302.1515", "submitter": "Ankur Moitra", "authors": "Ankur Moitra, Michael Saks", "title": "A Polynomial Time Algorithm for Lossy Population Recovery", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We give a polynomial time algorithm for the lossy population recovery\nproblem. In this problem, the goal is to approximately learn an unknown\ndistribution on binary strings of length $n$ from lossy samples: for some\nparameter $\\mu$ each coordinate of the sample is preserved with probability\n$\\mu$ and otherwise is replaced by a `?'. The running time and number of\nsamples needed for our algorithm is polynomial in $n$ and $1/\\varepsilon$ for\neach fixed $\\mu>0$. This improves on algorithm of Wigderson and Yehudayoff that\nruns in quasi-polynomial time for any $\\mu > 0$ and the polynomial time\nalgorithm of Dvir et al which was shown to work for $\\mu \\gtrapprox 0.30$ by\nBatman et al. In fact, our algorithm also works in the more general framework\nof Batman et al. in which there is no a priori bound on the size of the support\nof the distribution. The algorithm we analyze is implicit in previous work; our\nmain contribution is to analyze the algorithm by showing (via linear\nprogramming duality and connections to complex analysis) that a certain matrix\nassociated with the problem has a robust local inverse even though its\ncondition number is exponentially small. A corollary of our result is the first\npolynomial time algorithm for learning DNFs in the restriction access model of\nDvir et al.\n", "versions": [{"version": "v1", "created": "Wed, 6 Feb 2013 20:53:35 GMT"}, {"version": "v2", "created": "Wed, 10 Jul 2013 15:21:41 GMT"}], "update_date": "2013-07-11", "authors_parsed": [["Moitra", "Ankur", ""], ["Saks", "Michael", ""]]}, {"id": "1302.1519", "submitter": "Eric Bauer", "authors": "Eric Bauer, Daphne Koller, Yoram Singer", "title": "Update Rules for Parameter Estimation in Bayesian Networks", "comments": "Appears in Proceedings of the Thirteenth Conference on Uncertainty in\n  Artificial Intelligence (UAI1997)", "journal-ref": null, "doi": null, "report-no": "UAI-P-1997-PG-3-13", "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper re-examines the problem of parameter estimation in Bayesian\nnetworks with missing values and hidden variables from the perspective of\nrecent work in on-line learning [Kivinen & Warmuth, 1994]. We provide a unified\nframework for parameter estimation that encompasses both on-line learning,\nwhere the model is continuously adapted to new data cases as they arrive, and\nthe more traditional batch learning, where a pre-accumulated set of samples is\nused in a one-time model selection process. In the batch case, our framework\nencompasses both the gradient projection algorithm and the EM algorithm for\nBayesian networks. The framework also leads to new on-line and batch parameter\nupdate schemes, including a parameterized version of EM. We provide both\nempirical and theoretical results indicating that parameterized EM allows\nfaster convergence to the maximum likelihood parameters than does standard EM.\n", "versions": [{"version": "v1", "created": "Wed, 6 Feb 2013 15:53:33 GMT"}], "update_date": "2013-02-08", "authors_parsed": [["Bauer", "Eric", ""], ["Koller", "Daphne", ""], ["Singer", "Yoram", ""]]}, {"id": "1302.1528", "submitter": "Max Chickering", "authors": "David Maxwell Chickering, David Heckerman, Christopher Meek", "title": "A Bayesian Approach to Learning Bayesian Networks with Local Structure", "comments": "Appears in Proceedings of the Thirteenth Conference on Uncertainty in\n  Artificial Intelligence (UAI1997)", "journal-ref": null, "doi": null, "report-no": "UAI-P-1997-PG-80-89", "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently several researchers have investigated techniques for using data to\nlearn Bayesian networks containing compact representations for the conditional\nprobability distributions (CPDs) stored at each node. The majority of this work\nhas concentrated on using decision-tree representations for the CPDs. In\naddition, researchers typically apply non-Bayesian (or asymptotically Bayesian)\nscoring functions such as MDL to evaluate the goodness-of-fit of networks to\nthe data. In this paper we investigate a Bayesian approach to learning Bayesian\nnetworks that contain the more general decision-graph representations of the\nCPDs. First, we describe how to evaluate the posterior probability that is, the\nBayesian score of such a network, given a database of observed cases. Second,\nwe describe various search spaces that can be used, in conjunction with a\nscoring function and a search procedure, to identify one or more high-scoring\nnetworks. Finally, we present an experimental evaluation of the search spaces,\nusing a greedy algorithm and a Bayesian scoring function.\n", "versions": [{"version": "v1", "created": "Wed, 6 Feb 2013 15:54:25 GMT"}, {"version": "v2", "created": "Sat, 16 May 2015 23:29:15 GMT"}], "update_date": "2015-05-19", "authors_parsed": [["Chickering", "David Maxwell", ""], ["Heckerman", "David", ""], ["Meek", "Christopher", ""]]}, {"id": "1302.1529", "submitter": "TongSheng Chu", "authors": "TongSheng Chu, Yang Xiang", "title": "Exploring Parallelism in Learning Belief Networks", "comments": "Appears in Proceedings of the Thirteenth Conference on Uncertainty in\n  Artificial Intelligence (UAI1997)", "journal-ref": null, "doi": null, "report-no": "UAI-P-1997-PG-90-98", "categories": "cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It has been shown that a class of probabilistic domain models cannot be\nlearned correctly by several existing algorithms which employ a single-link\nlook ahead search. When a multi-link look ahead search is used, the\ncomputational complexity of the learning algorithm increases. We study how to\nuse parallelism to tackle the increased complexity in learning such models and\nto speed up learning in large domains. An algorithm is proposed to decompose\nthe learning task for parallel processing. A further task decomposition is used\nto balance load among processors and to increase the speed-up and efficiency.\nFor learning from very large datasets, we present a regrouping of the available\nprocessors such that slow data access through file can be replaced by fast\nmemory access. Our implementation in a parallel computer demonstrates the\neffectiveness of the algorithm.\n", "versions": [{"version": "v1", "created": "Wed, 6 Feb 2013 15:54:31 GMT"}], "update_date": "2013-02-08", "authors_parsed": [["Chu", "TongSheng", ""], ["Xiang", "Yang", ""]]}, {"id": "1302.1538", "submitter": "Nir Friedman", "authors": "Nir Friedman, Moises Goldszmidt", "title": "Sequential Update of Bayesian Network Structure", "comments": "Appears in Proceedings of the Thirteenth Conference on Uncertainty in\n  Artificial Intelligence (UAI1997)", "journal-ref": null, "doi": null, "report-no": "UAI-P-1997-PG-165-174", "categories": "cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There is an obvious need for improving the performance and accuracy of a\nBayesian network as new data is observed. Because of errors in model\nconstruction and changes in the dynamics of the domains, we cannot afford to\nignore the information in new data. While sequential update of parameters for a\nfixed structure can be accomplished using standard techniques, sequential\nupdate of network structure is still an open problem. In this paper, we\ninvestigate sequential update of Bayesian networks were both parameters and\nstructure are expected to change. We introduce a new approach that allows for\nthe flexible manipulation of the tradeoff between the quality of the learned\nnetworks and the amount of information that is maintained about past\nobservations. We formally describe our approach including the necessary\nmodifications to the scoring functions for learning Bayesian networks, evaluate\nits effectiveness through an empirical study, and extend it to the case of\nmissing data.\n", "versions": [{"version": "v1", "created": "Wed, 6 Feb 2013 15:55:21 GMT"}], "update_date": "2013-02-08", "authors_parsed": [["Friedman", "Nir", ""], ["Goldszmidt", "Moises", ""]]}, {"id": "1302.1542", "submitter": "Russell Greiner", "authors": "Russell Greiner, Adam J. Grove, Dale Schuurmans", "title": "Learning Bayesian Nets that Perform Well", "comments": "Appears in Proceedings of the Thirteenth Conference on Uncertainty in\n  Artificial Intelligence (UAI1997)", "journal-ref": null, "doi": null, "report-no": "UAI-P-1997-PG-198-207", "categories": "cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A Bayesian net (BN) is more than a succinct way to encode a probabilistic\ndistribution; it also corresponds to a function used to answer queries. A BN\ncan therefore be evaluated by the accuracy of the answers it returns. Many\nalgorithms for learning BNs, however, attempt to optimize another criterion\n(usually likelihood, possibly augmented with a regularizing term), which is\nindependent of the distribution of queries that are posed. This paper takes the\n\"performance criteria\" seriously, and considers the challenge of computing the\nBN whose performance - read \"accuracy over the distribution of queries\" - is\noptimal. We show that many aspects of this learning task are more difficult\nthan the corresponding subtasks in the standard model.\n", "versions": [{"version": "v1", "created": "Wed, 6 Feb 2013 15:55:43 GMT"}], "update_date": "2013-02-08", "authors_parsed": [["Greiner", "Russell", ""], ["Grove", "Adam J.", ""], ["Schuurmans", "Dale", ""]]}, {"id": "1302.1545", "submitter": "David Heckerman", "authors": "David Heckerman, Christopher Meek", "title": "Models and Selection Criteria for Regression and Classification", "comments": "Appears in Proceedings of the Thirteenth Conference on Uncertainty in\n  Artificial Intelligence (UAI1997)", "journal-ref": null, "doi": null, "report-no": "UAI-P-1997-PG-223-228", "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When performing regression or classification, we are interested in the\nconditional probability distribution for an outcome or class variable Y given a\nset of explanatoryor input variables X. We consider Bayesian models for this\ntask. In particular, we examine a special class of models, which we call\nBayesian regression/classification (BRC) models, that can be factored into\nindependent conditional (y|x) and input (x) models. These models are\nconvenient, because the conditional model (the portion of the full model that\nwe care about) can be analyzed by itself. We examine the practice of\ntransforming arbitrary Bayesian models to BRC models, and argue that this\npractice is often inappropriate because it ignores prior knowledge that may be\nimportant for learning. In addition, we examine Bayesian methods for learning\nmodels from data. We discuss two criteria for Bayesian model selection that are\nappropriate for repression/classification: one described by Spiegelhalter et\nal. (1993), and another by Buntine (1993). We contrast these two criteria using\nthe prequential framework of Dawid (1984), and give sufficient conditions under\nwhich the criteria agree.\n", "versions": [{"version": "v1", "created": "Wed, 6 Feb 2013 15:56:07 GMT"}], "update_date": "2013-02-08", "authors_parsed": [["Heckerman", "David", ""], ["Meek", "Christopher", ""]]}, {"id": "1302.1549", "submitter": "Jun Hu", "authors": "Jun Hu, Yang Xiang", "title": "Learning Belief Networks in Domains with Recursively Embedded Pseudo\n  Independent Submodels", "comments": "Appears in Proceedings of the Thirteenth Conference on Uncertainty in\n  Artificial Intelligence (UAI1997)", "journal-ref": null, "doi": null, "report-no": "UAI-P-1997-PG-258-265", "categories": "cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A pseudo independent (PI) model is a probabilistic domain model (PDM) where\nproper subsets of a set of collectively dependent variables display marginal\nindependence. PI models cannot be learned correctly by many algorithms that\nrely on a single link search. Earlier work on learning PI models has suggested\na straightforward multi-link search algorithm. However, when a domain contains\nrecursively embedded PI submodels, it may escape the detection of such an\nalgorithm. In this paper, we propose an improved algorithm that ensures the\nlearning of all embedded PI submodels whose sizes are upper bounded by a\npredetermined parameter. We show that this improved learning capability only\nincreases the complexity slightly beyond that of the previous algorithm. The\nperformance of the new algorithm is demonstrated through experiment.\n", "versions": [{"version": "v1", "created": "Wed, 6 Feb 2013 15:56:57 GMT"}], "update_date": "2013-02-08", "authors_parsed": [["Hu", "Jun", ""], ["Xiang", "Yang", ""]]}, {"id": "1302.1552", "submitter": "Michael Kearns", "authors": "Michael Kearns, Yishay Mansour, Andrew Y. Ng", "title": "An Information-Theoretic Analysis of Hard and Soft Assignment Methods\n  for Clustering", "comments": "Appears in Proceedings of the Thirteenth Conference on Uncertainty in\n  Artificial Intelligence (UAI1997)", "journal-ref": null, "doi": null, "report-no": "UAI-P-1997-PG-282-293", "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Assignment methods are at the heart of many algorithms for unsupervised\nlearning and clustering - in particular, the well-known K-means and\nExpectation-Maximization (EM) algorithms. In this work, we study several\ndifferent methods of assignment, including the \"hard\" assignments used by\nK-means and the ?soft' assignments used by EM. While it is known that K-means\nminimizes the distortion on the data and EM maximizes the likelihood, little is\nknown about the systematic differences of behavior between the two algorithms.\nHere we shed light on these differences via an information-theoretic analysis.\nThe cornerstone of our results is a simple decomposition of the expected\ndistortion, showing that K-means (and its extension for inferring general\nparametric densities from unlabeled sample data) must implicitly manage a\ntrade-off between how similar the data assigned to each cluster are, and how\nthe data are balanced among the clusters. How well the data are balanced is\nmeasured by the entropy of the partition defined by the hard assignments. In\naddition to letting us predict and verify systematic differences between\nK-means and EM on specific examples, the decomposition allows us to give a\nrather general argument showing that K ?means will consistently find densities\nwith less \"overlap\" than EM. We also study a third natural assignment method\nthat we call posterior assignment, that is close in spirit to the soft\nassignments of EM, but leads to a surprisingly different algorithm.\n", "versions": [{"version": "v1", "created": "Wed, 6 Feb 2013 15:57:20 GMT"}], "update_date": "2013-02-08", "authors_parsed": [["Kearns", "Michael", ""], ["Mansour", "Yishay", ""], ["Ng", "Andrew Y.", ""]]}, {"id": "1302.1561", "submitter": "Chris Meek", "authors": "Christopher Meek, David Heckerman", "title": "Structure and Parameter Learning for Causal Independence and Causal\n  Interaction Models", "comments": "Appears in Proceedings of the Thirteenth Conference on Uncertainty in\n  Artificial Intelligence (UAI1997)", "journal-ref": null, "doi": null, "report-no": "UAI-P-1997-PG-366-375", "categories": "cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper discusses causal independence models and a generalization of these\nmodels called causal interaction models. Causal interaction models are models\nthat have independent mechanisms where a mechanism can have several causes. In\naddition to introducing several particular types of causal interaction models,\nwe show how we can apply the Bayesian approach to learning causal interaction\nmodels obtaining approximate posterior distributions for the models and obtain\nMAP and ML estimates for the parameters. We illustrate the approach with a\nsimulation study of learning model posteriors.\n", "versions": [{"version": "v1", "created": "Wed, 6 Feb 2013 15:58:24 GMT"}, {"version": "v2", "created": "Sat, 16 May 2015 23:30:56 GMT"}], "update_date": "2015-05-19", "authors_parsed": [["Meek", "Christopher", ""], ["Heckerman", "David", ""]]}, {"id": "1302.1565", "submitter": "Marco Ramoni", "authors": "Marco Ramoni, Paola Sebastiani", "title": "Learning Bayesian Networks from Incomplete Databases", "comments": "Appears in Proceedings of the Thirteenth Conference on Uncertainty in\n  Artificial Intelligence (UAI1997)", "journal-ref": null, "doi": null, "report-no": "UAI-P-1997-PG-401-408", "categories": "cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bayesian approaches to learn the graphical structure of Bayesian Belief\nNetworks (BBNs) from databases share the assumption that the database is\ncomplete, that is, no entry is reported as unknown. Attempts to relax this\nassumption involve the use of expensive iterative methods to discriminate among\ndifferent structures. This paper introduces a deterministic method to learn the\ngraphical structure of a BBN from a possibly incomplete database. Experimental\nevaluations show a significant robustness of this method and a remarkable\nindependence of its execution time from the number of missing data.\n", "versions": [{"version": "v1", "created": "Wed, 6 Feb 2013 15:58:45 GMT"}], "update_date": "2013-02-08", "authors_parsed": [["Ramoni", "Marco", ""], ["Sebastiani", "Paola", ""]]}, {"id": "1302.1611", "submitter": "Philippe Rigollet", "authors": "S\\'ebastien Bubeck, Vianney Perchet and Philippe Rigollet", "title": "Bounded regret in stochastic multi-armed bandits", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.LG stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the stochastic multi-armed bandit problem when one knows the value\n$\\mu^{(\\star)}$ of an optimal arm, as a well as a positive lower bound on the\nsmallest positive gap $\\Delta$. We propose a new randomized policy that attains\na regret {\\em uniformly bounded over time} in this setting. We also prove\nseveral lower bounds, which show in particular that bounded regret is not\npossible if one only knows $\\Delta$, and bounded regret of order $1/\\Delta$ is\nnot possible if one only knows $\\mu^{(\\star)}$\n", "versions": [{"version": "v1", "created": "Wed, 6 Feb 2013 23:20:20 GMT"}, {"version": "v2", "created": "Tue, 12 Feb 2013 15:48:55 GMT"}], "update_date": "2013-02-13", "authors_parsed": [["Bubeck", "S\u00e9bastien", ""], ["Perchet", "Vianney", ""], ["Rigollet", "Philippe", ""]]}, {"id": "1302.1733", "submitter": "Llu\\'is Belanche-Mu\\~noz", "authors": "Fernando Gonz\\'alez, Llu\\'is A. Belanche", "title": "Feature Selection for Microarray Gene Expression Data using Simulated\n  Annealing guided by the Multivariate Joint Entropy", "comments": "12 pages, 6 Tables, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.QM cs.CE cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work a new way to calculate the multivariate joint entropy is\npresented. This measure is the basis for a fast information-theoretic based\nevaluation of gene relevance in a Microarray Gene Expression data context. Its\nlow complexity is based on the reuse of previous computations to calculate\ncurrent feature relevance. The mu-TAFS algorithm --named as such to\ndifferentiate it from previous TAFS algorithms-- implements a simulated\nannealing technique specially designed for feature subset selection. The\nalgorithm is applied to the maximization of gene subset relevance in several\npublic-domain microarray data sets. The experimental results show a notoriously\nhigh classification performance and low size subsets formed by biologically\nmeaningful genes.\n", "versions": [{"version": "v1", "created": "Thu, 7 Feb 2013 12:49:57 GMT"}], "update_date": "2013-02-08", "authors_parsed": [["Gonz\u00e1lez", "Fernando", ""], ["Belanche", "Llu\u00eds A.", ""]]}, {"id": "1302.1772", "submitter": "Vahid Majidnezhad", "authors": "Vahid Majidnezhad and Igor Kheidorov", "title": "An ANN-based Method for Detecting Vocal Fold Pathology", "comments": "4 pages, 3 figures, Published with International Journal of Computer\n  Applications (IJCA)", "journal-ref": "International Journal of Computer Applications 62(7):1-4, January\n  2013", "doi": "10.5120/10089-4722", "report-no": null, "categories": "cs.LG cs.CV cs.SD", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There are different algorithms for vocal fold pathology diagnosis. These\nalgorithms usually have three stages which are Feature Extraction, Feature\nReduction and Classification. While the third stage implies a choice of a\nvariety of machine learning methods, the first and second stages play a\ncritical role in performance and accuracy of the classification system. In this\npaper we present initial study of feature extraction and feature reduction in\nthe task of vocal fold pathology diagnosis. A new type of feature vector, based\non wavelet packet decomposition and Mel-Frequency-Cepstral-Coefficients\n(MFCCs), is proposed. Also Principal Component Analysis (PCA) is used for\nfeature reduction. An Artificial Neural Network is used as a classifier for\nevaluating the performance of our proposed method.\n", "versions": [{"version": "v1", "created": "Thu, 7 Feb 2013 15:03:24 GMT"}], "update_date": "2013-02-08", "authors_parsed": [["Majidnezhad", "Vahid", ""], ["Kheidorov", "Igor", ""]]}, {"id": "1302.2157", "submitter": "Mehrdad Mahdavi", "authors": "Mehrdad Mahdavi and Rong Jin", "title": "Passive Learning with Target Risk", "comments": null, "journal-ref": "Conference on Learning Theory (COLT 2013)", "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we consider learning in passive setting but with a slight\nmodification. We assume that the target expected loss, also referred to as\ntarget risk, is provided in advance for learner as prior knowledge. Unlike most\nstudies in the learning theory that only incorporate the prior knowledge into\nthe generalization bounds, we are able to explicitly utilize the target risk in\nthe learning process. Our analysis reveals a surprising result on the sample\ncomplexity of learning: by exploiting the target risk in the learning\nalgorithm, we show that when the loss function is both strongly convex and\nsmooth, the sample complexity reduces to $\\O(\\log (\\frac{1}{\\epsilon}))$, an\nexponential improvement compared to the sample complexity\n$\\O(\\frac{1}{\\epsilon})$ for learning with strongly convex loss functions.\nFurthermore, our proof is constructive and is based on a computationally\nefficient stochastic optimization algorithm for such settings which demonstrate\nthat the proposed algorithm is practically useful.\n", "versions": [{"version": "v1", "created": "Fri, 8 Feb 2013 21:18:24 GMT"}, {"version": "v2", "created": "Sun, 19 May 2013 00:39:52 GMT"}], "update_date": "2013-05-21", "authors_parsed": [["Mahdavi", "Mehrdad", ""], ["Jin", "Rong", ""]]}, {"id": "1302.2176", "submitter": "Hugh Brendan McMahan", "authors": "H. Brendan McMahan", "title": "Minimax Optimal Algorithms for Unconstrained Linear Optimization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We design and analyze minimax-optimal algorithms for online linear\noptimization games where the player's choice is unconstrained. The player\nstrives to minimize regret, the difference between his loss and the loss of a\npost-hoc benchmark strategy. The standard benchmark is the loss of the best\nstrategy chosen from a bounded comparator set. When the the comparison set and\nthe adversary's gradients satisfy L_infinity bounds, we give the value of the\ngame in closed form and prove it approaches sqrt(2T/pi) as T -> infinity.\n  Interesting algorithms result when we consider soft constraints on the\ncomparator, rather than restricting it to a bounded set. As a warmup, we\nanalyze the game with a quadratic penalty. The value of this game is exactly\nT/2, and this value is achieved by perhaps the simplest online algorithm of\nall: unprojected gradient descent with a constant learning rate.\n  We then derive a minimax-optimal algorithm for a much softer penalty\nfunction. This algorithm achieves good bounds under the standard notion of\nregret for any comparator point, without needing to specify the comparator set\nin advance. The value of this game converges to sqrt{e} as T ->infinity; we\ngive a closed-form for the exact value as a function of T. The resulting\nalgorithm is natural in unconstrained investment or betting scenarios, since it\nguarantees at worst constant loss, while allowing for exponential reward\nagainst an \"easy\" adversary.\n", "versions": [{"version": "v1", "created": "Fri, 8 Feb 2013 23:16:04 GMT"}], "update_date": "2013-02-12", "authors_parsed": [["McMahan", "H. Brendan", ""]]}, {"id": "1302.2273", "submitter": "Pranav Garg", "authors": "Pranav Garg, Christof Loding, P. Madhusudan, Daniel Neider", "title": "Learning Universally Quantified Invariants of Linear Data Structures", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.FL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new automaton model, called quantified data automata over words,\nthat can model quantified invariants over linear data structures, and build\npoly-time active learning algorithms for them, where the learner is allowed to\nquery the teacher with membership and equivalence queries. In order to express\ninvariants in decidable logics, we invent a decidable subclass of QDAs, called\nelastic QDAs, and prove that every QDA has a unique\nminimally-over-approximating elastic QDA. We then give an application of these\ntheoretically sound and efficient active learning algorithms in a passive\nlearning framework and show that we can efficiently learn quantified linear\ndata structure invariants from samples obtained from dynamic runs for a large\nclass of programs.\n", "versions": [{"version": "v1", "created": "Sat, 9 Feb 2013 21:41:03 GMT"}], "update_date": "2013-02-12", "authors_parsed": [["Garg", "Pranav", ""], ["Loding", "Christof", ""], ["Madhusudan", "P.", ""], ["Neider", "Daniel", ""]]}, {"id": "1302.2277", "submitter": "Houtao Deng", "authors": "Houtao Deng, George Runger, Eugene Tuv, Martyanov Vladimir", "title": "A Time Series Forest for Classification and Feature Extraction", "comments": null, "journal-ref": "Information Sciences 239: 142-153 (2013)", "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a tree ensemble method, referred to as time series forest (TSF),\nfor time series classification. TSF employs a combination of the entropy gain\nand a distance measure, referred to as the Entrance (entropy and distance)\ngain, for evaluating the splits. Experimental studies show that the Entrance\ngain criterion improves the accuracy of TSF. TSF randomly samples features at\neach tree node and has a computational complexity linear in the length of a\ntime series and can be built using parallel computing techniques such as\nmulti-core computing used here. The temporal importance curve is also proposed\nto capture the important temporal characteristics useful for classification.\nExperimental studies show that TSF using simple features such as mean,\ndeviation and slope outperforms strong competitors such as one-nearest-neighbor\nclassifiers with dynamic time warping, is computationally efficient, and can\nprovide insights into the temporal characteristics.\n", "versions": [{"version": "v1", "created": "Sat, 9 Feb 2013 22:56:45 GMT"}, {"version": "v2", "created": "Mon, 18 Feb 2013 00:10:56 GMT"}], "update_date": "2013-06-04", "authors_parsed": [["Deng", "Houtao", ""], ["Runger", "George", ""], ["Tuv", "Eugene", ""], ["Vladimir", "Martyanov", ""]]}, {"id": "1302.2436", "submitter": "Mahmood Ali Mohd", "authors": "Mohd Mahmood Ali, Mohd S Qaseem, Lakshmi Rajamani, A Govardhan", "title": "Extracting useful rules through improved decision tree induction using\n  information entropy", "comments": "15 pages, 7 figures, 4 tables, International Journal of Information\n  Sciences and Techniques (IJIST) Vol.3, No.1, January 2013", "journal-ref": null, "doi": "10.5121/ijist.2013.3103", "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Classification is widely used technique in the data mining domain, where\nscalability and efficiency are the immediate problems in classification\nalgorithms for large databases. We suggest improvements to the existing C4.5\ndecision tree algorithm. In this paper attribute oriented induction (AOI) and\nrelevance analysis are incorporated with concept hierarchys knowledge and\nHeightBalancePriority algorithm for construction of decision tree along with\nMulti level mining. The assignment of priorities to attributes is done by\nevaluating information entropy, at different levels of abstraction for building\ndecision tree using HeightBalancePriority algorithm. Modified DMQL queries are\nused to understand and explore the shortcomings of the decision trees generated\nby C4.5 classifier for education dataset and the results are compared with the\nproposed approach.\n", "versions": [{"version": "v1", "created": "Mon, 11 Feb 2013 10:29:17 GMT"}], "update_date": "2013-02-12", "authors_parsed": [["Ali", "Mohd Mahmood", ""], ["Qaseem", "Mohd S", ""], ["Rajamani", "Lakshmi", ""], ["Govardhan", "A", ""]]}, {"id": "1302.2550", "submitter": "Daniil Ryabko", "authors": "Ronald Ortner and Daniil Ryabko", "title": "Online Regret Bounds for Undiscounted Continuous Reinforcement Learning", "comments": null, "journal-ref": "in proceedings of NIPS 2012, pp. 1772--1780", "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We derive sublinear regret bounds for undiscounted reinforcement learning in\ncontinuous state space. The proposed algorithm combines state aggregation with\nthe use of upper confidence bounds for implementing optimism in the face of\nuncertainty. Beside the existence of an optimal policy which satisfies the\nPoisson equation, the only assumptions made are Holder continuity of rewards\nand transition probabilities.\n", "versions": [{"version": "v1", "created": "Mon, 11 Feb 2013 17:44:10 GMT"}], "update_date": "2013-02-12", "authors_parsed": [["Ortner", "Ronald", ""], ["Ryabko", "Daniil", ""]]}, {"id": "1302.2552", "submitter": "Daniil Ryabko", "authors": "Odalric-Ambrym Maillard, R\\'emi Munos, Daniil Ryabko", "title": "Selecting the State-Representation in Reinforcement Learning", "comments": null, "journal-ref": "NIPS 2011, pp. 2627-2635", "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The problem of selecting the right state-representation in a reinforcement\nlearning problem is considered. Several models (functions mapping past\nobservations to a finite set) of the observations are given, and it is known\nthat for at least one of these models the resulting state dynamics are indeed\nMarkovian. Without knowing neither which of the models is the correct one, nor\nwhat are the probabilistic characteristics of the resulting MDP, it is required\nto obtain as much reward as the optimal policy for the correct model (or for\nthe best of the correct models, if there are several). We propose an algorithm\nthat achieves that, with a regret of order T^{2/3} where T is the horizon time.\n", "versions": [{"version": "v1", "created": "Mon, 11 Feb 2013 17:49:38 GMT"}], "update_date": "2013-02-12", "authors_parsed": [["Maillard", "Odalric-Ambrym", ""], ["Munos", "R\u00e9mi", ""], ["Ryabko", "Daniil", ""]]}, {"id": "1302.2553", "submitter": "Daniil Ryabko", "authors": "Odalric-Ambrym Maillard, Phuong Nguyen, Ronald Ortner, Daniil Ryabko", "title": "Optimal Regret Bounds for Selecting the State Representation in\n  Reinforcement Learning", "comments": null, "journal-ref": "In Proceedings of ICML, JMLR W&CP 28(1):543-551, 2013", "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider an agent interacting with an environment in a single stream of\nactions, observations, and rewards, with no reset. This process is not assumed\nto be a Markov Decision Process (MDP). Rather, the agent has several\nrepresentations (mapping histories of past interactions to a discrete state\nspace) of the environment with unknown dynamics, only some of which result in\nan MDP. The goal is to minimize the average regret criterion against an agent\nwho knows an MDP representation giving the highest optimal reward, and acts\noptimally in it. Recent regret bounds for this setting are of order\n$O(T^{2/3})$ with an additive term constant yet exponential in some\ncharacteristics of the optimal MDP. We propose an algorithm whose regret after\n$T$ time steps is $O(\\sqrt{T})$, with all constants reasonably small. This is\noptimal in $T$ since $O(\\sqrt{T})$ is the optimal regret in the setting of\nlearning in a (single discrete) MDP.\n", "versions": [{"version": "v1", "created": "Mon, 11 Feb 2013 17:55:49 GMT"}, {"version": "v2", "created": "Mon, 18 Mar 2013 09:11:15 GMT"}], "update_date": "2013-03-19", "authors_parsed": [["Maillard", "Odalric-Ambrym", ""], ["Nguyen", "Phuong", ""], ["Ortner", "Ronald", ""], ["Ryabko", "Daniil", ""]]}, {"id": "1302.2576", "submitter": "Oluwasanmi Koyejo", "authors": "Oluwasanmi Koyejo and Cheng Lee and Joydeep Ghosh", "title": "The trace norm constrained matrix-variate Gaussian process for multitask\n  bipartite ranking", "comments": "14 pages, 9 figures, 5 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel hierarchical model for multitask bipartite ranking. The\nproposed approach combines a matrix-variate Gaussian process with a generative\nmodel for task-wise bipartite ranking. In addition, we employ a novel trace\nconstrained variational inference approach to impose low rank structure on the\nposterior matrix-variate Gaussian process. The resulting posterior covariance\nfunction is derived in closed form, and the posterior mean function is the\nsolution to a matrix-variate regression with a novel spectral elastic net\nregularizer. Further, we show that variational inference for the trace\nconstrained matrix-variate Gaussian process combined with maximum likelihood\nparameter estimation for the bipartite ranking model is jointly convex. Our\nmotivating application is the prioritization of candidate disease genes. The\ngoal of this task is to aid the identification of unobserved associations\nbetween human genes and diseases using a small set of observed associations as\nwell as kernels induced by gene-gene interaction networks and disease\nontologies. Our experimental results illustrate the performance of the proposed\nmodel on real world datasets. Moreover, we find that the resulting low rank\nsolution improves the computational scalability of training and testing as\ncompared to baseline models.\n", "versions": [{"version": "v1", "created": "Mon, 11 Feb 2013 19:16:25 GMT"}], "update_date": "2013-02-12", "authors_parsed": [["Koyejo", "Oluwasanmi", ""], ["Lee", "Cheng", ""], ["Ghosh", "Joydeep", ""]]}, {"id": "1302.2645", "submitter": "Alexander Gorban", "authors": "E. M. Mirkes, A. Zinovyev, A. N. Gorban", "title": "Geometrical complexity of data approximators", "comments": "10 pages, 3 figures, minor correction and extension", "journal-ref": "IWANN 2013, Advances in Computation Intelligence, Springer LNCS\n  7902, pp. 500-509, 2013", "doi": "10.1007/978-3-642-38679-4_50", "report-no": null, "categories": "stat.ML cs.LG", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  There are many methods developed to approximate a cloud of vectors embedded\nin high-dimensional space by simpler objects: starting from principal points\nand linear manifolds to self-organizing maps, neural gas, elastic maps, various\ntypes of principal curves and principal trees, and so on. For each type of\napproximators the measure of the approximator complexity was developed too.\nThese measures are necessary to find the balance between accuracy and\ncomplexity and to define the optimal approximations of a given type. We propose\na measure of complexity (geometrical complexity) which is applicable to\napproximators of several types and which allows comparing data approximations\nof different types.\n", "versions": [{"version": "v1", "created": "Mon, 11 Feb 2013 21:14:43 GMT"}, {"version": "v2", "created": "Sat, 4 May 2013 01:22:48 GMT"}], "update_date": "2016-09-01", "authors_parsed": [["Mirkes", "E. M.", ""], ["Zinovyev", "A.", ""], ["Gorban", "A. N.", ""]]}, {"id": "1302.2671", "submitter": "Yoon-Sik Cho", "authors": "Yoon-Sik Cho, Aram Galstyan, P. Jeffrey Brantingham, George Tita", "title": "Latent Self-Exciting Point Process Model for Spatial-Temporal Networks", "comments": "20 pages, 6 figures (v3); 11 pages, 6 figures (v2); previous version\n  appeared in the 9th Bayesian Modeling Applications Workshop, UAI'12", "journal-ref": "DISCRETE AND CONTINUOUS DYNAMICAL SYSTEMS SERIES B, Vol. 19, pp.\n  1335-1354, 2014", "doi": "10.3934/dcdsb.2014.19.1335", "report-no": null, "categories": "cs.SI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a latent self-exciting point process model that describes\ngeographically distributed interactions between pairs of entities. In contrast\nto most existing approaches that assume fully observable interactions, here we\nconsider a scenario where certain interaction events lack information about\nparticipants. Instead, this information needs to be inferred from the available\nobservations. We develop an efficient approximate algorithm based on\nvariational expectation-maximization to infer unknown participants in an event\ngiven the location and the time of the event. We validate the model on\nsynthetic as well as real-world data, and obtain very promising results on the\nidentity-inference task. We also use our model to predict the timing and\nparticipants of future events, and demonstrate that it compares favorably with\nbaseline approaches.\n", "versions": [{"version": "v1", "created": "Tue, 12 Feb 2013 00:01:02 GMT"}, {"version": "v2", "created": "Fri, 15 Feb 2013 18:02:36 GMT"}, {"version": "v3", "created": "Wed, 30 Apr 2014 23:42:52 GMT"}], "update_date": "2014-05-02", "authors_parsed": [["Cho", "Yoon-Sik", ""], ["Galstyan", "Aram", ""], ["Brantingham", "P. Jeffrey", ""], ["Tita", "George", ""]]}, {"id": "1302.2672", "submitter": "Alexander Rakhlin", "authors": "Wei Han, Alexander Rakhlin, Karthik Sridharan", "title": "Competing With Strategies", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.GT cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of online learning with a notion of regret defined with\nrespect to a set of strategies. We develop tools for analyzing the minimax\nrates and for deriving regret-minimization algorithms in this scenario. While\nthe standard methods for minimizing the usual notion of regret fail, through\nour analysis we demonstrate existence of regret-minimization methods that\ncompete with such sets of strategies as: autoregressive algorithms, strategies\nbased on statistical models, regularized least squares, and follow the\nregularized leader strategies. In several cases we also derive efficient\nlearning algorithms.\n", "versions": [{"version": "v1", "created": "Tue, 12 Feb 2013 00:14:44 GMT"}], "update_date": "2013-02-13", "authors_parsed": [["Han", "Wei", ""], ["Rakhlin", "Alexander", ""], ["Sridharan", "Karthik", ""]]}, {"id": "1302.2684", "submitter": "Rong Ge", "authors": "Anima Anandkumar, Rong Ge, Daniel Hsu, Sham M. Kakade", "title": "A Tensor Approach to Learning Mixed Membership Community Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.SI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Community detection is the task of detecting hidden communities from observed\ninteractions. Guaranteed community detection has so far been mostly limited to\nmodels with non-overlapping communities such as the stochastic block model. In\nthis paper, we remove this restriction, and provide guaranteed community\ndetection for a family of probabilistic network models with overlapping\ncommunities, termed as the mixed membership Dirichlet model, first introduced\nby Airoldi et al. This model allows for nodes to have fractional memberships in\nmultiple communities and assumes that the community memberships are drawn from\na Dirichlet distribution. Moreover, it contains the stochastic block model as a\nspecial case. We propose a unified approach to learning these models via a\ntensor spectral decomposition method. Our estimator is based on low-order\nmoment tensor of the observed network, consisting of 3-star counts. Our\nlearning method is fast and is based on simple linear algebraic operations,\ne.g. singular value decomposition and tensor power iterations. We provide\nguaranteed recovery of community memberships and model parameters and present a\ncareful finite sample analysis of our learning method. As an important special\ncase, our results match the best known scaling requirements for the\n(homogeneous) stochastic block model.\n", "versions": [{"version": "v1", "created": "Tue, 12 Feb 2013 01:48:14 GMT"}, {"version": "v2", "created": "Sat, 29 Jun 2013 10:10:41 GMT"}, {"version": "v3", "created": "Tue, 10 Sep 2013 03:55:04 GMT"}, {"version": "v4", "created": "Thu, 24 Oct 2013 21:30:08 GMT"}], "update_date": "2013-10-28", "authors_parsed": [["Anandkumar", "Anima", ""], ["Ge", "Rong", ""], ["Hsu", "Daniel", ""], ["Kakade", "Sham M.", ""]]}, {"id": "1302.2752", "submitter": "Aryeh Kontorovich", "authors": "Lee-Ad Gottlieb, Aryeh Kontorovich, Robert Krauthgamer", "title": "Adaptive Metric Dimensionality Reduction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study adaptive data-dependent dimensionality reduction in the context of\nsupervised learning in general metric spaces. Our main statistical contribution\nis a generalization bound for Lipschitz functions in metric spaces that are\ndoubling, or nearly doubling. On the algorithmic front, we describe an analogue\nof PCA for metric spaces: namely an efficient procedure that approximates the\ndata's intrinsic dimension, which is often much lower than the ambient\ndimension. Our approach thus leverages the dual benefits of low dimensionality:\n(1) more efficient algorithms, e.g., for proximity search, and (2) more\noptimistic generalization bounds.\n", "versions": [{"version": "v1", "created": "Tue, 12 Feb 2013 10:20:21 GMT"}, {"version": "v2", "created": "Sun, 12 May 2013 14:58:17 GMT"}, {"version": "v3", "created": "Wed, 25 Mar 2015 12:18:55 GMT"}], "update_date": "2015-03-26", "authors_parsed": [["Gottlieb", "Lee-Ad", ""], ["Kontorovich", "Aryeh", ""], ["Krauthgamer", "Robert", ""]]}, {"id": "1302.2767", "submitter": "Louis Theran", "authors": "Franz J. Kir\\'aly and Louis Theran", "title": "Coherence and sufficient sampling densities for reconstruction in\n  compressed sensing", "comments": "16 pages, 1 figure. v2 streamlines the exposition", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.IT math.AG math.IT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We give a new, very general, formulation of the compressed sensing problem in\nterms of coordinate projections of an analytic variety, and derive sufficient\nsampling rates for signal reconstruction. Our bounds are linear in the\ncoherence of the signal space, a geometric parameter independent of the\nspecific signal and measurement, and logarithmic in the ambient dimension where\nthe signal is presented. We exemplify our approach by deriving sufficient\nsampling densities for low-rank matrix completion and distance matrix\ncompletion which are independent of the true matrix.\n", "versions": [{"version": "v1", "created": "Tue, 12 Feb 2013 12:15:20 GMT"}, {"version": "v2", "created": "Sat, 2 Nov 2013 14:02:46 GMT"}], "update_date": "2013-11-05", "authors_parsed": [["Kir\u00e1ly", "Franz J.", ""], ["Theran", "Louis", ""]]}, {"id": "1302.3219", "submitter": "Chunhua Shen", "authors": "Chunhua Shen, Junae Kim, Fayao Liu, Lei Wang, Anton van den Hengel", "title": "An Efficient Dual Approach to Distance Metric Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Distance metric learning is of fundamental interest in machine learning\nbecause the distance metric employed can significantly affect the performance\nof many learning methods. Quadratic Mahalanobis metric learning is a popular\napproach to the problem, but typically requires solving a semidefinite\nprogramming (SDP) problem, which is computationally expensive. Standard\ninterior-point SDP solvers typically have a complexity of $O(D^{6.5})$ (with\n$D$ the dimension of input data), and can thus only practically solve problems\nexhibiting less than a few thousand variables. Since the number of variables is\n$D (D+1) / 2 $, this implies a limit upon the size of problem that can\npractically be solved of around a few hundred dimensions. The complexity of the\npopular quadratic Mahalanobis metric learning approach thus limits the size of\nproblem to which metric learning can be applied. Here we propose a\nsignificantly more efficient approach to the metric learning problem based on\nthe Lagrange dual formulation of the problem. The proposed formulation is much\nsimpler to implement, and therefore allows much larger Mahalanobis metric\nlearning problems to be solved. The time complexity of the proposed method is\n$O (D ^ 3) $, which is significantly lower than that of the SDP approach.\nExperiments on a variety of datasets demonstrate that the proposed method\nachieves an accuracy comparable to the state-of-the-art, but is applicable to\nsignificantly larger problems. We also show that the proposed method can be\napplied to solve more general Frobenius-norm regularized SDP problems\napproximately.\n", "versions": [{"version": "v1", "created": "Wed, 13 Feb 2013 08:48:53 GMT"}], "update_date": "2013-02-15", "authors_parsed": [["Shen", "Chunhua", ""], ["Kim", "Junae", ""], ["Liu", "Fayao", ""], ["Wang", "Lei", ""], ["Hengel", "Anton van den", ""]]}, {"id": "1302.3268", "submitter": "Aleksandrs Slivkins", "authors": "Ittai Abraham, Omar Alonso, Vasilis Kandylas and Aleksandrs Slivkins", "title": "Adaptive Crowdsourcing Algorithms for the Bandit Survey Problem", "comments": "Full version of a paper in COLT 2013", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Very recently crowdsourcing has become the de facto platform for distributing\nand collecting human computation for a wide range of tasks and applications\nsuch as information retrieval, natural language processing and machine\nlearning. Current crowdsourcing platforms have some limitations in the area of\nquality control. Most of the effort to ensure good quality has to be done by\nthe experimenter who has to manage the number of workers needed to reach good\nresults.\n  We propose a simple model for adaptive quality control in crowdsourced\nmultiple-choice tasks which we call the \\emph{bandit survey problem}. This\nmodel is related to, but technically different from the well-known multi-armed\nbandit problem. We present several algorithms for this problem, and support\nthem with analysis and simulations. Our approach is based in our experience\nconducting relevance evaluation for a large commercial search engine.\n", "versions": [{"version": "v1", "created": "Wed, 13 Feb 2013 22:42:44 GMT"}, {"version": "v2", "created": "Mon, 20 May 2013 15:02:15 GMT"}], "update_date": "2013-05-21", "authors_parsed": [["Abraham", "Ittai", ""], ["Alonso", "Omar", ""], ["Kandylas", "Vasilis", ""], ["Slivkins", "Aleksandrs", ""]]}, {"id": "1302.3283", "submitter": "Chunhua Shen", "authors": "Chunhua Shen, Guosheng Lin, Anton van den Hengel", "title": "StructBoost: Boosting Methods for Predicting Structured Output Variables", "comments": "Published in: IEEE Transactions on Pattern Analysis and Machine\n  Intelligence (Volume: 36 , Issue: 10 , Oct. 1 2014). Fixed reference error", "journal-ref": null, "doi": "10.1109/TPAMI.2014.2315792", "report-no": null, "categories": "cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Boosting is a method for learning a single accurate predictor by linearly\ncombining a set of less accurate weak learners. Recently, structured learning\nhas found many applications in computer vision. Inspired by structured support\nvector machines (SSVM), here we propose a new boosting algorithm for structured\noutput prediction, which we refer to as StructBoost. StructBoost supports\nnonlinear structured learning by combining a set of weak structured learners.\nAs SSVM generalizes SVM, our StructBoost generalizes standard boosting\napproaches such as AdaBoost, or LPBoost to structured learning. The resulting\noptimization problem of StructBoost is more challenging than SSVM in the sense\nthat it may involve exponentially many variables and constraints. In contrast,\nfor SSVM one usually has an exponential number of constraints and a\ncutting-plane method is used. In order to efficiently solve StructBoost, we\nformulate an equivalent $ 1 $-slack formulation and solve it using a\ncombination of cutting planes and column generation. We show the versatility\nand usefulness of StructBoost on a range of problems such as optimizing the\ntree loss for hierarchical multi-class classification, optimizing the Pascal\noverlap criterion for robust visual tracking and learning conditional random\nfield parameters for image segmentation.\n", "versions": [{"version": "v1", "created": "Thu, 14 Feb 2013 01:01:24 GMT"}, {"version": "v2", "created": "Thu, 8 Aug 2013 00:37:40 GMT"}, {"version": "v3", "created": "Sun, 9 Feb 2014 01:46:31 GMT"}, {"version": "v4", "created": "Mon, 9 Mar 2020 03:33:35 GMT"}], "update_date": "2020-03-10", "authors_parsed": [["Shen", "Chunhua", ""], ["Lin", "Guosheng", ""], ["Hengel", "Anton van den", ""]]}, {"id": "1302.3407", "submitter": "Azadeh Khaleghi", "authors": "Azaden Khaleghi and Daniil Ryabko", "title": "A consistent clustering-based approach to estimating the number of\n  change-points in highly dependent time-series", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.IT cs.LG math.IT math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The problem of change-point estimation is considered under a general\nframework where the data are generated by unknown stationary ergodic process\ndistributions. In this context, the consistent estimation of the number of\nchange-points is provably impossible. However, it is shown that a consistent\nclustering method may be used to estimate the number of change points, under\nthe additional constraint that the correct number of process distributions that\ngenerate the data is provided. This additional parameter has a natural\ninterpretation in many real-world applications. An algorithm is proposed that\nestimates the number of change-points and locates the changes. The proposed\nalgorithm is shown to be asymptotically consistent; its empirical evaluations\nare provided.\n", "versions": [{"version": "v1", "created": "Thu, 14 Feb 2013 14:15:14 GMT"}], "update_date": "2013-02-15", "authors_parsed": [["Khaleghi", "Azaden", ""], ["Ryabko", "Daniil", ""]]}, {"id": "1302.3447", "submitter": "Xinjia Chen", "authors": "Zhengjia Chen and Xinjia Chen", "title": "Exact Methods for Multistage Estimation of a Binomial Proportion", "comments": "38 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.LG cs.NA math.PR stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We first review existing sequential methods for estimating a binomial\nproportion. Afterward, we propose a new family of group sequential sampling\nschemes for estimating a binomial proportion with prescribed margin of error\nand confidence level. In particular, we establish the uniform controllability\nof coverage probability and the asymptotic optimality for such a family of\nsampling schemes. Our theoretical results establish the possibility that the\nparameters of this family of sampling schemes can be determined so that the\nprescribed level of confidence is guaranteed with little waste of samples.\nAnalytic bounds for the cumulative distribution functions and expectations of\nsample numbers are derived. Moreover, we discuss the inherent connection of\nvarious sampling schemes. Numerical issues are addressed for improving the\naccuracy and efficiency of computation. Computational experiments are conducted\nfor comparing sampling schemes. Illustrative examples are given for\napplications in clinical trials.\n", "versions": [{"version": "v1", "created": "Wed, 13 Feb 2013 18:13:23 GMT"}], "update_date": "2013-11-05", "authors_parsed": [["Chen", "Zhengjia", ""], ["Chen", "Xinjia", ""]]}, {"id": "1302.3566", "submitter": "David Maxwell Chickering", "authors": "David Maxwell Chickering", "title": "Learning Equivalence Classes of Bayesian Networks Structures", "comments": "Appears in Proceedings of the Twelfth Conference on Uncertainty in\n  Artificial Intelligence (UAI1996)", "journal-ref": null, "doi": null, "report-no": "UAI-P-1996-PG-150-157", "categories": "cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Approaches to learning Bayesian networks from data typically combine a\nscoring function with a heuristic search procedure. Given a Bayesian network\nstructure, many of the scoring functions derived in the literature return a\nscore for the entire equivalence class to which the structure belongs. When\nusing such a scoring function, it is appropriate for the heuristic search\nalgorithm to search over equivalence classes of Bayesian networks as opposed to\nindividual structures. We present the general formulation of a search space for\nwhich the states of the search correspond to equivalence classes of structures.\nUsing this space, any one of a number of heuristic search algorithms can easily\nbe applied. We compare greedy search performance in the proposed search space\nto greedy search performance in a search space for which the states correspond\nto individual Bayesian network structures.\n", "versions": [{"version": "v1", "created": "Wed, 13 Feb 2013 14:12:58 GMT"}], "update_date": "2013-02-18", "authors_parsed": [["Chickering", "David Maxwell", ""]]}, {"id": "1302.3567", "submitter": "Max Chickering", "authors": "David Maxwell Chickering, David Heckerman", "title": "Efficient Approximations for the Marginal Likelihood of Incomplete Data\n  Given a Bayesian Network", "comments": "Appears in Proceedings of the Twelfth Conference on Uncertainty in\n  Artificial Intelligence (UAI1996)", "journal-ref": null, "doi": null, "report-no": "UAI-P-1996-PG-158-168", "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We discuss Bayesian methods for learning Bayesian networks when data sets are\nincomplete. In particular, we examine asymptotic approximations for the\nmarginal likelihood of incomplete data given a Bayesian network. We consider\nthe Laplace approximation and the less accurate but more efficient BIC/MDL\napproximation. We also consider approximations proposed by Draper (1993) and\nCheeseman and Stutz (1995). These approximations are as efficient as BIC/MDL,\nbut their accuracy has not been studied in any depth. We compare the accuracy\nof these approximations under the assumption that the Laplace approximation is\nthe most accurate. In experiments using synthetic data generated from discrete\nnaive-Bayes models having a hidden root node, we find that the CS measure is\nthe most accurate.\n", "versions": [{"version": "v1", "created": "Wed, 13 Feb 2013 14:13:03 GMT"}, {"version": "v2", "created": "Sun, 17 May 2015 00:07:34 GMT"}], "update_date": "2015-05-19", "authors_parsed": [["Chickering", "David Maxwell", ""], ["Heckerman", "David", ""]]}, {"id": "1302.3577", "submitter": "Nir Friedman", "authors": "Nir Friedman, Moises Goldszmidt", "title": "Learning Bayesian Networks with Local Structure", "comments": "Appears in Proceedings of the Twelfth Conference on Uncertainty in\n  Artificial Intelligence (UAI1996)", "journal-ref": null, "doi": null, "report-no": "UAI-P-1996-PG-252-262", "categories": "cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we examine a novel addition to the known methods for learning\nBayesian networks from data that improves the quality of the learned networks.\nOur approach explicitly represents and learns the local structure in the\nconditional probability tables (CPTs), that quantify these networks. This\nincreases the space of possible models, enabling the representation of CPTs\nwith a variable number of parameters that depends on the learned local\nstructures. The resulting learning procedure is capable of inducing models that\nbetter emulate the real complexity of the interactions present in the data. We\ndescribe the theoretical foundations and practical aspects of learning local\nstructures, as well as an empirical evaluation of the proposed method. This\nevaluation indicates that learning curves characterizing the procedure that\nexploits the local structure converge faster than these of the standard\nprocedure. Our results also show that networks learned with local structure\ntend to be more complex (in terms of arcs), yet require less parameters.\n", "versions": [{"version": "v1", "created": "Wed, 13 Feb 2013 14:14:02 GMT"}], "update_date": "2013-02-18", "authors_parsed": [["Friedman", "Nir", ""], ["Goldszmidt", "Moises", ""]]}, {"id": "1302.3579", "submitter": "Nir Friedman", "authors": "Nir Friedman, Zohar Yakhini", "title": "On the Sample Complexity of Learning Bayesian Networks", "comments": "Appears in Proceedings of the Twelfth Conference on Uncertainty in\n  Artificial Intelligence (UAI1996)", "journal-ref": null, "doi": null, "report-no": "UAI-P-1996-PG-274-282", "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years there has been an increasing interest in learning Bayesian\nnetworks from data. One of the most effective methods for learning such\nnetworks is based on the minimum description length (MDL) principle. Previous\nwork has shown that this learning procedure is asymptotically successful: with\nprobability one, it will converge to the target distribution, given a\nsufficient number of samples. However, the rate of this convergence has been\nhitherto unknown. In this work we examine the sample complexity of MDL based\nlearning procedures for Bayesian networks. We show that the number of samples\nneeded to learn an epsilon-close approximation (in terms of entropy distance)\nwith confidence delta is O((1/epsilon)^(4/3)log(1/epsilon)log(1/delta)loglog\n(1/delta)). This means that the sample complexity is a low-order polynomial in\nthe error threshold and sub-linear in the confidence bound. We also discuss how\nthe constants in this term depend on the complexity of the target distribution.\nFinally, we address questions of asymptotic minimality and propose a method for\nusing the sample complexity results to speed up the learning process.\n", "versions": [{"version": "v1", "created": "Wed, 13 Feb 2013 14:14:13 GMT"}], "update_date": "2013-02-18", "authors_parsed": [["Friedman", "Nir", ""], ["Yakhini", "Zohar", ""]]}, {"id": "1302.3580", "submitter": "Dan Geiger", "authors": "Dan Geiger, David Heckerman, Christopher Meek", "title": "Asymptotic Model Selection for Directed Networks with Hidden Variables", "comments": "Appears in Proceedings of the Twelfth Conference on Uncertainty in\n  Artificial Intelligence (UAI1996)", "journal-ref": null, "doi": null, "report-no": "UAI-P-1996-PG-283-290", "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We extend the Bayesian Information Criterion (BIC), an asymptotic\napproximation for the marginal likelihood, to Bayesian networks with hidden\nvariables. This approximation can be used to select models given large samples\nof data. The standard BIC as well as our extension punishes the complexity of a\nmodel according to the dimension of its parameters. We argue that the dimension\nof a Bayesian network with hidden variables is the rank of the Jacobian matrix\nof the transformation between the parameters of the network and the parameters\nof the observable variables. We compute the dimensions of several networks\nincluding the naive Bayes model with a hidden root node.\n", "versions": [{"version": "v1", "created": "Wed, 13 Feb 2013 14:14:19 GMT"}, {"version": "v2", "created": "Sat, 16 May 2015 23:35:58 GMT"}], "update_date": "2015-05-19", "authors_parsed": [["Geiger", "Dan", ""], ["Heckerman", "David", ""], ["Meek", "Christopher", ""]]}, {"id": "1302.3590", "submitter": "Kathryn Blackmond Laskey", "authors": "Kathryn Blackmond Laskey, Laura Martignon", "title": "Bayesian Learning of Loglinear Models for Neural Connectivity", "comments": "Appears in Proceedings of the Twelfth Conference on Uncertainty in\n  Artificial Intelligence (UAI1996)", "journal-ref": null, "doi": null, "report-no": "UAI-P-1996-PG-373-380", "categories": "cs.LG q-bio.NC stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a Bayesian approach to learning the connectivity\nstructure of a group of neurons from data on configuration frequencies. A major\nobjective of the research is to provide statistical tools for detecting changes\nin firing patterns with changing stimuli. Our framework is not restricted to\nthe well-understood case of pair interactions, but generalizes the Boltzmann\nmachine model to allow for higher order interactions. The paper applies a\nMarkov Chain Monte Carlo Model Composition (MC3) algorithm to search over\nconnectivity structures and uses Laplace's method to approximate posterior\nprobabilities of structures. Performance of the methods was tested on synthetic\ndata. The models were also applied to data obtained by Vaadia on multi-unit\nrecordings of several neurons in the visual cortex of a rhesus monkey in two\ndifferent attentional states. Results confirmed the experimenters' conjecture\nthat different attentional states were associated with different interaction\nstructures.\n", "versions": [{"version": "v1", "created": "Wed, 13 Feb 2013 14:15:20 GMT"}], "update_date": "2013-02-18", "authors_parsed": [["Laskey", "Kathryn Blackmond", ""], ["Martignon", "Laura", ""]]}, {"id": "1302.3639", "submitter": "George Chen", "authors": "George H. Chen, Stanislav Nikolov, Devavrat Shah", "title": "A Latent Source Model for Nonparametric Time Series Classification", "comments": "Advances in Neural Information Processing Systems (NIPS 2013)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For classifying time series, a nearest-neighbor approach is widely used in\npractice with performance often competitive with or better than more elaborate\nmethods such as neural networks, decision trees, and support vector machines.\nWe develop theoretical justification for the effectiveness of\nnearest-neighbor-like classification of time series. Our guiding hypothesis is\nthat in many applications, such as forecasting which topics will become trends\non Twitter, there aren't actually that many prototypical time series to begin\nwith, relative to the number of time series we have access to, e.g., topics\nbecome trends on Twitter only in a few distinct manners whereas we can collect\nmassive amounts of Twitter data. To operationalize this hypothesis, we propose\na latent source model for time series, which naturally leads to a \"weighted\nmajority voting\" classification rule that can be approximated by a\nnearest-neighbor classifier. We establish nonasymptotic performance guarantees\nof both weighted majority voting and nearest-neighbor classification under our\nmodel accounting for how much of the time series we observe and the model\ncomplexity. Experimental results on synthetic data show weighted majority\nvoting achieving the same misclassification rate as nearest-neighbor\nclassification while observing less of the time series. We then use weighted\nmajority to forecast which news topics on Twitter become trends, where we are\nable to detect such \"trending topics\" in advance of Twitter 79% of the time,\nwith a mean early advantage of 1 hour and 26 minutes, a true positive rate of\n95%, and a false positive rate of 4%.\n", "versions": [{"version": "v1", "created": "Thu, 14 Feb 2013 22:12:40 GMT"}, {"version": "v2", "created": "Mon, 25 Mar 2013 01:37:27 GMT"}, {"version": "v3", "created": "Tue, 26 Mar 2013 15:28:08 GMT"}, {"version": "v4", "created": "Sat, 9 Nov 2013 00:21:07 GMT"}, {"version": "v5", "created": "Fri, 13 Dec 2013 04:20:34 GMT"}], "update_date": "2013-12-16", "authors_parsed": [["Chen", "George H.", ""], ["Nikolov", "Stanislav", ""], ["Shah", "Devavrat", ""]]}, {"id": "1302.3668", "submitter": "Ajit Narayanan", "authors": "Ajit Narayanan and Yi Chen", "title": "Bio-inspired data mining: Treating malware signatures as biosequences", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG q-bio.QM stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The application of machine learning to bioinformatics problems is well\nestablished. Less well understood is the application of bioinformatics\ntechniques to machine learning and, in particular, the representation of\nnon-biological data as biosequences. The aim of this paper is to explore the\neffects of giving amino acid representation to problematic machine learning\ndata and to evaluate the benefits of supplementing traditional machine learning\nwith bioinformatics tools and techniques. The signatures of 60 computer viruses\nand 60 computer worms were converted into amino acid representations and first\nmultiply aligned separately to identify conserved regions across different\nfamilies within each class (virus and worm). This was followed by a second\nalignment of all 120 aligned signatures together so that non-conserved regions\nwere identified prior to input to a number of machine learning techniques.\nDifferences in length between virus and worm signatures after the first\nalignment were resolved by the second alignment. Our first set of experiments\nindicates that representing computer malware signatures as amino acid sequences\nfollowed by alignment leads to greater classification and prediction accuracy.\nOur second set of experiments indicates that checking the results of data\nmining from artificial virus and worm data against known proteins can lead to\ngeneralizations being made from the domain of naturally occurring proteins to\nmalware signatures. However, further work is needed to determine the advantages\nand disadvantages of different representations and sequence alignment methods\nfor handling problematic machine learning data.\n", "versions": [{"version": "v1", "created": "Fri, 15 Feb 2013 03:54:53 GMT"}], "update_date": "2013-02-18", "authors_parsed": [["Narayanan", "Ajit", ""], ["Chen", "Yi", ""]]}, {"id": "1302.3700", "submitter": "John Quinn", "authors": "John A. Quinn, Masashi Sugiyama", "title": "Density Ratio Hidden Markov Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hidden Markov models and their variants are the predominant sequential\nclassification method in such domains as speech recognition, bioinformatics and\nnatural language processing. Being generative rather than discriminative\nmodels, however, their classification performance is a drawback. In this paper\nwe apply ideas from the field of density ratio estimation to bypass the\ndifficult step of learning likelihood functions in HMMs. By reformulating\ninference and model fitting in terms of density ratios and applying a fast\nkernel-based estimation method, we show that it is possible to obtain a\nstriking increase in discriminative performance while retaining the\nprobabilistic qualities of the HMM. We demonstrate experimentally that this\nformulation makes more efficient use of training data than alternative\napproaches.\n", "versions": [{"version": "v1", "created": "Fri, 15 Feb 2013 08:16:14 GMT"}], "update_date": "2013-02-18", "authors_parsed": [["Quinn", "John A.", ""], ["Sugiyama", "Masashi", ""]]}, {"id": "1302.3721", "submitter": "Joseph Mellor", "authors": "Joseph Mellor, Jonathan Shapiro", "title": "Thompson Sampling in Switching Environments with Bayesian Online Change\n  Point Detection", "comments": "A version will appear in the Sixteenth international conference on\n  Artificial Intelligence and Statistics (AIStats 2013)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Thompson Sampling has recently been shown to be optimal in the Bernoulli\nMulti-Armed Bandit setting[Kaufmann et al., 2012]. This bandit problem assumes\nstationary distributions for the rewards. It is often unrealistic to model the\nreal world as a stationary distribution. In this paper we derive and evaluate\nalgorithms using Thompson Sampling for a Switching Multi-Armed Bandit Problem.\nWe propose a Thompson Sampling strategy equipped with a Bayesian change point\nmechanism to tackle this problem. We develop algorithms for a variety of cases\nwith constant switching rate: when switching occurs all arms change (Global\nSwitching), switching occurs independently for each arm (Per-Arm Switching),\nwhen the switching rate is known and when it must be inferred from data. This\nleads to a family of algorithms we collectively term Change-Point Thompson\nSampling (CTS). We show empirical results of the algorithm in 4 artificial\nenvironments, and 2 derived from real world data; news click-through[Yahoo!,\n2011] and foreign exchange data[Dukascopy, 2012], comparing them to some other\nbandit algorithms. In real world data CTS is the most effective.\n", "versions": [{"version": "v1", "created": "Fri, 15 Feb 2013 10:48:57 GMT"}], "update_date": "2013-02-18", "authors_parsed": [["Mellor", "Joseph", ""], ["Shapiro", "Jonathan", ""]]}, {"id": "1302.3931", "submitter": "Xiaozhao Zhao", "authors": "Xiaozhao Zhao and Yuexian Hou and Qian Yu and Dawei Song and Wenjie Li", "title": "Understanding Boltzmann Machine and Deep Learning via A Confident\n  Information First Principle", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Typical dimensionality reduction methods focus on directly reducing the\nnumber of random variables while retaining maximal variations in the data. In\nthis paper, we consider the dimensionality reduction in parameter spaces of\nbinary multivariate distributions. We propose a general\nConfident-Information-First (CIF) principle to maximally preserve parameters\nwith confident estimates and rule out unreliable or noisy parameters. Formally,\nthe confidence of a parameter can be assessed by its Fisher information, which\nestablishes a connection with the inverse variance of any unbiased estimate for\nthe parameter via the Cram\\'{e}r-Rao bound. We then revisit Boltzmann machines\n(BM) and theoretically show that both single-layer BM without hidden units\n(SBM) and restricted BM (RBM) can be solidly derived using the CIF principle.\nThis can not only help us uncover and formalize the essential parts of the\ntarget density that SBM and RBM capture, but also suggest that the deep neural\nnetwork consisting of several layers of RBM can be seen as the layer-wise\napplication of CIF. Guided by the theoretical analysis, we develop a\nsample-specific CIF-based contrastive divergence (CD-CIF) algorithm for SBM and\na CIF-based iterative projection procedure (IP) for RBM. Both CD-CIF and IP are\nstudied in a series of density estimation experiments.\n", "versions": [{"version": "v1", "created": "Sat, 16 Feb 2013 05:49:15 GMT"}, {"version": "v2", "created": "Mon, 25 Feb 2013 06:53:55 GMT"}, {"version": "v3", "created": "Sat, 23 Mar 2013 06:20:51 GMT"}, {"version": "v4", "created": "Fri, 12 Apr 2013 13:28:59 GMT"}, {"version": "v5", "created": "Wed, 17 Apr 2013 11:05:06 GMT"}, {"version": "v6", "created": "Mon, 3 Jun 2013 11:18:46 GMT"}, {"version": "v7", "created": "Wed, 9 Oct 2013 16:55:26 GMT"}], "update_date": "2013-10-10", "authors_parsed": [["Zhao", "Xiaozhao", ""], ["Hou", "Yuexian", ""], ["Yu", "Qian", ""], ["Song", "Dawei", ""], ["Li", "Wenjie", ""]]}, {"id": "1302.3956", "submitter": "Farzad Didehvar", "authors": "Raheleh Namayandeh, Farzad Didehvar, Zahra Shojaei", "title": "Clustering validity based on the most similarity", "comments": "4 pages,2 figueres, 6 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One basic requirement of many studies is the necessity of classifying data.\nClustering is a proposed method for summarizing networks. Clustering methods\ncan be divided into two categories named model-based approaches and algorithmic\napproaches. Since the most of clustering methods depend on their input\nparameters, it is important to evaluate the result of a clustering algorithm\nwith its different input parameters, to choose the most appropriate one. There\nare several clustering validity techniques based on inner density and outer\ndensity of clusters that represent different metrics to choose the most\nappropriate clustering independent of the input parameters. According to\ndependency of previous methods on the input parameters, one challenge in facing\nwith large systems, is to complete data incrementally that effects on the final\nchoice of the most appropriate clustering. Those methods define the existence\nof high intensity in a cluster, and low intensity among different clusters as\nthe measure of choosing the optimal clustering. This measure has a tremendous\nproblem, not availing all data at the first stage. In this paper, we introduce\nan efficient measure in which maximum number of repetitions for various initial\nvalues occurs.\n", "versions": [{"version": "v1", "created": "Sat, 16 Feb 2013 11:10:17 GMT"}], "update_date": "2013-02-19", "authors_parsed": [["Namayandeh", "Raheleh", ""], ["Didehvar", "Farzad", ""], ["Shojaei", "Zahra", ""]]}, {"id": "1302.4141", "submitter": "David  Gao", "authors": "Vittorio Latorre and David Yang Gao", "title": "Canonical dual solutions to nonconvex radial basis neural network\n  optimization problem", "comments": "10 pages, 7 figures, one table", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Radial Basis Functions Neural Networks (RBFNNs) are tools widely used in\nregression problems. One of their principal drawbacks is that the formulation\ncorresponding to the training with the supervision of both the centers and the\nweights is a highly non-convex optimization problem, which leads to some\nfundamentally difficulties for traditional optimization theory and methods.\n  This paper presents a generalized canonical duality theory for solving this\nchallenging problem. We demonstrate that by sequential canonical dual\ntransformations, the nonconvex optimization problem of the RBFNN can be\nreformulated as a canonical dual problem (without duality gap). Both global\noptimal solution and local extrema can be classified. Several applications to\none of the most used Radial Basis Functions, the Gaussian function, are\nillustrated. Our results show that even for one-dimensional case, the global\nminimizer of the nonconvex problem may not be the best solution to the RBFNNs,\nand the canonical dual theory is a promising tool for solving general neural\nnetworks training problems.\n", "versions": [{"version": "v1", "created": "Mon, 18 Feb 2013 00:28:31 GMT"}], "update_date": "2013-02-19", "authors_parsed": [["Latorre", "Vittorio", ""], ["Gao", "David Yang", ""]]}, {"id": "1302.4242", "submitter": "Sylvain Chevallier", "authors": "Sylvain Chevallier and Quentin Barth\\'elemy and Jamal Atif", "title": "Metrics for Multivariate Dictionaries", "comments": null, "journal-ref": null, "doi": "10.1109/ICASSP.2014.6854993", "report-no": null, "categories": "cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  Overcomplete representations and dictionary learning algorithms kept\nattracting a growing interest in the machine learning community. This paper\naddresses the emerging problem of comparing multivariate overcomplete\nrepresentations. Despite a recurrent need to rely on a distance for learning or\nassessing multivariate overcomplete representations, no metrics in their\nunderlying spaces have yet been proposed. Henceforth we propose to study\novercomplete representations from the perspective of frame theory and matrix\nmanifolds. We consider distances between multivariate dictionaries as distances\nbetween their spans which reveal to be elements of a Grassmannian manifold. We\nintroduce Wasserstein-like set-metrics defined on Grassmannian spaces and study\ntheir properties both theoretically and numerically. Indeed a deep experimental\nstudy based on tailored synthetic datasetsand real EEG signals for\nBrain-Computer Interfaces (BCI) have been conducted. In particular, the\nintroduced metrics have been embedded in clustering algorithm and applied to\nBCI Competition IV-2a for dataset quality assessment. Besides, a principled\nconnection is made between three close but still disjoint research fields,\nnamely, Grassmannian packing, dictionary learning and compressed sensing.\n", "versions": [{"version": "v1", "created": "Mon, 18 Feb 2013 12:25:07 GMT"}, {"version": "v2", "created": "Tue, 26 Feb 2013 09:19:18 GMT"}], "update_date": "2021-02-11", "authors_parsed": [["Chevallier", "Sylvain", ""], ["Barth\u00e9lemy", "Quentin", ""], ["Atif", "Jamal", ""]]}, {"id": "1302.4297", "submitter": "Sivan Sabato", "authors": "Sivan Sabato and Adam Kalai", "title": "Feature Multi-Selection among Subjective Features", "comments": null, "journal-ref": "S. Sabato and A. Kalai, \"Feature Multi-Selection among Subjective\n  Features\", Proceedings of the 30th International Conference on Machine\n  Learning (ICML), 2013", "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When dealing with subjective, noisy, or otherwise nebulous features, the\n\"wisdom of crowds\" suggests that one may benefit from multiple judgments of the\nsame feature on the same object. We give theoretically-motivated `feature\nmulti-selection' algorithms that choose, among a large set of candidate\nfeatures, not only which features to judge but how many times to judge each\none. We demonstrate the effectiveness of this approach for linear regression on\na crowdsourced learning task of predicting people's height and weight from\nphotos, using features such as 'gender' and 'estimated weight' as well as\nculturally fraught ones such as 'attractive'.\n", "versions": [{"version": "v1", "created": "Mon, 18 Feb 2013 15:00:47 GMT"}, {"version": "v2", "created": "Wed, 17 Apr 2013 17:03:56 GMT"}, {"version": "v3", "created": "Tue, 14 May 2013 21:35:25 GMT"}], "update_date": "2013-05-16", "authors_parsed": [["Sabato", "Sivan", ""], ["Kalai", "Adam", ""]]}, {"id": "1302.4343", "submitter": "Purushottam Kar", "authors": "Purushottam Kar and Harish Karnick", "title": "On Translation Invariant Kernels and Screw Functions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.FA cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We explore the connection between Hilbertian metrics and positive definite\nkernels on the real line. In particular, we look at a well-known\ncharacterization of translation invariant Hilbertian metrics on the real line\nby von Neumann and Schoenberg (1941). Using this result we are able to give an\nalternate proof of Bochner's theorem for translation invariant positive\ndefinite kernels on the real line (Rudin, 1962).\n", "versions": [{"version": "v1", "created": "Mon, 18 Feb 2013 16:42:27 GMT"}], "update_date": "2013-02-19", "authors_parsed": [["Kar", "Purushottam", ""], ["Karnick", "Harish", ""]]}, {"id": "1302.4387", "submitter": "Ohad Shamir", "authors": "Nicolo Cesa-Bianchi, Ofer Dekel and Ohad Shamir", "title": "Online Learning with Switching Costs and Other Adaptive Adversaries", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the power of different types of adaptive (nonoblivious) adversaries\nin the setting of prediction with expert advice, under both full-information\nand bandit feedback. We measure the player's performance using a new notion of\nregret, also known as policy regret, which better captures the adversary's\nadaptiveness to the player's behavior. In a setting where losses are allowed to\ndrift, we characterize ---in a nearly complete manner--- the power of adaptive\nadversaries with bounded memories and switching costs. In particular, we show\nthat with switching costs, the attainable rate with bandit feedback is\n$\\widetilde{\\Theta}(T^{2/3})$. Interestingly, this rate is significantly worse\nthan the $\\Theta(\\sqrt{T})$ rate attainable with switching costs in the\nfull-information case. Via a novel reduction from experts to bandits, we also\nshow that a bounded memory adversary can force $\\widetilde{\\Theta}(T^{2/3})$\nregret even in the full information case, proving that switching costs are\neasier to control than bounded memory adversaries. Our lower bounds rely on a\nnew stochastic adversary strategy that generates loss processes with strong\ndependencies.\n", "versions": [{"version": "v1", "created": "Mon, 18 Feb 2013 18:46:37 GMT"}, {"version": "v2", "created": "Sat, 1 Jun 2013 09:35:15 GMT"}], "update_date": "2013-06-04", "authors_parsed": [["Cesa-Bianchi", "Nicolo", ""], ["Dekel", "Ofer", ""], ["Shamir", "Ohad", ""]]}, {"id": "1302.4389", "submitter": "Ian Goodfellow", "authors": "Ian J. Goodfellow and David Warde-Farley and Mehdi Mirza and Aaron\n  Courville and Yoshua Bengio", "title": "Maxout Networks", "comments": "This is the version of the paper that appears in ICML 2013", "journal-ref": "JMLR WCP 28 (3): 1319-1327, 2013", "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of designing models to leverage a recently introduced\napproximate model averaging technique called dropout. We define a simple new\nmodel called maxout (so named because its output is the max of a set of inputs,\nand because it is a natural companion to dropout) designed to both facilitate\noptimization by dropout and improve the accuracy of dropout's fast approximate\nmodel averaging technique. We empirically verify that the model successfully\naccomplishes both of these tasks. We use maxout and dropout to demonstrate\nstate of the art classification performance on four benchmark datasets: MNIST,\nCIFAR-10, CIFAR-100, and SVHN.\n", "versions": [{"version": "v1", "created": "Mon, 18 Feb 2013 18:59:07 GMT"}, {"version": "v2", "created": "Tue, 19 Feb 2013 04:39:48 GMT"}, {"version": "v3", "created": "Wed, 20 Feb 2013 22:33:13 GMT"}, {"version": "v4", "created": "Fri, 20 Sep 2013 08:54:35 GMT"}], "update_date": "2013-09-23", "authors_parsed": [["Goodfellow", "Ian J.", ""], ["Warde-Farley", "David", ""], ["Mirza", "Mehdi", ""], ["Courville", "Aaron", ""], ["Bengio", "Yoshua", ""]]}, {"id": "1302.4549", "submitter": "Nir Ailon", "authors": "Nir Ailon and Yudong Chen and Xu Huan", "title": "Breaking the Small Cluster Barrier of Graph Clustering", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper investigates graph clustering in the planted cluster model in the\npresence of {\\em small clusters}. Traditional results dictate that for an\nalgorithm to provably correctly recover the clusters, {\\em all} clusters must\nbe sufficiently large (in particular, $\\tilde{\\Omega}(\\sqrt{n})$ where $n$ is\nthe number of nodes of the graph). We show that this is not really a\nrestriction: by a more refined analysis of the trace-norm based recovery\napproach proposed in Jalali et al. (2011) and Chen et al. (2012), we prove that\nsmall clusters, under certain mild assumptions, do not hinder recovery of large\nones.\n  Based on this result, we further devise an iterative algorithm to recover\n{\\em almost all clusters} via a \"peeling strategy\", i.e., recover large\nclusters first, leading to a reduced problem, and repeat this procedure. These\nresults are extended to the {\\em partial observation} setting, in which only a\n(chosen) part of the graph is observed.The peeling strategy gives rise to an\nactive learning algorithm, in which edges adjacent to smaller clusters are\nqueried more often as large clusters are learned (and removed).\n  From a high level, this paper sheds novel insights on high-dimensional\nstatistics and learning structured data, by presenting a structured matrix\nlearning problem for which a one shot convex relaxation approach necessarily\nfails, but a carefully constructed sequence of convex relaxationsdoes the job.\n", "versions": [{"version": "v1", "created": "Tue, 19 Feb 2013 09:21:09 GMT"}, {"version": "v2", "created": "Wed, 20 Feb 2013 08:35:39 GMT"}], "update_date": "2013-02-21", "authors_parsed": [["Ailon", "Nir", ""], ["Chen", "Yudong", ""], ["Huan", "Xu", ""]]}, {"id": "1302.4773", "submitter": "Paulo Urriza", "authors": "Paulo Urriza, Eric Rebeiz, Danijela Cabric", "title": "Optimal Discriminant Functions Based On Sampled Distribution Distance\n  for Modulation Classification", "comments": "4 pages, 3 figures, submitted to IEEE Communications Letters", "journal-ref": null, "doi": "10.1109/LCOMM.2013.082113.131131", "report-no": null, "categories": "stat.ML cs.LG cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this letter, we derive the optimal discriminant functions for modulation\nclassification based on the sampled distribution distance. The proposed method\nclassifies various candidate constellations using a low complexity approach\nbased on the distribution distance at specific testpoints along the cumulative\ndistribution function. This method, based on the Bayesian decision criteria,\nasymptotically provides the minimum classification error possible given a set\nof testpoints. Testpoint locations are also optimized to improve classification\nperformance. The method provides significant gains over existing approaches\nthat also use the distribution of the signal features.\n", "versions": [{"version": "v1", "created": "Tue, 19 Feb 2013 22:59:44 GMT"}], "update_date": "2016-11-15", "authors_parsed": [["Urriza", "Paulo", ""], ["Rebeiz", "Eric", ""], ["Cabric", "Danijela", ""]]}, {"id": "1302.4874", "submitter": "Gon\\c{c}alo Sim\\~oes", "authors": "Gon\\c{c}alo Sim\\~oes, Helena Galhardas, David Matos", "title": "A Labeled Graph Kernel for Relationship Extraction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose an approach for Relationship Extraction (RE) based\non labeled graph kernels. The kernel we propose is a particularization of a\nrandom walk kernel that exploits two properties previously studied in the RE\nliterature: (i) the words between the candidate entities or connecting them in\na syntactic representation are particularly likely to carry information\nregarding the relationship; and (ii) combining information from distinct\nsources in a kernel may help the RE system make better decisions. We performed\nexperiments on a dataset of protein-protein interactions and the results show\nthat our approach obtains effectiveness values that are comparable with the\nstate-of-the art kernel methods. Moreover, our approach is able to outperform\nthe state-of-the-art kernels when combined with other kernel methods.\n", "versions": [{"version": "v1", "created": "Wed, 20 Feb 2013 11:06:25 GMT"}], "update_date": "2013-02-21", "authors_parsed": [["Sim\u00f5es", "Gon\u00e7alo", ""], ["Galhardas", "Helena", ""], ["Matos", "David", ""]]}, {"id": "1302.4886", "submitter": "Aleksandr Aravkin", "authors": "Aleksandr Y. Aravkin and Rajiv Kumar and Hassan Mansour and Ben Recht\n  and Felix J. Herrmann", "title": "Fast methods for denoising matrix completion formulations, with\n  applications to robust seismic data interpolation", "comments": "26 pages, 13 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent SVD-free matrix factorization formulations have enabled rank\nminimization for systems with millions of rows and columns, paving the way for\nmatrix completion in extremely large-scale applications, such as seismic data\ninterpolation.\n  In this paper, we consider matrix completion formulations designed to hit a\ntarget data-fitting error level provided by the user, and propose an algorithm\ncalled LR-BPDN that is able to exploit factorized formulations to solve the\ncorresponding optimization problem. Since practitioners typically have strong\nprior knowledge about target error level, this innovation makes it easy to\napply the algorithm in practice, leaving only the factor rank to be determined.\n  Within the established framework, we propose two extensions that are highly\nrelevant to solving practical challenges of data interpolation. First, we\npropose a weighted extension that allows known subspace information to improve\nthe results of matrix completion formulations. We show how this weighting can\nbe used in the context of frequency continuation, an essential aspect to\nseismic data interpolation. Second, we propose matrix completion formulations\nthat are robust to large measurement errors in the available data.\n  We illustrate the advantages of LR-BPDN on the collaborative filtering\nproblem using the MovieLens 1M, 10M, and Netflix 100M datasets. Then, we use\nthe new method, along with its robust and subspace re-weighted extensions, to\nobtain high-quality reconstructions for large scale seismic interpolation\nproblems with real data, even in the presence of data contamination.\n", "versions": [{"version": "v1", "created": "Wed, 20 Feb 2013 12:31:30 GMT"}, {"version": "v2", "created": "Wed, 1 May 2013 10:03:30 GMT"}, {"version": "v3", "created": "Wed, 5 Mar 2014 10:29:18 GMT"}], "update_date": "2014-03-06", "authors_parsed": [["Aravkin", "Aleksandr Y.", ""], ["Kumar", "Rajiv", ""], ["Mansour", "Hassan", ""], ["Recht", "Ben", ""], ["Herrmann", "Felix J.", ""]]}, {"id": "1302.4922", "submitter": "David Duvenaud", "authors": "David Duvenaud, James Robert Lloyd, Roger Grosse, Joshua B. Tenenbaum,\n  Zoubin Ghahramani", "title": "Structure Discovery in Nonparametric Regression through Compositional\n  Kernel Search", "comments": "9 pages, 7 figures, To appear in proceedings of the 2013\n  International Conference on Machine Learning", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite its importance, choosing the structural form of the kernel in\nnonparametric regression remains a black art. We define a space of kernel\nstructures which are built compositionally by adding and multiplying a small\nnumber of base kernels. We present a method for searching over this space of\nstructures which mirrors the scientific discovery process. The learned\nstructures can often decompose functions into interpretable components and\nenable long-range extrapolation on time-series datasets. Our structure search\nmethod outperforms many widely used kernels and kernel combination methods on a\nvariety of prediction tasks.\n", "versions": [{"version": "v1", "created": "Wed, 20 Feb 2013 14:53:13 GMT"}, {"version": "v2", "created": "Tue, 5 Mar 2013 11:48:12 GMT"}, {"version": "v3", "created": "Fri, 5 Apr 2013 16:53:30 GMT"}, {"version": "v4", "created": "Mon, 13 May 2013 13:10:31 GMT"}], "update_date": "2013-05-15", "authors_parsed": [["Duvenaud", "David", ""], ["Lloyd", "James Robert", ""], ["Grosse", "Roger", ""], ["Tenenbaum", "Joshua B.", ""], ["Ghahramani", "Zoubin", ""]]}, {"id": "1302.4949", "submitter": "Dan Geiger", "authors": "Dan Geiger, David Heckerman", "title": "A Characterization of the Dirichlet Distribution with Application to\n  Learning Bayesian Networks", "comments": "Appears in Proceedings of the Eleventh Conference on Uncertainty in\n  Artificial Intelligence (UAI1995)", "journal-ref": null, "doi": null, "report-no": "UAI-P-1995-PG-196-207", "categories": "cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We provide a new characterization of the Dirichlet distribution. This\ncharacterization implies that under assumptions made by several previous\nauthors for learning belief networks, a Dirichlet prior on the parameters is\ninevitable.\n", "versions": [{"version": "v1", "created": "Wed, 20 Feb 2013 15:20:41 GMT"}], "update_date": "2013-02-21", "authors_parsed": [["Geiger", "Dan", ""], ["Heckerman", "David", ""]]}, {"id": "1302.4964", "submitter": "George H. John", "authors": "George H. John, Pat Langley", "title": "Estimating Continuous Distributions in Bayesian Classifiers", "comments": "Appears in Proceedings of the Eleventh Conference on Uncertainty in\n  Artificial Intelligence (UAI1995)", "journal-ref": null, "doi": null, "report-no": "UAI-P-1995-PG-338-345", "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When modeling a probability distribution with a Bayesian network, we are\nfaced with the problem of how to handle continuous variables. Most previous\nwork has either solved the problem by discretizing, or assumed that the data\nare generated by a single Gaussian. In this paper we abandon the normality\nassumption and instead use statistical methods for nonparametric density\nestimation. For a naive Bayesian classifier, we present experimental results on\na variety of natural and artificial domains, comparing two methods of density\nestimation: assuming normality and modeling each conditional distribution with\na single Gaussian; and using nonparametric kernel density estimation. We\nobserve large reductions in error on several natural and artificial data sets,\nwhich suggests that kernel estimation is a useful tool for learning Bayesian\nmodels.\n", "versions": [{"version": "v1", "created": "Wed, 20 Feb 2013 15:22:01 GMT"}], "update_date": "2013-02-21", "authors_parsed": [["John", "George H.", ""], ["Langley", "Pat", ""]]}, {"id": "1302.5010", "submitter": "Mingkui Tan", "authors": "Mingkui Tan and Ivor W. Tsang and Li Wang", "title": "Matching Pursuit LASSO Part II: Applications and Sparse Recovery over\n  Batch Signals", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Matching Pursuit LASSIn Part I \\cite{TanPMLPart1}, a Matching Pursuit LASSO\n({MPL}) algorithm has been presented for solving large-scale sparse recovery\n(SR) problems. In this paper, we present a subspace search to further improve\nthe performance of MPL, and then continue to address another major challenge of\nSR -- batch SR with many signals, a consideration which is absent from most of\nprevious $\\ell_1$-norm methods. As a result, a batch-mode {MPL} is developed to\nvastly speed up sparse recovery of many signals simultaneously. Comprehensive\nnumerical experiments on compressive sensing and face recognition tasks\ndemonstrate the superior performance of MPL and BMPL over other methods\nconsidered in this paper, in terms of sparse recovery ability and efficiency.\nIn particular, BMPL is up to 400 times faster than existing $\\ell_1$-norm\nmethods considered to be state-of-the-art.O Part II: Applications and Sparse\nRecovery over Batch Signals\n", "versions": [{"version": "v1", "created": "Wed, 20 Feb 2013 16:09:38 GMT"}, {"version": "v2", "created": "Wed, 24 Dec 2014 00:14:31 GMT"}], "update_date": "2014-12-25", "authors_parsed": [["Tan", "Mingkui", ""], ["Tsang", "Ivor W.", ""], ["Wang", "Li", ""]]}, {"id": "1302.5056", "submitter": "Yangqing Jia", "authors": "Yangqing Jia, Oriol Vinyals, Trevor Darrell", "title": "Pooling-Invariant Image Feature Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Unsupervised dictionary learning has been a key component in state-of-the-art\ncomputer vision recognition architectures. While highly effective methods exist\nfor patch-based dictionary learning, these methods may learn redundant features\nafter the pooling stage in a given early vision architecture. In this paper, we\noffer a novel dictionary learning scheme to efficiently take into account the\ninvariance of learned features after the spatial pooling stage. The algorithm\nis built on simple clustering, and thus enjoys efficiency and scalability. We\ndiscuss the underlying mechanism that justifies the use of clustering\nalgorithms, and empirically show that the algorithm finds better dictionaries\nthan patch-based methods with the same dictionary size.\n", "versions": [{"version": "v1", "created": "Tue, 15 Jan 2013 18:47:11 GMT"}], "update_date": "2013-02-21", "authors_parsed": [["Jia", "Yangqing", ""], ["Vinyals", "Oriol", ""], ["Darrell", "Trevor", ""]]}, {"id": "1302.5125", "submitter": "Oren Rippel", "authors": "Oren Rippel, Ryan Prescott Adams", "title": "High-Dimensional Probability Estimation with Deep Density Models", "comments": "12 pages, 4 figures, 1 table. Submitted for publication", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One of the fundamental problems in machine learning is the estimation of a\nprobability distribution from data. Many techniques have been proposed to study\nthe structure of data, most often building around the assumption that\nobservations lie on a lower-dimensional manifold of high probability. It has\nbeen more difficult, however, to exploit this insight to build explicit,\ntractable density models for high-dimensional data. In this paper, we introduce\nthe deep density model (DDM), a new approach to density estimation. We exploit\ninsights from deep learning to construct a bijective map to a representation\nspace, under which the transformation of the distribution of the data is\napproximately factorized and has identical and known marginal densities. The\nsimplicity of the latent distribution under the model allows us to feasibly\nexplore it, and the invertibility of the map to characterize contraction of\nmeasure across it. This enables us to compute normalized densities for\nout-of-sample data. This combination of tractability and flexibility allows us\nto tackle a variety of probabilistic tasks on high-dimensional datasets,\nincluding: rapid computation of normalized densities at test-time without\nevaluating a partition function; generation of samples without MCMC; and\ncharacterization of the joint entropy of the data.\n", "versions": [{"version": "v1", "created": "Wed, 20 Feb 2013 21:20:30 GMT"}], "update_date": "2013-02-22", "authors_parsed": [["Rippel", "Oren", ""], ["Adams", "Ryan Prescott", ""]]}, {"id": "1302.5145", "submitter": "Kai-Yang Chiang", "authors": "Kai-Yang Chiang, Cho-Jui Hsieh, Nagarajan Natarajan, Ambuj Tewari and\n  Inderjit S. Dhillon", "title": "Prediction and Clustering in Signed Networks: A Local to Global\n  Perspective", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The study of social networks is a burgeoning research area. However, most\nexisting work deals with networks that simply encode whether relationships\nexist or not. In contrast, relationships in signed networks can be positive\n(\"like\", \"trust\") or negative (\"dislike\", \"distrust\"). The theory of social\nbalance shows that signed networks tend to conform to some local patterns that,\nin turn, induce certain global characteristics. In this paper, we exploit both\nlocal as well as global aspects of social balance theory for two fundamental\nproblems in the analysis of signed networks: sign prediction and clustering.\nMotivated by local patterns of social balance, we first propose two families of\nsign prediction methods: measures of social imbalance (MOIs), and supervised\nlearning using high order cycles (HOCs). These methods predict signs of edges\nbased on triangles and \\ell-cycles for relatively small values of \\ell.\nInterestingly, by examining measures of social imbalance, we show that the\nclassic Katz measure, which is used widely in unsigned link prediction,\nactually has a balance theoretic interpretation when applied to signed\nnetworks. Furthermore, motivated by the global structure of balanced networks,\nwe propose an effective low rank modeling approach for both sign prediction and\nclustering. For the low rank modeling approach, we provide theoretical\nperformance guarantees via convex relaxations, scale it up to large problem\nsizes using a matrix factorization based algorithm, and provide extensive\nexperimental validation including comparisons with local approaches. Our\nexperimental results indicate that, by adopting a more global viewpoint of\nbalance structure, we get significant performance and computational gains in\nprediction and clustering tasks on signed networks. Our work therefore\nhighlights the usefulness of the global aspect of balance theory for the\nanalysis of signed networks.\n", "versions": [{"version": "v1", "created": "Wed, 20 Feb 2013 23:15:57 GMT"}, {"version": "v2", "created": "Tue, 5 Mar 2013 03:35:09 GMT"}], "update_date": "2013-03-06", "authors_parsed": [["Chiang", "Kai-Yang", ""], ["Hsieh", "Cho-Jui", ""], ["Natarajan", "Nagarajan", ""], ["Tewari", "Ambuj", ""], ["Dhillon", "Inderjit S.", ""]]}, {"id": "1302.5348", "submitter": "Ben London", "authors": "Ben London and Bert Huang and Lise Getoor", "title": "Graph-based Generalization Bounds for Learning Binary Relations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate the generalizability of learned binary relations: functions\nthat map pairs of instances to a logical indicator. This problem has\napplication in numerous areas of machine learning, such as ranking, entity\nresolution and link prediction. Our learning framework incorporates an example\nlabeler that, given a sequence $X$ of $n$ instances and a desired training size\n$m$, subsamples $m$ pairs from $X \\times X$ without replacement. The challenge\nin analyzing this learning scenario is that pairwise combinations of random\nvariables are inherently dependent, which prevents us from using traditional\nlearning-theoretic arguments. We present a unified, graph-based analysis, which\nallows us to analyze this dependence using well-known graph identities. We are\nthen able to bound the generalization error of learned binary relations using\nRademacher complexity and algorithmic stability. The rate of uniform\nconvergence is partially determined by the labeler's subsampling process. We\nthus examine how various assumptions about subsampling affect generalization;\nunder a natural random subsampling process, our bounds guarantee\n$\\tilde{O}(1/\\sqrt{n})$ uniform convergence.\n", "versions": [{"version": "v1", "created": "Thu, 21 Feb 2013 17:30:42 GMT"}, {"version": "v2", "created": "Wed, 27 Feb 2013 03:33:47 GMT"}, {"version": "v3", "created": "Fri, 31 May 2013 21:12:47 GMT"}], "update_date": "2013-06-04", "authors_parsed": [["London", "Ben", ""], ["Huang", "Bert", ""], ["Getoor", "Lise", ""]]}, {"id": "1302.5449", "submitter": "Juan Andres Bazerque", "authors": "Juan Andres Bazerque and Georgios B. Giannakis", "title": "Nonparametric Basis Pursuit via Sparse Kernel-based Learning", "comments": "IEEE SIGNAL PROCESSING MAGAZINE, 2013 (TO APPEAR)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.IT math.IT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Signal processing tasks as fundamental as sampling, reconstruction, minimum\nmean-square error interpolation and prediction can be viewed under the prism of\nreproducing kernel Hilbert spaces. Endowing this vantage point with\ncontemporary advances in sparsity-aware modeling and processing, promotes the\nnonparametric basis pursuit advocated in this paper as the overarching\nframework for the confluence of kernel-based learning (KBL) approaches\nleveraging sparse linear regression, nuclear-norm regularization, and\ndictionary learning. The novel sparse KBL toolbox goes beyond translating\nsparse parametric approaches to their nonparametric counterparts, to\nincorporate new possibilities such as multi-kernel selection and matrix\nsmoothing. The impact of sparse KBL to signal processing applications is\nillustrated through test cases from cognitive radio sensing, microarray data\nimputation, and network traffic prediction.\n", "versions": [{"version": "v1", "created": "Thu, 21 Feb 2013 22:59:12 GMT"}], "update_date": "2013-02-25", "authors_parsed": [["Bazerque", "Juan Andres", ""], ["Giannakis", "Georgios B.", ""]]}, {"id": "1302.5565", "submitter": "Michael Fairbank Mr", "authors": "Michael Fairbank", "title": "The Importance of Clipping in Neurocontrol by Direct Gradient Descent on\n  the Cost-to-Go Function and in Adaptive Dynamic Programming", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In adaptive dynamic programming, neurocontrol and reinforcement learning, the\nobjective is for an agent to learn to choose actions so as to minimise a total\ncost function. In this paper we show that when discretized time is used to\nmodel the motion of the agent, it can be very important to do \"clipping\" on the\nmotion of the agent in the final time step of the trajectory. By clipping we\nmean that the final time step of the trajectory is to be truncated such that\nthe agent stops exactly at the first terminal state reached, and no distance\nfurther. We demonstrate that when clipping is omitted, learning performance can\nfail to reach the optimum; and when clipping is done properly, learning\nperformance can improve significantly.\n  The clipping problem we describe affects algorithms which use explicit\nderivatives of the model functions of the environment to calculate a learning\ngradient. These include Backpropagation Through Time for Control, and methods\nbased on Dual Heuristic Dynamic Programming. However the clipping problem does\nnot significantly affect methods based on Heuristic Dynamic Programming,\nTemporal Differences or Policy Gradient Learning algorithms. Similarly, the\nclipping problem does not affect fixed-length finite-horizon problems.\n", "versions": [{"version": "v1", "created": "Fri, 22 Feb 2013 12:11:42 GMT"}], "update_date": "2013-02-25", "authors_parsed": [["Fairbank", "Michael", ""]]}, {"id": "1302.5608", "submitter": "Tobias Glasmachers", "authors": "Tobias Glasmachers and \\\"Ur\\\"un Dogan", "title": "Accelerated Linear SVM Training with Adaptive Variable Selection\n  Frequencies", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Support vector machine (SVM) training is an active research area since the\ndawn of the method. In recent years there has been increasing interest in\nspecialized solvers for the important case of linear models. The algorithm\npresented by Hsieh et al., probably best known under the name of the\n\"liblinear\" implementation, marks a major breakthrough. The method is analog to\nestablished dual decomposition algorithms for training of non-linear SVMs, but\nwith greatly reduced computational complexity per update step. This comes at\nthe cost of not keeping track of the gradient of the objective any more, which\nexcludes the application of highly developed working set selection algorithms.\nWe present an algorithmic improvement to this method. We replace uniform\nworking set selection with an online adaptation of selection frequencies. The\nadaptation criterion is inspired by modern second order working set selection\nmethods. The same mechanism replaces the shrinking heuristic. This novel\ntechnique speeds up training in some cases by more than an order of magnitude.\n", "versions": [{"version": "v1", "created": "Fri, 22 Feb 2013 14:36:59 GMT"}], "update_date": "2013-02-25", "authors_parsed": [["Glasmachers", "Tobias", ""], ["Dogan", "\u00dcr\u00fcn", ""]]}, {"id": "1302.5729", "submitter": "Ivan Selesnick", "authors": "Ivan W. Selesnick and Ilker Bayram", "title": "Sparse Signal Estimation by Maximally Sparse Convex Optimization", "comments": "13 pages, 9 figures", "journal-ref": null, "doi": "10.1109/TSP.2014.2298839", "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper addresses the problem of sparsity penalized least squares for\napplications in sparse signal processing, e.g. sparse deconvolution. This paper\naims to induce sparsity more strongly than L1 norm regularization, while\navoiding non-convex optimization. For this purpose, this paper describes the\ndesign and use of non-convex penalty functions (regularizers) constrained so as\nto ensure the convexity of the total cost function, F, to be minimized. The\nmethod is based on parametric penalty functions, the parameters of which are\nconstrained to ensure convexity of F. It is shown that optimal parameters can\nbe obtained by semidefinite programming (SDP). This maximally sparse convex\n(MSC) approach yields maximally non-convex sparsity-inducing penalty functions\nconstrained such that the total cost function, F, is convex. It is demonstrated\nthat iterative MSC (IMSC) can yield solutions substantially more sparse than\nthe standard convex sparsity-inducing approach, i.e., L1 norm minimization.\n", "versions": [{"version": "v1", "created": "Fri, 22 Feb 2013 22:36:08 GMT"}, {"version": "v2", "created": "Thu, 8 Aug 2013 13:47:53 GMT"}, {"version": "v3", "created": "Fri, 3 Jan 2014 15:48:14 GMT"}], "update_date": "2015-06-15", "authors_parsed": [["Selesnick", "Ivan W.", ""], ["Bayram", "Ilker", ""]]}, {"id": "1302.5797", "submitter": "Gergely Neu", "authors": "Luc Devroye, G\\'abor Lugosi, Gergely Neu", "title": "Prediction by Random-Walk Perturbation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a version of the follow-the-perturbed-leader online prediction\nalgorithm in which the cumulative losses are perturbed by independent symmetric\nrandom walks. The forecaster is shown to achieve an expected regret of the\noptimal order O(sqrt(n log N)) where n is the time horizon and N is the number\nof experts. More importantly, it is shown that the forecaster changes its\nprediction at most O(sqrt(n log N)) times, in expectation. We also extend the\nanalysis to online combinatorial optimization and show that even in this more\ngeneral setting, the forecaster rarely switches between experts while having a\nregret of near-optimal order.\n", "versions": [{"version": "v1", "created": "Sat, 23 Feb 2013 13:33:09 GMT"}], "update_date": "2013-02-26", "authors_parsed": [["Devroye", "Luc", ""], ["Lugosi", "G\u00e1bor", ""], ["Neu", "Gergely", ""]]}, {"id": "1302.6009", "submitter": "Aryeh Kontorovich", "authors": "Aryeh Kontorovich, Boaz Nadler, Roi Weiss", "title": "On learning parametric-output HMMs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a novel approach for learning an HMM whose outputs are distributed\naccording to a parametric family. This is done by {\\em decoupling} the learning\ntask into two steps: first estimating the output parameters, and then\nestimating the hidden states transition probabilities. The first step is\naccomplished by fitting a mixture model to the output stationary distribution.\nGiven the parameters of this mixture model, the second step is formulated as\nthe solution of an easily solvable convex quadratic program. We provide an\nerror analysis for the estimated transition probabilities and show they are\nrobust to small perturbations in the estimates of the mixture parameters.\nFinally, we support our analysis with some encouraging empirical results.\n", "versions": [{"version": "v1", "created": "Mon, 25 Feb 2013 07:20:19 GMT"}], "update_date": "2013-02-26", "authors_parsed": [["Kontorovich", "Aryeh", ""], ["Nadler", "Boaz", ""], ["Weiss", "Roi", ""]]}, {"id": "1302.6194", "submitter": "Ondrej \\v{S}uch", "authors": "Ondrej Such and Lenka Mackovicova", "title": "Phoneme discrimination using $KS$-algebra II", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  $KS$-algebra consists of expressions constructed with four kinds operations,\nthe minimum, maximum, difference and additively homogeneous generalized means.\nFive families of $Z$-classifiers are investigated on binary classification\ntasks between English phonemes. It is shown that the classifiers are able to\nreflect well known formant characteristics of vowels, while having very small\nKolmogoroff's complexity.\n", "versions": [{"version": "v1", "created": "Mon, 25 Feb 2013 18:56:49 GMT"}], "update_date": "2013-02-26", "authors_parsed": [["Such", "Ondrej", ""], ["Mackovicova", "Lenka", ""]]}, {"id": "1302.6210", "submitter": "Ratnadip Adhikari", "authors": "Ratnadip Adhikari, R. K. Agrawal", "title": "A Homogeneous Ensemble of Artificial Neural Networks for Time Series\n  Forecasting", "comments": "8 pages, 4 figures, 2 tables, 26 references, international journal", "journal-ref": "International Journal of Computer Applications, Vol. 32, No. 7,\n  October 2011, pp. 1-8", "doi": "10.5120/3913-5505", "report-no": null, "categories": "cs.NE cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Enhancing the robustness and accuracy of time series forecasting models is an\nactive area of research. Recently, Artificial Neural Networks (ANNs) have found\nextensive applications in many practical forecasting problems. However, the\nstandard backpropagation ANN training algorithm has some critical issues, e.g.\nit has a slow convergence rate and often converges to a local minimum, the\ncomplex pattern of error surfaces, lack of proper training parameters selection\nmethods, etc. To overcome these drawbacks, various improved training methods\nhave been developed in literature; but, still none of them can be guaranteed as\nthe best for all problems. In this paper, we propose a novel weighted ensemble\nscheme which intelligently combines multiple training algorithms to increase\nthe ANN forecast accuracies. The weight for each training algorithm is\ndetermined from the performance of the corresponding ANN model on the\nvalidation dataset. Experimental results on four important time series depicts\nthat our proposed technique reduces the mentioned shortcomings of individual\nANN training algorithms to a great extent. Also it achieves significantly\nbetter forecast accuracies than two other popular statistical models.\n", "versions": [{"version": "v1", "created": "Mon, 25 Feb 2013 20:09:19 GMT"}], "update_date": "2013-02-27", "authors_parsed": [["Adhikari", "Ratnadip", ""], ["Agrawal", "R. K.", ""]]}, {"id": "1302.6315", "submitter": "Kazuho Watanabe", "authors": "Kazuho Watanabe", "title": "Rate-Distortion Bounds for an Epsilon-Insensitive Distortion Measure", "comments": "5 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.LG math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Direct evaluation of the rate-distortion function has rarely been achieved\nwhen it is strictly greater than its Shannon lower bound. In this paper, we\nconsider the rate-distortion function for the distortion measure defined by an\nepsilon-insensitive loss function. We first present the Shannon lower bound\napplicable to any source distribution with finite differential entropy. Then,\nfocusing on the Laplacian and Gaussian sources, we prove that the\nrate-distortion functions of these sources are strictly greater than their\nShannon lower bounds and obtain analytically evaluable upper bounds for the\nrate-distortion functions. Small distortion limit and numerical evaluation of\nthe bounds suggest that the Shannon lower bound provides a good approximation\nto the rate-distortion function for the epsilon-insensitive distortion measure.\n", "versions": [{"version": "v1", "created": "Tue, 26 Feb 2013 05:17:55 GMT"}], "update_date": "2013-02-27", "authors_parsed": [["Watanabe", "Kazuho", ""]]}, {"id": "1302.6390", "submitter": "Mohammed El anbari", "authors": "Mohammed El Anbari and Abdallah Mkhadri", "title": "The adaptive Gril estimator with a diverging number of parameters", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of variables selection and estimation in linear\nregression model in situations where the number of parameters diverges with the\nsample size. We propose the adaptive Generalized Ridge-Lasso (\\mbox{AdaGril})\nwhich is an extension of the the adaptive Elastic Net. AdaGril incorporates\ninformation redundancy among correlated variables for model selection and\nestimation. It combines the strengths of the quadratic regularization and the\nadaptively weighted Lasso shrinkage. In this paper, we highlight the grouped\nselection property for AdaCnet method (one type of AdaGril) in the equal\ncorrelation case. Under weak conditions, we establish the oracle property of\nAdaGril which ensures the optimal large performance when the dimension is high.\nConsequently, it achieves both goals of handling the problem of collinearity in\nhigh dimension and enjoys the oracle property. Moreover, we show that AdaGril\nestimator achieves a Sparsity Inequality, i. e., a bound in terms of the number\nof non-zero components of the 'true' regression coefficient. This bound is\nobtained under a similar weak Restricted Eigenvalue (RE) condition used for\nLasso. Simulations studies show that some particular cases of AdaGril\noutperform its competitors.\n", "versions": [{"version": "v1", "created": "Tue, 26 Feb 2013 10:50:38 GMT"}], "update_date": "2013-02-27", "authors_parsed": [["Anbari", "Mohammed El", ""], ["Mkhadri", "Abdallah", ""]]}, {"id": "1302.6421", "submitter": "Jonathan Heras", "authors": "J\\'onathan Heras and Ekaterina Komendantskaya", "title": "ML4PG in Computer Algebra verification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  ML4PG is a machine-learning extension that provides statistical proof hints\nduring the process of Coq/SSReflect proof development. In this paper, we use\nML4PG to find proof patterns in the CoqEAL library -- a library that was\ndevised to verify the correctness of Computer Algebra algorithms. In\nparticular, we use ML4PG to help us in the formalisation of an efficient\nalgorithm to compute the inverse of triangular matrices.\n", "versions": [{"version": "v1", "created": "Tue, 26 Feb 2013 13:02:36 GMT"}, {"version": "v2", "created": "Fri, 26 Apr 2013 15:25:05 GMT"}, {"version": "v3", "created": "Fri, 24 May 2013 08:01:20 GMT"}], "update_date": "2013-05-27", "authors_parsed": [["Heras", "J\u00f3nathan", ""], ["Komendantskaya", "Ekaterina", ""]]}, {"id": "1302.6452", "submitter": "Jing Lei", "authors": "Jing Lei, Alessandro Rinaldo, Larry Wasserman", "title": "A Conformal Prediction Approach to Explore Functional Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper applies conformal prediction techniques to compute simultaneous\nprediction bands and clustering trees for functional data. These tools can be\nused to detect outliers and clusters. Both our prediction bands and clustering\ntrees provide prediction sets for the underlying stochastic process with a\nguaranteed finite sample behavior, under no distributional assumptions. The\nprediction sets are also informative in that they correspond to the high\ndensity region of the underlying process. While ordinary conformal prediction\nhas high computational cost for functional data, we use the inductive conformal\npredictor, together with several novel choices of conformity scores, to\nsimplify the computation. Our methods are illustrated on some real data\nexamples.\n", "versions": [{"version": "v1", "created": "Tue, 26 Feb 2013 15:16:32 GMT"}], "update_date": "2013-02-27", "authors_parsed": [["Lei", "Jing", ""], ["Rinaldo", "Alessandro", ""], ["Wasserman", "Larry", ""]]}, {"id": "1302.6523", "submitter": "Ivan Selesnick", "authors": "Yin Ding and Ivan W. Selesnick", "title": "Sparse Frequency Analysis with Sparse-Derivative Instantaneous Amplitude\n  and Phase Functions", "comments": "13 pages, 12 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper addresses the problem of expressing a signal as a sum of frequency\ncomponents (sinusoids) wherein each sinusoid may exhibit abrupt changes in its\namplitude and/or phase. The Fourier transform of a narrow-band signal, with a\ndiscontinuous amplitude and/or phase function, exhibits spectral and temporal\nspreading. The proposed method aims to avoid such spreading by explicitly\nmodeling the signal of interest as a sum of sinusoids with time-varying\namplitudes. So as to accommodate abrupt changes, it is further assumed that the\namplitude/phase functions are approximately piecewise constant (i.e., their\ntime-derivatives are sparse). The proposed method is based on a convex\nvariational (optimization) approach wherein the total variation (TV) of the\namplitude functions are regularized subject to a perfect (or approximate)\nreconstruction constraint. A computationally efficient algorithm is derived\nbased on convex optimization techniques. The proposed technique can be used to\nperform band-pass filtering that is relatively insensitive to narrow-band\namplitude/phase jumps present in data, which normally pose a challenge (due to\ntransients, leakage, etc.). The method is illustrated using both synthetic\nsignals and human EEG data for the purpose of band-pass filtering and the\nestimation of phase synchrony indexes.\n", "versions": [{"version": "v1", "created": "Tue, 26 Feb 2013 18:18:35 GMT"}], "update_date": "2013-02-27", "authors_parsed": [["Ding", "Yin", ""], ["Selesnick", "Ivan W.", ""]]}, {"id": "1302.6584", "submitter": "Qiang Liu", "authors": "Qiang Liu and Alexander Ihler", "title": "Variational Algorithms for Marginal MAP", "comments": "This is a journal version of our conference paper \"variational\n  algorithms for marginal MAP\" in UAI 201 [arXiv:1202.3742]; this version is\n  considerably expanded, with more detail in its development, examples,\n  algorithms, and proofs; additional experiments; and a junction graph version\n  of the central message-passing algorithm", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.IT cs.LG math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The marginal maximum a posteriori probability (MAP) estimation problem, which\ncalculates the mode of the marginal posterior distribution of a subset of\nvariables with the remaining variables marginalized, is an important inference\nproblem in many models, such as those with hidden variables or uncertain\nparameters. Unfortunately, marginal MAP can be NP-hard even on trees, and has\nattracted less attention in the literature compared to the joint MAP\n(maximization) and marginalization problems. We derive a general dual\nrepresentation for marginal MAP that naturally integrates the marginalization\nand maximization operations into a joint variational optimization problem,\nmaking it possible to easily extend most or all variational-based algorithms to\nmarginal MAP. In particular, we derive a set of \"mixed-product\" message passing\nalgorithms for marginal MAP, whose form is a hybrid of max-product, sum-product\nand a novel \"argmax-product\" message updates. We also derive a class of\nconvergent algorithms based on proximal point methods, including one that\ntransforms the marginal MAP problem into a sequence of standard marginalization\nproblems. Theoretically, we provide guarantees under which our algorithms give\nglobally or locally optimal solutions, and provide novel upper bounds on the\noptimal objectives. Empirically, we demonstrate that our algorithms\nsignificantly outperform the existing approaches, including a state-of-the-art\nalgorithm based on local search methods.\n", "versions": [{"version": "v1", "created": "Tue, 26 Feb 2013 20:58:59 GMT"}, {"version": "v2", "created": "Wed, 17 Jul 2013 18:33:03 GMT"}, {"version": "v3", "created": "Thu, 18 Jul 2013 00:29:57 GMT"}], "update_date": "2013-07-19", "authors_parsed": [["Liu", "Qiang", ""], ["Ihler", "Alexander", ""]]}, {"id": "1302.6613", "submitter": "Ratnadip Adhikari", "authors": "Ratnadip Adhikari, R. K. Agrawal", "title": "An Introductory Study on Time Series Modeling and Forecasting", "comments": "67 pages, 29 figures, 33 references, book", "journal-ref": "LAP Lambert Academic Publishing, Germany, 2013", "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Time series modeling and forecasting has fundamental importance to various\npractical domains. Thus a lot of active research works is going on in this\nsubject during several years. Many important models have been proposed in\nliterature for improving the accuracy and effectiveness of time series\nforecasting. The aim of this dissertation work is to present a concise\ndescription of some popular time series forecasting models used in practice,\nwith their salient features. In this thesis, we have described three important\nclasses of time series models, viz. the stochastic, neural networks and SVM\nbased models, together with their inherent forecasting strengths and\nweaknesses. We have also discussed about the basic issues related to time\nseries modeling, such as stationarity, parsimony, overfitting, etc. Our\ndiscussion about different time series models is supported by giving the\nexperimental forecast results, performed on six real time series datasets.\nWhile fitting a model to a dataset, special care is taken to select the most\nparsimonious one. To evaluate forecast accuracy as well as to compare among\ndifferent models fitted to a time series, we have used the five performance\nmeasures, viz. MSE, MAD, RMSE, MAPE and Theil's U-statistics. For each of the\nsix datasets, we have shown the obtained forecast diagram which graphically\ndepicts the closeness between the original and forecasted observations. To have\nauthenticity as well as clarity in our discussion about time series modeling\nand forecasting, we have taken the help of various published research works\nfrom reputed journals and some standard books.\n", "versions": [{"version": "v1", "created": "Tue, 26 Feb 2013 22:18:55 GMT"}], "update_date": "2013-02-28", "authors_parsed": [["Adhikari", "Ratnadip", ""], ["Agrawal", "R. K.", ""]]}, {"id": "1302.6617", "submitter": "Timothy Hunter", "authors": "Timothy Hunter, Aude Hofleitner, Jack Reilly, Walid Krichene, Jerome\n  Thai, Anastasios Kouvelas, Pieter Abbeel, Alexandre Bayen", "title": "Arriving on time: estimating travel time distributions on large-scale\n  road networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most optimal routing problems focus on minimizing travel time or distance\ntraveled. Oftentimes, a more useful objective is to maximize the probability of\non-time arrival, which requires statistical distributions of travel times,\nrather than just mean values. We propose a method to estimate travel time\ndistributions on large-scale road networks, using probe vehicle data collected\nfrom GPS. We present a framework that works with large input of data, and\nscales linearly with the size of the network. Leveraging the planar topology of\nthe graph, the method computes efficiently the time correlations between\nneighboring streets. First, raw probe vehicle traces are compressed into pairs\nof travel times and number of stops for each traversed road segment using a\n`stop-and-go' algorithm developed for this work. The compressed data is then\nused as input for training a path travel time model, which couples a Markov\nmodel along with a Gaussian Markov random field. Finally, scalable inference\nalgorithms are developed for obtaining path travel time distributions from the\ncomposite MM-GMRF model. We illustrate the accuracy and scalability of our\nmodel on a 505,000 road link network spanning the San Francisco Bay Area.\n", "versions": [{"version": "v1", "created": "Tue, 26 Feb 2013 22:36:46 GMT"}], "update_date": "2013-02-28", "authors_parsed": [["Hunter", "Timothy", ""], ["Hofleitner", "Aude", ""], ["Reilly", "Jack", ""], ["Krichene", "Walid", ""], ["Thai", "Jerome", ""], ["Kouvelas", "Anastasios", ""], ["Abbeel", "Pieter", ""], ["Bayen", "Alexandre", ""]]}, {"id": "1302.6677", "submitter": "Ashish Sabharwal", "authors": "Stefano Ermon, Carla P. Gomes, Ashish Sabharwal, Bart Selman", "title": "Taming the Curse of Dimensionality: Discrete Integration by Hashing and\n  Optimization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Integration is affected by the curse of dimensionality and quickly becomes\nintractable as the dimensionality of the problem grows. We propose a randomized\nalgorithm that, with high probability, gives a constant-factor approximation of\na general discrete integral defined over an exponentially large set. This\nalgorithm relies on solving only a small number of instances of a discrete\ncombinatorial optimization problem subject to randomly generated parity\nconstraints used as a hash function. As an application, we demonstrate that\nwith a small number of MAP queries we can efficiently approximate the partition\nfunction of discrete graphical models, which can in turn be used, for instance,\nfor marginal computation or model selection.\n", "versions": [{"version": "v1", "created": "Wed, 27 Feb 2013 06:45:28 GMT"}], "update_date": "2013-02-28", "authors_parsed": [["Ermon", "Stefano", ""], ["Gomes", "Carla P.", ""], ["Sabharwal", "Ashish", ""], ["Selman", "Bart", ""]]}, {"id": "1302.6764", "submitter": "Marcelo Serraro Zanetti", "authors": "Marcelo Serrano Zanetti, Ingo Scholtes, Claudio Juan Tessone and Frank\n  Schweitzer", "title": "Categorizing Bugs with Social Networks: A Case Study on Four Open Source\n  Software Communities", "comments": "preprint of conference proceedings of the 35th International\n  Conference on Software Engineering (ICSE 2013) - Software Engineering in\n  Practice (SEIP) Track", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SE cs.LG cs.SI nlin.AO physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Efficient bug triaging procedures are an important precondition for\nsuccessful collaborative software engineering projects. Triaging bugs can\nbecome a laborious task particularly in open source software (OSS) projects\nwith a large base of comparably inexperienced part-time contributors. In this\npaper, we propose an efficient and practical method to identify valid bug\nreports which a) refer to an actual software bug, b) are not duplicates and c)\ncontain enough information to be processed right away. Our classification is\nbased on nine measures to quantify the social embeddedness of bug reporters in\nthe collaboration network. We demonstrate its applicability in a case study,\nusing a comprehensive data set of more than 700,000 bug reports obtained from\nthe Bugzilla installation of four major OSS communities, for a period of more\nthan ten years. For those projects that exhibit the lowest fraction of valid\nbug reports, we find that the bug reporters' position in the collaboration\nnetwork is a strong indicator for the quality of bug reports. Based on this\nfinding, we develop an automated classification scheme that can easily be\nintegrated into bug tracking platforms and analyze its performance in the\nconsidered OSS communities. A support vector machine (SVM) to identify valid\nbug reports based on the nine measures yields a precision of up to 90.3% with\nan associated recall of 38.9%. With this, we significantly improve the results\nobtained in previous case studies for an automated early identification of bugs\nthat are eventually fixed. Furthermore, our study highlights the potential of\nusing quantitative measures of social organization in collaborative software\nengineering. It also opens a broad perspective for the integration of social\nawareness in the design of support infrastructures.\n", "versions": [{"version": "v1", "created": "Wed, 27 Feb 2013 13:32:15 GMT"}, {"version": "v2", "created": "Thu, 28 Feb 2013 22:26:41 GMT"}], "update_date": "2013-03-04", "authors_parsed": [["Zanetti", "Marcelo Serrano", ""], ["Scholtes", "Ingo", ""], ["Tessone", "Claudio Juan", ""], ["Schweitzer", "Frank", ""]]}, {"id": "1302.6768", "submitter": "Gil Shabat", "authors": "Gil Shabat, Yaniv Shmueli and Amir Averbuch", "title": "Missing Entries Matrix Approximation and Completion", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.NA cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe several algorithms for matrix completion and matrix approximation\nwhen only some of its entries are known. The approximation constraint can be\nany whose approximated solution is known for the full matrix. For low rank\napproximations, similar algorithms appears recently in the literature under\ndifferent names. In this work, we introduce new theorems for matrix\napproximation and show that these algorithms can be extended to handle\ndifferent constraints such as nuclear norm, spectral norm, orthogonality\nconstraints and more that are different than low rank approximations. As the\nalgorithms can be viewed from an optimization point of view, we discuss their\nconvergence to global solution for the convex case. We also discuss the optimal\nstep size and show that it is fixed in each iteration. In addition, the derived\nmatrix completion flow is robust and does not require any parameters. This\nmatrix completion flow is applicable to different spectral minimizations and\ncan be applied to physics, mathematics and electrical engineering problems such\nas data reconstruction of images and data coming from PDEs such as Helmholtz\nequation used for electromagnetic waves.\n", "versions": [{"version": "v1", "created": "Wed, 27 Feb 2013 13:47:45 GMT"}, {"version": "v2", "created": "Sun, 29 Jun 2014 09:25:20 GMT"}], "update_date": "2014-07-01", "authors_parsed": [["Shabat", "Gil", ""], ["Shmueli", "Yaniv", ""], ["Averbuch", "Amir", ""]]}, {"id": "1302.6808", "submitter": "David Heckerman", "authors": "Dan Geiger and David Heckerman", "title": "Learning Gaussian Networks", "comments": "This version has improved pointers to the literature", "journal-ref": null, "doi": null, "report-no": "UAI-P-1994-PG-235-243", "categories": "cs.AI cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We describe algorithms for learning Bayesian networks from a combination of\nuser knowledge and statistical data. The algorithms have two components: a\nscoring metric and a search procedure. The scoring metric takes a network\nstructure, statistical data, and a user's prior knowledge, and returns a score\nproportional to the posterior probability of the network structure given the\ndata. The search procedure generates networks for evaluation by the scoring\nmetric. Previous work has concentrated on metrics for domains containing only\ndiscrete variables, under the assumption that data represents a multinomial\nsample. In this paper, we extend this work, developing scoring metrics for\ndomains containing all continuous variables or a mixture of discrete and\ncontinuous variables, under the assumption that continuous data is sampled from\na multivariate normal distribution. Our work extends traditional statistical\napproaches for identifying vanishing regression coefficients in that we\nidentify two important assumptions, called event equivalence and parameter\nmodularity, that when combined allow the construction of prior distributions\nfor multivariate normal parameters from a single prior Bayesian network\nspecified by a user.\n", "versions": [{"version": "v1", "created": "Wed, 27 Feb 2013 14:16:08 GMT"}, {"version": "v2", "created": "Thu, 13 May 2021 13:10:19 GMT"}, {"version": "v3", "created": "Sun, 27 Jun 2021 16:16:19 GMT"}], "update_date": "2021-06-29", "authors_parsed": [["Geiger", "Dan", ""], ["Heckerman", "David", ""]]}, {"id": "1302.6828", "submitter": "Pat Langley", "authors": "Pat Langley, Stephanie Sage", "title": "Induction of Selective Bayesian Classifiers", "comments": "Appears in Proceedings of the Tenth Conference on Uncertainty in\n  Artificial Intelligence (UAI1994)", "journal-ref": null, "doi": null, "report-no": "UAI-P-1994-PG-399-406", "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we examine previous work on the naive Bayesian classifier and\nreview its limitations, which include a sensitivity to correlated features. We\nrespond to this problem by embedding the naive Bayesian induction scheme within\nan algorithm that c arries out a greedy search through the space of features.\nWe hypothesize that this approach will improve asymptotic accuracy in domains\nthat involve correlated features without reducing the rate of learning in ones\nthat do not. We report experimental results on six natural domains, including\ncomparisons with decision-tree induction, that support these hypotheses. In\nclosing, we discuss other approaches to extending naive Bayesian classifiers\nand outline some directions for future research.\n", "versions": [{"version": "v1", "created": "Wed, 27 Feb 2013 14:18:05 GMT"}], "update_date": "2013-02-28", "authors_parsed": [["Langley", "Pat", ""], ["Sage", "Stephanie", ""]]}, {"id": "1302.6927", "submitter": "Oren Anava", "authors": "Oren Anava, Elad Hazan, Shie Mannor, Ohad Shamir", "title": "Online Learning for Time Series Prediction", "comments": "17 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we address the problem of predicting a time series using the\nARMA (autoregressive moving average) model, under minimal assumptions on the\nnoise terms. Using regret minimization techniques, we develop effective online\nlearning algorithms for the prediction problem, without assuming that the noise\nterms are Gaussian, identically distributed or even independent. Furthermore,\nwe show that our algorithm's performances asymptotically approaches the\nperformance of the best ARMA model in hindsight.\n", "versions": [{"version": "v1", "created": "Wed, 27 Feb 2013 17:14:14 GMT"}], "update_date": "2013-02-28", "authors_parsed": [["Anava", "Oren", ""], ["Hazan", "Elad", ""], ["Mannor", "Shie", ""], ["Shamir", "Ohad", ""]]}, {"id": "1302.6937", "submitter": "Oren Anava", "authors": "Oren Anava, Elad Hazan, Shie Mannor", "title": "Online Convex Optimization Against Adversaries with Memory and\n  Application to Statistical Arbitrage", "comments": "22 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The framework of online learning with memory naturally captures learning\nproblems with temporal constraints, and was previously studied for the experts\nsetting. In this work we extend the notion of learning with memory to the\ngeneral Online Convex Optimization (OCO) framework, and present two algorithms\nthat attain low regret. The first algorithm applies to Lipschitz continuous\nloss functions, obtaining optimal regret bounds for both convex and strongly\nconvex losses. The second algorithm attains the optimal regret bounds and\napplies more broadly to convex losses without requiring Lipschitz continuity,\nyet is more complicated to implement. We complement our theoretic results with\nan application to statistical arbitrage in finance: we devise algorithms for\nconstructing mean-reverting portfolios.\n", "versions": [{"version": "v1", "created": "Wed, 27 Feb 2013 17:46:43 GMT"}, {"version": "v2", "created": "Tue, 10 Jun 2014 07:41:36 GMT"}], "update_date": "2014-06-11", "authors_parsed": [["Anava", "Oren", ""], ["Hazan", "Elad", ""], ["Mannor", "Shie", ""]]}, {"id": "1302.6974", "submitter": "M. Sadegh Talebi", "authors": "Marc Lelarge and Alexandre Proutiere and M. Sadegh Talebi", "title": "Spectrum Bandit Optimization", "comments": "21 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NI math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of allocating radio channels to links in a wireless\nnetwork. Links interact through interference, modelled as a conflict graph\n(i.e., two interfering links cannot be simultaneously active on the same\nchannel). We aim at identifying the channel allocation maximizing the total\nnetwork throughput over a finite time horizon. Should we know the average radio\nconditions on each channel and on each link, an optimal allocation would be\nobtained by solving an Integer Linear Program (ILP). When radio conditions are\nunknown a priori, we look for a sequential channel allocation policy that\nconverges to the optimal allocation while minimizing on the way the throughput\nloss or {\\it regret} due to the need for exploring sub-optimal allocations. We\nformulate this problem as a generic linear bandit problem, and analyze it first\nin a stochastic setting where radio conditions are driven by a stationary\nstochastic process, and then in an adversarial setting where radio conditions\ncan evolve arbitrarily. We provide new algorithms in both settings and derive\nupper bounds on their regrets.\n", "versions": [{"version": "v1", "created": "Wed, 27 Feb 2013 20:01:24 GMT"}, {"version": "v2", "created": "Thu, 2 May 2013 09:47:33 GMT"}, {"version": "v3", "created": "Wed, 24 Jul 2013 16:08:13 GMT"}, {"version": "v4", "created": "Tue, 17 Feb 2015 11:30:13 GMT"}], "update_date": "2015-02-18", "authors_parsed": [["Lelarge", "Marc", ""], ["Proutiere", "Alexandre", ""], ["Talebi", "M. Sadegh", ""]]}, {"id": "1302.7043", "submitter": "Evangelos Papalexakis", "authors": "Evangelos E. Papalexakis, Tom M. Mitchell, Nicholas D. Sidiropoulos,\n  Christos Faloutsos, Partha Pratim Talukdar, Brian Murphy", "title": "Scoup-SMT: Scalable Coupled Sparse Matrix-Tensor Factorization", "comments": "9 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  How can we correlate neural activity in the human brain as it responds to\nwords, with behavioral data expressed as answers to questions about these same\nwords? In short, we want to find latent variables, that explain both the brain\nactivity, as well as the behavioral responses. We show that this is an instance\nof the Coupled Matrix-Tensor Factorization (CMTF) problem. We propose\nScoup-SMT, a novel, fast, and parallel algorithm that solves the CMTF problem\nand produces a sparse latent low-rank subspace of the data. In our experiments,\nwe find that Scoup-SMT is 50-100 times faster than a state-of-the-art algorithm\nfor CMTF, along with a 5 fold increase in sparsity. Moreover, we extend\nScoup-SMT to handle missing data without degradation of performance. We apply\nScoup-SMT to BrainQ, a dataset consisting of a (nouns, brain voxels, human\nsubjects) tensor and a (nouns, properties) matrix, with coupling along the\nnouns dimension. Scoup-SMT is able to find meaningful latent variables, as well\nas to predict brain activity with competitive accuracy. Finally, we demonstrate\nthe generality of Scoup-SMT, by applying it on a Facebook dataset (users,\nfriends, wall-postings); there, Scoup-SMT spots spammer-like anomalies.\n", "versions": [{"version": "v1", "created": "Thu, 28 Feb 2013 00:37:29 GMT"}], "update_date": "2013-03-01", "authors_parsed": [["Papalexakis", "Evangelos E.", ""], ["Mitchell", "Tom M.", ""], ["Sidiropoulos", "Nicholas D.", ""], ["Faloutsos", "Christos", ""], ["Talukdar", "Partha Pratim", ""], ["Murphy", "Brian", ""]]}, {"id": "1302.7069", "submitter": "Achilles Beros", "authors": "Achilles Beros", "title": "Learning Theory in the Arithmetic Hierarchy", "comments": "19 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.LO cs.LG cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the arithmetic complexity of index sets of uniformly computably\nenumerable families learnable under different learning criteria. We determine\nthe exact complexity of these sets for the standard notions of finite learning,\nlearning in the limit, behaviorally correct learning and anomalous learning in\nthe limit. In proving the $\\Sigma_5^0$-completeness result for behaviorally\ncorrect learning we prove a result of independent interest; if a uniformly\ncomputably enumerable family is not learnable, then for any computable learner\nthere is a $\\Delta_2^0$ enumeration witnessing failure.\n", "versions": [{"version": "v1", "created": "Thu, 28 Feb 2013 03:35:18 GMT"}], "update_date": "2013-03-01", "authors_parsed": [["Beros", "Achilles", ""]]}, {"id": "1302.7175", "submitter": "Hado van Hasselt", "authors": "Hado van Hasselt", "title": "Estimating the Maximum Expected Value: An Analysis of (Nested) Cross\n  Validation and the Maximum Sample Average", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.LG stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate the accuracy of the two most common estimators for the maximum\nexpected value of a general set of random variables: a generalization of the\nmaximum sample average, and cross validation. No unbiased estimator exists and\nwe show that it is non-trivial to select a good estimator without knowledge\nabout the distributions of the random variables. We investigate and bound the\nbias and variance of the aforementioned estimators and prove consistency. The\nvariance of cross validation can be significantly reduced, but not without\nrisking a large bias. The bias and variance of different variants of cross\nvalidation are shown to be very problem-dependent, and a wrong choice can lead\nto very inaccurate estimates.\n", "versions": [{"version": "v1", "created": "Thu, 28 Feb 2013 12:48:32 GMT"}, {"version": "v2", "created": "Fri, 1 Mar 2013 15:04:48 GMT"}], "update_date": "2013-03-04", "authors_parsed": [["van Hasselt", "Hado", ""]]}, {"id": "1302.7263", "submitter": "Mark Herbster", "authors": "Claudio Gentile, Mark Herbster, Stephen Pasteris", "title": "Online Similarity Prediction of Networked Data from Known and Unknown\n  Graphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider online similarity prediction problems over networked data. We\nbegin by relating this task to the more standard class prediction problem,\nshowing that, given an arbitrary algorithm for class prediction, we can\nconstruct an algorithm for similarity prediction with \"nearly\" the same mistake\nbound, and vice versa. After noticing that this general construction is\ncomputationally infeasible, we target our study to {\\em feasible} similarity\nprediction algorithms on networked data. We initially assume that the network\nstructure is {\\em known} to the learner. Here we observe that Matrix Winnow\n\\cite{w07} has a near-optimal mistake guarantee, at the price of cubic\nprediction time per round. This motivates our effort for an efficient\nimplementation of a Perceptron algorithm with a weaker mistake guarantee but\nwith only poly-logarithmic prediction time. Our focus then turns to the\nchallenging case of networks whose structure is initially {\\em unknown} to the\nlearner. In this novel setting, where the network structure is only\nincrementally revealed, we obtain a mistake-bounded algorithm with a quadratic\nprediction time per round.\n", "versions": [{"version": "v1", "created": "Thu, 28 Feb 2013 17:15:55 GMT"}, {"version": "v2", "created": "Fri, 1 Mar 2013 16:57:09 GMT"}, {"version": "v3", "created": "Fri, 15 Mar 2013 12:52:33 GMT"}], "update_date": "2013-03-18", "authors_parsed": [["Gentile", "Claudio", ""], ["Herbster", "Mark", ""], ["Pasteris", "Stephen", ""]]}, {"id": "1302.7280", "submitter": "Eric Lock", "authors": "Eric F. Lock and David B. Dunson", "title": "Bayesian Consensus Clustering", "comments": "32 pages, 13 figures", "journal-ref": "Bioinformatics 29 (2013) 2610-2616", "doi": "10.1093/bioinformatics/btt425", "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The task of clustering a set of objects based on multiple sources of data\narises in several modern applications. We propose an integrative statistical\nmodel that permits a separate clustering of the objects for each data source.\nThese separate clusterings adhere loosely to an overall consensus clustering,\nand hence they are not independent. We describe a computationally scalable\nBayesian framework for simultaneous estimation of both the consensus clustering\nand the source-specific clusterings. We demonstrate that this flexible approach\nis more robust than joint clustering of all data sources, and is more powerful\nthan clustering each data source separately. This work is motivated by the\nintegrated analysis of heterogeneous biomedical data, and we present an\napplication to subtype identification of breast cancer tumor samples using\npublicly available data from The Cancer Genome Atlas. Software is available at\nhttp://people.duke.edu/~el113/software.html.\n", "versions": [{"version": "v1", "created": "Thu, 28 Feb 2013 18:40:14 GMT"}], "update_date": "2015-12-01", "authors_parsed": [["Lock", "Eric F.", ""], ["Dunson", "David B.", ""]]}, {"id": "1302.7283", "submitter": "Emad Grais", "authors": "Emad M. Grais, Hakan Erdogan", "title": "Source Separation using Regularized NMF with MMSE Estimates under GMM\n  Priors with Online Learning for The Uncertainties", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new method to enforce priors on the solution of the nonnegative\nmatrix factorization (NMF). The proposed algorithm can be used for denoising or\nsingle-channel source separation (SCSS) applications. The NMF solution is\nguided to follow the Minimum Mean Square Error (MMSE) estimates under Gaussian\nmixture prior models (GMM) for the source signal. In SCSS applications, the\nspectra of the observed mixed signal are decomposed as a weighted linear\ncombination of trained basis vectors for each source using NMF. In this work,\nthe NMF decomposition weight matrices are treated as a distorted image by a\ndistortion operator, which is learned directly from the observed signals. The\nMMSE estimate of the weights matrix under GMM prior and log-normal distribution\nfor the distortion is then found to improve the NMF decomposition results. The\nMMSE estimate is embedded within the optimization objective to form a novel\nregularized NMF cost function. The corresponding update rules for the new\nobjectives are derived in this paper. Experimental results show that, the\nproposed regularized NMF algorithm improves the source separation performance\ncompared with using NMF without prior or with other prior models.\n", "versions": [{"version": "v1", "created": "Thu, 28 Feb 2013 18:56:56 GMT"}], "update_date": "2013-03-01", "authors_parsed": [["Grais", "Emad M.", ""], ["Erdogan", "Hakan", ""]]}]