[{"id": "1312.0048", "submitter": "Qi Qian", "authors": "Rong Jin", "title": "Stochastic Optimization of Smooth Loss", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we first prove a high probability bound rather than an\nexpectation bound for stochastic optimization with smooth loss. Furthermore,\nthe existing analysis requires the knowledge of optimal classifier for tuning\nthe step size in order to achieve the desired bound. However, this information\nis usually not accessible in advanced. We also propose a strategy to address\nthe limitation.\n", "versions": [{"version": "v1", "created": "Sat, 30 Nov 2013 01:07:25 GMT"}], "update_date": "2013-12-03", "authors_parsed": [["Jin", "Rong", ""]]}, {"id": "1312.0049", "submitter": "Shehroz Khan", "authors": "Shehroz S.Khan, Michael G.Madden", "title": "One-Class Classification: Taxonomy of Study and Review of Techniques", "comments": "24 pages + 11 pages of references, 8 figures", "journal-ref": "The Knowledge Engineering Review, pp 1-30, 2014", "doi": "10.1017/S026988891300043X", "report-no": null, "categories": "cs.LG cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One-class classification (OCC) algorithms aim to build classification models\nwhen the negative class is either absent, poorly sampled or not well defined.\nThis unique situation constrains the learning of efficient classifiers by\ndefining class boundary just with the knowledge of positive class. The OCC\nproblem has been considered and applied under many research themes, such as\noutlier/novelty detection and concept learning. In this paper we present a\nunified view of the general problem of OCC by presenting a taxonomy of study\nfor OCC problems, which is based on the availability of training data,\nalgorithms used and the application domains applied. We further delve into each\nof the categories of the proposed taxonomy and present a comprehensive\nliterature review of the OCC algorithms, techniques and methodologies with a\nfocus on their significance, limitations and applications. We conclude our\npaper by discussing some open research problems in the field of OCC and present\nour vision for future research.\n", "versions": [{"version": "v1", "created": "Sat, 30 Nov 2013 01:52:36 GMT"}], "update_date": "2018-02-05", "authors_parsed": [["Khan", "Shehroz S.", ""], ["Madden", "Michael G.", ""]]}, {"id": "1312.0232", "submitter": "Hemant Tyagi", "authors": "Hemant Tyagi, Sebastian Stich and Bernd G\\\"artner", "title": "Stochastic continuum armed bandit problem of few linear parameters in\n  high dimensions", "comments": "Changes from previous version: (a) Corrected typos throughout. (b) In\n  earlier version, regret was defined as a conditional expectation (and hence\n  bounded w.h.p); this is changed to an expectation now resulting in minor\n  changes in statements of Lemma 1, Theorems 1,2 and Corollary 1. See Remark 1.\n  (c) Added Remark 3, and corrected statement of Proposition 3", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider a stochastic continuum armed bandit problem where the arms are\nindexed by the $\\ell_2$ ball $B_{d}(1+\\nu)$ of radius $1+\\nu$ in\n$\\mathbb{R}^d$. The reward functions $r :B_{d}(1+\\nu) \\rightarrow \\mathbb{R}$\nare considered to intrinsically depend on $k \\ll d$ unknown linear parameters\nso that $r(\\mathbf{x}) = g(\\mathbf{A} \\mathbf{x})$ where $\\mathbf{A}$ is a full\nrank $k \\times d$ matrix. Assuming the mean reward function to be smooth we\nmake use of results from low-rank matrix recovery literature and derive an\nefficient randomized algorithm which achieves a regret bound of $O(C(k,d)\nn^{\\frac{1+k}{2+k}} (\\log n)^{\\frac{1}{2+k}})$ with high probability. Here\n$C(k,d)$ is at most polynomial in $d$ and $k$ and $n$ is the number of rounds\nor the sampling budget which is assumed to be known beforehand.\n", "versions": [{"version": "v1", "created": "Sun, 1 Dec 2013 15:16:25 GMT"}, {"version": "v2", "created": "Fri, 7 Feb 2014 16:17:59 GMT"}, {"version": "v3", "created": "Mon, 23 Mar 2015 12:14:55 GMT"}, {"version": "v4", "created": "Tue, 30 May 2017 13:19:17 GMT"}], "update_date": "2017-05-31", "authors_parsed": [["Tyagi", "Hemant", ""], ["Stich", "Sebastian", ""], ["G\u00e4rtner", "Bernd", ""]]}, {"id": "1312.0286", "submitter": "William L Hamilton", "authors": "William L. Hamilton, Mahdi Milani Fard, and Joelle Pineau", "title": "Efficient Learning and Planning with Compressed Predictive States", "comments": "45 pages, 10 figures, submitted to the Journal of Machine Learning\n  Research", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Predictive state representations (PSRs) offer an expressive framework for\nmodelling partially observable systems. By compactly representing systems as\nfunctions of observable quantities, the PSR learning approach avoids using\nlocal-minima prone expectation-maximization and instead employs a globally\noptimal moment-based algorithm. Moreover, since PSRs do not require a\npredetermined latent state structure as an input, they offer an attractive\nframework for model-based reinforcement learning when agents must plan without\na priori access to a system model. Unfortunately, the expressiveness of PSRs\ncomes with significant computational cost, and this cost is a major factor\ninhibiting the use of PSRs in applications. In order to alleviate this\nshortcoming, we introduce the notion of compressed PSRs (CPSRs). The CPSR\nlearning approach combines recent advancements in dimensionality reduction,\nincremental matrix decomposition, and compressed sensing. We show how this\napproach provides a principled avenue for learning accurate approximations of\nPSRs, drastically reducing the computational costs associated with learning\nwhile also providing effective regularization. Going further, we propose a\nplanning framework which exploits these learned models. And we show that this\napproach facilitates model-learning and planning in large complex partially\nobservable domains, a task that is infeasible without the principled use of\ncompression.\n", "versions": [{"version": "v1", "created": "Sun, 1 Dec 2013 23:17:06 GMT"}, {"version": "v2", "created": "Sun, 20 Jul 2014 20:16:44 GMT"}], "update_date": "2014-07-22", "authors_parsed": [["Hamilton", "William L.", ""], ["Fard", "Mahdi Milani", ""], ["Pineau", "Joelle", ""]]}, {"id": "1312.0412", "submitter": "Arnim Bleier", "authors": "Arnim Bleier", "title": "Practical Collapsed Stochastic Variational Inference for the HDP", "comments": "NIPS Workshop; Topic Models: Computation, Application, and Evaluation", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent advances have made it feasible to apply the stochastic variational\nparadigm to a collapsed representation of latent Dirichlet allocation (LDA).\nWhile the stochastic variational paradigm has successfully been applied to an\nuncollapsed representation of the hierarchical Dirichlet process (HDP), no\nattempts to apply this type of inference in a collapsed setting of\nnon-parametric topic modeling have been put forward so far. In this paper we\nexplore such a collapsed stochastic variational Bayes inference for the HDP.\nThe proposed online algorithm is easy to implement and accounts for the\ninference of hyper-parameters. First experiments show a promising improvement\nin predictive performance.\n", "versions": [{"version": "v1", "created": "Mon, 2 Dec 2013 10:58:01 GMT"}], "update_date": "2013-12-03", "authors_parsed": [["Bleier", "Arnim", ""]]}, {"id": "1312.0451", "submitter": "Aryeh Kontorovich", "authors": "Daniel Berend and Aryeh Kontorovich", "title": "Consistency of weighted majority votes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We revisit the classical decision-theoretic problem of weighted expert voting\nfrom a statistical learning perspective. In particular, we examine the\nconsistency (both asymptotic and finitary) of the optimal Nitzan-Paroush\nweighted majority and related rules. In the case of known expert competence\nlevels, we give sharp error estimates for the optimal rule. When the competence\nlevels are unknown, they must be empirically estimated. We provide frequentist\nand Bayesian analyses for this situation. Some of our proof techniques are\nnon-standard and may be of independent interest. The bounds we derive are\nnearly optimal, and several challenging open problems are posed. Experimental\nresults are provided to illustrate the theory.\n", "versions": [{"version": "v1", "created": "Mon, 2 Dec 2013 13:41:44 GMT"}, {"version": "v2", "created": "Thu, 5 Dec 2013 13:02:17 GMT"}, {"version": "v3", "created": "Mon, 9 Dec 2013 17:13:01 GMT"}, {"version": "v4", "created": "Sun, 22 Dec 2013 11:23:48 GMT"}, {"version": "v5", "created": "Tue, 21 Jan 2014 08:24:07 GMT"}], "update_date": "2014-01-22", "authors_parsed": [["Berend", "Daniel", ""], ["Kontorovich", "Aryeh", ""]]}, {"id": "1312.0493", "submitter": "Ozan \\.Irsoy", "authors": "Ozan \\.Irsoy, Claire Cardie", "title": "Bidirectional Recursive Neural Networks for Token-Level Labeling with\n  Structure", "comments": "9 pages, 5 figures, NIPS Deep Learning Workshop 2013", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, deep architectures, such as recurrent and recursive neural networks\nhave been successfully applied to various natural language processing tasks.\nInspired by bidirectional recurrent neural networks which use representations\nthat summarize the past and future around an instance, we propose a novel\narchitecture that aims to capture the structural information around an input,\nand use it to label instances. We apply our method to the task of opinion\nexpression extraction, where we employ the binary parse tree of a sentence as\nthe structure, and word vector representations as the initial representation of\na single token. We conduct preliminary experiments to investigate its\nperformance and compare it to the sequential approach.\n", "versions": [{"version": "v1", "created": "Mon, 2 Dec 2013 15:54:40 GMT"}], "update_date": "2013-12-03", "authors_parsed": [["\u0130rsoy", "Ozan", ""], ["Cardie", "Claire", ""]]}, {"id": "1312.0512", "submitter": "Weicong Ding", "authors": "Weicong Ding, Prakash Ishwar, Venkatesh Saligrama, W. Clem Karl", "title": "Sensing-Aware Kernel SVM", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel approach for designing kernels for support vector machines\n(SVMs) when the class label is linked to the observation through a latent state\nand the likelihood function of the observation given the state (the sensing\nmodel) is available. We show that the Bayes-optimum decision boundary is a\nhyperplane under a mapping defined by the likelihood function. Combining this\nwith the maximum margin principle yields kernels for SVMs that leverage\nknowledge of the sensing model in an optimal way. We derive the optimum kernel\nfor the bag-of-words (BoWs) sensing model and demonstrate its superior\nperformance over other kernels in document and image classification tasks.\nThese results indicate that such optimum sensing-aware kernel SVMs can match\nthe performance of rather sophisticated state-of-the-art approaches.\n", "versions": [{"version": "v1", "created": "Mon, 2 Dec 2013 16:47:10 GMT"}, {"version": "v2", "created": "Thu, 13 Mar 2014 12:02:10 GMT"}], "update_date": "2014-03-14", "authors_parsed": [["Ding", "Weicong", ""], ["Ishwar", "Prakash", ""], ["Saligrama", "Venkatesh", ""], ["Karl", "W. Clem", ""]]}, {"id": "1312.0516", "submitter": "Vassilis Kekatos", "authors": "Vassilis Kekatos, Georgios B. Giannakis, Ross Baldick", "title": "Grid Topology Identification using Electricity Prices", "comments": "PES General Meeting 2014 submission", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.SY stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The potential of recovering the topology of a grid using solely publicly\navailable market data is explored here. In contemporary whole-sale electricity\nmarkets, real-time prices are typically determined by solving the\nnetwork-constrained economic dispatch problem. Under a linear DC model,\nlocational marginal prices (LMPs) correspond to the Lagrange multipliers of the\nlinear program involved. The interesting observation here is that the matrix of\nspatiotemporally varying LMPs exhibits the following property: Once\npremultiplied by the weighted grid Laplacian, it yields a low-rank and sparse\nmatrix. Leveraging this rich structure, a regularized maximum likelihood\nestimator (MLE) is developed to recover the grid Laplacian from the LMPs. The\nconvex optimization problem formulated includes low rank- and\nsparsity-promoting regularizers, and it is solved using a scalable algorithm.\nNumerical tests on prices generated for the IEEE 14-bus benchmark provide\nencouraging topology recovery results.\n", "versions": [{"version": "v1", "created": "Mon, 2 Dec 2013 16:58:10 GMT"}, {"version": "v2", "created": "Fri, 14 Feb 2014 00:35:43 GMT"}], "update_date": "2014-02-17", "authors_parsed": [["Kekatos", "Vassilis", ""], ["Giannakis", "Georgios B.", ""], ["Baldick", "Ross", ""]]}, {"id": "1312.0579", "submitter": "Alexander Grubb", "authors": "Alexander Grubb, Daniel Munoz, J. Andrew Bagnell, Martial Hebert", "title": "SpeedMachines: Anytime Structured Prediction", "comments": "17 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Structured prediction plays a central role in machine learning applications\nfrom computational biology to computer vision. These models require\nsignificantly more computation than unstructured models, and, in many\napplications, algorithms may need to make predictions within a computational\nbudget or in an anytime fashion. In this work we propose an anytime technique\nfor learning structured prediction that, at training time, incorporates both\nstructural elements and feature computation trade-offs that affect test-time\ninference. We apply our technique to the challenging problem of scene\nunderstanding in computer vision and demonstrate efficient and anytime\npredictions that gradually improve towards state-of-the-art classification\nperformance as the allotted time increases.\n", "versions": [{"version": "v1", "created": "Mon, 2 Dec 2013 20:26:41 GMT"}], "update_date": "2013-12-03", "authors_parsed": [["Grubb", "Alexander", ""], ["Munoz", "Daniel", ""], ["Bagnell", "J. Andrew", ""], ["Hebert", "Martial", ""]]}, {"id": "1312.0624", "submitter": "Uri Shalit", "authors": "Uri Shalit and Gal Chechik", "title": "Efficient coordinate-descent for orthogonal matrices through Givens\n  rotations", "comments": "A shorter version of this paper will appear in the proceedings of the\n  31st International Conference for Machine Learning (ICML 2014)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Optimizing over the set of orthogonal matrices is a central component in\nproblems like sparse-PCA or tensor decomposition. Unfortunately, such\noptimization is hard since simple operations on orthogonal matrices easily\nbreak orthogonality, and correcting orthogonality usually costs a large amount\nof computation. Here we propose a framework for optimizing orthogonal matrices,\nthat is the parallel of coordinate-descent in Euclidean spaces. It is based on\n{\\em Givens-rotations}, a fast-to-compute operation that affects a small number\nof entries in the learned matrix, and preserves orthogonality. We show two\napplications of this approach: an algorithm for tensor decomposition that is\nused in learning mixture models, and an algorithm for sparse-PCA. We study the\nparameter regime where a Givens rotation approach converges faster and achieves\na superior model on a genome-wide brain-wide mRNA expression dataset.\n", "versions": [{"version": "v1", "created": "Mon, 2 Dec 2013 21:09:40 GMT"}, {"version": "v2", "created": "Fri, 13 Dec 2013 18:47:20 GMT"}], "update_date": "2013-12-16", "authors_parsed": [["Shalit", "Uri", ""], ["Chechik", "Gal", ""]]}, {"id": "1312.0786", "submitter": "Yong Liu", "authors": "Yiyi Liao, Yue Wang, Yong Liu", "title": "Image Representation Learning Using Graph Regularized Auto-Encoders", "comments": "9pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of image representation for the tasks of unsupervised\nlearning and semi-supervised learning. In those learning tasks, the raw image\nvectors may not provide enough representation for their intrinsic structures\ndue to their highly dense feature space. To overcome this problem, the raw\nimage vectors should be mapped to a proper representation space which can\ncapture the latent structure of the original data and represent the data\nexplicitly for further learning tasks such as clustering.\n  Inspired by the recent research works on deep neural network and\nrepresentation learning, in this paper, we introduce the multiple-layer\nauto-encoder into image representation, we also apply the locally invariant\nideal to our image representation with auto-encoders and propose a novel\nmethod, called Graph regularized Auto-Encoder (GAE). GAE can provide a compact\nrepresentation which uncovers the hidden semantics and simultaneously respects\nthe intrinsic geometric structure.\n  Extensive experiments on image clustering show encouraging results of the\nproposed algorithm in comparison to the state-of-the-art algorithms on\nreal-word cases.\n", "versions": [{"version": "v1", "created": "Tue, 3 Dec 2013 11:59:57 GMT"}, {"version": "v2", "created": "Wed, 19 Feb 2014 11:13:57 GMT"}], "update_date": "2014-02-20", "authors_parsed": [["Liao", "Yiyi", ""], ["Wang", "Yue", ""], ["Liu", "Yong", ""]]}, {"id": "1312.0790", "submitter": "Sneha Chaudhari", "authors": "Sneha Chaudhari, Pankaj Dayama, Vinayaka Pandit, Indrajit Bhattacharya", "title": "Test Set Selection using Active Information Acquisition for Predictive\n  Models", "comments": "The paper has been withdrawn by the authors. The current version is\n  incomplete and the work is still on going. The algorithm gives poor results\n  for a particular setting and we are working on it. However, we are not\n  planning to submit a revision of the paper. This work is going to take some\n  time and we want to withdraw the current version since it is not in a good\n  shape and needs a lot more work to be in publishable condition", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we consider active information acquisition when the prediction\nmodel is meant to be applied on a targeted subset of the population. The goal\nis to label a pre-specified fraction of customers in the target or test set by\niteratively querying for information from the non-target or training set. The\nnumber of queries is limited by an overall budget. Arising in the context of\ntwo rather disparate applications- banking and medical diagnosis, we pose the\nactive information acquisition problem as a constrained optimization problem.\nWe propose two greedy iterative algorithms for solving the above problem. We\nconduct experiments with synthetic data and compare results of our proposed\nalgorithms with few other baseline approaches. The experimental results show\nthat our proposed approaches perform better than the baseline schemes.\n", "versions": [{"version": "v1", "created": "Tue, 3 Dec 2013 12:12:23 GMT"}, {"version": "v2", "created": "Fri, 14 Mar 2014 16:36:36 GMT"}], "update_date": "2014-03-17", "authors_parsed": [["Chaudhari", "Sneha", ""], ["Dayama", "Pankaj", ""], ["Pandit", "Vinayaka", ""], ["Bhattacharya", "Indrajit", ""]]}, {"id": "1312.0925", "submitter": "Moritz Hardt", "authors": "Moritz Hardt", "title": "Understanding Alternating Minimization for Matrix Completion", "comments": "Slightly improved main theorem and a correction: The tail bound\n  stated in Lemma A.5 of the previous version is incorrect. See manuscript for\n  fix", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Alternating Minimization is a widely used and empirically successful\nheuristic for matrix completion and related low-rank optimization problems.\nTheoretical guarantees for Alternating Minimization have been hard to come by\nand are still poorly understood. This is in part because the heuristic is\niterative and non-convex in nature. We give a new algorithm based on\nAlternating Minimization that provably recovers an unknown low-rank matrix from\na random subsample of its entries under a standard incoherence assumption. Our\nresults reduce the sample size requirements of the Alternating Minimization\napproach by at least a quartic factor in the rank and the condition number of\nthe unknown matrix. These improvements apply even if the matrix is only close\nto low-rank in the Frobenius norm. Our algorithm runs in nearly linear time in\nthe dimension of the matrix and, in a broad range of parameters, gives the\nstrongest sample bounds among all subquadratic time algorithms that we are\naware of.\n  Underlying our work is a new robust convergence analysis of the well-known\nPower Method for computing the dominant singular vectors of a matrix. This\nviewpoint leads to a conceptually simple understanding of Alternating\nMinimization. In addition, we contribute a new technique for controlling the\ncoherence of intermediate solutions arising in iterative algorithms based on a\nsmoothed analysis of the QR factorization. These techniques may be of interest\nbeyond their application here.\n", "versions": [{"version": "v1", "created": "Tue, 3 Dec 2013 20:37:28 GMT"}, {"version": "v2", "created": "Wed, 9 Apr 2014 21:28:27 GMT"}, {"version": "v3", "created": "Wed, 14 May 2014 19:54:58 GMT"}], "update_date": "2014-05-15", "authors_parsed": [["Hardt", "Moritz", ""]]}, {"id": "1312.1031", "submitter": "Tianbao Yang", "authors": "Tianbao Yang, Shenghuo Zhu, Rong Jin, Yuanqing Lin", "title": "Analysis of Distributed Stochastic Dual Coordinate Ascent", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In \\citep{Yangnips13}, the author presented distributed stochastic dual\ncoordinate ascent (DisDCA) algorithms for solving large-scale regularized loss\nminimization. Extraordinary performances have been observed and reported for\nthe well-motivated updates, as referred to the practical updates, compared to\nthe naive updates. However, no serious analysis has been provided to understand\nthe updates and therefore the convergence rates. In the paper, we bridge the\ngap by providing a theoretical analysis of the convergence rates of the\npractical DisDCA algorithm. Our analysis helped by empirical studies has shown\nthat it could yield an exponential speed-up in the convergence by increasing\nthe number of dual updates at each iteration. This result justifies the\nsuperior performances of the practical DisDCA as compared to the naive variant.\nAs a byproduct, our analysis also reveals the convergence behavior of the\none-communication DisDCA.\n", "versions": [{"version": "v1", "created": "Wed, 4 Dec 2013 05:48:30 GMT"}, {"version": "v2", "created": "Sun, 23 Mar 2014 22:13:17 GMT"}], "update_date": "2014-03-25", "authors_parsed": [["Yang", "Tianbao", ""], ["Zhu", "Shenghuo", ""], ["Jin", "Rong", ""], ["Lin", "Yuanqing", ""]]}, {"id": "1312.1054", "submitter": "Gautam Kamath", "authors": "Constantinos Daskalakis, Gautam Kamath", "title": "Faster and Sample Near-Optimal Algorithms for Proper Learning Mixtures\n  of Gaussians", "comments": "31 pages, to appear in COLT 2014", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.LG math.PR math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We provide an algorithm for properly learning mixtures of two\nsingle-dimensional Gaussians without any separability assumptions. Given\n$\\tilde{O}(1/\\varepsilon^2)$ samples from an unknown mixture, our algorithm\noutputs a mixture that is $\\varepsilon$-close in total variation distance, in\ntime $\\tilde{O}(1/\\varepsilon^5)$. Our sample complexity is optimal up to\nlogarithmic factors, and significantly improves upon both Kalai et al., whose\nalgorithm has a prohibitive dependence on $1/\\varepsilon$, and Feldman et al.,\nwhose algorithm requires bounds on the mixture parameters and depends\npseudo-polynomially in these parameters.\n  One of our main contributions is an improved and generalized algorithm for\nselecting a good candidate distribution from among competing hypotheses.\nNamely, given a collection of $N$ hypotheses containing at least one candidate\nthat is $\\varepsilon$-close to an unknown distribution, our algorithm outputs a\ncandidate which is $O(\\varepsilon)$-close to the distribution. The algorithm\nrequires ${O}(\\log{N}/\\varepsilon^2)$ samples from the unknown distribution and\n${O}(N \\log N/\\varepsilon^2)$ time, which improves previous such results (such\nas the Scheff\\'e estimator) from a quadratic dependence of the running time on\n$N$ to quasilinear. Given the wide use of such results for the purpose of\nhypothesis selection, our improved algorithm implies immediate improvements to\nany such use.\n", "versions": [{"version": "v1", "created": "Wed, 4 Dec 2013 08:31:58 GMT"}, {"version": "v2", "created": "Mon, 28 Apr 2014 16:35:23 GMT"}, {"version": "v3", "created": "Mon, 19 May 2014 13:26:05 GMT"}], "update_date": "2014-05-20", "authors_parsed": [["Daskalakis", "Constantinos", ""], ["Kamath", "Gautam", ""]]}, {"id": "1312.1099", "submitter": "Francesca Petralia", "authors": "Francesca Petralia, Joshua Vogelstein and David B. Dunson", "title": "Multiscale Dictionary Learning for Estimating Conditional Distributions", "comments": null, "journal-ref": "Proceeding of Neural Information Processing Systems, Lake Tahoe,\n  Nevada December 2013", "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Nonparametric estimation of the conditional distribution of a response given\nhigh-dimensional features is a challenging problem. It is important to allow\nnot only the mean but also the variance and shape of the response density to\nchange flexibly with features, which are massive-dimensional. We propose a\nmultiscale dictionary learning model, which expresses the conditional response\ndensity as a convex combination of dictionary densities, with the densities\nused and their weights dependent on the path through a tree decomposition of\nthe feature space. A fast graph partitioning algorithm is applied to obtain the\ntree decomposition, with Bayesian methods then used to adaptively prune and\naverage over different sub-trees in a soft probabilistic manner. The algorithm\nscales efficiently to approximately one million features. State of the art\npredictive performance is demonstrated for toy examples and two neuroscience\napplications including up to a million features.\n", "versions": [{"version": "v1", "created": "Wed, 4 Dec 2013 10:44:01 GMT"}], "update_date": "2013-12-05", "authors_parsed": [["Petralia", "Francesca", ""], ["Vogelstein", "Joshua", ""], ["Dunson", "David B.", ""]]}, {"id": "1312.1121", "submitter": "Jan Palczewski", "authors": "Anna Palczewska and Jan Palczewski and Richard Marchese Robinson and\n  Daniel Neagu", "title": "Interpreting random forest classification models using a feature\n  contribution method", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Model interpretation is one of the key aspects of the model evaluation\nprocess. The explanation of the relationship between model variables and\noutputs is relatively easy for statistical models, such as linear regressions,\nthanks to the availability of model parameters and their statistical\nsignificance. For \"black box\" models, such as random forest, this information\nis hidden inside the model structure. This work presents an approach for\ncomputing feature contributions for random forest classification models. It\nallows for the determination of the influence of each variable on the model\nprediction for an individual instance. By analysing feature contributions for a\ntraining dataset, the most significant variables can be determined and their\ntypical contribution towards predictions made for individual classes, i.e.,\nclass-specific feature contribution \"patterns\", are discovered. These patterns\nrepresent a standard behaviour of the model and allow for an additional\nassessment of the model reliability for a new data. Interpretation of feature\ncontributions for two UCI benchmark datasets shows the potential of the\nproposed methodology. The robustness of results is demonstrated through an\nextensive analysis of feature contributions calculated for a large number of\ngenerated random forest models.\n", "versions": [{"version": "v1", "created": "Wed, 4 Dec 2013 11:57:53 GMT"}], "update_date": "2013-12-05", "authors_parsed": [["Palczewska", "Anna", ""], ["Palczewski", "Jan", ""], ["Robinson", "Richard Marchese", ""], ["Neagu", "Daniel", ""]]}, {"id": "1312.1277", "submitter": "Aleksandrs Slivkins", "authors": "Robert Kleinberg, Aleksandrs Slivkins and Eli Upfal", "title": "Bandits and Experts in Metric Spaces", "comments": "This manuscript is a merged and definitive version of (R. Kleinberg,\n  Slivkins, Upfal: STOC 2008) and (R. Kleinberg, Slivkins: SODA 2010), with a\n  significantly revised presentation", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In a multi-armed bandit problem, an online algorithm chooses from a set of\nstrategies in a sequence of trials so as to maximize the total payoff of the\nchosen strategies. While the performance of bandit algorithms with a small\nfinite strategy set is quite well understood, bandit problems with large\nstrategy sets are still a topic of very active investigation, motivated by\npractical applications such as online auctions and web advertisement. The goal\nof such research is to identify broad and natural classes of strategy sets and\npayoff functions which enable the design of efficient solutions.\n  In this work we study a very general setting for the multi-armed bandit\nproblem in which the strategies form a metric space, and the payoff function\nsatisfies a Lipschitz condition with respect to the metric. We refer to this\nproblem as the \"Lipschitz MAB problem\". We present a solution for the\nmulti-armed bandit problem in this setting. That is, for every metric space we\ndefine an isometry invariant which bounds from below the performance of\nLipschitz MAB algorithms for this metric space, and we present an algorithm\nwhich comes arbitrarily close to meeting this bound. Furthermore, our technique\ngives even better results for benign payoff functions. We also address the\nfull-feedback (\"best expert\") version of the problem, where after every round\nthe payoffs from all arms are revealed.\n", "versions": [{"version": "v1", "created": "Wed, 4 Dec 2013 18:48:00 GMT"}, {"version": "v2", "created": "Thu, 19 Nov 2015 14:26:27 GMT"}, {"version": "v3", "created": "Fri, 27 Apr 2018 22:17:00 GMT"}, {"version": "v4", "created": "Mon, 15 Apr 2019 14:49:36 GMT"}], "update_date": "2019-04-16", "authors_parsed": [["Kleinberg", "Robert", ""], ["Slivkins", "Aleksandrs", ""], ["Upfal", "Eli", ""]]}, {"id": "1312.1530", "submitter": "Nir Ailon", "authors": "Nir Ailon and Kohei Hatano and Eiji Takimoto", "title": "Bandit Online Optimization Over the Permutahedron", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The permutahedron is the convex polytope with vertex set consisting of the\nvectors $(\\pi(1),\\dots, \\pi(n))$ for all permutations (bijections) $\\pi$ over\n$\\{1,\\dots, n\\}$. We study a bandit game in which, at each step $t$, an\nadversary chooses a hidden weight weight vector $s_t$, a player chooses a\nvertex $\\pi_t$ of the permutahedron and suffers an observed loss of\n$\\sum_{i=1}^n \\pi(i) s_t(i)$.\n  A previous algorithm CombBand of Cesa-Bianchi et al (2009) guarantees a\nregret of $O(n\\sqrt{T \\log n})$ for a time horizon of $T$. Unfortunately,\nCombBand requires at each step an $n$-by-$n$ matrix permanent approximation to\nwithin improved accuracy as $T$ grows, resulting in a total running time that\nis super linear in $T$, making it impractical for large time horizons.\n  We provide an algorithm of regret $O(n^{3/2}\\sqrt{T})$ with total time\ncomplexity $O(n^3T)$. The ideas are a combination of CombBand and a recent\nalgorithm by Ailon (2013) for online optimization over the permutahedron in the\nfull information setting. The technical core is a bound on the variance of the\nPlackett-Luce noisy sorting process's \"pseudo loss\". The bound is obtained by\nestablishing positive semi-definiteness of a family of 3-by-3 matrices\ngenerated from rational functions of exponentials of 3 parameters.\n", "versions": [{"version": "v1", "created": "Thu, 5 Dec 2013 13:00:23 GMT"}, {"version": "v2", "created": "Sun, 6 Jul 2014 12:47:36 GMT"}], "update_date": "2014-07-08", "authors_parsed": [["Ailon", "Nir", ""], ["Hatano", "Kohei", ""], ["Takimoto", "Eiji", ""]]}, {"id": "1312.1613", "submitter": "Jim Wang J-Y", "authors": "Jim Jing-Yan Wang", "title": "Max-Min Distance Nonnegative Matrix Factorization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG cs.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Nonnegative Matrix Factorization (NMF) has been a popular representation\nmethod for pattern classification problem. It tries to decompose a nonnegative\nmatrix of data samples as the product of a nonnegative basic matrix and a\nnonnegative coefficient matrix, and the coefficient matrix is used as the new\nrepresentation. However, traditional NMF methods ignore the class labels of the\ndata samples. In this paper, we proposed a supervised novel NMF algorithm to\nimprove the discriminative ability of the new representation. Using the class\nlabels, we separate all the data sample pairs into within-class pairs and\nbetween-class pairs. To improve the discriminate ability of the new NMF\nrepresentations, we hope that the maximum distance of the within-class pairs in\nthe new NMF space could be minimized, while the minimum distance of the\nbetween-class pairs pairs could be maximized. With this criterion, we construct\nan objective function and optimize it with regard to basic and coefficient\nmatrices and slack variables alternatively, resulting in a iterative algorithm.\n", "versions": [{"version": "v1", "created": "Thu, 5 Dec 2013 16:49:05 GMT"}], "update_date": "2013-12-06", "authors_parsed": [["Wang", "Jim Jing-Yan", ""]]}, {"id": "1312.1666", "submitter": "Jakub Kone\\v{c}n\\'y", "authors": "Jakub Kone\\v{c}n\\'y and Peter Richt\\'arik", "title": "Semi-Stochastic Gradient Descent Methods", "comments": "19 pages, 3 figures, 2 algorithms, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG cs.NA math.NA math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we study the problem of minimizing the average of a large\nnumber ($n$) of smooth convex loss functions. We propose a new method, S2GD\n(Semi-Stochastic Gradient Descent), which runs for one or several epochs in\neach of which a single full gradient and a random number of stochastic\ngradients is computed, following a geometric law. The total work needed for the\nmethod to output an $\\varepsilon$-accurate solution in expectation, measured in\nthe number of passes over data, or equivalently, in units equivalent to the\ncomputation of a single gradient of the loss, is\n$O((\\kappa/n)\\log(1/\\varepsilon))$, where $\\kappa$ is the condition number.\nThis is achieved by running the method for $O(\\log(1/\\varepsilon))$ epochs,\nwith a single gradient evaluation and $O(\\kappa)$ stochastic gradient\nevaluations in each. The SVRG method of Johnson and Zhang arises as a special\ncase. If our method is limited to a single epoch only, it needs to evaluate at\nmost $O((\\kappa/\\varepsilon)\\log(1/\\varepsilon))$ stochastic gradients. In\ncontrast, SVRG requires $O(\\kappa/\\varepsilon^2)$ stochastic gradients. To\nillustrate our theoretical results, S2GD only needs the workload equivalent to\nabout 2.1 full gradient evaluations to find an $10^{-6}$-accurate solution for\na problem with $n=10^9$ and $\\kappa=10^3$.\n", "versions": [{"version": "v1", "created": "Thu, 5 Dec 2013 20:04:52 GMT"}, {"version": "v2", "created": "Tue, 16 Jun 2015 05:05:40 GMT"}], "update_date": "2015-06-17", "authors_parsed": [["Kone\u010dn\u00fd", "Jakub", ""], ["Richt\u00e1rik", "Peter", ""]]}, {"id": "1312.1737", "submitter": "J\\'er\\^ome Louradour J\\'er\\^ome Louradour", "authors": "J\\'er\\^ome Louradour and Christopher Kermorvant", "title": "Curriculum Learning for Handwritten Text Line Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recurrent Neural Networks (RNN) have recently achieved the best performance\nin off-line Handwriting Text Recognition. At the same time, learning RNN by\ngradient descent leads to slow convergence, and training times are particularly\nlong when the training database consists of full lines of text. In this paper,\nwe propose an easy way to accelerate stochastic gradient descent in this\nset-up, and in the general context of learning to recognize sequences. The\nprinciple is called Curriculum Learning, or shaping. The idea is to first learn\nto recognize short sequences before training on all available training\nsequences. Experiments on three different handwritten text databases (Rimes,\nIAM, OpenHaRT) show that a simple implementation of this strategy can\nsignificantly speed up the training of RNN for Text Recognition, and even\nsignificantly improve performance in some cases.\n", "versions": [{"version": "v1", "created": "Thu, 5 Dec 2013 23:53:45 GMT"}], "update_date": "2013-12-09", "authors_parsed": [["Louradour", "J\u00e9r\u00f4me", ""], ["Kermorvant", "Christopher", ""]]}, {"id": "1312.1743", "submitter": "Deva Ramanan", "authors": "Deva Ramanan", "title": "Dual coordinate solvers for large-scale structural SVMs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This manuscript describes a method for training linear SVMs (including binary\nSVMs, SVM regression, and structural SVMs) from large, out-of-core training\ndatasets. Current strategies for large-scale learning fall into one of two\ncamps; batch algorithms which solve the learning problem given a finite\ndatasets, and online algorithms which can process out-of-core datasets. The\nformer typically requires datasets small enough to fit in memory. The latter is\noften phrased as a stochastic optimization problem; such algorithms enjoy\nstrong theoretical properties but often require manual tuned annealing\nschedules, and may converge slowly for problems with large output spaces (e.g.,\nstructural SVMs). We discuss an algorithm for an \"intermediate\" regime in which\nthe data is too large to fit in memory, but the active constraints (support\nvectors) are small enough to remain in memory. In this case, one can design\nrather efficient learning algorithms that are as stable as batch algorithms,\nbut capable of processing out-of-core datasets. We have developed such a\nMATLAB-based solver and used it to train a collection of recognition systems\nfor articulated pose estimation, facial analysis, 3D object recognition, and\naction classification, all with publicly-available code. This writeup describes\nthe solver in detail.\n", "versions": [{"version": "v1", "created": "Fri, 6 Dec 2013 00:55:51 GMT"}, {"version": "v2", "created": "Fri, 13 Jun 2014 04:10:06 GMT"}], "update_date": "2014-06-16", "authors_parsed": [["Ramanan", "Deva", ""]]}, {"id": "1312.1847", "submitter": "David Eigen", "authors": "David Eigen, Jason Rolfe, Rob Fergus, Yann LeCun", "title": "Understanding Deep Architectures using a Recursive Convolutional Network", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A key challenge in designing convolutional network models is sizing them\nappropriately. Many factors are involved in these decisions, including number\nof layers, feature maps, kernel sizes, etc. Complicating this further is the\nfact that each of these influence not only the numbers and dimensions of the\nactivation units, but also the total number of parameters. In this paper we\nfocus on assessing the independent contributions of three of these linked\nvariables: The numbers of layers, feature maps, and parameters. To accomplish\nthis, we employ a recursive convolutional network whose weights are tied\nbetween layers; this allows us to vary each of the three factors in a\ncontrolled setting. We find that while increasing the numbers of layers and\nparameters each have clear benefit, the number of feature maps (and hence\ndimensionality of the representation) appears ancillary, and finds most of its\nbenefit through the introduction of more weights. Our results (i) empirically\nconfirm the notion that adding layers alone increases computational power,\nwithin the context of convolutional layers, and (ii) suggest that precise\nsizing of convolutional feature map dimensions is itself of little concern;\nmore attention should be paid to the number of parameters in these layers\ninstead.\n", "versions": [{"version": "v1", "created": "Fri, 6 Dec 2013 12:55:05 GMT"}, {"version": "v2", "created": "Wed, 19 Feb 2014 17:55:37 GMT"}], "update_date": "2014-02-20", "authors_parsed": [["Eigen", "David", ""], ["Rolfe", "Jason", ""], ["Fergus", "Rob", ""], ["LeCun", "Yann", ""]]}, {"id": "1312.1909", "submitter": "Qi Wang", "authors": "Qi Wang and Joseph JaJa", "title": "From Maxout to Channel-Out: Encoding Information on Sparse Pathways", "comments": "10 pages including the appendix, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motivated by an important insight from neural science, we propose a new\nframework for understanding the success of the recently proposed \"maxout\"\nnetworks. The framework is based on encoding information on sparse pathways and\nrecognizing the correct pathway at inference time. Elaborating further on this\ninsight, we propose a novel deep network architecture, called \"channel-out\"\nnetwork, which takes a much better advantage of sparse pathway encoding. In\nchannel-out networks, pathways are not only formed a posteriori, but they are\nalso actively selected according to the inference outputs from the lower\nlayers. From a mathematical perspective, channel-out networks can represent a\nwider class of piece-wise continuous functions, thereby endowing the network\nwith more expressive power than that of maxout networks. We test our\nchannel-out networks on several well-known image classification benchmarks,\nsetting new state-of-the-art performance on CIFAR-100 and STL-10, which\nrepresent some of the \"harder\" image classification benchmarks.\n", "versions": [{"version": "v1", "created": "Mon, 18 Nov 2013 17:56:11 GMT"}], "update_date": "2013-12-09", "authors_parsed": [["Wang", "Qi", ""], ["JaJa", "Joseph", ""]]}, {"id": "1312.2132", "submitter": "Dorsa Sadigh", "authors": "Dorsa Sadigh, Henrik Ohlsson, S. Shankar Sastry, Sanjit A. Seshia", "title": "Robust Subspace System Identification via Weighted Nuclear Norm\n  Optimization", "comments": "Submitted to the IFAC World Congress 2014", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SY cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Subspace identification is a classical and very well studied problem in\nsystem identification. The problem was recently posed as a convex optimization\nproblem via the nuclear norm relaxation. Inspired by robust PCA, we extend this\nframework to handle outliers. The proposed framework takes the form of a convex\noptimization problem with an objective that trades off fit, rank and sparsity.\nAs in robust PCA, it can be problematic to find a suitable regularization\nparameter. We show how the space in which a suitable parameter should be sought\ncan be limited to a bounded open set of the two dimensional parameter space. In\npractice, this is very useful since it restricts the parameter space that is\nneeded to be surveyed.\n", "versions": [{"version": "v1", "created": "Sat, 7 Dec 2013 19:19:03 GMT"}], "update_date": "2013-12-10", "authors_parsed": [["Sadigh", "Dorsa", ""], ["Ohlsson", "Henrik", ""], ["Sastry", "S. Shankar", ""], ["Seshia", "Sanjit A.", ""]]}, {"id": "1312.2137", "submitter": "Dimitri Palaz", "authors": "Dimitri Palaz, Ronan Collobert, Mathew Magimai.-Doss", "title": "End-to-end Phoneme Sequence Recognition using Convolutional Neural\n  Networks", "comments": "NIPS Deep Learning Workshop, 2013", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most phoneme recognition state-of-the-art systems rely on a classical neural\nnetwork classifiers, fed with highly tuned features, such as MFCC or PLP\nfeatures. Recent advances in ``deep learning'' approaches questioned such\nsystems, but while some attempts were made with simpler features such as\nspectrograms, state-of-the-art systems still rely on MFCCs. This might be\nviewed as a kind of failure from deep learning approaches, which are often\nclaimed to have the ability to train with raw signals, alleviating the need of\nhand-crafted features. In this paper, we investigate a convolutional neural\nnetwork approach for raw speech signals. While convolutional architectures got\ntremendous success in computer vision or text processing, they seem to have\nbeen let down in the past recent years in the speech processing field. We show\nthat it is possible to learn an end-to-end phoneme sequence classifier system\ndirectly from raw signal, with similar performance on the TIMIT and WSJ\ndatasets than existing systems based on MFCC, questioning the need of complex\nhand-crafted features on large datasets.\n", "versions": [{"version": "v1", "created": "Sat, 7 Dec 2013 19:55:02 GMT"}], "update_date": "2013-12-10", "authors_parsed": [["Palaz", "Dimitri", ""], ["Collobert", "Ronan", ""], ["-Doss", "Mathew Magimai.", ""]]}, {"id": "1312.2154", "submitter": "Koji Eguchi", "authors": "Tomoki Kobayashi, Koji Eguchi", "title": "Sequential Monte Carlo Inference of Mixed Membership Stochastic\n  Blockmodels for Dynamic Social Networks", "comments": "NIPS 2013 Workshop on Frontiers of Network Analysis: Methods, Models,\n  and Applications", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many kinds of data can be represented as a network or graph. It is crucial to\ninfer the latent structure underlying such a network and to predict unobserved\nlinks in the network. Mixed Membership Stochastic Blockmodel (MMSB) is a\npromising model for network data. Latent variables and unknown parameters in\nMMSB have been estimated through Bayesian inference with the entire network;\nhowever, it is important to estimate them online for evolving networks. In this\npaper, we first develop online inference methods for MMSB through sequential\nMonte Carlo methods, also known as particle filters. We then extend them for\ntime-evolving networks, taking into account the temporal dependency of the\nnetwork structure. We demonstrate through experiments that the time-dependent\nparticle filter outperformed several baselines in terms of prediction\nperformance in an online condition.\n", "versions": [{"version": "v1", "created": "Sat, 7 Dec 2013 23:42:55 GMT"}], "update_date": "2013-12-10", "authors_parsed": [["Kobayashi", "Tomoki", ""], ["Eguchi", "Koji", ""]]}, {"id": "1312.2164", "submitter": "Le Song", "authors": "Nan Du, Yingyu Liang, Maria Florina Balcan, Le Song", "title": "Budgeted Influence Maximization for Multiple Products", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.SI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The typical algorithmic problem in viral marketing aims to identify a set of\ninfluential users in a social network, who, when convinced to adopt a product,\nshall influence other users in the network and trigger a large cascade of\nadoptions. However, the host (the owner of an online social platform) often\nfaces more constraints than a single product, endless user attentions,\nunlimited budget and unbounded time; in reality, multiple products need to be\nadvertised, each user can tolerate only a small number of recommendations,\ninfluencing user has a cost and advertisers have only limited budgets, and the\nadoptions need to be maximized within a short time window.\n  Given theses myriads of user, monetary, and timing constraints, it is\nextremely challenging for the host to design principled and efficient viral\nmarket algorithms with provable guarantees. In this paper, we provide a novel\nsolution by formulating the problem as a submodular maximization in a\ncontinuous-time diffusion model under an intersection of a matroid and multiple\nknapsack constraints. We also propose an adaptive threshold greedy algorithm\nwhich can be faster than the traditional greedy algorithm with lazy evaluation,\nand scalable to networks with million of nodes. Furthermore, our mathematical\nformulation allows us to prove that the algorithm can achieve an approximation\nfactor of $k_a/(2+2 k)$ when $k_a$ out of the $k$ knapsack constraints are\nactive, which also improves over previous guarantees from combinatorial\noptimization literature. In the case when influencing each user has uniform\ncost, the approximation becomes even better to a factor of $1/3$. Extensive\nsynthetic and real world experiments demonstrate that our budgeted influence\nmaximization algorithm achieves the-state-of-the-art in terms of both\neffectiveness and scalability, often beating the next best by significant\nmargins.\n", "versions": [{"version": "v1", "created": "Sun, 8 Dec 2013 01:58:39 GMT"}, {"version": "v2", "created": "Wed, 16 Apr 2014 03:53:04 GMT"}], "update_date": "2014-04-17", "authors_parsed": [["Du", "Nan", ""], ["Liang", "Yingyu", ""], ["Balcan", "Maria Florina", ""], ["Song", "Le", ""]]}, {"id": "1312.2171", "submitter": "Adam Kapelner", "authors": "Adam Kapelner and Justin Bleich", "title": "bartMachine: Machine Learning with Bayesian Additive Regression Trees", "comments": "39 pages, 13 figures, 4 tables, 2 appendices", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a new package in R implementing Bayesian additive regression trees\n(BART). The package introduces many new features for data analysis using BART\nsuch as variable selection, interaction detection, model diagnostic plots,\nincorporation of missing data and the ability to save trees for future\nprediction. It is significantly faster than the current R implementation,\nparallelized, and capable of handling both large sample sizes and\nhigh-dimensional data.\n", "versions": [{"version": "v1", "created": "Sun, 8 Dec 2013 03:40:47 GMT"}, {"version": "v2", "created": "Fri, 28 Feb 2014 18:38:50 GMT"}, {"version": "v3", "created": "Mon, 24 Nov 2014 19:21:22 GMT"}], "update_date": "2014-11-25", "authors_parsed": [["Kapelner", "Adam", ""], ["Bleich", "Justin", ""]]}, {"id": "1312.2177", "submitter": "Mahdi Zamani", "authors": "Mahdi Zamani and Mahnush Movahedi", "title": "Machine Learning Techniques for Intrusion Detection", "comments": "11 pages", "journal-ref": null, "doi": null, "report-no": "UNM-48066-F13", "categories": "cs.CR cs.LG cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An Intrusion Detection System (IDS) is a software that monitors a single or a\nnetwork of computers for malicious activities (attacks) that are aimed at\nstealing or censoring information or corrupting network protocols. Most\ntechniques used in today's IDS are not able to deal with the dynamic and\ncomplex nature of cyber attacks on computer networks. Hence, efficient adaptive\nmethods like various techniques of machine learning can result in higher\ndetection rates, lower false alarm rates and reasonable computation and\ncommunication costs. In this paper, we study several such schemes and compare\ntheir performance. We divide the schemes into methods based on classical\nartificial intelligence (AI) and methods based on computational intelligence\n(CI). We explain how various characteristics of CI techniques can be used to\nbuild efficient IDS.\n", "versions": [{"version": "v1", "created": "Sun, 8 Dec 2013 06:56:21 GMT"}, {"version": "v2", "created": "Sat, 9 May 2015 06:07:35 GMT"}], "update_date": "2015-05-12", "authors_parsed": [["Zamani", "Mahdi", ""], ["Movahedi", "Mahnush", ""]]}, {"id": "1312.2451", "submitter": "Sarwat Nizamani", "authors": "Sarwat Nizamani, Nasrullah Memon", "title": "CEAI: CCM based Email Authorship Identification Model", "comments": null, "journal-ref": "Egyptian Informatics Journal,Volume 14, Issue 3, November 2013", "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  In this paper we present a model for email authorship identification (EAI) by\nemploying a Cluster-based Classification (CCM) technique. Traditionally,\nstylometric features have been successfully employed in various authorship\nanalysis tasks; we extend the traditional feature-set to include some more\ninteresting and effective features for email authorship identification (e.g.\nthe last punctuation mark used in an email, the tendency of an author to use\ncapitalization at the start of an email, or the punctuation after a greeting or\nfarewell). We also included Info Gain feature selection based content features.\nIt is observed that the use of such features in the authorship identification\nprocess has a positive impact on the accuracy of the authorship identification\ntask. We performed experiments to justify our arguments and compared the\nresults with other base line models. Experimental results reveal that the\nproposed CCM-based email authorship identification model, along with the\nproposed feature set, outperforms the state-of-the-art support vector machine\n(SVM)-based models, as well as the models proposed by Iqbal et al. [1, 2]. The\nproposed model attains an accuracy rate of 94% for 10 authors, 89% for 25\nauthors, and 81% for 50 authors, respectively on Enron dataset, while 89.5%\naccuracy has been achieved on authors' constructed real email dataset. The\nresults on Enron dataset have been achieved on quite a large number of authors\nas compared to the models proposed by Iqbal et al. [1, 2].\n", "versions": [{"version": "v1", "created": "Fri, 6 Dec 2013 18:25:15 GMT"}], "update_date": "2013-12-10", "authors_parsed": [["Nizamani", "Sarwat", ""], ["Memon", "Nasrullah", ""]]}, {"id": "1312.2482", "submitter": "Jesse Berwald", "authors": "Jesse Berwald, Marian Gidea and Mikael Vejdemo-Johansson", "title": "Automatic recognition and tagging of topologically different regimes in\n  dynamical systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CG cs.LG math.DS nlin.CD physics.data-an", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Complex systems are commonly modeled using nonlinear dynamical systems. These\nmodels are often high-dimensional and chaotic. An important goal in studying\nphysical systems through the lens of mathematical models is to determine when\nthe system undergoes changes in qualitative behavior. A detailed description of\nthe dynamics can be difficult or impossible to obtain for high-dimensional and\nchaotic systems. Therefore, a more sensible goal is to recognize and mark\ntransitions of a system between qualitatively different regimes of behavior. In\npractice, one is interested in developing techniques for detection of such\ntransitions from sparse observations, possibly contaminated by noise. In this\npaper we develop a framework to accurately tag different regimes of complex\nsystems based on topological features. In particular, our framework works with\na high degree of success in picking out a cyclically orbiting regime from a\nstationary equilibrium regime in high-dimensional stochastic dynamical systems.\n", "versions": [{"version": "v1", "created": "Mon, 9 Dec 2013 16:02:23 GMT"}, {"version": "v2", "created": "Mon, 24 Mar 2014 14:33:37 GMT"}], "update_date": "2014-03-25", "authors_parsed": [["Berwald", "Jesse", ""], ["Gidea", "Marian", ""], ["Vejdemo-Johansson", "Mikael", ""]]}, {"id": "1312.2578", "submitter": "Cong Li", "authors": "Cong Li, Michael Georgiopoulos, Georgios C. Anagnostopoulos", "title": "Kernel-based Distance Metric Learning in the Output Space", "comments": "11 pages, 7 figures, appeared in the Proceedings of 2013\n  International Joint Conference on Neural Networks (IJCNN)", "journal-ref": null, "doi": "10.1109/IJCNN.2013.6706862", "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we present two related, kernel-based Distance Metric Learning\n(DML) methods. Their respective models non-linearly map data from their\noriginal space to an output space, and subsequent distance measurements are\nperformed in the output space via a Mahalanobis metric. The dimensionality of\nthe output space can be directly controlled to facilitate the learning of a\nlow-rank metric. Both methods allow for simultaneous inference of the\nassociated metric and the mapping to the output space, which can be used to\nvisualize the data, when the output space is 2- or 3-dimensional. Experimental\nresults for a collection of classification tasks illustrate the advantages of\nthe proposed methods over other traditional and kernel-based DML approaches.\n", "versions": [{"version": "v1", "created": "Mon, 9 Dec 2013 20:58:16 GMT"}, {"version": "v2", "created": "Mon, 28 Apr 2014 20:08:47 GMT"}], "update_date": "2014-04-30", "authors_parsed": [["Li", "Cong", ""], ["Georgiopoulos", "Michael", ""], ["Anagnostopoulos", "Georgios C.", ""]]}, {"id": "1312.2606", "submitter": "Cong Li", "authors": "Cong Li, Michael Georgiopoulos, Georgios C. Anagnostopoulos", "title": "Multi-Task Classification Hypothesis Space with Improved Generalization\n  Bounds", "comments": "18 pages, 4 figures, submitted to IEEE Transactions on Neural\n  Networks and Learning Systems", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a RKHS, in general, of vector-valued functions intended\nto be used as hypothesis space for multi-task classification. It extends\nsimilar hypothesis spaces that have previously considered in the literature.\nAssuming this space, an improved Empirical Rademacher Complexity-based\ngeneralization bound is derived. The analysis is itself extended to an MKL\nsetting. The connection between the proposed hypothesis space and a Group-Lasso\ntype regularizer is discussed. Finally, experimental results, with some\nSVM-based Multi-Task Learning problems, underline the quality of the derived\nbounds and validate the paper's analysis.\n", "versions": [{"version": "v1", "created": "Mon, 9 Dec 2013 21:27:23 GMT"}], "update_date": "2013-12-11", "authors_parsed": [["Li", "Cong", ""], ["Georgiopoulos", "Michael", ""], ["Anagnostopoulos", "Georgios C.", ""]]}, {"id": "1312.2710", "submitter": "Anugrah Kumar", "authors": "Sarvesh SS Rawat, Dheeraj Dilip Mor, Anugrah Kumar, Sanjiban Shekar\n  Roy, Rohit kumar", "title": "Improving circuit miniaturization and its efficiency using Rough Set\n  Theory", "comments": "The International Conference on Machine Intelligence Research and\n  Advancement,ICMIRA-2013", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  High-speed, accuracy, meticulousness and quick response are notion of the\nvital necessities for modern digital world. An efficient electronic circuit\nunswervingly affects the maneuver of the whole system. Different tools are\nrequired to unravel different types of engineering tribulations. Improving the\nefficiency, accuracy and low power consumption in an electronic circuit is\nalways been a bottle neck problem. So the need of circuit miniaturization is\nalways there. It saves a lot of time and power that is wasted in switching of\ngates, the wiring-crises is reduced, cross-sectional area of chip is reduced,\nthe number of transistors that can implemented in chip is multiplied many\nfolds. Therefore to trounce with this problem we have proposed an Artificial\nintelligence (AI) based approach that make use of Rough Set Theory for its\nimplementation. Theory of rough set has been proposed by Z Pawlak in the year\n1982. Rough set theory is a new mathematical tool which deals with uncertainty\nand vagueness. Decisions can be generated using rough set theory by reducing\nthe unwanted and superfluous data. We have condensed the number of gates\nwithout upsetting the productivity of the given circuit. This paper proposes an\napproach with the help of rough set theory which basically lessens the number\nof gates in the circuit, based on decision rules.\n", "versions": [{"version": "v1", "created": "Tue, 10 Dec 2013 08:11:14 GMT"}], "update_date": "2013-12-11", "authors_parsed": [["Rawat", "Sarvesh SS", ""], ["Mor", "Dheeraj Dilip", ""], ["Kumar", "Anugrah", ""], ["Roy", "Sanjiban Shekar", ""], ["kumar", "Rohit", ""]]}, {"id": "1312.2789", "submitter": "Chanabasayya Vastrad M", "authors": "Doreswamy and Chanabasayya .M. Vastrad", "title": "Performance Analysis Of Regularized Linear Regression Models For\n  Oxazolines And Oxazoles Derivitive Descriptor Dataset", "comments": null, "journal-ref": "published International Journal of Computational Science and\n  Information Technology (IJCSITY) Vol.1, No.4, November 2013", "doi": "10.5121/ijcsity.2013.1408", "report-no": null, "categories": "cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  Regularized regression techniques for linear regression have been created the\nlast few ten years to reduce the flaws of ordinary least squares regression\nwith regard to prediction accuracy. In this paper, new methods for using\nregularized regression in model choice are introduced, and we distinguish the\nconditions in which regularized regression develops our ability to discriminate\nmodels. We applied all the five methods that use penalty-based (regularization)\nshrinkage to handle Oxazolines and Oxazoles derivatives descriptor dataset with\nfar more predictors than observations. The lasso, ridge, elasticnet, lars and\nrelaxed lasso further possess the desirable property that they simultaneously\nselect relevant predictive descriptors and optimally estimate their effects.\nHere, we comparatively evaluate the performance of five regularized linear\nregression methods The assessment of the performance of each model by means of\nbenchmark experiments is an established exercise. Cross-validation and\nresampling methods are generally used to arrive point evaluates the\nefficiencies which are compared to recognize methods with acceptable features.\nPredictive accuracy was evaluated using the root mean squared error (RMSE) and\nSquare of usual correlation between predictors and observed mean inhibitory\nconcentration of antitubercular activity (R square). We found that all five\nregularized regression models were able to produce feasible models and\nefficient capturing the linearity in the data. The elastic net and lars had\nsimilar accuracies as well as lasso and relaxed lasso had similar accuracies\nbut outperformed ridge regression in terms of the RMSE and R square metrics.\n", "versions": [{"version": "v1", "created": "Tue, 10 Dec 2013 13:16:02 GMT"}], "update_date": "2013-12-13", "authors_parsed": [["Doreswamy", "", ""], ["Vastrad", "Chanabasayya . M.", ""]]}, {"id": "1312.2936", "submitter": "Julian Togelius", "authors": "Julian Togelius, Noor Shaker, Georgios N. Yannakakis", "title": "Active Player Modelling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We argue for the use of active learning methods for player modelling. In\nactive learning, the learning algorithm chooses where to sample the search\nspace so as to optimise learning progress. We hypothesise that player modelling\nbased on active learning could result in vastly more efficient learning, but\nwill require big changes in how data is collected. Some example active player\nmodelling scenarios are described. A particular form of active learning is also\nequivalent to an influential formalisation of (human and machine) curiosity,\nand games with active learning could therefore be seen as being curious about\nthe player. We further hypothesise that this form of curiosity is symmetric,\nand therefore that games that explore their players based on the principles of\nactive learning will turn out to select game configurations that are\ninteresting to the player that is being explored.\n", "versions": [{"version": "v1", "created": "Tue, 10 Dec 2013 20:36:04 GMT"}], "update_date": "2013-12-11", "authors_parsed": [["Togelius", "Julian", ""], ["Shaker", "Noor", ""], ["Yannakakis", "Georgios N.", ""]]}, {"id": "1312.2988", "submitter": "Jinbo Xu", "authors": "Jianzhu Ma, Sheng Wang, Zhiyong Wang and Jinbo Xu", "title": "Protein Contact Prediction by Integrating Joint Evolutionary Coupling\n  Analysis and Supervised Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.QM cs.LG math.OC q-bio.BM stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Protein contacts contain important information for protein structure and\nfunctional study, but contact prediction from sequence remains very\nchallenging. Both evolutionary coupling (EC) analysis and supervised machine\nlearning methods are developed to predict contacts, making use of different\ntypes of information, respectively. This paper presents a group graphical lasso\n(GGL) method for contact prediction that integrates joint multi-family EC\nanalysis and supervised learning. Different from existing single-family EC\nanalysis that uses residue co-evolution information in only the target protein\nfamily, our joint EC analysis uses residue co-evolution in both the target\nfamily and its related families, which may have divergent sequences but similar\nfolds. To implement joint EC analysis, we model a set of related protein\nfamilies using Gaussian graphical models (GGM) and then co-estimate their\nprecision matrices by maximum-likelihood, subject to the constraint that the\nprecision matrices shall share similar residue co-evolution patterns. To\nfurther improve the accuracy of the estimated precision matrices, we employ a\nsupervised learning method to predict contact probability from a variety of\nevolutionary and non-evolutionary information and then incorporate the\npredicted probability as prior into our GGL framework. Experiments show that\nour method can predict contacts much more accurately than existing methods, and\nthat our method performs better on both conserved and family-specific contacts.\n", "versions": [{"version": "v1", "created": "Tue, 10 Dec 2013 22:45:06 GMT"}, {"version": "v2", "created": "Thu, 12 Dec 2013 15:12:55 GMT"}, {"version": "v3", "created": "Tue, 14 Jan 2014 03:39:26 GMT"}, {"version": "v4", "created": "Wed, 15 Jan 2014 01:47:21 GMT"}, {"version": "v5", "created": "Wed, 8 Apr 2015 14:21:09 GMT"}], "update_date": "2015-04-09", "authors_parsed": [["Ma", "Jianzhu", ""], ["Wang", "Sheng", ""], ["Wang", "Zhiyong", ""], ["Xu", "Jinbo", ""]]}, {"id": "1312.3386", "submitter": "Yoshikazu Terada", "authors": "Yoshikazu Terada", "title": "Clustering for high-dimension, low-sample size data using distance\n  vectors", "comments": "16 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In high-dimension, low-sample size (HDLSS) data, it is not always true that\ncloseness of two objects reflects a hidden cluster structure. We point out the\nimportant fact that it is not the closeness, but the \"values\" of distance that\ncontain information of the cluster structure in high-dimensional space. Based\non this fact, we propose an efficient and simple clustering approach, called\ndistance vector clustering, for HDLSS data. Under the assumptions given in the\nwork of Hall et al. (2005), we show the proposed approach provides a true\ncluster label under milder conditions when the dimension tends to infinity with\nthe sample size fixed. The effectiveness of the distance vector clustering\napproach is illustrated through a numerical experiment and real data analysis.\n", "versions": [{"version": "v1", "created": "Thu, 12 Dec 2013 02:37:36 GMT"}, {"version": "v2", "created": "Wed, 25 Dec 2013 13:10:44 GMT"}], "update_date": "2013-12-30", "authors_parsed": [["Terada", "Yoshikazu", ""]]}, {"id": "1312.3388", "submitter": "Tianlin Shi", "authors": "Tianlin Shi and Jun Zhu", "title": "Online Bayesian Passive-Aggressive Learning", "comments": "10 Pages. ICML 2014, Beijing, China", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  Online Passive-Aggressive (PA) learning is an effective framework for\nperforming max-margin online learning. But the deterministic formulation and\nestimated single large-margin model could limit its capability in discovering\ndescriptive structures underlying complex data. This pa- per presents online\nBayesian Passive-Aggressive (BayesPA) learning, which subsumes the online PA\nand extends naturally to incorporate latent variables and perform nonparametric\nBayesian inference, thus providing great flexibility for explorative analysis.\nWe apply BayesPA to topic modeling and derive efficient online learning\nalgorithms for max-margin topic models. We further develop nonparametric\nmethods to resolve the number of topics. Experimental results on real datasets\nshow that our approaches significantly improve time efficiency while\nmaintaining comparable results with the batch counterparts.\n", "versions": [{"version": "v1", "created": "Thu, 12 Dec 2013 02:46:07 GMT"}], "update_date": "2013-12-13", "authors_parsed": [["Shi", "Tianlin", ""], ["Zhu", "Jun", ""]]}, {"id": "1312.3393", "submitter": "Masrour Zoghi", "authors": "Masrour Zoghi, Shimon Whiteson, Remi Munos, Maarten de Rijke", "title": "Relative Upper Confidence Bound for the K-Armed Dueling Bandit Problem", "comments": "13 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a new method for the K-armed dueling bandit problem, a\nvariation on the regular K-armed bandit problem that offers only relative\nfeedback about pairs of arms. Our approach extends the Upper Confidence Bound\nalgorithm to the relative setting by using estimates of the pairwise\nprobabilities to select a promising arm and applying Upper Confidence Bound\nwith the winner as a benchmark. We prove a finite-time regret bound of order\nO(log t). In addition, our empirical results using real data from an\ninformation retrieval application show that it greatly outperforms the state of\nthe art.\n", "versions": [{"version": "v1", "created": "Thu, 12 Dec 2013 03:08:46 GMT"}, {"version": "v2", "created": "Tue, 17 Dec 2013 10:30:42 GMT"}], "update_date": "2013-12-18", "authors_parsed": [["Zoghi", "Masrour", ""], ["Whiteson", "Shimon", ""], ["Munos", "Remi", ""], ["de Rijke", "Maarten", ""]]}, {"id": "1312.3429", "submitter": "Kishore Konda", "authors": "Kishore Konda, Roland Memisevic", "title": "Unsupervised learning of depth and motion", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a model for the joint estimation of disparity and motion. The\nmodel is based on learning about the interrelations between images from\nmultiple cameras, multiple frames in a video, or the combination of both. We\nshow that learning depth and motion cues, as well as their combinations, from\ndata is possible within a single type of architecture and a single type of\nlearning algorithm, by using biologically inspired \"complex cell\" like units,\nwhich encode correlations between the pixels across image pairs. Our\nexperimental results show that the learning of depth and motion makes it\npossible to achieve state-of-the-art performance in 3-D activity analysis, and\nto outperform existing hand-engineered 3-D motion features by a very large\nmargin.\n", "versions": [{"version": "v1", "created": "Thu, 12 Dec 2013 10:03:47 GMT"}, {"version": "v2", "created": "Mon, 16 Dec 2013 16:11:52 GMT"}], "update_date": "2013-12-17", "authors_parsed": [["Konda", "Kishore", ""], ["Memisevic", "Roland", ""]]}, {"id": "1312.3522", "submitter": "Weizhi  Lu", "authors": "Weizhi Lu and Weiyu Li and Kidiyo Kpalma and Joseph Ronsin", "title": "Sparse Matrix-based Random Projection for Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  As a typical dimensionality reduction technique, random projection can be\nsimply implemented with linear projection, while maintaining the pairwise\ndistances of high-dimensional data with high probability. Considering this\ntechnique is mainly exploited for the task of classification, this paper is\ndeveloped to study the construction of random matrix from the viewpoint of\nfeature selection, rather than of traditional distance preservation. This\nyields a somewhat surprising theoretical result, that is, the sparse random\nmatrix with exactly one nonzero element per column, can present better feature\nselection performance than other more dense matrices, if the projection\ndimension is sufficiently large (namely, not much smaller than the number of\nfeature elements); otherwise, it will perform comparably to others. For random\nprojection, this theoretical result implies considerable improvement on both\ncomplexity and performance, which is widely confirmed with the classification\nexperiments on both synthetic data and real data.\n", "versions": [{"version": "v1", "created": "Thu, 12 Dec 2013 15:26:57 GMT"}, {"version": "v2", "created": "Sun, 30 Mar 2014 13:56:04 GMT"}, {"version": "v3", "created": "Sun, 12 Oct 2014 22:10:13 GMT"}], "update_date": "2014-10-14", "authors_parsed": [["Lu", "Weizhi", ""], ["Li", "Weiyu", ""], ["Kpalma", "Kidiyo", ""], ["Ronsin", "Joseph", ""]]}, {"id": "1312.3811", "submitter": "Frank Sehnke", "authors": "Frank Sehnke", "title": "Efficient Baseline-free Sampling in Parameter Exploring Policy\n  Gradients: Super Symmetric PGPE", "comments": "Artificial Neural Networks and Machine Learning - ICANN 2013 Springer\n  Berlin Heidelberg 2013. 130-137", "journal-ref": null, "doi": "10.1007/978-3-642-40728-4_17", "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Policy Gradient methods that explore directly in parameter space are among\nthe most effective and robust direct policy search methods and have drawn a lot\nof attention lately. The basic method from this field, Policy Gradients with\nParameter-based Exploration, uses two samples that are symmetric around the\ncurrent hypothesis to circumvent misleading reward in \\emph{asymmetrical}\nreward distributed problems gathered with the usual baseline approach. The\nexploration parameters are still updated by a baseline approach - leaving the\nexploration prone to asymmetric reward distributions. In this paper we will\nshow how the exploration parameters can be sampled quasi symmetric despite\nhaving limited instead of free parameters for exploration. We give a\ntransformation approximation to get quasi symmetric samples with respect to the\nexploration without changing the overall sampling distribution. Finally we will\ndemonstrate that sampling symmetrically also for the exploration parameters is\nsuperior in needs of samples and robustness than the original sampling\napproach.\n", "versions": [{"version": "v1", "created": "Fri, 13 Dec 2013 14:10:30 GMT"}], "update_date": "2013-12-16", "authors_parsed": [["Sehnke", "Frank", ""]]}, {"id": "1312.3903", "submitter": "Marlos C. Machado", "authors": "Marlos C. Machado", "title": "A Methodology for Player Modeling based on Machine Learning", "comments": "Thesis presented by Marlos C. Machado as part of the requirements for\n  the degree or Master of Science in Computer Science granted by the\n  Universidade Federal de Minas Gerais. February, 18th, 2013", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  AI is gradually receiving more attention as a fundamental feature to increase\nthe immersion in digital games. Among the several AI approaches, player\nmodeling is becoming an important one. The main idea is to understand and model\nthe player characteristics and behaviors in order to develop a better AI. In\nthis work, we discuss several aspects of this new field. We proposed a taxonomy\nto organize the area, discussing several facets of this topic, ranging from\nimplementation decisions up to what a model attempts to describe. We then\nclassify, in our taxonomy, some of the most important works in this field. We\nalso presented a generic approach to deal with player modeling using ML, and we\ninstantiated this approach to model players' preferences in the game\nCivilization IV. The instantiation of this approach has several steps. We first\ndiscuss a generic representation, regardless of what is being modeled, and\nevaluate it performing experiments with the strategy game Civilization IV.\nContinuing the instantiation of the proposed approach we evaluated the\napplicability of using game score information to distinguish different\npreferences. We presented a characterization of virtual agents in the game,\ncomparing their behavior with their stated preferences. Once we have\ncharacterized these agents, we were able to observe that different preferences\ngenerate different behaviors, measured by several game indicators. We then\ntackled the preference modeling problem as a binary classification task, with a\nsupervised learning approach. We compared four different methods, based on\ndifferent paradigms (SVM, AdaBoost, NaiveBayes and JRip), evaluating them on a\nset of matches played by different virtual agents. We conclude our work using\nthe learned models to infer human players' preferences. Using some of the\nevaluated classifiers we obtained accuracies over 60% for most of the inferred\npreferences.\n", "versions": [{"version": "v1", "created": "Fri, 13 Dec 2013 18:32:51 GMT"}], "update_date": "2013-12-16", "authors_parsed": [["Machado", "Marlos C.", ""]]}, {"id": "1312.3970", "submitter": "Michael Smith", "authors": "Michael R. Smith and Tony Martinez", "title": "An Extensive Evaluation of Filtering Misclassified Instances in\n  Supervised Classification Tasks", "comments": "29 pages, 3 Figures, 20 Tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Removing or filtering outliers and mislabeled instances prior to training a\nlearning algorithm has been shown to increase classification accuracy. A\npopular approach for handling outliers and mislabeled instances is to remove\nany instance that is misclassified by a learning algorithm. However, an\nexamination of which learning algorithms to use for filtering as well as their\neffects on multiple learning algorithms over a large set of data sets has not\nbeen done. Previous work has generally been limited due to the large\ncomputational requirements to run such an experiment, and, thus, the\nexamination has generally been limited to learning algorithms that are\ncomputationally inexpensive and using a small number of data sets. In this\npaper, we examine 9 learning algorithms as filtering algorithms as well as\nexamining the effects of filtering in the 9 chosen learning algorithms on a set\nof 54 data sets. In addition to using each learning algorithm individually as a\nfilter, we also use the set of learning algorithms as an ensemble filter and\nuse an adaptive algorithm that selects a subset of the learning algorithms for\nfiltering for a specific task and learning algorithm. We find that for most\ncases, using an ensemble of learning algorithms for filtering produces the\ngreatest increase in classification accuracy. We also compare filtering with a\nmajority voting ensemble. The voting ensemble significantly outperforms\nfiltering unless there are high amounts of noise present in the data set.\nAdditionally, we find that a majority voting ensemble is robust to noise as\nfiltering with a voting ensemble does not increase the classification accuracy\nof the voting ensemble.\n", "versions": [{"version": "v1", "created": "Fri, 13 Dec 2013 21:59:00 GMT"}], "update_date": "2013-12-17", "authors_parsed": [["Smith", "Michael R.", ""], ["Martinez", "Tony", ""]]}, {"id": "1312.3989", "submitter": "Nima Hatami", "authors": "Nima Hatami and Camelia Chira", "title": "Classifiers With a Reject Option for Early Time-Series Classification", "comments": null, "journal-ref": "Computational Intelligence and Ensemble Learning (CIEL), IEEE\n  Symposium on, 9-16, 2013", "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Early classification of time-series data in a dynamic environment is a\nchallenging problem of great importance in signal processing. This paper\nproposes a classifier architecture with a reject option capable of online\ndecision making without the need to wait for the entire time series signal to\nbe present. The main idea is to classify an odor/gas signal with an acceptable\naccuracy as early as possible. Instead of using posterior probability of a\nclassifier, the proposed method uses the \"agreement\" of an ensemble to decide\nwhether to accept or reject the candidate label. The introduced algorithm is\napplied to the bio-chemistry problem of odor classification to build a novel\nElectronic-Nose called Forefront-Nose. Experimental results on wind tunnel\ntest-bed facility confirms the robustness of the forefront-nose compared to the\nstandard classifiers from both earliness and recognition perspectives.\n", "versions": [{"version": "v1", "created": "Sat, 14 Dec 2013 00:28:32 GMT"}], "update_date": "2013-12-17", "authors_parsed": [["Hatami", "Nima", ""], ["Chira", "Camelia", ""]]}, {"id": "1312.3990", "submitter": "Nima Hatami", "authors": "Nima Hatami, Reza Ebrahimpour, Reza Ghaderi", "title": "ECOC-Based Training of Neural Networks for Face Recognition", "comments": null, "journal-ref": "Cybernetics and Intelligent Systems, IEEE Conference on, 450-454,\n  2008", "doi": "10.1109/ICCIS.2008.4670763", "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Error Correcting Output Codes, ECOC, is an output representation method\ncapable of discovering some of the errors produced in classification tasks.\nThis paper describes the application of ECOC to the training of feed forward\nneural networks, FFNN, for improving the overall accuracy of classification\nsystems. Indeed, to improve the generalization of FFNN classifiers, this paper\nproposes an ECOC-Based training method for Neural Networks that use ECOC as the\noutput representation, and adopts the traditional Back-Propagation algorithm,\nBP, to adjust weights of the network. Experimental results for face recognition\nproblem on Yale database demonstrate the effectiveness of our method. With a\nrejection scheme defined by a simple robustness rate, high reliability is\nachieved in this application.\n", "versions": [{"version": "v1", "created": "Sat, 14 Dec 2013 00:29:36 GMT"}], "update_date": "2016-11-18", "authors_parsed": [["Hatami", "Nima", ""], ["Ebrahimpour", "Reza", ""], ["Ghaderi", "Reza", ""]]}, {"id": "1312.4092", "submitter": "Edouard Grave", "authors": "Edouard Grave (LIENS, INRIA Paris - Rocquencourt), Guillaume Obozinski\n  (LIGM), Francis Bach (LIENS, INRIA Paris - Rocquencourt)", "title": "Domain adaptation for sequence labeling using hidden Markov models", "comments": "New Directions in Transfer and Multi-Task: Learning Across Domains\n  and Tasks (NIPS Workshop) (2013)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most natural language processing systems based on machine learning are not\nrobust to domain shift. For example, a state-of-the-art syntactic dependency\nparser trained on Wall Street Journal sentences has an absolute drop in\nperformance of more than ten points when tested on textual data from the Web.\nAn efficient solution to make these methods more robust to domain shift is to\nfirst learn a word representation using large amounts of unlabeled data from\nboth domains, and then use this representation as features in a supervised\nlearning algorithm. In this paper, we propose to use hidden Markov models to\nlearn word representations for part-of-speech tagging. In particular, we study\nthe influence of using data from the source, the target or both domains to\nlearn the representation and the different ways to represent words using an\nHMM.\n", "versions": [{"version": "v1", "created": "Sat, 14 Dec 2013 21:48:49 GMT"}], "update_date": "2013-12-17", "authors_parsed": [["Grave", "Edouard", "", "LIENS, INRIA Paris - Rocquencourt"], ["Obozinski", "Guillaume", "", "LIGM"], ["Bach", "Francis", "", "LIENS, INRIA Paris - Rocquencourt"]]}, {"id": "1312.4108", "submitter": "F. Ozgur Catak", "authors": "Ferhat \\\"Ozg\\\"ur \\c{C}atak, Mehmet Erdal Balaban", "title": "A MapReduce based distributed SVM algorithm for binary classification", "comments": "19 Pages. arXiv admin note: text overlap with arXiv:1301.0082", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Although Support Vector Machine (SVM) algorithm has a high generalization\nproperty to classify for unseen examples after training phase and it has small\nloss value, the algorithm is not suitable for real-life classification and\nregression problems. SVMs cannot solve hundreds of thousands examples in\ntraining dataset. In previous studies on distributed machine learning\nalgorithms, SVM is trained over a costly and preconfigured computer\nenvironment. In this research, we present a MapReduce based distributed\nparallel SVM training algorithm for binary classification problems. This work\nshows how to distribute optimization problem over cloud computing systems with\nMapReduce technique. In the second step of this work, we used statistical\nlearning theory to find the predictive hypothesis that minimize our empirical\nrisks from hypothesis spaces that created with reduce function of MapReduce.\nThe results of this research are important for training of big datasets for SVM\nalgorithm based classification problems. We provided that iterative training of\nsplit dataset with MapReduce technique; accuracy of the classifier function\nwill converge to global optimal classifier function's accuracy in finite\niteration size. The algorithm performance was measured on samples from letter\nrecognition and pen-based recognition of handwritten digits dataset.\n", "versions": [{"version": "v1", "created": "Sun, 15 Dec 2013 05:42:51 GMT"}], "update_date": "2013-12-17", "authors_parsed": [["\u00c7atak", "Ferhat \u00d6zg\u00fcr", ""], ["Balaban", "Mehmet Erdal", ""]]}, {"id": "1312.4176", "submitter": "Gabriele Oliva", "authors": "Gabriele Oliva, Roberto Setola, and Christoforos N. Hadjicostis", "title": "Distributed k-means algorithm", "comments": "preprint submitted to IEEE transactions on mobile computing", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we provide a fully distributed implementation of the k-means\nclustering algorithm, intended for wireless sensor networks where each agent is\nendowed with a possibly high-dimensional observation (e.g., position, humidity,\ntemperature, etc.) The proposed algorithm, by means of one-hop communication,\npartitions the agents into measure-dependent groups that have small in-group\nand large out-group \"distances\". Since the partitions may not have a relation\nwith the topology of the network--members of the same clusters may not be\nspatially close--the algorithm is provided with a mechanism to compute the\nclusters'centroids even when the clusters are disconnected in several\nsub-clusters.The results of the proposed distributed algorithm coincide, in\nterms of minimization of the objective function, with the centralized k-means\nalgorithm. Some numerical examples illustrate the capabilities of the proposed\nsolution.\n", "versions": [{"version": "v1", "created": "Sun, 15 Dec 2013 18:08:27 GMT"}, {"version": "v2", "created": "Thu, 9 Jan 2014 22:38:49 GMT"}, {"version": "v3", "created": "Mon, 10 Nov 2014 13:36:34 GMT"}], "update_date": "2014-11-11", "authors_parsed": [["Oliva", "Gabriele", ""], ["Setola", "Roberto", ""], ["Hadjicostis", "Christoforos N.", ""]]}, {"id": "1312.4209", "submitter": "Richard Davis", "authors": "Richard Davis, Sanjay Chawla, Philip Leong", "title": "Feature Graph Architectures", "comments": "9 pages, with 5 pages of supplementary material (appendices)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this article we propose feature graph architectures (FGA), which are deep\nlearning systems employing a structured initialisation and training method\nbased on a feature graph which facilitates improved generalisation performance\ncompared with a standard shallow architecture. The goal is to explore\nalternative perspectives on the problem of deep network training. We evaluate\nFGA performance for deep SVMs on some experimental datasets, and show how\ngeneralisation and stability results may be derived for these models. We\ndescribe the effect of permutations on the model accuracy, and give a criterion\nfor the optimal permutation in terms of feature correlations. The experimental\nresults show that the algorithm produces robust and significant test set\nimprovements over a standard shallow SVM training method for a range of\ndatasets. These gains are achieved with a moderate increase in time complexity.\n", "versions": [{"version": "v1", "created": "Sun, 15 Dec 2013 23:40:49 GMT"}], "update_date": "2013-12-17", "authors_parsed": [["Davis", "Richard", ""], ["Chawla", "Sanjay", ""], ["Leong", "Philip", ""]]}, {"id": "1312.4314", "submitter": "David Eigen", "authors": "David Eigen, Marc'Aurelio Ranzato, Ilya Sutskever", "title": "Learning Factored Representations in a Deep Mixture of Experts", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mixtures of Experts combine the outputs of several \"expert\" networks, each of\nwhich specializes in a different part of the input space. This is achieved by\ntraining a \"gating\" network that maps each input to a distribution over the\nexperts. Such models show promise for building larger networks that are still\ncheap to compute at test time, and more parallelizable at training time. In\nthis this work, we extend the Mixture of Experts to a stacked model, the Deep\nMixture of Experts, with multiple sets of gating and experts. This\nexponentially increases the number of effective experts by associating each\ninput with a combination of experts at each layer, yet maintains a modest model\nsize. On a randomly translated version of the MNIST dataset, we find that the\nDeep Mixture of Experts automatically learns to develop location-dependent\n(\"where\") experts at the first layer, and class-specific (\"what\") experts at\nthe second layer. In addition, we see that the different combinations are in\nuse when the model is applied to a dataset of speech monophones. These\ndemonstrate effective use of all expert combinations.\n", "versions": [{"version": "v1", "created": "Mon, 16 Dec 2013 11:15:10 GMT"}, {"version": "v2", "created": "Wed, 19 Feb 2014 17:57:53 GMT"}, {"version": "v3", "created": "Sun, 9 Mar 2014 20:15:03 GMT"}], "update_date": "2014-03-11", "authors_parsed": [["Eigen", "David", ""], ["Ranzato", "Marc'Aurelio", ""], ["Sutskever", "Ilya", ""]]}, {"id": "1312.4384", "submitter": "Eren Golge", "authors": "Eren Golge and Pinar Duygulu", "title": "Rectifying Self Organizing Maps for Automatic Concept Learning from Web\n  Images", "comments": "present CVPR2014 submission", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We attack the problem of learning concepts automatically from noisy web image\nsearch results. Going beyond low level attributes, such as colour and texture,\nwe explore weakly-labelled datasets for the learning of higher level concepts,\nsuch as scene categories. The idea is based on discovering common\ncharacteristics shared among subsets of images by posing a method that is able\nto organise the data while eliminating irrelevant instances. We propose a novel\nclustering and outlier detection method, namely Rectifying Self Organizing Maps\n(RSOM). Given an image collection returned for a concept query, RSOM provides\nclusters pruned from outliers. Each cluster is used to train a model\nrepresenting a different characteristics of the concept. The proposed method\noutperforms the state-of-the-art studies on the task of learning low-level\nconcepts, and it is competitive in learning higher level concepts as well. It\nis capable to work at large scale with no supervision through exploiting the\navailable sources.\n", "versions": [{"version": "v1", "created": "Mon, 16 Dec 2013 14:51:00 GMT"}], "update_date": "2013-12-17", "authors_parsed": [["Golge", "Eren", ""], ["Duygulu", "Pinar", ""]]}, {"id": "1312.4400", "submitter": "Min Lin", "authors": "Min Lin, Qiang Chen, Shuicheng Yan", "title": "Network In Network", "comments": "10 pages, 4 figures, for iclr2014", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel deep network structure called \"Network In Network\" (NIN)\nto enhance model discriminability for local patches within the receptive field.\nThe conventional convolutional layer uses linear filters followed by a\nnonlinear activation function to scan the input. Instead, we build micro neural\nnetworks with more complex structures to abstract the data within the receptive\nfield. We instantiate the micro neural network with a multilayer perceptron,\nwhich is a potent function approximator. The feature maps are obtained by\nsliding the micro networks over the input in a similar manner as CNN; they are\nthen fed into the next layer. Deep NIN can be implemented by stacking mutiple\nof the above described structure. With enhanced local modeling via the micro\nnetwork, we are able to utilize global average pooling over feature maps in the\nclassification layer, which is easier to interpret and less prone to\noverfitting than traditional fully connected layers. We demonstrated the\nstate-of-the-art classification performances with NIN on CIFAR-10 and\nCIFAR-100, and reasonable performances on SVHN and MNIST datasets.\n", "versions": [{"version": "v1", "created": "Mon, 16 Dec 2013 15:34:13 GMT"}, {"version": "v2", "created": "Wed, 18 Dec 2013 09:30:27 GMT"}, {"version": "v3", "created": "Tue, 4 Mar 2014 05:15:42 GMT"}], "update_date": "2014-03-05", "authors_parsed": [["Lin", "Min", ""], ["Chen", "Qiang", ""], ["Yan", "Shuicheng", ""]]}, {"id": "1312.4405", "submitter": "Xiao-Lei Zhang", "authors": "Xiao-Lei Zhang", "title": "Learning Deep Representations By Distributed Random Samplings", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose an extremely simple deep model for the unsupervised\nnonlinear dimensionality reduction -- deep distributed random samplings, which\nperforms like a stack of unsupervised bootstrap aggregating. First, its network\nstructure is novel: each layer of the network is a group of mutually\nindependent $k$-centers clusterings. Second, its learning method is extremely\nsimple: the $k$ centers of each clustering are only $k$ randomly selected\nexamples from the training data; for small-scale data sets, the $k$ centers are\nfurther randomly reconstructed by a simple cyclic-shift operation. Experimental\nresults on nonlinear dimensionality reduction show that the proposed method can\nlearn abstract representations on both large-scale and small-scale problems,\nand meanwhile is much faster than deep neural networks on large-scale problems.\n", "versions": [{"version": "v1", "created": "Mon, 16 Dec 2013 15:40:05 GMT"}], "update_date": "2013-12-17", "authors_parsed": [["Zhang", "Xiao-Lei", ""]]}, {"id": "1312.4426", "submitter": "R.J. Vanderbei", "authors": "Robert Vanderbei and Han Liu and Lie Wang and Kevin Lin", "title": "Optimization for Compressed Sensing: the Simplex Method and Kronecker\n  Sparsification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we present two new approaches to efficiently solve large-scale\ncompressed sensing problems. These two ideas are independent of each other and\ncan therefore be used either separately or together. We consider all\npossibilities.\n  For the first approach, we note that the zero vector can be taken as the\ninitial basic (infeasible) solution for the linear programming problem and\ntherefore, if the true signal is very sparse, some variants of the simplex\nmethod can be expected to take only a small number of pivots to arrive at a\nsolution. We implemented one such variant and demonstrate a dramatic\nimprovement in computation time on very sparse signals.\n  The second approach requires a redesigned sensing mechanism in which the\nvector signal is stacked into a matrix. This allows us to exploit the Kronecker\ncompressed sensing (KCS) mechanism. We show that the Kronecker sensing requires\nstronger conditions for perfect recovery compared to the original vector\nproblem. However, the Kronecker sensing, modeled correctly, is a much sparser\nlinear optimization problem. Hence, algorithms that benefit from sparse problem\nrepresentation, such as interior-point methods, can solve the Kronecker sensing\nproblems much faster than the corresponding vector problem. In our numerical\nstudies, we demonstrate a ten-fold improvement in the computation time.\n", "versions": [{"version": "v1", "created": "Mon, 16 Dec 2013 16:51:51 GMT"}], "update_date": "2013-12-17", "authors_parsed": [["Vanderbei", "Robert", ""], ["Liu", "Han", ""], ["Wang", "Lie", ""], ["Lin", "Kevin", ""]]}, {"id": "1312.4461", "submitter": "Andrew Davis", "authors": "Andrew Davis, Itamar Arel", "title": "Low-Rank Approximations for Conditional Feedforward Computation in Deep\n  Neural Networks", "comments": "10 pages, 5 figures. Submitted to ICLR 2014", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Scalability properties of deep neural networks raise key research questions,\nparticularly as the problems considered become larger and more challenging.\nThis paper expands on the idea of conditional computation introduced by Bengio,\net. al., where the nodes of a deep network are augmented by a set of gating\nunits that determine when a node should be calculated. By factorizing the\nweight matrix into a low-rank approximation, an estimation of the sign of the\npre-nonlinearity activation can be efficiently obtained. For networks using\nrectified-linear hidden units, this implies that the computation of a hidden\nunit with an estimated negative pre-nonlinearity can be ommitted altogether, as\nits value will become zero when nonlinearity is applied. For sparse neural\nnetworks, this can result in considerable speed gains. Experimental results\nusing the MNIST and SVHN data sets with a fully-connected deep neural network\ndemonstrate the performance robustness of the proposed scheme with respect to\nthe error introduced by the conditional computation process.\n", "versions": [{"version": "v1", "created": "Mon, 16 Dec 2013 18:58:34 GMT"}, {"version": "v2", "created": "Wed, 18 Dec 2013 18:11:21 GMT"}, {"version": "v3", "created": "Sat, 21 Dec 2013 16:57:47 GMT"}, {"version": "v4", "created": "Tue, 28 Jan 2014 22:29:55 GMT"}], "update_date": "2014-01-30", "authors_parsed": [["Davis", "Andrew", ""], ["Arel", "Itamar", ""]]}, {"id": "1312.4479", "submitter": "Jean-Baptiste Durand", "authors": "Pierre Fernique (VP, AGAP), Jean-Baptiste Durand (VP, INRIA Grenoble\n  Rh\\^one-Alpes / LJK Laboratoire Jean Kuntzmann), Yann Gu\\'edon (VP, AGAP)", "title": "Parametric Modelling of Multivariate Count Data Using Probabilistic\n  Graphical Models", "comments": null, "journal-ref": "3rd Workshop on Algorithmic issues for Inference in Graphical\n  Models - AIGM13, Paris : France (2013)", "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multivariate count data are defined as the number of items of different\ncategories issued from sampling within a population, which individuals are\ngrouped into categories. The analysis of multivariate count data is a recurrent\nand crucial issue in numerous modelling problems, particularly in the fields of\nbiology and ecology (where the data can represent, for example, children counts\nassociated with multitype branching processes), sociology and econometrics. We\nfocus on I) Identifying categories that appear simultaneously, or on the\ncontrary that are mutually exclusive. This is achieved by identifying\nconditional independence relationships between the variables; II)Building\nparsimonious parametric models consistent with these relationships; III)\nCharacterising and testing the effects of covariates on the joint distribution\nof the counts. To achieve these goals, we propose an approach based on\ngraphical probabilistic models, and more specifically partially directed\nacyclic graphs.\n", "versions": [{"version": "v1", "created": "Mon, 16 Dec 2013 19:38:35 GMT"}], "update_date": "2013-12-17", "authors_parsed": [["Fernique", "Pierre", "", "VP, AGAP"], ["Durand", "Jean-Baptiste", "", "VP, INRIA Grenoble\n  Rh\u00f4ne-Alpes / LJK Laboratoire Jean Kuntzmann"], ["Gu\u00e9don", "Yann", "", "VP, AGAP"]]}, {"id": "1312.4527", "submitter": "Khoat Than", "authors": "Khoat Than and Tu Bao Ho", "title": "Probable convexity and its application to Correlated Topic Models", "comments": "22 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Non-convex optimization problems often arise from probabilistic modeling,\nsuch as estimation of posterior distributions. Non-convexity makes the problems\nintractable, and poses various obstacles for us to design efficient algorithms.\nIn this work, we attack non-convexity by first introducing the concept of\n\\emph{probable convexity} for analyzing convexity of real functions in\npractice. We then use the new concept to analyze an inference problem in the\n\\emph{Correlated Topic Model} (CTM) and related nonconjugate models. Contrary\nto the existing belief of intractability, we show that this inference problem\nis concave under certain conditions. One consequence of our analyses is a novel\nalgorithm for learning CTM which is significantly more scalable and qualitative\nthan existing methods. Finally, we highlight that stochastic gradient\nalgorithms might be a practical choice to resolve efficiently non-convex\nproblems. This finding might find beneficial in many contexts which are beyond\nprobabilistic modeling.\n", "versions": [{"version": "v1", "created": "Mon, 16 Dec 2013 09:34:43 GMT"}], "update_date": "2013-12-18", "authors_parsed": [["Than", "Khoat", ""], ["Ho", "Tu Bao", ""]]}, {"id": "1312.4551", "submitter": "Aram Galstyan", "authors": "Armen E. Allahverdyan and Aram Galstyan", "title": "Comparative Analysis of Viterbi Training and Maximum Likelihood\n  Estimation for HMMs", "comments": "Appeared in Neural Information Processing Systems (NIPS) 2011", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an asymptotic analysis of Viterbi Training (VT) and contrast it\nwith a more conventional Maximum Likelihood (ML) approach to parameter\nestimation in Hidden Markov Models. While ML estimator works by (locally)\nmaximizing the likelihood of the observed data, VT seeks to maximize the\nprobability of the most likely hidden state sequence. We develop an analytical\nframework based on a generating function formalism and illustrate it on an\nexactly solvable model of HMM with one unambiguous symbol. For this particular\nmodel the ML objective function is continuously degenerate. VT objective, in\ncontrast, is shown to have only finite degeneracy. Furthermore, VT converges\nfaster and results in sparser (simpler) models, thus realizing an automatic\nOccam's razor for HMM learning. For more general scenario VT can be worse\ncompared to ML but still capable of correctly recovering most of the\nparameters.\n", "versions": [{"version": "v1", "created": "Mon, 16 Dec 2013 21:03:28 GMT"}], "update_date": "2013-12-18", "authors_parsed": [["Allahverdyan", "Armen E.", ""], ["Galstyan", "Aram", ""]]}, {"id": "1312.4564", "submitter": "Peilin Zhao", "authors": "Peilin Zhao, Jinwei Yang, Tong Zhang, Ping Li", "title": "Adaptive Stochastic Alternating Direction Method of Multipliers", "comments": "13 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Alternating Direction Method of Multipliers (ADMM) has been studied for\nyears. The traditional ADMM algorithm needs to compute, at each iteration, an\n(empirical) expected loss function on all training examples, resulting in a\ncomputational complexity proportional to the number of training examples. To\nreduce the time complexity, stochastic ADMM algorithms were proposed to replace\nthe expected function with a random loss function associated with one uniformly\ndrawn example plus a Bregman divergence. The Bregman divergence, however, is\nderived from a simple second order proximal function, the half squared norm,\nwhich could be a suboptimal choice.\n  In this paper, we present a new family of stochastic ADMM algorithms with\noptimal second order proximal functions, which produce a new family of adaptive\nsubgradient methods. We theoretically prove that their regret bounds are as\ngood as the bounds which could be achieved by the best proximal function that\ncan be chosen in hindsight. Encouraging empirical results on a variety of\nreal-world datasets confirm the effectiveness and efficiency of the proposed\nalgorithms.\n", "versions": [{"version": "v1", "created": "Mon, 16 Dec 2013 21:22:46 GMT"}, {"version": "v2", "created": "Sun, 22 Dec 2013 01:59:05 GMT"}, {"version": "v3", "created": "Thu, 5 Jun 2014 07:03:48 GMT"}, {"version": "v4", "created": "Mon, 9 Jun 2014 09:31:13 GMT"}], "update_date": "2014-06-10", "authors_parsed": [["Zhao", "Peilin", ""], ["Yang", "Jinwei", ""], ["Zhang", "Tong", ""], ["Li", "Ping", ""]]}, {"id": "1312.4569", "submitter": "Vu Pham", "authors": "Vu Pham, Th\\'eodore Bluche, Christopher Kermorvant, J\\'er\\^ome\n  Louradour", "title": "Dropout improves Recurrent Neural Networks for Handwriting Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recurrent neural networks (RNNs) with Long Short-Term memory cells currently\nhold the best known results in unconstrained handwriting recognition. We show\nthat their performance can be greatly improved using dropout - a recently\nproposed regularization method for deep architectures. While previous works\nshowed that dropout gave superior performance in the context of convolutional\nnetworks, it had never been applied to RNNs. In our approach, dropout is\ncarefully used in the network so that it does not affect the recurrent\nconnections, hence the power of RNNs in modeling sequence is preserved.\nExtensive experiments on a broad range of handwritten databases confirm the\neffectiveness of dropout on deep architectures even when the network mainly\nconsists of recurrent and shared connections.\n", "versions": [{"version": "v1", "created": "Tue, 5 Nov 2013 10:45:48 GMT"}, {"version": "v2", "created": "Mon, 10 Mar 2014 15:34:55 GMT"}], "update_date": "2014-03-11", "authors_parsed": [["Pham", "Vu", ""], ["Bluche", "Th\u00e9odore", ""], ["Kermorvant", "Christopher", ""], ["Louradour", "J\u00e9r\u00f4me", ""]]}, {"id": "1312.4599", "submitter": "Arka Bhattacharya", "authors": "Arka Bhattacharya", "title": "Evolution and Computational Learning Theory: A survey on Valiant's paper", "comments": "17 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Darwin's theory of evolution is considered to be one of the greatest\nscientific gems in modern science. It not only gives us a description of how\nliving things evolve, but also shows how a population evolves through time and\nalso, why only the fittest individuals continue the generation forward. The\npaper basically gives a high level analysis of the works of Valiant[1]. Though,\nwe know the mechanisms of evolution, but it seems that there does not exist any\nstrong quantitative and mathematical theory of the evolution of certain\nmechanisms. What is defined exactly as the fitness of an individual, why is\nthat only certain individuals in a population tend to mutate, how computation\nis done in finite time when we have exponentially many examples: there seems to\nbe a lot of questions which need to be answered. [1] basically treats Darwinian\ntheory as a form of computational learning theory, which calculates the net\nfitness of the hypotheses and thus distinguishes functions and their classes\nwhich could be evolvable using polynomial amount of resources. Evolution is\nconsidered as a function of the environment and the previous evolutionary\nstages that chooses the best hypothesis using learning techniques that makes\nmutation possible and hence, gives a quantitative idea that why only the\nfittest individuals tend to survive and have the power to mutate.\n", "versions": [{"version": "v1", "created": "Tue, 17 Dec 2013 00:32:43 GMT"}], "update_date": "2013-12-18", "authors_parsed": [["Bhattacharya", "Arka", ""]]}, {"id": "1312.4626", "submitter": "Alex Gittens", "authors": "Raffay Hamid and Ying Xiao and Alex Gittens and Dennis DeCoste", "title": "Compact Random Feature Maps", "comments": "9 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Kernel approximation using randomized feature maps has recently gained a lot\nof interest. In this work, we identify that previous approaches for polynomial\nkernel approximation create maps that are rank deficient, and therefore do not\nutilize the capacity of the projected feature space effectively. To address\nthis challenge, we propose compact random feature maps (CRAFTMaps) to\napproximate polynomial kernels more concisely and accurately. We prove the\nerror bounds of CRAFTMaps demonstrating their superior kernel reconstruction\nperformance compared to the previous approximation schemes. We show how\nstructured random matrices can be used to efficiently generate CRAFTMaps, and\npresent a single-pass algorithm using CRAFTMaps to learn non-linear multi-class\nclassifiers. We present experiments on multiple standard data-sets with\nperformance competitive with state-of-the-art results.\n", "versions": [{"version": "v1", "created": "Tue, 17 Dec 2013 03:33:08 GMT"}], "update_date": "2013-12-18", "authors_parsed": [["Hamid", "Raffay", ""], ["Xiao", "Ying", ""], ["Gittens", "Alex", ""], ["DeCoste", "Dennis", ""]]}, {"id": "1312.4695", "submitter": "Wiktor Mlynarski", "authors": "Wiktor Mlynarski", "title": "Sparse, complex-valued representations of natural sounds learned with\n  phase and amplitude continuity priors", "comments": "11 + 7 pages This version includes changes suggested by ICLR 2014\n  reviewers", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.SD q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Complex-valued sparse coding is a data representation which employs a\ndictionary of two-dimensional subspaces, while imposing a sparse, factorial\nprior on complex amplitudes. When trained on a dataset of natural image\npatches, it learns phase invariant features which closely resemble receptive\nfields of complex cells in the visual cortex. Features trained on natural\nsounds however, rarely reveal phase invariance and capture other aspects of the\ndata. This observation is a starting point of the present work. As its first\ncontribution, it provides an analysis of natural sound statistics by means of\nlearning sparse, complex representations of short speech intervals. Secondly,\nit proposes priors over the basis function set, which bias them towards\nphase-invariant solutions. In this way, a dictionary of complex basis functions\ncan be learned from the data statistics, while preserving the phase invariance\nproperty. Finally, representations trained on speech sounds with and without\npriors are compared. Prior-based basis functions reveal performance comparable\nto unconstrained sparse coding, while explicitely representing phase as a\ntemporal shift. Such representations can find applications in many perceptual\nand machine learning tasks.\n", "versions": [{"version": "v1", "created": "Tue, 17 Dec 2013 09:12:55 GMT"}, {"version": "v2", "created": "Wed, 18 Dec 2013 10:48:17 GMT"}, {"version": "v3", "created": "Tue, 18 Feb 2014 10:20:25 GMT"}], "update_date": "2014-02-19", "authors_parsed": [["Mlynarski", "Wiktor", ""]]}, {"id": "1312.4986", "submitter": "Michael Smith", "authors": "Michael R. Smith and Tony Martinez", "title": "A Comparative Evaluation of Curriculum Learning with Filtering and\n  Boosting", "comments": "19 pages, 2 figures, 6 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Not all instances in a data set are equally beneficial for inferring a model\nof the data. Some instances (such as outliers) are detrimental to inferring a\nmodel of the data. Several machine learning techniques treat instances in a\ndata set differently during training such as curriculum learning, filtering,\nand boosting. However, an automated method for determining how beneficial an\ninstance is for inferring a model of the data does not exist. In this paper, we\npresent an automated method that orders the instances in a data set by\ncomplexity based on the their likelihood of being misclassified (instance\nhardness). The underlying assumption of this method is that instances with a\nhigh likelihood of being misclassified represent more complex concepts in a\ndata set. Ordering the instances in a data set allows a learning algorithm to\nfocus on the most beneficial instances and ignore the detrimental ones. We\ncompare ordering the instances in a data set in curriculum learning, filtering\nand boosting. We find that ordering the instances significantly increases\nclassification accuracy and that filtering has the largest impact on\nclassification accuracy. On a set of 52 data sets, ordering the instances\nincreases the average accuracy from 81% to 84%.\n", "versions": [{"version": "v1", "created": "Tue, 17 Dec 2013 22:12:52 GMT"}], "update_date": "2013-12-19", "authors_parsed": [["Smith", "Michael R.", ""], ["Martinez", "Tony", ""]]}, {"id": "1312.5021", "submitter": "Zhen Qin", "authors": "Zhen Qin, Vaclav Petricek, Nikos Karampatziakis, Lihong Li, John\n  Langford", "title": "Efficient Online Bootstrapping for Large Scale Learning", "comments": "5 pages, appeared at Big Learning Workshop at Neural Information\n  Processing Systems 2013", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bootstrapping is a useful technique for estimating the uncertainty of a\npredictor, for example, confidence intervals for prediction. It is typically\nused on small to moderate sized datasets, due to its high computation cost.\nThis work describes a highly scalable online bootstrapping strategy,\nimplemented inside Vowpal Wabbit, that is several times faster than traditional\nstrategies. Our experiments indicate that, in addition to providing a black\nbox-like method for estimating uncertainty, our implementation of online\nbootstrapping may also help to train models with better prediction performance\ndue to model averaging.\n", "versions": [{"version": "v1", "created": "Wed, 18 Dec 2013 02:10:21 GMT"}], "update_date": "2013-12-19", "authors_parsed": [["Qin", "Zhen", ""], ["Petricek", "Vaclav", ""], ["Karampatziakis", "Nikos", ""], ["Li", "Lihong", ""], ["Langford", "John", ""]]}, {"id": "1312.5023", "submitter": "Matt Wytock", "authors": "Matt Wytock and J. Zico Kolter", "title": "Contextually Supervised Source Separation with Application to Energy\n  Disaggregation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new framework for single-channel source separation that lies\nbetween the fully supervised and unsupervised setting. Instead of supervision,\nwe provide input features for each source signal and use convex methods to\nestimate the correlations between these features and the unobserved signal\ndecomposition. We analyze the case of $\\ell_2$ loss theoretically and show that\nrecovery of the signal components depends only on cross-correlation between\nfeatures for different signals, not on correlations between features for the\nsame signal. Contextually supervised source separation is a natural fit for\ndomains with large amounts of data but no explicit supervision; our motivating\napplication is energy disaggregation of hourly smart meter data (the separation\nof whole-home power signals into different energy uses). Here we apply\ncontextual supervision to disaggregate the energy usage of thousands homes over\nfour years, a significantly larger scale than previously published efforts, and\ndemonstrate on synthetic data that our method outperforms the unsupervised\napproach.\n", "versions": [{"version": "v1", "created": "Wed, 18 Dec 2013 02:21:01 GMT"}], "update_date": "2013-12-19", "authors_parsed": [["Wytock", "Matt", ""], ["Kolter", "J. Zico", ""]]}, {"id": "1312.5124", "submitter": "Paul Fogel", "authors": "Paul Fogel", "title": "Permuted NMF: A Simple Algorithm Intended to Minimize the Volume of the\n  Score Matrix", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Non-Negative Matrix Factorization, NMF, attempts to find a number of\narchetypal response profiles, or parts, such that any sample profile in the\ndataset can be approximated by a close profile among these archetypes or a\nlinear combination of these profiles. The non-negativity constraint is imposed\nwhile estimating archetypal profiles, due to the non-negative nature of the\nobserved signal. Apart from non negativity, a volume constraint can be applied\non the Score matrix W to enhance the ability of learning parts of NMF. In this\nreport, we describe a very simple algorithm, which in effect achieves volume\nminimization, although indirectly.\n", "versions": [{"version": "v1", "created": "Wed, 18 Dec 2013 13:13:39 GMT"}], "update_date": "2013-12-19", "authors_parsed": [["Fogel", "Paul", ""]]}, {"id": "1312.5179", "submitter": "Leonardo Jost", "authors": "Matthias Hein, Simon Setzer, Leonardo Jost, Syama Sundar Rangapuram", "title": "The Total Variation on Hypergraphs - Learning on Hypergraphs Revisited", "comments": "Long version of paper accepted at NIPS 2013", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hypergraphs allow one to encode higher-order relationships in data and are\nthus a very flexible modeling tool. Current learning methods are either based\non approximations of the hypergraphs via graphs or on tensor methods which are\nonly applicable under special conditions. In this paper, we present a new\nlearning framework on hypergraphs which fully uses the hypergraph structure.\nThe key element is a family of regularization functionals based on the total\nvariation on hypergraphs.\n", "versions": [{"version": "v1", "created": "Wed, 18 Dec 2013 15:35:32 GMT"}], "update_date": "2013-12-19", "authors_parsed": [["Hein", "Matthias", ""], ["Setzer", "Simon", ""], ["Jost", "Leonardo", ""], ["Rangapuram", "Syama Sundar", ""]]}, {"id": "1312.5192", "submitter": "Leonardo Jost", "authors": "Leonardo Jost, Simon Setzer, Matthias Hein", "title": "Nonlinear Eigenproblems in Data Analysis - Balanced Graph Cuts and the\n  RatioDCA-Prox", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It has been recently shown that a large class of balanced graph cuts allows\nfor an exact relaxation into a nonlinear eigenproblem. We review briefly some\nof these results and propose a family of algorithms to compute nonlinear\neigenvectors which encompasses previous work as special cases. We provide a\ndetailed analysis of the properties and the convergence behavior of these\nalgorithms and then discuss their application in the area of balanced graph\ncuts.\n", "versions": [{"version": "v1", "created": "Wed, 18 Dec 2013 16:01:25 GMT"}, {"version": "v2", "created": "Mon, 24 Mar 2014 10:10:42 GMT"}], "update_date": "2014-03-25", "authors_parsed": [["Jost", "Leonardo", ""], ["Setzer", "Simon", ""], ["Hein", "Matthias", ""]]}, {"id": "1312.5198", "submitter": "Ashutosh Modi", "authors": "Ashutosh Modi and Ivan Titov", "title": "Learning Semantic Script Knowledge with Event Embeddings", "comments": "4 Pages, 1 figure, ICLR Workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CL stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Induction of common sense knowledge about prototypical sequences of events\nhas recently received much attention. Instead of inducing this knowledge in the\nform of graphs, as in much of the previous work, in our method, distributed\nrepresentations of event realizations are computed based on distributed\nrepresentations of predicates and their arguments, and then these\nrepresentations are used to predict prototypical event orderings. The\nparameters of the compositional process for computing the event representations\nand the ranking component of the model are jointly estimated from texts. We\nshow that this approach results in a substantial boost in ordering performance\nwith respect to previous methods.\n", "versions": [{"version": "v1", "created": "Wed, 18 Dec 2013 16:13:08 GMT"}, {"version": "v2", "created": "Mon, 20 Jan 2014 10:42:35 GMT"}, {"version": "v3", "created": "Mon, 17 Feb 2014 17:08:34 GMT"}, {"version": "v4", "created": "Fri, 25 Apr 2014 13:31:53 GMT"}], "update_date": "2017-02-13", "authors_parsed": [["Modi", "Ashutosh", ""], ["Titov", "Ivan", ""]]}, {"id": "1312.5242", "submitter": "Alexey Dosovitskiy", "authors": "Alexey Dosovitskiy, Jost Tobias Springenberg and Thomas Brox", "title": "Unsupervised feature learning by augmenting single images", "comments": "ICLR 2014 workshop track submission (7 pages, 4 figures, 1 table)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When deep learning is applied to visual object recognition, data augmentation\nis often used to generate additional training data without extra labeling cost.\nIt helps to reduce overfitting and increase the performance of the algorithm.\nIn this paper we investigate if it is possible to use data augmentation as the\nmain component of an unsupervised feature learning architecture. To that end we\nsample a set of random image patches and declare each of them to be a separate\nsingle-image surrogate class. We then extend these trivial one-element classes\nby applying a variety of transformations to the initial 'seed' patches. Finally\nwe train a convolutional neural network to discriminate between these surrogate\nclasses. The feature representation learned by the network can then be used in\nvarious vision tasks. We find that this simple feature learning algorithm is\nsurprisingly successful, achieving competitive classification results on\nseveral popular vision datasets (STL-10, CIFAR-10, Caltech-101).\n", "versions": [{"version": "v1", "created": "Wed, 18 Dec 2013 17:44:17 GMT"}, {"version": "v2", "created": "Fri, 24 Jan 2014 18:02:09 GMT"}, {"version": "v3", "created": "Sun, 16 Feb 2014 13:07:23 GMT"}], "update_date": "2014-02-18", "authors_parsed": [["Dosovitskiy", "Alexey", ""], ["Springenberg", "Jost Tobias", ""], ["Brox", "Thomas", ""]]}, {"id": "1312.5258", "submitter": "Vincent Dumoulin", "authors": "Vincent Dumoulin, Ian J. Goodfellow, Aaron Courville, Yoshua Bengio", "title": "On the Challenges of Physical Implementations of RBMs", "comments": null, "journal-ref": "Proc. AAAI 2014, pp. 1199-1205", "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Restricted Boltzmann machines (RBMs) are powerful machine learning models,\nbut learning and some kinds of inference in the model require sampling-based\napproximations, which, in classical digital computers, are implemented using\nexpensive MCMC. Physical computation offers the opportunity to reduce the cost\nof sampling by building physical systems whose natural dynamics correspond to\ndrawing samples from the desired RBM distribution. Such a system avoids the\nburn-in and mixing cost of a Markov chain. However, hardware implementations of\nthis variety usually entail limitations such as low-precision and limited range\nof the parameters and restrictions on the size and topology of the RBM. We\nconduct software simulations to determine how harmful each of these\nrestrictions is. Our simulations are designed to reproduce aspects of the\nD-Wave quantum computer, but the issues we investigate arise in most forms of\nphysical computation.\n", "versions": [{"version": "v1", "created": "Wed, 18 Dec 2013 18:30:51 GMT"}, {"version": "v2", "created": "Fri, 24 Oct 2014 19:16:14 GMT"}], "update_date": "2014-10-27", "authors_parsed": [["Dumoulin", "Vincent", ""], ["Goodfellow", "Ian J.", ""], ["Courville", "Aaron", ""], ["Bengio", "Yoshua", ""]]}, {"id": "1312.5354", "submitter": "Yaqub Alwan", "authors": "Yaqub Alwan, Zoran Cvetkovic, Michael Curtis", "title": "Classification of Human Ventricular Arrhythmia in High Dimensional\n  Representation Spaces", "comments": "9 pages, 2 tables, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CE cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We studied classification of human ECGs labelled as normal sinus rhythm,\nventricular fibrillation and ventricular tachycardia by means of support vector\nmachines in different representation spaces, using different observation\nlengths. ECG waveform segments of duration 0.5-4 s, their Fourier magnitude\nspectra, and lower dimensional projections of Fourier magnitude spectra were\nused for classification. All considered representations were of much higher\ndimension than in published studies. Classification accuracy improved with\nsegment duration up to 2 s, with 4 s providing little improvement. We found\nthat it is possible to discriminate between ventricular tachycardia and\nventricular fibrillation by the present approach with much shorter runs of ECG\n(2 s, minimum 86% sensitivity per class) than previously imagined. Ensembles of\nclassifiers acting on 1 s segments taken over 5 s observation windows gave best\nresults, with sensitivities of detection for all classes exceeding 93%.\n", "versions": [{"version": "v1", "created": "Wed, 18 Dec 2013 22:08:07 GMT"}, {"version": "v2", "created": "Mon, 28 Jul 2014 09:44:01 GMT"}], "update_date": "2014-07-29", "authors_parsed": [["Alwan", "Yaqub", ""], ["Cvetkovic", "Zoran", ""], ["Curtis", "Michael", ""]]}, {"id": "1312.5394", "submitter": "Michael S. Gashler Ph.D.", "authors": "Michael S. Gashler, Michael R. Smith, Richard Morris, Tony Martinez", "title": "Missing Value Imputation With Unsupervised Backpropagation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many data mining and data analysis techniques operate on dense matrices or\ncomplete tables of data. Real-world data sets, however, often contain unknown\nvalues. Even many classification algorithms that are designed to operate with\nmissing values still exhibit deteriorated accuracy. One approach to handling\nmissing values is to fill in (impute) the missing values. In this paper, we\npresent a technique for unsupervised learning called Unsupervised\nBackpropagation (UBP), which trains a multi-layer perceptron to fit to the\nmanifold sampled by a set of observed point-vectors. We evaluate UBP with the\ntask of imputing missing values in datasets, and show that UBP is able to\npredict missing values with significantly lower sum-squared error than other\ncollaborative filtering and imputation techniques. We also demonstrate with 24\ndatasets and 9 supervised learning algorithms that classification accuracy is\nusually higher when randomly-withheld values are imputed using UBP, rather than\nwith other methods.\n", "versions": [{"version": "v1", "created": "Thu, 19 Dec 2013 02:38:40 GMT"}], "update_date": "2013-12-20", "authors_parsed": [["Gashler", "Michael S.", ""], ["Smith", "Michael R.", ""], ["Morris", "Richard", ""], ["Martinez", "Tony", ""]]}, {"id": "1312.5398", "submitter": "Michael Tetelman", "authors": "Michael Tetelman", "title": "Continuous Learning: Engineering Super Features With Feature Algebras", "comments": "Submitted to ICLR 2014", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we consider a problem of searching a space of predictive models\nfor a given training data set. We propose an iterative procedure for deriving a\nsequence of improving models and a corresponding sequence of sets of non-linear\nfeatures on the original input space. After a finite number of iterations N,\nthe non-linear features become 2^N -degree polynomials on the original space.\nWe show that in a limit of an infinite number of iterations derived non-linear\nfeatures must form an associative algebra: a product of two features is equal\nto a linear combination of features from the same feature space for any given\ninput point. Because each iteration consists of solving a series of convex\nproblems that contain all previous solutions, the likelihood of the models in\nthe sequence is increasing with each iteration while the dimension of the model\nparameter space is set to a limited controlled value.\n", "versions": [{"version": "v1", "created": "Thu, 19 Dec 2013 03:24:58 GMT"}, {"version": "v2", "created": "Mon, 17 Feb 2014 20:32:00 GMT"}], "update_date": "2014-02-18", "authors_parsed": [["Tetelman", "Michael", ""]]}, {"id": "1312.5412", "submitter": "Taichi Kiwaki Mr", "authors": "Taichi Kiwaki, Takaki Makino, Kazuyuki Aihara", "title": "Approximated Infomax Early Stopping: Revisiting Gaussian RBMs on Natural\n  Images", "comments": "9 pages with 1 page appendix", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We pursue an early stopping technique that helps Gaussian Restricted\nBoltzmann Machines (GRBMs) to gain good natural image representations in terms\nof overcompleteness and data fitting. GRBMs are widely considered as an\nunsuitable model for natural images because they gain non-overcomplete\nrepresentations which include uniform filters that do not represent useful\nimage features. We have recently found that GRBMs once gain and subsequently\nlose useful filters during their training, contrary to this common perspective.\nWe attribute this phenomenon to a tradeoff between overcompleteness of GRBM\nrepresentations and data fitting. To gain GRBM representations that are\novercomplete and fit data well, we propose a measure for GRBM representation\nquality, approximated mutual information, and an early stopping technique based\non this measure. The proposed method boosts performance of classifiers trained\non GRBM representations.\n", "versions": [{"version": "v1", "created": "Thu, 19 Dec 2013 05:37:50 GMT"}, {"version": "v2", "created": "Thu, 2 Jan 2014 16:45:15 GMT"}, {"version": "v3", "created": "Mon, 6 Jan 2014 20:07:09 GMT"}], "update_date": "2014-01-07", "authors_parsed": [["Kiwaki", "Taichi", ""], ["Makino", "Takaki", ""], ["Aihara", "Kazuyuki", ""]]}, {"id": "1312.5419", "submitter": "Jinseok Nam", "authors": "Jinseok Nam, Jungi Kim, Eneldo Loza Menc\\'ia, Iryna Gurevych, Johannes\n  F\\\"urnkranz", "title": "Large-scale Multi-label Text Classification - Revisiting Neural Networks", "comments": "16 pages, 4 figures, submitted to ECML 2014", "journal-ref": "Proceedings ECML/PKDD (2) 2014: 437-452", "doi": "10.1007/978-3-662-44851-9_28", "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural networks have recently been proposed for multi-label classification\nbecause they are able to capture and model label dependencies in the output\nlayer. In this work, we investigate limitations of BP-MLL, a neural network\n(NN) architecture that aims at minimizing pairwise ranking error. Instead, we\npropose to use a comparably simple NN approach with recently proposed learning\ntechniques for large-scale multi-label text classification tasks. In\nparticular, we show that BP-MLL's ranking loss minimization can be efficiently\nand effectively replaced with the commonly used cross entropy error function,\nand demonstrate that several advances in neural network training that have been\ndeveloped in the realm of deep learning can be effectively employed in this\nsetting. Our experimental results show that simple NN models equipped with\nadvanced techniques such as rectified linear units, dropout, and AdaGrad\nperform as well as or even outperform state-of-the-art approaches on six\nlarge-scale textual datasets with diverse characteristics.\n", "versions": [{"version": "v1", "created": "Thu, 19 Dec 2013 06:53:24 GMT"}, {"version": "v2", "created": "Mon, 30 Dec 2013 05:41:15 GMT"}, {"version": "v3", "created": "Thu, 15 May 2014 11:32:03 GMT"}], "update_date": "2020-12-09", "authors_parsed": [["Nam", "Jinseok", ""], ["Kim", "Jungi", ""], ["Menc\u00eda", "Eneldo Loza", ""], ["Gurevych", "Iryna", ""], ["F\u00fcrnkranz", "Johannes", ""]]}, {"id": "1312.5434", "submitter": "Xiaochuan Zhao", "authors": "Xiaochuan Zhao and Ali H. Sayed", "title": "Asynchronous Adaptation and Learning over Networks --- Part I: Modeling\n  and Stability Analysis", "comments": "40 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SY cs.IT cs.LG math.IT math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work and the supporting Parts II [2] and III [3], we provide a rather\ndetailed analysis of the stability and performance of asynchronous strategies\nfor solving distributed optimization and adaptation problems over networks. We\nexamine asynchronous networks that are subject to fairly general sources of\nuncertainties, such as changing topologies, random link failures, random data\narrival times, and agents turning on and off randomly. Under this model, agents\nin the network may stop updating their solutions or may stop sending or\nreceiving information in a random manner and without coordination with other\nagents. We establish in Part I conditions on the first and second-order moments\nof the relevant parameter distributions to ensure mean-square stable behavior.\nWe derive in Part II expressions that reveal how the various parameters of the\nasynchronous behavior influence network performance. We compare in Part III the\nperformance of asynchronous networks to the performance of both centralized\nsolutions and synchronous networks. One notable conclusion is that the\nmean-square-error performance of asynchronous networks shows a degradation only\nof the order of $O(\\nu)$, where $\\nu$ is a small step-size parameter, while the\nconvergence rate remains largely unaltered. The results provide a solid\njustification for the remarkable resilience of cooperative networks in the face\nof random failures at multiple levels: agents, links, data arrivals, and\ntopology.\n", "versions": [{"version": "v1", "created": "Thu, 19 Dec 2013 08:29:57 GMT"}, {"version": "v2", "created": "Sun, 27 Jul 2014 01:00:16 GMT"}, {"version": "v3", "created": "Tue, 16 Dec 2014 08:16:46 GMT"}], "update_date": "2014-12-17", "authors_parsed": [["Zhao", "Xiaochuan", ""], ["Sayed", "Ali H.", ""]]}, {"id": "1312.5438", "submitter": "Xiaochuan Zhao", "authors": "Xiaochuan Zhao and Ali H. Sayed", "title": "Asynchronous Adaptation and Learning over Networks - Part II:\n  Performance Analysis", "comments": "43 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SY cs.IT cs.LG math.IT math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In Part I \\cite{Zhao13TSPasync1}, we introduced a fairly general model for\nasynchronous events over adaptive networks including random topologies, random\nlink failures, random data arrival times, and agents turning on and off\nrandomly. We performed a stability analysis and established the notable fact\nthat the network is still able to converge in the mean-square-error sense to\nthe desired solution. Once stable behavior is guaranteed, it becomes important\nto evaluate how fast the iterates converge and how close they get to the\noptimal solution. This is a demanding task due to the various asynchronous\nevents and due to the fact that agents influence each other. In this Part II,\nwe carry out a detailed analysis of the mean-square-error performance of\nasynchronous strategies for solving distributed optimization and adaptation\nproblems over networks. We derive analytical expressions for the mean-square\nconvergence rate and the steady-state mean-square-deviation. The expressions\nreveal how the various parameters of the asynchronous behavior influence\nnetwork performance. In the process, we establish the interesting conclusion\nthat even under the influence of asynchronous events, all agents in the\nadaptive network can still reach an $O(\\nu^{1 + \\gamma_o'})$ near-agreement\nwith some $\\gamma_o' > 0$ while approaching the desired solution within\n$O(\\nu)$ accuracy, where $\\nu$ is proportional to the small step-size parameter\nfor adaptation.\n", "versions": [{"version": "v1", "created": "Thu, 19 Dec 2013 08:39:58 GMT"}, {"version": "v2", "created": "Sun, 27 Jul 2014 01:14:23 GMT"}, {"version": "v3", "created": "Tue, 16 Dec 2014 08:28:11 GMT"}], "update_date": "2014-12-17", "authors_parsed": [["Zhao", "Xiaochuan", ""], ["Sayed", "Ali H.", ""]]}, {"id": "1312.5439", "submitter": "Xiaochuan Zhao", "authors": "Xiaochuan Zhao and Ali H. Sayed", "title": "Asynchronous Adaptation and Learning over Networks - Part III:\n  Comparison Analysis", "comments": "39 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SY cs.IT cs.LG math.IT math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In Part II [3] we carried out a detailed mean-square-error analysis of the\nperformance of asynchronous adaptation and learning over networks under a\nfairly general model for asynchronous events including random topologies,\nrandom link failures, random data arrival times, and agents turning on and off\nrandomly. In this Part III, we compare the performance of synchronous and\nasynchronous networks. We also compare the performance of decentralized\nadaptation against centralized stochastic-gradient (batch) solutions. Two\ninteresting conclusions stand out. First, the results establish that the\nperformance of adaptive networks is largely immune to the effect of\nasynchronous events: the mean and mean-square convergence rates and the\nasymptotic bias values are not degraded relative to synchronous or centralized\nimplementations. Only the steady-state mean-square-deviation suffers a\ndegradation in the order of $\\nu$, which represents the small step-size\nparameters used for adaptation. Second, the results show that the adaptive\ndistributed network matches the performance of the centralized solution. These\nconclusions highlight another critical benefit of cooperation by networked\nagents: cooperation does not only enhance performance in comparison to\nstand-alone single-agent processing, but it also endows the network with\nremarkable resilience to various forms of random failure events and is able to\ndeliver performance that is as powerful as batch solutions.\n", "versions": [{"version": "v1", "created": "Thu, 19 Dec 2013 08:45:42 GMT"}, {"version": "v2", "created": "Sun, 27 Jul 2014 01:20:49 GMT"}, {"version": "v3", "created": "Tue, 16 Dec 2014 08:31:57 GMT"}], "update_date": "2014-12-17", "authors_parsed": [["Zhao", "Xiaochuan", ""], ["Sayed", "Ali H.", ""]]}, {"id": "1312.5457", "submitter": "Yonatan Vaizman", "authors": "Yonatan Vaizman, Brian McFee and Gert Lanckriet", "title": "Codebook based Audio Feature Representation for Music Information\n  Retrieval", "comments": "Journal paper. Submitted to IEEE transactions on Audio, Speech and\n  Language Processing. Submitted on Dec 18th, 2013", "journal-ref": null, "doi": "10.1109/TASLP.2014.2337842", "report-no": null, "categories": "cs.IR cs.LG cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Digital music has become prolific in the web in recent decades. Automated\nrecommendation systems are essential for users to discover music they love and\nfor artists to reach appropriate audience. When manual annotations and user\npreference data is lacking (e.g. for new artists) these systems must rely on\n\\emph{content based} methods. Besides powerful machine learning tools for\nclassification and retrieval, a key component for successful recommendation is\nthe \\emph{audio content representation}.\n  Good representations should capture informative musical patterns in the audio\nsignal of songs. These representations should be concise, to enable efficient\n(low storage, easy indexing, fast search) management of huge music\nrepositories, and should also be easy and fast to compute, to enable real-time\ninteraction with a user supplying new songs to the system.\n  Before designing new audio features, we explore the usage of traditional\nlocal features, while adding a stage of encoding with a pre-computed\n\\emph{codebook} and a stage of pooling to get compact vectorial\nrepresentations. We experiment with different encoding methods, namely\n\\emph{the LASSO}, \\emph{vector quantization (VQ)} and \\emph{cosine similarity\n(CS)}. We evaluate the representations' quality in two music information\nretrieval applications: query-by-tag and query-by-example. Our results show\nthat concise representations can be used for successful performance in both\napplications. We recommend using top-$\\tau$ VQ encoding, which consistently\nperforms well in both applications, and requires much less computation time\nthan the LASSO.\n", "versions": [{"version": "v1", "created": "Thu, 19 Dec 2013 09:40:03 GMT"}], "update_date": "2016-11-15", "authors_parsed": [["Vaizman", "Yonatan", ""], ["McFee", "Brian", ""], ["Lanckriet", "Gert", ""]]}, {"id": "1312.5465", "submitter": "Shaobo Lin", "authors": "Shaobo Lin, Jinshan Zeng, Jian Fang and Zongben Xu", "title": "Learning rates of $l^q$ coefficient regularization learning with\n  Gaussian kernel", "comments": "26 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Regularization is a well recognized powerful strategy to improve the\nperformance of a learning machine and $l^q$ regularization schemes with\n$0<q<\\infty$ are central in use. It is known that different $q$ leads to\ndifferent properties of the deduced estimators, say, $l^2$ regularization leads\nto smooth estimators while $l^1$ regularization leads to sparse estimators.\nThen, how does the generalization capabilities of $l^q$ regularization learning\nvary with $q$? In this paper, we study this problem in the framework of\nstatistical learning theory and show that implementing $l^q$ coefficient\nregularization schemes in the sample dependent hypothesis space associated with\nGaussian kernel can attain the same almost optimal learning rates for all\n$0<q<\\infty$. That is, the upper and lower bounds of learning rates for $l^q$\nregularization learning are asymptotically identical for all $0<q<\\infty$. Our\nfinding tentatively reveals that, in some modeling contexts, the choice of $q$\nmight not have a strong impact with respect to the generalization capability.\nFrom this perspective, $q$ can be arbitrarily specified, or specified merely by\nother no generalization criteria like smoothness, computational complexity,\nsparsity, etc..\n", "versions": [{"version": "v1", "created": "Thu, 19 Dec 2013 10:10:02 GMT"}, {"version": "v2", "created": "Thu, 18 Sep 2014 09:55:10 GMT"}, {"version": "v3", "created": "Thu, 25 Sep 2014 02:31:30 GMT"}], "update_date": "2014-09-26", "authors_parsed": [["Lin", "Shaobo", ""], ["Zeng", "Jinshan", ""], ["Fang", "Jian", ""], ["Xu", "Zongben", ""]]}, {"id": "1312.5542", "submitter": "R\\'emi Lebret", "authors": "R\\'emi Lebret and Ronan Collobert", "title": "Word Emdeddings through Hellinger PCA", "comments": "9 pages, 5 tables", "journal-ref": "Conference of the European Chapter of the Association for\n  Computational Linguistics (EACL), 2014", "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Word embeddings resulting from neural language models have been shown to be\nsuccessful for a large variety of NLP tasks. However, such architecture might\nbe difficult to train and time-consuming. Instead, we propose to drastically\nsimplify the word embeddings computation through a Hellinger PCA of the word\nco-occurence matrix. We compare those new word embeddings with some well-known\nembeddings on NER and movie review tasks and show that we can reach similar or\neven better performance. Although deep learning is not really necessary for\ngenerating good word embeddings, we show that it can provide an easy way to\nadapt embeddings to specific tasks.\n", "versions": [{"version": "v1", "created": "Thu, 19 Dec 2013 13:31:11 GMT"}, {"version": "v2", "created": "Tue, 18 Mar 2014 10:23:35 GMT"}, {"version": "v3", "created": "Wed, 4 Jan 2017 17:01:11 GMT"}], "update_date": "2017-01-05", "authors_parsed": [["Lebret", "R\u00e9mi", ""], ["Collobert", "Ronan", ""]]}, {"id": "1312.5578", "submitter": "Li Yao", "authors": "Sherjil Ozair, Li Yao and Yoshua Bengio", "title": "Multimodal Transitions for Generative Stochastic Networks", "comments": "7 figures, 9 pages, submitted to ICLR14", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generative Stochastic Networks (GSNs) have been recently introduced as an\nalternative to traditional probabilistic modeling: instead of parametrizing the\ndata distribution directly, one parametrizes a transition operator for a Markov\nchain whose stationary distribution is an estimator of the data generating\ndistribution. The result of training is therefore a machine that generates\nsamples through this Markov chain. However, the previously introduced GSN\nconsistency theorems suggest that in order to capture a wide class of\ndistributions, the transition operator in general should be multimodal,\nsomething that has not been done before this paper. We introduce for the first\ntime multimodal transition distributions for GSNs, in particular using models\nin the NADE family (Neural Autoregressive Density Estimator) as output\ndistributions of the transition operator. A NADE model is related to an RBM\n(and can thus model multimodal distributions) but its likelihood (and\nlikelihood gradient) can be computed easily. The parameters of the NADE are\nobtained as a learned function of the previous state of the learned Markov\nchain. Experiments clearly illustrate the advantage of such multimodal\ntransition distributions over unimodal GSNs.\n", "versions": [{"version": "v1", "created": "Thu, 19 Dec 2013 15:08:37 GMT"}, {"version": "v2", "created": "Mon, 30 Dec 2013 01:25:05 GMT"}, {"version": "v3", "created": "Mon, 6 Jan 2014 05:39:34 GMT"}, {"version": "v4", "created": "Fri, 24 Jan 2014 22:24:15 GMT"}], "update_date": "2014-09-04", "authors_parsed": [["Ozair", "Sherjil", ""], ["Yao", "Li", ""], ["Bengio", "Yoshua", ""]]}, {"id": "1312.5602", "submitter": "Volodymyr Mnih", "authors": "Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Alex Graves, Ioannis\n  Antonoglou, Daan Wierstra, Martin Riedmiller", "title": "Playing Atari with Deep Reinforcement Learning", "comments": "NIPS Deep Learning Workshop 2013", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present the first deep learning model to successfully learn control\npolicies directly from high-dimensional sensory input using reinforcement\nlearning. The model is a convolutional neural network, trained with a variant\nof Q-learning, whose input is raw pixels and whose output is a value function\nestimating future rewards. We apply our method to seven Atari 2600 games from\nthe Arcade Learning Environment, with no adjustment of the architecture or\nlearning algorithm. We find that it outperforms all previous approaches on six\nof the games and surpasses a human expert on three of them.\n", "versions": [{"version": "v1", "created": "Thu, 19 Dec 2013 16:00:08 GMT"}], "update_date": "2013-12-20", "authors_parsed": [["Mnih", "Volodymyr", ""], ["Kavukcuoglu", "Koray", ""], ["Silver", "David", ""], ["Graves", "Alex", ""], ["Antonoglou", "Ioannis", ""], ["Wierstra", "Daan", ""], ["Riedmiller", "Martin", ""]]}, {"id": "1312.5604", "submitter": "Qiang Qiu", "authors": "Qiang Qiu, Guillermo Sapiro", "title": "Learning Transformations for Classification Forests", "comments": "arXiv admin note: text overlap with arXiv:1309.2074", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work introduces a transformation-based learner model for classification\nforests. The weak learner at each split node plays a crucial role in a\nclassification tree. We propose to optimize the splitting objective by learning\na linear transformation on subspaces using nuclear norm as the optimization\ncriteria. The learned linear transformation restores a low-rank structure for\ndata from the same class, and, at the same time, maximizes the separation\nbetween different classes, thereby improving the performance of the split\nfunction. Theoretical and experimental results support the proposed framework.\n", "versions": [{"version": "v1", "created": "Thu, 19 Dec 2013 16:01:41 GMT"}, {"version": "v2", "created": "Thu, 6 Feb 2014 12:24:54 GMT"}], "update_date": "2014-02-07", "authors_parsed": [["Qiu", "Qiang", ""], ["Sapiro", "Guillermo", ""]]}, {"id": "1312.5650", "submitter": "Samy Bengio", "authors": "Mohammad Norouzi and Tomas Mikolov and Samy Bengio and Yoram Singer\n  and Jonathon Shlens and Andrea Frome and Greg S. Corrado and Jeffrey Dean", "title": "Zero-Shot Learning by Convex Combination of Semantic Embeddings", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Several recent publications have proposed methods for mapping images into\ncontinuous semantic embedding spaces. In some cases the embedding space is\ntrained jointly with the image transformation. In other cases the semantic\nembedding space is established by an independent natural language processing\ntask, and then the image transformation into that space is learned in a second\nstage. Proponents of these image embedding systems have stressed their\nadvantages over the traditional \\nway{} classification framing of image\nunderstanding, particularly in terms of the promise for zero-shot learning --\nthe ability to correctly annotate images of previously unseen object\ncategories. In this paper, we propose a simple method for constructing an image\nembedding system from any existing \\nway{} image classifier and a semantic word\nembedding model, which contains the $\\n$ class labels in its vocabulary. Our\nmethod maps images into the semantic embedding space via convex combination of\nthe class label embedding vectors, and requires no additional training. We show\nthat this simple and direct method confers many of the advantages associated\nwith more complex image embedding schemes, and indeed outperforms state of the\nart methods on the ImageNet zero-shot learning task.\n", "versions": [{"version": "v1", "created": "Thu, 19 Dec 2013 17:30:31 GMT"}, {"version": "v2", "created": "Fri, 20 Dec 2013 23:30:47 GMT"}, {"version": "v3", "created": "Fri, 21 Mar 2014 23:47:20 GMT"}], "update_date": "2017-02-28", "authors_parsed": [["Norouzi", "Mohammad", ""], ["Mikolov", "Tomas", ""], ["Bengio", "Samy", ""], ["Singer", "Yoram", ""], ["Shlens", "Jonathon", ""], ["Frome", "Andrea", ""], ["Corrado", "Greg S.", ""], ["Dean", "Jeffrey", ""]]}, {"id": "1312.5663", "submitter": "Alireza Makhzani", "authors": "Alireza Makhzani, Brendan Frey", "title": "k-Sparse Autoencoders", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, it has been observed that when representations are learnt in a way\nthat encourages sparsity, improved performance is obtained on classification\ntasks. These methods involve combinations of activation functions, sampling\nsteps and different kinds of penalties. To investigate the effectiveness of\nsparsity by itself, we propose the k-sparse autoencoder, which is an\nautoencoder with linear activation function, where in hidden layers only the k\nhighest activities are kept. When applied to the MNIST and NORB datasets, we\nfind that this method achieves better classification results than denoising\nautoencoders, networks trained with dropout, and RBMs. k-sparse autoencoders\nare simple to train and the encoding stage is very fast, making them\nwell-suited to large problem sizes, where conventional sparse coding algorithms\ncannot be applied.\n", "versions": [{"version": "v1", "created": "Thu, 19 Dec 2013 17:46:46 GMT"}, {"version": "v2", "created": "Sat, 22 Mar 2014 17:12:07 GMT"}], "update_date": "2014-03-25", "authors_parsed": [["Makhzani", "Alireza", ""], ["Frey", "Brendan", ""]]}, {"id": "1312.5697", "submitter": "Andrew Rabinovich", "authors": "Samy Bengio, Jeff Dean, Dumitru Erhan, Eugene Ie, Quoc Le, Andrew\n  Rabinovich, Jonathon Shlens, Yoram Singer", "title": "Using Web Co-occurrence Statistics for Improving Image Categorization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Object recognition and localization are important tasks in computer vision.\nThe focus of this work is the incorporation of contextual information in order\nto improve object recognition and localization. For instance, it is natural to\nexpect not to see an elephant to appear in the middle of an ocean. We consider\na simple approach to encapsulate such common sense knowledge using\nco-occurrence statistics from web documents. By merely counting the number of\ntimes nouns (such as elephants, sharks, oceans, etc.) co-occur in web\ndocuments, we obtain a good estimate of expected co-occurrences in visual data.\nWe then cast the problem of combining textual co-occurrence statistics with the\npredictions of image-based classifiers as an optimization problem. The\nresulting optimization problem serves as a surrogate for our inference\nprocedure. Albeit the simplicity of the resulting optimization problem, it is\neffective in improving both recognition and localization accuracy. Concretely,\nwe observe significant improvements in recognition and localization rates for\nboth ImageNet Detection 2012 and Sun 2012 datasets.\n", "versions": [{"version": "v1", "created": "Thu, 19 Dec 2013 18:53:47 GMT"}, {"version": "v2", "created": "Fri, 20 Dec 2013 18:12:16 GMT"}], "update_date": "2013-12-23", "authors_parsed": [["Bengio", "Samy", ""], ["Dean", "Jeff", ""], ["Erhan", "Dumitru", ""], ["Ie", "Eugene", ""], ["Le", "Quoc", ""], ["Rabinovich", "Andrew", ""], ["Shlens", "Jonathon", ""], ["Singer", "Yoram", ""]]}, {"id": "1312.5734", "submitter": "Andrew Lan", "authors": "Andrew S. Lan, Christoph Studer and Richard G. Baraniuk", "title": "Time-varying Learning and Content Analytics via Sparse Factor Analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.OC stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose SPARFA-Trace, a new machine learning-based framework for\ntime-varying learning and content analytics for education applications. We\ndevelop a novel message passing-based, blind, approximate Kalman filter for\nsparse factor analysis (SPARFA), that jointly (i) traces learner concept\nknowledge over time, (ii) analyzes learner concept knowledge state transitions\n(induced by interacting with learning resources, such as textbook sections,\nlecture videos, etc, or the forgetting effect), and (iii) estimates the content\norganization and intrinsic difficulty of the assessment questions. These\nquantities are estimated solely from binary-valued (correct/incorrect) graded\nlearner response data and a summary of the specific actions each learner\nperforms (e.g., answering a question or studying a learning resource) at each\ntime instance. Experimental results on two online course datasets demonstrate\nthat SPARFA-Trace is capable of tracing each learner's concept knowledge\nevolution over time, as well as analyzing the quality and content organization\nof learning resources, the question-concept associations, and the question\nintrinsic difficulties. Moreover, we show that SPARFA-Trace achieves comparable\nor better performance in predicting unobserved learner responses than existing\ncollaborative filtering and knowledge tracing approaches for personalized\neducation.\n", "versions": [{"version": "v1", "created": "Thu, 19 Dec 2013 20:44:44 GMT"}], "update_date": "2013-12-20", "authors_parsed": [["Lan", "Andrew S.", ""], ["Studer", "Christoph", ""], ["Baraniuk", "Richard G.", ""]]}, {"id": "1312.5753", "submitter": "Matias Carrasco Kind", "authors": "M. Carrasco Kind and R. J. Brunner (Department of Astronomy,\n  University of Illinois at Urbana-Champaign)", "title": "SOMz: photometric redshift PDFs with self organizing maps and random\n  atlas", "comments": "14 pages, 8 figures. Accepted for publication in MNRAS. The code can\n  be found at http://lcdm.astro.illinois.edu/research/SOMZ.html", "journal-ref": null, "doi": "10.1093/mnras/stt2456", "report-no": null, "categories": "astro-ph.IM astro-ph.CO cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we explore the applicability of the unsupervised machine\nlearning technique of Self Organizing Maps (SOM) to estimate galaxy photometric\nredshift probability density functions (PDFs). This technique takes a\nspectroscopic training set, and maps the photometric attributes, but not the\nredshifts, to a two dimensional surface by using a process of competitive\nlearning where neurons compete to more closely resemble the training data\nmultidimensional space. The key feature of a SOM is that it retains the\ntopology of the input set, revealing correlations between the attributes that\nare not easily identified. We test three different 2D topological mapping:\nrectangular, hexagonal, and spherical, by using data from the DEEP2 survey. We\nalso explore different implementations and boundary conditions on the map and\nalso introduce the idea of a random atlas where a large number of different\nmaps are created and their individual predictions are aggregated to produce a\nmore robust photometric redshift PDF. We also introduced a new metric, the\n$I$-score, which efficiently incorporates different metrics, making it easier\nto compare different results (from different parameters or different\nphotometric redshift codes). We find that by using a spherical topology mapping\nwe obtain a better representation of the underlying multidimensional topology,\nwhich provides more accurate results that are comparable to other,\nstate-of-the-art machine learning algorithms. Our results illustrate that\nunsupervised approaches have great potential for many astronomical problems,\nand in particular for the computation of photometric redshifts.\n", "versions": [{"version": "v1", "created": "Wed, 18 Dec 2013 20:18:33 GMT"}], "update_date": "2015-06-18", "authors_parsed": [["Kind", "M. Carrasco", "", "Department of Astronomy,\n  University of Illinois at Urbana-Champaign"], ["Brunner", "R. J.", "", "Department of Astronomy,\n  University of Illinois at Urbana-Champaign"]]}, {"id": "1312.5766", "submitter": "Seunghak Lee", "authors": "Seunghak Lee, Jin Kyu Kim, Qirong Ho, Garth A. Gibson, Eric P. Xing", "title": "Structure-Aware Dynamic Scheduler for Parallel Machine Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Training large machine learning (ML) models with many variables or parameters\ncan take a long time if one employs sequential procedures even with stochastic\nupdates. A natural solution is to turn to distributed computing on a cluster;\nhowever, naive, unstructured parallelization of ML algorithms does not usually\nlead to a proportional speedup and can even result in divergence, because\ndependencies between model elements can attenuate the computational gains from\nparallelization and compromise correctness of inference. Recent efforts toward\nthis issue have benefited from exploiting the static, a priori block structures\nresiding in ML algorithms. In this paper, we take this path further by\nexploring the dynamic block structures and workloads therein present during ML\nprogram execution, which offers new opportunities for improving convergence,\ncorrectness, and load balancing in distributed ML. We propose and showcase a\ngeneral-purpose scheduler, STRADS, for coordinating distributed updates in ML\nalgorithms, which harnesses the aforementioned opportunities in a systematic\nway. We provide theoretical guarantees for our scheduler, and demonstrate its\nefficacy versus static block structures on Lasso and Matrix Factorization.\n", "versions": [{"version": "v1", "created": "Thu, 19 Dec 2013 22:05:11 GMT"}, {"version": "v2", "created": "Mon, 30 Dec 2013 06:20:05 GMT"}], "update_date": "2013-12-31", "authors_parsed": [["Lee", "Seunghak", ""], ["Kim", "Jin Kyu", ""], ["Ho", "Qirong", ""], ["Gibson", "Garth A.", ""], ["Xing", "Eric P.", ""]]}, {"id": "1312.5770", "submitter": "Samory Kpotufe", "authors": "Samory Kpotufe, Eleni Sgouritsa, Dominik Janzing, and Bernhard\n  Sch\\\"olkopf", "title": "Consistency of Causal Inference under the Additive Noise Model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We analyze a family of methods for statistical causal inference from sample\nunder the so-called Additive Noise Model. While most work on the subject has\nconcentrated on establishing the soundness of the Additive Noise Model, the\nstatistical consistency of the resulting inference methods has received little\nattention. We derive general conditions under which the given family of\ninference methods consistently infers the causal direction in a nonparametric\nsetting.\n", "versions": [{"version": "v1", "created": "Thu, 19 Dec 2013 22:15:40 GMT"}, {"version": "v2", "created": "Mon, 23 Dec 2013 16:59:04 GMT"}, {"version": "v3", "created": "Wed, 5 Feb 2014 03:37:30 GMT"}], "update_date": "2014-02-06", "authors_parsed": [["Kpotufe", "Samory", ""], ["Sgouritsa", "Eleni", ""], ["Janzing", "Dominik", ""], ["Sch\u00f6lkopf", "Bernhard", ""]]}, {"id": "1312.5783", "submitter": "Yunlong He `", "authors": "Yunlong He, Koray Kavukcuoglu, Yun Wang, Arthur Szlam, Yanjun Qi", "title": "Unsupervised Feature Learning by Deep Sparse Coding", "comments": "9 pages, submitted to ICLR", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a new unsupervised feature learning framework,\nnamely Deep Sparse Coding (DeepSC), that extends sparse coding to a multi-layer\narchitecture for visual object recognition tasks. The main innovation of the\nframework is that it connects the sparse-encoders from different layers by a\nsparse-to-dense module. The sparse-to-dense module is a composition of a local\nspatial pooling step and a low-dimensional embedding process, which takes\nadvantage of the spatial smoothness information in the image. As a result, the\nnew method is able to learn several levels of sparse representation of the\nimage which capture features at a variety of abstraction levels and\nsimultaneously preserve the spatial smoothness between the neighboring image\npatches. Combining the feature representations from multiple layers, DeepSC\nachieves the state-of-the-art performance on multiple object recognition tasks.\n", "versions": [{"version": "v1", "created": "Fri, 20 Dec 2013 00:21:36 GMT"}], "update_date": "2013-12-23", "authors_parsed": [["He", "Yunlong", ""], ["Kavukcuoglu", "Koray", ""], ["Wang", "Yun", ""], ["Szlam", "Arthur", ""], ["Qi", "Yanjun", ""]]}, {"id": "1312.5813", "submitter": "Jun Li", "authors": "Jun Li, Wei Luo, Jian Yang, Xiaotong Yuan", "title": "Unsupervised Pretraining Encourages Moderate-Sparseness", "comments": "6 pages, 2 figures, (to appear) ICML-Workshop on Unsupervised\n  Learning from Bioacoustic Big Data (uLearnBio) 2014", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  It is well known that direct training of deep neural networks will generally\nlead to poor results. A major progress in recent years is the invention of\nvarious pretraining methods to initialize network parameters and it was shown\nthat such methods lead to good prediction performance. However, the reason for\nthe success of pretraining has not been fully understood, although it was\nargued that regularization and better optimization play certain roles. This\npaper provides another explanation for the effectiveness of pretraining, where\nwe show pretraining leads to a sparseness of hidden unit activation in the\nresulting neural networks. The main reason is that the pretraining models can\nbe interpreted as an adaptive sparse coding. Compared to deep neural network\nwith sigmoid function, our experimental results on MNIST and Birdsong further\nsupport this sparseness observation.\n", "versions": [{"version": "v1", "created": "Fri, 20 Dec 2013 05:22:20 GMT"}, {"version": "v2", "created": "Mon, 9 Jun 2014 08:39:37 GMT"}], "update_date": "2014-06-10", "authors_parsed": [["Li", "Jun", ""], ["Luo", "Wei", ""], ["Yang", "Jian", ""], ["Yuan", "Xiaotong", ""]]}, {"id": "1312.5845", "submitter": "Takashi Shinozaki", "authors": "Takashi Shinozaki and Yasushi Naruse", "title": "Competitive Learning with Feedforward Supervisory Signal for Pre-trained\n  Multilayered Networks", "comments": "This paper has been withdrawn by the author since the review process\n  for the conference to which it was applied ended", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel learning method for multilayered neural networks which\nuses feedforward supervisory signal and associates classification of a new\ninput with that of pre-trained input. The proposed method effectively uses rich\ninput information in the earlier layer for robust leaning and revising internal\nrepresentation in a multilayer neural network.\n", "versions": [{"version": "v1", "created": "Fri, 20 Dec 2013 08:24:48 GMT"}, {"version": "v2", "created": "Tue, 24 Dec 2013 08:59:56 GMT"}, {"version": "v3", "created": "Mon, 3 Feb 2014 11:09:07 GMT"}, {"version": "v4", "created": "Mon, 17 Feb 2014 19:11:14 GMT"}, {"version": "v5", "created": "Fri, 21 Feb 2014 02:11:46 GMT"}, {"version": "v6", "created": "Fri, 9 May 2014 00:50:33 GMT"}, {"version": "v7", "created": "Mon, 16 Feb 2015 09:37:18 GMT"}], "update_date": "2015-02-17", "authors_parsed": [["Shinozaki", "Takashi", ""], ["Naruse", "Yasushi", ""]]}, {"id": "1312.5847", "submitter": "Sergey Plis", "authors": "Sergey M. Plis and Devon R. Hjelm and Ruslan Salakhutdinov and Vince\n  D. Calhoun", "title": "Deep learning for neuroimaging: a validation study", "comments": "ICLR 2014 revisions", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning methods have recently made notable advances in the tasks of\nclassification and representation learning. These tasks are important for brain\nimaging and neuroscience discovery, making the methods attractive for porting\nto a neuroimager's toolbox. Success of these methods is, in part, explained by\nthe flexibility of deep learning models. However, this flexibility makes the\nprocess of porting to new areas a difficult parameter optimization problem. In\nthis work we demonstrate our results (and feasible parameter ranges) in\napplication of deep learning methods to structural and functional brain imaging\ndata. We also describe a novel constraint-based approach to visualizing high\ndimensional data. We use it to analyze the effect of parameter choices on data\ntransformations. Our results show that deep learning methods are able to learn\nphysiologically important representations and detect latent relations in\nneuroimaging data.\n", "versions": [{"version": "v1", "created": "Fri, 20 Dec 2013 08:30:55 GMT"}, {"version": "v2", "created": "Wed, 8 Jan 2014 04:22:55 GMT"}, {"version": "v3", "created": "Wed, 19 Feb 2014 16:00:08 GMT"}], "update_date": "2014-02-20", "authors_parsed": [["Plis", "Sergey M.", ""], ["Hjelm", "Devon R.", ""], ["Salakhutdinov", "Ruslan", ""], ["Calhoun", "Vince D.", ""]]}, {"id": "1312.5851", "submitter": "Mikael Henaff", "authors": "Michael Mathieu, Mikael Henaff, Yann LeCun", "title": "Fast Training of Convolutional Networks through FFTs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolutional networks are one of the most widely employed architectures in\ncomputer vision and machine learning. In order to leverage their ability to\nlearn complex functions, large amounts of data are required for training.\nTraining a large convolutional network to produce state-of-the-art results can\ntake weeks, even when using modern GPUs. Producing labels using a trained\nnetwork can also be costly when dealing with web-scale datasets. In this work,\nwe present a simple algorithm which accelerates training and inference by a\nsignificant factor, and can yield improvements of over an order of magnitude\ncompared to existing state-of-the-art implementations. This is done by\ncomputing convolutions as pointwise products in the Fourier domain while\nreusing the same transformed feature map many times. The algorithm is\nimplemented on a GPU architecture and addresses a number of related challenges.\n", "versions": [{"version": "v1", "created": "Fri, 20 Dec 2013 08:42:21 GMT"}, {"version": "v2", "created": "Wed, 22 Jan 2014 00:28:06 GMT"}, {"version": "v3", "created": "Tue, 28 Jan 2014 01:33:21 GMT"}, {"version": "v4", "created": "Tue, 18 Feb 2014 03:20:51 GMT"}, {"version": "v5", "created": "Thu, 6 Mar 2014 23:27:18 GMT"}], "update_date": "2015-06-09", "authors_parsed": [["Mathieu", "Michael", ""], ["Henaff", "Mikael", ""], ["LeCun", "Yann", ""]]}, {"id": "1312.5853", "submitter": "Marc'Aurelio Ranzato", "authors": "Omry Yadan, Keith Adams, Yaniv Taigman, Marc'Aurelio Ranzato", "title": "Multi-GPU Training of ConvNets", "comments": "Machine Learning, Deep Learning, Convolutional Networks, Computer\n  Vision, GPU, CUDA", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work we evaluate different approaches to parallelize computation of\nconvolutional neural networks across several GPUs.\n", "versions": [{"version": "v1", "created": "Fri, 20 Dec 2013 08:45:07 GMT"}, {"version": "v2", "created": "Mon, 23 Dec 2013 15:50:49 GMT"}, {"version": "v3", "created": "Sat, 28 Dec 2013 08:31:12 GMT"}, {"version": "v4", "created": "Tue, 18 Feb 2014 21:35:13 GMT"}], "update_date": "2014-02-20", "authors_parsed": [["Yadan", "Omry", ""], ["Adams", "Keith", ""], ["Taigman", "Yaniv", ""], ["Ranzato", "Marc'Aurelio", ""]]}, {"id": "1312.5857", "submitter": "Dawen Liang", "authors": "Dawen Liang, Matthew D. Hoffman, Gautham J. Mysore", "title": "A Generative Product-of-Filters Model of Audio", "comments": "ICLR 2014 conference-track submission. Added link to the source code", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose the product-of-filters (PoF) model, a generative model that\ndecomposes audio spectra as sparse linear combinations of \"filters\" in the\nlog-spectral domain. PoF makes similar assumptions to those used in the classic\nhomomorphic filtering approach to signal processing, but replaces hand-designed\ndecompositions built of basic signal processing operations with a learned\ndecomposition based on statistical inference. This paper formulates the PoF\nmodel and derives a mean-field method for posterior inference and a variational\nEM algorithm to estimate the model's free parameters. We demonstrate PoF's\npotential for audio processing on a bandwidth expansion task, and show that PoF\ncan serve as an effective unsupervised feature extractor for a speaker\nidentification task.\n", "versions": [{"version": "v1", "created": "Fri, 20 Dec 2013 08:59:36 GMT"}, {"version": "v2", "created": "Mon, 23 Dec 2013 15:22:16 GMT"}, {"version": "v3", "created": "Mon, 17 Feb 2014 06:10:11 GMT"}, {"version": "v4", "created": "Tue, 18 Feb 2014 16:55:01 GMT"}, {"version": "v5", "created": "Tue, 25 Nov 2014 22:26:12 GMT"}], "update_date": "2014-11-27", "authors_parsed": [["Liang", "Dawen", ""], ["Hoffman", "Matthew D.", ""], ["Mysore", "Gautham J.", ""]]}, {"id": "1312.5869", "submitter": "Dimitrios Athanasakis Mr", "authors": "Dimitrios Athanasakis, John Shawe-Taylor, Delmiro Fernandez-Reyes", "title": "Principled Non-Linear Feature Selection", "comments": "arXiv admin note: substantial text overlap with arXiv:1311.5636", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent non-linear feature selection approaches employing greedy optimisation\nof Centred Kernel Target Alignment(KTA) exhibit strong results in terms of\ngeneralisation accuracy and sparsity. However, they are computationally\nprohibitive for large datasets. We propose randSel, a randomised feature\nselection algorithm, with attractive scaling properties. Our theoretical\nanalysis of randSel provides strong probabilistic guarantees for correct\nidentification of relevant features. RandSel's characteristics make it an ideal\ncandidate for identifying informative learned representations. We've conducted\nexperimentation to establish the performance of this approach, and present\nencouraging results, including a 3rd position result in the recent ICML black\nbox learning challenge as well as competitive results for signal peptide\nprediction, an important problem in bioinformatics.\n", "versions": [{"version": "v1", "created": "Fri, 20 Dec 2013 10:16:13 GMT"}, {"version": "v2", "created": "Tue, 18 Feb 2014 17:25:43 GMT"}], "update_date": "2014-02-19", "authors_parsed": [["Athanasakis", "Dimitrios", ""], ["Shawe-Taylor", "John", ""], ["Fernandez-Reyes", "Delmiro", ""]]}, {"id": "1312.5921", "submitter": "Arto Klami", "authors": "Arto Klami, Guillaume Bouchard and Abhishek Tripathi", "title": "Group-sparse Embeddings in Collective Matrix Factorization", "comments": "9+2 pages, submitted for International Conference on Learning\n  Representations 2014. This version fixes minor typographic mistakes, has one\n  new paragraph on computational efficiency, and describes the algorithm in\n  more detail in the Supplementary material", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  CMF is a technique for simultaneously learning low-rank representations based\non a collection of matrices with shared entities. A typical example is the\njoint modeling of user-item, item-property, and user-feature matrices in a\nrecommender system. The key idea in CMF is that the embeddings are shared\nacross the matrices, which enables transferring information between them. The\nexisting solutions, however, break down when the individual matrices have\nlow-rank structure not shared with others. In this work we present a novel CMF\nsolution that allows each of the matrices to have a separate low-rank structure\nthat is independent of the other matrices, as well as structures that are\nshared only by a subset of them. We compare MAP and variational Bayesian\nsolutions based on alternating optimization algorithms and show that the model\nautomatically infers the nature of each factor using group-wise sparsity. Our\napproach supports in a principled way continuous, binary and count observations\nand is efficient for sparse matrices involving missing data. We illustrate the\nsolution on a number of examples, focusing in particular on an interesting\nuse-case of augmented multi-view learning.\n", "versions": [{"version": "v1", "created": "Fri, 20 Dec 2013 12:42:15 GMT"}, {"version": "v2", "created": "Tue, 18 Feb 2014 09:44:23 GMT"}], "update_date": "2014-11-19", "authors_parsed": [["Klami", "Arto", ""], ["Bouchard", "Guillaume", ""], ["Tripathi", "Abhishek", ""]]}, {"id": "1312.5946", "submitter": "Kathrin Bujna", "authors": "Johannes Bl\\\"omer and Kathrin Bujna", "title": "Adaptive Seeding for Gaussian Mixture Models", "comments": "This is a preprint of a paper that has been accepted for publication\n  in the Proceedings of the 20th Pacific Asia Conference on Knowledge Discovery\n  and Data Mining (PAKDD) 2016. The final publication is available at\n  link.springer.com (http://link.springer.com/chapter/10.1007/978-3-319-31750-2\n  24)", "journal-ref": null, "doi": "10.1007/978-3-319-31750-2_24", "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present new initialization methods for the expectation-maximization\nalgorithm for multivariate Gaussian mixture models. Our methods are adaptions\nof the well-known $K$-means++ initialization and the Gonzalez algorithm.\nThereby we aim to close the gap between simple random, e.g. uniform, and\ncomplex methods, that crucially depend on the right choice of hyperparameters.\nOur extensive experiments indicate the usefulness of our methods compared to\ncommon techniques and methods, which e.g. apply the original $K$-means++ and\nGonzalez directly, with respect to artificial as well as real-world data sets.\n", "versions": [{"version": "v1", "created": "Fri, 20 Dec 2013 14:08:48 GMT"}, {"version": "v2", "created": "Mon, 1 Aug 2016 08:33:13 GMT"}, {"version": "v3", "created": "Tue, 30 May 2017 07:44:37 GMT"}], "update_date": "2017-05-31", "authors_parsed": [["Bl\u00f6mer", "Johannes", ""], ["Bujna", "Kathrin", ""]]}, {"id": "1312.5985", "submitter": "Tamara Polajnar", "authors": "Tamara Polajnar and Luana Fagarasan and Stephen Clark", "title": "Learning Type-Driven Tensor-Based Meaning Representations", "comments": "Submitted as part of the open review process for ICLR'14. The paper\n  contains 10 pages, 3 figures, 4 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper investigates the learning of 3rd-order tensors representing the\nsemantics of transitive verbs. The meaning representations are part of a\ntype-driven tensor-based semantic framework, from the newly emerging field of\ncompositional distributional semantics. Standard techniques from the neural\nnetworks literature are used to learn the tensors, which are tested on a\nselectional preference-style task with a simple 2-dimensional sentence space.\nPromising results are obtained against a competitive corpus-based baseline. We\nargue that extending this work beyond transitive verbs, and to\nhigher-dimensional sentence spaces, is an interesting and challenging problem\nfor the machine learning community to consider.\n", "versions": [{"version": "v1", "created": "Fri, 20 Dec 2013 15:21:15 GMT"}, {"version": "v2", "created": "Tue, 18 Feb 2014 15:27:24 GMT"}], "update_date": "2014-02-19", "authors_parsed": [["Polajnar", "Tamara", ""], ["Fagarasan", "Luana", ""], ["Clark", "Stephen", ""]]}, {"id": "1312.6002", "submitter": "Mathias Berglund", "authors": "Mathias Berglund, Tapani Raiko", "title": "Stochastic Gradient Estimate Variance in Contrastive Divergence and\n  Persistent Contrastive Divergence", "comments": "ICLR2014 Workshop Track submission. Rephrased parts of text. Results\n  unchanged", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Contrastive Divergence (CD) and Persistent Contrastive Divergence (PCD) are\npopular methods for training the weights of Restricted Boltzmann Machines.\nHowever, both methods use an approximate method for sampling from the model\ndistribution. As a side effect, these approximations yield significantly\ndifferent biases and variances for stochastic gradient estimates of individual\ndata points. It is well known that CD yields a biased gradient estimate. In\nthis paper we however show empirically that CD has a lower stochastic gradient\nestimate variance than exact sampling, while the mean of subsequent PCD\nestimates has a higher variance than exact sampling. The results give one\nexplanation to the finding that CD can be used with smaller minibatches or\nhigher learning rates than PCD.\n", "versions": [{"version": "v1", "created": "Fri, 20 Dec 2013 16:13:54 GMT"}, {"version": "v2", "created": "Tue, 7 Jan 2014 11:27:22 GMT"}, {"version": "v3", "created": "Fri, 14 Feb 2014 09:47:11 GMT"}], "update_date": "2014-02-17", "authors_parsed": [["Berglund", "Mathias", ""], ["Raiko", "Tapani", ""]]}, {"id": "1312.6026", "submitter": "KyungHyun Cho", "authors": "Razvan Pascanu, Caglar Gulcehre, Kyunghyun Cho, Yoshua Bengio", "title": "How to Construct Deep Recurrent Neural Networks", "comments": "Accepted at ICLR 2014 (Conference Track). 10-page text + 3-page\n  references", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we explore different ways to extend a recurrent neural network\n(RNN) to a \\textit{deep} RNN. We start by arguing that the concept of depth in\nan RNN is not as clear as it is in feedforward neural networks. By carefully\nanalyzing and understanding the architecture of an RNN, however, we find three\npoints of an RNN which may be made deeper; (1) input-to-hidden function, (2)\nhidden-to-hidden transition and (3) hidden-to-output function. Based on this\nobservation, we propose two novel architectures of a deep RNN which are\northogonal to an earlier attempt of stacking multiple recurrent layers to build\na deep RNN (Schmidhuber, 1992; El Hihi and Bengio, 1996). We provide an\nalternative interpretation of these deep RNNs using a novel framework based on\nneural operators. The proposed deep RNNs are empirically evaluated on the tasks\nof polyphonic music prediction and language modeling. The experimental result\nsupports our claim that the proposed deep RNNs benefit from the depth and\noutperform the conventional, shallow RNNs.\n", "versions": [{"version": "v1", "created": "Fri, 20 Dec 2013 16:39:39 GMT"}, {"version": "v2", "created": "Sun, 5 Jan 2014 18:39:22 GMT"}, {"version": "v3", "created": "Wed, 29 Jan 2014 22:56:47 GMT"}, {"version": "v4", "created": "Fri, 14 Feb 2014 18:15:46 GMT"}, {"version": "v5", "created": "Thu, 24 Apr 2014 15:17:07 GMT"}], "update_date": "2014-04-25", "authors_parsed": [["Pascanu", "Razvan", ""], ["Gulcehre", "Caglar", ""], ["Cho", "Kyunghyun", ""], ["Bengio", "Yoshua", ""]]}, {"id": "1312.6042", "submitter": "Gabriella Contardo", "authors": "Gabriella Contardo and Ludovic Denoyer and Thierry Artieres and\n  Patrick Gallinari", "title": "Learning States Representations in POMDP", "comments": "4 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose to deal with sequential processes where only partial observations\nare available by learning a latent representation space on which policies may\nbe accurately learned.\n", "versions": [{"version": "v1", "created": "Fri, 20 Dec 2013 17:03:50 GMT"}, {"version": "v2", "created": "Sat, 8 Feb 2014 10:10:53 GMT"}, {"version": "v3", "created": "Tue, 11 Mar 2014 14:12:36 GMT"}, {"version": "v4", "created": "Tue, 17 Jun 2014 10:24:51 GMT"}], "update_date": "2014-06-18", "authors_parsed": [["Contardo", "Gabriella", ""], ["Denoyer", "Ludovic", ""], ["Artieres", "Thierry", ""], ["Gallinari", "Patrick", ""]]}, {"id": "1312.6055", "submitter": "Tom Schaul", "authors": "Tom Schaul, Ioannis Antonoglou, David Silver", "title": "Unit Tests for Stochastic Optimization", "comments": "Final submission to ICLR 2014 (revised according to reviews,\n  additional results added)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Optimization by stochastic gradient descent is an important component of many\nlarge-scale machine learning algorithms. A wide variety of such optimization\nalgorithms have been devised; however, it is unclear whether these algorithms\nare robust and widely applicable across many different optimization landscapes.\nIn this paper we develop a collection of unit tests for stochastic\noptimization. Each unit test rapidly evaluates an optimization algorithm on a\nsmall-scale, isolated, and well-understood difficulty, rather than in\nreal-world scenarios where many such issues are entangled. Passing these unit\ntests is not sufficient, but absolutely necessary for any algorithms with\nclaims to generality or robustness. We give initial quantitative and\nqualitative results on numerous established algorithms. The testing framework\nis open-source, extensible, and easy to apply to new algorithms.\n", "versions": [{"version": "v1", "created": "Fri, 20 Dec 2013 17:44:06 GMT"}, {"version": "v2", "created": "Tue, 7 Jan 2014 20:43:40 GMT"}, {"version": "v3", "created": "Tue, 25 Feb 2014 18:16:54 GMT"}], "update_date": "2014-02-26", "authors_parsed": [["Schaul", "Tom", ""], ["Antonoglou", "Ioannis", ""], ["Silver", "David", ""]]}, {"id": "1312.6062", "submitter": "Jordi Delgado", "authors": "David Buchaca, Enrique Romero, Ferran Mazzanti, Jordi Delgado", "title": "Stopping Criteria in Contrastive Divergence: Alternatives to the\n  Reconstruction Error", "comments": "7 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Restricted Boltzmann Machines (RBMs) are general unsupervised learning\ndevices to ascertain generative models of data distributions. RBMs are often\ntrained using the Contrastive Divergence learning algorithm (CD), an\napproximation to the gradient of the data log-likelihood. A simple\nreconstruction error is often used to decide whether the approximation provided\nby the CD algorithm is good enough, though several authors (Schulz et al.,\n2010; Fischer & Igel, 2010) have raised doubts concerning the feasibility of\nthis procedure. However, not many alternatives to the reconstruction error have\nbeen used in the literature. In this manuscript we investigate simple\nalternatives to the reconstruction error in order to detect as soon as possible\nthe decrease in the log-likelihood during learning.\n", "versions": [{"version": "v1", "created": "Fri, 20 Dec 2013 18:14:44 GMT"}, {"version": "v2", "created": "Wed, 9 Apr 2014 07:42:24 GMT"}], "update_date": "2014-04-10", "authors_parsed": [["Buchaca", "David", ""], ["Romero", "Enrique", ""], ["Mazzanti", "Ferran", ""], ["Delgado", "Jordi", ""]]}, {"id": "1312.6086", "submitter": "Balazs Kegl", "authors": "Bal\\'azs K\\'egl", "title": "The return of AdaBoost.MH: multi-class Hamming trees", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Within the framework of AdaBoost.MH, we propose to train vector-valued\ndecision trees to optimize the multi-class edge without reducing the\nmulti-class problem to $K$ binary one-against-all classifications. The key\nelement of the method is a vector-valued decision stump, factorized into an\ninput-independent vector of length $K$ and label-independent scalar classifier.\nAt inner tree nodes, the label-dependent vector is discarded and the binary\nclassifier can be used for partitioning the input space into two regions. The\nalgorithm retains the conceptual elegance, power, and computational efficiency\nof binary AdaBoost. In experiments it is on par with support vector machines\nand with the best existing multi-class boosting algorithm AOSOLogitBoost, and\nit is significantly better than other known implementations of AdaBoost.MH.\n", "versions": [{"version": "v1", "created": "Fri, 20 Dec 2013 19:33:26 GMT"}], "update_date": "2013-12-23", "authors_parsed": [["K\u00e9gl", "Bal\u00e1zs", ""]]}, {"id": "1312.6098", "submitter": "Razvan Pascanu", "authors": "Razvan Pascanu and Guido Montufar and Yoshua Bengio", "title": "On the number of response regions of deep feed forward networks with\n  piece-wise linear activations", "comments": "17 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper explores the complexity of deep feedforward networks with linear\npre-synaptic couplings and rectified linear activations. This is a contribution\nto the growing body of work contrasting the representational power of deep and\nshallow network architectures. In particular, we offer a framework for\ncomparing deep and shallow models that belong to the family of piecewise linear\nfunctions based on computational geometry. We look at a deep rectifier\nmulti-layer perceptron (MLP) with linear outputs units and compare it with a\nsingle layer version of the model. In the asymptotic regime, when the number of\ninputs stays constant, if the shallow model has $kn$ hidden units and $n_0$\ninputs, then the number of linear regions is $O(k^{n_0}n^{n_0})$. For a $k$\nlayer model with $n$ hidden units on each layer it is $\\Omega(\\left\\lfloor\n{n}/{n_0}\\right\\rfloor^{k-1}n^{n_0})$. The number\n$\\left\\lfloor{n}/{n_0}\\right\\rfloor^{k-1}$ grows faster than $k^{n_0}$ when $n$\ntends to infinity or when $k$ tends to infinity and $n \\geq 2n_0$.\nAdditionally, even when $k$ is small, if we restrict $n$ to be $2n_0$, we can\nshow that a deep model has considerably more linear regions that a shallow one.\nWe consider this as a first step towards understanding the complexity of these\nmodels and specifically towards providing suitable mathematical tools for\nfuture analysis.\n", "versions": [{"version": "v1", "created": "Fri, 20 Dec 2013 20:22:31 GMT"}, {"version": "v2", "created": "Mon, 6 Jan 2014 19:53:34 GMT"}, {"version": "v3", "created": "Mon, 27 Jan 2014 22:13:09 GMT"}, {"version": "v4", "created": "Mon, 10 Feb 2014 17:24:12 GMT"}, {"version": "v5", "created": "Fri, 14 Feb 2014 17:52:12 GMT"}], "update_date": "2014-02-17", "authors_parsed": [["Pascanu", "Razvan", ""], ["Montufar", "Guido", ""], ["Bengio", "Yoshua", ""]]}, {"id": "1312.6108", "submitter": "Nan Wang", "authors": "Nan Wang, Dirk Jancke, Laurenz Wiskott", "title": "Modeling correlations in spontaneous activity of visual cortex with\n  centered Gaussian-binary deep Boltzmann machines", "comments": "9 pages, 4 figures, for openreview ICLR2014, 2nd revision", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.LG q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Spontaneous cortical activity -- the ongoing cortical activities in absence\nof intentional sensory input -- is considered to play a vital role in many\naspects of both normal brain functions and mental dysfunctions. We present a\ncentered Gaussian-binary Deep Boltzmann Machine (GDBM) for modeling the\nactivity in early cortical visual areas and relate the random sampling in GDBMs\nto the spontaneous cortical activity. After training the proposed model on\nnatural image patches, we show that the samples collected from the model's\nprobability distribution encompass similar activity patterns as found in the\nspontaneous activity. Specifically, filters having the same orientation\npreference tend to be active together during random sampling. Our work\ndemonstrates the centered GDBM is a meaningful model approach for basic\nreceptive field properties and the emergence of spontaneous activity patterns\nin early cortical visual areas. Besides, we show empirically that centered\nGDBMs do not suffer from the difficulties during training as GDBMs do and can\nbe properly trained without the layer-wise pretraining.\n", "versions": [{"version": "v1", "created": "Fri, 20 Dec 2013 20:47:28 GMT"}, {"version": "v2", "created": "Mon, 10 Feb 2014 13:46:25 GMT"}, {"version": "v3", "created": "Mon, 17 Feb 2014 16:41:30 GMT"}], "update_date": "2014-02-18", "authors_parsed": [["Wang", "Nan", ""], ["Jancke", "Dirk", ""], ["Wiskott", "Laurenz", ""]]}, {"id": "1312.6114", "submitter": "Diederik P Kingma M.Sc.", "authors": "Diederik P Kingma, Max Welling", "title": "Auto-Encoding Variational Bayes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  How can we perform efficient inference and learning in directed probabilistic\nmodels, in the presence of continuous latent variables with intractable\nposterior distributions, and large datasets? We introduce a stochastic\nvariational inference and learning algorithm that scales to large datasets and,\nunder some mild differentiability conditions, even works in the intractable\ncase. Our contributions is two-fold. First, we show that a reparameterization\nof the variational lower bound yields a lower bound estimator that can be\nstraightforwardly optimized using standard stochastic gradient methods. Second,\nwe show that for i.i.d. datasets with continuous latent variables per\ndatapoint, posterior inference can be made especially efficient by fitting an\napproximate inference model (also called a recognition model) to the\nintractable posterior using the proposed lower bound estimator. Theoretical\nadvantages are reflected in experimental results.\n", "versions": [{"version": "v1", "created": "Fri, 20 Dec 2013 20:58:10 GMT"}, {"version": "v10", "created": "Thu, 1 May 2014 15:43:28 GMT"}, {"version": "v2", "created": "Mon, 23 Dec 2013 13:19:52 GMT"}, {"version": "v3", "created": "Tue, 24 Dec 2013 16:08:10 GMT"}, {"version": "v4", "created": "Fri, 27 Dec 2013 16:59:25 GMT"}, {"version": "v5", "created": "Thu, 9 Jan 2014 20:28:50 GMT"}, {"version": "v6", "created": "Tue, 21 Jan 2014 19:41:37 GMT"}, {"version": "v7", "created": "Tue, 4 Feb 2014 14:10:27 GMT"}, {"version": "v8", "created": "Mon, 3 Mar 2014 16:41:45 GMT"}, {"version": "v9", "created": "Thu, 10 Apr 2014 16:06:37 GMT"}], "update_date": "2014-05-02", "authors_parsed": [["Kingma", "Diederik P", ""], ["Welling", "Max", ""]]}, {"id": "1312.6115", "submitter": "David Reichert", "authors": "David P. Reichert, Thomas Serre", "title": "Neuronal Synchrony in Complex-Valued Deep Networks", "comments": "ICLR 2014, accepted to conference track. This version: added\n  proceedings note, minor additions", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG cs.NE q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning has recently led to great successes in tasks such as image\nrecognition (e.g Krizhevsky et al., 2012). However, deep networks are still\noutmatched by the power and versatility of the brain, perhaps in part due to\nthe richer neuronal computations available to cortical circuits. The challenge\nis to identify which neuronal mechanisms are relevant, and to find suitable\nabstractions to model them. Here, we show how aspects of spike timing, long\nhypothesized to play a crucial role in cortical information processing, could\nbe incorporated into deep networks to build richer, versatile representations.\n  We introduce a neural network formulation based on complex-valued neuronal\nunits that is not only biologically meaningful but also amenable to a variety\nof deep learning frameworks. Here, units are attributed both a firing rate and\na phase, the latter indicating properties of spike timing. We show how this\nformulation qualitatively captures several aspects thought to be related to\nneuronal synchrony, including gating of information processing and dynamic\nbinding of distributed object representations. Focusing on the latter, we\ndemonstrate the potential of the approach in several simple experiments. Thus,\nneuronal synchrony could be a flexible mechanism that fulfills multiple\nfunctional roles in deep networks.\n", "versions": [{"version": "v1", "created": "Fri, 20 Dec 2013 20:59:11 GMT"}, {"version": "v2", "created": "Mon, 23 Dec 2013 13:34:08 GMT"}, {"version": "v3", "created": "Mon, 30 Dec 2013 18:42:17 GMT"}, {"version": "v4", "created": "Tue, 18 Feb 2014 01:28:47 GMT"}, {"version": "v5", "created": "Sat, 22 Mar 2014 20:25:27 GMT"}], "update_date": "2014-03-25", "authors_parsed": [["Reichert", "David P.", ""], ["Serre", "Thomas", ""]]}, {"id": "1312.6116", "submitter": "Jost Tobias Springenberg", "authors": "Jost Tobias Springenberg, Martin Riedmiller", "title": "Improving Deep Neural Networks with Probabilistic Maxout Units", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a probabilistic variant of the recently introduced maxout unit.\nThe success of deep neural networks utilizing maxout can partly be attributed\nto favorable performance under dropout, when compared to rectified linear\nunits. It however also depends on the fact that each maxout unit performs a\npooling operation over a group of linear transformations and is thus partially\ninvariant to changes in its input. Starting from this observation we ask the\nquestion: Can the desirable properties of maxout units be preserved while\nimproving their invariance properties ? We argue that our probabilistic maxout\n(probout) units successfully achieve this balance. We quantitatively verify\nthis claim and report classification performance matching or exceeding the\ncurrent state of the art on three challenging image classification benchmarks\n(CIFAR-10, CIFAR-100 and SVHN).\n", "versions": [{"version": "v1", "created": "Fri, 20 Dec 2013 20:59:15 GMT"}, {"version": "v2", "created": "Wed, 19 Feb 2014 11:13:48 GMT"}], "update_date": "2014-02-20", "authors_parsed": [["Springenberg", "Jost Tobias", ""], ["Riedmiller", "Martin", ""]]}, {"id": "1312.6117", "submitter": "Kamran Kowsari", "authors": "Kamran Kowsari", "title": "Comparison three methods of clustering: k-means, spectral clustering and\n  hierarchical clustering", "comments": "This paper has been withdrawn by the author due to improve add more\n  results", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Comparison of three kind of the clustering and find cost function and loss\nfunction and calculate them. Error rate of the clustering methods and how to\ncalculate the error percentage always be one on the important factor for\nevaluating the clustering methods, so this paper introduce one way to calculate\nthe error rate of clustering methods. Clustering algorithms can be divided into\nseveral categories including partitioning clustering algorithms, hierarchical\nalgorithms and density based algorithms. Generally speaking we should compare\nclustering algorithms by Scalability, Ability to work with different attribute,\nClusters formed by conventional, Having minimal knowledge of the computer to\nrecognize the input parameters, Classes for dealing with noise and extra\ndeposition that same error rate for clustering a new data, Thus, there is no\neffect on the input data, different dimensions of high levels, K-means is one\nof the simplest approach to clustering that clustering is an unsupervised\nproblem.\n", "versions": [{"version": "v1", "created": "Thu, 19 Dec 2013 21:45:10 GMT"}, {"version": "v2", "created": "Thu, 13 Nov 2014 05:52:05 GMT"}], "update_date": "2014-11-14", "authors_parsed": [["Kowsari", "Kamran", ""]]}, {"id": "1312.6120", "submitter": "Andrew Saxe", "authors": "Andrew M. Saxe, James L. McClelland, Surya Ganguli", "title": "Exact solutions to the nonlinear dynamics of learning in deep linear\n  neural networks", "comments": "Submission to ICLR2014. Revised based on reviewer feedback", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cond-mat.dis-nn cs.CV cs.LG q-bio.NC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite the widespread practical success of deep learning methods, our\ntheoretical understanding of the dynamics of learning in deep neural networks\nremains quite sparse. We attempt to bridge the gap between the theory and\npractice of deep learning by systematically analyzing learning dynamics for the\nrestricted case of deep linear neural networks. Despite the linearity of their\ninput-output map, such networks have nonlinear gradient descent dynamics on\nweights that change with the addition of each new hidden layer. We show that\ndeep linear networks exhibit nonlinear learning phenomena similar to those seen\nin simulations of nonlinear networks, including long plateaus followed by rapid\ntransitions to lower error solutions, and faster convergence from greedy\nunsupervised pretraining initial conditions than from random initial\nconditions. We provide an analytical description of these phenomena by finding\nnew exact solutions to the nonlinear dynamics of deep learning. Our theoretical\nanalysis also reveals the surprising finding that as the depth of a network\napproaches infinity, learning speed can nevertheless remain finite: for a\nspecial class of initial conditions on the weights, very deep networks incur\nonly a finite, depth independent, delay in learning speed relative to shallow\nnetworks. We show that, under certain conditions on the training data,\nunsupervised pretraining can find this special class of initial conditions,\nwhile scaled random Gaussian initializations cannot. We further exhibit a new\nclass of random orthogonal initial conditions on weights that, like\nunsupervised pre-training, enjoys depth independent learning times. We further\nshow that these initial conditions also lead to faithful propagation of\ngradients even in deep nonlinear networks, as long as they operate in a special\nregime known as the edge of chaos.\n", "versions": [{"version": "v1", "created": "Fri, 20 Dec 2013 20:24:00 GMT"}, {"version": "v2", "created": "Fri, 24 Jan 2014 20:39:04 GMT"}, {"version": "v3", "created": "Wed, 19 Feb 2014 17:26:57 GMT"}], "update_date": "2014-02-20", "authors_parsed": [["Saxe", "Andrew M.", ""], ["McClelland", "James L.", ""], ["Ganguli", "Surya", ""]]}, {"id": "1312.6157", "submitter": "Mohammad Pezeshki", "authors": "Mohammad Pezeshki, Sajjad Gholami, Ahmad Nickabadi", "title": "Distinction between features extracted using deep belief networks", "comments": "4 pages, 4 figures, ICLR 2014 workshop track", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data representation is an important pre-processing step in many machine\nlearning algorithms. There are a number of methods used for this task such as\nDeep Belief Networks (DBNs) and Discrete Fourier Transforms (DFTs). Since some\nof the features extracted using automated feature extraction methods may not\nalways be related to a specific machine learning task, in this paper we propose\ntwo methods in order to make a distinction between extracted features based on\ntheir relevancy to the task. We applied these two methods to a Deep Belief\nNetwork trained for a face recognition task.\n", "versions": [{"version": "v1", "created": "Fri, 20 Dec 2013 21:52:08 GMT"}, {"version": "v2", "created": "Thu, 2 Jan 2014 17:06:25 GMT"}], "update_date": "2014-01-03", "authors_parsed": [["Pezeshki", "Mohammad", ""], ["Gholami", "Sajjad", ""], ["Nickabadi", "Ahmad", ""]]}, {"id": "1312.6158", "submitter": "Mohammad Pezeshki", "authors": "Mohammad Ali Keyvanrad, Mohammad Pezeshki, and Mohammad Ali\n  Homayounpour", "title": "Deep Belief Networks for Image Denoising", "comments": "ICLR 2014 Conference track", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep Belief Networks which are hierarchical generative models are effective\ntools for feature representation and extraction. Furthermore, DBNs can be used\nin numerous aspects of Machine Learning such as image denoising. In this paper,\nwe propose a novel method for image denoising which relies on the DBNs' ability\nin feature representation. This work is based upon learning of the noise\nbehavior. Generally, features which are extracted using DBNs are presented as\nthe values of the last layer nodes. We train a DBN a way that the network\ntotally distinguishes between nodes presenting noise and nodes presenting image\ncontent in the last later of DBN, i.e. the nodes in the last layer of trained\nDBN are divided into two distinct groups of nodes. After detecting the nodes\nwhich are presenting the noise, we are able to make the noise nodes inactive\nand reconstruct a noiseless image. In section 4 we explore the results of\napplying this method on the MNIST dataset of handwritten digits which is\ncorrupted with additive white Gaussian noise (AWGN). A reduction of 65.9% in\naverage mean square error (MSE) was achieved when the proposed method was used\nfor the reconstruction of the noisy images.\n", "versions": [{"version": "v1", "created": "Fri, 20 Dec 2013 21:56:38 GMT"}, {"version": "v2", "created": "Thu, 2 Jan 2014 17:04:35 GMT"}], "update_date": "2014-01-03", "authors_parsed": [["Keyvanrad", "Mohammad Ali", ""], ["Pezeshki", "Mohammad", ""], ["Homayounpour", "Mohammad Ali", ""]]}, {"id": "1312.6168", "submitter": "Anjan Nepal", "authors": "Anjan Nepal and Alexander Yates", "title": "Factorial Hidden Markov Models for Learning Representations of Natural\n  Language", "comments": "12 pages, 2 tables, ICLR-2014", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most representation learning algorithms for language and image processing are\nlocal, in that they identify features for a data point based on surrounding\npoints. Yet in language processing, the correct meaning of a word often depends\non its global context. As a step toward incorporating global context into\nrepresentation learning, we develop a representation learning algorithm that\nincorporates joint prediction into its technique for producing features for a\nword. We develop efficient variational methods for learning Factorial Hidden\nMarkov Models from large texts, and use variational distributions to produce\nfeatures for each word that are sensitive to the entire input sequence, not\njust to a local context window. Experiments on part-of-speech tagging and\nchunking indicate that the features are competitive with or better than\nexisting state-of-the-art representation learning methods.\n", "versions": [{"version": "v1", "created": "Fri, 20 Dec 2013 22:44:26 GMT"}, {"version": "v2", "created": "Wed, 15 Jan 2014 04:49:47 GMT"}, {"version": "v3", "created": "Tue, 18 Feb 2014 11:22:30 GMT"}], "update_date": "2014-02-19", "authors_parsed": [["Nepal", "Anjan", ""], ["Yates", "Alexander", ""]]}, {"id": "1312.6169", "submitter": "C\\'edric Lagnier", "authors": "C\\'edric Lagnier, Simon Bourigault, Sylvain Lamprier, Ludovic Denoyer\n  and Patrick Gallinari", "title": "Learning Information Spread in Content Networks", "comments": "4 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.SI physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a model for predicting the diffusion of content information on\nsocial media. When propagation is usually modeled on discrete graph structures,\nwe introduce here a continuous diffusion model, where nodes in a diffusion\ncascade are projected onto a latent space with the property that their\nproximity in this space reflects the temporal diffusion process. We focus on\nthe task of predicting contaminated users for an initial initial information\nsource and provide preliminary results on differents datasets.\n", "versions": [{"version": "v1", "created": "Fri, 20 Dec 2013 22:49:01 GMT"}, {"version": "v2", "created": "Sun, 2 Feb 2014 20:36:57 GMT"}], "update_date": "2014-02-04", "authors_parsed": [["Lagnier", "C\u00e9dric", ""], ["Bourigault", "Simon", ""], ["Lamprier", "Sylvain", ""], ["Denoyer", "Ludovic", ""], ["Gallinari", "Patrick", ""]]}, {"id": "1312.6171", "submitter": "Daniel Silver Dr.", "authors": "Ti Wang and Daniel L. Silver", "title": "Learning Paired-associate Images with An Unsupervised Deep Learning\n  Architecture", "comments": "9 pages, for ICLR-2014", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents an unsupervised multi-modal learning system that learns\nassociative representation from two input modalities, or channels, such that\ninput on one channel will correctly generate the associated response at the\nother and vice versa. In this way, the system develops a kind of supervised\nclassification model meant to simulate aspects of human associative memory. The\nsystem uses a deep learning architecture (DLA) composed of two input/output\nchannels formed from stacked Restricted Boltzmann Machines (RBM) and an\nassociative memory network that combines the two channels. The DLA is trained\non pairs of MNIST handwritten digit images to develop hierarchical features and\nassociative representations that are able to reconstruct one image given its\npaired-associate. Experiments show that the multi-modal learning system\ngenerates models that are as accurate as back-propagation networks but with the\nadvantage of a bi-directional network and unsupervised learning from either\npaired or non-paired training examples.\n", "versions": [{"version": "v1", "created": "Fri, 20 Dec 2013 23:07:25 GMT"}, {"version": "v2", "created": "Fri, 10 Jan 2014 23:19:26 GMT"}], "update_date": "2014-01-14", "authors_parsed": [["Wang", "Ti", ""], ["Silver", "Daniel L.", ""]]}, {"id": "1312.6180", "submitter": "Weifeng Liu", "authors": "W. Liu, H. Liu, D.Tao, Y. Wang, K. Lu", "title": "Manifold regularized kernel logistic regression for web image annotation", "comments": "submitted to Neurocomputing", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the rapid advance of Internet technology and smart devices, users often\nneed to manage large amounts of multimedia information using smart devices,\nsuch as personal image and video accessing and browsing. These requirements\nheavily rely on the success of image (video) annotation, and thus large scale\nimage annotation through innovative machine learning methods has attracted\nintensive attention in recent years. One representative work is support vector\nmachine (SVM). Although it works well in binary classification, SVM has a\nnon-smooth loss function and can not naturally cover multi-class case. In this\npaper, we propose manifold regularized kernel logistic regression (KLR) for web\nimage annotation. Compared to SVM, KLR has the following advantages: (1) the\nKLR has a smooth loss function; (2) the KLR produces an explicit estimate of\nthe probability instead of class label; and (3) the KLR can naturally be\ngeneralized to the multi-class case. We carefully conduct experiments on MIR\nFLICKR dataset and demonstrate the effectiveness of manifold regularized kernel\nlogistic regression for image annotation.\n", "versions": [{"version": "v1", "created": "Sat, 21 Dec 2013 00:32:24 GMT"}], "update_date": "2013-12-24", "authors_parsed": [["Liu", "W.", ""], ["Liu", "H.", ""], ["Tao", "D.", ""], ["Wang", "Y.", ""], ["Lu", "K.", ""]]}, {"id": "1312.6182", "submitter": "Weifeng Liu", "authors": "W. Liu, H. Zhang, D. Tao, Y. Wang, K. Lu", "title": "Large-Scale Paralleled Sparse Principal Component Analysis", "comments": "submitted to Multimedia Tools and Applications", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MS cs.LG cs.NA stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Principal component analysis (PCA) is a statistical technique commonly used\nin multivariate data analysis. However, PCA can be difficult to interpret and\nexplain since the principal components (PCs) are linear combinations of the\noriginal variables. Sparse PCA (SPCA) aims to balance statistical fidelity and\ninterpretability by approximating sparse PCs whose projections capture the\nmaximal variance of original data. In this paper we present an efficient and\nparalleled method of SPCA using graphics processing units (GPUs), which can\nprocess large blocks of data in parallel. Specifically, we construct parallel\nimplementations of the four optimization formulations of the generalized power\nmethod of SPCA (GP-SPCA), one of the most efficient and effective SPCA\napproaches, on a GPU. The parallel GPU implementation of GP-SPCA (using CUBLAS)\nis up to eleven times faster than the corresponding CPU implementation (using\nCBLAS), and up to 107 times faster than a MatLab implementation. Extensive\ncomparative experiments in several real-world datasets confirm that SPCA offers\na practical advantage.\n", "versions": [{"version": "v1", "created": "Sat, 21 Dec 2013 00:38:02 GMT"}], "update_date": "2013-12-24", "authors_parsed": [["Liu", "W.", ""], ["Zhang", "H.", ""], ["Tao", "D.", ""], ["Wang", "Y.", ""], ["Lu", "K.", ""]]}, {"id": "1312.6184", "submitter": "Jimmy Ba", "authors": "Lei Jimmy Ba, Rich Caruana", "title": "Do Deep Nets Really Need to be Deep?", "comments": "final revision coming soon", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Currently, deep neural networks are the state of the art on problems such as\nspeech recognition and computer vision. In this extended abstract, we show that\nshallow feed-forward networks can learn the complex functions previously\nlearned by deep nets and achieve accuracies previously only achievable with\ndeep models. Moreover, in some cases the shallow neural nets can learn these\ndeep functions using a total number of parameters similar to the original deep\nmodel. We evaluate our method on the TIMIT phoneme recognition task and are\nable to train shallow fully-connected nets that perform similarly to complex,\nwell-engineered, deep convolutional architectures. Our success in training\nshallow neural nets to mimic deeper models suggests that there probably exist\nbetter algorithms for training shallow feed-forward nets than those currently\navailable.\n", "versions": [{"version": "v1", "created": "Sat, 21 Dec 2013 00:47:43 GMT"}, {"version": "v2", "created": "Fri, 3 Jan 2014 03:32:10 GMT"}, {"version": "v3", "created": "Mon, 6 Jan 2014 20:49:04 GMT"}, {"version": "v4", "created": "Wed, 8 Jan 2014 17:34:30 GMT"}, {"version": "v5", "created": "Fri, 21 Feb 2014 20:04:00 GMT"}, {"version": "v6", "created": "Tue, 7 Oct 2014 21:12:27 GMT"}, {"version": "v7", "created": "Sat, 11 Oct 2014 00:19:10 GMT"}], "update_date": "2014-10-14", "authors_parsed": [["Ba", "Lei Jimmy", ""], ["Caruana", "Rich", ""]]}, {"id": "1312.6186", "submitter": "Tom Paine", "authors": "Thomas Paine, Hailin Jin, Jianchao Yang, Zhe Lin, Thomas Huang", "title": "GPU Asynchronous Stochastic Gradient Descent to Speed Up Neural Network\n  Training", "comments": "6 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.DC cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The ability to train large-scale neural networks has resulted in\nstate-of-the-art performance in many areas of computer vision. These results\nhave largely come from computational break throughs of two forms: model\nparallelism, e.g. GPU accelerated training, which has seen quick adoption in\ncomputer vision circles, and data parallelism, e.g. A-SGD, whose large scale\nhas been used mostly in industry. We report early experiments with a system\nthat makes use of both model parallelism and data parallelism, we call GPU\nA-SGD. We show using GPU A-SGD it is possible to speed up training of large\nconvolutional neural networks useful for computer vision. We believe GPU A-SGD\nwill make it possible to train larger networks on larger training sets in a\nreasonable amount of time.\n", "versions": [{"version": "v1", "created": "Sat, 21 Dec 2013 00:56:56 GMT"}], "update_date": "2013-12-24", "authors_parsed": [["Paine", "Thomas", ""], ["Jin", "Hailin", ""], ["Yang", "Jianchao", ""], ["Lin", "Zhe", ""], ["Huang", "Thomas", ""]]}, {"id": "1312.6190", "submitter": "Son Tran", "authors": "Son N. Tran, Artur d'Avila Garcez", "title": "Adaptive Feature Ranking for Unsupervised Transfer Learning", "comments": "9 pages 7 figures, new experimental results on ranking and transfer\n  have been added, typo fixed", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Transfer Learning is concerned with the application of knowledge gained from\nsolving a problem to a different but related problem domain. In this paper, we\npropose a method and efficient algorithm for ranking and selecting\nrepresentations from a Restricted Boltzmann Machine trained on a source domain\nto be transferred onto a target domain. Experiments carried out using the\nMNIST, ICDAR and TiCC image datasets show that the proposed adaptive feature\nranking and transfer learning method offers statistically significant\nimprovements on the training of RBMs. Our method is general in that the\nknowledge chosen by the ranking function does not depend on its relation to any\nspecific target domain, and it works with unsupervised learning and\nknowledge-based transfer.\n", "versions": [{"version": "v1", "created": "Sat, 21 Dec 2013 01:50:08 GMT"}, {"version": "v2", "created": "Wed, 28 May 2014 16:35:17 GMT"}], "update_date": "2014-05-29", "authors_parsed": [["Tran", "Son N.", ""], ["Garcez", "Artur d'Avila", ""]]}, {"id": "1312.6192", "submitter": "Samuel Bowman", "authors": "Samuel R. Bowman", "title": "Can recursive neural tensor networks learn logical reasoning?", "comments": "Submitted for presentation at ICLR 2014. Source code and data:\n  http://goo.gl/PSyF5u", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recursive neural network models and their accompanying vector representations\nfor words have seen success in an array of increasingly semantically\nsophisticated tasks, but almost nothing is known about their ability to\naccurately capture the aspects of linguistic meaning that are necessary for\ninterpretation or reasoning. To evaluate this, I train a recursive model on a\nnew corpus of constructed examples of logical reasoning in short sentences,\nlike the inference of \"some animal walks\" from \"some dog walks\" or \"some cat\nwalks,\" given that dogs and cats are animals. This model learns representations\nthat generalize well to new types of reasoning pattern in all but a few cases,\na result which is promising for the ability of learned representation models to\ncapture logical reasoning.\n", "versions": [{"version": "v1", "created": "Sat, 21 Dec 2013 02:29:42 GMT"}, {"version": "v2", "created": "Tue, 24 Dec 2013 01:42:09 GMT"}, {"version": "v3", "created": "Tue, 4 Feb 2014 18:02:09 GMT"}, {"version": "v4", "created": "Sat, 15 Feb 2014 20:59:04 GMT"}], "update_date": "2014-02-18", "authors_parsed": [["Bowman", "Samuel R.", ""]]}, {"id": "1312.6197", "submitter": "David Warde-Farley", "authors": "David Warde-Farley, Ian J. Goodfellow, Aaron Courville and Yoshua\n  Bengio", "title": "An empirical analysis of dropout in piecewise linear networks", "comments": "Extensive updates; 8 pages plus acknowledgements/references", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The recently introduced dropout training criterion for neural networks has\nbeen the subject of much attention due to its simplicity and remarkable\neffectiveness as a regularizer, as well as its interpretation as a training\nprocedure for an exponentially large ensemble of networks that share\nparameters. In this work we empirically investigate several questions related\nto the efficacy of dropout, specifically as it concerns networks employing the\npopular rectified linear activation function. We investigate the quality of the\ntest time weight-scaling inference procedure by evaluating the geometric\naverage exactly in small models, as well as compare the performance of the\ngeometric mean to the arithmetic mean more commonly employed by ensemble\ntechniques. We explore the effect of tied weights on the ensemble\ninterpretation by training ensembles of masked networks without tied weights.\nFinally, we investigate an alternative criterion based on a biased estimator of\nthe maximum likelihood ensemble gradient.\n", "versions": [{"version": "v1", "created": "Sat, 21 Dec 2013 03:19:33 GMT"}, {"version": "v2", "created": "Thu, 2 Jan 2014 12:26:53 GMT"}], "update_date": "2014-01-03", "authors_parsed": [["Warde-Farley", "David", ""], ["Goodfellow", "Ian J.", ""], ["Courville", "Aaron", ""], ["Bengio", "Yoshua", ""]]}, {"id": "1312.6199", "submitter": "Joan Bruna", "authors": "Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna,\n  Dumitru Erhan, Ian Goodfellow, Rob Fergus", "title": "Intriguing properties of neural networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.NE", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  Deep neural networks are highly expressive models that have recently achieved\nstate of the art performance on speech and visual recognition tasks. While\ntheir expressiveness is the reason they succeed, it also causes them to learn\nuninterpretable solutions that could have counter-intuitive properties. In this\npaper we report two such properties.\n  First, we find that there is no distinction between individual high level\nunits and random linear combinations of high level units, according to various\nmethods of unit analysis. It suggests that it is the space, rather than the\nindividual units, that contains of the semantic information in the high layers\nof neural networks.\n  Second, we find that deep neural networks learn input-output mappings that\nare fairly discontinuous to a significant extend. We can cause the network to\nmisclassify an image by applying a certain imperceptible perturbation, which is\nfound by maximizing the network's prediction error. In addition, the specific\nnature of these perturbations is not a random artifact of learning: the same\nperturbation can cause a different network, that was trained on a different\nsubset of the dataset, to misclassify the same input.\n", "versions": [{"version": "v1", "created": "Sat, 21 Dec 2013 03:36:08 GMT"}, {"version": "v2", "created": "Fri, 3 Jan 2014 04:37:34 GMT"}, {"version": "v3", "created": "Thu, 13 Feb 2014 17:40:08 GMT"}, {"version": "v4", "created": "Wed, 19 Feb 2014 16:33:14 GMT"}], "update_date": "2014-02-20", "authors_parsed": [["Szegedy", "Christian", ""], ["Zaremba", "Wojciech", ""], ["Sutskever", "Ilya", ""], ["Bruna", "Joan", ""], ["Erhan", "Dumitru", ""], ["Goodfellow", "Ian", ""], ["Fergus", "Rob", ""]]}, {"id": "1312.6203", "submitter": "Joan Bruna", "authors": "Joan Bruna, Wojciech Zaremba, Arthur Szlam and Yann LeCun", "title": "Spectral Networks and Locally Connected Networks on Graphs", "comments": "14 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolutional Neural Networks are extremely efficient architectures in image\nand audio recognition tasks, thanks to their ability to exploit the local\ntranslational invariance of signal classes over their domain. In this paper we\nconsider possible generalizations of CNNs to signals defined on more general\ndomains without the action of a translation group. In particular, we propose\ntwo constructions, one based upon a hierarchical clustering of the domain, and\nanother based on the spectrum of the graph Laplacian. We show through\nexperiments that for low-dimensional graphs it is possible to learn\nconvolutional layers with a number of parameters independent of the input size,\nresulting in efficient deep architectures.\n", "versions": [{"version": "v1", "created": "Sat, 21 Dec 2013 04:25:53 GMT"}, {"version": "v2", "created": "Thu, 20 Feb 2014 23:23:06 GMT"}, {"version": "v3", "created": "Wed, 21 May 2014 16:27:09 GMT"}], "update_date": "2014-05-22", "authors_parsed": [["Bruna", "Joan", ""], ["Zaremba", "Wojciech", ""], ["Szlam", "Arthur", ""], ["LeCun", "Yann", ""]]}, {"id": "1312.6204", "submitter": "Judy Hoffman", "authors": "Judy Hoffman, Eric Tzeng, Jeff Donahue, Yangqing Jia, Kate Saenko,\n  Trevor Darrell", "title": "One-Shot Adaptation of Supervised Deep Convolutional Models", "comments": null, "journal-ref": "ICLR Workshop 2014", "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Dataset bias remains a significant barrier towards solving real world\ncomputer vision tasks. Though deep convolutional networks have proven to be a\ncompetitive approach for image classification, a question remains: have these\nmodels have solved the dataset bias problem? In general, training or\nfine-tuning a state-of-the-art deep model on a new domain requires a\nsignificant amount of data, which for many applications is simply not\navailable. Transfer of models directly to new domains without adaptation has\nhistorically led to poor recognition performance. In this paper, we pose the\nfollowing question: is a single image dataset, much larger than previously\nexplored for adaptation, comprehensive enough to learn general deep models that\nmay be effectively applied to new image domains? In other words, are deep CNNs\ntrained on large amounts of labeled data as susceptible to dataset bias as\nprevious methods have been shown to be? We show that a generic supervised deep\nCNN model trained on a large dataset reduces, but does not remove, dataset\nbias. Furthermore, we propose several methods for adaptation with deep models\nthat are able to operate with little (one example per category) or no labeled\ndomain specific data. Our experiments show that adaptation of deep models on\nbenchmark visual domain adaptation datasets can provide a significant\nperformance boost.\n", "versions": [{"version": "v1", "created": "Sat, 21 Dec 2013 04:32:51 GMT"}, {"version": "v2", "created": "Tue, 18 Feb 2014 02:57:42 GMT"}], "update_date": "2017-11-10", "authors_parsed": [["Hoffman", "Judy", ""], ["Tzeng", "Eric", ""], ["Donahue", "Jeff", ""], ["Jia", "Yangqing", ""], ["Saenko", "Kate", ""], ["Darrell", "Trevor", ""]]}, {"id": "1312.6205", "submitter": "Sida Wang", "authors": "Sida I. Wang, Roy Frostig, Percy Liang, Christopher D. Manning", "title": "Relaxations for inference in restricted Boltzmann machines", "comments": "ICLR 2014 workshop track submission", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a relaxation-based approximate inference algorithm that samples\nnear-MAP configurations of a binary pairwise Markov random field. We experiment\non MAP inference tasks in several restricted Boltzmann machines. We also use\nour underlying sampler to estimate the log-partition function of restricted\nBoltzmann machines and compare against other sampling-based methods.\n", "versions": [{"version": "v1", "created": "Sat, 21 Dec 2013 04:53:56 GMT"}, {"version": "v2", "created": "Thu, 2 Jan 2014 07:50:44 GMT"}], "update_date": "2014-01-03", "authors_parsed": [["Wang", "Sida I.", ""], ["Frostig", "Roy", ""], ["Liang", "Percy", ""], ["Manning", "Christopher D.", ""]]}, {"id": "1312.6211", "submitter": "Ian Goodfellow", "authors": "Ian J. Goodfellow, Mehdi Mirza, Da Xiao, Aaron Courville, Yoshua\n  Bengio", "title": "An Empirical Investigation of Catastrophic Forgetting in Gradient-Based\n  Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Catastrophic forgetting is a problem faced by many machine learning models\nand algorithms. When trained on one task, then trained on a second task, many\nmachine learning models \"forget\" how to perform the first task. This is widely\nbelieved to be a serious problem for neural networks. Here, we investigate the\nextent to which the catastrophic forgetting problem occurs for modern neural\nnetworks, comparing both established and recent gradient-based training\nalgorithms and activation functions. We also examine the effect of the\nrelationship between the first task and the second task on catastrophic\nforgetting. We find that it is always best to train using the dropout\nalgorithm--the dropout algorithm is consistently best at adapting to the new\ntask, remembering the old task, and has the best tradeoff curve between these\ntwo extremes. We find that different tasks and relationships between tasks\nresult in very different rankings of activation function performance. This\nsuggests the choice of activation function should always be cross-validated.\n", "versions": [{"version": "v1", "created": "Sat, 21 Dec 2013 06:31:41 GMT"}, {"version": "v2", "created": "Mon, 6 Jan 2014 21:27:34 GMT"}, {"version": "v3", "created": "Wed, 4 Mar 2015 01:43:31 GMT"}], "update_date": "2015-03-05", "authors_parsed": [["Goodfellow", "Ian J.", ""], ["Mirza", "Mehdi", ""], ["Xiao", "Da", ""], ["Courville", "Aaron", ""], ["Bengio", "Yoshua", ""]]}, {"id": "1312.6214", "submitter": "Elad Hazan", "authors": "Elad Hazan and Zohar Karnin and Raghu Mehka", "title": "Volumetric Spanners: an Efficient Exploration Basis for Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Numerous machine learning problems require an exploration basis - a mechanism\nto explore the action space. We define a novel geometric notion of exploration\nbasis with low variance, called volumetric spanners, and give efficient\nalgorithms to construct such a basis.\n  We show how efficient volumetric spanners give rise to the first efficient\nand optimal regret algorithm for bandit linear optimization over general convex\nsets. Previously such results were known only for specific convex sets, or\nunder special conditions such as the existence of an efficient self-concordant\nbarrier for the underlying set.\n", "versions": [{"version": "v1", "created": "Sat, 21 Dec 2013 06:51:50 GMT"}, {"version": "v2", "created": "Sun, 26 Jan 2014 12:16:59 GMT"}, {"version": "v3", "created": "Sun, 25 May 2014 11:57:08 GMT"}], "update_date": "2014-05-27", "authors_parsed": [["Hazan", "Elad", ""], ["Karnin", "Zohar", ""], ["Mehka", "Raghu", ""]]}, {"id": "1312.6273", "submitter": "Minyar Sassi", "authors": "Sonia Alouane-Ksouri, Minyar Sassi-Hidri, Kamel Barkaoui", "title": "Parallel architectures for fuzzy triadic similarity learning", "comments": null, "journal-ref": "International Conference on Control, Engineering & Information\n  Technology (CEIT), Proceedings Engineering & Technology, Vol. 1, pp. 121-126,\n  2013", "doi": null, "report-no": null, "categories": "cs.DC cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In a context of document co-clustering, we define a new similarity measure\nwhich iteratively computes similarity while combining fuzzy sets in a\nthree-partite graph. The fuzzy triadic similarity (FT-Sim) model can deal with\nuncertainty offers by the fuzzy sets. Moreover, with the development of the Web\nand the high availability of storage spaces, more and more documents become\naccessible. Documents can be provided from multiple sites and make similarity\ncomputation an expensive processing. This problem motivated us to use parallel\ncomputing. In this paper, we introduce parallel architectures which are able to\ntreat large and multi-source data sets by a sequential, a merging or a\nsplitting-based process. Then, we proceed to a local and a central (or global)\ncomputing using the basic FT-Sim measure. The idea behind these architectures\nis to reduce both time and space complexities thanks to parallel computation.\n", "versions": [{"version": "v1", "created": "Sat, 21 Dec 2013 16:51:26 GMT"}], "update_date": "2013-12-24", "authors_parsed": [["Alouane-Ksouri", "Sonia", ""], ["Sassi-Hidri", "Minyar", ""], ["Barkaoui", "Kamel", ""]]}, {"id": "1312.6282", "submitter": "Fran\\c{c}ois Denis", "authors": "Fran\\c{c}ois Denis, Mattias Gybels and Amaury Habrard", "title": "Dimension-free Concentration Bounds on Hankel Matrices for Spectral\n  Learning", "comments": "Extended version of a paper to appear at ICML 2014", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning probabilistic models over strings is an important issue for many\napplications. Spectral methods propose elegant solutions to the problem of\ninferring weighted automata from finite samples of variable-length strings\ndrawn from an unknown target distribution. These methods rely on a singular\nvalue decomposition of a matrix $H_S$, called the Hankel matrix, that records\nthe frequencies of (some of) the observed strings. The accuracy of the learned\ndistribution depends both on the quantity of information embedded in $H_S$ and\non the distance between $H_S$ and its mean $H_r$. Existing concentration bounds\nseem to indicate that the concentration over $H_r$ gets looser with the size of\n$H_r$, suggesting to make a trade-off between the quantity of used information\nand the size of $H_r$. We propose new dimension-free concentration bounds for\nseveral variants of Hankel matrices. Experiments demonstrate that these bounds\nare tight and that they significantly improve existing bounds. These results\nsuggest that the concentration rate of the Hankel matrix around its mean does\nnot constitute an argument for limiting its size.\n", "versions": [{"version": "v1", "created": "Sat, 21 Dec 2013 18:10:59 GMT"}], "update_date": "2013-12-24", "authors_parsed": [["Denis", "Fran\u00e7ois", ""], ["Gybels", "Mattias", ""], ["Habrard", "Amaury", ""]]}, {"id": "1312.6430", "submitter": "Kota Hara", "authors": "Kota Hara and Rama Chellappa", "title": "Growing Regression Forests by Classification: Applications to Object\n  Pose Estimation", "comments": "Paper accepted by ECCV 2014", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we propose a novel node splitting method for regression trees\nand incorporate it into the regression forest framework. Unlike traditional\nbinary splitting, where the splitting rule is selected from a predefined set of\nbinary splitting rules via trial-and-error, the proposed node splitting method\nfirst finds clusters of the training data which at least locally minimize the\nempirical loss without considering the input space. Then splitting rules which\npreserve the found clusters as much as possible are determined by casting the\nproblem into a classification problem. Consequently, our new node splitting\nmethod enjoys more freedom in choosing the splitting rules, resulting in more\nefficient tree structures. In addition to the Euclidean target space, we\npresent a variant which can naturally deal with a circular target space by the\nproper use of circular statistics. We apply the regression forest employing our\nnode splitting to head pose estimation (Euclidean target space) and car\ndirection estimation (circular target space) and demonstrate that the proposed\nmethod significantly outperforms state-of-the-art methods (38.5% and 22.5%\nerror reduction respectively).\n", "versions": [{"version": "v1", "created": "Sun, 22 Dec 2013 22:10:42 GMT"}, {"version": "v2", "created": "Tue, 15 Jul 2014 02:51:13 GMT"}], "update_date": "2014-07-16", "authors_parsed": [["Hara", "Kota", ""], ["Chellappa", "Rama", ""]]}, {"id": "1312.6461", "submitter": "Sho Sonoda", "authors": "Sho Sonoda, Noboru Murata", "title": "Nonparametric Weight Initialization of Neural Networks via Integral\n  Representation", "comments": "For ICLR2014, revised into 9 pages; revised into 12 pages (with\n  supplements)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A new initialization method for hidden parameters in a neural network is\nproposed. Derived from the integral representation of the neural network, a\nnonparametric probability distribution of hidden parameters is introduced. In\nthis proposal, hidden parameters are initialized by samples drawn from this\ndistribution, and output parameters are fitted by ordinary linear regression.\nNumerical experiments show that backpropagation with proposed initialization\nconverges faster than uniformly random initialization. Also it is shown that\nthe proposed method achieves enough accuracy by itself without backpropagation\nin some cases.\n", "versions": [{"version": "v1", "created": "Mon, 23 Dec 2013 03:23:04 GMT"}, {"version": "v2", "created": "Tue, 24 Dec 2013 02:54:29 GMT"}, {"version": "v3", "created": "Wed, 19 Feb 2014 20:02:05 GMT"}], "update_date": "2014-02-20", "authors_parsed": [["Sonoda", "Sho", ""], ["Murata", "Noboru", ""]]}, {"id": "1312.6594", "submitter": "Gabriel Dulac-Arnold", "authors": "Gabriel Dulac-Arnold and Ludovic Denoyer and Nicolas Thome and\n  Matthieu Cord and Patrick Gallinari", "title": "Sequentially Generated Instance-Dependent Image Representations for\n  Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we investigate a new framework for image classification that\nadaptively generates spatial representations. Our strategy is based on a\nsequential process that learns to explore the different regions of any image in\norder to infer its category. In particular, the choice of regions is specific\nto each image, directed by the actual content of previously selected\nregions.The capacity of the system to handle incomplete image information as\nwell as its adaptive region selection allow the system to perform well in\nbudgeted classification tasks by exploiting a dynamicly generated\nrepresentation of each image. We demonstrate the system's abilities in a series\nof image-based exploration and classification tasks that highlight its learned\nexploration and inference abilities.\n", "versions": [{"version": "v1", "created": "Fri, 20 Dec 2013 16:36:40 GMT"}, {"version": "v2", "created": "Tue, 7 Jan 2014 14:44:42 GMT"}, {"version": "v3", "created": "Tue, 11 Feb 2014 17:07:21 GMT"}], "update_date": "2014-02-12", "authors_parsed": [["Dulac-Arnold", "Gabriel", ""], ["Denoyer", "Ludovic", ""], ["Thome", "Nicolas", ""], ["Cord", "Matthieu", ""], ["Gallinari", "Patrick", ""]]}, {"id": "1312.6597", "submitter": "Luis Marujo", "authors": "Luis Marujo, Anatole Gershman, Jaime Carbonell, David Martins de\n  Matos, Jo\\~ao P. Neto", "title": "Co-Multistage of Multiple Classifiers for Imbalanced Multiclass Learning", "comments": "Preliminary version of the paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we propose two stochastic architectural models (CMC and CMC-M)\nwith two layers of classifiers applicable to datasets with one and multiple\nskewed classes. This distinction becomes important when the datasets have a\nlarge number of classes. Therefore, we present a novel solution to imbalanced\nmulticlass learning with several skewed majority classes, which improves\nminority classes identification. This fact is particularly important for text\nclassification tasks, such as event detection. Our models combined with\npre-processing sampling techniques improved the classification results on six\nwell-known datasets. Finally, we have also introduced a new metric SG-Mean to\novercome the multiplication by zero limitation of G-Mean.\n", "versions": [{"version": "v1", "created": "Mon, 23 Dec 2013 16:52:56 GMT"}, {"version": "v2", "created": "Fri, 24 Jan 2014 23:09:17 GMT"}], "update_date": "2014-01-28", "authors_parsed": [["Marujo", "Luis", ""], ["Gershman", "Anatole", ""], ["Carbonell", "Jaime", ""], ["de Matos", "David Martins", ""], ["Neto", "Jo\u00e3o P.", ""]]}, {"id": "1312.6607", "submitter": "Jean-Marc Lasgouttes", "authors": "Victorin Martin, Jean-Marc Lasgouttes, Cyril Furtlehner", "title": "Using Latent Binary Variables for Online Reconstruction of Large Scale\n  Systems", "comments": null, "journal-ref": null, "doi": "10.1007/s10472-015-9470-x", "report-no": null, "categories": "math.PR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a probabilistic graphical model realizing a minimal encoding of\nreal variables dependencies based on possibly incomplete observation and an\nempirical cumulative distribution function per variable. The target application\nis a large scale partially observed system, like e.g. a traffic network, where\na small proportion of real valued variables are observed, and the other\nvariables have to be predicted. Our design objective is therefore to have good\nscalability in a real-time setting. Instead of attempting to encode the\ndependencies of the system directly in the description space, we propose a way\nto encode them in a latent space of binary variables, reflecting a rough\nperception of the observable (congested/non-congested for a traffic road). The\nmethod relies in part on message passing algorithms, i.e. belief propagation,\nbut the core of the work concerns the definition of meaningful latent variables\nassociated to the variables of interest and their pairwise dependencies.\nNumerical experiments demonstrate the applicability of the method in practice.\n", "versions": [{"version": "v1", "created": "Mon, 23 Dec 2013 17:11:59 GMT"}], "update_date": "2015-08-28", "authors_parsed": [["Martin", "Victorin", ""], ["Lasgouttes", "Jean-Marc", ""], ["Furtlehner", "Cyril", ""]]}, {"id": "1312.6652", "submitter": "Boaz Barak", "authors": "Boaz Barak, Jonathan Kelner, David Steurer", "title": "Rounding Sum-of-Squares Relaxations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.LG quant-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a general approach to rounding semidefinite programming\nrelaxations obtained by the Sum-of-Squares method (Lasserre hierarchy). Our\napproach is based on using the connection between these relaxations and the\nSum-of-Squares proof system to transform a *combining algorithm* -- an\nalgorithm that maps a distribution over solutions into a (possibly weaker)\nsolution -- into a *rounding algorithm* that maps a solution of the relaxation\nto a solution of the original problem.\n  Using this approach, we obtain algorithms that yield improved results for\nnatural variants of three well-known problems:\n  1) We give a quasipolynomial-time algorithm that approximates the maximum of\na low degree multivariate polynomial with non-negative coefficients over the\nEuclidean unit sphere. Beyond being of interest in its own right, this is\nrelated to an open question in quantum information theory, and our techniques\nhave already led to improved results in this area (Brand\\~{a}o and Harrow, STOC\n'13).\n  2) We give a polynomial-time algorithm that, given a d dimensional subspace\nof R^n that (almost) contains the characteristic function of a set of size n/k,\nfinds a vector $v$ in the subspace satisfying $|v|_4^4 > c(k/d^{1/3}) |v|_2^2$,\nwhere $|v|_p = (E_i v_i^p)^{1/p}$. Aside from being a natural relaxation, this\nis also motivated by a connection to the Small Set Expansion problem shown by\nBarak et al. (STOC 2012) and our results yield a certain improvement for that\nproblem.\n  3) We use this notion of L_4 vs. L_2 sparsity to obtain a polynomial-time\nalgorithm with substantially improved guarantees for recovering a planted\n$\\mu$-sparse vector v in a random d-dimensional subspace of R^n. If v has mu n\nnonzero coordinates, we can recover it with high probability whenever $\\mu <\nO(\\min(1,n/d^2))$, improving for $d < n^{2/3}$ prior methods which\nintrinsically required $\\mu < O(1/\\sqrt(d))$.\n", "versions": [{"version": "v1", "created": "Mon, 23 Dec 2013 19:30:46 GMT"}], "update_date": "2013-12-24", "authors_parsed": [["Barak", "Boaz", ""], ["Kelner", "Jonathan", ""], ["Steurer", "David", ""]]}, {"id": "1312.6661", "submitter": "Justin Kinney", "authors": "Justin B. Kinney", "title": "Rapid and deterministic estimation of probability densities using\n  scale-free field theories", "comments": "4 pages, 4 figures. Major revision in v3. The \"Density Estimation\n  using Field Theory\" (DEFT) software package is available at\n  https://github.com/jbkinney/13_deft", "journal-ref": "Phys. Rev. E 90, 011301 (2014)", "doi": "10.1103/PhysRevE.90.011301", "report-no": null, "categories": "physics.data-an cs.LG math.ST q-bio.QM stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The question of how best to estimate a continuous probability density from\nfinite data is an intriguing open problem at the interface of statistics and\nphysics. Previous work has argued that this problem can be addressed in a\nnatural way using methods from statistical field theory. Here I describe new\nresults that allow this field-theoretic approach to be rapidly and\ndeterministically computed in low dimensions, making it practical for use in\nday-to-day data analysis. Importantly, this approach does not impose a\nprivileged length scale for smoothness of the inferred probability density, but\nrather learns a natural length scale from the data due to the tradeoff between\ngoodness-of-fit and an Occam factor. Open source software implementing this\nmethod in one and two dimensions is provided.\n", "versions": [{"version": "v1", "created": "Mon, 23 Dec 2013 20:13:35 GMT"}, {"version": "v2", "created": "Fri, 17 Jan 2014 20:13:24 GMT"}, {"version": "v3", "created": "Fri, 18 Apr 2014 21:29:41 GMT"}], "update_date": "2014-07-16", "authors_parsed": [["Kinney", "Justin B.", ""]]}, {"id": "1312.6712", "submitter": "Josif Grabocka", "authors": "Josif Grabocka, Lars Schmidt-Thieme", "title": "Invariant Factorization Of Time-Series", "comments": null, "journal-ref": null, "doi": "10.1007/s10618-014-0364-z", "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Time-series classification is an important domain of machine learning and a\nplethora of methods have been developed for the task. In comparison to existing\napproaches, this study presents a novel method which decomposes a time-series\ndataset into latent patterns and membership weights of local segments to those\npatterns. The process is formalized as a constrained objective function and a\ntailored stochastic coordinate descent optimization is applied. The time-series\nare projected to a new feature representation consisting of the sums of the\nmembership weights, which captures frequencies of local patterns. Features from\nvarious sliding window sizes are concatenated in order to encapsulate the\ninteraction of patterns from different sizes. Finally, a large-scale\nexperimental comparison against 6 state of the art baselines and 43 real life\ndatasets is conducted. The proposed method outperforms all the baselines with\nstatistically significant margins in terms of prediction accuracy.\n", "versions": [{"version": "v1", "created": "Mon, 23 Dec 2013 22:15:59 GMT"}], "update_date": "2015-03-12", "authors_parsed": [["Grabocka", "Josif", ""], ["Schmidt-Thieme", "Lars", ""]]}, {"id": "1312.6724", "submitter": "Konstantin Voevodski", "authors": "Pranjal Awasthi and Maria-Florina Balcan and Konstantin Voevodski", "title": "Local algorithms for interactive clustering", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the design of interactive clustering algorithms for data sets\nsatisfying natural stability assumptions. Our algorithms start with any initial\nclustering and only make local changes in each step; both are desirable\nfeatures in many applications. We show that in this constrained setting one can\nstill design provably efficient algorithms that produce accurate clusterings.\nWe also show that our algorithms perform well on real-world data.\n", "versions": [{"version": "v1", "created": "Tue, 24 Dec 2013 00:16:37 GMT"}, {"version": "v2", "created": "Fri, 7 Feb 2014 05:12:20 GMT"}, {"version": "v3", "created": "Thu, 19 Mar 2015 23:45:54 GMT"}], "update_date": "2015-03-23", "authors_parsed": [["Awasthi", "Pranjal", ""], ["Balcan", "Maria-Florina", ""], ["Voevodski", "Konstantin", ""]]}, {"id": "1312.6807", "submitter": "Feng Xia", "authors": "Fengqi Li, Chuang Yu, Nanhai Yang, Feng Xia, Guangming Li, Fatemeh\n  Kaveh-Yazdy", "title": "Iterative Nearest Neighborhood Oversampling in Semisupervised Learning\n  from Imbalanced Data", "comments": null, "journal-ref": "The Scientific World Journal, Volume 2013, Article ID 875450, 2013", "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Transductive graph-based semi-supervised learning methods usually build an\nundirected graph utilizing both labeled and unlabeled samples as vertices.\nThose methods propagate label information of labeled samples to neighbors\nthrough their edges in order to get the predicted labels of unlabeled samples.\nMost popular semi-supervised learning approaches are sensitive to initial label\ndistribution happened in imbalanced labeled datasets. The class boundary will\nbe severely skewed by the majority classes in an imbalanced classification. In\nthis paper, we proposed a simple and effective approach to alleviate the\nunfavorable influence of imbalance problem by iteratively selecting a few\nunlabeled samples and adding them into the minority classes to form a balanced\nlabeled dataset for the learning methods afterwards. The experiments on UCI\ndatasets and MNIST handwritten digits dataset showed that the proposed approach\noutperforms other existing state-of-art methods.\n", "versions": [{"version": "v1", "created": "Tue, 24 Dec 2013 12:24:30 GMT"}], "update_date": "2013-12-25", "authors_parsed": [["Li", "Fengqi", ""], ["Yu", "Chuang", ""], ["Yang", "Nanhai", ""], ["Xia", "Feng", ""], ["Li", "Guangming", ""], ["Kaveh-Yazdy", "Fatemeh", ""]]}, {"id": "1312.6820", "submitter": "Ahmed Farahat", "authors": "Ahmed K. Farahat, Ali Ghodsi, Mohamed S. Kamel", "title": "A Fast Greedy Algorithm for Generalized Column Subset Selection", "comments": "NIPS'13 Workshop on Greedy Algorithms, Frank-Wolfe and Friends", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper defines a generalized column subset selection problem which is\nconcerned with the selection of a few columns from a source matrix A that best\napproximate the span of a target matrix B. The paper then proposes a fast\ngreedy algorithm for solving this problem and draws connections to different\nproblems that can be efficiently solved using the proposed algorithm.\n", "versions": [{"version": "v1", "created": "Tue, 24 Dec 2013 14:19:43 GMT"}], "update_date": "2013-12-25", "authors_parsed": [["Farahat", "Ahmed K.", ""], ["Ghodsi", "Ali", ""], ["Kamel", "Mohamed S.", ""]]}, {"id": "1312.6838", "submitter": "Ahmed Farahat", "authors": "Ahmed K. Farahat, Ahmed Elgohary, Ali Ghodsi, Mohamed S. Kamel", "title": "Greedy Column Subset Selection for Large-scale Data Sets", "comments": "Under consideration for publication in Knowledge and Information\n  Systems", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In today's information systems, the availability of massive amounts of data\nnecessitates the development of fast and accurate algorithms to summarize these\ndata and represent them in a succinct format. One crucial problem in big data\nanalytics is the selection of representative instances from large and\nmassively-distributed data, which is formally known as the Column Subset\nSelection (CSS) problem. The solution to this problem enables data analysts to\nunderstand the insights of the data and explore its hidden structure. The\nselected instances can also be used for data preprocessing tasks such as\nlearning a low-dimensional embedding of the data points or computing a low-rank\napproximation of the corresponding matrix. This paper presents a fast and\naccurate greedy algorithm for large-scale column subset selection. The\nalgorithm minimizes an objective function which measures the reconstruction\nerror of the data matrix based on the subset of selected columns. The paper\nfirst presents a centralized greedy algorithm for column subset selection which\ndepends on a novel recursive formula for calculating the reconstruction error\nof the data matrix. The paper then presents a MapReduce algorithm which selects\na few representative columns from a matrix whose columns are massively\ndistributed across several commodity machines. The algorithm first learns a\nconcise representation of all columns using random projection, and it then\nsolves a generalized column subset selection problem at each machine in which a\nsubset of columns are selected from the sub-matrix on that machine such that\nthe reconstruction error of the concise representation is minimized. The paper\ndemonstrates the effectiveness and efficiency of the proposed algorithm through\nan empirical evaluation on benchmark data sets.\n", "versions": [{"version": "v1", "created": "Tue, 24 Dec 2013 15:10:23 GMT"}], "update_date": "2013-12-27", "authors_parsed": [["Farahat", "Ahmed K.", ""], ["Elgohary", "Ahmed", ""], ["Ghodsi", "Ali", ""], ["Kamel", "Mohamed S.", ""]]}, {"id": "1312.6849", "submitter": "Zoran Cvetkovic", "authors": "Matthew Ager and Zoran Cvetkovic and Peter Sollich", "title": "Speech Recognition Front End Without Information Loss", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Speech representation and modelling in high-dimensional spaces of acoustic\nwaveforms, or a linear transformation thereof, is investigated with the aim of\nimproving the robustness of automatic speech recognition to additive noise. The\nmotivation behind this approach is twofold: (i) the information in acoustic\nwaveforms that is usually removed in the process of extracting low-dimensional\nfeatures might aid robust recognition by virtue of structured redundancy\nanalogous to channel coding, (ii) linear feature domains allow for exact noise\nadaptation, as opposed to representations that involve non-linear processing\nwhich makes noise adaptation challenging. Thus, we develop a generative\nframework for phoneme modelling in high-dimensional linear feature domains, and\nuse it in phoneme classification and recognition tasks. Results show that\nclassification and recognition in this framework perform better than analogous\nPLP and MFCC classifiers below 18 dB SNR. A combination of the high-dimensional\nand MFCC features at the likelihood level performs uniformly better than either\nof the individual representations across all noise levels.\n", "versions": [{"version": "v1", "created": "Tue, 24 Dec 2013 16:36:16 GMT"}, {"version": "v2", "created": "Mon, 30 Mar 2015 09:17:46 GMT"}], "update_date": "2015-03-31", "authors_parsed": [["Ager", "Matthew", ""], ["Cvetkovic", "Zoran", ""], ["Sollich", "Peter", ""]]}, {"id": "1312.6872", "submitter": "Anupriya Gogna", "authors": "Anupriya Gogna, Ankita Shukla and Angshul Majumdar", "title": "Matrix recovery using Split Bregman", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NA cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we address the problem of recovering a matrix, with inherent\nlow rank structure, from its lower dimensional projections. This problem is\nfrequently encountered in wide range of areas including pattern recognition,\nwireless sensor networks, control systems, recommender systems, image/video\nreconstruction etc. Both in theory and practice, the most optimal way to solve\nthe low rank matrix recovery problem is via nuclear norm minimization. In this\npaper, we propose a Split Bregman algorithm for nuclear norm minimization. The\nuse of Bregman technique improves the convergence speed of our algorithm and\ngives a higher success rate. Also, the accuracy of reconstruction is much\nbetter even for cases where small number of linear measurements are available.\nOur claim is supported by empirical results obtained using our algorithm and\nits comparison to other existing methods for matrix recovery. The algorithms\nare compared on the basis of NMSE, execution time and success rate for varying\nranks and sampling ratios.\n", "versions": [{"version": "v1", "created": "Tue, 17 Dec 2013 15:12:48 GMT"}], "update_date": "2013-12-25", "authors_parsed": [["Gogna", "Anupriya", ""], ["Shukla", "Ankita", ""], ["Majumdar", "Angshul", ""]]}, {"id": "1312.6885", "submitter": "Brody Huval", "authors": "Brody Huval, Adam Coates, Andrew Ng", "title": "Deep learning for class-generic object detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate the use of deep neural networks for the novel task of class\ngeneric object detection. We show that neural networks originally designed for\nimage recognition can be trained to detect objects within images, regardless of\ntheir class, including objects for which no bounding box labels have been\nprovided. In addition, we show that bounding box labels yield a 1% performance\nincrease on the ImageNet recognition challenge.\n", "versions": [{"version": "v1", "created": "Tue, 24 Dec 2013 20:38:18 GMT"}], "update_date": "2013-12-25", "authors_parsed": [["Huval", "Brody", ""], ["Coates", "Adam", ""], ["Ng", "Andrew", ""]]}, {"id": "1312.6956", "submitter": "Faicel Chamroukhi", "authors": "Faicel Chamroukhi, Samer Mohammed, Dorra Trabelsi, Latifa Oukhellou,\n  Yacine Amirat", "title": "Joint segmentation of multivariate time series with hidden process\n  regression for human activity recognition", "comments": null, "journal-ref": "Neurocomputing, Volume 120, Pages 633-644, November 2013", "doi": "10.1016/j.neucom.2013.04.003", "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The problem of human activity recognition is central for understanding and\npredicting the human behavior, in particular in a prospective of assistive\nservices to humans, such as health monitoring, well being, security, etc. There\nis therefore a growing need to build accurate models which can take into\naccount the variability of the human activities over time (dynamic models)\nrather than static ones which can have some limitations in such a dynamic\ncontext. In this paper, the problem of activity recognition is analyzed through\nthe segmentation of the multidimensional time series of the acceleration data\nmeasured in the 3-d space using body-worn accelerometers. The proposed model\nfor automatic temporal segmentation is a specific statistical latent process\nmodel which assumes that the observed acceleration sequence is governed by\nsequence of hidden (unobserved) activities. More specifically, the proposed\napproach is based on a specific multiple regression model incorporating a\nhidden discrete logistic process which governs the switching from one activity\nto another over time. The model is learned in an unsupervised context by\nmaximizing the observed-data log-likelihood via a dedicated\nexpectation-maximization (EM) algorithm. We applied it on a real-world\nautomatic human activity recognition problem and its performance was assessed\nby performing comparisons with alternative approaches, including well-known\nsupervised static classifiers and the standard hidden Markov model (HMM). The\nobtained results are very encouraging and show that the proposed approach is\nquite competitive even it works in an entirely unsupervised way and does not\nrequires a feature extraction preprocessing step.\n", "versions": [{"version": "v1", "created": "Wed, 25 Dec 2013 11:08:32 GMT"}], "update_date": "2013-12-30", "authors_parsed": [["Chamroukhi", "Faicel", ""], ["Mohammed", "Samer", ""], ["Trabelsi", "Dorra", ""], ["Oukhellou", "Latifa", ""], ["Amirat", "Yacine", ""]]}, {"id": "1312.6962", "submitter": "Ahmad Kamal", "authors": "Ahmad Kamal", "title": "Subjectivity Classification using Machine Learning Techniques for Mining\n  Feature-Opinion Pairs from Web Opinion Sources", "comments": "10 pages, 2 Color Photographs, 1 Diagram, 14 Charts, 2 Graphs,\n  International Journal of Computer Science Issues (IJCSI), Vol. 10, Issue 5,\n  No 1, September 2013", "journal-ref": "International Journal of Computer Science Issues (IJCSI), Volume\n  10 Issue 5, 2013, pp 191-200", "doi": null, "report-no": null, "categories": "cs.IR cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Due to flourish of the Web 2.0, web opinion sources are rapidly emerging\ncontaining precious information useful for both customers and manufactures.\nRecently, feature based opinion mining techniques are gaining momentum in which\ncustomer reviews are processed automatically for mining product features and\nuser opinions expressed over them. However, customer reviews may contain both\nopinionated and factual sentences. Distillations of factual contents improve\nmining performance by preventing noisy and irrelevant extraction. In this\npaper, combination of both supervised machine learning and rule-based\napproaches are proposed for mining feasible feature-opinion pairs from\nsubjective review sentences. In the first phase of the proposed approach, a\nsupervised machine learning technique is applied for classifying subjective and\nobjective sentences from customer reviews. In the next phase, a rule based\nmethod is implemented which applies linguistic and semantic analysis of texts\nto mine feasible feature-opinion pairs from subjective sentences retained after\nthe first phase. The effectiveness of the proposed methods is established\nthrough experimentation over customer reviews on different electronic products.\n", "versions": [{"version": "v1", "created": "Wed, 25 Dec 2013 12:38:17 GMT"}], "update_date": "2015-04-27", "authors_parsed": [["Kamal", "Ahmad", ""]]}, {"id": "1312.6965", "submitter": "Faicel Chamroukhi", "authors": "Dorra Trabelsi, Samer Mohammed, Faicel Chamroukhi, Latifa Oukhellou,\n  Yacine Amirat", "title": "An Unsupervised Approach for Automatic Activity Recognition based on\n  Hidden Markov Model Regression", "comments": null, "journal-ref": "IEEE Transactions on Automation Science and Engineering, Volume:\n  10, Issue: 3, July 2013, Pages: 829-835", "doi": "10.1109/TASE.2013.2256349", "report-no": null, "categories": "stat.ML cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Using supervised machine learning approaches to recognize human activities\nfrom on-body wearable accelerometers generally requires a large amount of\nlabelled data. When ground truth information is not available, too expensive,\ntime consuming or difficult to collect, one has to rely on unsupervised\napproaches. This paper presents a new unsupervised approach for human activity\nrecognition from raw acceleration data measured using inertial wearable\nsensors. The proposed method is based upon joint segmentation of\nmultidimensional time series using a Hidden Markov Model (HMM) in a multiple\nregression context. The model is learned in an unsupervised framework using the\nExpectation-Maximization (EM) algorithm where no activity labels are needed.\nThe proposed method takes into account the sequential appearance of the data.\nIt is therefore adapted for the temporal acceleration data to accurately detect\nthe activities. It allows both segmentation and classification of the human\nactivities. Experimental results are provided to demonstrate the efficiency of\nthe proposed approach with respect to standard supervised and unsupervised\nclassification approaches\n", "versions": [{"version": "v1", "created": "Wed, 25 Dec 2013 13:03:12 GMT"}], "update_date": "2013-12-30", "authors_parsed": [["Trabelsi", "Dorra", ""], ["Mohammed", "Samer", ""], ["Chamroukhi", "Faicel", ""], ["Oukhellou", "Latifa", ""], ["Amirat", "Yacine", ""]]}, {"id": "1312.6966", "submitter": "Faicel Chamroukhi", "authors": "Faicel Chamroukhi, Herv\\'e Glotin, Allou Sam\\'e", "title": "Model-based functional mixture discriminant analysis with hidden process\n  regression for curve classification", "comments": null, "journal-ref": "Neurocomputing, Volume 112, Pages 153-163, July 2013", "doi": "10.1016/j.neucom.2012.10.030", "report-no": null, "categories": "stat.ME cs.LG math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we study the modeling and the classification of functional\ndata presenting regime changes over time. We propose a new model-based\nfunctional mixture discriminant analysis approach based on a specific hidden\nprocess regression model that governs the regime changes over time. Our\napproach is particularly adapted to handle the problem of complex-shaped\nclasses of curves, where each class is potentially composed of several\nsub-classes, and to deal with the regime changes within each homogeneous\nsub-class. The proposed model explicitly integrates the heterogeneity of each\nclass of curves via a mixture model formulation, and the regime changes within\neach sub-class through a hidden logistic process. Each class of complex-shaped\ncurves is modeled by a finite number of homogeneous clusters, each of them\nbeing decomposed into several regimes. The model parameters of each class are\nlearned by maximizing the observed-data log-likelihood by using a dedicated\nexpectation-maximization (EM) algorithm. Comparisons are performed with\nalternative curve classification approaches, including functional linear\ndiscriminant analysis and functional mixture discriminant analysis with\npolynomial regression mixtures and spline regression mixtures. Results obtained\non simulated data and real data show that the proposed approach outperforms the\nalternative approaches in terms of discrimination, and significantly improves\nthe curves approximation.\n", "versions": [{"version": "v1", "created": "Wed, 25 Dec 2013 13:08:47 GMT"}], "update_date": "2013-12-30", "authors_parsed": [["Chamroukhi", "Faicel", ""], ["Glotin", "Herv\u00e9", ""], ["Sam\u00e9", "Allou", ""]]}, {"id": "1312.6967", "submitter": "Faicel Chamroukhi", "authors": "Allou Sam\\'e, Faicel Chamroukhi, G\\'erard Govaert, Patrice Aknin", "title": "Model-based clustering and segmentation of time series with changes in\n  regime", "comments": null, "journal-ref": "Advances in Data Analysis and Classification, December 2011,\n  Volume 5, Issue 4, pp 301-321", "doi": "10.1007/s11634-011-0096-5", "report-no": null, "categories": "stat.ME cs.LG math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mixture model-based clustering, usually applied to multidimensional data, has\nbecome a popular approach in many data analysis problems, both for its good\nstatistical properties and for the simplicity of implementation of the\nExpectation-Maximization (EM) algorithm. Within the context of a railway\napplication, this paper introduces a novel mixture model for dealing with time\nseries that are subject to changes in regime. The proposed approach consists in\nmodeling each cluster by a regression model in which the polynomial\ncoefficients vary according to a discrete hidden process. In particular, this\napproach makes use of logistic functions to model the (smooth or abrupt)\ntransitions between regimes. The model parameters are estimated by the maximum\nlikelihood method solved by an Expectation-Maximization algorithm. The proposed\napproach can also be regarded as a clustering approach which operates by\nfinding groups of time series having common changes in regime. In addition to\nproviding a time series partition, it therefore provides a time series\nsegmentation. The problem of selecting the optimal numbers of clusters and\nsegments is solved by means of the Bayesian Information Criterion (BIC). The\nproposed approach is shown to be efficient using a variety of simulated time\nseries and real-world time series of electrical power consumption from rail\nswitching operations.\n", "versions": [{"version": "v1", "created": "Wed, 25 Dec 2013 13:11:04 GMT"}], "update_date": "2013-12-30", "authors_parsed": [["Sam\u00e9", "Allou", ""], ["Chamroukhi", "Faicel", ""], ["Govaert", "G\u00e9rard", ""], ["Aknin", "Patrice", ""]]}, {"id": "1312.6968", "submitter": "Faicel Chamroukhi", "authors": "Faicel Chamroukhi, Allou Sam\\'e, G\\'erard Govaert, Patrice Aknin", "title": "A hidden process regression model for functional data description.\n  Application to curve discrimination", "comments": null, "journal-ref": "Neurocomputing, Volume 73, Issues 7-9, March 2010, Pages 1210-1221", "doi": "10.1016/j.neucom.2009.12.023", "report-no": null, "categories": "stat.ME cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A new approach for functional data description is proposed in this paper. It\nconsists of a regression model with a discrete hidden logistic process which is\nadapted for modeling curves with abrupt or smooth regime changes. The model\nparameters are estimated in a maximum likelihood framework through a dedicated\nExpectation Maximization (EM) algorithm. From the proposed generative model, a\ncurve discrimination rule is derived using the Maximum A Posteriori rule. The\nproposed model is evaluated using simulated curves and real world curves\nacquired during railway switch operations, by performing comparisons with the\npiecewise regression approach in terms of curve modeling and classification.\n", "versions": [{"version": "v1", "created": "Wed, 25 Dec 2013 13:13:09 GMT"}], "update_date": "2013-12-30", "authors_parsed": [["Chamroukhi", "Faicel", ""], ["Sam\u00e9", "Allou", ""], ["Govaert", "G\u00e9rard", ""], ["Aknin", "Patrice", ""]]}, {"id": "1312.6969", "submitter": "Faicel Chamroukhi", "authors": "Faicel Chamroukhi, Allou Sam\\'e, G\\'erard Govaert, Patrice Aknin", "title": "Time series modeling by a regression approach based on a latent process", "comments": null, "journal-ref": "Neural Networks 22(5-6): 593-602 (2009)", "doi": "10.1016/j.neunet.2009.06.040", "report-no": null, "categories": "stat.ME cs.LG math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Time series are used in many domains including finance, engineering,\neconomics and bioinformatics generally to represent the change of a measurement\nover time. Modeling techniques may then be used to give a synthetic\nrepresentation of such data. A new approach for time series modeling is\nproposed in this paper. It consists of a regression model incorporating a\ndiscrete hidden logistic process allowing for activating smoothly or abruptly\ndifferent polynomial regression models. The model parameters are estimated by\nthe maximum likelihood method performed by a dedicated Expectation Maximization\n(EM) algorithm. The M step of the EM algorithm uses a multi-class Iterative\nReweighted Least-Squares (IRLS) algorithm to estimate the hidden process\nparameters. To evaluate the proposed approach, an experimental study on\nsimulated data and real world data was performed using two alternative\napproaches: a heteroskedastic piecewise regression model using a global\noptimization algorithm based on dynamic programming, and a Hidden Markov\nRegression Model whose parameters are estimated by the Baum-Welch algorithm.\nFinally, in the context of the remote monitoring of components of the French\nrailway infrastructure, and more particularly the switch mechanism, the\nproposed approach has been applied to modeling and classifying time series\nrepresenting the condition measurements acquired during switch operations.\n", "versions": [{"version": "v1", "created": "Wed, 25 Dec 2013 13:13:55 GMT"}], "update_date": "2013-12-30", "authors_parsed": [["Chamroukhi", "Faicel", ""], ["Sam\u00e9", "Allou", ""], ["Govaert", "G\u00e9rard", ""], ["Aknin", "Patrice", ""]]}, {"id": "1312.6974", "submitter": "Faicel Chamroukhi", "authors": "Faicel Chamroukhi", "title": "Piecewise regression mixture for simultaneous functional data clustering\n  and optimal segmentation", "comments": "preprint", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.LG math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces a novel mixture model-based approach for simultaneous\nclustering and optimal segmentation of functional data which are curves\npresenting regime changes. The proposed model consists in a finite mixture of\npiecewise polynomial regression models. Each piecewise polynomial regression\nmodel is associated with a cluster, and within each cluster, each piecewise\npolynomial component is associated with a regime (i.e., a segment). We derive\ntwo approaches for learning the model parameters. The former is an estimation\napproach and consists in maximizing the observed-data likelihood via a\ndedicated expectation-maximization (EM) algorithm. A fuzzy partition of the\ncurves in K clusters is then obtained at convergence by maximizing the\nposterior cluster probabilities. The latter however is a classification\napproach and optimizes a specific classification likelihood criterion through a\ndedicated classification expectation-maximization (CEM) algorithm. The optimal\ncurve segmentation is performed by using dynamic programming. In the\nclassification approach, both the curve clustering and the optimal segmentation\nare performed simultaneously as the CEM learning proceeds. We show that the\nclassification approach is the probabilistic version that generalizes the\ndeterministic K-means-like algorithm proposed in H\\'ebrail et al. (2010). The\nproposed approach is evaluated using simulated curves and real-world curves.\nComparisons with alternatives including regression mixture models and the\nK-means like algorithm for piecewise regression demonstrate the effectiveness\nof the proposed approach.\n", "versions": [{"version": "v1", "created": "Wed, 25 Dec 2013 13:54:05 GMT"}, {"version": "v2", "created": "Wed, 30 Apr 2014 23:23:20 GMT"}], "update_date": "2014-05-02", "authors_parsed": [["Chamroukhi", "Faicel", ""]]}, {"id": "1312.6978", "submitter": "Faicel Chamroukhi", "authors": "Faicel Chamroukhi, Allou Sam\\'e, G\\'erard Govaert, Patrice Aknin", "title": "Mod\\`ele \\`a processus latent et algorithme EM pour la r\\'egression non\n  lin\\'eaire", "comments": null, "journal-ref": "Revue des Nouvelles Technologies de l'Information (RNTI),\n  Statistique et nouvelles technologies de l'information (2011) 15-32", "doi": null, "report-no": null, "categories": "math.ST cs.LG stat.ME stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A non linear regression approach which consists of a specific regression\nmodel incorporating a latent process, allowing various polynomial regression\nmodels to be activated preferentially and smoothly, is introduced in this\npaper. The model parameters are estimated by maximum likelihood performed via a\ndedicated expecation-maximization (EM) algorithm. An experimental study using\nsimulated and real data sets reveals good performances of the proposed\napproach.\n", "versions": [{"version": "v1", "created": "Wed, 25 Dec 2013 14:21:48 GMT"}], "update_date": "2013-12-30", "authors_parsed": [["Chamroukhi", "Faicel", ""], ["Sam\u00e9", "Allou", ""], ["Govaert", "G\u00e9rard", ""], ["Aknin", "Patrice", ""]]}, {"id": "1312.6994", "submitter": "Faicel Chamroukhi", "authors": "Faicel Chamroukhi, Allou Sam\\'e, G\\'erard Govaert, Patrice Aknin", "title": "A regression model with a hidden logistic process for signal\n  parametrization", "comments": "In Proceedings of the XVIIth European Symposium on Artificial Neural\n  Networks, Computational Intelligence and Machine Learning (ESANN), Pages\n  503-508, 2009, Bruges, Belgium", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A new approach for signal parametrization, which consists of a specific\nregression model incorporating a discrete hidden logistic process, is proposed.\nThe model parameters are estimated by the maximum likelihood method performed\nby a dedicated Expectation Maximization (EM) algorithm. The parameters of the\nhidden logistic process, in the inner loop of the EM algorithm, are estimated\nusing a multi-class Iterative Reweighted Least-Squares (IRLS) algorithm. An\nexperimental study using simulated and real data reveals good performances of\nthe proposed approach.\n", "versions": [{"version": "v1", "created": "Wed, 25 Dec 2013 18:07:41 GMT"}], "update_date": "2013-12-30", "authors_parsed": [["Chamroukhi", "Faicel", ""], ["Sam\u00e9", "Allou", ""], ["Govaert", "G\u00e9rard", ""], ["Aknin", "Patrice", ""]]}, {"id": "1312.6995", "submitter": "Sourav Bhattacharya", "authors": "Sourav Bhattacharya and Petteri Nurmi and Nils Hammerla and Thomas\n  Pl\\\"otz", "title": "Towards Using Unlabeled Data in a Sparse-coding Framework for Human\n  Activity Recognition", "comments": "18 pages, 12 figures, Pervasive and Mobile Computing, 2014", "journal-ref": null, "doi": "10.1016/j.pmcj.2014.05.006", "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a sparse-coding framework for activity recognition in ubiquitous\nand mobile computing that alleviates two fundamental problems of current\nsupervised learning approaches. (i) It automatically derives a compact, sparse\nand meaningful feature representation of sensor data that does not rely on\nprior expert knowledge and generalizes extremely well across domain boundaries.\n(ii) It exploits unlabeled sample data for bootstrapping effective activity\nrecognizers, i.e., substantially reduces the amount of ground truth annotation\nrequired for model estimation. Such unlabeled data is trivial to obtain, e.g.,\nthrough contemporary smartphones carried by users as they go about their\neveryday activities.\n  Based on the self-taught learning paradigm we automatically derive an\nover-complete set of basis vectors from unlabeled data that captures inherent\npatterns present within activity data. Through projecting raw sensor data onto\nthe feature space defined by such over-complete sets of basis vectors effective\nfeature extraction is pursued. Given these learned feature representations,\nclassification backends are then trained using small amounts of labeled\ntraining data.\n  We study the new approach in detail using two datasets which differ in terms\nof the recognition tasks and sensor modalities. Primarily we focus on\ntransportation mode analysis task, a popular task in mobile-phone based\nsensing. The sparse-coding framework significantly outperforms the\nstate-of-the-art in supervised learning approaches. Furthermore, we demonstrate\nthe great practical potential of the new approach by successfully evaluating\nits generalization capabilities across both domain and sensor modalities by\nconsidering the popular Opportunity dataset. Our feature learning approach\noutperforms state-of-the-art approaches to analyzing activities in daily\nliving.\n", "versions": [{"version": "v1", "created": "Wed, 25 Dec 2013 18:08:44 GMT"}, {"version": "v2", "created": "Sat, 5 Jul 2014 10:32:32 GMT"}, {"version": "v3", "created": "Wed, 23 Jul 2014 13:39:53 GMT"}], "update_date": "2014-07-24", "authors_parsed": [["Bhattacharya", "Sourav", ""], ["Nurmi", "Petteri", ""], ["Hammerla", "Nils", ""], ["Pl\u00f6tz", "Thomas", ""]]}, {"id": "1312.7001", "submitter": "Faicel Chamroukhi", "authors": "Faicel Chamroukhi, Allou Sam\\'e, G\\'erard Govaert and Patrice Aknin", "title": "A regression model with a hidden logistic process for feature extraction\n  from time series", "comments": "In Proceedings of the International Joint Conference on Neural\n  Networks (IJCNN), 2009, Atlanta, USA", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.LG math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A new approach for feature extraction from time series is proposed in this\npaper. This approach consists of a specific regression model incorporating a\ndiscrete hidden logistic process. The model parameters are estimated by the\nmaximum likelihood method performed by a dedicated Expectation Maximization\n(EM) algorithm. The parameters of the hidden logistic process, in the inner\nloop of the EM algorithm, are estimated using a multi-class Iterative\nReweighted Least-Squares (IRLS) algorithm. A piecewise regression algorithm and\nits iterative variant have also been considered for comparisons. An\nexperimental study using simulated and real data reveals good performances of\nthe proposed approach.\n", "versions": [{"version": "v1", "created": "Wed, 25 Dec 2013 18:48:12 GMT"}], "update_date": "2013-12-30", "authors_parsed": [["Chamroukhi", "Faicel", ""], ["Sam\u00e9", "Allou", ""], ["Govaert", "G\u00e9rard", ""], ["Aknin", "Patrice", ""]]}, {"id": "1312.7003", "submitter": "Faicel Chamroukhi", "authors": "Ra\\\"issa Onanena, Faicel Chamroukhi, Latifa Oukhellou, Denis Candusso,\n  Patrice Aknin, Daniel Hissel", "title": "Supervised learning of a regression model based on latent process.\n  Application to the estimation of fuel cell life time", "comments": "In Proceeding of the 8th IEEE International Conference on Machine\n  Learning and Applications (IEEE ICMLA'09), pages 632-637, 2009, Miami Beach,\n  FL, USA", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper describes a pattern recognition approach aiming to estimate fuel\ncell duration time from electrochemical impedance spectroscopy measurements. It\nconsists in first extracting features from both real and imaginary parts of the\nimpedance spectrum. A parametric model is considered in the case of the real\npart, whereas regression model with latent variables is used in the latter\ncase. Then, a linear regression model using different subsets of extracted\nfeatures is used fo r the estimation of fuel cell time duration. The\nperformances of the proposed approach are evaluated on experimental data set to\nshow its feasibility. This could lead to interesting perspectives for\npredictive maintenance policy of fuel cell.\n", "versions": [{"version": "v1", "created": "Wed, 25 Dec 2013 18:55:59 GMT"}], "update_date": "2013-12-30", "authors_parsed": [["Onanena", "Ra\u00efssa", ""], ["Chamroukhi", "Faicel", ""], ["Oukhellou", "Latifa", ""], ["Candusso", "Denis", ""], ["Aknin", "Patrice", ""], ["Hissel", "Daniel", ""]]}, {"id": "1312.7006", "submitter": "Yudong Chen", "authors": "Yudong Chen, Xinyang Yi, Constantine Caramanis", "title": "A Convex Formulation for Mixed Regression with Two Components: Minimax\n  Optimal Rates", "comments": "Added results on minimax lower bounds, which match our upper bounds\n  on recovery errors up to log factors. Appeared in the Conference on Learning\n  Theory (COLT), 2014. (JMLR W&CP 35 :560-604, 2014)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.IT cs.LG math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the mixed regression problem with two components, under\nadversarial and stochastic noise. We give a convex optimization formulation\nthat provably recovers the true solution, and provide upper bounds on the\nrecovery errors for both arbitrary noise and stochastic noise settings. We also\ngive matching minimax lower bounds (up to log factors), showing that under\ncertain assumptions, our algorithm is information-theoretically optimal. Our\nresults represent the first tractable algorithm guaranteeing successful\nrecovery with tight bounds on recovery errors and sample complexity.\n", "versions": [{"version": "v1", "created": "Wed, 25 Dec 2013 19:23:22 GMT"}, {"version": "v2", "created": "Fri, 13 Feb 2015 10:04:51 GMT"}], "update_date": "2015-02-16", "authors_parsed": [["Chen", "Yudong", ""], ["Yi", "Xinyang", ""], ["Caramanis", "Constantine", ""]]}, {"id": "1312.7007", "submitter": "Faicel Chamroukhi", "authors": "Faicel Chamroukhi, Her\\'e Glotin, C\\'eline Rabouy", "title": "Functional Mixture Discriminant Analysis with hidden process regression\n  for curve classification", "comments": "In Proceedings of the XXth European Symposium on Artificial Neural\n  Networks, Computational Intelligence and Machine Learning (ESANN), Pages\n  281-286, 2012, Bruges, Belgium", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a new mixture model-based discriminant analysis approach for\nfunctional data using a specific hidden process regression model. The approach\nallows for fitting flexible curve-models to each class of complex-shaped curves\npresenting regime changes. The model parameters are learned by maximizing the\nobserved-data log-likelihood for each class by using a dedicated\nexpectation-maximization (EM) algorithm. Comparisons on simulated data with\nalternative approaches show that the proposed approach provides better results.\n", "versions": [{"version": "v1", "created": "Wed, 25 Dec 2013 19:23:39 GMT"}], "update_date": "2013-12-30", "authors_parsed": [["Chamroukhi", "Faicel", ""], ["Glotin", "Her\u00e9", ""], ["Rabouy", "C\u00e9line", ""]]}, {"id": "1312.7018", "submitter": "Faicel Chamroukhi", "authors": "Faicel Chamroukhi, Herv\\'e Glotin", "title": "Mixture model-based functional discriminant analysis for curve\n  classification", "comments": "In Proceedings of the 2012 International Joint Conference on Neural\n  Networks (IJCNN), 2012, Pages: 1-8, Brisbane, Australia", "journal-ref": null, "doi": "10.1109/IJCNN.2012.6252818", "report-no": null, "categories": "stat.ME cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Statistical approaches for Functional Data Analysis concern the paradigm for\nwhich the individuals are functions or curves rather than finite dimensional\nvectors. In this paper, we particularly focus on the modeling and the\nclassification of functional data which are temporal curves presenting regime\nchanges over time. More specifically, we propose a new mixture model-based\ndiscriminant analysis approach for functional data using a specific hidden\nprocess regression model. Our approach is particularly adapted to both handle\nthe problem of complex-shaped classes of curves, where each class is composed\nof several sub-classes, and to deal with the regime changes within each\nhomogeneous sub-class. The model explicitly integrates the heterogeneity of\neach class of curves via a mixture model formulation, and the regime changes\nwithin each sub-class through a hidden logistic process. The approach allows\ntherefore for fitting flexible curve-models to each class of complex-shaped\ncurves presenting regime changes through an unsupervised learning scheme, to\nautomatically summarize it into a finite number of homogeneous clusters, each\nof them is decomposed into several regimes. The model parameters are learned by\nmaximizing the observed-data log-likelihood for each class by using a dedicated\nexpectation-maximization (EM) algorithm. Comparisons on simulated data and real\ndata with alternative approaches, including functional linear discriminant\nanalysis and functional mixture discriminant analysis with polynomial\nregression mixtures and spline regression mixtures, show that the proposed\napproach provides better results regarding the discrimination results and\nsignificantly improves the curves approximation.\n", "versions": [{"version": "v1", "created": "Wed, 25 Dec 2013 20:35:20 GMT"}], "update_date": "2013-12-30", "authors_parsed": [["Chamroukhi", "Faicel", ""], ["Glotin", "Herv\u00e9", ""]]}, {"id": "1312.7022", "submitter": "Faicel Chamroukhi", "authors": "Faicel Chamroukhi", "title": "Robust EM algorithm for model-based curve clustering", "comments": "In Proceedings of the 2013 International Joint Conference on Neural\n  Networks (IJCNN), 2013, Dallas, TX, USA", "journal-ref": "In Proceedings of the 2013 International Joint Conference on\n  Neural Networks (IJCNN), 2013, Dallas, TX, USA", "doi": null, "report-no": null, "categories": "stat.ME cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Model-based clustering approaches concern the paradigm of exploratory data\nanalysis relying on the finite mixture model to automatically find a latent\nstructure governing observed data. They are one of the most popular and\nsuccessful approaches in cluster analysis. The mixture density estimation is\ngenerally performed by maximizing the observed-data log-likelihood by using the\nexpectation-maximization (EM) algorithm. However, it is well-known that the EM\nalgorithm initialization is crucial. In addition, the standard EM algorithm\nrequires the number of clusters to be known a priori. Some solutions have been\nprovided in [31, 12] for model-based clustering with Gaussian mixture models\nfor multivariate data. In this paper we focus on model-based curve clustering\napproaches, when the data are curves rather than vectorial data, based on\nregression mixtures. We propose a new robust EM algorithm for clustering\ncurves. We extend the model-based clustering approach presented in [31] for\nGaussian mixture models, to the case of curve clustering by regression\nmixtures, including polynomial regression mixtures as well as spline or\nB-spline regressions mixtures. Our approach both handles the problem of\ninitialization and the one of choosing the optimal number of clusters as the EM\nlearning proceeds, rather than in a two-fold scheme. This is achieved by\noptimizing a penalized log-likelihood criterion. A simulation study confirms\nthe potential benefit of the proposed algorithm in terms of robustness\nregarding initialization and funding the actual number of clusters.\n", "versions": [{"version": "v1", "created": "Wed, 25 Dec 2013 21:04:08 GMT"}], "update_date": "2014-04-29", "authors_parsed": [["Chamroukhi", "Faicel", ""]]}, {"id": "1312.7024", "submitter": "Faicel Chamroukhi", "authors": "Faicel Chamroukhi, Allou Sam\\'e, Patrice Aknin, G\\'erard Govaert", "title": "Model-based clustering with Hidden Markov Model regression for time\n  series with regime changes", "comments": "In Proceedings of the 2011 International Joint Conference on Neural\n  Networks (IJCNN), 2011, Pages 2814 - 2821, San Jose, California", "journal-ref": null, "doi": "10.1109/IJCNN.2011.6033590", "report-no": null, "categories": "stat.ML cs.LG stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces a novel model-based clustering approach for clustering\ntime series which present changes in regime. It consists of a mixture of\npolynomial regressions governed by hidden Markov chains. The underlying hidden\nprocess for each cluster activates successively several polynomial regimes\nduring time. The parameter estimation is performed by the maximum likelihood\nmethod through a dedicated Expectation-Maximization (EM) algorithm. The\nproposed approach is evaluated using simulated time series and real-world time\nseries issued from a railway diagnosis application. Comparisons with existing\napproaches for time series clustering, including the stand EM for Gaussian\nmixtures, $K$-means clustering, the standard mixture of regression models and\nmixture of Hidden Markov Models, demonstrate the effectiveness of the proposed\napproach.\n", "versions": [{"version": "v1", "created": "Wed, 25 Dec 2013 21:25:41 GMT"}], "update_date": "2013-12-30", "authors_parsed": [["Chamroukhi", "Faicel", ""], ["Sam\u00e9", "Allou", ""], ["Aknin", "Patrice", ""], ["Govaert", "G\u00e9rard", ""]]}, {"id": "1312.7077", "submitter": "Ankur Parikh", "authors": "Ankur P. Parikh, Avneesh Saluja, Chris Dyer, Eric P. Xing", "title": "Language Modeling with Power Low Rank Ensembles", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present power low rank ensembles (PLRE), a flexible framework for n-gram\nlanguage modeling where ensembles of low rank matrices and tensors are used to\nobtain smoothed probability estimates of words in context. Our method can be\nunderstood as a generalization of n-gram modeling to non-integer n, and\nincludes standard techniques such as absolute discounting and Kneser-Ney\nsmoothing as special cases. PLRE training is efficient and our approach\noutperforms state-of-the-art modified Kneser Ney baselines in terms of\nperplexity on large corpora as well as on BLEU score in a downstream machine\ntranslation task.\n", "versions": [{"version": "v1", "created": "Thu, 26 Dec 2013 09:45:02 GMT"}, {"version": "v2", "created": "Fri, 3 Oct 2014 08:28:03 GMT"}], "update_date": "2014-10-06", "authors_parsed": [["Parikh", "Ankur P.", ""], ["Saluja", "Avneesh", ""], ["Dyer", "Chris", ""], ["Xing", "Eric P.", ""]]}, {"id": "1312.7167", "submitter": "Abhishek Kumar", "authors": "Abhishek Kumar, Vikas Sindhwani", "title": "Near-separable Non-negative Matrix Factorization with $\\ell_1$- and\n  Bregman Loss Functions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, a family of tractable NMF algorithms have been proposed under the\nassumption that the data matrix satisfies a separability condition Donoho &\nStodden (2003); Arora et al. (2012). Geometrically, this condition reformulates\nthe NMF problem as that of finding the extreme rays of the conical hull of a\nfinite set of vectors. In this paper, we develop several extensions of the\nconical hull procedures of Kumar et al. (2013) for robust ($\\ell_1$)\napproximations and Bregman divergences. Our methods inherit all the advantages\nof Kumar et al. (2013) including scalability and noise-tolerance. We show that\non foreground-background separation problems in computer vision, robust\nnear-separable NMFs match the performance of Robust PCA, considered state of\nthe art on these problems, with an order of magnitude faster training time. We\nalso demonstrate applications in exemplar selection settings.\n", "versions": [{"version": "v1", "created": "Fri, 27 Dec 2013 01:10:00 GMT"}], "update_date": "2013-12-30", "authors_parsed": [["Kumar", "Abhishek", ""], ["Sindhwani", "Vikas", ""]]}, {"id": "1312.7179", "submitter": "Patoomsiri Songsiri Ms.", "authors": "Patoomsiri Songsiri, Thimaporn Phetkaew, Ryutaro Ichise and Boonserm\n  Kijsirikul", "title": "Sub-Classifier Construction for Error Correcting Output Code Using\n  Minimum Weight Perfect Matching", "comments": "7 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.IT math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multi-class classification is mandatory for real world problems and one of\npromising techniques for multi-class classification is Error Correcting Output\nCode. We propose a method for constructing the Error Correcting Output Code to\nobtain the suitable combination of positive and negative classes encoded to\nrepresent binary classifiers. The minimum weight perfect matching algorithm is\napplied to find the optimal pairs of subset of classes by using the\ngeneralization performance as a weighting criterion. Based on our method, each\nsubset of classes with positive and negative labels is appropriately combined\nfor learning the binary classifiers. Experimental results show that our\ntechnique gives significantly higher performance compared to traditional\nmethods including the dense random code and the sparse random code both in\nterms of accuracy and classification times. Moreover, our method requires\nsignificantly smaller number of binary classifiers while maintaining accuracy\ncompared to the One-Versus-One.\n", "versions": [{"version": "v1", "created": "Fri, 27 Dec 2013 03:21:34 GMT"}], "update_date": "2013-12-30", "authors_parsed": [["Songsiri", "Patoomsiri", ""], ["Phetkaew", "Thimaporn", ""], ["Ichise", "Ryutaro", ""], ["Kijsirikul", "Boonserm", ""]]}, {"id": "1312.7258", "submitter": "Leto Peel", "authors": "Leto Peel", "title": "Active Discovery of Network Roles for Predicting the Classes of Network\n  Nodes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.SI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Nodes in real world networks often have class labels, or underlying\nattributes, that are related to the way in which they connect to other nodes.\nSometimes this relationship is simple, for instance nodes of the same class are\nmay be more likely to be connected. In other cases, however, this is not true,\nand the way that nodes link in a network exhibits a different, more complex\nrelationship to their attributes. Here, we consider networks in which we know\nhow the nodes are connected, but we do not know the class labels of the nodes\nor how class labels relate to the network links. We wish to identify the best\nsubset of nodes to label in order to learn this relationship between node\nattributes and network links. We can then use this discovered relationship to\naccurately predict the class labels of the rest of the network nodes.\n  We present a model that identifies groups of nodes with similar link\npatterns, which we call network roles, using a generative blockmodel. The model\nthen predicts labels by learning the mapping from network roles to class labels\nusing a maximum margin classifier. We choose a subset of nodes to label\naccording to an iterative margin-based active learning strategy. By integrating\nthe discovery of network roles with the classifier optimisation, the active\nlearning process can adapt the network roles to better represent the network\nfor node classification. We demonstrate the model by exploring a selection of\nreal world networks, including a marine food web and a network of English\nwords. We show that, in contrast to other network classifiers, this model\nachieves good classification accuracy for a range of networks with different\nrelationships between class labels and network links.\n", "versions": [{"version": "v1", "created": "Fri, 27 Dec 2013 13:21:51 GMT"}, {"version": "v2", "created": "Tue, 18 Mar 2014 01:25:28 GMT"}], "update_date": "2014-03-19", "authors_parsed": [["Peel", "Leto", ""]]}, {"id": "1312.7292", "submitter": "Prashanth L.A.", "authors": "Prashanth L.A., Abhranil Chatterjee and Shalabh Bhatnagar", "title": "Two Timescale Convergent Q-learning for Sleep--Scheduling in Wireless\n  Sensor Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SY cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we consider an intrusion detection application for Wireless\nSensor Networks (WSNs). We study the problem of scheduling the sleep times of\nthe individual sensors to maximize the network lifetime while keeping the\ntracking error to a minimum. We formulate this problem as a\npartially-observable Markov decision process (POMDP) with continuous\nstate-action spaces, in a manner similar to (Fuemmeler and Veeravalli [2008]).\nHowever, unlike their formulation, we consider infinite horizon discounted and\naverage cost objectives as performance criteria. For each criterion, we propose\na convergent on-policy Q-learning algorithm that operates on two timescales,\nwhile employing function approximation to handle the curse of dimensionality\nassociated with the underlying POMDP. Our proposed algorithm incorporates a\npolicy gradient update using a one-simulation simultaneous perturbation\nstochastic approximation (SPSA) estimate on the faster timescale, while the\nQ-value parameter (arising from a linear function approximation for the\nQ-values) is updated in an on-policy temporal difference (TD) algorithm-like\nfashion on the slower timescale. The feature selection scheme employed in each\nof our algorithms manages the energy and tracking components in a manner that\nassists the search for the optimal sleep-scheduling policy. For the sake of\ncomparison, in both discounted and average settings, we also develop a function\napproximation analogue of the Q-learning algorithm. This algorithm, unlike the\ntwo-timescale variant, does not possess theoretical convergence guarantees.\nFinally, we also adapt our algorithms to include a stochastic iterative\nestimation scheme for the intruder's mobility model. Our simulation results on\na 2-dimensional network setting suggest that our algorithms result in better\ntracking accuracy at the cost of only a few additional sensors, in comparison\nto a recent prior work.\n", "versions": [{"version": "v1", "created": "Fri, 27 Dec 2013 16:13:07 GMT"}, {"version": "v2", "created": "Sun, 23 Mar 2014 13:44:48 GMT"}], "update_date": "2014-03-25", "authors_parsed": [["A.", "Prashanth L.", ""], ["Chatterjee", "Abhranil", ""], ["Bhatnagar", "Shalabh", ""]]}, {"id": "1312.7302", "submitter": "Arjun Jain", "authors": "Arjun Jain, Jonathan Tompson, Mykhaylo Andriluka, Graham W. Taylor,\n  Christoph Bregler", "title": "Learning Human Pose Estimation Features with Convolutional Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": "NYU-TR-2013-CS0999", "categories": "cs.CV cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces a new architecture for human pose estimation using a\nmulti- layer convolutional network architecture and a modified learning\ntechnique that learns low-level features and higher-level weak spatial models.\nUnconstrained human pose estimation is one of the hardest problems in computer\nvision, and our new architecture and learning schema shows significant\nimprovement over the current state-of-the-art results. The main contribution of\nthis paper is showing, for the first time, that a specific variation of deep\nlearning is able to outperform all existing traditional architectures on this\ntask. The paper also discusses several lessons learned while researching\nalternatives, most notably, that it is possible to learn strong low-level\nfeature detectors on features that might even just cover a few pixels in the\nimage. Higher-level spatial models improve somewhat the overall result, but to\na much lesser extent then expected. Many researchers previously argued that the\nkinematic structure and top-down information is crucial for this domain, but\nwith our purely bottom up, and weak spatial model, we could improve other more\ncomplicated architectures that currently produce the best results. This mirrors\nwhat many other researchers, like those in the speech recognition, object\nrecognition, and other domains have experienced.\n", "versions": [{"version": "v1", "created": "Fri, 27 Dec 2013 17:41:13 GMT"}, {"version": "v2", "created": "Mon, 30 Dec 2013 04:29:34 GMT"}, {"version": "v3", "created": "Fri, 3 Jan 2014 20:56:34 GMT"}, {"version": "v4", "created": "Tue, 18 Feb 2014 16:22:38 GMT"}, {"version": "v5", "created": "Tue, 25 Feb 2014 05:32:32 GMT"}, {"version": "v6", "created": "Wed, 23 Apr 2014 19:23:46 GMT"}], "update_date": "2014-04-24", "authors_parsed": [["Jain", "Arjun", ""], ["Tompson", "Jonathan", ""], ["Andriluka", "Mykhaylo", ""], ["Taylor", "Graham W.", ""], ["Bregler", "Christoph", ""]]}, {"id": "1312.7308", "submitter": "Kevin Jamieson", "authors": "Kevin Jamieson, Matthew Malloy, Robert Nowak, S\\'ebastien Bubeck", "title": "lil' UCB : An Optimal Exploration Algorithm for Multi-Armed Bandits", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The paper proposes a novel upper confidence bound (UCB) procedure for\nidentifying the arm with the largest mean in a multi-armed bandit game in the\nfixed confidence setting using a small number of total samples. The procedure\ncannot be improved in the sense that the number of samples required to identify\nthe best arm is within a constant factor of a lower bound based on the law of\nthe iterated logarithm (LIL). Inspired by the LIL, we construct our confidence\nbounds to explicitly account for the infinite time horizon of the algorithm. In\naddition, by using a novel stopping time for the algorithm we avoid a union\nbound over the arms that has been observed in other UCB-type algorithms. We\nprove that the algorithm is optimal up to constants and also show through\nsimulations that it provides superior performance with respect to the\nstate-of-the-art.\n", "versions": [{"version": "v1", "created": "Fri, 27 Dec 2013 18:20:09 GMT"}], "update_date": "2013-12-30", "authors_parsed": [["Jamieson", "Kevin", ""], ["Malloy", "Matthew", ""], ["Nowak", "Robert", ""], ["Bubeck", "S\u00e9bastien", ""]]}, {"id": "1312.7335", "submitter": "Balazs Kegl", "authors": "Bal\\'azs K\\'egl", "title": "Correlation-based construction of neighborhood and edge features", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motivated by an abstract notion of low-level edge detector filters, we\npropose a simple method of unsupervised feature construction based on pairwise\nstatistics of features. In the first step, we construct neighborhoods of\nfeatures by regrouping features that correlate. Then we use these subsets as\nfilters to produce new neighborhood features. Next, we connect neighborhood\nfeatures that correlate, and construct edge features by subtracting the\ncorrelated neighborhood features of each other. To validate the usefulness of\nthe constructed features, we ran AdaBoost.MH on four multi-class classification\nproblems. Our most significant result is a test error of 0.94% on MNIST with an\nalgorithm which is essentially free of any image-specific priors. On CIFAR-10\nour method is suboptimal compared to today's best deep learning techniques,\nnevertheless, we show that the proposed method outperforms not only boosting on\nthe raw pixels, but also boosting on Haar filters.\n", "versions": [{"version": "v1", "created": "Fri, 20 Dec 2013 19:36:51 GMT"}, {"version": "v2", "created": "Sun, 16 Feb 2014 23:17:39 GMT"}], "update_date": "2014-02-18", "authors_parsed": [["K\u00e9gl", "Bal\u00e1zs", ""]]}, {"id": "1312.7381", "submitter": "Luis  Sanchez Giraldo", "authors": "Luis G. Sanchez Giraldo and Jose C. Principe", "title": "Rate-Distortion Auto-Encoders", "comments": "Submission International Conference on Learning Representations 2014", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A rekindled the interest in auto-encoder algorithms has been spurred by\nrecent work on deep learning. Current efforts have been directed towards\neffective training of auto-encoder architectures with a large number of coding\nunits. Here, we propose a learning algorithm for auto-encoders based on a\nrate-distortion objective that minimizes the mutual information between the\ninputs and the outputs of the auto-encoder subject to a fidelity constraint.\nThe goal is to learn a representation that is minimally committed to the input\ndata, but that is rich enough to reconstruct the inputs up to certain level of\ndistortion. Minimizing the mutual information acts as a regularization term\nwhereas the fidelity constraint can be understood as a risk functional in the\nconventional statistical learning setting. The proposed algorithm uses a\nrecently introduced measure of entropy based on infinitely divisible matrices\nthat avoids the plug in estimation of densities. Experiments using\nover-complete bases show that the rate-distortion auto-encoders can learn a\nregularized input-output mapping in an implicit manner.\n", "versions": [{"version": "v1", "created": "Sat, 28 Dec 2013 02:08:53 GMT"}, {"version": "v2", "created": "Thu, 17 Apr 2014 03:30:02 GMT"}], "update_date": "2014-04-18", "authors_parsed": [["Giraldo", "Luis G. Sanchez", ""], ["Principe", "Jose C.", ""]]}, {"id": "1312.7463", "submitter": "Kartik Audhkhasi", "authors": "Kartik Audhkhasi, Abhinav Sethy, Bhuvana Ramabhadran and Shrikanth S.\n  Narayanan", "title": "Generalized Ambiguity Decomposition for Understanding Ensemble Diversity", "comments": "32 pages, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Diversity or complementarity of experts in ensemble pattern recognition and\ninformation processing systems is widely-observed by researchers to be crucial\nfor achieving performance improvement upon fusion. Understanding this link\nbetween ensemble diversity and fusion performance is thus an important research\nquestion. However, prior works have theoretically characterized ensemble\ndiversity and have linked it with ensemble performance in very restricted\nsettings. We present a generalized ambiguity decomposition (GAD) theorem as a\nbroad framework for answering these questions. The GAD theorem applies to a\ngeneric convex ensemble of experts for any arbitrary twice-differentiable loss\nfunction. It shows that the ensemble performance approximately decomposes into\na difference of the average expert performance and the diversity of the\nensemble. It thus provides a theoretical explanation for the\nempirically-observed benefit of fusing outputs from diverse classifiers and\nregressors. It also provides a loss function-dependent, ensemble-dependent, and\ndata-dependent definition of diversity. We present extensions of this\ndecomposition to common regression and classification loss functions, and\nreport a simulation-based analysis of the diversity term and the accuracy of\nthe decomposition. We finally present experiments on standard pattern\nrecognition data sets which indicate the accuracy of the decomposition for\nreal-world classification and regression problems.\n", "versions": [{"version": "v1", "created": "Sat, 28 Dec 2013 19:18:44 GMT"}], "update_date": "2013-12-31", "authors_parsed": [["Audhkhasi", "Kartik", ""], ["Sethy", "Abhinav", ""], ["Ramabhadran", "Bhuvana", ""], ["Narayanan", "Shrikanth S.", ""]]}, {"id": "1312.7567", "submitter": "Larry Wasserman", "authors": "Christopher Genovese, Marco Perone-Pacifico, Isabella Verdinelli and\n  Larry Wasserman", "title": "Nonparametric Inference For Density Modes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We derive nonparametric confidence intervals for the eigenvalues of the\nHessian at modes of a density estimate. This provides information about the\nstrength and shape of modes and can also be used as a significance test. We use\na data-splitting approach in which potential modes are identified using the\nfirst half of the data and inference is done with the second half of the data.\nTo get valid confidence sets for the eigenvalues, we use a bootstrap based on\nan elementary-symmetric-polynomial (ESP) transformation. This leads to valid\nbootstrap confidence sets regardless of any multiplicities in the eigenvalues.\nWe also suggest a new method for bandwidth selection, namely, choosing the\nbandwidth to maximize the number of significant modes. We show by example that\nthis method works well. Even when the true distribution is singular, and hence\ndoes not have a density, (in which case cross validation chooses a zero\nbandwidth), our method chooses a reasonable bandwidth.\n", "versions": [{"version": "v1", "created": "Sun, 29 Dec 2013 18:13:41 GMT"}], "update_date": "2013-12-31", "authors_parsed": [["Genovese", "Christopher", ""], ["Perone-Pacifico", "Marco", ""], ["Verdinelli", "Isabella", ""], ["Wasserman", "Larry", ""]]}, {"id": "1312.7606", "submitter": "Sergio Valcarcel Macua", "authors": "Sergio Valcarcel Macua, Jianshu Chen, Santiago Zazo, Ali H. Sayed", "title": "Distributed Policy Evaluation Under Multiple Behavior Strategies", "comments": "36 pages, 4 figures, accepted for publication on IEEE Transactions on\n  Automatic Control", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MA cs.AI cs.DC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We apply diffusion strategies to develop a fully-distributed cooperative\nreinforcement learning algorithm in which agents in a network communicate only\nwith their immediate neighbors to improve predictions about their environment.\nThe algorithm can also be applied to off-policy learning, meaning that the\nagents can predict the response to a behavior different from the actual\npolicies they are following. The proposed distributed strategy is efficient,\nwith linear complexity in both computation time and memory footprint. We\nprovide a mean-square-error performance analysis and establish convergence\nunder constant step-size updates, which endow the network with continuous\nlearning capabilities. The results show a clear gain from cooperation: when the\nindividual agents can estimate the solution, cooperation increases stability\nand reduces bias and variance of the prediction error; but, more importantly,\nthe network is able to approach the optimal solution even when none of the\nindividual agents can (e.g., when the individual behavior policies restrict\neach agent to sample a small portion of the state space).\n", "versions": [{"version": "v1", "created": "Mon, 30 Dec 2013 00:16:34 GMT"}, {"version": "v2", "created": "Wed, 5 Nov 2014 19:50:03 GMT"}], "update_date": "2014-11-06", "authors_parsed": [["Macua", "Sergio Valcarcel", ""], ["Chen", "Jianshu", ""], ["Zazo", "Santiago", ""], ["Sayed", "Ali H.", ""]]}, {"id": "1312.7651", "submitter": "Wei Dai", "authors": "Eric P. Xing, Qirong Ho, Wei Dai, Jin Kyu Kim, Jinliang Wei, Seunghak\n  Lee, Xun Zheng, Pengtao Xie, Abhimanu Kumar, Yaoliang Yu", "title": "Petuum: A New Platform for Distributed Machine Learning on Big Data", "comments": "15 pages, 10 figures, final version in KDD 2015 under the same title", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG cs.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  What is a systematic way to efficiently apply a wide spectrum of advanced ML\nprograms to industrial scale problems, using Big Models (up to 100s of billions\nof parameters) on Big Data (up to terabytes or petabytes)? Modern\nparallelization strategies employ fine-grained operations and scheduling beyond\nthe classic bulk-synchronous processing paradigm popularized by MapReduce, or\neven specialized graph-based execution that relies on graph representations of\nML programs. The variety of approaches tends to pull systems and algorithms\ndesign in different directions, and it remains difficult to find a universal\nplatform applicable to a wide range of ML programs at scale. We propose a\ngeneral-purpose framework that systematically addresses data- and\nmodel-parallel challenges in large-scale ML, by observing that many ML programs\nare fundamentally optimization-centric and admit error-tolerant,\niterative-convergent algorithmic solutions. This presents unique opportunities\nfor an integrative system design, such as bounded-error network synchronization\nand dynamic scheduling based on ML program structure. We demonstrate the\nefficacy of these system designs versus well-known implementations of modern ML\nalgorithms, allowing ML programs to run in much less time and at considerably\nlarger model sizes, even on modestly-sized compute clusters.\n", "versions": [{"version": "v1", "created": "Mon, 30 Dec 2013 08:46:01 GMT"}, {"version": "v2", "created": "Thu, 14 May 2015 21:44:39 GMT"}], "update_date": "2015-05-18", "authors_parsed": [["Xing", "Eric P.", ""], ["Ho", "Qirong", ""], ["Dai", "Wei", ""], ["Kim", "Jin Kyu", ""], ["Wei", "Jinliang", ""], ["Lee", "Seunghak", ""], ["Zheng", "Xun", ""], ["Xie", "Pengtao", ""], ["Kumar", "Abhimanu", ""], ["Yu", "Yaoliang", ""]]}, {"id": "1312.7658", "submitter": "Nahum Shimkin", "authors": "Andrey Bernstein and Nahum Shimkin", "title": "Response-Based Approachability and its Application to Generalized\n  No-Regret Algorithms", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.GT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Approachability theory, introduced by Blackwell (1956), provides fundamental\nresults on repeated games with vector-valued payoffs, and has been usefully\napplied since in the theory of learning in games and to learning algorithms in\nthe online adversarial setup. Given a repeated game with vector payoffs, a\ntarget set $S$ is approachable by a certain player (the agent) if he can ensure\nthat the average payoff vector converges to that set no matter what his\nadversary opponent does. Blackwell provided two equivalent sets of conditions\nfor a convex set to be approachable. The first (primary) condition is a\ngeometric separation condition, while the second (dual) condition requires that\nthe set be {\\em non-excludable}, namely that for every mixed action of the\nopponent there exists a mixed action of the agent (a {\\em response}) such that\nthe resulting payoff vector belongs to $S$. Existing approachability algorithms\nrely on the primal condition and essentially require to compute at each stage a\nprojection direction from a given point to $S$. In this paper, we introduce an\napproachability algorithm that relies on Blackwell's {\\em dual} condition.\nThus, rather than projection, the algorithm relies on computation of the\nresponse to a certain action of the opponent at each stage. The utility of the\nproposed algorithm is demonstrated by applying it to certain generalizations of\nthe classical regret minimization problem, which include regret minimization\nwith side constraints and regret minimization for global cost functions. In\nthese problems, computation of the required projections is generally complex\nbut a response is readily obtainable.\n", "versions": [{"version": "v1", "created": "Mon, 30 Dec 2013 09:15:03 GMT"}], "update_date": "2013-12-31", "authors_parsed": [["Bernstein", "Andrey", ""], ["Shimkin", "Nahum", ""]]}, {"id": "1312.7853", "submitter": "Ohad Shamir", "authors": "Ohad Shamir, Nathan Srebro, Tong Zhang", "title": "Communication Efficient Distributed Optimization using an Approximate\n  Newton-type Method", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a novel Newton-type method for distributed optimization, which is\nparticularly well suited for stochastic optimization and learning problems. For\nquadratic objectives, the method enjoys a linear rate of convergence which\nprovably \\emph{improves} with the data size, requiring an essentially constant\nnumber of iterations under reasonable assumptions. We provide theoretical and\nempirical evidence of the advantages of our method compared to other\napproaches, such as one-shot parameter averaging and ADMM.\n", "versions": [{"version": "v1", "created": "Mon, 30 Dec 2013 20:23:38 GMT"}, {"version": "v2", "created": "Wed, 1 Jan 2014 08:27:56 GMT"}, {"version": "v3", "created": "Sat, 1 Feb 2014 08:10:18 GMT"}, {"version": "v4", "created": "Tue, 13 May 2014 20:24:28 GMT"}], "update_date": "2014-05-15", "authors_parsed": [["Shamir", "Ohad", ""], ["Srebro", "Nathan", ""], ["Zhang", "Tong", ""]]}, {"id": "1312.7869", "submitter": "Jinliang Wei", "authors": "Jinliang Wei, Wei Dai, Abhimanu Kumar, Xun Zheng, Qirong Ho and Eric\n  P. Xing", "title": "Consistent Bounded-Asynchronous Parameter Servers for Distributed ML", "comments": "Corrected Title", "journal-ref": null, "doi": null, "report-no": "CMU-ML-13-115", "categories": "stat.ML cs.DC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In distributed ML applications, shared parameters are usually replicated\namong computing nodes to minimize network overhead. Therefore, proper\nconsistency model must be carefully chosen to ensure algorithm's correctness\nand provide high throughput. Existing consistency models used in\ngeneral-purpose databases and modern distributed ML systems are either too\nloose to guarantee correctness of the ML algorithms or too strict and thus fail\nto fully exploit the computing power of the underlying distributed system.\n  Many ML algorithms fall into the category of \\emph{iterative convergent\nalgorithms} which start from a randomly chosen initial point and converge to\noptima by repeating iteratively a set of procedures. We've found that many such\nalgorithms are to a bounded amount of inconsistency and still converge\ncorrectly. This property allows distributed ML to relax strict consistency\nmodels to improve system performance while theoretically guarantees algorithmic\ncorrectness. In this paper, we present several relaxed consistency models for\nasynchronous parallel computation and theoretically prove their algorithmic\ncorrectness. The proposed consistency models are implemented in a distributed\nparameter server and evaluated in the context of a popular ML application:\ntopic modeling.\n", "versions": [{"version": "v1", "created": "Mon, 30 Dec 2013 20:53:09 GMT"}, {"version": "v2", "created": "Tue, 31 Dec 2013 22:07:17 GMT"}], "update_date": "2014-01-03", "authors_parsed": [["Wei", "Jinliang", ""], ["Dai", "Wei", ""], ["Kumar", "Abhimanu", ""], ["Zheng", "Xun", ""], ["Ho", "Qirong", ""], ["Xing", "Eric P.", ""]]}]