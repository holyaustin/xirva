[{"id": "0911.0054", "submitter": "Ambuj Tewari", "authors": "Sham M. Kakade, Ohad Shamir, Karthik Sridharan, Ambuj Tewari", "title": "Learning Exponential Families in High-Dimensions: Strong Convexity and\n  Sparsity", "comments": "Errata added. Incorrect claim about cumulants of the Bernoulli\n  distribution fixed", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The versatility of exponential families, along with their attendant convexity\nproperties, make them a popular and effective statistical model. A central\nissue is learning these models in high-dimensions, such as when there is some\nsparsity pattern of the optimal parameter. This work characterizes a certain\nstrong convexity property of general exponential families, which allow their\ngeneralization ability to be quantified. In particular, we show how this\nproperty can be used to analyze generic exponential families under L_1\nregularization.\n", "versions": [{"version": "v1", "created": "Sat, 31 Oct 2009 02:56:18 GMT"}, {"version": "v2", "created": "Sat, 16 May 2015 22:45:35 GMT"}], "update_date": "2015-05-19", "authors_parsed": [["Kakade", "Sham M.", ""], ["Shamir", "Ohad", ""], ["Sridharan", "Karthik", ""], ["Tewari", "Ambuj", ""]]}, {"id": "0911.0225", "submitter": "Rdv Ijcsis", "authors": "Dasika Ratna Deepthi, K. Eswaran", "title": "A Mirroring Theorem and its Application to a New Method of Unsupervised\n  Hierarchical Pattern Classification", "comments": "10 pages IEEE format, International Journal of Computer Science and\n  Information Security, IJCSIS 2009, ISSN 1947 5500, Impact Factor 0.423,\n  http://sites.google.com/site/ijcsis/", "journal-ref": "International Journal of Computer Science and Information\n  Security, IJCSIS, Vol. 6, No. 1, pp. 016-025, October 2009, USA", "doi": null, "report-no": "ISSN 1947 5500", "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we prove a crucial theorem called Mirroring Theorem which\naffirms that given a collection of samples with enough information in it such\nthat it can be classified into classes and subclasses then (i) There exists a\nmapping which classifies and subclassifies these samples (ii) There exists a\nhierarchical classifier which can be constructed by using Mirroring Neural\nNetworks (MNNs) in combination with a clustering algorithm that can approximate\nthis mapping. Thus, the proof of the Mirroring theorem provides a theoretical\nbasis for the existence and a practical feasibility of constructing\nhierarchical classifiers, given the maps. Our proposed Mirroring Theorem can\nalso be considered as an extension to Kolmogrovs theorem in providing a\nrealistic solution for unsupervised classification. The techniques we develop,\nare general in nature and have led to the construction of learning machines\nwhich are (i) tree like in structure, (ii) modular (iii) with each module\nrunning on a common algorithm (tandem algorithm) and (iv) selfsupervised. We\nhave actually built the architecture, developed the tandem algorithm of such a\nhierarchical classifier and demonstrated it on an example problem.\n", "versions": [{"version": "v1", "created": "Mon, 2 Nov 2009 19:53:01 GMT"}], "update_date": "2009-11-03", "authors_parsed": [["Deepthi", "Dasika Ratna", ""], ["Eswaran", "K.", ""]]}, {"id": "0911.0460", "submitter": "Lester Mackey", "authors": "Joseph Sill, Gabor Takacs, Lester Mackey, David Lin", "title": "Feature-Weighted Linear Stacking", "comments": "17 pages, 1 figure, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Ensemble methods, such as stacking, are designed to boost predictive accuracy\nby blending the predictions of multiple machine learning models. Recent work\nhas shown that the use of meta-features, additional inputs describing each\nexample in a dataset, can boost the performance of ensemble methods, but the\ngreatest reported gains have come from nonlinear procedures requiring\nsignificant tuning and training time. Here, we present a linear technique,\nFeature-Weighted Linear Stacking (FWLS), that incorporates meta-features for\nimproved accuracy while retaining the well-known virtues of linear regression\nregarding speed, stability, and interpretability. FWLS combines model\npredictions linearly using coefficients that are themselves linear functions of\nmeta-features. This technique was a key facet of the solution of the second\nplace team in the recently concluded Netflix Prize competition. Significant\nincreases in accuracy over standard linear stacking are demonstrated on the\nNetflix Prize collaborative filtering dataset.\n", "versions": [{"version": "v1", "created": "Tue, 3 Nov 2009 08:17:05 GMT"}, {"version": "v2", "created": "Wed, 4 Nov 2009 08:55:28 GMT"}], "update_date": "2009-11-04", "authors_parsed": [["Sill", "Joseph", ""], ["Takacs", "Gabor", ""], ["Mackey", "Lester", ""], ["Lin", "David", ""]]}, {"id": "0911.0462", "submitter": "Marvin Weinstein", "authors": "Marvin Weinstein", "title": "Strange Bedfellows: Quantum Mechanics and Data Mining", "comments": "11 pages, 7 figures, Invited Talk at Light Cone 2009", "journal-ref": null, "doi": "10.1016/j.nuclphysbps.2010.02.009", "report-no": "SLAC-PUB-13832", "categories": "cs.LG physics.data-an quant-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Last year, in 2008, I gave a talk titled {\\it Quantum Calisthenics}. This\nyear I am going to tell you about how the work I described then has spun off\ninto a most unlikely direction. What I am going to talk about is how one maps\nthe problem of finding clusters in a given data set into a problem in quantum\nmechanics. I will then use the tricks I described to let quantum evolution lets\nthe clusters come together on their own.\n", "versions": [{"version": "v1", "created": "Tue, 3 Nov 2009 00:27:36 GMT"}], "update_date": "2015-05-14", "authors_parsed": [["Weinstein", "Marvin", ""]]}, {"id": "0911.0485", "submitter": "Rdv Ijcsis", "authors": "Tich Phuoc Tran, Longbing Cao, Dat Tran, Cuong Duc Nguyen", "title": "Novel Intrusion Detection using Probabilistic Neural Network and\n  Adaptive Boosting", "comments": "9 pages IEEE format, International Journal of Computer Science and\n  Information Security, IJCSIS 2009, ISSN 1947 5500, Impact Factor 0.423,\n  http://sites.google.com/site/ijcsis/", "journal-ref": "International Journal of Computer Science and Information\n  Security, IJCSIS, Vol. 6, No. 1, pp. 083-091, October 2009, USA", "doi": null, "report-no": "ISSN 1947 5500", "categories": "cs.NE cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article applies Machine Learning techniques to solve Intrusion Detection\nproblems within computer networks. Due to complex and dynamic nature of\ncomputer networks and hacking techniques, detecting malicious activities\nremains a challenging task for security experts, that is, currently available\ndefense systems suffer from low detection capability and high number of false\nalarms. To overcome such performance limitations, we propose a novel Machine\nLearning algorithm, namely Boosted Subspace Probabilistic Neural Network\n(BSPNN), which integrates an adaptive boosting technique and a semi parametric\nneural network to obtain good tradeoff between accuracy and generality. As the\nresult, learning bias and generalization variance can be significantly\nminimized. Substantial experiments on KDD 99 intrusion benchmark indicate that\nour model outperforms other state of the art learning algorithms, with\nsignificantly improved detection accuracy, minimal false alarms and relatively\nsmall computational complexity.\n", "versions": [{"version": "v1", "created": "Tue, 3 Nov 2009 04:07:19 GMT"}], "update_date": "2009-11-04", "authors_parsed": [["Tran", "Tich Phuoc", ""], ["Cao", "Longbing", ""], ["Tran", "Dat", ""], ["Nguyen", "Cuong Duc", ""]]}, {"id": "0911.0645", "submitter": "Ruriko Yoshida", "authors": "Peter Huggins, Wenbin Li, David Haws, Thomas Friedrich, Jinze Liu,\n  Ruriko Yoshida", "title": "Bayes estimators for phylogenetic reconstruction", "comments": "31 pages, 4 figures, and 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.PE cs.LG q-bio.QM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Tree reconstruction methods are often judged by their accuracy, measured by\nhow close they get to the true tree. Yet most reconstruction methods like ML do\nnot explicitly maximize this accuracy. To address this problem, we propose a\nBayesian solution. Given tree samples, we propose finding the tree estimate\nwhich is closest on average to the samples. This ``median'' tree is known as\nthe Bayes estimator (BE). The BE literally maximizes posterior expected\naccuracy, measured in terms of closeness (distance) to the true tree. We\ndiscuss a unified framework of BE trees, focusing especially on tree distances\nwhich are expressible as squared euclidean distances. Notable examples include\nRobinson--Foulds distance, quartet distance, and squared path difference. Using\nsimulated data, we show Bayes estimators can be efficiently computed in\npractice by hill climbing. We also show that Bayes estimators achieve higher\naccuracy, compared to maximum likelihood and neighbor joining.\n", "versions": [{"version": "v1", "created": "Tue, 3 Nov 2009 18:43:43 GMT"}, {"version": "v2", "created": "Sun, 22 Nov 2009 00:09:42 GMT"}], "update_date": "2009-11-22", "authors_parsed": [["Huggins", "Peter", ""], ["Li", "Wenbin", ""], ["Haws", "David", ""], ["Friedrich", "Thomas", ""], ["Liu", "Jinze", ""], ["Yoshida", "Ruriko", ""]]}, {"id": "0911.1174", "submitter": "Aleksandrs Slivkins", "authors": "Robert Kleinberg and Aleksandrs Slivkins", "title": "Sharp Dichotomies for Regret Minimization in Metric Spaces", "comments": "Full version of a paper in ACM-SIAM SODA 2010", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Lipschitz multi-armed bandit (MAB) problem generalizes the classical\nmulti-armed bandit problem by assuming one is given side information consisting\nof a priori upper bounds on the difference in expected payoff between certain\npairs of strategies. Classical results of (Lai and Robbins 1985) and (Auer et\nal. 2002) imply a logarithmic regret bound for the Lipschitz MAB problem on\nfinite metric spaces. Recent results on continuum-armed bandit problems and\ntheir generalizations imply lower bounds of $\\sqrt{t}$, or stronger, for many\ninfinite metric spaces such as the unit interval. Is this dichotomy universal?\nWe prove that the answer is yes: for every metric space, the optimal regret of\na Lipschitz MAB algorithm is either bounded above by any $f\\in \\omega(\\log t)$,\nor bounded below by any $g\\in o(\\sqrt{t})$. Perhaps surprisingly, this\ndichotomy does not coincide with the distinction between finite and infinite\nmetric spaces; instead it depends on whether the completion of the metric space\nis compact and countable. Our proof connects upper and lower bound techniques\nin online learning with classical topological notions such as perfect sets and\nthe Cantor-Bendixson theorem. Among many other results, we show a similar\ndichotomy for the \"full-feedback\" (a.k.a., \"best-expert\") version.\n", "versions": [{"version": "v1", "created": "Fri, 6 Nov 2009 03:52:56 GMT"}], "update_date": "2009-11-09", "authors_parsed": [["Kleinberg", "Robert", ""], ["Slivkins", "Aleksandrs", ""]]}, {"id": "0911.1386", "submitter": "Emanuel Diamant", "authors": "Emanuel Diamant", "title": "Machine Learning: When and Where the Horses Went Astray?", "comments": "The paper is accepted to be published in the Machine Learning serie\n  of the InTech", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Machine Learning is usually defined as a subfield of AI, which is busy with\ninformation extraction from raw data sets. Despite of its common acceptance and\nwidespread recognition, this definition is wrong and groundless. Meaningful\ninformation does not belong to the data that bear it. It belongs to the\nobservers of the data and it is a shared agreement and a convention among them.\nTherefore, this private information cannot be extracted from the data by any\nmeans. Therefore, all further attempts of Machine Learning apologists to\njustify their funny business are inappropriate.\n", "versions": [{"version": "v1", "created": "Sat, 7 Nov 2009 02:52:53 GMT"}], "update_date": "2009-11-10", "authors_parsed": [["Diamant", "Emanuel", ""]]}, {"id": "0911.1419", "submitter": "Yusuke Watanabe", "authors": "Yusuke Watanabe and Michael Chertkov", "title": "Belief Propagation and Loop Calculus for the Permanent of a Non-Negative\n  Matrix", "comments": "11 pages; submitted to Journal of Physics A: Mathematical Theoretical", "journal-ref": null, "doi": "10.1088/1751-8113/43/24/242002", "report-no": null, "categories": "cs.DS cond-mat.stat-mech cs.DM cs.LG cs.NA math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider computation of permanent of a positive $(N\\times N)$ non-negative\nmatrix, $P=(P_i^j|i,j=1,\\cdots,N)$, or equivalently the problem of weighted\ncounting of the perfect matchings over the complete bipartite graph $K_{N,N}$.\nThe problem is known to be of likely exponential complexity. Stated as the\npartition function $Z$ of a graphical model, the problem allows exact Loop\nCalculus representation [Chertkov, Chernyak '06] in terms of an interior\nminimum of the Bethe Free Energy functional over non-integer doubly stochastic\nmatrix of marginal beliefs, $\\beta=(\\beta_i^j|i,j=1,\\cdots,N)$, also\ncorrespondent to a fixed point of the iterative message-passing algorithm of\nthe Belief Propagation (BP) type. Our main result is an explicit expression of\nthe exact partition function (permanent) in terms of the matrix of BP\nmarginals, $\\beta$, as $Z=\\mbox{Perm}(P)=Z_{BP}\n\\mbox{Perm}(\\beta_i^j(1-\\beta_i^j))/\\prod_{i,j}(1-\\beta_i^j)$, where $Z_{BP}$\nis the BP expression for the permanent stated explicitly in terms if $\\beta$.\nWe give two derivations of the formula, a direct one based on the Bethe Free\nEnergy and an alternative one combining the Ihara graph-$\\zeta$ function and\nthe Loop Calculus approaches. Assuming that the matrix $\\beta$ of the Belief\nPropagation marginals is calculated, we provide two lower bounds and one\nupper-bound to estimate the multiplicative term. Two complementary lower bounds\nare based on the Gurvits-van der Waerden theorem and on a relation between the\nmodified permanent and determinant respectively.\n", "versions": [{"version": "v1", "created": "Sun, 8 Nov 2009 04:15:01 GMT"}, {"version": "v2", "created": "Sun, 2 May 2010 15:58:46 GMT"}], "update_date": "2015-05-14", "authors_parsed": [["Watanabe", "Yusuke", ""], ["Chertkov", "Michael", ""]]}, {"id": "0911.2381", "submitter": "Ferm\\'in Moscoso del Prado Mart\\'in", "authors": "Ferm\\'in Moscoso del Prado Mart\\'in", "title": "Analytical Determination of Fractal Structure in Stochastic Time Series", "comments": "9 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.data-an cond-mat.stat-mech cs.LG nlin.CD stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Current methods for determining whether a time series exhibits fractal\nstructure (FS) rely on subjective assessments on estimators of the Hurst\nexponent (H). Here, I introduce the Bayesian Assessment of Scaling, an\nanalytical framework for drawing objective and accurate inferences on the FS of\ntime series. The technique exploits the scaling property of the diffusion\nassociated to a time series. The resulting criterion is simple to compute and\nrepresents an accurate characterization of the evidence supporting different\nhypotheses on the scaling regime of a time series. Additionally, a closed-form\nMaximum Likelihood estimator of H is derived from the criterion, and this\nestimator outperforms the best available estimators.\n", "versions": [{"version": "v1", "created": "Thu, 12 Nov 2009 13:08:20 GMT"}], "update_date": "2009-11-13", "authors_parsed": [["Mart\u00edn", "Ferm\u00edn Moscoso del Prado", ""]]}, {"id": "0911.2904", "submitter": "Maxim Raginsky", "authors": "Maxim Raginsky, Rebecca Willett, Corinne Horn, Jorge Silva, Roummel\n  Marcia", "title": "Sequential anomaly detection in the presence of noise and limited\n  feedback", "comments": "19 pages, 12 pdf figures; final version to be published in IEEE\n  Transactions on Information Theory", "journal-ref": null, "doi": "10.1109/TIT.2012.2201375", "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper describes a methodology for detecting anomalies from sequentially\nobserved and potentially noisy data. The proposed approach consists of two main\nelements: (1) {\\em filtering}, or assigning a belief or likelihood to each\nsuccessive measurement based upon our ability to predict it from previous noisy\nobservations, and (2) {\\em hedging}, or flagging potential anomalies by\ncomparing the current belief against a time-varying and data-adaptive\nthreshold. The threshold is adjusted based on the available feedback from an\nend user. Our algorithms, which combine universal prediction with recent work\non online convex programming, do not require computing posterior distributions\ngiven all current observations and involve simple primal-dual parameter\nupdates. At the heart of the proposed approach lie exponential-family models\nwhich can be used in a wide variety of contexts and applications, and which\nyield methods that achieve sublinear per-round regret against both static and\nslowly varying product distributions with marginals drawn from the same\nexponential family. Moreover, the regret against static distributions coincides\nwith the minimax value of the corresponding online strongly convex game. We\nalso prove bounds on the number of mistakes made during the hedging step\nrelative to the best offline choice of the threshold with access to all\nestimated beliefs and feedback signals. We validate the theory on synthetic\ndata drawn from a time-varying distribution over binary vectors of high\ndimensionality, as well as on the Enron email dataset.\n", "versions": [{"version": "v1", "created": "Sun, 15 Nov 2009 18:43:10 GMT"}, {"version": "v2", "created": "Wed, 14 Jul 2010 17:30:25 GMT"}, {"version": "v3", "created": "Sun, 5 Feb 2012 23:11:54 GMT"}, {"version": "v4", "created": "Tue, 13 Mar 2012 16:11:21 GMT"}], "update_date": "2016-11-17", "authors_parsed": [["Raginsky", "Maxim", ""], ["Willett", "Rebecca", ""], ["Horn", "Corinne", ""], ["Silva", "Jorge", ""], ["Marcia", "Roummel", ""]]}, {"id": "0911.2974", "submitter": "Zizhuo Wang", "authors": "Shipra Agrawal, Zizhuo Wang, Yinyu Ye", "title": "A Dynamic Near-Optimal Algorithm for Online Linear Programming", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A natural optimization model that formulates many online resource allocation\nand revenue management problems is the online linear program (LP) in which the\nconstraint matrix is revealed column by column along with the corresponding\nobjective coefficient. In such a model, a decision variable has to be set each\ntime a column is revealed without observing the future inputs and the goal is\nto maximize the overall objective function. In this paper, we provide a\nnear-optimal algorithm for this general class of online problems under the\nassumption of random order of arrival and some mild conditions on the size of\nthe LP right-hand-side input. Specifically, our learning-based algorithm works\nby dynamically updating a threshold price vector at geometric time intervals,\nwhere the dual prices learned from the revealed columns in the previous period\nare used to determine the sequential decisions in the current period. Due to\nthe feature of dynamic learning, the competitiveness of our algorithm improves\nover the past study of the same problem. We also present a worst-case example\nshowing that the performance of our algorithm is near-optimal.\n", "versions": [{"version": "v1", "created": "Mon, 16 Nov 2009 16:39:33 GMT"}, {"version": "v2", "created": "Sat, 4 May 2013 20:10:14 GMT"}, {"version": "v3", "created": "Wed, 9 Apr 2014 03:44:37 GMT"}], "update_date": "2014-04-10", "authors_parsed": [["Agrawal", "Shipra", ""], ["Wang", "Zizhuo", ""], ["Ye", "Yinyu", ""]]}, {"id": "0911.3298", "submitter": "Alejandro Chinea Manrique De Lara", "authors": "Alejandro Chinea", "title": "Understanding the Principles of Recursive Neural networks: A Generative\n  Approach to Tackle Model Complexity", "comments": "11 pages, 4 figures, Recurrent Networks session ICANN 2009", "journal-ref": "Alippi, C., Polycarpou, M. Panayiotou, C., Ellinas, G. (Eds.)\n  ICANN 2009, LNCS, vol. 5768 pp. 952-963, Springer, Heidelberg (2009)", "doi": null, "report-no": null, "categories": "cs.NE cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recursive Neural Networks are non-linear adaptive models that are able to\nlearn deep structured information. However, these models have not yet been\nbroadly accepted. This fact is mainly due to its inherent complexity. In\nparticular, not only for being extremely complex information processing models,\nbut also because of a computational expensive learning phase. The most popular\ntraining method for these models is back-propagation through the structure.\nThis algorithm has been revealed not to be the most appropriate for structured\nprocessing due to problems of convergence, while more sophisticated training\nmethods enhance the speed of convergence at the expense of increasing\nsignificantly the computational cost. In this paper, we firstly perform an\nanalysis of the underlying principles behind these models aimed at\nunderstanding their computational power. Secondly, we propose an approximate\nsecond order stochastic learning algorithm. The proposed algorithm dynamically\nadapts the learning rate throughout the training phase of the network without\nincurring excessively expensive computational effort. The algorithm operates in\nboth on-line and batch modes. Furthermore, the resulting learning scheme is\nrobust against the vanishing gradients problem. The advantages of the proposed\nalgorithm are demonstrated with a real-world application example.\n", "versions": [{"version": "v1", "created": "Tue, 17 Nov 2009 13:17:05 GMT"}], "update_date": "2009-11-18", "authors_parsed": [["Chinea", "Alejandro", ""]]}, {"id": "0911.3304", "submitter": "Romain Giot", "authors": "Romain Giot (GREYC), Mohamad El-Abed (GREYC), Christophe Rosenberger\n  (GREYC)", "title": "Keystroke Dynamics Authentication For Collaborative Systems", "comments": null, "journal-ref": "The IEEE International Symposium on Collaborative Technologies and\n  Systems (CTS), Baltimore : United States (2009)", "doi": "10.1109/CTS.2009.5067478", "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present in this paper a study on the ability and the benefits of using a\nkeystroke dynamics authentication method for collaborative systems.\nAuthentication is a challenging issue in order to guarantee the security of use\nof collaborative systems during the access control step. Many solutions exist\nin the state of the art such as the use of one time passwords or smart-cards.\nWe focus in this paper on biometric based solutions that do not necessitate any\nadditional sensor. Keystroke dynamics is an interesting solution as it uses\nonly the keyboard and is invisible for users. Many methods have been published\nin this field. We make a comparative study of many of them considering the\noperational constraints of use for collaborative systems.\n", "versions": [{"version": "v1", "created": "Tue, 17 Nov 2009 13:35:40 GMT"}], "update_date": "2009-11-18", "authors_parsed": [["Giot", "Romain", "", "GREYC"], ["El-Abed", "Mohamad", "", "GREYC"], ["Rosenberger", "Christophe", "", "GREYC"]]}, {"id": "0911.3633", "submitter": "Benjamin Rubinstein", "authors": "Benjamin I. P. Rubinstein and J. Hyam Rubinstein", "title": "A Geometric Approach to Sample Compression", "comments": "37 pages, 18 figures, submitted to the Journal of Machine Learning\n  Research", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.CO math.GT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Sample Compression Conjecture of Littlestone & Warmuth has remained\nunsolved for over two decades. This paper presents a systematic geometric\ninvestigation of the compression of finite maximum concept classes. Simple\narrangements of hyperplanes in Hyperbolic space, and Piecewise-Linear\nhyperplane arrangements, are shown to represent maximum classes, generalizing\nthe corresponding Euclidean result. A main result is that PL arrangements can\nbe swept by a moving hyperplane to unlabeled d-compress any finite maximum\nclass, forming a peeling scheme as conjectured by Kuzmin & Warmuth. A corollary\nis that some d-maximal classes cannot be embedded into any maximum class of VC\ndimension d+k, for any constant k. The construction of the PL sweeping involves\nPachner moves on the one-inclusion graph, corresponding to moves of a\nhyperplane across the intersection of d other hyperplanes. This extends the\nwell known Pachner moves for triangulations to cubical complexes.\n", "versions": [{"version": "v1", "created": "Wed, 18 Nov 2009 19:22:09 GMT"}], "update_date": "2014-02-04", "authors_parsed": [["Rubinstein", "Benjamin I. P.", ""], ["Rubinstein", "J. Hyam", ""]]}, {"id": "0911.3944", "submitter": "Patrick J. Wolfe", "authors": "Christopher M. White, Sanjeev P. Khudanpur, and Patrick J. Wolfe", "title": "Likelihood-based semi-supervised model selection with applications to\n  speech processing", "comments": "11 pages, 2 figures; submitted for publication", "journal-ref": "IEEE Journal of Selected Topics in Signal Processing, vol. 4, pp.\n  1016-1026, 2010", "doi": "10.1109/JSTSP.2010.2076050", "report-no": null, "categories": "stat.ML cs.CL cs.LG stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In conventional supervised pattern recognition tasks, model selection is\ntypically accomplished by minimizing the classification error rate on a set of\nso-called development data, subject to ground-truth labeling by human experts\nor some other means. In the context of speech processing systems and other\nlarge-scale practical applications, however, such labeled development data are\ntypically costly and difficult to obtain. This article proposes an alternative\nsemi-supervised framework for likelihood-based model selection that leverages\nunlabeled data by using trained classifiers representing each model to\nautomatically generate putative labels. The errors that result from this\nautomatic labeling are shown to be amenable to results from robust statistics,\nwhich in turn provide for minimax-optimal censored likelihood ratio tests that\nrecover the nonparametric sign test as a limiting case. This approach is then\nvalidated experimentally using a state-of-the-art automatic speech recognition\nsystem to select between candidate word pronunciations using unlabeled speech\ndata that only potentially contain instances of the words under test. Results\nprovide supporting evidence for the utility of this approach, and suggest that\nit may also find use in other applications of machine learning.\n", "versions": [{"version": "v1", "created": "Fri, 20 Nov 2009 01:30:36 GMT"}], "update_date": "2011-08-25", "authors_parsed": [["White", "Christopher M.", ""], ["Khudanpur", "Sanjeev P.", ""], ["Wolfe", "Patrick J.", ""]]}, {"id": "0911.4046", "submitter": "Ryota Tomioka", "authors": "Ryota Tomioka, Taiji Suzuki, Masashi Sugiyama", "title": "Super-Linear Convergence of Dual Augmented-Lagrangian Algorithm for\n  Sparsity Regularized Estimation", "comments": "51 pages, 9 figures", "journal-ref": "Journal of Machine Learning Research, 12(May):1537-1586, 2011", "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We analyze the convergence behaviour of a recently proposed algorithm for\nregularized estimation called Dual Augmented Lagrangian (DAL). Our analysis is\nbased on a new interpretation of DAL as a proximal minimization algorithm. We\ntheoretically show under some conditions that DAL converges super-linearly in a\nnon-asymptotic and global sense. Due to a special modelling of sparse\nestimation problems in the context of machine learning, the assumptions we make\nare milder and more natural than those made in conventional analysis of\naugmented Lagrangian algorithms. In addition, the new interpretation enables us\nto generalize DAL to wide varieties of sparse estimation problems. We\nexperimentally confirm our analysis in a large scale $\\ell_1$-regularized\nlogistic regression problem and extensively compare the efficiency of DAL\nalgorithm to previously proposed algorithms on both synthetic and benchmark\ndatasets.\n", "versions": [{"version": "v1", "created": "Fri, 20 Nov 2009 13:44:28 GMT"}, {"version": "v2", "created": "Wed, 12 May 2010 12:33:07 GMT"}, {"version": "v3", "created": "Sun, 2 Jan 2011 07:04:21 GMT"}], "update_date": "2011-06-07", "authors_parsed": [["Tomioka", "Ryota", ""], ["Suzuki", "Taiji", ""], ["Sugiyama", "Masashi", ""]]}, {"id": "0911.4262", "submitter": "Iza Marfisi-Schottman", "authors": "Iza Marfisi-Schottman (LIESP), Aymen Sghaier (LIESP), S\\'ebastien\n  George (LIESP), Franck Tarpin-Bernard (LIESP), Patrick Pr\\'ev\\^ot (LIESP)", "title": "Towards Industrialized Conception and Production of Serious Games", "comments": null, "journal-ref": "ICTE INTERNATIONAL CONFERENCE ON TECHNOLOGY AND EDUCATION, Paris :\n  France (2009)", "doi": null, "report-no": null, "categories": "cs.LG cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Serious Games (SGs) have experienced a tremendous outburst these last years.\nVideo game companies have been producing fun, user-friendly SGs, but their\neducational value has yet to be proven. Meanwhile, cognition research scientist\nhave been developing SGs in such a way as to guarantee an educational gain, but\nthe fun and attractive characteristics featured often would not meet the\npublic's expectations. The ideal SG must combine these two aspects while still\nbeing economically viable. In this article, we propose a production chain model\nto efficiently conceive and produce SGs that are certified for their\neducational gain and fun qualities. Each step of this chain will be described\nalong with the human actors, the tools and the documents that intervene.\n", "versions": [{"version": "v1", "created": "Sun, 22 Nov 2009 16:01:09 GMT"}], "update_date": "2009-11-24", "authors_parsed": [["Marfisi-Schottman", "Iza", "", "LIESP"], ["Sghaier", "Aymen", "", "LIESP"], ["George", "S\u00e9bastien", "", "LIESP"], ["Tarpin-Bernard", "Franck", "", "LIESP"], ["Pr\u00e9v\u00f4t", "Patrick", "", "LIESP"]]}, {"id": "0911.4863", "submitter": "Frank Nielsen", "authors": "Frank Nielsen and Vincent Garcia", "title": "Statistical exponential families: A digest with flash cards", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This document describes concisely the ubiquitous class of exponential family\ndistributions met in statistics. The first part recalls definitions and\nsummarizes main properties and duality with Bregman divergences (all proofs are\nskipped). The second part lists decompositions and related formula of common\nexponential family distributions. We recall the Fisher-Rao-Riemannian\ngeometries and the dual affine connection information geometries of statistical\nmanifolds. It is intended to maintain and update this document and catalog by\nadding new distribution items.\n", "versions": [{"version": "v1", "created": "Wed, 25 Nov 2009 14:26:54 GMT"}, {"version": "v2", "created": "Fri, 13 May 2011 01:52:49 GMT"}], "update_date": "2011-05-16", "authors_parsed": [["Nielsen", "Frank", ""], ["Garcia", "Vincent", ""]]}, {"id": "0911.5104", "submitter": "Pedro Alejandro Ortega", "authors": "Pedro A. Ortega, Daniel A. Braun", "title": "A Bayesian Rule for Adaptive Control based on Causal Interventions", "comments": "AGI-2010. 6 pages, 2 figures", "journal-ref": "AGI-2010", "doi": null, "report-no": null, "categories": "cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Explaining adaptive behavior is a central problem in artificial intelligence\nresearch. Here we formalize adaptive agents as mixture distributions over\nsequences of inputs and outputs (I/O). Each distribution of the mixture\nconstitutes a `possible world', but the agent does not know which of the\npossible worlds it is actually facing. The problem is to adapt the I/O stream\nin a way that is compatible with the true world. A natural measure of\nadaptation can be obtained by the Kullback-Leibler (KL) divergence between the\nI/O distribution of the true world and the I/O distribution expected by the\nagent that is uncertain about possible worlds. In the case of pure input\nstreams, the Bayesian mixture provides a well-known solution for this problem.\nWe show, however, that in the case of I/O streams this solution breaks down,\nbecause outputs are issued by the agent itself and require a different\nprobabilistic syntax as provided by intervention calculus. Based on this\ncalculus, we obtain a Bayesian control rule that allows modeling adaptive\nbehavior with mixture distributions over I/O streams. This rule might allow for\na novel approach to adaptive control based on a minimum KL-principle.\n", "versions": [{"version": "v1", "created": "Thu, 26 Nov 2009 15:52:33 GMT"}, {"version": "v2", "created": "Wed, 30 Dec 2009 23:34:14 GMT"}], "update_date": "2009-12-31", "authors_parsed": [["Ortega", "Pedro A.", ""], ["Braun", "Daniel A.", ""]]}, {"id": "0911.5372", "submitter": "Srinivas Turaga", "authors": "Srinivas C. Turaga, Kevin L. Briggman, Moritz Helmstaedter, Winfried\n  Denk, H. Sebastian Seung", "title": "Maximin affinity learning of image segmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Images can be segmented by first using a classifier to predict an affinity\ngraph that reflects the degree to which image pixels must be grouped together\nand then partitioning the graph to yield a segmentation. Machine learning has\nbeen applied to the affinity classifier to produce affinity graphs that are\ngood in the sense of minimizing edge misclassification rates. However, this\nerror measure is only indirectly related to the quality of segmentations\nproduced by ultimately partitioning the affinity graph. We present the first\nmachine learning algorithm for training a classifier to produce affinity graphs\nthat are good in the sense of producing segmentations that directly minimize\nthe Rand index, a well known segmentation performance measure. The Rand index\nmeasures segmentation performance by quantifying the classification of the\nconnectivity of image pixel pairs after segmentation. By using the simple graph\npartitioning algorithm of finding the connected components of the thresholded\naffinity graph, we are able to train an affinity classifier to directly\nminimize the Rand index of segmentations resulting from the graph partitioning.\nOur learning algorithm corresponds to the learning of maximin affinities\nbetween image pixel pairs, which are predictive of the pixel-pair connectivity.\n", "versions": [{"version": "v1", "created": "Sat, 28 Nov 2009 04:58:38 GMT"}], "update_date": "2009-12-01", "authors_parsed": [["Turaga", "Srinivas C.", ""], ["Briggman", "Kevin L.", ""], ["Helmstaedter", "Moritz", ""], ["Denk", "Winfried", ""], ["Seung", "H. Sebastian", ""]]}, {"id": "0911.5703", "submitter": "Stevan Harnad", "authors": "Olivier Picard, Alexandre Blondin-Masse, Stevan Harnad, Odile\n  Marcotte, Guillaume Chicoisne and Yassine Gargouri", "title": "Hierarchies in Dictionary Definition Space", "comments": "9 pages, 5 figues, 2 tables, 12 references, 23rd Annual Conference on\n  Neural Information Processing Systems (NIPS): Workshop on Analyzing Networks\n  and Learning With Graphs\n  http://nips.cc/Conferences/2009/Program/event.php?ID=1504", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A dictionary defines words in terms of other words. Definitions can tell you\nthe meanings of words you don't know, but only if you know the meanings of the\ndefining words. How many words do you need to know (and which ones) in order to\nbe able to learn all the rest from definitions? We reduced dictionaries to\ntheir \"grounding kernels\" (GKs), about 10% of the dictionary, from which all\nthe other words could be defined. The GK words turned out to have\npsycholinguistic correlates: they were learned at an earlier age and more\nconcrete than the rest of the dictionary. But one can compress still more: the\nGK turns out to have internal structure, with a strongly connected \"kernel\ncore\" (KC) and a surrounding layer, from which a hierarchy of definitional\ndistances can be derived, all the way out to the periphery of the full\ndictionary. These definitional distances, too, are correlated with\npsycholinguistic variables (age of acquisition, concreteness, imageability,\noral and written frequency) and hence perhaps with the \"mental lexicon\" in each\nof our heads.\n", "versions": [{"version": "v1", "created": "Mon, 30 Nov 2009 18:15:35 GMT"}], "update_date": "2009-12-01", "authors_parsed": [["Picard", "Olivier", ""], ["Blondin-Masse", "Alexandre", ""], ["Harnad", "Stevan", ""], ["Marcotte", "Odile", ""], ["Chicoisne", "Guillaume", ""], ["Gargouri", "Yassine", ""]]}, {"id": "0911.5708", "submitter": "Benjamin Rubinstein", "authors": "Benjamin I. P. Rubinstein, Peter L. Bartlett, Ling Huang, Nina Taft", "title": "Learning in a Large Function Space: Privacy-Preserving Mechanisms for\n  SVM Learning", "comments": "21 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Several recent studies in privacy-preserving learning have considered the\ntrade-off between utility or risk and the level of differential privacy\nguaranteed by mechanisms for statistical query processing. In this paper we\nstudy this trade-off in private Support Vector Machine (SVM) learning. We\npresent two efficient mechanisms, one for the case of finite-dimensional\nfeature mappings and one for potentially infinite-dimensional feature mappings\nwith translation-invariant kernels. For the case of translation-invariant\nkernels, the proposed mechanism minimizes regularized empirical risk in a\nrandom Reproducing Kernel Hilbert Space whose kernel uniformly approximates the\ndesired kernel with high probability. This technique, borrowed from large-scale\nlearning, allows the mechanism to respond with a finite encoding of the\nclassifier, even when the function class is of infinite VC dimension.\nDifferential privacy is established using a proof technique from algorithmic\nstability. Utility--the mechanism's response function is pointwise\nepsilon-close to non-private SVM with probability 1-delta--is proven by\nappealing to the smoothness of regularized empirical risk minimization with\nrespect to small perturbations to the feature mapping. We conclude with a lower\nbound on the optimal differential privacy of the SVM. This negative result\nstates that for any delta, no mechanism can be simultaneously\n(epsilon,delta)-useful and beta-differentially private for small epsilon and\nsmall beta.\n", "versions": [{"version": "v1", "created": "Mon, 30 Nov 2009 20:34:45 GMT"}], "update_date": "2009-12-01", "authors_parsed": [["Rubinstein", "Benjamin I. P.", ""], ["Bartlett", "Peter L.", ""], ["Huang", "Ling", ""], ["Taft", "Nina", ""]]}]