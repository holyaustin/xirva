[{"id": "1303.0073", "submitter": "Uri Kartoun", "authors": "Uri Kartoun", "title": "A Method for Comparing Hedge Funds", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.ST cs.IR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The paper presents new machine learning methods: signal composition, which\nclassifies time-series regardless of length, type, and quantity; and\nself-labeling, a supervised-learning enhancement. The paper describes further\nthe implementation of the methods on a financial search engine system to\nidentify behavioral similarities among time-series representing monthly returns\nof 11,312 hedge funds operated during approximately one decade (2000 - 2010).\nThe presented approach of cross-category and cross-location classification\nassists the investor to identify alternative investments.\n", "versions": [{"version": "v1", "created": "Fri, 1 Mar 2013 03:38:35 GMT"}, {"version": "v2", "created": "Tue, 19 Mar 2013 21:20:08 GMT"}], "update_date": "2013-05-14", "authors_parsed": [["Kartoun", "Uri", ""]]}, {"id": "1303.0076", "submitter": "Uri Kartoun", "authors": "Uri Kartoun", "title": "Bio-Signals-based Situation Comparison Approach to Predict Pain", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper describes a time-series-based classification approach to identify\nsimilarities between bio-medical-based situations. The proposed approach allows\nclassifying collections of time-series representing bio-medical measurements,\ni.e., situations, regardless of the type, the length and the quantity of the\ntime-series a situation comprised of.\n", "versions": [{"version": "v1", "created": "Fri, 1 Mar 2013 03:49:11 GMT"}, {"version": "v2", "created": "Tue, 19 Mar 2013 21:19:06 GMT"}], "update_date": "2013-05-14", "authors_parsed": [["Kartoun", "Uri", ""]]}, {"id": "1303.0095", "submitter": "Tomasz Kajdanowicz", "authors": "Tomasz Kajdanowicz, Przemyslaw Kazienko, Piotr Doskocz", "title": "Label-dependent Feature Extraction in Social Networks for Node\n  Classification", "comments": "feature extraction, label-dependent features, classification, social\n  network analysis, AMD social network", "journal-ref": "Kajdanowicz T., Kazienko P., Doskocz P.: Label-dependent Feature\n  Extraction in Social Networks for Node Classification. Lecture Notes in\n  Artificial Intelligence LNAI 6430, Springer, 2010, pp. 89-102", "doi": "10.1007/978-3-642-16567-2_7", "report-no": null, "categories": "cs.SI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A new method of feature extraction in the social network for within-network\nclassification is proposed in the paper. The method provides new features\ncalculated by combination of both: network structure information and class\nlabels assigned to nodes. The influence of various features on classification\nperformance has also been studied. The experiments on real-world data have\nshown that features created owing to the proposed method can lead to\nsignificant improvement of classification accuracy.\n", "versions": [{"version": "v1", "created": "Fri, 1 Mar 2013 06:31:02 GMT"}], "update_date": "2013-03-04", "authors_parsed": [["Kajdanowicz", "Tomasz", ""], ["Kazienko", "Przemyslaw", ""], ["Doskocz", "Piotr", ""]]}, {"id": "1303.0140", "submitter": "Edward Moroshko", "authors": "Nina Vaits, Edward Moroshko, Koby Crammer", "title": "Second-Order Non-Stationary Online Learning for Regression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The goal of a learner, in standard online learning, is to have the cumulative\nloss not much larger compared with the best-performing function from some fixed\nclass. Numerous algorithms were shown to have this gap arbitrarily close to\nzero, compared with the best function that is chosen off-line. Nevertheless,\nmany real-world applications, such as adaptive filtering, are non-stationary in\nnature, and the best prediction function may drift over time. We introduce two\nnovel algorithms for online regression, designed to work well in non-stationary\nenvironment. Our first algorithm performs adaptive resets to forget the\nhistory, while the second is last-step min-max optimal in context of a drift.\nWe analyze both algorithms in the worst-case regret framework and show that\nthey maintain an average loss close to that of the best slowly changing\nsequence of linear functions, as long as the cumulative drift is sublinear. In\naddition, in the stationary case, when no drift occurs, our algorithms suffer\nlogarithmic regret, as for previous algorithms. Our bounds improve over the\nexisting ones, and simulations demonstrate the usefulness of these algorithms\ncompared with other state-of-the-art approaches.\n", "versions": [{"version": "v1", "created": "Fri, 1 Mar 2013 10:50:46 GMT"}], "update_date": "2013-03-04", "authors_parsed": [["Vaits", "Nina", ""], ["Moroshko", "Edward", ""], ["Crammer", "Koby", ""]]}, {"id": "1303.0156", "submitter": "Llu\\'is Belanche-Mu\\~noz", "authors": "G. Prat and Ll. Belanche", "title": "Exploiting the Accumulated Evidence for Gene Selection in Microarray\n  Gene Expression Data", "comments": "10 pages, 2 algorithms A shorter version of this paper appeared in\n  the Procs. of the 19th European Conference on Artificial Intelligence (ECAI\n  2010)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CE cs.LG q-bio.QM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Machine Learning methods have of late made significant efforts to solving\nmultidisciplinary problems in the field of cancer classification using\nmicroarray gene expression data. Feature subset selection methods can play an\nimportant role in the modeling process, since these tasks are characterized by\na large number of features and a few observations, making the modeling a\nnon-trivial undertaking. In this particular scenario, it is extremely important\nto select genes by taking into account the possible interactions with other\ngene subsets. This paper shows that, by accumulating the evidence in favour (or\nagainst) each gene along the search process, the obtained gene subsets may\nconstitute better solutions, either in terms of predictive accuracy or gene\nsize, or in both. The proposed technique is extremely simple and applicable at\na negligible overhead in cost.\n", "versions": [{"version": "v1", "created": "Fri, 1 Mar 2013 12:46:06 GMT"}], "update_date": "2013-03-04", "authors_parsed": [["Prat", "G.", ""], ["Belanche", "Ll.", ""]]}, {"id": "1303.0283", "submitter": "Uri Kartoun", "authors": "Uri Kartoun", "title": "Inverse Signal Classification for Financial Instruments", "comments": "arXiv admin note: substantial text overlap with arXiv:1303.0073", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.IR q-fin.ST stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The paper presents new machine learning methods: signal composition, which\nclassifies time-series regardless of length, type, and quantity; and\nself-labeling, a supervised-learning enhancement. The paper describes further\nthe implementation of the methods on a financial search engine system using a\ncollection of 7,881 financial instruments traded during 2011 to identify\ninverse behavior among the time-series.\n", "versions": [{"version": "v1", "created": "Fri, 1 Mar 2013 03:45:42 GMT"}, {"version": "v2", "created": "Tue, 19 Mar 2013 21:17:56 GMT"}], "update_date": "2013-05-14", "authors_parsed": [["Kartoun", "Uri", ""]]}, {"id": "1303.0309", "submitter": "Krikamol Muandet", "authors": "Krikamol Muandet and Bernhard Sch\\\"olkopf", "title": "One-Class Support Measure Machines for Group Anomaly Detection", "comments": "Conference on Uncertainty in Artificial Intelligence (UAI2013)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose one-class support measure machines (OCSMMs) for group anomaly\ndetection which aims at recognizing anomalous aggregate behaviors of data\npoints. The OCSMMs generalize well-known one-class support vector machines\n(OCSVMs) to a space of probability measures. By formulating the problem as\nquantile estimation on distributions, we can establish an interesting\nconnection to the OCSVMs and variable kernel density estimators (VKDEs) over\nthe input space on which the distributions are defined, bridging the gap\nbetween large-margin methods and kernel density estimators. In particular, we\nshow that various types of VKDEs can be considered as solutions to a class of\nregularization problems studied in this paper. Experiments on Sloan Digital Sky\nSurvey dataset and High Energy Particle Physics dataset demonstrate the\nbenefits of the proposed framework in real-world applications.\n", "versions": [{"version": "v1", "created": "Fri, 1 Mar 2013 21:50:09 GMT"}, {"version": "v2", "created": "Sat, 1 Jun 2013 13:42:46 GMT"}], "update_date": "2013-06-04", "authors_parsed": [["Muandet", "Krikamol", ""], ["Sch\u00f6lkopf", "Bernhard", ""]]}, {"id": "1303.0339", "submitter": "Chunhua Shen", "authors": "Xi Li and Guosheng Lin and Chunhua Shen and Anton van den Hengel and\n  Anthony Dick", "title": "Learning Hash Functions Using Column Generation", "comments": "9 pages, published in International Conf. Machine Learning, 2013", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  Fast nearest neighbor searching is becoming an increasingly important tool in\nsolving many large-scale problems. Recently a number of approaches to learning\ndata-dependent hash functions have been developed. In this work, we propose a\ncolumn generation based method for learning data-dependent hash functions on\nthe basis of proximity comparison information. Given a set of triplets that\nencode the pairwise proximity comparison information, our method learns hash\nfunctions that preserve the relative comparison relationships in the data as\nwell as possible within the large-margin learning framework. The learning\nprocedure is implemented using column generation and hence is named CGHash. At\neach iteration of the column generation procedure, the best hash function is\nselected. Unlike most other hashing methods, our method generalizes to new data\npoints naturally; and has a training objective which is convex, thus ensuring\nthat the global optimum can be identified. Experiments demonstrate that the\nproposed method learns compact binary codes and that its retrieval performance\ncompares favorably with state-of-the-art methods when tested on a few benchmark\ndatasets.\n", "versions": [{"version": "v1", "created": "Sat, 2 Mar 2013 03:01:46 GMT"}], "update_date": "2013-03-05", "authors_parsed": [["Li", "Xi", ""], ["Lin", "Guosheng", ""], ["Shen", "Chunhua", ""], ["Hengel", "Anton van den", ""], ["Dick", "Anthony", ""]]}, {"id": "1303.0341", "submitter": "Wen-Xin Zhou", "authors": "T. Tony Cai, Wen-Xin Zhou", "title": "Matrix Completion via Max-Norm Constrained Optimization", "comments": "33 pages", "journal-ref": null, "doi": "10.1214/16-EJS1147", "report-no": null, "categories": "cs.LG cs.IT math.IT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Matrix completion has been well studied under the uniform sampling model and\nthe trace-norm regularized methods perform well both theoretically and\nnumerically in such a setting. However, the uniform sampling model is\nunrealistic for a range of applications and the standard trace-norm relaxation\ncan behave very poorly when the underlying sampling scheme is non-uniform.\n  In this paper we propose and analyze a max-norm constrained empirical risk\nminimization method for noisy matrix completion under a general sampling model.\nThe optimal rate of convergence is established under the Frobenius norm loss in\nthe context of approximately low-rank matrix reconstruction. It is shown that\nthe max-norm constrained method is minimax rate-optimal and yields a unified\nand robust approximate recovery guarantee, with respect to the sampling\ndistributions. The computational effectiveness of this method is also\ndiscussed, based on first-order algorithms for solving convex optimizations\ninvolving max-norm regularization.\n", "versions": [{"version": "v1", "created": "Sat, 2 Mar 2013 03:22:37 GMT"}, {"version": "v2", "created": "Fri, 16 May 2014 06:05:27 GMT"}, {"version": "v3", "created": "Fri, 28 Apr 2017 02:03:30 GMT"}], "update_date": "2017-05-01", "authors_parsed": [["Cai", "T. Tony", ""], ["Zhou", "Wen-Xin", ""]]}, {"id": "1303.0362", "submitter": "Xi Peng", "authors": "Xi Peng, Lei Zhang and Zhang Yi", "title": "Inductive Sparse Subspace Clustering", "comments": "2 pages", "journal-ref": "Electronics Letters, 2013, 49, (19), p. 1222-1224", "doi": "10.1049/el.2013.1789", "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sparse Subspace Clustering (SSC) has achieved state-of-the-art clustering\nquality by performing spectral clustering over a $\\ell^{1}$-norm based\nsimilarity graph. However, SSC is a transductive method which does not handle\nwith the data not used to construct the graph (out-of-sample data). For each\nnew datum, SSC requires solving $n$ optimization problems in O(n) variables for\nperforming the algorithm over the whole data set, where $n$ is the number of\ndata points. Therefore, it is inefficient to apply SSC in fast online\nclustering and scalable graphing. In this letter, we propose an inductive\nspectral clustering algorithm, called inductive Sparse Subspace Clustering\n(iSSC), which makes SSC feasible to cluster out-of-sample data. iSSC adopts the\nassumption that high-dimensional data actually lie on the low-dimensional\nmanifold such that out-of-sample data could be grouped in the embedding space\nlearned from in-sample data. Experimental results show that iSSC is promising\nin clustering out-of-sample data.\n", "versions": [{"version": "v1", "created": "Sat, 2 Mar 2013 07:47:21 GMT"}], "update_date": "2014-09-11", "authors_parsed": [["Peng", "Xi", ""], ["Zhang", "Lei", ""], ["Yi", "Zhang", ""]]}, {"id": "1303.0551", "submitter": "Dimitris S. Papailiopoulos", "authors": "Dimitris S. Papailiopoulos, Alexandros G. Dimakis, and Stavros\n  Korokythakis", "title": "Sparse PCA through Low-rank Approximations", "comments": "Long version of the ICML 2013 paper:\n  http://jmlr.org/proceedings/papers/v28/papailiopoulos13.html", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.IT cs.LG math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a novel algorithm that computes the $k$-sparse principal\ncomponent of a positive semidefinite matrix $A$. Our algorithm is combinatorial\nand operates by examining a discrete set of special vectors lying in a\nlow-dimensional eigen-subspace of $A$. We obtain provable approximation\nguarantees that depend on the spectral decay profile of the matrix: the faster\nthe eigenvalue decay, the better the quality of our approximation. For example,\nif the eigenvalues of $A$ follow a power-law decay, we obtain a polynomial-time\napproximation algorithm for any desired accuracy.\n  A key algorithmic component of our scheme is a combinatorial feature\nelimination step that is provably safe and in practice significantly reduces\nthe running complexity of our algorithm. We implement our algorithm and test it\non multiple artificial and real data sets. Due to the feature elimination step,\nit is possible to perform sparse PCA on data sets consisting of millions of\nentries in a few minutes. Our experimental evaluation shows that our scheme is\nnearly optimal while finding very sparse vectors. We compare to the prior state\nof the art and show that our scheme matches or outperforms previous algorithms\nin all tested data sets.\n", "versions": [{"version": "v1", "created": "Sun, 3 Mar 2013 19:08:55 GMT"}, {"version": "v2", "created": "Thu, 8 May 2014 00:30:12 GMT"}], "update_date": "2014-05-09", "authors_parsed": [["Papailiopoulos", "Dimitris S.", ""], ["Dimakis", "Alexandros G.", ""], ["Korokythakis", "Stavros", ""]]}, {"id": "1303.0561", "submitter": "Balaji Lakshminarayanan", "authors": "Balaji Lakshminarayanan, Daniel M. Roy and Yee Whye Teh", "title": "Top-down particle filtering for Bayesian decision trees", "comments": "ICML 2013", "journal-ref": "JMLR W&CP 28(3):280-288, 2013", "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Decision tree learning is a popular approach for classification and\nregression in machine learning and statistics, and Bayesian\nformulations---which introduce a prior distribution over decision trees, and\nformulate learning as posterior inference given data---have been shown to\nproduce competitive performance. Unlike classic decision tree learning\nalgorithms like ID3, C4.5 and CART, which work in a top-down manner, existing\nBayesian algorithms produce an approximation to the posterior distribution by\nevolving a complete tree (or collection thereof) iteratively via local Monte\nCarlo modifications to the structure of the tree, e.g., using Markov chain\nMonte Carlo (MCMC). We present a sequential Monte Carlo (SMC) algorithm that\ninstead works in a top-down manner, mimicking the behavior and speed of classic\nalgorithms. We demonstrate empirically that our approach delivers accuracy\ncomparable to the most popular MCMC method, but operates more than an order of\nmagnitude faster, and thus represents a better computation-accuracy tradeoff.\n", "versions": [{"version": "v1", "created": "Sun, 3 Mar 2013 20:36:44 GMT"}, {"version": "v2", "created": "Thu, 22 Aug 2013 23:10:00 GMT"}], "update_date": "2013-08-26", "authors_parsed": [["Lakshminarayanan", "Balaji", ""], ["Roy", "Daniel M.", ""], ["Teh", "Yee Whye", ""]]}, {"id": "1303.0642", "submitter": "Rajarshi Guhaniyogi", "authors": "Rajarshi Guhaniyogi and David B. Dunson", "title": "Bayesian Compressed Regression", "comments": "29 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As an alternative to variable selection or shrinkage in high dimensional\nregression, we propose to randomly compress the predictors prior to analysis.\nThis dramatically reduces storage and computational bottlenecks, performing\nwell when the predictors can be projected to a low dimensional linear subspace\nwith minimal loss of information about the response. As opposed to existing\nBayesian dimensionality reduction approaches, the exact posterior distribution\nconditional on the compressed data is available analytically, speeding up\ncomputation by many orders of magnitude while also bypassing robustness issues\ndue to convergence and mixing problems with MCMC. Model averaging is used to\nreduce sensitivity to the random projection matrix, while accommodating\nuncertainty in the subspace dimension. Strong theoretical support is provided\nfor the approach by showing near parametric convergence rates for the\npredictive density in the large p small n asymptotic paradigm. Practical\nperformance relative to competitors is illustrated in simulations and real data\napplications.\n", "versions": [{"version": "v1", "created": "Mon, 4 Mar 2013 08:39:34 GMT"}, {"version": "v2", "created": "Fri, 22 Mar 2013 20:15:31 GMT"}], "update_date": "2013-03-26", "authors_parsed": [["Guhaniyogi", "Rajarshi", ""], ["Dunson", "David B.", ""]]}, {"id": "1303.0663", "submitter": "Xiao-Lei Zhang", "authors": "Xiao-Lei Zhang and Ji Wu", "title": "Denoising Deep Neural Networks Based Voice Activity Detection", "comments": "This paper has been accepted by IEEE ICASSP-2013, and will be\n  published online after May, 2013", "journal-ref": null, "doi": "10.1109/ICASSP.2013.6637769", "report-no": null, "categories": "cs.LG cs.SD stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, the deep-belief-networks (DBN) based voice activity detection (VAD)\nhas been proposed. It is powerful in fusing the advantages of multiple\nfeatures, and achieves the state-of-the-art performance. However, the deep\nlayers of the DBN-based VAD do not show an apparent superiority to the\nshallower layers. In this paper, we propose a denoising-deep-neural-network\n(DDNN) based VAD to address the aforementioned problem. Specifically, we\npre-train a deep neural network in a special unsupervised denoising greedy\nlayer-wise mode, and then fine-tune the whole network in a supervised way by\nthe common back-propagation algorithm. In the pre-training phase, we take the\nnoisy speech signals as the visible layer and try to extract a new feature that\nminimizes the reconstruction cross-entropy loss between the noisy speech\nsignals and its corresponding clean speech signals. Experimental results show\nthat the proposed DDNN-based VAD not only outperforms the DBN-based VAD but\nalso shows an apparent performance improvement of the deep layers over\nshallower layers.\n", "versions": [{"version": "v1", "created": "Mon, 4 Mar 2013 10:17:49 GMT"}], "update_date": "2013-11-05", "authors_parsed": [["Zhang", "Xiao-Lei", ""], ["Wu", "Ji", ""]]}, {"id": "1303.0665", "submitter": "Florent Garcin", "authors": "Florent Garcin, Christos Dimitrakakis and Boi Faltings", "title": "Personalized News Recommendation with Context Trees", "comments": null, "journal-ref": "Proceedings of the 7th ACM conference on Recommender systems\n  (2013), pp. 105--112", "doi": "10.1145/2507157.2507166", "report-no": null, "categories": "cs.IR cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  The profusion of online news articles makes it difficult to find interesting\narticles, a problem that can be assuaged by using a recommender system to bring\nthe most relevant news stories to readers. However, news recommendation is\nchallenging because the most relevant articles are often new content seen by\nfew users. In addition, they are subject to trends and preference changes over\ntime, and in many cases we do not have sufficient information to profile the\nreader.\n  In this paper, we introduce a class of news recommendation systems based on\ncontext trees. They can provide high-quality news recommendation to anonymous\nvisitors based on present browsing behaviour. We show that context-tree\nrecommender systems provide good prediction accuracy and recommendation\nnovelty, and they are sufficiently flexible to capture the unique properties of\nnews articles.\n", "versions": [{"version": "v1", "created": "Mon, 4 Mar 2013 10:34:13 GMT"}, {"version": "v2", "created": "Mon, 3 Nov 2014 16:19:43 GMT"}], "update_date": "2014-11-04", "authors_parsed": [["Garcin", "Florent", ""], ["Dimitrakakis", "Christos", ""], ["Faltings", "Boi", ""]]}, {"id": "1303.0691", "submitter": "Jose M. Pe\\~na", "authors": "Jose M. Pe\\~na", "title": "Learning AMP Chain Graphs and some Marginal Models Thereof under\n  Faithfulness: Extended Version", "comments": "Changes from v1 to v2: The interpretation of the antecedent of the\n  rule R3 changed, which in turn implied modifying Lemma 6 and Theorem 1.\n  Changes from v2 to v3: Minor improvements in the first 12 pages. A shorter\n  version is to appear in International Journal of Approximate Reasoning, 2014", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper deals with chain graphs under the Andersson-Madigan-Perlman (AMP)\ninterpretation. In particular, we present a constraint based algorithm for\nlearning an AMP chain graph a given probability distribution is faithful to.\nMoreover, we show that the extension of Meek's conjecture to AMP chain graphs\ndoes not hold, which compromises the development of efficient and correct\nscore+search learning algorithms under assumptions weaker than faithfulness.\n  We also introduce a new family of graphical models that consists of\nundirected and bidirected edges. We name this new family maximal\ncovariance-concentration graphs (MCCGs) because it includes both covariance and\nconcentration graphs as subfamilies. However, every MCCG can be seen as the\nresult of marginalizing out some nodes in an AMP CG. We describe global, local\nand pairwise Markov properties for MCCGs and prove their equivalence. We\ncharacterize when two MCCGs are Markov equivalent, and show that every Markov\nequivalence class of MCCGs has a distinguished member. We present a constraint\nbased algorithm for learning a MCCG a given probability distribution is\nfaithful to.\n  Finally, we present a graphical criterion for reading dependencies from a\nMCCG of a probability distribution that satisfies the graphoid properties, weak\ntransitivity and composition. We prove that the criterion is sound and complete\nin certain sense.\n", "versions": [{"version": "v1", "created": "Mon, 4 Mar 2013 13:02:49 GMT"}, {"version": "v2", "created": "Mon, 19 Aug 2013 11:55:54 GMT"}, {"version": "v3", "created": "Fri, 17 Jan 2014 13:07:42 GMT"}], "update_date": "2014-01-20", "authors_parsed": [["Pe\u00f1a", "Jose M.", ""]]}, {"id": "1303.0742", "submitter": "Yoann Isaac", "authors": "Quentin Barth\\'elemy, C\\'edric Gouy-Pailler, Yoann Isaac, Antoine\n  Souloumiac, Anthony Larue, J\\'er\\^ome I. Mars", "title": "Multivariate Temporal Dictionary Learning for EEG", "comments": null, "journal-ref": "Published in Journal of Neuroscience Methods, vol. 215, pp. 19-28,\n  2013", "doi": null, "report-no": null, "categories": "cs.LG q-bio.NC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article addresses the issue of representing electroencephalographic\n(EEG) signals in an efficient way. While classical approaches use a fixed Gabor\ndictionary to analyze EEG signals, this article proposes a data-driven method\nto obtain an adapted dictionary. To reach an efficient dictionary learning,\nappropriate spatial and temporal modeling is required. Inter-channels links are\ntaken into account in the spatial multivariate model, and shift-invariance is\nused for the temporal model. Multivariate learned kernels are informative (a\nfew atoms code plentiful energy) and interpretable (the atoms can have a\nphysiological meaning). Using real EEG data, the proposed method is shown to\noutperform the classical multichannel matching pursuit used with a Gabor\ndictionary, as measured by the representative power of the learned dictionary\nand its spatial flexibility. Moreover, dictionary learning can capture\ninterpretable patterns: this ability is illustrated on real data, learning a\nP300 evoked potential.\n", "versions": [{"version": "v1", "created": "Mon, 4 Mar 2013 15:58:24 GMT"}], "update_date": "2013-03-22", "authors_parsed": [["Barth\u00e9lemy", "Quentin", ""], ["Gouy-Pailler", "C\u00e9dric", ""], ["Isaac", "Yoann", ""], ["Souloumiac", "Antoine", ""], ["Larue", "Anthony", ""], ["Mars", "J\u00e9r\u00f4me I.", ""]]}, {"id": "1303.0818", "submitter": "Yann Ollivier", "authors": "Yann Ollivier", "title": "Riemannian metrics for neural networks I: feedforward networks", "comments": "(5th version, minor changes)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.IT cs.LG math.DG math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe four algorithms for neural network training, each adapted to\ndifferent scalability constraints. These algorithms are mathematically\nprincipled and invariant under a number of transformations in data and network\nrepresentation, from which performance is thus independent. These algorithms\nare obtained from the setting of differential geometry, and are based on either\nthe natural gradient using the Fisher information matrix, or on Hessian\nmethods, scaled down in a specific way to allow for scalability while keeping\nsome of their key mathematical properties.\n", "versions": [{"version": "v1", "created": "Mon, 4 Mar 2013 20:41:09 GMT"}, {"version": "v2", "created": "Thu, 2 May 2013 16:07:30 GMT"}, {"version": "v3", "created": "Sun, 25 Aug 2013 20:34:13 GMT"}, {"version": "v4", "created": "Sat, 12 Jul 2014 14:37:30 GMT"}, {"version": "v5", "created": "Tue, 3 Feb 2015 18:24:30 GMT"}], "update_date": "2015-02-04", "authors_parsed": [["Ollivier", "Yann", ""]]}, {"id": "1303.0934", "submitter": "Matteo Santoro", "authors": "Andrea Tacchetti, Pavan K Mallapragada, Matteo Santoro, Lorenzo\n  Rosasco", "title": "GURLS: a Least Squares Library for Supervised Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.MS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present GURLS, a least squares, modular, easy-to-extend software library\nfor efficient supervised learning. GURLS is targeted to machine learning\npractitioners, as well as non-specialists. It offers a number state-of-the-art\ntraining strategies for medium and large-scale learning, and routines for\nefficient model selection. The library is particularly well suited for\nmulti-output problems (multi-category/multi-label). GURLS is currently\navailable in two independent implementations: Matlab and C++. It takes\nadvantage of the favorable properties of regularized least squares algorithm to\nexploit advanced tools in linear algebra. Routines to handle computations with\nvery large matrices by means of memory-mapped storage and distributed task\nexecution are available. The package is distributed under the BSD licence and\nis available for download at https://github.com/CBCL/GURLS.\n", "versions": [{"version": "v1", "created": "Tue, 5 Mar 2013 05:55:59 GMT"}], "update_date": "2013-03-06", "authors_parsed": [["Tacchetti", "Andrea", ""], ["Mallapragada", "Pavan K", ""], ["Santoro", "Matteo", ""], ["Rosasco", "Lorenzo", ""]]}, {"id": "1303.1152", "submitter": "Martin Jaggi", "authors": "Martin Jaggi", "title": "An Equivalence between the Lasso and Support Vector Machines", "comments": "Book chapter in Regularization, Optimization, Kernels, and Support\n  Vector Machines, Johan A.K. Suykens, Marco Signoretto, Andreas Argyriou\n  (Editors), 2014", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate the relation of two fundamental tools in machine learning and\nsignal processing, that is the support vector machine (SVM) for classification,\nand the Lasso technique used in regression. We show that the resulting\noptimization problems are equivalent, in the following sense. Given any\ninstance of an $\\ell_2$-loss soft-margin (or hard-margin) SVM, we construct a\nLasso instance having the same optimal solutions, and vice versa.\n  As a consequence, many existing optimization algorithms for both SVMs and\nLasso can also be applied to the respective other problem instances. Also, the\nequivalence allows for many known theoretical insights for SVM and Lasso to be\ntranslated between the two settings. One such implication gives a simple\nkernelized version of the Lasso, analogous to the kernels used in the SVM\nsetting. Another consequence is that the sparsity of a Lasso solution is equal\nto the number of support vectors for the corresponding SVM instance, and that\none can use screening rules to prune the set of support vectors. Furthermore,\nwe can relate sublinear time algorithms for the two problems, and give a new\nsuch algorithm variant for the Lasso. We also study the regularization paths\nfor both methods.\n", "versions": [{"version": "v1", "created": "Tue, 5 Mar 2013 19:59:13 GMT"}, {"version": "v2", "created": "Fri, 25 Apr 2014 12:03:24 GMT"}], "update_date": "2014-04-28", "authors_parsed": [["Jaggi", "Martin", ""]]}, {"id": "1303.1208", "submitter": "Clayton Scott", "authors": "Gilles Blanchard, Marek Flaska, Gregory Handy, Sara Pozzi, Clayton\n  Scott", "title": "Classification with Asymmetric Label Noise: Consistency and Maximal\n  Denoising", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In many real-world classification problems, the labels of training examples\nare randomly corrupted. Most previous theoretical work on classification with\nlabel noise assumes that the two classes are separable, that the label noise is\nindependent of the true class label, or that the noise proportions for each\nclass are known. In this work, we give conditions that are necessary and\nsufficient for the true class-conditional distributions to be identifiable.\nThese conditions are weaker than those analyzed previously, and allow for the\nclasses to be nonseparable and the noise levels to be asymmetric and unknown.\nThe conditions essentially state that a majority of the observed labels are\ncorrect and that the true class-conditional distributions are \"mutually\nirreducible,\" a concept we introduce that limits the similarity of the two\ndistributions. For any label noise problem, there is a unique pair of true\nclass-conditional distributions satisfying the proposed conditions, and we\nargue that this pair corresponds in a certain sense to maximal denoising of the\nobserved distributions.\n  Our results are facilitated by a connection to \"mixture proportion\nestimation,\" which is the problem of estimating the maximal proportion of one\ndistribution that is present in another. We establish a novel rate of\nconvergence result for mixture proportion estimation, and apply this to obtain\nconsistency of a discrimination rule based on surrogate loss minimization.\nExperimental results on benchmark data and a nuclear particle classification\nproblem demonstrate the efficacy of our approach.\n", "versions": [{"version": "v1", "created": "Tue, 5 Mar 2013 22:23:14 GMT"}, {"version": "v2", "created": "Fri, 31 Jul 2015 22:36:23 GMT"}, {"version": "v3", "created": "Fri, 5 Aug 2016 15:38:43 GMT"}], "update_date": "2016-08-08", "authors_parsed": [["Blanchard", "Gilles", ""], ["Flaska", "Marek", ""], ["Handy", "Gregory", ""], ["Pozzi", "Sara", ""], ["Scott", "Clayton", ""]]}, {"id": "1303.1264", "submitter": "Radim Belohlavek", "authors": "Radim Belohlavek and Vilem Vychodil", "title": "Discovery of factors in matrices with grades", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an approach to decomposition and factor analysis of matrices with\nordinal data. The matrix entries are grades to which objects represented by\nrows satisfy attributes represented by columns, e.g. grades to which an image\nis red, a product has a given feature, or a person performs well in a test. We\nassume that the grades form a bounded scale equipped with certain aggregation\noperators and conforms to the structure of a complete residuated lattice. We\npresent a greedy approximation algorithm for the problem of decomposition of\nsuch matrix in a product of two matrices with grades under the restriction that\nthe number of factors be small. Our algorithm is based on a geometric insight\nprovided by a theorem identifying particular rectangular-shaped submatrices as\noptimal factors for the decompositions. These factors correspond to formal\nconcepts of the input data and allow an easy interpretation of the\ndecomposition. We present illustrative examples and experimental evaluation.\n", "versions": [{"version": "v1", "created": "Wed, 6 Mar 2013 07:58:14 GMT"}], "update_date": "2013-03-07", "authors_parsed": [["Belohlavek", "Radim", ""], ["Vychodil", "Vilem", ""]]}, {"id": "1303.1271", "submitter": "Zhi-Hua Zhou", "authors": "Yu-Feng Li, Ivor W. Tsang, James T. Kwok and Zhi-Hua Zhou", "title": "Convex and Scalable Weakly Labeled SVMs", "comments": null, "journal-ref": "Journal of Machine Learning Research, 2013, 14: 2151-2188", "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we study the problem of learning from weakly labeled data,\nwhere labels of the training examples are incomplete. This includes, for\nexample, (i) semi-supervised learning where labels are partially known; (ii)\nmulti-instance learning where labels are implicitly known; and (iii) clustering\nwhere labels are completely unknown. Unlike supervised learning, learning with\nweak labels involves a difficult Mixed-Integer Programming (MIP) problem.\nTherefore, it can suffer from poor scalability and may also get stuck in local\nminimum. In this paper, we focus on SVMs and propose the WellSVM via a novel\nlabel generation strategy. This leads to a convex relaxation of the original\nMIP, which is at least as tight as existing convex Semi-Definite Programming\n(SDP) relaxations. Moreover, the WellSVM can be solved via a sequence of SVM\nsubproblems that are much more scalable than previous convex SDP relaxations.\nExperiments on three weakly labeled learning tasks, namely, (i) semi-supervised\nlearning; (ii) multi-instance learning for locating regions of interest in\ncontent-based information retrieval; and (iii) clustering, clearly demonstrate\nimproved performance, and WellSVM is also readily applicable on large data\nsets.\n", "versions": [{"version": "v1", "created": "Wed, 6 Mar 2013 08:20:33 GMT"}, {"version": "v2", "created": "Wed, 1 May 2013 09:03:18 GMT"}, {"version": "v3", "created": "Sun, 7 Jul 2013 03:38:54 GMT"}, {"version": "v4", "created": "Mon, 12 Aug 2013 05:26:50 GMT"}, {"version": "v5", "created": "Thu, 22 Aug 2013 04:29:26 GMT"}], "update_date": "2020-07-07", "authors_parsed": [["Li", "Yu-Feng", ""], ["Tsang", "Ivor W.", ""], ["Kwok", "James T.", ""], ["Zhou", "Zhi-Hua", ""]]}, {"id": "1303.1280", "submitter": "Remi Lajugie", "authors": "R\\'emi Lajugie (LIENS), Sylvain Arlot (LIENS), Francis Bach (LIENS)", "title": "Large-Margin Metric Learning for Partitioning Problems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we consider unsupervised partitioning problems, such as\nclustering, image segmentation, video segmentation and other change-point\ndetection problems. We focus on partitioning problems based explicitly or\nimplicitly on the minimization of Euclidean distortions, which include\nmean-based change-point detection, K-means, spectral clustering and normalized\ncuts. Our main goal is to learn a Mahalanobis metric for these unsupervised\nproblems, leading to feature weighting and/or selection. This is done in a\nsupervised way by assuming the availability of several potentially partially\nlabelled datasets that share the same metric. We cast the metric learning\nproblem as a large-margin structured prediction problem, with proper definition\nof regularizers and losses, leading to a convex optimization problem which can\nbe solved efficiently with iterative techniques. We provide experiments where\nwe show how learning the metric may significantly improve the partitioning\nperformance in synthetic examples, bioinformatics, video segmentation and image\nsegmentation problems.\n", "versions": [{"version": "v1", "created": "Wed, 6 Mar 2013 09:23:45 GMT"}], "update_date": "2013-03-07", "authors_parsed": [["Lajugie", "R\u00e9mi", "", "LIENS"], ["Arlot", "Sylvain", "", "LIENS"], ["Bach", "Francis", "", "LIENS"]]}, {"id": "1303.1733", "submitter": "Ben London", "authors": "Ben London, Theodoros Rekatsinas, Bert Huang, and Lise Getoor", "title": "Multi-relational Learning Using Weighted Tensor Decomposition with\n  Modular Loss", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a modular framework for multi-relational learning via tensor\ndecomposition. In our learning setting, the training data contains multiple\ntypes of relationships among a set of objects, which we represent by a sparse\nthree-mode tensor. The goal is to predict the values of the missing entries. To\ndo so, we model each relationship as a function of a linear combination of\nlatent factors. We learn this latent representation by computing a low-rank\ntensor decomposition, using quasi-Newton optimization of a weighted objective\nfunction. Sparsity in the observed data is captured by the weighted objective,\nleading to improved accuracy when training data is limited. Exploiting sparsity\nalso improves efficiency, potentially up to an order of magnitude over\nunweighted approaches. In addition, our framework accommodates arbitrary\ncombinations of smooth, task-specific loss functions, making it better suited\nfor learning different types of relations. For the typical cases of real-valued\nfunctions and binary relations, we propose several loss functions and derive\nthe associated parameter gradients. We evaluate our method on synthetic and\nreal data, showing significant improvements in both accuracy and scalability\nover related factorization techniques.\n", "versions": [{"version": "v1", "created": "Thu, 7 Mar 2013 16:10:44 GMT"}, {"version": "v2", "created": "Fri, 31 May 2013 21:09:20 GMT"}], "update_date": "2013-06-04", "authors_parsed": [["London", "Ben", ""], ["Rekatsinas", "Theodoros", ""], ["Huang", "Bert", ""], ["Getoor", "Lise", ""]]}, {"id": "1303.1849", "submitter": "Alex Gittens", "authors": "Alex Gittens and Michael W. Mahoney", "title": "Revisiting the Nystrom Method for Improved Large-Scale Machine Learning", "comments": "60 pages, 15 color figures; updated proof of Frobenius norm bounds,\n  added comparison to projection-based low-rank approximations, and an analysis\n  of the power method applied to SPSD sketches", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS cs.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We reconsider randomized algorithms for the low-rank approximation of\nsymmetric positive semi-definite (SPSD) matrices such as Laplacian and kernel\nmatrices that arise in data analysis and machine learning applications. Our\nmain results consist of an empirical evaluation of the performance quality and\nrunning time of sampling and projection methods on a diverse suite of SPSD\nmatrices. Our results highlight complementary aspects of sampling versus\nprojection methods; they characterize the effects of common data preprocessing\nsteps on the performance of these algorithms; and they point to important\ndifferences between uniform sampling and nonuniform sampling methods based on\nleverage scores. In addition, our empirical results illustrate that existing\ntheory is so weak that it does not provide even a qualitative guide to\npractice. Thus, we complement our empirical results with a suite of worst-case\ntheoretical bounds for both random sampling and random projection methods.\nThese bounds are qualitatively superior to existing bounds---e.g. improved\nadditive-error bounds for spectral and Frobenius norm error and relative-error\nbounds for trace norm error---and they point to future directions to make these\nalgorithms useful in even larger-scale machine learning applications.\n", "versions": [{"version": "v1", "created": "Thu, 7 Mar 2013 23:16:16 GMT"}, {"version": "v2", "created": "Mon, 3 Jun 2013 20:07:19 GMT"}], "update_date": "2013-06-05", "authors_parsed": [["Gittens", "Alex", ""], ["Mahoney", "Michael W.", ""]]}, {"id": "1303.2054", "submitter": "Wajdi Dhifli Wajdi DHIFLI", "authors": "Wajdi Dhifli, Rabie Saidi, Engelbert Mephu Nguifo", "title": "Mining Representative Unsubstituted Graph Patterns Using Prior\n  Similarity Matrix", "comments": null, "journal-ref": "Information Systems (Elsevier) 69: 155-163, 2017", "doi": "10.1016/j.is.2017.05.006", "report-no": null, "categories": "cs.CE cs.LG", "license": "http://creativecommons.org/licenses/publicdomain/", "abstract": "  One of the most powerful techniques to study protein structures is to look\nfor recurrent fragments (also called substructures or spatial motifs), then use\nthem as patterns to characterize the proteins under study. An emergent trend\nconsists in parsing proteins three-dimensional (3D) structures into graphs of\namino acids. Hence, the search of recurrent spatial motifs is formulated as a\nprocess of frequent subgraph discovery where each subgraph represents a spatial\nmotif. In this scope, several efficient approaches for frequent subgraph\ndiscovery have been proposed in the literature. However, the set of discovered\nfrequent subgraphs is too large to be efficiently analyzed and explored in any\nfurther process. In this paper, we propose a novel pattern selection approach\nthat shrinks the large number of discovered frequent subgraphs by selecting the\nrepresentative ones. Existing pattern selection approaches do not exploit the\ndomain knowledge. Yet, in our approach we incorporate the evolutionary\ninformation of amino acids defined in the substitution matrices in order to\nselect the representative subgraphs. We show the effectiveness of our approach\non a number of real datasets. The results issued from our experiments show that\nour approach is able to considerably decrease the number of motifs while\nenhancing their interestingness.\n", "versions": [{"version": "v1", "created": "Fri, 8 Mar 2013 16:57:18 GMT"}], "update_date": "2018-03-02", "authors_parsed": [["Dhifli", "Wajdi", ""], ["Saidi", "Rabie", ""], ["Nguifo", "Engelbert Mephu", ""]]}, {"id": "1303.2104", "submitter": "Xiao-Lei Zhang", "authors": "Xiao-Lei Zhang, Ji Wu", "title": "Transfer Learning for Voice Activity Detection: A Denoising Deep Neural\n  Network Perspective", "comments": "This paper has been submitted to the conference \"INTERSPEECH2013\" in\n  March 4, 2013 for review", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mismatching problem between the source and target noisy corpora severely\nhinder the practical use of the machine-learning-based voice activity detection\n(VAD). In this paper, we try to address this problem in the transfer learning\nprospective. Transfer learning tries to find a common learning machine or a\ncommon feature subspace that is shared by both the source corpus and the target\ncorpus. The denoising deep neural network is used as the learning machine.\nThree transfer techniques, which aim to learn common feature representations,\nare used for analysis. Experimental results demonstrate the effectiveness of\nthe transfer learning schemes on the mismatch problem.\n", "versions": [{"version": "v1", "created": "Fri, 8 Mar 2013 20:46:27 GMT"}], "update_date": "2013-03-11", "authors_parsed": [["Zhang", "Xiao-Lei", ""], ["Wu", "Ji", ""]]}, {"id": "1303.2130", "submitter": "Xiao-Lei Zhang", "authors": "Xiao-Lei Zhang", "title": "Convex Discriminative Multitask Clustering", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multitask clustering tries to improve the clustering performance of multiple\ntasks simultaneously by taking their relationship into account. Most existing\nmultitask clustering algorithms fall into the type of generative clustering,\nand none are formulated as convex optimization problems. In this paper, we\npropose two convex Discriminative Multitask Clustering (DMTC) algorithms to\naddress the problems. Specifically, we first propose a Bayesian DMTC framework.\nThen, we propose two convex DMTC objectives within the framework. The first\none, which can be seen as a technical combination of the convex multitask\nfeature learning and the convex Multiclass Maximum Margin Clustering (M3C),\naims to learn a shared feature representation. The second one, which can be\nseen as a combination of the convex multitask relationship learning and M3C,\naims to learn the task relationship. The two objectives are solved in a uniform\nprocedure by the efficient cutting-plane algorithm. Experimental results on a\ntoy problem and two benchmark datasets demonstrate the effectiveness of the\nproposed algorithms.\n", "versions": [{"version": "v1", "created": "Fri, 8 Mar 2013 21:32:52 GMT"}, {"version": "v2", "created": "Mon, 21 Oct 2013 15:06:36 GMT"}], "update_date": "2013-10-22", "authors_parsed": [["Zhang", "Xiao-Lei", ""]]}, {"id": "1303.2132", "submitter": "Xiao-Lei Zhang", "authors": "Xiao-Lei Zhang", "title": "Heuristic Ternary Error-Correcting Output Codes Via Weight Optimization\n  and Layered Clustering-Based Approach", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One important classifier ensemble for multiclass classification problems is\nError-Correcting Output Codes (ECOCs). It bridges multiclass problems and\nbinary-class classifiers by decomposing multiclass problems to a serial\nbinary-class problems. In this paper, we present a heuristic ternary code,\nnamed Weight Optimization and Layered Clustering-based ECOC (WOLC-ECOC). It\nstarts with an arbitrary valid ECOC and iterates the following two steps until\nthe training risk converges. The first step, named Layered Clustering based\nECOC (LC-ECOC), constructs multiple strong classifiers on the most confusing\nbinary-class problem. The second step adds the new classifiers to ECOC by a\nnovel Optimized Weighted (OW) decoding algorithm, where the optimization\nproblem of the decoding is solved by the cutting plane algorithm. Technically,\nLC-ECOC makes the heuristic training process not blocked by some difficult\nbinary-class problem. OW decoding guarantees the non-increase of the training\nrisk for ensuring a small code length. Results on 14 UCI datasets and a music\ngenre classification problem demonstrate the effectiveness of WOLC-ECOC.\n", "versions": [{"version": "v1", "created": "Fri, 8 Mar 2013 21:40:42 GMT"}, {"version": "v2", "created": "Wed, 23 Apr 2014 00:59:58 GMT"}], "update_date": "2014-04-24", "authors_parsed": [["Zhang", "Xiao-Lei", ""]]}, {"id": "1303.2184", "submitter": "Pantelis Bouboulis", "authors": "Pantelis Bouboulis, Sergios Theodoridis, Charalampos Mavroforakis,\n  Leoni Dalla", "title": "Complex Support Vector Machines for Regression and Quaternary\n  Classification", "comments": "Manuscript accepted in IEEE Transactions on Neural Networks and\n  Learning Systems", "journal-ref": null, "doi": "10.1109/TNNLS.2014.2336679", "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The paper presents a new framework for complex Support Vector Regression as\nwell as Support Vector Machines for quaternary classification. The method\nexploits the notion of widely linear estimation to model the input-out relation\nfor complex-valued data and considers two cases: a) the complex data are split\ninto their real and imaginary parts and a typical real kernel is employed to\nmap the complex data to a complexified feature space and b) a pure complex\nkernel is used to directly map the data to the induced complex feature space.\nThe recently developed Wirtinger's calculus on complex reproducing kernel\nHilbert spaces (RKHS) is employed in order to compute the Lagrangian and derive\nthe dual optimization problem. As one of our major results, we prove that any\ncomplex SVM/SVR task is equivalent with solving two real SVM/SVR tasks\nexploiting a specific real kernel which is generated by the chosen complex\nkernel. In particular, the case of pure complex kernels leads to the generation\nof new kernels, which have not been considered before. In the classification\ncase, the proposed framework inherently splits the complex space into four\nparts. This leads naturally in solving the four class-task (quaternary\nclassification), instead of the typical two classes of the real SVM. In turn,\nthis rationale can be used in a multiclass problem as a split-class scenario\nbased on four classes, as opposed to the one-versus-all method; this can lead\nto significant computational savings. Experiments demonstrate the effectiveness\nof the proposed framework for regression and classification tasks that involve\ncomplex data.\n", "versions": [{"version": "v1", "created": "Sat, 9 Mar 2013 09:09:54 GMT"}, {"version": "v2", "created": "Mon, 29 Jul 2013 06:04:33 GMT"}, {"version": "v3", "created": "Tue, 15 Jul 2014 09:42:04 GMT"}], "update_date": "2016-11-15", "authors_parsed": [["Bouboulis", "Pantelis", ""], ["Theodoridis", "Sergios", ""], ["Mavroforakis", "Charalampos", ""], ["Dalla", "Leoni", ""]]}, {"id": "1303.2221", "submitter": "Xiaowen Dong", "authors": "Xiaowen Dong, Pascal Frossard, Pierre Vandergheynst, Nikolai Nefedov", "title": "Clustering on Multi-Layer Graphs via Subspace Analysis on Grassmann\n  Manifolds", "comments": null, "journal-ref": "IEEE Transactions on Signal Processing, vol. 62, no. 4, pp.\n  905-918, February 2014", "doi": "10.1109/TSP.2013.2295553", "report-no": null, "categories": "cs.LG cs.CV cs.SI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Relationships between entities in datasets are often of multiple nature, like\ngeographical distance, social relationships, or common interests among people\nin a social network, for example. This information can naturally be modeled by\na set of weighted and undirected graphs that form a global multilayer graph,\nwhere the common vertex set represents the entities and the edges on different\nlayers capture the similarities of the entities in term of the different\nmodalities. In this paper, we address the problem of analyzing multi-layer\ngraphs and propose methods for clustering the vertices by efficiently merging\nthe information provided by the multiple modalities. To this end, we propose to\ncombine the characteristics of individual graph layers using tools from\nsubspace analysis on a Grassmann manifold. The resulting combination can then\nbe viewed as a low dimensional representation of the original data which\npreserves the most important information from diverse relationships between\nentities. We use this information in new clustering methods and test our\nalgorithm on several synthetic and real world datasets where we demonstrate\nsuperior or competitive performances compared to baseline and state-of-the-art\ntechniques. Our generic framework further extends to numerous analysis and\nlearning problems that involve different types of information on graphs.\n", "versions": [{"version": "v1", "created": "Sat, 9 Mar 2013 15:31:48 GMT"}], "update_date": "2015-08-31", "authors_parsed": [["Dong", "Xiaowen", ""], ["Frossard", "Pascal", ""], ["Vandergheynst", "Pierre", ""], ["Nefedov", "Nikolai", ""]]}, {"id": "1303.2270", "submitter": "Panayotis Mertikopoulos", "authors": "Pierre Coucheney, Bruno Gaujal, Panayotis Mertikopoulos", "title": "Penalty-regulated dynamics and robust learning procedures in games", "comments": "33 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.GT cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Starting from a heuristic learning scheme for N-person games, we derive a new\nclass of continuous-time learning dynamics consisting of a replicator-like\ndrift adjusted by a penalty term that renders the boundary of the game's\nstrategy space repelling. These penalty-regulated dynamics are equivalent to\nplayers keeping an exponentially discounted aggregate of their on-going payoffs\nand then using a smooth best response to pick an action based on these\nperformance scores. Owing to this inherent duality, the proposed dynamics\nsatisfy a variant of the folk theorem of evolutionary game theory and they\nconverge to (arbitrarily precise) approximations of Nash equilibria in\npotential games. Motivated by applications to traffic engineering, we exploit\nthis duality further to design a discrete-time, payoff-based learning algorithm\nwhich retains these convergence properties and only requires players to observe\ntheir in-game payoffs: moreover, the algorithm remains robust in the presence\nof stochastic perturbations and observation errors, and it does not require any\nsynchronization between players.\n", "versions": [{"version": "v1", "created": "Sat, 9 Mar 2013 21:49:25 GMT"}, {"version": "v2", "created": "Sun, 6 Apr 2014 20:59:47 GMT"}], "update_date": "2014-04-08", "authors_parsed": [["Coucheney", "Pierre", ""], ["Gaujal", "Bruno", ""], ["Mertikopoulos", "Panayotis", ""]]}, {"id": "1303.2314", "submitter": "Nathan Srebro", "authors": "Martin Tak\\'a\\v{c} and Avleen Bijral and Peter Richt\\'arik and Nathan\n  Srebro", "title": "Mini-Batch Primal and Dual Methods for SVMs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We address the issue of using mini-batches in stochastic optimization of\nSVMs. We show that the same quantity, the spectral norm of the data, controls\nthe parallelization speedup obtained for both primal stochastic subgradient\ndescent (SGD) and stochastic dual coordinate ascent (SCDA) methods and use it\nto derive novel variants of mini-batched SDCA. Our guarantees for both methods\nare expressed in terms of the original nonsmooth primal problem based on the\nhinge-loss.\n", "versions": [{"version": "v1", "created": "Sun, 10 Mar 2013 12:00:59 GMT"}], "update_date": "2013-03-12", "authors_parsed": [["Tak\u00e1\u010d", "Martin", ""], ["Bijral", "Avleen", ""], ["Richt\u00e1rik", "Peter", ""], ["Srebro", "Nathan", ""]]}, {"id": "1303.2395", "submitter": "Xu Sun", "authors": "Xu Sun, Jinqiao Duan, Xiaofan Li, Xiangjun Wang", "title": "State estimation under non-Gaussian Levy noise: A modified Kalman\n  filtering method", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.DS cs.IT cs.LG math.IT math.PR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Kalman filter is extensively used for state estimation for linear systems\nunder Gaussian noise. When non-Gaussian L\\'evy noise is present, the\nconventional Kalman filter may fail to be effective due to the fact that the\nnon-Gaussian L\\'evy noise may have infinite variance. A modified Kalman filter\nfor linear systems with non-Gaussian L\\'evy noise is devised. It works\neffectively with reasonable computational cost. Simulation results are\npresented to illustrate this non-Gaussian filtering method.\n", "versions": [{"version": "v1", "created": "Sun, 10 Mar 2013 23:20:12 GMT"}], "update_date": "2013-03-12", "authors_parsed": [["Sun", "Xu", ""], ["Duan", "Jinqiao", ""], ["Li", "Xiaofan", ""], ["Wang", "Xiangjun", ""]]}, {"id": "1303.2417", "submitter": "GuangGang Geng", "authors": "Xiao-Bo Jin and Guang-Gang Geng", "title": "Linear NDCG and Pair-wise Loss", "comments": "5 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Linear NDCG is used for measuring the performance of the Web content quality\nassessment in ECML/PKDD Discovery Challenge 2010. In this paper, we will prove\nthat the DCG error equals a new pair-wise loss.\n", "versions": [{"version": "v1", "created": "Mon, 11 Mar 2013 03:29:35 GMT"}], "update_date": "2013-03-12", "authors_parsed": [["Jin", "Xiao-Bo", ""], ["Geng", "Guang-Gang", ""]]}, {"id": "1303.2506", "submitter": "Christos Dimitrakakis", "authors": "Christos Dimitrakakis", "title": "Monte-Carlo utility estimates for Bayesian reinforcement learning", "comments": "6 pages, 4 figures, 1 table, submitted to IEEE conference on decision\n  and control", "journal-ref": null, "doi": "10.1109/CDC.2013.6761048", "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces a set of algorithms for Monte-Carlo Bayesian\nreinforcement learning. Firstly, Monte-Carlo estimation of upper bounds on the\nBayes-optimal value function is employed to construct an optimistic policy.\nSecondly, gradient-based algorithms for approximate upper and lower bounds are\nintroduced. Finally, we introduce a new class of gradient algorithms for\nBayesian Bellman error minimisation. We theoretically show that the gradient\nmethods are sound. Experimentally, we demonstrate the superiority of the upper\nbound method in terms of reward obtained. However, we also show that the\nBayesian Bellman error method is a close second, despite its significant\ncomputational simplicity.\n", "versions": [{"version": "v1", "created": "Mon, 11 Mar 2013 13:06:49 GMT"}], "update_date": "2016-11-18", "authors_parsed": [["Dimitrakakis", "Christos", ""]]}, {"id": "1303.2643", "submitter": "Hairong Liu", "authors": "Hairong Liu, Longin Jan Latecki, Shuicheng Yan", "title": "Revealing Cluster Structure of Graph by Path Following Replicator\n  Dynamic", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.GT", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  In this paper, we propose a path following replicator dynamic, and\ninvestigate its potentials in uncovering the underlying cluster structure of a\ngraph. The proposed dynamic is a generalization of the discrete replicator\ndynamic. The replicator dynamic has been successfully used to extract dense\nclusters of graphs; however, it is often sensitive to the degree distribution\nof a graph, and usually biased by vertices with large degrees, thus may fail to\ndetect the densest cluster. To overcome this problem, we introduce a dynamic\nparameter, called path parameter, into the evolution process. The path\nparameter can be interpreted as the maximal possible probability of a current\ncluster containing a vertex, and it monotonically increases as evolution\nprocess proceeds. By limiting the maximal probability, the phenomenon of some\nvertices dominating the early stage of evolution process is suppressed, thus\nmaking evolution process more robust. To solve the optimization problem with a\nfixed path parameter, we propose an efficient fixed point algorithm. The time\ncomplexity of the path following replicator dynamic is only linear in the\nnumber of edges of a graph, thus it can analyze graphs with millions of\nvertices and tens of millions of edges on a common PC in a few minutes.\nBesides, it can be naturally generalized to hypergraph and graph with edges of\ndifferent orders. We apply it to four important problems: maximum clique\nproblem, densest k-subgraph problem, structure fitting, and discovery of\nhigh-density regions. The extensive experimental results clearly demonstrate\nits advantages, in terms of robustness, scalability and flexility.\n", "versions": [{"version": "v1", "created": "Mon, 11 Mar 2013 19:52:48 GMT"}], "update_date": "2013-03-12", "authors_parsed": [["Liu", "Hairong", ""], ["Latecki", "Longin Jan", ""], ["Yan", "Shuicheng", ""]]}, {"id": "1303.2651", "submitter": "Djallel Bouneffouf", "authors": "Djallel Bouneffouf", "title": "Hybrid Q-Learning Applied to Ubiquitous recommender system", "comments": "arXiv admin note: substantial text overlap with arXiv:1301.4351,\n  arXiv:1303.2308", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Ubiquitous information access becomes more and more important nowadays and\nresearch is aimed at making it adapted to users. Our work consists in applying\nmachine learning techniques in order to bring a solution to some of the\nproblems concerning the acceptance of the system by users. To achieve this, we\npropose a fundamental shift in terms of how we model the learning of\nrecommender system: inspired by models of human reasoning developed in robotic,\nwe combine reinforcement learning and case-base reasoning to define a\nrecommendation process that uses these two approaches for generating\nrecommendations on different context dimensions (social, temporal, geographic).\nWe describe an implementation of the recommender system based on this\nframework. We also present preliminary results from experiments with the system\nand show how our approach increases the recommendation quality.\n", "versions": [{"version": "v1", "created": "Sun, 10 Mar 2013 12:51:03 GMT"}, {"version": "v2", "created": "Sun, 30 Mar 2014 08:26:31 GMT"}], "update_date": "2014-04-01", "authors_parsed": [["Bouneffouf", "Djallel", ""]]}, {"id": "1303.2663", "submitter": "Allon G. Percus", "authors": "Laura M. Smith, Kristina Lerman, Cristina Garcia-Cardona, Allon G.\n  Percus, Rumi Ghosh", "title": "Spectral Clustering with Epidemic Diffusion", "comments": "6 pages, to appear in Physical Review E", "journal-ref": null, "doi": "10.1103/PhysRevE.88.042813", "report-no": null, "categories": "cs.SI cs.LG physics.soc-ph stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Spectral clustering is widely used to partition graphs into distinct modules\nor communities. Existing methods for spectral clustering use the eigenvalues\nand eigenvectors of the graph Laplacian, an operator that is closely associated\nwith random walks on graphs. We propose a new spectral partitioning method that\nexploits the properties of epidemic diffusion. An epidemic is a dynamic process\nthat, unlike the random walk, simultaneously transitions to all the neighbors\nof a given node. We show that the replicator, an operator describing epidemic\ndiffusion, is equivalent to the symmetric normalized Laplacian of a reweighted\ngraph with edges reweighted by the eigenvector centralities of their incident\nnodes. Thus, more weight is given to edges connecting more central nodes. We\ndescribe a method that partitions the nodes based on the componentwise ratio of\nthe replicator's second eigenvector to the first, and compare its performance\nto traditional spectral clustering techniques on synthetic graphs with known\ncommunity structure. We demonstrate that the replicator gives preference to\ndense, clique-like structures, enabling it to more effectively discover\ncommunities that may be obscured by dense intercommunity linking.\n", "versions": [{"version": "v1", "created": "Mon, 11 Mar 2013 20:00:32 GMT"}, {"version": "v2", "created": "Fri, 4 Oct 2013 05:12:35 GMT"}], "update_date": "2015-06-15", "authors_parsed": [["Smith", "Laura M.", ""], ["Lerman", "Kristina", ""], ["Garcia-Cardona", "Cristina", ""], ["Percus", "Allon G.", ""], ["Ghosh", "Rumi", ""]]}, {"id": "1303.2739", "submitter": "Maumita Bhattacharya", "authors": "Maumita Bhattacharya", "title": "Machine Learning for Bioclimatic Modelling", "comments": "8 pages", "journal-ref": "(IJACSA) International Journal of Advanced Computer Science and\n  Applications,Vol. 4, No. 2, 2013, pp. 1-8", "doi": null, "report-no": null, "categories": "cs.LG stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many machine learning (ML) approaches are widely used to generate bioclimatic\nmodels for prediction of geographic range of organism as a function of climate.\nApplications such as prediction of range shift in organism, range of invasive\nspecies influenced by climate change are important parameters in understanding\nthe impact of climate change. However, success of machine learning-based\napproaches depends on a number of factors. While it can be safely said that no\nparticular ML technique can be effective in all applications and success of a\ntechnique is predominantly dependent on the application or the type of the\nproblem, it is useful to understand their behavior to ensure informed choice of\ntechniques. This paper presents a comprehensive review of machine\nlearning-based bioclimatic model generation and analyses the factors\ninfluencing success of such models. Considering the wide use of statistical\ntechniques, in our discussion we also include conventional statistical\ntechniques used in bioclimatic modelling.\n", "versions": [{"version": "v1", "created": "Tue, 12 Mar 2013 01:13:44 GMT"}], "update_date": "2013-03-13", "authors_parsed": [["Bhattacharya", "Maumita", ""]]}, {"id": "1303.2789", "submitter": "Hussein Saad", "authors": "Hussein Saad, Amr Mohamed and Tamer ElBatt", "title": "A Cooperative Q-learning Approach for Real-time Power Allocation in\n  Femtocell Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MA cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we address the problem of distributed interference management\nof cognitive femtocells that share the same frequency range with macrocells\n(primary user) using distributed multi-agent Q-learning. We formulate and solve\nthree problems representing three different Q-learning algorithms: namely,\ncentralized, distributed and partially distributed power control using\nQ-learning (CPC-Q, DPC-Q and PDPC-Q). CPCQ, although not of practical interest,\ncharacterizes the global optimum. Each of DPC-Q and PDPC-Q works in two\ndifferent learning paradigms: Independent (IL) and Cooperative (CL). The former\nis considered the simplest form for applying Qlearning in multi-agent\nscenarios, where all the femtocells learn independently. The latter is the\nproposed scheme in which femtocells share partial information during the\nlearning process in order to strike a balance between practical relevance and\nperformance. In terms of performance, the simulation results showed that the CL\nparadigm outperforms the IL paradigm and achieves an aggregate femtocells\ncapacity that is very close to the optimal one. For the practical relevance\nissue, we evaluate the robustness and scalability of DPC-Q, in real time, by\ndeploying new femtocells in the system during the learning process, where we\nshowed that DPC-Q in the CL paradigm is scalable to large number of femtocells\nand more robust to the network dynamics compared to the IL paradigm\n", "versions": [{"version": "v1", "created": "Tue, 12 Mar 2013 07:00:04 GMT"}], "update_date": "2013-03-13", "authors_parsed": [["Saad", "Hussein", ""], ["Mohamed", "Amr", ""], ["ElBatt", "Tamer", ""]]}, {"id": "1303.2823", "submitter": "Steven Van Vaerenbergh", "authors": "Fernando P\\'erez-Cruz, Steven Van Vaerenbergh, Juan Jos\\'e\n  Murillo-Fuentes, Miguel L\\'azaro-Gredilla and Ignacio Santamaria", "title": "Gaussian Processes for Nonlinear Signal Processing", "comments": null, "journal-ref": "IEEE Signal Processing Magazine, vol.30, no.4, pp.40-50, July 2013", "doi": "10.1109/MSP.2013.2250352", "report-no": null, "categories": "cs.LG cs.IT math.IT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Gaussian processes (GPs) are versatile tools that have been successfully\nemployed to solve nonlinear estimation problems in machine learning, but that\nare rarely used in signal processing. In this tutorial, we present GPs for\nregression as a natural nonlinear extension to optimal Wiener filtering. After\nestablishing their basic formulation, we discuss several important aspects and\nextensions, including recursive and adaptive algorithms for dealing with\nnon-stationarity, low-complexity solutions, non-Gaussian noise models and\nclassification scenarios. Furthermore, we provide a selection of relevant\napplications to wireless digital communications.\n", "versions": [{"version": "v1", "created": "Tue, 12 Mar 2013 10:16:29 GMT"}, {"version": "v2", "created": "Fri, 27 Sep 2013 11:07:52 GMT"}], "update_date": "2013-09-30", "authors_parsed": [["P\u00e9rez-Cruz", "Fernando", ""], ["Van Vaerenbergh", "Steven", ""], ["Murillo-Fuentes", "Juan Jos\u00e9", ""], ["L\u00e1zaro-Gredilla", "Miguel", ""], ["Santamaria", "Ignacio", ""]]}, {"id": "1303.3055", "submitter": "Yasin Abbasi-Yadkori", "authors": "Yasin Abbasi-Yadkori and Peter L. Bartlett and Csaba Szepesvari", "title": "Online Learning in Markov Decision Processes with Adversarially Chosen\n  Transition Probability Distributions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of learning Markov decision processes with finite state\nand action spaces when the transition probability distributions and loss\nfunctions are chosen adversarially and are allowed to change with time. We\nintroduce an algorithm whose regret with respect to any policy in a comparison\nclass grows as the square root of the number of rounds of the game, provided\nthe transition probabilities satisfy a uniform mixing condition. Our approach\nis efficient as long as the comparison class is polynomial and we can compute\nexpectations over sample paths for each policy. Designing an efficient\nalgorithm with small regret for the general case remains an open problem.\n", "versions": [{"version": "v1", "created": "Tue, 12 Mar 2013 23:25:37 GMT"}], "update_date": "2013-03-14", "authors_parsed": [["Abbasi-Yadkori", "Yasin", ""], ["Bartlett", "Peter L.", ""], ["Szepesvari", "Csaba", ""]]}, {"id": "1303.3163", "submitter": "Kenji Kawaguchi", "authors": "Kenji Kawaguchi and Mauricio Araya", "title": "A Greedy Approximation of Bayesian Reinforcement Learning with Probably\n  Optimistic Transition Model", "comments": "the 13th International Workshop on Adaptive and Learning Agents at\n  AAMAS'13", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bayesian Reinforcement Learning (RL) is capable of not only incorporating\ndomain knowledge, but also solving the exploration-exploitation dilemma in a\nnatural way. As Bayesian RL is intractable except for special cases, previous\nwork has proposed several approximation methods. However, these methods are\nusually too sensitive to parameter values, and finding an acceptable parameter\nsetting is practically impossible in many applications. In this paper, we\npropose a new algorithm that greedily approximates Bayesian RL to achieve\nrobustness in parameter space. We show that for a desired learning behavior,\nour proposed algorithm has a polynomial sample complexity that is lower than\nthose of existing algorithms. We also demonstrate that the proposed algorithm\nnaturally outperforms other existing algorithms when the prior distributions\nare not significantly misleading. On the other hand, the proposed algorithm\ncannot handle greatly misspecified priors as well as the other algorithms can.\nThis is a natural consequence of the fact that the proposed algorithm is\ngreedier than the other algorithms. Accordingly, we discuss a way to select an\nappropriate algorithm for different tasks based on the algorithms' greediness.\nWe also introduce a new way of simplifying Bayesian planning, based on which\nfuture work would be able to derive new algorithms.\n", "versions": [{"version": "v1", "created": "Wed, 13 Mar 2013 14:06:21 GMT"}, {"version": "v2", "created": "Mon, 29 Apr 2013 03:01:40 GMT"}, {"version": "v3", "created": "Thu, 13 Jun 2013 01:04:03 GMT"}], "update_date": "2013-06-14", "authors_parsed": [["Kawaguchi", "Kenji", ""], ["Araya", "Mauricio", ""]]}, {"id": "1303.3183", "submitter": "Aivar Sootla", "authors": "Aivar Sootla, Natalja Strelkowa, Damien Ernst, Mauricio Barahona,\n  Guy-Bart Stan", "title": "Toggling a Genetic Switch Using Reinforcement Learning", "comments": "12 pages, presented at the 9th French Meeting on Planning, Decision\n  Making and Learning, Li\\`ege (Belgium), May 12-13, 2014", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SY cs.CE cs.LG q-bio.MN", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we consider the problem of optimal exogenous control of gene\nregulatory networks. Our approach consists in adapting an established\nreinforcement learning algorithm called the fitted Q iteration. This algorithm\ninfers the control law directly from the measurements of the system's response\nto external control inputs without the use of a mathematical model of the\nsystem. The measurement data set can either be collected from wet-lab\nexperiments or artificially created by computer simulations of dynamical models\nof the system. The algorithm is applicable to a wide range of biological\nsystems due to its ability to deal with nonlinear and stochastic system\ndynamics. To illustrate the application of the algorithm to a gene regulatory\nnetwork, the regulation of the toggle switch system is considered. The control\nobjective of this problem is to drive the concentrations of two specific\nproteins to a target region in the state space.\n", "versions": [{"version": "v1", "created": "Tue, 12 Mar 2013 15:34:41 GMT"}, {"version": "v2", "created": "Wed, 25 Feb 2015 13:06:00 GMT"}], "update_date": "2015-02-26", "authors_parsed": [["Sootla", "Aivar", ""], ["Strelkowa", "Natalja", ""], ["Ernst", "Damien", ""], ["Barahona", "Mauricio", ""], ["Stan", "Guy-Bart", ""]]}, {"id": "1303.3207", "submitter": "Luca Baldassarre", "authors": "Luca Baldassarre and Nirav Bhan and Volkan Cevher and Anastasios\n  Kyrillidis and Siddhartha Satpathi", "title": "Group-Sparse Model Selection: Hardness and Relaxations", "comments": "34 pages. Submitted to IEEE Trans. on Information Theory", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.IT math.IT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Group-based sparsity models are proven instrumental in linear regression\nproblems for recovering signals from much fewer measurements than standard\ncompressive sensing. The main promise of these models is the recovery of\n\"interpretable\" signals through the identification of their constituent groups.\nIn this paper, we establish a combinatorial framework for group-model selection\nproblems and highlight the underlying tractability issues. In particular, we\nshow that the group-model selection problem is equivalent to the well-known\nNP-hard weighted maximum coverage problem (WMC). Leveraging a graph-based\nunderstanding of group models, we describe group structures which enable\ncorrect model selection in polynomial time via dynamic programming.\nFurthermore, group structures that lead to totally unimodular constraints have\ntractable discrete as well as convex relaxations. We also present a\ngeneralization of the group-model that allows for within group sparsity, which\ncan be used to model hierarchical sparsity. Finally, we study the Pareto\nfrontier of group-sparse approximations for two tractable models, among which\nthe tree sparsity model, and illustrate selection and computation trade-offs\nbetween our framework and the existing convex relaxations.\n", "versions": [{"version": "v1", "created": "Wed, 13 Mar 2013 16:22:03 GMT"}, {"version": "v2", "created": "Thu, 28 Mar 2013 15:39:29 GMT"}, {"version": "v3", "created": "Tue, 2 Apr 2013 07:47:22 GMT"}, {"version": "v4", "created": "Wed, 4 Mar 2015 14:30:21 GMT"}], "update_date": "2015-03-05", "authors_parsed": [["Baldassarre", "Luca", ""], ["Bhan", "Nirav", ""], ["Cevher", "Volkan", ""], ["Kyrillidis", "Anastasios", ""], ["Satpathi", "Siddhartha", ""]]}, {"id": "1303.3240", "submitter": "Mihalis A. Nicolaou", "authors": "Mihalis A. Nicolaou, Stefanos Zafeiriou and Maja Pantic", "title": "A Unified Framework for Probabilistic Component Analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a unifying framework which reduces the construction of\nprobabilistic component analysis techniques to a mere selection of the latent\nneighbourhood, thus providing an elegant and principled framework for creating\nnovel component analysis models as well as constructing probabilistic\nequivalents of deterministic component analysis methods. Under our framework,\nwe unify many very popular and well-studied component analysis algorithms, such\nas Principal Component Analysis (PCA), Linear Discriminant Analysis (LDA),\nLocality Preserving Projections (LPP) and Slow Feature Analysis (SFA), some of\nwhich have no probabilistic equivalents in literature thus far. We firstly\ndefine the Markov Random Fields (MRFs) which encapsulate the latent\nconnectivity of the aforementioned component analysis techniques; subsequently,\nwe show that the projection directions produced by all PCA, LDA, LPP and SFA\nare also produced by the Maximum Likelihood (ML) solution of a single joint\nprobability density function, composed by selecting one of the defined MRF\npriors while utilising a simple observation model. Furthermore, we propose\nnovel Expectation Maximization (EM) algorithms, exploiting the proposed joint\nPDF, while we generalize the proposed methodologies to arbitrary connectivities\nvia parameterizable MRF products. Theoretical analysis and experiments on both\nsimulated and real world data show the usefulness of the proposed framework, by\nderiving methods which well outperform state-of-the-art equivalents.\n", "versions": [{"version": "v1", "created": "Wed, 13 Mar 2013 18:18:14 GMT"}, {"version": "v2", "created": "Fri, 14 Nov 2014 15:33:29 GMT"}], "update_date": "2014-11-17", "authors_parsed": [["Nicolaou", "Mihalis A.", ""], ["Zafeiriou", "Stefanos", ""], ["Pantic", "Maja", ""]]}, {"id": "1303.3257", "submitter": "Francesco Strino Ph.D.", "authors": "Fabio Parisi, Francesco Strino, Boaz Nadler and Yuval Kluger", "title": "Ranking and combining multiple predictors without labeled data", "comments": "Supplementary Information is included at the end of the manuscript.\n  This is a revision of our original submission of the manuscript entitled \"The\n  student's dilemma: ranking and improving prediction at test time without\n  access to training data\", which is now entitled \"Ranking and combining\n  multiple predictors without labeled data\"", "journal-ref": "Proc. Natl. Acad. Sci. U.S.A. 111 (2014) 1253-1258", "doi": "10.1073/pnas.1219097111", "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In a broad range of classification and decision making problems, one is given\nthe advice or predictions of several classifiers, of unknown reliability, over\nmultiple questions or queries. This scenario is different from the standard\nsupervised setting, where each classifier accuracy can be assessed using\navailable labeled data, and raises two questions: given only the predictions of\nseveral classifiers over a large set of unlabeled test data, is it possible to\na) reliably rank them; and b) construct a meta-classifier more accurate than\nmost classifiers in the ensemble? Here we present a novel spectral approach to\naddress these questions. First, assuming conditional independence between\nclassifiers, we show that the off-diagonal entries of their covariance matrix\ncorrespond to a rank-one matrix. Moreover, the classifiers can be ranked using\nthe leading eigenvector of this covariance matrix, as its entries are\nproportional to their balanced accuracies. Second, via a linear approximation\nto the maximum likelihood estimator, we derive the Spectral Meta-Learner (SML),\na novel ensemble classifier whose weights are equal to this eigenvector\nentries. On both simulated and real data, SML typically achieves a higher\naccuracy than most classifiers in the ensemble and can provide a better\nstarting point than majority voting, for estimating the maximum likelihood\nsolution. Furthermore, SML is robust to the presence of small malicious groups\nof classifiers designed to veer the ensemble prediction away from the (unknown)\nground truth.\n", "versions": [{"version": "v1", "created": "Wed, 13 Mar 2013 19:45:03 GMT"}, {"version": "v2", "created": "Thu, 29 Aug 2013 16:45:16 GMT"}, {"version": "v3", "created": "Sun, 24 Nov 2013 17:22:41 GMT"}], "update_date": "2014-02-07", "authors_parsed": [["Parisi", "Fabio", ""], ["Strino", "Francesco", ""], ["Nadler", "Boaz", ""], ["Kluger", "Yuval", ""]]}, {"id": "1303.3517", "submitter": "Yingyi Bu Yingyi Bu", "authors": "Joshua Rosen, Neoklis Polyzotis, Vinayak Borkar, Yingyi Bu, Michael J.\n  Carey, Markus Weimer, Tyson Condie, Raghu Ramakrishnan", "title": "Iterative MapReduce for Large Scale Machine Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DB cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Large datasets (\"Big Data\") are becoming ubiquitous because the potential\nvalue in deriving insights from data, across a wide range of business and\nscientific applications, is increasingly recognized. In particular, machine\nlearning - one of the foundational disciplines for data analysis, summarization\nand inference - on Big Data has become routine at most organizations that\noperate large clouds, usually based on systems such as Hadoop that support the\nMapReduce programming paradigm. It is now widely recognized that while\nMapReduce is highly scalable, it suffers from a critical weakness for machine\nlearning: it does not support iteration. Consequently, one has to program\naround this limitation, leading to fragile, inefficient code. Further, reliance\non the programmer is inherently flawed in a multi-tenanted cloud environment,\nsince the programmer does not have visibility into the state of the system when\nhis or her program executes. Prior work has sought to address this problem by\neither developing specialized systems aimed at stylized applications, or by\naugmenting MapReduce with ad hoc support for saving state across iterations\n(driven by an external loop). In this paper, we advocate support for looping as\na first-class construct, and propose an extension of the MapReduce programming\nparadigm called {\\em Iterative MapReduce}. We then develop an optimizer for a\nclass of Iterative MapReduce programs that cover most machine learning\ntechniques, provide theoretical justifications for the key optimization steps,\nand empirically demonstrate that system-optimized programs for significant\nmachine learning tasks are competitive with state-of-the-art specialized\nsolutions.\n", "versions": [{"version": "v1", "created": "Wed, 13 Mar 2013 04:24:12 GMT"}], "update_date": "2013-03-15", "authors_parsed": [["Rosen", "Joshua", ""], ["Polyzotis", "Neoklis", ""], ["Borkar", "Vinayak", ""], ["Bu", "Yingyi", ""], ["Carey", "Michael J.", ""], ["Weimer", "Markus", ""], ["Condie", "Tyson", ""], ["Ramakrishnan", "Raghu", ""]]}, {"id": "1303.3605", "submitter": "Adheen Ajay", "authors": "Adheen Ajay and D. Venkataraman", "title": "A survey on sensing methods and feature extraction algorithms for SLAM\n  problem", "comments": "5 pages, 1 figure,2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  This paper is a survey work for a bigger project for designing a Visual SLAM\nrobot to generate 3D dense map of an unknown unstructured environment. A lot of\nfactors have to be considered while designing a SLAM robot. Sensing method of\nthe SLAM robot should be determined by considering the kind of environment to\nbe modeled. Similarly the type of environment determines the suitable feature\nextraction method. This paper goes through the sensing methods used in some\nrecently published papers. The main objective of this survey is to conduct a\ncomparative study among the current sensing methods and feature extraction\nalgorithms and to extract out the best for our work.\n", "versions": [{"version": "v1", "created": "Thu, 14 Mar 2013 20:51:29 GMT"}], "update_date": "2013-03-18", "authors_parsed": [["Ajay", "Adheen", ""], ["Venkataraman", "D.", ""]]}, {"id": "1303.3632", "submitter": "Nikzad Babaii-Rizvandi", "authors": "Nikzad Babaii Rizvandi, Javid Taheri, Reza Moraveji, Albert Y. Zomaya", "title": "Statistical Regression to Predict Total Cumulative CPU Usage of\n  MapReduce Jobs", "comments": "16 pages- previously published as \"On Modelling and Prediction of\n  Total CPU Usage for Applications in MapReduce Enviornments\" in IEEE 12th\n  International Conference on Algorithms and Architectures for Parallel\n  Processing (ICA3PP-12), Fukuoka, Japan, 4-7 September, 2012", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.LG cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, businesses have started using MapReduce as a popular computation\nframework for processing large amount of data, such as spam detection, and\ndifferent data mining tasks, in both public and private clouds. Two of the\nchallenging questions in such environments are (1) choosing suitable values for\nMapReduce configuration parameters e.g., number of mappers, number of reducers,\nand DFS block size, and (2) predicting the amount of resources that a user\nshould lease from the service provider. Currently, the tasks of both choosing\nconfiguration parameters and estimating required resources are solely the users\nresponsibilities. In this paper, we present an approach to provision the total\nCPU usage in clock cycles of jobs in MapReduce environment. For a MapReduce\njob, a profile of total CPU usage in clock cycles is built from the job past\nexecutions with different values of two configuration parameters e.g., number\nof mappers, and number of reducers. Then, a polynomial regression is used to\nmodel the relation between these configuration parameters and total CPU usage\nin clock cycles of the job. We also briefly study the influence of input data\nscaling on measured total CPU usage in clock cycles. This derived model along\nwith the scaling result can then be used to provision the total CPU usage in\nclock cycles of the same jobs with different input data size. We validate the\naccuracy of our models using three realistic applications (WordCount, Exim\nMainLog parsing, and TeraSort). Results show that the predicted total CPU usage\nin clock cycles of generated resource provisioning options are less than 8% of\nthe measured total CPU usage in clock cycles in our 20-node virtual Hadoop\ncluster.\n", "versions": [{"version": "v1", "created": "Thu, 14 Mar 2013 22:40:32 GMT"}], "update_date": "2013-03-18", "authors_parsed": [["Rizvandi", "Nikzad Babaii", ""], ["Taheri", "Javid", ""], ["Moraveji", "Reza", ""], ["Zomaya", "Albert Y.", ""]]}, {"id": "1303.3664", "submitter": "Weicong Ding", "authors": "Weicong Ding, Mohammad H. Rohban, Prakash Ishwar, Venkatesh Saligrama", "title": "Topic Discovery through Data Dependent and Random Projections", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present algorithms for topic modeling based on the geometry of\ncross-document word-frequency patterns. This perspective gains significance\nunder the so called separability condition. This is a condition on existence of\nnovel-words that are unique to each topic. We present a suite of highly\nefficient algorithms based on data-dependent and random projections of\nword-frequency patterns to identify novel words and associated topics. We will\nalso discuss the statistical guarantees of the data-dependent projections\nmethod based on two mild assumptions on the prior density of topic document\nmatrix. Our key insight here is that the maximum and minimum values of\ncross-document frequency patterns projected along any direction are associated\nwith novel words. While our sample complexity bounds for topic recovery are\nsimilar to the state-of-art, the computational complexity of our random\nprojection scheme scales linearly with the number of documents and the number\nof words per document. We present several experiments on synthetic and\nreal-world datasets to demonstrate qualitative and quantitative merits of our\nscheme.\n", "versions": [{"version": "v1", "created": "Fri, 15 Mar 2013 02:37:19 GMT"}, {"version": "v2", "created": "Mon, 18 Mar 2013 13:11:02 GMT"}], "update_date": "2013-03-19", "authors_parsed": [["Ding", "Weicong", ""], ["Rohban", "Mohammad H.", ""], ["Ishwar", "Prakash", ""], ["Saligrama", "Venkatesh", ""]]}, {"id": "1303.3716", "submitter": "Reinhard Heckel", "authors": "Reinhard Heckel and Helmut B\\\"olcskei", "title": "Subspace Clustering via Thresholding and Spectral Clustering", "comments": "ICASSP 2013", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.LG math.IT math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of clustering a set of high-dimensional data points\ninto sets of low-dimensional linear subspaces. The number of subspaces, their\ndimensions, and their orientations are unknown. We propose a simple and\nlow-complexity clustering algorithm based on thresholding the correlations\nbetween the data points followed by spectral clustering. A probabilistic\nperformance analysis shows that this algorithm succeeds even when the subspaces\nintersect, and when the dimensions of the subspaces scale (up to a log-factor)\nlinearly in the ambient dimension. Moreover, we prove that the algorithm also\nsucceeds for data points that are subject to erasures with the number of\nerasures scaling (up to a log-factor) linearly in the ambient dimension.\nFinally, we propose a simple scheme that provably detects outliers.\n", "versions": [{"version": "v1", "created": "Fri, 15 Mar 2013 09:52:54 GMT"}], "update_date": "2013-03-18", "authors_parsed": [["Heckel", "Reinhard", ""], ["B\u00f6lcskei", "Helmut", ""]]}, {"id": "1303.3754", "submitter": "Edward Moroshko", "authors": "Edward Moroshko, Koby Crammer", "title": "A Last-Step Regression Algorithm for Non-Stationary Online Learning", "comments": "arXiv admin note: substantial text overlap with arXiv:1303.0140", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The goal of a learner in standard online learning is to maintain an average\nloss close to the loss of the best-performing single function in some class. In\nmany real-world problems, such as rating or ranking items, there is no single\nbest target function during the runtime of the algorithm, instead the best\n(local) target function is drifting over time. We develop a novel last-step\nminmax optimal algorithm in context of a drift. We analyze the algorithm in the\nworst-case regret framework and show that it maintains an average loss close to\nthat of the best slowly changing sequence of linear functions, as long as the\ntotal of drift is sublinear. In some situations, our bound improves over\nexisting bounds, and additionally the algorithm suffers logarithmic regret when\nthere is no drift. We also build on the H_infinity filter and its bound, and\ndevelop and analyze a second algorithm for drifting setting. Synthetic\nsimulations demonstrate the advantages of our algorithms in a worst-case\nconstant drift setting.\n", "versions": [{"version": "v1", "created": "Fri, 15 Mar 2013 12:20:53 GMT"}], "update_date": "2013-03-18", "authors_parsed": [["Moroshko", "Edward", ""], ["Crammer", "Koby", ""]]}, {"id": "1303.3934", "submitter": "Feng Tan", "authors": "Feng Tan and Jean-Jacques Slotine", "title": "A Quorum Sensing Inspired Algorithm for Dynamic Clustering", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Quorum sensing is a decentralized biological process, through which a\ncommunity of cells with no global awareness coordinate their functional\nbehaviors based solely on cell-medium interactions and local decisions. This\npaper draws inspirations from quorum sensing and colony competition to derive a\nnew algorithm for data clustering. The algorithm treats each data as a single\ncell, and uses knowledge of local connectivity to cluster cells into multiple\ncolonies simultaneously. It simulates auto-inducers secretion in quorum sensing\nto tune the influence radius for each cell. At the same time, sparsely\ndistributed core cells spread their influences to form colonies, and\ninteractions between colonies eventually determine each cell's identity. The\nalgorithm has the flexibility to analyze not only static but also time-varying\ndata, which surpasses the capacity of many existing algorithms. Its stability\nand convergence properties are established. The algorithm is tested on several\napplications, including both synthetic and real benchmarks data sets, alleles\nclustering, community detection, image segmentation. In particular, the\nalgorithm's distinctive capability to deal with time-varying data allows us to\nexperiment it on novel applications such as robotic swarms grouping and\nswitching model identification. We believe that the algorithm's promising\nperformance would stimulate many more exciting applications.\n", "versions": [{"version": "v1", "created": "Sat, 16 Mar 2013 00:49:56 GMT"}, {"version": "v2", "created": "Tue, 6 Oct 2015 21:12:52 GMT"}], "update_date": "2015-10-08", "authors_parsed": [["Tan", "Feng", ""], ["Slotine", "Jean-Jacques", ""]]}, {"id": "1303.3987", "submitter": "Liping  Wang", "authors": "Liping Wang and Songcan Chen", "title": "$l_{2,p}$ Matrix Norm and Its Application in Feature Selection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, $l_{2,1}$ matrix norm has been widely applied to many areas such as\ncomputer vision, pattern recognition, biological study and etc. As an extension\nof $l_1$ vector norm, the mixed $l_{2,1}$ matrix norm is often used to find\njointly sparse solutions. Moreover, an efficient iterative algorithm has been\ndesigned to solve $l_{2,1}$-norm involved minimizations. Actually,\ncomputational studies have showed that $l_p$-regularization ($0<p<1$) is\nsparser than $l_1$-regularization, but the extension to matrix norm has been\nseldom considered. This paper presents a definition of mixed $l_{2,p}$ $(p\\in\n(0, 1])$ matrix pseudo norm which is thought as both generalizations of $l_p$\nvector norm to matrix and $l_{2,1}$-norm to nonconvex cases $(0<p<1)$.\nFortunately, an efficient unified algorithm is proposed to solve the induced\n$l_{2,p}$-norm $(p\\in (0, 1])$ optimization problems. The convergence can also\nbe uniformly demonstrated for all $p\\in (0, 1]$. Typical $p\\in (0,1]$ are\napplied to select features in computational biology and the experimental\nresults show that some choices of $0<p<1$ do improve the sparse pattern of\nusing $p=1$.\n", "versions": [{"version": "v1", "created": "Sat, 16 Mar 2013 15:06:12 GMT"}], "update_date": "2013-03-19", "authors_parsed": [["Wang", "Liping", ""], ["Chen", "Songcan", ""]]}, {"id": "1303.4015", "submitter": "Sokol Koco", "authors": "Sokol Ko\\c{c}o (LIF), C\\'ecile Capponi (LIF)", "title": "On multi-class learning through the minimization of the confusion matrix\n  norm", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In imbalanced multi-class classification problems, the misclassification rate\nas an error measure may not be a relevant choice. Several methods have been\ndeveloped where the performance measure retained richer information than the\nmere misclassification rate: misclassification costs, ROC-based information,\netc. Following this idea of dealing with alternate measures of performance, we\npropose to address imbalanced classification problems by using a new measure to\nbe optimized: the norm of the confusion matrix. Indeed, recent results show\nthat using the norm of the confusion matrix as an error measure can be quite\ninteresting due to the fine-grain informations contained in the matrix,\nespecially in the case of imbalanced classes. Our first contribution then\nconsists in showing that optimizing criterion based on the confusion matrix\ngives rise to a common background for cost-sensitive methods aimed at dealing\nwith imbalanced classes learning problems. As our second contribution, we\npropose an extension of a recent multi-class boosting method --- namely\nAdaBoost.MM --- to the imbalanced class problem, by greedily minimizing the\nempirical norm of the confusion matrix. A theoretical analysis of the\nproperties of the proposed method is presented, while experimental results\nillustrate the behavior of the algorithm and show the relevancy of the approach\ncompared to other methods.\n", "versions": [{"version": "v1", "created": "Sat, 16 Mar 2013 20:09:16 GMT"}, {"version": "v2", "created": "Sun, 3 Nov 2013 10:25:48 GMT"}], "update_date": "2013-11-05", "authors_parsed": [["Ko\u00e7o", "Sokol", "", "LIF"], ["Capponi", "C\u00e9cile", "", "LIF"]]}, {"id": "1303.4169", "submitter": "Makiko Konoshima", "authors": "Yui Noma, Makiko Konoshima", "title": "Markov Chain Monte Carlo for Arrangement of Hyperplanes in\n  Locality-Sensitive Hashing", "comments": "13 pages, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Since Hamming distances can be calculated by bitwise computations, they can\nbe calculated with less computational load than L2 distances. Similarity\nsearches can therefore be performed faster in Hamming distance space. The\nelements of Hamming distance space are bit strings. On the other hand, the\narrangement of hyperplanes induce the transformation from the feature vectors\ninto feature bit strings. This transformation method is a type of\nlocality-sensitive hashing that has been attracting attention as a way of\nperforming approximate similarity searches at high speed. Supervised learning\nof hyperplane arrangements allows us to obtain a method that transforms them\ninto feature bit strings reflecting the information of labels applied to\nhigher-dimensional feature vectors. In this p aper, we propose a supervised\nlearning method for hyperplane arrangements in feature space that uses a Markov\nchain Monte Carlo (MCMC) method. We consider the probability density functions\nused during learning, and evaluate their performance. We also consider the\nsampling method for learning data pairs needed in learning, and we evaluate its\nperformance. We confirm that the accuracy of this learning method when using a\nsuitable probability density function and sampling method is greater than the\naccuracy of existing learning methods.\n", "versions": [{"version": "v1", "created": "Mon, 18 Mar 2013 07:14:15 GMT"}], "update_date": "2013-03-19", "authors_parsed": [["Noma", "Yui", ""], ["Konoshima", "Makiko", ""]]}, {"id": "1303.4172", "submitter": "Matus Telgarsky", "authors": "Matus Telgarsky", "title": "Margins, Shrinkage, and Boosting", "comments": "To appear, ICML 2013", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This manuscript shows that AdaBoost and its immediate variants can produce\napproximate maximum margin classifiers simply by scaling step size choices with\na fixed small constant. In this way, when the unscaled step size is an optimal\nchoice, these results provide guarantees for Friedman's empirically successful\n\"shrinkage\" procedure for gradient boosting (Friedman, 2000). Guarantees are\nalso provided for a variety of other step sizes, affirming the intuition that\nincreasingly regularized line searches provide improved margin guarantees. The\nresults hold for the exponential loss and similar losses, most notably the\nlogistic loss.\n", "versions": [{"version": "v1", "created": "Mon, 18 Mar 2013 07:33:29 GMT"}], "update_date": "2013-03-19", "authors_parsed": [["Telgarsky", "Matus", ""]]}, {"id": "1303.4207", "submitter": "Shusen Wang", "authors": "Shusen Wang, Zhihua Zhang", "title": "Improving CUR Matrix Decomposition and the Nystr\\\"{o}m Approximation via\n  Adaptive Sampling", "comments": null, "journal-ref": "Journal of Machine Learning Research, 14: 2549-2589, 2013", "doi": null, "report-no": null, "categories": "cs.LG cs.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The CUR matrix decomposition and the Nystr\\\"{o}m approximation are two\nimportant low-rank matrix approximation techniques. The Nystr\\\"{o}m method\napproximates a symmetric positive semidefinite matrix in terms of a small\nnumber of its columns, while CUR approximates an arbitrary data matrix by a\nsmall number of its columns and rows. Thus, CUR decomposition can be regarded\nas an extension of the Nystr\\\"{o}m approximation.\n  In this paper we establish a more general error bound for the adaptive\ncolumn/row sampling algorithm, based on which we propose more accurate CUR and\nNystr\\\"{o}m algorithms with expected relative-error bounds. The proposed CUR\nand Nystr\\\"{o}m algorithms also have low time complexity and can avoid\nmaintaining the whole data matrix in RAM. In addition, we give theoretical\nanalysis for the lower error bounds of the standard Nystr\\\"{o}m method and the\nensemble Nystr\\\"{o}m method. The main theoretical results established in this\npaper are novel, and our analysis makes no special assumption on the data\nmatrices.\n", "versions": [{"version": "v1", "created": "Mon, 18 Mar 2013 11:17:55 GMT"}, {"version": "v2", "created": "Wed, 20 Mar 2013 04:20:45 GMT"}, {"version": "v3", "created": "Tue, 9 Apr 2013 08:28:43 GMT"}, {"version": "v4", "created": "Tue, 28 May 2013 07:17:40 GMT"}, {"version": "v5", "created": "Fri, 31 May 2013 05:12:41 GMT"}, {"version": "v6", "created": "Thu, 12 Sep 2013 06:56:13 GMT"}, {"version": "v7", "created": "Tue, 1 Oct 2013 06:31:11 GMT"}], "update_date": "2013-10-02", "authors_parsed": [["Wang", "Shusen", ""], ["Zhang", "Zhihua", ""]]}, {"id": "1303.4434", "submitter": "Pinghua Gong", "authors": "Pinghua Gong, Changshui Zhang, Zhaosong Lu, Jianhua Huang, Jieping Ye", "title": "A General Iterative Shrinkage and Thresholding Algorithm for Non-convex\n  Regularized Optimization Problems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NA stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Non-convex sparsity-inducing penalties have recently received considerable\nattentions in sparse learning. Recent theoretical investigations have\ndemonstrated their superiority over the convex counterparts in several sparse\nlearning settings. However, solving the non-convex optimization problems\nassociated with non-convex penalties remains a big challenge. A commonly used\napproach is the Multi-Stage (MS) convex relaxation (or DC programming), which\nrelaxes the original non-convex problem to a sequence of convex problems. This\napproach is usually not very practical for large-scale problems because its\ncomputational cost is a multiple of solving a single convex problem. In this\npaper, we propose a General Iterative Shrinkage and Thresholding (GIST)\nalgorithm to solve the nonconvex optimization problem for a large class of\nnon-convex penalties. The GIST algorithm iteratively solves a proximal operator\nproblem, which in turn has a closed-form solution for many commonly used\npenalties. At each outer iteration of the algorithm, we use a line search\ninitialized by the Barzilai-Borwein (BB) rule that allows finding an\nappropriate step size quickly. The paper also presents a detailed convergence\nanalysis of the GIST algorithm. The efficiency of the proposed algorithm is\ndemonstrated by extensive experiments on large-scale data sets.\n", "versions": [{"version": "v1", "created": "Mon, 18 Mar 2013 21:41:53 GMT"}], "update_date": "2013-03-20", "authors_parsed": [["Gong", "Pinghua", ""], ["Zhang", "Changshui", ""], ["Lu", "Zhaosong", ""], ["Huang", "Jianhua", ""], ["Ye", "Jieping", ""]]}, {"id": "1303.4638", "submitter": "Xianfu Chen", "authors": "Xianfu Chen, Honggang Zhang, Tao Chen, Mika Lasanen, and Jacques\n  Palicot", "title": "On Improving Energy Efficiency within Green Femtocell Networks: A\n  Hierarchical Reinforcement Learning Approach", "comments": "arXiv admin note: substantial text overlap with arXiv:1209.2790", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.GT", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  One of the efficient solutions of improving coverage and increasing capacity\nin cellular networks is the deployment of femtocells. As the cellular networks\nare becoming more complex, energy consumption of whole network infrastructure\nis becoming important in terms of both operational costs and environmental\nimpacts. This paper investigates energy efficiency of two-tier femtocell\nnetworks through combining game theory and stochastic learning. With the\nStackelberg game formulation, a hierarchical reinforcement learning framework\nis applied for studying the joint expected utility maximization of macrocells\nand femtocells subject to the minimum signal-to-interference-plus-noise-ratio\nrequirements. In the learning procedure, the macrocells act as leaders and the\nfemtocells are followers. At each time step, the leaders commit to dynamic\nstrategies based on the best responses of the followers, while the followers\ncompete against each other with no further information but the leaders'\ntransmission parameters. In this paper, we propose two reinforcement learning\nbased intelligent algorithms to schedule each cell's stochastic power levels.\nNumerical experiments are presented to validate the investigations. The results\nshow that the two learning algorithms substantially improve the energy\nefficiency of the femtocell networks.\n", "versions": [{"version": "v1", "created": "Wed, 13 Mar 2013 10:22:09 GMT"}], "update_date": "2013-03-20", "authors_parsed": [["Chen", "Xianfu", ""], ["Zhang", "Honggang", ""], ["Chen", "Tao", ""], ["Lasanen", "Mika", ""], ["Palicot", "Jacques", ""]]}, {"id": "1303.4664", "submitter": "Hugh Brendan McMahan", "authors": "Daniel Golovin, D. Sculley, H. Brendan McMahan, Michael Young", "title": "Large-Scale Learning with Less RAM via Randomization", "comments": "Extended version of ICML 2013 paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We reduce the memory footprint of popular large-scale online learning methods\nby projecting our weight vector onto a coarse discrete set using randomized\nrounding. Compared to standard 32-bit float encodings, this reduces RAM usage\nby more than 50% during training and by up to 95% when making predictions from\na fixed model, with almost no loss in accuracy. We also show that randomized\ncounting can be used to implement per-coordinate learning rates, improving\nmodel quality with little additional RAM. We prove these memory-saving methods\nachieve regret guarantees similar to their exact variants. Empirical evaluation\nconfirms excellent performance, dominating standard approaches across memory\nversus accuracy tradeoffs.\n", "versions": [{"version": "v1", "created": "Tue, 19 Mar 2013 17:00:22 GMT"}], "update_date": "2013-03-20", "authors_parsed": [["Golovin", "Daniel", ""], ["Sculley", "D.", ""], ["McMahan", "H. Brendan", ""], ["Young", "Michael", ""]]}, {"id": "1303.4694", "submitter": "Jayaraman J. Thiagarajan", "authors": "Karthikeyan Natesan Ramamurthy, Jayaraman J. Thiagarajan and Andreas\n  Spanias", "title": "Recovering Non-negative and Combined Sparse Representations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.NA cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The non-negative solution to an underdetermined linear system can be uniquely\nrecovered sometimes, even without imposing any additional sparsity constraints.\nIn this paper, we derive conditions under which a unique non-negative solution\nfor such a system can exist, based on the theory of polytopes. Furthermore, we\ndevelop the paradigm of combined sparse representations, where only a part of\nthe coefficient vector is constrained to be non-negative, and the rest is\nunconstrained (general). We analyze the recovery of the unique, sparsest\nsolution, for combined representations, under three different cases of\ncoefficient support knowledge: (a) the non-zero supports of non-negative and\ngeneral coefficients are known, (b) the non-zero support of general\ncoefficients alone is known, and (c) both the non-zero supports are unknown.\nFor case (c), we propose the combined orthogonal matching pursuit algorithm for\ncoefficient recovery and derive the deterministic sparsity threshold under\nwhich recovery of the unique, sparsest coefficient vector is possible. We\nquantify the order complexity of the algorithms, and examine their performance\nin exact and approximate recovery of coefficients under various conditions of\nnoise. Furthermore, we also obtain their empirical phase transition\ncharacteristics. We show that the basis pursuit algorithm, with partial\nnon-negative constraints, and the proposed greedy algorithm perform better in\nrecovering the unique sparse representation when compared to their\nunconstrained counterparts. Finally, we demonstrate the utility of the proposed\nmethods in recovering images corrupted by saturation noise.\n", "versions": [{"version": "v1", "created": "Tue, 12 Mar 2013 04:33:14 GMT"}, {"version": "v2", "created": "Fri, 20 Sep 2013 20:22:33 GMT"}], "update_date": "2013-09-24", "authors_parsed": [["Ramamurthy", "Karthikeyan Natesan", ""], ["Thiagarajan", "Jayaraman J.", ""], ["Spanias", "Andreas", ""]]}, {"id": "1303.4756", "submitter": "Zhaoshi Meng", "authors": "Zhaoshi Meng, Dennis Wei, Ami Wiesel, Alfred O. Hero III", "title": "Marginal Likelihoods for Distributed Parameter Estimation of Gaussian\n  Graphical Models", "comments": null, "journal-ref": null, "doi": "10.1109/TSP.2014.2350956", "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider distributed estimation of the inverse covariance matrix, also\ncalled the concentration or precision matrix, in Gaussian graphical models.\nTraditional centralized estimation often requires global inference of the\ncovariance matrix, which can be computationally intensive in large dimensions.\nApproximate inference based on message-passing algorithms, on the other hand,\ncan lead to unstable and biased estimation in loopy graphical models. In this\npaper, we propose a general framework for distributed estimation based on a\nmaximum marginal likelihood (MML) approach. This approach computes local\nparameter estimates by maximizing marginal likelihoods defined with respect to\ndata collected from local neighborhoods. Due to the non-convexity of the MML\nproblem, we introduce and solve a convex relaxation. The local estimates are\nthen combined into a global estimate without the need for iterative\nmessage-passing between neighborhoods. The proposed algorithm is naturally\nparallelizable and computationally efficient, thereby making it suitable for\nhigh-dimensional problems. In the classical regime where the number of\nvariables $p$ is fixed and the number of samples $T$ increases to infinity, the\nproposed estimator is shown to be asymptotically consistent and to improve\nmonotonically as the local neighborhood size increases. In the high-dimensional\nscaling regime where both $p$ and $T$ increase to infinity, the convergence\nrate to the true parameters is derived and is seen to be comparable to\ncentralized maximum likelihood estimation. Extensive numerical experiments\ndemonstrate the improved performance of the two-hop version of the proposed\nestimator, which suffices to almost close the gap to the centralized maximum\nlikelihood estimator at a reduced computational cost.\n", "versions": [{"version": "v1", "created": "Tue, 19 Mar 2013 20:34:47 GMT"}, {"version": "v2", "created": "Sun, 19 Jan 2014 18:58:21 GMT"}, {"version": "v3", "created": "Fri, 16 May 2014 22:14:49 GMT"}, {"version": "v4", "created": "Tue, 3 Jun 2014 21:05:14 GMT"}, {"version": "v5", "created": "Tue, 24 Jun 2014 16:49:58 GMT"}, {"version": "v6", "created": "Wed, 13 Aug 2014 16:16:16 GMT"}], "update_date": "2015-06-15", "authors_parsed": [["Meng", "Zhaoshi", ""], ["Wei", "Dennis", ""], ["Wiesel", "Ami", ""], ["Hero", "Alfred O.", "III"]]}, {"id": "1303.4778", "submitter": "Eva Dyer", "authors": "Eva L. Dyer, Aswin C. Sankaranarayanan, Richard G. Baraniuk", "title": "Greedy Feature Selection for Subspace Clustering", "comments": "32 pages, 7 figures, 1 table", "journal-ref": "Journal of Machine Learning Research, Vol.14, Issue 1, pp.\n  2487-2517, January 2013", "doi": null, "report-no": null, "categories": "cs.LG math.NA stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Unions of subspaces provide a powerful generalization to linear subspace\nmodels for collections of high-dimensional data. To learn a union of subspaces\nfrom a collection of data, sets of signals in the collection that belong to the\nsame subspace must be identified in order to obtain accurate estimates of the\nsubspace structures present in the data. Recently, sparse recovery methods have\nbeen shown to provide a provable and robust strategy for exact feature\nselection (EFS)--recovering subsets of points from the ensemble that live in\nthe same subspace. In parallel with recent studies of EFS with L1-minimization,\nin this paper, we develop sufficient conditions for EFS with a greedy method\nfor sparse signal recovery known as orthogonal matching pursuit (OMP).\nFollowing our analysis, we provide an empirical study of feature selection\nstrategies for signals living on unions of subspaces and characterize the gap\nbetween sparse recovery methods and nearest neighbor (NN)-based approaches. In\nparticular, we demonstrate that sparse recovery methods provide significant\nadvantages over NN methods and the gap between the two approaches is\nparticularly pronounced when the sampling of subspaces in the dataset is\nsparse. Our results suggest that OMP may be employed to reliably recover exact\nfeature sets in a number of regimes where NN approaches fail to reveal the\nsubspace membership of points in the ensemble.\n", "versions": [{"version": "v1", "created": "Tue, 19 Mar 2013 22:17:20 GMT"}, {"version": "v2", "created": "Wed, 3 Jul 2013 19:07:34 GMT"}], "update_date": "2016-10-28", "authors_parsed": [["Dyer", "Eva L.", ""], ["Sankaranarayanan", "Aswin C.", ""], ["Baraniuk", "Richard G.", ""]]}, {"id": "1303.5145", "submitter": "Karthik Mohan", "authors": "Karthik Mohan, Palma London, Maryam Fazel, Daniela Witten, Su-In Lee", "title": "Node-Based Learning of Multiple Gaussian Graphical Models", "comments": "42 pages, 16 figures. Accepted to JMLR, 2014", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of estimating high-dimensional Gaussian graphical\nmodels corresponding to a single set of variables under several distinct\nconditions. This problem is motivated by the task of recovering transcriptional\nregulatory networks on the basis of gene expression data {containing\nheterogeneous samples, such as different disease states, multiple species, or\ndifferent developmental stages}. We assume that most aspects of the conditional\ndependence networks are shared, but that there are some structured differences\nbetween them. Rather than assuming that similarities and differences between\nnetworks are driven by individual edges, we take a node-based approach, which\nin many cases provides a more intuitive interpretation of the network\ndifferences. We consider estimation under two distinct assumptions: (1)\ndifferences between the K networks are due to individual nodes that are\nperturbed across conditions, or (2) similarities among the K networks are due\nto the presence of common hub nodes that are shared across all K networks.\nUsing a row-column overlap norm penalty function, we formulate two convex\noptimization problems that correspond to these two assumptions. We solve these\nproblems using an alternating direction method of multipliers algorithm, and we\nderive a set of necessary and sufficient conditions that allows us to decompose\nthe problem into independent subproblems so that our algorithm can be scaled to\nhigh-dimensional settings. Our proposal is illustrated on synthetic data, a\nwebpage data set, and a brain cancer gene expression data set.\n", "versions": [{"version": "v1", "created": "Thu, 21 Mar 2013 02:10:10 GMT"}, {"version": "v2", "created": "Sat, 23 Mar 2013 05:31:35 GMT"}, {"version": "v3", "created": "Wed, 21 Aug 2013 04:14:09 GMT"}, {"version": "v4", "created": "Wed, 22 Jan 2014 21:30:33 GMT"}], "update_date": "2014-01-24", "authors_parsed": [["Mohan", "Karthik", ""], ["London", "Palma", ""], ["Fazel", "Maryam", ""], ["Witten", "Daniela", ""], ["Lee", "Su-In", ""]]}, {"id": "1303.5148", "submitter": "Mark Dredze", "authors": "Damianos Karakos and Mark Dredze and Sanjeev Khudanpur", "title": "Estimating Confusions in the ASR Channel for Improved Topic-based\n  Language Model Adaptation", "comments": "Technical Report 8, Human Language Technology Center of Excellence,\n  Johns Hopkins University", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Human language is a combination of elemental languages/domains/styles that\nchange across and sometimes within discourses. Language models, which play a\ncrucial role in speech recognizers and machine translation systems, are\nparticularly sensitive to such changes, unless some form of adaptation takes\nplace. One approach to speech language model adaptation is self-training, in\nwhich a language model's parameters are tuned based on automatically\ntranscribed audio. However, transcription errors can misguide self-training,\nparticularly in challenging settings such as conversational speech. In this\nwork, we propose a model that considers the confusions (errors) of the ASR\nchannel. By modeling the likely confusions in the ASR output instead of using\njust the 1-best, we improve self-training efficacy by obtaining a more reliable\nreference transcription estimate. We demonstrate improved topic-based language\nmodeling adaptation results over both 1-best and lattice self-training using\nour ASR channel confusion estimates on telephone conversations.\n", "versions": [{"version": "v1", "created": "Thu, 21 Mar 2013 02:56:43 GMT"}], "update_date": "2013-03-22", "authors_parsed": [["Karakos", "Damianos", ""], ["Dredze", "Mark", ""], ["Khudanpur", "Sanjeev", ""]]}, {"id": "1303.5244", "submitter": "Martin Kleinsteuber", "authors": "Simon Hawe, Matthias Seibert, and Martin Kleinsteuber", "title": "Separable Dictionary Learning", "comments": "12 pages, 2 figures, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many techniques in computer vision, machine learning, and statistics rely on\nthe fact that a signal of interest admits a sparse representation over some\ndictionary. Dictionaries are either available analytically, or can be learned\nfrom a suitable training set. While analytic dictionaries permit to capture the\nglobal structure of a signal and allow a fast implementation, learned\ndictionaries often perform better in applications as they are more adapted to\nthe considered class of signals. In imagery, unfortunately, the numerical\nburden for (i) learning a dictionary and for (ii) employing the dictionary for\nreconstruction tasks only allows to deal with relatively small image patches\nthat only capture local image information. The approach presented in this paper\naims at overcoming these drawbacks by allowing a separable structure on the\ndictionary throughout the learning process. On the one hand, this permits\nlarger patch-sizes for the learning phase, on the other hand, the dictionary is\napplied efficiently in reconstruction tasks. The learning procedure is based on\noptimizing over a product of spheres which updates the dictionary as a whole,\nthus enforces basic dictionary properties such as mutual coherence explicitly\nduring the learning procedure. In the special case where no separable structure\nis enforced, our method competes with state-of-the-art dictionary learning\nmethods like K-SVD.\n", "versions": [{"version": "v1", "created": "Thu, 21 Mar 2013 12:40:05 GMT"}], "update_date": "2013-03-22", "authors_parsed": [["Hawe", "Simon", ""], ["Seibert", "Matthias", ""], ["Kleinsteuber", "Martin", ""]]}, {"id": "1303.5403", "submitter": "Dan Geiger", "authors": "Dan Geiger", "title": "An Entropy-based Learning Algorithm of Bayesian Conditional Trees", "comments": "Appears in Proceedings of the Eighth Conference on Uncertainty in\n  Artificial Intelligence (UAI1992)", "journal-ref": null, "doi": null, "report-no": "UAI-P-1992-PG-92-97", "categories": "cs.LG cs.AI cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article offers a modification of Chow and Liu's learning algorithm in\nthe context of handwritten digit recognition. The modified algorithm directs\nthe user to group digits into several classes consisting of digits that are\nhard to distinguish and then constructing an optimal conditional tree\nrepresentation for each class of digits instead of for each single digit as\ndone by Chow and Liu (1968). Advantages and extensions of the new method are\ndiscussed. Related works of Wong and Wang (1977) and Wong and Poon (1989) which\noffer a different entropy-based learning algorithm are shown to rest on\ninappropriate assumptions.\n", "versions": [{"version": "v1", "created": "Wed, 13 Mar 2013 12:52:37 GMT"}], "update_date": "2013-03-25", "authors_parsed": [["Geiger", "Dan", ""]]}, {"id": "1303.5508", "submitter": "George Chen", "authors": "George H. Chen, Christian Wachinger, Polina Golland", "title": "Sparse Projections of Medical Images onto Manifolds", "comments": "International Conference on Information Processing in Medical Imaging\n  (IPMI 2013)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Manifold learning has been successfully applied to a variety of medical\nimaging problems. Its use in real-time applications requires fast projection\nonto the low-dimensional space. To this end, out-of-sample extensions are\napplied by constructing an interpolation function that maps from the input\nspace to the low-dimensional manifold. Commonly used approaches such as the\nNystr\\\"{o}m extension and kernel ridge regression require using all training\npoints. We propose an interpolation function that only depends on a small\nsubset of the input training data. Consequently, in the testing phase each new\npoint only needs to be compared against a small number of input training data\nin order to project the point onto the low-dimensional space. We interpret our\nmethod as an out-of-sample extension that approximates kernel ridge regression.\nOur method involves solving a simple convex optimization problem and has the\nattractive property of guaranteeing an upper bound on the approximation error,\nwhich is crucial for medical applications. Tuning this error bound controls the\nsparsity of the resulting interpolation function. We illustrate our method in\ntwo clinical applications that require fast mapping of input images onto a\nlow-dimensional space.\n", "versions": [{"version": "v1", "created": "Fri, 22 Mar 2013 03:24:10 GMT"}, {"version": "v2", "created": "Thu, 28 Mar 2013 19:21:33 GMT"}], "update_date": "2013-03-29", "authors_parsed": [["Chen", "George H.", ""], ["Wachinger", "Christian", ""], ["Golland", "Polina", ""]]}, {"id": "1303.5613", "submitter": "Steven Smith", "authors": "Steven T. Smith, Kenneth D. Senne, Scott Philips, Edward K. Kao, and\n  Garrett Bernstein", "title": "Network Detection Theory and Performance", "comments": "Submitted to IEEE Trans. Signal Processing", "journal-ref": "IEEE Trans. Signal Process., vol. 62, no. 20, pp. 5324-5338,\n  October 2014", "doi": "10.1109/TSP.2014.2336613", "report-no": null, "categories": "cs.SI cs.LG math.ST physics.soc-ph stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Network detection is an important capability in many areas of applied\nresearch in which data can be represented as a graph of entities and\nrelationships. Oftentimes the object of interest is a relatively small subgraph\nin an enormous, potentially uninteresting background. This aspect characterizes\nnetwork detection as a \"big data\" problem. Graph partitioning and network\ndiscovery have been major research areas over the last ten years, driven by\ninterest in internet search, cyber security, social networks, and criminal or\nterrorist activities. The specific problem of network discovery is addressed as\na special case of graph partitioning in which membership in a small subgraph of\ninterest must be determined. Algebraic graph theory is used as the basis to\nanalyze and compare different network detection methods. A new Bayesian network\ndetection framework is introduced that partitions the graph based on prior\ninformation and direct observations. The new approach, called space-time threat\npropagation, is proved to maximize the probability of detection and is\ntherefore optimum in the Neyman-Pearson sense. This optimality criterion is\ncompared to spectral community detection approaches which divide the global\ngraph into subsets or communities with optimal connectivity properties. We also\nexplore a new generative stochastic model for covert networks and analyze using\nreceiver operating characteristics the detection performance of both classes of\noptimal detection techniques.\n", "versions": [{"version": "v1", "created": "Fri, 22 Mar 2013 13:34:28 GMT"}], "update_date": "2018-04-12", "authors_parsed": [["Smith", "Steven T.", ""], ["Senne", "Kenneth D.", ""], ["Philips", "Scott", ""], ["Kao", "Edward K.", ""], ["Bernstein", "Garrett", ""]]}, {"id": "1303.5685", "submitter": "Andrew Lan", "authors": "Andrew S. Lan, Andrew E. Waters, Christoph Studer and Richard G.\n  Baraniuk", "title": "Sparse Factor Analysis for Learning and Content Analytics", "comments": null, "journal-ref": "Journal of Machine Learning Research, vol. 15, pp. 1959-2008,\n  June, 2014", "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.OC stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop a new model and algorithms for machine learning-based learning\nanalytics, which estimate a learner's knowledge of the concepts underlying a\ndomain, and content analytics, which estimate the relationships among a\ncollection of questions and those concepts. Our model represents the\nprobability that a learner provides the correct response to a question in terms\nof three factors: their understanding of a set of underlying concepts, the\nconcepts involved in each question, and each question's intrinsic difficulty.\nWe estimate these factors given the graded responses to a collection of\nquestions. The underlying estimation problem is ill-posed in general,\nespecially when only a subset of the questions are answered. The key\nobservation that enables a well-posed solution is the fact that typical\neducational domains of interest involve only a small number of key concepts.\nLeveraging this observation, we develop both a bi-convex maximum-likelihood and\na Bayesian solution to the resulting SPARse Factor Analysis (SPARFA) problem.\nWe also incorporate user-defined tags on questions to facilitate the\ninterpretability of the estimated factors. Experiments with synthetic and\nreal-world data demonstrate the efficacy of our approach. Finally, we make a\nconnection between SPARFA and noisy, binary-valued (1-bit) dictionary learning\nthat is of independent interest.\n", "versions": [{"version": "v1", "created": "Fri, 22 Mar 2013 18:44:56 GMT"}, {"version": "v2", "created": "Fri, 19 Jul 2013 20:33:18 GMT"}], "update_date": "2015-01-20", "authors_parsed": [["Lan", "Andrew S.", ""], ["Waters", "Andrew E.", ""], ["Studer", "Christoph", ""], ["Baraniuk", "Richard G.", ""]]}, {"id": "1303.5913", "submitter": "Marcus Chen", "authors": "Marcus Chen, Cham Tat Jen, Pang Sze Kim, Alvina Goh", "title": "A Diffusion Process on Riemannian Manifold for Visual Tracking", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.RO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Robust visual tracking for long video sequences is a research area that has\nmany important applications. The main challenges include how the target image\ncan be modeled and how this model can be updated. In this paper, we model the\ntarget using a covariance descriptor, as this descriptor is robust to problems\nsuch as pixel-pixel misalignment, pose and illumination changes, that commonly\noccur in visual tracking. We model the changes in the template using a\ngenerative process. We introduce a new dynamical model for the template update\nusing a random walk on the Riemannian manifold where the covariance descriptors\nlie in. This is done using log-transformed space of the manifold to free the\nconstraints imposed inherently by positive semidefinite matrices. Modeling\ntemplate variations and poses kinetics together in the state space enables us\nto jointly quantify the uncertainties relating to the kinematic states and the\ntemplate in a principled way. Finally, the sequential inference of the\nposterior distribution of the kinematic states and the template is done using a\nparticle filter. Our results shows that this principled approach can be robust\nto changes in illumination, poses and spatial affine transformation. In the\nexperiments, our method outperformed the current state-of-the-art algorithm -\nthe incremental Principal Component Analysis method, particularly when a target\nunderwent fast poses changes and also maintained a comparable performance in\nstable target tracking cases.\n", "versions": [{"version": "v1", "created": "Sun, 24 Mar 2013 04:55:40 GMT"}], "update_date": "2013-03-26", "authors_parsed": [["Chen", "Marcus", ""], ["Jen", "Cham Tat", ""], ["Kim", "Pang Sze", ""], ["Goh", "Alvina", ""]]}, {"id": "1303.5976", "submitter": "Lorenzo Rosasco", "authors": "Silvia Villa, Lorenzo Rosasco and Tomaso Poggio", "title": "On Learnability, Complexity and Stability", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the fundamental question of learnability of a hypotheses class in\nthe supervised learning setting and in the general learning setting introduced\nby Vladimir Vapnik. We survey classic results characterizing learnability in\nterm of suitable notions of complexity, as well as more recent results that\nestablish the connection between learnability and stability of a learning\nalgorithm.\n", "versions": [{"version": "v1", "created": "Sun, 24 Mar 2013 18:32:38 GMT"}], "update_date": "2013-03-26", "authors_parsed": [["Villa", "Silvia", ""], ["Rosasco", "Lorenzo", ""], ["Poggio", "Tomaso", ""]]}, {"id": "1303.5984", "submitter": "Adel Javanmard", "authors": "Morteza Ibrahimi and Adel Javanmard and Benjamin Van Roy", "title": "Efficient Reinforcement Learning for High Dimensional Linear Quadratic\n  Systems", "comments": "16 pages", "journal-ref": "Advances in Neural Information Processing Systems (NIPS) 2012:\n  2645-2653", "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of adaptive control of a high dimensional linear\nquadratic (LQ) system. Previous work established the asymptotic convergence to\nan optimal controller for various adaptive control schemes. More recently, for\nthe average cost LQ problem, a regret bound of ${O}(\\sqrt{T})$ was shown, apart\nform logarithmic factors. However, this bound scales exponentially with $p$,\nthe dimension of the state space. In this work we consider the case where the\nmatrices describing the dynamic of the LQ system are sparse and their\ndimensions are large. We present an adaptive control scheme that achieves a\nregret bound of ${O}(p \\sqrt{T})$, apart from logarithmic factors. In\nparticular, our algorithm has an average cost of $(1+\\eps)$ times the optimum\ncost after $T = \\polylog(p) O(1/\\eps^2)$. This is in comparison to previous\nwork on the dense dynamics where the algorithm requires time that scales\nexponentially with dimension in order to achieve regret of $\\eps$ times the\noptimal cost.\n  We believe that our result has prominent applications in the emerging area of\ncomputational advertising, in particular targeted online advertising and\nadvertising in social networks.\n", "versions": [{"version": "v1", "created": "Sun, 24 Mar 2013 19:56:49 GMT"}], "update_date": "2013-03-26", "authors_parsed": [["Ibrahimi", "Morteza", ""], ["Javanmard", "Adel", ""], ["Van Roy", "Benjamin", ""]]}, {"id": "1303.6001", "submitter": "Bal\\'azs Szalkai", "authors": "Bal\\'azs Szalkai", "title": "Generalizing k-means for an arbitrary distance matrix", "comments": "3 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The original k-means clustering method works only if the exact vectors\nrepresenting the data points are known. Therefore calculating the distances\nfrom the centroids needs vector operations, since the average of abstract data\npoints is undefined. Existing algorithms can be extended for those cases when\nthe sole input is the distance matrix, and the exact representing vectors are\nunknown. This extension may be named relational k-means after a notation for a\nsimilar algorithm invented for fuzzy clustering. A method is then proposed for\ngeneralizing k-means for scenarios when the data points have absolutely no\nconnection with a Euclidean space.\n", "versions": [{"version": "v1", "created": "Sun, 24 Mar 2013 22:33:15 GMT"}], "update_date": "2013-03-26", "authors_parsed": [["Szalkai", "Bal\u00e1zs", ""]]}, {"id": "1303.6086", "submitter": "Massimiliano Pontil", "authors": "Andreas Argyriou, Luca Baldassarre, Charles A. Micchelli, Massimiliano\n  Pontil", "title": "On Sparsity Inducing Regularization Methods for Machine Learning", "comments": "12 pages. arXiv admin note: text overlap with arXiv:1104.1436", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  During the past years there has been an explosion of interest in learning\nmethods based on sparsity regularization. In this paper, we discuss a general\nclass of such methods, in which the regularizer can be expressed as the\ncomposition of a convex function $\\omega$ with a linear function. This setting\nincludes several methods such the group Lasso, the Fused Lasso, multi-task\nlearning and many more. We present a general approach for solving\nregularization problems of this kind, under the assumption that the proximity\noperator of the function $\\omega$ is available. Furthermore, we comment on the\napplication of this approach to support vector machines, a technique pioneered\nby the groundbreaking work of Vladimir Vapnik.\n", "versions": [{"version": "v1", "created": "Mon, 25 Mar 2013 11:09:08 GMT"}], "update_date": "2013-03-26", "authors_parsed": [["Argyriou", "Andreas", ""], ["Baldassarre", "Luca", ""], ["Micchelli", "Charles A.", ""], ["Pontil", "Massimiliano", ""]]}, {"id": "1303.6149", "submitter": "Francis Bach", "authors": "Francis Bach (INRIA Paris - Rocquencourt, LIENS)", "title": "Adaptivity of averaged stochastic gradient descent to local strong\n  convexity for logistic regression", "comments": null, "journal-ref": "Journal of Machine Learning Research 15 (2014) 595-627", "doi": null, "report-no": null, "categories": "math.ST cs.LG math.OC stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we consider supervised learning problems such as logistic\nregression and study the stochastic gradient method with averaging, in the\nusual stochastic approximation setting where observations are used only once.\nWe show that after $N$ iterations, with a constant step-size proportional to\n$1/R^2 \\sqrt{N}$ where $N$ is the number of observations and $R$ is the maximum\nnorm of the observations, the convergence rate is always of order\n$O(1/\\sqrt{N})$, and improves to $O(R^2 / \\mu N)$ where $\\mu$ is the lowest\neigenvalue of the Hessian at the global optimum (when this eigenvalue is\ngreater than $R^2/\\sqrt{N}$). Since $\\mu$ does not need to be known in advance,\nthis shows that averaged stochastic gradient is adaptive to \\emph{unknown\nlocal} strong convexity of the objective function. Our proof relies on the\ngeneralized self-concordance properties of the logistic loss and thus extends\nto all generalized linear models with uniformly bounded features.\n", "versions": [{"version": "v1", "created": "Mon, 25 Mar 2013 14:53:33 GMT"}, {"version": "v2", "created": "Sat, 26 Oct 2013 18:14:11 GMT"}, {"version": "v3", "created": "Sun, 16 Mar 2014 06:25:08 GMT"}], "update_date": "2014-03-18", "authors_parsed": [["Bach", "Francis", "", "INRIA Paris - Rocquencourt, LIENS"]]}, {"id": "1303.6163", "submitter": "Juan Nunez-Iglesias", "authors": "Juan Nunez-Iglesias, Ryan Kennedy, Toufiq Parag, Jianbo Shi, Dmitri B.\n  Chklovskii", "title": "Machine learning of hierarchical clustering to segment 2D and 3D images", "comments": "15 pages, 8 figures", "journal-ref": "PLoS ONE, 2013, 8(8): e71715", "doi": "10.1371/journal.pone.0071715", "report-no": null, "categories": "cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  We aim to improve segmentation through the use of machine learning tools\nduring region agglomeration. We propose an active learning approach for\nperforming hierarchical agglomerative segmentation from superpixels. Our method\ncombines multiple features at all scales of the agglomerative process, works\nfor data with an arbitrary number of dimensions, and scales to very large\ndatasets. We advocate the use of variation of information to measure\nsegmentation accuracy, particularly in 3D electron microscopy (EM) images of\nneural tissue, and using this metric demonstrate an improvement over competing\nalgorithms in EM and natural images.\n", "versions": [{"version": "v1", "created": "Mon, 25 Mar 2013 15:20:09 GMT"}, {"version": "v2", "created": "Mon, 13 May 2013 17:37:05 GMT"}, {"version": "v3", "created": "Tue, 23 Jul 2013 11:15:25 GMT"}], "update_date": "2015-03-13", "authors_parsed": [["Nunez-Iglesias", "Juan", ""], ["Kennedy", "Ryan", ""], ["Parag", "Toufiq", ""], ["Shi", "Jianbo", ""], ["Chklovskii", "Dmitri B.", ""]]}, {"id": "1303.6370", "submitter": "Ryota Tomioka", "authors": "Ryota Tomioka, Taiji Suzuki", "title": "Convex Tensor Decomposition via Structured Schatten Norm Regularization", "comments": "12 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG cs.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We discuss structured Schatten norms for tensor decomposition that includes\ntwo recently proposed norms (\"overlapped\" and \"latent\") for\nconvex-optimization-based tensor decomposition, and connect tensor\ndecomposition with wider literature on structured sparsity. Based on the\nproperties of the structured Schatten norms, we mathematically analyze the\nperformance of \"latent\" approach for tensor decomposition, which was\nempirically found to perform better than the \"overlapped\" approach in some\nsettings. We show theoretically that this is indeed the case. In particular,\nwhen the unknown true tensor is low-rank in a specific mode, this approach\nperforms as good as knowing the mode with the smallest rank. Along the way, we\nshow a novel duality result for structures Schatten norms, establish the\nconsistency, and discuss the identifiability of this approach. We confirm\nthrough numerical simulations that our theoretical prediction can precisely\npredict the scaling behavior of the mean squared error.\n", "versions": [{"version": "v1", "created": "Tue, 26 Mar 2013 02:36:49 GMT"}], "update_date": "2013-03-27", "authors_parsed": [["Tomioka", "Ryota", ""], ["Suzuki", "Taiji", ""]]}, {"id": "1303.6390", "submitter": "Matthew Blaschko", "authors": "Matthew Blaschko (INRIA Saclay - Ile de France, CVN)", "title": "A Note on k-support Norm Regularized Risk Minimization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The k-support norm has been recently introduced to perform correlated\nsparsity regularization. Although Argyriou et al. only reported experiments\nusing squared loss, here we apply it to several other commonly used settings\nresulting in novel machine learning algorithms with interesting and familiar\nlimit cases. Source code for the algorithms described here is available.\n", "versions": [{"version": "v1", "created": "Tue, 26 Mar 2013 06:01:34 GMT"}, {"version": "v2", "created": "Wed, 27 Mar 2013 16:23:48 GMT"}], "update_date": "2013-03-28", "authors_parsed": [["Blaschko", "Matthew", "", "INRIA Saclay - Ile de France, CVN"]]}, {"id": "1303.6746", "submitter": "Matthew W. Hoffman", "authors": "Matthew W. Hoffman, Bobak Shahriari, Nando de Freitas", "title": "Exploiting correlation and budget constraints in Bayesian multi-armed\n  bandit optimization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We address the problem of finding the maximizer of a nonlinear smooth\nfunction, that can only be evaluated point-wise, subject to constraints on the\nnumber of permitted function evaluations. This problem is also known as\nfixed-budget best arm identification in the multi-armed bandit literature. We\nintroduce a Bayesian approach for this problem and show that it empirically\noutperforms both the existing frequentist counterpart and other Bayesian\noptimization methods. The Bayesian approach places emphasis on detailed\nmodelling, including the modelling of correlations among the arms. As a result,\nit can perform well in situations where the number of arms is much larger than\nthe number of allowed function evaluation, whereas the frequentist counterpart\nis inapplicable. This feature enables us to develop and deploy practical\napplications, such as automatic machine learning toolboxes. The paper presents\ncomprehensive comparisons of the proposed approach, Thompson sampling,\nclassical Bayesian optimization techniques, more recent Bayesian bandit\napproaches, and state-of-the-art best arm identification methods. This is the\nfirst comparison of many of these methods in the literature and allows us to\nexamine the relative merits of their different features.\n", "versions": [{"version": "v1", "created": "Wed, 27 Mar 2013 06:17:09 GMT"}, {"version": "v2", "created": "Tue, 18 Jun 2013 21:17:52 GMT"}, {"version": "v3", "created": "Fri, 8 Nov 2013 12:10:07 GMT"}, {"version": "v4", "created": "Mon, 11 Nov 2013 10:52:24 GMT"}], "update_date": "2013-11-12", "authors_parsed": [["Hoffman", "Matthew W.", ""], ["Shahriari", "Bobak", ""], ["de Freitas", "Nando", ""]]}, {"id": "1303.6750", "submitter": "Gaurav Thakur", "authors": "Gaurav Thakur", "title": "Sequential testing over multiple stages and performance analysis of data\n  fusion", "comments": "SPIE Signal Processing, Sensor Fusion and Target Recognition XXII", "journal-ref": "Proc. SPIE Vol 8745, 87450S (2013)", "doi": "10.1117/12.2017754", "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe a methodology for modeling the performance of decision-level data\nfusion between different sensor configurations, implemented as part of the\nJIEDDO Analytic Decision Engine (JADE). We first discuss a Bayesian network\nformulation of classical probabilistic data fusion, which allows elementary\nfusion structures to be stacked and analyzed efficiently. We then present an\nextension of the Wald sequential test for combining the outputs of the Bayesian\nnetwork over time. We discuss an algorithm to compute its performance\nstatistics and illustrate the approach on some examples. This variant of the\nsequential test involves multiple, distinct stages, where the evidence\naccumulated from each stage is carried over into the next one, and is motivated\nby a need to keep certain sensors in the network inactive unless triggered by\nother sensors.\n", "versions": [{"version": "v1", "created": "Wed, 27 Mar 2013 06:53:26 GMT"}], "update_date": "2013-06-26", "authors_parsed": [["Thakur", "Gaurav", ""]]}, {"id": "1303.6935", "submitter": "Xiaocheng  Tang", "authors": "Xiaocheng Tang and Katya Scheinberg", "title": "Efficiently Using Second Order Information in Large l1 Regularization\n  Problems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel general algorithm LHAC that efficiently uses second-order\ninformation to train a class of large-scale l1-regularized problems. Our method\nexecutes cheap iterations while achieving fast local convergence rate by\nexploiting the special structure of a low-rank matrix, constructed via\nquasi-Newton approximation of the Hessian of the smooth loss function. A greedy\nactive-set strategy, based on the largest violations in the dual constraints,\nis employed to maintain a working set that iteratively estimates the complement\nof the optimal active set. This allows for smaller size of subproblems and\neventually identifies the optimal active set. Empirical comparisons confirm\nthat LHAC is highly competitive with several recently proposed state-of-the-art\nspecialized solvers for sparse logistic regression and sparse inverse\ncovariance matrix selection.\n", "versions": [{"version": "v1", "created": "Wed, 27 Mar 2013 19:34:05 GMT"}], "update_date": "2013-03-28", "authors_parsed": [["Tang", "Xiaocheng", ""], ["Scheinberg", "Katya", ""]]}, {"id": "1303.6977", "submitter": "Christos Dimitrakakis", "authors": "Christos Dimitrakakis, Nikolaos Tziortziotis", "title": "ABC Reinforcement Learning", "comments": "Corrected version of paper appearing in ICML 2013", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces a simple, general framework for likelihood-free\nBayesian reinforcement learning, through Approximate Bayesian Computation\n(ABC). The main advantage is that we only require a prior distribution on a\nclass of simulators (generative models). This is useful in domains where an\nanalytical probabilistic model of the underlying process is too complex to\nformulate, but where detailed simulation models are available. ABC-RL allows\nthe use of any Bayesian reinforcement learning technique, even in this case. In\naddition, it can be seen as an extension of rollout algorithms to the case\nwhere we do not know what the correct model to draw rollouts from is. We\nexperimentally demonstrate the potential of this approach in a comparison with\nLSPI. Finally, we introduce a theorem showing that ABC is a sound methodology\nin principle, even when non-sufficient statistics are used.\n", "versions": [{"version": "v1", "created": "Wed, 27 Mar 2013 20:51:33 GMT"}, {"version": "v2", "created": "Wed, 8 May 2013 12:54:53 GMT"}, {"version": "v3", "created": "Tue, 18 Jun 2013 09:42:59 GMT"}, {"version": "v4", "created": "Fri, 28 Jun 2013 11:18:26 GMT"}], "update_date": "2013-07-01", "authors_parsed": [["Dimitrakakis", "Christos", ""], ["Tziortziotis", "Nikolaos", ""]]}, {"id": "1303.7043", "submitter": "Chunhua Shen", "authors": "Fumin Shen, Chunhua Shen, Qinfeng Shi, Anton van den Hengel, Zhenmin\n  Tang", "title": "Inductive Hashing on Manifolds", "comments": "Appearing in IEEE Conf. Computer Vision and Pattern Recognition, 2013", "journal-ref": null, "doi": "10.1109/CVPR.2013.205", "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning based hashing methods have attracted considerable attention due to\ntheir ability to greatly increase the scale at which existing algorithms may\noperate. Most of these methods are designed to generate binary codes that\npreserve the Euclidean distance in the original space. Manifold learning\ntechniques, in contrast, are better able to model the intrinsic structure\nembedded in the original high-dimensional data. The complexity of these models,\nand the problems with out-of-sample data, have previously rendered them\nunsuitable for application to large-scale embedding, however. In this work, we\nconsider how to learn compact binary embeddings on their intrinsic manifolds.\nIn order to address the above-mentioned difficulties, we describe an efficient,\ninductive solution to the out-of-sample data problem, and a process by which\nnon-parametric manifold learning may be used as the basis of a hashing method.\nOur proposed approach thus allows the development of a range of new hashing\ntechniques exploiting the flexibility of the wide variety of manifold learning\napproaches available. We particularly show that hashing on the basis of t-SNE .\n", "versions": [{"version": "v1", "created": "Thu, 28 Mar 2013 05:45:21 GMT"}], "update_date": "2016-11-17", "authors_parsed": [["Shen", "Fumin", ""], ["Shen", "Chunhua", ""], ["Shi", "Qinfeng", ""], ["Hengel", "Anton van den", ""], ["Tang", "Zhenmin", ""]]}, {"id": "1303.7093", "submitter": "Aravind Kota Gopalakrishna", "authors": "Aravind Kota Gopalakrishna, Tanir Ozcelebi, Antonio Liotta, Johan J.\n  Lukkien", "title": "Relevance As a Metric for Evaluating Machine Learning Algorithms", "comments": "To Appear at International Conference on Machine Learning and Data\n  Mining (MLDM 2013), 14 pages, 6 figures", "journal-ref": null, "doi": "10.1007/978-3-642-39712-7_15", "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In machine learning, the choice of a learning algorithm that is suitable for\nthe application domain is critical. The performance metric used to compare\ndifferent algorithms must also reflect the concerns of users in the application\ndomain under consideration. In this work, we propose a novel probability-based\nperformance metric called Relevance Score for evaluating supervised learning\nalgorithms. We evaluate the proposed metric through empirical analysis on a\ndataset gathered from an intelligent lighting pilot installation. In comparison\nto the commonly used Classification Accuracy metric, the Relevance Score proves\nto be more appropriate for a certain class of applications.\n", "versions": [{"version": "v1", "created": "Thu, 28 Mar 2013 11:01:53 GMT"}, {"version": "v2", "created": "Fri, 5 Apr 2013 19:12:06 GMT"}, {"version": "v3", "created": "Mon, 8 Apr 2013 14:26:49 GMT"}], "update_date": "2013-07-19", "authors_parsed": [["Gopalakrishna", "Aravind Kota", ""], ["Ozcelebi", "Tanir", ""], ["Liotta", "Antonio", ""], ["Lukkien", "Johan J.", ""]]}, {"id": "1303.7117", "submitter": "Brittany Terese Fasy", "authors": "Brittany Terese Fasy, Fabrizio Lecci, Alessandro Rinaldo, Larry\n  Wasserman, Sivaraman Balakrishnan, Aarti Singh", "title": "Confidence sets for persistence diagrams", "comments": "Published in at http://dx.doi.org/10.1214/14-AOS1252 the Annals of\n  Statistics (http://www.imstat.org/aos/) by the Institute of Mathematical\n  Statistics (http://www.imstat.org)", "journal-ref": "Annals of Statistics 2014, Vol. 42, No. 6, 2301-2339", "doi": "10.1214/14-AOS1252", "report-no": "IMS-AOS-AOS1252", "categories": "math.ST cs.CG cs.LG stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Persistent homology is a method for probing topological properties of point\nclouds and functions. The method involves tracking the birth and death of\ntopological features (2000) as one varies a tuning parameter. Features with\nshort lifetimes are informally considered to be \"topological noise,\" and those\nwith a long lifetime are considered to be \"topological signal.\" In this paper,\nwe bring some statistical ideas to persistent homology. In particular, we\nderive confidence sets that allow us to separate topological signal from\ntopological noise.\n", "versions": [{"version": "v1", "created": "Thu, 28 Mar 2013 12:59:00 GMT"}, {"version": "v2", "created": "Fri, 7 Feb 2014 16:36:57 GMT"}, {"version": "v3", "created": "Thu, 20 Nov 2014 08:16:51 GMT"}], "update_date": "2014-11-21", "authors_parsed": [["Fasy", "Brittany Terese", ""], ["Lecci", "Fabrizio", ""], ["Rinaldo", "Alessandro", ""], ["Wasserman", "Larry", ""], ["Balakrishnan", "Sivaraman", ""], ["Singh", "Aarti", ""]]}, {"id": "1303.7226", "submitter": "Yudong Chen", "authors": "Yudong Chen, Vikas Kawadia, Rahul Urgaonkar", "title": "Detecting Overlapping Temporal Community Structure in Time-Evolving\n  Networks", "comments": "12 pages, 15 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.LG physics.soc-ph stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a principled approach for detecting overlapping temporal community\nstructure in dynamic networks. Our method is based on the following framework:\nfind the overlapping temporal community structure that maximizes a quality\nfunction associated with each snapshot of the network subject to a temporal\nsmoothness constraint. A novel quality function and a smoothness constraint are\nproposed to handle overlaps, and a new convex relaxation is used to solve the\nresulting combinatorial optimization problem. We provide theoretical guarantees\nas well as experimental results that reveal community structure in real and\nsynthetic networks. Our main insight is that certain structures can be\nidentified only when temporal correlation is considered and when communities\nare allowed to overlap. In general, discovering such overlapping temporal\ncommunity structure can enhance our understanding of real-world complex\nnetworks by revealing the underlying stability behind their seemingly chaotic\nevolution.\n", "versions": [{"version": "v1", "created": "Thu, 28 Mar 2013 19:56:39 GMT"}], "update_date": "2013-03-29", "authors_parsed": [["Chen", "Yudong", ""], ["Kawadia", "Vikas", ""], ["Urgaonkar", "Rahul", ""]]}, {"id": "1303.7264", "submitter": "Yaojia Zhu", "authors": "Yaojia Zhu, Xiaoran Yan, Lise Getoor and Cristopher Moore", "title": "Scalable Text and Link Analysis with Mixed-Topic Link Models", "comments": "11 pages, 4 figures", "journal-ref": "Proc. 19th SIGKDD Conference on Knowledge Discovery and Data\n  Mining (KDD) 2013, 473-481", "doi": "10.1145/2487575.2487693", "report-no": null, "categories": "cs.LG cs.IR cs.SI physics.data-an stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many data sets contain rich information about objects, as well as pairwise\nrelations between them. For instance, in networks of websites, scientific\npapers, and other documents, each node has content consisting of a collection\nof words, as well as hyperlinks or citations to other nodes. In order to\nperform inference on such data sets, and make predictions and recommendations,\nit is useful to have models that are able to capture the processes which\ngenerate the text at each node and the links between them. In this paper, we\ncombine classic ideas in topic modeling with a variant of the mixed-membership\nblock model recently developed in the statistical physics community. The\nresulting model has the advantage that its parameters, including the mixture of\ntopics of each document and the resulting overlapping communities, can be\ninferred with a simple and scalable expectation-maximization algorithm. We test\nour model on three data sets, performing unsupervised topic classification and\nlink prediction. For both tasks, our model outperforms several existing\nstate-of-the-art methods, achieving higher accuracy with significantly less\ncomputation, analyzing a data set with 1.3 million words and 44 thousand links\nin a few minutes.\n", "versions": [{"version": "v1", "created": "Thu, 28 Mar 2013 22:34:51 GMT"}], "update_date": "2014-10-30", "authors_parsed": [["Zhu", "Yaojia", ""], ["Yan", "Xiaoran", ""], ["Getoor", "Lise", ""], ["Moore", "Cristopher", ""]]}, {"id": "1303.7286", "submitter": "Frank Nielsen", "authors": "Frank Nielsen", "title": "On the symmetrical Kullback-Leibler Jeffreys centroids", "comments": "17 pages, 1 figure, source code in R", "journal-ref": "IEEE Signal Processing Letters (Volume:20 , Issue: 7 ), pp.\n  657-660, 2013", "doi": "10.1109/LSP.2013.2260538", "report-no": null, "categories": "cs.IT cs.LG math.IT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Due to the success of the bag-of-word modeling paradigm, clustering\nhistograms has become an important ingredient of modern information processing.\nClustering histograms can be performed using the celebrated $k$-means\ncentroid-based algorithm. From the viewpoint of applications, it is usually\nrequired to deal with symmetric distances. In this letter, we consider the\nJeffreys divergence that symmetrizes the Kullback-Leibler divergence, and\ninvestigate the computation of Jeffreys centroids. We first prove that the\nJeffreys centroid can be expressed analytically using the Lambert $W$ function\nfor positive histograms. We then show how to obtain a fast guaranteed\napproximation when dealing with frequency histograms. Finally, we conclude with\nsome remarks on the $k$-means histogram clustering.\n", "versions": [{"version": "v1", "created": "Fri, 29 Mar 2013 03:11:21 GMT"}, {"version": "v2", "created": "Thu, 25 Apr 2013 06:01:08 GMT"}, {"version": "v3", "created": "Wed, 22 Jan 2014 05:35:12 GMT"}], "update_date": "2014-01-23", "authors_parsed": [["Nielsen", "Frank", ""]]}, {"id": "1303.7461", "submitter": "Guido F.  Montufar", "authors": "Guido F. Mont\\'ufar", "title": "Universal Approximation Depth and Errors of Narrow Belief Networks with\n  Discrete Units", "comments": "19 pages, 5 figures, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.PR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We generalize recent theoretical work on the minimal number of layers of\nnarrow deep belief networks that can approximate any probability distribution\non the states of their visible units arbitrarily well. We relax the setting of\nbinary units (Sutskever and Hinton, 2008; Le Roux and Bengio, 2008, 2010;\nMont\\'ufar and Ay, 2011) to units with arbitrary finite state spaces, and the\nvanishing approximation error to an arbitrary approximation error tolerance.\nFor example, we show that a $q$-ary deep belief network with $L\\geq\n2+\\frac{q^{\\lceil m-\\delta \\rceil}-1}{q-1}$ layers of width $n \\leq m +\n\\log_q(m) + 1$ for some $m\\in \\mathbb{N}$ can approximate any probability\ndistribution on $\\{0,1,\\ldots,q-1\\}^n$ without exceeding a Kullback-Leibler\ndivergence of $\\delta$. Our analysis covers discrete restricted Boltzmann\nmachines and na\\\"ive Bayes models as special cases.\n", "versions": [{"version": "v1", "created": "Fri, 29 Mar 2013 19:15:04 GMT"}, {"version": "v2", "created": "Tue, 28 Jan 2014 21:50:07 GMT"}], "update_date": "2014-01-30", "authors_parsed": [["Mont\u00fafar", "Guido F.", ""]]}, {"id": "1303.7474", "submitter": "Matthew Anderson", "authors": "Matthew Anderson, Geng-Shen Fu, Ronald Phlypo, and T\\\"ulay Adal{\\i}", "title": "Independent Vector Analysis: Identification Conditions and Performance\n  Bounds", "comments": "14 pages, 5 figures, in review for IEEE Trans. on Signal Processing", "journal-ref": null, "doi": "10.1109/TSP.2014.2333554", "report-no": null, "categories": "cs.LG cs.IT math.IT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, an extension of independent component analysis (ICA) from one to\nmultiple datasets, termed independent vector analysis (IVA), has been the\nsubject of significant research interest. IVA has also been shown to be a\ngeneralization of Hotelling's canonical correlation analysis. In this paper, we\nprovide the identification conditions for a general IVA formulation, which\naccounts for linear, nonlinear, and sample-to-sample dependencies. The\nidentification conditions are a generalization of previous results for ICA and\nfor IVA when samples are independently and identically distributed.\nFurthermore, a principal aim of IVA is the identification of dependent sources\nbetween datasets. Thus, we provide the additional conditions for when the\narbitrary ordering of the sources within each dataset is common. Performance\nbounds in terms of the Cramer-Rao lower bound are also provided for the\ndemixing matrices and interference to source ratio. The performance of two IVA\nalgorithms are compared to the theoretical bounds.\n", "versions": [{"version": "v1", "created": "Fri, 29 Mar 2013 19:52:31 GMT"}], "update_date": "2016-08-11", "authors_parsed": [["Anderson", "Matthew", ""], ["Fu", "Geng-Shen", ""], ["Phlypo", "Ronald", ""], ["Adal\u0131", "T\u00fclay", ""]]}]