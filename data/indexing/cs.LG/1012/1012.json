[{"id": "1012.0498", "submitter": "Mingxuan Sun", "authors": "Mingxuan Sun, Guy Lebanon, Paul Kidwell", "title": "Estimating Probabilities in Recommendation Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recommendation systems are emerging as an important business application with\nsignificant economic impact. Currently popular systems include Amazon's book\nrecommendations, Netflix's movie recommendations, and Pandora's music\nrecommendations. In this paper we address the problem of estimating\nprobabilities associated with recommendation system data using non-parametric\nkernel smoothing. In our estimation we interpret missing items as randomly\ncensored observations and obtain efficient computation schemes using\ncombinatorial properties of generating functions. We demonstrate our approach\nwith several case studies involving real world movie recommendation data. The\nresults are comparable with state-of-the-art techniques while also providing\nprobabilistic preference estimates outside the scope of traditional recommender\nsystems.\n", "versions": [{"version": "v1", "created": "Thu, 2 Dec 2010 17:04:19 GMT"}], "update_date": "2010-12-03", "authors_parsed": [["Sun", "Mingxuan", ""], ["Lebanon", "Guy", ""], ["Kidwell", "Paul", ""]]}, {"id": "1012.0729", "submitter": "Prasad Raghavendra", "authors": "Vitaly Feldman, Venkatesan Guruswami, Prasad Raghavendra, Yi Wu", "title": "Agnostic Learning of Monomials by Halfspaces is Hard", "comments": "37 pages, Preliminary version appeared in FOCS 2009", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We prove the following strong hardness result for learning: Given a\ndistribution of labeled examples from the hypercube such that there exists a\nmonomial consistent with $(1-\\eps)$ of the examples, it is NP-hard to find a\nhalfspace that is correct on $(1/2+\\eps)$ of the examples, for arbitrary\nconstants $\\eps > 0$. In learning theory terms, weak agnostic learning of\nmonomials is hard, even if one is allowed to output a hypothesis from the much\nbigger concept class of halfspaces. This hardness result subsumes a long line\nof previous results, including two recent hardness results for the proper\nlearning of monomials and halfspaces. As an immediate corollary of our result\nwe show that weak agnostic learning of decision lists is NP-hard.\n  Our techniques are quite different from previous hardness proofs for\nlearning. We define distributions on positive and negative examples for\nmonomials whose first few moments match. We use the invariance principle to\nargue that regular halfspaces (all of whose coefficients have small absolute\nvalue relative to the total $\\ell_2$ norm) cannot distinguish between\ndistributions whose first few moments match. For highly non-regular subspaces,\nwe use a structural lemma from recent work on fooling halfspaces to argue that\nthey are ``junta-like'' and one can zero out all but the top few coefficients\nwithout affecting the performance of the halfspace. The top few coefficients\nform the natural list decoding of a halfspace in the context of dictatorship\ntests/Label Cover reductions.\n  We note that unlike previous invariance principle based proofs which are only\nknown to give Unique-Games hardness, we are able to reduce from a version of\nLabel Cover problem that is known to be NP-hard. This has inspired follow-up\nwork on bypassing the Unique Games conjecture in some optimal geometric\ninapproximability results.\n", "versions": [{"version": "v1", "created": "Fri, 3 Dec 2010 13:11:22 GMT"}], "update_date": "2010-12-06", "authors_parsed": [["Feldman", "Vitaly", ""], ["Guruswami", "Venkatesan", ""], ["Raghavendra", "Prasad", ""], ["Wu", "Yi", ""]]}, {"id": "1012.0735", "submitter": "Jos\\'e L Balc\\'azar Navarro", "authors": "Jos\\'e L. Balc\\'azar, Diego Garc\\'ia-Saiz, Domingo G\\'omez-P\\'erez,\n  Cristina T\\^irn\\u{a}uc\\u{a}", "title": "Closed-set-based Discovery of Bases of Association Rules", "comments": "Shorter version in: Ali Khenchaf and Pascal Poncelet (eds.),\n  Extraction et gestion des connaissances (EGC'2011)", "journal-ref": "Revue des Nouvelles Technologies de l'Information RNTI-E-20\n  (2011), pages 635-646", "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.LO math.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The output of an association rule miner is often huge in practice. This is\nwhy several concise lossless representations have been proposed, such as the\n\"essential\" or \"representative\" rules. We revisit the algorithm given by\nKryszkiewicz (Int. Symp. Intelligent Data Analysis 2001, Springer-Verlag LNCS\n2189, 350-359) for mining representative rules. We show that its output is\nsometimes incomplete, due to an oversight in its mathematical validation. We\npropose alternative complete generators and we extend the approach to an\nexisting closure-aware basis similar to, and often smaller than, the\nrepresentative rules, namely the basis B*.\n", "versions": [{"version": "v1", "created": "Fri, 3 Dec 2010 13:29:01 GMT"}, {"version": "v2", "created": "Thu, 24 Mar 2011 16:38:44 GMT"}], "update_date": "2011-04-25", "authors_parsed": [["Balc\u00e1zar", "Jos\u00e9 L.", ""], ["Garc\u00eda-Saiz", "Diego", ""], ["G\u00f3mez-P\u00e9rez", "Domingo", ""], ["T\u00eern\u0103uc\u0103", "Cristina", ""]]}, {"id": "1012.0742", "submitter": "Jos\\'e L Balc\\'azar Navarro", "authors": "Jos\\'e L. Balc\\'azar, Cristina T\\^irn\\u{a}uc\\u{a}", "title": "Border Algorithms for Computing Hasse Diagrams of Arbitrary Lattices", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG math.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Border algorithm and the iPred algorithm find the Hasse diagrams of FCA\nlattices. We show that they can be generalized to arbitrary lattices. In the\ncase of iPred, this requires the identification of a join-semilattice\nhomomorphism into a distributive lattice.\n", "versions": [{"version": "v1", "created": "Fri, 3 Dec 2010 13:57:32 GMT"}], "update_date": "2010-12-06", "authors_parsed": [["Balc\u00e1zar", "Jos\u00e9 L.", ""], ["T\u00eern\u0103uc\u0103", "Cristina", ""]]}, {"id": "1012.0774", "submitter": "Matthias Hein", "authors": "Matthias Hein and Thomas B\\\"uhler", "title": "An Inverse Power Method for Nonlinear Eigenproblems with Applications in\n  1-Spectral Clustering and Sparse PCA", "comments": "Long version of paper accepted at NIPS 2010", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many problems in machine learning and statistics can be formulated as\n(generalized) eigenproblems. In terms of the associated optimization problem,\ncomputing linear eigenvectors amounts to finding critical points of a quadratic\nfunction subject to quadratic constraints. In this paper we show that a certain\nclass of constrained optimization problems with nonquadratic objective and\nconstraints can be understood as nonlinear eigenproblems. We derive a\ngeneralization of the inverse power method which is guaranteed to converge to a\nnonlinear eigenvector. We apply the inverse power method to 1-spectral\nclustering and sparse PCA which can naturally be formulated as nonlinear\neigenproblems. In both applications we achieve state-of-the-art results in\nterms of solution quality and runtime. Moving beyond the standard eigenproblem\nshould be useful also in many other applications and our inverse power method\ncan be easily adapted to new problems.\n", "versions": [{"version": "v1", "created": "Fri, 3 Dec 2010 15:58:47 GMT"}], "update_date": "2015-03-17", "authors_parsed": [["Hein", "Matthias", ""], ["B\u00fchler", "Thomas", ""]]}, {"id": "1012.0841", "submitter": "Pekka Malo", "authors": "Pekka Malo and Pyry Siitari and Ankur Sinha", "title": "Automated Query Learning with Wikipedia and Genetic Programming", "comments": "44 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.IR cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most of the existing information retrieval systems are based on bag of words\nmodel and are not equipped with common world knowledge. Work has been done\ntowards improving the efficiency of such systems by using intelligent\nalgorithms to generate search queries, however, not much research has been done\nin the direction of incorporating human-and-society level knowledge in the\nqueries. This paper is one of the first attempts where such information is\nincorporated into the search queries using Wikipedia semantics. The paper\npresents an essential shift from conventional token based queries to concept\nbased queries, leading to an enhanced efficiency of information retrieval\nsystems. To efficiently handle the automated query learning problem, we propose\nWikipedia-based Evolutionary Semantics (Wiki-ES) framework where concept based\nqueries are learnt using a co-evolving evolutionary procedure. Learning concept\nbased queries using an intelligent evolutionary procedure yields significant\nimprovement in performance which is shown through an extensive study using\nReuters newswire documents. Comparison of the proposed framework is performed\nwith other information retrieval systems. Concept based approach has also been\nimplemented on other information retrieval systems to justify the effectiveness\nof a transition from token based queries to concept based queries.\n", "versions": [{"version": "v1", "created": "Fri, 3 Dec 2010 20:53:36 GMT"}], "update_date": "2015-03-17", "authors_parsed": [["Malo", "Pekka", ""], ["Siitari", "Pyry", ""], ["Sinha", "Ankur", ""]]}, {"id": "1012.0866", "submitter": "Michele Guindani", "authors": "Edoardo M. Airoldi, Thiago Costa, Federico Bassetti, Fabrizio Leisen\n  and Michele Guindani", "title": "Generalized Species Sampling Priors with Latent Beta reinforcements", "comments": "For correspondence purposes, Edoardo M. Airoldi's email is\n  airoldi@fas.harvard.edu; Federico Bassetti's email is\n  federico.bassetti@unipv.it; Michele Guindani's email is\n  mguindani@mdanderson.org ; Fabrizo Leisen's email is\n  fabrizio.leisen@gmail.com. To appear in the Journal of the American\n  Statistical Association", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.LG stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many popular Bayesian nonparametric priors can be characterized in terms of\nexchangeable species sampling sequences. However, in some applications,\nexchangeability may not be appropriate. We introduce a {novel and\nprobabilistically coherent family of non-exchangeable species sampling\nsequences characterized by a tractable predictive probability function with\nweights driven by a sequence of independent Beta random variables. We compare\ntheir theoretical clustering properties with those of the Dirichlet Process and\nthe two parameters Poisson-Dirichlet process. The proposed construction\nprovides a complete characterization of the joint process, differently from\nexisting work. We then propose the use of such process as prior distribution in\na hierarchical Bayes modeling framework, and we describe a Markov Chain Monte\nCarlo sampler for posterior inference. We evaluate the performance of the prior\nand the robustness of the resulting inference in a simulation study, providing\na comparison with popular Dirichlet Processes mixtures and Hidden Markov\nModels. Finally, we develop an application to the detection of chromosomal\naberrations in breast cancer by leveraging array CGH data.\n", "versions": [{"version": "v1", "created": "Sat, 4 Dec 2010 00:03:16 GMT"}, {"version": "v2", "created": "Tue, 27 Mar 2012 22:40:09 GMT"}, {"version": "v3", "created": "Sun, 6 Jan 2013 12:43:35 GMT"}, {"version": "v4", "created": "Fri, 1 Aug 2014 20:20:34 GMT"}], "update_date": "2015-03-17", "authors_parsed": [["Airoldi", "Edoardo M.", ""], ["Costa", "Thiago", ""], ["Bassetti", "Federico", ""], ["Leisen", "Fabrizio", ""], ["Guindani", "Michele", ""]]}, {"id": "1012.0930", "submitter": "Zhi-Hua Zhou", "authors": "Nan Li and Ivor W. Tsang and Zhi-Hua Zhou", "title": "Efficient Optimization of Performance Measures by Classifier Adaptation", "comments": "30 pages, 5 figures, to appear in IEEE Transactions on Pattern\n  Analysis and Machine Intelligence, 2012", "journal-ref": "IEEE Transactions on Pattern Analysis and Machine Intelligence,\n  2013, 35(6): 1370-1382", "doi": "10.1109/TPAMI.2012.172", "report-no": null, "categories": "cs.LG cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In practical applications, machine learning algorithms are often needed to\nlearn classifiers that optimize domain specific performance measures.\nPreviously, the research has focused on learning the needed classifier in\nisolation, yet learning nonlinear classifier for nonlinear and nonsmooth\nperformance measures is still hard. In this paper, rather than learning the\nneeded classifier by optimizing specific performance measure directly, we\ncircumvent this problem by proposing a novel two-step approach called as CAPO,\nnamely to first train nonlinear auxiliary classifiers with existing learning\nmethods, and then to adapt auxiliary classifiers for specific performance\nmeasures. In the first step, auxiliary classifiers can be obtained efficiently\nby taking off-the-shelf learning algorithms. For the second step, we show that\nthe classifier adaptation problem can be reduced to a quadratic program\nproblem, which is similar to linear SVMperf and can be efficiently solved. By\nexploiting nonlinear auxiliary classifiers, CAPO can generate nonlinear\nclassifier which optimizes a large variety of performance measures including\nall the performance measure based on the contingency table and AUC, whilst\nkeeping high computational efficiency. Empirical studies show that CAPO is\neffective and of high computational efficiency, and even it is more efficient\nthan linear SVMperf.\n", "versions": [{"version": "v1", "created": "Sat, 4 Dec 2010 16:08:08 GMT"}, {"version": "v2", "created": "Tue, 7 Dec 2010 08:27:25 GMT"}, {"version": "v3", "created": "Thu, 2 Aug 2012 11:27:42 GMT"}], "update_date": "2015-03-17", "authors_parsed": [["Li", "Nan", ""], ["Tsang", "Ivor W.", ""], ["Zhou", "Zhi-Hua", ""]]}, {"id": "1012.0975", "submitter": "Xiaohui Xie", "authors": "Gui-Bo Ye, Jian-Feng Cai, Xiaohui Xie", "title": "Split Bregman Method for Sparse Inverse Covariance Estimation with\n  Matrix Iteration Acceleration", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of estimating the inverse covariance matrix by\nmaximizing the likelihood function with a penalty added to encourage the\nsparsity of the resulting matrix. We propose a new approach based on the split\nBregman method to solve the regularized maximum likelihood estimation problem.\nWe show that our method is significantly faster than the widely used graphical\nlasso method, which is based on blockwise coordinate descent, on both\nartificial and real-world data. More importantly, different from the graphical\nlasso, the split Bregman based method is much more general, and can be applied\nto a class of regularization terms other than the $\\ell_1$ norm\n", "versions": [{"version": "v1", "created": "Sun, 5 Dec 2010 07:27:42 GMT"}, {"version": "v2", "created": "Thu, 23 Dec 2010 23:02:28 GMT"}], "update_date": "2015-03-17", "authors_parsed": [["Ye", "Gui-Bo", ""], ["Cai", "Jian-Feng", ""], ["Xie", "Xiaohui", ""]]}, {"id": "1012.1367", "submitter": "Ohad Shamir", "authors": "Ofer Dekel, Ran Gilad-Bachrach, Ohad Shamir and Lin Xiao", "title": "Optimal Distributed Online Prediction using Mini-Batches", "comments": "Final version of paper to appear in Journal of Machine Learning\n  Research (JMLR)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DC math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Online prediction methods are typically presented as serial algorithms\nrunning on a single processor. However, in the age of web-scale prediction\nproblems, it is increasingly common to encounter situations where a single\nprocessor cannot keep up with the high rate at which inputs arrive. In this\nwork, we present the \\emph{distributed mini-batch} algorithm, a method of\nconverting many serial gradient-based online prediction algorithms into\ndistributed algorithms. We prove a regret bound for this method that is\nasymptotically optimal for smooth convex loss functions and stochastic inputs.\nMoreover, our analysis explicitly takes into account communication latencies\nbetween nodes in the distributed environment. We show how our method can be\nused to solve the closely-related distributed stochastic optimization problem,\nachieving an asymptotically linear speed-up over multiple processors. Finally,\nwe demonstrate the merits of our approach on a web-scale online prediction\nproblem.\n", "versions": [{"version": "v1", "created": "Tue, 7 Dec 2010 00:00:22 GMT"}, {"version": "v2", "created": "Tue, 31 Jan 2012 18:12:21 GMT"}], "update_date": "2012-02-01", "authors_parsed": [["Dekel", "Ofer", ""], ["Gilad-Bachrach", "Ran", ""], ["Shamir", "Ohad", ""], ["Xiao", "Lin", ""]]}, {"id": "1012.1370", "submitter": "Ohad Shamir", "authors": "Ofer Dekel, Ran Gilad-Bachrach, Ohad Shamir and Lin Xiao", "title": "Robust Distributed Online Prediction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The standard model of online prediction deals with serial processing of\ninputs by a single processor. However, in large-scale online prediction\nproblems, where inputs arrive at a high rate, an increasingly common necessity\nis to distribute the computation across several processors. A non-trivial\nchallenge is to design distributed algorithms for online prediction, which\nmaintain good regret guarantees. In \\cite{DMB}, we presented the DMB algorithm,\nwhich is a generic framework to convert any serial gradient-based online\nprediction algorithm into a distributed algorithm. Moreover, its regret\nguarantee is asymptotically optimal for smooth convex loss functions and\nstochastic inputs. On the flip side, it is fragile to many types of failures\nthat are common in distributed environments. In this companion paper, we\npresent variants of the DMB algorithm, which are resilient to many types of\nnetwork failures, and tolerant to varying performance of the computing nodes.\n", "versions": [{"version": "v1", "created": "Tue, 7 Dec 2010 00:12:25 GMT"}], "update_date": "2010-12-08", "authors_parsed": [["Dekel", "Ofer", ""], ["Gilad-Bachrach", "Ran", ""], ["Shamir", "Ohad", ""], ["Xiao", "Lin", ""]]}, {"id": "1012.1501", "submitter": "Francis Bach", "authors": "Francis Bach (LIENS, INRIA Paris - Rocquencourt)", "title": "Shaping Level Sets with Submodular Functions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider a class of sparsity-inducing regularization terms based on\nsubmodular functions. While previous work has focused on non-decreasing\nfunctions, we explore symmetric submodular functions and their \\lova\nextensions. We show that the Lovasz extension may be seen as the convex\nenvelope of a function that depends on level sets (i.e., the set of indices\nwhose corresponding components of the underlying predictor are greater than a\ngiven constant): this leads to a class of convex structured regularization\nterms that impose prior knowledge on the level sets, and not only on the\nsupports of the underlying predictors. We provide a unified set of optimization\nalgorithms, such as proximal operators, and theoretical guarantees (allowed\nlevel sets and recovery conditions). By selecting specific submodular\nfunctions, we give a new interpretation to known norms, such as the total\nvariation; we also define new norms, in particular ones that are based on order\nstatistics with application to clustering and outlier detection, and on noisy\ncuts in graphs with application to change point detection in the presence of\noutliers.\n", "versions": [{"version": "v1", "created": "Tue, 7 Dec 2010 13:34:44 GMT"}, {"version": "v2", "created": "Fri, 10 Jun 2011 14:12:14 GMT"}], "update_date": "2011-06-13", "authors_parsed": [["Bach", "Francis", "", "LIENS, INRIA Paris - Rocquencourt"]]}, {"id": "1012.1552", "submitter": "Emad Saad", "authors": "Emad Saad", "title": "Bridging the Gap between Reinforcement Learning and Knowledge\n  Representation: A Logical Off- and On-Policy Framework", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Knowledge Representation is important issue in reinforcement learning. In\nthis paper, we bridge the gap between reinforcement learning and knowledge\nrepresentation, by providing a rich knowledge representation framework, based\non normal logic programs with answer set semantics, that is capable of solving\nmodel-free reinforcement learning problems for more complex do-mains and\nexploits the domain-specific knowledge. We prove the correctness of our\napproach. We show that the complexity of finding an offline and online policy\nfor a model-free reinforcement learning problem in our approach is NP-complete.\nMoreover, we show that any model-free reinforcement learning problem in MDP\nenvironment can be encoded as a SAT problem. The importance of that is\nmodel-free reinforcement\n", "versions": [{"version": "v1", "created": "Tue, 7 Dec 2010 16:57:54 GMT"}], "update_date": "2010-12-08", "authors_parsed": [["Saad", "Emad", ""]]}, {"id": "1012.1919", "submitter": "Yue Deng", "authors": "Yue Deng, Qionghai Dai, Risheng Liu, Zengke Zhang and Sanqing Hu", "title": "Low-Rank Structure Learning via Log-Sum Heuristic Recovery", "comments": "13 pages, 3 figures", "journal-ref": "Neural Networks and Learning Systems, IEEE Transactions on,\n  Volume:24 , Issue: 3, March, 2013", "doi": "10.1109/TNNLS.2012.2235082", "report-no": null, "categories": "cs.NA cs.IT cs.LG math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recovering intrinsic data structure from corrupted observations plays an\nimportant role in various tasks in the communities of machine learning and\nsignal processing. In this paper, we propose a novel model, named log-sum\nheuristic recovery (LHR), to learn the essential low-rank structure from\ncorrupted data. Different from traditional approaches, which directly utilize\n$\\ell_1$ norm to measure the sparseness, LHR introduces a more reasonable\nlog-sum measurement to enhance the sparsity in both the intrinsic low-rank\nstructure and in the sparse corruptions. Although the proposed LHR optimization\nis no longer convex, it still can be effectively solved by a\nmajorization-minimization (MM) type algorithm, with which the non-convex\nobjective function is iteratively replaced by its convex surrogate and LHR\nfinally falls into the general framework of reweighed approaches. We prove that\nthe MM-type algorithm can converge to a stationary point after successive\niteration. We test the performance of our proposed model by applying it to\nsolve two typical problems: robust principal component analysis (RPCA) and\nlow-rank representation (LRR).\n  For RPCA, we compare LHR with the benchmark Principal Component Pursuit (PCP)\nmethod from both the perspectives of simulations and practical applications.\nFor LRR, we apply LHR to compute the low-rank representation matrix for motion\nsegmentation and stock clustering. Experimental results on low rank structure\nlearning demonstrate that the proposed Log-sum based model performs much better\nthan the $\\ell_1$-based method on for data with higher rank and with denser\ncorruptions.\n", "versions": [{"version": "v1", "created": "Thu, 9 Dec 2010 03:54:44 GMT"}, {"version": "v2", "created": "Sun, 3 Apr 2011 15:28:58 GMT"}, {"version": "v3", "created": "Sat, 24 Mar 2012 15:37:12 GMT"}], "update_date": "2014-08-13", "authors_parsed": [["Deng", "Yue", ""], ["Dai", "Qionghai", ""], ["Liu", "Risheng", ""], ["Zhang", "Zengke", ""], ["Hu", "Sanqing", ""]]}, {"id": "1012.2514", "submitter": "Jaydip Sen", "authors": "Jaydip Sen, P. Balamuralidhar, M. Girish Chandra, Harihara S.G., and\n  Harish Reddy", "title": "Context Aware End-to-End Connectivity Management", "comments": "8 pages, 3 figures. Second International Conference on Embedded\n  Systems, Mobile Communication and Computing (ICEMCC), pp. 85 - 95, Bangalore,\n  August 2007, India", "journal-ref": null, "doi": null, "report-no": "ISSN: 0973-208X", "categories": "cs.LG cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In a dynamic heterogeneous environment, such as pervasive and ubiquitous\ncomputing, context-aware adaptation is a key concept to meet the varying\nrequirements of different users. Connectivity is an important context source\nthat can be utilized for optimal management of diverse networking resources.\nApplication QoS (Quality of service) is another important issue that should be\ntaken into consideration for design of a context-aware system. This paper\npresents connectivity from the view point of context awareness, identifies\nvarious relevant raw connectivity contexts, and discusses how high-level\ncontext information can be abstracted from the raw context information.\nFurther, rich context information is utilized in various policy representation\nwith respect to user profile and preference, application characteristics,\ndevice capability, and network QoS conditions. Finally, a context-aware\nend-to-end evaluation algorithm is presented for adaptive connectivity\nmanagement in a multi-access wireless network. Unlike the currently existing\nalgorithms, the proposed algorithm takes into account user QoS parameters, and\ntherefore, it is more practical.\n", "versions": [{"version": "v1", "created": "Sun, 12 Dec 2010 07:22:05 GMT"}], "update_date": "2010-12-14", "authors_parsed": [["Sen", "Jaydip", ""], ["Balamuralidhar", "P.", ""], ["Chandra", "M. Girish", ""], ["G.", "Harihara S.", ""], ["Reddy", "Harish", ""]]}, {"id": "1012.2599", "submitter": "Eric Brochu", "authors": "Eric Brochu and Vlad M. Cora and Nando de Freitas", "title": "A Tutorial on Bayesian Optimization of Expensive Cost Functions, with\n  Application to Active User Modeling and Hierarchical Reinforcement Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a tutorial on Bayesian optimization, a method of finding the\nmaximum of expensive cost functions. Bayesian optimization employs the Bayesian\ntechnique of setting a prior over the objective function and combining it with\nevidence to get a posterior function. This permits a utility-based selection of\nthe next observation to make on the objective function, which must take into\naccount both exploration (sampling from areas of high uncertainty) and\nexploitation (sampling areas likely to offer improvement over the current best\nobservation). We also present two detailed extensions of Bayesian optimization,\nwith experiments---active user modelling with preferences, and hierarchical\nreinforcement learning---and a discussion of the pros and cons of Bayesian\noptimization based on our experiences.\n", "versions": [{"version": "v1", "created": "Sun, 12 Dec 2010 22:53:04 GMT"}], "update_date": "2010-12-14", "authors_parsed": [["Brochu", "Eric", ""], ["Cora", "Vlad M.", ""], ["de Freitas", "Nando", ""]]}, {"id": "1012.2609", "submitter": "Deqing Wang", "authors": "Deqing Wang, Hui Zhang", "title": "Inverse-Category-Frequency based supervised term weighting scheme for\n  text categorization", "comments": "18 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  Term weighting schemes often dominate the performance of many classifiers,\nsuch as kNN, centroid-based classifier and SVMs. The widely used term weighting\nscheme in text categorization, i.e., tf.idf, is originated from information\nretrieval (IR) field. The intuition behind idf for text categorization seems\nless reasonable than IR. In this paper, we introduce inverse category frequency\n(icf) into term weighting scheme and propose two novel approaches, i.e., tf.icf\nand icf-based supervised term weighting schemes. The tf.icf adopts icf to\nsubstitute idf factor and favors terms occurring in fewer categories, rather\nthan fewer documents. And the icf-based approach combines icf and relevance\nfrequency (rf) to weight terms in a supervised way. Our cross-classifier and\ncross-corpus experiments have shown that our proposed approaches are superior\nor comparable to six supervised term weighting schemes and three traditional\nschemes in terms of macro-F1 and micro-F1.\n", "versions": [{"version": "v1", "created": "Mon, 13 Dec 2010 01:22:36 GMT"}, {"version": "v2", "created": "Tue, 14 Dec 2010 09:26:49 GMT"}, {"version": "v3", "created": "Sat, 24 Dec 2011 02:34:31 GMT"}, {"version": "v4", "created": "Wed, 6 Jun 2012 03:29:13 GMT"}], "update_date": "2012-06-07", "authors_parsed": [["Wang", "Deqing", ""], ["Zhang", "Hui", ""]]}, {"id": "1012.3005", "submitter": "Yi Gai", "authors": "Yi Gai, Bhaskar Krishnamachari and Mingyan Liu", "title": "On the Combinatorial Multi-Armed Bandit Problem with Markovian Rewards", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.LG cs.NI cs.SY math.PR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider a combinatorial generalization of the classical multi-armed\nbandit problem that is defined as follows. There is a given bipartite graph of\n$M$ users and $N \\geq M$ resources. For each user-resource pair $(i,j)$, there\nis an associated state that evolves as an aperiodic irreducible finite-state\nMarkov chain with unknown parameters, with transitions occurring each time the\nparticular user $i$ is allocated resource $j$. The user $i$ receives a reward\nthat depends on the corresponding state each time it is allocated the resource\n$j$. The system objective is to learn the best matching of users to resources\nso that the long-term sum of the rewards received by all users is maximized.\nThis corresponds to minimizing regret, defined here as the gap between the\nexpected total reward that can be obtained by the best-possible static matching\nand the expected total reward that can be achieved by a given algorithm. We\npresent a polynomial-storage and polynomial-complexity-per-step\nmatching-learning algorithm for this problem. We show that this algorithm can\nachieve a regret that is uniformly arbitrarily close to logarithmic in time and\npolynomial in the number of users and resources. This formulation is broadly\napplicable to scheduling and switching problems in networks and significantly\nextends prior results in the area.\n", "versions": [{"version": "v1", "created": "Tue, 14 Dec 2010 12:29:43 GMT"}, {"version": "v2", "created": "Sun, 20 Mar 2011 01:50:09 GMT"}], "update_date": "2015-03-17", "authors_parsed": [["Gai", "Yi", ""], ["Krishnamachari", "Bhaskar", ""], ["Liu", "Mingyan", ""]]}, {"id": "1012.3697", "submitter": "Daniel Kuntze", "authors": "Marcel R. Ackermann, Johannes Bl\\\"omer, Daniel Kuntze and Christian\n  Sohler", "title": "Analysis of Agglomerative Clustering", "comments": "A preliminary version of this article appeared in Proceedings of the\n  28th International Symposium on Theoretical Aspects of Computer Science\n  (STACS '11), March 2011, pp. 308-319. This article also appeared in\n  Algorithmica. The final publication is available at\n  http://link.springer.com/article/10.1007/s00453-012-9717-4", "journal-ref": "Ackermann, M. R., Bl\\\"omer, J., Kuntze, D., and Sohler, C. (2014).\n  Analysis of Agglomerative Clustering. Algorithmica, 69(1):184-215", "doi": "10.1007/s00453-012-9717-4", "report-no": null, "categories": "cs.DS cs.CG cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The diameter $k$-clustering problem is the problem of partitioning a finite\nsubset of $\\mathbb{R}^d$ into $k$ subsets called clusters such that the maximum\ndiameter of the clusters is minimized. One early clustering algorithm that\ncomputes a hierarchy of approximate solutions to this problem (for all values\nof $k$) is the agglomerative clustering algorithm with the complete linkage\nstrategy. For decades, this algorithm has been widely used by practitioners.\nHowever, it is not well studied theoretically. In this paper, we analyze the\nagglomerative complete linkage clustering algorithm. Assuming that the\ndimension $d$ is a constant, we show that for any $k$ the solution computed by\nthis algorithm is an $O(\\log k)$-approximation to the diameter $k$-clustering\nproblem. Our analysis does not only hold for the Euclidean distance but for any\nmetric that is based on a norm. Furthermore, we analyze the closely related\n$k$-center and discrete $k$-center problem. For the corresponding agglomerative\nalgorithms, we deduce an approximation factor of $O(\\log k)$ as well.\n", "versions": [{"version": "v1", "created": "Thu, 16 Dec 2010 17:46:07 GMT"}, {"version": "v2", "created": "Mon, 2 Apr 2012 09:56:24 GMT"}, {"version": "v3", "created": "Tue, 16 Oct 2012 12:55:49 GMT"}, {"version": "v4", "created": "Fri, 7 Mar 2014 19:37:47 GMT"}], "update_date": "2014-03-10", "authors_parsed": [["Ackermann", "Marcel R.", ""], ["Bl\u00f6mer", "Johannes", ""], ["Kuntze", "Daniel", ""], ["Sohler", "Christian", ""]]}, {"id": "1012.3877", "submitter": "Ying Cui", "authors": "Ying Cui, Qingqing Huang, Vincent K.N.Lau", "title": "Queue-Aware Dynamic Clustering and Power Allocation for Network MIMO\n  Systems via Distributive Stochastic Learning", "comments": null, "journal-ref": null, "doi": "10.1109/TSP.2010.2097253", "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a two-timescale delay-optimal dynamic clustering\nand power allocation design for downlink network MIMO systems. The dynamic\nclustering control is adaptive to the global queue state information (GQSI)\nonly and computed at the base station controller (BSC) over a longer time\nscale. On the other hand, the power allocations of all the BSs in one cluster\nare adaptive to both intra-cluster channel state information (CCSI) and\nintra-cluster queue state information (CQSI), and computed at the cluster\nmanager (CM) over a shorter time scale. We show that the two-timescale\ndelay-optimal control can be formulated as an infinite-horizon average cost\nConstrained Partially Observed Markov Decision Process (CPOMDP). By exploiting\nthe special problem structure, we shall derive an equivalent Bellman equation\nin terms of Pattern Selection Q-factor to solve the CPOMDP. To address the\ndistributive requirement and the issue of exponential memory requirement and\ncomputational complexity, we approximate the Pattern Selection Q-factor by the\nsum of Per-cluster Potential functions and propose a novel distributive online\nlearning algorithm to estimate the Per-cluster Potential functions (at each CM)\nas well as the Lagrange multipliers (LM) (at each BS). We show that the\nproposed distributive online learning algorithm converges almost surely (with\nprobability 1). By exploiting the birth-death structure of the queue dynamics,\nwe further decompose the Per-cluster Potential function into sum of Per-cluster\nPer-user Potential functions and formulate the instantaneous power allocation\nas a Per-stage QSI-aware Interference Game played among all the CMs. We also\npropose a QSI-aware Simultaneous Iterative Water-filling Algorithm (QSIWFA) and\nshow that it can achieve the Nash Equilibrium (NE).\n", "versions": [{"version": "v1", "created": "Fri, 17 Dec 2010 13:16:07 GMT"}], "update_date": "2017-04-26", "authors_parsed": [["Cui", "Ying", ""], ["Huang", "Qingqing", ""], ["Lau", "Vincent K. N.", ""]]}, {"id": "1012.4051", "submitter": "Zeyuan Allen Zhu", "authors": "Zeyuan Allen Zhu", "title": "Survey & Experiment: Towards the Learning Accuracy", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To attain the best learning accuracy, people move on with difficulties and\nfrustrations. Though one can optimize the empirical objective using a given set\nof samples, its generalization ability to the entire sample distribution\nremains questionable. Even if a fair generalization guarantee is offered, one\nstill wants to know what is to happen if the regularizer is removed, and/or how\nwell the artificial loss (like the hinge loss) relates to the accuracy.\n  For such reason, this report surveys four different trials towards the\nlearning accuracy, embracing the major advances in supervised learning theory\nin the past four years. Starting from the generic setting of learning, the\nfirst two trials introduce the best optimization and generalization bounds for\nconvex learning, and the third trial gets rid of the regularizer. As an\ninnovative attempt, the fourth trial studies the optimization when the\nobjective is exactly the accuracy, in the special case of binary\nclassification. This report also analyzes the last trial through experiments.\n", "versions": [{"version": "v1", "created": "Sat, 18 Dec 2010 03:25:44 GMT"}], "update_date": "2010-12-21", "authors_parsed": [["Zhu", "Zeyuan Allen", ""]]}, {"id": "1012.4249", "submitter": "Raffi Sevlian", "authors": "Raffi Sevlian, Ram Rajagopal", "title": "Travel Time Estimation Using Floating Car Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This report explores the use of machine learning techniques to accurately\npredict travel times in city streets and highways using floating car data\n(location information of user vehicles on a road network). The aim of this\nreport is twofold, first we present a general architecture of solving this\nproblem, then present and evaluate few techniques on real floating car data\ngathered over a month on a 5 Km highway in New Delhi.\n", "versions": [{"version": "v1", "created": "Mon, 20 Dec 2010 07:36:42 GMT"}], "update_date": "2010-12-21", "authors_parsed": [["Sevlian", "Raffi", ""], ["Rajagopal", "Ram", ""]]}, {"id": "1012.4571", "submitter": "Yannis Sismanis", "authors": "Yannis Sismanis", "title": "How I won the \"Chess Ratings - Elo vs the Rest of the World\" Competition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article discusses in detail the rating system that won the kaggle\ncompetition \"Chess Ratings: Elo vs the rest of the world\". The competition\nprovided a historical dataset of outcomes for chess games, and aimed to\ndiscover whether novel approaches can predict the outcomes of future games,\nmore accurately than the well-known Elo rating system. The winning rating\nsystem, called Elo++ in the rest of the article, builds upon the Elo rating\nsystem. Like Elo, Elo++ uses a single rating per player and predicts the\noutcome of a game, by using a logistic curve over the difference in ratings of\nthe players. The major component of Elo++ is a regularization technique that\navoids overfitting these ratings. The dataset of chess games and outcomes is\nrelatively small and one has to be careful not to draw \"too many conclusions\"\nout of the limited data. Many approaches tested in the competition showed signs\nof such an overfitting. The leader-board was dominated by attempts that did a\nvery good job on a small test dataset, but couldn't generalize well on the\nprivate hold-out dataset. The Elo++ regularization takes into account the\nnumber of games per player, the recency of these games and the ratings of the\nopponents. Finally, Elo++ employs a stochastic gradient descent scheme for\ntraining the ratings, and uses only two global parameters (white's advantage\nand regularization constant) that are optimized using cross-validation.\n", "versions": [{"version": "v1", "created": "Tue, 21 Dec 2010 09:11:53 GMT"}], "update_date": "2015-03-17", "authors_parsed": [["Sismanis", "Yannis", ""]]}, {"id": "1012.4928", "submitter": "Reza  Parhizkar", "authors": "Reza Parhizkar, Amin Karbasi, Sewoong Oh, Martin Vetterli", "title": "Calibration Using Matrix Completion with Application to Ultrasound\n  Tomography", "comments": "submitted to IEEE Transaction on Signal Processing", "journal-ref": null, "doi": "10.1109/TSP.2013.2272925", "report-no": null, "categories": "cs.LG cs.IT math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the calibration process in circular ultrasound tomography devices\nwhere the sensor positions deviate from the circumference of a perfect circle.\nThis problem arises in a variety of applications in signal processing ranging\nfrom breast imaging to sensor network localization. We introduce a novel method\nof calibration/localization based on the time-of-flight (ToF) measurements\nbetween sensors when the enclosed medium is homogeneous. In the presence of all\nthe pairwise ToFs, one can easily estimate the sensor positions using\nmulti-dimensional scaling (MDS) method. In practice however, due to the\ntransitional behaviour of the sensors and the beam form of the transducers, the\nToF measurements for close-by sensors are unavailable. Further, random\nmalfunctioning of the sensors leads to random missing ToF measurements. On top\nof the missing entries, in practice an unknown time delay is also added to the\nmeasurements. In this work, we incorporate the fact that a matrix defined from\nall the ToF measurements is of rank at most four. In order to estimate the\nmissing ToFs, we apply a state-of-the-art low-rank matrix completion algorithm,\nOPTSPACE . To find the correct positions of the sensors (our ultimate goal) we\nthen apply MDS. We show analytic bounds on the overall error of the whole\nprocess in the presence of noise and hence deduce its robustness. Finally, we\nconfirm the functionality of our method in practice by simulations mimicking\nthe measurements of a circular ultrasound tomography device.\n", "versions": [{"version": "v1", "created": "Wed, 22 Dec 2010 10:30:26 GMT"}, {"version": "v2", "created": "Tue, 11 Oct 2011 23:42:37 GMT"}], "update_date": "2016-11-18", "authors_parsed": [["Parhizkar", "Reza", ""], ["Karbasi", "Amin", ""], ["Oh", "Sewoong", ""], ["Vetterli", "Martin", ""]]}]