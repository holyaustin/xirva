[{"id": "1211.0025", "submitter": "Vladimir Vovk", "authors": "Vladimir Vovk and Ivan Petej", "title": "Venn-Abers predictors", "comments": "18 pages; to appear in the UAI 2014 Proceedings", "journal-ref": null, "doi": null, "report-no": "OCM07", "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper continues study, both theoretical and empirical, of the method of\nVenn prediction, concentrating on binary prediction problems. Venn predictors\nproduce probability-type predictions for the labels of test objects which are\nguaranteed to be well calibrated under the standard assumption that the\nobservations are generated independently from the same distribution. We give a\nsimple formalization and proof of this property. We also introduce Venn-Abers\npredictors, a new class of Venn predictors based on the idea of isotonic\nregression, and report promising empirical results both for Venn-Abers\npredictors and for their more computationally efficient simplified version.\n", "versions": [{"version": "v1", "created": "Wed, 31 Oct 2012 20:38:04 GMT"}, {"version": "v2", "created": "Sat, 21 Jun 2014 13:47:44 GMT"}], "update_date": "2014-06-24", "authors_parsed": [["Vovk", "Vladimir", ""], ["Petej", "Ivan", ""]]}, {"id": "1211.0028", "submitter": "Qirong Ho", "authors": "Qirong Ho, Rong Yan, Rajat Raina, Eric P. Xing", "title": "Understanding the Interaction between Interests, Conversations and\n  Friendships in Facebook", "comments": null, "journal-ref": null, "doi": null, "report-no": "CMU-ML-12-109", "categories": "cs.SI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we explore salient questions about user interests,\nconversations and friendships in the Facebook social network, using a novel\nlatent space model that integrates several data types. A key challenge of\nstudying Facebook's data is the wide range of data modalities such as text,\nnetwork links, and categorical labels. Our latent space model seamlessly\ncombines all three data modalities over millions of users, allowing us to study\nthe interplay between user friendships, interests, and higher-order\nnetwork-wide social trends on Facebook. The recovered insights not only answer\nour initial questions, but also reveal surprising facts about user interests in\nthe context of Facebook's ecosystem. We also confirm that our results are\nsignificant with respect to evidential information from the study subjects.\n", "versions": [{"version": "v1", "created": "Wed, 31 Oct 2012 20:56:16 GMT"}], "update_date": "2012-11-02", "authors_parsed": [["Ho", "Qirong", ""], ["Yan", "Rong", ""], ["Raina", "Rajat", ""], ["Xing", "Eric P.", ""]]}, {"id": "1211.0053", "submitter": "David Shuman", "authors": "David I Shuman, Sunil K. Narang, Pascal Frossard, Antonio Ortega, and\n  Pierre Vandergheynst", "title": "The Emerging Field of Signal Processing on Graphs: Extending\n  High-Dimensional Data Analysis to Networks and Other Irregular Domains", "comments": "To appear in the IEEE Signal Processing Magazine", "journal-ref": null, "doi": "10.1109/MSP.2012.2235192", "report-no": null, "categories": "cs.DM cs.LG cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In applications such as social, energy, transportation, sensor, and neuronal\nnetworks, high-dimensional data naturally reside on the vertices of weighted\ngraphs. The emerging field of signal processing on graphs merges algebraic and\nspectral graph theoretic concepts with computational harmonic analysis to\nprocess such signals on graphs. In this tutorial overview, we outline the main\nchallenges of the area, discuss different ways to define graph spectral\ndomains, which are the analogues to the classical frequency domain, and\nhighlight the importance of incorporating the irregular structures of graph\ndata domains when processing signals on graphs. We then review methods to\ngeneralize fundamental operations such as filtering, translation, modulation,\ndilation, and downsampling to the graph setting, and survey the localized,\nmultiscale transforms that have been proposed to efficiently extract\ninformation from high-dimensional data on graphs. We conclude with a brief\ndiscussion of open issues and possible extensions.\n", "versions": [{"version": "v1", "created": "Wed, 31 Oct 2012 23:18:43 GMT"}, {"version": "v2", "created": "Sun, 10 Mar 2013 15:04:40 GMT"}], "update_date": "2015-06-12", "authors_parsed": [["Shuman", "David I", ""], ["Narang", "Sunil K.", ""], ["Frossard", "Pascal", ""], ["Ortega", "Antonio", ""], ["Vandergheynst", "Pierre", ""]]}, {"id": "1211.0056", "submitter": "Zhaosong Lu", "authors": "Zhaosong Lu", "title": "Iterative Hard Thresholding Methods for $l_0$ Regularized Convex Cone\n  Programming", "comments": "25 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.LG math.NA stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we consider $l_0$ regularized convex cone programming problems.\nIn particular, we first propose an iterative hard thresholding (IHT) method and\nits variant for solving $l_0$ regularized box constrained convex programming.\nWe show that the sequence generated by these methods converges to a local\nminimizer. Also, we establish the iteration complexity of the IHT method for\nfinding an $\\epsilon$-local-optimal solution. We then propose a method for\nsolving $l_0$ regularized convex cone programming by applying the IHT method to\nits quadratic penalty relaxation and establish its iteration complexity for\nfinding an $\\epsilon$-approximate local minimizer. Finally, we propose a\nvariant of this method in which the associated penalty parameter is dynamically\nupdated, and show that every accumulation point is a local minimizer of the\nproblem.\n", "versions": [{"version": "v1", "created": "Wed, 31 Oct 2012 23:47:04 GMT"}, {"version": "v2", "created": "Fri, 2 Nov 2012 04:04:35 GMT"}], "update_date": "2012-11-05", "authors_parsed": [["Lu", "Zhaosong", ""]]}, {"id": "1211.0210", "submitter": "Sundararajan Sellamanickam", "authors": "Sathiya Keerthi Selvaraj, Sundararajan Sellamanickam, Shirish Shevade", "title": "Extension of TSVM to Multi-Class and Hierarchical Text Classification\n  Problems With General Losses", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Transductive SVM (TSVM) is a well known semi-supervised large margin learning\nmethod for binary text classification. In this paper we extend this method to\nmulti-class and hierarchical classification problems. We point out that the\ndetermination of labels of unlabeled examples with fixed classifier weights is\na linear programming problem. We devise an efficient technique for solving it.\nThe method is applicable to general loss functions. We demonstrate the value of\nthe new method using large margin loss on a number of multi-class and\nhierarchical classification datasets. For maxent loss we show empirically that\nour method is better than expectation regularization/constraint and posterior\nregularization methods, and competitive with the version of entropy\nregularization method which uses label constraints.\n", "versions": [{"version": "v1", "created": "Thu, 1 Nov 2012 15:52:11 GMT"}], "update_date": "2012-11-02", "authors_parsed": [["Selvaraj", "Sathiya Keerthi", ""], ["Sellamanickam", "Sundararajan", ""], ["Shevade", "Shirish", ""]]}, {"id": "1211.0358", "submitter": "Andreas Damianou Mr", "authors": "Andreas C. Damianou, Neil D. Lawrence", "title": "Deep Gaussian Processes", "comments": "9 pages, 8 figures. Appearing in Proceedings of the 16th\n  International Conference on Artificial Intelligence and Statistics (AISTATS)\n  2013", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.PR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we introduce deep Gaussian process (GP) models. Deep GPs are a\ndeep belief network based on Gaussian process mappings. The data is modeled as\nthe output of a multivariate GP. The inputs to that Gaussian process are then\ngoverned by another GP. A single layer model is equivalent to a standard GP or\nthe GP latent variable model (GP-LVM). We perform inference in the model by\napproximate variational marginalization. This results in a strict lower bound\non the marginal likelihood of the model which we use for model selection\n(number of layers and nodes per layer). Deep belief networks are typically\napplied to relatively large data sets using stochastic gradient descent for\noptimization. Our fully Bayesian treatment allows for the application of deep\nmodels even when data is scarce. Model selection by our variational bound shows\nthat a five layer hierarchy is justified even when modelling a digit data set\ncontaining only 150 examples.\n", "versions": [{"version": "v1", "created": "Fri, 2 Nov 2012 03:13:08 GMT"}, {"version": "v2", "created": "Sat, 23 Mar 2013 01:23:34 GMT"}], "update_date": "2013-03-26", "authors_parsed": [["Damianou", "Andreas C.", ""], ["Lawrence", "Neil D.", ""]]}, {"id": "1211.0439", "submitter": "Peter Sollich", "authors": "Simon R. F. Ashton and Peter Sollich", "title": "Learning curves for multi-task Gaussian process regression", "comments": "9 pages, to appear in Advances in Neural Information Processing\n  Systems 25", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cond-mat.dis-nn stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the average case performance of multi-task Gaussian process (GP)\nregression as captured in the learning curve, i.e. the average Bayes error for\na chosen task versus the total number of examples $n$ for all tasks. For GP\ncovariances that are the product of an input-dependent covariance function and\na free-form inter-task covariance matrix, we show that accurate approximations\nfor the learning curve can be obtained for an arbitrary number of tasks $T$. We\nuse these to study the asymptotic learning behaviour for large $n$.\nSurprisingly, multi-task learning can be asymptotically essentially useless, in\nthe sense that examples from other tasks help only when the degree of\ninter-task correlation, $\\rho$, is near its maximal value $\\rho=1$. This effect\nis most extreme for learning of smooth target functions as described by e.g.\nsquared exponential kernels. We also demonstrate that when learning many tasks,\nthe learning curves separate into an initial phase, where the Bayes error on\neach task is reduced down to a plateau value by \"collective learning\" even\nthough most tasks have not seen examples, and a final decay that occurs once\nthe number of examples is proportional to the number of tasks.\n", "versions": [{"version": "v1", "created": "Fri, 2 Nov 2012 12:46:24 GMT"}], "update_date": "2012-11-05", "authors_parsed": [["Ashton", "Simon R. F.", ""], ["Sollich", "Peter", ""]]}, {"id": "1211.0447", "submitter": "Yongjun Liao", "authors": "Wei Du and Yongjun Liao and and Pierre Geurts and Guy Leduc", "title": "Ordinal Rating of Network Performance and Inference by Matrix Completion", "comments": "submitted to the Passive and Active Measurement Conference (PAM),\n  2013", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper addresses the large-scale acquisition of end-to-end network\nperformance. We made two distinct contributions: ordinal rating of network\nperformance and inference by matrix completion. The former reduces measurement\ncosts and unifies various metrics which eases their processing in applications.\nThe latter enables scalable and accurate inference with no requirement of\nstructural information of the network nor geometric constraints. By combining\nboth, the acquisition problem bears strong similarities to recommender systems.\nThis paper investigates the applicability of various matrix factorization\nmodels used in recommender systems. We found that the simple regularized matrix\nfactorization is not only practical but also produces accurate results that are\nbeneficial for peer selection.\n", "versions": [{"version": "v1", "created": "Fri, 2 Nov 2012 13:21:48 GMT"}], "update_date": "2012-11-05", "authors_parsed": [["Du", "Wei", ""], ["Liao", "Yongjun", ""], ["Geurts", "and Pierre", ""], ["Leduc", "Guy", ""]]}, {"id": "1211.0587", "submitter": "Joel Veness", "authors": "Joel Veness, Martha White, Michael Bowling, Andr\\'as Gy\\\"orgy", "title": "Partition Tree Weighting", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.LG math.IT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces the Partition Tree Weighting technique, an efficient\nmeta-algorithm for piecewise stationary sources. The technique works by\nperforming Bayesian model averaging over a large class of possible partitions\nof the data into locally stationary segments. It uses a prior, closely related\nto the Context Tree Weighting technique of Willems, that is well suited to data\ncompression applications. Our technique can be applied to any coding\ndistribution at an additional time and space cost only logarithmic in the\nsequence length. We provide a competitive analysis of the redundancy of our\nmethod, and explore its application in a variety of settings. The order of the\nredundancy and the complexity of our algorithm matches those of the best\ncompetitors available in the literature, and the new algorithm exhibits a\nsuperior complexity-performance trade-off in our experiments.\n", "versions": [{"version": "v1", "created": "Sat, 3 Nov 2012 00:41:46 GMT"}, {"version": "v2", "created": "Wed, 21 Nov 2012 12:52:44 GMT"}], "update_date": "2012-11-22", "authors_parsed": [["Veness", "Joel", ""], ["White", "Martha", ""], ["Bowling", "Michael", ""], ["Gy\u00f6rgy", "Andr\u00e1s", ""]]}, {"id": "1211.0616", "submitter": "Amit Daniely", "authors": "Amit Daniely and Nati Linial and Shai Shalev-Shwartz", "title": "The complexity of learning halfspaces using generalized linear methods", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many popular learning algorithms (E.g. Regression, Fourier-Transform based\nalgorithms, Kernel SVM and Kernel ridge regression) operate by reducing the\nproblem to a convex optimization problem over a vector space of functions.\nThese methods offer the currently best approach to several central problems\nsuch as learning half spaces and learning DNF's. In addition they are widely\nused in numerous application domains. Despite their importance, there are still\nvery few proof techniques to show limits on the power of these algorithms.\n  We study the performance of this approach in the problem of (agnostically and\nimproperly) learning halfspaces with margin $\\gamma$. Let $\\mathcal{D}$ be a\ndistribution over labeled examples. The $\\gamma$-margin error of a hyperplane\n$h$ is the probability of an example to fall on the wrong side of $h$ or at a\ndistance $\\le\\gamma$ from it. The $\\gamma$-margin error of the best $h$ is\ndenoted $\\mathrm{Err}_\\gamma(\\mathcal{D})$. An $\\alpha(\\gamma)$-approximation\nalgorithm receives $\\gamma,\\epsilon$ as input and, using i.i.d. samples of\n$\\mathcal{D}$, outputs a classifier with error rate $\\le\n\\alpha(\\gamma)\\mathrm{Err}_\\gamma(\\mathcal{D}) + \\epsilon$. Such an algorithm\nis efficient if it uses $\\mathrm{poly}(\\frac{1}{\\gamma},\\frac{1}{\\epsilon})$\nsamples and runs in time polynomial in the sample size.\n  The best approximation ratio achievable by an efficient algorithm is\n$O\\left(\\frac{1/\\gamma}{\\sqrt{\\log(1/\\gamma)}}\\right)$ and is achieved using an\nalgorithm from the above class. Our main result shows that the approximation\nratio of every efficient algorithm from this family must be $\\ge\n\\Omega\\left(\\frac{1/\\gamma}{\\mathrm{poly}\\left(\\log\\left(1/\\gamma\\right)\\right)}\\right)$,\nessentially matching the best known upper bound.\n", "versions": [{"version": "v1", "created": "Sat, 3 Nov 2012 15:14:40 GMT"}, {"version": "v2", "created": "Thu, 8 Nov 2012 15:06:46 GMT"}, {"version": "v3", "created": "Sun, 20 Oct 2013 17:46:03 GMT"}, {"version": "v4", "created": "Sat, 10 May 2014 11:15:05 GMT"}], "update_date": "2014-05-13", "authors_parsed": [["Daniely", "Amit", ""], ["Linial", "Nati", ""], ["Shalev-Shwartz", "Shai", ""]]}, {"id": "1211.0632", "submitter": "Hua Ouyang", "authors": "Hua Ouyang, Niao He, Alexander Gray", "title": "Stochastic ADMM for Nonsmooth Optimization", "comments": "A short version of this paper appears in the 5th NIPS Workshop on\n  Optimization for Machine Learning, Lake Tahoe, Nevada, USA, 2012", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a stochastic setting for optimization problems with nonsmooth\nconvex separable objective functions over linear equality constraints. To solve\nsuch problems, we propose a stochastic Alternating Direction Method of\nMultipliers (ADMM) algorithm. Our algorithm applies to a more general class of\nnonsmooth convex functions that does not necessarily have a closed-form\nsolution by minimizing the augmented function directly. We also demonstrate the\nrates of convergence for our algorithm under various structural assumptions of\nthe stochastic functions: $O(1/\\sqrt{t})$ for convex functions and $O(\\log\nt/t)$ for strongly convex functions. Compared to previous literature, we\nestablish the convergence rate of ADMM algorithm, for the first time, in terms\nof both the objective value and the feasibility violation.\n", "versions": [{"version": "v1", "created": "Sat, 3 Nov 2012 19:05:56 GMT"}, {"version": "v2", "created": "Tue, 22 Jan 2013 17:07:37 GMT"}], "update_date": "2013-01-23", "authors_parsed": [["Ouyang", "Hua", ""], ["He", "Niao", ""], ["Gray", "Alexander", ""]]}, {"id": "1211.0801", "submitter": "Ming Yuan", "authors": "Ming Yuan", "title": "Discussion: Latent variable graphical model selection via convex\n  optimization", "comments": "Published in at http://dx.doi.org/10.1214/12-AOS979 the Annals of\n  Statistics (http://www.imstat.org/aos/) by the Institute of Mathematical\n  Statistics (http://www.imstat.org)", "journal-ref": "Annals of Statistics 2012, Vol. 40, No. 4, 1968-1972", "doi": "10.1214/12-AOS979", "report-no": "IMS-AOS-AOS979", "categories": "math.ST cs.LG stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Discussion of \"Latent variable graphical model selection via convex\noptimization\" by Venkat Chandrasekaran, Pablo A. Parrilo and Alan S. Willsky\n[arXiv:1008.1290].\n", "versions": [{"version": "v1", "created": "Mon, 5 Nov 2012 09:36:40 GMT"}], "update_date": "2012-11-06", "authors_parsed": [["Yuan", "Ming", ""]]}, {"id": "1211.0806", "submitter": "Nicolai Meinshausen", "authors": "Steffen Lauritzen, Nicolai Meinshausen", "title": "Discussion: Latent variable graphical model selection via convex\n  optimization", "comments": "Published in at http://dx.doi.org/10.1214/12-AOS980 the Annals of\n  Statistics (http://www.imstat.org/aos/) by the Institute of Mathematical\n  Statistics (http://www.imstat.org)", "journal-ref": "Annals of Statistics 2012, Vol. 40, No. 4, 1973-1977", "doi": "10.1214/12-AOS980", "report-no": "IMS-AOS-AOS980", "categories": "math.ST cs.LG stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Discussion of \"Latent variable graphical model selection via convex\noptimization\" by Venkat Chandrasekaran, Pablo A. Parrilo and Alan S. Willsky\n[arXiv:1008.1290].\n", "versions": [{"version": "v1", "created": "Mon, 5 Nov 2012 09:51:07 GMT"}], "update_date": "2012-11-06", "authors_parsed": [["Lauritzen", "Steffen", ""], ["Meinshausen", "Nicolai", ""]]}, {"id": "1211.0808", "submitter": "Martin J. Wainwright", "authors": "Martin J. Wainwright", "title": "Discussion: Latent variable graphical model selection via convex\n  optimization", "comments": "Published in at http://dx.doi.org/10.1214/12-AOS981 the Annals of\n  Statistics (http://www.imstat.org/aos/) by the Institute of Mathematical\n  Statistics (http://www.imstat.org)", "journal-ref": "Annals of Statistics 2012, Vol. 40, No. 4, 1978-1983", "doi": "10.1214/12-AOS981", "report-no": "IMS-AOS-AOS981", "categories": "math.ST cs.LG stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Discussion of \"Latent variable graphical model selection via convex\noptimization\" by Venkat Chandrasekaran, Pablo A. Parrilo and Alan S. Willsky\n[arXiv:1008.1290].\n", "versions": [{"version": "v1", "created": "Mon, 5 Nov 2012 09:59:33 GMT"}], "update_date": "2012-11-06", "authors_parsed": [["Wainwright", "Martin J.", ""]]}, {"id": "1211.0817", "submitter": "Emmanuel J. Cand\\'{e}s", "authors": "Emmanuel J. Cand\\'es, Mahdi Soltanolkotabi", "title": "Discussion: Latent variable graphical model selection via convex\n  optimization", "comments": "Published in at http://dx.doi.org/10.1214/12-AOS1001 the Annals of\n  Statistics (http://www.imstat.org/aos/) by the Institute of Mathematical\n  Statistics (http://www.imstat.org)", "journal-ref": "Annals of Statistics 2012, Vol. 40, No. 4, 1997-2004", "doi": "10.1214/12-AOS1001", "report-no": "IMS-AOS-AOS1001", "categories": "math.ST cs.LG stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Discussion of \"Latent variable graphical model selection via convex\noptimization\" by Venkat Chandrasekaran, Pablo A. Parrilo and Alan S. Willsky\n[arXiv:1008.1290].\n", "versions": [{"version": "v1", "created": "Mon, 5 Nov 2012 10:32:57 GMT"}], "update_date": "2012-11-06", "authors_parsed": [["Cand\u00e9s", "Emmanuel J.", ""], ["Soltanolkotabi", "Mahdi", ""]]}, {"id": "1211.0835", "submitter": "Venkat Chandrasekaran", "authors": "Venkat Chandrasekaran, Pablo A. Parrilo, Alan S. Willsky", "title": "Rejoinder: Latent variable graphical model selection via convex\n  optimization", "comments": "Published in at http://dx.doi.org/10.1214/12-AOS1020 the Annals of\n  Statistics (http://www.imstat.org/aos/) by the Institute of Mathematical\n  Statistics (http://www.imstat.org)", "journal-ref": "Annals of Statistics 2012, Vol. 40, No. 4, 2005-2013", "doi": "10.1214/12-AOS1020", "report-no": "IMS-AOS-AOS1020", "categories": "math.ST cs.LG stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Rejoinder to \"Latent variable graphical model selection via convex\noptimization\" by Venkat Chandrasekaran, Pablo A. Parrilo and Alan S. Willsky\n[arXiv:1008.1290].\n", "versions": [{"version": "v1", "created": "Mon, 5 Nov 2012 11:33:03 GMT"}], "update_date": "2012-11-06", "authors_parsed": [["Chandrasekaran", "Venkat", ""], ["Parrilo", "Pablo A.", ""], ["Willsky", "Alan S.", ""]]}, {"id": "1211.0879", "submitter": "Yanshan Shi", "authors": "Yanshan Shi", "title": "Comparing K-Nearest Neighbors and Potential Energy Method in\n  classification problem. A case study using KNN applet by E.M. Mirkes and real\n  life benchmark data sets", "comments": "23 pages, 27 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  K-nearest neighbors (KNN) method is used in many supervised learning\nclassification problems. Potential Energy (PE) method is also developed for\nclassification problems based on its physical metaphor. The energy potential\nused in the experiments are Yukawa potential and Gaussian Potential. In this\npaper, I use both applet and MATLAB program with real life benchmark data to\nanalyze the performances of KNN and PE method in classification problems. The\nresults show that in general, KNN and PE methods have similar performance. In\nparticular, PE with Yukawa potential has worse performance than KNN when the\ndensity of the data is higher in the distribution of the database. When the\nGaussian potential is applied, the results from PE and KNN have similar\nbehavior. The indicators used are correlation coefficients and information\ngain.\n", "versions": [{"version": "v1", "created": "Mon, 5 Nov 2012 14:48:15 GMT"}], "update_date": "2012-11-06", "authors_parsed": [["Shi", "Yanshan", ""]]}, {"id": "1211.0889", "submitter": "Yi Yu", "authors": "Yi Yu and Yang Feng", "title": "APPLE: Approximate Path for Penalized Likelihood Estimators", "comments": "24 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In high-dimensional data analysis, penalized likelihood estimators are shown\nto provide superior results in both variable selection and parameter\nestimation. A new algorithm, APPLE, is proposed for calculating the Approximate\nPath for Penalized Likelihood Estimators. Both the convex penalty (such as\nLASSO) and the nonconvex penalty (such as SCAD and MCP) cases are considered.\nThe APPLE efficiently computes the solution path for the penalized likelihood\nestimator using a hybrid of the modified predictor-corrector method and the\ncoordinate-descent algorithm. APPLE is compared with several well-known\npackages via simulation and analysis of two gene expression data sets.\n", "versions": [{"version": "v1", "created": "Fri, 2 Nov 2012 07:42:54 GMT"}, {"version": "v2", "created": "Thu, 2 May 2013 14:53:21 GMT"}, {"version": "v3", "created": "Sat, 4 May 2013 06:22:23 GMT"}], "update_date": "2013-05-07", "authors_parsed": [["Yu", "Yi", ""], ["Feng", "Yang", ""]]}, {"id": "1211.0906", "submitter": "Frank Hutter", "authors": "Frank Hutter, Lin Xu, Holger H. Hoos, Kevin Leyton-Brown", "title": "Algorithm Runtime Prediction: Methods & Evaluation", "comments": "51 pages, 13 figures, 8 tables. Added references, feature cost, and\n  experiments with subsets of features; reworded Sections 1&2", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG cs.PF stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Perhaps surprisingly, it is possible to predict how long an algorithm will\ntake to run on a previously unseen input, using machine learning techniques to\nbuild a model of the algorithm's runtime as a function of problem-specific\ninstance features. Such models have important applications to algorithm\nanalysis, portfolio-based algorithm selection, and the automatic configuration\nof parameterized algorithms. Over the past decade, a wide variety of techniques\nhave been studied for building such models. Here, we describe extensions and\nimprovements of existing models, new families of models, and -- perhaps most\nimportantly -- a much more thorough treatment of algorithm parameters as model\ninputs. We also comprehensively describe new and existing features for\npredicting algorithm runtime for propositional satisfiability (SAT), travelling\nsalesperson (TSP) and mixed integer programming (MIP) problems. We evaluate\nthese innovations through the largest empirical analysis of its kind, comparing\nto a wide range of runtime modelling techniques from the literature. Our\nexperiments consider 11 algorithms and 35 instance distributions; they also\nspan a very wide range of SAT, MIP, and TSP instances, with the least\nstructured having been generated uniformly at random and the most structured\nhaving emerged from real industrial applications. Overall, we demonstrate that\nour new models yield substantially better runtime predictions than previous\napproaches in terms of their generalization to new problem instances, to new\nalgorithms from a parameterized space, and to both simultaneously.\n", "versions": [{"version": "v1", "created": "Mon, 5 Nov 2012 16:15:16 GMT"}, {"version": "v2", "created": "Sat, 26 Oct 2013 09:00:50 GMT"}], "update_date": "2013-10-29", "authors_parsed": [["Hutter", "Frank", ""], ["Xu", "Lin", ""], ["Hoos", "Holger H.", ""], ["Leyton-Brown", "Kevin", ""]]}, {"id": "1211.0996", "submitter": "Varun Kanade", "authors": "Pranjal Awasthi, Vitaly Feldman, Varun Kanade", "title": "Learning using Local Membership Queries", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a new model of membership query (MQ) learning, where the\nlearning algorithm is restricted to query points that are \\emph{close} to\nrandom examples drawn from the underlying distribution. The learning model is\nintermediate between the PAC model (Valiant, 1984) and the PAC+MQ model (where\nthe queries are allowed to be arbitrary points).\n  Membership query algorithms are not popular among machine learning\npractitioners. Apart from the obvious difficulty of adaptively querying\nlabelers, it has also been observed that querying \\emph{unnatural} points leads\nto increased noise from human labelers (Lang and Baum, 1992). This motivates\nour study of learning algorithms that make queries that are close to examples\ngenerated from the data distribution.\n  We restrict our attention to functions defined on the $n$-dimensional Boolean\nhypercube and say that a membership query is local if its Hamming distance from\nsome example in the (random) training data is at most $O(\\log(n))$. We show the\nfollowing results in this model:\n  (i) The class of sparse polynomials (with coefficients in R) over $\\{0,1\\}^n$\nis polynomial time learnable under a large class of \\emph{locally smooth}\ndistributions using $O(\\log(n))$-local queries. This class also includes the\nclass of $O(\\log(n))$-depth decision trees.\n  (ii) The class of polynomial-sized decision trees is polynomial time\nlearnable under product distributions using $O(\\log(n))$-local queries.\n  (iii) The class of polynomial size DNF formulas is learnable under the\nuniform distribution using $O(\\log(n))$-local queries in time\n$n^{O(\\log(\\log(n)))}$.\n  (iv) In addition we prove a number of results relating the proposed model to\nthe traditional PAC model and the PAC+MQ model.\n", "versions": [{"version": "v1", "created": "Mon, 5 Nov 2012 20:42:16 GMT"}, {"version": "v2", "created": "Wed, 17 Apr 2013 21:04:47 GMT"}], "update_date": "2013-04-19", "authors_parsed": [["Awasthi", "Pranjal", ""], ["Feldman", "Vitaly", ""], ["Kanade", "Varun", ""]]}, {"id": "1211.1041", "submitter": "Moritz Hardt", "authors": "Moritz Hardt and Ankur Moitra", "title": "Algorithms and Hardness for Robust Subspace Recovery", "comments": "Appeared in Proceedings of COLT 2013", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.DS cs.IT cs.LG math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider a fundamental problem in unsupervised learning called\n\\emph{subspace recovery}: given a collection of $m$ points in $\\mathbb{R}^n$,\nif many but not necessarily all of these points are contained in a\n$d$-dimensional subspace $T$ can we find it? The points contained in $T$ are\ncalled {\\em inliers} and the remaining points are {\\em outliers}. This problem\nhas received considerable attention in computer science and in statistics. Yet\nefficient algorithms from computer science are not robust to {\\em adversarial}\noutliers, and the estimators from robust statistics are hard to compute in high\ndimensions.\n  Are there algorithms for subspace recovery that are both robust to outliers\nand efficient? We give an algorithm that finds $T$ when it contains more than a\n$\\frac{d}{n}$ fraction of the points. Hence, for say $d = n/2$ this estimator\nis both easy to compute and well-behaved when there are a constant fraction of\noutliers. We prove that it is Small Set Expansion hard to find $T$ when the\nfraction of errors is any larger, thus giving evidence that our estimator is an\n{\\em optimal} compromise between efficiency and robustness.\n  As it turns out, this basic problem has a surprising number of connections to\nother areas including small set expansion, matroid theory and functional\nanalysis that we make use of here.\n", "versions": [{"version": "v1", "created": "Mon, 5 Nov 2012 21:39:22 GMT"}, {"version": "v2", "created": "Tue, 20 Nov 2012 14:32:57 GMT"}, {"version": "v3", "created": "Tue, 3 Dec 2013 21:51:26 GMT"}], "update_date": "2013-12-05", "authors_parsed": [["Hardt", "Moritz", ""], ["Moitra", "Ankur", ""]]}, {"id": "1211.1043", "submitter": "Jose Hernandez-Orallo", "authors": "Jose Hernandez-Orallo", "title": "Soft (Gaussian CDE) regression models and loss functions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Regression, unlike classification, has lacked a comprehensive and effective\napproach to deal with cost-sensitive problems by the reuse (and not a\nre-training) of general regression models. In this paper, a wide variety of\ncost-sensitive problems in regression (such as bids, asymmetric losses and\nrejection rules) can be solved effectively by a lightweight but powerful\napproach, consisting of: (1) the conversion of any traditional one-parameter\ncrisp regression model into a two-parameter soft regression model, seen as a\nnormal conditional density estimator, by the use of newly-introduced enrichment\nmethods; and (2) the reframing of an enriched soft regression model to new\ncontexts by an instance-dependent optimisation of the expected loss derived\nfrom the conditional normal distribution.\n", "versions": [{"version": "v1", "created": "Mon, 5 Nov 2012 21:40:38 GMT"}], "update_date": "2012-11-07", "authors_parsed": [["Hernandez-Orallo", "Jose", ""]]}, {"id": "1211.1082", "submitter": "Phil Long", "authors": "Maria Florina Balcan and Philip M. Long", "title": "Active and passive learning of linear separators under log-concave\n  distributions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We provide new results concerning label efficient, polynomial time, passive\nand active learning of linear separators. We prove that active learning\nprovides an exponential improvement over PAC (passive) learning of homogeneous\nlinear separators under nearly log-concave distributions. Building on this, we\nprovide a computationally efficient PAC algorithm with optimal (up to a\nconstant factor) sample complexity for such problems. This resolves an open\nquestion concerning the sample complexity of efficient PAC algorithms under the\nuniform distribution in the unit ball. Moreover, it provides the first bound\nfor a polynomial-time PAC algorithm that is tight for an interesting infinite\nclass of hypothesis functions under a general and natural class of\ndata-distributions, providing significant progress towards a longstanding open\nquestion.\n  We also provide new bounds for active and passive learning in the case that\nthe data might not be linearly separable, both in the agnostic case and and\nunder the Tsybakov low-noise condition. To derive our results, we provide new\nstructural results for (nearly) log-concave distributions, which might be of\nindependent interest as well.\n", "versions": [{"version": "v1", "created": "Tue, 6 Nov 2012 00:21:32 GMT"}, {"version": "v2", "created": "Tue, 5 Mar 2013 00:08:44 GMT"}, {"version": "v3", "created": "Fri, 26 Apr 2013 17:50:21 GMT"}], "update_date": "2013-04-29", "authors_parsed": [["Balcan", "Maria Florina", ""], ["Long", "Philip M.", ""]]}, {"id": "1211.1127", "submitter": "Erik Rodner", "authors": "Erik Rodner", "title": "Visual Transfer Learning: Informal Introduction and Literature Overview", "comments": "part of my PhD thesis", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Transfer learning techniques are important to handle small training sets and\nto allow for quick generalization even from only a few examples. The following\npaper is the introduction as well as the literature overview part of my thesis\nrelated to the topic of transfer learning for visual recognition problems.\n", "versions": [{"version": "v1", "created": "Tue, 6 Nov 2012 07:26:49 GMT"}], "update_date": "2012-11-07", "authors_parsed": [["Rodner", "Erik", ""]]}, {"id": "1211.1255", "submitter": "Antonio Giuliano Zippo Dr.", "authors": "Antonio G. Zippo, Giuliana Gelsomino, Sara Nencini, Gabriele E. M.\n  Biella", "title": "Handwritten digit recognition by bio-inspired hierarchical networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The human brain processes information showing learning and prediction\nabilities but the underlying neuronal mechanisms still remain unknown.\nRecently, many studies prove that neuronal networks are able of both\ngeneralizations and associations of sensory inputs. In this paper, following a\nset of neurophysiological evidences, we propose a learning framework with a\nstrong biological plausibility that mimics prominent functions of cortical\ncircuitries. We developed the Inductive Conceptual Network (ICN), that is a\nhierarchical bio-inspired network, able to learn invariant patterns by\nVariable-order Markov Models implemented in its nodes. The outputs of the\ntop-most node of ICN hierarchy, representing the highest input generalization,\nallow for automatic classification of inputs. We found that the ICN clusterized\nMNIST images with an error of 5.73% and USPS images with an error of 12.56%.\n", "versions": [{"version": "v1", "created": "Tue, 6 Nov 2012 15:15:48 GMT"}], "update_date": "2012-11-07", "authors_parsed": [["Zippo", "Antonio G.", ""], ["Gelsomino", "Giuliana", ""], ["Nencini", "Sara", ""], ["Biella", "Gabriele E. M.", ""]]}, {"id": "1211.1328", "submitter": "Matthew Urry Dr", "authors": "Matthew Urry and Peter Sollich", "title": "Random walk kernels and learning curves for Gaussian process regression\n  on random graphs", "comments": null, "journal-ref": "JMLR(14):1801-1835 2013", "doi": null, "report-no": null, "categories": "stat.ML cond-mat.dis-nn cond-mat.stat-mech cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider learning on graphs, guided by kernels that encode similarity\nbetween vertices. Our focus is on random walk kernels, the analogues of squared\nexponential kernels in Euclidean spaces. We show that on large, locally\ntreelike, graphs these have some counter-intuitive properties, specifically in\nthe limit of large kernel lengthscales. We consider using these kernels as\ncovariance matrices of e.g.\\ Gaussian processes (GPs). In this situation one\ntypically scales the prior globally to normalise the average of the prior\nvariance across vertices. We demonstrate that, in contrast to the Euclidean\ncase, this generically leads to significant variation in the prior variance\nacross vertices, which is undesirable from the probabilistic modelling point of\nview. We suggest the random walk kernel should be normalised locally, so that\neach vertex has the same prior variance, and analyse the consequences of this\nby studying learning curves for Gaussian process regression. Numerical\ncalculations as well as novel theoretical predictions for the learning curves\nusing belief propagation make it clear that one obtains distinctly different\nprobabilistic models depending on the choice of normalisation. Our method for\npredicting the learning curves using belief propagation is significantly more\naccurate than previous approximations and should become exact in the limit of\nlarge random graphs.\n", "versions": [{"version": "v1", "created": "Tue, 6 Nov 2012 17:58:39 GMT"}, {"version": "v2", "created": "Mon, 30 Sep 2013 10:36:51 GMT"}], "update_date": "2013-10-01", "authors_parsed": [["Urry", "Matthew", ""], ["Sollich", "Peter", ""]]}, {"id": "1211.1513", "submitter": "Naresh Manwani", "authors": "Naresh Manwani, P. S. Sastry", "title": "K-Plane Regression", "comments": null, "journal-ref": null, "doi": "10.1016/j.ins.2014.08.058", "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present a novel algorithm for piecewise linear regression\nwhich can learn continuous as well as discontinuous piecewise linear functions.\nThe main idea is to repeatedly partition the data and learn a liner model in in\neach partition. While a simple algorithm incorporating this idea does not work\nwell, an interesting modification results in a good algorithm. The proposed\nalgorithm is similar in spirit to $k$-means clustering algorithm. We show that\nour algorithm can also be viewed as an EM algorithm for maximum likelihood\nestimation of parameters under a reasonable probability model. We empirically\ndemonstrate the effectiveness of our approach by comparing its performance with\nthe state of art regression learning algorithms on some real world datasets.\n", "versions": [{"version": "v1", "created": "Wed, 7 Nov 2012 10:57:38 GMT"}, {"version": "v2", "created": "Wed, 27 Mar 2013 09:00:24 GMT"}], "update_date": "2014-09-10", "authors_parsed": [["Manwani", "Naresh", ""], ["Sastry", "P. S.", ""]]}, {"id": "1211.1526", "submitter": "Liyong Shen", "authors": "Xiaofei Wang, Mingming Zhang, Liyong Shen, Suixiang Gao", "title": "Explosion prediction of oil gas using SVM and Logistic Regression", "comments": "14pages,7 figures, 7 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CE cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The prevention of dangerous chemical accidents is a primary problem of\nindustrial manufacturing. In the accidents of dangerous chemicals, the oil gas\nexplosion plays an important role. The essential task of the explosion\nprevention is to estimate the better explosion limit of a given oil gas. In\nthis paper, Support Vector Machines (SVM) and Logistic Regression (LR) are used\nto predict the explosion of oil gas. LR can get the explicit probability\nformula of explosion, and the explosive range of the concentrations of oil gas\naccording to the concentration of oxygen. Meanwhile, SVM gives higher accuracy\nof prediction. Furthermore, considering the practical requirements, the effects\nof penalty parameter on the distribution of two types of errors are discussed.\n", "versions": [{"version": "v1", "created": "Wed, 7 Nov 2012 12:47:57 GMT"}, {"version": "v2", "created": "Thu, 8 Nov 2012 13:54:13 GMT"}], "update_date": "2012-11-09", "authors_parsed": [["Wang", "Xiaofei", ""], ["Zhang", "Mingming", ""], ["Shen", "Liyong", ""], ["Gao", "Suixiang", ""]]}, {"id": "1211.1544", "submitter": "Harold Christopher Burger Harold Christopher Burger", "authors": "Harold Christopher Burger, Christian J. Schuler, Stefan Harmeling", "title": "Image denoising with multi-layer perceptrons, part 1: comparison with\n  existing algorithms and with bounds", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image denoising can be described as the problem of mapping from a noisy image\nto a noise-free image. The best currently available denoising methods\napproximate this mapping with cleverly engineered algorithms. In this work we\nattempt to learn this mapping directly with plain multi layer perceptrons (MLP)\napplied to image patches. We will show that by training on large image\ndatabases we are able to outperform the current state-of-the-art image\ndenoising methods. In addition, our method achieves results that are superior\nto one type of theoretical bound and goes a large way toward closing the gap\nwith a second type of theoretical bound. Our approach is easily adapted to less\nextensively studied types of noise, such as mixed Poisson-Gaussian noise, JPEG\nartifacts, salt-and-pepper noise and noise resembling stripes, for which we\nachieve excellent results as well. We will show that combining a block-matching\nprocedure with MLPs can further improve the results on certain images. In a\nsecond paper, we detail the training trade-offs and the inner mechanisms of our\nMLPs.\n", "versions": [{"version": "v1", "created": "Wed, 7 Nov 2012 13:35:52 GMT"}, {"version": "v2", "created": "Thu, 8 Nov 2012 09:23:10 GMT"}, {"version": "v3", "created": "Fri, 9 Nov 2012 10:36:22 GMT"}], "update_date": "2012-11-12", "authors_parsed": [["Burger", "Harold Christopher", ""], ["Schuler", "Christian J.", ""], ["Harmeling", "Stefan", ""]]}, {"id": "1211.1550", "submitter": "Bamdev Mishra", "authors": "B. Mishra, K. Adithya Apuroop and R. Sepulchre", "title": "A Riemannian geometry for low-rank matrix completion", "comments": "Title modified, Typos removed. arXiv admin note: text overlap with\n  arXiv:1209.0430", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NA math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new Riemannian geometry for fixed-rank matrices that is\nspecifically tailored to the low-rank matrix completion problem. Exploiting the\ndegree of freedom of a quotient space, we tune the metric on our search space\nto the particular least square cost function. At one level, it illustrates in a\nnovel way how to exploit the versatile framework of optimization on quotient\nmanifold. At another level, our algorithm can be considered as an improved\nversion of LMaFit, the state-of-the-art Gauss-Seidel algorithm. We develop\nnecessary tools needed to perform both first-order and second-order\noptimization. In particular, we propose gradient descent schemes (steepest\ndescent and conjugate gradient) and trust-region algorithms. We also show that,\nthanks to the simplicity of the cost function, it is numerically cheap to\nperform an exact linesearch given a search direction, which makes our\nalgorithms competitive with the state-of-the-art on standard low-rank matrix\ncompletion instances.\n", "versions": [{"version": "v1", "created": "Wed, 7 Nov 2012 13:49:26 GMT"}, {"version": "v2", "created": "Mon, 12 Nov 2012 17:50:39 GMT"}], "update_date": "2012-11-13", "authors_parsed": [["Mishra", "B.", ""], ["Apuroop", "K. Adithya", ""], ["Sepulchre", "R.", ""]]}, {"id": "1211.1552", "submitter": "Harold Christopher Burger Harold Christopher Burger", "authors": "Harold Christopher Burger, Christian J. Schuler, Stefan Harmeling", "title": "Image denoising with multi-layer perceptrons, part 2: training\n  trade-offs and analysis of their mechanisms", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image denoising can be described as the problem of mapping from a noisy image\nto a noise-free image. In another paper, we show that multi-layer perceptrons\ncan achieve outstanding image denoising performance for various types of noise\n(additive white Gaussian noise, mixed Poisson-Gaussian noise, JPEG artifacts,\nsalt-and-pepper noise and noise resembling stripes). In this work we discuss in\ndetail which trade-offs have to be considered during the training procedure. We\nwill show how to achieve good results and which pitfalls to avoid. By analysing\nthe activation patterns of the hidden units we are able to make observations\nregarding the functioning principle of multi-layer perceptrons trained for\nimage denoising.\n", "versions": [{"version": "v1", "created": "Wed, 7 Nov 2012 13:50:19 GMT"}], "update_date": "2012-11-08", "authors_parsed": [["Burger", "Harold Christopher", ""], ["Schuler", "Christian J.", ""], ["Harmeling", "Stefan", ""]]}, {"id": "1211.1690", "submitter": "Debadeepta Dey", "authors": "Stephane Ross, Narek Melik-Barkhudarov, Kumar Shaurya Shankar, Andreas\n  Wendel, Debadeepta Dey, J. Andrew Bagnell, Martial Hebert", "title": "Learning Monocular Reactive UAV Control in Cluttered Natural\n  Environments", "comments": "8 pages, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV cs.LG cs.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Autonomous navigation for large Unmanned Aerial Vehicles (UAVs) is fairly\nstraight-forward, as expensive sensors and monitoring devices can be employed.\nIn contrast, obstacle avoidance remains a challenging task for Micro Aerial\nVehicles (MAVs) which operate at low altitude in cluttered environments. Unlike\nlarge vehicles, MAVs can only carry very light sensors, such as cameras, making\nautonomous navigation through obstacles much more challenging. In this paper,\nwe describe a system that navigates a small quadrotor helicopter autonomously\nat low altitude through natural forest environments. Using only a single cheap\ncamera to perceive the environment, we are able to maintain a constant velocity\nof up to 1.5m/s. Given a small set of human pilot demonstrations, we use recent\nstate-of-the-art imitation learning techniques to train a controller that can\navoid trees by adapting the MAVs heading. We demonstrate the performance of our\nsystem in a more controlled environment indoors, and in real natural forest\nenvironments outdoors.\n", "versions": [{"version": "v1", "created": "Wed, 7 Nov 2012 21:20:23 GMT"}], "update_date": "2012-11-09", "authors_parsed": [["Ross", "Stephane", ""], ["Melik-Barkhudarov", "Narek", ""], ["Shankar", "Kumar Shaurya", ""], ["Wendel", "Andreas", ""], ["Dey", "Debadeepta", ""], ["Bagnell", "J. Andrew", ""], ["Hebert", "Martial", ""]]}, {"id": "1211.1716", "submitter": "James Voss", "authors": "Mikhail Belkin, Luis Rademacher, James Voss", "title": "Blind Signal Separation in the Presence of Gaussian Noise", "comments": "16 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A prototypical blind signal separation problem is the so-called cocktail\nparty problem, with n people talking simultaneously and n different microphones\nwithin a room. The goal is to recover each speech signal from the microphone\ninputs. Mathematically this can be modeled by assuming that we are given\nsamples from an n-dimensional random variable X=AS, where S is a vector whose\ncoordinates are independent random variables corresponding to each speaker. The\nobjective is to recover the matrix A^{-1} given random samples from X. A range\nof techniques collectively known as Independent Component Analysis (ICA) have\nbeen proposed to address this problem in the signal processing and machine\nlearning literature. Many of these techniques are based on using the kurtosis\nor other cumulants to recover the components.\n  In this paper we propose a new algorithm for solving the blind signal\nseparation problem in the presence of additive Gaussian noise, when we are\ngiven samples from X=AS+\\eta, where \\eta is drawn from an unknown, not\nnecessarily spherical n-dimensional Gaussian distribution. Our approach is\nbased on a method for decorrelating a sample with additive Gaussian noise under\nthe assumption that the underlying distribution is a linear transformation of a\ndistribution with independent components. Our decorrelation routine is based on\nthe properties of cumulant tensors and can be combined with any standard\ncumulant-based method for ICA to get an algorithm that is provably robust in\nthe presence of Gaussian noise. We derive polynomial bounds for the sample\ncomplexity and error propagation of our method.\n", "versions": [{"version": "v1", "created": "Wed, 7 Nov 2012 22:50:51 GMT"}, {"version": "v2", "created": "Sun, 9 Jun 2013 04:43:53 GMT"}], "update_date": "2013-06-11", "authors_parsed": [["Belkin", "Mikhail", ""], ["Rademacher", "Luis", ""], ["Voss", "James", ""]]}, {"id": "1211.1722", "submitter": "Ilias Diakonikolas", "authors": "Anindya De, Ilias Diakonikolas, Rocco A. Servedio", "title": "Inverse problems in approximate uniform generation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.DS cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We initiate the study of \\emph{inverse} problems in approximate uniform\ngeneration, focusing on uniform generation of satisfying assignments of various\ntypes of Boolean functions. In such an inverse problem, the algorithm is given\nuniform random satisfying assignments of an unknown function $f$ belonging to a\nclass $\\C$ of Boolean functions, and the goal is to output a probability\ndistribution $D$ which is $\\epsilon$-close, in total variation distance, to the\nuniform distribution over $f^{-1}(1)$.\n  Positive results: We prove a general positive result establishing sufficient\nconditions for efficient inverse approximate uniform generation for a class\n$\\C$. We define a new type of algorithm called a \\emph{densifier} for $\\C$, and\nshow (roughly speaking) how to combine (i) a densifier, (ii) an approximate\ncounting / uniform generation algorithm, and (iii) a Statistical Query learning\nalgorithm, to obtain an inverse approximate uniform generation algorithm. We\napply this general result to obtain a poly$(n,1/\\eps)$-time algorithm for the\nclass of halfspaces; and a quasipoly$(n,1/\\eps)$-time algorithm for the class\nof $\\poly(n)$-size DNF formulas.\n  Negative results: We prove a general negative result establishing that the\nexistence of certain types of signature schemes in cryptography implies the\nhardness of certain inverse approximate uniform generation problems. This\nimplies that there are no {subexponential}-time inverse approximate uniform\ngeneration algorithms for 3-CNF formulas; for intersections of two halfspaces;\nfor degree-2 polynomial threshold functions; and for monotone 2-CNF formulas.\n  Finally, we show that there is no general relationship between the complexity\nof the \"forward\" approximate uniform generation problem and the complexity of\nthe inverse problem for a class $\\C$ -- it is possible for either one to be\neasy while the other is hard.\n", "versions": [{"version": "v1", "created": "Wed, 7 Nov 2012 23:12:00 GMT"}], "update_date": "2012-11-09", "authors_parsed": [["De", "Anindya", ""], ["Diakonikolas", "Ilias", ""], ["Servedio", "Rocco A.", ""]]}, {"id": "1211.1799", "submitter": "Ji\\v{r}\\'i Kaiser", "authors": "Ji\\v{r}\\'i Kaiser", "title": "Algorithm for Missing Values Imputation in Categorical Data with Use of\n  Association Rules", "comments": "4 pages, 3 tables, 2011 Third International Joint Journal Conference\n  in Computer, Electronics and Electrical, ACEEE International Journal on\n  Recent Trends in Engineering & Technology, Vol. 06, Is. 01, Nov 2011", "journal-ref": "ACEEE International Journal on Recent Trends in Engineering &\n  Technology 6 (2011) 111-114", "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents algorithm for missing values imputation in categorical\ndata. The algorithm is based on using association rules and is presented in\nthree variants. Experimental shows better accuracy of missing values imputation\nusing the algorithm then using most common attribute value.\n", "versions": [{"version": "v1", "created": "Thu, 8 Nov 2012 09:22:11 GMT"}], "update_date": "2012-11-09", "authors_parsed": [["Kaiser", "Ji\u0159\u00ed", ""]]}, {"id": "1211.1893", "submitter": "Sofia Karygianni", "authors": "Sofia Karygianni and Pascal Frossard", "title": "Tangent-based manifold approximation with locally linear models", "comments": null, "journal-ref": null, "doi": "10.1016/j.sigpro.2014.03.047", "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we consider the problem of manifold approximation with affine\nsubspaces. Our objective is to discover a set of low dimensional affine\nsubspaces that represents manifold data accurately while preserving the\nmanifold's structure. For this purpose, we employ a greedy technique that\npartitions manifold samples into groups that can be each approximated by a low\ndimensional subspace. We start by considering each manifold sample as a\ndifferent group and we use the difference of tangents to determine appropriate\ngroup mergings. We repeat this procedure until we reach the desired number of\nsample groups. The best low dimensional affine subspaces corresponding to the\nfinal groups constitute our approximate manifold representation. Our\nexperiments verify the effectiveness of the proposed scheme and show its\nsuperior performance compared to state-of-the-art methods for manifold\napproximation.\n", "versions": [{"version": "v1", "created": "Tue, 6 Nov 2012 19:13:21 GMT"}], "update_date": "2015-09-08", "authors_parsed": [["Karygianni", "Sofia", ""], ["Frossard", "Pascal", ""]]}, {"id": "1211.2073", "submitter": "Yang Lu", "authors": "Yang Lu and Mengying Wang and Kenny Q. Zhu and Bo Yuan", "title": "LAGE: A Java Framework to reconstruct Gene Regulatory Networks from\n  Large-Scale Continues Expression Data", "comments": "2 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CE q-bio.QM stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  LAGE is a systematic framework developed in Java. The motivation of LAGE is\nto provide a scalable and parallel solution to reconstruct Gene Regulatory\nNetworks (GRNs) from continuous gene expression data for very large amount of\ngenes. The basic idea of our framework is motivated by the philosophy of\ndivideand-conquer. Specifically, LAGE recursively partitions genes into\nmultiple overlapping communities with much smaller sizes, learns\nintra-community GRNs respectively before merge them altogether. Besides, the\ncomplete information of overlapping communities serves as the byproduct, which\ncould be used to mine meaningful functional modules in biological networks.\n", "versions": [{"version": "v1", "created": "Fri, 9 Nov 2012 08:34:25 GMT"}], "update_date": "2012-11-12", "authors_parsed": [["Lu", "Yang", ""], ["Wang", "Mengying", ""], ["Zhu", "Kenny Q.", ""], ["Yuan", "Bo", ""]]}, {"id": "1211.2190", "submitter": "Luca Martino", "authors": "Jesse Read, Luca Martino, David Luengo", "title": "Efficient Monte Carlo Methods for Multi-Dimensional Learning with\n  Classifier Chains", "comments": "Submitted to Pattern Recognition", "journal-ref": "Pattern Recognition, Volume 47, Issue 3, Pages: 1535-1546, 2014", "doi": "10.1016/j.patcog.2013.10.006", "report-no": null, "categories": "cs.LG stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multi-dimensional classification (MDC) is the supervised learning problem\nwhere an instance is associated with multiple classes, rather than with a\nsingle class, as in traditional classification problems. Since these classes\nare often strongly correlated, modeling the dependencies between them allows\nMDC methods to improve their performance - at the expense of an increased\ncomputational cost. In this paper we focus on the classifier chains (CC)\napproach for modeling dependencies, one of the most popular and highest-\nperforming methods for multi-label classification (MLC), a particular case of\nMDC which involves only binary classes (i.e., labels). The original CC\nalgorithm makes a greedy approximation, and is fast but tends to propagate\nerrors along the chain. Here we present novel Monte Carlo schemes, both for\nfinding a good chain sequence and performing efficient inference. Our\nalgorithms remain tractable for high-dimensional data sets and obtain the best\npredictive performance across several real data sets.\n", "versions": [{"version": "v1", "created": "Fri, 9 Nov 2012 17:21:48 GMT"}, {"version": "v2", "created": "Mon, 17 Dec 2012 18:39:35 GMT"}, {"version": "v3", "created": "Mon, 15 Apr 2013 16:43:20 GMT"}, {"version": "v4", "created": "Sat, 7 Sep 2013 13:10:06 GMT"}], "update_date": "2014-05-20", "authors_parsed": [["Read", "Jesse", ""], ["Martino", "Luca", ""], ["Luengo", "David", ""]]}, {"id": "1211.2227", "submitter": "Joseph Anderson", "authors": "Joseph Anderson, Navin Goyal, Luis Rademacher", "title": "Efficient learning of simplices", "comments": "New author added to this version, Joseph Anderson. New results:\n  reductions from learning a simplex and a linearly transformed l_p ball to ICA\n  (sections 7 and 8)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We show an efficient algorithm for the following problem: Given uniformly\nrandom points from an arbitrary n-dimensional simplex, estimate the simplex.\nThe size of the sample and the number of arithmetic operations of our algorithm\nare polynomial in n. This answers a question of Frieze, Jerrum and Kannan\n[FJK]. Our result can also be interpreted as efficiently learning the\nintersection of n+1 half-spaces in R^n in the model where the intersection is\nbounded and we are given polynomially many uniform samples from it. Our proof\nuses the local search technique from Independent Component Analysis (ICA), also\nused by [FJK]. Unlike these previous algorithms, which were based on analyzing\nthe fourth moment, ours is based on the third moment.\n  We also show a direct connection between the problem of learning a simplex\nand ICA: a simple randomized reduction to ICA from the problem of learning a\nsimplex. The connection is based on a known representation of the uniform\nmeasure on a simplex. Similar representations lead to a reduction from the\nproblem of learning an affine transformation of an n-dimensional l_p ball to\nICA.\n", "versions": [{"version": "v1", "created": "Fri, 9 Nov 2012 20:47:23 GMT"}, {"version": "v2", "created": "Fri, 22 Feb 2013 22:54:40 GMT"}, {"version": "v3", "created": "Thu, 6 Jun 2013 02:52:50 GMT"}], "update_date": "2013-06-07", "authors_parsed": [["Anderson", "Joseph", ""], ["Goyal", "Navin", ""], ["Rademacher", "Luis", ""]]}, {"id": "1211.2260", "submitter": "Hugh Brendan McMahan", "authors": "Matthew Streeter and H. Brendan McMahan", "title": "No-Regret Algorithms for Unconstrained Online Convex Optimization", "comments": "To appear", "journal-ref": "NIPS 2012", "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Some of the most compelling applications of online convex optimization,\nincluding online prediction and classification, are unconstrained: the natural\nfeasible set is R^n. Existing algorithms fail to achieve sub-linear regret in\nthis setting unless constraints on the comparator point x^* are known in\nadvance. We present algorithms that, without such prior knowledge, offer\nnear-optimal regret bounds with respect to any choice of x^*. In particular,\nregret with respect to x^* = 0 is constant. We then prove lower bounds showing\nthat our guarantees are near-optimal in this setting.\n", "versions": [{"version": "v1", "created": "Fri, 9 Nov 2012 22:13:10 GMT"}], "update_date": "2012-11-13", "authors_parsed": [["Streeter", "Matthew", ""], ["McMahan", "H. Brendan", ""]]}, {"id": "1211.2304", "submitter": "Ayan Acharya", "authors": "Ayan Acharya, Eduardo R. Hruschka, Joydeep Ghosh, Badrul Sarwar,\n  Jean-David Ruvini", "title": "Probabilistic Combination of Classifier and Cluster Ensembles for\n  Non-transductive Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Unsupervised models can provide supplementary soft constraints to help\nclassify new target data under the assumption that similar objects in the\ntarget set are more likely to share the same class label. Such models can also\nhelp detect possible differences between training and target distributions,\nwhich is useful in applications where concept drift may take place. This paper\ndescribes a Bayesian framework that takes as input class labels from existing\nclassifiers (designed based on labeled data from the source domain), as well as\ncluster labels from a cluster ensemble operating solely on the target data to\nbe classified, and yields a consensus labeling of the target data. This\nframework is particularly useful when the statistics of the target data drift\nor change from those of the training data. We also show that the proposed\nframework is privacy-aware and allows performing distributed learning when\ndata/models have sharing restrictions. Experiments show that our framework can\nyield superior results to those provided by applying classifier ensembles only.\n", "versions": [{"version": "v1", "created": "Sat, 10 Nov 2012 07:37:44 GMT"}], "update_date": "2015-03-13", "authors_parsed": [["Acharya", "Ayan", ""], ["Hruschka", "Eduardo R.", ""], ["Ghosh", "Joydeep", ""], ["Sarwar", "Badrul", ""], ["Ruvini", "Jean-David", ""]]}, {"id": "1211.2378", "submitter": "Cyril Voyant", "authors": "Cyril Voyant (SPE, CHD Castellucio), Marc Muselli (SPE), Christophe\n  Paoli (SPE), Marie Laure Nivet (SPE)", "title": "Hybrid methodology for hourly global radiation forecasting in\n  Mediterranean area", "comments": null, "journal-ref": null, "doi": "10.1016/j.renene.2012.10.049", "report-no": null, "categories": "cs.NE cs.LG physics.ao-ph stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The renewable energies prediction and particularly global radiation\nforecasting is a challenge studied by a growing number of research teams. This\npaper proposes an original technique to model the insolation time series based\non combining Artificial Neural Network (ANN) and Auto-Regressive and Moving\nAverage (ARMA) model. While ANN by its non-linear nature is effective to\npredict cloudy days, ARMA techniques are more dedicated to sunny days without\ncloud occurrences. Thus, three hybrids models are suggested: the first proposes\nsimply to use ARMA for 6 months in spring and summer and to use an optimized\nANN for the other part of the year; the second model is equivalent to the first\nbut with a seasonal learning; the last model depends on the error occurred the\nprevious hour. These models were used to forecast the hourly global radiation\nfor five places in Mediterranean area. The forecasting performance was compared\namong several models: the 3 above mentioned models, the best ANN and ARMA for\neach location. In the best configuration, the coupling of ANN and ARMA allows\nan improvement of more than 1%, with a maximum in autumn (3.4%) and a minimum\nin winter (0.9%) where ANN alone is the best.\n", "versions": [{"version": "v1", "created": "Sun, 11 Nov 2012 07:16:56 GMT"}], "update_date": "2012-11-13", "authors_parsed": [["Voyant", "Cyril", "", "SPE, CHD Castellucio"], ["Muselli", "Marc", "", "SPE"], ["Paoli", "Christophe", "", "SPE"], ["Nivet", "Marie Laure", "", "SPE"]]}, {"id": "1211.2459", "submitter": "Luis  Sanchez Giraldo", "authors": "Luis G. Sanchez Giraldo and Murali Rao and Jose C. Principe", "title": "Measures of Entropy from Data Using Infinitely Divisible Kernels", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.IT math.IT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Information theory provides principled ways to analyze different inference\nand learning problems such as hypothesis testing, clustering, dimensionality\nreduction, classification, among others. However, the use of information\ntheoretic quantities as test statistics, that is, as quantities obtained from\nempirical data, poses a challenging estimation problem that often leads to\nstrong simplifications such as Gaussian models, or the use of plug in density\nestimators that are restricted to certain representation of the data. In this\npaper, a framework to non-parametrically obtain measures of entropy directly\nfrom data using operators in reproducing kernel Hilbert spaces defined by\ninfinitely divisible kernels is presented. The entropy functionals, which bear\nresemblance with quantum entropies, are defined on positive definite matrices\nand satisfy similar axioms to those of Renyi's definition of entropy.\nConvergence of the proposed estimators follows from concentration results on\nthe difference between the ordered spectrum of the Gram matrices and the\nintegral operators associated to the population quantities. In this way,\ncapitalizing on both the axiomatic definition of entropy and on the\nrepresentation power of positive definite kernels, the proposed measure of\nentropy avoids the estimation of the probability distribution underlying the\ndata. Moreover, estimators of kernel-based conditional entropy and mutual\ninformation are also defined. Numerical experiments on independence tests\ncompare favourably with state of the art.\n", "versions": [{"version": "v1", "created": "Sun, 11 Nov 2012 20:49:28 GMT"}, {"version": "v2", "created": "Mon, 9 Dec 2013 19:38:31 GMT"}, {"version": "v3", "created": "Mon, 1 Sep 2014 21:52:55 GMT"}], "update_date": "2014-09-03", "authors_parsed": [["Giraldo", "Luis G. Sanchez", ""], ["Rao", "Murali", ""], ["Principe", "Jose C.", ""]]}, {"id": "1211.2476", "submitter": "Hossein Azari Soufiani", "authors": "Hossein Azari Soufiani, David C. Parkes, Lirong Xia", "title": "Random Utility Theory for Social Choice", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MA cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Random utility theory models an agent's preferences on alternatives by\ndrawing a real-valued score on each alternative (typically independently) from\na parameterized distribution, and then ranking the alternatives according to\nscores. A special case that has received significant attention is the\nPlackett-Luce model, for which fast inference methods for maximum likelihood\nestimators are available. This paper develops conditions on general random\nutility models that enable fast inference within a Bayesian framework through\nMC-EM, providing concave loglikelihood functions and bounded sets of global\nmaxima solutions. Results on both real-world and simulated data provide support\nfor the scalability of the approach and capability for model selection among\ngeneral random utility models including Plackett-Luce.\n", "versions": [{"version": "v1", "created": "Sun, 11 Nov 2012 23:09:02 GMT"}], "update_date": "2012-11-13", "authors_parsed": [["Soufiani", "Hossein Azari", ""], ["Parkes", "David C.", ""], ["Xia", "Lirong", ""]]}, {"id": "1211.2512", "submitter": "Hong Zhao", "authors": "Hong Zhao, Fan Min and William Zhu", "title": "Minimal cost feature selection of data with normal distribution\n  measurement errors", "comments": "This paper has been withdrawn by the author due to an error of the\n  title", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Minimal cost feature selection is devoted to obtain a trade-off between test\ncosts and misclassification costs. This issue has been addressed recently on\nnominal data. In this paper, we consider numerical data with measurement errors\nand study minimal cost feature selection in this model. First, we build a data\nmodel with normal distribution measurement errors. Second, the neighborhood of\neach data item is constructed through the confidence interval. Comparing with\ndiscretized intervals, neighborhoods are more reasonable to maintain the\ninformation of data. Third, we define a new minimal total cost feature\nselection problem through considering the trade-off between test costs and\nmisclassification costs. Fourth, we proposed a backtracking algorithm with\nthree effective pruning techniques to deal with this problem. The algorithm is\ntested on four UCI data sets. Experimental results indicate that the pruning\ntechniques are effective, and the algorithm is efficient for data sets with\nnearly one thousand objects.\n", "versions": [{"version": "v1", "created": "Mon, 12 Nov 2012 05:26:20 GMT"}, {"version": "v2", "created": "Mon, 3 Jun 2013 02:43:45 GMT"}], "update_date": "2013-06-04", "authors_parsed": [["Zhao", "Hong", ""], ["Min", "Fan", ""], ["Zhu", "William", ""]]}, {"id": "1211.2532", "submitter": "Benjamin Rolfs", "authors": "Dominique Guillot and Bala Rajaratnam and Benjamin T. Rolfs and Arian\n  Maleki and Ian Wong", "title": "Iterative Thresholding Algorithm for Sparse Inverse Covariance\n  Estimation", "comments": "25 pages, 1 figure, 4 tables. Conference paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The L1-regularized maximum likelihood estimation problem has recently become\na topic of great interest within the machine learning, statistics, and\noptimization communities as a method for producing sparse inverse covariance\nestimators. In this paper, a proximal gradient method (G-ISTA) for performing\nL1-regularized covariance matrix estimation is presented. Although numerous\nalgorithms have been proposed for solving this problem, this simple proximal\ngradient method is found to have attractive theoretical and numerical\nproperties. G-ISTA has a linear rate of convergence, resulting in an O(log e)\niteration complexity to reach a tolerance of e. This paper gives eigenvalue\nbounds for the G-ISTA iterates, providing a closed-form linear convergence\nrate. The rate is shown to be closely related to the condition number of the\noptimal point. Numerical convergence results and timing comparisons for the\nproposed method are presented. G-ISTA is shown to perform very well, especially\nwhen the optimal point is well-conditioned.\n", "versions": [{"version": "v1", "created": "Mon, 12 Nov 2012 08:35:26 GMT"}, {"version": "v2", "created": "Wed, 14 Nov 2012 01:22:30 GMT"}, {"version": "v3", "created": "Tue, 27 Nov 2012 04:48:51 GMT"}], "update_date": "2012-11-28", "authors_parsed": [["Guillot", "Dominique", ""], ["Rajaratnam", "Bala", ""], ["Rolfs", "Benjamin T.", ""], ["Maleki", "Arian", ""], ["Wong", "Ian", ""]]}, {"id": "1211.2556", "submitter": "Fatai Anifowose", "authors": "Fatai Adesina Anifowose", "title": "A Comparative Study of Gaussian Mixture Model and Radial Basis Function\n  for Voice Recognition", "comments": "9 pages, 10 figures; International Journal of Advanced Computer\n  Science and Applications (IJACSA), Vol. 1, No.3, September 2010", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A comparative study of the application of Gaussian Mixture Model (GMM) and\nRadial Basis Function (RBF) in biometric recognition of voice has been carried\nout and presented. The application of machine learning techniques to biometric\nauthentication and recognition problems has gained a widespread acceptance. In\nthis research, a GMM model was trained, using Expectation Maximization (EM)\nalgorithm, on a dataset containing 10 classes of vowels and the model was used\nto predict the appropriate classes using a validation dataset. For experimental\nvalidity, the model was compared to the performance of two different versions\nof RBF model using the same learning and validation datasets. The results\nshowed very close recognition accuracy between the GMM and the standard RBF\nmodel, but with GMM performing better than the standard RBF by less than 1% and\nthe two models outperformed similar models reported in literature. The DTREG\nversion of RBF outperformed the other two models by producing 94.8% recognition\naccuracy. In terms of recognition time, the standard RBF was found to be the\nfastest among the three models.\n", "versions": [{"version": "v1", "created": "Mon, 12 Nov 2012 10:42:58 GMT"}], "update_date": "2012-11-13", "authors_parsed": [["Anifowose", "Fatai Adesina", ""]]}, {"id": "1211.2717", "submitter": "Tong Zhang", "authors": "Shai Shalev-Shwartz and Tong Zhang", "title": "Proximal Stochastic Dual Coordinate Ascent", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a proximal version of dual coordinate ascent method. We\ndemonstrate how the derived algorithmic framework can be used for numerous\nregularized loss minimization problems, including $\\ell_1$ regularization and\nstructured output SVM. The convergence rates we obtain match, and sometimes\nimprove, state-of-the-art results.\n", "versions": [{"version": "v1", "created": "Mon, 12 Nov 2012 18:08:34 GMT"}], "update_date": "2012-11-13", "authors_parsed": [["Shalev-Shwartz", "Shai", ""], ["Zhang", "Tong", ""]]}, {"id": "1211.2881", "submitter": "Junyoung Chung", "authors": "Junyoung Chung, Donghoon Lee, Youngjoo Seo, and Chang D. Yoo", "title": "Deep Attribute Networks", "comments": "This paper has been withdrawn by the author due to a crucial\n  grammatical errors", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Obtaining compact and discriminative features is one of the major challenges\nin many of the real-world image classification tasks such as face verification\nand object recognition. One possible approach is to represent input image on\nthe basis of high-level features that carry semantic meaning which humans can\nunderstand. In this paper, a model coined deep attribute network (DAN) is\nproposed to address this issue. For an input image, the model outputs the\nattributes of the input image without performing any classification. The\nefficacy of the proposed model is evaluated on unconstrained face verification\nand real-world object recognition tasks using the LFW and the a-PASCAL\ndatasets. We demonstrate the potential of deep learning for attribute-based\nclassification by showing comparable results with existing state-of-the-art\nresults. Once properly trained, the DAN is fast and does away with calculating\nlow-level features which are maybe unreliable and computationally expensive.\n", "versions": [{"version": "v1", "created": "Tue, 13 Nov 2012 03:41:31 GMT"}, {"version": "v2", "created": "Tue, 20 Nov 2012 11:30:46 GMT"}, {"version": "v3", "created": "Wed, 28 Nov 2012 08:39:03 GMT"}], "update_date": "2012-11-29", "authors_parsed": [["Chung", "Junyoung", ""], ["Lee", "Donghoon", ""], ["Seo", "Youngjoo", ""], ["Yoo", "Chang D.", ""]]}, {"id": "1211.2891", "submitter": "Lior Rokach", "authors": "Ariel Bar, Lior Rokach, Guy Shani, Bracha Shapira, Alon Schclar", "title": "Boosting Simple Collaborative Filtering Models Using Ensemble Methods", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we examine the effect of applying ensemble learning to the\nperformance of collaborative filtering methods. We present several systematic\napproaches for generating an ensemble of collaborative filtering models based\non a single collaborative filtering algorithm (single-model or homogeneous\nensemble). We present an adaptation of several popular ensemble techniques in\nmachine learning for the collaborative filtering domain, including bagging,\nboosting, fusion and randomness injection. We evaluate the proposed approach on\nseveral types of collaborative filtering base models: k- NN, matrix\nfactorization and a neighborhood matrix factorization model. Empirical\nevaluation shows a prediction improvement compared to all base CF algorithms.\nIn particular, we show that the performance of an ensemble of simple (weak) CF\nmodels such as k-NN is competitive compared with a single strong CF model (such\nas matrix factorization) while requiring an order of magnitude less\ncomputational cost.\n", "versions": [{"version": "v1", "created": "Tue, 13 Nov 2012 05:30:36 GMT"}], "update_date": "2012-11-14", "authors_parsed": [["Bar", "Ariel", ""], ["Rokach", "Lior", ""], ["Shani", "Guy", ""], ["Shapira", "Bracha", ""], ["Schclar", "Alon", ""]]}, {"id": "1211.2980", "submitter": "Shay Moran", "authors": "Shay Moran", "title": "Shattering-Extremal Systems", "comments": "42 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.CO cs.CG cs.DM cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Shatters relation and the VC dimension have been investigated since the\nearly seventies. These concepts have found numerous applications in statistics,\ncombinatorics, learning theory and computational geometry. Shattering extremal\nsystems are set-systems with a very rich structure and many different\ncharacterizations. The goal of this thesis is to elaborate on the structure of\nthese systems.\n", "versions": [{"version": "v1", "created": "Tue, 13 Nov 2012 13:16:48 GMT"}], "update_date": "2012-11-14", "authors_parsed": [["Moran", "Shay", ""]]}, {"id": "1211.3010", "submitter": "Sriharsha Veeramachaneni", "authors": "Sriharsha Veeramachaneni", "title": "Time-series Scenario Forecasting", "comments": "16 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many applications require the ability to judge uncertainty of time-series\nforecasts. Uncertainty is often specified as point-wise error bars around a\nmean or median forecast. Due to temporal dependencies, such a method obscures\nsome information. We would ideally have a way to query the posterior\nprobability of the entire time-series given the predictive variables, or at a\nminimum, be able to draw samples from this distribution. We use a Bayesian\ndictionary learning algorithm to statistically generate an ensemble of\nforecasts. We show that the algorithm performs as well as a physics-based\nensemble method for temperature forecasts for Houston. We conclude that the\nmethod shows promise for scenario forecasting where physics-based methods are\nabsent.\n", "versions": [{"version": "v1", "created": "Tue, 13 Nov 2012 14:54:47 GMT"}], "update_date": "2012-11-14", "authors_parsed": [["Veeramachaneni", "Sriharsha", ""]]}, {"id": "1211.3046", "submitter": "Lijun Zhang", "authors": "Lijun Zhang, Mehrdad Mahdavi, Rong Jin, Tianbao Yang, Shenghuo Zhu", "title": "Recovering the Optimal Solution by Dual Random Projection", "comments": "The 26th Annual Conference on Learning Theory (COLT 2013)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Random projection has been widely used in data classification. It maps\nhigh-dimensional data into a low-dimensional subspace in order to reduce the\ncomputational cost in solving the related optimization problem. While previous\nstudies are focused on analyzing the classification performance of using random\nprojection, in this work, we consider the recovery problem, i.e., how to\naccurately recover the optimal solution to the original optimization problem in\nthe high-dimensional space based on the solution learned from the subspace\nspanned by random projections. We present a simple algorithm, termed Dual\nRandom Projection, that uses the dual solution of the low-dimensional\noptimization problem to recover the optimal solution to the original problem.\nOur theoretical analysis shows that with a high probability, the proposed\nalgorithm is able to accurately recover the optimal solution to the original\nproblem, provided that the data matrix is of low rank or can be well\napproximated by a low rank matrix.\n", "versions": [{"version": "v1", "created": "Tue, 13 Nov 2012 16:39:45 GMT"}, {"version": "v2", "created": "Thu, 15 Nov 2012 22:09:06 GMT"}, {"version": "v3", "created": "Tue, 2 Apr 2013 19:05:01 GMT"}, {"version": "v4", "created": "Fri, 21 Feb 2014 20:57:42 GMT"}], "update_date": "2014-02-24", "authors_parsed": [["Zhang", "Lijun", ""], ["Mahdavi", "Mehrdad", ""], ["Jin", "Rong", ""], ["Yang", "Tianbao", ""], ["Zhu", "Shenghuo", ""]]}, {"id": "1211.3212", "submitter": "Varun Kanade", "authors": "Varun Kanade, Zhenming Liu, Bozidar Radunovic", "title": "Distributed Non-Stochastic Experts", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the online distributed non-stochastic experts problem, where the\ndistributed system consists of one coordinator node that is connected to $k$\nsites, and the sites are required to communicate with each other via the\ncoordinator. At each time-step $t$, one of the $k$ site nodes has to pick an\nexpert from the set ${1, ..., n}$, and the same site receives information about\npayoffs of all experts for that round. The goal of the distributed system is to\nminimize regret at time horizon $T$, while simultaneously keeping communication\nto a minimum.\n  The two extreme solutions to this problem are: (i) Full communication: This\nessentially simulates the non-distributed setting to obtain the optimal\n$O(\\sqrt{\\log(n)T})$ regret bound at the cost of $T$ communication. (ii) No\ncommunication: Each site runs an independent copy : the regret is\n$O(\\sqrt{log(n)kT})$ and the communication is 0. This paper shows the\ndifficulty of simultaneously achieving regret asymptotically better than\n$\\sqrt{kT}$ and communication better than $T$. We give a novel algorithm that\nfor an oblivious adversary achieves a non-trivial trade-off: regret\n$O(\\sqrt{k^{5(1+\\epsilon)/6} T})$ and communication $O(T/k^{\\epsilon})$, for\nany value of $\\epsilon \\in (0, 1/5)$. We also consider a variant of the model,\nwhere the coordinator picks the expert. In this model, we show that the\nlabel-efficient forecaster of Cesa-Bianchi et al. (2005) already gives us\nstrategy that is near optimal in regret vs communication trade-off.\n", "versions": [{"version": "v1", "created": "Wed, 14 Nov 2012 06:45:38 GMT"}], "update_date": "2012-11-15", "authors_parsed": [["Kanade", "Varun", ""], ["Liu", "Zhenming", ""], ["Radunovic", "Bozidar", ""]]}, {"id": "1211.3295", "submitter": "Diego Colombo", "authors": "Diego Colombo and Marloes H. Maathuis", "title": "Order-independent constraint-based causal structure learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider constraint-based methods for causal structure learning, such as\nthe PC-, FCI-, RFCI- and CCD- algorithms (Spirtes et al. (2000, 1993),\nRichardson (1996), Colombo et al. (2012), Claassen et al. (2013)). The first\nstep of all these algorithms consists of the PC-algorithm. This algorithm is\nknown to be order-dependent, in the sense that the output can depend on the\norder in which the variables are given. This order-dependence is a minor issue\nin low-dimensional settings. We show, however, that it can be very pronounced\nin high-dimensional settings, where it can lead to highly variable results. We\npropose several modifications of the PC-algorithm (and hence also of the other\nalgorithms) that remove part or all of this order-dependence. All proposed\nmodifications are consistent in high-dimensional settings under the same\nconditions as their original counterparts. We compare the PC-, FCI-, and\nRFCI-algorithms and their modifications in simulation studies and on a yeast\ngene expression data set. We show that our modifications yield similar\nperformance in low-dimensional settings and improved performance in\nhigh-dimensional settings. All software is implemented in the R-package pcalg.\n", "versions": [{"version": "v1", "created": "Wed, 14 Nov 2012 12:56:06 GMT"}, {"version": "v2", "created": "Fri, 27 Sep 2013 15:56:21 GMT"}], "update_date": "2013-09-30", "authors_parsed": [["Colombo", "Diego", ""], ["Maathuis", "Marloes H.", ""]]}, {"id": "1211.3412", "submitter": "Nesreen Ahmed", "authors": "Nesreen K. Ahmed and Jennifer Neville and Ramana Kompella", "title": "Network Sampling: From Static to Streaming Graphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.DS cs.LG physics.soc-ph stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Network sampling is integral to the analysis of social, information, and\nbiological networks. Since many real-world networks are massive in size,\ncontinuously evolving, and/or distributed in nature, the network structure is\noften sampled in order to facilitate study. For these reasons, a more thorough\nand complete understanding of network sampling is critical to support the field\nof network science. In this paper, we outline a framework for the general\nproblem of network sampling, by highlighting the different objectives,\npopulation and units of interest, and classes of network sampling methods. In\naddition, we propose a spectrum of computational models for network sampling\nmethods, ranging from the traditionally studied model based on the assumption\nof a static domain to a more challenging model that is appropriate for\nstreaming domains. We design a family of sampling methods based on the concept\nof graph induction that generalize across the full spectrum of computational\nmodels (from static to streaming) while efficiently preserving many of the\ntopological properties of the input graphs. Furthermore, we demonstrate how\ntraditional static sampling algorithms can be modified for graph streams for\neach of the three main classes of sampling methods: node, edge, and\ntopology-based sampling. Our experimental results indicate that our proposed\nfamily of sampling methods more accurately preserves the underlying properties\nof the graph for both static and streaming graphs. Finally, we study the impact\nof network sampling algorithms on the parameter estimation and performance\nevaluation of relational classification algorithms.\n", "versions": [{"version": "v1", "created": "Wed, 14 Nov 2012 01:48:37 GMT"}], "update_date": "2012-11-16", "authors_parsed": [["Ahmed", "Nesreen K.", ""], ["Neville", "Jennifer", ""], ["Kompella", "Ramana", ""]]}, {"id": "1211.3444", "submitter": "Deanna Needell", "authors": "B. Cung, T. Jin, J. Ramirez, A. Thompson, C. Boutsidis and D. Needell", "title": "Spectral Clustering: An empirical study of Approximation Algorithms and\n  its Application to the Attrition Problem", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.NA stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Clustering is the problem of separating a set of objects into groups (called\nclusters) so that objects within the same cluster are more similar to each\nother than to those in different clusters. Spectral clustering is a now\nwell-known method for clustering which utilizes the spectrum of the data\nsimilarity matrix to perform this separation. Since the method relies on\nsolving an eigenvector problem, it is computationally expensive for large\ndatasets. To overcome this constraint, approximation methods have been\ndeveloped which aim to reduce running time while maintaining accurate\nclassification. In this article, we summarize and experimentally evaluate\nseveral approximation methods for spectral clustering. From an applications\nstandpoint, we employ spectral clustering to solve the so-called attrition\nproblem, where one aims to identify from a set of employees those who are\nlikely to voluntarily leave the company from those who are not. Our study sheds\nlight on the empirical performance of existing approximate spectral clustering\nmethods and shows the applicability of these methods in an important business\noptimization related problem.\n", "versions": [{"version": "v1", "created": "Wed, 14 Nov 2012 22:05:09 GMT"}], "update_date": "2012-11-16", "authors_parsed": [["Cung", "B.", ""], ["Jin", "T.", ""], ["Ramirez", "J.", ""], ["Thompson", "A.", ""], ["Boutsidis", "C.", ""], ["Needell", "D.", ""]]}, {"id": "1211.3500", "submitter": "Guoxu Zhou", "authors": "Guoxu Zhou, Andrzej Cichocki, and Shengli Xie", "title": "Accelerated Canonical Polyadic Decomposition by Using Mode Reduction", "comments": "12 pages. Accepted by TNNLS", "journal-ref": null, "doi": "10.1109/TNNLS.2013.2271507", "report-no": null, "categories": "cs.NA cs.LG math.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Canonical Polyadic (or CANDECOMP/PARAFAC, CP) decompositions (CPD) are widely\napplied to analyze high order tensors. Existing CPD methods use alternating\nleast square (ALS) iterations and hence need to unfold tensors to each of the\n$N$ modes frequently, which is one major bottleneck of efficiency for\nlarge-scale data and especially when $N$ is large. To overcome this problem, in\nthis paper we proposed a new CPD method which converts the original $N$th\n($N>3$) order tensor to a 3rd-order tensor first. Then the full CPD is realized\nby decomposing this mode reduced tensor followed by a Khatri-Rao product\nprojection procedure. This way is quite efficient as unfolding to each of the\n$N$ modes are avoided, and dimensionality reduction can also be easily\nincorporated to further improve the efficiency. We show that, under mild\nconditions, any $N$th-order CPD can be converted into a 3rd-order case but\nwithout destroying the essential uniqueness, and theoretically gives the same\nresults as direct $N$-way CPD methods. Simulations show that, compared with\nstate-of-the-art CPD methods, the proposed method is more efficient and escape\nfrom local solutions more easily.\n", "versions": [{"version": "v1", "created": "Thu, 15 Nov 2012 05:50:30 GMT"}, {"version": "v2", "created": "Tue, 25 Jun 2013 03:06:52 GMT"}], "update_date": "2013-06-27", "authors_parsed": [["Zhou", "Guoxu", ""], ["Cichocki", "Andrzej", ""], ["Xie", "Shengli", ""]]}, {"id": "1211.3711", "submitter": "Alex Graves", "authors": "Alex Graves", "title": "Sequence Transduction with Recurrent Neural Networks", "comments": "First published in the International Conference of Machine Learning\n  (ICML) 2012 Workshop on Representation Learning", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many machine learning tasks can be expressed as the transformation---or\n\\emph{transduction}---of input sequences into output sequences: speech\nrecognition, machine translation, protein secondary structure prediction and\ntext-to-speech to name but a few. One of the key challenges in sequence\ntransduction is learning to represent both the input and output sequences in a\nway that is invariant to sequential distortions such as shrinking, stretching\nand translating. Recurrent neural networks (RNNs) are a powerful sequence\nlearning architecture that has proven capable of learning such representations.\nHowever RNNs traditionally require a pre-defined alignment between the input\nand output sequences to perform transduction. This is a severe limitation since\n\\emph{finding} the alignment is the most difficult aspect of many sequence\ntransduction problems. Indeed, even determining the length of the output\nsequence is often challenging. This paper introduces an end-to-end,\nprobabilistic sequence transduction system, based entirely on RNNs, that is in\nprinciple able to transform any input sequence into any finite, discrete output\nsequence. Experimental results for phoneme recognition are provided on the\nTIMIT speech corpus.\n", "versions": [{"version": "v1", "created": "Wed, 14 Nov 2012 19:25:21 GMT"}], "update_date": "2012-11-16", "authors_parsed": [["Graves", "Alex", ""]]}, {"id": "1211.3831", "submitter": "Youhei Akimoto", "authors": "Youhei Akimoto (INRIA Saclay - Ile de France), Yann Ollivier (LRI)", "title": "Objective Improvement in Information-Geometric Optimization", "comments": null, "journal-ref": "Foundations of Genetic Algorithms XII (2013)", "doi": null, "report-no": null, "categories": "cs.LG cs.AI math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Information-Geometric Optimization (IGO) is a unified framework of stochastic\nalgorithms for optimization problems. Given a family of probability\ndistributions, IGO turns the original optimization problem into a new\nmaximization problem on the parameter space of the probability distributions.\nIGO updates the parameter of the probability distribution along the natural\ngradient, taken with respect to the Fisher metric on the parameter manifold,\naiming at maximizing an adaptive transform of the objective function. IGO\nrecovers several known algorithms as particular instances: for the family of\nBernoulli distributions IGO recovers PBIL, for the family of Gaussian\ndistributions the pure rank-mu CMA-ES update is recovered, and for exponential\nfamilies in expectation parametrization the cross-entropy/ML method is\nrecovered. This article provides a theoretical justification for the IGO\nframework, by proving that any step size not greater than 1 guarantees monotone\nimprovement over the course of optimization, in terms of q-quantile values of\nthe objective function f. The range of admissible step sizes is independent of\nf and its domain. We extend the result to cover the case of different step\nsizes for blocks of the parameters in the IGO algorithm. Moreover, we prove\nthat expected fitness improves over time when fitness-proportional selection is\napplied, in which case the RPP algorithm is recovered.\n", "versions": [{"version": "v1", "created": "Fri, 16 Nov 2012 08:54:08 GMT"}, {"version": "v2", "created": "Thu, 29 Nov 2012 12:57:45 GMT"}, {"version": "v3", "created": "Thu, 7 Mar 2013 13:36:08 GMT"}], "update_date": "2013-03-08", "authors_parsed": [["Akimoto", "Youhei", "", "INRIA Saclay - Ile de France"], ["Ollivier", "Yann", "", "LRI"]]}, {"id": "1211.3955", "submitter": "Hugh Brendan McMahan", "authors": "H. Brendan McMahan and Omkar Muralidharan", "title": "On Calibrated Predictions for Auction Selection Mechanisms", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GT cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Calibration is a basic property for prediction systems, and algorithms for\nachieving it are well-studied in both statistics and machine learning. In many\napplications, however, the predictions are used to make decisions that select\nwhich observations are made. This makes calibration difficult, as adjusting\npredictions to achieve calibration changes future data. We focus on\nclick-through-rate (CTR) prediction for search ad auctions. Here, CTR\npredictions are used by an auction that determines which ads are shown, and we\nwant to maximize the value generated by the auction.\n  We show that certain natural notions of calibration can be impossible to\nachieve, depending on the details of the auction. We also show that it can be\nimpossible to maximize auction efficiency while using calibrated predictions.\nFinally, we give conditions under which calibration is achievable and\nsimultaneously maximizes auction efficiency: roughly speaking, bids and queries\nmust not contain information about CTRs that is not already captured by the\npredictions.\n", "versions": [{"version": "v1", "created": "Fri, 16 Nov 2012 17:07:33 GMT"}], "update_date": "2012-11-19", "authors_parsed": [["McMahan", "H. Brendan", ""], ["Muralidharan", "Omkar", ""]]}, {"id": "1211.3966", "submitter": "Jie  Wang", "authors": "Jie Wang, Peter Wonka, Jieping Ye", "title": "Lasso Screening Rules via Dual Polytope Projection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  Lasso is a widely used regression technique to find sparse representations.\nWhen the dimension of the feature space and the number of samples are extremely\nlarge, solving the Lasso problem remains challenging. To improve the efficiency\nof solving large-scale Lasso problems, El Ghaoui and his colleagues have\nproposed the SAFE rules which are able to quickly identify the inactive\npredictors, i.e., predictors that have $0$ components in the solution vector.\nThen, the inactive predictors or features can be removed from the optimization\nproblem to reduce its scale. By transforming the standard Lasso to its dual\nform, it can be shown that the inactive predictors include the set of inactive\nconstraints on the optimal dual solution. In this paper, we propose an\nefficient and effective screening rule via Dual Polytope Projections (DPP),\nwhich is mainly based on the uniqueness and nonexpansiveness of the optimal\ndual solution due to the fact that the feasible set in the dual space is a\nconvex and closed polytope. Moreover, we show that our screening rule can be\nextended to identify inactive groups in group Lasso. To the best of our\nknowledge, there is currently no \"exact\" screening rule for group Lasso. We\nhave evaluated our screening rule using synthetic and real data sets. Results\nshow that our rule is more effective in identifying inactive predictors than\nexisting state-of-the-art screening rules for Lasso.\n", "versions": [{"version": "v1", "created": "Fri, 16 Nov 2012 17:48:42 GMT"}, {"version": "v2", "created": "Sat, 1 Feb 2014 00:12:10 GMT"}, {"version": "v3", "created": "Wed, 15 Oct 2014 20:18:33 GMT"}], "update_date": "2014-10-17", "authors_parsed": [["Wang", "Jie", ""], ["Wonka", "Peter", ""], ["Ye", "Jieping", ""]]}, {"id": "1211.4116", "submitter": "Louis Theran", "authors": "Franz J. Kir\\'aly, Louis Theran, Ryota Tomioka", "title": "The Algebraic Combinatorial Approach for Low-Rank Matrix Completion", "comments": "37 pages, with an appendix by Takeaki Uno", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NA math.AG math.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a novel algebraic combinatorial view on low-rank matrix completion\nbased on studying relations between a few entries with tools from algebraic\ngeometry and matroid theory. The intrinsic locality of the approach allows for\nthe treatment of single entries in a closed theoretical and practical\nframework. More specifically, apart from introducing an algebraic combinatorial\ntheory of low-rank matrix completion, we present probability-one algorithms to\ndecide whether a particular entry of the matrix can be completed. We also\ndescribe methods to complete that entry from a few others, and to estimate the\nerror which is incurred by any method completing that entry. Furthermore, we\nshow how known results on matrix completion and their sampling assumptions can\nbe related to our new perspective and interpreted in terms of a completability\nphase transition.\n", "versions": [{"version": "v1", "created": "Sat, 17 Nov 2012 12:23:36 GMT"}, {"version": "v2", "created": "Mon, 26 Nov 2012 00:07:26 GMT"}, {"version": "v3", "created": "Wed, 17 Apr 2013 06:43:36 GMT"}, {"version": "v4", "created": "Tue, 19 Aug 2014 15:00:30 GMT"}], "update_date": "2014-08-20", "authors_parsed": [["Kir\u00e1ly", "Franz J.", ""], ["Theran", "Louis", ""], ["Tomioka", "Ryota", ""]]}, {"id": "1211.4142", "submitter": "Shaina Race", "authors": "Ralph Abbey, Jeremy Diepenbrock, Amy Langville, Carl Meyer, Shaina\n  Race, Dexin Zhou", "title": "Data Clustering via Principal Direction Gap Partitioning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We explore the geometrical interpretation of the PCA based clustering\nalgorithm Principal Direction Divisive Partitioning (PDDP). We give several\nexamples where this algorithm breaks down, and suggest a new method, gap\npartitioning, which takes into account natural gaps in the data between\nclusters. Geometric features of the PCA space are derived and illustrated and\nexperimental results are given which show our method is comparable on the\ndatasets used in the original paper on PDDP.\n", "versions": [{"version": "v1", "created": "Sat, 17 Nov 2012 18:28:30 GMT"}], "update_date": "2012-11-20", "authors_parsed": [["Abbey", "Ralph", ""], ["Diepenbrock", "Jeremy", ""], ["Langville", "Amy", ""], ["Meyer", "Carl", ""], ["Race", "Shaina", ""], ["Zhou", "Dexin", ""]]}, {"id": "1211.4150", "submitter": "Aaron Roth", "authors": "Morteza Zadimoghaddam and Aaron Roth", "title": "Efficiently Learning from Revealed Preference", "comments": "Extended abstract appears in WINE 2012", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GT cs.DS cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we consider the revealed preferences problem from a learning\nperspective. Every day, a price vector and a budget is drawn from an unknown\ndistribution, and a rational agent buys his most preferred bundle according to\nsome unknown utility function, subject to the given prices and budget\nconstraint. We wish not only to find a utility function which rationalizes a\nfinite set of observations, but to produce a hypothesis valuation function\nwhich accurately predicts the behavior of the agent in the future. We give\nefficient algorithms with polynomial sample-complexity for agents with linear\nvaluation functions, as well as for agents with linearly separable, concave\nvaluation functions with bounded second derivative.\n", "versions": [{"version": "v1", "created": "Sat, 17 Nov 2012 19:30:52 GMT"}], "update_date": "2012-11-20", "authors_parsed": [["Zadimoghaddam", "Morteza", ""], ["Roth", "Aaron", ""]]}, {"id": "1211.4246", "submitter": "Yoshua Bengio", "authors": "Guillaume Alain and Yoshua Bengio", "title": "What Regularized Auto-Encoders Learn from the Data Generating\n  Distribution", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  What do auto-encoders learn about the underlying data generating\ndistribution? Recent work suggests that some auto-encoder variants do a good\njob of capturing the local manifold structure of data. This paper clarifies\nsome of these previous observations by showing that minimizing a particular\nform of regularized reconstruction error yields a reconstruction function that\nlocally characterizes the shape of the data generating density. We show that\nthe auto-encoder captures the score (derivative of the log-density with respect\nto the input). It contradicts previous interpretations of reconstruction error\nas an energy function. Unlike previous results, the theorems provided here are\ncompletely generic and do not depend on the parametrization of the\nauto-encoder: they show what the auto-encoder would tend to if given enough\ncapacity and examples. These results are for a contractive training criterion\nwe show to be similar to the denoising auto-encoder training criterion with\nsmall corruption noise, but with contraction applied on the whole\nreconstruction function rather than just encoder. Similarly to score matching,\none can consider the proposed training criterion as a convenient alternative to\nmaximum likelihood because it does not involve a partition function. Finally,\nwe show how an approximate Metropolis-Hastings MCMC can be setup to recover\nsamples from the estimated distribution, and this is confirmed in sampling\nexperiments.\n", "versions": [{"version": "v1", "created": "Sun, 18 Nov 2012 19:06:37 GMT"}, {"version": "v2", "created": "Wed, 16 Jan 2013 03:11:05 GMT"}, {"version": "v3", "created": "Tue, 12 Mar 2013 18:40:20 GMT"}, {"version": "v4", "created": "Sat, 6 Jul 2013 11:56:20 GMT"}, {"version": "v5", "created": "Tue, 19 Aug 2014 15:12:19 GMT"}], "update_date": "2014-08-20", "authors_parsed": [["Alain", "Guillaume", ""], ["Bengio", "Yoshua", ""]]}, {"id": "1211.4289", "submitter": "Loc Tran H", "authors": "Loc Tran", "title": "Application of three graph Laplacian based semi-supervised learning\n  methods to protein function prediction problem", "comments": "16 pages, 9 tables", "journal-ref": null, "doi": "10.5121/ijbb.2013.3202", "report-no": null, "categories": "cs.LG cs.CE q-bio.QM stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Protein function prediction is the important problem in modern biology. In\nthis paper, the un-normalized, symmetric normalized, and random walk graph\nLaplacian based semi-supervised learning methods will be applied to the\nintegrated network combined from multiple networks to predict the functions of\nall yeast proteins in these multiple networks. These multiple networks are\nnetwork created from Pfam domain structure, co-participation in a protein\ncomplex, protein-protein interaction network, genetic interaction network, and\nnetwork created from cell cycle gene expression measurements. Multiple networks\nare combined with fixed weights instead of using convex optimization to\ndetermine the combination weights due to high time complexity of convex\noptimization method. This simple combination method will not affect the\naccuracy performance measures of the three semi-supervised learning methods.\nExperiment results show that the un-normalized and symmetric normalized graph\nLaplacian based methods perform slightly better than random walk graph\nLaplacian based method for integrated network. Moreover, the accuracy\nperformance measures of these three semi-supervised learning methods for\nintegrated network are much better than the best accuracy performance measures\nof these three methods for the individual network.\n", "versions": [{"version": "v1", "created": "Mon, 19 Nov 2012 02:59:14 GMT"}, {"version": "v2", "created": "Mon, 3 Dec 2012 11:36:19 GMT"}, {"version": "v3", "created": "Thu, 11 Jul 2013 10:29:29 GMT"}], "update_date": "2013-07-12", "authors_parsed": [["Tran", "Loc", ""]]}, {"id": "1211.4321", "submitter": "Francois Caron", "authors": "Francois Caron (INRIA Bordeaux - Sud-Ouest, IMB), Yee Whye Teh", "title": "Bayesian nonparametric models for ranked data", "comments": "NIPS - Neural Information Processing Systems (2012)", "journal-ref": null, "doi": null, "report-no": "RR-8140", "categories": "stat.ML cs.LG stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop a Bayesian nonparametric extension of the popular Plackett-Luce\nchoice model that can handle an infinite number of choice items. Our framework\nis based on the theory of random atomic measures, with the prior specified by a\ngamma process. We derive a posterior characterization and a simple and\neffective Gibbs sampler for posterior simulation. We develop a time-varying\nextension of our model, and apply it to the New York Times lists of weekly\nbestselling books.\n", "versions": [{"version": "v1", "created": "Mon, 19 Nov 2012 07:40:51 GMT"}], "update_date": "2012-11-20", "authors_parsed": [["Caron", "Francois", "", "INRIA Bordeaux - Sud-Ouest, IMB"], ["Teh", "Yee Whye", ""]]}, {"id": "1211.4384", "submitter": "Jan Oksanen", "authors": "Jan Oksanen, Visa Koivunen, H. Vincent Poor", "title": "A Sensing Policy Based on Confidence Bounds and a Restless Multi-Armed\n  Bandit Model", "comments": "In proceedings of the 46th Asilomar conference 2012", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.LG math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A sensing policy for the restless multi-armed bandit problem with stationary\nbut unknown reward distributions is proposed. The work is presented in the\ncontext of cognitive radios in which the bandit problem arises when deciding\nwhich parts of the spectrum to sense and exploit. It is shown that the proposed\npolicy attains asymptotically logarithmic weak regret rate when the rewards are\nbounded independent and identically distributed or finite state Markovian.\nSimulation results verifying uniformly logarithmic weak regret are also\npresented. The proposed policy is a centrally coordinated index policy, in\nwhich the index of a frequency band is comprised of a sample mean term and a\nconfidence term. The sample mean term promotes spectrum exploitation whereas\nthe confidence term encourages exploration. The confidence term is designed\nsuch that the time interval between consecutive sensing instances of any\nsuboptimal band grows exponentially. This exponential growth between suboptimal\nsensing time instances leads to logarithmically growing weak regret. Simulation\nresults demonstrate that the proposed policy performs better than other similar\nmethods in the literature.\n", "versions": [{"version": "v1", "created": "Mon, 19 Nov 2012 12:19:45 GMT"}], "update_date": "2012-11-20", "authors_parsed": [["Oksanen", "Jan", ""], ["Koivunen", "Visa", ""], ["Poor", "H. Vincent", ""]]}, {"id": "1211.4410", "submitter": "Sotirios Chatzis", "authors": "Emmanouil A. Platanios and Sotirios P. Chatzis", "title": "Mixture Gaussian Process Conditional Heteroscedasticity", "comments": "Technical Report, under preparation", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generalized autoregressive conditional heteroscedasticity (GARCH) models have\nlong been considered as one of the most successful families of approaches for\nvolatility modeling in financial return series. In this paper, we propose an\nalternative approach based on methodologies widely used in the field of\nstatistical machine learning. Specifically, we propose a novel nonparametric\nBayesian mixture of Gaussian process regression models, each component of which\nmodels the noise variance process that contaminates the observed data as a\nseparate latent Gaussian process driven by the observed data. This way, we\nessentially obtain a mixture Gaussian process conditional heteroscedasticity\n(MGPCH) model for volatility modeling in financial return series. We impose a\nnonparametric prior with power-law nature over the distribution of the model\nmixture components, namely the Pitman-Yor process prior, to allow for better\ncapturing modeled data distributions with heavy tails and skewness. Finally, we\nprovide a copula- based approach for obtaining a predictive posterior for the\ncovariances over the asset returns modeled by means of a postulated MGPCH\nmodel. We evaluate the efficacy of our approach in a number of benchmark\nscenarios, and compare its performance to state-of-the-art methodologies.\n", "versions": [{"version": "v1", "created": "Mon, 19 Nov 2012 13:33:55 GMT"}, {"version": "v2", "created": "Sun, 2 Dec 2012 20:58:33 GMT"}, {"version": "v3", "created": "Thu, 6 Dec 2012 11:48:29 GMT"}, {"version": "v4", "created": "Fri, 25 Jan 2013 22:03:25 GMT"}], "update_date": "2013-01-29", "authors_parsed": [["Platanios", "Emmanouil A.", ""], ["Chatzis", "Sotirios P.", ""]]}, {"id": "1211.4518", "submitter": "Zhenliang Zhang", "authors": "Zhenliang Zhang, Edwin K. P. Chong, Ali Pezeshki, and William Moran", "title": "Hypothesis Testing in Feedforward Networks with Broadcast Failures", "comments": null, "journal-ref": null, "doi": "10.1109/JSTSP.2013.2258657", "report-no": null, "categories": "cs.IT cs.LG math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Consider a countably infinite set of nodes, which sequentially make decisions\nbetween two given hypotheses. Each node takes a measurement of the underlying\ntruth, observes the decisions from some immediate predecessors, and makes a\ndecision between the given hypotheses. We consider two classes of broadcast\nfailures: 1) each node broadcasts a decision to the other nodes, subject to\nrandom erasure in the form of a binary erasure channel; 2) each node broadcasts\na randomly flipped decision to the other nodes in the form of a binary\nsymmetric channel. We are interested in whether there exists a decision\nstrategy consisting of a sequence of likelihood ratio tests such that the node\ndecisions converge in probability to the underlying truth. In both cases, we\nshow that if each node only learns from a bounded number of immediate\npredecessors, then there does not exist a decision strategy such that the\ndecisions converge in probability to the underlying truth. However, in case 1,\nwe show that if each node learns from an unboundedly growing number of\npredecessors, then the decisions converge in probability to the underlying\ntruth, even when the erasure probabilities converge to 1. We also derive the\nconvergence rate of the error probability. In case 2, we show that if each node\nlearns from all of its previous predecessors, then the decisions converge in\nprobability to the underlying truth when the flipping probabilities of the\nbinary symmetric channels are bounded away from 1/2. In the case where the\nflipping probabilities converge to 1/2, we derive a necessary condition on the\nconvergence rate of the flipping probabilities such that the decisions still\nconverge to the underlying truth. We also explicitly characterize the\nrelationship between the convergence rate of the error probability and the\nconvergence rate of the flipping probabilities.\n", "versions": [{"version": "v1", "created": "Mon, 19 Nov 2012 17:40:54 GMT"}, {"version": "v2", "created": "Sat, 2 Mar 2013 07:02:30 GMT"}, {"version": "v3", "created": "Mon, 25 Mar 2013 21:29:44 GMT"}], "update_date": "2015-06-12", "authors_parsed": [["Zhang", "Zhenliang", ""], ["Chong", "Edwin K. P.", ""], ["Pezeshki", "Ali", ""], ["Moran", "William", ""]]}, {"id": "1211.4657", "submitter": "Chen Chen", "authors": "Chen Chen and Yeqing Li and Junzhou Huang", "title": "Forest Sparsity for Multi-channel Compressive Sensing", "comments": "Accepted by IEEE Transactions on Signal Processing, 2014", "journal-ref": null, "doi": "10.1109/TSP.2014.2318138", "report-no": null, "categories": "cs.LG cs.CV cs.IT math.IT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we investigate a new compressive sensing model for\nmulti-channel sparse data where each channel can be represented as a\nhierarchical tree and different channels are highly correlated. Therefore, the\nfull data could follow the forest structure and we call this property as\n\\emph{forest sparsity}. It exploits both intra- and inter- channel correlations\nand enriches the family of existing model-based compressive sensing theories.\nThe proposed theory indicates that only $\\mathcal{O}(Tk+\\log(N/k))$\nmeasurements are required for multi-channel data with forest sparsity, where\n$T$ is the number of channels, $N$ and $k$ are the length and sparsity number\nof each channel respectively. This result is much better than\n$\\mathcal{O}(Tk+T\\log(N/k))$ of tree sparsity, $\\mathcal{O}(Tk+k\\log(N/k))$ of\njoint sparsity, and far better than $\\mathcal{O}(Tk+Tk\\log(N/k))$ of standard\nsparsity. In addition, we extend the forest sparsity theory to the multiple\nmeasurement vectors problem, where the measurement matrix is a block-diagonal\nmatrix. The result shows that the required measurement bound can be the same as\nthat for dense random measurement matrix, when the data shares equal energy in\neach channel. A new algorithm is developed and applied on four example\napplications to validate the benefit of the proposed model. Extensive\nexperiments demonstrate the effectiveness and efficiency of the proposed theory\nand algorithm.\n", "versions": [{"version": "v1", "created": "Tue, 20 Nov 2012 03:22:45 GMT"}, {"version": "v2", "created": "Thu, 1 May 2014 15:56:00 GMT"}], "update_date": "2014-05-02", "authors_parsed": [["Chen", "Chen", ""], ["Li", "Yeqing", ""], ["Huang", "Junzhou", ""]]}, {"id": "1211.4753", "submitter": "Nicholas Foti", "authors": "Nicholas J. Foti, Joseph D. Futoma, Daniel N. Rockmore, Sinead\n  Williamson", "title": "A unifying representation for a class of dependent random measures", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a general construction for dependent random measures based on\nthinning Poisson processes on an augmented space. The framework is not\nrestricted to dependent versions of a specific nonparametric model, but can be\napplied to all models that can be represented using completely random measures.\nSeveral existing dependent random measures can be seen as specific cases of\nthis framework. Interesting properties of the resulting measures are derived\nand the efficacy of the framework is demonstrated by constructing a\ncovariate-dependent latent feature model and topic model that obtain superior\npredictive performance.\n", "versions": [{"version": "v1", "created": "Tue, 20 Nov 2012 14:22:07 GMT"}], "update_date": "2012-11-21", "authors_parsed": [["Foti", "Nicholas J.", ""], ["Futoma", "Joseph D.", ""], ["Rockmore", "Daniel N.", ""], ["Williamson", "Sinead", ""]]}, {"id": "1211.4798", "submitter": "Nicholas Foti", "authors": "Nicholas J. Foti, Sinead Williamson", "title": "A survey of non-exchangeable priors for Bayesian nonparametric models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Dependent nonparametric processes extend distributions over measures, such as\nthe Dirichlet process and the beta process, to give distributions over\ncollections of measures, typically indexed by values in some covariate space.\nSuch models are appropriate priors when exchangeability assumptions do not\nhold, and instead we want our model to vary fluidly with some set of\ncovariates. Since the concept of dependent nonparametric processes was\nformalized by MacEachern [1], there have been a number of models proposed and\nused in the statistics and machine learning literatures. Many of these models\nexhibit underlying similarities, an understanding of which, we hope, will help\nin selecting an appropriate prior, developing new models, and leveraging\ninference techniques.\n", "versions": [{"version": "v1", "created": "Tue, 20 Nov 2012 16:29:13 GMT"}], "update_date": "2012-11-21", "authors_parsed": [["Foti", "Nicholas J.", ""], ["Williamson", "Sinead", ""]]}, {"id": "1211.4860", "submitter": "Oscar Beijbom Mr", "authors": "Oscar Beijbom", "title": "Domain Adaptations for Computer Vision Applications", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A basic assumption of statistical learning theory is that train and test data\nare drawn from the same underlying distribution. Unfortunately, this assumption\ndoesn't hold in many applications. Instead, ample labeled data might exist in a\nparticular `source' domain while inference is needed in another, `target'\ndomain. Domain adaptation methods leverage labeled data from both domains to\nimprove classification on unseen data in the target domain. In this work we\nsurvey domain transfer learning methods for various application domains with\nfocus on recent work in Computer Vision.\n", "versions": [{"version": "v1", "created": "Tue, 20 Nov 2012 20:54:30 GMT"}], "update_date": "2012-11-21", "authors_parsed": [["Beijbom", "Oscar", ""]]}, {"id": "1211.4888", "submitter": "Tuhin Sahai", "authors": "Tuhin Sahai, Stefan Klus and Michael Dellnitz", "title": "A Traveling Salesman Learns Bayesian Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Structure learning of Bayesian networks is an important problem that arises\nin numerous machine learning applications. In this work, we present a novel\napproach for learning the structure of Bayesian networks using the solution of\nan appropriately constructed traveling salesman problem. In our approach, one\ncomputes an optimal ordering (partially ordered set) of random variables using\nmethods for the traveling salesman problem. This ordering significantly reduces\nthe search space for the subsequent greedy optimization that computes the final\nstructure of the Bayesian network. We demonstrate our approach of learning\nBayesian networks on real world census and weather datasets. In both cases, we\ndemonstrate that the approach very accurately captures dependencies between\nrandom variables. We check the accuracy of the predictions based on independent\nstudies in both application domains.\n", "versions": [{"version": "v1", "created": "Tue, 20 Nov 2012 21:50:22 GMT"}], "update_date": "2012-11-22", "authors_parsed": [["Sahai", "Tuhin", ""], ["Klus", "Stefan", ""], ["Dellnitz", "Michael", ""]]}, {"id": "1211.4909", "submitter": "Benyuan Liu", "authors": "Benyuan Liu, Zhilin Zhang, Hongqi Fan, Qiang Fu", "title": "Fast Marginalized Block Sparse Bayesian Learning Algorithm", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.LG math.IT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The performance of sparse signal recovery from noise corrupted,\nunderdetermined measurements can be improved if both sparsity and correlation\nstructure of signals are exploited. One typical correlation structure is the\nintra-block correlation in block sparse signals. To exploit this structure, a\nframework, called block sparse Bayesian learning (BSBL), has been proposed\nrecently. Algorithms derived from this framework showed superior performance\nbut they are not very fast, which limits their applications. This work derives\nan efficient algorithm from this framework, using a marginalized likelihood\nmaximization method. Compared to existing BSBL algorithms, it has close\nrecovery performance but is much faster. Therefore, it is more suitable for\nlarge scale datasets and applications requiring real-time implementation.\n", "versions": [{"version": "v1", "created": "Wed, 21 Nov 2012 01:06:49 GMT"}, {"version": "v2", "created": "Thu, 29 Nov 2012 01:24:49 GMT"}, {"version": "v3", "created": "Fri, 18 Jan 2013 02:28:09 GMT"}, {"version": "v4", "created": "Tue, 22 Jan 2013 01:51:17 GMT"}, {"version": "v5", "created": "Mon, 4 Mar 2013 02:07:31 GMT"}, {"version": "v6", "created": "Mon, 16 Sep 2013 22:58:17 GMT"}, {"version": "v7", "created": "Sun, 29 Sep 2013 15:56:47 GMT"}], "update_date": "2013-10-01", "authors_parsed": [["Liu", "Benyuan", ""], ["Zhang", "Zhilin", ""], ["Fan", "Hongqi", ""], ["Fu", "Qiang", ""]]}, {"id": "1211.5037", "submitter": "Fran\\c{c}ois Caron", "authors": "Fran\\c{c}ois Caron, Yee Whye Teh, Thomas Brendan Murphy", "title": "Bayesian nonparametric Plackett-Luce models for the analysis of\n  preferences for college degree programmes", "comments": "Published in at http://dx.doi.org/10.1214/14-AOAS717 the Annals of\n  Applied Statistics (http://www.imstat.org/aoas/) by the Institute of\n  Mathematical Statistics (http://www.imstat.org)", "journal-ref": "Annals of Applied Statistics 2014, Vol. 8, No. 2, 1145-1181", "doi": "10.1214/14-AOAS717", "report-no": "IMS-AOAS-AOAS717", "categories": "stat.ML cs.LG stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we propose a Bayesian nonparametric model for clustering\npartial ranking data. We start by developing a Bayesian nonparametric extension\nof the popular Plackett-Luce choice model that can handle an infinite number of\nchoice items. Our framework is based on the theory of random atomic measures,\nwith the prior specified by a completely random measure. We characterise the\nposterior distribution given data, and derive a simple and effective Gibbs\nsampler for posterior simulation. We then develop a Dirichlet process mixture\nextension of our model and apply it to investigate the clustering of\npreferences for college degree programmes amongst Irish secondary school\ngraduates. The existence of clusters of applicants who have similar preferences\nfor degree programmes is established and we determine that subject matter and\ngeographical location of the third level institution characterise these\nclusters.\n", "versions": [{"version": "v1", "created": "Wed, 21 Nov 2012 14:09:56 GMT"}, {"version": "v2", "created": "Tue, 14 Jan 2014 19:34:49 GMT"}, {"version": "v3", "created": "Fri, 1 Aug 2014 06:34:00 GMT"}], "update_date": "2014-08-04", "authors_parsed": [["Caron", "Fran\u00e7ois", ""], ["Teh", "Yee Whye", ""], ["Murphy", "Thomas Brendan", ""]]}, {"id": "1211.5063", "submitter": "Razvan Pascanu", "authors": "Razvan Pascanu and Tomas Mikolov and Yoshua Bengio", "title": "On the difficulty of training Recurrent Neural Networks", "comments": "Improved description of the exploding gradient problem and\n  description and analysis of the vanishing gradient problem", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There are two widely known issues with properly training Recurrent Neural\nNetworks, the vanishing and the exploding gradient problems detailed in Bengio\net al. (1994). In this paper we attempt to improve the understanding of the\nunderlying issues by exploring these problems from an analytical, a geometric\nand a dynamical systems perspective. Our analysis is used to justify a simple\nyet effective solution. We propose a gradient norm clipping strategy to deal\nwith exploding gradients and a soft constraint for the vanishing gradients\nproblem. We validate empirically our hypothesis and proposed solutions in the\nexperimental section.\n", "versions": [{"version": "v1", "created": "Wed, 21 Nov 2012 15:40:11 GMT"}, {"version": "v2", "created": "Sat, 16 Feb 2013 00:35:48 GMT"}], "update_date": "2013-02-19", "authors_parsed": [["Pascanu", "Razvan", ""], ["Mikolov", "Tomas", ""], ["Bengio", "Yoshua", ""]]}, {"id": "1211.5189", "submitter": "Karthik Shankar", "authors": "Karthik H. Shankar and Marc W. Howard", "title": "Optimally fuzzy temporal memory", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Any learner with the ability to predict the future of a structured\ntime-varying signal must maintain a memory of the recent past. If the signal\nhas a characteristic timescale relevant to future prediction, the memory can be\na simple shift register---a moving window extending into the past, requiring\nstorage resources that linearly grows with the timescale to be represented.\nHowever, an independent general purpose learner cannot a priori know the\ncharacteristic prediction-relevant timescale of the signal. Moreover, many\nnaturally occurring signals show scale-free long range correlations implying\nthat the natural prediction-relevant timescale is essentially unbounded. Hence\nthe learner should maintain information from the longest possible timescale\nallowed by resource availability. Here we construct a fuzzy memory system that\noptimally sacrifices the temporal accuracy of information in a scale-free\nfashion in order to represent prediction-relevant information from\nexponentially long timescales. Using several illustrative examples, we\ndemonstrate the advantage of the fuzzy memory system over a shift register in\ntime series forecasting of natural signals. When the available storage\nresources are limited, we suggest that a general purpose learner would be\nbetter off committing to such a fuzzy memory system.\n", "versions": [{"version": "v1", "created": "Thu, 22 Nov 2012 02:38:16 GMT"}, {"version": "v2", "created": "Tue, 22 Oct 2013 21:51:42 GMT"}], "update_date": "2013-10-24", "authors_parsed": [["Shankar", "Karthik H.", ""], ["Howard", "Marc W.", ""]]}, {"id": "1211.5227", "submitter": "Vishnuvardhan Mannava M.E", "authors": "Vishnuvardhan Mannava and T. Ramesh", "title": "Service Composition Design Pattern for Autonomic Computing Systems using\n  Association Rule based Learning and Service-Oriented Architecture", "comments": "19 pages, 7 figures, International Journal of Grid Computing &\n  Applications (IJGCA). arXiv admin note: text overlap with arXiv:1208.3836", "journal-ref": "IJGCA, 3(3), 21-39 (2012)", "doi": null, "report-no": null, "categories": "cs.SE cs.DC cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  In this paper we present a Service Injection and composition Design Pattern\nfor Unstructured Peer-to-Peer networks, which is designed with Aspect-oriented\ndesign patterns, and amalgamation of the Strategy, Worker Object, and\nCheck-List Design Patterns used to design the Self-Adaptive Systems. It will\napply self reconfiguration planes dynamically without the interruption or\nintervention of the administrator for handling service failures at the servers.\nWhen a client requests for a complex service, Service Composition should be\ndone to fulfil the request. If a service is not available in the memory, it\nwill be injected as Aspectual Feature Module code. We used Service Oriented\nArchitecture (SOA) with Web Services in Java to Implement the composite Design\nPattern. As far as we know, there are no studies on composition of design\npatterns for Peer-to-peer computing domain. The pattern is described using a\njava-like notation for the classes and interfaces. A simple UML class and\nSequence diagrams are depicted.\n", "versions": [{"version": "v1", "created": "Thu, 22 Nov 2012 08:33:09 GMT"}], "update_date": "2015-06-01", "authors_parsed": [["Mannava", "Vishnuvardhan", ""], ["Ramesh", "T.", ""]]}, {"id": "1211.5414", "submitter": "Daniel Hsu", "authors": "Daniel Hsu and Sham M. Kakade and Tong Zhang", "title": "Analysis of a randomized approximation scheme for matrix multiplication", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.LG cs.NA stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This note gives a simple analysis of a randomized approximation scheme for\nmatrix multiplication proposed by Sarlos (2006) based on a random rotation\nfollowed by uniform column sampling. The result follows from a matrix version\nof Bernstein's inequality and a tail inequality for quadratic forms in\nsubgaussian random vectors.\n", "versions": [{"version": "v1", "created": "Fri, 23 Nov 2012 06:11:54 GMT"}], "update_date": "2012-11-26", "authors_parsed": [["Hsu", "Daniel", ""], ["Kakade", "Sham M.", ""], ["Zhang", "Tong", ""]]}, {"id": "1211.5590", "submitter": "Pascal Lamblin", "authors": "Fr\\'ed\\'eric Bastien, Pascal Lamblin, Razvan Pascanu, James Bergstra,\n  Ian Goodfellow, Arnaud Bergeron, Nicolas Bouchard, David Warde-Farley, Yoshua\n  Bengio", "title": "Theano: new features and speed improvements", "comments": "Presented at the Deep Learning Workshop, NIPS 2012", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Theano is a linear algebra compiler that optimizes a user's\nsymbolically-specified mathematical computations to produce efficient low-level\nimplementations. In this paper, we present new features and efficiency\nimprovements to Theano, and benchmarks demonstrating Theano's performance\nrelative to Torch7, a recently introduced machine learning library, and to\nRNNLM, a C++ library targeted at recurrent neural networks.\n", "versions": [{"version": "v1", "created": "Fri, 23 Nov 2012 20:42:41 GMT"}], "update_date": "2012-11-26", "authors_parsed": [["Bastien", "Fr\u00e9d\u00e9ric", ""], ["Lamblin", "Pascal", ""], ["Pascanu", "Razvan", ""], ["Bergstra", "James", ""], ["Goodfellow", "Ian", ""], ["Bergeron", "Arnaud", ""], ["Bouchard", "Nicolas", ""], ["Warde-Farley", "David", ""], ["Bengio", "Yoshua", ""]]}, {"id": "1211.5687", "submitter": "Heng Luo", "authors": "Heng Luo, Pierre Luc Carrier, Aaron Courville, Yoshua Bengio", "title": "Texture Modeling with Convolutional Spike-and-Slab RBMs and Deep\n  Extensions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We apply the spike-and-slab Restricted Boltzmann Machine (ssRBM) to texture\nmodeling. The ssRBM with tiled-convolution weight sharing (TssRBM) achieves or\nsurpasses the state-of-the-art on texture synthesis and inpainting by\nparametric models. We also develop a novel RBM model with a spike-and-slab\nvisible layer and binary variables in the hidden layer. This model is designed\nto be stacked on top of the TssRBM. We show the resulting deep belief network\n(DBN) is a powerful generative model that improves on single-layer models and\nis capable of modeling not only single high-resolution and challenging textures\nbut also multiple textures.\n", "versions": [{"version": "v1", "created": "Sat, 24 Nov 2012 17:51:57 GMT"}], "update_date": "2012-11-27", "authors_parsed": [["Luo", "Heng", ""], ["Carrier", "Pierre Luc", ""], ["Courville", "Aaron", ""], ["Bengio", "Yoshua", ""]]}, {"id": "1211.5901", "submitter": "Nicolas Chopin", "authors": "Sumeetpal S. Singh and Nicolas Chopin and Nick Whiteley", "title": "Bayesian learning of noisy Markov decision processes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the inverse reinforcement learning problem, that is, the problem\nof learning from, and then predicting or mimicking a controller based on\nstate/action data. We propose a statistical model for such data, derived from\nthe structure of a Markov decision process. Adopting a Bayesian approach to\ninference, we show how latent variables of the model can be estimated, and how\npredictions about actions can be made, in a unified framework. A new Markov\nchain Monte Carlo (MCMC) sampler is devised for simulation from the posterior\ndistribution. This step includes a parameter expansion step, which is shown to\nbe essential for good convergence properties of the MCMC sampler. As an\nillustration, the method is applied to learning a human controller.\n", "versions": [{"version": "v1", "created": "Mon, 26 Nov 2012 09:55:27 GMT"}], "update_date": "2012-11-27", "authors_parsed": [["Singh", "Sumeetpal S.", ""], ["Chopin", "Nicolas", ""], ["Whiteley", "Nick", ""]]}, {"id": "1211.6013", "submitter": "Mehrdad Mahdavi", "authors": "Mehrdad Mahdavi, Tianbao Yang, Rong Jin", "title": "Online Stochastic Optimization with Multiple Objectives", "comments": "NIPS Workshop on Optimization for Machine Learning", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.OC", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  In this paper we propose a general framework to characterize and solve the\nstochastic optimization problems with multiple objectives underlying many real\nworld learning applications. We first propose a projection based algorithm\nwhich attains an $O(T^{-1/3})$ convergence rate. Then, by leveraging on the\ntheory of Lagrangian in constrained optimization, we devise a novel primal-dual\nstochastic approximation algorithm which attains the optimal convergence rate\nof $O(T^{-1/2})$ for general Lipschitz continuous objectives.\n", "versions": [{"version": "v1", "created": "Mon, 26 Nov 2012 16:27:18 GMT"}, {"version": "v2", "created": "Sun, 14 Jul 2013 00:09:14 GMT"}], "update_date": "2013-07-16", "authors_parsed": [["Mahdavi", "Mehrdad", ""], ["Yang", "Tianbao", ""], ["Jin", "Rong", ""]]}, {"id": "1211.6085", "submitter": "Saurabh Paul", "authors": "Saurabh Paul, Christos Boutsidis, Malik Magdon-Ismail, Petros Drineas", "title": "Random Projections for Linear Support Vector Machines", "comments": "To appear in ACM TKDD, 2014. Shorter version appeared at AISTATS 2013", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Let X be a data matrix of rank \\rho, whose rows represent n points in\nd-dimensional space. The linear support vector machine constructs a hyperplane\nseparator that maximizes the 1-norm soft margin. We develop a new oblivious\ndimension reduction technique which is precomputed and can be applied to any\ninput matrix X. We prove that, with high probability, the margin and minimum\nenclosing ball in the feature space are preserved to within \\epsilon-relative\nerror, ensuring comparable generalization as in the original space in the case\nof classification. For regression, we show that the margin is preserved to\n\\epsilon-relative error with high probability. We present extensive experiments\nwith real and synthetic data to support our theory.\n", "versions": [{"version": "v1", "created": "Mon, 26 Nov 2012 20:35:12 GMT"}, {"version": "v2", "created": "Wed, 28 Nov 2012 16:26:48 GMT"}, {"version": "v3", "created": "Sat, 20 Apr 2013 21:42:22 GMT"}, {"version": "v4", "created": "Tue, 8 Oct 2013 23:57:41 GMT"}, {"version": "v5", "created": "Thu, 17 Apr 2014 19:07:11 GMT"}], "update_date": "2014-04-18", "authors_parsed": [["Paul", "Saurabh", ""], ["Boutsidis", "Christos", ""], ["Magdon-Ismail", "Malik", ""], ["Drineas", "Petros", ""]]}, {"id": "1211.6158", "submitter": "Ankan Saha", "authors": "Ankan Saha and Prateek Jain and Ambuj Tewari", "title": "The Interplay Between Stability and Regret in Online Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper considers the stability of online learning algorithms and its\nimplications for learnability (bounded regret). We introduce a novel quantity\ncalled {\\em forward regret} that intuitively measures how good an online\nlearning algorithm is if it is allowed a one-step look-ahead into the future.\nWe show that given stability, bounded forward regret is equivalent to bounded\nregret. We also show that the existence of an algorithm with bounded regret\nimplies the existence of a stable algorithm with bounded regret and bounded\nforward regret. The equivalence results apply to general, possibly non-convex\nproblems. To the best of our knowledge, our analysis provides the first general\nconnection between stability and regret in the online setting that is not\nrestricted to a particular class of algorithms. Our stability-regret connection\nprovides a simple recipe for analyzing regret incurred by any online learning\nalgorithm. Using our framework, we analyze several existing online learning\nalgorithms as well as the \"approximate\" versions of algorithms like RDA that\nsolve an optimization problem at each iteration. Our proofs are simpler than\nexisting analysis for the respective algorithms, show a clear trade-off between\nstability and forward regret, and provide tighter regret bounds in some cases.\nFurthermore, using our recipe, we analyze \"approximate\" versions of several\nalgorithms such as follow-the-regularized-leader (FTRL) that requires solving\nan optimization problem at each step.\n", "versions": [{"version": "v1", "created": "Mon, 26 Nov 2012 23:13:23 GMT"}], "update_date": "2012-11-28", "authors_parsed": [["Saha", "Ankan", ""], ["Jain", "Prateek", ""], ["Tewari", "Ambuj", ""]]}, {"id": "1211.6248", "submitter": "Arnim Bleier", "authors": "Arnim Bleier", "title": "A simple non-parametric Topic Mixture for Authors and Documents", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article reviews the Author-Topic Model and presents a new non-parametric\nextension based on the Hierarchical Dirichlet Process. The extension is\nespecially suitable when no prior information about the number of components\nnecessary is available. A blocked Gibbs sampler is described and focus put on\nstaying as close as possible to the original model with only the minimum of\ntheoretical and implementation overhead necessary.\n", "versions": [{"version": "v1", "created": "Tue, 27 Nov 2012 09:36:22 GMT"}, {"version": "v2", "created": "Tue, 4 Dec 2012 13:50:19 GMT"}], "update_date": "2012-12-05", "authors_parsed": [["Bleier", "Arnim", ""]]}, {"id": "1211.6302", "submitter": "Francis Bach", "authors": "Francis Bach (INRIA Paris - Rocquencourt, LIENS)", "title": "Duality between subgradient and conditional gradient methods", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given a convex optimization problem and its dual, there are many possible\nfirst-order algorithms. In this paper, we show the equivalence between mirror\ndescent algorithms and algorithms generalizing the conditional gradient method.\nThis is done through convex duality, and implies notably that for certain\nproblems, such as for supervised machine learning problems with non-smooth\nlosses or problems regularized by non-smooth regularizers, the primal\nsubgradient method and the dual conditional gradient method are formally\nequivalent. The dual interpretation leads to a form of line search for mirror\ndescent, as well as guarantees of convergence for primal-dual certificates.\n", "versions": [{"version": "v1", "created": "Tue, 27 Nov 2012 13:46:59 GMT"}, {"version": "v2", "created": "Wed, 2 Jan 2013 17:48:58 GMT"}, {"version": "v3", "created": "Fri, 18 Oct 2013 17:02:13 GMT"}], "update_date": "2013-10-21", "authors_parsed": [["Bach", "Francis", "", "INRIA Paris - Rocquencourt, LIENS"]]}, {"id": "1211.6340", "submitter": "Md. Hedayetul Islam Shovon", "authors": "Md. Hedayetul Islam Shovon, Mahfuza Haque", "title": "An Approach of Improving Students Academic Performance by using k means\n  clustering algorithm and Decision tree", "comments": "arXiv admin note: text overlap with arXiv:1002.2425 by other authors", "journal-ref": "International Journal of Advanced Computer Science and\n  Applications(IJACSA),Vol. 3, No. 8, Page no. 146-149, 2012", "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://creativecommons.org/licenses/publicdomain/", "abstract": "  Improving students academic performance is not an easy task for the academic\ncommunity of higher learning. The academic performance of engineering and\nscience students during their first year at university is a turning point in\ntheir educational path and usually encroaches on their General Point\nAverage,GPA in a decisive manner. The students evaluation factors like class\nquizzes mid and final exam assignment lab work are studied. It is recommended\nthat all these correlated information should be conveyed to the class teacher\nbefore the conduction of final exam. This study will help the teachers to\nreduce the drop out ratio to a significant level and improve the performance of\nstudents. In this paper, we present a hybrid procedure based on Decision Tree\nof Data mining method and Data Clustering that enables academicians to predict\nstudents GPA and based on that instructor can take necessary step to improve\nstudent academic performance.\n", "versions": [{"version": "v1", "created": "Fri, 9 Nov 2012 09:54:29 GMT"}], "update_date": "2012-11-28", "authors_parsed": [["Shovon", "Md. Hedayetul Islam", ""], ["Haque", "Mahfuza", ""]]}, {"id": "1211.6581", "submitter": "Eleftherios Spyromitros-Xioufis", "authors": "Eleftherios Spyromitros-Xioufis, Grigorios Tsoumakas, William Groves,\n  Ioannis Vlahavas", "title": "Multi-Target Regression via Input Space Expansion: Treating Targets as\n  Inputs", "comments": "Accepted for publication in Machine Learning journal. This\n  replacement contains major improvements compared to the previous version,\n  including a deeper theoretical and experimental analysis and an extended\n  discussion of related work", "journal-ref": null, "doi": "10.1007/s10994-016-5546-z", "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In many practical applications of supervised learning the task involves the\nprediction of multiple target variables from a common set of input variables.\nWhen the prediction targets are binary the task is called multi-label\nclassification, while when the targets are continuous the task is called\nmulti-target regression. In both tasks, target variables often exhibit\nstatistical dependencies and exploiting them in order to improve predictive\naccuracy is a core challenge. A family of multi-label classification methods\naddress this challenge by building a separate model for each target on an\nexpanded input space where other targets are treated as additional input\nvariables. Despite the success of these methods in the multi-label\nclassification domain, their applicability and effectiveness in multi-target\nregression has not been studied until now. In this paper, we introduce two new\nmethods for multi-target regression, called Stacked Single-Target and Ensemble\nof Regressor Chains, by adapting two popular multi-label classification methods\nof this family. Furthermore, we highlight an inherent problem of these methods\n- a discrepancy of the values of the additional input variables between\ntraining and prediction - and develop extensions that use out-of-sample\nestimates of the target variables during training in order to tackle this\nproblem. The results of an extensive experimental evaluation carried out on a\nlarge and diverse collection of datasets show that, when the discrepancy is\nappropriately mitigated, the proposed methods attain consistent improvements\nover the independent regressions baseline. Moreover, two versions of Ensemble\nof Regression Chains perform significantly better than four state-of-the-art\nmethods including regularization-based multi-task learning methods and a\nmulti-objective random forest approach.\n", "versions": [{"version": "v1", "created": "Wed, 28 Nov 2012 11:42:36 GMT"}, {"version": "v2", "created": "Thu, 3 Apr 2014 11:14:16 GMT"}, {"version": "v3", "created": "Thu, 17 Apr 2014 09:44:27 GMT"}, {"version": "v4", "created": "Tue, 17 Jun 2014 12:09:24 GMT"}, {"version": "v5", "created": "Wed, 27 Jan 2016 20:24:53 GMT"}], "update_date": "2016-01-28", "authors_parsed": [["Spyromitros-Xioufis", "Eleftherios", ""], ["Tsoumakas", "Grigorios", ""], ["Groves", "William", ""], ["Vlahavas", "Ioannis", ""]]}, {"id": "1211.6616", "submitter": "Rongpeng Li", "authors": "Rongpeng Li, Zhifeng Zhao, Xianfu Chen, Jacques Palicot, Honggang\n  Zhang", "title": "TACT: A Transfer Actor-Critic Learning Framework for Energy Saving in\n  Cellular Radio Access Networks", "comments": "11 figures, 30 pages, accepted in IEEE Transactions on Wireless\n  Communications 2014. IEEE Trans. Wireless Commun., Feb. 2014", "journal-ref": null, "doi": "10.1109/TWC.2014.022014.130840", "report-no": null, "categories": "cs.NI cs.AI cs.IT cs.LG math.IT", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  Recent works have validated the possibility of improving energy efficiency in\nradio access networks (RANs), achieved by dynamically turning on/off some base\nstations (BSs). In this paper, we extend the research over BS switching\noperations, which should match up with traffic load variations. Instead of\ndepending on the dynamic traffic loads which are still quite challenging to\nprecisely forecast, we firstly formulate the traffic variations as a Markov\ndecision process. Afterwards, in order to foresightedly minimize the energy\nconsumption of RANs, we design a reinforcement learning framework based BS\nswitching operation scheme. Furthermore, to avoid the underlying curse of\ndimensionality in reinforcement learning, a transfer actor-critic algorithm\n(TACT), which utilizes the transferred learning expertise in historical periods\nor neighboring regions, is proposed and provably converges. In the end, we\nevaluate our proposed scheme by extensive simulations under various practical\nconfigurations and show that the proposed TACT algorithm contributes to a\nperformance jumpstart and demonstrates the feasibility of significant energy\nefficiency improvement at the expense of tolerable delay performance.\n", "versions": [{"version": "v1", "created": "Wed, 28 Nov 2012 14:48:36 GMT"}, {"version": "v2", "created": "Mon, 24 Jun 2013 00:37:28 GMT"}, {"version": "v3", "created": "Fri, 4 Apr 2014 07:37:14 GMT"}], "update_date": "2014-04-07", "authors_parsed": [["Li", "Rongpeng", ""], ["Zhao", "Zhifeng", ""], ["Chen", "Xianfu", ""], ["Palicot", "Jacques", ""], ["Zhang", "Honggang", ""]]}, {"id": "1211.6653", "submitter": "Yuyang Wang", "authors": "Yuyang Wang, Roni Khardon", "title": "Nonparametric Bayesian Mixed-effect Model: a Sparse Gaussian Process\n  Approach", "comments": "Preliminary version appeared in ECML2012", "journal-ref": null, "doi": "10.1007/978-3-642-33460-3_51", "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multi-task learning models using Gaussian processes (GP) have been developed\nand successfully applied in various applications. The main difficulty with this\napproach is the computational cost of inference using the union of examples\nfrom all tasks. Therefore sparse solutions, that avoid using the entire data\ndirectly and instead use a set of informative \"representatives\" are desirable.\nThe paper investigates this problem for the grouped mixed-effect GP model where\neach individual response is given by a fixed-effect, taken from one of a set of\nunknown groups, plus a random individual effect function that captures\nvariations among individuals. Such models have been widely used in previous\nwork but no sparse solutions have been developed. The paper presents the first\nsparse solution for such problems, showing how the sparse approximation can be\nobtained by maximizing a variational lower bound on the marginal likelihood,\ngeneralizing ideas from single-task Gaussian processes to handle the\nmixed-effect model as well as grouping. Experiments using artificial and real\ndata validate the approach showing that it can recover the performance of\ninference with the full sample, that it outperforms baseline methods, and that\nit outperforms state of the art sparse solutions for other multi-task GP\nformulations.\n", "versions": [{"version": "v1", "created": "Wed, 28 Nov 2012 16:50:23 GMT"}], "update_date": "2012-11-29", "authors_parsed": [["Wang", "Yuyang", ""], ["Khardon", "Roni", ""]]}, {"id": "1211.6687", "submitter": "Nicolas Gillis", "authors": "Nicolas Gillis", "title": "Robustness Analysis of Hottopixx, a Linear Programming Model for\n  Factoring Nonnegative Matrices", "comments": "23 pages; new numerical results; Comparison with Arora et al.;\n  Accepted in SIAM J. Mat. Anal. Appl", "journal-ref": "SIAM J. Matrix Anal. & Appl. 34 (3), pp. 1189-1212, 2013", "doi": "10.1137/120900629", "report-no": null, "categories": "stat.ML cs.LG cs.NA math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Although nonnegative matrix factorization (NMF) is NP-hard in general, it has\nbeen shown very recently that it is tractable under the assumption that the\ninput nonnegative data matrix is close to being separable (separability\nrequires that all columns of the input matrix belongs to the cone spanned by a\nsmall subset of these columns). Since then, several algorithms have been\ndesigned to handle this subclass of NMF problems. In particular, Bittorf,\nRecht, R\\'e and Tropp (`Factoring nonnegative matrices with linear programs',\nNIPS 2012) proposed a linear programming model, referred to as Hottopixx. In\nthis paper, we provide a new and more general robustness analysis of their\nmethod. In particular, we design a provably more robust variant using a\npost-processing strategy which allows us to deal with duplicates and near\nduplicates in the dataset.\n", "versions": [{"version": "v1", "created": "Wed, 28 Nov 2012 18:05:56 GMT"}, {"version": "v2", "created": "Tue, 4 Dec 2012 16:06:55 GMT"}, {"version": "v3", "created": "Sun, 17 Feb 2013 08:53:06 GMT"}, {"version": "v4", "created": "Fri, 31 May 2013 15:06:57 GMT"}], "update_date": "2013-08-19", "authors_parsed": [["Gillis", "Nicolas", ""]]}, {"id": "1211.6727", "submitter": "Qichao Que", "authors": "Mikhail Belkin and Qichao Que and Yusu Wang and Xueyuan Zhou", "title": "Graph Laplacians on Singular Manifolds: Toward understanding complex\n  spaces: graph Laplacians on manifolds with singularities and boundaries", "comments": null, "journal-ref": "JMLR W&CP 23: 36.1 - 36.26, 2012", "doi": null, "report-no": null, "categories": "cs.AI cs.CG cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, much of the existing work in manifold learning has been done under\nthe assumption that the data is sampled from a manifold without boundaries and\nsingularities or that the functions of interest are evaluated away from such\npoints. At the same time, it can be argued that singularities and boundaries\nare an important aspect of the geometry of realistic data.\n  In this paper we consider the behavior of graph Laplacians at points at or\nnear boundaries and two main types of other singularities: intersections, where\ndifferent manifolds come together and sharp \"edges\", where a manifold sharply\nchanges direction. We show that the behavior of graph Laplacian near these\nsingularities is quite different from that in the interior of the manifolds. In\nfact, a phenomenon somewhat reminiscent of the Gibbs effect in the analysis of\nFourier series, can be observed in the behavior of graph Laplacian near such\npoints. Unlike in the interior of the domain, where graph Laplacian converges\nto the Laplace-Beltrami operator, near singularities graph Laplacian tends to a\nfirst-order differential operator, which exhibits different scaling behavior as\na function of the kernel width. One important implication is that while points\nnear the singularities occupy only a small part of the total volume, the\ndifference in scaling results in a disproportionately large contribution to the\ntotal behavior. Another significant finding is that while the scaling behavior\nof the operator is the same near different types of singularities, they are\nvery distinct at a more refined level of analysis.\n  We believe that a comprehensive understanding of these structures in addition\nto the standard case of a smooth manifold can take us a long way toward better\nmethods for analysis of complex non-linear data and can lead to significant\nprogress in algorithm design.\n", "versions": [{"version": "v1", "created": "Wed, 28 Nov 2012 20:10:42 GMT"}], "update_date": "2012-11-29", "authors_parsed": [["Belkin", "Mikhail", ""], ["Que", "Qichao", ""], ["Wang", "Yusu", ""], ["Zhou", "Xueyuan", ""]]}, {"id": "1211.6834", "submitter": "Ting Huang", "authors": "Zengyou He, Ting Huang, Peijun Zhu", "title": "On unbiased performance evaluation for protein inference", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP cs.LG q-bio.QM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This letter is a response to the comments of Serang (2012) on Huang and He\n(2012) in Bioinformatics. Serang (2012) claimed that the parameters for the\nFido algorithm should be specified using the grid search method in Serang et\nal. (2010) so as to generate a deserved accuracy in performance comparison. It\nseems that it is an argument on parameter tuning. However, it is indeed the\nissue of how to conduct an unbiased performance evaluation for comparing\ndifferent protein inference algorithms. In this letter, we would explain why we\ndon't use the grid search for parameter selection in Huang and He (2012) and\nshow that this procedure may result in an over-estimated performance that is\nunfair to competing algorithms. In fact, this issue has also been pointed out\nby Li and Radivojac (2012).\n", "versions": [{"version": "v1", "created": "Thu, 29 Nov 2012 07:54:45 GMT"}], "update_date": "2012-11-30", "authors_parsed": [["He", "Zengyou", ""], ["Huang", "Ting", ""], ["Zhu", "Peijun", ""]]}, {"id": "1211.6851", "submitter": "Chiheb-Eddine Ben n'cir C.B.N'cir", "authors": "Chiheb-Eddine Ben N'Cir and Nadia Essoussi", "title": "Classification Recouvrante Bas\\'ee sur les M\\'ethodes \\`a Noyau", "comments": "Les 43\\`emes Journ\\'ees de Statistique", "journal-ref": "Les 43\\`emes Journ\\'ees de Statistique 2011", "doi": null, "report-no": null, "categories": "cs.LG stat.CO stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Overlapping clustering problem is an important learning issue in which\nclusters are not mutually exclusive and each object may belongs simultaneously\nto several clusters. This paper presents a kernel based method that produces\noverlapping clusters on a high feature space using mercer kernel techniques to\nimprove separability of input patterns. The proposed method, called\nOKM-K(Overlapping $k$-means based kernel method), extends OKM (Overlapping\n$k$-means) method to produce overlapping schemes. Experiments are performed on\noverlapping dataset and empirical results obtained with OKM-K outperform\nresults obtained with OKM.\n", "versions": [{"version": "v1", "created": "Thu, 29 Nov 2012 09:22:19 GMT"}], "update_date": "2012-11-30", "authors_parsed": [["N'Cir", "Chiheb-Eddine Ben", ""], ["Essoussi", "Nadia", ""]]}, {"id": "1211.6859", "submitter": "Chiheb-Eddine Ben n'cir C.B.N'cir", "authors": "Chiheb-Eddine Ben N'Cir and Nadia Essoussi and Patrice Bertrand", "title": "Overlapping clustering based on kernel similarity metric", "comments": "Second Meeting on Statistics and Data Mining 2010", "journal-ref": "Second Meeting on Statistics and Data Mining Second Meeting on\n  Statistics and Data Mining March 11-12, 2010", "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Producing overlapping schemes is a major issue in clustering. Recent proposed\noverlapping methods relies on the search of an optimal covering and are based\non different metrics, such as Euclidean distance and I-Divergence, used to\nmeasure closeness between observations. In this paper, we propose the use of\nanother measure for overlapping clustering based on a kernel similarity metric\n.We also estimate the number of overlapped clusters using the Gram matrix.\nExperiments on both Iris and EachMovie datasets show the correctness of the\nestimation of number of clusters and show that measure based on kernel\nsimilarity metric improves the precision, recall and f-measure in overlapping\nclustering.\n", "versions": [{"version": "v1", "created": "Thu, 29 Nov 2012 09:35:30 GMT"}], "update_date": "2012-11-30", "authors_parsed": [["N'Cir", "Chiheb-Eddine Ben", ""], ["Essoussi", "Nadia", ""], ["Bertrand", "Patrice", ""]]}, {"id": "1211.6887", "submitter": "Marcin Mi{\\l}kowski", "authors": "Marcin Mi{\\l}kowski", "title": "Automating rule generation for grammar checkers", "comments": "Draft of the chapter published In: Explorations Across Languages and\n  Corpora. PALC 2009, ed. by S. Go\\'zd\\'z-Roszkowski, Peter Lang, 2011, p.\n  123-133", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, I describe several approaches to automatic or semi-automatic\ndevelopment of symbolic rules for grammar checkers from the information\ncontained in corpora. The rules obtained this way are an important addition to\nmanually-created rules that seem to dominate in rule-based checkers. However,\nthe manual process of creation of rules is costly, time-consuming and\nerror-prone. It seems therefore advisable to use machine-learning algorithms to\ncreate the rules automatically or semi-automatically. The results obtained seem\nto corroborate my initial hypothesis that symbolic machine learning algorithms\ncan be useful for acquiring new rules for grammar checking. It turns out,\nhowever, that for practical uses, error corpora cannot be the sole source of\ninformation used in grammar checking. I suggest therefore that only by using\ndifferent approaches, grammar-checkers, or more generally, computer-aided\nproofreading tools, will be able to cover most frequent and severe mistakes and\navoid false alarms that seem to distract users.\n", "versions": [{"version": "v1", "created": "Thu, 29 Nov 2012 11:35:25 GMT"}], "update_date": "2012-11-30", "authors_parsed": [["Mi\u0142kowski", "Marcin", ""]]}, {"id": "1211.6898", "submitter": "Bruno Scherrer", "authors": "Bruno Scherrer (INRIA Nancy - Grand Est / LORIA), Boris Lesner (INRIA\n  Nancy - Grand Est / LORIA)", "title": "On the Use of Non-Stationary Policies for Stationary Infinite-Horizon\n  Markov Decision Processes", "comments": null, "journal-ref": "NIPS 2012 (2012)", "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider infinite-horizon stationary $\\gamma$-discounted Markov Decision\nProcesses, for which it is known that there exists a stationary optimal policy.\nUsing Value and Policy Iteration with some error $\\epsilon$ at each iteration,\nit is well-known that one can compute stationary policies that are\n$\\frac{2\\gamma}{(1-\\gamma)^2}\\epsilon$-optimal. After arguing that this\nguarantee is tight, we develop variations of Value and Policy Iteration for\ncomputing non-stationary policies that can be up to\n$\\frac{2\\gamma}{1-\\gamma}\\epsilon$-optimal, which constitutes a significant\nimprovement in the usual situation when $\\gamma$ is close to 1. Surprisingly,\nthis shows that the problem of \"computing near-optimal non-stationary policies\"\nis much simpler than that of \"computing near-optimal stationary policies\".\n", "versions": [{"version": "v1", "created": "Thu, 29 Nov 2012 12:54:58 GMT"}], "update_date": "2012-11-30", "authors_parsed": [["Scherrer", "Bruno", "", "INRIA Nancy - Grand Est / LORIA"], ["Lesner", "Boris", "", "INRIA\n  Nancy - Grand Est / LORIA"]]}, {"id": "1211.7012", "submitter": "Cezary Kaliszyk", "authors": "Cezary Kaliszyk and Josef Urban", "title": "Learning-Assisted Automated Reasoning with Flyspeck", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.DL cs.LG cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The considerable mathematical knowledge encoded by the Flyspeck project is\ncombined with external automated theorem provers (ATPs) and machine-learning\npremise selection methods trained on the proofs, producing an AI system capable\nof answering a wide range of mathematical queries automatically. The\nperformance of this architecture is evaluated in a bootstrapping scenario\nemulating the development of Flyspeck from axioms to the last theorem, each\ntime using only the previous theorems and proofs. It is shown that 39% of the\n14185 theorems could be proved in a push-button mode (without any high-level\nadvice and user interaction) in 30 seconds of real time on a fourteen-CPU\nworkstation. The necessary work involves: (i) an implementation of sound\ntranslations of the HOL Light logic to ATP formalisms: untyped first-order,\npolymorphic typed first-order, and typed higher-order, (ii) export of the\ndependency information from HOL Light and ATP proofs for the machine learners,\nand (iii) choice of suitable representations and methods for learning from\nprevious proofs, and their integration as advisors with HOL Light. This work is\ndescribed and discussed here, and an initial analysis of the body of proofs\nthat were found fully automatically is provided.\n", "versions": [{"version": "v1", "created": "Thu, 29 Nov 2012 18:15:10 GMT"}, {"version": "v2", "created": "Sun, 17 Feb 2013 09:20:30 GMT"}, {"version": "v3", "created": "Sun, 26 Oct 2014 15:02:54 GMT"}], "update_date": "2014-10-28", "authors_parsed": [["Kaliszyk", "Cezary", ""], ["Urban", "Josef", ""]]}, {"id": "1211.7045", "submitter": "Lanhui Wang", "authors": "Lanhui Wang, Amit Singer, Zaiwen Wen", "title": "Orientation Determination from Cryo-EM images Using Least Unsquared\n  Deviation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.NA math.OC q-bio.BM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A major challenge in single particle reconstruction from cryo-electron\nmicroscopy is to establish a reliable ab-initio three-dimensional model using\ntwo-dimensional projection images with unknown orientations. Common-lines based\nmethods estimate the orientations without additional geometric information.\nHowever, such methods fail when the detection rate of common-lines is too low\ndue to the high level of noise in the images. An approximation to the least\nsquares global self consistency error was obtained using convex relaxation by\nsemidefinite programming. In this paper we introduce a more robust global self\nconsistency error and show that the corresponding optimization problem can be\nsolved via semidefinite relaxation. In order to prevent artificial clustering\nof the estimated viewing directions, we further introduce a spectral norm term\nthat is added as a constraint or as a regularization term to the relaxed\nminimization problem. The resulted problems are solved by using either the\nalternating direction method of multipliers or an iteratively reweighted least\nsquares procedure. Numerical experiments with both simulated and real images\ndemonstrate that the proposed methods significantly reduce the orientation\nestimation error when the detection rate of common-lines is low.\n", "versions": [{"version": "v1", "created": "Thu, 29 Nov 2012 20:39:41 GMT"}, {"version": "v2", "created": "Wed, 10 Apr 2013 13:35:21 GMT"}], "update_date": "2013-04-11", "authors_parsed": [["Wang", "Lanhui", ""], ["Singer", "Amit", ""], ["Wen", "Zaiwen", ""]]}, {"id": "1211.7219", "submitter": "Qian Zhao", "authors": "Qian Zhao and Deyu Meng and Zongben Xu", "title": "A recursive divide-and-conquer approach for sparse principal component\n  analysis", "comments": "35 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, a new method is proposed for sparse PCA based on the recursive\ndivide-and-conquer methodology. The main idea is to separate the original\nsparse PCA problem into a series of much simpler sub-problems, each having a\nclosed-form solution. By recursively solving these sub-problems in an\nanalytical way, an efficient algorithm is constructed to solve the sparse PCA\nproblem. The algorithm only involves simple computations and is thus easy to\nimplement. The proposed method can also be very easily extended to other sparse\nPCA problems with certain constraints, such as the nonnegative sparse PCA\nproblem. Furthermore, we have shown that the proposed algorithm converges to a\nstationary point of the problem, and its computational complexity is\napproximately linear in both data size and dimensionality. The effectiveness of\nthe proposed method is substantiated by extensive experiments implemented on a\nseries of synthetic and real data in both reconstruction-error-minimization and\ndata-variance-maximization viewpoints.\n", "versions": [{"version": "v1", "created": "Fri, 30 Nov 2012 11:50:21 GMT"}], "update_date": "2012-12-03", "authors_parsed": [["Zhao", "Qian", ""], ["Meng", "Deyu", ""], ["Xu", "Zongben", ""]]}, {"id": "1211.7276", "submitter": "Duc Son Pham", "authors": "Duc Son Pham and Svetha Venkatesh", "title": "Efficient algorithms for robust recovery of images from compressed data", "comments": "Sequel of a related IEEE Transactions on Image Processing paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.LG math.IT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Compressed sensing (CS) is an important theory for sub-Nyquist sampling and\nrecovery of compressible data. Recently, it has been extended by Pham and\nVenkatesh to cope with the case where corruption to the CS data is modeled as\nimpulsive noise. The new formulation, termed as robust CS, combines robust\nstatistics and CS into a single framework to suppress outliers in the CS\nrecovery. To solve the newly formulated robust CS problem, Pham and Venkatesh\nsuggested a scheme that iteratively solves a number of CS problems, the\nsolutions from which converge to the true robust compressed sensing solution.\nHowever, this scheme is rather inefficient as it has to use existing CS solvers\nas a proxy. To overcome limitation with the original robust CS algorithm, we\npropose to solve the robust CS problem directly in this paper and drive more\ncomputationally efficient algorithms by following latest advances in\nlarge-scale convex optimization for non-smooth regularization. Furthermore, we\nalso extend the robust CS formulation to various settings, including additional\naffine constraints, $\\ell_1$-norm loss function, mixed-norm regularization, and\nmulti-tasking, so as to further improve robust CS. We also derive simple but\neffective algorithms to solve these extensions. We demonstrate that the new\nalgorithms provide much better computational advantage over the original robust\nCS formulation, and effectively solve more sophisticated extensions where the\noriginal methods simply cannot. We demonstrate the usefulness of the extensions\non several CS imaging tasks.\n", "versions": [{"version": "v1", "created": "Mon, 26 Nov 2012 15:01:15 GMT"}], "update_date": "2012-12-03", "authors_parsed": [["Pham", "Duc Son", ""], ["Venkatesh", "Svetha", ""]]}, {"id": "1211.7369", "submitter": "Franz J. Kir\\'aly", "authors": "Franz J. Kir\\'aly and Andreas Ziehe", "title": "Approximate Rank-Detecting Factorization of Low-Rank Tensors", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an algorithm, AROFAC2, which detects the (CP-)rank of a degree 3\ntensor and calculates its factorization into rank-one components. We provide\ngenerative conditions for the algorithm to work and demonstrate on both\nsynthetic and real world data that AROFAC2 is a potentially outperforming\nalternative to the gold standard PARAFAC over which it has the advantages that\nit can intrinsically detect the true rank, avoids spurious components, and is\nstable with respect to outliers and non-Gaussian noise.\n", "versions": [{"version": "v1", "created": "Fri, 30 Nov 2012 20:50:40 GMT"}], "update_date": "2012-12-03", "authors_parsed": [["Kir\u00e1ly", "Franz J.", ""], ["Ziehe", "Andreas", ""]]}]