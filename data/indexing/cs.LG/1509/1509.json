[{"id": "1509.00061", "submitter": "Hao Yi Ong", "authors": "Hao Yi Ong", "title": "Value function approximation via low-rank models", "comments": "arXiv admin note: substantial text overlap with arXiv:0912.3599 by\n  other authors", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel value function approximation technique for Markov decision\nprocesses. We consider the problem of compactly representing the state-action\nvalue function using a low-rank and sparse matrix model. The problem is to\ndecompose a matrix that encodes the true value function into low-rank and\nsparse components, and we achieve this using Robust Principal Component\nAnalysis (PCA). Under minimal assumptions, this Robust PCA problem can be\nsolved exactly via the Principal Component Pursuit convex optimization problem.\nWe experiment the procedure on several examples and demonstrate that our method\nyields approximations essentially identical to the true function.\n", "versions": [{"version": "v1", "created": "Mon, 31 Aug 2015 20:46:23 GMT"}], "update_date": "2015-09-02", "authors_parsed": [["Ong", "Hao Yi", ""]]}, {"id": "1509.00083", "submitter": "Samuel Kadoury", "authors": "Samuel Kadoury, Eugene Vorontsov, An Tang", "title": "Metastatic liver tumour segmentation from discriminant Grassmannian\n  manifolds", "comments": null, "journal-ref": "Physics in Medicine and Biology 60 (2015)", "doi": "10.1088/0031-9155/60/16/6459", "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The early detection, diagnosis and monitoring of liver cancer progression can\nbe achieved with the precise delineation of metastatic tumours. However,\naccurate automated segmentation remains challenging due to the presence of\nnoise, inhomogeneity and the high appearance variability of malignant tissue.\nIn this paper, we propose an unsupervised metastatic liver tumour segmentation\nframework using a machine learning approach based on discriminant Grassmannian\nmanifolds which learns the appearance of tumours with respect to normal tissue.\nFirst, the framework learns within-class and between-class similarity\ndistributions from a training set of images to discover the optimal manifold\ndiscrimination between normal and pathological tissue in the liver. Second, a\nconditional optimisation scheme computes nonlocal pairwise as well as\npattern-based clique potentials from the manifold subspace to recognise regions\nwith similar labelings and to incorporate global consistency in the\nsegmentation process. The proposed framework was validated on a clinical\ndatabase of 43 CT images from patients with metastatic liver cancer. Compared\nto state-of-the-art methods, our method achieves a better performance on two\nseparate datasets of metastatic liver tumours from different clinical sites,\nyielding an overall mean Dice similarity coefficient of 90.7 +/- 2.4 in over 50\ntumours with an average volume of 27.3 mm3.\n", "versions": [{"version": "v1", "created": "Mon, 31 Aug 2015 21:45:40 GMT"}], "update_date": "2015-09-02", "authors_parsed": [["Kadoury", "Samuel", ""], ["Vorontsov", "Eugene", ""], ["Tang", "An", ""]]}, {"id": "1509.00114", "submitter": "Yao Xie", "authors": "Yang Cao, Yao Xie, and Nagi Gebraeel", "title": "Multi-Sensor Slope Change Detection", "comments": "Accepted with minor revision at ANOR", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop a mixture procedure for multi-sensor systems to monitor data\nstreams for a change-point that causes a gradual degradation to a subset of the\nstreams. Observations are assumed to be initially normal random variables with\nknown constant means and variances. After the change-point, observations in the\nsubset will have increasing or decreasing means. The subset and the\nrate-of-changes are unknown. Our procedure uses a mixture statistics, which\nassumes that each sensor is affected by the change-point with probability\n$p_0$. Analytic expressions are obtained for the average run length (ARL) and\nthe expected detection delay (EDD) of the mixture procedure, which are\ndemonstrated to be quite accurate numerically. We establish the asymptotic\noptimality of the mixture procedure. Numerical examples demonstrate the good\nperformance of the proposed procedure. We also discuss an adaptive mixture\nprocedure using empirical Bayes. This paper extends our earlier work on\ndetecting an abrupt change-point that causes a mean-shift, by tackling the\nchallenges posed by the non-stationarity of the slope-change problem.\n", "versions": [{"version": "v1", "created": "Tue, 1 Sep 2015 01:49:15 GMT"}, {"version": "v2", "created": "Thu, 18 Feb 2016 03:08:31 GMT"}], "update_date": "2016-02-19", "authors_parsed": [["Cao", "Yang", ""], ["Xie", "Yao", ""], ["Gebraeel", "Nagi", ""]]}, {"id": "1509.00137", "submitter": "Yao Xie", "authors": "Yao Xie, Ruiyang Song, Hanjun Dai, Qingbin Li, Le Song", "title": "Online Supervised Subspace Tracking", "comments": "Submitted for journal publication", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a framework for supervised subspace tracking, when there are two\ntime series $x_t$ and $y_t$, one being the high-dimensional predictors and the\nother being the response variables and the subspace tracking needs to take into\nconsideration of both sequences. It extends the classic online subspace\ntracking work which can be viewed as tracking of $x_t$ only. Our online\nsufficient dimensionality reduction (OSDR) is a meta-algorithm that can be\napplied to various cases including linear regression, logistic regression,\nmultiple linear regression, multinomial logistic regression, support vector\nmachine, the random dot product model and the multi-scale union-of-subspace\nmodel. OSDR reduces data-dimensionality on-the-fly with low-computational\ncomplexity and it can also handle missing data and dynamic data. OSDR uses an\nalternating minimization scheme and updates the subspace via gradient descent\non the Grassmannian manifold. The subspace update can be performed efficiently\nutilizing the fact that the Grassmannian gradient with respect to the subspace\nin many settings is rank-one (or low-rank in certain cases). The optimization\nproblem for OSDR is non-convex and hard to analyze in general; we provide\nconvergence analysis of OSDR in a simple linear regression setting. The good\nperformance of OSDR compared with the conventional unsupervised subspace\ntracking are demonstrated via numerical examples on simulated and real data.\n", "versions": [{"version": "v1", "created": "Tue, 1 Sep 2015 04:42:39 GMT"}], "update_date": "2015-09-02", "authors_parsed": [["Xie", "Yao", ""], ["Song", "Ruiyang", ""], ["Dai", "Hanjun", ""], ["Li", "Qingbin", ""], ["Song", "Le", ""]]}, {"id": "1509.00151", "submitter": "Zhangyang Wang", "authors": "Zhangyang Wang, Shiyu Chang, Jiayu Zhou, Meng Wang, Thomas S. Huang", "title": "Learning A Task-Specific Deep Architecture For Clustering", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While sparse coding-based clustering methods have shown to be successful,\ntheir bottlenecks in both efficiency and scalability limit the practical usage.\nIn recent years, deep learning has been proved to be a highly effective,\nefficient and scalable feature learning tool. In this paper, we propose to\nemulate the sparse coding-based clustering pipeline in the context of deep\nlearning, leading to a carefully crafted deep model benefiting from both. A\nfeed-forward network structure, named TAGnet, is constructed based on a\ngraph-regularized sparse coding algorithm. It is then trained with\ntask-specific loss functions from end to end. We discover that connecting deep\nlearning to sparse coding benefits not only the model performance, but also its\ninitialization and interpretation. Moreover, by introducing auxiliary\nclustering tasks to the intermediate feature hierarchy, we formulate DTAGnet\nand obtain a further performance boost. Extensive experiments demonstrate that\nthe proposed model gains remarkable margins over several state-of-the-art\nmethods.\n", "versions": [{"version": "v1", "created": "Tue, 1 Sep 2015 06:12:29 GMT"}, {"version": "v2", "created": "Tue, 29 Sep 2015 18:32:27 GMT"}, {"version": "v3", "created": "Fri, 16 Oct 2015 06:38:37 GMT"}], "update_date": "2015-10-19", "authors_parsed": [["Wang", "Zhangyang", ""], ["Chang", "Shiyu", ""], ["Zhou", "Jiayu", ""], ["Wang", "Meng", ""], ["Huang", "Thomas S.", ""]]}, {"id": "1509.00153", "submitter": "Zhangyang Wang", "authors": "Zhangyang Wang, Qing Ling, Thomas S. Huang", "title": "Learning Deep $\\ell_0$ Encoders", "comments": "Full paper at AAAI 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite its nonconvex nature, $\\ell_0$ sparse approximation is desirable in\nmany theoretical and application cases. We study the $\\ell_0$ sparse\napproximation problem with the tool of deep learning, by proposing Deep\n$\\ell_0$ Encoders. Two typical forms, the $\\ell_0$ regularized problem and the\n$M$-sparse problem, are investigated. Based on solid iterative algorithms, we\nmodel them as feed-forward neural networks, through introducing novel neurons\nand pooling functions. Enforcing such structural priors acts as an effective\nnetwork regularization. The deep encoders also enjoy faster inference, larger\nlearning capacity, and better scalability compared to conventional sparse\ncoding solutions. Furthermore, under task-driven losses, the models can be\nconveniently optimized from end to end. Numerical results demonstrate the\nimpressive performances of the proposed encoders.\n", "versions": [{"version": "v1", "created": "Tue, 1 Sep 2015 06:20:34 GMT"}, {"version": "v2", "created": "Sun, 22 Nov 2015 16:02:43 GMT"}], "update_date": "2015-11-24", "authors_parsed": [["Wang", "Zhangyang", ""], ["Ling", "Qing", ""], ["Huang", "Thomas S.", ""]]}, {"id": "1509.00181", "submitter": "Yingxue Zhou", "authors": "Pan Zhou, Yingxue Zhou, Dapeng Wu and Hai Jin", "title": "Differentially Private Online Learning for Cloud-Based Video\n  Recommendation with Multimedia Big Data in Social Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the rapid growth in multimedia services and the enormous offers of video\ncontents in online social networks, users have difficulty in obtaining their\ninterests. Therefore, various personalized recommendation systems have been\nproposed. However, they ignore that the accelerated proliferation of social\nmedia data has led to the big data era, which has greatly impeded the process\nof video recommendation. In addition, none of them has considered both the\nprivacy of users' contexts (e,g., social status, ages and hobbies) and video\nservice vendors' repositories, which are extremely sensitive and of significant\ncommercial value. To handle the problems, we propose a cloud-assisted\ndifferentially private video recommendation system based on distributed online\nlearning. In our framework, service vendors are modeled as distributed\ncooperative learners, recommending videos according to user's context, while\nsimultaneously adapting the video-selection strategy based on user-click\nfeedback to maximize total user clicks (reward). Considering the sparsity and\nheterogeneity of big social media data, we also propose a novel geometric\ndifferentially private model, which can greatly reduce the performance\n(recommendation accuracy) loss. Our simulation shows the proposed algorithms\noutperform other existing methods and keep a delicate balance between computing\naccuracy and privacy preserving level.\n", "versions": [{"version": "v1", "created": "Tue, 1 Sep 2015 09:01:07 GMT"}, {"version": "v2", "created": "Wed, 2 Sep 2015 03:03:14 GMT"}, {"version": "v3", "created": "Sat, 10 Oct 2015 04:59:08 GMT"}, {"version": "v4", "created": "Tue, 27 Oct 2015 07:46:32 GMT"}, {"version": "v5", "created": "Fri, 30 Oct 2015 08:30:28 GMT"}, {"version": "v6", "created": "Mon, 11 Jan 2016 06:20:50 GMT"}, {"version": "v7", "created": "Mon, 1 Feb 2016 05:06:37 GMT"}], "update_date": "2016-02-02", "authors_parsed": [["Zhou", "Pan", ""], ["Zhou", "Yingxue", ""], ["Wu", "Dapeng", ""], ["Jin", "Hai", ""]]}, {"id": "1509.00202", "submitter": "Vladimir Savic Dr", "authors": "Vladimir Savic and Erik G. Larsson", "title": "Fingerprinting-Based Positioning in Distributed Massive MIMO Systems", "comments": "Proc. of IEEE 82nd Vehicular Technology Conference (VTC2015-Fall)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.LG math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Location awareness in wireless networks may enable many applications such as\nemergency services, autonomous driving and geographic routing. Although there\nare many available positioning techniques, none of them is adapted to work with\nmassive multiple-in-multiple-out (MIMO) systems, which represent a leading 5G\ntechnology candidate. In this paper, we discuss possible solutions for\npositioning of mobile stations using a vector of signals at the base station,\nequipped with many antennas distributed over deployment area. Our main proposal\nis to use fingerprinting techniques based on a vector of received signal\nstrengths. This kind of methods are able to work in highly-cluttered multipath\nenvironments, and require just one base station, in contrast to standard\nrange-based and angle-based techniques. We also provide a solution for\nfingerprinting-based positioning based on Gaussian process regression, and\ndiscuss main applications and challenges.\n", "versions": [{"version": "v1", "created": "Tue, 1 Sep 2015 09:56:05 GMT"}], "update_date": "2015-09-02", "authors_parsed": [["Savic", "Vladimir", ""], ["Larsson", "Erik G.", ""]]}, {"id": "1509.00296", "submitter": "Tae-Hyun Oh", "authors": "Tae-Hyun Oh, Yasuyuki Matsushita, Yu-Wing Tai, In So Kweon", "title": "Fast Randomized Singular Value Thresholding for Low-rank Optimization", "comments": "Appeared in CVPR 2015, and under major revision of TPAMI. Source code\n  is available on http://thoh.kaist.ac.kr", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Rank minimization can be converted into tractable surrogate problems, such as\nNuclear Norm Minimization (NNM) and Weighted NNM (WNNM). The problems related\nto NNM, or WNNM, can be solved iteratively by applying a closed-form proximal\noperator, called Singular Value Thresholding (SVT), or Weighted SVT, but they\nsuffer from high computational cost of Singular Value Decomposition (SVD) at\neach iteration. We propose a fast and accurate approximation method for SVT,\nthat we call fast randomized SVT (FRSVT), with which we avoid direct\ncomputation of SVD. The key idea is to extract an approximate basis for the\nrange of the matrix from its compressed matrix. Given the basis, we compute\npartial singular values of the original matrix from the small factored matrix.\nIn addition, by developping a range propagation method, our method further\nspeeds up the extraction of approximate basis at each iteration. Our\ntheoretical analysis shows the relationship between the approximation bound of\nSVD and its effect to NNM via SVT. Along with the analysis, our empirical\nresults quantitatively and qualitatively show that our approximation rarely\nharms the convergence of the host algorithms. We assess the efficiency and\naccuracy of the proposed method on various computer vision problems, e.g.,\nsubspace clustering, weather artifact removal, and simultaneous multi-image\nalignment and rectification.\n", "versions": [{"version": "v1", "created": "Tue, 1 Sep 2015 13:49:11 GMT"}, {"version": "v2", "created": "Mon, 22 Aug 2016 17:43:05 GMT"}], "update_date": "2019-02-18", "authors_parsed": [["Oh", "Tae-Hyun", ""], ["Matsushita", "Yasuyuki", ""], ["Tai", "Yu-Wing", ""], ["Kweon", "In So", ""]]}, {"id": "1509.00498", "submitter": "Dezhi Hong", "authors": "Dezhi Hong, Jorge Ortiz, Arka Bhattacharya, Kamin Whitehouse", "title": "Sensor-Type Classification in Buildings", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many sensors/meters are deployed in commercial buildings to monitor and\noptimize their performance. However, because sensor metadata is inconsistent\nacross buildings, software-based solutions are tightly coupled to the sensor\nmetadata conventions (i.e. schemas and naming) for each building. Running the\nsame software across buildings requires significant integration effort.\n  Metadata normalization is critical for scaling the deployment process and\nallows us to decouple building-specific conventions from the code written for\nbuilding applications. It also allows us to deal with missing metadata. One\nimportant aspect of normalization is to differentiate sensors by the typeof\nphenomena being observed. In this paper, we propose a general, simple, yet\neffective classification scheme to differentiate sensors in buildings by type.\nWe perform ensemble learning on data collected from over 2000 sensor streams in\ntwo buildings. Our approach is able to achieve more than 92% accuracy for\nclassification within buildings and more than 82% accuracy for across\nbuildings. We also introduce a method for identifying potential misclassified\nstreams. This is important because it allows us to identify opportunities to\nattain more input from experts -- input that could help improve classification\naccuracy when ground truth is unavailable. We show that by adjusting a\nthreshold value we are able to identify at least 30% of the misclassified\ninstances.\n", "versions": [{"version": "v1", "created": "Tue, 1 Sep 2015 20:46:19 GMT"}], "update_date": "2015-09-03", "authors_parsed": [["Hong", "Dezhi", ""], ["Ortiz", "Jorge", ""], ["Bhattacharya", "Arka", ""], ["Whitehouse", "Kamin", ""]]}, {"id": "1509.00519", "submitter": "Yuri Burda", "authors": "Yuri Burda, Roger Grosse, Ruslan Salakhutdinov", "title": "Importance Weighted Autoencoders", "comments": "Submitted to ICLR 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The variational autoencoder (VAE; Kingma, Welling (2014)) is a recently\nproposed generative model pairing a top-down generative network with a\nbottom-up recognition network which approximates posterior inference. It\ntypically makes strong assumptions about posterior inference, for instance that\nthe posterior distribution is approximately factorial, and that its parameters\ncan be approximated with nonlinear regression from the observations. As we show\nempirically, the VAE objective can lead to overly simplified representations\nwhich fail to use the network's entire modeling capacity. We present the\nimportance weighted autoencoder (IWAE), a generative model with the same\narchitecture as the VAE, but which uses a strictly tighter log-likelihood lower\nbound derived from importance weighting. In the IWAE, the recognition network\nuses multiple samples to approximate the posterior, giving it increased\nflexibility to model complex posteriors which do not fit the VAE modeling\nassumptions. We show empirically that IWAEs learn richer latent space\nrepresentations than VAEs, leading to improved test log-likelihood on density\nestimation benchmarks.\n", "versions": [{"version": "v1", "created": "Tue, 1 Sep 2015 22:33:13 GMT"}, {"version": "v2", "created": "Tue, 10 Nov 2015 19:43:20 GMT"}, {"version": "v3", "created": "Mon, 11 Jan 2016 05:47:05 GMT"}, {"version": "v4", "created": "Mon, 7 Nov 2016 17:29:24 GMT"}], "update_date": "2016-11-08", "authors_parsed": [["Burda", "Yuri", ""], ["Grosse", "Roger", ""], ["Salakhutdinov", "Ruslan", ""]]}, {"id": "1509.00692", "submitter": "Zahid Ansari", "authors": "Zahid Ansari, Waseem Ahmed, M.F. Azeem and A.Vinaya Babu", "title": "Discovery of Web Usage Profiles Using Various Clustering Techniques", "comments": "arXiv admin note: substantial text overlap with arXiv:1507.03340", "journal-ref": "International Journal of Computer Information Systems, pp. 18-27\n  Vol. 1, No. 3, July 2011. (ISSN 2229-5208, Silicon Valley Publishers, United\n  Kingdom)", "doi": null, "report-no": null, "categories": "cs.DB cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The explosive growth of World Wide Web (WWW) has necessitated the development\nof Web personalization systems in order to understand the user preferences to\ndynamically serve customized content to individual users. To reveal information\nabout user preferences from Web usage data, Web Usage Mining (WUM) techniques\nare extensively being applied to the Web log data. Clustering techniques are\nwidely used in WUM to capture similar interests and trends among users\naccessing a Web site. Clustering aims to divide a data set into groups or\nclusters where inter-cluster similarities are minimized while the intra cluster\nsimilarities are maximized. This paper reviews four of the popularly used\nclustering techniques: k-Means, k-Medoids, Leader and DBSCAN. These techniques\nare implemented and tested against the Web user navigational data. Performance\nand validity results of each technique are presented and compared.\n", "versions": [{"version": "v1", "created": "Tue, 1 Sep 2015 09:31:37 GMT"}], "update_date": "2015-09-03", "authors_parsed": [["Ansari", "Zahid", ""], ["Ahmed", "Waseem", ""], ["Azeem", "M. F.", ""], ["Babu", "A. Vinaya", ""]]}, {"id": "1509.00727", "submitter": "Anupama Nandi", "authors": "Joseph Anderson, Navin Goyal, Anupama Nandi, Luis Rademacher", "title": "Heavy-tailed Independent Component Analysis", "comments": "30 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.ST stat.CO stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Independent component analysis (ICA) is the problem of efficiently recovering\na matrix $A \\in \\mathbb{R}^{n\\times n}$ from i.i.d. observations of $X=AS$\nwhere $S \\in \\mathbb{R}^n$ is a random vector with mutually independent\ncoordinates. This problem has been intensively studied, but all existing\nefficient algorithms with provable guarantees require that the coordinates\n$S_i$ have finite fourth moments. We consider the heavy-tailed ICA problem\nwhere we do not make this assumption, about the second moment. This problem\nalso has received considerable attention in the applied literature. In the\npresent work, we first give a provably efficient algorithm that works under the\nassumption that for constant $\\gamma > 0$, each $S_i$ has finite\n$(1+\\gamma)$-moment, thus substantially weakening the moment requirement\ncondition for the ICA problem to be solvable. We then give an algorithm that\nworks under the assumption that matrix $A$ has orthogonal columns but requires\nno moment assumptions. Our techniques draw ideas from convex geometry and\nexploit standard properties of the multivariate spherical Gaussian distribution\nin a novel way.\n", "versions": [{"version": "v1", "created": "Wed, 2 Sep 2015 14:56:22 GMT"}], "update_date": "2015-09-03", "authors_parsed": [["Anderson", "Joseph", ""], ["Goyal", "Navin", ""], ["Nandi", "Anupama", ""], ["Rademacher", "Luis", ""]]}, {"id": "1509.00825", "submitter": "Rafael Menelau Oliveira E Cruz", "authors": "Rafael M. O. Cruz, Robert Sabourin, George D. C. Cavalcanti", "title": "A DEEP analysis of the META-DES framework for dynamic selection of\n  ensemble of classifiers", "comments": "47 Pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Dynamic ensemble selection (DES) techniques work by estimating the level of\ncompetence of each classifier from a pool of classifiers. Only the most\ncompetent ones are selected to classify a given test sample. Hence, the key\nissue in DES is the criterion used to estimate the level of competence of the\nclassifiers in predicting the label of a given test sample. In order to perform\na more robust ensemble selection, we proposed the META-DES framework using\nmeta-learning, where multiple criteria are encoded as meta-features and are\npassed down to a meta-classifier that is trained to estimate the competence\nlevel of a given classifier. In this technical report, we present a\nstep-by-step analysis of each phase of the framework during training and test.\nWe show how each set of meta-features is extracted as well as their impact on\nthe estimation of the competence level of the base classifier. Moreover, an\nanalysis of the impact of several factors in the system performance, such as\nthe number of classifiers in the pool, the use of different linear base\nclassifiers, as well as the size of the validation data. We show that using the\ndynamic selection of linear classifiers through the META-DES framework, we can\nsolve complex non-linear classification problems where other combination\ntechniques such as AdaBoost cannot.\n", "versions": [{"version": "v1", "created": "Wed, 2 Sep 2015 19:14:47 GMT"}, {"version": "v2", "created": "Fri, 11 Sep 2015 02:30:27 GMT"}], "update_date": "2015-09-14", "authors_parsed": [["Cruz", "Rafael M. O.", ""], ["Sabourin", "Robert", ""], ["Cavalcanti", "George D. C.", ""]]}, {"id": "1509.00838", "submitter": "Hongyuan Mei", "authors": "Hongyuan Mei and Mohit Bansal and Matthew R. Walter", "title": "What to talk about and how? Selective Generation using LSTMs with\n  Coarse-to-Fine Alignment", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose an end-to-end, domain-independent neural encoder-aligner-decoder\nmodel for selective generation, i.e., the joint task of content selection and\nsurface realization. Our model first encodes a full set of over-determined\ndatabase event records via an LSTM-based recurrent neural network, then\nutilizes a novel coarse-to-fine aligner to identify the small subset of salient\nrecords to talk about, and finally employs a decoder to generate free-form\ndescriptions of the aligned, selected records. Our model achieves the best\nselection and generation results reported to-date (with 59% relative\nimprovement in generation) on the benchmark WeatherGov dataset, despite using\nno specialized features or linguistic resources. Using an improved k-nearest\nneighbor beam filter helps further. We also perform a series of ablations and\nvisualizations to elucidate the contributions of our key model components.\nLastly, we evaluate the generalizability of our model on the RoboCup dataset,\nand get results that are competitive with or better than the state-of-the-art,\ndespite being severely data-starved.\n", "versions": [{"version": "v1", "created": "Wed, 2 Sep 2015 19:52:56 GMT"}, {"version": "v2", "created": "Fri, 8 Jan 2016 23:07:32 GMT"}], "update_date": "2016-01-12", "authors_parsed": [["Mei", "Hongyuan", ""], ["Bansal", "Mohit", ""], ["Walter", "Matthew R.", ""]]}, {"id": "1509.00913", "submitter": "Andrew Simpson", "authors": "Andrew J.R. Simpson", "title": "On-the-Fly Learning in a Perpetual Learning Machine", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite the promise of brain-inspired machine learning, deep neural networks\n(DNN) have frustratingly failed to bridge the deceptively large gap between\nlearning and memory. Here, we introduce a Perpetual Learning Machine; a new\ntype of DNN that is capable of brain-like dynamic 'on the fly' learning because\nit exists in a self-supervised state of Perpetual Stochastic Gradient Descent.\nThus, we provide the means to unify learning and memory within a machine\nlearning framework. We also explore the elegant duality of abstraction and\nsynthesis: the Yin and Yang of deep learning.\n", "versions": [{"version": "v1", "created": "Thu, 3 Sep 2015 01:30:29 GMT"}, {"version": "v2", "created": "Tue, 8 Sep 2015 07:54:41 GMT"}, {"version": "v3", "created": "Tue, 29 Sep 2015 16:57:42 GMT"}], "update_date": "2015-09-30", "authors_parsed": [["Simpson", "Andrew J. R.", ""]]}, {"id": "1509.01004", "submitter": "Yohei  Kondo", "authors": "Yohei Kondo, Kohei Hayashi, Shin-ichi Maeda", "title": "Bayesian Masking: Sparse Bayesian Estimation with Weaker Shrinkage Bias", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A common strategy for sparse linear regression is to introduce\nregularization, which eliminates irrelevant features by letting the\ncorresponding weights be zeros. However, regularization often shrinks the\nestimator for relevant features, which leads to incorrect feature selection.\nMotivated by the above-mentioned issue, we propose Bayesian masking (BM), a\nsparse estimation method which imposes no regularization on the weights. The\nkey concept of BM is to introduce binary latent variables that randomly mask\nfeatures. Estimating the masking rates determines the relevance of the features\nautomatically. We derive a variational Bayesian inference algorithm that\nmaximizes the lower bound of the factorized information criterion (FIC), which\nis a recently developed asymptotic criterion for evaluating the marginal\nlog-likelihood. In addition, we propose reparametrization to accelerate the\nconvergence of the derived algorithm. Finally, we show that BM outperforms\nLasso and automatic relevance determination (ARD) in terms of the\nsparsity-shrinkage trade-off.\n", "versions": [{"version": "v1", "created": "Thu, 3 Sep 2015 09:35:48 GMT"}, {"version": "v2", "created": "Tue, 6 Oct 2015 06:07:55 GMT"}], "update_date": "2015-10-07", "authors_parsed": [["Kondo", "Yohei", ""], ["Hayashi", "Kohei", ""], ["Maeda", "Shin-ichi", ""]]}, {"id": "1509.01053", "submitter": "Malte Probst", "authors": "Malte Probst and Franz Rothlauf", "title": "Training a Restricted Boltzmann Machine for Classification by Labeling\n  Model Samples", "comments": null, "journal-ref": null, "doi": null, "report-no": "Technical Report 04/2012", "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose an alternative method for training a classification model. Using\nthe MNIST set of handwritten digits and Restricted Boltzmann Machines, it is\npossible to reach a classification performance competitive to semi-supervised\nlearning if we first train a model in an unsupervised fashion on unlabeled data\nonly, and then manually add labels to model samples instead of training data\nsamples with the help of a GUI. This approach can benefit from the fact that\nmodel samples can be presented to the human labeler in a video-like fashion,\nresulting in a higher number of labeled examples. Also, after some initial\ntraining, hard-to-classify examples can be distinguished from easy ones\nautomatically, saving manual work.\n", "versions": [{"version": "v1", "created": "Thu, 3 Sep 2015 12:13:37 GMT"}], "update_date": "2015-09-04", "authors_parsed": [["Probst", "Malte", ""], ["Rothlauf", "Franz", ""]]}, {"id": "1509.01116", "submitter": "Nicol\\`o Navarin", "authors": "Giovanni Da San Martino, Nicol\\`o Navarin and Alessandro Sperduti", "title": "A tree-based kernel for graphs with continuous attributes", "comments": "This work has been submitted to the IEEE Transactions on Neural\n  Networks and Learning Systems for possible publication. Copyright may be\n  transferred without notice, after which this version may no longer be\n  accessible", "journal-ref": null, "doi": "10.1109/TNNLS.2017.2705694", "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The availability of graph data with node attributes that can be either\ndiscrete or real-valued is constantly increasing. While existing kernel methods\nare effective techniques for dealing with graphs having discrete node labels,\ntheir adaptation to non-discrete or continuous node attributes has been\nlimited, mainly for computational issues. Recently, a few kernels especially\ntailored for this domain, and that trade predictive performance for\ncomputational efficiency, have been proposed. In this paper, we propose a graph\nkernel for complex and continuous nodes' attributes, whose features are tree\nstructures extracted from specific graph visits. The kernel manages to keep the\nsame complexity of state-of-the-art kernels while implicitly using a larger\nfeature space. We further present an approximated variant of the kernel which\nreduces its complexity significantly. Experimental results obtained on six\nreal-world datasets show that the kernel is the best performing one on most of\nthem. Moreover, in most cases the approximated version reaches comparable\nperformances to current state-of-the-art kernels in terms of classification\naccuracy while greatly shortening the running times.\n", "versions": [{"version": "v1", "created": "Thu, 3 Sep 2015 14:59:10 GMT"}, {"version": "v2", "created": "Tue, 20 Dec 2016 16:54:02 GMT"}], "update_date": "2017-06-16", "authors_parsed": [["Martino", "Giovanni Da San", ""], ["Navarin", "Nicol\u00f2", ""], ["Sperduti", "Alessandro", ""]]}, {"id": "1509.01168", "submitter": "Andreas Damianou Dr", "authors": "Andreas Damianou, Neil D. Lawrence", "title": "Semi-described and semi-supervised learning with Gaussian processes", "comments": "Published in the proceedings for Uncertainty in Artificial\n  Intelligence (UAI), 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.LG math.PR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Propagating input uncertainty through non-linear Gaussian process (GP)\nmappings is intractable. This hinders the task of training GPs using uncertain\nand partially observed inputs. In this paper we refer to this task as\n\"semi-described learning\". We then introduce a GP framework that solves both,\nthe semi-described and the semi-supervised learning problems (where missing\nvalues occur in the outputs). Auto-regressive state space simulation is also\nrecognised as a special case of semi-described learning. To achieve our goal we\ndevelop variational methods for handling semi-described inputs in GPs, and\ncouple them with algorithms that allow for imputing the missing values while\ntreating the uncertainty in a principled, Bayesian manner. Extensive\nexperiments on simulated and real-world data study the problems of iterative\nforecasting and regression/classification with missing values. The results\nsuggest that the principled propagation of uncertainty stemming from our\nframework can significantly improve performance in these tasks.\n", "versions": [{"version": "v1", "created": "Thu, 3 Sep 2015 17:22:15 GMT"}], "update_date": "2015-09-04", "authors_parsed": [["Damianou", "Andreas", ""], ["Lawrence", "Neil D.", ""]]}, {"id": "1509.01208", "submitter": "Da Kuang", "authors": "Da Kuang, Barry Drake, Haesun Park", "title": "Fast Clustering and Topic Modeling Based on Rank-2 Nonnegative Matrix\n  Factorization", "comments": "This paper has been withdrawn by the author to clarify the authorship", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.IR cs.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The importance of unsupervised clustering and topic modeling is well\nrecognized with ever-increasing volumes of text data. In this paper, we propose\na fast method for hierarchical clustering and topic modeling called HierNMF2.\nOur method is based on fast Rank-2 nonnegative matrix factorization (NMF) that\nperforms binary clustering and an efficient node splitting rule. Further\nutilizing the final leaf nodes generated in HierNMF2 and the idea of\nnonnegative least squares fitting, we propose a new clustering/topic modeling\nmethod called FlatNMF2 that recovers a flat clustering/topic modeling result in\na very simple yet significantly more effective way than any other existing\nmethods. We implement highly optimized open source software in C++ for both\nHierNMF2 and FlatNMF2 for hierarchical and partitional clustering/topic\nmodeling of document data sets.\n  Substantial experimental tests are presented that illustrate significant\nimprovements both in computational time as well as quality of solutions. We\ncompare our methods to other clustering methods including K-means, standard\nNMF, and CLUTO, and also topic modeling methods including latent Dirichlet\nallocation (LDA) and recently proposed algorithms for NMF with separability\nconstraints. Overall, we present efficient tools for analyzing large-scale data\nsets, and techniques that can be generalized to many other data analytics\nproblem domains.\n", "versions": [{"version": "v1", "created": "Thu, 3 Sep 2015 18:55:28 GMT"}, {"version": "v2", "created": "Thu, 1 Oct 2015 04:29:04 GMT"}, {"version": "v3", "created": "Fri, 2 Oct 2015 18:06:13 GMT"}], "update_date": "2015-10-05", "authors_parsed": [["Kuang", "Da", ""], ["Drake", "Barry", ""], ["Park", "Haesun", ""]]}, {"id": "1509.01240", "submitter": "Moritz Hardt", "authors": "Moritz Hardt and Benjamin Recht and Yoram Singer", "title": "Train faster, generalize better: Stability of stochastic gradient\n  descent", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We show that parametric models trained by a stochastic gradient method (SGM)\nwith few iterations have vanishing generalization error. We prove our results\nby arguing that SGM is algorithmically stable in the sense of Bousquet and\nElisseeff. Our analysis only employs elementary tools from convex and\ncontinuous optimization. We derive stability bounds for both convex and\nnon-convex optimization under standard Lipschitz and smoothness assumptions.\n  Applying our results to the convex case, we provide new insights for why\nmultiple epochs of stochastic gradient methods generalize well in practice. In\nthe non-convex case, we give a new interpretation of common practices in neural\nnetworks, and formally show that popular techniques for training large deep\nmodels are indeed stability-promoting. Our findings conceptually underscore the\nimportance of reducing training time beyond its obvious benefit.\n", "versions": [{"version": "v1", "created": "Thu, 3 Sep 2015 19:53:40 GMT"}, {"version": "v2", "created": "Sun, 7 Feb 2016 17:06:58 GMT"}], "update_date": "2016-02-09", "authors_parsed": [["Hardt", "Moritz", ""], ["Recht", "Benjamin", ""], ["Singer", "Yoram", ""]]}, {"id": "1509.01270", "submitter": "Hamidreza Farhidzadeh", "authors": "Hamidreza Farhidzadeh", "title": "Machine Learning Methods to Analyze Arabidopsis Thaliana Plant Root\n  Growth", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One of the challenging problems in biology is to classify plants based on\ntheir reaction on genetic mutation. Arabidopsis Thaliana is a plant that is so\ninteresting, because its genetic structure has some similarities with that of\nhuman beings. Biologists classify the type of this plant to mutated and not\nmutated (wild) types. Phenotypic analysis of these types is a time-consuming\nand costly effort by individuals. In this paper, we propose a modified feature\nextraction step by using velocity and acceleration of root growth. In the\nsecond step, for plant classification, we employed different Support Vector\nMachine (SVM) kernels and two hybrid systems of neural networks. Gated Negative\nCorrelation Learning (GNCL) and Mixture of Negatively Correlated Experts (MNCE)\nare two ensemble methods based on complementary feature of classical\nclassifiers; Mixture of Expert (ME) and Negative Correlation Learning (NCL).\nThe hybrid systems conserve of advantages and decrease the effects of\ndisadvantages of NCL and ME. Our Experimental shows that MNCE and GNCL improve\nthe efficiency of classical classifiers, however, some SVM kernels function has\nbetter performance than classifiers based on neural network ensemble method.\nMoreover, kernels consume less time to obtain a classification rate.\n", "versions": [{"version": "v1", "created": "Thu, 3 Sep 2015 20:22:43 GMT"}], "update_date": "2015-09-07", "authors_parsed": [["Farhidzadeh", "Hamidreza", ""]]}, {"id": "1509.01271", "submitter": "Hamidreza Farhidzadeh", "authors": "Hamidreza Farhidzadeh", "title": "Probabilistic Neural Network Training for Semi-Supervised Classifiers", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose another version of help-training approach by\nemploying a Probabilistic Neural Network (PNN) that improves the performance of\nthe main discriminative classifier in the semi-supervised strategy. We\nintroduce the PNN-training algorithm and use it for training the support vector\nmachine (SVM) with a few numbers of labeled data and a large number of\nunlabeled data. We try to find the best labels for unlabeled data and then use\nSVM to enhance the classification rate. We test our method on two famous\nbenchmarks and show the efficiency of our method in comparison with pervious\nmethods.\n", "versions": [{"version": "v1", "created": "Thu, 3 Sep 2015 20:30:19 GMT"}], "update_date": "2015-09-07", "authors_parsed": [["Farhidzadeh", "Hamidreza", ""]]}, {"id": "1509.01288", "submitter": "Max Zimmermann", "authors": "Max Zimmermann, Eirini Ntoutsi, Myra Spiliopoulou", "title": "Incremental Active Opinion Learning Over a Stream of Opinionated\n  Documents", "comments": "10 pages, 14 figures, conference: WISDOM (KDD'15)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Applications that learn from opinionated documents, like tweets or product\nreviews, face two challenges. First, the opinionated documents constitute an\nevolving stream, where both the author's attitude and the vocabulary itself may\nchange. Second, labels of documents are scarce and labels of words are\nunreliable, because the sentiment of a word depends on the (unknown) context in\nthe author's mind. Most of the research on mining over opinionated streams\nfocuses on the first aspect of the problem, whereas for the second a continuous\nsupply of labels from the stream is assumed. Such an assumption though is\nutopian as the stream is infinite and the labeling cost is prohibitive. To this\nend, we investigate the potential of active stream learning algorithms that ask\nfor labels on demand. Our proposed ACOSTREAM 1 approach works with limited\nlabels: it uses an initial seed of labeled documents, occasionally requests\nadditional labels for documents from the human expert and incrementally adapts\nto the underlying stream while exploiting the available labeled documents. In\nits core, ACOSTREAM consists of a MNB classifier coupled with \"sampling\"\nstrategies for requesting class labels for new unlabeled documents. In the\nexperiments, we evaluate the classifier performance over time by varying: (a)\nthe class distribution of the opinionated stream, while assuming that the set\nof the words in the vocabulary is fixed but their polarities may change with\nthe class distribution; and (b) the number of unknown words arriving at each\nmoment, while the class polarity may also change. Our results show that active\nlearning on a stream of opinionated documents, delivers good performance while\nrequiring a small selection of labels\n", "versions": [{"version": "v1", "created": "Thu, 3 Sep 2015 22:11:10 GMT"}], "update_date": "2015-09-07", "authors_parsed": [["Zimmermann", "Max", ""], ["Ntoutsi", "Eirini", ""], ["Spiliopoulou", "Myra", ""]]}, {"id": "1509.01323", "submitter": "Yi Guo", "authors": "Xia Hong, Sheng Chen, Yi Guo, Junbin Gao", "title": "l1-norm Penalized Orthogonal Forward Regression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A l1-norm penalized orthogonal forward regression (l1-POFR) algorithm is\nproposed based on the concept of leaveone- out mean square error (LOOMSE).\nFirstly, a new l1-norm penalized cost function is defined in the constructed\northogonal space, and each orthogonal basis is associated with an individually\ntunable regularization parameter. Secondly, due to orthogonal computation, the\nLOOMSE can be analytically computed without actually splitting the data set,\nand moreover a closed form of the optimal regularization parameter in terms of\nminimal LOOMSE is derived. Thirdly, a lower bound for regularization parameters\nis proposed, which can be used for robust LOOMSE estimation by adaptively\ndetecting and removing regressors to an inactive set so that the computational\ncost of the algorithm is significantly reduced. Illustrative examples are\nincluded to demonstrate the effectiveness of this new l1-POFR approach.\n", "versions": [{"version": "v1", "created": "Fri, 4 Sep 2015 01:45:09 GMT"}], "update_date": "2015-09-07", "authors_parsed": [["Hong", "Xia", ""], ["Chen", "Sheng", ""], ["Guo", "Yi", ""], ["Gao", "Junbin", ""]]}, {"id": "1509.01346", "submitter": "Nayyar Zaidi", "authors": "Nayyar A. Zaidi, Geoffrey I. Webb, Mark J. Carman, Francois Petitjean", "title": "Deep Broad Learning - Big Models for Big Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning has demonstrated the power of detailed modeling of complex\nhigh-order (multivariate) interactions in data. For some learning tasks there\nis power in learning models that are not only Deep but also Broad. By Broad, we\nmean models that incorporate evidence from large numbers of features. This is\nof especial value in applications where many different features and\ncombinations of features all carry small amounts of information about the\nclass. The most accurate models will integrate all that information. In this\npaper, we propose an algorithm for Deep Broad Learning called DBL. The proposed\nalgorithm has a tunable parameter $n$, that specifies the depth of the model.\nIt provides straightforward paths towards out-of-core learning for large data.\nWe demonstrate that DBL learns models from large quantities of data with\naccuracy that is highly competitive with the state-of-the-art.\n", "versions": [{"version": "v1", "created": "Fri, 4 Sep 2015 06:01:11 GMT"}], "update_date": "2015-09-07", "authors_parsed": [["Zaidi", "Nayyar A.", ""], ["Webb", "Geoffrey I.", ""], ["Carman", "Mark J.", ""], ["Petitjean", "Francois", ""]]}, {"id": "1509.01349", "submitter": "Konstantin Avrachenkov", "authors": "Konstantin Avrachenkov (MAESTRO), Vivek Borkar, Krishnakant Saboo", "title": "Parallel and Distributed Approaches for Graph Based Semi-supervised\n  Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Two approaches for graph based semi-supervised learning are proposed. The\nfirstapproach is based on iteration of an affine map. A key element of the\naffine map iteration is sparsematrix-vector multiplication, which has several\nvery efficient parallel implementations. The secondapproach belongs to the\nclass of Markov Chain Monte Carlo (MCMC) algorithms. It is based onsampling of\nnodes by performing a random walk on the graph. The latter approach is\ndistributedby its nature and can be easily implemented on several processors or\nover the network. Boththeoretical and practical evaluations are provided. It is\nfound that the nodes are classified intotheir class with very small error. The\nsampling algorithm's ability to track new incoming nodesand to classify them is\nalso demonstrated.\n", "versions": [{"version": "v1", "created": "Fri, 4 Sep 2015 06:35:55 GMT"}], "update_date": "2015-09-07", "authors_parsed": [["Avrachenkov", "Konstantin", "", "MAESTRO"], ["Borkar", "Vivek", ""], ["Saboo", "Krishnakant", ""]]}, {"id": "1509.01352", "submitter": "Rangeet Mitra", "authors": "Rangeet Mitra and Vimal Bhatia", "title": "Diffusion-KLMS Algorithm and its Performance Analysis for Non-Linear\n  Distributed Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DC cs.IT cs.SY math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In a distributed network environment, the diffusion-least mean squares (LMS)\nalgorithm gives faster convergence than the original LMS algorithm. It has also\nbeen observed that, the diffusion-LMS generally outperforms other distributed\nLMS algorithms like spatial LMS and incremental LMS. However, both the original\nLMS and diffusion-LMS are not applicable in non-linear environments where data\nmay not be linearly separable. A variant of LMS called kernel-LMS (KLMS) has\nbeen proposed in the literature for such non-linearities. In this paper, we\npropose kernelised version of diffusion-LMS for non-linear distributed\nenvironments. Simulations show that the proposed approach has superior\nconvergence as compared to algorithms of the same genre. We also introduce a\ntechnique to predict the transient and steady-state behaviour of the proposed\nalgorithm. The techniques proposed in this work (or algorithms of same genre)\ncan be easily extended to distributed parameter estimation applications like\ncooperative spectrum sensing and massive multiple input multiple output (MIMO)\nreceiver design which are potential components for 5G communication systems.\n", "versions": [{"version": "v1", "created": "Fri, 4 Sep 2015 06:48:39 GMT"}], "update_date": "2015-09-07", "authors_parsed": [["Mitra", "Rangeet", ""], ["Bhatia", "Vimal", ""]]}, {"id": "1509.01354", "submitter": "Jinma Guo", "authors": "Jinma Guo and Jianmin Li", "title": "CNN Based Hashing for Image Retrieval", "comments": "16 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Along with data on the web increasing dramatically, hashing is becoming more\nand more popular as a method of approximate nearest neighbor search. Previous\nsupervised hashing methods utilized similarity/dissimilarity matrix to get\nsemantic information. But the matrix is not easy to construct for a new\ndataset. Rather than to reconstruct the matrix, we proposed a straightforward\nCNN-based hashing method, i.e. binarilizing the activations of a fully\nconnected layer with threshold 0 and taking the binary result as hash codes.\nThis method achieved the best performance on CIFAR-10 and was comparable with\nthe state-of-the-art on MNIST. And our experiments on CIFAR-10 suggested that\nthe signs of activations may carry more information than the relative values of\nactivations between samples, and that the co-adaption between feature extractor\nand hash functions is important for hashing.\n", "versions": [{"version": "v1", "created": "Fri, 4 Sep 2015 07:08:44 GMT"}], "update_date": "2015-09-07", "authors_parsed": [["Guo", "Jinma", ""], ["Li", "Jianmin", ""]]}, {"id": "1509.01386", "submitter": "Jawwad Ahmed Dr.", "authors": "Jawwad Ahmed, Andreas Johnsson, Rerngvit Yanggratoke, John Ardelius,\n  Christofer Flinta, Rolf Stadler", "title": "Predicting SLA Violations in Real Time using Online Machine Learning", "comments": "8 pages, 5 figures, 4 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI cs.LG cs.SE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Detecting faults and SLA violations in a timely manner is critical for\ntelecom providers, in order to avoid loss in business, revenue and reputation.\nAt the same time predicting SLA violations for user services in telecom\nenvironments is difficult, due to time-varying user demands and infrastructure\nload conditions.\n  In this paper, we propose a service-agnostic online learning approach,\nwhereby the behavior of the system is learned on the fly, in order to predict\nclient-side SLA violations. The approach uses device-level metrics, which are\ncollected in a streaming fashion on the server side.\n  Our results show that the approach can produce highly accurate predictions\n(>90% classification accuracy and < 10% false alarm rate) in scenarios where\nSLA violations are predicted for a video-on-demand service under changing load\npatterns. The paper also highlight the limitations of traditional offline\nlearning methods, which perform significantly worse in many of the considered\nscenarios.\n", "versions": [{"version": "v1", "created": "Fri, 4 Sep 2015 09:54:48 GMT"}], "update_date": "2015-09-07", "authors_parsed": [["Ahmed", "Jawwad", ""], ["Johnsson", "Andreas", ""], ["Yanggratoke", "Rerngvit", ""], ["Ardelius", "John", ""], ["Flinta", "Christofer", ""], ["Stadler", "Rolf", ""]]}, {"id": "1509.01404", "submitter": "Nicolas Gillis", "authors": "Arnaud Vandaele, Nicolas Gillis, Qi Lei, Kai Zhong, Inderjit Dhillon", "title": "Coordinate Descent Methods for Symmetric Nonnegative Matrix\n  Factorization", "comments": "25 pages, 5 figures, 7 tables. Main changes: comparison with another\n  symNMF algorithm (namely, BetaSNMF), and correction of an error in the\n  convergence proof", "journal-ref": "IEEE Transactions on Signal Processing 64 (21), pp. 5571-5584,\n  2016", "doi": "10.1109/TSP.2016.2591510", "report-no": null, "categories": "cs.NA cs.CV cs.LG math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given a symmetric nonnegative matrix $A$, symmetric nonnegative matrix\nfactorization (symNMF) is the problem of finding a nonnegative matrix $H$,\nusually with much fewer columns than $A$, such that $A \\approx HH^T$. SymNMF\ncan be used for data analysis and in particular for various clustering tasks.\nIn this paper, we propose simple and very efficient coordinate descent schemes\nto solve this problem, and that can handle large and sparse input matrices. The\neffectiveness of our methods is illustrated on synthetic and real-world data\nsets, and we show that they perform favorably compared to recent\nstate-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Fri, 4 Sep 2015 11:19:35 GMT"}, {"version": "v2", "created": "Tue, 31 May 2016 12:50:38 GMT"}], "update_date": "2016-10-07", "authors_parsed": [["Vandaele", "Arnaud", ""], ["Gillis", "Nicolas", ""], ["Lei", "Qi", ""], ["Zhong", "Kai", ""], ["Dhillon", "Inderjit", ""]]}, {"id": "1509.01469", "submitter": "Ruiqi Guo", "authors": "Ruiqi Guo, Sanjiv Kumar, Krzysztof Choromanski and David Simcha", "title": "Quantization based Fast Inner Product Search", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a quantization based approach for fast approximate Maximum Inner\nProduct Search (MIPS). Each database vector is quantized in multiple subspaces\nvia a set of codebooks, learned directly by minimizing the inner product\nquantization error. Then, the inner product of a query to a database vector is\napproximated as the sum of inner products with the subspace quantizers.\nDifferent from recently proposed LSH approaches to MIPS, the database vectors\nand queries do not need to be augmented in a higher dimensional feature space.\nWe also provide a theoretical analysis of the proposed approach, consisting of\nthe concentration results under mild assumptions. Furthermore, if a small\nsample of example queries is given at the training time, we propose a modified\ncodebook learning procedure which further improves the accuracy. Experimental\nresults on a variety of datasets including those arising from deep neural\nnetworks show that the proposed approach significantly outperforms the existing\nstate-of-the-art.\n", "versions": [{"version": "v1", "created": "Fri, 4 Sep 2015 14:43:11 GMT"}], "update_date": "2015-09-07", "authors_parsed": [["Guo", "Ruiqi", ""], ["Kumar", "Sanjiv", ""], ["Choromanski", "Krzysztof", ""], ["Simcha", "David", ""]]}, {"id": "1509.01509", "submitter": "Radu Horaud P", "authors": "Israel D. Gebru, Xavier Alameda-Pineda, Florence Forbes and Radu\n  Horaud", "title": "EM Algorithms for Weighted-Data Clustering with Application to\n  Audio-Visual Scene Analysis", "comments": "14 pages, 4 figures, 4 tables", "journal-ref": "IEEE Transactions on Pattern Analysis and Machine Intelligence,\n  volume 38, number 12, 2402 - 2415, 2016", "doi": "10.1109/TPAMI.2016.2522425", "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data clustering has received a lot of attention and numerous methods,\nalgorithms and software packages are available. Among these techniques,\nparametric finite-mixture models play a central role due to their interesting\nmathematical properties and to the existence of maximum-likelihood estimators\nbased on expectation-maximization (EM). In this paper we propose a new mixture\nmodel that associates a weight with each observed point. We introduce the\nweighted-data Gaussian mixture and we derive two EM algorithms. The first one\nconsiders a fixed weight for each observation. The second one treats each\nweight as a random variable following a gamma distribution. We propose a model\nselection method based on a minimum message length criterion, provide a weight\ninitialization strategy, and validate the proposed algorithms by comparing them\nwith several state of the art parametric and non-parametric clustering\ntechniques. We also demonstrate the effectiveness and robustness of the\nproposed clustering technique in the presence of heterogeneous data, namely\naudio-visual scene analysis.\n", "versions": [{"version": "v1", "created": "Fri, 4 Sep 2015 15:51:17 GMT"}, {"version": "v2", "created": "Mon, 25 Jan 2016 11:17:13 GMT"}], "update_date": "2017-01-31", "authors_parsed": [["Gebru", "Israel D.", ""], ["Alameda-Pineda", "Xavier", ""], ["Forbes", "Florence", ""], ["Horaud", "Radu", ""]]}, {"id": "1509.01546", "submitter": "David Hofmeyr", "authors": "David P. Hofmeyr and Nicos G. Pavlidis and Idris A. Eckley", "title": "Minimum Spectral Connectivity Projection Pursuit", "comments": null, "journal-ref": "Statistics and Computing (2019) 29: 391", "doi": "10.1007/s11222-018-9814-6", "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of determining the optimal low dimensional projection\nfor maximising the separability of a binary partition of an unlabelled dataset,\nas measured by spectral graph theory. This is achieved by finding projections\nwhich minimise the second eigenvalue of the graph Laplacian of the projected\ndata, which corresponds to a non-convex, non-smooth optimisation problem. We\nshow that the optimal univariate projection based on spectral connectivity\nconverges to the vector normal to the maximum margin hyperplane through the\ndata, as the scaling parameter is reduced to zero. This establishes a\nconnection between connectivity as measured by spectral graph theory and\nmaximal Euclidean separation. The computational cost associated with each\neigen-problem is quadratic in the number of data. To mitigate this issue, we\npropose an approximation method using microclusters with provable approximation\nerror bounds. Combining multiple binary partitions within a divisive\nhierarchical model allows us to construct clustering solutions admitting\nclusters with varying scales and lying within different subspaces. We evaluate\nthe performance of the proposed method on a large collection of benchmark\ndatasets and find that it compares favourably with existing methods for\nprojection pursuit and dimension reduction for data clustering.\n", "versions": [{"version": "v1", "created": "Fri, 4 Sep 2015 18:06:31 GMT"}, {"version": "v2", "created": "Thu, 28 Apr 2016 21:31:02 GMT"}, {"version": "v3", "created": "Mon, 13 Nov 2017 05:45:33 GMT"}], "update_date": "2019-05-29", "authors_parsed": [["Hofmeyr", "David P.", ""], ["Pavlidis", "Nicos G.", ""], ["Eckley", "Idris A.", ""]]}, {"id": "1509.01549", "submitter": "Matthew Lai", "authors": "Matthew Lai", "title": "Giraffe: Using Deep Reinforcement Learning to Play Chess", "comments": "MSc Dissertation", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This report presents Giraffe, a chess engine that uses self-play to discover\nall its domain-specific knowledge, with minimal hand-crafted knowledge given by\nthe programmer. Unlike previous attempts using machine learning only to perform\nparameter-tuning on hand-crafted evaluation functions, Giraffe's learning\nsystem also performs automatic feature extraction and pattern recognition. The\ntrained evaluation function performs comparably to the evaluation functions of\nstate-of-the-art chess engines - all of which containing thousands of lines of\ncarefully hand-crafted pattern recognizers, tuned over many years by both\ncomputer chess experts and human chess masters. Giraffe is the most successful\nattempt thus far at using end-to-end machine learning to play chess.\n", "versions": [{"version": "v1", "created": "Fri, 4 Sep 2015 18:21:52 GMT"}, {"version": "v2", "created": "Mon, 14 Sep 2015 15:42:35 GMT"}], "update_date": "2015-09-15", "authors_parsed": [["Lai", "Matthew", ""]]}, {"id": "1509.01618", "submitter": "Chengtao Li", "authors": "Chengtao Li, Stefanie Jegelka and Suvrit Sra", "title": "Efficient Sampling for k-Determinantal Point Processes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Determinantal Point Processes (DPPs) are elegant probabilistic models of\nrepulsion and diversity over discrete sets of items. But their applicability to\nlarge sets is hindered by expensive cubic-complexity matrix operations for\nbasic tasks such as sampling. In light of this, we propose a new method for\napproximate sampling from discrete $k$-DPPs. Our method takes advantage of the\ndiversity property of subsets sampled from a DPP, and proceeds in two stages:\nfirst it constructs coresets for the ground set of items; thereafter, it\nefficiently samples subsets based on the constructed coresets. As opposed to\nprevious approaches, our algorithm aims to minimize the total variation\ndistance to the original distribution. Experiments on both synthetic and real\ndatasets indicate that our sampling algorithm works efficiently on large data\nsets, and yields more accurate samples than previous approaches.\n", "versions": [{"version": "v1", "created": "Fri, 4 Sep 2015 21:38:17 GMT"}, {"version": "v2", "created": "Sat, 28 May 2016 00:37:56 GMT"}], "update_date": "2016-05-31", "authors_parsed": [["Li", "Chengtao", ""], ["Jegelka", "Stefanie", ""], ["Sra", "Suvrit", ""]]}, {"id": "1509.01626", "submitter": "Xiang Zhang", "authors": "Xiang Zhang, Junbo Zhao, Yann LeCun", "title": "Character-level Convolutional Networks for Text Classification", "comments": "An early version of this work entitled \"Text Understanding from\n  Scratch\" was posted in Feb 2015 as arXiv:1502.01710. The present paper has\n  considerably more experimental results and a rewritten introduction, Advances\n  in Neural Information Processing Systems 28 (NIPS 2015)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article offers an empirical exploration on the use of character-level\nconvolutional networks (ConvNets) for text classification. We constructed\nseveral large-scale datasets to show that character-level convolutional\nnetworks could achieve state-of-the-art or competitive results. Comparisons are\noffered against traditional models such as bag of words, n-grams and their\nTFIDF variants, and deep learning models such as word-based ConvNets and\nrecurrent neural networks.\n", "versions": [{"version": "v1", "created": "Fri, 4 Sep 2015 22:31:53 GMT"}, {"version": "v2", "created": "Thu, 10 Sep 2015 17:12:43 GMT"}, {"version": "v3", "created": "Mon, 4 Apr 2016 02:34:30 GMT"}], "update_date": "2016-04-05", "authors_parsed": [["Zhang", "Xiang", ""], ["Zhao", "Junbo", ""], ["LeCun", "Yann", ""]]}, {"id": "1509.01644", "submitter": "Warwick Masson", "authors": "Warwick Masson, Pravesh Ranchod, George Konidaris", "title": "Reinforcement Learning with Parameterized Actions", "comments": "Accepted for AAAI 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a model-free algorithm for learning in Markov decision processes\nwith parameterized actions-discrete actions with continuous parameters. At each\nstep the agent must select both which action to use and which parameters to use\nwith that action. We introduce the Q-PAMDP algorithm for learning in these\ndomains, show that it converges to a local optimum, and compare it to direct\npolicy search in the goal-scoring and Platform domains.\n", "versions": [{"version": "v1", "created": "Sat, 5 Sep 2015 00:17:35 GMT"}, {"version": "v2", "created": "Tue, 15 Sep 2015 20:44:11 GMT"}, {"version": "v3", "created": "Tue, 22 Sep 2015 14:48:21 GMT"}, {"version": "v4", "created": "Thu, 26 Nov 2015 12:00:42 GMT"}], "update_date": "2015-11-30", "authors_parsed": [["Masson", "Warwick", ""], ["Ranchod", "Pravesh", ""], ["Konidaris", "George", ""]]}, {"id": "1509.01659", "submitter": "Armen Aghajanyan", "authors": "Armen Aghajanyan", "title": "Gravitational Clustering", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The downfall of many supervised learning algorithms, such as neural networks,\nis the inherent need for a large amount of training data. Although there is a\nlot of buzz about big data, there is still the problem of doing classification\nfrom a small dataset. Other methods such as support vector machines, although\ncapable of dealing with few samples, are inherently binary classifiers, and are\nin need of learning strategies such as One vs All in the case of\nmulti-classification. In the presence of a large number of classes this can\nbecome problematic. In this paper we present, a novel approach to supervised\nlearning through the method of clustering. Unlike traditional methods such as\nK-Means, Gravitational Clustering does not require the initial number of\nclusters, and automatically builds the clusters, individual samples can be\narbitrarily weighted and it requires only few samples while staying resilient\nto over-fitting.\n", "versions": [{"version": "v1", "created": "Sat, 5 Sep 2015 03:37:50 GMT"}], "update_date": "2015-09-08", "authors_parsed": [["Aghajanyan", "Armen", ""]]}, {"id": "1509.01698", "submitter": "Umut \\c{S}im\\c{s}ekli", "authors": "Kamer Kaya, Figen \\\"Oztoprak, \\c{S}. \\.Ilker Birbil, A. Taylan Cemgil,\n  Umut \\c{S}im\\c{s}ekli, Nurdan Kuru, Hazal Koptagel, M. Kaan \\\"Ozt\\\"urk", "title": "HAMSI: A Parallel Incremental Optimization Algorithm Using Quadratic\n  Approximations for Solving Partially Separable Problems", "comments": "The software is available at https://github.com/spartensor/hamsi-mf", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose HAMSI (Hessian Approximated Multiple Subsets Iteration), which is\na provably convergent, second order incremental algorithm for solving\nlarge-scale partially separable optimization problems. The algorithm is based\non a local quadratic approximation, and hence, allows incorporating curvature\ninformation to speed-up the convergence. HAMSI is inherently parallel and it\nscales nicely with the number of processors. Combined with techniques for\neffectively utilizing modern parallel computer architectures, we illustrate\nthat the proposed method converges more rapidly than a parallel stochastic\ngradient descent when both methods are used to solve large-scale matrix\nfactorization problems. This performance gain comes only at the expense of\nusing memory that scales linearly with the total size of the optimization\nvariables. We conclude that HAMSI may be considered as a viable alternative in\nmany large scale problems, where first order methods based on variants of\nstochastic gradient descent are applicable.\n", "versions": [{"version": "v1", "created": "Sat, 5 Sep 2015 12:48:01 GMT"}, {"version": "v2", "created": "Sun, 27 Sep 2015 12:15:23 GMT"}, {"version": "v3", "created": "Wed, 21 Dec 2016 15:09:50 GMT"}, {"version": "v4", "created": "Fri, 4 Aug 2017 04:37:32 GMT"}], "update_date": "2017-08-07", "authors_parsed": [["Kaya", "Kamer", ""], ["\u00d6ztoprak", "Figen", ""], ["Birbil", "\u015e. \u0130lker", ""], ["Cemgil", "A. Taylan", ""], ["\u015eim\u015fekli", "Umut", ""], ["Kuru", "Nurdan", ""], ["Koptagel", "Hazal", ""], ["\u00d6zt\u00fcrk", "M. Kaan", ""]]}, {"id": "1509.01710", "submitter": "Wenhao Jiang", "authors": "Wenhao Jiang, Cheng Deng, Wei Liu, Feiping Nie, Fu-lai Chung, Heng\n  Huang", "title": "Theoretic Analysis and Extremely Easy Algorithms for Domain Adaptive\n  Feature Learning", "comments": "ijcai", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Domain adaptation problems arise in a variety of applications, where a\ntraining dataset from the \\textit{source} domain and a test dataset from the\n\\textit{target} domain typically follow different distributions. The primary\ndifficulty in designing effective learning models to solve such problems lies\nin how to bridge the gap between the source and target distributions. In this\npaper, we provide comprehensive analysis of feature learning algorithms used in\nconjunction with linear classifiers for domain adaptation. Our analysis shows\nthat in order to achieve good adaptation performance, the second moments of the\nsource domain distribution and target domain distribution should be similar.\nBased on our new analysis, a novel extremely easy feature learning algorithm\nfor domain adaptation is proposed. Furthermore, our algorithm is extended by\nleveraging multiple layers, leading to a deep linear model. We evaluate the\neffectiveness of the proposed algorithms in terms of domain adaptation tasks on\nthe Amazon review dataset and the spam dataset from the ECML/PKDD 2006\ndiscovery challenge.\n", "versions": [{"version": "v1", "created": "Sat, 5 Sep 2015 15:44:33 GMT"}, {"version": "v2", "created": "Thu, 10 Aug 2017 10:17:03 GMT"}], "update_date": "2017-08-11", "authors_parsed": [["Jiang", "Wenhao", ""], ["Deng", "Cheng", ""], ["Liu", "Wei", ""], ["Nie", "Feiping", ""], ["Chung", "Fu-lai", ""], ["Huang", "Heng", ""]]}, {"id": "1509.01770", "submitter": "Kishan Wimalawarne", "authors": "Kishan Wimalawarne, Ryota Tomioka and Masashi Sugiyama", "title": "Theoretical and Experimental Analyses of Tensor-Based Regression and\n  Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We theoretically and experimentally investigate tensor-based regression and\nclassification. Our focus is regularization with various tensor norms,\nincluding the overlapped trace norm, the latent trace norm, and the scaled\nlatent trace norm. We first give dual optimization methods using the\nalternating direction method of multipliers, which is computationally efficient\nwhen the number of training samples is moderate. We then theoretically derive\nan excess risk bound for each tensor norm and clarify their behavior. Finally,\nwe perform extensive experiments using simulated and real data and demonstrate\nthe superiority of tensor-based learning methods over vector- and matrix-based\nlearning methods.\n", "versions": [{"version": "v1", "created": "Sun, 6 Sep 2015 05:03:27 GMT"}], "update_date": "2015-09-08", "authors_parsed": [["Wimalawarne", "Kishan", ""], ["Tomioka", "Ryota", ""], ["Sugiyama", "Masashi", ""]]}, {"id": "1509.01771", "submitter": "Gibran Fuentes-Pineda", "authors": "Gibran Fuentes-Pineda and Ivan Vladimir Meza-Ruiz", "title": "Sampled Weighted Min-Hashing for Large-Scale Topic Mining", "comments": "10 pages, Proceedings of the Mexican Conference on Pattern\n  Recognition 2015", "journal-ref": null, "doi": "10.1007/978-3-319-19264-2_20", "report-no": null, "categories": "cs.LG cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present Sampled Weighted Min-Hashing (SWMH), a randomized approach to\nautomatically mine topics from large-scale corpora. SWMH generates multiple\nrandom partitions of the corpus vocabulary based on term co-occurrence and\nagglomerates highly overlapping inter-partition cells to produce the mined\ntopics. While other approaches define a topic as a probabilistic distribution\nover a vocabulary, SWMH topics are ordered subsets of such vocabulary.\nInterestingly, the topics mined by SWMH underlie themes from the corpus at\ndifferent levels of granularity. We extensively evaluate the meaningfulness of\nthe mined topics both qualitatively and quantitatively on the NIPS (1.7 K\ndocuments), 20 Newsgroups (20 K), Reuters (800 K) and Wikipedia (4 M) corpora.\nAdditionally, we compare the quality of SWMH with Online LDA topics for\ndocument representation in classification.\n", "versions": [{"version": "v1", "created": "Sun, 6 Sep 2015 06:09:28 GMT"}, {"version": "v2", "created": "Tue, 8 Sep 2015 03:14:12 GMT"}], "update_date": "2015-09-09", "authors_parsed": [["Fuentes-Pineda", "Gibran", ""], ["Meza-Ruiz", "Ivan Vladimir", ""]]}, {"id": "1509.01815", "submitter": "Valery Vilisov", "authors": "Valery Vilisov", "title": "Research: Analysis of Transport Model that Approximates Decision Taker's\n  Preferences", "comments": null, "journal-ref": null, "doi": "10.13140/RG.2.1.5085.6166", "report-no": null, "categories": "cs.LG cs.AI math.OC stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Paper provides a method for solving the reverse Monge-Kantorovich transport\nproblem (TP). It allows to accumulate positive decision-taking experience made\nby decision-taker in situations that can be presented in the form of TP. The\ninitial data for the solution of the inverse TP is the information on orders,\ninventories and effective decisions take by decision-taker. The result of\nsolving the inverse TP contains evaluations of the TPs payoff matrix elements.\nIt can be used in new situations to select the solution corresponding to the\npreferences of the decision-taker. The method allows to gain decision-taker\nexperience, so it can be used by others. The method allows to build the model\nof decision-taker preferences in a specific application area. The model can be\nupdated regularly to ensure its relevance and adequacy to the decision-taker\nsystem of preferences. This model is adaptive to the current preferences of the\ndecision taker.\n", "versions": [{"version": "v1", "created": "Sun, 6 Sep 2015 14:25:45 GMT"}], "update_date": "2015-09-08", "authors_parsed": [["Vilisov", "Valery", ""]]}, {"id": "1509.01817", "submitter": "Gaurav Pandey", "authors": "Gaurav Pandey and Ambedkar Dukkipati", "title": "On collapsed representation of hierarchical Completely Random Measures", "comments": "11 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.LG stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The aim of the paper is to provide an exact approach for generating a Poisson\nprocess sampled from a hierarchical CRM, without having to instantiate the\ninfinitely many atoms of the random measures. We use completely random\nmeasures~(CRM) and hierarchical CRM to define a prior for Poisson processes. We\nderive the marginal distribution of the resultant point process, when the\nunderlying CRM is marginalized out. Using well known properties unique to\nPoisson processes, we were able to derive an exact approach for instantiating a\nPoisson process with a hierarchical CRM prior. Furthermore, we derive Gibbs\nsampling strategies for hierarchical CRM models based on Chinese restaurant\nfranchise sampling scheme. As an example, we present the sum of generalized\ngamma process (SGGP), and show its application in topic-modelling. We show that\none can determine the power-law behaviour of the topics and words in a Bayesian\nfashion, by defining a prior on the parameters of SGGP.\n", "versions": [{"version": "v1", "created": "Sun, 6 Sep 2015 14:44:38 GMT"}, {"version": "v2", "created": "Thu, 2 Jun 2016 06:46:28 GMT"}], "update_date": "2016-06-03", "authors_parsed": [["Pandey", "Gaurav", ""], ["Dukkipati", "Ambedkar", ""]]}, {"id": "1509.01851", "submitter": "David Balduzzi", "authors": "David Balduzzi", "title": "Deep Online Convex Optimization by Putting Forecaster to Sleep", "comments": "Rendered obsolete by arXiv:1604.01952. The new version contains the\n  same basic results, with major changes to exposition and minor changes to\n  terminology", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.GT cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Methods from convex optimization such as accelerated gradient descent are\nwidely used as building blocks for deep learning algorithms. However, the\nreasons for their empirical success are unclear, since neural networks are not\nconvex and standard guarantees do not apply. This paper develops the first\nrigorous link between online convex optimization and error backpropagation on\nconvolutional networks. The first step is to introduce circadian games, a mild\ngeneralization of convex games with similar convergence properties. The main\nresult is that error backpropagation on a convolutional network is equivalent\nto playing out a circadian game. It follows immediately that the waking-regret\nof players in the game (the units in the neural network) controls the overall\nrate of convergence of the network. Finally, we explore some implications of\nthe results: (i) we describe the representations learned by a neural network\ngame-theoretically, (ii) propose a learning setting at the level of individual\nunits that can be plugged into deep architectures, and (iii) propose a new\napproach to adaptive model selection by applying bandit algorithms to choose\nwhich players to wake on each round.\n", "versions": [{"version": "v1", "created": "Sun, 6 Sep 2015 20:25:32 GMT"}, {"version": "v2", "created": "Fri, 8 Apr 2016 00:44:00 GMT"}], "update_date": "2016-04-11", "authors_parsed": [["Balduzzi", "David", ""]]}, {"id": "1509.01951", "submitter": "Atul Katole", "authors": "Atul Laxman Katole, Krishna Prasad Yellapragada, Amish Kumar Bedi,\n  Sehaj Singh Kalra and Mynepalli Siva Chaitanya", "title": "Hierarchical Deep Learning Architecture For 10K Objects Classification", "comments": "As appeared in proceedings for CS & IT 2015 - Second International\n  Conference on Computer Science & Engineering (CSEN 2015)", "journal-ref": "Computer Science & Information Technology (CS & IT) (2015) 77-93", "doi": "10.5121/csit.2015.51408", "report-no": null, "categories": "cs.CV cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Evolution of visual object recognition architectures based on Convolutional\nNeural Networks & Convolutional Deep Belief Networks paradigms has\nrevolutionized artificial Vision Science. These architectures extract & learn\nthe real world hierarchical visual features utilizing supervised & unsupervised\nlearning approaches respectively. Both the approaches yet cannot scale up\nrealistically to provide recognition for a very large number of objects as high\nas 10K. We propose a two level hierarchical deep learning architecture inspired\nby divide & conquer principle that decomposes the large scale recognition\narchitecture into root & leaf level model architectures. Each of the root &\nleaf level models is trained exclusively to provide superior results than\npossible by any 1-level deep learning architecture prevalent today. The\nproposed architecture classifies objects in two steps. In the first step the\nroot level model classifies the object in a high level category. In the second\nstep, the leaf level recognition model for the recognized high level category\nis selected among all the leaf models. This leaf level model is presented with\nthe same input object image which classifies it in a specific category. Also we\npropose a blend of leaf level models trained with either supervised or\nunsupervised learning approaches. Unsupervised learning is suitable whenever\nlabelled data is scarce for the specific leaf level models. Currently the\ntraining of leaf level models is in progress; where we have trained 25 out of\nthe total 47 leaf level models as of now. We have trained the leaf models with\nthe best case top-5 error rate of 3.2% on the validation data set for the\nparticular leaf models. Also we demonstrate that the validation error of the\nleaf level models saturates towards the above mentioned accuracy as the number\nof epochs are increased to more than sixty.\n", "versions": [{"version": "v1", "created": "Mon, 7 Sep 2015 08:49:39 GMT"}], "update_date": "2015-09-08", "authors_parsed": [["Katole", "Atul Laxman", ""], ["Yellapragada", "Krishna Prasad", ""], ["Bedi", "Amish Kumar", ""], ["Kalra", "Sehaj Singh", ""], ["Chaitanya", "Mynepalli Siva", ""]]}, {"id": "1509.01957", "submitter": "Jos\\'e Halloy", "authors": "A. Gribovskiy, F. Mondada, J.L. Deneubourg, L. Cazenille, N. Bredeche,\n  J. Halloy", "title": "Automated Analysis of Behavioural Variability and Filial Imprinting of\n  Chicks (G. gallus), using Autonomous Robots", "comments": "17 pages, 17 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.QM cs.LG cs.RO physics.bio-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Inter-individual variability has various impacts in animal social behaviour.\nThis implies that not only collective behaviours have to be studied but also\nthe behavioural variability of each member composing the groups. To understand\nthose effects on group behaviour, we develop a quantitative methodology based\non automated ethograms and autonomous robots to study the inter-individual\nvariability among social animals. We choose chicks of \\textit{Gallus gallus\ndomesticus} as a classic social animal model system for their suitability in\nlaboratory and controlled experimentation. Moreover, even domesticated chicken\npresent social structures implying forms or leadership and filial imprinting.\nWe develop an imprinting methodology on autonomous robots to study individual\nand social behaviour of free moving animals. This allows to quantify the\nbehaviours of large number of animals. We develop an automated experimental\nmethodology that allows to make relatively fast controlled experiments and\nefficient data analysis. Our analysis are based on high-throughput data\nallowing a fine quantification of individual behavioural traits. We quantify\nthe efficiency of various state-of-the-art algorithms to automate data analysis\nand produce automated ethograms. We show that the use of robots allows to\nprovide controlled and quantified stimuli to the animals in absence of human\nintervention. We quantify the individual behaviour of 205 chicks obtained from\nhatching after synchronized fecundation. Our results show a high variability of\nindividual behaviours and of imprinting quality and success. Three classes of\nchicks are observed with various level of imprinting. Our study shows that the\nconcomitant use of autonomous robots and automated ethograms allows detailed\nand quantitative analysis of behavioural patterns of animals in controlled\nlaboratory experiments.\n", "versions": [{"version": "v1", "created": "Mon, 7 Sep 2015 09:22:43 GMT"}], "update_date": "2016-01-28", "authors_parsed": [["Gribovskiy", "A.", ""], ["Mondada", "F.", ""], ["Deneubourg", "J. L.", ""], ["Cazenille", "L.", ""], ["Bredeche", "N.", ""], ["Halloy", "J.", ""]]}, {"id": "1509.02409", "submitter": "Mortaza Doulaty", "authors": "Mortaza Doulaty, Oscar Saz, Thomas Hain", "title": "Data-selective Transfer Learning for Multi-Domain Speech Recognition", "comments": null, "journal-ref": "16th Interspeech.Proc. (2015) 2897-2901", "doi": null, "report-no": null, "categories": "cs.LG cs.CL cs.SD", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Negative transfer in training of acoustic models for automatic speech\nrecognition has been reported in several contexts such as domain change or\nspeaker characteristics. This paper proposes a novel technique to overcome\nnegative transfer by efficient selection of speech data for acoustic model\ntraining. Here data is chosen on relevance for a specific target. A submodular\nfunction based on likelihood ratios is used to determine how acoustically\nsimilar each training utterance is to a target test set. The approach is\nevaluated on a wide-domain data set, covering speech from radio and TV\nbroadcasts, telephone conversations, meetings, lectures and read speech.\nExperiments demonstrate that the proposed technique both finds relevant data\nand limits negative transfer. Results on a 6--hour test set show a relative\nimprovement of 4% with data selection over using all data in PLP based models,\nand 2% with DNN features.\n", "versions": [{"version": "v1", "created": "Tue, 8 Sep 2015 15:20:12 GMT"}], "update_date": "2015-09-18", "authors_parsed": [["Doulaty", "Mortaza", ""], ["Saz", "Oscar", ""], ["Hain", "Thomas", ""]]}, {"id": "1509.02437", "submitter": "Rishabh Soni", "authors": "Rishabh Soni, K. James Mathai", "title": "Improved Twitter Sentiment Prediction through Cluster-then-Predict Model", "comments": "5 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL cs.LG cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Over the past decade humans have experienced exponential growth in the use of\nonline resources, in particular social media and microblogging websites such as\nFacebook, Twitter, YouTube and also mobile applications such as WhatsApp, Line,\netc. Many companies have identified these resources as a rich mine of marketing\nknowledge. This knowledge provides valuable feedback which allows them to\nfurther develop the next generation of their product. In this paper, sentiment\nanalysis of a product is performed by extracting tweets about that product and\nclassifying the tweets showing it as positive and negative sentiment. The\nauthors propose a hybrid approach which combines unsupervised learning in the\nform of K-means clustering to cluster the tweets and then performing supervised\nlearning methods such as Decision Trees and Support Vector Machines for\nclassification.\n", "versions": [{"version": "v1", "created": "Tue, 8 Sep 2015 16:36:04 GMT"}], "update_date": "2015-09-09", "authors_parsed": [["Soni", "Rishabh", ""], ["Mathai", "K. James", ""]]}, {"id": "1509.02458", "submitter": "Yeounoh Chung", "authors": "Yeounoh Chung, Chang-yong Park, Noo-ri Kim, Hana Cho, Taebok Yoon,\n  Hunjoo Lee and Jee-Hyong Lee", "title": "A Behavior Analysis-Based Game Bot Detection Approach Considering\n  Various Play Styles", "comments": null, "journal-ref": "ETRI Journal 35.6 (2013): 1058-1067", "doi": "10.4218/etrij.13.2013.0049", "report-no": null, "categories": "cs.LG cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An approach for game bot detection in MMORPGs is proposed based on the\nanalysis of game playing behavior. Since MMORPGs are large scale games, users\ncan play in various ways. This variety in playing behavior makes it hard to\ndetect game bots based on play behaviors. In order to cope with this problem,\nthe proposed approach observes game playing behaviors of users and groups them\nby their behavioral similarities. Then, it develops a local bot detection model\nfor each player group. Since the locally optimized models can more accurately\ndetect game bots within each player group, the combination of those models\nbrings about overall improvement. For a practical purpose of reducing the\nworkloads of the game servers in service, the game data is collected at a low\nresolution in time. Behavioral features are selected and developed to\naccurately detect game bots with the low resolution data, considering common\naspects of MMORPG playing. Through the experiment with the real data from a\ngame currently in service, it is shown that the proposed local model approach\nyields more accurate results.\n", "versions": [{"version": "v1", "created": "Tue, 8 Sep 2015 17:36:31 GMT"}], "update_date": "2015-09-09", "authors_parsed": [["Chung", "Yeounoh", ""], ["Park", "Chang-yong", ""], ["Kim", "Noo-ri", ""], ["Cho", "Hana", ""], ["Yoon", "Taebok", ""], ["Lee", "Hunjoo", ""], ["Lee", "Jee-Hyong", ""]]}, {"id": "1509.02470", "submitter": "Jianguo Li", "authors": "Jianwei Luo and Jianguo Li and Jun Wang and Zhiguo Jiang and Yurong\n  Chen", "title": "Deep Attributes from Context-Aware Regional Neural Codes", "comments": "10 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, many researches employ middle-layer output of convolutional neural\nnetwork models (CNN) as features for different visual recognition tasks.\nAlthough promising results have been achieved in some empirical studies, such\ntype of representations still suffer from the well-known issue of semantic gap.\nThis paper proposes so-called deep attribute framework to alleviate this issue\nfrom three aspects. First, we introduce object region proposals as intermedia\nto represent target images, and extract features from region proposals. Second,\nwe study aggregating features from different CNN layers for all region\nproposals. The aggregation yields a holistic yet compact representation of\ninput images. Results show that cross-region max-pooling of soft-max layer\noutput outperform all other layers. As soft-max layer directly corresponds to\nsemantic concepts, this representation is named \"deep attributes\". Third, we\nobserve that only a small portion of generated regions by object proposals\nalgorithm are correlated to classification target. Therefore, we introduce\ncontext-aware region refining algorithm to pick out contextual regions and\nbuild context-aware classifiers.\n  We apply the proposed deep attributes framework for various vision tasks.\nExtensive experiments are conducted on standard benchmarks for three visual\nrecognition tasks, i.e., image classification, fine-grained recognition and\nvisual instance retrieval. Results show that deep attribute approaches achieve\nstate-of-the-art results, and outperforms existing peer methods with a\nsignificant margin, even though some benchmarks have little overlap of concepts\nwith the pre-trained CNN models.\n", "versions": [{"version": "v1", "created": "Tue, 8 Sep 2015 17:53:54 GMT"}], "update_date": "2015-09-09", "authors_parsed": [["Luo", "Jianwei", ""], ["Li", "Jianguo", ""], ["Wang", "Jun", ""], ["Jiang", "Zhiguo", ""], ["Chen", "Yurong", ""]]}, {"id": "1509.02487", "submitter": "Ahmad Mahmoody", "authors": "Ahmad Mahmoody, Evgenios M. Kornaropoulos, and Eli Upfal", "title": "Optimizing Static and Adaptive Probing Schedules for Rapid Event\n  Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We formulate and study a fundamental search and detection problem, Schedule\nOptimization, motivated by a variety of real-world applications, ranging from\nmonitoring content changes on the web, social networks, and user activities to\ndetecting failure on large systems with many individual machines.\n  We consider a large system consists of many nodes, where each node has its\nown rate of generating new events, or items. A monitoring application can probe\na small number of nodes at each step, and our goal is to compute a probing\nschedule that minimizes the expected number of undiscovered items at the\nsystem, or equivalently, minimizes the expected time to discover a new item in\nthe system.\n  We study the Schedule Optimization problem both for deterministic and\nrandomized memoryless algorithms. We provide lower bounds on the cost of an\noptimal schedule and construct close to optimal schedules with rigorous\nmathematical guarantees. Finally, we present an adaptive algorithm that starts\nwith no prior information on the system and converges to the optimal memoryless\nalgorithms by adapting to observed data.\n", "versions": [{"version": "v1", "created": "Tue, 8 Sep 2015 18:28:24 GMT"}, {"version": "v2", "created": "Thu, 10 Sep 2015 02:22:51 GMT"}], "update_date": "2015-09-11", "authors_parsed": [["Mahmoody", "Ahmad", ""], ["Kornaropoulos", "Evgenios M.", ""], ["Upfal", "Eli", ""]]}, {"id": "1509.02512", "submitter": "Justice Amoh", "authors": "Justice Amoh and Kofi Odame", "title": "DeepCough: A Deep Convolutional Neural Network in A Wearable Cough\n  Detection System", "comments": "BioCAS-2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  In this paper, we present a system that employs a wearable acoustic sensor\nand a deep convolutional neural network for detecting coughs. We evaluate the\nperformance of our system on 14 healthy volunteers and compare it to that of\nother cough detection systems that have been reported in the literature.\nExperimental results show that our system achieves a classification sensitivity\nof 95.1% and a specificity of 99.5%.\n", "versions": [{"version": "v1", "created": "Tue, 8 Sep 2015 19:59:19 GMT"}], "update_date": "2015-09-09", "authors_parsed": [["Amoh", "Justice", ""], ["Odame", "Kofi", ""]]}, {"id": "1509.02597", "submitter": "Tsung-Hui Chang", "authors": "Tsung-Hui Chang, Mingyi Hong, Wei-Cheng Liao and Xiangfeng Wang", "title": "Asynchronous Distributed ADMM for Large-Scale Optimization- Part I:\n  Algorithm and Convergence Analysis", "comments": "37 pages", "journal-ref": null, "doi": "10.1109/TSP.2016.2537271", "report-no": null, "categories": "cs.DC cs.LG cs.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Aiming at solving large-scale learning problems, this paper studies\ndistributed optimization methods based on the alternating direction method of\nmultipliers (ADMM). By formulating the learning problem as a consensus problem,\nthe ADMM can be used to solve the consensus problem in a fully parallel fashion\nover a computer network with a star topology. However, traditional synchronized\ncomputation does not scale well with the problem size, as the speed of the\nalgorithm is limited by the slowest workers. This is particularly true in a\nheterogeneous network where the computing nodes experience different\ncomputation and communication delays. In this paper, we propose an asynchronous\ndistributed ADMM (AD-AMM) which can effectively improve the time efficiency of\ndistributed optimization. Our main interest lies in analyzing the convergence\nconditions of the AD-ADMM, under the popular partially asynchronous model,\nwhich is defined based on a maximum tolerable delay of the network.\nSpecifically, by considering general and possibly non-convex cost functions, we\nshow that the AD-ADMM is guaranteed to converge to the set of\nKarush-Kuhn-Tucker (KKT) points as long as the algorithm parameters are chosen\nappropriately according to the network delay. We further illustrate that the\nasynchrony of the ADMM has to be handled with care, as slightly modifying the\nimplementation of the AD-ADMM can jeopardize the algorithm convergence, even\nunder a standard convex setting.\n", "versions": [{"version": "v1", "created": "Wed, 9 Sep 2015 01:45:08 GMT"}, {"version": "v2", "created": "Fri, 19 Feb 2016 06:02:38 GMT"}], "update_date": "2016-05-04", "authors_parsed": [["Chang", "Tsung-Hui", ""], ["Hong", "Mingyi", ""], ["Liao", "Wei-Cheng", ""], ["Wang", "Xiangfeng", ""]]}, {"id": "1509.02604", "submitter": "Tsung-Hui Chang", "authors": "Tsung-Hui Chang, Wei-Cheng Liao, Mingyi Hong and Xiangfeng Wang", "title": "Asynchronous Distributed ADMM for Large-Scale Optimization- Part II:\n  Linear Convergence Analysis and Numerical Performance", "comments": "submitted for publication, 28 pages", "journal-ref": null, "doi": "10.1109/TSP.2016.2537261", "report-no": null, "categories": "cs.DC cs.LG cs.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The alternating direction method of multipliers (ADMM) has been recognized as\na versatile approach for solving modern large-scale machine learning and signal\nprocessing problems efficiently. When the data size and/or the problem\ndimension is large, a distributed version of ADMM can be used, which is capable\nof distributing the computation load and the data set to a network of computing\nnodes. Unfortunately, a direct synchronous implementation of such algorithm\ndoes not scale well with the problem size, as the algorithm speed is limited by\nthe slowest computing nodes. To address this issue, in a companion paper, we\nhave proposed an asynchronous distributed ADMM (AD-ADMM) and studied its\nworst-case convergence conditions. In this paper, we further the study by\ncharacterizing the conditions under which the AD-ADMM achieves linear\nconvergence. Our conditions as well as the resulting linear rates reveal the\nimpact that various algorithm parameters, network delay and network size have\non the algorithm performance. To demonstrate the superior time efficiency of\nthe proposed AD-ADMM, we test the AD-ADMM on a high-performance computer\ncluster by solving a large-scale logistic regression problem.\n", "versions": [{"version": "v1", "created": "Wed, 9 Sep 2015 02:07:27 GMT"}], "update_date": "2016-05-04", "authors_parsed": [["Chang", "Tsung-Hui", ""], ["Liao", "Wei-Cheng", ""], ["Hong", "Mingyi", ""], ["Wang", "Xiangfeng", ""]]}, {"id": "1509.02730", "submitter": "Rangeet Mitra", "authors": "Rangeet Mitra and Vimal Bhatia", "title": "Finite Dictionary Variants of the Diffusion KLMS Algorithm", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SY cs.DC cs.IT cs.LG math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The diffusion based distributed learning approaches have been found to be a\nviable solution for learning over linearly separable datasets over a network.\nHowever, approaches till date are suitable for linearly separable datasets and\nneed to be extended to scenarios in which we need to learn a non-linearity. In\nsuch scenarios, the recently proposed diffusion kernel least mean squares\n(KLMS) has been found to be performing better than diffusion least mean squares\n(LMS). The drawback of diffusion KLMS is that it requires infinite storage for\nobservations (also called dictionary). This paper formulates the diffusion KLMS\nin a fixed budget setting such that the storage requirement is curtailed while\nmaintaining appreciable performance in terms of convergence. Simulations have\nbeen carried out to validate the two newly proposed algorithms named as\nquantised diffusion KLMS (QDKLMS) and fixed budget diffusion KLMS (FBDKLMS)\nagainst KLMS, which indicate that both the proposed algorithms deliver better\nperformance as compared to the KLMS while reducing the dictionary size storage\nrequirement.\n", "versions": [{"version": "v1", "created": "Wed, 9 Sep 2015 11:38:01 GMT"}], "update_date": "2015-09-15", "authors_parsed": [["Mitra", "Rangeet", ""], ["Bhatia", "Vimal", ""]]}, {"id": "1509.02805", "submitter": "Teng Qiu", "authors": "Teng Qiu, Yongjie Li", "title": "Clustering by Hierarchical Nearest Neighbor Descent (H-NND)", "comments": "19 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CV cs.LG stat.ME", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Previously in 2014, we proposed the Nearest Descent (ND) method, capable of\ngenerating an efficient Graph, called the in-tree (IT). Due to some beautiful\nand effective features, this IT structure proves well suited for data\nclustering. Although there exist some redundant edges in IT, they usually have\nsalient features and thus it is not hard to remove them.\n  Subsequently, in order to prevent the seemingly redundant edges from\noccurring, we proposed the Nearest Neighbor Descent (NND) by adding the\n\"Neighborhood\" constraint on ND. Consequently, clusters automatically emerged,\nwithout the additional requirement of removing the redundant edges. However,\nNND proved still not perfect, since it brought in a new yet worse problem, the\n\"over-partitioning\" problem.\n  Now, in this paper, we propose a method, called the Hierarchical Nearest\nNeighbor Descent (H-NND), which overcomes the over-partitioning problem of NND\nvia using the hierarchical strategy. Specifically, H-NND uses ND to effectively\nmerge the over-segmented sub-graphs or clusters that NND produces. Like ND,\nH-NND also generates the IT structure, in which the redundant edges once again\nappear. This seemingly comes back to the situation that ND faces. However,\ncompared with ND, the redundant edges in the IT structure generated by H-NND\ngenerally become more salient, thus being much easier and more reliable to be\nidentified even by the simplest edge-removing method which takes the edge\nlength as the only measure. In other words, the IT structure constructed by\nH-NND becomes more fitted for data clustering. We prove this on several\nclustering datasets of varying shapes, dimensions and attributes. Besides,\ncompared with ND, H-NND generally takes less computation time to construct the\nIT data structure for the input data.\n", "versions": [{"version": "v1", "created": "Wed, 9 Sep 2015 15:15:44 GMT"}, {"version": "v2", "created": "Mon, 14 Sep 2015 15:43:25 GMT"}, {"version": "v3", "created": "Fri, 4 Mar 2016 15:50:58 GMT"}], "update_date": "2016-03-07", "authors_parsed": [["Qiu", "Teng", ""], ["Li", "Yongjie", ""]]}, {"id": "1509.02900", "submitter": "Michael Hoffman", "authors": "Beate Franke and Jean-Fran\\c{c}ois Plante and Ribana Roscher and Annie\n  Lee and Cathal Smyth and Armin Hatefi and Fuqi Chen and Einat Gil and\n  Alexander Schwing and Alessandro Selvitella and Michael M. Hoffman and Roger\n  Grosse and Dieter Hendricks and Nancy Reid", "title": "Statistical Inference, Learning and Models in Big Data", "comments": "Thematic Program on Statistical Inference, Learning, and Models for\n  Big Data, Fields Institute; 23 pages, 2 figures", "journal-ref": "Int Stat Rev 84 (2017) 371-389", "doi": "10.1111/insr.12176", "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The need for new methods to deal with big data is a common theme in most\nscientific fields, although its definition tends to vary with the context.\nStatistical ideas are an essential part of this, and as a partial response, a\nthematic program on statistical inference, learning, and models in big data was\nheld in 2015 in Canada, under the general direction of the Canadian Statistical\nSciences Institute, with major funding from, and most activities located at,\nthe Fields Institute for Research in Mathematical Sciences. This paper gives an\noverview of the topics covered, describing challenges and strategies that seem\ncommon to many different areas of application, and including some examples of\napplications to make these challenges and strategies more concrete.\n", "versions": [{"version": "v1", "created": "Wed, 9 Sep 2015 19:33:31 GMT"}, {"version": "v2", "created": "Thu, 28 Jan 2016 20:26:03 GMT"}], "update_date": "2018-09-25", "authors_parsed": [["Franke", "Beate", ""], ["Plante", "Jean-Fran\u00e7ois", ""], ["Roscher", "Ribana", ""], ["Lee", "Annie", ""], ["Smyth", "Cathal", ""], ["Hatefi", "Armin", ""], ["Chen", "Fuqi", ""], ["Gil", "Einat", ""], ["Schwing", "Alexander", ""], ["Selvitella", "Alessandro", ""], ["Hoffman", "Michael M.", ""], ["Grosse", "Roger", ""], ["Hendricks", "Dieter", ""], ["Reid", "Nancy", ""]]}, {"id": "1509.02954", "submitter": "Joseph Wang", "authors": "Joseph Wang, Kirill Trapeznikov, Venkatesh Saligrama", "title": "Sensor Selection by Linear Programming", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We learn sensor trees from training data to minimize sensor acquisition costs\nduring test time. Our system adaptively selects sensors at each stage if\nnecessary to make a confident classification. We pose the problem as empirical\nrisk minimization over the choice of trees and node decision rules. We\ndecompose the problem, which is known to be intractable, into combinatorial\n(tree structures) and continuous parts (node decision rules) and propose to\nsolve them separately. Using training data we greedily solve for the\ncombinatorial tree structures and for the continuous part, which is a\nnon-convex multilinear objective function, we derive convex surrogate loss\nfunctions that are piecewise linear. The resulting problem can be cast as a\nlinear program and has the advantage of guaranteed convergence, global\noptimality, repeatability and computational efficiency. We show that our\nproposed approach outperforms the state-of-art on a number of benchmark\ndatasets.\n", "versions": [{"version": "v1", "created": "Wed, 9 Sep 2015 21:15:32 GMT"}], "update_date": "2015-09-11", "authors_parsed": [["Wang", "Joseph", ""], ["Trapeznikov", "Kirill", ""], ["Saligrama", "Venkatesh", ""]]}, {"id": "1509.02971", "submitter": "Jonathan Hunt", "authors": "Timothy P. Lillicrap, Jonathan J. Hunt, Alexander Pritzel, Nicolas\n  Heess, Tom Erez, Yuval Tassa, David Silver, Daan Wierstra", "title": "Continuous control with deep reinforcement learning", "comments": "10 pages + supplementary", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We adapt the ideas underlying the success of Deep Q-Learning to the\ncontinuous action domain. We present an actor-critic, model-free algorithm\nbased on the deterministic policy gradient that can operate over continuous\naction spaces. Using the same learning algorithm, network architecture and\nhyper-parameters, our algorithm robustly solves more than 20 simulated physics\ntasks, including classic problems such as cartpole swing-up, dexterous\nmanipulation, legged locomotion and car driving. Our algorithm is able to find\npolicies whose performance is competitive with those found by a planning\nalgorithm with full access to the dynamics of the domain and its derivatives.\nWe further demonstrate that for many of the tasks the algorithm can learn\npolicies end-to-end: directly from raw pixel inputs.\n", "versions": [{"version": "v1", "created": "Wed, 9 Sep 2015 23:01:36 GMT"}, {"version": "v2", "created": "Wed, 18 Nov 2015 17:34:41 GMT"}, {"version": "v3", "created": "Thu, 7 Jan 2016 19:09:07 GMT"}, {"version": "v4", "created": "Tue, 19 Jan 2016 20:30:47 GMT"}, {"version": "v5", "created": "Mon, 29 Feb 2016 18:45:53 GMT"}, {"version": "v6", "created": "Fri, 5 Jul 2019 10:47:27 GMT"}], "update_date": "2019-07-08", "authors_parsed": [["Lillicrap", "Timothy P.", ""], ["Hunt", "Jonathan J.", ""], ["Pritzel", "Alexander", ""], ["Heess", "Nicolas", ""], ["Erez", "Tom", ""], ["Tassa", "Yuval", ""], ["Silver", "David", ""], ["Wierstra", "Daan", ""]]}, {"id": "1509.03005", "submitter": "David Balduzzi", "authors": "David Balduzzi, Muhammad Ghifary", "title": "Compatible Value Gradients for Reinforcement Learning of Continuous Deep\n  Policies", "comments": "27 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes GProp, a deep reinforcement learning algorithm for\ncontinuous policies with compatible function approximation. The algorithm is\nbased on two innovations. Firstly, we present a temporal-difference based\nmethod for learning the gradient of the value-function. Secondly, we present\nthe deviator-actor-critic (DAC) model, which comprises three neural networks\nthat estimate the value function, its gradient, and determine the actor's\npolicy respectively. We evaluate GProp on two challenging tasks: a contextual\nbandit problem constructed from nonparametric regression datasets that is\ndesigned to probe the ability of reinforcement learning algorithms to\naccurately estimate gradients; and the octopus arm, a challenging reinforcement\nlearning benchmark. GProp is competitive with fully supervised methods on the\nbandit task and achieves the best performance to date on the octopus arm.\n", "versions": [{"version": "v1", "created": "Thu, 10 Sep 2015 04:14:54 GMT"}], "update_date": "2015-09-11", "authors_parsed": [["Balduzzi", "David", ""], ["Ghifary", "Muhammad", ""]]}, {"id": "1509.03025", "submitter": "Yudong Chen", "authors": "Yudong Chen, Martin J. Wainwright", "title": "Fast low-rank estimation by projected gradient descent: General\n  statistical and algorithmic guarantees", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.LG stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Optimization problems with rank constraints arise in many applications,\nincluding matrix regression, structured PCA, matrix completion and matrix\ndecomposition problems. An attractive heuristic for solving such problems is to\nfactorize the low-rank matrix, and to run projected gradient descent on the\nnonconvex factorized optimization problem. The goal of this problem is to\nprovide a general theoretical framework for understanding when such methods\nwork well, and to characterize the nature of the resulting fixed point. We\nprovide a simple set of conditions under which projected gradient descent, when\ngiven a suitable initialization, converges geometrically to a statistically\nuseful solution. Our results are applicable even when the initial solution is\noutside any region of local convexity, and even when the problem is globally\nconcave. Working in a non-asymptotic framework, we show that our conditions are\nsatisfied for a wide range of concrete models, including matrix regression,\nstructured PCA, matrix completion with real and quantized observations, matrix\ndecomposition, and graph clustering problems. Simulation results show excellent\nagreement with the theoretical predictions.\n", "versions": [{"version": "v1", "created": "Thu, 10 Sep 2015 06:07:52 GMT"}], "update_date": "2015-09-11", "authors_parsed": [["Chen", "Yudong", ""], ["Wainwright", "Martin J.", ""]]}, {"id": "1509.03044", "submitter": "Xiujun Li", "authors": "Xiujun Li, Lihong Li, Jianfeng Gao, Xiaodong He, Jianshu Chen, Li\n  Deng, Ji He", "title": "Recurrent Reinforcement Learning: A Hybrid Approach", "comments": "11 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Successful applications of reinforcement learning in real-world problems\noften require dealing with partially observable states. It is in general very\nchallenging to construct and infer hidden states as they often depend on the\nagent's entire interaction history and may require substantial domain\nknowledge. In this work, we investigate a deep-learning approach to learning\nthe representation of states in partially observable tasks, with minimal prior\nknowledge of the domain. In particular, we propose a new family of hybrid\nmodels that combines the strength of both supervised learning (SL) and\nreinforcement learning (RL), trained in a joint fashion: The SL component can\nbe a recurrent neural networks (RNN) or its long short-term memory (LSTM)\nversion, which is equipped with the desired property of being able to capture\nlong-term dependency on history, thus providing an effective way of learning\nthe representation of hidden states. The RL component is a deep Q-network (DQN)\nthat learns to optimize the control for maximizing long-term rewards. Extensive\nexperiments in a direct mailing campaign problem demonstrate the effectiveness\nand advantages of the proposed approach, which performs the best among a set of\nprevious state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Thu, 10 Sep 2015 07:45:30 GMT"}, {"version": "v2", "created": "Thu, 19 Nov 2015 19:32:08 GMT"}], "update_date": "2015-11-20", "authors_parsed": [["Li", "Xiujun", ""], ["Li", "Lihong", ""], ["Gao", "Jianfeng", ""], ["He", "Xiaodong", ""], ["Chen", "Jianshu", ""], ["Deng", "Li", ""], ["He", "Ji", ""]]}, {"id": "1509.03185", "submitter": "Andrew Simpson", "authors": "Andrew J.R. Simpson", "title": "Use it or Lose it: Selective Memory and Forgetting in a Perpetual\n  Learning Machine", "comments": "arXiv admin note: substantial text overlap with arXiv:1509.00913", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In a recent article we described a new type of deep neural network - a\nPerpetual Learning Machine (PLM) - which is capable of learning 'on the fly'\nlike a brain by existing in a state of Perpetual Stochastic Gradient Descent\n(PSGD). Here, by simulating the process of practice, we demonstrate both\nselective memory and selective forgetting when we introduce statistical recall\nbiases during PSGD. Frequently recalled memories are remembered, whilst\nmemories recalled rarely are forgotten. This results in a 'use it or lose it'\nstimulus driven memory process that is similar to human memory.\n", "versions": [{"version": "v1", "created": "Thu, 10 Sep 2015 15:12:00 GMT"}], "update_date": "2015-09-30", "authors_parsed": [["Simpson", "Andrew J. R.", ""]]}, {"id": "1509.03200", "submitter": "Abhishek Kumar Mr", "authors": "Abhishek Kumar and Suresh Chandra Gupta", "title": "A new Initial Centroid finding Method based on Dissimilarity Tree for\n  K-means Algorithm", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cluster analysis is one of the primary data analysis technique in data mining\nand K-means is one of the commonly used partitioning clustering algorithm. In\nK-means algorithm, resulting set of clusters depend on the choice of initial\ncentroids. If we can find initial centroids which are coherent with the\narrangement of data, the better set of clusters can be obtained. This paper\nproposes a method based on the Dissimilarity Tree to find, the better initial\ncentroid as well as every bit more accurate cluster with less computational\ntime. Theory analysis and experimental results indicate that the proposed\nmethod can effectively improve the accuracy of clusters and reduce the\ncomputational complexity of the K-means algorithm.\n", "versions": [{"version": "v1", "created": "Fri, 19 Jun 2015 11:43:14 GMT"}], "update_date": "2015-09-11", "authors_parsed": [["Kumar", "Abhishek", ""], ["Gupta", "Suresh Chandra", ""]]}, {"id": "1509.03242", "submitter": "Yogesh Girdhar Yogesh Girdhar", "authors": "Yogesh Girdhar and Gregory Dudek", "title": "Gibbs Sampling Strategies for Semantic Perception of Streaming Video\n  Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Topic modeling of streaming sensor data can be used for high level perception\nof the environment by a mobile robot. In this paper we compare various Gibbs\nsampling strategies for topic modeling of streaming spatiotemporal data, such\nas video captured by a mobile robot. Compared to previous work on online topic\nmodeling, such as o-LDA and incremental LDA, we show that the proposed\ntechnique results in lower online and final perplexity, given the realtime\nconstraints.\n", "versions": [{"version": "v1", "created": "Thu, 10 Sep 2015 17:25:50 GMT"}], "update_date": "2015-09-11", "authors_parsed": [["Girdhar", "Yogesh", ""], ["Dudek", "Gregory", ""]]}, {"id": "1509.03248", "submitter": "George Trigeorgis", "authors": "George Trigeorgis, Konstantinos Bousmalis, Stefanos Zafeiriou, Bjoern\n  W.Schuller", "title": "A deep matrix factorization method for learning attribute\n  representations", "comments": "Submitted to TPAMI (16-Mar-2015)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Semi-Non-negative Matrix Factorization is a technique that learns a\nlow-dimensional representation of a dataset that lends itself to a clustering\ninterpretation. It is possible that the mapping between this new representation\nand our original data matrix contains rather complex hierarchical information\nwith implicit lower-level hidden attributes, that classical one level\nclustering methodologies can not interpret. In this work we propose a novel\nmodel, Deep Semi-NMF, that is able to learn such hidden representations that\nallow themselves to an interpretation of clustering according to different,\nunknown attributes of a given dataset. We also present a semi-supervised\nversion of the algorithm, named Deep WSF, that allows the use of (partial)\nprior information for each of the known attributes of a dataset, that allows\nthe model to be used on datasets with mixed attribute knowledge. Finally, we\nshow that our models are able to learn low-dimensional representations that are\nbetter suited for clustering, but also classification, outperforming\nSemi-Non-negative Matrix Factorization, but also other state-of-the-art\nmethodologies variants.\n", "versions": [{"version": "v1", "created": "Thu, 10 Sep 2015 17:57:03 GMT"}], "update_date": "2015-09-11", "authors_parsed": [["Trigeorgis", "George", ""], ["Bousmalis", "Konstantinos", ""], ["Zafeiriou", "Stefanos", ""], ["Schuller", "Bjoern W.", ""]]}, {"id": "1509.03302", "submitter": "Matt Barnes", "authors": "Matt Barnes, Kyle Miller, Artur Dubrawski", "title": "Performance Bounds for Pairwise Entity Resolution", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CY cs.DB cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One significant challenge to scaling entity resolution algorithms to massive\ndatasets is understanding how performance changes after moving beyond the realm\nof small, manually labeled reference datasets. Unlike traditional machine\nlearning tasks, when an entity resolution algorithm performs well on small\nhold-out datasets, there is no guarantee this performance holds on larger\nhold-out datasets. We prove simple bounding properties between the performance\nof a match function on a small validation set and the performance of a pairwise\nentity resolution algorithm on arbitrarily sized datasets. Thus, our approach\nenables optimization of pairwise entity resolution algorithms for large\ndatasets, using a small set of labeled data.\n", "versions": [{"version": "v1", "created": "Thu, 10 Sep 2015 19:58:44 GMT"}], "update_date": "2015-09-15", "authors_parsed": [["Barnes", "Matt", ""], ["Miller", "Kyle", ""], ["Dubrawski", "Artur", ""]]}, {"id": "1509.03475", "submitter": "Minhyung Cho", "authors": "Minhyung Cho, Chandra Shekhar Dhir, Jaehyung Lee", "title": "Hessian-free Optimization for Learning Deep Multidimensional Recurrent\n  Neural Networks", "comments": "to appear at NIPS 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multidimensional recurrent neural networks (MDRNNs) have shown a remarkable\nperformance in the area of speech and handwriting recognition. The performance\nof an MDRNN is improved by further increasing its depth, and the difficulty of\nlearning the deeper network is overcome by using Hessian-free (HF)\noptimization. Given that connectionist temporal classification (CTC) is\nutilized as an objective of learning an MDRNN for sequence labeling, the\nnon-convexity of CTC poses a problem when applying HF to the network. As a\nsolution, a convex approximation of CTC is formulated and its relationship with\nthe EM algorithm and the Fisher information matrix is discussed. An MDRNN up to\na depth of 15 layers is successfully trained using HF, resulting in an improved\nperformance for sequence labeling.\n", "versions": [{"version": "v1", "created": "Fri, 11 Sep 2015 12:28:36 GMT"}, {"version": "v2", "created": "Fri, 23 Oct 2015 07:14:04 GMT"}], "update_date": "2015-10-26", "authors_parsed": [["Cho", "Minhyung", ""], ["Dhir", "Chandra Shekhar", ""], ["Lee", "Jaehyung", ""]]}, {"id": "1509.03600", "submitter": "Satyen Kale", "authors": "Satyen Kale and Chansoo Lee and D\\'avid P\\'al", "title": "Hardness of Online Sleeping Combinatorial Optimization Problems", "comments": "A version of this paper was published in NIPS 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We show that several online combinatorial optimization problems that admit\nefficient no-regret algorithms become computationally hard in the sleeping\nsetting where a subset of actions becomes unavailable in each round.\nSpecifically, we show that the sleeping versions of these problems are at least\nas hard as PAC learning DNF expressions, a long standing open problem. We show\nhardness for the sleeping versions of Online Shortest Paths, Online Minimum\nSpanning Tree, Online $k$-Subsets, Online $k$-Truncated Permutations, Online\nMinimum Cut, and Online Bipartite Matching. The hardness result for the\nsleeping version of the Online Shortest Paths problem resolves an open problem\npresented at COLT 2015 (Koolen et al., 2015).\n", "versions": [{"version": "v1", "created": "Fri, 11 Sep 2015 18:27:42 GMT"}, {"version": "v2", "created": "Tue, 22 Sep 2015 18:12:37 GMT"}, {"version": "v3", "created": "Mon, 19 Dec 2016 22:28:15 GMT"}], "update_date": "2016-12-21", "authors_parsed": [["Kale", "Satyen", ""], ["Lee", "Chansoo", ""], ["P\u00e1l", "D\u00e1vid", ""]]}, {"id": "1509.03755", "submitter": "Gabriel Prat", "authors": "Gabriel Prat Masramon and Llu\\'is A. Belanche Mu\\~noz", "title": "Toward better feature weighting algorithms: a focus on Relief", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Feature weighting algorithms try to solve a problem of great importance\nnowadays in machine learning: The search of a relevance measure for the\nfeatures of a given domain. This relevance is primarily used for feature\nselection as feature weighting can be seen as a generalization of it, but it is\nalso useful to better understand a problem's domain or to guide an inductor in\nits learning process. Relief family of algorithms are proven to be very\neffective in this task. Some other feature weighting methods are reviewed in\norder to give some context and then the different existing extensions to the\noriginal algorithm are explained.\n  One of Relief's known issues is the performance degradation of its estimates\nwhen redundant features are present. A novel theoretical definition of\nredundancy level is given in order to guide the work towards an extension of\nthe algorithm that is more robust against redundancy. A new extension is\npresented that aims for improving the algorithms performance. Some experiments\nwere driven to test this new extension against the existing ones with a set of\nartificial and real datasets and denoted that in certain cases it improves the\nweight's estimation accuracy.\n", "versions": [{"version": "v1", "created": "Sat, 12 Sep 2015 15:10:15 GMT"}, {"version": "v2", "created": "Wed, 16 Sep 2015 11:58:32 GMT"}], "update_date": "2015-09-17", "authors_parsed": [["Masramon", "Gabriel Prat", ""], ["Mu\u00f1oz", "Llu\u00eds A. Belanche", ""]]}, {"id": "1509.03917", "submitter": "Anastasios Kyrillidis", "authors": "Srinadh Bhojanapalli, Anastasios Kyrillidis, Sujay Sanghavi", "title": "Dropping Convexity for Faster Semi-definite Optimization", "comments": "40 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.DS cs.IT cs.LG cs.NA math.IT math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the minimization of a convex function $f(X)$ over the set of\n$n\\times n$ positive semi-definite matrices, but when the problem is recast as\n$\\min_U g(U) := f(UU^\\top)$, with $U \\in \\mathbb{R}^{n \\times r}$ and $r \\leq\nn$. We study the performance of gradient descent on $g$---which we refer to as\nFactored Gradient Descent (FGD)---under standard assumptions on the original\nfunction $f$.\n  We provide a rule for selecting the step size and, with this choice, show\nthat the local convergence rate of FGD mirrors that of standard gradient\ndescent on the original $f$: i.e., after $k$ steps, the error is $O(1/k)$ for\nsmooth $f$, and exponentially small in $k$ when $f$ is (restricted) strongly\nconvex. In addition, we provide a procedure to initialize FGD for (restricted)\nstrongly convex objectives and when one only has access to $f$ via a\nfirst-order oracle; for several problem instances, such proper initialization\nleads to global convergence guarantees.\n  FGD and similar procedures are widely used in practice for problems that can\nbe posed as matrix factorization. To the best of our knowledge, this is the\nfirst paper to provide precise convergence rate guarantees for general convex\nfunctions under standard convex assumptions.\n", "versions": [{"version": "v1", "created": "Mon, 14 Sep 2015 00:40:11 GMT"}, {"version": "v2", "created": "Mon, 16 Nov 2015 01:45:02 GMT"}, {"version": "v3", "created": "Sat, 16 Apr 2016 03:17:46 GMT"}], "update_date": "2016-04-19", "authors_parsed": [["Bhojanapalli", "Srinadh", ""], ["Kyrillidis", "Anastasios", ""], ["Sanghavi", "Sujay", ""]]}, {"id": "1509.03946", "submitter": "Yoshinobu Kawahara", "authors": "Yoshinobu Kawahara and Yutaro Yamaguchi", "title": "Parametric Maxflows for Structured Sparse Learning with Convex\n  Relaxations of Submodular Functions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The proximal problem for structured penalties obtained via convex relaxations\nof submodular functions is known to be equivalent to minimizing separable\nconvex functions over the corresponding submodular polyhedra. In this paper, we\nreveal a comprehensive class of structured penalties for which penalties this\nproblem can be solved via an efficiently solvable class of parametric maxflow\noptimization. We then show that the parametric maxflow algorithm proposed by\nGallo et al. and its variants, which runs, in the worst-case, at the cost of\nonly a constant factor of a single computation of the corresponding maxflow\noptimization, can be adapted to solve the proximal problems for those\npenalties. Several existing structured penalties satisfy these conditions;\nthus, regularized learning with these penalties is solvable quickly using the\nparametric maxflow algorithm. We also investigate the empirical runtime\nperformance of the proposed framework.\n", "versions": [{"version": "v1", "created": "Mon, 14 Sep 2015 04:11:02 GMT"}], "update_date": "2015-09-15", "authors_parsed": [["Kawahara", "Yoshinobu", ""], ["Yamaguchi", "Yutaro", ""]]}, {"id": "1509.03977", "submitter": "Pablo Escandell-Montero", "authors": "Pablo Escandell-Montero, Milena Chermisi, Jos\\'e M.\n  Mart\\'inez-Mart\\'inez, Juan G\\'omez-Sanchis, Carlo Barbieri, Emilio\n  Soria-Olivas, Flavio Mari, Joan Vila-Franc\\'es, Andrea Stopper, Emanuele\n  Gatti and Jos\\'e D. Mart\\'in-Guerrero", "title": "Optimization of anemia treatment in hemodialysis patients via\n  reinforcement learning", "comments": "17 pages, 10 figures", "journal-ref": "Artificial Intelligence in Medicine, Volume 62, Issue 1, September\n  2014, Pages 47-60", "doi": "10.1016/j.artmed.2014.07.004", "report-no": null, "categories": "stat.ML cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Objective: Anemia is a frequent comorbidity in hemodialysis patients that can\nbe successfully treated by administering erythropoiesis-stimulating agents\n(ESAs). ESAs dosing is currently based on clinical protocols that often do not\naccount for the high inter- and intra-individual variability in the patient's\nresponse. As a result, the hemoglobin level of some patients oscillates around\nthe target range, which is associated with multiple risks and side-effects.\nThis work proposes a methodology based on reinforcement learning (RL) to\noptimize ESA therapy.\n  Methods: RL is a data-driven approach for solving sequential decision-making\nproblems that are formulated as Markov decision processes (MDPs). Computing\noptimal drug administration strategies for chronic diseases is a sequential\ndecision-making problem in which the goal is to find the best sequence of drug\ndoses. MDPs are particularly suitable for modeling these problems due to their\nability to capture the uncertainty associated with the outcome of the treatment\nand the stochastic nature of the underlying process. The RL algorithm employed\nin the proposed methodology is fitted Q iteration, which stands out for its\nability to make an efficient use of data.\n  Results: The experiments reported here are based on a computational model\nthat describes the effect of ESAs on the hemoglobin level. The performance of\nthe proposed method is evaluated and compared with the well-known Q-learning\nalgorithm and with a standard protocol. Simulation results show that the\nperformance of Q-learning is substantially lower than FQI and the protocol.\n  Conclusion: Although prospective validation is required, promising results\ndemonstrate the potential of RL to become an alternative to current protocols.\n", "versions": [{"version": "v1", "created": "Mon, 14 Sep 2015 07:52:00 GMT"}], "update_date": "2015-09-15", "authors_parsed": [["Escandell-Montero", "Pablo", ""], ["Chermisi", "Milena", ""], ["Mart\u00ednez-Mart\u00ednez", "Jos\u00e9 M.", ""], ["G\u00f3mez-Sanchis", "Juan", ""], ["Barbieri", "Carlo", ""], ["Soria-Olivas", "Emilio", ""], ["Mari", "Flavio", ""], ["Vila-Franc\u00e9s", "Joan", ""], ["Stopper", "Andrea", ""], ["Gatti", "Emanuele", ""], ["Mart\u00edn-Guerrero", "Jos\u00e9 D.", ""]]}, {"id": "1509.04098", "submitter": "Stefano Cresci", "authors": "Stefano Cresci, Roberto Di Pietro, Marinella Petrocchi, Angelo\n  Spognardi, Maurizio Tesconi", "title": "Fame for sale: efficient detection of fake Twitter followers", "comments": null, "journal-ref": "Decision Support Systems, 80, 56-71, 2015", "doi": "10.1016/j.dss.2015.09.003", "report-no": null, "categories": "cs.SI cs.CR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  $\\textit{Fake followers}$ are those Twitter accounts specifically created to\ninflate the number of followers of a target account. Fake followers are\ndangerous for the social platform and beyond, since they may alter concepts\nlike popularity and influence in the Twittersphere - hence impacting on\neconomy, politics, and society. In this paper, we contribute along different\ndimensions. First, we review some of the most relevant existing features and\nrules (proposed by Academia and Media) for anomalous Twitter accounts\ndetection. Second, we create a baseline dataset of verified human and fake\nfollower accounts. Such baseline dataset is publicly available to the\nscientific community. Then, we exploit the baseline dataset to train a set of\nmachine-learning classifiers built over the reviewed rules and features. Our\nresults show that most of the rules proposed by Media provide unsatisfactory\nperformance in revealing fake followers, while features proposed in the past by\nAcademia for spam detection provide good results. Building on the most\npromising features, we revise the classifiers both in terms of reduction of\noverfitting and cost for gathering the data needed to compute the features. The\nfinal result is a novel $\\textit{Class A}$ classifier, general enough to thwart\noverfitting, lightweight thanks to the usage of the less costly features, and\nstill able to correctly classify more than 95% of the accounts of the original\ntraining set. We ultimately perform an information fusion-based sensitivity\nanalysis, to assess the global sensitivity of each of the features employed by\nthe classifier. The findings reported in this paper, other than being supported\nby a thorough experimental methodology and interesting on their own, also pave\nthe way for further investigation on the novel issue of fake Twitter followers.\n", "versions": [{"version": "v1", "created": "Mon, 14 Sep 2015 13:59:11 GMT"}, {"version": "v2", "created": "Tue, 10 Nov 2015 17:31:40 GMT"}], "update_date": "2020-06-29", "authors_parsed": [["Cresci", "Stefano", ""], ["Di Pietro", "Roberto", ""], ["Petrocchi", "Marinella", ""], ["Spognardi", "Angelo", ""], ["Tesconi", "Maurizio", ""]]}, {"id": "1509.04210", "submitter": "Wei Zhang", "authors": "Suyog Gupta, Wei Zhang, Fei Wang", "title": "Model Accuracy and Runtime Tradeoff in Distributed Deep Learning:A\n  Systematic Study", "comments": "Accepted by The IEEE International Conference on Data Mining 2016\n  (ICDM 2016)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.DC cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents Rudra, a parameter server based distributed computing\nframework tuned for training large-scale deep neural networks. Using variants\nof the asynchronous stochastic gradient descent algorithm we study the impact\nof synchronization protocol, stale gradient updates, minibatch size, learning\nrates, and number of learners on runtime performance and model accuracy. We\nintroduce a new learning rate modulation strategy to counter the effect of\nstale gradients and propose a new synchronization protocol that can effectively\nbound the staleness in gradients, improve runtime performance and achieve good\nmodel accuracy. Our empirical investigation reveals a principled approach for\ndistributed training of neural networks: the mini-batch size per learner should\nbe reduced as more learners are added to the system to preserve the model\naccuracy. We validate this approach using commonly-used image classification\nbenchmarks: CIFAR10 and ImageNet.\n", "versions": [{"version": "v1", "created": "Mon, 14 Sep 2015 17:14:52 GMT"}, {"version": "v2", "created": "Tue, 15 Sep 2015 01:31:18 GMT"}, {"version": "v3", "created": "Mon, 5 Dec 2016 21:26:38 GMT"}], "update_date": "2016-12-07", "authors_parsed": [["Gupta", "Suyog", ""], ["Zhang", "Wei", ""], ["Wang", "Fei", ""]]}, {"id": "1509.04265", "submitter": "Gabriel Prat", "authors": "Gabriel Prat Masramon and Llu\\'is A. Belanche Mu\\~noz", "title": "Double Relief with progressive weighting function", "comments": "arXiv admin note: substantial text overlap with arXiv:1509.03755", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Feature weighting algorithms try to solve a problem of great importance\nnowadays in machine learning: The search of a relevance measure for the\nfeatures of a given domain. This relevance is primarily used for feature\nselection as feature weighting can be seen as a generalization of it, but it is\nalso useful to better understand a problem's domain or to guide an inductor in\nits learning process. Relief family of algorithms are proven to be very\neffective in this task.\n  On previous work, a new extension was proposed that aimed for improving the\nalgorithm's performance and it was shown that in certain cases it improved the\nweights' estimation accuracy. However, it also seemed to be sensible to some\ncharacteristics of the data. An improvement of that previously presented\nextension is presented in this work that aims to make it more robust to problem\nspecific characteristics. An experimental design is proposed to test its\nperformance. Results of the tests prove that it indeed increase the robustness\nof the previously proposed extension.\n", "versions": [{"version": "v1", "created": "Sat, 12 Sep 2015 15:28:08 GMT"}, {"version": "v2", "created": "Wed, 16 Sep 2015 12:09:28 GMT"}], "update_date": "2015-09-17", "authors_parsed": [["Masramon", "Gabriel Prat", ""], ["Mu\u00f1oz", "Llu\u00eds A. Belanche", ""]]}, {"id": "1509.04340", "submitter": "Vitaly Kuznetsov", "authors": "Corinna Cortes, Prasoon Goyal, Vitaly Kuznetsov, Mehryar Mohri", "title": "Voted Kernel Regularization", "comments": "16 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents an algorithm, Voted Kernel Regularization , that provides\nthe flexibility of using potentially very complex kernel functions such as\npredictors based on much higher-degree polynomial kernels, while benefitting\nfrom strong learning guarantees. The success of our algorithm arises from\nderived bounds that suggest a new regularization penalty in terms of the\nRademacher complexities of the corresponding families of kernel maps. In a\nseries of experiments we demonstrate the improved performance of our algorithm\nas compared to baselines. Furthermore, the algorithm enjoys several favorable\nproperties. The optimization problem is convex, it allows for learning with\nnon-PDS kernels, and the solutions are highly sparse, resulting in improved\nclassification speed and memory requirements.\n", "versions": [{"version": "v1", "created": "Mon, 14 Sep 2015 21:58:43 GMT"}], "update_date": "2015-09-16", "authors_parsed": [["Cortes", "Corinna", ""], ["Goyal", "Prasoon", ""], ["Kuznetsov", "Vitaly", ""], ["Mohri", "Mehryar", ""]]}, {"id": "1509.04355", "submitter": "Qi Qian", "authors": "Qi Qian, Rong Jin, Lijun Zhang and Shenghuo Zhu", "title": "Towards Making High Dimensional Distance Metric Learning Practical", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we study distance metric learning (DML) for high dimensional\ndata. A typical approach for DML with high dimensional data is to perform the\ndimensionality reduction first before learning the distance metric. The main\nshortcoming of this approach is that it may result in a suboptimal solution due\nto the subspace removed by the dimensionality reduction method. In this work,\nwe present a dual random projection frame for DML with high dimensional data\nthat explicitly addresses the limitation of dimensionality reduction for DML.\nThe key idea is to first project all the data points into a low dimensional\nspace by random projection, and compute the dual variables using the projected\nvectors. It then reconstructs the distance metric in the original space using\nthe estimated dual variables. The proposed method, on one hand, enjoys the\nlight computation of random projection, and on the other hand, alleviates the\nlimitation of most dimensionality reduction methods. We verify both empirically\nand theoretically the effectiveness of the proposed algorithm for high\ndimensional DML.\n", "versions": [{"version": "v1", "created": "Tue, 15 Sep 2015 00:04:24 GMT"}], "update_date": "2015-09-16", "authors_parsed": [["Qian", "Qi", ""], ["Jin", "Rong", ""], ["Zhang", "Lijun", ""], ["Zhu", "Shenghuo", ""]]}, {"id": "1509.04376", "submitter": "Weiyu Xu", "authors": "Bingwen Zhang, Weiyu Xu, Jian-Feng Cai and Lifeng Lai", "title": "Precise Phase Transition of Total Variation Minimization", "comments": "6 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.LG math.IT math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Characterizing the phase transitions of convex optimizations in recovering\nstructured signals or data is of central importance in compressed sensing,\nmachine learning and statistics. The phase transitions of many convex\noptimization signal recovery methods such as $\\ell_1$ minimization and nuclear\nnorm minimization are well understood through recent years' research. However,\nrigorously characterizing the phase transition of total variation (TV)\nminimization in recovering sparse-gradient signal is still open. In this paper,\nwe fully characterize the phase transition curve of the TV minimization. Our\nproof builds on Donoho, Johnstone and Montanari's conjectured phase transition\ncurve for the TV approximate message passing algorithm (AMP), together with the\nlinkage between the minmax Mean Square Error of a denoising problem and the\nhigh-dimensional convex geometry for TV minimization.\n", "versions": [{"version": "v1", "created": "Tue, 15 Sep 2015 02:18:58 GMT"}], "update_date": "2015-09-16", "authors_parsed": [["Zhang", "Bingwen", ""], ["Xu", "Weiyu", ""], ["Cai", "Jian-Feng", ""], ["Lai", "Lifeng", ""]]}, {"id": "1509.04397", "submitter": "Suriya Gunasekar", "authors": "Suriya Gunasekar, Pradeep Ravikumar, Joydeep Ghosh", "title": "Exponential Family Matrix Completion under Structural Constraints", "comments": "20 pages, 9 figures", "journal-ref": "Gunasekar, Suriya, Pradeep Ravikumar, and Joydeep Ghosh.\n  \"Exponential family matrix completion under structural constraints\".\n  Proceedings of The 31st International Conference on Machine Learning, pp.\n  1917-1925, 2014", "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the matrix completion problem of recovering a structured matrix\nfrom noisy and partial measurements. Recent works have proposed tractable\nestimators with strong statistical guarantees for the case where the underlying\nmatrix is low--rank, and the measurements consist of a subset, either of the\nexact individual entries, or of the entries perturbed by additive Gaussian\nnoise, which is thus implicitly suited for thin--tailed continuous data.\nArguably, common applications of matrix completion require estimators for (a)\nheterogeneous data--types, such as skewed--continuous, count, binary, etc., (b)\nfor heterogeneous noise models (beyond Gaussian), which capture varied\nuncertainty in the measurements, and (c) heterogeneous structural constraints\nbeyond low--rank, such as block--sparsity, or a superposition structure of\nlow--rank plus elementwise sparseness, among others. In this paper, we provide\na vastly unified framework for generalized matrix completion by considering a\nmatrix completion setting wherein the matrix entries are sampled from any\nmember of the rich family of exponential family distributions; and impose\ngeneral structural constraints on the underlying matrix, as captured by a\ngeneral regularizer $\\mathcal{R}(.)$. We propose a simple convex regularized\n$M$--estimator for the generalized framework, and provide a unified and novel\nstatistical analysis for this general class of estimators. We finally\ncorroborate our theoretical results on simulated datasets.\n", "versions": [{"version": "v1", "created": "Tue, 15 Sep 2015 04:49:57 GMT"}], "update_date": "2015-09-16", "authors_parsed": [["Gunasekar", "Suriya", ""], ["Ravikumar", "Pradeep", ""], ["Ghosh", "Joydeep", ""]]}, {"id": "1509.04612", "submitter": "Alan Mosca", "authors": "Alan Mosca and George D. Magoulas", "title": "Adapting Resilient Propagation for Deep Learning", "comments": "Published in the proceedings of the UK workshop on Computational\n  Intelligence 2015 (UKCI)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Resilient Propagation (Rprop) algorithm has been very popular for\nbackpropagation training of multilayer feed-forward neural networks in various\napplications. The standard Rprop however encounters difficulties in the context\nof deep neural networks as typically happens with gradient-based learning\nalgorithms. In this paper, we propose a modification of the Rprop that combines\nstandard Rprop steps with a special drop out technique. We apply the method for\ntraining Deep Neural Networks as standalone components and in ensemble\nformulations. Results on the MNIST dataset show that the proposed modification\nalleviates standard Rprop's problems demonstrating improved learning speed and\naccuracy.\n", "versions": [{"version": "v1", "created": "Tue, 15 Sep 2015 15:55:29 GMT"}, {"version": "v2", "created": "Wed, 16 Sep 2015 11:45:48 GMT"}], "update_date": "2015-09-17", "authors_parsed": [["Mosca", "Alan", ""], ["Magoulas", "George D.", ""]]}, {"id": "1509.04640", "submitter": "Laurent Charlin", "authors": "Laurent Charlin and Rajesh Ranganath and James McInerney and David M.\n  Blei", "title": "Dynamic Poisson Factorization", "comments": "RecSys 2015", "journal-ref": null, "doi": "10.1145/2792838.2800174", "report-no": null, "categories": "cs.LG cs.IR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Models for recommender systems use latent factors to explain the preferences\nand behaviors of users with respect to a set of items (e.g., movies, books,\nacademic papers). Typically, the latent factors are assumed to be static and,\ngiven these factors, the observed preferences and behaviors of users are\nassumed to be generated without order. These assumptions limit the explorative\nand predictive capabilities of such models, since users' interests and item\npopularity may evolve over time. To address this, we propose dPF, a dynamic\nmatrix factorization model based on the recent Poisson factorization model for\nrecommendations. dPF models the time evolving latent factors with a Kalman\nfilter and the actions with Poisson distributions. We derive a scalable\nvariational inference algorithm to infer the latent factors. Finally, we\ndemonstrate dPF on 10 years of user click data from arXiv.org, one of the\nlargest repository of scientific papers and a formidable source of information\nabout the behavior of scientists. Empirically we show performance improvement\nover both static and, more recently proposed, dynamic recommendation models. We\nalso provide a thorough exploration of the inferred posteriors over the latent\nvariables.\n", "versions": [{"version": "v1", "created": "Tue, 15 Sep 2015 16:57:15 GMT"}], "update_date": "2015-09-16", "authors_parsed": [["Charlin", "Laurent", ""], ["Ranganath", "Rajesh", ""], ["McInerney", "James", ""], ["Blei", "David M.", ""]]}, {"id": "1509.04705", "submitter": "Evgeny Nikulchev", "authors": "N.N. Astakhova, L.A. Demidova, E.V. Nikulchev", "title": "Forecasting Method for Grouped Time Series with the Use of k-Means\n  Algorithm", "comments": "18 pages", "journal-ref": "Applied Mathematical Sciences, 2015, 9(97):4813-4830", "doi": "10.12988/ams.2015.55391", "report-no": null, "categories": "cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The paper is focused on the forecasting method for time series groups with\nthe use of algorithms for cluster analysis. $K$-means algorithm is suggested to\nbe a basic one for clustering. The coordinates of the centers of clusters have\nbeen put in correspondence with summarizing time series data the centroids of\nthe clusters. A description of time series, the centroids of the clusters, is\nimplemented with the use of forecasting models. They are based on strict binary\ntrees and a modified clonal selection algorithm. With the help of such\nforecasting models, the possibility of forming analytic dependences is shown.\nIt is suggested to use a common forecasting model, which is constructed for\ntime series the centroid of the cluster, in forecasting the private\n(individual) time series in the cluster. The promising application of the\nsuggested method for grouped time series forecasting is demonstrated.\n", "versions": [{"version": "v1", "created": "Tue, 15 Sep 2015 17:15:08 GMT"}], "update_date": "2015-09-17", "authors_parsed": [["Astakhova", "N. N.", ""], ["Demidova", "L. A.", ""], ["Nikulchev", "E. V.", ""]]}, {"id": "1509.05009", "submitter": "Nadav Cohen", "authors": "Nadav Cohen, Or Sharir, Amnon Shashua", "title": "On the Expressive Power of Deep Learning: A Tensor Analysis", "comments": null, "journal-ref": "29th Annual Conference on Learning Theory, pp. 698-728, 2016", "doi": null, "report-no": null, "categories": "cs.NE cs.LG cs.NA stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It has long been conjectured that hypotheses spaces suitable for data that is\ncompositional in nature, such as text or images, may be more efficiently\nrepresented with deep hierarchical networks than with shallow ones. Despite the\nvast empirical evidence supporting this belief, theoretical justifications to\ndate are limited. In particular, they do not account for the locality, sharing\nand pooling constructs of convolutional networks, the most successful deep\nlearning architecture to date. In this work we derive a deep network\narchitecture based on arithmetic circuits that inherently employs locality,\nsharing and pooling. An equivalence between the networks and hierarchical\ntensor factorizations is established. We show that a shallow network\ncorresponds to CP (rank-1) decomposition, whereas a deep network corresponds to\nHierarchical Tucker decomposition. Using tools from measure theory and matrix\nalgebra, we prove that besides a negligible set, all functions that can be\nimplemented by a deep network of polynomial size, require exponential size in\norder to be realized (or even approximated) by a shallow network. Since\nlog-space computation transforms our networks into SimNets, the result applies\ndirectly to a deep learning architecture demonstrating promising empirical\nperformance. The construction and theory developed in this paper shed new light\non various practices and ideas employed by the deep learning community.\n", "versions": [{"version": "v1", "created": "Wed, 16 Sep 2015 19:32:54 GMT"}, {"version": "v2", "created": "Sun, 14 Feb 2016 16:31:49 GMT"}, {"version": "v3", "created": "Fri, 27 May 2016 19:07:22 GMT"}], "update_date": "2016-10-18", "authors_parsed": [["Cohen", "Nadav", ""], ["Sharir", "Or", ""], ["Shashua", "Amnon", ""]]}, {"id": "1509.05086", "submitter": "Eduardo Cotilla-Sanchez", "authors": "Jordan Landford, Rich Meier, Richard Barella, Xinghui Zhao, Eduardo\n  Cotilla-Sanchez, Robert B. Bass, Scott Wallace", "title": "Fast Sequence Component Analysis for Attack Detection in Synchrophasor\n  Networks", "comments": "8 pages, 4 figures, submitted to IEEE Transactions", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modern power systems have begun integrating synchrophasor technologies into\npart of daily operations. Given the amount of solutions offered and the\nmaturity rate of application development it is not a matter of \"if\" but a\nmatter of \"when\" in regards to these technologies becoming ubiquitous in\ncontrol centers around the world. While the benefits are numerous, the\nfunctionality of operator-level applications can easily be nullified by\ninjection of deceptive data signals disguised as genuine measurements. Such\ndeceptive action is a common precursor to nefarious, often malicious activity.\nA correlation coefficient characterization and machine learning methodology are\nproposed to detect and identify injection of spoofed data signals. The proposed\nmethod utilizes statistical relationships intrinsic to power system parameters,\nwhich are quantified and presented. Several spoofing schemes have been\ndeveloped to qualitatively and quantitatively demonstrate detection\ncapabilities.\n", "versions": [{"version": "v1", "created": "Thu, 17 Sep 2015 00:21:22 GMT"}], "update_date": "2015-09-18", "authors_parsed": [["Landford", "Jordan", ""], ["Meier", "Rich", ""], ["Barella", "Richard", ""], ["Zhao", "Xinghui", ""], ["Cotilla-Sanchez", "Eduardo", ""], ["Bass", "Robert B.", ""], ["Wallace", "Scott", ""]]}, {"id": "1509.05113", "submitter": "Nathan Kallus", "authors": "Nathan Kallus, Madeleine Udell", "title": "Revealed Preference at Scale: Learning Personalized Preferences from\n  Assortment Choices", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of learning the preferences of a heterogeneous\npopulation by observing choices from an assortment of products, ads, or other\nofferings. Our observation model takes a form common in assortment planning\napplications: each arriving customer is offered an assortment consisting of a\nsubset of all possible offerings; we observe only the assortment and the\ncustomer's single choice.\n  In this paper we propose a mixture choice model with a natural underlying\nlow-dimensional structure, and show how to estimate its parameters. In our\nmodel, the preferences of each customer or segment follow a separate parametric\nchoice model, but the underlying structure of these parameters over all the\nmodels has low dimension. We show that a nuclear-norm regularized maximum\nlikelihood estimator can learn the preferences of all customers using a number\nof observations much smaller than the number of item-customer combinations.\nThis result shows the potential for structural assumptions to speed up learning\nand improve revenues in assortment planning and customization. We provide a\nspecialized factored gradient descent algorithm and study the success of the\napproach empirically.\n", "versions": [{"version": "v1", "created": "Thu, 17 Sep 2015 03:37:38 GMT"}, {"version": "v2", "created": "Tue, 7 Jun 2016 23:40:10 GMT"}], "update_date": "2016-06-09", "authors_parsed": [["Kallus", "Nathan", ""], ["Udell", "Madeleine", ""]]}, {"id": "1509.05142", "submitter": "Rajiv Sambasivan", "authors": "Sourish Das, Sasanka Roy, Rajiv Sambasivan", "title": "Fast Gaussian Process Regression for Big Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Gaussian Processes are widely used for regression tasks. A known limitation\nin the application of Gaussian Processes to regression tasks is that the\ncomputation of the solution requires performing a matrix inversion. The\nsolution also requires the storage of a large matrix in memory. These factors\nrestrict the application of Gaussian Process regression to small and moderate\nsize data sets. We present an algorithm that combines estimates from models\ndeveloped using subsets of the data obtained in a manner similar to the\nbootstrap. The sample size is a critical parameter for this algorithm.\nGuidelines for reasonable choices of algorithm parameters, based on detailed\nexperimental study, are provided. Various techniques have been proposed to\nscale Gaussian Processes to large scale regression tasks. The most appropriate\nchoice depends on the problem context. The proposed method is most appropriate\nfor problems where an additive model works well and the response depends on a\nsmall number of features. The minimax rate of convergence for such problems is\nattractive and we can build effective models with a small subset of the data.\nThe Stochastic Variational Gaussian Process and the Sparse Gaussian Process are\nalso appropriate choices for such problems. These methods pick a subset of data\nbased on theoretical considerations. The proposed algorithm uses bagging and\nrandom sampling. Results from experiments conducted as part of this study\nindicate that the algorithm presented in this work can be as effective as these\nmethods. Model stacking can be used to combine the model developed with the\nproposed method with models from other methods for large scale regression such\nas Gradient Boosted Trees. This can yield performance gains.\n", "versions": [{"version": "v1", "created": "Thu, 17 Sep 2015 06:18:08 GMT"}, {"version": "v2", "created": "Mon, 21 Sep 2015 12:16:05 GMT"}, {"version": "v3", "created": "Fri, 11 Mar 2016 17:08:48 GMT"}, {"version": "v4", "created": "Mon, 14 Mar 2016 03:40:11 GMT"}, {"version": "v5", "created": "Mon, 26 Sep 2016 12:07:46 GMT"}, {"version": "v6", "created": "Sat, 19 Aug 2017 02:06:53 GMT"}], "update_date": "2017-08-22", "authors_parsed": [["Das", "Sourish", ""], ["Roy", "Sasanka", ""], ["Sambasivan", "Rajiv", ""]]}, {"id": "1509.05172", "submitter": "Assaf Hallak", "authors": "Assaf Hallak, Aviv Tamar, Remi Munos, Shie Mannor", "title": "Generalized Emphatic Temporal Difference Learning: Bias-Variance\n  Analysis", "comments": "arXiv admin note: text overlap with arXiv:1508.03411", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the off-policy evaluation problem in Markov decision processes\nwith function approximation. We propose a generalization of the recently\nintroduced \\emph{emphatic temporal differences} (ETD) algorithm\n\\citep{SuttonMW15}, which encompasses the original ETD($\\lambda$), as well as\nseveral other off-policy evaluation algorithms as special cases. We call this\nframework \\ETD, where our introduced parameter $\\beta$ controls the decay rate\nof an importance-sampling term. We study conditions under which the projected\nfixed-point equation underlying \\ETD\\ involves a contraction operator, allowing\nus to present the first asymptotic error bounds (bias) for \\ETD. Our results\nshow that the original ETD algorithm always involves a contraction operator,\nand its bias is bounded. Moreover, by controlling $\\beta$, our proposed\ngeneralization allows trading-off bias for variance reduction, thereby\nachieving a lower total error.\n", "versions": [{"version": "v1", "created": "Thu, 17 Sep 2015 09:03:35 GMT"}, {"version": "v2", "created": "Fri, 27 Nov 2015 07:17:55 GMT"}], "update_date": "2015-11-30", "authors_parsed": [["Hallak", "Assaf", ""], ["Tamar", "Aviv", ""], ["Munos", "Remi", ""], ["Mannor", "Shie", ""]]}, {"id": "1509.05173", "submitter": "Andrew Simpson", "authors": "Andrew J.R. Simpson", "title": "Taming the ReLU with Parallel Dither in a Deep Neural Network", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Rectified Linear Units (ReLU) seem to have displaced traditional 'smooth'\nnonlinearities as activation-function-du-jour in many - but not all - deep\nneural network (DNN) applications. However, nobody seems to know why. In this\narticle, we argue that ReLU are useful because they are ideal demodulators -\nthis helps them perform fast abstract learning. However, this fast learning\ncomes at the expense of serious nonlinear distortion products - decoy features.\nWe show that Parallel Dither acts to suppress the decoy features, preventing\noverfitting and leaving the true features cleanly demodulated for rapid,\nreliable learning.\n", "versions": [{"version": "v1", "created": "Thu, 17 Sep 2015 09:04:30 GMT"}], "update_date": "2015-09-18", "authors_parsed": [["Simpson", "Andrew J. R.", ""]]}, {"id": "1509.05257", "submitter": "Ernesto Diaz-Aviles", "authors": "Hoang Thanh Lam and Ernesto Diaz-Aviles and Alessandra Pascale and\n  Yiannis Gkoufas and Bei Chen", "title": "(Blue) Taxi Destination and Trip Time Prediction from Partial\n  Trajectories", "comments": "ECML/PKDD Discovery Challenge 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.CY cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Real-time estimation of destination and travel time for taxis is of great\nimportance for existing electronic dispatch systems. We present an approach\nbased on trip matching and ensemble learning, in which we leverage the patterns\nobserved in a dataset of roughly 1.7 million taxi journeys to predict the\ncorresponding final destination and travel time for ongoing taxi trips, as a\nsolution for the ECML/PKDD Discovery Challenge 2015 competition. The results of\nour empirical evaluation show that our approach is effective and very robust,\nwhich led our team -- BlueTaxi -- to the 3rd and 7th position of the final\nrankings for the trip time and destination prediction tasks, respectively.\nGiven the fact that the final rankings were computed using a very small test\nset (with only 320 trips) we believe that our approach is one of the most\nrobust solutions for the challenge based on the consistency of our good results\nacross the test sets.\n", "versions": [{"version": "v1", "created": "Thu, 17 Sep 2015 13:51:55 GMT"}], "update_date": "2015-09-30", "authors_parsed": [["Lam", "Hoang Thanh", ""], ["Diaz-Aviles", "Ernesto", ""], ["Pascale", "Alessandra", ""], ["Gkoufas", "Yiannis", ""], ["Chen", "Bei", ""]]}, {"id": "1509.05371", "submitter": "Felix Trier", "authors": "Peter Burkert, Felix Trier, Muhammad Zeshan Afzal, Andreas Dengel,\n  Marcus Liwicki", "title": "DeXpression: Deep Convolutional Neural Network for Expression\n  Recognition", "comments": "Under consideration for publication in Pattern Recognition Letters", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a convolutional neural network (CNN) architecture for facial\nexpression recognition. The proposed architecture is independent of any\nhand-crafted feature extraction and performs better than the earlier proposed\nconvolutional neural network based approaches. We visualize the automatically\nextracted features which have been learned by the network in order to provide a\nbetter understanding. The standard datasets, i.e. Extended Cohn-Kanade (CKP)\nand MMI Facial Expression Databse are used for the quantitative evaluation. On\nthe CKP set the current state of the art approach, using CNNs, achieves an\naccuracy of 99.2%. For the MMI dataset, currently the best accuracy for emotion\nrecognition is 93.33%. The proposed architecture achieves 99.6% for CKP and\n98.63% for MMI, therefore performing better than the state of the art using\nCNNs. Automatic facial expression recognition has a broad spectrum of\napplications such as human-computer interaction and safety systems. This is due\nto the fact that non-verbal cues are important forms of communication and play\na pivotal role in interpersonal communication. The performance of the proposed\narchitecture endorses the efficacy and reliable usage of the proposed work for\nreal world applications.\n", "versions": [{"version": "v1", "created": "Thu, 17 Sep 2015 18:49:10 GMT"}, {"version": "v2", "created": "Wed, 17 Aug 2016 19:34:55 GMT"}], "update_date": "2016-08-18", "authors_parsed": [["Burkert", "Peter", ""], ["Trier", "Felix", ""], ["Afzal", "Muhammad Zeshan", ""], ["Dengel", "Andreas", ""], ["Liwicki", "Marcus", ""]]}, {"id": "1509.05472", "submitter": "Jun Wang", "authors": "Jun Wang, Wei Liu, Sanjiv Kumar, Shih-Fu Chang", "title": "Learning to Hash for Indexing Big Data - A Survey", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The explosive growth in big data has attracted much attention in designing\nefficient indexing and search methods recently. In many critical applications\nsuch as large-scale search and pattern matching, finding the nearest neighbors\nto a query is a fundamental research problem. However, the straightforward\nsolution using exhaustive comparison is infeasible due to the prohibitive\ncomputational complexity and memory requirement. In response, Approximate\nNearest Neighbor (ANN) search based on hashing techniques has become popular\ndue to its promising performance in both efficiency and accuracy. Prior\nrandomized hashing methods, e.g., Locality-Sensitive Hashing (LSH), explore\ndata-independent hash functions with random projections or permutations.\nAlthough having elegant theoretic guarantees on the search quality in certain\nmetric spaces, performance of randomized hashing has been shown insufficient in\nmany real-world applications. As a remedy, new approaches incorporating\ndata-driven learning methods in development of advanced hash functions have\nemerged. Such learning to hash methods exploit information such as data\ndistributions or class labels when optimizing the hash codes or functions.\nImportantly, the learned hash codes are able to preserve the proximity of\nneighboring data in the original feature spaces in the hash code spaces. The\ngoal of this paper is to provide readers with systematic understanding of\ninsights, pros and cons of the emerging techniques. We provide a comprehensive\nsurvey of the learning to hash framework and representative techniques of\nvarious types, including unsupervised, semi-supervised, and supervised. In\naddition, we also summarize recent hashing approaches utilizing the deep\nlearning models. Finally, we discuss the future direction and trends of\nresearch in this area.\n", "versions": [{"version": "v1", "created": "Thu, 17 Sep 2015 23:19:07 GMT"}], "update_date": "2015-09-21", "authors_parsed": [["Wang", "Jun", ""], ["Liu", "Wei", ""], ["Kumar", "Sanjiv", ""], ["Chang", "Shih-Fu", ""]]}, {"id": "1509.05473", "submitter": "Alexey Milovanov", "authors": "Alexey Milovanov", "title": "Algorithmic statistics, prediction and machine learning", "comments": "22 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.IT math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Algorithmic statistics considers the following problem: given a binary string\n$x$ (e.g., some experimental data), find a \"good\" explanation of this data. It\nuses algorithmic information theory to define formally what is a good\nexplanation. In this paper we extend this framework in two directions.\n  First, the explanations are not only interesting in themselves but also used\nfor prediction: we want to know what kind of data we may reasonably expect in\nsimilar situations (repeating the same experiment). We show that some kind of\nhierarchy can be constructed both in terms of algorithmic statistics and using\nthe notion of a priori probability, and these two approaches turn out to be\nequivalent.\n  Second, a more realistic approach that goes back to machine learning theory,\nassumes that we have not a single data string $x$ but some set of \"positive\nexamples\" $x_1,\\ldots,x_l$ that all belong to some unknown set $A$, a property\nthat we want to learn. We want this set $A$ to contain all positive examples\nand to be as small and simple as possible. We show how algorithmic statistic\ncan be extended to cover this situation.\n", "versions": [{"version": "v1", "created": "Thu, 17 Sep 2015 23:25:00 GMT"}], "update_date": "2015-09-21", "authors_parsed": [["Milovanov", "Alexey", ""]]}, {"id": "1509.05647", "submitter": "Dan Garber", "authors": "Dan Garber and Elad Hazan", "title": "Fast and Simple PCA via Convex Optimization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.LG cs.NA math.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The problem of principle component analysis (PCA) is traditionally solved by\nspectral or algebraic methods. We show how computing the leading principal\ncomponent could be reduced to solving a \\textit{small} number of\nwell-conditioned {\\it convex} optimization problems. This gives rise to a new\nefficient method for PCA based on recent advances in stochastic methods for\nconvex optimization.\n  In particular we show that given a $d\\times d$ matrix $\\X =\n\\frac{1}{n}\\sum_{i=1}^n\\x_i\\x_i^{\\top}$ with top eigenvector $\\u$ and top\neigenvalue $\\lambda_1$ it is possible to: \\begin{itemize} \\item compute a unit\nvector $\\w$ such that $(\\w^{\\top}\\u)^2 \\geq 1-\\epsilon$ in\n$\\tilde{O}\\left({\\frac{d}{\\delta^2}+N}\\right)$ time, where $\\delta = \\lambda_1\n- \\lambda_2$ and $N$ is the total number of non-zero entries in\n$\\x_1,...,\\x_n$,\n  \\item compute a unit vector $\\w$ such that $\\w^{\\top}\\X\\w \\geq\n\\lambda_1-\\epsilon$ in $\\tilde{O}(d/\\epsilon^2)$ time. \\end{itemize} To the\nbest of our knowledge, these bounds are the fastest to date for a wide regime\nof parameters. These results could be further accelerated when $\\delta$ (in the\nfirst case) and $\\epsilon$ (in the second case) are smaller than $\\sqrt{d/N}$.\n", "versions": [{"version": "v1", "created": "Fri, 18 Sep 2015 15:03:03 GMT"}, {"version": "v2", "created": "Wed, 7 Oct 2015 12:50:14 GMT"}, {"version": "v3", "created": "Sun, 25 Oct 2015 19:10:31 GMT"}, {"version": "v4", "created": "Wed, 25 Nov 2015 12:07:31 GMT"}], "update_date": "2015-11-26", "authors_parsed": [["Garber", "Dan", ""], ["Hazan", "Elad", ""]]}, {"id": "1509.05760", "submitter": "Scott Yang", "authors": "Mehryar Mohri, Scott Yang", "title": "Accelerating Optimization via Adaptive Prediction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a powerful general framework for designing data-dependent\noptimization algorithms, building upon and unifying recent techniques in\nadaptive regularization, optimistic gradient predictions, and problem-dependent\nrandomization. We first present a series of new regret guarantees that hold at\nany time and under very minimal assumptions, and then show how different\nrelaxations recover existing algorithms, both basic as well as more recent\nsophisticated ones. Finally, we show how combining adaptivity, optimism, and\nproblem-dependent randomization can guide the design of algorithms that benefit\nfrom more favorable guarantees than recent state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Fri, 18 Sep 2015 19:49:54 GMT"}, {"version": "v2", "created": "Fri, 2 Oct 2015 20:13:42 GMT"}, {"version": "v3", "created": "Tue, 13 Oct 2015 16:40:18 GMT"}], "update_date": "2015-10-14", "authors_parsed": [["Mohri", "Mehryar", ""], ["Yang", "Scott", ""]]}, {"id": "1509.05765", "submitter": "Andrew Simpson", "authors": "Andrew J.R. Simpson", "title": "\"Oddball SGD\": Novelty Driven Stochastic Gradient Descent for Training\n  Deep Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Stochastic Gradient Descent (SGD) is arguably the most popular of the machine\nlearning methods applied to training deep neural networks (DNN) today. It has\nrecently been demonstrated that SGD can be statistically biased so that certain\nelements of the training set are learned more rapidly than others. In this\narticle, we place SGD into a feedback loop whereby the probability of selection\nis proportional to error magnitude. This provides a novelty-driven oddball SGD\nprocess that learns more rapidly than traditional SGD by prioritising those\nelements of the training set with the largest novelty (error). In our DNN\nexample, oddball SGD trains some 50x faster than regular SGD.\n", "versions": [{"version": "v1", "created": "Fri, 18 Sep 2015 19:58:24 GMT"}], "update_date": "2015-09-21", "authors_parsed": [["Simpson", "Andrew J. R.", ""]]}, {"id": "1509.05789", "submitter": "Alessandro Checco", "authors": "Alessandro Checco, Giuseppe Bianchi, Doug Leith", "title": "BLC: Private Matrix Factorization Recommenders via Automatic Group\n  Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a privacy-enhanced matrix factorization recommender that exploits\nthe fact that users can often be grouped together by interest. This allows a\nform of \"hiding in the crowd\" privacy. We introduce a novel matrix\nfactorization approach suited to making recommendations in a shared group (or\nnym) setting and the BLC algorithm for carrying out this matrix factorization\nin a privacy-enhanced manner. We demonstrate that the increased privacy does\nnot come at the cost of reduced recommendation accuracy.\n", "versions": [{"version": "v1", "created": "Fri, 18 Sep 2015 20:21:43 GMT"}, {"version": "v2", "created": "Tue, 22 Mar 2016 22:28:38 GMT"}, {"version": "v3", "created": "Mon, 27 Feb 2017 23:51:40 GMT"}], "update_date": "2017-03-01", "authors_parsed": [["Checco", "Alessandro", ""], ["Bianchi", "Giuseppe", ""], ["Leith", "Doug", ""]]}, {"id": "1509.05808", "submitter": "Tatsunori Hashimoto", "authors": "Tatsunori B. Hashimoto, David Alvarez-Melis, Tommi S. Jaakkola", "title": "Word, graph and manifold embedding from Markov processes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Continuous vector representations of words and objects appear to carry\nsurprisingly rich semantic content. In this paper, we advance both the\nconceptual and theoretical understanding of word embeddings in three ways.\nFirst, we ground embeddings in semantic spaces studied in\ncognitive-psychometric literature and introduce new evaluation tasks. Second,\nin contrast to prior work, we take metric recovery as the key object of study,\nunify existing algorithms as consistent metric recovery methods based on\nco-occurrence counts from simple Markov random walks, and propose a new\nrecovery algorithm. Third, we generalize metric recovery to graphs and\nmanifolds, relating co-occurence counts on random walks in graphs and random\nprocesses on manifolds to the underlying metric to be recovered, thereby\nreconciling manifold estimation and embedding algorithms. We compare embedding\nalgorithms across a range of tasks, from nonlinear dimensionality reduction to\nthree semantic language tasks, including analogies, sequence completion, and\nclassification.\n", "versions": [{"version": "v1", "created": "Fri, 18 Sep 2015 21:50:38 GMT"}], "update_date": "2015-09-22", "authors_parsed": [["Hashimoto", "Tatsunori B.", ""], ["Alvarez-Melis", "David", ""], ["Jaakkola", "Tommi S.", ""]]}, {"id": "1509.05936", "submitter": "Yoshua Bengio", "authors": "Yoshua Bengio, Thomas Mesnard, Asja Fischer, Saizheng Zhang and Yuhuai\n  Wu", "title": "STDP as presynaptic activity times rate of change of postsynaptic\n  activity", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.LG q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a weight update formula that is expressed only in terms of\nfiring rates and their derivatives and that results in changes consistent with\nthose associated with spike-timing dependent plasticity (STDP) rules and\nbiological observations, even though the explicit timing of spikes is not\nneeded. The new rule changes a synaptic weight in proportion to the product of\nthe presynaptic firing rate and the temporal rate of change of activity on the\npostsynaptic side. These quantities are interesting for studying theoretical\nexplanation for synaptic changes from a machine learning perspective. In\nparticular, if neural dynamics moved neural activity towards reducing some\nobjective function, then this STDP rule would correspond to stochastic gradient\ndescent on that objective function.\n", "versions": [{"version": "v1", "created": "Sat, 19 Sep 2015 21:05:18 GMT"}, {"version": "v2", "created": "Mon, 21 Mar 2016 10:54:18 GMT"}], "update_date": "2016-03-22", "authors_parsed": [["Bengio", "Yoshua", ""], ["Mesnard", "Thomas", ""], ["Fischer", "Asja", ""], ["Zhang", "Saizheng", ""], ["Wu", "Yuhuai", ""]]}, {"id": "1509.05962", "submitter": "Rakesh Achanta", "authors": "Rakesh Achanta, Trevor Hastie", "title": "Telugu OCR Framework using Deep Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.CV cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we address the task of Optical Character Recognition(OCR) for\nthe Telugu script. We present an end-to-end framework that segments the text\nimage, classifies the characters and extracts lines using a language model. The\nsegmentation is based on mathematical morphology. The classification module,\nwhich is the most challenging task of the three, is a deep convolutional neural\nnetwork. The language is modelled as a third degree markov chain at the glyph\nlevel. Telugu script is a complex alphasyllabary and the language is\nagglutinative, making the problem hard. In this paper we apply the latest\nadvances in neural networks to achieve state-of-the-art error rates. We also\nreview convolutional neural networks in great detail and expound the\nstatistical justification behind the many tricks needed to make Deep Learning\nwork.\n", "versions": [{"version": "v1", "created": "Sun, 20 Sep 2015 03:35:05 GMT"}, {"version": "v2", "created": "Wed, 15 Feb 2017 02:29:04 GMT"}], "update_date": "2017-02-16", "authors_parsed": [["Achanta", "Rakesh", ""], ["Hastie", "Trevor", ""]]}, {"id": "1509.05982", "submitter": "Dan Stowell", "authors": "Dan Stowell and Richard E. Turner", "title": "Denoising without access to clean data using a partitioned autoencoder", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Training a denoising autoencoder neural network requires access to truly\nclean data, a requirement which is often impractical. To remedy this, we\nintroduce a method to train an autoencoder using only noisy data, having\nexamples with and without the signal class of interest. The autoencoder learns\na partitioned representation of signal and noise, learning to reconstruct each\nseparately. We illustrate the method by denoising birdsong audio (available\nabundantly in uncontrolled noisy datasets) using a convolutional autoencoder.\n", "versions": [{"version": "v1", "created": "Sun, 20 Sep 2015 09:03:48 GMT"}, {"version": "v2", "created": "Tue, 22 Sep 2015 20:51:05 GMT"}], "update_date": "2015-09-24", "authors_parsed": [["Stowell", "Dan", ""], ["Turner", "Richard E.", ""]]}, {"id": "1509.06041", "submitter": "Quanzeng You", "authors": "Quanzeng You, Jiebo Luo, Hailin Jin, Jianchao Yang", "title": "Robust Image Sentiment Analysis Using Progressively Trained and Domain\n  Transferred Deep Networks", "comments": "9 pages, 5 figures, AAAI 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sentiment analysis of online user generated content is important for many\nsocial media analytics tasks. Researchers have largely relied on textual\nsentiment analysis to develop systems to predict political elections, measure\neconomic indicators, and so on. Recently, social media users are increasingly\nusing images and videos to express their opinions and share their experiences.\nSentiment analysis of such large scale visual content can help better extract\nuser sentiments toward events or topics, such as those in image tweets, so that\nprediction of sentiment from visual content is complementary to textual\nsentiment analysis. Motivated by the needs in leveraging large scale yet noisy\ntraining data to solve the extremely challenging problem of image sentiment\nanalysis, we employ Convolutional Neural Networks (CNN). We first design a\nsuitable CNN architecture for image sentiment analysis. We obtain half a\nmillion training samples by using a baseline sentiment algorithm to label\nFlickr images. To make use of such noisy machine labeled data, we employ a\nprogressive strategy to fine-tune the deep network. Furthermore, we improve the\nperformance on Twitter images by inducing domain transfer with a small number\nof manually labeled Twitter images. We have conducted extensive experiments on\nmanually labeled Twitter images. The results show that the proposed CNN can\nachieve better performance in image sentiment analysis than competing\nalgorithms.\n", "versions": [{"version": "v1", "created": "Sun, 20 Sep 2015 18:36:01 GMT"}], "update_date": "2015-09-22", "authors_parsed": [["You", "Quanzeng", ""], ["Luo", "Jiebo", ""], ["Jin", "Hailin", ""], ["Yang", "Jianchao", ""]]}, {"id": "1509.06088", "submitter": "Qiyi Lu", "authors": "Qiyi Lu, Xingye Qiao", "title": "Significance Analysis of High-Dimensional, Low-Sample Size Partially\n  Labeled Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Classification and clustering are both important topics in statistical\nlearning. A natural question herein is whether predefined classes are really\ndifferent from one another, or whether clusters are really there. Specifically,\nwe may be interested in knowing whether the two classes defined by some class\nlabels (when they are provided), or the two clusters tagged by a clustering\nalgorithm (where class labels are not provided), are from the same underlying\ndistribution. Although both are challenging questions for the high-dimensional,\nlow-sample size data, there has been some recent development for both. However,\nwhen it is costly to manually place labels on observations, it is often that\nonly a small portion of the class labels is available. In this article, we\npropose a significance analysis approach for such type of data, namely\npartially labeled data. Our method makes use of the whole data and tries to\ntest the class difference as if all the labels were observed. Compared to a\ntesting method that ignores the label information, our method provides a\ngreater power, meanwhile, maintaining the size, illustrated by a comprehensive\nsimulation study. Theoretical properties of the proposed method are studied\nwith emphasis on the high-dimensional, low-sample size setting. Our simulated\nexamples help to understand when and how the information extracted from the\nlabeled data can be effective. A real data example further illustrates the\nusefulness of the proposed method.\n", "versions": [{"version": "v1", "created": "Mon, 21 Sep 2015 01:23:45 GMT"}], "update_date": "2015-09-22", "authors_parsed": [["Lu", "Qiyi", ""], ["Qiao", "Xingye", ""]]}, {"id": "1509.06095", "submitter": "Xiao-Lei Zhang", "authors": "Xiao-Lei Zhang", "title": "Multilayer bootstrap network for unsupervised speaker recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.SD", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We apply multilayer bootstrap network (MBN), a recent proposed unsupervised\nlearning method, to unsupervised speaker recognition. The proposed method first\nextracts supervectors from an unsupervised universal background model, then\nreduces the dimension of the high-dimensional supervectors by multilayer\nbootstrap network, and finally conducts unsupervised speaker recognition by\nclustering the low-dimensional data. The comparison results with 2 unsupervised\nand 1 supervised speaker recognition techniques demonstrate the effectiveness\nand robustness of the proposed method.\n", "versions": [{"version": "v1", "created": "Mon, 21 Sep 2015 02:28:44 GMT"}], "update_date": "2015-09-22", "authors_parsed": [["Zhang", "Xiao-Lei", ""]]}, {"id": "1509.06113", "submitter": "Chelsea Finn", "authors": "Chelsea Finn, Xin Yu Tan, Yan Duan, Trevor Darrell, Sergey Levine,\n  Pieter Abbeel", "title": "Deep Spatial Autoencoders for Visuomotor Learning", "comments": "Published in the International Conference on Robotics and Automation\n  (ICRA)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Reinforcement learning provides a powerful and flexible framework for\nautomated acquisition of robotic motion skills. However, applying reinforcement\nlearning requires a sufficiently detailed representation of the state,\nincluding the configuration of task-relevant objects. We present an approach\nthat automates state-space construction by learning a state representation\ndirectly from camera images. Our method uses a deep spatial autoencoder to\nacquire a set of feature points that describe the environment for the current\ntask, such as the positions of objects, and then learns a motion skill with\nthese feature points using an efficient reinforcement learning method based on\nlocal linear models. The resulting controller reacts continuously to the\nlearned feature points, allowing the robot to dynamically manipulate objects in\nthe world with closed-loop control. We demonstrate our method with a PR2 robot\non tasks that include pushing a free-standing toy block, picking up a bag of\nrice using a spatula, and hanging a loop of rope on a hook at various\npositions. In each task, our method automatically learns to track task-relevant\nobjects and manipulate their configuration with the robot's arm.\n", "versions": [{"version": "v1", "created": "Mon, 21 Sep 2015 06:15:12 GMT"}, {"version": "v2", "created": "Tue, 27 Oct 2015 18:03:25 GMT"}, {"version": "v3", "created": "Tue, 1 Mar 2016 17:24:50 GMT"}], "update_date": "2016-03-02", "authors_parsed": [["Finn", "Chelsea", ""], ["Tan", "Xin Yu", ""], ["Duan", "Yan", ""], ["Darrell", "Trevor", ""], ["Levine", "Sergey", ""], ["Abbeel", "Pieter", ""]]}, {"id": "1509.06163", "submitter": "Shubhendu Trivedi", "authors": "Shubhendu Trivedi, Zachary A. Pardos, Neil T. Heffernan", "title": "The Utility of Clustering in Prediction Tasks", "comments": "An experimental research report, dated 11 September 2011", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We explore the utility of clustering in reducing error in various prediction\ntasks. Previous work has hinted at the improvement in prediction accuracy\nattributed to clustering algorithms if used to pre-process the data. In this\nwork we more deeply investigate the direct utility of using clustering to\nimprove prediction accuracy and provide explanations for why this may be so. We\nlook at a number of datasets, run k-means at different scales and for each\nscale we train predictors. This produces k sets of predictions. These\npredictions are then combined by a na\\\"ive ensemble. We observed that this use\nof a predictor in conjunction with clustering improved the prediction accuracy\nin most datasets. We believe this indicates the predictive utility of\nexploiting structure in the data and the data compression handed over by\nclustering. We also found that using this method improves upon the prediction\nof even a Random Forests predictor which suggests this method is providing a\nnovel, and useful source of variance in the prediction process.\n", "versions": [{"version": "v1", "created": "Mon, 21 Sep 2015 09:42:50 GMT"}], "update_date": "2015-09-22", "authors_parsed": [["Trivedi", "Shubhendu", ""], ["Pardos", "Zachary A.", ""], ["Heffernan", "Neil T.", ""]]}, {"id": "1509.06279", "submitter": "Anant Baijal", "authors": "Anant Baijal, Jaeyoun Cho, Woojung Lee and Byeong-Seob Ko", "title": "Sports highlights generation based on acoustic events detection: A rugby\n  case study", "comments": "IEEE International Conference on Consumer Electronics (IEEE ICCE\n  2015)", "journal-ref": null, "doi": "10.1109/ICCE.2015.7066303", "report-no": null, "categories": "cs.SD cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We approach the challenging problem of generating highlights from sports\nbroadcasts utilizing audio information only. A language-independent,\nmulti-stage classification approach is employed for detection of key acoustic\nevents which then act as a platform for summarization of highlight scenes.\nObjective results and human experience indicate that our system is highly\nefficient.\n", "versions": [{"version": "v1", "created": "Fri, 18 Sep 2015 12:47:09 GMT"}], "update_date": "2015-09-22", "authors_parsed": [["Baijal", "Anant", ""], ["Cho", "Jaeyoun", ""], ["Lee", "Woojung", ""], ["Ko", "Byeong-Seob", ""]]}, {"id": "1509.06449", "submitter": "Yingxiang Yang", "authors": "Yingxiang Yang, Jalal Etesami, Negar Kiyavash", "title": "Efficient Neighborhood Selection for Gaussian Graphical Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.IT cs.LG math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper addresses the problem of neighborhood selection for Gaussian\ngraphical models. We present two heuristic algorithms: a forward-backward\ngreedy algorithm for general Gaussian graphical models based on mutual\ninformation test, and a threshold-based algorithm for walk summable Gaussian\ngraphical models. Both algorithms are shown to be structurally consistent, and\nefficient. Numerical results show that both algorithms work very well.\n", "versions": [{"version": "v1", "created": "Tue, 22 Sep 2015 02:51:30 GMT"}], "update_date": "2015-09-23", "authors_parsed": [["Yang", "Yingxiang", ""], ["Etesami", "Jalal", ""], ["Kiyavash", "Negar", ""]]}, {"id": "1509.06458", "submitter": "Jian Sun", "authors": "Zuoqiang Shi and Jian Sun and Minghao Tian", "title": "Harmonic Extension", "comments": "10 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we consider the harmonic extension problem, which is widely\nused in many applications of machine learning. We find that the transitional\nmethod of graph Laplacian fails to produce a good approximation of the\nclassical harmonic function. To tackle this problem, we propose a new method\ncalled the point integral method (PIM). We consider the harmonic extension\nproblem from the point of view of solving PDEs on manifolds. The basic idea of\nthe PIM method is to approximate the harmonicity using an integral equation,\nwhich is easy to be discretized from points. Based on the integral equation, we\nexplain the reason why the transitional graph Laplacian may fail to approximate\nthe harmonicity in the classical sense and propose a different approach which\nwe call the volume constraint method (VCM). Theoretically, both the PIM and the\nVCM computes a harmonic function with convergence guarantees, and practically,\nthey are both simple, which amount to solve a linear system. One important\napplication of the harmonic extension in machine learning is semi-supervised\nlearning. We run a popular semi-supervised learning algorithm by Zhu et al.\nover a couple of well-known datasets and compare the performance of the\naforementioned approaches. Our experiments show the PIM performs the best.\n", "versions": [{"version": "v1", "created": "Tue, 22 Sep 2015 04:13:38 GMT"}], "update_date": "2015-09-23", "authors_parsed": [["Shi", "Zuoqiang", ""], ["Sun", "Jian", ""], ["Tian", "Minghao", ""]]}, {"id": "1509.06461", "submitter": "Hado van Hasselt", "authors": "Hado van Hasselt, Arthur Guez, David Silver", "title": "Deep Reinforcement Learning with Double Q-learning", "comments": "AAAI 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The popular Q-learning algorithm is known to overestimate action values under\ncertain conditions. It was not previously known whether, in practice, such\noverestimations are common, whether they harm performance, and whether they can\ngenerally be prevented. In this paper, we answer all these questions\naffirmatively. In particular, we first show that the recent DQN algorithm,\nwhich combines Q-learning with a deep neural network, suffers from substantial\noverestimations in some games in the Atari 2600 domain. We then show that the\nidea behind the Double Q-learning algorithm, which was introduced in a tabular\nsetting, can be generalized to work with large-scale function approximation. We\npropose a specific adaptation to the DQN algorithm and show that the resulting\nalgorithm not only reduces the observed overestimations, as hypothesized, but\nthat this also leads to much better performance on several games.\n", "versions": [{"version": "v1", "created": "Tue, 22 Sep 2015 04:40:22 GMT"}, {"version": "v2", "created": "Fri, 20 Nov 2015 15:20:50 GMT"}, {"version": "v3", "created": "Tue, 8 Dec 2015 21:19:16 GMT"}], "update_date": "2015-12-10", "authors_parsed": [["van Hasselt", "Hado", ""], ["Guez", "Arthur", ""], ["Silver", "David", ""]]}, {"id": "1509.06569", "submitter": "Alexander Novikov", "authors": "Alexander Novikov, Dmitry Podoprikhin, Anton Osokin, Dmitry Vetrov", "title": "Tensorizing Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural networks currently demonstrate state-of-the-art performance in\nseveral domains. At the same time, models of this class are very demanding in\nterms of computational resources. In particular, a large amount of memory is\nrequired by commonly used fully-connected layers, making it hard to use the\nmodels on low-end devices and stopping the further increase of the model size.\nIn this paper we convert the dense weight matrices of the fully-connected\nlayers to the Tensor Train format such that the number of parameters is reduced\nby a huge factor and at the same time the expressive power of the layer is\npreserved. In particular, for the Very Deep VGG networks we report the\ncompression factor of the dense weight matrix of a fully-connected layer up to\n200000 times leading to the compression factor of the whole network up to 7\ntimes.\n", "versions": [{"version": "v1", "created": "Tue, 22 Sep 2015 12:31:03 GMT"}, {"version": "v2", "created": "Sun, 20 Dec 2015 11:44:05 GMT"}], "update_date": "2015-12-22", "authors_parsed": [["Novikov", "Alexander", ""], ["Podoprikhin", "Dmitry", ""], ["Osokin", "Anton", ""], ["Vetrov", "Dmitry", ""]]}, {"id": "1509.06589", "submitter": "Nicol\\`o Navarin", "authors": "Giovanni Da San Martino, Nicol\\`o Navarin, and Alessandro Sperduti", "title": "Graph Kernels exploiting Weisfeiler-Lehman Graph Isomorphism Test\n  Extensions", "comments": null, "journal-ref": "Neural Information Processing, Volume 8835 of the series Lecture\n  Notes in Computer Science pp 93-100, 2014 Springer International Publishing", "doi": "10.1007/978-3-319-12640-1_12", "report-no": null, "categories": "cs.LG cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we present a novel graph kernel framework inspired the by the\nWeisfeiler-Lehman (WL) isomorphism tests. Any WL test comprises a relabelling\nphase of the nodes based on test-specific information extracted from the graph,\nfor example the set of neighbours of a node. We defined a novel relabelling and\nderived two kernels of the framework from it. The novel kernels are very fast\nto compute and achieve state-of-the-art results on five real-world datasets.\n", "versions": [{"version": "v1", "created": "Tue, 22 Sep 2015 13:21:08 GMT"}], "update_date": "2015-09-23", "authors_parsed": [["Martino", "Giovanni Da San", ""], ["Navarin", "Nicol\u00f2", ""], ["Sperduti", "Alessandro", ""]]}, {"id": "1509.06664", "submitter": "Tim Rockt\\\"aschel", "authors": "Tim Rockt\\\"aschel, Edward Grefenstette, Karl Moritz Hermann,\n  Tom\\'a\\v{s} Ko\\v{c}isk\\'y, Phil Blunsom", "title": "Reasoning about Entailment with Neural Attention", "comments": "ICLR 2016 camera-ready, 9 pages, 10 figures (incl. subfigures)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While most approaches to automatically recognizing entailment relations have\nused classifiers employing hand engineered features derived from complex\nnatural language processing pipelines, in practice their performance has been\nonly slightly better than bag-of-word pair classifiers using only lexical\nsimilarity. The only attempt so far to build an end-to-end differentiable\nneural network for entailment failed to outperform such a simple similarity\nclassifier. In this paper, we propose a neural model that reads two sentences\nto determine entailment using long short-term memory units. We extend this\nmodel with a word-by-word neural attention mechanism that encourages reasoning\nover entailments of pairs of words and phrases. Furthermore, we present a\nqualitative analysis of attention weights produced by this model, demonstrating\nsuch reasoning capabilities. On a large entailment dataset this model\noutperforms the previous best neural model and a classifier with engineered\nfeatures by a substantial margin. It is the first generic end-to-end\ndifferentiable system that achieves state-of-the-art accuracy on a textual\nentailment dataset.\n", "versions": [{"version": "v1", "created": "Tue, 22 Sep 2015 16:08:24 GMT"}, {"version": "v2", "created": "Tue, 10 Nov 2015 22:12:52 GMT"}, {"version": "v3", "created": "Mon, 18 Jan 2016 17:28:30 GMT"}, {"version": "v4", "created": "Tue, 1 Mar 2016 10:32:06 GMT"}], "update_date": "2016-03-02", "authors_parsed": [["Rockt\u00e4schel", "Tim", ""], ["Grefenstette", "Edward", ""], ["Hermann", "Karl Moritz", ""], ["Ko\u010disk\u00fd", "Tom\u00e1\u0161", ""], ["Blunsom", "Phil", ""]]}, {"id": "1509.06791", "submitter": "Gregory Kahn", "authors": "Tianhao Zhang, Gregory Kahn, Sergey Levine, Pieter Abbeel", "title": "Learning Deep Control Policies for Autonomous Aerial Vehicles with\n  MPC-Guided Policy Search", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Model predictive control (MPC) is an effective method for controlling robotic\nsystems, particularly autonomous aerial vehicles such as quadcopters. However,\napplication of MPC can be computationally demanding, and typically requires\nestimating the state of the system, which can be challenging in complex,\nunstructured environments. Reinforcement learning can in principle forego the\nneed for explicit state estimation and acquire a policy that directly maps\nsensor readings to actions, but is difficult to apply to unstable systems that\nare liable to fail catastrophically during training before an effective policy\nhas been found. We propose to combine MPC with reinforcement learning in the\nframework of guided policy search, where MPC is used to generate data at\ntraining time, under full state observations provided by an instrumented\ntraining environment. This data is used to train a deep neural network policy,\nwhich is allowed to access only the raw observations from the vehicle's onboard\nsensors. After training, the neural network policy can successfully control the\nrobot without knowledge of the full state, and at a fraction of the\ncomputational cost of MPC. We evaluate our method by learning obstacle\navoidance policies for a simulated quadrotor, using simulated onboard sensors\nand no explicit state estimation at test time.\n", "versions": [{"version": "v1", "created": "Tue, 22 Sep 2015 21:27:27 GMT"}, {"version": "v2", "created": "Tue, 16 Feb 2016 06:49:26 GMT"}], "update_date": "2016-02-17", "authors_parsed": [["Zhang", "Tianhao", ""], ["Kahn", "Gregory", ""], ["Levine", "Sergey", ""], ["Abbeel", "Pieter", ""]]}, {"id": "1509.06807", "submitter": "Ke Li", "authors": "Ke Li and Jitendra Malik", "title": "Bandit Label Inference for Weakly Supervised Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The scarcity of data annotated at the desired level of granularity is a\nrecurring issue in many applications. Significant amounts of effort have been\ndevoted to developing weakly supervised methods tailored to each individual\nsetting, which are often carefully designed to take advantage of the particular\nproperties of weak supervision regimes, form of available data and prior\nknowledge of the task at hand. Unfortunately, it is difficult to adapt these\nmethods to new tasks and/or forms of data, which often require different weak\nsupervision regimes or models. We present a general-purpose method that can\nsolve any weakly supervised learning problem irrespective of the weak\nsupervision regime or the model. The proposed method turns any off-the-shelf\nstrongly supervised classifier into a weakly supervised classifier and allows\nthe user to specify any arbitrary weakly supervision regime via a loss\nfunction. We apply the method to several different weak supervision regimes and\ndemonstrate competitive results compared to methods specifically engineered for\nthose settings.\n", "versions": [{"version": "v1", "created": "Tue, 22 Sep 2015 23:13:12 GMT"}], "update_date": "2015-09-24", "authors_parsed": [["Li", "Ke", ""], ["Malik", "Jitendra", ""]]}, {"id": "1509.06812", "submitter": "Jimmy Ba", "authors": "Jimmy Ba, Roger Grosse, Ruslan Salakhutdinov, Brendan Frey", "title": "Learning Wake-Sleep Recurrent Attention Models", "comments": "To appear in NIPS 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite their success, convolutional neural networks are computationally\nexpensive because they must examine all image locations. Stochastic\nattention-based models have been shown to improve computational efficiency at\ntest time, but they remain difficult to train because of intractable posterior\ninference and high variance in the stochastic gradient estimates. Borrowing\ntechniques from the literature on training deep generative models, we present\nthe Wake-Sleep Recurrent Attention Model, a method for training stochastic\nattention networks which improves posterior inference and which reduces the\nvariability in the stochastic gradients. We show that our method can greatly\nspeed up the training time for stochastic attention networks in the domains of\nimage classification and caption generation.\n", "versions": [{"version": "v1", "created": "Tue, 22 Sep 2015 23:52:30 GMT"}], "update_date": "2015-09-24", "authors_parsed": [["Ba", "Jimmy", ""], ["Grosse", "Roger", ""], ["Salakhutdinov", "Ruslan", ""], ["Frey", "Brendan", ""]]}, {"id": "1509.06824", "submitter": "Christopher Xie", "authors": "Christopher Xie, Sachin Patil, Teodor Moldovan, Sergey Levine, Pieter\n  Abbeel", "title": "Model-based Reinforcement Learning with Parametrized Physical Models and\n  Optimism-Driven Exploration", "comments": "8 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present a robotic model-based reinforcement learning method\nthat combines ideas from model identification and model predictive control. We\nuse a feature-based representation of the dynamics that allows the dynamics\nmodel to be fitted with a simple least squares procedure, and the features are\nidentified from a high-level specification of the robot's morphology,\nconsisting of the number and connectivity structure of its links. Model\npredictive control is then used to choose the actions under an optimistic model\nof the dynamics, which produces an efficient and goal-directed exploration\nstrategy. We present real time experimental results on standard benchmark\nproblems involving the pendulum, cartpole, and double pendulum systems.\nExperiments indicate that our method is able to learn a range of benchmark\ntasks substantially faster than the previous best methods. To evaluate our\napproach on a realistic robotic control task, we also demonstrate real time\ncontrol of a simulated 7 degree of freedom arm.\n", "versions": [{"version": "v1", "created": "Wed, 23 Sep 2015 02:04:18 GMT"}, {"version": "v2", "created": "Tue, 15 Mar 2016 07:53:33 GMT"}], "update_date": "2016-03-16", "authors_parsed": [["Xie", "Christopher", ""], ["Patil", "Sachin", ""], ["Moldovan", "Teodor", ""], ["Levine", "Sergey", ""], ["Abbeel", "Pieter", ""]]}, {"id": "1509.06825", "submitter": "Lerrel Pinto Mr", "authors": "Lerrel Pinto and Abhinav Gupta", "title": "Supersizing Self-supervision: Learning to Grasp from 50K Tries and 700\n  Robot Hours", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Current learning-based robot grasping approaches exploit human-labeled\ndatasets for training the models. However, there are two problems with such a\nmethodology: (a) since each object can be grasped in multiple ways, manually\nlabeling grasp locations is not a trivial task; (b) human labeling is biased by\nsemantics. While there have been attempts to train robots using trial-and-error\nexperiments, the amount of data used in such experiments remains substantially\nlow and hence makes the learner prone to over-fitting. In this paper, we take\nthe leap of increasing the available training data to 40 times more than prior\nwork, leading to a dataset size of 50K data points collected over 700 hours of\nrobot grasping attempts. This allows us to train a Convolutional Neural Network\n(CNN) for the task of predicting grasp locations without severe overfitting. In\nour formulation, we recast the regression problem to an 18-way binary\nclassification over image patches. We also present a multi-stage learning\napproach where a CNN trained in one stage is used to collect hard negatives in\nsubsequent stages. Our experiments clearly show the benefit of using\nlarge-scale datasets (and multi-stage training) for the task of grasping. We\nalso compare to several baselines and show state-of-the-art performance on\ngeneralization to unseen objects for grasping.\n", "versions": [{"version": "v1", "created": "Wed, 23 Sep 2015 02:08:02 GMT"}], "update_date": "2015-09-24", "authors_parsed": [["Pinto", "Lerrel", ""], ["Gupta", "Abhinav", ""]]}, {"id": "1509.06841", "submitter": "Justin Fu", "authors": "Justin Fu, Sergey Levine, Pieter Abbeel", "title": "One-Shot Learning of Manipulation Skills with Online Dynamics Adaptation\n  and Neural Network Priors", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One of the key challenges in applying reinforcement learning to complex\nrobotic control tasks is the need to gather large amounts of experience in\norder to find an effective policy for the task at hand. Model-based\nreinforcement learning can achieve good sample efficiency, but requires the\nability to learn a model of the dynamics that is good enough to learn an\neffective policy. In this work, we develop a model-based reinforcement learning\nalgorithm that combines prior knowledge from previous tasks with online\nadaptation of the dynamics model. These two ingredients enable highly\nsample-efficient learning even in regimes where estimating the true dynamics is\nvery difficult, since the online model adaptation allows the method to locally\ncompensate for unmodeled variation in the dynamics. We encode the prior\nexperience into a neural network dynamics model, adapt it online by\nprogressively refitting a local linear model of the dynamics, and use model\npredictive control to plan under these dynamics. Our experimental results show\nthat this approach can be used to solve a variety of complex robotic\nmanipulation tasks in just a single attempt, using prior data from other\nmanipulation behaviors.\n", "versions": [{"version": "v1", "created": "Wed, 23 Sep 2015 04:19:14 GMT"}, {"version": "v2", "created": "Tue, 15 Mar 2016 06:46:32 GMT"}, {"version": "v3", "created": "Thu, 11 Aug 2016 05:41:16 GMT"}], "update_date": "2016-08-12", "authors_parsed": [["Fu", "Justin", ""], ["Levine", "Sergey", ""], ["Abbeel", "Pieter", ""]]}, {"id": "1509.06893", "submitter": "Andrey Y. Lokhov", "authors": "Andrey Y. Lokhov, Theodor Misiakiewicz", "title": "Efficient reconstruction of transmission probabilities in a spreading\n  process from partial observations", "comments": "5 pages, 9 pages of supplemental material", "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.soc-ph cond-mat.stat-mech cs.LG cs.SI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An important problem of reconstruction of diffusion network and transmission\nprobabilities from the data has attracted a considerable attention in the past\nseveral years. A number of recent papers introduced efficient algorithms for\nthe estimation of spreading parameters, based on the maximization of the\nlikelihood of observed cascades, assuming that the full information for all the\nnodes in the network is available. In this work, we focus on a more realistic\nand restricted scenario, in which only a partial information on the cascades is\navailable: either the set of activation times for a limited number of nodes, or\nthe states of nodes for a subset of observation times. To tackle this problem,\nwe first introduce a framework based on the maximization of the likelihood of\nthe incomplete diffusion trace. However, we argue that the computation of this\nincomplete likelihood is a computationally hard problem, and show that a fast\nand robust reconstruction of transmission probabilities in sparse networks can\nbe achieved with a new algorithm based on recently introduced dynamic\nmessage-passing equations for the spreading processes. The suggested approach\ncan be easily generalized to a large class of discrete and continuous dynamic\nmodels, as well as to the cases of dynamically-changing networks and noisy\ninformation.\n", "versions": [{"version": "v1", "created": "Wed, 23 Sep 2015 09:14:34 GMT"}], "update_date": "2015-09-24", "authors_parsed": [["Lokhov", "Andrey Y.", ""], ["Misiakiewicz", "Theodor", ""]]}, {"id": "1509.06957", "submitter": "Ville Hyv\\\"onen", "authors": "Ville Hyv\\\"onen, Teemu Pitk\\\"anen, Sotiris Tasoulis, Elias\n  J\\\"a\\\"asaari, Risto Tuomainen, Liang Wang, Jukka Corander, Teemu Roos", "title": "Fast k-NN search", "comments": null, "journal-ref": "IEEE International Conference on Big Data 2016, p. 881-888", "doi": "10.1109/BigData.2016.7840682", "report-no": null, "categories": "stat.ML cs.DS cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Efficient index structures for fast approximate nearest neighbor queries are\nrequired in many applications such as recommendation systems. In\nhigh-dimensional spaces, many conventional methods suffer from excessive usage\nof memory and slow response times. We propose a method where multiple random\nprojection trees are combined by a novel voting scheme. The key idea is to\nexploit the redundancy in a large number of candidate sets obtained by\nindependently generated random projections in order to reduce the number of\nexpensive exact distance evaluations. The method is straightforward to\nimplement using sparse projections which leads to a reduced memory footprint\nand fast index construction. Furthermore, it enables grouping of the required\ncomputations into big matrix multiplications, which leads to additional savings\ndue to cache effects and low-level parallelization. We demonstrate by extensive\nexperiments on a wide variety of data sets that the method is faster than\nexisting partitioning tree or hashing based approaches, making it the fastest\navailable technique on high accuracy levels.\n", "versions": [{"version": "v1", "created": "Wed, 23 Sep 2015 13:10:36 GMT"}, {"version": "v2", "created": "Fri, 19 Aug 2016 12:54:40 GMT"}], "update_date": "2019-04-24", "authors_parsed": [["Hyv\u00f6nen", "Ville", ""], ["Pitk\u00e4nen", "Teemu", ""], ["Tasoulis", "Sotiris", ""], ["J\u00e4\u00e4saari", "Elias", ""], ["Tuomainen", "Risto", ""], ["Wang", "Liang", ""], ["Corander", "Jukka", ""], ["Roos", "Teemu", ""]]}, {"id": "1509.07065", "submitter": "Soumi Chaki", "authors": "Soumi Chaki, Aurobinda Routray and William K. Mohanty", "title": "A Novel Pre-processing Scheme to Improve the Prediction of Sand Fraction\n  from Seismic Attributes using Neural Networks", "comments": "13 pages, volume 8, no 4, pp. 1808-1820, April 2015 in IEE Journal of\n  Selected Topics in Applied Earth Observations and Remote Sensing, 2015", "journal-ref": null, "doi": "10.1109/JSTARS.2015.2404808", "report-no": null, "categories": "cs.CE cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a novel pre-processing scheme to improve the prediction\nof sand fraction from multiple seismic attributes such as seismic impedance,\namplitude and frequency using machine learning and information filtering. The\navailable well logs along with the 3-D seismic data have been used to benchmark\nthe proposed pre-processing stage using a methodology which primarily consists\nof three steps: pre-processing, training and post-processing. An Artificial\nNeural Network (ANN) with conjugate-gradient learning algorithm has been used\nto model the sand fraction. The available sand fraction data from the high\nresolution well logs has far more information content than the low resolution\nseismic attributes. Therefore, regularization schemes based on Fourier\nTransform (FT), Wavelet Decomposition (WD) and Empirical Mode Decomposition\n(EMD) have been proposed to shape the high resolution sand fraction data for\neffective machine learning. The input data sets have been segregated into\ntraining, testing and validation sets. The test results are primarily used to\ncheck different network structures and activation function performances. Once\nthe network passes the testing phase with an acceptable performance in terms of\nthe selected evaluators, the validation phase follows. In the validation stage,\nthe prediction model is tested against unseen data. The network yielding\nsatisfactory performance in the validation stage is used to predict\nlithological properties from seismic attributes throughout a given volume.\nFinally, a post-processing scheme using 3-D spatial filtering is implemented\nfor smoothing the sand fraction in the volume. Prediction of lithological\nproperties using this framework is helpful for Reservoir Characterization.\n", "versions": [{"version": "v1", "created": "Wed, 23 Sep 2015 17:15:21 GMT"}], "update_date": "2015-10-06", "authors_parsed": [["Chaki", "Soumi", ""], ["Routray", "Aurobinda", ""], ["Mohanty", "William K.", ""]]}, {"id": "1509.07078", "submitter": "Kelum Gajamannage", "authors": "Kelum Gajamannage, Erik M. Bollt", "title": "Detecting phase transitions in collective behavior using manifold's\n  curvature", "comments": "17 pages, 9 figures, accepted in Journal of Mathematical Bioscience\n  and Engineering", "journal-ref": null, "doi": "10.3934/mbe.2017027", "report-no": null, "categories": "math.DS cs.LG cs.MA math.GT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  If a given behavior of a multi-agent system restricts the phase variable to a\ninvariant manifold, then we define a phase transition as change of physical\ncharacteristics such as speed, coordination, and structure. We define such a\nphase transition as splitting an underlying manifold into two sub-manifolds\nwith distinct dimensionalities around the singularity where the phase\ntransition physically exists. Here, we propose a method of detecting phase\ntransitions and splitting the manifold into phase transitions free\nsub-manifolds. Therein, we utilize a relationship between curvature and\nsingular value ratio of points sampled in a curve, and then extend the\nassertion into higher-dimensions using the shape operator. Then we attest that\nthe same phase transition can also be approximated by singular value ratios\ncomputed locally over the data in a neighborhood on the manifold. We validate\nthe phase transitions detection method using one particle simulation and three\nreal world examples.\n", "versions": [{"version": "v1", "created": "Wed, 23 Sep 2015 18:04:56 GMT"}, {"version": "v2", "created": "Thu, 15 Sep 2016 16:41:30 GMT"}], "update_date": "2017-07-21", "authors_parsed": [["Gajamannage", "Kelum", ""], ["Bollt", "Erik M.", ""]]}, {"id": "1509.07087", "submitter": "Zhe Gan", "authors": "Zhe Gan, Chunyuan Li, Ricardo Henao, David Carlson and Lawrence Carin", "title": "Deep Temporal Sigmoid Belief Networks for Sequence Modeling", "comments": "to appear in NIPS 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep dynamic generative models are developed to learn sequential dependencies\nin time-series data. The multi-layered model is designed by constructing a\nhierarchy of temporal sigmoid belief networks (TSBNs), defined as a sequential\nstack of sigmoid belief networks (SBNs). Each SBN has a contextual hidden\nstate, inherited from the previous SBNs in the sequence, and is used to\nregulate its hidden bias. Scalable learning and inference algorithms are\nderived by introducing a recognition model that yields fast sampling from the\nvariational posterior. This recognition model is trained jointly with the\ngenerative model, by maximizing its variational lower bound on the\nlog-likelihood. Experimental results on bouncing balls, polyphonic music,\nmotion capture, and text streams show that the proposed approach achieves\nstate-of-the-art predictive performance, and has the capacity to synthesize\nvarious sequences.\n", "versions": [{"version": "v1", "created": "Wed, 23 Sep 2015 18:36:42 GMT"}], "update_date": "2015-09-24", "authors_parsed": [["Gan", "Zhe", ""], ["Li", "Chunyuan", ""], ["Henao", "Ricardo", ""], ["Carlson", "David", ""], ["Carin", "Lawrence", ""]]}, {"id": "1509.07093", "submitter": "Pablo Estevez Prof.", "authors": "David Nova and Pablo A. Estevez", "title": "A review of learning vector quantization classifiers", "comments": "14 pages", "journal-ref": "Neural Computing & Applications, vol. 25, pp. 511-524, 2014", "doi": "10.1007/s00521-013-1535-3", "report-no": null, "categories": "cs.LG astro-ph.IM cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work we present a review of the state of the art of Learning Vector\nQuantization (LVQ) classifiers. A taxonomy is proposed which integrates the\nmost relevant LVQ approaches to date. The main concepts associated with modern\nLVQ approaches are defined. A comparison is made among eleven LVQ classifiers\nusing one real-world and two artificial datasets.\n", "versions": [{"version": "v1", "created": "Wed, 23 Sep 2015 18:46:31 GMT"}], "update_date": "2015-09-24", "authors_parsed": [["Nova", "David", ""], ["Estevez", "Pablo A.", ""]]}, {"id": "1509.07107", "submitter": "David Vaughn", "authors": "David Vaughn, Derek Justice", "title": "On The Direct Maximization of Quadratic Weighted Kappa", "comments": "realized some inaccuracies, and some sloppy reasoning. Need some time\n  to fix", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, quadratic weighted kappa has been growing in popularity in\nthe machine learning community as an evaluation metric in domains where the\ntarget labels to be predicted are drawn from integer ratings, usually obtained\nfrom human experts. For example, it was the metric of choice in several recent,\nhigh profile machine learning contests hosted on Kaggle :\nhttps://www.kaggle.com/c/asap-aes , https://www.kaggle.com/c/asap-sas ,\nhttps://www.kaggle.com/c/diabetic-retinopathy-detection . Yet, little is\nunderstood about the nature of this metric, its underlying mathematical\nproperties, where it fits among other common evaluation metrics such as mean\nsquared error (MSE) and correlation, or if it can be optimized analytically,\nand if so, how. Much of this is due to the cumbersome way that this metric is\ncommonly defined. In this paper we first derive an equivalent but much simpler,\nand more useful, definition for quadratic weighted kappa, and then employ this\nalternate form to address the above issues.\n", "versions": [{"version": "v1", "created": "Wed, 23 Sep 2015 19:39:39 GMT"}, {"version": "v2", "created": "Tue, 29 Sep 2015 21:30:43 GMT"}, {"version": "v3", "created": "Sun, 6 Dec 2015 15:16:19 GMT"}], "update_date": "2015-12-08", "authors_parsed": [["Vaughn", "David", ""], ["Justice", "Derek", ""]]}, {"id": "1509.07179", "submitter": "Kai-Wei Chang", "authors": "Kai-Wei Chang and Shyam Upadhyay and Ming-Wei Chang and Vivek Srikumar\n  and Dan Roth", "title": "IllinoisSL: A JAVA Library for Structured Prediction", "comments": "http://cogcomp.cs.illinois.edu/software/illinois-sl", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  IllinoisSL is a Java library for learning structured prediction models. It\nsupports structured Support Vector Machines and structured Perceptron. The\nlibrary consists of a core learning module and several applications, which can\nbe executed from command-lines. Documentation is provided to guide users. In\nComparison to other structured learning libraries, IllinoisSL is efficient,\ngeneral, and easy to use.\n", "versions": [{"version": "v1", "created": "Wed, 23 Sep 2015 23:22:38 GMT"}], "update_date": "2015-09-25", "authors_parsed": [["Chang", "Kai-Wei", ""], ["Upadhyay", "Shyam", ""], ["Chang", "Ming-Wei", ""], ["Srikumar", "Vivek", ""], ["Roth", "Dan", ""]]}, {"id": "1509.07234", "submitter": "Yin Ding", "authors": "Yin Ding and Ivan W. Selesnick", "title": "Sparsity-based Correction of Exponential Artifacts", "comments": null, "journal-ref": null, "doi": "10.1016/j.sigpro.2015.09.017", "report-no": null, "categories": "cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper describes an exponential transient excision algorithm (ETEA). In\nbiomedical time series analysis, e.g., in vivo neural recording and\nelectrocorticography (ECoG), some measurement artifacts take the form of\npiecewise exponential transients. The proposed method is formulated as an\nunconstrained convex optimization problem, regularized by smoothed l1-norm\npenalty function, which can be solved by majorization-minimization (MM) method.\nWith a slight modification of the regularizer, ETEA can also suppress more\nirregular piecewise smooth artifacts, especially, ocular artifacts (OA) in\nelectroencephalog- raphy (EEG) data. Examples of synthetic signal, EEG data,\nand ECoG data are presented to illustrate the proposed algorithms.\n", "versions": [{"version": "v1", "created": "Thu, 24 Sep 2015 04:50:00 GMT"}], "update_date": "2015-09-25", "authors_parsed": [["Ding", "Yin", ""], ["Selesnick", "Ivan W.", ""]]}, {"id": "1509.07385", "submitter": "Uri Shaham", "authors": "Uri Shaham, Alexander Cloninger, Ronald R. Coifman", "title": "Provable approximation properties for deep neural networks", "comments": "accepted for publication in Applied and Computational Harmonic\n  Analysis", "journal-ref": null, "doi": "10.1016/j.acha.2016.04.003", "report-no": null, "categories": "stat.ML cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We discuss approximation of functions using deep neural nets. Given a\nfunction $f$ on a $d$-dimensional manifold $\\Gamma \\subset \\mathbb{R}^m$, we\nconstruct a sparsely-connected depth-4 neural network and bound its error in\napproximating $f$. The size of the network depends on dimension and curvature\nof the manifold $\\Gamma$, the complexity of $f$, in terms of its wavelet\ndescription, and only weakly on the ambient dimension $m$. Essentially, our\nnetwork computes wavelet functions, which are computed from Rectified Linear\nUnits (ReLU)\n", "versions": [{"version": "v1", "created": "Thu, 24 Sep 2015 14:20:29 GMT"}, {"version": "v2", "created": "Thu, 1 Oct 2015 14:31:54 GMT"}, {"version": "v3", "created": "Mon, 28 Mar 2016 13:46:06 GMT"}], "update_date": "2017-04-24", "authors_parsed": [["Shaham", "Uri", ""], ["Cloninger", "Alexander", ""], ["Coifman", "Ronald R.", ""]]}, {"id": "1509.07422", "submitter": "Craig Wilson", "authors": "Craig Wilson and Venugopal V. Veeravalli", "title": "Adaptive Sequential Optimization with Applications to Machine Learning", "comments": "submitted to ICASSP 2016, extended version", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A framework is introduced for solving a sequence of slowly changing\noptimization problems, including those arising in regression and classification\napplications, using optimization algorithms such as stochastic gradient descent\n(SGD). The optimization problems change slowly in the sense that the minimizers\nchange at either a fixed or bounded rate. A method based on estimates of the\nchange in the minimizers and properties of the optimization algorithm is\nintroduced for adaptively selecting the number of samples needed from the\ndistributions underlying each problem in order to ensure that the excess risk,\ni.e., the expected gap between the loss achieved by the approximate minimizer\nproduced by the optimization algorithm and the exact minimizer, does not exceed\na target level. Experiments with synthetic and real data are used to confirm\nthat this approach performs well.\n", "versions": [{"version": "v1", "created": "Thu, 24 Sep 2015 16:19:43 GMT"}], "update_date": "2015-09-25", "authors_parsed": [["Wilson", "Craig", ""], ["Veeravalli", "Venugopal V.", ""]]}, {"id": "1509.07450", "submitter": "Yi Chen", "authors": "Yi Chen, Enyi Yao, Arindam Basu", "title": "A 128 channel Extreme Learning Machine based Neural Decoder for Brain\n  Machine Interfaces", "comments": "13 pages, 17 figures, accepted by IEEE Transactions on Biomedical\n  Circuits and Systems, 2015", "journal-ref": null, "doi": "10.1109/TBCAS.2015.2483618", "report-no": null, "categories": "cs.LG cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Currently, state-of-the-art motor intention decoding algorithms in\nbrain-machine interfaces are mostly implemented on a PC and consume significant\namount of power. A machine learning co-processor in 0.35um CMOS for motor\nintention decoding in brain-machine interfaces is presented in this paper.\nUsing Extreme Learning Machine algorithm and low-power analog processing, it\nachieves an energy efficiency of 290 GMACs/W at a classification rate of 50 Hz.\nThe learning in second stage and corresponding digitally stored coefficients\nare used to increase robustness of the core analog processor. The chip is\nverified with neural data recorded in monkey finger movements experiment,\nachieving a decoding accuracy of 99.3% for movement type. The same co-processor\nis also used to decode time of movement from asynchronous neural spikes. With\ntime-delayed feature dimension enhancement, the classification accuracy can be\nincreased by 5% with limited number of input channels. Further, a sparsity\npromoting training scheme enables reduction of number of programmable weights\nby ~2X.\n", "versions": [{"version": "v1", "created": "Tue, 22 Sep 2015 06:30:16 GMT"}, {"version": "v2", "created": "Sun, 27 Sep 2015 07:39:03 GMT"}], "update_date": "2016-11-15", "authors_parsed": [["Chen", "Yi", ""], ["Yao", "Enyi", ""], ["Basu", "Arindam", ""]]}, {"id": "1509.07481", "submitter": "Zhiguang Wang", "authors": "Zhiguang Wang and Tim Oates", "title": "Spatially Encoding Temporal Correlations to Classify Temporal Data Using\n  Convolutional Neural Networks", "comments": "Submit to JCSS. Preliminary versions are appeared in AAAI 2015\n  workshop and IJCAI 2016 [arXiv:1506.00327]", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose an off-line approach to explicitly encode temporal patterns\nspatially as different types of images, namely, Gramian Angular Fields and\nMarkov Transition Fields. This enables the use of techniques from computer\nvision for feature learning and classification. We used Tiled Convolutional\nNeural Networks to learn high-level features from individual GAF, MTF, and\nGAF-MTF images on 12 benchmark time series datasets and two real\nspatial-temporal trajectory datasets. The classification results of our\napproach are competitive with state-of-the-art approaches on both types of\ndata. An analysis of the features and weights learned by the CNNs explains why\nthe approach works.\n", "versions": [{"version": "v1", "created": "Thu, 24 Sep 2015 19:14:20 GMT"}], "update_date": "2015-09-25", "authors_parsed": [["Wang", "Zhiguang", ""], ["Oates", "Tim", ""]]}, {"id": "1509.07553", "submitter": "Danica J. Sutherland", "authors": "Danica J. Sutherland and Junier B. Oliva and Barnab\\'as P\\'oczos and\n  Jeff Schneider", "title": "Linear-time Learning on Distributions with Approximate Kernel Embeddings", "comments": null, "journal-ref": "AAAI'16: Proceedings of the Thirtieth AAAI Conference on\n  Artificial Intelligence, February 2016, 2073-2079", "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many interesting machine learning problems are best posed by considering\ninstances that are distributions, or sample sets drawn from distributions.\nPrevious work devoted to machine learning tasks with distributional inputs has\ndone so through pairwise kernel evaluations between pdfs (or sample sets).\nWhile such an approach is fine for smaller datasets, the computation of an $N\n\\times N$ Gram matrix is prohibitive in large datasets. Recent scalable\nestimators that work over pdfs have done so only with kernels that use\nEuclidean metrics, like the $L_2$ distance. However, there are a myriad of\nother useful metrics available, such as total variation, Hellinger distance,\nand the Jensen-Shannon divergence. This work develops the first random features\nfor pdfs whose dot product approximates kernels using these non-Euclidean\nmetrics, allowing estimators using such kernels to scale to large datasets by\nworking in a primal space, without computing large Gram matrices. We provide an\nanalysis of the approximation error in using our proposed random features and\nshow empirically the quality of our approximation both in estimating a Gram\nmatrix and in solving learning tasks in real-world and synthetic data.\n", "versions": [{"version": "v1", "created": "Thu, 24 Sep 2015 22:26:02 GMT"}, {"version": "v2", "created": "Thu, 14 Jan 2021 06:30:48 GMT"}], "update_date": "2021-01-15", "authors_parsed": [["Sutherland", "Danica J.", ""], ["Oliva", "Junier B.", ""], ["P\u00f3czos", "Barnab\u00e1s", ""], ["Schneider", "Jeff", ""]]}, {"id": "1509.07577", "submitter": "Jorge R. Vergara", "authors": "Jorge R. Vergara, Pablo A. Est\\'evez", "title": "A Review of Feature Selection Methods Based on Mutual Information", "comments": null, "journal-ref": "Neural Computing & Applications, vol. 24 (1), pp. 175-186, 2014", "doi": "10.1007/s00521-013-1368-0", "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work we present a review of the state of the art of information\ntheoretic feature selection methods. The concepts of feature relevance,\nredundance and complementarity (synergy) are clearly defined, as well as Markov\nblanket. The problem of optimal feature selection is defined. A unifying\ntheoretical framework is described, which can retrofit successful heuristic\ncriteria, indicating the approximations made by each method. A number of open\nproblems in the field are presented.\n", "versions": [{"version": "v1", "created": "Thu, 24 Sep 2015 16:36:10 GMT"}], "update_date": "2015-09-28", "authors_parsed": [["Vergara", "Jorge R.", ""], ["Est\u00e9vez", "Pablo A.", ""]]}, {"id": "1509.07728", "submitter": "Lijun Zhang", "authors": "Lijun Zhang, Tianbao Yang, Rong Jin, Zhi-Hua Zhou", "title": "Online Stochastic Linear Optimization under One-bit Feedback", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we study a special bandit setting of online stochastic linear\noptimization, where only one-bit of information is revealed to the learner at\neach round. This problem has found many applications including online\nadvertisement and online recommendation. We assume the binary feedback is a\nrandom variable generated from the logit model, and aim to minimize the regret\ndefined by the unknown linear function. Although the existing method for\ngeneralized linear bandit can be applied to our problem, the high computational\ncost makes it impractical for real-world problems. To address this challenge,\nwe develop an efficient online learning algorithm by exploiting particular\nstructures of the observation model. Specifically, we adopt online Newton step\nto estimate the unknown parameter and derive a tight confidence region based on\nthe exponential concavity of the logistic loss. Our analysis shows that the\nproposed algorithm achieves a regret bound of $O(d\\sqrt{T})$, which matches the\noptimal result of stochastic linear bandits.\n", "versions": [{"version": "v1", "created": "Fri, 25 Sep 2015 14:05:09 GMT"}], "update_date": "2015-09-28", "authors_parsed": [["Zhang", "Lijun", ""], ["Yang", "Tianbao", ""], ["Jin", "Rong", ""], ["Zhou", "Zhi-Hua", ""]]}, {"id": "1509.07755", "submitter": "Li Heng Liou", "authors": "Cheng-Shang Chang, Wanjiun Liao, Yu-Sheng Chen, and Li-Heng Liou", "title": "A Mathematical Theory for Clustering in Metric Spaces", "comments": null, "journal-ref": null, "doi": "10.1109/TNSE.2016.2516339", "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Clustering is one of the most fundamental problems in data analysis and it\nhas been studied extensively in the literature. Though many clustering\nalgorithms have been proposed, clustering theories that justify the use of\nthese clustering algorithms are still unsatisfactory. In particular, one of the\nfundamental challenges is to address the following question:\n  What is a cluster in a set of data points?\n  In this paper, we make an attempt to address such a question by considering a\nset of data points associated with a distance measure (metric). We first\npropose a new cohesion measure in terms of the distance measure. Using the\ncohesion measure, we define a cluster as a set of points that are cohesive to\nthemselves. For such a definition, we show there are various equivalent\nstatements that have intuitive explanations. We then consider the second\nquestion:\n  How do we find clusters and good partitions of clusters under such a\ndefinition?\n  For such a question, we propose a hierarchical agglomerative algorithm and a\npartitional algorithm. Unlike standard hierarchical agglomerative algorithms,\nour hierarchical agglomerative algorithm has a specific stopping criterion and\nit stops with a partition of clusters. Our partitional algorithm, called the\nK-sets algorithm in the paper, appears to be a new iterative algorithm. Unlike\nthe Lloyd iteration that needs two-step minimization, our K-sets algorithm only\ntakes one-step minimization.\n  One of the most interesting findings of our paper is the duality result\nbetween a distance measure and a cohesion measure. Such a duality result leads\nto a dual K-sets algorithm for clustering a set of data points with a cohesion\nmeasure. The dual K-sets algorithm converges in the same way as a sequential\nversion of the classical kernel K-means algorithm. The key difference is that a\ncohesion measure does not need to be positive semi-definite.\n", "versions": [{"version": "v1", "created": "Fri, 25 Sep 2015 15:30:18 GMT"}], "update_date": "2016-02-22", "authors_parsed": [["Chang", "Cheng-Shang", ""], ["Liao", "Wanjiun", ""], ["Chen", "Yu-Sheng", ""], ["Liou", "Li-Heng", ""]]}, {"id": "1509.07823", "submitter": "Pablo Huijse Ph.D", "authors": "Pablo Huijse and Pablo A. Estevez and Pavlos Protopapas and Jose C.\n  Principe and Pablo Zegers", "title": "Computational Intelligence Challenges and Applications on Large-Scale\n  Astronomical Time Series Databases", "comments": null, "journal-ref": "IEEE Computational Intelligence Magazine, vol. 9, n. 3, pp. 27-39,\n  2014", "doi": "10.1109/MCI.2014.2326100", "report-no": null, "categories": "astro-ph.IM cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Time-domain astronomy (TDA) is facing a paradigm shift caused by the\nexponential growth of the sample size, data complexity and data generation\nrates of new astronomical sky surveys. For example, the Large Synoptic Survey\nTelescope (LSST), which will begin operations in northern Chile in 2022, will\ngenerate a nearly 150 Petabyte imaging dataset of the southern hemisphere sky.\nThe LSST will stream data at rates of 2 Terabytes per hour, effectively\ncapturing an unprecedented movie of the sky. The LSST is expected not only to\nimprove our understanding of time-varying astrophysical objects, but also to\nreveal a plethora of yet unknown faint and fast-varying phenomena. To cope with\na change of paradigm to data-driven astronomy, the fields of astroinformatics\nand astrostatistics have been created recently. The new data-oriented paradigms\nfor astronomy combine statistics, data mining, knowledge discovery, machine\nlearning and computational intelligence, in order to provide the automated and\nrobust methods needed for the rapid detection and classification of known\nastrophysical objects as well as the unsupervised characterization of novel\nphenomena. In this article we present an overview of machine learning and\ncomputational intelligence applications to TDA. Future big data challenges and\nnew lines of research in TDA, focusing on the LSST, are identified and\ndiscussed from the viewpoint of computational intelligence/machine learning.\nInterdisciplinary collaboration will be required to cope with the challenges\nposed by the deluge of astronomical data coming from the LSST.\n", "versions": [{"version": "v1", "created": "Fri, 25 Sep 2015 18:24:48 GMT"}], "update_date": "2015-09-28", "authors_parsed": [["Huijse", "Pablo", ""], ["Estevez", "Pablo A.", ""], ["Protopapas", "Pavlos", ""], ["Principe", "Jose C.", ""], ["Zegers", "Pablo", ""]]}, {"id": "1509.07831", "submitter": "Jaeyong Sung", "authors": "Jaeyong Sung, Ian Lenz, Ashutosh Saxena", "title": "Deep Multimodal Embedding: Manipulating Novel Objects with Point-clouds,\n  Language and Trajectories", "comments": "IEEE International Conference on Robotics and Automation (ICRA), 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AI cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A robot operating in a real-world environment needs to perform reasoning over\na variety of sensor modalities such as vision, language and motion\ntrajectories. However, it is extremely challenging to manually design features\nrelating such disparate modalities. In this work, we introduce an algorithm\nthat learns to embed point-cloud, natural language, and manipulation trajectory\ndata into a shared embedding space with a deep neural network. To learn\nsemantically meaningful spaces throughout our network, we use a loss-based\nmargin to bring embeddings of relevant pairs closer together while driving\nless-relevant cases from different modalities further apart. We use this both\nto pre-train its lower layers and fine-tune our final embedding space, leading\nto a more robust representation. We test our algorithm on the task of\nmanipulating novel objects and appliances based on prior experience with other\nobjects. On a large dataset, we achieve significant improvements in both\naccuracy and inference time over the previous state of the art. We also perform\nend-to-end experiments on a PR2 robot utilizing our learned embedding space.\n", "versions": [{"version": "v1", "created": "Fri, 25 Sep 2015 18:55:45 GMT"}, {"version": "v2", "created": "Wed, 17 May 2017 15:12:33 GMT"}], "update_date": "2017-05-18", "authors_parsed": [["Sung", "Jaeyong", ""], ["Lenz", "Ian", ""], ["Saxena", "Ashutosh", ""]]}, {"id": "1509.07892", "submitter": "Alex Kantchelian", "authors": "Alex Kantchelian, J. D. Tygar, Anthony D. Joseph", "title": "Evasion and Hardening of Tree Ensemble Classifiers", "comments": "11 pages, 7 figures, Appears in Proceedings of the 33rd International\n  Conference on Machine Learning (ICML), New York, NY, USA, 2016. JMLR: W&CP\n  volume 48", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Classifier evasion consists in finding for a given instance $x$ the nearest\ninstance $x'$ such that the classifier predictions of $x$ and $x'$ are\ndifferent. We present two novel algorithms for systematically computing\nevasions for tree ensembles such as boosted trees and random forests. Our first\nalgorithm uses a Mixed Integer Linear Program solver and finds the optimal\nevading instance under an expressive set of constraints. Our second algorithm\ntrades off optimality for speed by using symbolic prediction, a novel algorithm\nfor fast finite differences on tree ensembles. On a digit recognition task, we\ndemonstrate that both gradient boosted trees and random forests are extremely\nsusceptible to evasions. Finally, we harden a boosted tree model without loss\nof predictive accuracy by augmenting the training set of each boosting round\nwith evading instances, a technique we call adversarial boosting.\n", "versions": [{"version": "v1", "created": "Fri, 25 Sep 2015 20:57:35 GMT"}, {"version": "v2", "created": "Fri, 27 May 2016 01:09:22 GMT"}], "update_date": "2016-05-30", "authors_parsed": [["Kantchelian", "Alex", ""], ["Tygar", "J. D.", ""], ["Joseph", "Anthony D.", ""]]}, {"id": "1509.07927", "submitter": "Manjesh Kumar Hanawal", "authors": "Manjesh K. Hanawal and Amir Leshem and Venkatesh Saligrama", "title": "Algorithms for Linear Bandits on Polyhedral Sets", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study stochastic linear optimization problem with bandit feedback. The set\nof arms take values in an $N$-dimensional space and belong to a bounded\npolyhedron described by finitely many linear inequalities. We provide a lower\nbound for the expected regret that scales as $\\Omega(N\\log T)$. We then provide\na nearly optimal algorithm and show that its expected regret scales as\n$O(N\\log^{1+\\epsilon}(T))$ for an arbitrary small $\\epsilon >0$. The algorithm\nalternates between exploration and exploitation intervals sequentially where\ndeterministic set of arms are played in the exploration intervals and greedily\nselected arm is played in the exploitation intervals. We also develop an\nalgorithm that achieves the optimal regret when sub-Gaussianity parameter of\nthe noise term is known. Our key insight is that for a polyhedron the optimal\narm is robust to small perturbations in the reward function. Consequently, a\ngreedily selected arm is guaranteed to be optimal when the estimation error\nfalls below some suitable threshold. Our solution resolves a question posed by\nRusmevichientong and Tsitsiklis (2011) that left open the possibility of\nefficient algorithms with asymptotic logarithmic regret bounds. We also show\nthat the regret upper bounds hold with probability $1$. Our numerical\ninvestigations show that while theoretical results are asymptotic the\nperformance of our algorithms compares favorably to state-of-the-art algorithms\nin finite time as well.\n", "versions": [{"version": "v1", "created": "Sat, 26 Sep 2015 00:17:38 GMT"}], "update_date": "2015-09-29", "authors_parsed": [["Hanawal", "Manjesh K.", ""], ["Leshem", "Amir", ""], ["Saligrama", "Venkatesh", ""]]}, {"id": "1509.07943", "submitter": "Qingqing Huang", "authors": "Qingqing Huang, Sham M. Kakade", "title": "Super-Resolution Off the Grid", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Super-resolution is the problem of recovering a superposition of point\nsources using bandlimited measurements, which may be corrupted with noise. This\nsignal processing problem arises in numerous imaging problems, ranging from\nastronomy to biology to spectroscopy, where it is common to take (coarse)\nFourier measurements of an object. Of particular interest is in obtaining\nestimation procedures which are robust to noise, with the following desirable\nstatistical and computational properties: we seek to use coarse Fourier\nmeasurements (bounded by some cutoff frequency); we hope to take a\n(quantifiably) small number of measurements; we desire our algorithm to run\nquickly.\n  Suppose we have k point sources in d dimensions, where the points are\nseparated by at least \\Delta from each other (in Euclidean distance). This work\nprovides an algorithm with the following favorable guarantees: - The algorithm\nuses Fourier measurements, whose frequencies are bounded by O(1/\\Delta) (up to\nlog factors). Previous algorithms require a cutoff frequency which may be as\nlarge as {\\Omega}( d/\\Delta). - The number of measurements taken by and the\ncomputational complexity of our algorithm are bounded by a polynomial in both\nthe number of points k and the dimension d, with no dependence on the\nseparation \\Delta. In contrast, previous algorithms depended inverse\npolynomially on the minimal separation and exponentially on the dimension for\nboth of these quantities.\n  Our estimation procedure itself is simple: we take random bandlimited\nmeasurements (as opposed to taking an exponential number of measurements on the\nhyper-grid). Furthermore, our analysis and algorithm are elementary (based on\nconcentration bounds for sampling and the singular value decomposition).\n", "versions": [{"version": "v1", "created": "Sat, 26 Sep 2015 03:49:27 GMT"}], "update_date": "2015-09-29", "authors_parsed": [["Huang", "Qingqing", ""], ["Kakade", "Sham M.", ""]]}, {"id": "1509.07975", "submitter": "Yogesh Girdhar Yogesh Girdhar", "authors": "Yogesh Girdhar, Gregory Dudek", "title": "Modeling Curiosity in a Mobile Robot for Long-Term Autonomous\n  Exploration and Monitoring", "comments": "20 pages, in-press, Autonomous Robots, 2015. arXiv admin note:\n  substantial text overlap with arXiv:1310.6767", "journal-ref": null, "doi": "10.1007/s10514-015-9500-x", "report-no": null, "categories": "cs.RO cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a novel approach to modeling curiosity in a mobile robot,\nwhich is useful for monitoring and adaptive data collection tasks, especially\nin the context of long term autonomous missions where pre-programmed missions\nare likely to have limited utility. We use a realtime topic modeling technique\nto build a semantic perception model of the environment, using which, we plan a\npath through the locations in the world with high semantic information content.\nThe life-long learning behavior of the proposed perception model makes it\nsuitable for long-term exploration missions. We validate the approach using\nsimulated exploration experiments using aerial and underwater data, and\ndemonstrate an implementation on the Aqua underwater robot in a variety of\nscenarios. We find that the proposed exploration paths that are biased towards\nlocations with high topic perplexity, produce better terrain models with high\ndiscriminative power. Moreover, we show that the proposed algorithm implemented\non Aqua robot is able to do tasks such as coral reef inspection, diver\nfollowing, and sea floor exploration, without any prior training or\npreparation.\n", "versions": [{"version": "v1", "created": "Sat, 26 Sep 2015 13:16:52 GMT"}], "update_date": "2015-09-30", "authors_parsed": [["Girdhar", "Yogesh", ""], ["Dudek", "Gregory", ""]]}, {"id": "1509.07983", "submitter": "Soledad Villar", "authors": "Takayuki Iguchi, Dustin G. Mixon, Jesse Peterson, Soledad Villar", "title": "Probably certifiably correct k-means clustering", "comments": "Major revision from previous version. This paper is a extension of\n  and improvement to the authors' preprint [arXiv:1505.04778]", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.DS cs.LG math.IT math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, Bandeira [arXiv:1509.00824] introduced a new type of algorithm (the\nso-called probably certifiably correct algorithm) that combines fast solvers\nwith the optimality certificates provided by convex relaxations. In this paper,\nwe devise such an algorithm for the problem of k-means clustering. First, we\nprove that Peng and Wei's semidefinite relaxation of k-means is tight with high\nprobability under a distribution of planted clusters called the stochastic ball\nmodel. Our proof follows from a new dual certificate for integral solutions of\nthis semidefinite program. Next, we show how to test the optimality of a\nproposed k-means solution using this dual certificate in quasilinear time.\nFinally, we analyze a version of spectral clustering from Peng and Wei that is\ndesigned to solve k-means in the case of two clusters. In particular, we show\nthat this quasilinear-time method typically recovers planted clusters under the\nstochastic ball model.\n", "versions": [{"version": "v1", "created": "Sat, 26 Sep 2015 14:09:56 GMT"}, {"version": "v2", "created": "Sat, 23 Apr 2016 19:55:12 GMT"}], "update_date": "2016-04-26", "authors_parsed": [["Iguchi", "Takayuki", ""], ["Mixon", "Dustin G.", ""], ["Peterson", "Jesse", ""], ["Villar", "Soledad", ""]]}, {"id": "1509.08038", "submitter": "Wentao Zhu", "authors": "Wentao Zhu, Jun Miao, Laiyun Qing, Xilin Chen", "title": "Deep Trans-layer Unsupervised Networks for Representation Learning", "comments": "21 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning features from massive unlabelled data is a vast prevalent topic for\nhigh-level tasks in many machine learning applications. The recent great\nimprovements on benchmark data sets achieved by increasingly complex\nunsupervised learning methods and deep learning models with lots of parameters\nusually requires many tedious tricks and much expertise to tune. However,\nfilters learned by these complex architectures are quite similar to standard\nhand-crafted features visually. In this paper, unsupervised learning methods,\nsuch as PCA or auto-encoder, are employed as the building block to learn filter\nbanks at each layer. The lower layer responses are transferred to the last\nlayer (trans-layer) to form a more complete representation retaining more\ninformation. In addition, some beneficial methods such as local contrast\nnormalization and whitening are added to the proposed deep trans-layer networks\nto further boost performance. The trans-layer representations are followed by\nblock histograms with binary encoder schema to learn translation and rotation\ninvariant representations, which are utilized to do high-level tasks such as\nrecognition and classification. Compared to traditional deep learning methods,\nthe implemented feature learning method has much less parameters and is\nvalidated in several typical experiments, such as digit recognition on MNIST\nand MNIST variations, object recognition on Caltech 101 dataset and face\nverification on LFW dataset. The deep trans-layer unsupervised learning\nachieves 99.45% accuracy on MNIST dataset, 67.11% accuracy on 15 samples per\nclass and 75.98% accuracy on 30 samples per class on Caltech 101 dataset,\n87.10% on LFW dataset.\n", "versions": [{"version": "v1", "created": "Sun, 27 Sep 2015 00:46:08 GMT"}], "update_date": "2015-09-29", "authors_parsed": [["Zhu", "Wentao", ""], ["Miao", "Jun", ""], ["Qing", "Laiyun", ""], ["Chen", "Xilin", ""]]}, {"id": "1509.08062", "submitter": "Georg Heigold", "authors": "Georg Heigold, Ignacio Moreno, Samy Bengio, Noam Shazeer", "title": "End-to-End Text-Dependent Speaker Verification", "comments": "submitted to ICASSP 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.SD", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we present a data-driven, integrated approach to speaker\nverification, which maps a test utterance and a few reference utterances\ndirectly to a single score for verification and jointly optimizes the system's\ncomponents using the same evaluation protocol and metric as at test time. Such\nan approach will result in simple and efficient systems, requiring little\ndomain-specific knowledge and making few model assumptions. We implement the\nidea by formulating the problem as a single neural network architecture,\nincluding the estimation of a speaker model on only a few utterances, and\nevaluate it on our internal \"Ok Google\" benchmark for text-dependent speaker\nverification. The proposed approach appears to be very effective for big data\napplications like ours that require highly accurate, easy-to-maintain systems\nwith a small footprint.\n", "versions": [{"version": "v1", "created": "Sun, 27 Sep 2015 07:43:36 GMT"}], "update_date": "2015-09-29", "authors_parsed": [["Heigold", "Georg", ""], ["Moreno", "Ignacio", ""], ["Bengio", "Samy", ""], ["Shazeer", "Noam", ""]]}, {"id": "1509.08067", "submitter": "Tianfu Wu", "authors": "Tianfu Wu and Yang Lu and Song-Chun Zhu", "title": "Online Object Tracking, Learning and Parsing with And-Or Graphs", "comments": "17 pages, Reproducibility: The source code is released with this\n  paper for reproducing all results, which is available at\n  https://github.com/tfwu/RGM-AOGTracker", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a method, called AOGTracker, for simultaneously tracking,\nlearning and parsing (TLP) of unknown objects in video sequences with a\nhierarchical and compositional And-Or graph (AOG) representation. %The AOG\ncaptures both structural and appearance variations of a target object in a\nprincipled way. The TLP method is formulated in the Bayesian framework with a\nspatial and a temporal dynamic programming (DP) algorithms inferring object\nbounding boxes on-the-fly. During online learning, the AOG is discriminatively\nlearned using latent SVM to account for appearance (e.g., lighting and partial\nocclusion) and structural (e.g., different poses and viewpoints) variations of\na tracked object, as well as distractors (e.g., similar objects) in background.\nThree key issues in online inference and learning are addressed: (i)\nmaintaining purity of positive and negative examples collected online, (ii)\ncontroling model complexity in latent structure learning, and (iii) identifying\ncritical moments to re-learn the structure of AOG based on its intrackability.\nThe intrackability measures uncertainty of an AOG based on its score maps in a\nframe. In experiments, our AOGTracker is tested on two popular tracking\nbenchmarks with the same parameter setting: the TB-100/50/CVPR2013 benchmarks,\nand the VOT benchmarks --- VOT 2013, 2014, 2015 and TIR2015 (thermal imagery\ntracking). In the former, our AOGTracker outperforms state-of-the-art tracking\nalgorithms including two trackers based on deep convolutional network. In the\nlatter, our AOGTracker outperforms all other trackers in VOT2013 and is\ncomparable to the state-of-the-art methods in VOT2014, 2015 and TIR2015.\n", "versions": [{"version": "v1", "created": "Sun, 27 Sep 2015 08:14:57 GMT"}, {"version": "v2", "created": "Fri, 6 May 2016 01:10:42 GMT"}, {"version": "v3", "created": "Wed, 11 May 2016 19:35:37 GMT"}, {"version": "v4", "created": "Sun, 15 May 2016 06:02:48 GMT"}, {"version": "v5", "created": "Wed, 18 May 2016 05:44:50 GMT"}, {"version": "v6", "created": "Sat, 3 Sep 2016 00:26:58 GMT"}], "update_date": "2016-09-06", "authors_parsed": [["Wu", "Tianfu", ""], ["Lu", "Yang", ""], ["Zhu", "Song-Chun", ""]]}, {"id": "1509.08083", "submitter": "Jan Vyb\\'iral", "authors": "Anton Kolleck, Jan Vyb\\'iral", "title": "Non-asymptotic Analysis of $\\ell_1$-norm Support Vector Machines", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.LG math.FA math.IT math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Support Vector Machines (SVM) with $\\ell_1$ penalty became a standard tool in\nanalysis of highdimensional classification problems with sparsity constraints\nin many applications including bioinformatics and signal processing. Although\nSVM have been studied intensively in the literature, this paper has to our\nknowledge first non-asymptotic results on the performance of $\\ell_1$-SVM in\nidentification of sparse classifiers. We show that a $d$-dimensional $s$-sparse\nclassification vector can be (with high probability) well approximated from\nonly $O(s\\log(d))$ Gaussian trials. The methods used in the proof include\nconcentration of measure and probability in Banach spaces.\n", "versions": [{"version": "v1", "created": "Sun, 27 Sep 2015 11:07:03 GMT"}], "update_date": "2015-09-29", "authors_parsed": [["Kolleck", "Anton", ""], ["Vyb\u00edral", "Jan", ""]]}, {"id": "1509.08101", "submitter": "Matus Telgarsky", "authors": "Matus Telgarsky", "title": "Representation Benefits of Deep Feedforward Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This note provides a family of classification problems, indexed by a positive\ninteger $k$, where all shallow networks with fewer than exponentially (in $k$)\nmany nodes exhibit error at least $1/6$, whereas a deep network with 2 nodes in\neach of $2k$ layers achieves zero error, as does a recurrent network with 3\ndistinct nodes iterated $k$ times. The proof is elementary, and the networks\nare standard feedforward networks with ReLU (Rectified Linear Unit)\nnonlinearities.\n", "versions": [{"version": "v1", "created": "Sun, 27 Sep 2015 15:26:58 GMT"}, {"version": "v2", "created": "Tue, 29 Sep 2015 13:44:37 GMT"}], "update_date": "2015-09-30", "authors_parsed": [["Telgarsky", "Matus", ""]]}, {"id": "1509.08102", "submitter": "Shin Ando Ph. D.", "authors": "Shin Ando", "title": "Discriminative Learning of the Prototype Set for Nearest Neighbor\n  Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The nearest neighbor rule is a classic yet essential classification model,\nparticularly in problems where the supervising information is given by pairwise\ndissimilarities and the embedding function are not easily obtained. Prototype\nselection provides means of generalization and improving efficiency of the\nnearest neighbor model, but many existing methods assume and rely on the\nanalyses of the input vector space. In this paper, we explore a\ndissimilarity-based, parametrized model of the nearest neighbor rule. In the\nproposed model, the selection of the nearest prototypes is influenced by the\nparameters of the respective prototypes. It provides a formulation for\nminimizing the violation of the extended nearest neighbor rule over the\ntraining set in a tractable form to exploit numerical techniques. We show that\nthe minimization problem reduces to a large-margin principle learning and\ndemonstrate its advantage by empirical comparisons with other prototype\nselection methods.\n", "versions": [{"version": "v1", "created": "Sun, 27 Sep 2015 15:43:33 GMT"}, {"version": "v2", "created": "Thu, 31 Dec 2015 00:44:17 GMT"}, {"version": "v3", "created": "Tue, 5 Jan 2016 07:49:27 GMT"}, {"version": "v4", "created": "Sun, 17 Jan 2016 11:54:57 GMT"}, {"version": "v5", "created": "Sun, 21 Aug 2016 14:40:20 GMT"}, {"version": "v6", "created": "Sat, 6 Jan 2018 00:13:09 GMT"}], "update_date": "2018-01-09", "authors_parsed": [["Ando", "Shin", ""]]}, {"id": "1509.08112", "submitter": "Jayadeva", "authors": "Phool Preet, Sanjit Singh Batra, Jayadeva", "title": "Feature Selection for classification of hyperspectral data by minimizing\n  a tight bound on the VC dimension", "comments": "basic papers are on http://www.jayadeva.net", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hyperspectral data consists of large number of features which require\nsophisticated analysis to be extracted. A popular approach to reduce\ncomputational cost, facilitate information representation and accelerate\nknowledge discovery is to eliminate bands that do not improve the\nclassification and analysis methods being applied. In particular, algorithms\nthat perform band elimination should be designed to take advantage of the\nspecifics of the classification method being used. This paper employs a\nrecently proposed filter-feature-selection algorithm based on minimizing a\ntight bound on the VC dimension. We have successfully applied this algorithm to\ndetermine a reasonable subset of bands without any user-defined stopping\ncriteria on widely used hyperspectral images and demonstrate that this method\noutperforms state-of-the-art methods in terms of both sparsity of feature set\nas well as accuracy of classification.\\end{abstract}\n", "versions": [{"version": "v1", "created": "Sun, 27 Sep 2015 17:36:18 GMT"}], "update_date": "2015-09-29", "authors_parsed": [["Preet", "Phool", ""], ["Batra", "Sanjit Singh", ""], ["Jayadeva", "", ""]]}, {"id": "1509.08144", "submitter": "Gautier Marti", "authors": "Gautier Marti, Frank Nielsen, Philippe Donnat", "title": "Optimal Copula Transport for Clustering Multivariate Time Series", "comments": "Accepted at ICASSP 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a new methodology for clustering multivariate time series\nleveraging optimal transport between copulas. Copulas are used to encode both\n(i) intra-dependence of a multivariate time series, and (ii) inter-dependence\nbetween two time series. Then, optimal copula transport allows us to define two\ndistances between multivariate time series: (i) one for measuring\nintra-dependence dissimilarity, (ii) another one for measuring inter-dependence\ndissimilarity based on a new multivariate dependence coefficient which is\nrobust to noise, deterministic, and which can target specified dependencies.\n", "versions": [{"version": "v1", "created": "Sun, 27 Sep 2015 21:17:09 GMT"}, {"version": "v2", "created": "Mon, 11 Jan 2016 17:05:49 GMT"}], "update_date": "2016-01-12", "authors_parsed": [["Marti", "Gautier", ""], ["Nielsen", "Frank", ""], ["Donnat", "Philippe", ""]]}, {"id": "1509.08239", "submitter": "Biju Issac", "authors": "Mohanad Albayati and Biju Issac", "title": "Analysis of Intelligent Classifiers and Enhancing the Detection Accuracy\n  for Intrusion Detection System", "comments": null, "journal-ref": "International Journal of Computational Intelligence Systems, 8:5,\n  841-853 (2015)", "doi": "10.1080/18756891.2015.1084705", "report-no": null, "categories": "cs.CR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we discuss and analyze some of the intelligent classifiers\nwhich allows for automatic detection and classification of networks attacks for\nany intrusion detection system. We will proceed initially with their analysis\nusing the WEKA software to work with the classifiers on a well-known IDS\n(Intrusion Detection Systems) dataset like NSL-KDD dataset. The NSL-KDD dataset\nof network attacks was created in a military network by MIT Lincoln Labs. Then\nwe will discuss and experiment some of the hybrid AI (Artificial Intelligence)\nclassifiers that can be used for IDS, and finally we developed a Java software\nwith three most efficient classifiers and compared it with other options. The\noutputs would show the detection accuracy and efficiency of the single and\ncombined classifiers used.\n", "versions": [{"version": "v1", "created": "Mon, 28 Sep 2015 09:01:30 GMT"}], "update_date": "2015-09-29", "authors_parsed": [["Albayati", "Mohanad", ""], ["Issac", "Biju", ""]]}, {"id": "1509.08333", "submitter": "Hsiang-Fu Yu", "authors": "Hsiang-Fu Yu and Nikhil Rao and Inderjit S. Dhillon", "title": "High-dimensional Time Series Prediction with Missing Values", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  High-dimensional time series prediction is needed in applications as diverse\nas demand forecasting and climatology. Often, such applications require methods\nthat are both highly scalable, and deal with noisy data in terms of corruptions\nor missing values. Classical time series methods usually fall short of handling\nboth these issues. In this paper, we propose to adapt matrix matrix completion\napproaches that have previously been successfully applied to large scale noisy\ndata, but which fail to adequately model high-dimensional time series due to\ntemporal dependencies. We present a novel temporal regularized matrix\nfactorization (TRMF) framework which supports data-driven temporal dependency\nlearning and enables forecasting ability to our new matrix factorization\napproach. TRMF is highly general, and subsumes many existing matrix\nfactorization approaches for time series data. We make interesting connections\nto graph regularized matrix factorization methods in the context of learning\nthe dependencies. Experiments on both real and synthetic data show that TRMF\noutperforms several existing approaches for common time series tasks.\n", "versions": [{"version": "v1", "created": "Mon, 28 Sep 2015 14:37:14 GMT"}, {"version": "v2", "created": "Tue, 13 Oct 2015 19:58:05 GMT"}, {"version": "v3", "created": "Wed, 17 Feb 2016 02:50:31 GMT"}], "update_date": "2016-02-18", "authors_parsed": [["Yu", "Hsiang-Fu", ""], ["Rao", "Nikhil", ""], ["Dhillon", "Inderjit S.", ""]]}, {"id": "1509.08360", "submitter": "Dinesh Ramasamy", "authors": "Dinesh Ramasamy and Upamanyu Madhow", "title": "Compressive spectral embedding: sidestepping the SVD", "comments": "NIPS 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Spectral embedding based on the Singular Value Decomposition (SVD) is a\nwidely used \"preprocessing\" step in many learning tasks, typically leading to\ndimensionality reduction by projecting onto a number of dominant singular\nvectors and rescaling the coordinate axes (by a predefined function of the\nsingular value). However, the number of such vectors required to capture\nproblem structure grows with problem size, and even partial SVD computation\nbecomes a bottleneck. In this paper, we propose a low-complexity it compressive\nspectral embedding algorithm, which employs random projections and finite order\npolynomial expansions to compute approximations to SVD-based embedding. For an\nm times n matrix with T non-zeros, its time complexity is O((T+m+n)log(m+n)),\nand the embedding dimension is O(log(m+n)), both of which are independent of\nthe number of singular vectors whose effect we wish to capture. To the best of\nour knowledge, this is the first work to circumvent this dependence on the\nnumber of singular vectors for general SVD-based embeddings. The key to\nsidestepping the SVD is the observation that, for downstream inference tasks\nsuch as clustering and classification, we are only interested in using the\nresulting embedding to evaluate pairwise similarity metrics derived from the\neuclidean norm, rather than capturing the effect of the underlying matrix on\narbitrary vectors as a partial SVD tries to do. Our numerical results on\nnetwork datasets demonstrate the efficacy of the proposed method, and motivate\nfurther exploration of its application to large-scale inference tasks.\n", "versions": [{"version": "v1", "created": "Mon, 28 Sep 2015 15:32:20 GMT"}], "update_date": "2015-09-29", "authors_parsed": [["Ramasamy", "Dinesh", ""], ["Madhow", "Upamanyu", ""]]}, {"id": "1509.08387", "submitter": "John Lipor", "authors": "John Lipor, Brandon Wong, Donald Scavia, Branko Kerkez, and Laura\n  Balzano", "title": "Distance-Penalized Active Learning Using Quantile Search", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Adaptive sampling theory has shown that, with proper assumptions on the\nsignal class, algorithms exist to reconstruct a signal in $\\mathbb{R}^{d}$ with\nan optimal number of samples. We generalize this problem to the case of spatial\nsignals, where the sampling cost is a function of both the number of samples\ntaken and the distance traveled during estimation. This is motivated by our\nwork studying regions of low oxygen concentration in the Great Lakes. We show\nthat for one-dimensional threshold classifiers, a tradeoff between the number\nof samples taken and distance traveled can be achieved using a generalization\nof binary search, which we refer to as quantile search. We characterize both\nthe estimation error after a fixed number of samples and the distance traveled\nin the noiseless case, as well as the estimation error in the case of noisy\nmeasurements. We illustrate our results in both simulations and experiments and\nshow that our method outperforms existing algorithms in the majority of\npractical scenarios.\n", "versions": [{"version": "v1", "created": "Mon, 28 Sep 2015 16:48:39 GMT"}, {"version": "v2", "created": "Thu, 16 Feb 2017 22:38:40 GMT"}], "update_date": "2017-02-20", "authors_parsed": [["Lipor", "John", ""], ["Wong", "Brandon", ""], ["Scavia", "Donald", ""], ["Kerkez", "Branko", ""], ["Balzano", "Laura", ""]]}, {"id": "1509.08455", "submitter": "Maximilian Karl", "authors": "Maximilian Karl, Justin Bayer, Patrick van der Smagt", "title": "Efficient Empowerment", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Empowerment quantifies the influence an agent has on its environment. This is\nformally achieved by the maximum of the expected KL-divergence between the\ndistribution of the successor state conditioned on a specific action and a\ndistribution where the actions are marginalised out. This is a natural\ncandidate for an intrinsic reward signal in the context of reinforcement\nlearning: the agent will place itself in a situation where its action have\nmaximum stability and maximum influence on the future. The limiting factor so\nfar has been the computational complexity of the method: the only way of\ncalculation has so far been a brute force algorithm, reducing the applicability\nof the method to environments with a small set discrete states. In this work,\nwe propose to use an efficient approximation for marginalising out the actions\nin the case of continuous environments. This allows fast evaluation of\nempowerment, paving the way towards challenging environments such as real world\nrobotics. The method is presented on a pendulum swing up problem.\n", "versions": [{"version": "v1", "created": "Mon, 28 Sep 2015 19:58:31 GMT"}], "update_date": "2015-09-29", "authors_parsed": [["Karl", "Maximilian", ""], ["Bayer", "Justin", ""], ["van der Smagt", "Patrick", ""]]}, {"id": "1509.08581", "submitter": "Zhaosong Lu", "authors": "Zhaosong Lu", "title": "Optimization over Sparse Symmetric Sets via a Nonmonotone Projected\n  Gradient Method", "comments": "30 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.LG cs.NA stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of minimizing a Lipschitz differentiable function\nover a class of sparse symmetric sets that has wide applications in engineering\nand science. For this problem, it is known that any accumulation point of the\nclassical projected gradient (PG) method with a constant stepsize $1/L$\nsatisfies the $L$-stationarity optimality condition that was introduced in [3].\nIn this paper we introduce a new optimality condition that is stronger than the\n$L$-stationarity optimality condition. We also propose a nonmonotone projected\ngradient (NPG) method for this problem by incorporating some support-changing\nand coordintate-swapping strategies into a projected gradient method with\nvariable stepsizes. It is shown that any accumulation point of NPG satisfies\nthe new optimality condition and moreover it is a coordinatewise stationary\npoint. Under some suitable assumptions, we further show that it is a global or\na local minimizer of the problem. Numerical experiments are conducted to\ncompare the performance of PG and NPG. The computational results demonstrate\nthat NPG has substantially better solution quality than PG, and moreover, it is\nat least comparable to, but sometimes can be much faster than PG in terms of\nspeed.\n", "versions": [{"version": "v1", "created": "Tue, 29 Sep 2015 03:39:01 GMT"}, {"version": "v2", "created": "Sat, 21 Nov 2015 22:19:11 GMT"}, {"version": "v3", "created": "Sun, 29 Nov 2015 18:47:57 GMT"}], "update_date": "2015-12-01", "authors_parsed": [["Lu", "Zhaosong", ""]]}, {"id": "1509.08627", "submitter": "David Balduzzi", "authors": "David Balduzzi", "title": "Semantics, Representations and Grammars for Deep Learning", "comments": "20 pages, many diagrams", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning is currently the subject of intensive study. However,\nfundamental concepts such as representations are not formally defined --\nresearchers \"know them when they see them\" -- and there is no common language\nfor describing and analyzing algorithms. This essay proposes an abstract\nframework that identifies the essential features of current practice and may\nprovide a foundation for future developments.\n  The backbone of almost all deep learning algorithms is backpropagation, which\nis simply a gradient computation distributed over a neural network. The main\ningredients of the framework are thus, unsurprisingly: (i) game theory, to\nformalize distributed optimization; and (ii) communication protocols, to track\nthe flow of zeroth and first-order information. The framework allows natural\ndefinitions of semantics (as the meaning encoded in functions), representations\n(as functions whose semantics is chosen to optimized a criterion) and grammars\n(as communication protocols equipped with first-order convergence guarantees).\n  Much of the essay is spent discussing examples taken from the literature. The\nultimate aim is to develop a graphical language for describing the structure of\ndeep learning algorithms that backgrounds the details of the optimization\nprocedure and foregrounds how the components interact. Inspiration is taken\nfrom probabilistic graphical models and factor graphs, which capture the\nessential structural features of multivariate distributions.\n", "versions": [{"version": "v1", "created": "Tue, 29 Sep 2015 08:14:21 GMT"}], "update_date": "2015-09-30", "authors_parsed": [["Balduzzi", "David", ""]]}, {"id": "1509.08634", "submitter": "Takayuki Osogami", "authors": "Takayuki Osogami and Makoto Otsuka", "title": "Learning dynamic Boltzmann machines with spike-timing dependent\n  plasticity", "comments": "Preliminary and substantially different version of the paper appeared\n  in http://www.nature.com/articles/srep14149", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a particularly structured Boltzmann machine, which we refer to as\na dynamic Boltzmann machine (DyBM), as a stochastic model of a\nmulti-dimensional time-series. The DyBM can have infinitely many layers of\nunits but allows exact and efficient inference and learning when its parameters\nhave a proposed structure. This proposed structure is motivated by postulates\nand observations, from biological neural networks, that the synaptic weight is\nstrengthened or weakened, depending on the timing of spikes (i.e., spike-timing\ndependent plasticity or STDP). We show that the learning rule of updating the\nparameters of the DyBM in the direction of maximizing the likelihood of given\ntime-series can be interpreted as STDP with long term potentiation and long\nterm depression. The learning rule has a guarantee of convergence and can be\nperformed in a distributed matter (i.e., local in space) with limited memory\n(i.e., local in time).\n", "versions": [{"version": "v1", "created": "Tue, 29 Sep 2015 08:30:12 GMT"}], "update_date": "2015-09-30", "authors_parsed": [["Osogami", "Takayuki", ""], ["Otsuka", "Makoto", ""]]}, {"id": "1509.08731", "submitter": "Shakir Mohamed", "authors": "Shakir Mohamed and Danilo Jimenez Rezende", "title": "Variational Information Maximisation for Intrinsically Motivated\n  Reinforcement Learning", "comments": "Proceedings of the 29th Conference on Neural Information Processing\n  Systems (NIPS 2015)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The mutual information is a core statistical quantity that has applications\nin all areas of machine learning, whether this is in training of density models\nover multiple data modalities, in maximising the efficiency of noisy\ntransmission channels, or when learning behaviour policies for exploration by\nartificial agents. Most learning algorithms that involve optimisation of the\nmutual information rely on the Blahut-Arimoto algorithm --- an enumerative\nalgorithm with exponential complexity that is not suitable for modern machine\nlearning applications. This paper provides a new approach for scalable\noptimisation of the mutual information by merging techniques from variational\ninference and deep learning. We develop our approach by focusing on the problem\nof intrinsically-motivated learning, where the mutual information forms the\ndefinition of a well-known internal drive known as empowerment. Using a\nvariational lower bound on the mutual information, combined with convolutional\nnetworks for handling visual input streams, we develop a stochastic\noptimisation algorithm that allows for scalable information maximisation and\nempowerment-based reasoning directly from pixels to actions.\n", "versions": [{"version": "v1", "created": "Tue, 29 Sep 2015 13:04:03 GMT"}], "update_date": "2015-09-30", "authors_parsed": [["Mohamed", "Shakir", ""], ["Rezende", "Danilo Jimenez", ""]]}, {"id": "1509.08745", "submitter": "Vincent Gripon", "authors": "Guillaume Souli\\'e, Vincent Gripon, Ma\\\"elys Robert", "title": "Compression of Deep Neural Networks on the Fly", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Thanks to their state-of-the-art performance, deep neural networks are\nincreasingly used for object recognition. To achieve these results, they use\nmillions of parameters to be trained. However, when targeting embedded\napplications the size of these models becomes problematic. As a consequence,\ntheir usage on smartphones or other resource limited devices is prohibited. In\nthis paper we introduce a novel compression method for deep neural networks\nthat is performed during the learning phase. It consists in adding an extra\nregularization term to the cost function of fully-connected layers. We combine\nthis method with Product Quantization (PQ) of the trained weights for higher\nsavings in storage consumption. We evaluate our method on two data sets (MNIST\nand CIFAR10), on which we achieve significantly larger compression rates than\nstate-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Tue, 29 Sep 2015 13:32:30 GMT"}, {"version": "v2", "created": "Thu, 12 Nov 2015 10:22:13 GMT"}, {"version": "v3", "created": "Mon, 16 Nov 2015 13:08:50 GMT"}, {"version": "v4", "created": "Thu, 7 Jan 2016 12:58:32 GMT"}, {"version": "v5", "created": "Fri, 18 Mar 2016 09:33:01 GMT"}], "update_date": "2016-03-21", "authors_parsed": [["Souli\u00e9", "Guillaume", ""], ["Gripon", "Vincent", ""], ["Robert", "Ma\u00eblys", ""]]}, {"id": "1509.08830", "submitter": "Evgeniy Vodolazskiy", "authors": "Michail Schlesinger and Evgeniy Vodolazskiy", "title": "How to Formulate and Solve Statistical Recognition and Learning Problems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We formulate problems of statistical recognition and learning in a common\nframework of complex hypothesis testing. Based on arguments from multi-criteria\noptimization, we identify strategies that are improper for solving these\nproblems and derive a common form of the remaining strategies. We show that\nsome widely used approaches to recognition and learning are improper in this\nsense. We then propose a generalized formulation of the recognition and\nlearning problem which embraces the whole range of sizes of the learning\nsample, including the zero size. Learning becomes a special case of recognition\nwithout learning. We define the concept of closest to optimal strategy, being a\nsolution to the formulated problem, and describe a technique for finding such a\nstrategy. On several illustrative cases, the strategy is shown to be superior\nto the widely used learning methods based on maximal likelihood estimation.\n", "versions": [{"version": "v1", "created": "Tue, 29 Sep 2015 16:23:28 GMT"}], "update_date": "2015-09-30", "authors_parsed": [["Schlesinger", "Michail", ""], ["Vodolazskiy", "Evgeniy", ""]]}, {"id": "1509.08880", "submitter": "Dmitry Storcheus", "authors": "Mehryar Mohri, Afshin Rostamizadeh, Dmitry Storcheus", "title": "Foundations of Coupled Nonlinear Dimensionality Reduction", "comments": "12 pages, 3 figures, authors in alphabetical order", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we introduce and analyze the learning scenario of \\emph{coupled\nnonlinear dimensionality reduction}, which combines two major steps of machine\nlearning pipeline: projection onto a manifold and subsequent supervised\nlearning. First, we present new generalization bounds for this scenario and,\nsecond, we introduce an algorithm that follows from these bounds. The\ngeneralization error bound is based on a careful analysis of the empirical\nRademacher complexity of the relevant hypothesis set. In particular, we show an\nupper bound on the Rademacher complexity that is in $\\widetilde\nO(\\sqrt{\\Lambda_{(r)}/m})$, where $m$ is the sample size and $\\Lambda_{(r)}$\nthe upper bound on the Ky-Fan $r$-norm of the associated kernel matrix. We give\nboth upper and lower bound guarantees in terms of that Ky-Fan $r$-norm, which\nstrongly justifies the definition of our hypothesis set. To the best of our\nknowledge, these are the first learning guarantees for the problem of coupled\ndimensionality reduction. Our analysis and learning guarantees further apply to\nseveral special cases, such as that of using a fixed kernel with supervised\ndimensionality reduction or that of unsupervised learning of a kernel for\ndimensionality reduction followed by a supervised learning algorithm. Based on\ntheoretical analysis, we suggest a structural risk minimization algorithm\nconsisting of the coupled fitting of a low dimensional manifold and a\nseparation function on that manifold.\n", "versions": [{"version": "v1", "created": "Tue, 29 Sep 2015 18:33:57 GMT"}, {"version": "v2", "created": "Wed, 25 Nov 2015 16:51:31 GMT"}], "update_date": "2015-11-26", "authors_parsed": [["Mohri", "Mehryar", ""], ["Rostamizadeh", "Afshin", ""], ["Storcheus", "Dmitry", ""]]}, {"id": "1509.08888", "submitter": "Hamid Reza Hassanzadeh", "authors": "Hamid Reza Hassanzadeh and John H. Phan and May D. Wang", "title": "A Semi-Supervised Method for Predicting Cancer Survival Using Incomplete\n  Clinical Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Prediction of survival for cancer patients is an open area of research.\nHowever, many of these studies focus on datasets with a large number of\npatients. We present a novel method that is specifically designed to address\nthe challenge of data scarcity, which is often the case for cancer datasets.\nOur method is able to use unlabeled data to improve classification by adopting\na semi-supervised training approach to learn an ensemble classifier. The\nresults of applying our method to three cancer datasets show the promise of\nsemi-supervised learning for prediction of cancer survival.\n", "versions": [{"version": "v1", "created": "Tue, 29 Sep 2015 18:53:04 GMT"}], "update_date": "2015-09-30", "authors_parsed": [["Hassanzadeh", "Hamid Reza", ""], ["Phan", "John H.", ""], ["Wang", "May D.", ""]]}, {"id": "1509.08985", "submitter": "Chen-Yu Lee", "authors": "Chen-Yu Lee, Patrick W. Gallagher, Zhuowen Tu", "title": "Generalizing Pooling Functions in Convolutional Neural Networks: Mixed,\n  Gated, and Tree", "comments": "Patent disclosure, UCSD Docket No. SD2015-184, \"Forest Convolutional\n  Neural Network\", filed on March 4, 2015. UCSD Docket No. SD2016-053,\n  \"Generalizing Pooling Functions in Convolutional Neural Network\", filed on\n  Sept 23, 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We seek to improve deep neural networks by generalizing the pooling\noperations that play a central role in current architectures. We pursue a\ncareful exploration of approaches to allow pooling to learn and to adapt to\ncomplex and variable patterns. The two primary directions lie in (1) learning a\npooling function via (two strategies of) combining of max and average pooling,\nand (2) learning a pooling function in the form of a tree-structured fusion of\npooling filters that are themselves learned. In our experiments every\ngeneralized pooling operation we explore improves performance when used in\nplace of average or max pooling. We experimentally demonstrate that the\nproposed pooling operations provide a boost in invariance properties relative\nto conventional pooling and set the state of the art on several widely adopted\nbenchmark datasets; they are also easy to implement, and can be applied within\nvarious deep neural network architectures. These benefits come with only a\nlight increase in computational overhead during training and a very modest\nincrease in the number of model parameters.\n", "versions": [{"version": "v1", "created": "Wed, 30 Sep 2015 01:06:36 GMT"}, {"version": "v2", "created": "Sat, 10 Oct 2015 03:18:45 GMT"}], "update_date": "2015-10-13", "authors_parsed": [["Lee", "Chen-Yu", ""], ["Gallagher", "Patrick W.", ""], ["Tu", "Zhuowen", ""]]}, {"id": "1509.08990", "submitter": "Mohammad Amin Rahimian", "authors": "Mohammad Amin Rahimian and Ali Jadbabaie", "title": "Learning without Recall: A Case for Log-Linear Learning", "comments": "in 5th IFAC Workshop on Distributed Estimation and Control in\n  Networked Systems, (NecSys 2015)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.LG cs.SY math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We analyze a model of learning and belief formation in networks in which\nagents follow Bayes rule yet they do not recall their history of past\nobservations and cannot reason about how other agents' beliefs are formed. They\ndo so by making rational inferences about their observations which include a\nsequence of independent and identically distributed private signals as well as\nthe beliefs of their neighboring agents at each time. Fully rational agents\nwould successively apply Bayes rule to the entire history of observations. This\nleads to forebodingly complex inferences due to lack of knowledge about the\nglobal network structure that causes those observations. To address these\ncomplexities, we consider a Learning without Recall model, which in addition to\nproviding a tractable framework for analyzing the behavior of rational agents\nin social networks, can also provide a behavioral foundation for the variety of\nnon-Bayesian update rules in the literature. We present the implications of\nvarious choices for time-varying priors of such agents and how this choice\naffects learning and its rate.\n", "versions": [{"version": "v1", "created": "Wed, 30 Sep 2015 01:43:09 GMT"}], "update_date": "2015-10-01", "authors_parsed": [["Rahimian", "Mohammad Amin", ""], ["Jadbabaie", "Ali", ""]]}, {"id": "1509.08992", "submitter": "Justin Domke", "authors": "Justin Domke", "title": "Maximum Likelihood Learning With Arbitrary Treewidth via Fast-Mixing\n  Parameter Sets", "comments": "Advances in Neural Information Processing Systems 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Inference is typically intractable in high-treewidth undirected graphical\nmodels, making maximum likelihood learning a challenge. One way to overcome\nthis is to restrict parameters to a tractable set, most typically the set of\ntree-structured parameters. This paper explores an alternative notion of a\ntractable set, namely a set of \"fast-mixing parameters\" where Markov chain\nMonte Carlo (MCMC) inference can be guaranteed to quickly converge to the\nstationary distribution. While it is common in practice to approximate the\nlikelihood gradient using samples obtained from MCMC, such procedures lack\ntheoretical guarantees. This paper proves that for any exponential family with\nbounded sufficient statistics, (not just graphical models) when parameters are\nconstrained to a fast-mixing set, gradient descent with gradients approximated\nby sampling will approximate the maximum likelihood solution inside the set\nwith high-probability. When unregularized, to find a solution epsilon-accurate\nin log-likelihood requires a total amount of effort cubic in 1/epsilon,\ndisregarding logarithmic factors. When ridge-regularized, strong convexity\nallows a solution epsilon-accurate in parameter distance with effort quadratic\nin 1/epsilon. Both of these provide of a fully-polynomial time randomized\napproximation scheme.\n", "versions": [{"version": "v1", "created": "Wed, 30 Sep 2015 01:44:41 GMT"}, {"version": "v2", "created": "Fri, 30 Oct 2015 07:29:08 GMT"}], "update_date": "2015-11-02", "authors_parsed": [["Domke", "Justin", ""]]}, {"id": "1509.09002", "submitter": "Ohad Shamir", "authors": "Ohad Shamir", "title": "Convergence of Stochastic Gradient Descent for PCA", "comments": "Added analysis of the positive eigengap scenario, with new results;\n  Some minor corrections", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of principal component analysis (PCA) in a streaming\nstochastic setting, where our goal is to find a direction of approximate\nmaximal variance, based on a stream of i.i.d. data points in $\\reals^d$. A\nsimple and computationally cheap algorithm for this is stochastic gradient\ndescent (SGD), which incrementally updates its estimate based on each new data\npoint. However, due to the non-convex nature of the problem, analyzing its\nperformance has been a challenge. In particular, existing guarantees rely on a\nnon-trivial eigengap assumption on the covariance matrix, which is intuitively\nunnecessary. In this paper, we provide (to the best of our knowledge) the first\neigengap-free convergence guarantees for SGD in the context of PCA. This also\npartially resolves an open problem posed in \\cite{hardt2014noisy}. Moreover,\nunder an eigengap assumption, we show that the same techniques lead to new SGD\nconvergence guarantees with better dependence on the eigengap.\n", "versions": [{"version": "v1", "created": "Wed, 30 Sep 2015 03:02:59 GMT"}, {"version": "v2", "created": "Mon, 4 Jan 2016 08:25:56 GMT"}], "update_date": "2016-01-05", "authors_parsed": [["Shamir", "Ohad", ""]]}, {"id": "1509.09011", "submitter": "Junpei Komiyama", "authors": "Junpei Komiyama, Junya Honda, Hiroshi Nakagawa", "title": "Regret Lower Bound and Optimal Algorithm in Finite Stochastic Partial\n  Monitoring", "comments": "24 pages, to appear in NIPS2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Partial monitoring is a general model for sequential learning with limited\nfeedback formalized as a game between two players. In this game, the learner\nchooses an action and at the same time the opponent chooses an outcome, then\nthe learner suffers a loss and receives a feedback signal. The goal of the\nlearner is to minimize the total loss. In this paper, we study partial\nmonitoring with finite actions and stochastic outcomes. We derive a logarithmic\ndistribution-dependent regret lower bound that defines the hardness of the\nproblem. Inspired by the DMED algorithm (Honda and Takemura, 2010) for the\nmulti-armed bandit problem, we propose PM-DMED, an algorithm that minimizes the\ndistribution-dependent regret. PM-DMED significantly outperforms\nstate-of-the-art algorithms in numerical experiments. To show the optimality of\nPM-DMED with respect to the regret bound, we slightly modify the algorithm by\nintroducing a hinge function (PM-DMED-Hinge). Then, we derive an asymptotically\noptimal regret upper bound of PM-DMED-Hinge that matches the lower bound.\n", "versions": [{"version": "v1", "created": "Wed, 30 Sep 2015 04:36:40 GMT"}], "update_date": "2015-10-01", "authors_parsed": [["Komiyama", "Junpei", ""], ["Honda", "Junya", ""], ["Nakagawa", "Hiroshi", ""]]}, {"id": "1509.09030", "submitter": "Sourangshu Bhattacharya", "authors": "Ayan Das and Sourangshu Bhattacharya", "title": "Distributed Weighted Parameter Averaging for SVM Training on Big Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Two popular approaches for distributed training of SVMs on big data are\nparameter averaging and ADMM. Parameter averaging is efficient but suffers from\nloss of accuracy with increase in number of partitions, while ADMM in the\nfeature space is accurate but suffers from slow convergence. In this paper, we\nreport a hybrid approach called weighted parameter averaging (WPA), which\noptimizes the regularized hinge loss with respect to weights on parameters. The\nproblem is shown to be same as solving SVM in a projected space. We also\ndemonstrate an $O(\\frac{1}{N})$ stability bound on final hypothesis given by\nWPA, using novel proof techniques. Experimental results on a variety of toy and\nreal world datasets show that our approach is significantly more accurate than\nparameter averaging for high number of partitions. It is also seen the proposed\nmethod enjoys much faster convergence compared to ADMM in features space.\n", "versions": [{"version": "v1", "created": "Wed, 30 Sep 2015 06:59:31 GMT"}], "update_date": "2015-10-01", "authors_parsed": [["Das", "Ayan", ""], ["Bhattacharya", "Sourangshu", ""]]}, {"id": "1509.09130", "submitter": "Claire Vernade", "authors": "Claire Vernade (LTCI), Olivier Capp\\'e (LTCI)", "title": "Learning From Missing Data Using Selection Bias in Movie Recommendation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.IR cs.LG cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recommending items to users is a challenging task due to the large amount of\nmissing information. In many cases, the data solely consist of ratings or tags\nvoluntarily contributed by each user on a very limited subset of the available\nitems, so that most of the data of potential interest is actually missing.\nCurrent approaches to recommendation usually assume that the unobserved data is\nmissing at random. In this contribution, we provide statistical evidence that\nexisting movie recommendation datasets reveal a significant positive\nassociation between the rating of items and the propensity to select these\nitems. We propose a computationally efficient variational approach that makes\nit possible to exploit this selection bias so as to improve the estimation of\nratings from small populations of users. Results obtained with this approach\napplied to neighborhood-based collaborative filtering illustrate its potential\nfor improving the reliability of the recommendation.\n", "versions": [{"version": "v1", "created": "Wed, 30 Sep 2015 11:40:21 GMT"}], "update_date": "2015-10-01", "authors_parsed": [["Vernade", "Claire", "", "LTCI"], ["Capp\u00e9", "Olivier", "", "LTCI"]]}, {"id": "1509.09187", "submitter": "Xiuyuan Cheng", "authors": "Xiuyuan Cheng, Xu Chen, Stephane Mallat", "title": "Deep Haar Scattering Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An orthogonal Haar scattering transform is a deep network, computed with a\nhierarchy of additions, subtractions and absolute values, over pairs of\ncoefficients. It provides a simple mathematical model for unsupervised deep\nnetwork learning. It implements non-linear contractions, which are optimized\nfor classification, with an unsupervised pair matching algorithm, of polynomial\ncomplexity. A structured Haar scattering over graph data computes permutation\ninvariant representations of groups of connected points in the graph. If the\ngraph connectivity is unknown, unsupervised Haar pair learning can provide a\nconsistent estimation of connected dyadic groups of points. Classification\nresults are given on image data bases, defined on regular grids or graphs, with\na connectivity which may be known or unknown.\n", "versions": [{"version": "v1", "created": "Wed, 30 Sep 2015 14:20:29 GMT"}], "update_date": "2015-10-01", "authors_parsed": [["Cheng", "Xiuyuan", ""], ["Chen", "Xu", ""], ["Mallat", "Stephane", ""]]}, {"id": "1509.09236", "submitter": "Nicolas Gillis", "authors": "Nicolas Gillis, Stephen A. Vavasis", "title": "On the Complexity of Robust PCA and $\\ell_1$-norm Low-Rank Matrix\n  Approximation", "comments": "16 pages, some typos corrected", "journal-ref": "Mathematics of Operations Research 43 (4), pp. 1072-1084, 2018", "doi": "10.1287/moor.2017.0895", "report-no": null, "categories": "cs.LG cs.CC math.NA math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The low-rank matrix approximation problem with respect to the component-wise\n$\\ell_1$-norm ($\\ell_1$-LRA), which is closely related to robust principal\ncomponent analysis (PCA), has become a very popular tool in data mining and\nmachine learning. Robust PCA aims at recovering a low-rank matrix that was\nperturbed with sparse noise, with applications for example in\nforeground-background video separation. Although $\\ell_1$-LRA is strongly\nbelieved to be NP-hard, there is, to the best of our knowledge, no formal proof\nof this fact. In this paper, we prove that $\\ell_1$-LRA is NP-hard, already in\nthe rank-one case, using a reduction from MAX CUT. Our derivations draw\ninteresting connections between $\\ell_1$-LRA and several other well-known\nproblems, namely, robust PCA, $\\ell_0$-LRA, binary matrix factorization, a\nparticular densest bipartite subgraph problem, the computation of the cut norm\nof $\\{-1,+1\\}$ matrices, and the discrete basis problem, which we all prove to\nbe NP-hard.\n", "versions": [{"version": "v1", "created": "Wed, 30 Sep 2015 16:05:50 GMT"}, {"version": "v2", "created": "Sun, 8 Nov 2015 16:48:44 GMT"}, {"version": "v3", "created": "Wed, 19 Apr 2017 10:12:17 GMT"}], "update_date": "2018-12-19", "authors_parsed": [["Gillis", "Nicolas", ""], ["Vavasis", "Stephen A.", ""]]}, {"id": "1509.09292", "submitter": "David Duvenaud", "authors": "David Duvenaud, Dougal Maclaurin, Jorge Aguilera-Iparraguirre, Rafael\n  G\\'omez-Bombarelli, Timothy Hirzel, Al\\'an Aspuru-Guzik, Ryan P. Adams", "title": "Convolutional Networks on Graphs for Learning Molecular Fingerprints", "comments": "9 pages, 5 figures. To appear in Neural Information Processing\n  Systems (NIPS)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We introduce a convolutional neural network that operates directly on graphs.\nThese networks allow end-to-end learning of prediction pipelines whose inputs\nare graphs of arbitrary size and shape. The architecture we present generalizes\nstandard molecular feature extraction methods based on circular fingerprints.\nWe show that these data-driven features are more interpretable, and have better\npredictive performance on a variety of tasks.\n", "versions": [{"version": "v1", "created": "Wed, 30 Sep 2015 18:33:50 GMT"}, {"version": "v2", "created": "Tue, 3 Nov 2015 17:18:32 GMT"}], "update_date": "2015-11-04", "authors_parsed": [["Duvenaud", "David", ""], ["Maclaurin", "Dougal", ""], ["Aguilera-Iparraguirre", "Jorge", ""], ["G\u00f3mez-Bombarelli", "Rafael", ""], ["Hirzel", "Timothy", ""], ["Aspuru-Guzik", "Al\u00e1n", ""], ["Adams", "Ryan P.", ""]]}, {"id": "1509.09308", "submitter": "Andrew Lavin", "authors": "Andrew Lavin and Scott Gray", "title": "Fast Algorithms for Convolutional Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep convolutional neural networks take GPU days of compute time to train on\nlarge data sets. Pedestrian detection for self driving cars requires very low\nlatency. Image recognition for mobile phones is constrained by limited\nprocessing resources. The success of convolutional neural networks in these\nsituations is limited by how fast we can compute them. Conventional FFT based\nconvolution is fast for large filters, but state of the art convolutional\nneural networks use small, 3x3 filters. We introduce a new class of fast\nalgorithms for convolutional neural networks using Winograd's minimal filtering\nalgorithms. The algorithms compute minimal complexity convolution over small\ntiles, which makes them fast with small filters and small batch sizes. We\nbenchmark a GPU implementation of our algorithm with the VGG network and show\nstate of the art throughput at batch sizes from 1 to 64.\n", "versions": [{"version": "v1", "created": "Wed, 30 Sep 2015 19:39:20 GMT"}, {"version": "v2", "created": "Tue, 10 Nov 2015 20:08:41 GMT"}], "update_date": "2015-11-11", "authors_parsed": [["Lavin", "Andrew", ""], ["Gray", "Scott", ""]]}]