[{"id": "1306.0125", "submitter": "Jacob Whitehill", "authors": "Jacob Whitehill", "title": "Understanding ACT-R - an Outsider's Perspective", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The ACT-R theory of cognition developed by John Anderson and colleagues\nendeavors to explain how humans recall chunks of information and how they solve\nproblems. ACT-R also serves as a theoretical basis for \"cognitive tutors\",\ni.e., automatic tutoring systems that help students learn mathematics, computer\nprogramming, and other subjects. The official ACT-R definition is distributed\nacross a large body of literature spanning many articles and monographs, and\nhence it is difficult for an \"outsider\" to learn the most important aspects of\nthe theory. This paper aims to provide a tutorial to the core components of the\nACT-R theory.\n", "versions": [{"version": "v1", "created": "Sat, 1 Jun 2013 15:48:58 GMT"}], "update_date": "2013-06-04", "authors_parsed": [["Whitehill", "Jacob", ""]]}, {"id": "1306.0155", "submitter": "Aleksandrs Slivkins", "authors": "Aleksandrs Slivkins", "title": "Dynamic Ad Allocation: Bandits with Budgets", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider an application of multi-armed bandits to internet advertising\n(specifically, to dynamic ad allocation in the pay-per-click model, with\nuncertainty on the click probabilities). We focus on an important practical\nissue that advertisers are constrained in how much money they can spend on\ntheir ad campaigns. This issue has not been considered in the prior work on\nbandit-based approaches for ad allocation, to the best of our knowledge.\n  We define a simple, stylized model where an algorithm picks one ad to display\nin each round, and each ad has a \\emph{budget}: the maximal amount of money\nthat can be spent on this ad. This model admits a natural variant of UCB1, a\nwell-known algorithm for multi-armed bandits with stochastic rewards. We derive\nstrong provable guarantees for this algorithm.\n", "versions": [{"version": "v1", "created": "Sat, 1 Jun 2013 22:00:03 GMT"}], "update_date": "2013-06-04", "authors_parsed": [["Slivkins", "Aleksandrs", ""]]}, {"id": "1306.0160", "submitter": "Praneeth Netrapalli", "authors": "Praneeth Netrapalli and Prateek Jain and Sujay Sanghavi", "title": "Phase Retrieval using Alternating Minimization", "comments": "Accepted for publication in IEEE Transactions on Signal Processing", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.IT cs.LG math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Phase retrieval problems involve solving linear equations, but with missing\nsign (or phase, for complex numbers) information. More than four decades after\nit was first proposed, the seminal error reduction algorithm of (Gerchberg and\nSaxton 1972) and (Fienup 1982) is still the popular choice for solving many\nvariants of this problem. The algorithm is based on alternating minimization;\ni.e. it alternates between estimating the missing phase information, and the\ncandidate solution. Despite its wide usage in practice, no global convergence\nguarantees for this algorithm are known. In this paper, we show that a\n(resampling) variant of this approach converges geometrically to the solution\nof one such problem -- finding a vector $\\mathbf{x}$ from\n$\\mathbf{y},\\mathbf{A}$, where $\\mathbf{y} =\n\\left|\\mathbf{A}^{\\top}\\mathbf{x}\\right|$ and $|\\mathbf{z}|$ denotes a vector\nof element-wise magnitudes of $\\mathbf{z}$ -- under the assumption that\n$\\mathbf{A}$ is Gaussian.\n  Empirically, we demonstrate that alternating minimization performs similar to\nrecently proposed convex techniques for this problem (which are based on\n\"lifting\" to a convex matrix problem) in sample complexity and robustness to\nnoise. However, it is much more efficient and can scale to large problems.\nAnalytically, for a resampling version of alternating minimization, we show\ngeometric convergence to the solution, and sample complexity that is off by log\nfactors from obvious lower bounds. We also establish close to optimal scaling\nfor the case when the unknown vector is sparse. Our work represents the first\ntheoretical guarantee for alternating minimization (albeit with resampling) for\nany variant of phase retrieval problems in the non-convex setting.\n", "versions": [{"version": "v1", "created": "Sun, 2 Jun 2013 00:45:12 GMT"}, {"version": "v2", "created": "Fri, 12 Jun 2015 11:45:50 GMT"}], "update_date": "2015-06-15", "authors_parsed": [["Netrapalli", "Praneeth", ""], ["Jain", "Prateek", ""], ["Sanghavi", "Sujay", ""]]}, {"id": "1306.0186", "submitter": "Iain Murray", "authors": "Benigno Uria, Iain Murray, Hugo Larochelle", "title": "RNADE: The real-valued neural autoregressive density-estimator", "comments": "12 pages, 3 figures, 3 tables, 2 algorithms. Merges the published\n  paper and supplementary material into one document", "journal-ref": "Advances in Neural Information Processing Systems 26:2175-2183,\n  2013", "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce RNADE, a new model for joint density estimation of real-valued\nvectors. Our model calculates the density of a datapoint as the product of\none-dimensional conditionals modeled using mixture density networks with shared\nparameters. RNADE learns a distributed representation of the data, while having\na tractable expression for the calculation of densities. A tractable likelihood\nallows direct comparison with other methods and training by standard\ngradient-based optimizers. We compare the performance of RNADE on several\ndatasets of heterogeneous and perceptual data, finding it outperforms mixture\nmodels in all but one case.\n", "versions": [{"version": "v1", "created": "Sun, 2 Jun 2013 09:37:53 GMT"}, {"version": "v2", "created": "Thu, 9 Jan 2014 11:14:27 GMT"}], "update_date": "2014-01-10", "authors_parsed": [["Uria", "Benigno", ""], ["Murray", "Iain", ""], ["Larochelle", "Hugo", ""]]}, {"id": "1306.0237", "submitter": "Houtao Deng", "authors": "Houtao Deng", "title": "Guided Random Forest in the RRF Package", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Random Forest (RF) is a powerful supervised learner and has been popularly\nused in many applications such as bioinformatics.\n  In this work we propose the guided random forest (GRF) for feature selection.\nSimilar to a feature selection method called guided regularized random forest\n(GRRF), GRF is built using the importance scores from an ordinary RF. However,\nthe trees in GRRF are built sequentially, are highly correlated and do not\nallow for parallel computing, while the trees in GRF are built independently\nand can be implemented in parallel. Experiments on 10 high-dimensional gene\ndata sets show that, with a fixed parameter value (without tuning the\nparameter), RF applied to features selected by GRF outperforms RF applied to\nall features on 9 data sets and 7 of them have significant differences at the\n0.05 level. Therefore, both accuracy and interpretability are significantly\nimproved. GRF selects more features than GRRF, however, leads to better\nclassification accuracy. Note in this work the guided random forest is guided\nby the importance scores from an ordinary random forest, however, it can also\nbe guided by other methods such as human insights (by specifying $\\lambda_i$).\nGRF can be used in \"RRF\" v1.4 (and later versions), a package that also\nincludes the regularized random forest methods.\n", "versions": [{"version": "v1", "created": "Sun, 2 Jun 2013 18:30:45 GMT"}, {"version": "v2", "created": "Sat, 12 Oct 2013 03:56:07 GMT"}, {"version": "v3", "created": "Mon, 18 Nov 2013 08:52:49 GMT"}], "update_date": "2013-11-19", "authors_parsed": [["Deng", "Houtao", ""]]}, {"id": "1306.0239", "submitter": "Yichuan Tang", "authors": "Yichuan Tang", "title": "Deep Learning using Linear Support Vector Machines", "comments": "Contribution to the ICML 2013 Challenges in Representation Learning\n  Workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, fully-connected and convolutional neural networks have been trained\nto achieve state-of-the-art performance on a wide variety of tasks such as\nspeech recognition, image classification, natural language processing, and\nbioinformatics. For classification tasks, most of these \"deep learning\" models\nemploy the softmax activation function for prediction and minimize\ncross-entropy loss. In this paper, we demonstrate a small but consistent\nadvantage of replacing the softmax layer with a linear support vector machine.\nLearning minimizes a margin-based loss instead of the cross-entropy loss. While\nthere have been various combinations of neural nets and SVMs in prior art, our\nresults using L2-SVMs show that by simply replacing softmax with linear SVMs\ngives significant gains on popular deep learning datasets MNIST, CIFAR-10, and\nthe ICML 2013 Representation Learning Workshop's face expression recognition\nchallenge.\n", "versions": [{"version": "v1", "created": "Sun, 2 Jun 2013 18:46:58 GMT"}, {"version": "v2", "created": "Tue, 9 Jul 2013 21:30:59 GMT"}, {"version": "v3", "created": "Mon, 23 Dec 2013 21:16:45 GMT"}, {"version": "v4", "created": "Sat, 21 Feb 2015 16:58:39 GMT"}], "update_date": "2015-02-24", "authors_parsed": [["Tang", "Yichuan", ""]]}, {"id": "1306.0271", "submitter": "Marina Danilevsky", "authors": "Marina Danilevsky, Chi Wang, Nihit Desai, Jingyi Guo, Jiawei Han", "title": "KERT: Automatic Extraction and Ranking of Topical Keyphrases from\n  Content-Representative Document Titles", "comments": "9 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce KERT (Keyphrase Extraction and Ranking by Topic), a framework\nfor topical keyphrase generation and ranking. By shifting from the\nunigram-centric traditional methods of unsupervised keyphrase extraction to a\nphrase-centric approach, we are able to directly compare and rank phrases of\ndifferent lengths. We construct a topical keyphrase ranking function which\nimplements the four criteria that represent high quality topical keyphrases\n(coverage, purity, phraseness, and completeness). The effectiveness of our\napproach is demonstrated on two collections of content-representative titles in\nthe domains of Computer Science and Physics.\n", "versions": [{"version": "v1", "created": "Mon, 3 Jun 2013 01:44:28 GMT"}], "update_date": "2013-06-04", "authors_parsed": [["Danilevsky", "Marina", ""], ["Wang", "Chi", ""], ["Desai", "Nihit", ""], ["Guo", "Jingyi", ""], ["Han", "Jiawei", ""]]}, {"id": "1306.0308", "submitter": "Philipp Hennig PhD", "authors": "Philipp Hennig and S{\\o}ren Hauberg", "title": "Probabilistic Solutions to Differential Equations and their Application\n  to Riemannian Statistics", "comments": "11 page (9 page conference paper, plus supplements)", "journal-ref": "Proceedings of the 17th International Conference on Artificial\n  Intelligence and Statistics (AISTATS) 2014, Reykjavik, Iceland. Journal of\n  Machine Learning Research: W&CP volume 33", "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study a probabilistic numerical method for the solution of both boundary\nand initial value problems that returns a joint Gaussian process posterior over\nthe solution. Such methods have concrete value in the statistics on Riemannian\nmanifolds, where non-analytic ordinary differential equations are involved in\nvirtually all computations. The probabilistic formulation permits marginalising\nthe uncertainty of the numerical solution such that statistics are less\nsensitive to inaccuracies. This leads to new Riemannian algorithms for mean\nvalue computations and principal geodesic analysis. Marginalisation also means\nresults can be less precise than point estimates, enabling a noticeable\nspeed-up over the state of the art. Our approach is an argument for a wider\npoint that uncertainty caused by numerical calculations should be tracked\nthroughout the pipeline of machine learning algorithms.\n", "versions": [{"version": "v1", "created": "Mon, 3 Jun 2013 06:56:47 GMT"}, {"version": "v2", "created": "Wed, 12 Feb 2014 12:51:32 GMT"}], "update_date": "2014-02-13", "authors_parsed": [["Hennig", "Philipp", ""], ["Hauberg", "S\u00f8ren", ""]]}, {"id": "1306.0393", "submitter": "Yuyi Wang", "authors": "Yuyi Wang, Jan Ramon and Zheng-Chu Guo", "title": "Learning from networked examples in a k-partite graph", "comments": "a special case", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many machine learning algorithms are based on the assumption that training\nexamples are drawn independently. However, this assumption does not hold\nanymore when learning from a networked sample where two or more training\nexamples may share common features. We propose an efficient weighting method\nfor learning from networked examples and show the sample error bound which is\nbetter than previous work.\n", "versions": [{"version": "v1", "created": "Mon, 3 Jun 2013 13:10:35 GMT"}, {"version": "v2", "created": "Tue, 2 Jul 2013 15:18:16 GMT"}, {"version": "v3", "created": "Sat, 18 Feb 2017 00:34:19 GMT"}], "update_date": "2017-02-21", "authors_parsed": [["Wang", "Yuyi", ""], ["Ramon", "Jan", ""], ["Guo", "Zheng-Chu", ""]]}, {"id": "1306.0514", "submitter": "Yann Ollivier", "authors": "Yann Ollivier", "title": "Riemannian metrics for neural networks II: recurrent networks and\n  learning symbolic data sequences", "comments": "4th version: some changes in notation, more experiments", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recurrent neural networks are powerful models for sequential data, able to\nrepresent complex dependencies in the sequence that simpler models such as\nhidden Markov models cannot handle. Yet they are notoriously hard to train.\nHere we introduce a training procedure using a gradient ascent in a Riemannian\nmetric: this produces an algorithm independent from design choices such as the\nencoding of parameters and unit activities. This metric gradient ascent is\ndesigned to have an algorithmic cost close to backpropagation through time for\nsparsely connected networks. We use this procedure on gated leaky neural\nnetworks (GLNNs), a variant of recurrent neural networks with an architecture\ninspired by finite automata and an evolution equation inspired by\ncontinuous-time networks. GLNNs trained with a Riemannian gradient are\ndemonstrated to effectively capture a variety of structures in synthetic\nproblems: basic block nesting as in context-free grammars (an important feature\nof natural languages, but difficult to learn), intersections of multiple\nindependent Markov-type relations, or long-distance relationships such as the\ndistant-XOR problem. This method does not require adjusting the network\nstructure or initial parameters: the network used is a sparse random graph and\nthe initialization is identical for all problems considered.\n", "versions": [{"version": "v1", "created": "Mon, 3 Jun 2013 17:36:14 GMT"}, {"version": "v2", "created": "Tue, 27 Aug 2013 16:19:13 GMT"}, {"version": "v3", "created": "Sat, 12 Jul 2014 14:35:22 GMT"}, {"version": "v4", "created": "Tue, 3 Feb 2015 18:35:36 GMT"}], "update_date": "2015-02-04", "authors_parsed": [["Ollivier", "Yann", ""]]}, {"id": "1306.0539", "submitter": "Bruno Scherrer", "authors": "Bruno Scherrer (INRIA Nancy - Grand Est / LORIA)", "title": "On the Performance Bounds of some Policy Search Dynamic Programming\n  Algorithms", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the infinite-horizon discounted optimal control problem\nformalized by Markov Decision Processes. We focus on Policy Search algorithms,\nthat compute an approximately optimal policy by following the standard Policy\nIteration (PI) scheme via an -approximate greedy operator (Kakade and Langford,\n2002; Lazaric et al., 2010). We describe existing and a few new performance\nbounds for Direct Policy Iteration (DPI) (Lagoudakis and Parr, 2003; Fern et\nal., 2006; Lazaric et al., 2010) and Conservative Policy Iteration (CPI)\n(Kakade and Langford, 2002). By paying a particular attention to the\nconcentrability constants involved in such guarantees, we notably argue that\nthe guarantee of CPI is much better than that of DPI, but this comes at the\ncost of a relative--exponential in $\\frac{1}{\\epsilon}$-- increase of time\ncomplexity. We then describe an algorithm, Non-Stationary Direct Policy\nIteration (NSDPI), that can either be seen as 1) a variation of Policy Search\nby Dynamic Programming by Bagnell et al. (2003) to the infinite horizon\nsituation or 2) a simplified version of the Non-Stationary PI with growing\nperiod of Scherrer and Lesner (2012). We provide an analysis of this algorithm,\nthat shows in particular that it enjoys the best of both worlds: its\nperformance guarantee is similar to that of CPI, but within a time complexity\nsimilar to that of DPI.\n", "versions": [{"version": "v1", "created": "Mon, 3 Jun 2013 19:13:53 GMT"}], "update_date": "2013-06-04", "authors_parsed": [["Scherrer", "Bruno", "", "INRIA Nancy - Grand Est / LORIA"]]}, {"id": "1306.0541", "submitter": "Uri Kartoun", "authors": "Uri Kartoun", "title": "Identifying Pairs in Simulated Bio-Medical Time-Series", "comments": "arXiv admin note: text overlap with arXiv:1303.0073", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The paper presents a time-series-based classification approach to identify\nsimilarities in pairs of simulated human-generated patterns. An example for a\npattern is a time-series representing a heart rate during a specific\ntime-range, wherein the time-series is a sequence of data points that represent\nthe changes in the heart rate values. A bio-medical simulator system was\ndeveloped to acquire a collection of 7,871 price patterns of financial\ninstruments. The financial instruments traded in real-time on three American\nstock exchanges, NASDAQ, NYSE, and AMEX, simulate bio-medical measurements. The\nsystem simulates a human in which each price pattern represents one bio-medical\nsensor. Data provided during trading hours from the stock exchanges allowed\nreal-time classification. Classification is based on new machine learning\ntechniques: self-labeling, which allows the application of supervised learning\nmethods on unlabeled time-series and similarity ranking, which applied on a\ndecision tree learning algorithm to classify time-series regardless of type and\nquantity.\n", "versions": [{"version": "v1", "created": "Sun, 12 May 2013 22:00:09 GMT"}], "update_date": "2013-06-04", "authors_parsed": [["Kartoun", "Uri", ""]]}, {"id": "1306.0543", "submitter": "Misha Denil", "authors": "Misha Denil, Babak Shakibi, Laurent Dinh, Marc'Aurelio Ranzato, Nando\n  de Freitas", "title": "Predicting Parameters in Deep Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We demonstrate that there is significant redundancy in the parameterization\nof several deep learning models. Given only a few weight values for each\nfeature it is possible to accurately predict the remaining values. Moreover, we\nshow that not only can the parameter values be predicted, but many of them need\nnot be learned at all. We train several different architectures by learning\nonly a small number of weights and predicting the rest. In the best case we are\nable to predict more than 95% of the weights of a network without any drop in\naccuracy.\n", "versions": [{"version": "v1", "created": "Mon, 3 Jun 2013 19:16:26 GMT"}, {"version": "v2", "created": "Mon, 27 Oct 2014 11:49:08 GMT"}], "update_date": "2014-10-28", "authors_parsed": [["Denil", "Misha", ""], ["Shakibi", "Babak", ""], ["Dinh", "Laurent", ""], ["Ranzato", "Marc'Aurelio", ""], ["de Freitas", "Nando", ""]]}, {"id": "1306.0604", "submitter": "Yingyu Liang", "authors": "Maria Florina Balcan, Steven Ehrlich, Yingyu Liang", "title": "Distributed k-Means and k-Median Clustering on General Topologies", "comments": "Corrected Theorem 4 in the appendix", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper provides new algorithms for distributed clustering for two popular\ncenter-based objectives, k-median and k-means. These algorithms have provable\nguarantees and improve communication complexity over existing approaches.\nFollowing a classic approach in clustering by \\cite{har2004coresets}, we reduce\nthe problem of finding a clustering with low cost to the problem of finding a\ncoreset of small size. We provide a distributed method for constructing a\nglobal coreset which improves over the previous methods by reducing the\ncommunication complexity, and which works over general communication\ntopologies. Experimental results on large scale data sets show that this\napproach outperforms other coreset-based distributed clustering algorithms.\n", "versions": [{"version": "v1", "created": "Mon, 3 Jun 2013 21:49:19 GMT"}, {"version": "v2", "created": "Sun, 30 Jun 2013 02:26:33 GMT"}, {"version": "v3", "created": "Wed, 30 Oct 2013 19:20:30 GMT"}, {"version": "v4", "created": "Sat, 25 Jan 2020 23:23:11 GMT"}], "update_date": "2020-01-28", "authors_parsed": [["Balcan", "Maria Florina", ""], ["Ehrlich", "Steven", ""], ["Liang", "Yingyu", ""]]}, {"id": "1306.0618", "submitter": "Adam Kapelner", "authors": "Adam Kapelner and Justin Bleich", "title": "Prediction with Missing Data via Bayesian Additive Regression Trees", "comments": "18 pages, 3 figures, 2 tables, 1 algorithm", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a method for incorporating missing data in non-parametric\nstatistical learning without the need for imputation. We focus on a tree-based\nmethod, Bayesian Additive Regression Trees (BART), enhanced with \"Missingness\nIncorporated in Attributes,\" an approach recently proposed incorporating\nmissingness into decision trees (Twala, 2008). This procedure takes advantage\nof the partitioning mechanisms found in tree-based models. Simulations on\ngenerated models and real data indicate that our proposed method can forecast\nwell on complicated missing-at-random and not-missing-at-random models as well\nas models where missingness itself influences the response. Our procedure has\nhigher predictive performance and is more stable than competitors in many\ncases. We also illustrate BART's abilities to incorporate missingness into\nuncertainty intervals and to detect the influence of missingness on the model\nfit.\n", "versions": [{"version": "v1", "created": "Mon, 3 Jun 2013 22:57:20 GMT"}, {"version": "v2", "created": "Wed, 28 Aug 2013 01:28:09 GMT"}, {"version": "v3", "created": "Wed, 12 Feb 2014 22:01:18 GMT"}], "update_date": "2014-02-14", "authors_parsed": [["Kapelner", "Adam", ""], ["Bleich", "Justin", ""]]}, {"id": "1306.0626", "submitter": "Prateek Jain", "authors": "Prateek Jain and Inderjit S. Dhillon", "title": "Provable Inductive Matrix Completion", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.IT math.IT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Consider a movie recommendation system where apart from the ratings\ninformation, side information such as user's age or movie's genre is also\navailable. Unlike standard matrix completion, in this setting one should be\nable to predict inductively on new users/movies. In this paper, we study the\nproblem of inductive matrix completion in the exact recovery setting. That is,\nwe assume that the ratings matrix is generated by applying feature vectors to a\nlow-rank matrix and the goal is to recover back the underlying matrix.\nFurthermore, we generalize the problem to that of low-rank matrix estimation\nusing rank-1 measurements. We study this generic problem and provide conditions\nthat the set of measurements should satisfy so that the alternating\nminimization method (which otherwise is a non-convex method with no convergence\nguarantees) is able to recover back the {\\em exact} underlying low-rank matrix.\n  In addition to inductive matrix completion, we show that two other low-rank\nestimation problems can be studied in our framework: a) general low-rank matrix\nsensing using rank-1 measurements, and b) multi-label regression with missing\nlabels. For both the problems, we provide novel and interesting bounds on the\nnumber of measurements required by alternating minimization to provably\nconverges to the {\\em exact} low-rank matrix. In particular, our analysis for\nthe general low rank matrix sensing problem significantly improves the required\nstorage and computational cost than that required by the RIP-based matrix\nsensing methods \\cite{RechtFP2007}. Finally, we provide empirical validation of\nour approach and demonstrate that alternating minimization is able to recover\nthe true matrix for the above mentioned problems using a small number of\nmeasurements.\n", "versions": [{"version": "v1", "created": "Tue, 4 Jun 2013 00:38:17 GMT"}], "update_date": "2013-06-05", "authors_parsed": [["Jain", "Prateek", ""], ["Dhillon", "Inderjit S.", ""]]}, {"id": "1306.0686", "submitter": "Pooria Joulani", "authors": "Pooria Joulani, Andr\\'as Gy\\\"orgy, Csaba Szepesv\\'ari", "title": "Online Learning under Delayed Feedback", "comments": "Extended version of a paper accepted to ICML-2013", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Online learning with delayed feedback has received increasing attention\nrecently due to its several applications in distributed, web-based learning\nproblems. In this paper we provide a systematic study of the topic, and analyze\nthe effect of delay on the regret of online learning algorithms. Somewhat\nsurprisingly, it turns out that delay increases the regret in a multiplicative\nway in adversarial problems, and in an additive way in stochastic problems. We\ngive meta-algorithms that transform, in a black-box fashion, algorithms\ndeveloped for the non-delayed case into ones that can handle the presence of\ndelays in the feedback loop. Modifications of the well-known UCB algorithm are\nalso developed for the bandit problem with delayed feedback, with the advantage\nover the meta-algorithms that they can be implemented with lower complexity.\n", "versions": [{"version": "v1", "created": "Tue, 4 Jun 2013 07:39:21 GMT"}, {"version": "v2", "created": "Wed, 5 Jun 2013 01:01:04 GMT"}], "update_date": "2015-07-02", "authors_parsed": [["Joulani", "Pooria", ""], ["Gy\u00f6rgy", "Andr\u00e1s", ""], ["Szepesv\u00e1ri", "Csaba", ""]]}, {"id": "1306.0733", "submitter": "Durk Kingma", "authors": "Diederik P Kingma", "title": "Fast Gradient-Based Inference with Continuous Latent Variable Models in\n  Auxiliary Form", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a technique for increasing the efficiency of gradient-based\ninference and learning in Bayesian networks with multiple layers of continuous\nlatent vari- ables. We show that, in many cases, it is possible to express such\nmodels in an auxiliary form, where continuous latent variables are\nconditionally deterministic given their parents and a set of independent\nauxiliary variables. Variables of mod- els in this auxiliary form have much\nlarger Markov blankets, leading to significant speedups in gradient-based\ninference, e.g. rapid mixing Hybrid Monte Carlo and efficient gradient-based\noptimization. The relative efficiency is confirmed in ex- periments.\n", "versions": [{"version": "v1", "created": "Tue, 4 Jun 2013 11:28:32 GMT"}], "update_date": "2013-06-05", "authors_parsed": [["Kingma", "Diederik P", ""]]}, {"id": "1306.0811", "submitter": "Giovanni Zappella", "authors": "Nicol\\`o Cesa-Bianchi, Claudio Gentile and Giovanni Zappella", "title": "A Gang of Bandits", "comments": "NIPS 2013", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.SI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multi-armed bandit problems are receiving a great deal of attention because\nthey adequately formalize the exploration-exploitation trade-offs arising in\nseveral industrially relevant applications, such as online advertisement and,\nmore generally, recommendation systems. In many cases, however, these\napplications have a strong social component, whose integration in the bandit\nalgorithm could lead to a dramatic performance increase. For instance, we may\nwant to serve content to a group of users by taking advantage of an underlying\nnetwork of social relationships among them. In this paper, we introduce novel\nalgorithmic approaches to the solution of such networked bandit problems. More\nspecifically, we design and analyze a global strategy which allocates a bandit\nalgorithm to each network node (user) and allows it to \"share\" signals\n(contexts and payoffs) with the neghboring nodes. We then derive two more\nscalable variants of this strategy based on different ways of clustering the\ngraph nodes. We experimentally compare the algorithm and its variants to\nstate-of-the-art methods for contextual bandits that do not use the relational\ninformation. Our experiments, carried out on synthetic and real-world datasets,\nshow a marked increase in prediction performance obtained by exploiting the\nnetwork structure.\n", "versions": [{"version": "v1", "created": "Tue, 4 Jun 2013 14:24:31 GMT"}, {"version": "v2", "created": "Fri, 25 Oct 2013 16:32:25 GMT"}, {"version": "v3", "created": "Mon, 4 Nov 2013 10:07:42 GMT"}], "update_date": "2013-11-05", "authors_parsed": [["Cesa-Bianchi", "Nicol\u00f2", ""], ["Gentile", "Claudio", ""], ["Zappella", "Giovanni", ""]]}, {"id": "1306.0842", "submitter": "Krikamol Muandet", "authors": "Krikamol Muandet, Kenji Fukumizu, Bharath Sriperumbudur, Arthur\n  Gretton, Bernhard Sch\\\"olkopf", "title": "Kernel Mean Estimation and Stein's Effect", "comments": "first draft", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A mean function in reproducing kernel Hilbert space, or a kernel mean, is an\nimportant part of many applications ranging from kernel principal component\nanalysis to Hilbert-space embedding of distributions. Given finite samples, an\nempirical average is the standard estimate for the true kernel mean. We show\nthat this estimator can be improved via a well-known phenomenon in statistics\ncalled Stein's phenomenon. After consideration, our theoretical analysis\nreveals the existence of a wide class of estimators that are better than the\nstandard. Focusing on a subset of this class, we propose efficient shrinkage\nestimators for the kernel mean. Empirical evaluations on several benchmark\napplications clearly demonstrate that the proposed estimators outperform the\nstandard kernel mean estimator.\n", "versions": [{"version": "v1", "created": "Tue, 4 Jun 2013 16:09:20 GMT"}, {"version": "v2", "created": "Thu, 6 Jun 2013 17:18:25 GMT"}], "update_date": "2013-06-07", "authors_parsed": [["Muandet", "Krikamol", ""], ["Fukumizu", "Kenji", ""], ["Sriperumbudur", "Bharath", ""], ["Gretton", "Arthur", ""], ["Sch\u00f6lkopf", "Bernhard", ""]]}, {"id": "1306.0886", "submitter": "Felix X. Yu", "authors": "Felix X. Yu, Dong Liu, Sanjiv Kumar, Tony Jebara, Shih-Fu Chang", "title": "$\\propto$SVM for learning with label proportions", "comments": "Appears in Proceedings of the 30th International Conference on\n  Machine Learning (ICML 2013)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of learning with label proportions in which the training\ndata is provided in groups and only the proportion of each class in each group\nis known. We propose a new method called proportion-SVM, or $\\propto$SVM, which\nexplicitly models the latent unknown instance labels together with the known\ngroup label proportions in a large-margin framework. Unlike the existing works,\nour approach avoids making restrictive assumptions about the data. The\n$\\propto$SVM model leads to a non-convex integer programming problem. In order\nto solve it efficiently, we propose two algorithms: one based on simple\nalternating optimization and the other based on a convex relaxation. Extensive\nexperiments on standard datasets show that $\\propto$SVM outperforms the\nstate-of-the-art, especially for larger group sizes.\n", "versions": [{"version": "v1", "created": "Tue, 4 Jun 2013 19:35:31 GMT"}], "update_date": "2013-06-05", "authors_parsed": [["Yu", "Felix X.", ""], ["Liu", "Dong", ""], ["Kumar", "Sanjiv", ""], ["Jebara", "Tony", ""], ["Chang", "Shih-Fu", ""]]}, {"id": "1306.0940", "submitter": "Ian Osband", "authors": "Ian Osband, Daniel Russo, Benjamin Van Roy", "title": "(More) Efficient Reinforcement Learning via Posterior Sampling", "comments": "10 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most provably-efficient learning algorithms introduce optimism about\npoorly-understood states and actions to encourage exploration. We study an\nalternative approach for efficient exploration, posterior sampling for\nreinforcement learning (PSRL). This algorithm proceeds in repeated episodes of\nknown duration. At the start of each episode, PSRL updates a prior distribution\nover Markov decision processes and takes one sample from this posterior. PSRL\nthen follows the policy that is optimal for this sample during the episode. The\nalgorithm is conceptually simple, computationally efficient and allows an agent\nto encode prior knowledge in a natural way. We establish an $\\tilde{O}(\\tau S\n\\sqrt{AT})$ bound on the expected regret, where $T$ is time, $\\tau$ is the\nepisode length and $S$ and $A$ are the cardinalities of the state and action\nspaces. This bound is one of the first for an algorithm not based on optimism,\nand close to the state of the art for any reinforcement learning algorithm. We\nshow through simulation that PSRL significantly outperforms existing algorithms\nwith similar regret bounds.\n", "versions": [{"version": "v1", "created": "Tue, 4 Jun 2013 23:00:56 GMT"}, {"version": "v2", "created": "Wed, 25 Sep 2013 05:14:31 GMT"}, {"version": "v3", "created": "Thu, 26 Sep 2013 00:38:51 GMT"}, {"version": "v4", "created": "Wed, 13 Nov 2013 19:31:26 GMT"}, {"version": "v5", "created": "Thu, 26 Dec 2013 09:20:29 GMT"}], "update_date": "2013-12-30", "authors_parsed": [["Osband", "Ian", ""], ["Russo", "Daniel", ""], ["Van Roy", "Benjamin", ""]]}, {"id": "1306.1066", "submitter": "Benjamin Rubinstein", "authors": "Christos Dimitrakakis and Blaine Nelson and and Zuhe Zhang and\n  Aikaterini Mitrokotsa and Benjamin Rubinstein", "title": "Bayesian Differential Privacy through Posterior Sampling", "comments": "38 pages; An earlier version of this article was published in ALT\n  2014. This version has corrections and additional results", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Differential privacy formalises privacy-preserving mechanisms that provide\naccess to a database. We pose the question of whether Bayesian inference itself\ncan be used directly to provide private access to data, with no modification.\nThe answer is affirmative: under certain conditions on the prior, sampling from\nthe posterior distribution can be used to achieve a desired level of privacy\nand utility. To do so, we generalise differential privacy to arbitrary dataset\nmetrics, outcome spaces and distribution families. This allows us to also deal\nwith non-i.i.d or non-tabular datasets. We prove bounds on the sensitivity of\nthe posterior to the data, which gives a measure of robustness. We also show\nhow to use posterior sampling to provide differentially private responses to\nqueries, within a decision-theoretic framework. Finally, we provide bounds on\nthe utility and on the distinguishability of datasets. The latter are\ncomplemented by a novel use of Le Cam's method to obtain lower bounds. All our\ngeneral results hold for arbitrary database metrics, including those for the\ncommon definition of differential privacy. For specific choices of the metric,\nwe give a number of examples satisfying our assumptions.\n", "versions": [{"version": "v1", "created": "Wed, 5 Jun 2013 11:38:46 GMT"}, {"version": "v2", "created": "Sat, 1 Feb 2014 13:40:36 GMT"}, {"version": "v3", "created": "Sun, 30 Mar 2014 15:31:32 GMT"}, {"version": "v4", "created": "Sun, 12 Jul 2015 03:44:30 GMT"}, {"version": "v5", "created": "Fri, 23 Dec 2016 12:28:36 GMT"}], "update_date": "2016-12-26", "authors_parsed": [["Dimitrakakis", "Christos", ""], ["Nelson", "Blaine", ""], ["Zhang", "and Zuhe", ""], ["Mitrokotsa", "Aikaterini", ""], ["Rubinstein", "Benjamin", ""]]}, {"id": "1306.1083", "submitter": "Puneet Kumar", "authors": "Pierre-Yves Baudin (INRIA Saclay - Ile de France), Danny Goodman,\n  Puneet Kumar (INRIA Saclay - Ile de France, CVN), Noura Azzabou (MIRCEN,\n  UPMC), Pierre G. Carlier (UPMC), Nikos Paragios (INRIA Saclay - Ile de\n  France, LIGM, ENPC, MAS), M. Pawan Kumar (INRIA Saclay - Ile de France, CVN)", "title": "Discriminative Parameter Estimation for Random Walks Segmentation:\n  Technical Report", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Random Walks (RW) algorithm is one of the most e - cient and easy-to-use\nprobabilistic segmentation methods. By combining contrast terms with prior\nterms, it provides accurate segmentations of medical images in a fully\nautomated manner. However, one of the main drawbacks of using the RW algorithm\nis that its parameters have to be hand-tuned. we propose a novel discriminative\nlearning framework that estimates the parameters using a training dataset. The\nmain challenge we face is that the training samples are not fully supervised.\nSpeci cally, they provide a hard segmentation of the images, instead of a\nproba-bilistic segmentation. We overcome this challenge by treating the optimal\nprobabilistic segmentation that is compatible with the given hard segmentation\nas a latent variable. This allows us to employ the latent support vector\nmachine formulation for parameter estimation. We show that our approach signi\ncantly outperforms the baseline methods on a challenging dataset consisting of\nreal clinical 3D MRI volumes of skeletal muscles.\n", "versions": [{"version": "v1", "created": "Wed, 5 Jun 2013 12:48:02 GMT"}], "update_date": "2013-06-06", "authors_parsed": [["Baudin", "Pierre-Yves", "", "INRIA Saclay - Ile de France"], ["Goodman", "Danny", "", "INRIA Saclay - Ile de France, CVN"], ["Kumar", "Puneet", "", "INRIA Saclay - Ile de France, CVN"], ["Azzabou", "Noura", "", "MIRCEN,\n  UPMC"], ["Carlier", "Pierre G.", "", "UPMC"], ["Paragios", "Nikos", "", "INRIA Saclay - Ile de\n  France, LIGM, ENPC, MAS"], ["Kumar", "M. Pawan", "", "INRIA Saclay - Ile de France, CVN"]]}, {"id": "1306.1091", "submitter": "Yoshua Bengio", "authors": "Yoshua Bengio, \\'Eric Thibodeau-Laufer, Guillaume Alain and Jason\n  Yosinski", "title": "Deep Generative Stochastic Networks Trainable by Backprop", "comments": "arXiv admin note: text overlap with arXiv:1305.0445, Also published\n  in ICML'2014", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a novel training principle for probabilistic models that is an\nalternative to maximum likelihood. The proposed Generative Stochastic Networks\n(GSN) framework is based on learning the transition operator of a Markov chain\nwhose stationary distribution estimates the data distribution. The transition\ndistribution of the Markov chain is conditional on the previous state,\ngenerally involving a small move, so this conditional distribution has fewer\ndominant modes, being unimodal in the limit of small moves. Thus, it is easier\nto learn because it is easier to approximate its partition function, more like\nlearning to perform supervised function approximation, with gradients that can\nbe obtained by backprop. We provide theorems that generalize recent work on the\nprobabilistic interpretation of denoising autoencoders and obtain along the way\nan interesting justification for dependency networks and generalized\npseudolikelihood, along with a definition of an appropriate joint distribution\nand sampling mechanism even when the conditionals are not consistent. GSNs can\nbe used with missing inputs and can be used to sample subsets of variables\ngiven the rest. We validate these theoretical results with experiments on two\nimage datasets using an architecture that mimics the Deep Boltzmann Machine\nGibbs sampler but allows training to proceed with simple backprop, without the\nneed for layerwise pretraining.\n", "versions": [{"version": "v1", "created": "Wed, 5 Jun 2013 13:01:14 GMT"}, {"version": "v2", "created": "Fri, 7 Jun 2013 16:55:38 GMT"}, {"version": "v3", "created": "Fri, 25 Oct 2013 07:04:58 GMT"}, {"version": "v4", "created": "Wed, 18 Dec 2013 19:46:07 GMT"}, {"version": "v5", "created": "Sat, 24 May 2014 00:05:18 GMT"}], "update_date": "2014-05-27", "authors_parsed": [["Bengio", "Yoshua", ""], ["Thibodeau-Laufer", "\u00c9ric", ""], ["Alain", "Guillaume", ""], ["Yosinski", "Jason", ""]]}, {"id": "1306.1185", "submitter": "Thomas Laurent", "authors": "Xavier Bresson, Thomas Laurent, David Uminsky and James H. von Brecht", "title": "Multiclass Total Variation Clustering", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Ideas from the image processing literature have recently motivated a new set\nof clustering algorithms that rely on the concept of total variation. While\nthese algorithms perform well for bi-partitioning tasks, their recursive\nextensions yield unimpressive results for multiclass clustering tasks. This\npaper presents a general framework for multiclass total variation clustering\nthat does not rely on recursion. The results greatly outperform previous total\nvariation algorithms and compare well with state-of-the-art NMF approaches.\n", "versions": [{"version": "v1", "created": "Wed, 5 Jun 2013 17:42:57 GMT"}], "update_date": "2013-06-06", "authors_parsed": [["Bresson", "Xavier", ""], ["Laurent", "Thomas", ""], ["Uminsky", "David", ""], ["von Brecht", "James H.", ""]]}, {"id": "1306.1298", "submitter": "Allon G. Percus", "authors": "Cristina Garcia-Cardona, Arjuna Flenner, Allon G. Percus", "title": "Multiclass Semi-Supervised Learning on Graphs using Ginzburg-Landau\n  Functional Minimization", "comments": "16 pages, to appear in Springer's Lecture Notes in Computer Science\n  volume \"Pattern Recognition Applications and Methods 2013\", part of series on\n  Advances in Intelligent and Soft Computing", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.ST physics.data-an stat.TH", "license": "http://creativecommons.org/licenses/publicdomain/", "abstract": "  We present a graph-based variational algorithm for classification of\nhigh-dimensional data, generalizing the binary diffuse interface model to the\ncase of multiple classes. Motivated by total variation techniques, the method\ninvolves minimizing an energy functional made up of three terms. The first two\nterms promote a stepwise continuous classification function with sharp\ntransitions between classes, while preserving symmetry among the class labels.\nThe third term is a data fidelity term, allowing us to incorporate prior\ninformation into the model in a semi-supervised framework. The performance of\nthe algorithm on synthetic data, as well as on the COIL and MNIST benchmark\ndatasets, is competitive with state-of-the-art graph-based multiclass\nsegmentation methods.\n", "versions": [{"version": "v1", "created": "Thu, 6 Jun 2013 05:32:00 GMT"}], "update_date": "2013-06-07", "authors_parsed": [["Garcia-Cardona", "Cristina", ""], ["Flenner", "Arjuna", ""], ["Percus", "Allon G.", ""]]}, {"id": "1306.1323", "submitter": "E.N.Sathishkumar", "authors": "T. Chandrasekhar, K. Thangavel, E.N. Sathishkumar", "title": "Verdict Accuracy of Quick Reduct Algorithm using Clustering and\n  Classification Techniques for Gene Expression Data", "comments": "7 pages, 3 figures", "journal-ref": "IJCSI International Journal of Computer Science Issues, Vol. 9,\n  Issue 1, No 1, January 2012, page no. 357-363", "doi": null, "report-no": null, "categories": "cs.LG cs.CE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In most gene expression data, the number of training samples is very small\ncompared to the large number of genes involved in the experiments. However,\namong the large amount of genes, only a small fraction is effective for\nperforming a certain task. Furthermore, a small subset of genes is desirable in\ndeveloping gene expression based diagnostic tools for delivering reliable and\nunderstandable results. With the gene selection results, the cost of biological\nexperiment and decision can be greatly reduced by analyzing only the marker\ngenes. An important application of gene expression data in functional genomics\nis to classify samples according to their gene expression profiles. Feature\nselection (FS) is a process which attempts to select more informative features.\nIt is one of the important steps in knowledge discovery. Conventional\nsupervised FS methods evaluate various feature subsets using an evaluation\nfunction or metric to select only those features which are related to the\ndecision classes of the data under consideration. This paper studies a feature\nselection method based on rough set theory. Further K-Means, Fuzzy C-Means\n(FCM) algorithm have implemented for the reduced feature set without\nconsidering class labels. Then the obtained results are compared with the\noriginal class labels. Back Propagation Network (BPN) has also been used for\nclassification. Then the performance of K-Means, FCM, and BPN are analyzed\nthrough the confusion matrix. It is found that the BPN is performing well\ncomparatively.\n", "versions": [{"version": "v1", "created": "Thu, 6 Jun 2013 07:26:06 GMT"}], "update_date": "2013-06-07", "authors_parsed": [["Chandrasekhar", "T.", ""], ["Thangavel", "K.", ""], ["Sathishkumar", "E. N.", ""]]}, {"id": "1306.1326", "submitter": "E.N.Sathishkumar", "authors": "A. Nisthana Parveen, H. Hannah Inbarani, E.N. Sathishkumar", "title": "Performance analysis of unsupervised feature selection methods", "comments": "7 pages, Conference Publications", "journal-ref": null, "doi": "10.1109/ICCCA.2012.6179181", "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Feature selection (FS) is a process which attempts to select more informative\nfeatures. In some cases, too many redundant or irrelevant features may\noverpower main features for classification. Feature selection can remedy this\nproblem and therefore improve the prediction accuracy and reduce the\ncomputational overhead of classification algorithms. The main aim of feature\nselection is to determine a minimal feature subset from a problem domain while\nretaining a suitably high accuracy in representing the original features. In\nthis paper, Principal Component Analysis (PCA), Rough PCA, Unsupervised Quick\nReduct (USQR) algorithm and Empirical Distribution Ranking (EDR) approaches are\napplied to discover discriminative features that will be the most adequate ones\nfor classification. Efficiency of the approaches is evaluated using standard\nclassification metrics.\n", "versions": [{"version": "v1", "created": "Thu, 6 Jun 2013 07:42:33 GMT"}], "update_date": "2013-06-07", "authors_parsed": [["Parveen", "A. Nisthana", ""], ["Inbarani", "H. Hannah", ""], ["Sathishkumar", "E. N.", ""]]}, {"id": "1306.1350", "submitter": "Tuomo Sipola", "authors": "Tuomo Sipola, Fengyu Cong, Tapani Ristaniemi, Vinoo Alluri, Petri\n  Toiviainen, Elvira Brattico, Asoke K. Nandi", "title": "Diffusion map for clustering fMRI spatial maps extracted by independent\n  component analysis", "comments": "6 pages. 8 figures. Copyright (c) 2013 IEEE. Published at 2013 IEEE\n  International Workshop on Machine Learning for Signal Processing", "journal-ref": null, "doi": "10.1109/MLSP.2013.6661923", "report-no": null, "categories": "cs.CE cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Functional magnetic resonance imaging (fMRI) produces data about activity\ninside the brain, from which spatial maps can be extracted by independent\ncomponent analysis (ICA). In datasets, there are n spatial maps that contain p\nvoxels. The number of voxels is very high compared to the number of analyzed\nspatial maps. Clustering of the spatial maps is usually based on correlation\nmatrices. This usually works well, although such a similarity matrix inherently\ncan explain only a certain amount of the total variance contained in the\nhigh-dimensional data where n is relatively small but p is large. For\nhigh-dimensional space, it is reasonable to perform dimensionality reduction\nbefore clustering. In this research, we used the recently developed diffusion\nmap for dimensionality reduction in conjunction with spectral clustering. This\nresearch revealed that the diffusion map based clustering worked as well as the\nmore traditional methods, and produced more compact clusters when needed.\n", "versions": [{"version": "v1", "created": "Thu, 6 Jun 2013 09:29:25 GMT"}, {"version": "v2", "created": "Fri, 14 Jun 2013 06:44:37 GMT"}, {"version": "v3", "created": "Sun, 14 Jul 2013 16:03:54 GMT"}, {"version": "v4", "created": "Fri, 27 Sep 2013 08:58:30 GMT"}], "update_date": "2016-11-17", "authors_parsed": [["Sipola", "Tuomo", ""], ["Cong", "Fengyu", ""], ["Ristaniemi", "Tapani", ""], ["Alluri", "Vinoo", ""], ["Toiviainen", "Petri", ""], ["Brattico", "Elvira", ""], ["Nandi", "Asoke K.", ""]]}, {"id": "1306.1433", "submitter": "Spencer Greenberg", "authors": "Spencer Greenberg, Mehryar Mohri", "title": "Tight Lower Bound on the Probability of a Binomial Exceeding its\n  Expectation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We give the proof of a tight lower bound on the probability that a binomial\nrandom variable exceeds its expected value. The inequality plays an important\nrole in a variety of contexts, including the analysis of relative deviation\nbounds in learning theory and generalization bounds for unbounded loss\nfunctions.\n", "versions": [{"version": "v1", "created": "Thu, 6 Jun 2013 15:15:07 GMT"}, {"version": "v2", "created": "Thu, 7 Nov 2013 16:40:39 GMT"}, {"version": "v3", "created": "Mon, 11 Nov 2013 04:43:10 GMT"}], "update_date": "2013-11-12", "authors_parsed": [["Greenberg", "Spencer", ""], ["Mohri", "Mehryar", ""]]}, {"id": "1306.1467", "submitter": "Munther Abualkibash", "authors": "Munther Abualkibash, Ahmed ElSayed, Ausif Mahmood", "title": "Highly Scalable, Parallel and Distributed AdaBoost Algorithm using Light\n  Weight Threads and Web Services on a Network of Multi-Core Machines", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  AdaBoost is an important algorithm in machine learning and is being widely\nused in object detection. AdaBoost works by iteratively selecting the best\namongst weak classifiers, and then combines several weak classifiers to obtain\na strong classifier. Even though AdaBoost has proven to be very effective, its\nlearning execution time can be quite large depending upon the application e.g.,\nin face detection, the learning time can be several days. Due to its increasing\nuse in computer vision applications, the learning time needs to be drastically\nreduced so that an adaptive near real time object detection system can be\nincorporated. In this paper, we develop a hybrid parallel and distributed\nAdaBoost algorithm that exploits the multiple cores in a CPU via light weight\nthreads, and also uses multiple machines via a web service software\narchitecture to achieve high scalability. We present a novel hierarchical web\nservices based distributed architecture and achieve nearly linear speedup up to\nthe number of processors available to us. In comparison with the previously\npublished work, which used a single level master-slave parallel and distributed\nimplementation [1] and only achieved a speedup of 2.66 on four nodes, we\nachieve a speedup of 95.1 on 31 workstations each having a quad-core processor,\nresulting in a learning time of only 4.8 seconds per feature.\n", "versions": [{"version": "v1", "created": "Thu, 6 Jun 2013 16:38:26 GMT"}], "update_date": "2013-06-07", "authors_parsed": [["Abualkibash", "Munther", ""], ["ElSayed", "Ahmed", ""], ["Mahmood", "Ausif", ""]]}, {"id": "1306.1491", "submitter": "Kian Hsiang Low", "authors": "Jie Chen, Kian Hsiang Low, Colin Keng-Yan Tan", "title": "Gaussian Process-Based Decentralized Data Fusion and Active Sensing for\n  Mobility-on-Demand System", "comments": "Robotics: Science and Systems (RSS 2013), Extended version with\n  proofs, 10 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.DC cs.LG cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mobility-on-demand (MoD) systems have recently emerged as a promising\nparadigm of one-way vehicle sharing for sustainable personal urban mobility in\ndensely populated cities. In this paper, we enhance the capability of a MoD\nsystem by deploying robotic shared vehicles that can autonomously cruise the\nstreets to be hailed by users. A key challenge to managing the MoD system\neffectively is that of real-time, fine-grained mobility demand sensing and\nprediction. This paper presents a novel decentralized data fusion and active\nsensing algorithm for real-time, fine-grained mobility demand sensing and\nprediction with a fleet of autonomous robotic vehicles in a MoD system. Our\nGaussian process (GP)-based decentralized data fusion algorithm can achieve a\nfine balance between predictive power and time efficiency. We theoretically\nguarantee its predictive performance to be equivalent to that of a\nsophisticated centralized sparse approximation for the GP model: The\ncomputation of such a sparse approximate GP model can thus be distributed among\nthe MoD vehicles, hence achieving efficient and scalable demand prediction.\nThough our decentralized active sensing strategy is devised to gather the most\ninformative demand data for demand prediction, it can achieve a dual effect of\nfleet rebalancing to service the mobility demands. Empirical evaluation on\nreal-world mobility demand data shows that our proposed algorithm can achieve a\nbetter balance between predictive accuracy and time efficiency than\nstate-of-the-art algorithms.\n", "versions": [{"version": "v1", "created": "Sun, 2 Jun 2013 14:05:49 GMT"}], "update_date": "2013-06-07", "authors_parsed": [["Chen", "Jie", ""], ["Low", "Kian Hsiang", ""], ["Tan", "Colin Keng-Yan", ""]]}, {"id": "1306.1520", "submitter": "Bruno Scherrer", "authors": "Bruno Scherrer (INRIA Nancy - Grand Est / LORIA), Matthieu Geist", "title": "Policy Search: Any Local Optimum Enjoys a Global Performance Guarantee", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.RO math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Local Policy Search is a popular reinforcement learning approach for handling\nlarge state spaces. Formally, it searches locally in a paramet erized policy\nspace in order to maximize the associated value function averaged over some\npredefined distribution. It is probably commonly b elieved that the best one\ncan hope in general from such an approach is to get a local optimum of this\ncriterion. In this article, we show th e following surprising result:\n\\emph{any} (approximate) \\emph{local optimum} enjoys a \\emph{global performance\nguarantee}. We compare this g uarantee with the one that is satisfied by Direct\nPolicy Iteration, an approximate dynamic programming algorithm that does some\nform of Poli cy Search: if the approximation error of Local Policy Search may\ngenerally be bigger (because local search requires to consider a space of s\ntochastic policies), we argue that the concentrability coefficient that appears\nin the performance bound is much nicer. Finally, we discuss several practical\nand theoretical consequences of our analysis.\n", "versions": [{"version": "v1", "created": "Thu, 6 Jun 2013 19:27:01 GMT"}], "update_date": "2013-06-07", "authors_parsed": [["Scherrer", "Bruno", "", "INRIA Nancy - Grand Est / LORIA"], ["Geist", "Matthieu", ""]]}, {"id": "1306.1716", "submitter": "Alexander Petukhov", "authors": "Alexander Petukhov and Inna Kozlov", "title": "Fast greedy algorithm for subspace clustering from corrupted and\n  incomplete data", "comments": "arXiv admin note: substantial text overlap with arXiv:1304.4282", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS math.NA stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe the Fast Greedy Sparse Subspace Clustering (FGSSC) algorithm\nproviding an efficient method for clustering data belonging to a few\nlow-dimensional linear or affine subspaces. The main difference of our\nalgorithm from predecessors is its ability to work with noisy data having a\nhigh rate of erasures (missed entries with the known coordinates) and errors\n(corrupted entries with unknown coordinates). We discuss here how to implement\nthe fast version of the greedy algorithm with the maximum efficiency whose\ngreedy strategy is incorporated into iterations of the basic algorithm.\n  We provide numerical evidences that, in the subspace clustering capability,\nthe fast greedy algorithm outperforms not only the existing state-of-the art\nSSC algorithm taken by the authors as a basic algorithm but also the recent\nGSSC algorithm. At the same time, its computational cost is only slightly\nhigher than the cost of SSC.\n  The numerical evidence of the algorithm significant advantage is presented\nfor a few synthetic models as well as for the Extended Yale B dataset of facial\nimages. In particular, the face recognition misclassification rate turned out\nto be 6-20 times lower than for the SSC algorithm. We provide also the\nnumerical evidence that the FGSSC algorithm is able to perform clustering of\ncorrupted data efficiently even when the sum of subspace dimensions\nsignificantly exceeds the dimension of the ambient space.\n", "versions": [{"version": "v1", "created": "Fri, 7 Jun 2013 13:14:50 GMT"}], "update_date": "2013-06-10", "authors_parsed": [["Petukhov", "Alexander", ""], ["Kozlov", "Inna", ""]]}, {"id": "1306.1840", "submitter": "Paul Mineiro", "authors": "Paul Mineiro, Nikos Karampatziakis", "title": "Loss-Proportional Subsampling for Subsequent ERM", "comments": "Appears in the proceedings of the 30th International Conference on\n  Machine Learning", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a sampling scheme suitable for reducing a data set prior to\nselecting a hypothesis with minimum empirical risk. The sampling only considers\na subset of the ultimate (unknown) hypothesis set, but can nonetheless\nguarantee that the final excess risk will compare favorably with utilizing the\nentire original data set. We demonstrate the practical benefits of our approach\non a large dataset which we subsample and subsequently fit with boosted trees.\n", "versions": [{"version": "v1", "created": "Fri, 7 Jun 2013 20:12:17 GMT"}, {"version": "v2", "created": "Sun, 23 Jun 2013 05:32:31 GMT"}], "update_date": "2013-06-25", "authors_parsed": [["Mineiro", "Paul", ""], ["Karampatziakis", "Nikos", ""]]}, {"id": "1306.1913", "submitter": "Zoltan Szabo", "authors": "Andras Lorincz, Laszlo Jeni, Zoltan Szabo, Jeffrey Cohn, Takeo Kanade", "title": "Emotional Expression Classification using Time-Series Kernels", "comments": "IEEE International Workshop on Analysis and Modeling of Faces and\n  Gestures, Portland, Oregon, 28 June 2013 (accepted)", "journal-ref": null, "doi": "10.1109/CVPRW.2013.131", "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Estimation of facial expressions, as spatio-temporal processes, can take\nadvantage of kernel methods if one considers facial landmark positions and\ntheir motion in 3D space. We applied support vector classification with kernels\nderived from dynamic time-warping similarity measures. We achieved over 99%\naccuracy - measured by area under ROC curve - using only the 'motion pattern'\nof the PCA compressed representation of the marker point vector, the so-called\nshape parameters. Beyond the classification of full motion patterns, several\nexpressions were recognized with over 90% accuracy in as few as 5-6 frames from\ntheir onset, about 200 milliseconds.\n", "versions": [{"version": "v1", "created": "Sat, 8 Jun 2013 12:57:39 GMT"}], "update_date": "2016-11-17", "authors_parsed": [["Lorincz", "Andras", ""], ["Jeni", "Laszlo", ""], ["Szabo", "Zoltan", ""], ["Cohn", "Jeffrey", ""], ["Kanade", "Takeo", ""]]}, {"id": "1306.2035", "submitter": "Martin Azizyan", "authors": "Martin Azizyan, Aarti Singh, Larry Wasserman", "title": "Minimax Theory for High-dimensional Gaussian Mixtures with Sparse Mean\n  Separation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While several papers have investigated computationally and statistically\nefficient methods for learning Gaussian mixtures, precise minimax bounds for\ntheir statistical performance as well as fundamental limits in high-dimensional\nsettings are not well-understood. In this paper, we provide precise information\ntheoretic bounds on the clustering accuracy and sample complexity of learning a\nmixture of two isotropic Gaussians in high dimensions under small mean\nseparation. If there is a sparse subset of relevant dimensions that determine\nthe mean separation, then the sample complexity only depends on the number of\nrelevant dimensions and mean separation, and can be achieved by a simple\ncomputationally efficient procedure. Our results provide the first step of a\ntheoretical basis for recent methods that combine feature selection and\nclustering.\n", "versions": [{"version": "v1", "created": "Sun, 9 Jun 2013 16:28:56 GMT"}], "update_date": "2013-06-11", "authors_parsed": [["Azizyan", "Martin", ""], ["Singh", "Aarti", ""], ["Wasserman", "Larry", ""]]}, {"id": "1306.2084", "submitter": "Maximilian Nickel", "authors": "Maximilian Nickel, Volker Tresp", "title": "Logistic Tensor Factorization for Multi-Relational Data", "comments": "Accepted at ICML 2013 Workshop \"Structured Learning: Inferring Graphs\n  from Structured and Unstructured Inputs\" (SLG 2013)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Tensor factorizations have become increasingly popular approaches for various\nlearning tasks on structured data. In this work, we extend the RESCAL tensor\nfactorization, which has shown state-of-the-art results for multi-relational\nlearning, to account for the binary nature of adjacency tensors. We study the\nimprovements that can be gained via this approach on various benchmark datasets\nand show that the logistic extension can improve the prediction results\nsignificantly.\n", "versions": [{"version": "v1", "created": "Mon, 10 Jun 2013 01:45:49 GMT"}], "update_date": "2013-06-11", "authors_parsed": [["Nickel", "Maximilian", ""], ["Tresp", "Volker", ""]]}, {"id": "1306.2094", "submitter": "Si-Chi Chin Si-Chi Chin", "authors": "Kiyana Zolfaghar, Nele Verbiest, Jayshree Agarwal, Naren Meadem,\n  Si-Chi Chin, Senjuti Basu Roy, Ankur Teredesai, David Hazel, Paul Amoroso,\n  Lester Reed", "title": "Predicting Risk-of-Readmission for Congestive Heart Failure Patients: A\n  Multi-Layer Approach", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mitigating risk-of-readmission of Congestive Heart Failure (CHF) patients\nwithin 30 days of discharge is important because such readmissions are not only\nexpensive but also critical indicator of provider care and quality of\ntreatment. Accurately predicting the risk-of-readmission may allow hospitals to\nidentify high-risk patients and eventually improve quality of care by\nidentifying factors that contribute to such readmissions in many scenarios. In\nthis paper, we investigate the problem of predicting risk-of-readmission as a\nsupervised learning problem, using a multi-layer classification approach.\nEarlier contributions inadequately attempted to assess a risk value for 30 day\nreadmission by building a direct predictive model as opposed to our approach.\nWe first split the problem into various stages, (a) at risk in general (b) risk\nwithin 60 days (c) risk within 30 days, and then build suitable classifiers for\neach stage, thereby increasing the ability to accurately predict the risk using\nmultiple layers of decision. The advantage of our approach is that we can use\ndifferent classification models for the subtasks that are more suited for the\nrespective problems. Moreover, each of the subtasks can be solved using\ndifferent features and training data leading to a highly confident diagnosis or\nrisk compared to a one-shot single layer approach. An experimental evaluation\non actual hospital patient record data from Multicare Health Systems shows that\nour model is significantly better at predicting risk-of-readmission of CHF\npatients within 30 days after discharge compared to prior attempts.\n", "versions": [{"version": "v1", "created": "Mon, 10 Jun 2013 03:18:25 GMT"}], "update_date": "2013-06-11", "authors_parsed": [["Zolfaghar", "Kiyana", ""], ["Verbiest", "Nele", ""], ["Agarwal", "Jayshree", ""], ["Meadem", "Naren", ""], ["Chin", "Si-Chi", ""], ["Roy", "Senjuti Basu", ""], ["Teredesai", "Ankur", ""], ["Hazel", "David", ""], ["Amoroso", "Paul", ""], ["Reed", "Lester", ""]]}, {"id": "1306.2118", "submitter": "E.N.Sathishkumar", "authors": "E.N.Sathishkumar, K.Thangavel, T.Chandrasekhar", "title": "A Novel Approach for Single Gene Selection Using Clustering and\n  Dimensionality Reduction", "comments": "6 pages, 4 figures. arXiv admin note: text overlap with\n  arXiv:1306.1323", "journal-ref": "International Journal of Scientific & Engineering Research, Volume\n  4, Issue 5, May-2013, page no 1540-1545", "doi": null, "report-no": null, "categories": "cs.CE cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We extend the standard rough set-based approach to deal with huge amounts of\nnumeric attributes versus small amount of available objects. Here, a novel\napproach of clustering along with dimensionality reduction; Hybrid Fuzzy C\nMeans-Quick Reduct (FCMQR) algorithm is proposed for single gene selection.\nGene selection is a process to select genes which are more informative. It is\none of the important steps in knowledge discovery. The problem is that all\ngenes are not important in gene expression data. Some of the genes may be\nredundant, and others may be irrelevant and noisy. In this study, the entire\ndataset is divided in proper grouping of similar genes by applying Fuzzy C\nMeans (FCM) algorithm. A high class discriminated genes has been selected based\non their degree of dependence by applying Quick Reduct algorithm based on Rough\nSet Theory to all the resultant clusters. Average Correlation Value (ACV) is\ncalculated for the high class discriminated genes. The clusters which have the\nACV value a s 1 is determined as significant clusters, whose classification\naccuracy will be equal or high when comparing to the accuracy of the entire\ndataset. The proposed algorithm is evaluated using WEKA classifiers and\ncompared. Finally, experimental results related to the leukemia cancer data\nconfirm that our approach is quite promising, though it surely requires further\nresearch.\n", "versions": [{"version": "v1", "created": "Mon, 10 Jun 2013 07:28:51 GMT"}], "update_date": "2013-06-11", "authors_parsed": [["Sathishkumar", "E. N.", ""], ["Thangavel", "K.", ""], ["Chandrasekhar", "T.", ""]]}, {"id": "1306.2119", "submitter": "Francis Bach", "authors": "Francis Bach (INRIA Paris - Rocquencourt, LIENS), Eric Moulines (LTCI)", "title": "Non-strongly-convex smooth stochastic approximation with convergence\n  rate O(1/n)", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the stochastic approximation problem where a convex function has\nto be minimized, given only the knowledge of unbiased estimates of its\ngradients at certain points, a framework which includes machine learning\nmethods based on the minimization of the empirical risk. We focus on problems\nwithout strong convexity, for which all previously known algorithms achieve a\nconvergence rate for function values of O(1/n^{1/2}). We consider and analyze\ntwo algorithms that achieve a rate of O(1/n) for classical supervised learning\nproblems. For least-squares regression, we show that averaged stochastic\ngradient descent with constant step-size achieves the desired rate. For\nlogistic regression, this is achieved by a simple novel stochastic gradient\nalgorithm that (a) constructs successive local quadratic approximations of the\nloss functions, while (b) preserving the same running time complexity as\nstochastic gradient descent. For these algorithms, we provide a non-asymptotic\nanalysis of the generalization error (in expectation, and also in high\nprobability for least-squares), and run extensive experiments on standard\nmachine learning benchmarks showing that they often outperform existing\napproaches.\n", "versions": [{"version": "v1", "created": "Mon, 10 Jun 2013 07:31:10 GMT"}], "update_date": "2013-06-11", "authors_parsed": [["Bach", "Francis", "", "INRIA Paris - Rocquencourt, LIENS"], ["Moulines", "Eric", "", "LTCI"]]}, {"id": "1306.2290", "submitter": "Xinjia Chen", "authors": "Xinjia Chen", "title": "Asymptotically Optimal Sequential Estimation of the Mean Based on\n  Inclusion Principle", "comments": "75 pages, no figures. The main results of this paper appeared in\n  Proceeding of SPIE Conferences", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.LG math.PR stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A large class of problems in sciences and engineering can be formulated as\nthe general problem of constructing random intervals with pre-specified\ncoverage probabilities for the mean. Wee propose a general approach for\nstatistical inference of mean values based on accumulated observational data.\nWe show that the construction of such random intervals can be accomplished by\ncomparing the endpoints of random intervals with confidence sequences for the\nmean. Asymptotic results are obtained for such sequential methods.\n", "versions": [{"version": "v1", "created": "Mon, 10 Jun 2013 19:11:25 GMT"}], "update_date": "2013-06-11", "authors_parsed": [["Chen", "Xinjia", ""]]}, {"id": "1306.2295", "submitter": "Alejandro Edera", "authors": "Alejandro Edera, Facundo Bromberg, and Federico Schl\\\"uter", "title": "Markov random fields factorization with context-specific independences", "comments": "7 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Markov random fields provide a compact representation of joint probability\ndistributions by representing its independence properties in an undirected\ngraph. The well-known Hammersley-Clifford theorem uses these conditional\nindependences to factorize a Gibbs distribution into a set of factors. However,\nan important issue of using a graph to represent independences is that it\ncannot encode some types of independence relations, such as the\ncontext-specific independences (CSIs). They are a particular case of\nconditional independences that is true only for a certain assignment of its\nconditioning set; in contrast to conditional independences that must hold for\nall its assignments. This work presents a method for factorizing a Markov\nrandom field according to CSIs present in a distribution, and formally\nguarantees that this factorization is correct. This is presented in our main\ncontribution, the context-specific Hammersley-Clifford theorem, a\ngeneralization to CSIs of the Hammersley-Clifford theorem that applies for\nconditional independences.\n", "versions": [{"version": "v1", "created": "Mon, 10 Jun 2013 19:36:31 GMT"}], "update_date": "2013-06-12", "authors_parsed": [["Edera", "Alejandro", ""], ["Bromberg", "Facundo", ""], ["Schl\u00fcter", "Federico", ""]]}, {"id": "1306.2298", "submitter": "Sadegh Motallebi", "authors": "Sadegh Motallebi, Sadegh Aliakbary, Jafar Habibi", "title": "Generative Model Selection Using a Scalable and Size-Independent Complex\n  Network Classifier", "comments": null, "journal-ref": "Chaos 23, 043127 (2013);", "doi": "10.1063/1.4840235", "report-no": null, "categories": "cs.SI cs.LG physics.soc-ph stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Real networks exhibit nontrivial topological features such as heavy-tailed\ndegree distribution, high clustering, and small-worldness. Researchers have\ndeveloped several generative models for synthesizing artificial networks that\nare structurally similar to real networks. An important research problem is to\nidentify the generative model that best fits to a target network. In this\npaper, we investigate this problem and our goal is to select the model that is\nable to generate graphs similar to a given network instance. By the means of\ngenerating synthetic networks with seven outstanding generative models, we have\nutilized machine learning methods to develop a decision tree for model\nselection. Our proposed method, which is named \"Generative Model Selection for\nComplex Networks\" (GMSCN), outperforms existing methods with respect to\naccuracy, scalability and size-independence.\n", "versions": [{"version": "v1", "created": "Mon, 10 Jun 2013 19:42:10 GMT"}, {"version": "v2", "created": "Wed, 19 Jun 2013 09:46:04 GMT"}, {"version": "v3", "created": "Sat, 1 Feb 2014 10:42:30 GMT"}], "update_date": "2014-02-04", "authors_parsed": [["Motallebi", "Sadegh", ""], ["Aliakbary", "Sadegh", ""], ["Habibi", "Jafar", ""]]}, {"id": "1306.2347", "submitter": "Sivan Sabato", "authors": "Sivan Sabato and Anand D. Sarwate and Nathan Srebro", "title": "Auditing: Active Learning with Outcome-Dependent Query Costs", "comments": "Corrections in section 5", "journal-ref": "Neural Information Processing Systems 26 (NIPS), 512-520, 2013", "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a learning setting in which unlabeled data is free, and the cost\nof a label depends on its value, which is not known in advance. We study binary\nclassification in an extreme case, where the algorithm only pays for negative\nlabels. Our motivation are applications such as fraud detection, in which\ninvestigating an honest transaction should be avoided if possible. We term the\nsetting auditing, and consider the auditing complexity of an algorithm: the\nnumber of negative labels the algorithm requires in order to learn a hypothesis\nwith low relative error. We design auditing algorithms for simple hypothesis\nclasses (thresholds and rectangles), and show that with these algorithms, the\nauditing complexity can be significantly lower than the active label\ncomplexity. We also discuss a general competitive approach for auditing and\npossible modifications to the framework.\n", "versions": [{"version": "v1", "created": "Mon, 10 Jun 2013 20:18:48 GMT"}, {"version": "v2", "created": "Fri, 27 Sep 2013 17:57:33 GMT"}, {"version": "v3", "created": "Tue, 15 Oct 2013 18:27:07 GMT"}, {"version": "v4", "created": "Sun, 12 Jul 2015 10:11:57 GMT"}], "update_date": "2015-07-14", "authors_parsed": [["Sabato", "Sivan", ""], ["Sarwate", "Anand D.", ""], ["Srebro", "Nathan", ""]]}, {"id": "1306.2533", "submitter": "Praneeth Vepakomma Praneeth Vepakomma", "authors": "Praneeth Vepakomma and Ahmed Elgammal", "title": "DISCOMAX: A Proximity-Preserving Distance Correlation Maximization\n  Algorithm", "comments": "Withdrawing as an updated and enhanced version of this paper is on\n  arxiv under my name as well titled Supervised Dimensionality Reduction via\n  Distance Correlation Maximization. See arXiv:1601.00236. That makes this\n  version pointless", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In a regression setting we propose algorithms that reduce the dimensionality\nof the features while simultaneously maximizing a statistical measure of\ndependence known as distance correlation between the low-dimensional features\nand a response variable. This helps in solving the prediction problem with a\nlow-dimensional set of features. Our setting is different from subset-selection\nalgorithms where the problem is to choose the best subset of features for\nregression. Instead, we attempt to generate a new set of low-dimensional\nfeatures as in a feature-learning setting. We attempt to keep our proposed\napproach as model-free and our algorithm does not assume the application of any\nspecific regression model in conjunction with the low-dimensional features that\nit learns. The algorithm is iterative and is fomulated as a combination of the\nmajorization-minimization and concave-convex optimization procedures. We also\npresent spectral radius based convergence results for the proposed iterations.\n", "versions": [{"version": "v1", "created": "Tue, 11 Jun 2013 14:13:46 GMT"}, {"version": "v2", "created": "Wed, 26 Jun 2013 04:12:48 GMT"}, {"version": "v3", "created": "Fri, 17 Feb 2017 13:37:25 GMT"}], "update_date": "2017-02-20", "authors_parsed": [["Vepakomma", "Praneeth", ""], ["Elgammal", "Ahmed", ""]]}, {"id": "1306.2547", "submitter": "Aryeh Kontorovich", "authors": "Lee-Ad Gottlieb and Aryeh Kontorovich and Robert Krauthgamer", "title": "Efficient Classification for Metric Data", "comments": "This is the full version of an extended abstract that appeared in\n  Proceedings of the 23rd COLT, 2010", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent advances in large-margin classification of data residing in general\nmetric spaces (rather than Hilbert spaces) enable classification under various\nnatural metrics, such as string edit and earthmover distance. A general\nframework developed for this purpose by von Luxburg and Bousquet [JMLR, 2004]\nleft open the questions of computational efficiency and of providing direct\nbounds on generalization error.\n  We design a new algorithm for classification in general metric spaces, whose\nruntime and accuracy depend on the doubling dimension of the data points, and\ncan thus achieve superior classification performance in many common scenarios.\nThe algorithmic core of our approach is an approximate (rather than exact)\nsolution to the classical problems of Lipschitz extension and of Nearest\nNeighbor Search. The algorithm's generalization performance is guaranteed via\nthe fat-shattering dimension of Lipschitz classifiers, and we present\nexperimental evidence of its superiority to some common kernel methods. As a\nby-product, we offer a new perspective on the nearest neighbor classifier,\nwhich yields significantly sharper risk asymptotics than the classic analysis\nof Cover and Hart [IEEE Trans. Info. Theory, 1967].\n", "versions": [{"version": "v1", "created": "Tue, 11 Jun 2013 15:00:35 GMT"}, {"version": "v2", "created": "Tue, 8 Jul 2014 19:56:43 GMT"}, {"version": "v3", "created": "Thu, 10 Jul 2014 21:33:44 GMT"}], "update_date": "2014-07-14", "authors_parsed": [["Gottlieb", "Lee-Ad", ""], ["Kontorovich", "Aryeh", ""], ["Krauthgamer", "Robert", ""]]}, {"id": "1306.2554", "submitter": "Richard Combes", "authors": "Richard Combes and Ilham El Bouloumi and Stephane Senecal and Zwi\n  Altman", "title": "The association problem in wireless networks: a Policy Gradient\n  Reinforcement Learning approach", "comments": "Working version", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI cs.IT cs.LG math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The purpose of this paper is to develop a self-optimized association\nalgorithm based on PGRL (Policy Gradient Reinforcement Learning), which is both\nscalable, stable and robust. The term robust means that performance degradation\nin the learning phase should be forbidden or limited to predefined thresholds.\nThe algorithm is model-free (as opposed to Value Iteration) and robust (as\nopposed to Q-Learning). The association problem is modeled as a Markov Decision\nProcess (MDP). The policy space is parameterized. The parameterized family of\npolicies is then used as expert knowledge for the PGRL. The PGRL converges\ntowards a local optimum and the average cost decreases monotonically during the\nlearning process. The properties of the solution make it a good candidate for\npractical implementation. Furthermore, the robustness property allows to use\nthe PGRL algorithm in an \"always-on\" learning mode.\n", "versions": [{"version": "v1", "created": "Tue, 11 Jun 2013 15:31:25 GMT"}], "update_date": "2013-06-12", "authors_parsed": [["Combes", "Richard", ""], ["Bouloumi", "Ilham El", ""], ["Senecal", "Stephane", ""], ["Altman", "Zwi", ""]]}, {"id": "1306.2557", "submitter": "L.A. Prashanth", "authors": "L.A. Prashanth, Nathaniel Korda and R\\'emi Munos", "title": "Concentration bounds for temporal difference learning with linear\n  function approximation: The case of batch data and uniform sampling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a stochastic approximation (SA) based method with randomization of\nsamples for policy evaluation using the least squares temporal difference\n(LSTD) algorithm. Our proposed scheme is equivalent to running regular temporal\ndifference learning with linear function approximation, albeit with samples\npicked uniformly from a given dataset. Our method results in an $O(d)$\nimprovement in complexity in comparison to LSTD, where $d$ is the dimension of\nthe data. We provide non-asymptotic bounds for our proposed method, both in\nhigh probability and in expectation, under the assumption that the matrix\nunderlying the LSTD solution is positive definite. The latter assumption can be\neasily satisfied for the pathwise LSTD variant proposed in [23]. Moreover, we\nalso establish that using our method in place of LSTD does not impact the rate\nof convergence of the approximate value function to the true value function.\nThese rate results coupled with the low computational complexity of our method\nmake it attractive for implementation in big data settings, where $d$ is large.\nA similar low-complexity alternative for least squares regression is well-known\nas the stochastic gradient descent (SGD) algorithm. We provide finite-time\nbounds for SGD. We demonstrate the practicality of our method as an efficient\nalternative for pathwise LSTD empirically by combining it with the least\nsquares policy iteration (LSPI) algorithm in a traffic signal control\napplication. We also conduct another set of experiments that combines the SA\nbased low-complexity variant for least squares regression with the LinUCB\nalgorithm for contextual bandits, using the large scale news recommendation\ndataset from Yahoo.\n", "versions": [{"version": "v1", "created": "Tue, 11 Jun 2013 15:42:00 GMT"}, {"version": "v2", "created": "Wed, 19 Feb 2014 00:01:53 GMT"}, {"version": "v3", "created": "Mon, 16 Jun 2014 13:39:28 GMT"}, {"version": "v4", "created": "Wed, 18 Jun 2014 17:13:38 GMT"}, {"version": "v5", "created": "Tue, 28 Nov 2017 14:16:23 GMT"}, {"version": "v6", "created": "Fri, 24 Jan 2020 16:44:09 GMT"}], "update_date": "2020-01-27", "authors_parsed": [["Prashanth", "L. A.", ""], ["Korda", "Nathaniel", ""], ["Munos", "R\u00e9mi", ""]]}, {"id": "1306.2663", "submitter": "Guoqiang Zhong", "authors": "Guoqiang Zhong and Mohamed Cheriet", "title": "Large Margin Low Rank Tensor Analysis", "comments": "30 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NA", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  Other than vector representations, the direct objects of human cognition are\ngenerally high-order tensors, such as 2D images and 3D textures. From this\nfact, two interesting questions naturally arise: How does the human brain\nrepresent these tensor perceptions in a \"manifold\" way, and how can they be\nrecognized on the \"manifold\"? In this paper, we present a supervised model to\nlearn the intrinsic structure of the tensors embedded in a high dimensional\nEuclidean space. With the fixed point continuation procedures, our model\nautomatically and jointly discovers the optimal dimensionality and the\nrepresentations of the low dimensional embeddings. This makes it an effective\nsimulation of the cognitive process of human brain. Furthermore, the\ngeneralization of our model based on similarity between the learned low\ndimensional embeddings can be viewed as counterpart of recognition of human\nbrain. Experiments on applications for object recognition and face recognition\ndemonstrate the superiority of our proposed model over state-of-the-art\napproaches.\n", "versions": [{"version": "v1", "created": "Tue, 11 Jun 2013 21:39:56 GMT"}], "update_date": "2013-06-13", "authors_parsed": [["Zhong", "Guoqiang", ""], ["Cheriet", "Mohamed", ""]]}, {"id": "1306.2665", "submitter": "Weiyu Xu", "authors": "Myung Cho and Weiyu Xu", "title": "Precisely Verifying the Null Space Conditions in Compressed Sensing: A\n  Sandwiching Algorithm", "comments": "30 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.LG cs.SY math.IT math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose new efficient algorithms to verify the null space\ncondition in compressed sensing (CS). Given an $(n-m) \\times n$ ($m>0$) CS\nmatrix $A$ and a positive $k$, we are interested in computing $\\displaystyle\n\\alpha_k = \\max_{\\{z: Az=0,z\\neq 0\\}}\\max_{\\{K: |K|\\leq k\\}}$ ${\\|z_K\n\\|_{1}}{\\|z\\|_{1}}$, where $K$ represents subsets of $\\{1,2,...,n\\}$, and $|K|$\nis the cardinality of $K$. In particular, we are interested in finding the\nmaximum $k$ such that $\\alpha_k < {1}{2}$. However, computing $\\alpha_k$ is\nknown to be extremely challenging. In this paper, we first propose a series of\nnew polynomial-time algorithms to compute upper bounds on $\\alpha_k$. Based on\nthese new polynomial-time algorithms, we further design a new sandwiching\nalgorithm, to compute the \\emph{exact} $\\alpha_k$ with greatly reduced\ncomplexity. When needed, this new sandwiching algorithm also achieves a smooth\ntradeoff between computational complexity and result accuracy. Empirical\nresults show the performance improvements of our algorithm over existing known\nmethods; and our algorithm outputs precise values of $\\alpha_k$, with much\nlower complexity than exhaustive search.\n", "versions": [{"version": "v1", "created": "Tue, 11 Jun 2013 21:57:47 GMT"}, {"version": "v2", "created": "Wed, 19 Jun 2013 22:26:55 GMT"}, {"version": "v3", "created": "Sat, 10 Aug 2013 01:14:46 GMT"}], "update_date": "2013-08-13", "authors_parsed": [["Cho", "Myung", ""], ["Xu", "Weiyu", ""]]}, {"id": "1306.2672", "submitter": "Bamdev Mishra", "authors": "B. Mishra and R. Sepulchre", "title": "R3MC: A Riemannian three-factor algorithm for low-rank matrix completion", "comments": "Accepted for publication in the proceedings of the 53rd IEEE\n  Conference on Decision and Control, 2014", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We exploit the versatile framework of Riemannian optimization on quotient\nmanifolds to develop R3MC, a nonlinear conjugate-gradient method for low-rank\nmatrix completion. The underlying search space of fixed-rank matrices is\nendowed with a novel Riemannian metric that is tailored to the least-squares\ncost. Numerical comparisons suggest that R3MC robustly outperforms\nstate-of-the-art algorithms across different problem instances, especially\nthose that combine scarcely sampled and ill-conditioned data.\n", "versions": [{"version": "v1", "created": "Tue, 11 Jun 2013 22:42:21 GMT"}, {"version": "v2", "created": "Sat, 20 Sep 2014 12:59:58 GMT"}], "update_date": "2014-09-23", "authors_parsed": [["Mishra", "B.", ""], ["Sepulchre", "R.", ""]]}, {"id": "1306.2685", "submitter": "Alfredo Kalaitzis", "authors": "Alfredo Kalaitzis and Ricardo Silva", "title": "Flexible sampling of discrete data correlations without the marginal\n  distributions", "comments": "An overhauled version of the experimental section moved to the main\n  paper. Old experimental section moved to supplementary material", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning the joint dependence of discrete variables is a fundamental problem\nin machine learning, with many applications including prediction, clustering\nand dimensionality reduction. More recently, the framework of copula modeling\nhas gained popularity due to its modular parametrization of joint\ndistributions. Among other properties, copulas provide a recipe for combining\nflexible models for univariate marginal distributions with parametric families\nsuitable for potentially high dimensional dependence structures. More\nradically, the extended rank likelihood approach of Hoff (2007) bypasses\nlearning marginal models completely when such information is ancillary to the\nlearning task at hand as in, e.g., standard dimensionality reduction problems\nor copula parameter estimation. The main idea is to represent data by their\nobservable rank statistics, ignoring any other information from the marginals.\nInference is typically done in a Bayesian framework with Gaussian copulas, and\nit is complicated by the fact this implies sampling within a space where the\nnumber of constraints increases quadratically with the number of data points.\nThe result is slow mixing when using off-the-shelf Gibbs sampling. We present\nan efficient algorithm based on recent advances on constrained Hamiltonian\nMarkov chain Monte Carlo that is simple to implement and does not require\npaying for a quadratic cost in sample size.\n", "versions": [{"version": "v1", "created": "Wed, 12 Jun 2013 01:13:46 GMT"}, {"version": "v2", "created": "Thu, 8 Aug 2013 18:23:45 GMT"}, {"version": "v3", "created": "Thu, 14 Nov 2013 15:31:46 GMT"}], "update_date": "2013-11-15", "authors_parsed": [["Kalaitzis", "Alfredo", ""], ["Silva", "Ricardo", ""]]}, {"id": "1306.2733", "submitter": "Xuhui Fan", "authors": "Xuhui Fan, Longbing Cao, Richard Yi Da Xu", "title": "Copula Mixed-Membership Stochastic Blockmodel for Intra-Subgroup\n  Correlations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The \\emph{Mixed-Membership Stochastic Blockmodel (MMSB)} is a popular\nframework for modeling social network relationships. It can fully exploit each\nindividual node's participation (or membership) in a social structure. Despite\nits powerful representations, this model makes an assumption that the\ndistributions of relational membership indicators between two nodes are\nindependent. Under many social network settings, however, it is possible that\ncertain known subgroups of people may have high or low correlations in terms of\ntheir membership categories towards each other, and such prior information\nshould be incorporated into the model. To this end, we introduce a \\emph{Copula\nMixed-Membership Stochastic Blockmodel (cMMSB)} where an individual Copula\nfunction is employed to jointly model the membership pairs of those nodes\nwithin the subgroup of interest. The model enables the use of various Copula\nfunctions to suit the scenario, while maintaining the membership's marginal\ndistribution, as needed, for modeling membership indicators with other nodes\noutside of the subgroup of interest. We describe the proposed model and its\ninference algorithm in detail for both the finite and infinite cases. In the\nexperiment section, we compare our algorithms with other popular models in\nterms of link prediction, using both synthetic and real world data.\n", "versions": [{"version": "v1", "created": "Wed, 12 Jun 2013 07:42:15 GMT"}, {"version": "v2", "created": "Sun, 6 Oct 2013 05:51:41 GMT"}], "update_date": "2013-10-08", "authors_parsed": [["Fan", "Xuhui", ""], ["Cao", "Longbing", ""], ["Da Xu", "Richard Yi", ""]]}, {"id": "1306.2759", "submitter": "Jingjing Xie", "authors": "Jingjing Xie, Bing Xu, Zhang Chuang", "title": "Horizontal and Vertical Ensemble with Deep Representation for\n  Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Representation learning, especially which by using deep learning, has been\nwidely applied in classification. However, how to use limited size of labeled\ndata to achieve good classification performance with deep neural network, and\nhow can the learned features further improve classification remain indefinite.\nIn this paper, we propose Horizontal Voting Vertical Voting and Horizontal\nStacked Ensemble methods to improve the classification performance of deep\nneural networks. In the ICML 2013 Black Box Challenge, via using these methods\nindependently, Bing Xu achieved 3rd in public leaderboard, and 7th in private\nleaderboard; Jingjing Xie achieved 4th in public leaderboard, and 5th in\nprivate leaderboard.\n", "versions": [{"version": "v1", "created": "Wed, 12 Jun 2013 08:57:35 GMT"}], "update_date": "2013-06-13", "authors_parsed": [["Xie", "Jingjing", ""], ["Xu", "Bing", ""], ["Chuang", "Zhang", ""]]}, {"id": "1306.2801", "submitter": "KyungHyun Cho", "authors": "Kyunghyun Cho", "title": "Understanding Dropout: Training Multi-Layer Perceptrons with Auxiliary\n  Independent Stochastic Neurons", "comments": "ICONIP 2013: Special Session in Deep Learning (v4)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, a simple, general method of adding auxiliary stochastic\nneurons to a multi-layer perceptron is proposed. It is shown that the proposed\nmethod is a generalization of recently successful methods of dropout (Hinton et\nal., 2012), explicit noise injection (Vincent et al., 2010; Bishop, 1995) and\nsemantic hashing (Salakhutdinov & Hinton, 2009). Under the proposed framework,\nan extension of dropout which allows using separate dropping probabilities for\ndifferent hidden neurons, or layers, is found to be available. The use of\ndifferent dropping probabilities for hidden layers separately is empirically\ninvestigated.\n", "versions": [{"version": "v1", "created": "Wed, 12 Jun 2013 12:38:40 GMT"}, {"version": "v2", "created": "Wed, 19 Jun 2013 15:09:29 GMT"}, {"version": "v3", "created": "Mon, 22 Jul 2013 11:42:18 GMT"}, {"version": "v4", "created": "Sun, 18 Aug 2013 21:39:12 GMT"}], "update_date": "2013-08-20", "authors_parsed": [["Cho", "Kyunghyun", ""]]}, {"id": "1306.2861", "submitter": "Roger Frigola", "authors": "Roger Frigola, Fredrik Lindsten, Thomas B. Sch\\\"on, Carl E. Rasmussen", "title": "Bayesian Inference and Learning in Gaussian Process State-Space Models\n  with Particle MCMC", "comments": null, "journal-ref": "Published in NIPS 2013, Advances in Neural Information Processing\n  Systems 26, pp. 3156--3164", "doi": null, "report-no": null, "categories": "stat.ML cs.LG cs.SY", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  State-space models are successfully used in many areas of science,\nengineering and economics to model time series and dynamical systems. We\npresent a fully Bayesian approach to inference \\emph{and learning} (i.e. state\nestimation and system identification) in nonlinear nonparametric state-space\nmodels. We place a Gaussian process prior over the state transition dynamics,\nresulting in a flexible model able to capture complex dynamical phenomena. To\nenable efficient inference, we marginalize over the transition dynamics\nfunction and infer directly the joint smoothing distribution using specially\ntailored Particle Markov Chain Monte Carlo samplers. Once a sample from the\nsmoothing distribution is computed, the state transition predictive\ndistribution can be formulated analytically. Our approach preserves the full\nnonparametric expressivity of the model and can make use of sparse Gaussian\nprocesses to greatly reduce computational complexity.\n", "versions": [{"version": "v1", "created": "Wed, 12 Jun 2013 15:20:28 GMT"}, {"version": "v2", "created": "Tue, 17 Dec 2013 16:10:24 GMT"}], "update_date": "2013-12-18", "authors_parsed": [["Frigola", "Roger", ""], ["Lindsten", "Fredrik", ""], ["Sch\u00f6n", "Thomas B.", ""], ["Rasmussen", "Carl E.", ""]]}, {"id": "1306.2906", "submitter": "Yasmine Kawthar Zergat", "authors": "Kawthar Yasmine Zergat, Abderrahmane Amrouche", "title": "Robust Support Vector Machines for Speaker Verification Task", "comments": "5 pages,5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.SD stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An important step in speaker verification is extracting features that best\ncharacterize the speaker voice. This paper investigates a front-end processing\nthat aims at improving the performance of speaker verification based on the\nSVMs classifier, in text independent mode. This approach combines features\nbased on conventional Mel-cepstral Coefficients (MFCCs) and Line Spectral\nFrequencies (LSFs) to constitute robust multivariate feature vectors. To reduce\nthe high dimensionality required for training these feature vectors, we use a\ndimension reduction method called principal component analysis (PCA). In order\nto evaluate the robustness of these systems, different noisy environments have\nbeen used. The obtained results using TIMIT database showed that, using the\nparadigm that combines these spectral cues leads to a significant improvement\nin verification accuracy, especially with PCA reduction for low signal-to-noise\nratio noisy environment.\n", "versions": [{"version": "v1", "created": "Wed, 12 Jun 2013 17:32:02 GMT"}], "update_date": "2013-06-13", "authors_parsed": [["Zergat", "Kawthar Yasmine", ""], ["Amrouche", "Abderrahmane", ""]]}, {"id": "1306.2918", "submitter": "Mario Bravo", "authors": "Mario Bravo (ISCI), Mathieu Faure (AMSE)", "title": "Reinforcement learning with restrictions on the action set", "comments": "28 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GT cs.LG math.PR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Consider a 2-player normal-form game repeated over time. We introduce an\nadaptive learning procedure, where the players only observe their own realized\npayoff at each stage. We assume that agents do not know their own payoff\nfunction, and have no information on the other player. Furthermore, we assume\nthat they have restrictions on their own action set such that, at each stage,\ntheir choice is limited to a subset of their action set. We prove that the\nempirical distributions of play converge to the set of Nash equilibria for\nzero-sum and potential games, and games where one player has two actions.\n", "versions": [{"version": "v1", "created": "Wed, 12 Jun 2013 18:37:10 GMT"}], "update_date": "2013-06-13", "authors_parsed": [["Bravo", "Mario", "", "ISCI"], ["Faure", "Mathieu", "", "AMSE"]]}, {"id": "1306.2979", "submitter": "Srinadh Bhojanapalli", "authors": "Yudong Chen, Srinadh Bhojanapalli, Sujay Sanghavi, Rachel Ward", "title": "Completing Any Low-rank Matrix, Provably", "comments": "Added a new necessary condition(Theorem 6) and a result on completion\n  of row coherent matrices(Corollary 4). Partial results appeared in the\n  International Conference on Machine Learning 2014, under the title 'Coherent\n  Matrix Completion'. (34 pages, 4 figures)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.IT cs.LG math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Matrix completion, i.e., the exact and provable recovery of a low-rank matrix\nfrom a small subset of its elements, is currently only known to be possible if\nthe matrix satisfies a restrictive structural constraint---known as {\\em\nincoherence}---on its row and column spaces. In these cases, the subset of\nelements is sampled uniformly at random.\n  In this paper, we show that {\\em any} rank-$ r $ $ n$-by-$ n $ matrix can be\nexactly recovered from as few as $O(nr \\log^2 n)$ randomly chosen elements,\nprovided this random choice is made according to a {\\em specific biased\ndistribution}: the probability of any element being sampled should be\nproportional to the sum of the leverage scores of the corresponding row, and\ncolumn. Perhaps equally important, we show that this specific form of sampling\nis nearly necessary, in a natural precise sense; this implies that other\nperhaps more intuitive sampling schemes fail.\n  We further establish three ways to use the above result for the setting when\nleverage scores are not known \\textit{a priori}: (a) a sampling strategy for\nthe case when only one of the row or column spaces are incoherent, (b) a\ntwo-phase sampling procedure for general matrices that first samples to\nestimate leverage scores followed by sampling for exact recovery, and (c) an\nanalysis showing the advantages of weighted nuclear/trace-norm minimization\nover the vanilla un-weighted formulation for the case of non-uniform sampling.\n", "versions": [{"version": "v1", "created": "Wed, 12 Jun 2013 21:26:00 GMT"}, {"version": "v2", "created": "Tue, 8 Oct 2013 22:04:45 GMT"}, {"version": "v3", "created": "Fri, 11 Jul 2014 13:09:18 GMT"}, {"version": "v4", "created": "Mon, 21 Jul 2014 09:48:19 GMT"}], "update_date": "2014-07-22", "authors_parsed": [["Chen", "Yudong", ""], ["Bhojanapalli", "Srinadh", ""], ["Sanghavi", "Sujay", ""], ["Ward", "Rachel", ""]]}, {"id": "1306.2999", "submitter": "Xuhui Fan", "authors": "Xuhui Fan, Longbing Cao, Richard Yi Da Xu", "title": "Dynamic Infinite Mixed-Membership Stochastic Blockmodel", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Directional and pairwise measurements are often used to model\ninter-relationships in a social network setting. The Mixed-Membership\nStochastic Blockmodel (MMSB) was a seminal work in this area, and many of its\ncapabilities were extended since then. In this paper, we propose the\n\\emph{Dynamic Infinite Mixed-Membership stochastic blockModel (DIM3)}, a\ngeneralised framework that extends the existing work to a potentially infinite\nnumber of communities and mixture memberships for each of the network's nodes.\nThis model is in a dynamic setting, where additional model parameters are\nintroduced to reflect the degree of persistence between one's memberships at\nconsecutive times. Accordingly, two effective posterior sampling strategies and\ntheir results are presented using both synthetic and real data.\n", "versions": [{"version": "v1", "created": "Thu, 13 Jun 2013 00:42:19 GMT"}], "update_date": "2013-06-14", "authors_parsed": [["Fan", "Xuhui", ""], ["Cao", "Longbing", ""], ["Da Xu", "Richard Yi", ""]]}, {"id": "1306.3002", "submitter": "Xuhui Fan", "authors": "Xuhui Fan, Longbing Cao", "title": "A Convergence Theorem for the Graph Shift-type Algorithms", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Graph Shift (GS) algorithms are recently focused as a promising approach for\ndiscovering dense subgraphs in noisy data. However, there are no theoretical\nfoundations for proving the convergence of the GS Algorithm. In this paper, we\npropose a generic theoretical framework consisting of three key GS components:\nsimplex of generated sequence set, monotonic and continuous objective function\nand closed mapping. We prove that GS algorithms with such components can be\ntransformed to fit the Zangwill's convergence theorem, and the sequence set\ngenerated by the GS procedures always terminates at a local maximum, or at\nworst, contains a subsequence which converges to a local maximum of the\nsimilarity measure function. The framework is verified by expanding it to other\nGS-type algorithms and experimental results.\n", "versions": [{"version": "v1", "created": "Thu, 13 Jun 2013 01:00:21 GMT"}], "update_date": "2013-06-14", "authors_parsed": [["Fan", "Xuhui", ""], ["Cao", "Longbing", ""]]}, {"id": "1306.3003", "submitter": "Xuhui Fan", "authors": "Xuhui Fan, Yiling Zeng, Longbing Cao", "title": "Non-parametric Power-law Data Clustering", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It has always been a great challenge for clustering algorithms to\nautomatically determine the cluster numbers according to the distribution of\ndatasets. Several approaches have been proposed to address this issue,\nincluding the recent promising work which incorporate Bayesian Nonparametrics\ninto the $k$-means clustering procedure. This approach shows simplicity in\nimplementation and solidity in theory, while it also provides a feasible way to\ninference in large scale datasets. However, several problems remains unsolved\nin this pioneering work, including the power-law data applicability, mechanism\nto merge centers to avoid the over-fitting problem, clustering order problem,\ne.t.c.. To address these issues, the Pitman-Yor Process based k-means (namely\n\\emph{pyp-means}) is proposed in this paper. Taking advantage of the Pitman-Yor\nProcess, \\emph{pyp-means} treats clusters differently by dynamically and\nadaptively changing the threshold to guarantee the generation of power-law\nclustering results. Also, one center agglomeration procedure is integrated into\nthe implementation to be able to merge small but close clusters and then\nadaptively determine the cluster number. With more discussion on the clustering\norder, the convergence proof, complexity analysis and extension to spectral\nclustering, our approach is compared with traditional clustering algorithm and\nvariational inference methods. The advantages and properties of pyp-means are\nvalidated by experiments on both synthetic datasets and real world datasets.\n", "versions": [{"version": "v1", "created": "Thu, 13 Jun 2013 01:20:50 GMT"}], "update_date": "2013-06-14", "authors_parsed": [["Fan", "Xuhui", ""], ["Zeng", "Yiling", ""], ["Cao", "Longbing", ""]]}, {"id": "1306.3058", "submitter": "Sebastien Paris", "authors": "S\\'ebastien Paris and Yann Doh and Herv\\'e Glotin and Xanadu Halkias\n  and Joseph Razik", "title": "Physeter catodon localization by sparse coding", "comments": "6 pages, 6 figures, workshop ICML4B in ICML 2013 conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a spermwhale' localization architecture using jointly a\nbag-of-features (BoF) approach and machine learning framework. BoF methods are\nknown, especially in computer vision, to produce from a collection of local\nfeatures a global representation invariant to principal signal transformations.\nOur idea is to regress supervisely from these local features two rough\nestimates of the distance and azimuth thanks to some datasets where both\nacoustic events and ground-truth position are now available. Furthermore, these\nestimates can feed a particle filter system in order to obtain a precise\nspermwhale' position even in mono-hydrophone configuration. Anti-collision\nsystem and whale watching are considered applications of this work.\n", "versions": [{"version": "v1", "created": "Thu, 13 Jun 2013 09:05:08 GMT"}], "update_date": "2013-06-14", "authors_parsed": [["Paris", "S\u00e9bastien", ""], ["Doh", "Yann", ""], ["Glotin", "Herv\u00e9", ""], ["Halkias", "Xanadu", ""], ["Razik", "Joseph", ""]]}, {"id": "1306.3108", "submitter": "Yiming Ying", "authors": "Zheng-Chu Guo and Yiming Ying", "title": "Guaranteed Classification via Regularized Similarity Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning an appropriate (dis)similarity function from the available data is a\ncentral problem in machine learning, since the success of many machine learning\nalgorithms critically depends on the choice of a similarity function to compare\nexamples. Despite many approaches for similarity metric learning have been\nproposed, there is little theoretical study on the links between similarity\nmet- ric learning and the classification performance of the result classifier.\nIn this paper, we propose a regularized similarity learning formulation\nassociated with general matrix-norms, and establish their generalization\nbounds. We show that the generalization error of the resulting linear separator\ncan be bounded by the derived generalization bound of similarity learning. This\nshows that a good gen- eralization of the learnt similarity function guarantees\na good classification of the resulting linear classifier. Our results extend\nand improve those obtained by Bellet at al. [3]. Due to the techniques\ndependent on the notion of uniform stability [6], the bound obtained there\nholds true only for the Frobenius matrix- norm regularization. Our techniques\nusing the Rademacher complexity [5] and its related Khinchin-type inequality\nenable us to establish bounds for regularized similarity learning formulations\nassociated with general matrix-norms including sparse L 1 -norm and mixed\n(2,1)-norm.\n", "versions": [{"version": "v1", "created": "Thu, 13 Jun 2013 13:47:51 GMT"}, {"version": "v2", "created": "Thu, 29 Aug 2013 15:38:27 GMT"}], "update_date": "2013-08-30", "authors_parsed": [["Guo", "Zheng-Chu", ""], ["Ying", "Yiming", ""]]}, {"id": "1306.3161", "submitter": "Maksim Lapin", "authors": "Maksim Lapin, Matthias Hein, Bernt Schiele", "title": "Learning Using Privileged Information: SVM+ and Weighted SVM", "comments": "18 pages, 8 figures; integrated reviewer comments, improved\n  typesetting", "journal-ref": "Neural Networks 53C (2014), pp. 95-108", "doi": "10.1016/j.neunet.2014.02.002", "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Prior knowledge can be used to improve predictive performance of learning\nalgorithms or reduce the amount of data required for training. The same goal is\npursued within the learning using privileged information paradigm which was\nrecently introduced by Vapnik et al. and is aimed at utilizing additional\ninformation available only at training time -- a framework implemented by SVM+.\nWe relate the privileged information to importance weighting and show that the\nprior knowledge expressible with privileged features can also be encoded by\nweights associated with every training example. We show that a weighted SVM can\nalways replicate an SVM+ solution, while the converse is not true and we\nconstruct a counterexample highlighting the limitations of SVM+. Finally, we\ntouch on the problem of choosing weights for weighted SVMs when privileged\nfeatures are not available.\n", "versions": [{"version": "v1", "created": "Thu, 13 Jun 2013 16:36:07 GMT"}, {"version": "v2", "created": "Sun, 2 Mar 2014 13:57:55 GMT"}], "update_date": "2014-03-04", "authors_parsed": [["Lapin", "Maksim", ""], ["Hein", "Matthias", ""], ["Schiele", "Bernt", ""]]}, {"id": "1306.3162", "submitter": "Kishore Konda", "authors": "Kishore Reddy Konda, Roland Memisevic, Vincent Michalski", "title": "Learning to encode motion using spatio-temporal synchrony", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the task of learning to extract motion from videos. To this end,\nwe show that the detection of spatial transformations can be viewed as the\ndetection of synchrony between the image sequence and a sequence of features\nundergoing the motion we wish to detect. We show that learning about synchrony\nis possible using very fast, local learning rules, by introducing\nmultiplicative \"gating\" interactions between hidden units across frames. This\nmakes it possible to achieve competitive performance in a wide variety of\nmotion estimation tasks, using a small fraction of the time required to learn\nfeatures, and to outperform hand-crafted spatio-temporal features by a large\nmargin. We also show how learning about synchrony can be viewed as performing\ngreedy parameter estimation in the well-known motion energy model.\n", "versions": [{"version": "v1", "created": "Thu, 13 Jun 2013 16:46:03 GMT"}, {"version": "v2", "created": "Mon, 16 Dec 2013 20:14:24 GMT"}, {"version": "v3", "created": "Mon, 10 Feb 2014 11:19:23 GMT"}], "update_date": "2014-02-11", "authors_parsed": [["Konda", "Kishore Reddy", ""], ["Memisevic", "Roland", ""], ["Michalski", "Vincent", ""]]}, {"id": "1306.3171", "submitter": "Andrea Montanari", "authors": "Adel Javanmard and Andrea Montanari", "title": "Confidence Intervals and Hypothesis Testing for High-Dimensional\n  Regression", "comments": "40 pages, 4 pdf figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.IT cs.LG math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fitting high-dimensional statistical models often requires the use of\nnon-linear parameter estimation procedures. As a consequence, it is generally\nimpossible to obtain an exact characterization of the probability distribution\nof the parameter estimates. This in turn implies that it is extremely\nchallenging to quantify the \\emph{uncertainty} associated with a certain\nparameter estimate. Concretely, no commonly accepted procedure exists for\ncomputing classical measures of uncertainty and statistical significance as\nconfidence intervals or $p$-values for these models.\n  We consider here high-dimensional linear regression problem, and propose an\nefficient algorithm for constructing confidence intervals and $p$-values. The\nresulting confidence intervals have nearly optimal size. When testing for the\nnull hypothesis that a certain parameter is vanishing, our method has nearly\noptimal power.\n  Our approach is based on constructing a `de-biased' version of regularized\nM-estimators. The new construction improves over recent work in the field in\nthat it does not assume a special structure on the design matrix. We test our\nmethod on synthetic data and a high-throughput genomic data set about\nriboflavin production rate.\n", "versions": [{"version": "v1", "created": "Thu, 13 Jun 2013 17:19:39 GMT"}, {"version": "v2", "created": "Wed, 2 Apr 2014 00:29:37 GMT"}], "update_date": "2014-04-03", "authors_parsed": [["Javanmard", "Adel", ""], ["Montanari", "Andrea", ""]]}, {"id": "1306.3203", "submitter": "Huahua Wang", "authors": "Huahua Wang and Arindam Banerjee", "title": "Bregman Alternating Direction Method of Multipliers", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The mirror descent algorithm (MDA) generalizes gradient descent by using a\nBregman divergence to replace squared Euclidean distance. In this paper, we\nsimilarly generalize the alternating direction method of multipliers (ADMM) to\nBregman ADMM (BADMM), which allows the choice of different Bregman divergences\nto exploit the structure of problems. BADMM provides a unified framework for\nADMM and its variants, including generalized ADMM, inexact ADMM and Bethe ADMM.\n  We establish the global convergence and the $O(1/T)$ iteration complexity for\nBADMM. In some cases, BADMM can be faster than ADMM by a factor of\n$O(n/\\log(n))$. In solving the linear program of mass transportation problem,\nBADMM leads to massive parallelism and can easily run on GPU. BADMM is several\ntimes faster than highly optimized commercial software Gurobi.\n", "versions": [{"version": "v1", "created": "Thu, 13 Jun 2013 19:22:16 GMT"}, {"version": "v2", "created": "Tue, 1 Jul 2014 05:57:36 GMT"}, {"version": "v3", "created": "Tue, 8 Jul 2014 03:55:36 GMT"}], "update_date": "2014-07-09", "authors_parsed": [["Wang", "Huahua", ""], ["Banerjee", "Arindam", ""]]}, {"id": "1306.3212", "submitter": "Cho-Jui Hsieh Cho-Jui Hsieh", "authors": "Cho-Jui Hsieh, Matyas A. Sustik, Inderjit S. Dhillon and Pradeep\n  Ravikumar", "title": "Sparse Inverse Covariance Matrix Estimation Using Quadratic\n  Approximation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The L1-regularized Gaussian maximum likelihood estimator (MLE) has been shown\nto have strong statistical guarantees in recovering a sparse inverse covariance\nmatrix, or alternatively the underlying graph structure of a Gaussian Markov\nRandom Field, from very limited samples. We propose a novel algorithm for\nsolving the resulting optimization problem which is a regularized\nlog-determinant program. In contrast to recent state-of-the-art methods that\nlargely use first order gradient information, our algorithm is based on\nNewton's method and employs a quadratic approximation, but with some\nmodifications that leverage the structure of the sparse Gaussian MLE problem.\nWe show that our method is superlinearly convergent, and present experimental\nresults using synthetic and real-world application data that demonstrate the\nconsiderable improvements in performance of our method when compared to other\nstate-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Thu, 13 Jun 2013 19:51:59 GMT"}], "update_date": "2013-06-14", "authors_parsed": [["Hsieh", "Cho-Jui", ""], ["Sustik", "Matyas A.", ""], ["Dhillon", "Inderjit S.", ""], ["Ravikumar", "Pradeep", ""]]}, {"id": "1306.3343", "submitter": "Zheng Pan", "authors": "Zheng Pan, Changshui Zhang", "title": "Relaxed Sparse Eigenvalue Conditions for Sparse Estimation via\n  Non-convex Regularized Regression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NA stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Non-convex regularizers usually improve the performance of sparse estimation\nin practice. To prove this fact, we study the conditions of sparse estimations\nfor the sharp concave regularizers which are a general family of non-convex\nregularizers including many existing regularizers. For the global solutions of\nthe regularized regression, our sparse eigenvalue based conditions are weaker\nthan that of L1-regularization for parameter estimation and sparseness\nestimation. For the approximate global and approximate stationary (AGAS)\nsolutions, almost the same conditions are also enough. We show that the desired\nAGAS solutions can be obtained by coordinate descent (CD) based methods.\nFinally, we perform some experiments to show the performance of CD methods on\ngiving AGAS solutions and the degree of weakness of the estimation conditions\nrequired by the sharp concave regularizers.\n", "versions": [{"version": "v1", "created": "Fri, 14 Jun 2013 09:10:00 GMT"}, {"version": "v2", "created": "Wed, 3 Jul 2013 06:25:21 GMT"}, {"version": "v3", "created": "Wed, 12 Feb 2014 09:27:57 GMT"}], "update_date": "2014-02-13", "authors_parsed": [["Pan", "Zheng", ""], ["Zhang", "Changshui", ""]]}, {"id": "1306.3409", "submitter": "Thomas B\\\"uhler", "authors": "Thomas B\\\"uhler, Syama Sundar Rangapuram, Simon Setzer, Matthias Hein", "title": "Constrained fractional set programs and their application in local\n  clustering and community detection", "comments": "Long version of paper accepted at ICML 2013", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The (constrained) minimization of a ratio of set functions is a problem\nfrequently occurring in clustering and community detection. As these\noptimization problems are typically NP-hard, one uses convex or spectral\nrelaxations in practice. While these relaxations can be solved globally\noptimally, they are often too loose and thus lead to results far away from the\noptimum. In this paper we show that every constrained minimization problem of a\nratio of non-negative set functions allows a tight relaxation into an\nunconstrained continuous optimization problem. This result leads to a flexible\nframework for solving constrained problems in network analysis. While a\nglobally optimal solution for the resulting non-convex problem cannot be\nguaranteed, we outperform the loose convex or spectral relaxations by a large\nmargin on constrained local clustering problems.\n", "versions": [{"version": "v1", "created": "Fri, 14 Jun 2013 14:20:29 GMT"}], "update_date": "2013-06-17", "authors_parsed": [["B\u00fchler", "Thomas", ""], ["Rangapuram", "Syama Sundar", ""], ["Setzer", "Simon", ""], ["Hein", "Matthias", ""]]}, {"id": "1306.3474", "submitter": "Yijun Wang", "authors": "Yijun Wang", "title": "Classifying Single-Trial EEG during Motor Imagery with a Small Training\n  Set", "comments": "13 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.HC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Before the operation of a motor imagery based brain-computer interface (BCI)\nadopting machine learning techniques, a cumbersome training procedure is\nunavoidable. The development of a practical BCI posed the challenge of\nclassifying single-trial EEG with a small training set. In this letter, we\naddressed this problem by employing a series of signal processing and machine\nlearning approaches to alleviate overfitting and obtained test accuracy similar\nto training accuracy on the datasets from BCI Competition III and our own\nexperiments.\n", "versions": [{"version": "v1", "created": "Fri, 14 Jun 2013 18:24:19 GMT"}], "update_date": "2013-06-17", "authors_parsed": [["Wang", "Yijun", ""]]}, {"id": "1306.3476", "submitter": "James Bergstra", "authors": "James Bergstra and David D. Cox", "title": "Hyperparameter Optimization and Boosting for Classifying Facial\n  Expressions: How good can a \"Null\" Model be?", "comments": "Presented at the Workshop on Representation and Learning, ICML 2013", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One of the goals of the ICML workshop on representation and learning is to\nestablish benchmark scores for a new data set of labeled facial expressions.\nThis paper presents the performance of a \"Null\" model consisting of\nconvolutions with random weights, PCA, pooling, normalization, and a linear\nreadout. Our approach focused on hyperparameter optimization rather than novel\nmodel components. On the Facial Expression Recognition Challenge held by the\nKaggle website, our hyperparameter optimization approach achieved a score of\n60% accuracy on the test data. This paper also introduces a new ensemble\nconstruction variant that combines hyperparameter optimization with the\nconstruction of ensembles. This algorithm constructed an ensemble of four\nmodels that scored 65.5% accuracy. These scores rank 12th and 5th respectively\namong the 56 challenge participants. It is worth noting that our approach was\ndeveloped prior to the release of the data set, and applied without\nmodification; our strong competition performance suggests that the TPE\nhyperparameter optimization algorithm and domain expertise encoded in our Null\nmodel can generalize to new image classification data sets.\n", "versions": [{"version": "v1", "created": "Fri, 14 Jun 2013 18:28:52 GMT"}], "update_date": "2013-06-17", "authors_parsed": [["Bergstra", "James", ""], ["Cox", "David D.", ""]]}, {"id": "1306.3525", "submitter": "Sudipto Guha", "authors": "Sudipto Guha and Kamesh Munagala", "title": "Approximation Algorithms for Bayesian Multi-Armed Bandit Problems", "comments": "arXiv admin note: text overlap with arXiv:1011.1161", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we consider several finite-horizon Bayesian multi-armed bandit\nproblems with side constraints which are computationally intractable (NP-Hard)\nand for which no optimal (or near optimal) algorithms are known to exist with\nsub-exponential running time. All of these problems violate the standard\nexchange property, which assumes that the reward from the play of an arm is not\ncontingent upon when the arm is played. Not only are index policies suboptimal\nin these contexts, there has been little analysis of such policies in these\nproblem settings. We show that if we consider near-optimal policies, in the\nsense of approximation algorithms, then there exists (near) index policies.\nConceptually, if we can find policies that satisfy an approximate version of\nthe exchange property, namely, that the reward from the play of an arm depends\non when the arm is played to within a constant factor, then we have an avenue\ntowards solving these problems. However such an approximate version of the\nidling bandit property does not hold on a per-play basis and are shown to hold\nin a global sense. Clearly, such a property is not necessarily true of\narbitrary single arm policies and finding such single arm policies is\nnontrivial. We show that by restricting the state spaces of arms we can find\nsingle arm policies and that these single arm policies can be combined into\nglobal (near) index policies where the approximate version of the exchange\nproperty is true in expectation. The number of different bandit problems that\ncan be addressed by this technique already demonstrate its wide applicability.\n", "versions": [{"version": "v1", "created": "Fri, 14 Jun 2013 22:24:29 GMT"}, {"version": "v2", "created": "Wed, 17 Jul 2013 19:16:47 GMT"}], "update_date": "2013-07-18", "authors_parsed": [["Guha", "Sudipto", ""], ["Munagala", "Kamesh", ""]]}, {"id": "1306.3558", "submitter": "Fabrizio Angiulli", "authors": "Fabrizio Angiulli and Fabio Fassetti and Luigi Palopoli and Giuseppe\n  Manco", "title": "Outlying Property Detection with Numerical Attributes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DB stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The outlying property detection problem is the problem of discovering the\nproperties distinguishing a given object, known in advance to be an outlier in\na database, from the other database objects. In this paper, we analyze the\nproblem within a context where numerical attributes are taken into account,\nwhich represents a relevant case left open in the literature. We introduce a\nmeasure to quantify the degree the outlierness of an object, which is\nassociated with the relative likelihood of the value, compared to the to the\nrelative likelihood of other objects in the database. As a major contribution,\nwe present an efficient algorithm to compute the outlierness relative to\nsignificant subsets of the data. The latter subsets are characterized in a\n\"rule-based\" fashion, and hence the basis for the underlying explanation of the\noutlierness.\n", "versions": [{"version": "v1", "created": "Sat, 15 Jun 2013 08:52:46 GMT"}], "update_date": "2013-06-18", "authors_parsed": [["Angiulli", "Fabrizio", ""], ["Fassetti", "Fabio", ""], ["Palopoli", "Luigi", ""], ["Manco", "Giuseppe", ""]]}, {"id": "1306.3721", "submitter": "Huahua Wang", "authors": "Huahua Wang and Arindam Banerjee", "title": "Online Alternating Direction Method (longer version)", "comments": "Longer version of arXiv:1206.6448", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Online optimization has emerged as powerful tool in large scale optimization.\nIn this pa- per, we introduce efficient online optimization algorithms based on\nthe alternating direction method (ADM), which can solve online convex\noptimization under linear constraints where the objective could be non-smooth.\nWe introduce new proof techniques for ADM in the batch setting, which yields a\nO(1/T) convergence rate for ADM and forms the basis for regret anal- ysis in\nthe online setting. We consider two scenarios in the online setting, based on\nwhether an additional Bregman divergence is needed or not. In both settings, we\nestablish regret bounds for both the objective function as well as constraints\nviolation for general and strongly convex functions. We also consider inexact\nADM updates where certain terms are linearized to yield efficient updates and\nshow the stochastic convergence rates. In addition, we briefly discuss that\nonline ADM can be used as projection- free online learning algorithm in some\nscenarios. Preliminary results are presented to illustrate the performance of\nthe proposed algorithms.\n", "versions": [{"version": "v1", "created": "Mon, 17 Jun 2013 01:27:10 GMT"}, {"version": "v2", "created": "Wed, 10 Jul 2013 18:36:18 GMT"}], "update_date": "2013-07-11", "authors_parsed": [["Wang", "Huahua", ""], ["Banerjee", "Arindam", ""]]}, {"id": "1306.3729", "submitter": "Arun Tejasvi Chaganty", "authors": "Arun Tejasvi Chaganty and Percy Liang", "title": "Spectral Experts for Estimating Mixtures of Linear Regressions", "comments": "Accepted at ICML 2013. Includes supplementary material", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Discriminative latent-variable models are typically learned using EM or\ngradient-based optimization, which suffer from local optima. In this paper, we\ndevelop a new computationally efficient and provably consistent estimator for a\nmixture of linear regressions, a simple instance of a discriminative\nlatent-variable model. Our approach relies on a low-rank linear regression to\nrecover a symmetric tensor, which can be factorized into the parameters using a\ntensor power method. We prove rates of convergence for our estimator and\nprovide an empirical evaluation illustrating its strengths relative to local\noptimization (EM).\n", "versions": [{"version": "v1", "created": "Mon, 17 Jun 2013 03:02:05 GMT"}], "update_date": "2013-06-18", "authors_parsed": [["Chaganty", "Arun Tejasvi", ""], ["Liang", "Percy", ""]]}, {"id": "1306.3860", "submitter": "Samuel R\\\"onnqvist", "authors": "Peter Sarlin and Samuel R\\\"onnqvist", "title": "Cluster coloring of the Self-Organizing Map: An information\n  visualization perspective", "comments": "Forthcoming in Proceedings of 17th International Conference\n  Information Visualisation (2013)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.HC", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  This paper takes an information visualization perspective to visual\nrepresentations in the general SOM paradigm. This involves viewing SOM-based\nvisualizations through the eyes of Bertin's and Tufte's theories on data\ngraphics. The regular grid shape of the Self-Organizing Map (SOM), while being\na virtue for linking visualizations to it, restricts representation of cluster\nstructures. From the viewpoint of information visualization, this paper\nprovides a general, yet simple, solution to projection-based coloring of the\nSOM that reveals structures. First, the proposed color space is easy to\nconstruct and customize to the purpose of use, while aiming at being\nperceptually correct and informative through two separable dimensions. Second,\nthe coloring method is not dependent on any specific method of projection, but\nis rather modular to fit any objective function suitable for the task at hand.\nThe cluster coloring is illustrated on two datasets: the iris data, and welfare\nand poverty indicators.\n", "versions": [{"version": "v1", "created": "Mon, 17 Jun 2013 13:57:00 GMT"}], "update_date": "2013-06-26", "authors_parsed": [["Sarlin", "Peter", ""], ["R\u00f6nnqvist", "Samuel", ""]]}, {"id": "1306.3895", "submitter": "Jiazhong Nie", "authors": "Jiazhong Nie and Wojciech Kotlowski and Manfred K. Warmuth", "title": "On-line PCA with Optimal Regrets", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We carefully investigate the on-line version of PCA, where in each trial a\nlearning algorithm plays a k-dimensional subspace, and suffers the compression\nloss on the next instance when projected into the chosen subspace. In this\nsetting, we analyze two popular on-line algorithms, Gradient Descent (GD) and\nExponentiated Gradient (EG). We show that both algorithms are essentially\noptimal in the worst-case. This comes as a surprise, since EG is known to\nperform sub-optimally when the instances are sparse. This different behavior of\nEG for PCA is mainly related to the non-negativity of the loss in this case,\nwhich makes the PCA setting qualitatively different from other settings studied\nin the literature. Furthermore, we show that when considering regret bounds as\nfunction of a loss budget, EG remains optimal and strictly outperforms GD.\nNext, we study the extension of the PCA setting, in which the Nature is allowed\nto play with dense instances, which are positive matrices with bounded largest\neigenvalue. Again we can show that EG is optimal and strictly better than GD in\nthis setting.\n", "versions": [{"version": "v1", "created": "Mon, 17 Jun 2013 15:29:00 GMT"}, {"version": "v2", "created": "Fri, 9 May 2014 05:28:39 GMT"}], "update_date": "2014-05-12", "authors_parsed": [["Nie", "Jiazhong", ""], ["Kotlowski", "Wojciech", ""], ["Warmuth", "Manfred K.", ""]]}, {"id": "1306.3905", "submitter": "Julien Audiffren", "authors": "Julien Audiffren (LIF), Hachem Kadri (LIF)", "title": "Stability of Multi-Task Kernel Regression Algorithms", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the stability properties of nonlinear multi-task regression in\nreproducing Hilbert spaces with operator-valued kernels. Such kernels, a.k.a.\nmulti-task kernels, are appropriate for learning prob- lems with nonscalar\noutputs like multi-task learning and structured out- put prediction. We show\nthat multi-task kernel regression algorithms are uniformly stable in the\ngeneral case of infinite-dimensional output spaces. We then derive under mild\nassumption on the kernel generaliza- tion bounds of such algorithms, and we\nshow their consistency even with non Hilbert-Schmidt operator-valued kernels .\nWe demonstrate how to apply the results to various multi-task kernel regression\nmethods such as vector-valued SVR and functional ridge regression.\n", "versions": [{"version": "v1", "created": "Mon, 17 Jun 2013 15:44:30 GMT"}], "update_date": "2013-06-18", "authors_parsed": [["Audiffren", "Julien", "", "LIF"], ["Kadri", "Hachem", "", "LIF"]]}, {"id": "1306.3917", "submitter": "Matthew Malloy", "authors": "Kevin Jamieson, Matthew Malloy, Robert Nowak, Sebastien Bubeck", "title": "On Finding the Largest Mean Among Many", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sampling from distributions to find the one with the largest mean arises in a\nbroad range of applications, and it can be mathematically modeled as a\nmulti-armed bandit problem in which each distribution is associated with an\narm. This paper studies the sample complexity of identifying the best arm\n(largest mean) in a multi-armed bandit problem. Motivated by large-scale\napplications, we are especially interested in identifying situations where the\ntotal number of samples that are necessary and sufficient to find the best arm\nscale linearly with the number of arms. We present a single-parameter\nmulti-armed bandit model that spans the range from linear to superlinear sample\ncomplexity. We also give a new algorithm for best arm identification, called\nPRISM, with linear sample complexity for a wide range of mean distributions.\nThe algorithm, like most exploration procedures for multi-armed bandits, is\nadaptive in the sense that the next arms to sample are selected based on\nprevious samples. We compare the sample complexity of adaptive procedures with\nsimpler non-adaptive procedures using new lower bounds. For many problem\ninstances, the increased sample complexity required by non-adaptive procedures\nis a polynomial factor of the number of arms.\n", "versions": [{"version": "v1", "created": "Mon, 17 Jun 2013 16:24:13 GMT"}], "update_date": "2013-06-18", "authors_parsed": [["Jamieson", "Kevin", ""], ["Malloy", "Matthew", ""], ["Nowak", "Robert", ""], ["Bubeck", "Sebastien", ""]]}, {"id": "1306.4080", "submitter": "An Bian", "authors": "An Bian, Xiong Li, Yuncai Liu, Ming-Hsuan Yang", "title": "Parallel Coordinate Descent Newton Method for Efficient\n  $\\ell_1$-Regularized Minimization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The recent years have witnessed advances in parallel algorithms for large\nscale optimization problems. Notwithstanding demonstrated success, existing\nalgorithms that parallelize over features are usually limited by divergence\nissues under high parallelism or require data preprocessing to alleviate these\nproblems. In this work, we propose a Parallel Coordinate Descent Newton\nalgorithm using multidimensional approximate Newton steps (PCDN), where the\noff-diagonal elements of the Hessian are set to zero to enable parallelization.\nIt randomly partitions the feature set into $b$ bundles/subsets with size of\n$P$, and sequentially processes each bundle by first computing the descent\ndirections for each feature in parallel and then conducting $P$-dimensional\nline search to obtain the step size. We show that: (1) PCDN is guaranteed to\nconverge globally despite increasing parallelism; (2) PCDN converges to the\nspecified accuracy $\\epsilon$ within the limited iteration number of\n$T_\\epsilon$, and $T_\\epsilon$ decreases with increasing parallelism (bundle\nsize $P$). Using the implementation technique of maintaining intermediate\nquantities, we minimize the data transfer and synchronization cost of the\n$P$-dimensional line search. For concreteness, the proposed PCDN algorithm is\napplied to $\\ell_1$-regularized logistic regression and $\\ell_2$-loss SVM.\nExperimental evaluations on six benchmark datasets show that the proposed PCDN\nalgorithm exploits parallelism well and outperforms the state-of-the-art\nmethods in speed without losing accuracy.\n", "versions": [{"version": "v1", "created": "Tue, 18 Jun 2013 07:03:16 GMT"}, {"version": "v2", "created": "Fri, 27 Dec 2013 08:41:37 GMT"}, {"version": "v3", "created": "Tue, 18 Mar 2014 14:55:49 GMT"}, {"version": "v4", "created": "Thu, 7 Dec 2017 09:16:27 GMT"}], "update_date": "2017-12-08", "authors_parsed": [["Bian", "An", ""], ["Li", "Xiong", ""], ["Liu", "Yuncai", ""], ["Yang", "Ming-Hsuan", ""]]}, {"id": "1306.4152", "submitter": "Maumita Bhattacharya", "authors": "Maumita Bhattacharya", "title": "Bioclimating Modelling: A Machine Learning Perspective", "comments": "8 pages, In the Proceedings of the 2012 International Joint\n  Conferences on Computer, Information, and Systems Sciences, and Engineering\n  (CISSE 2012)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many machine learning (ML) approaches are widely used to generate bioclimatic\nmodels for prediction of geographic range of organism as a function of climate.\nApplications such as prediction of range shift in organism, range of invasive\nspecies influenced by climate change are important parameters in understanding\nthe impact of climate change. However, success of machine learning-based\napproaches depends on a number of factors. While it can be safely said that no\nparticular ML technique can be effective in all applications and success of a\ntechnique is predominantly dependent on the application or the type of the\nproblem, it is useful to understand their behaviour to ensure informed choice\nof techniques. This paper presents a comprehensive review of machine\nlearning-based bioclimatic model generation and analyses the factors\ninfluencing success of such models. Considering the wide use of statistical\ntechniques, in our discussion we also include conventional statistical\ntechniques used in bioclimatic modelling.\n", "versions": [{"version": "v1", "created": "Tue, 18 Jun 2013 11:42:03 GMT"}], "update_date": "2013-06-19", "authors_parsed": [["Bhattacharya", "Maumita", ""]]}, {"id": "1306.4410", "submitter": "Junhui Wang", "authors": "Junhui Wang", "title": "Joint estimation of sparse multivariate regression and conditional\n  graphical models", "comments": "29 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multivariate regression model is a natural generalization of the classical\nunivari- ate regression model for fitting multiple responses. In this paper, we\npropose a high- dimensional multivariate conditional regression model for\nconstructing sparse estimates of the multivariate regression coefficient matrix\nthat accounts for the dependency struc- ture among the multiple responses. The\nproposed method decomposes the multivariate regression problem into a series of\npenalized conditional log-likelihood of each response conditioned on the\ncovariates and other responses. It allows simultaneous estimation of the sparse\nregression coefficient matrix and the sparse inverse covariance matrix. The\nasymptotic selection consistency and normality are established for the\ndiverging dimension of the covariates and number of responses. The\neffectiveness of the pro- posed method is also demonstrated in a variety of\nsimulated examples as well as an application to the Glioblastoma multiforme\ncancer data.\n", "versions": [{"version": "v1", "created": "Wed, 19 Jun 2013 01:56:29 GMT"}], "update_date": "2016-11-26", "authors_parsed": [["Wang", "Junhui", ""]]}, {"id": "1306.4447", "submitter": "Antonio Villani", "authors": "Giuseppe Ateniese, Giovanni Felici, Luigi V. Mancini, Angelo\n  Spognardi, Antonio Villani, Domenico Vitali", "title": "Hacking Smart Machines with Smarter Ones: How to Extract Meaningful Data\n  from Machine Learning Classifiers", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Machine Learning (ML) algorithms are used to train computers to perform a\nvariety of complex tasks and improve with experience. Computers learn how to\nrecognize patterns, make unintended decisions, or react to a dynamic\nenvironment. Certain trained machines may be more effective than others because\nthey are based on more suitable ML algorithms or because they were trained\nthrough superior training sets. Although ML algorithms are known and publicly\nreleased, training sets may not be reasonably ascertainable and, indeed, may be\nguarded as trade secrets. While much research has been performed about the\nprivacy of the elements of training sets, in this paper we focus our attention\non ML classifiers and on the statistical information that can be unconsciously\nor maliciously revealed from them. We show that it is possible to infer\nunexpected but useful information from ML classifiers. In particular, we build\na novel meta-classifier and train it to hack other classifiers, obtaining\nmeaningful information about their training sets. This kind of information\nleakage can be exploited, for example, by a vendor to build more effective\nclassifiers or to simply acquire trade secrets from a competitor's apparatus,\npotentially violating its intellectual property rights.\n", "versions": [{"version": "v1", "created": "Wed, 19 Jun 2013 07:51:49 GMT"}], "update_date": "2013-06-20", "authors_parsed": [["Ateniese", "Giuseppe", ""], ["Felici", "Giovanni", ""], ["Mancini", "Luigi V.", ""], ["Spognardi", "Angelo", ""], ["Villani", "Antonio", ""], ["Vitali", "Domenico", ""]]}, {"id": "1306.4631", "submitter": "Rachana Parikh", "authors": "Rachana Parikh and Avani R. Vasant", "title": "Table of Content detection using Machine Learning", "comments": "International Journal of Artificial Intelligence and Applications,\n  May-2013", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Table of content (TOC) detection has drawn attention now a day because it\nplays an important role in digitization of multipage document. Generally book\ndocument is multipage document. So it becomes necessary to detect Table of\nContent page for easy navigation of multipage document and also to make\ninformation retrieval faster for desirable data from the multipage document.\nAll the Table of content pages follow the different layout, different way of\npresenting the contents of the document like chapter, section, subsection etc.\nThis paper introduces a new method to detect Table of content using machine\nlearning technique with different features. With the main aim to detect Table\nof Content pages is to structure the document according to their contents.\n", "versions": [{"version": "v1", "created": "Thu, 6 Jun 2013 08:08:22 GMT"}], "update_date": "2013-06-20", "authors_parsed": [["Parikh", "Rachana", ""], ["Vasant", "Avani R.", ""]]}, {"id": "1306.4633", "submitter": "Mayank Shishodia B.Tech", "authors": "Sumit Goswami and Mayank Singh Shishodia", "title": "A Fuzzy Based Approach to Text Mining and Document Clustering", "comments": "10 pages, 6 tables, 1 figure, review paper, International Journal of\n  Data Mining & Knowledge Management Process (IJDKP) ISSN : 2230 - 9608[Online]\n  ; 2231 - 007X [Print]. Paper can be found at\n  http://airccse.org/journal/ijdkp/current2013.html", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fuzzy logic deals with degrees of truth. In this paper, we have shown how to\napply fuzzy logic in text mining in order to perform document clustering. We\ntook an example of document clustering where the documents had to be clustered\ninto two categories. The method involved cleaning up the text and stemming of\nwords. Then, we chose m number of features which differ significantly in their\nword frequencies (WF), normalized by document length, between documents\nbelonging to these two clusters. The documents to be clustered were represented\nas a collection of m normalized WF values. Fuzzy c-means (FCM) algorithm was\nused to cluster these documents into two clusters. After the FCM execution\nfinished, the documents in the two clusters were analysed for the values of\ntheir respective m features. It was known that documents belonging to a\ndocument type, say X, tend to have higher WF values for some particular\nfeatures. If the documents belonging to a cluster had higher WF values for\nthose same features, then that cluster was said to represent X. By fuzzy logic,\nwe not only get the cluster name, but also the degree to which a document\nbelongs to a cluster.\n", "versions": [{"version": "v1", "created": "Thu, 6 Jun 2013 07:35:23 GMT"}], "update_date": "2013-06-20", "authors_parsed": [["Goswami", "Sumit", ""], ["Shishodia", "Mayank Singh", ""]]}, {"id": "1306.4650", "submitter": "Julien Mairal", "authors": "Julien Mairal (INRIA Grenoble Rh\\^one-Alpes / LJK Laboratoire Jean\n  Kuntzmann)", "title": "Stochastic Majorization-Minimization Algorithms for Large-Scale\n  Optimization", "comments": "accepted for publication for Neural Information Processing Systems\n  (NIPS) 2013. This is the 9-pages version followed by 16 pages of appendices.\n  The title has changed compared to the first technical report", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Majorization-minimization algorithms consist of iteratively minimizing a\nmajorizing surrogate of an objective function. Because of its simplicity and\nits wide applicability, this principle has been very popular in statistics and\nin signal processing. In this paper, we intend to make this principle scalable.\nWe introduce a stochastic majorization-minimization scheme which is able to\ndeal with large-scale or possibly infinite data sets. When applied to convex\noptimization problems under suitable assumptions, we show that it achieves an\nexpected convergence rate of $O(1/\\sqrt{n})$ after $n$ iterations, and of\n$O(1/n)$ for strongly convex functions. Equally important, our scheme almost\nsurely converges to stationary points for a large class of non-convex problems.\nWe develop several efficient algorithms based on our framework. First, we\npropose a new stochastic proximal gradient method, which experimentally matches\nstate-of-the-art solvers for large-scale $\\ell_1$-logistic regression. Second,\nwe develop an online DC programming algorithm for non-convex sparse estimation.\nFinally, we demonstrate the effectiveness of our approach for solving\nlarge-scale structured matrix factorization problems.\n", "versions": [{"version": "v1", "created": "Wed, 19 Jun 2013 19:21:48 GMT"}, {"version": "v2", "created": "Tue, 10 Sep 2013 12:29:41 GMT"}], "update_date": "2013-09-11", "authors_parsed": [["Mairal", "Julien", "", "INRIA Grenoble Rh\u00f4ne-Alpes / LJK Laboratoire Jean\n  Kuntzmann"]]}, {"id": "1306.4653", "submitter": "Satyen Kale", "authors": "Satyen Kale", "title": "Multiarmed Bandits With Limited Expert Advice", "comments": "Updated with tighter upper bound based on PolyINF algorithm, lower\n  bound nearly matching the upper bound, and fixed some typos", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We solve the COLT 2013 open problem of \\citet{SCB} on minimizing regret in\nthe setting of advice-efficient multiarmed bandits with expert advice. We give\nan algorithm for the setting of K arms and N experts out of which we are\nallowed to query and use only M experts' advices in each round, which has a\nregret bound of \\tilde{O}\\bigP{\\sqrt{\\frac{\\min\\{K, M\\} N}{M} T}} after T\nrounds. We also prove that any algorithm for this problem must have expected\nregret at least \\tilde{\\Omega}\\bigP{\\sqrt{\\frac{\\min\\{K, M\\} N}{M}T}}, thus\nshowing that our upper bound is nearly tight.\n", "versions": [{"version": "v1", "created": "Wed, 19 Jun 2013 19:25:51 GMT"}, {"version": "v2", "created": "Thu, 27 Jun 2013 19:48:35 GMT"}, {"version": "v3", "created": "Fri, 28 Jun 2013 18:35:06 GMT"}, {"version": "v4", "created": "Mon, 8 Jul 2013 19:05:49 GMT"}], "update_date": "2013-07-09", "authors_parsed": [["Kale", "Satyen", ""]]}, {"id": "1306.4753", "submitter": "Geoffrey Gordon", "authors": "Geoffrey J. Gordon", "title": "Galerkin Methods for Complementarity Problems and Variational\n  Inequalities", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Complementarity problems and variational inequalities arise in a wide variety\nof areas, including machine learning, planning, game theory, and physical\nsimulation. In all of these areas, to handle large-scale problem instances, we\nneed fast approximate solution methods. One promising idea is Galerkin\napproximation, in which we search for the best answer within the span of a\ngiven set of basis functions. Bertsekas proposed one possible Galerkin method\nfor variational inequalities. However, this method can exhibit two problems in\npractice: its approximation error is worse than might be expected based on the\nability of the basis to represent the desired solution, and each iteration\nrequires a projection step that is not always easy to implement efficiently.\nSo, in this paper, we present a new Galerkin method with improved behavior: our\nnew error bounds depend directly on the distance from the true solution to the\nsubspace spanned by our basis, and the only projections we require are onto the\nfeasible region or onto the span of our basis.\n", "versions": [{"version": "v1", "created": "Thu, 20 Jun 2013 04:48:37 GMT"}], "update_date": "2013-06-21", "authors_parsed": [["Gordon", "Geoffrey J.", ""]]}, {"id": "1306.4905", "submitter": "Martin Trnecka", "authors": "Radim Belohlavek, Martin Trnecka", "title": "From-Below Approximations in Boolean Matrix Factorization: Geometry and\n  New Algorithm", "comments": null, "journal-ref": null, "doi": "10.1016/j.jcss.2015.06.002", "report-no": null, "categories": "cs.NA cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present new results on Boolean matrix factorization and a new algorithm\nbased on these results. The results emphasize the significance of\nfactorizations that provide from-below approximations of the input matrix.\nWhile the previously proposed algorithms do not consider the possibly different\nsignificance of different matrix entries, our results help measure such\nsignificance and suggest where to focus when computing factors. An experimental\nevaluation of the new algorithm on both synthetic and real data demonstrates\nits good performance in terms of good coverage by the first k factors as well\nas a small number of factors needed for exact decomposition and indicates that\nthe algorithm outperforms the available ones in these terms. We also propose\nfuture research topics.\n", "versions": [{"version": "v1", "created": "Thu, 20 Jun 2013 15:19:22 GMT"}], "update_date": "2015-06-26", "authors_parsed": [["Belohlavek", "Radim", ""], ["Trnecka", "Martin", ""]]}, {"id": "1306.4947", "submitter": "Xiaojin Zhu", "authors": "Xiaojin Zhu", "title": "Machine Teaching for Bayesian Learners in the Exponential Family", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  What if there is a teacher who knows the learning goal and wants to design\ngood training data for a machine learner? We propose an optimal teaching\nframework aimed at learners who employ Bayesian models. Our framework is\nexpressed as an optimization problem over teaching examples that balance the\nfuture loss of the learner and the effort of the teacher. This optimization\nproblem is in general hard. In the case where the learner employs conjugate\nexponential family models, we present an approximate algorithm for finding the\noptimal teaching set. Our algorithm optimizes the aggregate sufficient\nstatistics, then unpacks them into actual teaching examples. We give several\nexamples to illustrate our framework.\n", "versions": [{"version": "v1", "created": "Thu, 20 Jun 2013 18:04:24 GMT"}, {"version": "v2", "created": "Thu, 3 Oct 2013 17:15:45 GMT"}], "update_date": "2013-10-04", "authors_parsed": [["Zhu", "Xiaojin", ""]]}, {"id": "1306.5056", "submitter": "Tyler Sanderson", "authors": "Tyler Sanderson and Clayton Scott", "title": "Class Proportion Estimation with Application to Multiclass Anomaly\n  Rejection", "comments": "Accepted to AISTATS 2014. 15 pages. 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work addresses two classification problems that fall under the heading\nof domain adaptation, wherein the distributions of training and testing\nexamples differ. The first problem studied is that of class proportion\nestimation, which is the problem of estimating the class proportions in an\nunlabeled testing data set given labeled examples of each class. Compared to\nprevious work on this problem, our approach has the novel feature that it does\nnot require labeled training data from one of the classes. This property allows\nus to address the second domain adaptation problem, namely, multiclass anomaly\nrejection. Here, the goal is to design a classifier that has the option of\nassigning a \"reject\" label, indicating that the instance did not arise from a\nclass present in the training data. We establish consistent learning strategies\nfor both of these domain adaptation problems, which to our knowledge are the\nfirst of their kind. We also implement the class proportion estimation\ntechnique and demonstrate its performance on several benchmark data sets.\n", "versions": [{"version": "v1", "created": "Fri, 21 Jun 2013 06:25:54 GMT"}, {"version": "v2", "created": "Thu, 27 Jun 2013 03:36:55 GMT"}, {"version": "v3", "created": "Sat, 22 Feb 2014 08:58:10 GMT"}], "update_date": "2014-02-25", "authors_parsed": [["Sanderson", "Tyler", ""], ["Scott", "Clayton", ""]]}, {"id": "1306.5349", "submitter": "Erick Stattner", "authors": "Erick Stattner and Wilfried Segretier and Martine Collard and Philippe\n  Hunel and Nicolas Vidot", "title": "Song-based Classification techniques for Endangered Bird Conservation", "comments": "6 pages, 4 figures. In ICML 2013 Workshop on Machine Learning for\n  Bioacoustics", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The work presented in this paper is part of a global framework which long\nterm goal is to design a wireless sensor network able to support the\nobservation of a population of endangered birds. We present the first stage for\nwhich we have conducted a knowledge discovery approach on a sample of\nacoustical data. We use MFCC features extracted from bird songs and we exploit\ntwo knowledge discovery techniques. One that relies on clustering-based\napproaches, that highlights the homogeneity in the songs of the species. The\nother, based on predictive modeling, that demonstrates the good performances of\nvarious machine learning techniques for the identification process. The\nknowledge elicited provides promising results to consider a widespread study\nand to elicit guidelines for designing a first version of the automatic\napproach for data collection based on acoustic sensors.\n", "versions": [{"version": "v1", "created": "Sat, 22 Jun 2013 19:32:05 GMT"}], "update_date": "2013-06-25", "authors_parsed": [["Stattner", "Erick", ""], ["Segretier", "Wilfried", ""], ["Collard", "Martine", ""], ["Hunel", "Philippe", ""], ["Vidot", "Nicolas", ""]]}, {"id": "1306.5362", "submitter": "Michael Mahoney", "authors": "Ping Ma and Michael W. Mahoney and Bin Yu", "title": "A Statistical Perspective on Algorithmic Leveraging", "comments": "44 pages, 17 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One popular method for dealing with large-scale data sets is sampling. For\nexample, by using the empirical statistical leverage scores as an importance\nsampling distribution, the method of algorithmic leveraging samples and\nrescales rows/columns of data matrices to reduce the data size before\nperforming computations on the subproblem. This method has been successful in\nimproving computational efficiency of algorithms for matrix problems such as\nleast-squares approximation, least absolute deviations approximation, and\nlow-rank matrix approximation. Existing work has focused on algorithmic issues\nsuch as worst-case running times and numerical issues associated with providing\nhigh-quality implementations, but none of it addresses statistical aspects of\nthis method.\n  In this paper, we provide a simple yet effective framework to evaluate the\nstatistical properties of algorithmic leveraging in the context of estimating\nparameters in a linear regression model with a fixed number of predictors. We\nshow that from the statistical perspective of bias and variance, neither\nleverage-based sampling nor uniform sampling dominates the other. This result\nis particularly striking, given the well-known result that, from the\nalgorithmic perspective of worst-case analysis, leverage-based sampling\nprovides uniformly superior worst-case algorithmic results, when compared with\nuniform sampling. Based on these theoretical results, we propose and analyze\ntwo new leveraging algorithms. A detailed empirical evaluation of existing\nleverage-based methods as well as these two new methods is carried out on both\nsynthetic and real data sets. The empirical results indicate that our theory is\na good predictor of practical performance of existing and new leverage-based\nalgorithms and that the new algorithms achieve improved performance.\n", "versions": [{"version": "v1", "created": "Sun, 23 Jun 2013 00:31:15 GMT"}], "update_date": "2013-06-25", "authors_parsed": [["Ma", "Ping", ""], ["Mahoney", "Michael W.", ""], ["Yu", "Bin", ""]]}, {"id": "1306.5487", "submitter": "Jose Hernandez-Orallo", "authors": "Celestine-Periale Maguedong-Djoumessi", "title": "Model Reframing by Feature Context Change", "comments": "MSc Thesis, 126 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The feature space (including both input and output variables) characterises a\ndata mining problem. In predictive (supervised) problems, the quality and\navailability of features determines the predictability of the dependent\nvariable, and the performance of data mining models in terms of\nmisclassification or regression error. Good features, however, are usually\ndifficult to obtain. It is usual that many instances come with missing values,\neither because the actual value for a given attribute was not available or\nbecause it was too expensive. This is usually interpreted as a utility or\ncost-sensitive learning dilemma, in this case between misclassification (or\nregression error) costs and attribute tests costs. Both misclassification cost\n(MC) and test cost (TC) can be integrated into a single measure, known as joint\ncost (JC). We introduce methods and plots (such as the so-called JROC plots)\nthat can work with any of-the-shelf predictive technique, including ensembles,\nsuch that we re-frame the model to use the appropriate subset of attributes\n(the feature configuration) during deployment time. In other words, models are\ntrained with the available attributes (once and for all) and then deployed by\nsetting missing values on the attributes that are deemed ineffective for\nreducing the joint cost. As the number of feature configuration combinations\ngrows exponentially with the number of features we introduce quadratic methods\nthat are able to approximate the optimal configuration and model choices, as\nshown by the experimental results.\n", "versions": [{"version": "v1", "created": "Sun, 23 Jun 2013 23:36:40 GMT"}], "update_date": "2013-06-25", "authors_parsed": [["Maguedong-Djoumessi", "Celestine-Periale", ""]]}, {"id": "1306.5532", "submitter": "St\\'ephane Mallat", "authors": "St\\'ephane Mallat and Ir\\`ene Waldspurger", "title": "Deep Learning by Scattering", "comments": "10 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce general scattering transforms as mathematical models of deep\nneural networks with l2 pooling. Scattering networks iteratively apply complex\nvalued unitary operators, and the pooling is performed by a complex modulus. An\nexpected scattering defines a contractive representation of a high-dimensional\nprobability distribution, which preserves its mean-square norm. We show that\nunsupervised learning can be casted as an optimization of the space contraction\nto preserve the volume occupied by unlabeled examples, at each layer of the\nnetwork. Supervised learning and classification are performed with an averaged\nscattering, which provides scattering estimations for multiple classes.\n", "versions": [{"version": "v1", "created": "Mon, 24 Jun 2013 07:52:45 GMT"}, {"version": "v2", "created": "Thu, 25 Jun 2015 17:26:01 GMT"}], "update_date": "2015-06-26", "authors_parsed": [["Mallat", "St\u00e9phane", ""], ["Waldspurger", "Ir\u00e8ne", ""]]}, {"id": "1306.5554", "submitter": "Brian McWilliams", "authors": "Brian McWilliams, David Balduzzi and Joachim M. Buhmann", "title": "Correlated random features for fast semi-supervised learning", "comments": "15 pages, 3 figures, 6 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents Correlated Nystrom Views (XNV), a fast semi-supervised\nalgorithm for regression and classification. The algorithm draws on two main\nideas. First, it generates two views consisting of computationally inexpensive\nrandom features. Second, XNV applies multiview regression using Canonical\nCorrelation Analysis (CCA) on unlabeled data to bias the regression towards\nuseful features. It has been shown that, if the views contains accurate\nestimators, CCA regression can substantially reduce variance with a minimal\nincrease in bias. Random views are justified by recent theoretical and\nempirical work showing that regression with random features closely\napproximates kernel regression, implying that random views can be expected to\ncontain accurate estimators. We show that XNV consistently outperforms a\nstate-of-the-art algorithm for semi-supervised learning: substantially\nimproving predictive performance and reducing the variability of performance on\na wide variety of real-world datasets, whilst also reducing runtime by orders\nof magnitude.\n", "versions": [{"version": "v1", "created": "Mon, 24 Jun 2013 09:49:08 GMT"}, {"version": "v2", "created": "Tue, 5 Nov 2013 11:28:33 GMT"}], "update_date": "2013-11-06", "authors_parsed": [["McWilliams", "Brian", ""], ["Balduzzi", "David", ""], ["Buhmann", "Joachim M.", ""]]}, {"id": "1306.5707", "submitter": "Jaeyong Sung", "authors": "Jaeyong Sung, Bart Selman, Ashutosh Saxena", "title": "Synthesizing Manipulation Sequences for Under-Specified Tasks using\n  Unrolled Markov Random Fields", "comments": "To Appear in IEEE/RSJ International Conference on Intelligent Robots\n  and Systems (IROS) 2014 (A preliminary version of this work was presented at\n  International Conference of Machine Learning (ICML) workshop on Prediction\n  with Sequential Models, 2013)", "journal-ref": null, "doi": "10.1109/IROS.2014.6942972", "report-no": null, "categories": "cs.RO cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many tasks in human environments require performing a sequence of navigation\nand manipulation steps involving objects. In unstructured human environments,\nthe location and configuration of the objects involved often change in\nunpredictable ways. This requires a high-level planning strategy that is robust\nand flexible in an uncertain environment. We propose a novel dynamic planning\nstrategy, which can be trained from a set of example sequences. High level\ntasks are expressed as a sequence of primitive actions or controllers (with\nappropriate parameters). Our score function, based on Markov Random Field\n(MRF), captures the relations between environment, controllers, and their\narguments. By expressing the environment using sets of attributes, the approach\ngeneralizes well to unseen scenarios. We train the parameters of our MRF using\na maximum margin learning method. We provide a detailed empirical validation of\nour overall framework demonstrating successful plan strategies for a variety of\ntasks.\n", "versions": [{"version": "v1", "created": "Mon, 24 Jun 2013 18:48:54 GMT"}, {"version": "v2", "created": "Tue, 24 Jun 2014 05:10:50 GMT"}], "update_date": "2015-04-14", "authors_parsed": [["Sung", "Jaeyong", ""], ["Selman", "Bart", ""], ["Saxena", "Ashutosh", ""]]}, {"id": "1306.5825", "submitter": "Ying Xiao", "authors": "Navin Goyal, Santosh Vempala and Ying Xiao", "title": "Fourier PCA and Robust Tensor Decomposition", "comments": "Extensively revised; details added; minor errors corrected;\n  exposition improved", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fourier PCA is Principal Component Analysis of a matrix obtained from higher\norder derivatives of the logarithm of the Fourier transform of a\ndistribution.We make this method algorithmic by developing a tensor\ndecomposition method for a pair of tensors sharing the same vectors in rank-$1$\ndecompositions. Our main application is the first provably polynomial-time\nalgorithm for underdetermined ICA, i.e., learning an $n \\times m$ matrix $A$\nfrom observations $y=Ax$ where $x$ is drawn from an unknown product\ndistribution with arbitrary non-Gaussian components. The number of component\ndistributions $m$ can be arbitrarily higher than the dimension $n$ and the\ncolumns of $A$ only need to satisfy a natural and efficiently verifiable\nnondegeneracy condition. As a second application, we give an alternative\nalgorithm for learning mixtures of spherical Gaussians with linearly\nindependent means. These results also hold in the presence of Gaussian noise.\n", "versions": [{"version": "v1", "created": "Tue, 25 Jun 2013 01:44:46 GMT"}, {"version": "v2", "created": "Tue, 23 Jul 2013 05:58:50 GMT"}, {"version": "v3", "created": "Fri, 15 Nov 2013 00:58:34 GMT"}, {"version": "v4", "created": "Fri, 30 May 2014 04:39:23 GMT"}, {"version": "v5", "created": "Fri, 27 Jun 2014 20:37:17 GMT"}], "update_date": "2014-07-01", "authors_parsed": [["Goyal", "Navin", ""], ["Vempala", "Santosh", ""], ["Xiao", "Ying", ""]]}, {"id": "1306.5884", "submitter": "Sandeep Venkatesh", "authors": "Sandeep Venkatesh, Meera V Patil, Nanditha Swamy", "title": "Design of an Agent for Answering Back in Smart Phones", "comments": "This paper has been withdrawn by the author due to a crucial sign\n  erro", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.HC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The objective of the paper is to design an agent which provides efficient\nresponse to the caller when a call goes unanswered in smartphones. The agent\nprovides responses through text messages, email etc stating the most likely\nreason as to why the callee is unable to answer a call. Responses are composed\ntaking into consideration the importance of the present call and the situation\nthe callee is in at the moment like driving, sleeping, at work etc. The agent\nmakes decisons in the compostion of response messages based on the patterns it\nhas come across in the learning environment. Initially the user helps the agent\nto compose response messages. The agent associates this message to the percept\nit recieves with respect to the environment the callee is in. The user may\nthereafter either choose to make to response system automatic or choose to\nrecieve suggestions from the agent for responses messages and confirm what is\nto be sent to the caller.\n", "versions": [{"version": "v1", "created": "Tue, 25 Jun 2013 08:56:58 GMT"}, {"version": "v2", "created": "Wed, 1 Jan 2014 11:02:00 GMT"}], "update_date": "2014-01-03", "authors_parsed": [["Venkatesh", "Sandeep", ""], ["Patil", "Meera V", ""], ["Swamy", "Nanditha", ""]]}, {"id": "1306.5918", "submitter": "Zhaosong Lu", "authors": "Zhaosong Lu and Lin Xiao", "title": "A Randomized Nonmonotone Block Proximal Gradient Method for a Class of\n  Structured Nonlinear Programming", "comments": "The previous title was \"Randomized Block Coordinate Non-Monotone\n  Gradient Method for a Class of Nonlinear Programming\"", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.LG cs.NA math.NA stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a randomized nonmonotone block proximal gradient (RNBPG) method\nfor minimizing the sum of a smooth (possibly nonconvex) function and a\nblock-separable (possibly nonconvex nonsmooth) function. At each iteration,\nthis method randomly picks a block according to any prescribed probability\ndistribution and solves typically several associated proximal subproblems that\nusually have a closed-form solution, until a certain progress on objective\nvalue is achieved. In contrast to the usual randomized block coordinate descent\nmethod [23,20], our method has a nonmonotone flavor and uses variable stepsizes\nthat can partially utilize the local curvature information of the smooth\ncomponent of objective function. We show that any accumulation point of the\nsolution sequence of the method is a stationary point of the problem {\\it\nalmost surely} and the method is capable of finding an approximate stationary\npoint with high probability. We also establish a sublinear rate of convergence\nfor the method in terms of the minimal expected squared norm of certain\nproximal gradients over the iterations. When the problem under consideration is\nconvex, we show that the expected objective values generated by RNBPG converge\nto the optimal value of the problem. Under some assumptions, we further\nestablish a sublinear and linear rate of convergence on the expected objective\nvalues generated by a monotone version of RNBPG. Finally, we conduct some\npreliminary experiments to test the performance of RNBPG on the\n$\\ell_1$-regularized least-squares problem and a dual SVM problem in machine\nlearning. The computational results demonstrate that our method substantially\noutperforms the randomized block coordinate {\\it descent} method with fixed or\nvariable stepsizes.\n", "versions": [{"version": "v1", "created": "Tue, 25 Jun 2013 11:11:42 GMT"}, {"version": "v2", "created": "Sat, 21 Mar 2015 01:11:56 GMT"}], "update_date": "2015-03-24", "authors_parsed": [["Lu", "Zhaosong", ""], ["Xiao", "Lin", ""]]}, {"id": "1306.6111", "submitter": "David Darmon", "authors": "David Darmon, Jared Sylvester, Michelle Girvan, William Rand", "title": "Understanding the Predictive Power of Computational Mechanics and Echo\n  State Networks in Social Media", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.LG physics.soc-ph stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There is a large amount of interest in understanding users of social media in\norder to predict their behavior in this space. Despite this interest, user\npredictability in social media is not well-understood. To examine this\nquestion, we consider a network of fifteen thousand users on Twitter over a\nseven week period. We apply two contrasting modeling paradigms: computational\nmechanics and echo state networks. Both methods attempt to model the behavior\nof users on the basis of their past behavior. We demonstrate that the behavior\nof users on Twitter can be well-modeled as processes with self-feedback. We\nfind that the two modeling approaches perform very similarly for most users,\nbut that they differ in performance on a small subset of the users. By\nexploring the properties of these performance-differentiated users, we\nhighlight the challenges faced in applying predictive models to dynamic social\ndata.\n", "versions": [{"version": "v1", "created": "Wed, 26 Jun 2013 00:58:39 GMT"}, {"version": "v2", "created": "Fri, 23 Aug 2013 20:13:27 GMT"}], "update_date": "2013-08-27", "authors_parsed": [["Darmon", "David", ""], ["Sylvester", "Jared", ""], ["Girvan", "Michelle", ""], ["Rand", "William", ""]]}, {"id": "1306.6189", "submitter": "Aviv Tamar", "authors": "Aviv Tamar, Huan Xu, Shie Mannor", "title": "Scaling Up Robust MDPs by Reinforcement Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider large-scale Markov decision processes (MDPs) with parameter\nuncertainty, under the robust MDP paradigm. Previous studies showed that robust\nMDPs, based on a minimax approach to handle uncertainty, can be solved using\ndynamic programming for small to medium sized problems. However, due to the\n\"curse of dimensionality\", MDPs that model real-life problems are typically\nprohibitively large for such approaches. In this work we employ a reinforcement\nlearning approach to tackle this planning problem: we develop a robust\napproximate dynamic programming method based on a projected fixed point\nequation to approximately solve large scale robust MDPs. We show that the\nproposed method provably succeeds under certain technical conditions, and\ndemonstrate its effectiveness through simulation of an option pricing problem.\nTo the best of our knowledge, this is the first attempt to scale up the robust\nMDPs paradigm.\n", "versions": [{"version": "v1", "created": "Wed, 26 Jun 2013 09:52:51 GMT"}], "update_date": "2013-06-27", "authors_parsed": [["Tamar", "Aviv", ""], ["Xu", "Huan", ""], ["Mannor", "Shie", ""]]}, {"id": "1306.6302", "submitter": "Roni Khardon", "authors": "S. Joshi, R. Khardon, P. Tadepalli, A. Raghavan, A. Fern", "title": "Solving Relational MDPs with Exogenous Events and Additive Rewards", "comments": "This is an extended version of our ECML/PKDD 2013 paper including all\n  proofs. (v2 corrects typos and updates ref [10] to cite this report as the\n  full version)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We formalize a simple but natural subclass of service domains for relational\nplanning problems with object-centered, independent exogenous events and\nadditive rewards capturing, for example, problems in inventory control.\nFocusing on this subclass, we present a new symbolic planning algorithm which\nis the first algorithm that has explicit performance guarantees for relational\nMDPs with exogenous events. In particular, under some technical conditions, our\nplanning algorithm provides a monotonic lower bound on the optimal value\nfunction. To support this algorithm we present novel evaluation and reduction\ntechniques for generalized first order decision diagrams, a knowledge\nrepresentation for real-valued functions over relational world states. Our\nplanning algorithm uses a set of focus states, which serves as a training set,\nto simplify and approximate the symbolic solution, and can thus be seen to\nperform learning for planning. A preliminary experimental evaluation\ndemonstrates the validity of our approach.\n", "versions": [{"version": "v1", "created": "Wed, 26 Jun 2013 17:59:49 GMT"}, {"version": "v2", "created": "Thu, 27 Jun 2013 13:57:19 GMT"}], "update_date": "2013-06-28", "authors_parsed": [["Joshi", "S.", ""], ["Khardon", "R.", ""], ["Tadepalli", "P.", ""], ["Raghavan", "A.", ""], ["Fern", "A.", ""]]}, {"id": "1306.6482", "submitter": "Shun Kataoka", "authors": "Shun Kataoka, Muneki Yasuda, Cyril Furtlehner and Kazuyuki Tanaka", "title": "Traffic data reconstruction based on Markov random field modeling", "comments": "12 pages, 4 figures", "journal-ref": "Inverse Problems 30 (2014) 025003", "doi": "10.1088/0266-5611/30/2/025003", "report-no": null, "categories": "stat.ML cond-mat.dis-nn cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the traffic data reconstruction problem. Suppose we have the\ntraffic data of an entire city that are incomplete because some road data are\nunobserved. The problem is to reconstruct the unobserved parts of the data. In\nthis paper, we propose a new method to reconstruct incomplete traffic data\ncollected from various traffic sensors. Our approach is based on Markov random\nfield modeling of road traffic. The reconstruction is achieved by using\nmean-field method and a machine learning method. We numerically verify the\nperformance of our method using realistic simulated traffic data for the real\nroad network of Sendai, Japan.\n", "versions": [{"version": "v1", "created": "Thu, 27 Jun 2013 12:43:09 GMT"}], "update_date": "2014-02-07", "authors_parsed": [["Kataoka", "Shun", ""], ["Yasuda", "Muneki", ""], ["Furtlehner", "Cyril", ""], ["Tanaka", "Kazuyuki", ""]]}, {"id": "1306.6709", "submitter": "Aur\\'elien Bellet", "authors": "Aur\\'elien Bellet, Amaury Habrard and Marc Sebban", "title": "A Survey on Metric Learning for Feature Vectors and Structured Data", "comments": "Technical report, 59 pages. Changes in v2: fixed typos and improved\n  presentation. Changes in v3: fixed typos. Changes in v4: fixed typos and new\n  methods", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The need for appropriate ways to measure the distance or similarity between\ndata is ubiquitous in machine learning, pattern recognition and data mining,\nbut handcrafting such good metrics for specific problems is generally\ndifficult. This has led to the emergence of metric learning, which aims at\nautomatically learning a metric from data and has attracted a lot of interest\nin machine learning and related fields for the past ten years. This survey\npaper proposes a systematic review of the metric learning literature,\nhighlighting the pros and cons of each approach. We pay particular attention to\nMahalanobis distance metric learning, a well-studied and successful framework,\nbut additionally present a wide range of methods that have recently emerged as\npowerful alternatives, including nonlinear metric learning, similarity learning\nand local metric learning. Recent trends and extensions, such as\nsemi-supervised metric learning, metric learning for histogram data and the\nderivation of generalization guarantees, are also covered. Finally, this survey\naddresses metric learning for structured data, in particular edit distance\nlearning, and attempts to give an overview of the remaining challenges in\nmetric learning for the years to come.\n", "versions": [{"version": "v1", "created": "Fri, 28 Jun 2013 03:56:15 GMT"}, {"version": "v2", "created": "Wed, 3 Jul 2013 04:48:05 GMT"}, {"version": "v3", "created": "Mon, 19 Aug 2013 21:28:07 GMT"}, {"version": "v4", "created": "Wed, 12 Feb 2014 07:45:11 GMT"}], "update_date": "2019-01-25", "authors_parsed": [["Bellet", "Aur\u00e9lien", ""], ["Habrard", "Amaury", ""], ["Sebban", "Marc", ""]]}, {"id": "1306.6802", "submitter": "Aris Kosmopoulos", "authors": "Aris Kosmopoulos, Ioannis Partalas, Eric Gaussier, Georgios Paliouras,\n  Ion Androutsopoulos", "title": "Evaluation Measures for Hierarchical Classification: a unified view and\n  novel approaches", "comments": "Submitted to journal", "journal-ref": null, "doi": "10.1007/s10618-014-0382-x", "report-no": null, "categories": "cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hierarchical classification addresses the problem of classifying items into a\nhierarchy of classes. An important issue in hierarchical classification is the\nevaluation of different classification algorithms, which is complicated by the\nhierarchical relations among the classes. Several evaluation measures have been\nproposed for hierarchical classification using the hierarchy in different ways.\nThis paper studies the problem of evaluation in hierarchical classification by\nanalyzing and abstracting the key components of the existing performance\nmeasures. It also proposes two alternative generic views of hierarchical\nevaluation and introduces two corresponding novel measures. The proposed\nmeasures, along with the state-of-the art ones, are empirically tested on three\nlarge datasets from the domain of text classification. The empirical results\nillustrate the undesirable behavior of existing approaches and how the proposed\nmethods overcome most of these methods across a range of cases.\n", "versions": [{"version": "v1", "created": "Fri, 28 Jun 2013 11:49:53 GMT"}, {"version": "v2", "created": "Mon, 1 Jul 2013 17:33:58 GMT"}], "update_date": "2015-04-01", "authors_parsed": [["Kosmopoulos", "Aris", ""], ["Partalas", "Ioannis", ""], ["Gaussier", "Eric", ""], ["Paliouras", "Georgios", ""], ["Androutsopoulos", "Ion", ""]]}]