[{"id": "2101.00001", "submitter": "Djallel Bouneffouf", "authors": "Djallel Bouneffouf", "title": "Etat de l'art sur l'application des bandits multi-bras", "comments": "in French", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The Multi-armed bandit offer the advantage to learn and exploit the already\nlearnt knowledge at the same time. This capability allows this approach to be\napplied in different domains, going from clinical trials where the goal is\ninvestigating the effects of different experimental treatments while minimizing\npatient losses, to adaptive routing where the goal is to minimize the delays in\na network. This article provides a review of the recent results on applying\nbandit to real-life scenario and summarize the state of the art for each of\nthese fields. Different techniques has been proposed to solve this problem\nsetting, like epsilon-greedy, Upper confident bound (UCB) and Thompson Sampling\n(TS). We are showing here how this algorithms were adapted to solve the\ndifferent problems of exploration exploitation.\n", "versions": [{"version": "v1", "created": "Mon, 4 Jan 2021 18:12:28 GMT"}], "update_date": "2021-01-05", "authors_parsed": [["Bouneffouf", "Djallel", ""]]}, {"id": "2101.00002", "submitter": "Luca Magri", "authors": "Alberto Racca and Luca Magri", "title": "Automatic-differentiated Physics-Informed Echo State Network (API-ESN)", "comments": "7 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG nlin.CD", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose the Automatic-differentiated Physics-Informed Echo State Network\n(API-ESN). The network is constrained by the physical equations through the\nreservoir's exact time-derivative, which is computed by automatic\ndifferentiation. As compared to the original Physics-Informed Echo State\nNetwork, the accuracy of the time-derivative is increased by up to seven orders\nof magnitude. This increased accuracy is key in chaotic dynamical systems,\nwhere errors grows exponentially in time. The network is showcased in the\nreconstruction of unmeasured (hidden) states of a chaotic system. The API-ESN\neliminates a source of error, which is present in existing physics-informed\necho state networks, in the computation of the time-derivative. This opens up\nnew possibilities for an accurate reconstruction of chaotic dynamical states.\n", "versions": [{"version": "v1", "created": "Mon, 28 Dec 2020 20:44:17 GMT"}, {"version": "v2", "created": "Wed, 24 Mar 2021 16:33:42 GMT"}], "update_date": "2021-03-25", "authors_parsed": [["Racca", "Alberto", ""], ["Magri", "Luca", ""]]}, {"id": "2101.00004", "submitter": "Julia Siekiera", "authors": "Julia Siekiera and Stefan Kramer", "title": "Deep Unsupervised Identification of Selected SNPs between Adapted\n  Populations on Pool-seq Data", "comments": "12 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.GN cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  The exploration of selected single nucleotide polymorphisms (SNPs) to\nidentify genetic diversity between different sequencing population pools\n(Pool-seq) is a fundamental task in genetic research. As underlying sequence\nreads and their alignment are error-prone and univariate statistical solutions\nonly take individual positions of the genome into account, the identification\nof selected SNPs remains a challenging process. Deep learning models like\nconvolutional neural networks (CNNs) are able to consider large input areas in\ntheir decisions. We suggest an unsupervised pipeline to be independent of a\nrarely known ground truth. We train a supervised discriminator CNN to\ndistinguish alignments from different populations and utilize the model for\nunsupervised SNP calling by applying explainable artificial intelligence\nmethods. Our proposed multivariate method is based on two main assumptions: We\nassume (i) that instances having a high predictive certainty of being\ndistinguishable are likely to contain genetic variants, and (ii) that selected\nSNPs are located at regions with input features having the highest influence on\nthe model's decision process. We directly compare our method with statistical\nresults on two different Pool-seq datasets and show that our solution is able\nto extend statistical results.\n", "versions": [{"version": "v1", "created": "Mon, 28 Dec 2020 22:28:44 GMT"}], "update_date": "2021-01-05", "authors_parsed": [["Siekiera", "Julia", ""], ["Kramer", "Stefan", ""]]}, {"id": "2101.00008", "submitter": "Farah Shamout", "authors": "Munachiso Nwadike, Takumi Miyawaki, Esha Sarkar, Michail Maniatakos,\n  Farah Shamout", "title": "Explainability Matters: Backdoor Attacks on Medical Imaging", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural networks have been shown to be vulnerable to backdoor attacks,\nwhich could be easily introduced to the training set prior to model training.\nRecent work has focused on investigating backdoor attacks on natural images or\ntoy datasets. Consequently, the exact impact of backdoors is not yet fully\nunderstood in complex real-world applications, such as in medical imaging where\nmisdiagnosis can be very costly. In this paper, we explore the impact of\nbackdoor attacks on a multi-label disease classification task using chest\nradiography, with the assumption that the attacker can manipulate the training\ndataset to execute the attack. Extensive evaluation of a state-of-the-art\narchitecture demonstrates that by introducing images with few-pixel\nperturbations into the training set, an attacker can execute the backdoor\nsuccessfully without having to be involved with the training procedure. A\nsimple 3$\\times$3 pixel trigger can achieve up to 1.00 Area Under the Receiver\nOperating Characteristic (AUROC) curve on the set of infected images. In the\nset of clean images, the backdoored neural network could still achieve up to\n0.85 AUROC, highlighting the stealthiness of the attack. As the use of deep\nlearning based diagnostic systems proliferates in clinical practice, we also\nshow how explainability is indispensable in this context, as it can identify\nspatially localized backdoors in inference time.\n", "versions": [{"version": "v1", "created": "Wed, 30 Dec 2020 09:41:19 GMT"}], "update_date": "2021-01-05", "authors_parsed": [["Nwadike", "Munachiso", ""], ["Miyawaki", "Takumi", ""], ["Sarkar", "Esha", ""], ["Maniatakos", "Michail", ""], ["Shamout", "Farah", ""]]}, {"id": "2101.00009", "submitter": "Vasilis Syrgkanis", "authors": "Victor Chernozhukov, Whitney Newey, Rahul Singh, Vasilis Syrgkanis", "title": "Adversarial Estimation of Riesz Representers", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "econ.EM cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We provide an adversarial approach to estimating Riesz representers of linear\nfunctionals within arbitrary function spaces. We prove oracle inequalities\nbased on the localized Rademacher complexity of the function space used to\napproximate the Riesz representer and the approximation error. These\ninequalities imply fast finite sample mean-squared-error rates for many\nfunction spaces of interest, such as high-dimensional sparse linear functions,\nneural networks and reproducing kernel Hilbert spaces. Our approach offers a\nnew way of estimating Riesz representers with a plethora of recently introduced\nmachine learning techniques. We show how our estimator can be used in the\ncontext of de-biasing structural/causal parameters in semi-parametric models,\nfor automated orthogonalization of moment equations and for estimating the\nstochastic discount factor in the context of asset pricing.\n", "versions": [{"version": "v1", "created": "Wed, 30 Dec 2020 19:46:57 GMT"}], "update_date": "2021-01-05", "authors_parsed": [["Chernozhukov", "Victor", ""], ["Newey", "Whitney", ""], ["Singh", "Rahul", ""], ["Syrgkanis", "Vasilis", ""]]}, {"id": "2101.00010", "submitter": "Koustuv Sinha", "authors": "Koustuv Sinha, Prasanna Parthasarathi, Joelle Pineau, Adina Williams", "title": "UnNatural Language Inference", "comments": "Accepted at ACL 2021 (Long Paper), 9 pages + Appendix", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent investigations into the inner-workings of state-of-the-art large-scale\npre-trained Transformer-based Natural Language Understanding (NLU) models\nindicate that they appear to know humanlike syntax, at least to some extent. We\nprovide novel evidence that complicates this claim: we find that\nstate-of-the-art Natural Language Inference (NLI) models assign the same labels\nto permuted examples as they do to the original, i.e. they are largely\ninvariant to random word-order permutations. This behavior notably differs from\nthat of humans; we struggle with ungrammatical sentences. To measure the\nseverity of this issue, we propose a suite of metrics and investigate which\nproperties of particular permutations lead models to be word-order invariant.\nIn the MNLI dataset, for example, we find almost all (98.7%) examples contain\nat least one permutation which elicits the gold label. Models are sometimes\neven able to assign gold labels to permutations that they originally failed to\npredict correctly. We provide a comprehensive empirical evaluation of this\nphenomenon, and further show that this issue exists for both Transformers and\npre-Transformer RNN / ConvNet based encoders, as well as across multiple\nlanguages (English and Mandarin Chinese). Our code and data are available at\nhttps://github.com/facebookresearch/unlu.\n", "versions": [{"version": "v1", "created": "Wed, 30 Dec 2020 20:40:48 GMT"}, {"version": "v2", "created": "Fri, 11 Jun 2021 03:44:22 GMT"}], "update_date": "2021-06-14", "authors_parsed": [["Sinha", "Koustuv", ""], ["Parthasarathi", "Prasanna", ""], ["Pineau", "Joelle", ""], ["Williams", "Adina", ""]]}, {"id": "2101.00029", "submitter": "Maciej Skorski", "authors": "Maciej Skorski", "title": "Random Embeddings with Optimal Accuracy", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.PR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work constructs Jonson-Lindenstrauss embeddings with best accuracy, as\nmeasured by variance, mean-squared error and exponential concentration of the\nlength distortion. Lower bounds for any data and embedding dimensions are\ndetermined, and accompanied by matching and efficiently samplable constructions\n(built on orthogonal matrices). Novel techniques: a unit sphere\nparametrization, the use of singular-value latent variables and Schur-convexity\nare of independent interest.\n", "versions": [{"version": "v1", "created": "Thu, 31 Dec 2020 19:00:31 GMT"}], "update_date": "2021-01-05", "authors_parsed": [["Skorski", "Maciej", ""]]}, {"id": "2101.00035", "submitter": "Kailong Liu", "authors": "Kailong Liu, Xiaosong Hu, Zhongbao Wei, Yi Li, and Yan Jiang", "title": "Modified Gaussian Process Regression Models for Cyclic Capacity\n  Prediction of Lithium-ion Batteries", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.SY eess.SY", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  This paper presents the development of machine learning-enabled data-driven\nmodels for effective capacity predictions for lithium-ion batteries under\ndifferent cyclic conditions. To achieve this, a model structure is first\nproposed with the considerations of battery ageing tendency and the\ncorresponding operational temperature and depth-of-discharge. Then based on a\nsystematic understanding of covariance functions within the Gaussian process\nregression, two related data-driven models are developed. Specifically, by\nmodifying the isotropic squared exponential kernel with an automatic relevance\ndetermination structure, 'Model A' could extract the highly relevant input\nfeatures for capacity predictions. Through coupling the Arrhenius law and a\npolynomial equation into a compositional kernel, 'Model B' is capable of\nconsidering the electrochemical and empirical knowledge of battery degradation.\nThe developed models are validated and compared on the Nickel Manganese Cobalt\nOxide (NMC) lithium-ion batteries with various cycling patterns. Experimental\nresults demonstrate that the modified Gaussian process regression model\nconsidering the battery electrochemical and empirical ageing signature\noutperforms other counterparts and is able to achieve satisfactory results for\nboth one-step and multi-step predictions. The proposed technique is promising\nfor battery capacity predictions under various cycling cases.\n", "versions": [{"version": "v1", "created": "Thu, 31 Dec 2020 19:05:27 GMT"}], "update_date": "2021-01-05", "authors_parsed": [["Liu", "Kailong", ""], ["Hu", "Xiaosong", ""], ["Wei", "Zhongbao", ""], ["Li", "Yi", ""], ["Jiang", "Yan", ""]]}, {"id": "2101.00041", "submitter": "Philippe Casgrain", "authors": "Philippe Casgrain, Anastasis Kratsios", "title": "Optimizing Optimizers: Regret-optimal gradient descent algorithms", "comments": "12 pages body, 42 pages total, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.OC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The need for fast and robust optimization algorithms are of critical\nimportance in all areas of machine learning. This paper treats the task of\ndesigning optimization algorithms as an optimal control problem. Using regret\nas a metric for an algorithm's performance, we study the existence, uniqueness\nand consistency of regret-optimal algorithms. By providing first-order\noptimality conditions for the control problem, we show that regret-optimal\nalgorithms must satisfy a specific structure in their dynamics which we show is\nequivalent to performing dual-preconditioned gradient descent on the value\nfunction generated by its regret. Using these optimal dynamics, we provide\nbounds on their rates of convergence to solutions of convex optimization\nproblems. Though closed-form optimal dynamics cannot be obtained in general, we\npresent fast numerical methods for approximating them, generating optimization\nalgorithms which directly optimize their long-term regret. Lastly, these are\nbenchmarked against commonly used optimization algorithms to demonstrate their\neffectiveness.\n", "versions": [{"version": "v1", "created": "Thu, 31 Dec 2020 19:13:53 GMT"}, {"version": "v2", "created": "Tue, 19 Jan 2021 22:50:56 GMT"}], "update_date": "2021-01-21", "authors_parsed": [["Casgrain", "Philippe", ""], ["Kratsios", "Anastasis", ""]]}, {"id": "2101.00049", "submitter": "Suresh Kondati Natarajan PhD", "authors": "Suresh Kondati Natarajan and Miguel A. Caro", "title": "Particle Swarm Based Hyper-Parameter Optimization for Machine Learned\n  Interatomic Potentials", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cond-mat.mtrl-sci cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Modeling non-empirical and highly flexible interatomic potential energy\nsurfaces (PES) using machine learning (ML) approaches is becoming popular in\nmolecular and materials research. Training an ML-PES is typically performed in\ntwo stages: feature extraction and structure-property relationship modeling.\nThe feature extraction stage transforms atomic positions into a\nsymmetry-invariant mathematical representation. This representation can be\nfine-tuned by adjusting on a set of so-called \"hyper-parameters\" (HPs).\nSubsequently, an ML algorithm such as neural networks or Gaussian process\nregression (GPR) is used to model the structure-PES relationship based on\nanother set of HPs. Choosing optimal values for the two sets of HPs is critical\nto ensure the high quality of the resulting ML-PES model.\n  In this paper, we explore HP optimization strategies tailored for ML-PES\ngeneration using a custom-coded parallel particle swarm optimizer (available\nfreely at https://github.com/suresh0807/PPSO.git). We employ the smooth overlap\nof atomic positions (SOAP) descriptor in combination with GPR-based Gaussian\napproximation potentials (GAP) and optimize HPs for four distinct systems: a\ntoy C dimer, amorphous carbon, $\\alpha$-Fe, and small organic molecules (QM9\ndataset). We propose a two-step optimization strategy in which the HPs related\nto the feature extraction stage are optimized first, followed by the\noptimization of the HPs in the training stage. This strategy is computationally\nmore efficient than optimizing all HPs at the same time by means of\nsignificantly reducing the number of ML models needed to be trained to obtain\nthe optimal HPs. This approach can be trivially extended to other combinations\nof descriptor and ML algorithm and brings us another step closer to fully\nautomated ML-PES generation.\n", "versions": [{"version": "v1", "created": "Thu, 31 Dec 2020 19:27:17 GMT"}], "update_date": "2021-01-05", "authors_parsed": [["Natarajan", "Suresh Kondati", ""], ["Caro", "Miguel A.", ""]]}, {"id": "2101.00052", "submitter": "Guannan Liang", "authors": "Qianqian Tong, Guannan Liang, Tan Zhu and Jinbo Bi", "title": "Federated Nonconvex Sparse Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Nonconvex sparse learning plays an essential role in many areas, such as\nsignal processing and deep network compression. Iterative hard thresholding\n(IHT) methods are the state-of-the-art for nonconvex sparse learning due to\ntheir capability of recovering true support and scalability with large\ndatasets. Theoretical analysis of IHT is currently based on centralized IID\ndata. In realistic large-scale situations, however, data are distributed,\nhardly IID, and private to local edge computing devices. It is thus necessary\nto examine the property of IHT in federated settings, which update in parallel\non local devices and communicate with a central server only once in a while\nwithout sharing local data.\n  In this paper, we propose two IHT methods: Federated Hard Thresholding\n(Fed-HT) and Federated Iterative Hard Thresholding (FedIter-HT). We prove that\nboth algorithms enjoy a linear convergence rate and have strong guarantees to\nrecover the optimal sparse estimator, similar to traditional IHT methods, but\nnow with decentralized non-IID data. Empirical results demonstrate that the\nFed-HT and FedIter-HT outperform their competitor - a distributed IHT, in terms\nof decreasing the objective values with lower requirements on communication\nrounds and bandwidth.\n", "versions": [{"version": "v1", "created": "Thu, 31 Dec 2020 19:43:45 GMT"}], "update_date": "2021-01-05", "authors_parsed": [["Tong", "Qianqian", ""], ["Liang", "Guannan", ""], ["Zhu", "Tan", ""], ["Bi", "Jinbo", ""]]}, {"id": "2101.00054", "submitter": "Minje Kim", "authors": "Kai Zhen, Mi Suk Lee, Jongmo Sung, Seungkwon Beack, Minje Kim", "title": "Psychoacoustic Calibration of Loss Functions for Efficient End-to-End\n  Neural Audio Coding", "comments": null, "journal-ref": "IEEE Signal Processing Letters, vol. 27, pp. 2159-2163, 2020", "doi": "10.1109/LSP.2020.3039765", "report-no": null, "categories": "cs.SD cs.LG eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Conventional audio coding technologies commonly leverage human perception of\nsound, or psychoacoustics, to reduce the bitrate while preserving the\nperceptual quality of the decoded audio signals. For neural audio codecs,\nhowever, the objective nature of the loss function usually leads to suboptimal\nsound quality as well as high run-time complexity due to the large model size.\nIn this work, we present a psychoacoustic calibration scheme to re-define the\nloss functions of neural audio coding systems so that it can decode signals\nmore perceptually similar to the reference, yet with a much lower model\ncomplexity. The proposed loss function incorporates the global masking\nthreshold, allowing the reconstruction error that corresponds to inaudible\nartifacts. Experimental results show that the proposed model outperforms the\nbaseline neural codec twice as large and consuming 23.4% more bits per second.\nWith the proposed method, a lightweight neural codec, with only 0.9 million\nparameters, performs near-transparent audio coding comparable with the\ncommercial MPEG-1 Audio Layer III codec at 112 kbps.\n", "versions": [{"version": "v1", "created": "Thu, 31 Dec 2020 19:46:46 GMT"}], "update_date": "2021-01-05", "authors_parsed": [["Zhen", "Kai", ""], ["Lee", "Mi Suk", ""], ["Sung", "Jongmo", ""], ["Beack", "Seungkwon", ""], ["Kim", "Minje", ""]]}, {"id": "2101.00072", "submitter": "Qianli Liao", "authors": "Tomaso Poggio and Qianli Liao", "title": "Explicit regularization and implicit bias in deep network classifiers\n  trained with the square loss", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Deep ReLU networks trained with the square loss have been observed to perform\nwell in classification tasks. We provide here a theoretical justification based\non analysis of the associated gradient flow. We show that convergence to a\nsolution with the absolute minimum norm is expected when normalization\ntechniques such as Batch Normalization (BN) or Weight Normalization (WN) are\nused together with Weight Decay (WD). The main property of the minimizers that\nbounds their expected error is the norm: we prove that among all the\nclose-to-interpolating solutions, the ones associated with smaller Frobenius\nnorms of the unnormalized weight matrices have better margin and better bounds\non the expected classification error. With BN but in the absence of WD, the\ndynamical system is singular. Implicit dynamical regularization -- that is\nzero-initial conditions biasing the dynamics towards high margin solutions --\nis also possible in the no-BN and no-WD case. The theory yields several\npredictions, including the role of BN and weight decay, aspects of Papyan, Han\nand Donoho's Neural Collapse and the constraints induced by BN on the network\nweights.\n", "versions": [{"version": "v1", "created": "Thu, 31 Dec 2020 21:07:56 GMT"}], "update_date": "2021-01-05", "authors_parsed": [["Poggio", "Tomaso", ""], ["Liao", "Qianli", ""]]}, {"id": "2101.00074", "submitter": "Shiv Shankar", "authors": "Shiv Shankar, Daniel Sheldon, Tao Sun, John Pickering, and Thomas G.\n  Dietterich", "title": "Three-quarter Sibling Regression for Denoising Observational Data", "comments": null, "journal-ref": "IJCAI 2019", "doi": null, "report-no": null, "categories": "stat.ME cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many ecological studies and conservation policies are based on field\nobservations of species, which can be affected by systematic variability\nintroduced by the observation process. A recently introduced causal modeling\ntechnique called 'half-sibling regression' can detect and correct for\nsystematic errors in measurements of multiple independent random variables.\nHowever, it will remove intrinsic variability if the variables are dependent,\nand therefore does not apply to many situations, including modeling of species\ncounts that are controlled by common causes. We present a technique called\n'three-quarter sibling regression' to partially overcome this limitation. It\ncan filter the effect of systematic noise when the latent variables have\nobserved common causes. We provide theoretical justification of this approach,\ndemonstrate its effectiveness on synthetic data, and show that it reduces\nsystematic detection variability due to moon brightness in moth surveys.\n", "versions": [{"version": "v1", "created": "Thu, 31 Dec 2020 21:18:01 GMT"}], "update_date": "2021-01-05", "authors_parsed": [["Shankar", "Shiv", ""], ["Sheldon", "Daniel", ""], ["Sun", "Tao", ""], ["Pickering", "John", ""], ["Dietterich", "Thomas G.", ""]]}, {"id": "2101.00079", "submitter": "Kimberly Stachenfeld", "authors": "Kimberly Stachenfeld, Jonathan Godwin, Peter Battaglia", "title": "Graph Networks with Spectral Message Passing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Graph Neural Networks (GNNs) are the subject of intense focus by the machine\nlearning community for problems involving relational reasoning. GNNs can be\nbroadly divided into spatial and spectral approaches. Spatial approaches use a\nform of learned message-passing, in which interactions among vertices are\ncomputed locally, and information propagates over longer distances on the graph\nwith greater numbers of message-passing steps. Spectral approaches use\neigendecompositions of the graph Laplacian to produce a generalization of\nspatial convolutions to graph structured data which access information over\nshort and long time scales simultaneously. Here we introduce the Spectral Graph\nNetwork, which applies message passing to both the spatial and spectral\ndomains. Our model projects vertices of the spatial graph onto the Laplacian\neigenvectors, which are each represented as vertices in a fully connected\n\"spectral graph\", and then applies learned message passing to them. We apply\nthis model to various benchmark tasks including a graph-based variant of MNIST\nclassification, molecular property prediction on MoleculeNet and QM9, and\nshortest path problems on random graphs. Our results show that the Spectral GN\npromotes efficient training, reaching high performance with fewer training\niterations despite having more parameters. The model also provides robustness\nto edge dropout and outperforms baselines for the classification tasks. We also\nexplore how these performance benefits depend on properties of the dataset.\n", "versions": [{"version": "v1", "created": "Thu, 31 Dec 2020 21:33:17 GMT"}], "update_date": "2021-01-05", "authors_parsed": [["Stachenfeld", "Kimberly", ""], ["Godwin", "Jonathan", ""], ["Battaglia", "Peter", ""]]}, {"id": "2101.00082", "submitter": "Shiv Shankar", "authors": "Shiv Shankar, Don Towsley", "title": "Bosonic Random Walk Networks for Graph Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The development of Graph Neural Networks (GNNs) has led to great progress in\nmachine learning on graph-structured data. These networks operate via diffusing\ninformation across the graph nodes while capturing the structure of the graph.\nRecently there has also seen tremendous progress in quantum computing\ntechniques. In this work, we explore applications of multi-particle quantum\nwalks on diffusing information across graphs. Our model is based on learning\nthe operators that govern the dynamics of quantum random walkers on graphs. We\ndemonstrate the effectiveness of our method on classification and regression\ntasks.\n", "versions": [{"version": "v1", "created": "Thu, 31 Dec 2020 21:40:40 GMT"}], "update_date": "2021-01-05", "authors_parsed": [["Shankar", "Shiv", ""], ["Towsley", "Don", ""]]}, {"id": "2101.00122", "submitter": "Xiulong Yang", "authors": "Xiulong Yang, Hui Ye, Yang Ye, Xiang Li, Shihao Ji", "title": "Generative Max-Mahalanobis Classifiers for Image Classification,\n  Generation and More", "comments": "Accepted as a conference paper at ECML2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Joint Energy-based Model (JEM) of Grathwohl et al. shows that a standard\nsoftmax classifier can be reinterpreted as an energy-based model (EBM) for the\njoint distribution p(x,y); the resulting model can be optimized to improve\ncalibration, robustness, and out-of-distribution detection, while generating\nsamples rivaling the quality of recent GAN-based approaches. However, the\nsoftmax classifier that JEM exploits is inherently discriminative and its\nlatent feature space is not well formulated as probabilistic distributions,\nwhich may hinder its potential for image generation and incur training\ninstability. We hypothesize that generative classifiers, such as Linear\nDiscriminant Analysis (LDA), might be more suitable for image generation since\ngenerative classifiers model the data generation process explicitly. This paper\ntherefore investigates an LDA classifier for image classification and\ngeneration. In particular, the Max-Mahalanobis Classifier (MMC), a special case\nof LDA, fits our goal very well. We show that our Generative MMC (GMMC) can be\ntrained discriminatively, generatively, or jointly for image classification and\ngeneration. Extensive experiments on multiple datasets show that GMMC achieves\nstate-of-the-art discriminative and generative performances, while\noutperforming JEM in calibration, adversarial robustness, and\nout-of-distribution detection by a significant margin. Our source code is\navailable at https://github.com/sndnyang/GMMC.\n", "versions": [{"version": "v1", "created": "Fri, 1 Jan 2021 00:42:04 GMT"}, {"version": "v2", "created": "Thu, 25 Feb 2021 13:35:35 GMT"}, {"version": "v3", "created": "Fri, 2 Apr 2021 22:30:49 GMT"}, {"version": "v4", "created": "Thu, 1 Jul 2021 21:29:26 GMT"}], "update_date": "2021-07-05", "authors_parsed": [["Yang", "Xiulong", ""], ["Ye", "Hui", ""], ["Ye", "Yang", ""], ["Li", "Xiang", ""], ["Ji", "Shihao", ""]]}, {"id": "2101.00124", "submitter": "I-Hung Hsu", "authors": "I-Hung Hsu, Xiao Guo, Wael AbdAlmageed, Premkumar Natarajan, Nanyun\n  Peng", "title": "MrGCN: Mirror Graph Convolution Network for Relation Extraction with\n  Long-Term Dependencies", "comments": "13 pages, page 11-13 appendix, 7 figures. The first two authors\n  contribute equally", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The ability to capture complex linguistic structures and long-term\ndependencies among words in the passage is essential for relation extraction\n(RE) tasks. Graph neural networks (GNNs), one of the means to encode dependency\ngraphs, have been shown to be effective in prior works. However, relatively\nlittle attention has been paid to receptive fields of GNNs, which can be\ncrucial for tasks with extremely long text that requires discourse\nunderstanding. In this work, we leverage the idea of graph pooling and propose\nthe Mirror Graph Convolution Network, a GNN model with a pooling-unpooling\nstructure tailored to RE tasks. The pooling branch reduces the graph size and\nenables the GNN to obtain larger receptive fields within fewer layers; the\nunpooling branch restores the pooled graph to its original resolution for\ntoken-level RE tasks. Experiments on two discourse-level relation extraction\ndatasets demonstrate the effectiveness of our method, showing significant\nimprovements over prior methods especially when modeling long-term dependencies\nis necessary. Moreover, we propose Clause Matching (CM), a novel graph pooling\nmethod that merges nodes based on dependency relations in graph. CM can largely\nreduce the graph size while retaining the main semantics of the input text.\n", "versions": [{"version": "v1", "created": "Fri, 1 Jan 2021 00:52:53 GMT"}, {"version": "v2", "created": "Sat, 15 May 2021 07:33:34 GMT"}], "update_date": "2021-05-18", "authors_parsed": [["Hsu", "I-Hung", ""], ["Guo", "Xiao", ""], ["AbdAlmageed", "Wael", ""], ["Natarajan", "Premkumar", ""], ["Peng", "Nanyun", ""]]}, {"id": "2101.00135", "submitter": "Amin Nikanjam", "authors": "Amin Nikanjam, Mohammad Mehdi Morovati, Foutse Khomh, Houssem Ben\n  Braiek", "title": "Faults in Deep Reinforcement Learning Programs: A Taxonomy and A\n  Detection Approach", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SE cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A growing demand is witnessed in both industry and academia for employing\nDeep Learning (DL) in various domains to solve real-world problems. Deep\nReinforcement Learning (DRL) is the application of DL in the domain of\nReinforcement Learning (RL). Like any software systems, DRL applications can\nfail because of faults in their programs. In this paper, we present the first\nattempt to categorize faults occurring in DRL programs. We manually analyzed\n761 artifacts of DRL programs (from Stack Overflow posts and GitHub issues)\ndeveloped using well-known DRL frameworks (OpenAI Gym, Dopamine, Keras-rl,\nTensorforce) and identified faults reported by developers/users. We labeled and\ntaxonomized the identified faults through several rounds of discussions. The\nresulting taxonomy is validated using an online survey with 19\ndevelopers/researchers. To allow for the automatic detection of faults in DRL\nprograms, we have defined a meta-model of DRL programs and developed DRLinter,\na model-based fault detection approach that leverages static analysis and graph\ntransformations. The execution flow of DRLinter consists in parsing a DRL\nprogram to generate a model conforming to our meta-model and applying detection\nrules on the model to identify faults occurrences. The effectiveness of\nDRLinter is evaluated using 15 synthetic DRLprograms in which we injected\nfaults observed in the analyzed artifacts of the taxonomy. The results show\nthat DRLinter can successfully detect faults in all synthetic faulty programs.\n", "versions": [{"version": "v1", "created": "Fri, 1 Jan 2021 01:49:03 GMT"}, {"version": "v2", "created": "Tue, 5 Jan 2021 21:39:04 GMT"}], "update_date": "2021-07-08", "authors_parsed": [["Nikanjam", "Amin", ""], ["Morovati", "Mohammad Mehdi", ""], ["Khomh", "Foutse", ""], ["Braiek", "Houssem Ben", ""]]}, {"id": "2101.00150", "submitter": "Pablo Navarrete Michelini", "authors": "Pablo Navarrete Michelini, Wenbin Chen, Hanwen Liu, Dan Zhu, Xingqun\n  Jiang", "title": "Multi-Grid Back-Projection Networks", "comments": "Accepted for publication in IEEE Journal of Selected Topics in Signal\n  Processing (J-STSP). arXiv admin note: text overlap with arXiv:1809.10711", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Multi-Grid Back-Projection (MGBP) is a fully-convolutional network\narchitecture that can learn to restore images and videos with upscaling\nartifacts. Using the same strategy of multi-grid partial differential equation\n(PDE) solvers this multiscale architecture scales computational complexity\nefficiently with increasing output resolutions. The basic processing block is\ninspired in the iterative back-projection (IBP) algorithm and constitutes a\ntype of cross-scale residual block with feedback from low resolution\nreferences. The architecture performs in par with state-of-the-arts\nalternatives for regression targets that aim to recover an exact copy of a high\nresolution image or video from which only a downscale image is known. A\nperceptual quality target aims to create more realistic outputs by introducing\nartificial changes that can be different from a high resolution original\ncontent as long as they are consistent with the low resolution input. For this\ntarget we propose a strategy using noise inputs in different resolution scales\nto control the amount of artificial details generated in the output. The noise\ninput controls the amount of innovation that the network uses to create\nartificial realistic details. The effectiveness of this strategy is shown in\nbenchmarks and it is explained as a particular strategy to traverse the\nperception-distortion plane.\n", "versions": [{"version": "v1", "created": "Fri, 1 Jan 2021 03:17:34 GMT"}], "update_date": "2021-01-05", "authors_parsed": [["Michelini", "Pablo Navarrete", ""], ["Chen", "Wenbin", ""], ["Liu", "Hanwen", ""], ["Zhu", "Dan", ""], ["Jiang", "Xingqun", ""]]}, {"id": "2101.00151", "submitter": "Hung Le", "authors": "Hung Le and Chinnadhurai Sankar and Seungwhan Moon and Ahmad Beirami\n  and Alborz Geramifard and Satwik Kottur", "title": "DVD: A Diagnostic Dataset for Multi-step Reasoning in Video Grounded\n  Dialogue", "comments": "20 pages, 14 figures, 8 tables", "journal-ref": "Association for Computational Linguistics (2021)", "doi": null, "report-no": null, "categories": "cs.AI cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A video-grounded dialogue system is required to understand both dialogue,\nwhich contains semantic dependencies from turn to turn, and video, which\ncontains visual cues of spatial and temporal scene variations. Building such\ndialogue systems is a challenging problem, involving various reasoning types on\nboth visual and language inputs. Existing benchmarks do not have enough\nannotations to thoroughly analyze dialogue systems and understand their\ncapabilities and limitations in isolation. These benchmarks are also not\nexplicitly designed to minimise biases that models can exploit without actual\nreasoning. To address these limitations, in this paper, we present DVD, a\nDiagnostic Dataset for Video-grounded Dialogues. The dataset is designed to\ncontain minimal biases and has detailed annotations for the different types of\nreasoning over the spatio-temporal space of video. Dialogues are synthesized\nover multiple question turns, each of which is injected with a set of\ncross-turn semantic relationships. We use DVD to analyze existing approaches,\nproviding interesting insights into their abilities and limitations. In total,\nDVD is built from $11k$ CATER synthetic videos and contains $10$ instances of\n$10$-round dialogues for each video, resulting in more than $100k$ dialogues\nand $1M$ question-answer pairs. Our code and dataset are publicly available at\nhttps://github.com/facebookresearch/DVDialogues.\n", "versions": [{"version": "v1", "created": "Fri, 1 Jan 2021 03:20:22 GMT"}, {"version": "v2", "created": "Mon, 14 Jun 2021 15:55:57 GMT"}], "update_date": "2021-06-15", "authors_parsed": [["Le", "Hung", ""], ["Sankar", "Chinnadhurai", ""], ["Moon", "Seungwhan", ""], ["Beirami", "Ahmad", ""], ["Geramifard", "Alborz", ""], ["Kottur", "Satwik", ""]]}, {"id": "2101.00157", "submitter": "Jing Lin", "authors": "Jing Lin, Ryan Luley, and Kaiqi Xiong", "title": "Active Learning Under Malicious Mislabeling and Poisoning Attacks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Deep neural networks usually require large labeled datasets for training to\nachieve the start-of-the-art performance in many tasks, such as image\nclassification and natural language processing. Though a lot of data is created\neach day by active Internet users through various distributed systems across\nthe world, most of these data are unlabeled and are vulnerable to data\npoisoning attacks. In this paper, we develop an efficient active learning\nmethod that requires fewer labeled instances and incorporates the technique of\nadversarial retraining in which additional labeled artificial data are\ngenerated without increasing the labeling budget. The generated adversarial\nexamples also provide a way to measure the vulnerability of the model. To check\nthe performance of the proposed method under an adversarial setting, i.e.,\nmalicious mislabeling and data poisoning attacks, we perform an extensive\nevaluation on the reduced CIFAR-10 dataset, which contains only two classes:\n'airplane' and 'frog' by using the private cloud on campus. Our experimental\nresults demonstrate that the proposed active learning method is efficient for\ndefending against malicious mislabeling and data poisoning attacks.\nSpecifically, whereas the baseline active learning method based on the random\nsampling strategy performs poorly (about 50%) under a malicious mislabeling\nattack, the proposed active learning method can achieve the desired accuracy of\n89% using only one-third of the dataset on average.\n", "versions": [{"version": "v1", "created": "Fri, 1 Jan 2021 03:43:36 GMT"}, {"version": "v2", "created": "Wed, 24 Mar 2021 01:07:29 GMT"}, {"version": "v3", "created": "Sun, 16 May 2021 20:06:13 GMT"}], "update_date": "2021-05-18", "authors_parsed": [["Lin", "Jing", ""], ["Luley", "Ryan", ""], ["Xiong", "Kaiqi", ""]]}, {"id": "2101.00159", "submitter": "Zaid Al-Ars", "authors": "David Enthoven and Zaid Al-Ars", "title": "Fidel: Reconstructing Private Training Samples from Weight Updates in\n  Federated Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  With the increasing number of data collectors such as smartphones, immense\namounts of data are available. Federated learning was developed to allow for\ndistributed learning on a massive scale whilst still protecting each users'\nprivacy. This privacy is claimed by the notion that the centralized server does\nnot have any access to a client's data, solely the client's model update. In\nthis paper, we evaluate a novel attack method within regular federated learning\nwhich we name the First Dense Layer Attack (Fidel). The methodology of using\nthis attack is discussed, and as a proof of viability we show how this attack\nmethod can be used to great effect for densely connected networks and\nconvolutional neural networks. We evaluate some key design decisions and show\nthat the usage of ReLu and Dropout are detrimental to the privacy of a client's\nlocal dataset. We show how to recover on average twenty out of thirty private\ndata samples from a client's model update employing a fully connected neural\nnetwork with very little computational resources required. Similarly, we show\nthat over thirteen out of twenty samples can be recovered from a convolutional\nneural network update.\n", "versions": [{"version": "v1", "created": "Fri, 1 Jan 2021 04:00:23 GMT"}], "update_date": "2021-01-05", "authors_parsed": [["Enthoven", "David", ""], ["Al-Ars", "Zaid", ""]]}, {"id": "2101.00165", "submitter": "Bahareh Nakisa", "authors": "Mohammad Naim Rastgoo, Bahareh Nakisa, Andry Rakotonirainy, Frederic\n  Maire, Vinod Chandran", "title": "ECG-Based Driver Stress Levels Detection System Using Hyperparameter\n  Optimization", "comments": "17 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SP cs.LG cs.NE", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Stress and driving are a dangerous combination which can lead to crashes, as\nevidenced by the large number of road traffic crashes that involve stress.\nMotivated by the need to address the significant costs of driver stress, it is\nessential to build a practical system that can classify driver stress level\nwith high accuracy. However, the performance of an accurate driving stress\nlevels classification system depends on hyperparameter optimization choices\nsuch as data segmentation (windowing hyperparameters). The configuration\nsetting of hyperparameters, which has an enormous impact on the system\nperformance, are typically hand-tuned while evaluating the algorithm. This\ntuning process is time consuming and often depends on personal experience.\nThere are also no generic optimal values for hyperparameters values. In this\nwork, we propose a meta-heuristic approach to support automated hyperparameter\noptimization and provide a real-time driver stress detection system. This is\nthe first systematic study of optimizing windowing hyperparameters based on\nElectrocardiogram (ECG) signal in the domain of driving safety. Our approach is\nto propose a framework based on Particle Swarm Optimization algorithm (PSO) to\nselect an optimal/near optimal windowing hyperparameters values. The\nperformance of the proposed framework is evaluated on two datasets: a public\ndataset (DRIVEDB dataset) and our collected dataset using an advanced\nsimulator. DRIVEDB dataset was collected in a real time driving scenario, and\nour dataset was collected using an advanced driving simulator in the control\nenvironment. We demonstrate that optimising the windowing hyperparameters\nyields significant improvement in terms of accuracy. The most accurate built\nmodel applied to the public dataset and our dataset, based on the selected\nwindowing hyperparameters, achieved 92.12% and 77.78% accuracy, respectively.\n", "versions": [{"version": "v1", "created": "Fri, 1 Jan 2021 05:18:46 GMT"}], "update_date": "2021-01-05", "authors_parsed": [["Rastgoo", "Mohammad Naim", ""], ["Nakisa", "Bahareh", ""], ["Rakotonirainy", "Andry", ""], ["Maire", "Frederic", ""], ["Chandran", "Vinod", ""]]}, {"id": "2101.00169", "submitter": "Daniel Szelogowski", "authors": "Daniel Szelogowski", "title": "Generative Deep Learning for Virtuosic Classical Music: Generative\n  Adversarial Networks as Renowned Composers", "comments": "13 pages, 6 figures Update: Revised format to align closer to IEEE\n  standards", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.LG cs.NE eess.AS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Current AI-generated music lacks fundamental principles of good compositional\ntechniques. By narrowing down implementation issues both programmatically and\nmusically, we can create a better understanding of what parameters are\nnecessary for a generated composition nearly indistinguishable from that of a\nmaster composer.\n", "versions": [{"version": "v1", "created": "Fri, 1 Jan 2021 05:40:12 GMT"}, {"version": "v2", "created": "Mon, 19 Apr 2021 22:40:56 GMT"}], "update_date": "2021-04-21", "authors_parsed": [["Szelogowski", "Daniel", ""]]}, {"id": "2101.00183", "submitter": "Md. Touhidul Islam", "authors": "Md. Touhidul Islam, Sanjida Reza Rafa, Md. Golam Kibria", "title": "Early Prediction of Heart Disease Using PCA and Hybrid Genetic Algorithm\n  with k-Means", "comments": "6 pages, 9 figures, Presented in the Proceedings of the 23rd\n  International Conference on Computer and Information Technology (ICCIT),\n  19-21 December, 2020, Dhaka, Bangladesh", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Worldwide research shows that millions of lives lost per year because of\nheart disease. The healthcare sector produces massive volumes of data on heart\ndisease that are sadly not used to locate secret knowledge for successful\ndecision making. One of the most important aspects at this moment is detecting\nheart disease at an early stage. Researchers have applied distinct techniques\nto the UCI Machine Learning heart disease dataset. Many researchers have tried\nto apply some complex techniques to this dataset, where detailed studies are\nstill missing. In this paper, Principal Component Analysis (PCA) has been used\nto reduce attributes. Apart from a Hybrid genetic algorithm (HGA) with k-means\nused for final clustering. Typically, the k-means method is using for\nclustering the data. This type of clustering can get stuck in the local optima\nbecause this method is heuristic. We used the Hybrid Genetic Algorithm (HGA)\nfor data clustering to avoid this problem. Our proposed method can predict\nearly heart disease with an accuracy of 94.06%.\n", "versions": [{"version": "v1", "created": "Fri, 1 Jan 2021 07:14:38 GMT"}], "update_date": "2021-01-05", "authors_parsed": [["Islam", "Md. Touhidul", ""], ["Rafa", "Sanjida Reza", ""], ["Kibria", "Md. Golam", ""]]}, {"id": "2101.00186", "submitter": "Tianyu Wang", "authors": "Tianyu Wang, Vikas Dhiman, Nikolay Atanasov", "title": "Inverse reinforcement learning for autonomous navigation via\n  differentiable semantic mapping and planning", "comments": "16 pages, 12 figures. arXiv admin note: text overlap with\n  arXiv:2006.05043", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.RO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper focuses on inverse reinforcement learning for autonomous\nnavigation using distance and semantic category observations. The objective is\nto infer a cost function that explains demonstrated behavior while relying only\non the expert's observations and state-control trajectory. We develop a map\nencoder, that infers semantic category probabilities from the observation\nsequence, and a cost encoder, defined as a deep neural network over the\nsemantic features. Since the expert cost is not directly observable, the model\nparameters can only be optimized by differentiating the error between\ndemonstrated controls and a control policy computed from the cost estimate. We\npropose a new model of expert behavior that enables error minimization using a\nclosed-form subgradient computed only over a subset of promising states via a\nmotion planning algorithm. Our approach allows generalizing the learned\nbehavior to new environments with new spatial configurations of the semantic\ncategories. We analyze the different components of our model in a minigrid\nenvironment. We also demonstrate that our approach learns to follow traffic\nrules in the autonomous driving CARLA simulator by relying on semantic\nobservations of buildings, sidewalks, and road lanes.\n", "versions": [{"version": "v1", "created": "Fri, 1 Jan 2021 07:41:08 GMT"}], "update_date": "2021-01-05", "authors_parsed": [["Wang", "Tianyu", ""], ["Dhiman", "Vikas", ""], ["Atanasov", "Nikolay", ""]]}, {"id": "2101.00191", "submitter": "Yuris Mulya Saputra", "authors": "Yuris Mulya Saputra, Dinh Thai Hoang, Diep N. Nguyen, Le-Nam Tran,\n  Shimin Gong, and Eryk Dutkiewicz", "title": "Dynamic Federated Learning-Based Economic Framework for\n  Internet-of-Vehicles", "comments": "18 pages, 12 figures, submitted to an IEEE journal", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Federated learning (FL) can empower Internet-of-Vehicles (IoV) networks by\nleveraging smart vehicles (SVs) to participate in the learning process with\nminimum data exchanges and privacy disclosure. The collected data and learned\nknowledge can help the vehicular service provider (VSP) improve the global\nmodel accuracy, e.g., for road safety as well as better profits for both VSP\nand participating SVs. Nonetheless, there exist major challenges when\nimplementing the FL in IoV networks, such as dynamic activities and diverse\nquality-of-information (QoI) from a large number of SVs, VSP's limited payment\nbudget, and profit competition among SVs. In this paper, we propose a novel\ndynamic FL-based economic framework for an IoV network to address these\nchallenges. Specifically, the VSP first implements an SV selection method to\ndetermine a set of the best SVs for the FL process according to the\nsignificance of their current locations and information history at each\nlearning round. Then, each selected SV can collect on-road information and\noffer a payment contract to the VSP based on its collected QoI. For that, we\ndevelop a multi-principal one-agent contract-based policy to maximize the\nprofits of the VSP and learning SVs under the VSP's limited payment budget and\nasymmetric information between the VSP and SVs. Through experimental results\nusing real-world on-road datasets, we show that our framework can converge 57%\nfaster (even with only 10% of active SVs in the network) and obtain much higher\nsocial welfare of the network (up to 27.2 times) compared with those of other\nbaseline FL methods.\n", "versions": [{"version": "v1", "created": "Fri, 1 Jan 2021 08:11:18 GMT"}, {"version": "v2", "created": "Thu, 11 Mar 2021 06:32:58 GMT"}], "update_date": "2021-03-12", "authors_parsed": [["Saputra", "Yuris Mulya", ""], ["Hoang", "Dinh Thai", ""], ["Nguyen", "Diep N.", ""], ["Tran", "Le-Nam", ""], ["Gong", "Shimin", ""], ["Dutkiewicz", "Eryk", ""]]}, {"id": "2101.00195", "submitter": "Hrithika Dodia", "authors": "Hrithika Dodia", "title": "Detecting residues of cosmic events using residual neural network", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "astro-ph.IM astro-ph.CO cs.LG gr-qc", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The detection of gravitational waves is considered to be one of the most\nmagnificent discoveries of the century. Due to the high computational cost of\nmatched filtering pipeline, there is a hunt for an alternative powerful system.\nI present, for the first time, the use of 1D residual neural network for\ndetection of gravitational waves. Residual networks have transformed many\nfields like image classification, face recognition and object detection with\ntheir robust structure. With increase in sensitivity of LIGO detectors we\nexpect many more sources of gravitational waves in the universe to be detected.\nHowever, deep learning networks are trained only once. When used for\nclassification task, deep neural networks are trained to predict only a fixed\nnumber of classes. Therefore, when a new type of gravitational wave is to be\ndetected, this turns out to be a drawback of deep learning. Shallow neural\nnetworks can be used to learn data with simple patterns but fail to give good\nresults with increase in complexity of data. Remodelling the neural network\nwith detection of each new type of GW is highly infeasible. In this letter, I\nalso discuss ways to reduce the time required to adapt to such changes in\ndetection of gravitational waves for deep learning methods. Primarily, I aim to\ncreate a custom residual neural network for 1-dimensional time series inputs,\nwhich can learn a ton of features from dataset without giving up on increasing\nthe number of classes or increasing the complexity of data. I use the two class\nof binary coalescence signals (Binary Black Hole Merger and Binary Neutron Star\nMerger signals) detected by LIGO to check the performance of residual structure\non gravitational waves detection.\n", "versions": [{"version": "v1", "created": "Fri, 1 Jan 2021 08:44:58 GMT"}], "update_date": "2021-01-05", "authors_parsed": [["Dodia", "Hrithika", ""]]}, {"id": "2101.00203", "submitter": "Anish Madan", "authors": "Anish Madan, Ranjitha Prasad", "title": "B-SMALL: A Bayesian Neural Network approach to Sparse Model-Agnostic\n  Meta-Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There is a growing interest in the learning-to-learn paradigm, also known as\nmeta-learning, where models infer on new tasks using a few training examples.\nRecently, meta-learning based methods have been widely used in few-shot\nclassification, regression, reinforcement learning, and domain adaptation. The\nmodel-agnostic meta-learning (MAML) algorithm is a well-known algorithm that\nobtains model parameter initialization at meta-training phase. In the meta-test\nphase, this initialization is rapidly adapted to new tasks by using gradient\ndescent. However, meta-learning models are prone to overfitting since there are\ninsufficient training tasks resulting in over-parameterized models with poor\ngeneralization performance for unseen tasks. In this paper, we propose a\nBayesian neural network based MAML algorithm, which we refer to as the B-SMALL\nalgorithm. The proposed framework incorporates a sparse variational loss term\nalongside the loss function of MAML, which uses a sparsifying approximated KL\ndivergence as a regularizer. We demonstrate the performance of B-MAML using\nclassification and regression tasks, and highlight that training a sparsifying\nBNN using MAML indeed improves the parameter footprint of the model while\nperforming at par or even outperforming the MAML approach. We also illustrate\napplicability of our approach in distributed sensor networks, where sparsity\nand meta-learning can be beneficial.\n", "versions": [{"version": "v1", "created": "Fri, 1 Jan 2021 09:19:48 GMT"}], "update_date": "2021-01-05", "authors_parsed": [["Madan", "Anish", ""], ["Prasad", "Ranjitha", ""]]}, {"id": "2101.00205", "submitter": "Dawei Li", "authors": "Dawei Li and Ruoyu Sun", "title": "On a Faster $R$-Linear Convergence Rate of the Barzilai-Borwein Method", "comments": "11 pages. Corrected typos", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Barzilai-Borwein (BB) method has demonstrated great empirical success in\nnonlinear optimization. However, the convergence speed of BB method is not well\nunderstood, as the known convergence rate of BB method for quadratic problems\nis much worse than the steepest descent (SD) method. Therefore, there is a\nlarge discrepancy between theory and practice. To shrink this gap, we prove\nthat the BB method converges $R$-linearly at a rate of $1-1/\\kappa$, where\n$\\kappa$ is the condition number, for strongly convex quadratic problems. In\naddition, an example with the theoretical rate of convergence is constructed,\nindicating the tightness of our bound.\n", "versions": [{"version": "v1", "created": "Fri, 1 Jan 2021 09:49:47 GMT"}, {"version": "v2", "created": "Fri, 22 Jan 2021 09:31:58 GMT"}], "update_date": "2021-01-25", "authors_parsed": [["Li", "Dawei", ""], ["Sun", "Ruoyu", ""]]}, {"id": "2101.00214", "submitter": "Ginni Garg", "authors": "Ginni Garg, Dheeraj Kumar, ArvinderPal, Yash Sonker, Ritu Garg", "title": "A Hybrid MLP-SVM Model for Classification using Spatial-Spectral\n  Features on Hyper-Spectral Images", "comments": "9 pages, 5 figures, 4 Tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There are many challenges in the classification of hyper spectral images such\nas large dimensionality, scarcity of labeled data and spatial variability of\nspectral signatures. In this proposed method, we make a hybrid classifier\n(MLP-SVM) using multilayer perceptron (MLP) and support vector machine (SVM)\nwhich aimed to improve the various classification parameters such as accuracy,\nprecision, recall, f-score and to predict the region without ground truth. In\nproposed method, outputs from the last hidden layer of the neural net-ork\nbecome the input to the SVM, which finally classifies into various desired\nclasses. In the present study, we worked on Indian Pines, U. Pavia and Salinas\ndataset with 16, 9, 16 classes and 200, 103 and 204 reflectance bands\nrespectively, which is provided by AVIRIS and ROSIS sensor of NASA Jet\npropulsion laboratory. The proposed method significantly increases the accuracy\non testing dataset to 93.22%, 96.87%, 93.81% as compare to 86.97%, 88.58%,\n88.85% and 91.61%, 96.20%, 90.68% based on individual classifiers SVM and MLP\non Indian Pines, U. Pavia and Salinas datasets respectively.\n", "versions": [{"version": "v1", "created": "Fri, 1 Jan 2021 11:47:23 GMT"}], "update_date": "2021-01-05", "authors_parsed": [["Garg", "Ginni", ""], ["Kumar", "Dheeraj", ""], ["ArvinderPal", "", ""], ["Sonker", "Yash", ""], ["Garg", "Ritu", ""]]}, {"id": "2101.00218", "submitter": "Yingshi Chen", "authors": "Yingshi Chen", "title": "An iterative K-FAC algorithm for Deep Learning", "comments": "5 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NA math.NA stat.ML", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Kronecker-factored Approximate Curvature (K-FAC) method is a high efficiency\nsecond order optimizer for the deep learning. Its training time is less than\nSGD(or other first-order method) with same accuracy in many large-scale\nproblems. The key of K-FAC is to approximates Fisher information matrix (FIM)\nas a block-diagonal matrix where each block is an inverse of tiny Kronecker\nfactors. In this short note, we present CG-FAC -- an new iterative K-FAC\nalgorithm. It uses conjugate gradient method to approximate the nature\ngradient. This CG-FAC method is matrix-free, that is, no need to generate the\nFIM matrix, also no need to generate the Kronecker factors A and G. We prove\nthat the time and memory complexity of iterative CG-FAC is much less than that\nof standard K-FAC algorithm.\n", "versions": [{"version": "v1", "created": "Fri, 1 Jan 2021 12:04:01 GMT"}], "update_date": "2021-01-05", "authors_parsed": [["Chen", "Yingshi", ""]]}, {"id": "2101.00234", "submitter": "Machel Reid", "authors": "Machel Reid, Edison Marrese-Taylor and Yutaka Matsuo", "title": "Subformer: Exploring Weight Sharing for Parameter Efficiency in\n  Generative Transformers", "comments": "Work in progress", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The advent of the Transformer can arguably be described as a driving force\nbehind many of the recent advances in natural language processing. However,\ndespite their sizeable performance improvements, as recently shown, the model\nis severely over-parameterized, being parameter inefficient and computationally\nexpensive to train. Inspired by the success of parameter-sharing in pretrained\ndeep contextualized word representation encoders, we explore parameter-sharing\nmethods in Transformers, with a specific focus on encoder-decoder models for\nsequence-to-sequence tasks such as neural machine translation. We perform an\nanalysis of different parameter sharing/reduction methods and develop the\nSubformer, a parameter efficient Transformer-based model which combines the\nnewly proposed Sandwich-style parameter sharing technique - designed to\novercome the deficiencies in naive cross-layer parameter sharing for generative\nmodels - and self-attentive embedding factorization (SAFE). Experiments on\nmachine translation, abstractive summarization, and language modeling show that\nthe Subformer can outperform the Transformer even when using significantly\nfewer parameters.\n", "versions": [{"version": "v1", "created": "Fri, 1 Jan 2021 13:53:22 GMT"}], "update_date": "2021-01-05", "authors_parsed": [["Reid", "Machel", ""], ["Marrese-Taylor", "Edison", ""], ["Matsuo", "Yutaka", ""]]}, {"id": "2101.00236", "submitter": "Yuan Yao", "authors": "Jinshan Zeng and Yixuan Zha and Ke Ma and Yuan Yao", "title": "On Stochastic Variance Reduced Gradient Method for Semidefinite\n  Optimization", "comments": "27 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The low-rank stochastic semidefinite optimization has attracted rising\nattention due to its wide range of applications. The nonconvex reformulation\nbased on the low-rank factorization, significantly improves the computational\nefficiency but brings some new challenge to the analysis. The stochastic\nvariance reduced gradient (SVRG) method has been regarded as one of the most\neffective methods. SVRG in general consists of two loops, where a reference\nfull gradient is first evaluated in the outer loop and then used to yield a\nvariance reduced estimate of the current gradient in the inner loop. Two\noptions have been suggested to yield the output of the inner loop, where Option\nI sets the output as its last iterate, and Option II yields the output via\nrandom sampling from all the iterates in the inner loop. However, there is a\nsignificant gap between the theory and practice of SVRG when adapted to the\nstochastic semidefinite programming (SDP). SVRG practically works better with\nOption I, while most of existing theoretical results focus on Option II. In\nthis paper, we fill this gap via exploiting a new semi-stochastic variant of\nthe original SVRG with Option I adapted to the semidefinite optimization.\nEquipped with this, we establish the global linear submanifold convergence\n(i.e., converging exponentially fast to a submanifold of a global minimum under\nthe orthogonal group action) of the proposed SVRG method, given a provable\ninitialization scheme and under certain smoothness and restricted strongly\nconvex assumptions. Our analysis includes the effects of the mini-batch size\nand update frequency in the inner loop as well as two practical step size\nstrategies, the fixed and stabilized Barzilai-Borwein step sizes. Some\nnumerical results in matrix sensing demonstrate the efficiency of proposed SVRG\nmethod outperforming Option II counterpart as well as others.\n", "versions": [{"version": "v1", "created": "Fri, 1 Jan 2021 13:55:32 GMT"}], "update_date": "2021-01-05", "authors_parsed": [["Zeng", "Jinshan", ""], ["Zha", "Yixuan", ""], ["Ma", "Ke", ""], ["Yao", "Yuan", ""]]}, {"id": "2101.00238", "submitter": "Hui Zhong", "authors": "Hui Zhong, Zaiyi Chen, Chuan Qin, Zai Huang, Vincent W. Zheng, Tong\n  Xu, Enhong Chen", "title": "Adam revisited: a weighted past gradients perspective", "comments": "Zhong, Hui, et al. \"Adam revisited: a weighted past gradients\n  perspective.\" Frontiers of Computer Science 14.5 (2020): 1-16", "journal-ref": "Front. Comput. Sci. 14, 145309 (2020)", "doi": "10.1007/s11704-019-8457-x", "report-no": null, "categories": "cs.LG math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Adaptive learning rate methods have been successfully applied in many fields,\nespecially in training deep neural networks. Recent results have shown that\nadaptive methods with exponential increasing weights on squared past gradients\n(i.e., ADAM, RMSPROP) may fail to converge to the optimal solution. Though many\nalgorithms, such as AMSGRAD and ADAMNC, have been proposed to fix the\nnon-convergence issues, achieving a data-dependent regret bound similar to or\nbetter than ADAGRAD is still a challenge to these methods. In this paper, we\npropose a novel adaptive method weighted adaptive algorithm (WADA) to tackle\nthe non-convergence issues. Unlike AMSGRAD and ADAMNC, we consider using a\nmilder growing weighting strategy on squared past gradient, in which weights\ngrow linearly. Based on this idea, we propose weighted adaptive gradient method\nframework (WAGMF) and implement WADA algorithm on this framework. Moreover, we\nprove that WADA can achieve a weighted data-dependent regret bound, which could\nbe better than the original regret bound of ADAGRAD when the gradients decrease\nrapidly. This bound may partially explain the good performance of ADAM in\npractice. Finally, extensive experiments demonstrate the effectiveness of WADA\nand its variants in comparison with several variants of ADAM on training convex\nproblems and deep neural networks.\n", "versions": [{"version": "v1", "created": "Fri, 1 Jan 2021 14:01:52 GMT"}], "update_date": "2021-01-05", "authors_parsed": [["Zhong", "Hui", ""], ["Chen", "Zaiyi", ""], ["Qin", "Chuan", ""], ["Huang", "Zai", ""], ["Zheng", "Vincent W.", ""], ["Xu", "Tong", ""], ["Chen", "Enhong", ""]]}, {"id": "2101.00240", "submitter": "Siddique Latif", "authors": "Siddique Latif, Heriberto Cuay\\'ahuitl, Farrukh Pervez, Fahad\n  Shamshad, Hafiz Shehbaz Ali, and Erik Cambria", "title": "A Survey on Deep Reinforcement Learning for Audio-Based Applications", "comments": "Under Review", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.LG eess.AS", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Deep reinforcement learning (DRL) is poised to revolutionise the field of\nartificial intelligence (AI) by endowing autonomous systems with high levels of\nunderstanding of the real world. Currently, deep learning (DL) is enabling DRL\nto effectively solve various intractable problems in various fields. Most\nimportantly, DRL algorithms are also being employed in audio signal processing\nto learn directly from speech, music and other sound signals in order to create\naudio-based autonomous systems that have many promising application in the real\nworld. In this article, we conduct a comprehensive survey on the progress of\nDRL in the audio domain by bringing together the research studies across\ndifferent speech and music-related areas. We begin with an introduction to the\ngeneral field of DL and reinforcement learning (RL), then progress to the main\nDRL methods and their applications in the audio domain. We conclude by\npresenting challenges faced by audio-based DRL agents and highlighting open\nareas for future research and investigation.\n", "versions": [{"version": "v1", "created": "Fri, 1 Jan 2021 14:15:20 GMT"}], "update_date": "2021-01-05", "authors_parsed": [["Latif", "Siddique", ""], ["Cuay\u00e1huitl", "Heriberto", ""], ["Pervez", "Farrukh", ""], ["Shamshad", "Fahad", ""], ["Ali", "Hafiz Shehbaz", ""], ["Cambria", "Erik", ""]]}, {"id": "2101.00245", "submitter": "Er-Dong Guo", "authors": "Erdong Guo and David Draper", "title": "The Bayesian Method of Tensor Networks", "comments": "13 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CV cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bayesian learning is a powerful learning framework which combines the\nexternal information of the data (background information) with the internal\ninformation (training data) in a logically consistent way in inference and\nprediction. By Bayes rule, the external information (prior distribution) and\nthe internal information (training data likelihood) are combined coherently,\nand the posterior distribution and the posterior predictive (marginal)\ndistribution obtained by Bayes rule summarize the total information needed in\nthe inference and prediction, respectively. In this paper, we study the\nBayesian framework of the Tensor Network from two perspective. First, we\nintroduce the prior distribution to the weights in the Tensor Network and\npredict the labels of the new observations by the posterior predictive\n(marginal) distribution. Since the intractability of the parameter integral in\nthe normalization constant computation, we approximate the posterior predictive\ndistribution by Laplace approximation and obtain the out-product approximation\nof the hessian matrix of the posterior distribution of the Tensor Network\nmodel. Second, to estimate the parameters of the stationary mode, we propose a\nstable initialization trick to accelerate the inference process by which the\nTensor Network can converge to the stationary path more efficiently and stably\nwith gradient descent method. We verify our work on the MNIST, Phishing Website\nand Breast Cancer data set. We study the Bayesian properties of the Bayesian\nTensor Network by visualizing the parameters of the model and the decision\nboundaries in the two dimensional synthetic data set. For a application\npurpose, our work can reduce the overfitting and improve the performance of\nnormal Tensor Network model.\n", "versions": [{"version": "v1", "created": "Fri, 1 Jan 2021 14:59:15 GMT"}], "update_date": "2021-01-05", "authors_parsed": [["Guo", "Erdong", ""], ["Draper", "David", ""]]}, {"id": "2101.00257", "submitter": "Bin Li", "authors": "Bin Li", "title": "Efficient Learning-based Scheduling for Information Freshness in\n  Wireless Networks", "comments": "This paper has been accepted by IEEE INFOCOM 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PF cs.LG cs.NI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Motivated by the recent trend of integrating artificial intelligence into the\nInternet-of-Things (IoT), we consider the problem of scheduling packets from\nmultiple sensing sources to a central controller over a wireless network. Here,\npackets from different sensing sources have different values or degrees of\nimportance to the central controller for intelligent decision making. In such a\nsetup, it is critical to provide timely and valuable information for the\ncentral controller. In this paper, we develop a parameterized maximum-weight\ntype scheduling policy that combines both the AoI metrics and Upper Confidence\nBound (UCB) estimates in its weight measure with parameter $\\eta$. Here, UCB\nestimates balance the tradeoff between exploration and exploitation in learning\nand are critical for yielding a small cumulative regret. We show that our\nproposed algorithm yields the running average total age at most by\n$O(N^2\\eta)$. We also prove that our proposed algorithm achieves the cumulative\nregret over time horizon $T$ at most by $O(NT/\\eta+\\sqrt{NT\\log T})$. This\nreveals a tradeoff between the cumulative regret and the running average total\nage: when increasing $\\eta$, the cumulative regret becomes smaller, but is at\nthe cost of increasing running average total age. Simulation results are\nprovided to evaluate the efficiency of our proposed algorithm.\n", "versions": [{"version": "v1", "created": "Fri, 1 Jan 2021 15:59:59 GMT"}], "update_date": "2021-01-05", "authors_parsed": [["Li", "Bin", ""]]}, {"id": "2101.00259", "submitter": "Sajad Norouzi", "authors": "Sajad Norouzi, Keyi Tang, Yanshuai Cao", "title": "Code Generation from Natural Language with Less Prior and More\n  Monolingual Data", "comments": "ACL 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Training datasets for semantic parsing are typically small due to the higher\nexpertise required for annotation than most other NLP tasks. As a result,\nmodels for this application usually need additional prior knowledge to be built\ninto the architecture or algorithm. The increased dependency on human experts\nhinders automation and raises the development and maintenance costs in\npractice. This work investigates whether a generic transformer-based seq2seq\nmodel can achieve competitive performance with minimal code-generation-specific\ninductive bias design. By exploiting a relatively sizeable monolingual corpus\nof the target programming language, which is cheap to mine from the web, we\nachieved 81.03% exact match accuracy on Django and 32.57 BLEU score on CoNaLa.\nBoth are SOTA to the best of our knowledge. This positive evidence highlights a\npotentially easier path toward building accurate semantic parsers in practice.\n", "versions": [{"version": "v1", "created": "Fri, 1 Jan 2021 16:02:38 GMT"}, {"version": "v2", "created": "Thu, 10 Jun 2021 15:51:02 GMT"}], "update_date": "2021-06-11", "authors_parsed": [["Norouzi", "Sajad", ""], ["Tang", "Keyi", ""], ["Cao", "Yanshuai", ""]]}, {"id": "2101.00265", "submitter": "Xiaopeng Lu", "authors": "Xiaopeng Lu, Tiancheng Zhao, Kyusong Lee", "title": "VisualSparta: An Embarrassingly Simple Approach to Large-scale\n  Text-to-Image Search with Weighted Bag-of-words", "comments": "Accepted to ACL2021 (10 pages)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Text-to-image retrieval is an essential task in cross-modal information\nretrieval, i.e., retrieving relevant images from a large and unlabelled dataset\ngiven textual queries. In this paper, we propose VisualSparta, a novel\n(Visual-text Sparse Transformer Matching) model that shows significant\nimprovement in terms of both accuracy and efficiency. VisualSparta is capable\nof outperforming previous state-of-the-art scalable methods in MSCOCO and\nFlickr30K. We also show that it achieves substantial retrieving speed\nadvantages, i.e., for a 1 million image index, VisualSparta using CPU gets\n~391X speedup compared to CPU vector search and ~5.4X speedup compared to\nvector search with GPU acceleration. Experiments show that this speed advantage\neven gets bigger for larger datasets because VisualSparta can be efficiently\nimplemented as an inverted index. To the best of our knowledge, VisualSparta is\nthe first transformer-based text-to-image retrieval model that can achieve\nreal-time searching for large-scale datasets, with significant accuracy\nimprovement compared to previous state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Fri, 1 Jan 2021 16:29:17 GMT"}, {"version": "v2", "created": "Fri, 21 May 2021 03:10:15 GMT"}], "update_date": "2021-05-24", "authors_parsed": [["Lu", "Xiaopeng", ""], ["Zhao", "Tiancheng", ""], ["Lee", "Kyusong", ""]]}, {"id": "2101.00295", "submitter": "Ali Tourani", "authors": "Ali Tourani, Sajjad Soroori, Asadollah Shahbahrami, and Alireza\n  Akoushideh", "title": "Iranis: A Large-scale Dataset of Farsi License Plate Characters", "comments": "9 pages, 4 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Providing huge amounts of data is a fundamental demand when dealing with Deep\nNeural Networks (DNNs). Employing these algorithms to solve computer vision\nproblems resulted in the advent of various image datasets to feed the most\ncommon visual imagery deep structures, known as Convolutional Neural Networks\n(CNNs). In this regard, some datasets can be found that contain hundreds or\neven thousands of images for license plate detection and optical character\nrecognition purposes. However, no publicly available image dataset provides\nsuch data for the recognition of Farsi characters used in car license plates.\nThe gap has to be filled due to the numerous advantages of developing accurate\ndeep learning-based systems for law enforcement and surveillance purposes. This\npaper introduces a large-scale dataset that includes images of numbers and\ncharacters used in Iranian car license plates. The dataset, named Iranis,\ncontains more than 83,000 images of Farsi numbers and letters collected from\nreal-world license plate images captured by various cameras. The variety of\ninstances in terms of camera shooting angle, illumination, resolution, and\ncontrast make the dataset a proper choice for training DNNs. Dataset images are\nmanually annotated for object detection and image classification. Finally, and\nto build a baseline for Farsi character recognition, the paper provides a\nperformance analysis using a YOLO v.3 object detector.\n", "versions": [{"version": "v1", "created": "Fri, 1 Jan 2021 18:54:44 GMT"}], "update_date": "2021-01-05", "authors_parsed": [["Tourani", "Ali", ""], ["Soroori", "Sajjad", ""], ["Shahbahrami", "Asadollah", ""], ["Akoushideh", "Alireza", ""]]}, {"id": "2101.00300", "submitter": "Dhruv Malik", "authors": "Dhruv Malik, Yuanzhi Li, Pradeep Ravikumar", "title": "When Is Generalizable Reinforcement Learning Tractable?", "comments": "v2 extends results to function approximation setting", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Agents trained by reinforcement learning (RL) often fail to generalize beyond\nthe environment they were trained in, even when presented with new scenarios\nthat seem similar to the training environment. We study the query complexity\nrequired to train RL agents that generalize to multiple environments.\nIntuitively, tractable generalization is only possible when the environments\nare similar or close in some sense. To capture this, we introduce Weak\nProximity, a natural structural condition that requires the environments to\nhave highly similar transition and reward functions and share a policy\nproviding optimal value. Despite such shared structure, we prove that tractable\ngeneralization is impossible in the worst case. This holds even when each\nindividual environment can be efficiently solved to obtain an optimal linear\npolicy, and when the agent possesses a generative model. Our lower bound\napplies to the more complex task of representation learning for the purpose of\nefficient generalization to multiple environments. On the positive side, we\nintroduce Strong Proximity, a strengthened condition which we prove is\nsufficient for efficient generalization.\n", "versions": [{"version": "v1", "created": "Fri, 1 Jan 2021 19:08:24 GMT"}, {"version": "v2", "created": "Fri, 28 May 2021 01:35:52 GMT"}], "update_date": "2021-05-31", "authors_parsed": [["Malik", "Dhruv", ""], ["Li", "Yuanzhi", ""], ["Ravikumar", "Pradeep", ""]]}, {"id": "2101.00304", "submitter": "Shahabeddin Sotudian", "authors": "Shahabeddin Sotudian and Mohammad Hossein Fazel Zarandi", "title": "Interval Type-2 Enhanced Possibilistic Fuzzy C-Means Clustering for Gene\n  Expression Data Analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.GN cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Both FCM and PCM clustering methods have been widely applied to pattern\nrecognition and data clustering. Nevertheless, FCM is sensitive to noise and\nPCM occasionally generates coincident clusters. PFCM is an extension of the PCM\nmodel by combining FCM and PCM, but this method still suffers from the\nweaknesses of PCM and FCM. In the current paper, the weaknesses of the PFCM\nalgorithm are corrected and the enhanced possibilistic fuzzy c-means (EPFCM)\nclustering algorithm is presented. EPFCM can still be sensitive to noise.\nTherefore, we propose an interval type-2 enhanced possibilistic fuzzy c-means\n(IT2EPFCM) clustering method by utilizing two fuzzifiers $(m_1, m_2)$ for fuzzy\nmemberships and two fuzzifiers $({\\theta}_1, {\\theta}_2)$ for possibilistic\ntypicalities. Our computational results show the superiority of the proposed\napproaches compared with several state-of-the-art techniques in the literature.\nFinally, the proposed methods are implemented for analyzing microarray gene\nexpression data.\n", "versions": [{"version": "v1", "created": "Fri, 1 Jan 2021 19:29:24 GMT"}], "update_date": "2021-01-05", "authors_parsed": [["Sotudian", "Shahabeddin", ""], ["Zarandi", "Mohammad Hossein Fazel", ""]]}, {"id": "2101.00307", "submitter": "Jiawei Xue", "authors": "Jiawei Xue, Nan Jiang, Senwei Liang, Qiyuan Pang, Satish V. Ukkusuri,\n  Jianzhu Ma", "title": "Quantifying spatial homogeneity of urban road networks via graph neural\n  networks", "comments": "22 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.soc-ph cs.LG cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The spatial homogeneity of an urban road network (URN) measures whether each\ndistinct component is analogous to the whole network and can serve as a\nquantitative manner bridging network structure and dynamics. However, given the\ncomplexity of cities, it is challenging to quantify spatial homogeneity simply\nbased on conventional network statistics. In this work, we use Graph Neural\nNetworks to model the 11,790 URN samples across 30 cities worldwide and use its\npredictability to define the spatial homogeneity. The proposed measurement can\nbe viewed as a non-linear integration of multiple geometric properties, such as\ndegree, betweenness, road network type, and a strong indicator of mixed\nsocio-economic events, such as GDP and population growth. City clusters derived\nfrom transferring spatial homogeneity can be interpreted well by continental\nurbanization histories. We expect this novel metric supports various subsequent\ntasks in transportation, urban planning, and geography.\n", "versions": [{"version": "v1", "created": "Fri, 1 Jan 2021 19:45:04 GMT"}], "update_date": "2021-01-05", "authors_parsed": [["Xue", "Jiawei", ""], ["Jiang", "Nan", ""], ["Liang", "Senwei", ""], ["Pang", "Qiyuan", ""], ["Ukkusuri", "Satish V.", ""], ["Ma", "Jianzhu", ""]]}, {"id": "2101.00316", "submitter": "Xiaofeng Liu", "authors": "Xiaofeng Liu, Bo Hu, Xiongchang Liu, Jun Lu, Jane You, Lingsheng Kong", "title": "Energy-constrained Self-training for Unsupervised Domain Adaptation", "comments": "Accepted to 25th International Conference on Pattern Recognition\n  (ICPR 2020)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG cs.RO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Unsupervised domain adaptation (UDA) aims to transfer the knowledge on a\nlabeled source domain distribution to perform well on an unlabeled target\ndomain. Recently, the deep self-training involves an iterative process of\npredicting on the target domain and then taking the confident predictions as\nhard pseudo-labels for retraining. However, the pseudo-labels are usually\nunreliable, and easily leading to deviated solutions with propagated errors. In\nthis paper, we resort to the energy-based model and constrain the training of\nthe unlabeled target sample with the energy function minimization objective. It\ncan be applied as a simple additional regularization. In this framework, it is\npossible to gain the benefits of the energy-based model, while retaining strong\ndiscriminative performance following a plug-and-play fashion. We deliver\nextensive experiments on the most popular and large scale UDA benchmarks of\nimage classification as well as semantic segmentation to demonstrate its\ngenerality and effectiveness.\n", "versions": [{"version": "v1", "created": "Fri, 1 Jan 2021 21:02:18 GMT"}], "update_date": "2021-01-05", "authors_parsed": [["Liu", "Xiaofeng", ""], ["Hu", "Bo", ""], ["Liu", "Xiongchang", ""], ["Lu", "Jun", ""], ["You", "Jane", ""], ["Kong", "Lingsheng", ""]]}, {"id": "2101.00317", "submitter": "Xiaofeng Liu", "authors": "Xiaofeng Liu, Linghao Jin, Xu Han, Jun Lu, Jane You, Lingsheng Kong", "title": "Identity-aware Facial Expression Recognition in Compressed Video", "comments": "Accepted as the Oral paper at ICPR 2020 (<4.4%). arXiv admin note:\n  substantial text overlap with arXiv:2010.10637", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG cs.MM", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper targets to explore the inter-subject variations eliminated facial\nexpression representation in the compressed video domain. Most of the previous\nmethods process the RGB images of a sequence, while the off-the-shelf and\nvaluable expression-related muscle movement already embedded in the compression\nformat. In the up to two orders of magnitude compressed domain, we can\nexplicitly infer the expression from the residual frames and possible to\nextract identity factors from the I frame with a pre-trained face recognition\nnetwork. By enforcing the marginal independent of them, the expression feature\nis expected to be purer for the expression and be robust to identity shifts. We\ndo not need the identity label or multiple expression samples from the same\nperson for identity elimination. Moreover, when the apex frame is annotated in\nthe dataset, the complementary constraint can be further added to regularize\nthe feature-level game. In testing, only the compressed residual frames are\nrequired to achieve expression prediction. Our solution can achieve comparable\nor better performance than the recent decoded image based methods on the\ntypical FER benchmarks with about 3$\\times$ faster inference with compressed\ndata.\n", "versions": [{"version": "v1", "created": "Fri, 1 Jan 2021 21:03:13 GMT"}, {"version": "v2", "created": "Thu, 7 Jan 2021 23:46:22 GMT"}], "update_date": "2021-01-11", "authors_parsed": [["Liu", "Xiaofeng", ""], ["Jin", "Linghao", ""], ["Han", "Xu", ""], ["Lu", "Jun", ""], ["You", "Jane", ""], ["Kong", "Lingsheng", ""]]}, {"id": "2101.00318", "submitter": "Xiaofeng Liu", "authors": "Xiaofeng Liu, Xiongchang Liu, Bo Hu, Wenxuan Ji, Fangxu Xing, Jun Lu,\n  Jane You, C.-C. Jay Kuo, Georges El Fakhri, Jonghye Woo", "title": "Subtype-aware Unsupervised Domain Adaptation for Medical Diagnosis", "comments": "Accepted to AAAI 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Recent advances in unsupervised domain adaptation (UDA) show that\ntransferable prototypical learning presents a powerful means for class\nconditional alignment, which encourages the closeness of cross-domain class\ncentroids. However, the cross-domain inner-class compactness and the underlying\nfine-grained subtype structure remained largely underexplored. In this work, we\npropose to adaptively carry out the fine-grained subtype-aware alignment by\nexplicitly enforcing the class-wise separation and subtype-wise compactness\nwith intermediate pseudo labels. Our key insight is that the unlabeled subtypes\nof a class can be divergent to one another with different conditional and label\nshifts, while inheriting the local proximity within a subtype. The cases of\nwith or without the prior information on subtype numbers are investigated to\ndiscover the underlying subtype structure in an online fashion. The proposed\nsubtype-aware dynamic UDA achieves promising results on medical diagnosis\ntasks.\n", "versions": [{"version": "v1", "created": "Fri, 1 Jan 2021 21:04:50 GMT"}, {"version": "v2", "created": "Mon, 11 Jan 2021 15:09:03 GMT"}], "update_date": "2021-01-12", "authors_parsed": [["Liu", "Xiaofeng", ""], ["Liu", "Xiongchang", ""], ["Hu", "Bo", ""], ["Ji", "Wenxuan", ""], ["Xing", "Fangxu", ""], ["Lu", "Jun", ""], ["You", "Jane", ""], ["Kuo", "C. -C. Jay", ""], ["Fakhri", "Georges El", ""], ["Woo", "Jonghye", ""]]}, {"id": "2101.00323", "submitter": "Chengrun Yang", "authors": "Chengrun Yang, Lijun Ding, Ziyang Wu, Madeleine Udell", "title": "TenIPS: Inverse Propensity Sampling for Tensor Completion", "comments": "AISTATS 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.OC", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Tensors are widely used to represent multiway arrays of data. The recovery of\nmissing entries in a tensor has been extensively studied, generally under the\nassumption that entries are missing completely at random (MCAR). However, in\nmost practical settings, observations are missing not at random (MNAR): the\nprobability that a given entry is observed (also called the propensity) may\ndepend on other entries in the tensor or even on the value of the missing\nentry. In this paper, we study the problem of completing a partially observed\ntensor with MNAR observations, without prior information about the\npropensities. To complete the tensor, we assume that both the original tensor\nand the tensor of propensities have low multilinear rank. The algorithm first\nestimates the propensities using a convex relaxation and then predicts missing\nvalues using a higher-order SVD approach, reweighting the observed tensor by\nthe inverse propensities. We provide finite-sample error bounds on the\nresulting complete tensor. Numerical experiments demonstrate the effectiveness\nof our approach.\n", "versions": [{"version": "v1", "created": "Fri, 1 Jan 2021 22:13:19 GMT"}, {"version": "v2", "created": "Mon, 1 Mar 2021 23:34:26 GMT"}, {"version": "v3", "created": "Thu, 22 Apr 2021 15:41:38 GMT"}], "update_date": "2021-04-23", "authors_parsed": [["Yang", "Chengrun", ""], ["Ding", "Lijun", ""], ["Wu", "Ziyang", ""], ["Udell", "Madeleine", ""]]}, {"id": "2101.00336", "submitter": "Hanxun Huang", "authors": "Hanxun Huang, Xingjun Ma, Sarah M. Erfani, James Bailey", "title": "Neural Architecture Search via Combinatorial Multi-Armed Bandit", "comments": "10 pages, 7 figures", "journal-ref": "International Joint Conference on Neural Networks (IJCNN) 2021", "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Neural Architecture Search (NAS) has gained significant popularity as an\neffective tool for designing high performance deep neural networks (DNNs). NAS\ncan be performed via policy gradient, evolutionary algorithms, differentiable\narchitecture search or tree-search methods. While significant progress has been\nmade for both policy gradient and differentiable architecture search,\ntree-search methods have so far failed to achieve comparable accuracy or search\nefficiency. In this paper, we formulate NAS as a Combinatorial Multi-Armed\nBandit (CMAB) problem (CMAB-NAS). This allows the decomposition of a large\nsearch space into smaller blocks where tree-search methods can be applied more\neffectively and efficiently. We further leverage a tree-based method called\nNested Monte-Carlo Search to tackle the CMAB-NAS problem. On CIFAR-10, our\napproach discovers a cell structure that achieves a low error rate that is\ncomparable to the state-of-the-art, using only 0.58 GPU days, which is 20 times\nfaster than current tree-search methods. Moreover, the discovered structure\ntransfers well to large-scale datasets such as ImageNet.\n", "versions": [{"version": "v1", "created": "Fri, 1 Jan 2021 23:29:33 GMT"}, {"version": "v2", "created": "Sat, 24 Apr 2021 14:13:15 GMT"}], "update_date": "2021-04-27", "authors_parsed": [["Huang", "Hanxun", ""], ["Ma", "Xingjun", ""], ["Erfani", "Sarah M.", ""], ["Bailey", "James", ""]]}, {"id": "2101.00337", "submitter": "Tobias Schlosser", "authors": "Tobias Schlosser, Frederik Beuth, and Danny Kowerko", "title": "Biologically Inspired Hexagonal Deep Learning for Hexagonal Image\n  Generation", "comments": "Accepted for: 2020 IEEE 27th International Conference on Image\n  Processing (ICIP); this article draws heavily from arXiv:1911.11251", "journal-ref": null, "doi": "10.1109/ICIP40778.2020.9190995", "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Whereas conventional state-of-the-art image processing systems of recording\nand output devices almost exclusively utilize square arranged methods,\nbiological models, however, suggest an alternative, evolutionarily-based\nstructure. Inspired by the human visual perception system, hexagonal image\nprocessing in the context of machine learning offers a number of key advantages\nthat can benefit both researchers and users alike. The hexagonal deep learning\nframework Hexnet leveraged in this contribution serves therefore the generation\nof hexagonal images by utilizing hexagonal deep neural networks (H-DNN). As the\nresults of our created test environment show, the proposed models can surpass\ncurrent approaches of conventional image generation. While resulting in a\nreduction of the models' complexity in the form of trainable parameters, they\nfurthermore allow an increase of test rates in comparison to their square\ncounterparts.\n", "versions": [{"version": "v1", "created": "Fri, 1 Jan 2021 23:30:21 GMT"}, {"version": "v2", "created": "Thu, 18 Mar 2021 23:21:50 GMT"}], "update_date": "2021-03-22", "authors_parsed": [["Schlosser", "Tobias", ""], ["Beuth", "Frederik", ""], ["Kowerko", "Danny", ""]]}, {"id": "2101.00339", "submitter": "Angus Baird", "authors": "Angus Baird and Stefano Giani", "title": "An Artificial Intelligence System for Combined Fruit Detection and\n  Georeferencing, Using RTK-Based Perspective Projection in Drone Imagery", "comments": "12 pages, 12 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This work presents an Artificial Intelligence (AI) system, based on the\nFaster Region-Based Convolution Neural Network (Faster R-CNN) framework, which\ndetects and counts apples from oblique, aerial drone imagery of giant\ncommercial orchards. To reduce computational cost, a novel precursory stage to\nthe network is designed to preprocess raw imagery into cropped images of\nindividual trees. Unique geospatial identifiers are allocated to these using\nthe perspective projection model. This employs Real-Time Kinematic (RTK) data,\nDigital Terrain and Surface Models (DTM and DSM), as well as internal and\nexternal camera parameters. The bulk of experiments however focus on tuning\nhyperparameters in the detection network itself. Apples which are on trees and\napples which are on the ground are treated as separate classes. A mean Average\nPrecision (mAP) metric, calibrated by the size of the two classes, is devised\nto mitigate spurious results. Anchor box design is of key interest due to the\nscale of the apples. As such, a k-means clustering approach, never before seen\nin literature for Faster R-CNN, resulted in the most significant improvements\nto calibrated mAP. Other experiments showed that the maximum number of box\nproposals should be 225; the initial learning rate of 0.001 is best applied to\nthe adaptive RMS Prop optimiser; and ResNet 101 is the ideal base feature\nextractor when considering mAP and, to a lesser extent, inference time. The\namalgamation of the optimal hyperparameters leads to a model with a calibrated\nmAP of 0.7627.\n", "versions": [{"version": "v1", "created": "Fri, 1 Jan 2021 23:39:55 GMT"}], "update_date": "2021-01-05", "authors_parsed": [["Baird", "Angus", ""], ["Giani", "Stefano", ""]]}, {"id": "2101.00345", "submitter": "Yasumasa Onoe", "authors": "Yasumasa Onoe, Michael Boratko, Andrew McCallum, Greg Durrett", "title": "Modeling Fine-Grained Entity Types with Box Embeddings", "comments": "ACL 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural entity typing models typically represent fine-grained entity types as\nvectors in a high-dimensional space, but such spaces are not well-suited to\nmodeling these types' complex interdependencies. We study the ability of box\nembeddings, which embed concepts as d-dimensional hyperrectangles, to capture\nhierarchies of types even when these relationships are not defined explicitly\nin the ontology. Our model represents both types and entity mentions as boxes.\nEach mention and its context are fed into a BERT-based model to embed that\nmention in our box space; essentially, this model leverages typological clues\npresent in the surface text to hypothesize a type representation for the\nmention. Box containment can then be used to derive both the posterior\nprobability of a mention exhibiting a given type and the conditional\nprobability relations between types themselves. We compare our approach with a\nvector-based typing model and observe state-of-the-art performance on several\nentity typing benchmarks. In addition to competitive typing performance, our\nbox-based model shows better performance in prediction consistency (predicting\na supertype and a subtype together) and confidence (i.e., calibration),\ndemonstrating that the box-based model captures the latent type hierarchies\nbetter than the vector-based model does.\n", "versions": [{"version": "v1", "created": "Sat, 2 Jan 2021 00:59:10 GMT"}, {"version": "v2", "created": "Thu, 3 Jun 2021 05:51:55 GMT"}], "update_date": "2021-06-04", "authors_parsed": [["Onoe", "Yasumasa", ""], ["Boratko", "Michael", ""], ["McCallum", "Andrew", ""], ["Durrett", "Greg", ""]]}, {"id": "2101.00346", "submitter": "John Hawkins", "authors": "John Hawkins", "title": "Minimum Viable Model Estimates for Machine Learning Projects", "comments": "11 pages, 4 figures", "journal-ref": "Proceedings of the 6th International Conference on Computer\n  Science, Engineering And Applications (CSEA 2020), December 18 ~ 19, pp.\n  37-46, Volume 10, Number 18", "doi": "10.5121/csit.2020.101803", "report-no": null, "categories": "cs.LG cs.SE", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Prioritization of machine learning projects requires estimates of both the\npotential ROI of the business case and the technical difficulty of building a\nmodel with the required characteristics. In this work we present a technique\nfor estimating the minimum required performance characteristics of a predictive\nmodel given a set of information about how it will be used. This technique will\nresult in robust, objective comparisons between potential projects. The\nresulting estimates will allow data scientists and managers to evaluate whether\na proposed machine learning project is likely to succeed before any modelling\nneeds to be done.\n  The technique has been implemented into the open source application MinViME\n(Minimum Viable Model Estimator) which can be installed via the PyPI python\npackage management system, or downloaded directly from the GitHub repository.\nAvailable at https://github.com/john-hawkins/MinViME\n", "versions": [{"version": "v1", "created": "Sat, 2 Jan 2021 01:01:20 GMT"}], "update_date": "2021-01-05", "authors_parsed": [["Hawkins", "John", ""]]}, {"id": "2101.00352", "submitter": "Amanda Coston", "authors": "Amanda Coston and Ashesh Rambachan and Alexandra Chouldechova", "title": "Characterizing Fairness Over the Set of Good Models Under Selective\n  Labels", "comments": "Added comparison methods to the empirical lending analysis", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Algorithmic risk assessments are used to inform decisions in a wide variety\nof high-stakes settings. Often multiple predictive models deliver similar\noverall performance but differ markedly in their predictions for individual\ncases, an empirical phenomenon known as the \"Rashomon Effect.\" These models may\nhave different properties over various groups, and therefore have different\npredictive fairness properties. We develop a framework for characterizing\npredictive fairness properties over the set of models that deliver similar\noverall performance, or \"the set of good models.\" Our framework addresses the\nempirically relevant challenge of selectively labelled data in the setting\nwhere the selection decision and outcome are unconfounded given the observed\ndata features. Our framework can be used to 1) replace an existing model with\none that has better fairness properties; or 2) audit for predictive bias. We\nillustrate these uses cases on a real-world credit-scoring task and a\nrecidivism prediction task.\n", "versions": [{"version": "v1", "created": "Sat, 2 Jan 2021 02:11:37 GMT"}, {"version": "v2", "created": "Wed, 13 Jan 2021 00:25:36 GMT"}, {"version": "v3", "created": "Sat, 1 May 2021 00:21:59 GMT"}], "update_date": "2021-05-04", "authors_parsed": [["Coston", "Amanda", ""], ["Rambachan", "Ashesh", ""], ["Chouldechova", "Alexandra", ""]]}, {"id": "2101.00354", "submitter": "Mehmet Kolcu", "authors": "Mehmet Kolcu, Alper E. Murat", "title": "Integrated Optimization of Predictive and Prescriptive Tasks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In traditional machine learning techniques, the degree of closeness between\ntrue and predicted values generally measures the quality of predictions.\nHowever, these learning algorithms do not consider prescription problems where\nthe predicted values will be used as input to decision problems. In this paper,\nwe efficiently leverage feature variables, and we propose a new framework\ndirectly integrating predictive tasks under prescriptive tasks in order to\nprescribe consistent decisions. We train the parameters of predictive algorithm\nwithin a prescription problem via bilevel optimization techniques. We present\nthe structure of our method and demonstrate its performance using synthetic\ndata compared to classical methods like point-estimate-based, stochastic\noptimization and recently developed machine learning based optimization\nmethods. In addition, we control generalization error using different penalty\napproaches and optimize the integration over validation data set.\n", "versions": [{"version": "v1", "created": "Sat, 2 Jan 2021 02:43:10 GMT"}], "update_date": "2021-01-05", "authors_parsed": [["Kolcu", "Mehmet", ""], ["Murat", "Alper E.", ""]]}, {"id": "2101.00355", "submitter": "Lei Zhang", "authors": "Yehua Wei, Lei Zhang, Ruiyi Zhang, Shijing Si, Hao Zhang, Lawrence\n  Carin", "title": "Reinforcement Learning for Flexibility Design Problems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Flexibility design problems are a class of problems that appear in strategic\ndecision-making across industries, where the objective is to design a ($e.g.$,\nmanufacturing) network that affords flexibility and adaptivity. The underlying\ncombinatorial nature and stochastic objectives make flexibility design problems\nchallenging for standard optimization methods. In this paper, we develop a\nreinforcement learning (RL) framework for flexibility design problems.\nSpecifically, we carefully design mechanisms with noisy exploration and\nvariance reduction to ensure empirical success and show the unique advantage of\nRL in terms of fast-adaptation. Empirical results show that the RL-based method\nconsistently finds better solutions compared to classical heuristics.\n", "versions": [{"version": "v1", "created": "Sat, 2 Jan 2021 02:44:39 GMT"}, {"version": "v2", "created": "Mon, 18 Jan 2021 14:35:06 GMT"}], "update_date": "2021-01-19", "authors_parsed": [["Wei", "Yehua", ""], ["Zhang", "Lei", ""], ["Zhang", "Ruiyi", ""], ["Si", "Shijing", ""], ["Zhang", "Hao", ""], ["Carin", "Lawrence", ""]]}, {"id": "2101.00387", "submitter": "Jui Shah Shah", "authors": "Jui Shah, Yaman Kumar Singla, Changyou Chen, Rajiv Ratn Shah", "title": "What all do audio transformer models hear? Probing Acoustic\n  Representations for Language Delivery and its Structure", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG cs.SD eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent times, BERT based transformer models have become an inseparable\npart of the 'tech stack' of text processing models. Similar progress is being\nobserved in the speech domain with a multitude of models observing\nstate-of-the-art results by using audio transformer models to encode speech.\nThis begs the question of what are these audio transformer models learning.\nMoreover, although the standard methodology is to choose the last layer\nembedding for any downstream task, but is it the optimal choice? We try to\nanswer these questions for the two recent audio transformer models, Mockingjay\nand wave2vec2.0. We compare them on a comprehensive set of language delivery\nand structure features including audio, fluency and pronunciation features.\nAdditionally, we probe the audio models' understanding of textual surface,\nsyntax, and semantic features and compare them to BERT. We do this over\nexhaustive settings for native, non-native, synthetic, read and spontaneous\nspeech datasets\n", "versions": [{"version": "v1", "created": "Sat, 2 Jan 2021 06:29:12 GMT"}, {"version": "v2", "created": "Mon, 12 Jul 2021 22:46:37 GMT"}], "update_date": "2021-07-14", "authors_parsed": [["Shah", "Jui", ""], ["Singla", "Yaman Kumar", ""], ["Chen", "Changyou", ""], ["Shah", "Rajiv Ratn", ""]]}, {"id": "2101.00388", "submitter": "Houjin Yu", "authors": "Houjin Yu, Xian-Ling Mao, Zewen Chi, Wei Wei and Heyan Huang", "title": "A Robust and Domain-Adaptive Approach for Low-Resource Named Entity\n  Recognition", "comments": "Best Student Paper of 2020 IEEE International Conference on Knowledge\n  Graph (ICKG)", "journal-ref": "2020 IEEE International Conference on Knowledge Graph (ICKG) (pp.\n  297-304)-", "doi": "10.1109/ICBK50248.2020.00050", "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, it has attracted much attention to build reliable named entity\nrecognition (NER) systems using limited annotated data. Nearly all existing\nworks heavily rely on domain-specific resources, such as external lexicons and\nknowledge bases. However, such domain-specific resources are often not\navailable, meanwhile it's difficult and expensive to construct the resources,\nwhich has become a key obstacle to wider adoption. To tackle the problem, in\nthis work, we propose a novel robust and domain-adaptive approach RDANER for\nlow-resource NER, which only uses cheap and easily obtainable resources.\nExtensive experiments on three benchmark datasets demonstrate that our approach\nachieves the best performance when only using cheap and easily obtainable\nresources, and delivers competitive results against state-of-the-art methods\nwhich use difficultly obtainable domainspecific resources. All our code and\ncorpora can be found on https://github.com/houking-can/RDANER.\n", "versions": [{"version": "v1", "created": "Sat, 2 Jan 2021 06:47:01 GMT"}], "update_date": "2021-01-05", "authors_parsed": [["Yu", "Houjin", ""], ["Mao", "Xian-Ling", ""], ["Chi", "Zewen", ""], ["Wei", "Wei", ""], ["Huang", "Heyan", ""]]}, {"id": "2101.00400", "submitter": "Paul Liu", "authors": "Aram Ebtekar and Paul Liu", "title": "An Elo-like System for Massive Multiplayer Competitions", "comments": "12 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.GT cs.LG cs.SI stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Rating systems play an important role in competitive sports and games. They\nprovide a measure of player skill, which incentivizes competitive performances\nand enables balanced match-ups. In this paper, we present a novel Bayesian\nrating system for contests with many participants. It is widely applicable to\ncompetition formats with discrete ranked matches, such as online programming\ncompetitions, obstacle courses races, and some video games. The simplicity of\nour system allows us to prove theoretical bounds on robustness and runtime. In\naddition, we show that the system aligns incentives: that is, a player who\nseeks to maximize their rating will never want to underperform. Experimentally,\nthe rating system rivals or surpasses existing systems in prediction accuracy,\nand computes faster than existing systems by up to an order of magnitude.\n", "versions": [{"version": "v1", "created": "Sat, 2 Jan 2021 08:14:31 GMT"}], "update_date": "2021-01-05", "authors_parsed": [["Ebtekar", "Aram", ""], ["Liu", "Paul", ""]]}, {"id": "2101.00401", "submitter": "Hiroshi Kera", "authors": "Hiroshi Kera", "title": "Border Basis Computation with Gradient-Weighted Norm", "comments": "19 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SC cs.LG math.AC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Normalization of polynomials plays an essential role in the approximate basis\ncomputation of vanishing ideals. In computer algebra, coefficient\nnormalization, which normalizes a polynomial by its coefficient norm, is the\nmost common method. In this study, we propose gradient-weighted normalization\nfor the approximate border basis computation of vanishing ideals, inspired by\nthe recent results in machine learning. The data-dependent nature of\ngradient-weighted normalization leads to powerful properties such as better\nstability against perturbation and consistency in the scaling of input points,\nwhich cannot be attained by the conventional coefficient normalization. With a\nslight modification, the analysis of algorithms with coefficient normalization\nstill works with gradient-weighted normalization and the time complexity does\nnot change. We also provide an upper bound on the coefficient norm based on the\ngradient-weighted norm, which allows us to discuss the approximate border bases\nwith gradient-weighted normalization from the perspective of the coefficient\nnorm.\n", "versions": [{"version": "v1", "created": "Sat, 2 Jan 2021 08:29:51 GMT"}, {"version": "v2", "created": "Thu, 14 Jan 2021 05:59:05 GMT"}], "update_date": "2021-01-15", "authors_parsed": [["Kera", "Hiroshi", ""]]}, {"id": "2101.00407", "submitter": "Liyuan Wang", "authors": "Liyuan Wang, Kuo Yang, Chongxuan Li, Lanqing Hong, Zhenguo Li, Jun Zhu", "title": "ORDisCo: Effective and Efficient Usage of Incremental Unlabeled Data for\n  Semi-supervised Continual Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Continual learning usually assumes the incoming data are fully labeled, which\nmight not be applicable in real applications. In this work, we consider\nsemi-supervised continual learning (SSCL) that incrementally learns from\npartially labeled data. Observing that existing continual learning methods lack\nthe ability to continually exploit the unlabeled data, we propose deep Online\nReplay with Discriminator Consistency (ORDisCo) to interdependently learn a\nclassifier with a conditional generative adversarial network (GAN), which\ncontinually passes the learned data distribution to the classifier. In\nparticular, ORDisCo replays data sampled from the conditional generator to the\nclassifier in an online manner, exploiting unlabeled data in a time- and\nstorage-efficient way. Further, to explicitly overcome the catastrophic\nforgetting of unlabeled data, we selectively stabilize parameters of the\ndiscriminator that are important for discriminating the pairs of old unlabeled\ndata and their pseudo-labels predicted by the classifier. We extensively\nevaluate ORDisCo on various semi-supervised learning benchmark datasets for\nSSCL, and show that ORDisCo achieves significant performance improvement on\nSVHN, CIFAR10 and Tiny-ImageNet, compared to strong baselines.\n", "versions": [{"version": "v1", "created": "Sat, 2 Jan 2021 09:04:14 GMT"}, {"version": "v2", "created": "Fri, 9 Apr 2021 01:57:03 GMT"}], "update_date": "2021-04-12", "authors_parsed": [["Wang", "Liyuan", ""], ["Yang", "Kuo", ""], ["Li", "Chongxuan", ""], ["Hong", "Lanqing", ""], ["Li", "Zhenguo", ""], ["Zhu", "Jun", ""]]}, {"id": "2101.00417", "submitter": "Xing Li", "authors": "Xing Li, Wei Wei, Xiangnan Feng, Zhiming Zheng", "title": "Representation Learning of Reconstructed Graphs Using Random Walk Graph\n  Convolutional Network", "comments": "8 pages, 3 figures. arXiv admin note: text overlap with\n  arXiv:2007.15838", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Graphs are often used to organize data because of their simple topological\nstructure, and therefore play a key role in machine learning. And it turns out\nthat the low-dimensional embedded representation obtained by graph\nrepresentation learning are extremely useful in various typical tasks, such as\nnode classification, content recommendation and link prediction. However, the\nexisting methods mostly start from the microstructure (i.e., the edges) in the\ngraph, ignoring the mesoscopic structure (high-order local structure). Here, we\npropose wGCN -- a novel framework that utilizes random walk to obtain the\nnode-specific mesoscopic structures of the graph, and utilizes these mesoscopic\nstructures to reconstruct the graph And organize the characteristic information\nof the nodes. Our method can effectively generate node embeddings for\npreviously unseen data, which has been proven in a series of experiments\nconducted on citation networks and social networks (our method has advantages\nover baseline methods). We believe that combining high-order local structural\ninformation can more efficiently explore the potential of the network, which\nwill greatly improve the learning efficiency of graph neural network and\npromote the establishment of new learning models.\n", "versions": [{"version": "v1", "created": "Sat, 2 Jan 2021 10:31:14 GMT"}], "update_date": "2021-01-05", "authors_parsed": [["Li", "Xing", ""], ["Wei", "Wei", ""], ["Feng", "Xiangnan", ""], ["Zheng", "Zhiming", ""]]}, {"id": "2101.00420", "submitter": "Qinyuan Ye", "authors": "Qinyuan Ye, Xiang Ren", "title": "Learning to Generate Task-Specific Adapters from Task Description", "comments": "Accepted to ACL 2021. Camera-ready version. Code:\n  https://github.com/INK-USC/hypter", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Pre-trained text-to-text transformers such as BART have achieved impressive\nperformance across a range of NLP tasks. Recent study further shows that they\ncan learn to generalize to novel tasks, by including task descriptions as part\nof the source sequence and training the model with (source, target) examples.\nAt test time, these fine-tuned models can make inferences on new tasks using\nthe new task descriptions as part of the input. However, this approach has\npotential limitations, as the model learns to solve individual (source, target)\nexamples (i.e., at the instance level), instead of learning to solve tasks by\ntaking all examples within a task as a whole (i.e., at the task level). To this\nend, we introduce Hypter, a framework that improves text-to-text transformer's\ngeneralization ability to unseen tasks by training a hypernetwork to generate\ntask-specific, light-weight adapters from task descriptions. Experiments on\nZEST dataset and a synthetic SQuAD dataset demonstrate that Hypter improves\nupon fine-tuning baselines. Notably, when using BART-Large as the main network,\nHypter brings 11.3% comparative improvement on ZEST dataset.\n", "versions": [{"version": "v1", "created": "Sat, 2 Jan 2021 10:50:23 GMT"}, {"version": "v2", "created": "Tue, 15 Jun 2021 05:50:01 GMT"}], "update_date": "2021-06-16", "authors_parsed": [["Ye", "Qinyuan", ""], ["Ren", "Xiang", ""]]}, {"id": "2101.00443", "submitter": "Sourav Garg", "authors": "Sourav Garg, Niko S\\\"underhauf, Feras Dayoub, Douglas Morrison,\n  Akansel Cosgun, Gustavo Carneiro, Qi Wu, Tat-Jun Chin, Ian Reid, Stephen\n  Gould, Peter Corke, Michael Milford", "title": "Semantics for Robotic Mapping, Perception and Interaction: A Survey", "comments": "81 pages, 1 figure, published in Foundations and Trends in Robotics,\n  2020", "journal-ref": "Foundations and Trends in Robotics: Vol. 8: No. 1-2, pp 1-224\n  (2020)", "doi": "10.1561/2300000059", "report-no": null, "categories": "cs.RO cs.CV cs.HC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For robots to navigate and interact more richly with the world around them,\nthey will likely require a deeper understanding of the world in which they\noperate. In robotics and related research fields, the study of understanding is\noften referred to as semantics, which dictates what does the world \"mean\" to a\nrobot, and is strongly tied to the question of how to represent that meaning.\nWith humans and robots increasingly operating in the same world, the prospects\nof human-robot interaction also bring semantics and ontology of natural\nlanguage into the picture. Driven by need, as well as by enablers like\nincreasing availability of training data and computational resources, semantics\nis a rapidly growing research area in robotics. The field has received\nsignificant attention in the research literature to date, but most reviews and\nsurveys have focused on particular aspects of the topic: the technical research\nissues regarding its use in specific robotic topics like mapping or\nsegmentation, or its relevance to one particular application domain like\nautonomous driving. A new treatment is therefore required, and is also timely\nbecause so much relevant research has occurred since many of the key surveys\nwere published. This survey therefore provides an overarching snapshot of where\nsemantics in robotics stands today. We establish a taxonomy for semantics\nresearch in or relevant to robotics, split into four broad categories of\nactivity, in which semantics are extracted, used, or both. Within these broad\ncategories we survey dozens of major topics including fundamentals from the\ncomputer vision field and key robotics research areas utilizing semantics,\nincluding mapping, navigation and interaction with the world. The survey also\ncovers key practical considerations, including enablers like increased data\navailability and improved computational hardware, and major application areas\nwhere...\n", "versions": [{"version": "v1", "created": "Sat, 2 Jan 2021 12:34:39 GMT"}], "update_date": "2021-01-05", "authors_parsed": [["Garg", "Sourav", ""], ["S\u00fcnderhauf", "Niko", ""], ["Dayoub", "Feras", ""], ["Morrison", "Douglas", ""], ["Cosgun", "Akansel", ""], ["Carneiro", "Gustavo", ""], ["Wu", "Qi", ""], ["Chin", "Tat-Jun", ""], ["Reid", "Ian", ""], ["Gould", "Stephen", ""], ["Corke", "Peter", ""], ["Milford", "Michael", ""]]}, {"id": "2101.00479", "submitter": "Ramesha Karunasena", "authors": "Ramesha Karunasena, Piumi Sandarenu, Madushi Pinto, Achala Athukorala,\n  Ranga Rodrigo, Peshala Jayasekara", "title": "DEVI: Open-source Human-Robot Interface for Interactive Receptionist\n  Systems", "comments": "Published in: 2019 IEEE 4th International Conference on Advanced\n  Robotics and Mechatronics (ICARM)", "journal-ref": null, "doi": "10.1109/ICARM.2019.8834299", "report-no": null, "categories": "cs.RO cs.HC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Humanoid robots that act as human-robot interfaces equipped with social\nskills can assist people in many of their daily activities. Receptionist robots\nare one such application where social skills and appearance are of utmost\nimportance. Many existing robot receptionist systems suffer from high cost and\nthey do not disclose internal architectures for further development for robot\nresearchers. Moreover, there does not exist customizable open-source robot\nreceptionist frameworks to be deployed for any given application. In this paper\nwe present an open-source robot receptionist intelligence core -- \"DEVI\"(means\n'lady' in Sinhala), that provides researchers with ease of creating customized\nrobot receptionists according to the requirements (cost, external appearance,\nand required processing power). Moreover, this paper also presents details on a\nprototype implementation of a physical robot using the DEVI system. The robot\ncan give directional guidance with physical gestures, answer basic queries\nusing a speech recognition and synthesis system, recognize and greet known\npeople using face recognition and register new people in its database, using a\nself-learning neural network. Experiments conducted with DEVI show the\neffectiveness of the proposed system.\n", "versions": [{"version": "v1", "created": "Sat, 2 Jan 2021 17:08:20 GMT"}], "update_date": "2021-01-05", "authors_parsed": [["Karunasena", "Ramesha", ""], ["Sandarenu", "Piumi", ""], ["Pinto", "Madushi", ""], ["Athukorala", "Achala", ""], ["Rodrigo", "Ranga", ""], ["Jayasekara", "Peshala", ""]]}, {"id": "2101.00480", "submitter": "Somya Mohanty", "authors": "Somya D. Mohanty and Brown Biggers and Saed Sayedahmed and Nastaran\n  Pourebrahim and Evan B. Goldstein and Rick Bunch and Guangqing Chi and\n  Fereidoon Sadri and Tom P. McCoy and Arthur Cosby", "title": "A multi-modal approach towards mining social media data during natural\n  disasters -- a case study of Hurricane Irma", "comments": "46 pages, 11 Figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.IR cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Streaming social media provides a real-time glimpse of extreme weather\nimpacts. However, the volume of streaming data makes mining information a\nchallenge for emergency managers, policy makers, and disciplinary scientists.\nHere we explore the effectiveness of data learned approaches to mine and filter\ninformation from streaming social media data from Hurricane Irma's landfall in\nFlorida, USA. We use 54,383 Twitter messages (out of 784K geolocated messages)\nfrom 16,598 users from Sept. 10 - 12, 2017 to develop 4 independent models to\nfilter data for relevance: 1) a geospatial model based on forcing conditions at\nthe place and time of each tweet, 2) an image classification model for tweets\nthat include images, 3) a user model to predict the reliability of the tweeter,\nand 4) a text model to determine if the text is related to Hurricane Irma. All\nfour models are independently tested, and can be combined to quickly filter and\nvisualize tweets based on user-defined thresholds for each submodel. We\nenvision that this type of filtering and visualization routine can be useful as\na base model for data capture from noisy sources such as Twitter. The data can\nthen be subsequently used by policy makers, environmental managers, emergency\nmanagers, and domain scientists interested in finding tweets with specific\nattributes to use during different stages of the disaster (e.g., preparedness,\nresponse, and recovery), or for detailed research.\n", "versions": [{"version": "v1", "created": "Sat, 2 Jan 2021 17:08:53 GMT"}], "update_date": "2021-01-05", "authors_parsed": [["Mohanty", "Somya D.", ""], ["Biggers", "Brown", ""], ["Sayedahmed", "Saed", ""], ["Pourebrahim", "Nastaran", ""], ["Goldstein", "Evan B.", ""], ["Bunch", "Rick", ""], ["Chi", "Guangqing", ""], ["Sadri", "Fereidoon", ""], ["McCoy", "Tom P.", ""], ["Cosby", "Arthur", ""]]}, {"id": "2101.00489", "submitter": "Adriano Pinto", "authors": "Adriano Pinto, S\\'ergio Pereira, Raphael Meier, Roland Wiest, Victor\n  Alves, Mauricio Reyes, Carlos A.Silva", "title": "Combining unsupervised and supervised learning for predicting the final\n  stroke lesion", "comments": "Accepted at Medical Image Analysis (MedIA)", "journal-ref": null, "doi": "10.1016/j.media.2020.101888", "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Predicting the final ischaemic stroke lesion provides crucial information\nregarding the volume of salvageable hypoperfused tissue, which helps physicians\nin the difficult decision-making process of treatment planning and\nintervention. Treatment selection is influenced by clinical diagnosis, which\nrequires delineating the stroke lesion, as well as characterising cerebral\nblood flow dynamics using neuroimaging acquisitions. Nonetheless, predicting\nthe final stroke lesion is an intricate task, due to the variability in lesion\nsize, shape, location and the underlying cerebral haemodynamic processes that\noccur after the ischaemic stroke takes place. Moreover, since elapsed time\nbetween stroke and treatment is related to the loss of brain tissue, assessing\nand predicting the final stroke lesion needs to be performed in a short period\nof time, which makes the task even more complex. Therefore, there is a need for\nautomatic methods that predict the final stroke lesion and support physicians\nin the treatment decision process. We propose a fully automatic deep learning\nmethod based on unsupervised and supervised learning to predict the final\nstroke lesion after 90 days. Our aim is to predict the final stroke lesion\nlocation and extent, taking into account the underlying cerebral blood flow\ndynamics that can influence the prediction. To achieve this, we propose a\ntwo-branch Restricted Boltzmann Machine, which provides specialized data-driven\nfeatures from different sets of standard parametric Magnetic Resonance Imaging\nmaps. These data-driven feature maps are then combined with the parametric\nMagnetic Resonance Imaging maps, and fed to a Convolutional and Recurrent\nNeural Network architecture. We evaluated our proposal on the publicly\navailable ISLES 2017 testing dataset, reaching a Dice score of 0.38, Hausdorff\nDistance of 29.21 mm, and Average Symmetric Surface Distance of 5.52 mm.\n", "versions": [{"version": "v1", "created": "Sat, 2 Jan 2021 17:56:47 GMT"}], "update_date": "2021-01-05", "authors_parsed": [["Pinto", "Adriano", ""], ["Pereira", "S\u00e9rgio", ""], ["Meier", "Raphael", ""], ["Wiest", "Roland", ""], ["Alves", "Victor", ""], ["Reyes", "Mauricio", ""], ["Silva", "Carlos A.", ""]]}, {"id": "2101.00494", "submitter": "Lin Yang", "authors": "Minbo Gao, Tianle Xie, Simon S. Du, Lin F. Yang", "title": "A Provably Efficient Algorithm for Linear Markov Decision Process with\n  Low Switching Cost", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many real-world applications, such as those in medical domains,\nrecommendation systems, etc, can be formulated as large state space\nreinforcement learning problems with only a small budget of the number of\npolicy changes, i.e., low switching cost. This paper focuses on the linear\nMarkov Decision Process (MDP) recently studied in [Yang et al 2019, Jin et al\n2020] where the linear function approximation is used for generalization on the\nlarge state space. We present the first algorithm for linear MDP with a low\nswitching cost. Our algorithm achieves an\n$\\widetilde{O}\\left(\\sqrt{d^3H^4K}\\right)$ regret bound with a near-optimal\n$O\\left(d H\\log K\\right)$ global switching cost where $d$ is the feature\ndimension, $H$ is the planning horizon and $K$ is the number of episodes the\nagent plays. Our regret bound matches the best existing polynomial algorithm by\n[Jin et al 2020] and our switching cost is exponentially smaller than theirs.\nWhen specialized to tabular MDP, our switching cost bound improves those in\n[Bai et al 2019, Zhang et al 20020]. We complement our positive result with an\n$\\Omega\\left(dH/\\log d\\right)$ global switching cost lower bound for any\nno-regret algorithm.\n", "versions": [{"version": "v1", "created": "Sat, 2 Jan 2021 18:41:27 GMT"}], "update_date": "2021-01-05", "authors_parsed": [["Gao", "Minbo", ""], ["Xie", "Tianle", ""], ["Du", "Simon S.", ""], ["Yang", "Lin F.", ""]]}, {"id": "2101.00503", "submitter": "Florian Klimm", "authors": "Florian Klimm, Nick S. Jones and Michael T. Schaub", "title": "Modularity maximisation for graphons", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO cs.LG cs.SI nlin.AO physics.soc-ph", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Networks are a widely-used tool to investigate the large-scale connectivity\nstructure in complex systems and graphons have been proposed as an infinite\nsize limit of dense networks. The detection of communities or other meso-scale\nstructures is a prominent topic in network science as it allows the\nidentification of functional building blocks in complex systems. When such\nbuilding blocks may be present in graphons is an open question. In this paper,\nwe define a graphon-modularity and demonstrate that it can be maximised to\ndetect communities in graphons. We then investigate specific synthetic graphons\nand show that they may show a wide range of different community structures. We\nalso reformulate the graphon-modularity maximisation as a continuous\noptimisation problem and so prove the optimal community structure or lack\nthereof for some graphons, something that is usually not possible for networks.\nFurthermore, we demonstrate that estimating a graphon from network data as an\nintermediate step can improve the detection of communities, in comparison with\nexclusively maximising the modularity of the network. While the choice of\ngraphon-estimator may strongly influence the accord between the community\nstructure of a network and its estimated graphon, we find that there is a\nsubstantial overlap if an appropriate estimator is used. Our study demonstrates\nthat community detection for graphons is possible and may serve as a\nprivacy-preserving way to cluster network data.\n", "versions": [{"version": "v1", "created": "Sat, 2 Jan 2021 19:44:44 GMT"}], "update_date": "2021-01-05", "authors_parsed": [["Klimm", "Florian", ""], ["Jones", "Nick S.", ""], ["Schaub", "Michael T.", ""]]}, {"id": "2101.00509", "submitter": "Benjamin Maschler", "authors": "Benjamin Maschler, Thi Thu Huong Pham, Michael Weyrich", "title": "Regularization-based Continual Learning for Anomaly Detection in\n  Discrete Manufacturing", "comments": "6 pages, 5 figures, 3 tables, submitted to the CIRP Conference on\n  Manufacturing Systems 2021", "journal-ref": null, "doi": "10.13140/RG.2.2.15631.00163", "report-no": null, "categories": "cs.LG cs.AI cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The early and robust detection of anomalies occurring in discrete\nmanufacturing processes allows operators to prevent harm, e.g. defects in\nproduction machinery or products. While current approaches for data-driven\nanomaly detection provide good results on the exact processes they were trained\non, they often lack the ability to flexibly adapt to changes, e.g. in products.\nContinual learning promises such flexibility, allowing for an automatic\nadaption of previously learnt knowledge to new tasks. Therefore, this article\ndiscusses different continual learning approaches from the group of\nregularization strategies, which are implemented, evaluated and compared based\non a real industrial metal forming dataset.\n", "versions": [{"version": "v1", "created": "Sat, 2 Jan 2021 20:06:00 GMT"}], "update_date": "2021-01-05", "authors_parsed": [["Maschler", "Benjamin", ""], ["Pham", "Thi Thu Huong", ""], ["Weyrich", "Michael", ""]]}, {"id": "2101.00522", "submitter": "Mohammad Rostami", "authors": "Serban Stan, Mohammad Rostami", "title": "Privacy Preserving Domain Adaptation for Semantic Segmentation of\n  Medical Images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CR cs.LG eess.IV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Convolutional neural networks (CNNs) have led to significant improvements in\ntasks involving semantic segmentation of images. CNNs are vulnerable in the\narea of biomedical image segmentation because of distributional gap between two\nsource and target domains with different data modalities which leads to domain\nshift. Domain shift makes data annotations in new modalities necessary because\nmodels must be retrained from scratch. Unsupervised domain adaptation (UDA) is\nproposed to adapt a model to new modalities using solely unlabeled target\ndomain data. Common UDA algorithms require access to data points in the source\ndomain which may not be feasible in medical imaging due to privacy concerns. In\nthis work, we develop an algorithm for UDA in a privacy-constrained setting,\nwhere the source domain data is inaccessible. Our idea is based on encoding the\ninformation from the source samples into a prototypical distribution that is\nused as an intermediate distribution for aligning the target domain\ndistribution with the source domain distribution. We demonstrate the\neffectiveness of our algorithm by comparing it to state-of-the-art medical\nimage semantic segmentation approaches on two medical image semantic\nsegmentation datasets.\n", "versions": [{"version": "v1", "created": "Sat, 2 Jan 2021 22:12:42 GMT"}, {"version": "v2", "created": "Sun, 11 Jul 2021 02:58:49 GMT"}], "update_date": "2021-07-13", "authors_parsed": [["Stan", "Serban", ""], ["Rostami", "Mohammad", ""]]}, {"id": "2101.00529", "submitter": "Pengchuan Zhang", "authors": "Pengchuan Zhang, Xiujun Li, Xiaowei Hu, Jianwei Yang, Lei Zhang,\n  Lijuan Wang, Yejin Choi, Jianfeng Gao", "title": "VinVL: Revisiting Visual Representations in Vision-Language Models", "comments": null, "journal-ref": "CVPR 2021", "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a detailed study of improving visual representations for\nvision language (VL) tasks and develops an improved object detection model to\nprovide object-centric representations of images. Compared to the most widely\nused \\emph{bottom-up and top-down} model \\cite{anderson2018bottom}, the new\nmodel is bigger, better-designed for VL tasks, and pre-trained on much larger\ntraining corpora that combine multiple public annotated object detection\ndatasets. Therefore, it can generate representations of a richer collection of\nvisual objects and concepts. While previous VL research focuses mainly on\nimproving the vision-language fusion model and leaves the object detection\nmodel improvement untouched, we show that visual features matter significantly\nin VL models. In our experiments we feed the visual features generated by the\nnew object detection model into a Transformer-based VL fusion model \\oscar\n\\cite{li2020oscar}, and utilize an improved approach \\short\\ to pre-train the\nVL model and fine-tune it on a wide range of downstream VL tasks. Our results\nshow that the new visual features significantly improve the performance across\nall VL tasks, creating new state-of-the-art results on seven public benchmarks.\nWe will release the new object detection model to public.\n", "versions": [{"version": "v1", "created": "Sat, 2 Jan 2021 23:35:27 GMT"}, {"version": "v2", "created": "Wed, 10 Mar 2021 01:27:16 GMT"}], "update_date": "2021-03-11", "authors_parsed": [["Zhang", "Pengchuan", ""], ["Li", "Xiujun", ""], ["Hu", "Xiaowei", ""], ["Yang", "Jianwei", ""], ["Zhang", "Lei", ""], ["Wang", "Lijuan", ""], ["Choi", "Yejin", ""], ["Gao", "Jianfeng", ""]]}, {"id": "2101.00531", "submitter": "Baiming Chen", "authors": "Baiming Chen, Zuxin Liu, Jiacheng Zhu, Mengdi Xu, Wenhao Ding, Ding\n  Zhao", "title": "Context-Aware Safe Reinforcement Learning for Non-Stationary\n  Environments", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Safety is a critical concern when deploying reinforcement learning agents for\nrealistic tasks. Recently, safe reinforcement learning algorithms have been\ndeveloped to optimize the agent's performance while avoiding violations of\nsafety constraints. However, few studies have addressed the non-stationary\ndisturbances in the environments, which may cause catastrophic outcomes. In\nthis paper, we propose the context-aware safe reinforcement learning (CASRL)\nmethod, a meta-learning framework to realize safe adaptation in non-stationary\nenvironments. We use a probabilistic latent variable model to achieve fast\ninference of the posterior environment transition distribution given the\ncontext data. Safety constraints are then evaluated with uncertainty-aware\ntrajectory sampling. The high cost of safety violations leads to the rareness\nof unsafe records in the dataset. We address this issue by enabling prioritized\nsampling during model training and formulating prior safety constraints with\ndomain knowledge during constrained planning. The algorithm is evaluated in\nrealistic safety-critical environments with non-stationary disturbances.\nResults show that the proposed algorithm significantly outperforms existing\nbaselines in terms of safety and robustness.\n", "versions": [{"version": "v1", "created": "Sat, 2 Jan 2021 23:52:22 GMT"}], "update_date": "2021-01-05", "authors_parsed": [["Chen", "Baiming", ""], ["Liu", "Zuxin", ""], ["Zhu", "Jiacheng", ""], ["Xu", "Mengdi", ""], ["Ding", "Wenhao", ""], ["Zhao", "Ding", ""]]}, {"id": "2101.00554", "submitter": "Kai Fukami", "authors": "Kai Fukami, Romit Maulik, Nesar Ramachandra, Koji Fukagata, and\n  Kunihiko Taira", "title": "Global field reconstruction from sparse sensors with Voronoi\n  tessellation-assisted deep learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.flu-dyn cs.LG physics.comp-ph", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Achieving accurate and robust global situational awareness of a complex\ntime-evolving field from a limited number of sensors has been a longstanding\nchallenge. This reconstruction problem is especially difficult when sensors are\nsparsely positioned in a seemingly random or unorganized manner, which is often\nencountered in a range of scientific and engineering problems. Moreover, these\nsensors can be in motion and can become online or offline over time. The key\nleverage in addressing this scientific issue is the wealth of data accumulated\nfrom the sensors. As a solution to this problem, we propose a data-driven\nspatial field recovery technique founded on a structured grid-based\ndeep-learning approach for arbitrary positioned sensors of any numbers. It\nshould be noted that the na\\\"ive use of machine learning becomes prohibitively\nexpensive for global field reconstruction and is furthermore not adaptable to\nan arbitrary number of sensors. In the present work, we consider the use of\nVoronoi tessellation to obtain a structured-grid representation from sensor\nlocations enabling the computationally tractable use of convolutional neural\nnetworks. One of the central features of the present method is its\ncompatibility with deep-learning based super-resolution reconstruction\ntechniques for structured sensor data that are established for image\nprocessing. The proposed reconstruction technique is demonstrated for unsteady\nwake flow, geophysical data, and three-dimensional turbulence. The current\nframework is able to handle an arbitrary number of moving sensors, and thereby\novercomes a major limitation with existing reconstruction methods. The\npresented technique opens a new pathway towards the practical use of neural\nnetworks for real-time global field estimation.\n", "versions": [{"version": "v1", "created": "Sun, 3 Jan 2021 03:43:53 GMT"}, {"version": "v2", "created": "Thu, 22 Jul 2021 09:34:11 GMT"}], "update_date": "2021-07-23", "authors_parsed": [["Fukami", "Kai", ""], ["Maulik", "Romit", ""], ["Ramachandra", "Nesar", ""], ["Fukagata", "Koji", ""], ["Taira", "Kunihiko", ""]]}, {"id": "2101.00557", "submitter": "Sairam Sri Vatsavai", "authors": "Sairam Sri Vatsavai, Ishan Thakkar", "title": "Silicon Photonic Microring Based Chip-Scale Accelerator for Delayed\n  Feedback Reservoir Computing", "comments": "Paper accepted at VLSID 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR cs.ET cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To perform temporal and sequential machine learning tasks, the use of\nconventional Recurrent Neural Networks (RNNs) has been dwindling due to the\ntraining complexities of RNNs. To this end, accelerators for delayed feedback\nreservoir computing (DFRC) have attracted attention in lieu of RNNs, due to\ntheir simple hardware implementations. A typical implementation of a DFRC\naccelerator consists of a delay loop and a single nonlinear neuron, together\nacting as multiple virtual nodes for computing. In prior work, photonic DFRC\naccelerators have shown an undisputed advantage of fast computation over their\nelectronic counterparts. In this paper, we propose a more energy-efficient\nchip-scale DFRC accelerator that employs a silicon photonic microring (MR)\nbased nonlinear neuron along with on-chip photonic waveguides-based delayed\nfeedback loop. Our evaluations show that, compared to a well-known photonic\nDFRC accelerator from prior work, our proposed MR-based DFRC accelerator\nachieves 35% and 98.7% lower normalized root mean square error (NRMSE),\nrespectively, for the prediction tasks of NARMA10 and Santa Fe time series. In\naddition, our MR-based DFRC accelerator achieves 58.8% lower symbol error rate\n(SER) for the Non-Linear Channel Equalization task. Moreover, our MR-based DFRC\naccelerator has 98% and 93% faster training time, respectively, compared to an\nelectronic and a photonic DFRC accelerators from prior work.\n", "versions": [{"version": "v1", "created": "Sun, 3 Jan 2021 04:13:30 GMT"}], "update_date": "2021-01-05", "authors_parsed": [["Vatsavai", "Sairam Sri", ""], ["Thakkar", "Ishan", ""]]}, {"id": "2101.00562", "submitter": "Arkabandhu Chowdhury", "authors": "Arkabandhu Chowdhury, Mingchao Jiang, Chris Jermaine", "title": "Few-shot Image Classification: Just Use a Library of Pre-trained Feature\n  Extractors and a Simple Classifier", "comments": "17 pages including appendix and references. 2 figures in the main\n  paper, 1 figure in appendix. Under submission at a conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent papers have suggested that transfer learning can outperform\nsophisticated meta-learning methods for few-shot image classification. We take\nthis hypothesis to its logical conclusion, and suggest the use of an ensemble\nof high-quality, pre-trained feature extractors for few-shot image\nclassification. We show experimentally that a library of pre-trained feature\nextractors combined with a simple feed-forward network learned with an\nL2-regularizer can be an excellent option for solving cross-domain few-shot\nimage classification. Our experimental results suggest that this simpler\nsample-efficient approach far outperforms several well-established\nmeta-learning algorithms on a variety of few-shot tasks.\n", "versions": [{"version": "v1", "created": "Sun, 3 Jan 2021 05:30:36 GMT"}], "update_date": "2021-01-05", "authors_parsed": [["Chowdhury", "Arkabandhu", ""], ["Jiang", "Mingchao", ""], ["Jermaine", "Chris", ""]]}, {"id": "2101.00563", "submitter": "Sahil Sidheekh", "authors": "Sahil Sidheekh", "title": "Learning Neural Networks on SVD Boosted Latent Spaces for Semantic\n  Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The availability of large amounts of data and compelling computation power\nhave made deep learning models much popular for text classification and\nsentiment analysis. Deep neural networks have achieved competitive performance\non the above tasks when trained on naive text representations such as word\ncount, term frequency, and binary matrix embeddings. However, many of the above\nrepresentations result in the input space having a dimension of the order of\nthe vocabulary size, which is enormous. This leads to a blow-up in the number\nof parameters to be learned, and the computational cost becomes infeasible when\nscaling to domains that require retaining a colossal vocabulary. This work\nproposes using singular value decomposition to transform the high dimensional\ninput space to a lower-dimensional latent space. We show that neural networks\ntrained on this lower-dimensional space are not only able to retain performance\nwhile savoring significant reduction in the computational complexity but, in\nmany situations, also outperforms the classical neural networks trained on the\nnative input space.\n", "versions": [{"version": "v1", "created": "Sun, 3 Jan 2021 05:30:37 GMT"}], "update_date": "2021-01-05", "authors_parsed": [["Sidheekh", "Sahil", ""]]}, {"id": "2101.00574", "submitter": "Amir Zadeh", "authors": "Amir Zadeh, Santiago Benoit, Louis-Philippe Morency", "title": "StarNet: Gradient-free Training of Deep Generative Models using\n  Determined System of Linear Equations", "comments": "Work in progress at CMU", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper we present an approach for training deep generative models\nsolely based on solving determined systems of linear equations. A network that\nuses this approach, called a StarNet, has the following desirable properties:\n1) training requires no gradient as solution to the system of linear equations\nis not stochastic, 2) is highly scalable when solving the system of linear\nequations w.r.t the latent codes, and similarly for the parameters of the\nmodel, and 3) it gives desirable least-square bounds for the estimation of\nlatent codes and network parameters within each layer.\n", "versions": [{"version": "v1", "created": "Sun, 3 Jan 2021 08:06:42 GMT"}], "update_date": "2021-01-08", "authors_parsed": [["Zadeh", "Amir", ""], ["Benoit", "Santiago", ""], ["Morency", "Louis-Philippe", ""]]}, {"id": "2101.00575", "submitter": "Nimrod Segol", "authors": "Nimrod Segol, Boaz Nadler", "title": "Improved Convergence Guarantees for Learning Gaussian Mixture Models by\n  EM and Gradient EM", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of estimating the parameters a Gaussian Mixture Model\nwith K components of known weights, all with an identity covariance matrix. We\nmake two contributions. First, at the population level, we present a sharper\nanalysis of the local convergence of EM and gradient EM, compared to previous\nworks. Assuming a separation of $\\Omega(\\sqrt{\\log K})$, we prove convergence\nof both methods to the global optima from an initialization region larger than\nthose of previous works. Specifically, the initial guess of each component can\nbe as far as (almost) half its distance to the nearest Gaussian. This is\nessentially the largest possible contraction region. Our second contribution\nare improved sample size requirements for accurate estimation by EM and\ngradient EM. In previous works, the required number of samples had a quadratic\ndependence on the maximal separation between the K components, and the\nresulting error estimate increased linearly with this maximal separation. In\nthis manuscript we show that both quantities depend only logarithmically on the\nmaximal separation.\n", "versions": [{"version": "v1", "created": "Sun, 3 Jan 2021 08:10:01 GMT"}], "update_date": "2021-01-05", "authors_parsed": [["Segol", "Nimrod", ""], ["Nadler", "Boaz", ""]]}, {"id": "2101.00576", "submitter": "Nick James", "authors": "Nick James", "title": "Dynamics, behaviours, and anomaly persistence in cryptocurrencies and\n  equities surrounding COVID-19", "comments": "As published in Physica A", "journal-ref": "Physica A: Statistical Mechanics and its Applications 570 (2021)\n  125831", "doi": "10.1016/j.physa.2021.125831", "report-no": null, "categories": "q-fin.ST cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper uses new and recently introduced methodologies to study the\nsimilarity in the dynamics and behaviours of cryptocurrencies and equities\nsurrounding the COVID-19 pandemic. We study two collections; 45\ncryptocurrencies and 72 equities, both independently and in conjunction. First,\nwe examine the evolution of cryptocurrency and equity market dynamics, with a\nparticular focus on their change during the COVID-19 pandemic. We demonstrate\nmarkedly more similar dynamics during times of crisis. Next, we apply recently\nintroduced methods to contrast trajectories, erratic behaviours, and extreme\nvalues among the two multivariate time series. Finally, we introduce a new\nframework for determining the persistence of market anomalies over time.\nSurprisingly, we find that although cryptocurrencies exhibit stronger\ncollective dynamics and correlation in all market conditions, equities behave\nmore similarly in their trajectories, extremes, and show greater persistence in\nanomalies over time.\n", "versions": [{"version": "v1", "created": "Sun, 3 Jan 2021 08:12:17 GMT"}, {"version": "v2", "created": "Sun, 10 Jan 2021 22:52:01 GMT"}, {"version": "v3", "created": "Thu, 18 Feb 2021 07:42:56 GMT"}], "update_date": "2021-07-30", "authors_parsed": [["James", "Nick", ""]]}, {"id": "2101.00582", "submitter": "Ninrui Zhao", "authors": "Ningrui Zhao, Jinwei Lu", "title": "Neural network algorithm and its application in temperature control of\n  distillation tower", "comments": "19 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.chem-ph cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Distillation process is a complex process of conduction, mass transfer and\nheat conduction, which is mainly manifested as follows: The mechanism is\ncomplex and changeable with uncertainty; the process is multivariate and strong\ncoupling; the system is nonlinear, hysteresis and time-varying. Neural networks\ncan perform effective learning based on corresponding samples, do not rely on\nfixed mechanisms, have the ability to approximate arbitrary nonlinear mappings,\nand can be used to establish system input and output models. The temperature\nsystem of the rectification tower has a complicated structure and high accuracy\nrequirements. The neural network is used to control the temperature of the\nsystem, which satisfies the requirements of the production process. This\narticle briefly describes the basic concepts and research progress of neural\nnetwork and distillation tower temperature control, and systematically\nsummarizes the application of neural network in distillation tower control,\naiming to provide reference for the development of related industries.\n", "versions": [{"version": "v1", "created": "Sun, 3 Jan 2021 08:33:05 GMT"}], "update_date": "2021-01-05", "authors_parsed": [["Zhao", "Ningrui", ""], ["Lu", "Jinwei", ""]]}, {"id": "2101.00583", "submitter": "Lihi Dery", "authors": "Lihi Dery", "title": "Multi-label Ranking: Mining Multi-label and Label Ranking Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We survey multi-label ranking tasks, specifically multi-label classification\nand label ranking classification. We highlight the unique challenges, and\nre-categorize the methods, as they no longer fit into the traditional\ncategories of transformation and adaptation. We survey developments in the last\ndemi-decade, with a special focus on state-of-the-art methods in deep learning\nmulti-label mining, extreme multi-label classification and label ranking. We\nconclude by offering a few future research directions.\n", "versions": [{"version": "v1", "created": "Sun, 3 Jan 2021 08:36:45 GMT"}], "update_date": "2021-01-05", "authors_parsed": [["Dery", "Lihi", ""]]}, {"id": "2101.00598", "submitter": "Sanket Kamthe", "authors": "Sanket Kamthe, Samuel Assefa, Marc Deisenroth", "title": "Copula Flows for Synthetic Data Generation", "comments": "Working paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.AP", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  The ability to generate high-fidelity synthetic data is crucial when\navailable (real) data is limited or where privacy and data protection standards\nallow only for limited use of the given data, e.g., in medical and financial\ndata-sets. Current state-of-the-art methods for synthetic data generation are\nbased on generative models, such as Generative Adversarial Networks (GANs).\nEven though GANs have achieved remarkable results in synthetic data generation,\nthey are often challenging to interpret.Furthermore, GAN-based methods can\nsuffer when used with mixed real and categorical variables.Moreover, loss\nfunction (discriminator loss) design itself is problem specific, i.e., the\ngenerative model may not be useful for tasks it was not explicitly trained for.\nIn this paper, we propose to use a probabilistic model as a synthetic data\ngenerator. Learning the probabilistic model for the data is equivalent to\nestimating the density of the data. Based on the copula theory, we divide the\ndensity estimation task into two parts, i.e., estimating univariate marginals\nand estimating the multivariate copula density over the univariate marginals.\nWe use normalising flows to learn both the copula density and univariate\nmarginals. We benchmark our method on both simulated and real data-sets in\nterms of density estimation as well as the ability to generate high-fidelity\nsynthetic data\n", "versions": [{"version": "v1", "created": "Sun, 3 Jan 2021 10:06:23 GMT"}], "update_date": "2021-01-05", "authors_parsed": [["Kamthe", "Sanket", ""], ["Assefa", "Samuel", ""], ["Deisenroth", "Marc", ""]]}, {"id": "2101.00646", "submitter": "Tong Xia", "authors": "Tong Xia and Yunhan Qi and Jie Feng and Fengli Xu and Funing Sun and\n  Diansheng Guo and Yong Li", "title": "AttnMove: History Enhanced Trajectory Recovery via Attentional Network", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A considerable amount of mobility data has been accumulated due to the\nproliferation of location-based service. Nevertheless, compared with mobility\ndata from transportation systems like the GPS module in taxis, this kind of\ndata is commonly sparse in terms of individual trajectories in the sense that\nusers do not access mobile services and contribute their data all the time.\nConsequently, the sparsity inevitably weakens the practical value of the data\neven it has a high user penetration rate. To solve this problem, we propose a\nnovel attentional neural network-based model, named AttnMove, to densify\nindividual trajectories by recovering unobserved locations at a fine-grained\nspatial-temporal resolution. To tackle the challenges posed by sparsity, we\ndesign various intra- and inter- trajectory attention mechanisms to better\nmodel the mobility regularity of users and fully exploit the periodical pattern\nfrom long-term history. We evaluate our model on two real-world datasets, and\nextensive results demonstrate the performance gain compared with the\nstate-of-the-art methods. This also shows that, by providing high-quality\nmobility data, our model can benefit a variety of mobility-oriented down-stream\napplications.\n", "versions": [{"version": "v1", "created": "Sun, 3 Jan 2021 15:45:35 GMT"}], "update_date": "2021-01-05", "authors_parsed": [["Xia", "Tong", ""], ["Qi", "Yunhan", ""], ["Feng", "Jie", ""], ["Xu", "Fengli", ""], ["Sun", "Funing", ""], ["Guo", "Diansheng", ""], ["Li", "Yong", ""]]}, {"id": "2101.00650", "submitter": "Songting Shi", "authors": "Songting Shi", "title": "A Tutorial on the Mathematical Model of Single Cell Variational\n  Inference", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.OT cs.LG q-bio.GN stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  As the large amount of sequencing data accumulated in past decades and it is\nstill accumulating, we need to handle the more and more sequencing data. As the\nfast development of the computing technologies, we now can handle a large\namount of data by a reasonable of time using the neural network based model.\nThis tutorial will introduce the the mathematical model of the single cell\nvariational inference (scVI), which use the variational auto-encoder (building\non the neural networks) to learn the distribution of the data to gain insights.\nIt was written for beginners in the simple and intuitive way with many\ndeduction details to encourage more researchers into this field.\n", "versions": [{"version": "v1", "created": "Sun, 3 Jan 2021 16:02:36 GMT"}], "update_date": "2021-01-05", "authors_parsed": [["Shi", "Songting", ""]]}, {"id": "2101.00661", "submitter": "David R\\\"ugamer", "authors": "Cornelius Fritz, Emilio Dorigatti, David R\\\"ugamer", "title": "Combining Graph Neural Networks and Spatio-temporal Disease Models to\n  Predict COVID-19 Cases in Germany", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  During 2020, the infection rate of COVID-19 has been investigated by many\nscholars from different research fields. In this context, reliable and\ninterpretable forecasts of disease incidents are a vital tool for policymakers\nto manage healthcare resources. Several experts have called for the necessity\nto account for human mobility to explain the spread of COVID-19. Existing\napproaches are often applying standard models of the respective research field.\nThis habit, however, often comes along with certain restrictions. For instance,\nmost statistical or epidemiological models cannot directly incorporate\nunstructured data sources, including relational data that may encode human\nmobility. In contrast, machine learning approaches may yield better predictions\nby exploiting these data structures, yet lack intuitive interpretability as\nthey are often categorized as black-box models. We propose a trade-off between\nboth research directions and present a multimodal learning approach that\ncombines the advantages of statistical regression and machine learning models\nfor predicting local COVID-19 cases in Germany. This novel approach enables the\nuse of a richer collection of data types, including mobility flows and\ncolocation probabilities, and yields the lowest MSE scores throughout our\nobservational period in our benchmark study. The results corroborate the\nnecessity of including mobility data and showcase the flexibility and\ninterpretability of our approach.\n", "versions": [{"version": "v1", "created": "Sun, 3 Jan 2021 16:39:00 GMT"}], "update_date": "2021-01-05", "authors_parsed": [["Fritz", "Cornelius", ""], ["Dorigatti", "Emilio", ""], ["R\u00fcgamer", "David", ""]]}, {"id": "2101.00672", "submitter": "Ozan Kayaalp", "authors": "Ozan Kaan Kayaalp", "title": "Learning optimal Bayesian prior probabilities from data", "comments": "25 pages, 10 figures, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Noninformative uniform priors are staples of Bayesian inference, especially\nin Bayesian machine learning. This study challenges the assumption that they\nare optimal and their use in Bayesian inference yields optimal outcomes.\nInstead of using arbitrary noninformative uniform priors, we propose a machine\nlearning based alternative method, learning optimal priors from data by\nmaximizing a target function of interest. Applying na\\\"ive Bayes text\nclassification methodology and a search algorithm developed for this study, our\nsystem learned priors from data using the positive predictive value metric as\nthe target function. The task was to find Wikipedia articles that had not (but\nshould have) been categorized under certain Wikipedia categories. We conducted\nfive sets of experiments using separate Wikipedia categories. While the\nbaseline models used the popular Bayes-Laplace priors, the study models learned\nthe optimal priors for each set of experiments separately before using them.\nThe results showed that the study models consistently outperformed the baseline\nmodels with a wide margin of statistical significance (p < 0.001). The measured\nperformance improvement of the study model over the baseline was as high as\n443% with the mean value of 193% over five Wikipedia categories.\n", "versions": [{"version": "v1", "created": "Sun, 3 Jan 2021 17:38:08 GMT"}], "update_date": "2021-01-05", "authors_parsed": [["Kayaalp", "Ozan Kaan", ""]]}, {"id": "2101.00686", "submitter": "M. F. Mridha", "authors": "Md. Mohsin Kabir, Abu Quwsar Ohi, Md. Saifur Rahman, M. F. Mridha", "title": "An Evolution of CNN Object Classifiers on Low-Resolution Images", "comments": "Accepted in IEEE Honet 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Object classification is a significant task in computer vision. It has become\nan effective research area as an important aspect of image processing and the\nbuilding block of image localization, detection, and scene parsing. Object\nclassification from low-quality images is difficult for the variance of object\ncolors, aspect ratios, and cluttered backgrounds. The field of object\nclassification has seen remarkable advancements, with the development of deep\nconvolutional neural networks (DCNNs). Deep neural networks have been\ndemonstrated as very powerful systems for facing the challenge of object\nclassification from high-resolution images, but deploying such object\nclassification networks on the embedded device remains challenging due to the\nhigh computational and memory requirements. Using high-quality images often\ncauses high computational and memory complexity, whereas low-quality images can\nsolve this issue. Hence, in this paper, we investigate an optimal architecture\nthat accurately classifies low-quality images using DCNNs architectures. To\nvalidate different baselines on lowquality images, we perform experiments using\nwebcam captured image datasets of 10 different objects. In this research work,\nwe evaluate the proposed architecture by implementing popular CNN\narchitectures. The experimental results validate that the MobileNet\narchitecture delivers better than most of the available CNN architectures for\nlow-resolution webcam image datasets.\n", "versions": [{"version": "v1", "created": "Sun, 3 Jan 2021 18:44:23 GMT"}], "update_date": "2021-01-05", "authors_parsed": [["Kabir", "Md. Mohsin", ""], ["Ohi", "Abu Quwsar", ""], ["Rahman", "Md. Saifur", ""], ["Mridha", "M. F.", ""]]}, {"id": "2101.00687", "submitter": "Joberto Martins Prof. Dr.", "authors": "Carlos E. Arruda, Pedro F. Moraes, Nazim Agoulmine, Joberto S. B.\n  Martins", "title": "Enhanced Pub/Sub Communications for Massive IoT Traffic with SARSA\n  Reinforcement Learning", "comments": "3rd International Conference on Machine Learning for Networking - MLN\n  2020, Paris, 20 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG cs.NI", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Sensors are being extensively deployed and are expected to expand at\nsignificant rates in the coming years. They typically generate a large volume\nof data on the internet of things (IoT) application areas like smart cities,\nintelligent traffic systems, smart grid, and e-health. Cloud, edge and fog\ncomputing are potential and competitive strategies for collecting, processing,\nand distributing IoT data. However, cloud, edge, and fog-based solutions need\nto tackle the distribution of a high volume of IoT data efficiently through\nconstrained and limited resource network infrastructures. This paper addresses\nthe issue of conveying a massive volume of IoT data through a network with\nlimited communications resources (bandwidth) using a cognitive communications\nresource allocation based on Reinforcement Learning (RL) with SARSA algorithm.\nThe proposed network infrastructure (PSIoTRL) uses a Publish/ Subscribe\narchitecture to access massive and highly distributed IoT data. It is\ndemonstrated that the PSIoTRL bandwidth allocation for buffer flushing based on\nSARSA enhances the IoT aggregator buffer occupation and network link\nutilization. The PSIoTRL dynamically adapts the IoT aggregator traffic flushing\naccording to the Pub/Sub topic's priority and network constraint requirements.\n", "versions": [{"version": "v1", "created": "Sun, 3 Jan 2021 18:46:01 GMT"}], "update_date": "2021-01-05", "authors_parsed": [["Arruda", "Carlos E.", ""], ["Moraes", "Pedro F.", ""], ["Agoulmine", "Nazim", ""], ["Martins", "Joberto S. B.", ""]]}, {"id": "2101.00688", "submitter": "Maxwell Libbrecht", "authors": "Maxwell W Libbrecht, Rachel CW Chan, Michael M Hoffman", "title": "Segmentation and genome annotation algorithms", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.GN cs.LG", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Segmentation and genome annotation (SAGA) algorithms are widely used to\nunderstand genome activity and gene regulation. These algorithms take as input\nepigenomic datasets, such as chromatin immunoprecipitation-sequencing\n(ChIP-seq) measurements of histone modifications or transcription factor\nbinding. They partition the genome and assign a label to each segment such that\npositions with the same label exhibit similar patterns of input data. SAGA\nalgorithms discover categories of activity such as promoters, enhancers, or\nparts of genes without prior knowledge of known genomic elements. In this\nsense, they generally act in an unsupervised fashion like clustering\nalgorithms, but with the additional simultaneous function of segmenting the\ngenome. Here, we review the common methodological framework that underlies\nthese methods, review variants of and improvements upon this basic framework,\ncatalogue existing large-scale reference annotations, and discuss the outlook\nfor future work.\n", "versions": [{"version": "v1", "created": "Sun, 3 Jan 2021 19:08:36 GMT"}], "update_date": "2021-01-05", "authors_parsed": [["Libbrecht", "Maxwell W", ""], ["Chan", "Rachel CW", ""], ["Hoffman", "Michael M", ""]]}, {"id": "2101.00693", "submitter": "Rakesh Dhakhsina Murthy", "authors": "Rakesh Dhakshinamurthy", "title": "Neural Networks for Keyword Spotting on IoT Devices", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We explore Neural Networks (NNs) for keyword spotting (KWS) on IoT devices\nlike smart speakers and wearables. Since we target to execute our NN on a\nconstrained memory and computation footprint, we propose a CNN design that. (i)\nuses a limited number of multiplies. (ii) uses a limited number of model\nparameters.\n", "versions": [{"version": "v1", "created": "Sun, 3 Jan 2021 19:57:45 GMT"}], "update_date": "2021-01-05", "authors_parsed": [["Dhakshinamurthy", "Rakesh", ""]]}, {"id": "2101.00698", "submitter": "Omer Bobrowski", "authors": "Yohai Reani, Omer Bobrowski", "title": "Cycle Registration in Persistent Homology with Applications in\n  Topological Bootstrap", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CG math.AT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this article we propose a novel approach for comparing the persistent\nhomology representations of two spaces (filtrations). Commonly used methods are\nbased on numerical summaries such as persistence diagrams and persistence\nlandscapes, along with suitable metrics (e.g. Wasserstein). These summaries are\nuseful for computational purposes, but they are merely a marginal of the actual\ntopological information that persistent homology can provide. Instead, our\napproach compares between two topological representations directly in the data\nspace. We do so by defining a correspondence relation between individual\npersistent cycles of two different spaces, and devising a method for computing\nthis correspondence. Our matching of cycles is based on both the persistence\nintervals and the spatial placement of each feature. We demonstrate our new\nframework in the context of topological inference, where we use statistical\nbootstrap methods in order to differentiate between real features and noise in\npoint cloud data.\n", "versions": [{"version": "v1", "created": "Sun, 3 Jan 2021 20:12:00 GMT"}], "update_date": "2021-01-05", "authors_parsed": [["Reani", "Yohai", ""], ["Bobrowski", "Omer", ""]]}, {"id": "2101.00699", "submitter": "Adrian Lewis", "authors": "Adrian Lewis and Tonghua Tian", "title": "The structure of conservative gradient fields", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.LG cs.NA math.NA", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The classical Clarke subdifferential alone is inadequate for understanding\nautomatic differentiation in nonsmooth contexts. Instead, we can sometimes rely\non enlarged generalized gradients called \"conservative fields\", defined through\nthe natural path-wise chain rule: one application is the convergence analysis\nof gradient-based deep learning algorithms. In the semi-algebraic case, we show\nthat all conservative fields are in fact just Clarke subdifferentials plus\nnormals of manifolds in underlying Whitney stratifications.\n", "versions": [{"version": "v1", "created": "Sun, 3 Jan 2021 20:30:29 GMT"}], "update_date": "2021-01-05", "authors_parsed": [["Lewis", "Adrian", ""], ["Tian", "Tonghua", ""]]}, {"id": "2101.00701", "submitter": "Carlos Lordelo", "authors": "Carlos Lordelo, Emmanouil Benetos, Simon Dixon, Sven Ahlb\\\"ack, and\n  Patrik Ohlsson", "title": "Adversarial Unsupervised Domain Adaptation for Harmonic-Percussive\n  Source Separation", "comments": "5 pages, 2 figures and 1 table. Accepted for publication in IEEE\n  Signal Processing Letters", "journal-ref": null, "doi": "10.1109/LSP.2020.3045915 10.5281/zenodo.4308731", "report-no": null, "categories": "cs.SD cs.LG eess.AS", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  This paper addresses the problem of domain adaptation for the task of music\nsource separation. Using datasets from two different domains, we compare the\nperformance of a deep learning-based harmonic-percussive source separation\nmodel under different training scenarios, including supervised joint training\nusing data from both domains and pre-training in one domain with fine-tuning in\nanother. We propose an adversarial unsupervised domain adaptation approach\nsuitable for the case where no labelled data (ground-truth source signals) from\na target domain is available. By leveraging unlabelled data (only mixtures)\nfrom this domain, experiments show that our framework can improve separation\nperformance on the new domain without losing any considerable performance on\nthe original domain. The paper also introduces the Tap & Fiddle dataset, a\ndataset containing recordings of Scandinavian fiddle tunes along with isolated\ntracks for 'foot-tapping' and 'violin'.\n", "versions": [{"version": "v1", "created": "Sun, 3 Jan 2021 20:46:42 GMT"}], "update_date": "2021-01-05", "authors_parsed": [["Lordelo", "Carlos", ""], ["Benetos", "Emmanouil", ""], ["Dixon", "Simon", ""], ["Ahlb\u00e4ck", "Sven", ""], ["Ohlsson", "Patrik", ""]]}, {"id": "2101.00702", "submitter": "Tanvir Mahmud", "authors": "Tanvir Mahmud, A. Q. M. Sazzad Sayyed, Shaikh Anowarul Fattah,\n  Sun-Yuan Kung", "title": "A Novel Multi-Stage Training Approach for Human Activity Recognition\n  from Multimodal Wearable Sensor Data Using Deep Neural Network", "comments": "12 Pages, 7 Figures. This article has been published in IEEE Sensors\n  Journal", "journal-ref": "IEEE Sensors Journal, Volume: 21, Issue:2, Page(s): 1715 - 1726,\n  January 2021", "doi": "10.1109/JSEN.2020.3015781", "report-no": null, "categories": "eess.SP cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural network is an effective choice to automatically recognize human\nactions utilizing data from various wearable sensors. These networks automate\nthe process of feature extraction relying completely on data. However, various\nnoises in time series data with complex inter-modal relationships among sensors\nmake this process more complicated. In this paper, we have proposed a novel\nmulti-stage training approach that increases diversity in this feature\nextraction process to make accurate recognition of actions by combining\nvarieties of features extracted from diverse perspectives. Initially, instead\nof using single type of transformation, numerous transformations are employed\non time series data to obtain variegated representations of the features\nencoded in raw data. An efficient deep CNN architecture is proposed that can be\nindividually trained to extract features from different transformed spaces.\nLater, these CNN feature extractors are merged into an optimal architecture\nfinely tuned for optimizing diversified extracted features through a combined\ntraining stage or multiple sequential training stages. This approach offers the\nopportunity to explore the encoded features in raw sensor data utilizing\nmultifarious observation windows with immense scope for efficient selection of\nfeatures for final convergence. Extensive experimentations have been carried\nout in three publicly available datasets that provide outstanding performance\nconsistently with average five-fold cross-validation accuracy of 99.29% on UCI\nHAR database, 99.02% on USC HAR database, and 97.21% on SKODA database\noutperforming other state-of-the-art approaches.\n", "versions": [{"version": "v1", "created": "Sun, 3 Jan 2021 20:48:56 GMT"}], "update_date": "2021-01-05", "authors_parsed": [["Mahmud", "Tanvir", ""], ["Sayyed", "A. Q. M. Sazzad", ""], ["Fattah", "Shaikh Anowarul", ""], ["Kung", "Sun-Yuan", ""]]}, {"id": "2101.00703", "submitter": "Samit Chakraborty", "authors": "Samit Chakraborty, Marguerite Moore, Lisa Parrillo-Chapman", "title": "Automatic Defect Detection of Print Fabric Using Convolutional Neural\n  Network", "comments": "8 pages, 4 figures, Conference", "journal-ref": "Digital Fashion Innovation e-Symposium, 2020", "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Automatic defect detection is a challenging task because of the variability\nin texture and type of fabric defects. An effective defect detection system\nenables manufacturers to improve the quality of processes and products.\nAutomation across the textile manufacturing systems would reduce fabric wastage\nand increase profitability by saving cost and resources. There are different\ncontemporary research on automatic defect detection systems using image\nprocessing and machine learning techniques. These techniques differ from each\nother based on the manufacturing processes and defect types. Researchers have\nalso been able to establish real-time defect detection system during weaving.\nAlthough, there has been research on patterned fabric defect detection, these\ndefects are related to weaving faults such as holes, and warp and weft defects.\nBut, there has not been any research that is designed to detect defects that\narise during such as spot and print mismatch. This research has fulfilled this\ngap by developing a print fabric database and implementing deep convolutional\nneural network (CNN).\n", "versions": [{"version": "v1", "created": "Sun, 3 Jan 2021 20:56:56 GMT"}], "update_date": "2021-01-19", "authors_parsed": [["Chakraborty", "Samit", ""], ["Moore", "Marguerite", ""], ["Parrillo-Chapman", "Lisa", ""]]}, {"id": "2101.00717", "submitter": "Ozgur Ceyhan", "authors": "Ozgur Ceyhan", "title": "Algorithmic Complexities in Backpropagation and Tropical Neural Networks", "comments": "This note is a summary of the lecture given at S\\'eminaire \"Fables\n  G\\'eom\\'etriques\" in University of Geneva on December 9, 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.LG math.AG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this note, we propose a novel technique to reduce the algorithmic\ncomplexity of neural network training by using matrices of tropical real\nnumbers instead of matrices of real numbers. Since the tropical arithmetics\nreplaces multiplication with addition, and addition with max, we theoretically\nachieve several order of magnitude better constant factors in time complexities\nin the training phase. The fact that we replace the field of real numbers with\nthe tropical semiring of real numbers and yet achieve the same classification\nresults via neural networks come from deep results in topology and analysis,\nwhich we verify in our note. We then explore artificial neural networks in\nterms of tropical arithmetics and tropical algebraic geometry, and introduce\nthe multi-layered tropical neural networks as universal approximators. After\ngiving a tropical reformulation of the backpropagation algorithm, we verify the\nalgorithmic complexity is substantially lower than the usual backpropagation as\nthe tropical arithmetic is free of the complexity of usual multiplication.\n", "versions": [{"version": "v1", "created": "Sun, 3 Jan 2021 22:19:17 GMT"}], "update_date": "2021-01-05", "authors_parsed": [["Ceyhan", "Ozgur", ""]]}, {"id": "2101.00719", "submitter": "Sridhar Ravula", "authors": "Sridhar Ravula", "title": "Bankruptcy prediction using disclosure text features", "comments": "36 pages, 14 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.GN cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  A public firm's bankruptcy prediction is an important financial research\nproblem because of the security price downside risks. Traditional methods rely\non accounting metrics that suffer from shortcomings like window dressing and\nretrospective focus. While disclosure text-based metrics overcome some of these\nissues, current methods excessively focus on disclosure tone and sentiment.\nThere is a requirement to relate meaningful signals in the disclosure text to\nfinancial outcomes and quantify the disclosure text data. This work proposes a\nnew distress dictionary based on the sentences used by managers in explaining\nfinancial status. It demonstrates the significant differences in linguistic\nfeatures between bankrupt and non-bankrupt firms. Further, using a large sample\nof 500 bankrupt firms, it builds predictive models and compares the performance\nagainst two dictionaries used in financial text analysis. This research shows\nthat the proposed stress dictionary captures unique information from\ndisclosures and the predictive models based on its features have the highest\naccuracy.\n", "versions": [{"version": "v1", "created": "Sun, 3 Jan 2021 22:23:22 GMT"}], "update_date": "2021-01-05", "authors_parsed": [["Ravula", "Sridhar", ""]]}, {"id": "2101.00728", "submitter": "Dom Huh", "authors": "Dom Huh", "title": "Synthetic Embedding-based Data Generation Methods for Student\n  Performance", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Given the inherent class imbalance issue within student performance datasets,\nsamples belonging to the edges of the target class distribution pose a\nchallenge for predictive machine learning algorithms to learn. In this paper,\nwe introduce a general framework for synthetic embedding-based data generation\n(SEDG), a search-based approach to generate new synthetic samples using\nembeddings to correct the detriment effects of class imbalances optimally. We\ncompare the SEDG framework to past synthetic data generation methods, including\ndeep generative models, and traditional sampling methods. In our results, we\nfind SEDG to outperform the traditional re-sampling methods for deep neural\nnetworks and perform competitively for common machine learning classifiers on\nthe student performance task in several standard performance metrics.\n", "versions": [{"version": "v1", "created": "Sun, 3 Jan 2021 23:43:36 GMT"}], "update_date": "2021-01-05", "authors_parsed": [["Huh", "Dom", ""]]}, {"id": "2101.00729", "submitter": "Ruduan Plug", "authors": "Ruduan Plug", "title": "Meta-Learning Conjugate Priors for Few-Shot Bayesian Optimization", "comments": "6 pages, 8 figures, code available on Github", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Bayesian Optimization is methodology used in statistical modelling that\nutilizes a Gaussian process prior distribution to iteratively update a\nposterior distribution towards the true distribution of the data. Finding\nunbiased informative priors to sample from is challenging and can greatly\ninfluence the outcome on the posterior distribution if only few data are\navailable. In this paper we propose a novel approach to utilize meta-learning\nto automate the estimation of informative conjugate prior distributions given a\ndistribution class. From this process we generate priors that require only few\ndata to estimate the shape parameters of the original distribution of the data.\n", "versions": [{"version": "v1", "created": "Sun, 3 Jan 2021 23:58:32 GMT"}], "update_date": "2021-01-05", "authors_parsed": [["Plug", "Ruduan", ""]]}, {"id": "2101.00731", "submitter": "Harsh Dhillon", "authors": "Harsh Dhillon, Anwar Haque", "title": "Towards Network Traffic Monitoring Using Deep Transfer Learning", "comments": "Accepted by 2020 IEEE 19th International Conference on Trust,\n  Security and Privacy in Computing and Communications (TrustCom)", "journal-ref": null, "doi": "10.1109/TrustCom50675.2020.00144", "report-no": null, "categories": "cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Network traffic is growing at an outpaced speed globally. The modern network\ninfrastructure makes classic network intrusion detection methods inefficient to\nclassify an inflow of vast network traffic. This paper aims to present a modern\napproach towards building a network intrusion detection system (NIDS) by using\nvarious deep learning methods. To further improve our proposed scheme and make\nit effective in real-world settings, we use deep transfer learning techniques\nwhere we transfer the knowledge learned by our model in a source domain with\nplentiful computational and data resources to a target domain with sparse\navailability of both the resources. Our proposed method achieved 98.30%\nclassification accuracy score in the source domain and an improved 98.43%\nclassification accuracy score in the target domain with a boost in the\nclassification speed using UNSW-15 dataset. This study demonstrates that deep\ntransfer learning techniques make it possible to construct large deep learning\nmodels to perform network classification, which can be deployed in the real\nworld target domains where they can maintain their classification performance\nand improve their classification speed despite the limited accessibility of\nresources.\n", "versions": [{"version": "v1", "created": "Mon, 4 Jan 2021 00:32:05 GMT"}], "update_date": "2021-01-05", "authors_parsed": [["Dhillon", "Harsh", ""], ["Haque", "Anwar", ""]]}, {"id": "2101.00732", "submitter": "Muhtadyuzzaman Syed", "authors": "Muhtadyuzzaman Syed and Arvind Akpuram Srinivasan", "title": "Generalized Latency Performance Estimation for Once-For-All Neural\n  Architecture Search", "comments": "12 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural Architecture Search (NAS) has enabled the possibility of automated\nmachine learning by streamlining the manual development of deep neural network\narchitectures defining a search space, search strategy, and performance\nestimation strategy. To solve the need for multi-platform deployment of\nConvolutional Neural Network (CNN) models, Once-For-All (OFA) proposed to\ndecouple Training and Search to deliver a one-shot model of sub-networks that\nare constrained to various accuracy-latency tradeoffs. We find that the\nperformance estimation strategy for OFA's search severely lacks\ngeneralizability of different hardware deployment platforms due to single\nhardware latency lookup tables that require significant amount of time and\nmanual effort to build beforehand. In this work, we demonstrate the framework\nfor building latency predictors for neural network architectures to address the\nneed for heterogeneous hardware support and reduce the overhead of lookup\ntables altogether. We introduce two generalizability strategies which include\nfine-tuning using a base model trained on a specific hardware and NAS search\nspace, and GPU-generalization which trains a model on GPU hardware parameters\nsuch as Number of Cores, RAM Size, and Memory Bandwidth. With this, we provide\na family of latency prediction models that achieve over 50% lower RMSE loss as\ncompared to with ProxylessNAS. We also show that the use of these latency\npredictors match the NAS performance of the lookup table baseline approach if\nnot exceeding it in certain cases.\n", "versions": [{"version": "v1", "created": "Mon, 4 Jan 2021 00:48:09 GMT"}], "update_date": "2021-01-05", "authors_parsed": [["Syed", "Muhtadyuzzaman", ""], ["Srinivasan", "Arvind Akpuram", ""]]}, {"id": "2101.00734", "submitter": "Benyamin Ghojogh", "authors": "Benyamin Ghojogh, Ali Ghodsi, Fakhri Karray, Mark Crowley", "title": "Factor Analysis, Probabilistic Principal Component Analysis, Variational\n  Inference, and Variational Autoencoder: Tutorial and Survey", "comments": "To appear as a part of an upcoming textbook on dimensionality\n  reduction and manifold learning", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This is a tutorial and survey paper on factor analysis, probabilistic\nPrincipal Component Analysis (PCA), variational inference, and Variational\nAutoencoder (VAE). These methods, which are tightly related, are dimensionality\nreduction and generative models. They asssume that every data point is\ngenerated from or caused by a low-dimensional latent factor. By learning the\nparameters of distribution of latent space, the corresponding low-dimensional\nfactors are found for the sake of dimensionality reduction. For their\nstochastic and generative behaviour, these models can also be used for\ngeneration of new data points in the data space. In this paper, we first start\nwith variational inference where we derive the Evidence Lower Bound (ELBO) and\nExpectation Maximization (EM) for learning the parameters. Then, we introduce\nfactor analysis, derive its joint and marginal distributions, and work out its\nEM steps. Probabilistic PCA is then explained, as a special case of factor\nanalysis, and its closed-form solutions are derived. Finally, VAE is explained\nwhere the encoder, decoder and sampling from the latent space are introduced.\nTraining VAE using both EM and backpropagation are explained.\n", "versions": [{"version": "v1", "created": "Mon, 4 Jan 2021 01:29:09 GMT"}], "update_date": "2021-01-05", "authors_parsed": [["Ghojogh", "Benyamin", ""], ["Ghodsi", "Ali", ""], ["Karray", "Fakhri", ""], ["Crowley", "Mark", ""]]}, {"id": "2101.00738", "submitter": "Thejan Rajapakshe", "authors": "Thejan Rajapakshe, Rajib Rana, Sara Khalifa, Bj\\\"orn W. Schuller,\n  Jiajun Liu", "title": "A novel policy for pre-trained Deep Reinforcement Learning for Speech\n  Emotion Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.LG eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Reinforcement Learning (RL) is a semi-supervised learning paradigm which an\nagent learns by interacting with an environment. Deep learning in combination\nwith RL provides an efficient method to learn how to interact with the\nenvironment is called Deep Reinforcement Learning (deep RL). Deep RL has gained\ntremendous success in gaming - such as AlphaGo, but its potential have rarely\nbeing explored for challenging tasks like Speech Emotion Recognition (SER). The\ndeep RL being used for SER can potentially improve the performance of an\nautomated call centre agent by dynamically learning emotional-aware response to\ncustomer queries. While the policy employed by the RL agent plays a major role\nin action selection, there is no current RL policy tailored for SER. In\naddition, extended learning period is a general challenge for deep RL which can\nimpact the speed of learning for SER. Therefore, in this paper, we introduce a\nnovel policy - \"Zeta policy\" which is tailored for SER and apply Pre-training\nin deep RL to achieve faster learning rate. Pre-training with cross dataset was\nalso studied to discover the feasibility of pre-training the RL Agent with a\nsimilar dataset in a scenario of where no real environmental data is not\navailable. IEMOCAP and SAVEE datasets were used for the evaluation with the\nproblem being to recognize four emotions happy, sad, angry and neutral in the\nutterances provided. Experimental results show that the proposed \"Zeta policy\"\nperforms better than existing policies. The results also support that\npre-training can reduce the training time upon reducing the warm-up period and\nis robust to cross-corpus scenario.\n", "versions": [{"version": "v1", "created": "Mon, 4 Jan 2021 02:13:26 GMT"}, {"version": "v2", "created": "Sun, 31 Jan 2021 10:06:52 GMT"}], "update_date": "2021-02-02", "authors_parsed": [["Rajapakshe", "Thejan", ""], ["Rana", "Rajib", ""], ["Khalifa", "Sara", ""], ["Schuller", "Bj\u00f6rn W.", ""], ["Liu", "Jiajun", ""]]}, {"id": "2101.00744", "submitter": "Faramarz Jabbarvaziri", "authors": "Seyedrazieh Bayati, Faramarz Jabbarvaziri", "title": "Learning to Optimize Under Constraints with Unsupervised Deep Neural\n  Networks", "comments": "7 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper, we propose a machine learning (ML) method to learn how to\nsolve a generic constrained continuous optimization problem. To the best of our\nknowledge, the generic methods that learn to optimize, focus on unconstrained\noptimization problems and those dealing with constrained problems are not\neasy-to-generalize. This approach is quite useful in optimization tasks where\nthe problem's parameters constantly change and require resolving the\noptimization task per parameter update. In such problems, the computational\ncomplexity of optimization algorithms such as gradient descent or interior\npoint method preclude near-optimal designs in real-time applications. In this\npaper, we propose an unsupervised deep learning (DL) solution for solving\nconstrained optimization problems in real-time by relegating the main\ncomputation load to offline training phase. This paper's main contribution is\nproposing a method for enforcing the equality and inequality constraints to the\nDL-generated solutions for generic optimization tasks.\n", "versions": [{"version": "v1", "created": "Mon, 4 Jan 2021 02:58:37 GMT"}], "update_date": "2021-01-05", "authors_parsed": [["Bayati", "Seyedrazieh", ""], ["Jabbarvaziri", "Faramarz", ""]]}, {"id": "2101.00745", "submitter": "Yuke Wang", "authors": "Yuke Wang, Boyuan Feng, Yufei Ding", "title": "DSXplore: Optimizing Convolutional Neural Networks via Sliding-Channel\n  Convolutions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  As the key advancement of the convolutional neural networks (CNNs), depthwise\nseparable convolutions (DSCs) are becoming one of the most popular techniques\nto reduce the computations and parameters size of CNNs meanwhile maintaining\nthe model accuracy. It also brings profound impact to improve the applicability\nof the compute- and memory-intensive CNNs to a broad range of applications,\nsuch as mobile devices, which are generally short of computation power and\nmemory. However, previous research in DSCs are largely focusing on compositing\nthe limited existing DSC designs, thus, missing the opportunities to explore\nmore potential designs that can achieve better accuracy and higher\ncomputation/parameter reduction. Besides, the off-the-shelf convolution\nimplementations offer limited computing schemes, therefore, lacking support for\nDSCs with different convolution patterns.\n  To this end, we introduce, DSXplore, the first optimized design for exploring\nDSCs on CNNs. Specifically, at the algorithm level, DSXplore incorporates a\nnovel factorized kernel -- sliding-channel convolution (SCC), featured with\ninput-channel overlapping to balance the accuracy performance and the reduction\nof computation and memory cost. SCC also offers enormous space for design\nexploration by introducing adjustable kernel parameters. Further, at the\nimplementation level, we carry out an optimized GPU-implementation tailored for\nSCC by leveraging several key techniques, such as the input-centric backward\ndesign and the channel-cyclic optimization. Intensive experiments on different\ndatasets across mainstream CNNs show the advantages of DSXplore in balancing\naccuracy and computation/parameter reduction over the standard convolution and\nthe existing DSCs.\n", "versions": [{"version": "v1", "created": "Mon, 4 Jan 2021 02:59:10 GMT"}], "update_date": "2021-01-05", "authors_parsed": [["Wang", "Yuke", ""], ["Feng", "Boyuan", ""], ["Ding", "Yufei", ""]]}, {"id": "2101.00746", "submitter": "Liwen Zhu", "authors": "Liwen Zhu, Peixi Peng, Zongqing Lu, Xiangqian Wang, Yonghong Tian", "title": "Variationally and Intrinsically motivated reinforcement learning for\n  decentralized traffic signal control", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.MA", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  One of the biggest challenges in multi-agent reinforcement learning is\ncoordination, a typical application scenario of this is traffic signal control.\nRecently, it has attracted a rising number of researchers and has become a hot\nresearch field with great practical significance. In this paper, we propose a\nnovel method called MetaVRS~(Meta Variational RewardShaping) for traffic signal\ncoordination control. By heuristically applying the intrinsic reward to the\nenvironmental reward, MetaVRS can wisely capture the agent-to-agent interplay.\nBesides, latent variables generated by VAE are brought into policy for\nautomatically tradeoff between exploration and exploitation to optimize the\npolicy. In addition, meta learning was used in decoder for faster adaptation\nand better approximation. Empirically, we demonstate that MetaVRS substantially\noutperforms existing methods and shows superior adaptability, which predictably\nhas a far-reaching significance to the multi-agent traffic signal coordination\ncontrol.\n", "versions": [{"version": "v1", "created": "Mon, 4 Jan 2021 03:06:08 GMT"}, {"version": "v2", "created": "Tue, 5 Jan 2021 07:09:38 GMT"}, {"version": "v3", "created": "Wed, 6 Jan 2021 02:18:51 GMT"}, {"version": "v4", "created": "Wed, 20 Jan 2021 06:56:50 GMT"}], "update_date": "2021-01-21", "authors_parsed": [["Zhu", "Liwen", ""], ["Peng", "Peixi", ""], ["Lu", "Zongqing", ""], ["Wang", "Xiangqian", ""], ["Tian", "Yonghong", ""]]}, {"id": "2101.00747", "submitter": "Zhiqin Xu", "authors": "Yuheng Ma, Zhi-Qin John Xu, Jiwei Zhang", "title": "Frequency Principle in Deep Learning Beyond Gradient-descent-based\n  Training", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Frequency perspective recently makes progress in understanding deep learning.\nIt has been widely verified in both empirical and theoretical studies that deep\nneural networks (DNNs) often fit the target function from low to high\nfrequency, namely Frequency Principle (F-Principle). F-Principle sheds light on\nthe strength and the weakness of DNNs and inspires a series of subsequent\nworks, including theoretical studies, empirical studies and the design of\nefficient DNN structures etc. Previous works examine the F-Principle in\ngradient-descent-based training. It remains unclear whether\ngradient-descent-based training is a necessary condition for the F-Principle.\nIn this paper, we show that the F-Principle exists stably in the training\nprocess of DNNs with non-gradient-descent-based training, including\noptimization algorithms with gradient information, such as conjugate gradient\nand BFGS, and algorithms without gradient information, such as Powell's method\nand Particle Swarm Optimization. These empirical studies show the universality\nof the F-Principle and provide hints for further study of F-Principle.\n", "versions": [{"version": "v1", "created": "Mon, 4 Jan 2021 03:11:03 GMT"}], "update_date": "2021-01-05", "authors_parsed": [["Ma", "Yuheng", ""], ["Xu", "Zhi-Qin John", ""], ["Zhang", "Jiwei", ""]]}, {"id": "2101.00752", "submitter": "Yuandong Wang", "authors": "Yuandong Wang and Hongzhi Yin and Tong Chen and Chunyang Liu and Ben\n  Wang and Tianyu Wo and Jie Xu", "title": "Passenger Mobility Prediction via Representation Learning for Dynamic\n  Directed and Weighted Graph", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, ride-hailing services have been increasingly prevalent as\nthey provide huge convenience for passengers. As a fundamental problem, the\ntimely prediction of passenger demands in different regions is vital for\neffective traffic flow control and route planning. As both spatial and temporal\npatterns are indispensable passenger demand prediction, relevant research has\nevolved from pure time series to graph-structured data for modeling historical\npassenger demand data, where a snapshot graph is constructed for each time slot\nby connecting region nodes via different relational edges (e.g.,\norigin-destination relationship, geographical distance, etc.). Consequently,\nthe spatiotemporal passenger demand records naturally carry dynamic patterns in\nthe constructed graphs, where the edges also encode important information about\nthe directions and volume (i.e., weights) of passenger demands between two\nconnected regions. However, existing graph-based solutions fail to\nsimultaneously consider those three crucial aspects of dynamic, directed, and\nweighted (DDW) graphs, leading to limited expressiveness when learning graph\nrepresentations for passenger demand prediction. Therefore, we propose a novel\nspatiotemporal graph attention network, namely Gallat (Graph prediction with\nall attention) as a solution. In Gallat, by comprehensively incorporating those\nthree intrinsic properties of DDW graphs, we build three attention layers to\nfully capture the spatiotemporal dependencies among different regions across\nall historical time slots. Moreover, the model employs a subtask to conduct\npretraining so that it can obtain accurate results more quickly. We evaluate\nthe proposed model on real-world datasets, and our experimental results\ndemonstrate that Gallat outperforms the state-of-the-art approaches.\n", "versions": [{"version": "v1", "created": "Mon, 4 Jan 2021 03:32:01 GMT"}], "update_date": "2021-01-05", "authors_parsed": [["Wang", "Yuandong", ""], ["Yin", "Hongzhi", ""], ["Chen", "Tong", ""], ["Liu", "Chunyang", ""], ["Wang", "Ben", ""], ["Wo", "Tianyu", ""], ["Xu", "Jie", ""]]}, {"id": "2101.00753", "submitter": "Rong Jin", "authors": "Rong Jin and Weili Wu", "title": "Schemes of Propagation Models and Source Estimators for Rumor Source\n  Detection in Online Social Networks: A Short Survey of a Decade of Research", "comments": null, "journal-ref": null, "doi": "10.1142/S1793830921300022", "report-no": null, "categories": "cs.SI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent years have seen various rumor diffusion models being assumed in\ndetection of rumor source research of the online social network. Diffusion\nmodel is arguably considered as a very important and challengeable factor for\nsource detection in networks but it is less studied. This paper provides an\noverview of three representative schemes of Independent Cascade-based,\nEpidemic-based, and Learning-based to model the patterns of rumor propagation\nas well as three major schemes of estimators for rumor sources since its\ninception a decade ago.\n", "versions": [{"version": "v1", "created": "Mon, 4 Jan 2021 03:34:17 GMT"}], "update_date": "2021-05-03", "authors_parsed": [["Jin", "Rong", ""], ["Wu", "Weili", ""]]}, {"id": "2101.00772", "submitter": "Mohammadhossein Toutiaee", "authors": "Mohammadhossein Toutiaee, John Miller", "title": "Gaussian Function On Response Surface Estimation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new framework for 2-D interpreting (features and samples)\nblack-box machine learning models via a metamodeling technique, by which we\nstudy the output and input relationships of the underlying machine learning\nmodel. The metamodel can be estimated from data generated via a trained complex\nmodel by running the computer experiment on samples of data in the region of\ninterest. We utilize a Gaussian process as a surrogate to capture the response\nsurface of a complex model, in which we incorporate two parts in the process:\ninterpolated values that are modeled by a stationary Gaussian process Z\ngoverned by a prior covariance function, and a mean function mu that captures\nthe known trends in the underlying model. The optimization procedure for the\nvariable importance parameter theta is to maximize the likelihood function.\nThis theta corresponds to the correlation of individual variables with the\ntarget response. There is no need for any pre-assumed models since it depends\non empirical observations. Experiments demonstrate the potential of the\ninterpretable model through quantitative assessment of the predicted samples.\n", "versions": [{"version": "v1", "created": "Mon, 4 Jan 2021 04:47:00 GMT"}], "update_date": "2021-01-05", "authors_parsed": [["Toutiaee", "Mohammadhossein", ""], ["Miller", "John", ""]]}, {"id": "2101.00787", "submitter": "Su Wang", "authors": "Su Wang, Mengyuan Lee, Seyyedali Hosseinalipour, Roberto Morabito,\n  Mung Chiang, and Christopher G. Brinton", "title": "Device Sampling for Heterogeneous Federated Learning: Theory,\n  Algorithms, and Implementation", "comments": "This paper is accepted for publication in the proceedings of 2021\n  IEEE International Conference on Computer Communications (INFOCOM)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI cs.DC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The conventional federated learning (FedL) architecture distributes machine\nlearning (ML) across worker devices by having them train local models that are\nperiodically aggregated by a server. FedL ignores two important characteristics\nof contemporary wireless networks, however: (i) the network may contain\nheterogeneous communication/computation resources, while (ii) there may be\nsignificant overlaps in devices' local data distributions. In this work, we\ndevelop a novel optimization methodology that jointly accounts for these\nfactors via intelligent device sampling complemented by device-to-device (D2D)\noffloading. Our optimization aims to select the best combination of sampled\nnodes and data offloading configuration to maximize FedL training accuracy\nsubject to realistic constraints on the network topology and device\ncapabilities. Theoretical analysis of the D2D offloading subproblem leads to\nnew FedL convergence bounds and an efficient sequential convex optimizer. Using\nthis result, we develop a sampling methodology based on graph convolutional\nnetworks (GCNs) which learns the relationship between network attributes,\nsampled nodes, and resulting offloading that maximizes FedL accuracy. Through\nevaluation on real-world datasets and network measurements from our IoT\ntestbed, we find that our methodology while sampling less than 5% of all\ndevices outperforms conventional FedL substantially both in terms of trained\nmodel accuracy and required resource utilization.\n", "versions": [{"version": "v1", "created": "Mon, 4 Jan 2021 05:59:50 GMT"}], "update_date": "2021-01-05", "authors_parsed": [["Wang", "Su", ""], ["Lee", "Mengyuan", ""], ["Hosseinalipour", "Seyyedali", ""], ["Morabito", "Roberto", ""], ["Chiang", "Mung", ""], ["Brinton", "Christopher G.", ""]]}, {"id": "2101.00793", "submitter": "Karthik E", "authors": "Karthik E", "title": "A Framework for Fast Scalable BNN Inference using Googlenet and Transfer\n  Learning", "comments": "22 pages, 15 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.AR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Efficient and accurate object detection in video and image analysis is one of\nthe major beneficiaries of the advancement in computer vision systems with the\nhelp of deep learning. With the aid of deep learning, more powerful tools\nevolved, which are capable to learn high-level and deeper features and thus can\novercome the existing problems in traditional architectures of object detection\nalgorithms. The work in this thesis aims to achieve high accuracy in object\ndetection with good real-time performance.\n  In the area of computer vision, a lot of research is going into the area of\ndetection and processing of visual information, by improving the existing\nalgorithms. The binarized neural network has shown high performance in various\nvision tasks such as image classification, object detection, and semantic\nsegmentation. The Modified National Institute of Standards and Technology\ndatabase (MNIST), Canadian Institute for Advanced Research (CIFAR), and Street\nView House Numbers (SVHN) datasets are used which is implemented using a\npre-trained convolutional neural network (CNN) that is 22 layers deep.\nSupervised learning is used in the work, which classifies the particular\ndataset with the proper structure of the model. In still images, to improve\naccuracy, Googlenet is used. The final layer of the Googlenet is replaced with\nthe transfer learning to improve the accuracy of the Googlenet. At the same\ntime, the accuracy in moving images can be maintained by transfer learning\ntechniques. Hardware is the main backbone for any model to obtain faster\nresults with a large number of datasets. Here, Nvidia Jetson Nano is used which\nis a graphics processing unit (GPU), that can handle a large number of\ncomputations in the process of object detection. Results show that the accuracy\nof objects detected by the transfer learning method is more when compared to\nthe existing methods.\n", "versions": [{"version": "v1", "created": "Mon, 4 Jan 2021 06:16:52 GMT"}, {"version": "v2", "created": "Tue, 5 Jan 2021 07:28:38 GMT"}], "update_date": "2021-01-06", "authors_parsed": [["E", "Karthik", ""]]}, {"id": "2101.00797", "submitter": "Deyu Bo", "authors": "Deyu Bo and Xiao Wang and Chuan Shi and Huawei Shen", "title": "Beyond Low-frequency Information in Graph Convolutional Networks", "comments": "Accepted by AAAI 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Graph neural networks (GNNs) have been proven to be effective in various\nnetwork-related tasks. Most existing GNNs usually exploit the low-frequency\nsignals of node features, which gives rise to one fundamental question: is the\nlow-frequency information all we need in the real world applications? In this\npaper, we first present an experimental investigation assessing the roles of\nlow-frequency and high-frequency signals, where the results clearly show that\nexploring low-frequency signal only is distant from learning an effective node\nrepresentation in different scenarios. How can we adaptively learn more\ninformation beyond low-frequency information in GNNs? A well-informed answer\ncan help GNNs enhance the adaptability. We tackle this challenge and propose a\nnovel Frequency Adaptation Graph Convolutional Networks (FAGCN) with a\nself-gating mechanism, which can adaptively integrate different signals in the\nprocess of message passing. For a deeper understanding, we theoretically\nanalyze the roles of low-frequency signals and high-frequency signals on\nlearning node representations, which further explains why FAGCN can perform\nwell on different types of networks. Extensive experiments on six real-world\nnetworks validate that FAGCN not only alleviates the over-smoothing problem,\nbut also has advantages over the state-of-the-arts.\n", "versions": [{"version": "v1", "created": "Mon, 4 Jan 2021 06:26:36 GMT"}], "update_date": "2021-01-05", "authors_parsed": [["Bo", "Deyu", ""], ["Wang", "Xiao", ""], ["Shi", "Chuan", ""], ["Shen", "Huawei", ""]]}, {"id": "2101.00813", "submitter": "Ya'nan Wang", "authors": "Ya'nan Wang, Zhuqing Jiang, Chang Liu, Kai Li, Aidong Men, Haiying\n  Wang", "title": "Shed Various Lights on a Low-Light Image: Multi-Level Enhancement Guided\n  by Arbitrary References", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is suggested that low-light image enhancement realizes one-to-many mapping\nsince we have different definitions of NORMAL-light given application scenarios\nor users' aesthetic. However, most existing methods ignore subjectivity of the\ntask, and simply produce one result with fixed brightness. This paper proposes\na neural network for multi-level low-light image enhancement, which is\nuser-friendly to meet various requirements by selecting different images as\nbrightness reference. Inspired by style transfer, our method decomposes an\nimage into two low-coupling feature components in the latent space, which\nallows the concatenation feasibility of the content components from low-light\nimages and the luminance components from reference images. In such a way, the\nnetwork learns to extract scene-invariant and brightness-specific information\nfrom a set of image pairs instead of learning brightness differences. Moreover,\ninformation except for the brightness is preserved to the greatest extent to\nalleviate color distortion. Extensive results show strong capacity and\nsuperiority of our network against existing methods.\n", "versions": [{"version": "v1", "created": "Mon, 4 Jan 2021 07:38:51 GMT"}], "update_date": "2021-01-05", "authors_parsed": [["Wang", "Ya'nan", ""], ["Jiang", "Zhuqing", ""], ["Liu", "Chang", ""], ["Li", "Kai", ""], ["Men", "Aidong", ""], ["Wang", "Haiying", ""]]}, {"id": "2101.00819", "submitter": "Babak Nouri-Moghaddam", "authors": "Babak Nouri-Moghaddam, Mehdi Ghazanfari, Mohammad Fathian", "title": "A Novel Bio-Inspired Hybrid Multi-Filter Wrapper Gene Selection Method\n  with Ensemble Classifier for Microarray Data", "comments": "22 pages, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.QM cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Microarray technology is known as one of the most important tools for\ncollecting DNA expression data. This technology allows researchers to\ninvestigate and examine types of diseases and their origins. However,\nmicroarray data are often associated with challenges such as small sample size,\na significant number of genes, imbalanced data, etc. that make classification\nmodels inefficient. Thus, a new hybrid solution based on multi-filter and\nadaptive chaotic multi-objective forest optimization algorithm (AC-MOFOA) is\npresented to solve the gene selection problem and construct the Ensemble\nClassifier. In the proposed solution, to reduce the dataset's dimensions, a\nmulti-filter model uses a combination of five filter methods to remove\nredundant and irrelevant genes. Then, an AC-MOFOA based on the concepts of\nnon-dominated sorting, crowding distance, chaos theory, and adaptive operators\nis presented. AC-MOFOA as a wrapper method aimed at reducing dataset\ndimensions, optimizing KELM, and increasing the accuracy of the classification,\nsimultaneously. Next, in this method, an ensemble classifier model is presented\nusing AC-MOFOA results to classify microarray data. The performance of the\nproposed algorithm was evaluated on nine public microarray datasets, and its\nresults were compared in terms of the number of selected genes, classification\nefficiency, execution time, time complexity, and hypervolume indicator\ncriterion with five hybrid multi-objective methods. According to the results,\nthe proposed hybrid method could increase the accuracy of the KELM in most\ndatasets by reducing the dataset's dimensions and achieve similar or superior\nperformance compared to other multi-objective methods. Furthermore, the\nproposed Ensemble Classifier model could provide better classification accuracy\nand generalizability in microarray data compared to conventional ensemble\nmethods.\n", "versions": [{"version": "v1", "created": "Mon, 4 Jan 2021 07:57:35 GMT"}], "update_date": "2021-01-05", "authors_parsed": [["Nouri-Moghaddam", "Babak", ""], ["Ghazanfari", "Mehdi", ""], ["Fathian", "Mohammad", ""]]}, {"id": "2101.00820", "submitter": "Yang Liu", "authors": "Yang Liu, Keze Wang, Haoyuan Lan, Liang Lin", "title": "Temporal Contrastive Graph Learning for Video Action Recognition and\n  Retrieval", "comments": "12 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Attempt to fully discover the temporal diversity and chronological\ncharacteristics for self-supervised video representation learning, this work\ntakes advantage of the temporal dependencies within videos and further proposes\na novel self-supervised method named Temporal Contrastive Graph Learning\n(TCGL). In contrast to the existing methods that ignore modeling elaborate\ntemporal dependencies, our TCGL roots in a hybrid graph contrastive learning\nstrategy to jointly regard the inter-snippet and intra-snippet temporal\ndependencies as self-supervision signals for temporal representation learning.\nTo model multi-scale temporal dependencies, our TCGL integrates the prior\nknowledge about the frame and snippet orders into graph structures, i.e., the\nintra-/inter- snippet temporal contrastive graphs. By randomly removing edges\nand masking nodes of the intra-snippet graphs or inter-snippet graphs, our TCGL\ncan generate different correlated graph views. Then, specific contrastive\nlearning modules are designed to maximize the agreement between nodes in\ndifferent views. To adaptively learn the global context representation and\nrecalibrate the channel-wise features, we introduce an adaptive video snippet\norder prediction module, which leverages the relational knowledge among video\nsnippets to predict the actual snippet orders. Experimental results demonstrate\nthe superiority of our TCGL over the state-of-the-art methods on large-scale\naction recognition and video retrieval benchmarks.\n", "versions": [{"version": "v1", "created": "Mon, 4 Jan 2021 08:11:39 GMT"}, {"version": "v2", "created": "Fri, 22 Jan 2021 03:33:28 GMT"}, {"version": "v3", "created": "Tue, 26 Jan 2021 08:51:36 GMT"}, {"version": "v4", "created": "Mon, 1 Feb 2021 03:43:47 GMT"}, {"version": "v5", "created": "Mon, 1 Mar 2021 08:51:25 GMT"}, {"version": "v6", "created": "Tue, 2 Mar 2021 08:34:02 GMT"}, {"version": "v7", "created": "Thu, 4 Mar 2021 13:08:41 GMT"}, {"version": "v8", "created": "Wed, 17 Mar 2021 03:32:52 GMT"}], "update_date": "2021-03-18", "authors_parsed": [["Liu", "Yang", ""], ["Wang", "Keze", ""], ["Lan", "Haoyuan", ""], ["Lin", "Liang", ""]]}, {"id": "2101.00822", "submitter": "Le Fang", "authors": "Le Fang, Tao Zeng, Chaochun Liu, Liefeng Bo, Wen Dong, Changyou Chen", "title": "Outline to Story: Fine-grained Controllable Story Generation from\n  Cascaded Events", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Large-scale pretrained language models have shown thrilling generation\ncapabilities, especially when they generate consistent long text in thousands\nof words with ease. However, users of these models can only control the prefix\nof sentences or certain global aspects of generated text. It is challenging to\nsimultaneously achieve fine-grained controllability and preserve the\nstate-of-the-art unconditional text generation capability. In this paper, we\nfirst propose a new task named \"Outline to Story\" (O2S) as a test bed for\nfine-grained controllable generation of long text, which generates a\nmulti-paragraph story from cascaded events, i.e. a sequence of outline events\nthat guide subsequent paragraph generation. We then create dedicate datasets\nfor future benchmarks, built by state-of-the-art keyword extraction techniques.\nFinally, we propose an extremely simple yet strong baseline method for the O2S\ntask, which fine tunes pre-trained language models on augmented sequences of\noutline-story pairs with simple language modeling objective. Our method does\nnot introduce any new parameters or perform any architecture modification,\nexcept several special tokens as delimiters to build augmented sequences.\nExtensive experiments on various datasets demonstrate state-of-the-art\nconditional story generation performance with our model, achieving better\nfine-grained controllability and user flexibility. Our paper is among the first\nones by our knowledge to propose a model and to create datasets for the task of\n\"outline to story\". Our work also instantiates research interest of\nfine-grained controllable generation of open-domain long text, where\ncontrolling inputs are represented by short text.\n", "versions": [{"version": "v1", "created": "Mon, 4 Jan 2021 08:16:21 GMT"}], "update_date": "2021-01-05", "authors_parsed": [["Fang", "Le", ""], ["Zeng", "Tao", ""], ["Liu", "Chaochun", ""], ["Bo", "Liefeng", ""], ["Dong", "Wen", ""], ["Chen", "Changyou", ""]]}, {"id": "2101.00828", "submitter": "Le Fang", "authors": "Le Fang, Tao Zeng, Chaochun Liu, Liefeng Bo, Wen Dong, Changyou Chen", "title": "Transformer-based Conditional Variational Autoencoder for Controllable\n  Story Generation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate large-scale latent variable models (LVMs) for neural story\ngeneration -- an under-explored application for open-domain long text -- with\nobjectives in two threads: generation effectiveness and controllability. LVMs,\nespecially the variational autoencoder (VAE), have achieved both effective and\ncontrollable generation through exploiting flexible distributional latent\nrepresentations. Recently, Transformers and its variants have achieved\nremarkable effectiveness without explicit latent representation learning, thus\nlack satisfying controllability in generation. In this paper, we advocate to\nrevive latent variable modeling, essentially the power of representation\nlearning, in the era of Transformers to enhance controllability without hurting\nstate-of-the-art generation effectiveness. Specifically, we integrate latent\nrepresentation vectors with a Transformer-based pre-trained architecture to\nbuild conditional variational autoencoder (CVAE). Model components such as\nencoder, decoder and the variational posterior are all built on top of\npre-trained language models -- GPT2 specifically in this paper. Experiments\ndemonstrate state-of-the-art conditional generation ability of our model, as\nwell as its excellent representation learning capability and controllability.\n", "versions": [{"version": "v1", "created": "Mon, 4 Jan 2021 08:31:11 GMT"}, {"version": "v2", "created": "Thu, 8 Jul 2021 17:18:13 GMT"}], "update_date": "2021-07-09", "authors_parsed": [["Fang", "Le", ""], ["Zeng", "Tao", ""], ["Liu", "Chaochun", ""], ["Bo", "Liefeng", ""], ["Dong", "Wen", ""], ["Chen", "Changyou", ""]]}, {"id": "2101.00853", "submitter": "Rahul Bhadani", "authors": "Rahul Bhadani", "title": "AutoEncoder for Interpolation", "comments": "5 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In physical science, sensor data are collected over time to produce\ntimeseries data. However, depending on the real-world condition and underlying\nphysics of the sensor, data might be noisy. Besides, the limitation of\nsample-time on sensors may not allow collecting data over all the timepoints,\nmay require some form of interpolation. Interpolation may not be smooth enough,\nfail to denoise data, and derivative operation on noisy sensor data may be poor\nthat do not reveal any high order dynamics. In this article, we propose to use\nAutoEncoder to perform interpolation that also denoise data simultaneously. A\nbrief example using a real-world is also provided.\n", "versions": [{"version": "v1", "created": "Mon, 4 Jan 2021 09:51:58 GMT"}, {"version": "v2", "created": "Wed, 6 Jan 2021 01:42:59 GMT"}], "update_date": "2021-01-07", "authors_parsed": [["Bhadani", "Rahul", ""]]}, {"id": "2101.00905", "submitter": "Johannes Haug", "authors": "Johannes Haug, Stefan Z\\\"urn, Peter El-Jiz, Gjergji Kasneci", "title": "On Baselines for Local Feature Attributions", "comments": "Accepted at the AAAI-21 Workshop on Explainable Agency in AI", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  High-performing predictive models, such as neural nets, usually operate as\nblack boxes, which raises serious concerns about their interpretability. Local\nfeature attribution methods help to explain black box models and are therefore\na powerful tool for assessing the reliability and fairness of predictions. To\nthis end, most attribution models compare the importance of input features with\na reference value, often called baseline. Recent studies show that the baseline\ncan heavily impact the quality of feature attributions. Yet, we frequently find\nsimplistic baselines, such as the zero vector, in practice. In this paper, we\nshow empirically that baselines can significantly alter the discriminative\npower of feature attributions. We conduct our analysis on tabular data sets,\nthus complementing recent works on image data. Besides, we propose a new\ntaxonomy of baseline methods. Our experimental study illustrates the\nsensitivity of popular attribution models to the baseline, thus laying the\nfoundation for a more in-depth discussion on sensible baseline methods for\ntabular data.\n", "versions": [{"version": "v1", "created": "Mon, 4 Jan 2021 11:48:42 GMT"}], "update_date": "2021-01-05", "authors_parsed": [["Haug", "Johannes", ""], ["Z\u00fcrn", "Stefan", ""], ["El-Jiz", "Peter", ""], ["Kasneci", "Gjergji", ""]]}, {"id": "2101.00909", "submitter": "Caterina Urban", "authors": "Francesco Ranzato, Caterina Urban, Marco Zanella", "title": "Fair Training of Decision Tree Classifiers", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CY cs.PL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We study the problem of formally verifying individual fairness of decision\ntree ensembles, as well as training tree models which maximize both accuracy\nand individual fairness. In our approach, fairness verification and\nfairness-aware training both rely on a notion of stability of a classification\nmodel, which is a variant of standard robustness under input perturbations used\nin adversarial machine learning. Our verification and training methods leverage\nabstract interpretation, a well established technique for static program\nanalysis which is able to automatically infer assertions about stability\nproperties of decision trees. By relying on a tool for adversarial training of\ndecision trees, our fairness-aware learning method has been implemented and\nexperimentally evaluated on the reference datasets used to assess fairness\nproperties. The experimental results show that our approach is able to train\ntree models exhibiting a high degree of individual fairness w.r.t. the natural\nstate-of-the-art CART trees and random forests. Moreover, as a by-product,\nthese fair decision trees turn out to be significantly compact, thus enhancing\nthe interpretability of their fairness properties.\n", "versions": [{"version": "v1", "created": "Mon, 4 Jan 2021 12:04:22 GMT"}], "update_date": "2021-01-05", "authors_parsed": [["Ranzato", "Francesco", ""], ["Urban", "Caterina", ""], ["Zanella", "Marco", ""]]}, {"id": "2101.00926", "submitter": "Yujiang He", "authors": "Yujiang He, Bernhard Sick", "title": "CLeaR: An Adaptive Continual Learning Framework for Regression Tasks", "comments": null, "journal-ref": "Published on AI Perspectives (2021)", "doi": "10.1186/s42467-021-00009-8", "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Catastrophic forgetting means that a trained neural network model gradually\nforgets the previously learned tasks when being retrained on new tasks.\nOvercoming the forgetting problem is a major problem in machine learning.\nNumerous continual learning algorithms are very successful in incremental\nlearning of classification tasks, where new samples with their labels appear\nfrequently. However, there is currently no research that addresses the\ncatastrophic forgetting problem in regression tasks as far as we know. This\nproblem has emerged as one of the primary constraints in some applications,\nsuch as renewable energy forecasts. This article clarifies problem-related\ndefinitions and proposes a new methodological framework that can forecast\ntargets and update itself by means of continual learning. The framework\nconsists of forecasting neural networks and buffers, which store newly\ncollected data from a non-stationary data stream in an application. The changed\nprobability distribution of the data stream, which the framework has\nidentified, will be learned sequentially. The framework is called CLeaR\n(Continual Learning for Regression Tasks), where components can be flexibly\ncustomized for a specific application scenario. We design two sets of\nexperiments to evaluate the CLeaR framework concerning fitting error\n(training), prediction error (test), and forgetting ratio. The first one is\nbased on an artificial time series to explore how hyperparameters affect the\nCLeaR framework. The second one is designed with data collected from European\nwind farms to evaluate the CLeaR framework's performance in a real-world\napplication. The experimental results demonstrate that the CLeaR framework can\ncontinually acquire knowledge in the data stream and improve the prediction\naccuracy. The article concludes with further research issues arising from\nrequirements to extend the framework.\n", "versions": [{"version": "v1", "created": "Mon, 4 Jan 2021 12:41:45 GMT"}, {"version": "v2", "created": "Wed, 17 Feb 2021 16:33:24 GMT"}, {"version": "v3", "created": "Fri, 28 May 2021 15:34:40 GMT"}, {"version": "v4", "created": "Fri, 16 Jul 2021 13:03:05 GMT"}], "update_date": "2021-07-19", "authors_parsed": [["He", "Yujiang", ""], ["Sick", "Bernhard", ""]]}, {"id": "2101.00935", "submitter": "Pavel Dvurechensky", "authors": "Pavel Dvurechensky and Mathias Staudigl and Shimrit Shtern", "title": "First-Order Methods for Convex Optimization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  First-order methods for solving convex optimization problems have been at the\nforefront of mathematical optimization in the last 20 years. The rapid\ndevelopment of this important class of algorithms is motivated by the success\nstories reported in various applications, including most importantly machine\nlearning, signal processing, imaging and control theory. First-order methods\nhave the potential to provide low accuracy solutions at low computational\ncomplexity which makes them an attractive set of tools in large-scale\noptimization problems. In this survey we cover a number of key developments in\ngradient-based optimization methods. This includes non-Euclidean extensions of\nthe classical proximal gradient method, and its accelerated versions.\nAdditionally we survey recent developments within the class of projection-free\nmethods, and proximal versions of primal-dual schemes. We give complete proofs\nfor various key results, and highlight the unifying aspects of several\noptimization algorithms.\n", "versions": [{"version": "v1", "created": "Mon, 4 Jan 2021 13:03:38 GMT"}, {"version": "v2", "created": "Wed, 6 Jan 2021 18:34:15 GMT"}], "update_date": "2021-01-07", "authors_parsed": [["Dvurechensky", "Pavel", ""], ["Staudigl", "Mathias", ""], ["Shtern", "Shimrit", ""]]}, {"id": "2101.00947", "submitter": "Thiago Ritto", "authors": "Luan S Prado and Thiago G Ritto", "title": "Data driven Dirichlet sampling on manifolds", "comments": "19 pages, 10 figures, 38 references", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This article presents a novel method to sampling on manifolds based on the\nDirichlet distribution. The proposed strategy allows to completely respect the\nunderlying manifold around which data is observed, and to do massive samplings\nwith low computational effort. This can be very helpful, for instance, in\nneural networks training process, as well as in uncertainty analysis and\nstochastic optimization. Due to its simplicity and efficiency, we believe that\nthe new method has great potential. Three manifolds (two dimensional ring,\nMobius strip and spider geometry) are considered to test the proposed\nmethodology, and then it is employed to an engineering application, related to\ngas seal coefficients.\n", "versions": [{"version": "v1", "created": "Tue, 29 Dec 2020 11:19:45 GMT"}], "update_date": "2021-01-05", "authors_parsed": [["Prado", "Luan S", ""], ["Ritto", "Thiago G", ""]]}, {"id": "2101.00961", "submitter": "Justin Hsu", "authors": "Subhajit Roy, Justin Hsu, Aws Albarghouthi", "title": "Learning Differentially Private Mechanisms", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.LG cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Differential privacy is a formal, mathematical definition of data privacy\nthat has gained traction in academia, industry, and government. The task of\ncorrectly constructing differentially private algorithms is non-trivial, and\nmistakes have been made in foundational algorithms. Currently, there is no\nautomated support for converting an existing, non-private program into a\ndifferentially private version. In this paper, we propose a technique for\nautomatically learning an accurate and differentially private version of a\ngiven non-private program. We show how to solve this difficult program\nsynthesis problem via a combination of techniques: carefully picking\nrepresentative example inputs, reducing the problem to continuous optimization,\nand mapping the results back to symbolic expressions. We demonstrate that our\napproach is able to learn foundational algorithms from the differential privacy\nliterature and significantly outperforms natural program synthesis baselines.\n", "versions": [{"version": "v1", "created": "Mon, 4 Jan 2021 13:33:57 GMT"}], "update_date": "2021-01-05", "authors_parsed": [["Roy", "Subhajit", ""], ["Hsu", "Justin", ""], ["Albarghouthi", "Aws", ""]]}, {"id": "2101.00962", "submitter": "Sebastian Mitusch", "authors": "Sebastian K. Mitusch, Simon W. Funke, Miroslav Kuchta", "title": "Hybrid FEM-NN models: Combining artificial neural networks with the\n  finite element method", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.NA cs.LG cs.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a methodology combining neural networks with physical principle\nconstraints in the form of partial differential equations (PDEs). The approach\nallows to train neural networks while respecting the PDEs as a strong\nconstraint in the optimisation as apposed to making them part of the loss\nfunction. The resulting models are discretised in space by the finite element\nmethod (FEM). The methodology applies to both stationary and transient as well\nas linear/nonlinear PDEs. We describe how the methodology can be implemented as\nan extension of the existing FEM framework FEniCS and its algorithmic\ndifferentiation tool dolfin-adjoint. Through series of examples we demonstrate\ncapabilities of the approach to recover coefficients and missing PDE operators\nfrom observations. Further, the proposed method is compared with alternative\nmethodologies, namely, physics informed neural networks and standard\nPDE-constrained optimisation. Finally, we demonstrate the method on a complex\ncardiac cell model problem using deep neural networks.\n", "versions": [{"version": "v1", "created": "Mon, 4 Jan 2021 13:36:06 GMT"}], "update_date": "2021-01-05", "authors_parsed": [["Mitusch", "Sebastian K.", ""], ["Funke", "Simon W.", ""], ["Kuchta", "Miroslav", ""]]}, {"id": "2101.00967", "submitter": "Lynn Wahab", "authors": "Lynn Wahab, Ezzat Chebaro, Jad Ismail, Amir Nasrelddine, Ali El-Zein", "title": "A Predictive Model for Geographic Distributions of Mangroves", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Climate change is an impending disaster which is of pressing concern more and\nmore every year. Countless efforts have been made to study the long-term\neffects of climate change on agriculture, land resources, and biodiversity.\nStudies involving marine life, however, are less prevalent in the literature.\nOur research studies the available data on the population of mangroves (groups\nof shrubs or small trees living in saline coastal intertidal zones) and their\ncorrelations to climate change variables, specifically, temperature, heat\ncontent, various sea levels, and sea salinity. Mangroves are especially\nrelevant to oceanic ecosystems because of their protective nature towards other\nmarine life, as well as their high absorption rate of carbon dioxide, and their\nability to withstand varying levels of salinity of our coasts. The change in\nglobal distribution was studied based on global distributions of the previous\nyear, as well as ocean heat content, salinity, temperature, halosteric sea\nlevel, thermosteric sea level, and total steric sea level. The best performing\npredictive model was a support vector regressor, which yielded a correlation\ncoefficient of 0.9998.\n", "versions": [{"version": "v1", "created": "Wed, 30 Dec 2020 22:52:50 GMT"}, {"version": "v2", "created": "Sun, 10 Jan 2021 18:42:49 GMT"}], "update_date": "2021-01-12", "authors_parsed": [["Wahab", "Lynn", ""], ["Chebaro", "Ezzat", ""], ["Ismail", "Jad", ""], ["Nasrelddine", "Amir", ""], ["El-Zein", "Ali", ""]]}, {"id": "2101.00973", "submitter": "Adil Karjauv", "authors": "Chaoning Zhang, Adil Karjauv, Philipp Benz, In So Kweon", "title": "Towards Robust Data Hiding Against (JPEG) Compression: A\n  Pseudo-Differentiable Deep Learning Approach", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CR cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Data hiding is one widely used approach for protecting authentication and\nownership. Most multimedia content like images and videos are transmitted or\nsaved in the compressed form. This kind of lossy compression, such as JPEG, can\ndestroy the hidden data, which raises the need of robust data hiding. It is\nstill an open challenge to achieve the goal of data hiding that can be against\nthese compressions. Recently, deep learning has shown large success in data\nhiding, while non-differentiability of JPEG makes it challenging to train a\ndeep pipeline for improving robustness against lossy compression. The existing\nSOTA approaches replace the non-differentiable parts with differentiable\nmodules that perform similar operations. Multiple limitations exist: (a) large\nengineering effort; (b) requiring a white-box knowledge of compression attacks;\n(c) only works for simple compression like JPEG. In this work, we propose a\nsimple yet effective approach to address all the above limitations at once.\nBeyond JPEG, our approach has been shown to improve robustness against various\nimage and video lossy compression algorithms.\n", "versions": [{"version": "v1", "created": "Wed, 30 Dec 2020 12:30:09 GMT"}], "update_date": "2021-01-05", "authors_parsed": [["Zhang", "Chaoning", ""], ["Karjauv", "Adil", ""], ["Benz", "Philipp", ""], ["Kweon", "In So", ""]]}, {"id": "2101.00977", "submitter": "Yilun Zhou", "authors": "Yilun Zhou, Adithya Renduchintala, Xian Li, Sida Wang, Yashar Mehdad,\n  Asish Ghoshal", "title": "Towards Understanding the Behaviors of Optimal Deep Active Learning\n  Algorithms", "comments": "AISTATS 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Active learning (AL) algorithms may achieve better performance with fewer\ndata because the model guides the data selection process. While many algorithms\nhave been proposed, there is little study on what the optimal AL algorithm\nlooks like, which would help researchers understand where their models fall\nshort and iterate on the design. In this paper, we present a simulated\nannealing algorithm to search for this optimal oracle and analyze it for\nseveral tasks. We present qualitative and quantitative insights into the\nbehaviors of this oracle, comparing and contrasting them with those of various\nheuristics. Moreover, we are able to consistently improve the heuristics using\none particular insight. We hope that our findings can better inform future\nactive learning research. The code is available at\nhttps://github.com/YilunZhou/optimal-active-learning.\n", "versions": [{"version": "v1", "created": "Tue, 29 Dec 2020 22:56:42 GMT"}, {"version": "v2", "created": "Sat, 20 Feb 2021 20:15:18 GMT"}], "update_date": "2021-02-23", "authors_parsed": [["Zhou", "Yilun", ""], ["Renduchintala", "Adithya", ""], ["Li", "Xian", ""], ["Wang", "Sida", ""], ["Mehdad", "Yashar", ""], ["Ghoshal", "Asish", ""]]}, {"id": "2101.00982", "submitter": "Michael Weiss", "authors": "Michael Weiss and Paolo Tonella", "title": "Uncertainty-Wizard: Fast and User-Friendly Neural Network Uncertainty\n  Quantification", "comments": "Accepted for publication at the IEEE International Conference on\n  Software Testing, Verification and Validation 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Uncertainty and confidence have been shown to be useful metrics in a wide\nvariety of techniques proposed for deep learning testing, including test data\nselection and system supervision.We present uncertainty-wizard, a tool that\nallows to quantify such uncertainty and confidence in artificial neural\nnetworks. It is built on top of the industry-leading tf.keras deep learning API\nand it provides a near-transparent and easy to understand interface. At the\nsame time, it includes major performance optimizations that we benchmarked on\ntwo different machines and different configurations.\n", "versions": [{"version": "v1", "created": "Tue, 29 Dec 2020 15:38:24 GMT"}, {"version": "v2", "created": "Thu, 28 Jan 2021 14:32:24 GMT"}], "update_date": "2021-01-29", "authors_parsed": [["Weiss", "Michael", ""], ["Tonella", "Paolo", ""]]}, {"id": "2101.00989", "submitter": "Yanghao Zhang", "authors": "Yanghao Zhang, Fu Wang and Wenjie Ruan", "title": "Fooling Object Detectors: Adversarial Attacks by Half-Neighbor Masks", "comments": "To appear in the Proceedings of the CIKM 2020 Workshops published by\n  CEUR-WS", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Although there are a great number of adversarial attacks on deep learning\nbased classifiers, how to attack object detection systems has been rarely\nstudied. In this paper, we propose a Half-Neighbor Masked Projected Gradient\nDescent (HNM-PGD) based attack, which can generate strong perturbation to fool\ndifferent kinds of detectors under strict constraints. We also applied the\nproposed HNM-PGD attack in the CIKM 2020 AnalytiCup Competition, which was\nranked within the top 1% on the leaderboard. We release the code at\nhttps://github.com/YanghaoZYH/HNM-PGD.\n", "versions": [{"version": "v1", "created": "Mon, 4 Jan 2021 14:03:22 GMT"}], "update_date": "2021-01-05", "authors_parsed": [["Zhang", "Yanghao", ""], ["Wang", "Fu", ""], ["Ruan", "Wenjie", ""]]}, {"id": "2101.00990", "submitter": "Alejandro Gonz\\'alez Alzate", "authors": "Manel Mateos, Alejandro Gonz\\'alez, Xavier Sevillano", "title": "Guiding GANs: How to control non-conditional pre-trained GANs for\n  conditional image generation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Generative Adversarial Networks (GANs) are an arrange of two neural networks\n-- the generator and the discriminator -- that are jointly trained to generate\nartificial data, such as images, from random inputs. The quality of these\ngenerated images has recently reached such levels that can often lead both\nmachines and humans into mistaking fake for real examples. However, the process\nperformed by the generator of the GAN has some limitations when we want to\ncondition the network to generate images from subcategories of a specific\nclass. Some recent approaches tackle this \\textit{conditional generation} by\nintroducing extra information prior to the training process, such as image\nsemantic segmentation or textual descriptions. While successful, these\ntechniques still require defining beforehand the desired subcategories and\ncollecting large labeled image datasets representing them to train the GAN from\nscratch. In this paper we present a novel and alternative method for guiding\ngeneric non-conditional GANs to behave as conditional GANs. Instead of\nre-training the GAN, our approach adds into the mix an encoder network to\ngenerate the high-dimensional random input vectors that are fed to the\ngenerator network of a non-conditional GAN to make it generate images from a\nspecific subcategory. In our experiments, when compared to training a\nconditional GAN from scratch, our guided GAN is able to generate artificial\nimages of perceived quality comparable to that of non-conditional GANs after\ntraining the encoder on just a few hundreds of images, which substantially\naccelerates the process and enables adding new subcategories seamlessly.\n", "versions": [{"version": "v1", "created": "Mon, 4 Jan 2021 14:03:32 GMT"}], "update_date": "2021-01-05", "authors_parsed": [["Mateos", "Manel", ""], ["Gonz\u00e1lez", "Alejandro", ""], ["Sevillano", "Xavier", ""]]}, {"id": "2101.01000", "submitter": "Haitao Lin", "authors": "Haitao Lin, Zhangyang Gao, Lirong Wu, Stan. Z. Li", "title": "Conditional Local Filters with Explainers for Spatio-Temporal\n  Forecasting", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Spatio-temporal prediction is challenging attributing to the high\nnonlinearity in temporal dynamics as well as complex dependency and\nlocation-characterized pattern in spatial domains, especially in fields like\ngeophysics, traffic flow, etc. In this work, a novel graph-based directed\nconvolution is proposed to capture the spatial dependency. To model the\nvariable local pattern, we propose conditional local filters for convolution on\nthe directed graph, parameterized by the functions on local representation of\ncoordinate based on tangent space. The filter is embedded in a Recurrent Neural\nNetwork (RNN) architecture for modeling the temporal dynamics with an explainer\nestablished for interpretability of different time intervals' pattern. The\nmethods are evaluated on real-world datasets including road network traffic\nflow, earth surface temperature \\& wind flows and disease spread datasets,\nachieving the state-of-the-art performance with improvements.\n", "versions": [{"version": "v1", "created": "Mon, 4 Jan 2021 14:22:11 GMT"}], "update_date": "2021-01-05", "authors_parsed": [["Lin", "Haitao", ""], ["Gao", "Zhangyang", ""], ["Wu", "Lirong", ""], ["Li", "Stan. Z.", ""]]}, {"id": "2101.01015", "submitter": "Anandharaju Durai Raju", "authors": "Anandharaju Durai Raju, Ke Wang", "title": "Echelon: Two-Tier Malware Detection for Raw Executables to Reduce False\n  Alarms", "comments": "12 pages, 8 figures, 5 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Existing malware detection approaches suffer from a simplistic trade-off\nbetween false positive rate (FPR) and true positive rate (TPR) due to a single\ntier classification approach, where the two measures adversely affect one\nanother. The practical implication for malware detection is that FPR must be\nkept at an acceptably low level while TPR remains high. To this end, we propose\na two-tiered learning, called ``Echelon\", from raw byte data with no need for\nhand-crafted features. The first tier locks FPR at a specified target level,\nwhereas the second tier improves TPR while maintaining the locked FPR. The core\nof Echelon lies at extracting activation information of the hidden layers of\nfirst tier model for constructing a stronger second tier model. Echelon is a\nframework in that it allows any existing CNN based model to be adapted in both\ntiers. We present experimental results of evaluating Echelon by adapting the\nstate-of-the-art malware detection model ``Malconv\" in the first and second\ntiers.\n", "versions": [{"version": "v1", "created": "Mon, 4 Jan 2021 14:54:20 GMT"}], "update_date": "2021-01-05", "authors_parsed": [["Raju", "Anandharaju Durai", ""], ["Wang", "Ke", ""]]}, {"id": "2101.01032", "submitter": "Tao Xiang", "authors": "Tao Xiang, Hangcheng Liu, Shangwei Guo, Tianwei Zhang, Xiaofeng Liao", "title": "Local Black-box Adversarial Attacks: A Query Efficient Approach", "comments": "This work has been submitted to the IEEE for possible publication", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CR cs.LG", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Adversarial attacks have threatened the application of deep neural networks\nin security-sensitive scenarios. Most existing black-box attacks fool the\ntarget model by interacting with it many times and producing global\nperturbations. However, global perturbations change the smooth and\ninsignificant background, which not only makes the perturbation more easily be\nperceived but also increases the query overhead. In this paper, we propose a\nnovel framework to perturb the discriminative areas of clean examples only\nwithin limited queries in black-box attacks. Our framework is constructed based\non two types of transferability. The first one is the transferability of model\ninterpretations. Based on this property, we identify the discriminative areas\nof a given clean example easily for local perturbations. The second is the\ntransferability of adversarial examples. It helps us to produce a local\npre-perturbation for improving query efficiency. After identifying the\ndiscriminative areas and pre-perturbing, we generate the final adversarial\nexamples from the pre-perturbed example by querying the targeted model with two\nkinds of black-box attack techniques, i.e., gradient estimation and random\nsearch. We conduct extensive experiments to show that our framework can\nsignificantly improve the query efficiency during black-box perturbing with a\nhigh attack success rate. Experimental results show that our attacks outperform\nstate-of-the-art black-box attacks under various system settings.\n", "versions": [{"version": "v1", "created": "Mon, 4 Jan 2021 15:32:16 GMT"}], "update_date": "2021-01-05", "authors_parsed": [["Xiang", "Tao", ""], ["Liu", "Hangcheng", ""], ["Guo", "Shangwei", ""], ["Zhang", "Tianwei", ""], ["Liao", "Xiaofeng", ""]]}, {"id": "2101.01041", "submitter": "Xiangyuan Zhang", "authors": "Kaiqing Zhang, Xiangyuan Zhang, Bin Hu, Tamer Ba\\c{s}ar", "title": "Derivative-Free Policy Optimization for Linear Risk-Sensitive and Robust\n  Control Design: Implicit Regularization and Sample Complexity", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.AI cs.LG cs.SY eess.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Direct policy search serves as one of the workhorses in modern reinforcement\nlearning (RL), and its applications in continuous control tasks have recently\nattracted increasing attention. In this work, we investigate the convergence\ntheory of policy gradient (PG) methods for learning the linear risk-sensitive\nand robust controller. In particular, we develop PG methods that can be\nimplemented in a derivative-free fashion by sampling system trajectories, and\nestablish both global convergence and sample complexity results in the\nsolutions of two fundamental settings in risk-sensitive and robust control: the\nfinite-horizon linear exponential quadratic Gaussian, and the finite-horizon\nlinear-quadratic disturbance attenuation problems. As a by-product, our results\nalso provide the first sample complexity for the global convergence of PG\nmethods on solving zero-sum linear-quadratic dynamic games, a\nnonconvex-nonconcave minimax optimization problem that serves as a baseline\nsetting in multi-agent reinforcement learning (MARL) with continuous spaces.\nOne feature of our algorithms is that during the learning phase, a certain\nlevel of robustness/risk-sensitivity of the controller is preserved, which we\ntermed as the implicit regularization property, and is an essential requirement\nin safety-critical control systems.\n", "versions": [{"version": "v1", "created": "Mon, 4 Jan 2021 16:00:46 GMT"}, {"version": "v2", "created": "Mon, 31 May 2021 02:41:16 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Zhang", "Kaiqing", ""], ["Zhang", "Xiangyuan", ""], ["Hu", "Bin", ""], ["Ba\u015far", "Tamer", ""]]}, {"id": "2101.01042", "submitter": "Gustavo de Rosa", "authors": "Gustavo H. de Rosa, Mateus Roder, Jo\\~ao P. Papa", "title": "Fast Ensemble Learning Using Adversarially-Generated Restricted\n  Boltzmann Machines", "comments": "26 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Machine Learning has been applied in a wide range of tasks throughout the\nlast years, ranging from image classification to autonomous driving and natural\nlanguage processing. Restricted Boltzmann Machine (RBM) has received recent\nattention and relies on an energy-based structure to model data probability\ndistributions. Notwithstanding, such a technique is susceptible to adversarial\nmanipulation, i.e., slightly or profoundly modified data. An alternative to\novercome the adversarial problem lies in the Generative Adversarial Networks\n(GAN), capable of modeling data distributions and generating adversarial data\nthat resemble the original ones. Therefore, this work proposes to artificially\ngenerate RBMs using Adversarial Learning, where pre-trained weight matrices\nserve as the GAN inputs. Furthermore, it proposes to sample copious amounts of\nmatrices and combine them into ensembles, alleviating the burden of training\nnew models'. Experimental results demonstrate the suitability of the proposed\napproach under image reconstruction and image classification tasks, and\ndescribe how artificial-based ensembles are alternatives to pre-training vast\namounts of RBMs.\n", "versions": [{"version": "v1", "created": "Mon, 4 Jan 2021 16:00:47 GMT"}], "update_date": "2021-01-05", "authors_parsed": [["de Rosa", "Gustavo H.", ""], ["Roder", "Mateus", ""], ["Papa", "Jo\u00e3o P.", ""]]}, {"id": "2101.01045", "submitter": "Ngoc Hoang Anh Mai", "authors": "Thi Lan Dinh and Ngoc Hoang Anh Mai", "title": "Comparing different subgradient methods for solving convex optimization\n  problems with functional constraints", "comments": "25 pages, 10 tables, 15 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We provide a dual subgradient method and a primal-dual subgradient method for\nstandard convex optimization problems with complexity\n$\\mathcal{O}(\\varepsilon^{-2})$ and $\\mathcal{O}(\\varepsilon^{-2r})$, for all\n$r> 1$, respectively. They are based on recent Metel-Takeda's work in\n[arXiv:2009.12769, 2020, pp. 1-12] and Boyd's method in [Lecture notes of\nEE364b, Stanford University, Spring 2013-14, pp. 1-39]. The efficiency of our\nmethods is numerically illustrated in a comparison to the others.\n", "versions": [{"version": "v1", "created": "Mon, 4 Jan 2021 16:09:55 GMT"}], "update_date": "2021-01-05", "authors_parsed": [["Dinh", "Thi Lan", ""], ["Mai", "Ngoc Hoang Anh", ""]]}, {"id": "2101.01052", "submitter": "Sagar Gubbi Venkatesh", "authors": "Sagar Gubbi and Shishir Kolathaya and Bharadwaj Amrutur", "title": "Imitation Learning for High Precision Peg-in-Hole Tasks", "comments": "Accepted at ICCAR 2020", "journal-ref": null, "doi": "10.1109/ICCAR49639.2020.9108072", "report-no": null, "categories": "cs.RO cs.LG", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Industrial robot manipulators are not able to match the precision and speed\nwith which humans are able to execute contact rich tasks even to this day.\nTherefore, as a means overcome this gap, we demonstrate generative methods for\nimitating a peg-in-hole insertion task in a 6-DOF robot manipulator. In\nparticular, generative adversarial imitation learning (GAIL) is used to\nsuccessfully achieve this task with a 10 um, and a 6 um peg-hole clearance on\nthe Yaskawa GP8 industrial robot. Experimental results show that the policy\nsuccessfully learns within 20 episodes from a handful of human expert\ndemonstrations on the robot (i.e., < 10 tele-operated robot demonstrations).\nThe insertion time improves from > 20 seconds (which also includes failed\ninsertions) to < 15 seconds, thereby validating the effectiveness of this\napproach.\n", "versions": [{"version": "v1", "created": "Sat, 26 Dec 2020 08:42:15 GMT"}], "update_date": "2021-01-05", "authors_parsed": [["Gubbi", "Sagar", ""], ["Kolathaya", "Shishir", ""], ["Amrutur", "Bharadwaj", ""]]}, {"id": "2101.01053", "submitter": "Sagar Gubbi Venkatesh", "authors": "Sagar Gubbi Venkatesh and Raviteja Upadrashta and Shishir Kolathaya\n  and Bharadwaj Amrutur", "title": "Multi-Instance Aware Localization for End-to-End Imitation Learning", "comments": "Accepted at IROS 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.LG", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Existing architectures for imitation learning using image-to-action policy\nnetworks perform poorly when presented with an input image containing multiple\ninstances of the object of interest, especially when the number of expert\ndemonstrations available for training are limited. We show that end-to-end\npolicy networks can be trained in a sample efficient manner by (a) appending\nthe feature map output of the vision layers with an embedding that can indicate\ninstance preference or take advantage of an implicit preference present in the\nexpert demonstrations, and (b) employing an autoregressive action generator\nnetwork for the control layers. The proposed architecture for localization has\nimproved accuracy and sample efficiency and can generalize to the presence of\nmore instances of objects than seen during training. When used for end-to-end\nimitation learning to perform reach, push, and pick-and-place tasks on a real\nrobot, training is achieved with as few as 15 expert demonstrations.\n", "versions": [{"version": "v1", "created": "Sat, 26 Dec 2020 08:23:08 GMT"}], "update_date": "2021-01-05", "authors_parsed": [["Venkatesh", "Sagar Gubbi", ""], ["Upadrashta", "Raviteja", ""], ["Kolathaya", "Shishir", ""], ["Amrutur", "Bharadwaj", ""]]}, {"id": "2101.01054", "submitter": "Sagar Gubbi Venkatesh", "authors": "Sagar Gubbi and Bharadwaj Amrutur", "title": "Scene Text Detection for Augmented Reality -- Character Bigram Approach\n  to reduce False Positive Rate", "comments": null, "journal-ref": null, "doi": "10.1007/s40012-018-0203-2", "report-no": null, "categories": "cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Natural scene text detection is an important aspect of scene understanding\nand could be a useful tool in building engaging augmented reality applications.\nIn this work, we address the problem of false positives in text spotting. We\npropose improving the performace of sliding window text spotters by looking for\ncharacter pairs (bigrams) rather than single characters. An efficient\nconvolutional neural network is designed and trained to detect bigrams. The\nproposed detector reduces false positive rate by 28.16% on the ICDAR 2015\ndataset. We demonstrate that detecting bigrams is a computationally inexpensive\nway to improve sliding window text spotters.\n", "versions": [{"version": "v1", "created": "Sat, 26 Dec 2020 08:56:10 GMT"}], "update_date": "2021-01-05", "authors_parsed": [["Gubbi", "Sagar", ""], ["Amrutur", "Bharadwaj", ""]]}, {"id": "2101.01055", "submitter": "Sagar Gubbi Venkatesh", "authors": "Sagar Gubbi Venkatesh and Nihesh Rathod and Shishir Kolathaya and\n  Bharadwaj Amrutur", "title": "Stochastic Action Prediction for Imitation Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.RO", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Imitation learning is a data-driven approach to acquiring skills that relies\non expert demonstrations to learn a policy that maps observations to actions.\nWhen performing demonstrations, experts are not always consistent and might\naccomplish the same task in slightly different ways. In this paper, we\ndemonstrate inherent stochasticity in demonstrations collected for tasks\nincluding line following with a remote-controlled car and manipulation tasks\nincluding reaching, pushing, and picking and placing an object. We model\nstochasticity in the data distribution using autoregressive action generation,\ngenerative adversarial nets, and variational prediction and compare the\nperformance of these approaches. We find that accounting for stochasticity in\nthe expert data leads to substantial improvement in the success rate of task\ncompletion.\n", "versions": [{"version": "v1", "created": "Sat, 26 Dec 2020 08:02:33 GMT"}], "update_date": "2021-01-05", "authors_parsed": [["Venkatesh", "Sagar Gubbi", ""], ["Rathod", "Nihesh", ""], ["Kolathaya", "Shishir", ""], ["Amrutur", "Bharadwaj", ""]]}, {"id": "2101.01076", "submitter": "Jiaheng Xie", "authors": "Jiaheng Xie, Yidong Chai, Xiao Liu", "title": "Understanding Health Misinformation Transmission: An Interpretable Deep\n  Learning Approach to Manage Infodemics", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Health misinformation on social media devastates physical and mental health,\ninvalidates health gains, and potentially costs lives. Understanding how health\nmisinformation is transmitted is an urgent goal for researchers, social media\nplatforms, health sectors, and policymakers to mitigate those ramifications.\nDeep learning methods have been deployed to predict the spread of\nmisinformation. While achieving the state-of-the-art predictive performance,\ndeep learning methods lack the interpretability due to their blackbox nature.\nTo remedy this gap, this study proposes a novel interpretable deep learning\napproach, Generative Adversarial Network based Piecewise Wide and Attention\nDeep Learning (GAN-PiWAD), to predict health misinformation transmission in\nsocial media. Improving upon state-of-the-art interpretable methods, GAN-PiWAD\ncaptures the interactions among multi-modal data, offers unbiased estimation of\nthe total effect of each feature, and models the dynamic total effect of each\nfeature when its value varies. We select features according to social exchange\ntheory and evaluate GAN-PiWAD on 4,445 misinformation videos. The proposed\napproach outperformed strong benchmarks. Interpretation of GAN-PiWAD indicates\nvideo description, negative video content, and channel credibility are key\nfeatures that drive viral transmission of misinformation. This study\ncontributes to IS with a novel interpretable deep learning method that is\ngeneralizable to understand other human decision factors. Our findings provide\ndirect implications for social media platforms and policymakers to design\nproactive interventions to identify misinformation, control transmissions, and\nmanage infodemics.\n", "versions": [{"version": "v1", "created": "Mon, 21 Dec 2020 15:49:19 GMT"}], "update_date": "2021-01-05", "authors_parsed": [["Xie", "Jiaheng", ""], ["Chai", "Yidong", ""], ["Liu", "Xiao", ""]]}, {"id": "2101.01078", "submitter": "Quanming Yao", "authors": "Hansi Yang and Quanming Yao and James Kwok", "title": "Tensorizing Subgraph Search in the Supernet", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, a special kind of graph, i.e., supernet, which allows two nodes\nconnected by multi-choice edges, has exhibited its power in neural architecture\nsearch (NAS) by searching for better architectures for computer vision (CV) and\nnatural language processing (NLP) tasks. In this paper, we discover that the\ndesign of such discrete architectures also appears in many other important\nlearning tasks, e.g., logical chain inference in knowledge graphs (KGs) and\nmeta-path discovery in heterogeneous information networks (HINs). Thus, we are\nmotivated to generalize the supernet search problem on a broader horizon.\nHowever, none of the existing works are effective since the supernet topology\nis highly task-dependent and diverse. To address this issue, we propose to\ntensorize the supernet, i.e., unify the subgraph search problems by a tensor\nformulation and encode the topology inside the supernet by a tensor network. We\nfurther propose an efficient algorithm that admits both stochastic and\ndeterministic objectives to solve the search problem. Finally, we perform\nextensive experiments on diverse learning tasks, i.e., architecture design for\nCV, logic inference for KG, and meta-path discovery for HIN. Empirical results\ndemonstrate that our method leads to better performance and architectures.\n", "versions": [{"version": "v1", "created": "Mon, 4 Jan 2021 16:38:00 GMT"}], "update_date": "2021-01-05", "authors_parsed": [["Yang", "Hansi", ""], ["Yao", "Quanming", ""], ["Kwok", "James", ""]]}, {"id": "2101.01086", "submitter": "Matthieu Jedor", "authors": "Matthieu Jedor, Jonathan Lou\\\"edec, Vianney Perchet", "title": "Be Greedy in Multi-Armed Bandits", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The Greedy algorithm is the simplest heuristic in sequential decision problem\nthat carelessly takes the locally optimal choice at each round, disregarding\nany advantages of exploring and/or information gathering. Theoretically, it is\nknown to sometimes have poor performances, for instance even a linear regret\n(with respect to the time horizon) in the standard multi-armed bandit problem.\nOn the other hand, this heuristic performs reasonably well in practice and it\neven has sublinear, and even near-optimal, regret bounds in some very specific\nlinear contextual and Bayesian bandit models. We build on a recent line of work\nand investigate bandit settings where the number of arms is relatively large\nand where simple greedy algorithms enjoy highly competitive performance, both\nin theory and in practice. We first provide a generic worst-case bound on the\nregret of the Greedy algorithm. When combined with some arms subsampling, we\nprove that it verifies near-optimal worst-case regret bounds in continuous,\ninfinite and many-armed bandit problems. Moreover, for shorter time spans, the\ntheoretical relative suboptimality of Greedy is even reduced. As a consequence,\nwe subversively claim that for many interesting problems and associated\nhorizons, the best compromise between theoretical guarantees, practical\nperformances and computational burden is definitely to follow the greedy\nheuristic. We support our claim by many numerical experiments that show\nsignificant improvements compared to the state-of-the-art, even for moderately\nlong time horizon.\n", "versions": [{"version": "v1", "created": "Mon, 4 Jan 2021 16:47:02 GMT"}], "update_date": "2021-01-05", "authors_parsed": [["Jedor", "Matthieu", ""], ["Lou\u00ebdec", "Jonathan", ""], ["Perchet", "Vianney", ""]]}, {"id": "2101.01097", "submitter": "Junyong You", "authors": "Junyong You, Jari Korhonen", "title": "Transformer for Image Quality Assessment", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Transformer has become the new standard method in natural language processing\n(NLP), and it also attracts research interests in computer vision area. In this\npaper we investigate the application of Transformer in Image Quality (TRIQ)\nassessment. Following the original Transformer encoder employed in Vision\nTransformer (ViT), we propose an architecture of using a shallow Transformer\nencoder on the top of a feature map extracted by convolution neural networks\n(CNN). Adaptive positional embedding is employed in the Transformer encoder to\nhandle images with arbitrary resolutions. Different settings of Transformer\narchitectures have been investigated on publicly available image quality\ndatabases. We have found that the proposed TRIQ architecture achieves\noutstanding performance. The implementation of TRIQ is published on Github\n(https://github.com/junyongyou/triq).\n", "versions": [{"version": "v1", "created": "Wed, 30 Dec 2020 18:43:11 GMT"}, {"version": "v2", "created": "Fri, 8 Jan 2021 12:12:32 GMT"}], "update_date": "2021-01-11", "authors_parsed": [["You", "Junyong", ""], ["Korhonen", "Jari", ""]]}, {"id": "2101.01100", "submitter": "Jason Altschuler", "authors": "Jason M. Altschuler and Enric Boix-Adsera", "title": "Wasserstein barycenters are NP-hard to compute", "comments": "18 pages (9 pages main text)", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.CC cs.DS cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The problem of computing Wasserstein barycenters (a.k.a. Optimal Transport\nbarycenters) has attracted considerable recent attention due to many\napplications in data science. While there exist polynomial-time algorithms in\nany fixed dimension, all known runtimes suffer exponentially in the dimension.\nIt is an open question whether this exponential dependence is improvable to a\npolynomial dependence. This paper proves that unless P=NP, the answer is no.\nThis uncovers a \"curse of dimensionality\" for Wasserstein barycenter\ncomputation which does not occur for Optimal Transport computation. Moreover,\nour hardness results for computing Wasserstein barycenters extend to\napproximate computation, to seemingly simple cases of the problem, and to\naveraging probability distributions in other Optimal Transport metrics.\n", "versions": [{"version": "v1", "created": "Mon, 4 Jan 2021 17:16:45 GMT"}], "update_date": "2021-01-05", "authors_parsed": [["Altschuler", "Jason M.", ""], ["Boix-Adsera", "Enric", ""]]}, {"id": "2101.01104", "submitter": "Zhen Fang", "authors": "Li Zhong, Zhen Fang, Feng Liu, Jie Lu, Bo Yuan, Guangquan Zhang", "title": "How does the Combined Risk Affect the Performance of Unsupervised Domain\n  Adaptation Approaches?", "comments": "9 pages, 3 figures, Accepted by Association for the Advancement of\n  Artificial Intelligence 2021 (AAAI 2021)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Unsupervised domain adaptation (UDA) aims to train a target classifier with\nlabeled samples from the source domain and unlabeled samples from the target\ndomain. Classical UDA learning bounds show that target risk is upper bounded by\nthree terms: source risk, distribution discrepancy, and combined risk. Based on\nthe assumption that the combined risk is a small fixed value, methods based on\nthis bound train a target classifier by only minimizing estimators of the\nsource risk and the distribution discrepancy. However, the combined risk may\nincrease when minimizing both estimators, which makes the target risk\nuncontrollable. Hence the target classifier cannot achieve ideal performance if\nwe fail to control the combined risk. To control the combined risk, the key\nchallenge takes root in the unavailability of the labeled samples in the target\ndomain. To address this key challenge, we propose a method named E-MixNet.\nE-MixNet employs enhanced mixup, a generic vicinal distribution, on the labeled\nsource samples and pseudo-labeled target samples to calculate a proxy of the\ncombined risk. Experiments show that the proxy can effectively curb the\nincrease of the combined risk when minimizing the source risk and distribution\ndiscrepancy. Furthermore, we show that if the proxy of the combined risk is\nadded into loss functions of four representative UDA methods, their performance\nis also improved.\n", "versions": [{"version": "v1", "created": "Wed, 30 Dec 2020 00:46:57 GMT"}], "update_date": "2021-01-05", "authors_parsed": [["Zhong", "Li", ""], ["Fang", "Zhen", ""], ["Liu", "Feng", ""], ["Lu", "Jie", ""], ["Yuan", "Bo", ""], ["Zhang", "Guangquan", ""]]}, {"id": "2101.01121", "submitter": "Konstantinos P. Panousis", "authors": "Konstantinos P. Panousis and Sotirios Chatzis and Antonios Alexos and\n  Sergios Theodoridis", "title": "Local Competition and Stochasticity for Adversarial Robustness in Deep\n  Learning", "comments": "Accepted AISTATS 2021. arXiv admin note: text overlap with\n  arXiv:2006.10620", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This work addresses adversarial robustness in deep learning by considering\ndeep networks with stochastic local winner-takes-all (LWTA) activations. This\ntype of network units result in sparse representations from each model layer,\nas the units are organized in blocks where only one unit generates a non-zero\noutput. The main operating principle of the introduced units lies on stochastic\narguments, as the network performs posterior sampling over competing units to\nselect the winner. We combine these LWTA arguments with tools from the field of\nBayesian non-parametrics, specifically the stick-breaking construction of the\nIndian Buffet Process, to allow for inferring the sub-part of each layer that\nis essential for modeling the data at hand. Then, inference is performed by\nmeans of stochastic variational Bayes. We perform a thorough experimental\nevaluation of our model using benchmark datasets. As we show, our method\nachieves high robustness to adversarial perturbations, with state-of-the-art\nperformance in powerful adversarial attack schemes.\n", "versions": [{"version": "v1", "created": "Mon, 4 Jan 2021 17:40:52 GMT"}, {"version": "v2", "created": "Mon, 29 Mar 2021 12:35:02 GMT"}], "update_date": "2021-03-30", "authors_parsed": [["Panousis", "Konstantinos P.", ""], ["Chatzis", "Sotirios", ""], ["Alexos", "Antonios", ""], ["Theodoridis", "Sergios", ""]]}, {"id": "2101.01134", "submitter": "Danica J. Sutherland", "authors": "Pritish Kamath and Akilesh Tangella and Danica J. Sutherland and\n  Nathan Srebro", "title": "Does Invariant Risk Minimization Capture Invariance?", "comments": "Code is available in the arXiv ancillary files, linked from this page", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We show that the Invariant Risk Minimization (IRM) formulation of Arjovsky et\nal. (2019) can fail to capture \"natural\" invariances, at least when used in its\npractical \"linear\" form, and even on very simple problems which directly follow\nthe motivating examples for IRM. This can lead to worse generalization on new\nenvironments, even when compared to unconstrained ERM. The issue stems from a\nsignificant gap between the linear variant (as in their concrete method IRMv1)\nand the full non-linear IRM formulation. Additionally, even when capturing the\n\"right\" invariances, we show that it is possible for IRM to learn a sub-optimal\npredictor, due to the loss function not being invariant across environments.\nThe issues arise even when measuring invariance on the population\ndistributions, but are exacerbated by the fact that IRM is extremely fragile to\nsampling.\n", "versions": [{"version": "v1", "created": "Mon, 4 Jan 2021 18:02:45 GMT"}, {"version": "v2", "created": "Fri, 26 Feb 2021 23:21:48 GMT"}], "update_date": "2021-03-02", "authors_parsed": [["Kamath", "Pritish", ""], ["Tangella", "Akilesh", ""], ["Sutherland", "Danica J.", ""], ["Srebro", "Nathan", ""]]}, {"id": "2101.01137", "submitter": "Haim Avron", "authors": "Paz Fink Shustin, Haim Avron", "title": "Gauss-Legendre Features for Gaussian Process Regression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.NA cs.DS cs.LG cs.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Gaussian processes provide a powerful probabilistic kernel learning\nframework, which allows learning high quality nonparametric regression models\nvia methods such as Gaussian process regression. Nevertheless, the learning\nphase of Gaussian process regression requires massive computations which are\nnot realistic for large datasets. In this paper, we present a Gauss-Legendre\nquadrature based approach for scaling up Gaussian process regression via a low\nrank approximation of the kernel matrix. We utilize the structure of the low\nrank approximation to achieve effective hyperparameter learning, training and\nprediction. Our method is very much inspired by the well-known random Fourier\nfeatures approach, which also builds low-rank approximations via numerical\nintegration. However, our method is capable of generating high quality\napproximation to the kernel using an amount of features which is\npoly-logarithmic in the number of training points, while similar guarantees\nwill require an amount that is at the very least linear in the number of\ntraining points when random Fourier features. Furthermore, the structure of the\nlow-rank approximation that our method builds is subtly different from the one\ngenerated by random Fourier features, and this enables much more efficient\nhyperparameter learning. The utility of our method for learning with\nlow-dimensional datasets is demonstrated using numerical experiments.\n", "versions": [{"version": "v1", "created": "Mon, 4 Jan 2021 18:09:25 GMT"}, {"version": "v2", "created": "Tue, 5 Jan 2021 13:30:34 GMT"}], "update_date": "2021-01-06", "authors_parsed": [["Shustin", "Paz Fink", ""], ["Avron", "Haim", ""]]}, {"id": "2101.01152", "submitter": "Quanquan Gu", "authors": "Spencer Frei and Yuan Cao and Quanquan Gu", "title": "Provable Generalization of SGD-trained Neural Networks of Any Width in\n  the Presence of Adversarial Label Noise", "comments": "30 pages, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider a one-hidden-layer leaky ReLU network of arbitrary width trained\nby stochastic gradient descent (SGD) following an arbitrary initialization. We\nprove that SGD produces neural networks that have classification accuracy\ncompetitive with that of the best halfspace over the distribution for a broad\nclass of distributions that includes log-concave isotropic and hard margin\ndistributions. Equivalently, such networks can generalize when the data\ndistribution is linearly separable but corrupted with adversarial label noise,\ndespite the capacity to overfit. To the best of our knowledge, this is the\nfirst work to show that overparameterized neural networks trained by SGD can\ngeneralize when the data is corrupted with adversarial label noise.\n", "versions": [{"version": "v1", "created": "Mon, 4 Jan 2021 18:32:49 GMT"}, {"version": "v2", "created": "Thu, 14 Jan 2021 18:57:11 GMT"}, {"version": "v3", "created": "Mon, 15 Feb 2021 18:57:47 GMT"}], "update_date": "2021-02-16", "authors_parsed": [["Frei", "Spencer", ""], ["Cao", "Yuan", ""], ["Gu", "Quanquan", ""]]}, {"id": "2101.01154", "submitter": "Nikolay Malkin", "authors": "Nikolay Malkin, Caleb Robinson, Nebojsa Jojic", "title": "High-resolution land cover change from low-resolution labels: Simple\n  baselines for the 2021 IEEE GRSS Data Fusion Contest", "comments": "10 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We present simple algorithms for land cover change detection in the 2021 IEEE\nGRSS Data Fusion Contest. The task of the contest is to create high-resolution\n(1m / pixel) land cover change maps of a study area in Maryland, USA, given\nmulti-resolution imagery and label data. We study several baseline models for\nthis task and discuss directions for further research.\n  See https://dfc2021.blob.core.windows.net/competition-data/dfc2021_index.txt\nfor the data and https://github.com/calebrob6/dfc2021-msd-baseline for an\nimplementation of these baselines.\n", "versions": [{"version": "v1", "created": "Mon, 4 Jan 2021 18:33:47 GMT"}], "update_date": "2021-01-05", "authors_parsed": [["Malkin", "Nikolay", ""], ["Robinson", "Caleb", ""], ["Jojic", "Nebojsa", ""]]}, {"id": "2101.01158", "submitter": "Thangarajah Akilan Mr", "authors": "Thangarajah Akilan and Edna Johnson and Japneet Sandhu and Ritika\n  Chadha and Gaurav Taluja", "title": "A Hybrid Learner for Simultaneous Localization and Mapping", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Simultaneous localization and mapping (SLAM) is used to predict the dynamic\nmotion path of a moving platform based on the location coordinates and the\nprecise mapping of the physical environment. SLAM has great potential in\naugmented reality (AR), autonomous vehicles, viz. self-driving cars, drones,\nAutonomous navigation robots (ANR). This work introduces a hybrid learning\nmodel that explores beyond feature fusion and conducts a multimodal weight\nsewing strategy towards improving the performance of a baseline SLAM algorithm.\nIt carries out weight enhancement of the front end feature extractor of the\nSLAM via mutation of different deep networks' top layers. At the same time, the\ntrajectory predictions from independently trained models are amalgamated to\nrefine the location detail. Thus, the integration of the aforesaid early and\nlate fusion techniques under a hybrid learning framework minimizes the\ntranslation and rotation errors of the SLAM model. This study exploits some\nwell-known deep learning (DL) architectures, including ResNet18, ResNet34,\nResNet50, ResNet101, VGG16, VGG19, and AlexNet for experimental analysis. An\nextensive experimental analysis proves that hybrid learner (HL) achieves\nsignificantly better results than the unimodal approaches and multimodal\napproaches with early or late fusion strategies. Hence, it is found that the\nApolloscape dataset taken in this work has never been used in the literature\nunder SLAM with fusion techniques, which makes this work unique and insightful.\n", "versions": [{"version": "v1", "created": "Mon, 4 Jan 2021 18:41:09 GMT"}], "update_date": "2021-01-05", "authors_parsed": [["Akilan", "Thangarajah", ""], ["Johnson", "Edna", ""], ["Sandhu", "Japneet", ""], ["Chadha", "Ritika", ""], ["Taluja", "Gaurav", ""]]}, {"id": "2101.01163", "submitter": "Xiaohan Chen", "authors": "Xiaohan Chen, Yang Zhao, Yue Wang, Pengfei Xu, Haoran You, Chaojian\n  Li, Yonggan Fu, Yingyan Lin, Zhangyang Wang", "title": "SmartDeal: Re-Modeling Deep Network Weights for Efficient Inference and\n  Training", "comments": "arXiv admin note: substantial text overlap with arXiv:2005.03403", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The record-breaking performance of deep neural networks (DNNs) comes with\nheavy parameterization, leading to external dynamic random-access memory (DRAM)\nfor storage. The prohibitive energy of DRAM accesses makes it non-trivial to\ndeploy DNN on resource-constrained devices, calling for minimizing the weight\nand data movements to improve the energy efficiency. We present SmartDeal (SD),\nan algorithm framework to trade higher-cost memory storage/access for\nlower-cost computation, in order to aggressively boost the storage and energy\nefficiency, for both inference and training. The core of SD is a novel weight\ndecomposition with structural constraints, carefully crafted to unleash the\nhardware efficiency potential. Specifically, we decompose each weight tensor as\nthe product of a small basis matrix and a large structurally sparse coefficient\nmatrix whose non-zeros are quantized to power-of-2. The resulting sparse and\nquantized DNNs enjoy greatly reduced energy for data movement and weight\nstorage, incurring minimal overhead to recover the original weights thanks to\nthe sparse bit-operations and cost-favorable computations. Beyond inference, we\ntake another leap to embrace energy-efficient training, introducing innovative\ntechniques to address the unique roadblocks arising in training while\npreserving the SD structures. We also design a dedicated hardware accelerator\nto fully utilize the SD structure to improve the real energy efficiency and\nlatency. We conduct experiments on both multiple tasks, models and datasets in\ndifferent settings. Results show that: 1) applied to inference, SD achieves up\nto 2.44x energy efficiency as evaluated via real hardware implementations; 2)\napplied to training, SD leads to 10.56x and 4.48x reduction in the storage and\ntraining energy, with negligible accuracy loss compared to state-of-the-art\ntraining baselines. Our source codes are available online.\n", "versions": [{"version": "v1", "created": "Mon, 4 Jan 2021 18:54:07 GMT"}], "update_date": "2021-01-05", "authors_parsed": [["Chen", "Xiaohan", ""], ["Zhao", "Yang", ""], ["Wang", "Yue", ""], ["Xu", "Pengfei", ""], ["You", "Haoran", ""], ["Li", "Chaojian", ""], ["Fu", "Yonggan", ""], ["Lin", "Yingyan", ""], ["Wang", "Zhangyang", ""]]}, {"id": "2101.01169", "submitter": "Salman Khan Dr.", "authors": "Salman Khan, Muzammal Naseer, Munawar Hayat, Syed Waqas Zamir, Fahad\n  Shahbaz Khan, Mubarak Shah", "title": "Transformers in Vision: A Survey", "comments": "28 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Astounding results from Transformer models on natural language tasks have\nintrigued the vision community to study their application to computer vision\nproblems. Among their salient benefits, Transformers enable modeling long\ndependencies between input sequence elements and support parallel processing of\nsequence as compared to recurrent networks e.g., Long short-term memory (LSTM).\nDifferent from convolutional networks, Transformers require minimal inductive\nbiases for their design and are naturally suited as set-functions. Furthermore,\nthe straightforward design of Transformers allows processing multiple\nmodalities (e.g., images, videos, text and speech) using similar processing\nblocks and demonstrates excellent scalability to very large capacity networks\nand huge datasets. These strengths have led to exciting progress on a number of\nvision tasks using Transformer networks. This survey aims to provide a\ncomprehensive overview of the Transformer models in the computer vision\ndiscipline. We start with an introduction to fundamental concepts behind the\nsuccess of Transformers i.e., self-attention, large-scale pre-training, and\nbidirectional encoding. We then cover extensive applications of transformers in\nvision including popular recognition tasks (e.g., image classification, object\ndetection, action recognition, and segmentation), generative modeling,\nmulti-modal tasks (e.g., visual-question answering, visual reasoning, and\nvisual grounding), video processing (e.g., activity recognition, video\nforecasting), low-level vision (e.g., image super-resolution, image\nenhancement, and colorization) and 3D analysis (e.g., point cloud\nclassification and segmentation). We compare the respective advantages and\nlimitations of popular techniques both in terms of architectural design and\ntheir experimental value. Finally, we provide an analysis on open research\ndirections and possible future works.\n", "versions": [{"version": "v1", "created": "Mon, 4 Jan 2021 18:57:24 GMT"}, {"version": "v2", "created": "Mon, 22 Feb 2021 11:40:11 GMT"}], "update_date": "2021-02-23", "authors_parsed": [["Khan", "Salman", ""], ["Naseer", "Muzammal", ""], ["Hayat", "Munawar", ""], ["Zamir", "Syed Waqas", ""], ["Khan", "Fahad Shahbaz", ""], ["Shah", "Mubarak", ""]]}, {"id": "2101.01178", "submitter": "Jeffrey Ede BSc MPhys", "authors": "Jeffrey M. Ede", "title": "Advances in Electron Microscopy with Deep Learning", "comments": "295 pages, phd thesis, 100 figures + 12 tables, papers are compressed", "journal-ref": null, "doi": "10.5281/zenodo.4399748", "report-no": null, "categories": "eess.IV cond-mat.mtrl-sci cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  This doctoral thesis covers some of my advances in electron microscopy with\ndeep learning. Highlights include a comprehensive review of deep learning in\nelectron microscopy; large new electron microscopy datasets for machine\nlearning, dataset search engines based on variational autoencoders, and\nautomatic data clustering by t-distributed stochastic neighbour embedding;\nadaptive learning rate clipping to stabilize learning; generative adversarial\nnetworks for compressed sensing with spiral, uniformly spaced and other fixed\nsparse scan paths; recurrent neural networks trained to piecewise adapt sparse\nscan paths to specimens by reinforcement learning; improving signal-to-noise;\nand conditional generative adversarial networks for exit wavefunction\nreconstruction from single transmission electron micrographs. This thesis adds\nto my publications by presenting their relationships, reflections, and holistic\nconclusions. This version of my thesis is typeset for online dissemination to\nimprove readability, whereas the thesis submitted to the University of Warwick\nin support of my application for the degree of Doctor of Philosophy in Physics\nis typeset for physical printing and binding.\n", "versions": [{"version": "v1", "created": "Mon, 4 Jan 2021 13:49:37 GMT"}, {"version": "v2", "created": "Sat, 9 Jan 2021 17:30:04 GMT"}, {"version": "v3", "created": "Fri, 5 Mar 2021 12:06:00 GMT"}, {"version": "v4", "created": "Tue, 9 Mar 2021 14:53:24 GMT"}, {"version": "v5", "created": "Thu, 11 Mar 2021 17:25:33 GMT"}], "update_date": "2021-03-12", "authors_parsed": [["Ede", "Jeffrey M.", ""]]}, {"id": "2101.01190", "submitter": "Frank Sch\\\"afer Mr", "authors": "Frank Sch\\\"afer, Pavel Sekatski, Martin Koppenh\\\"ofer, Christoph\n  Bruder, Michal Kloc", "title": "Control of Stochastic Quantum Dynamics by Differentiable Programming", "comments": "equivalent to published version, 19+17 pages, 5+3 figures", "journal-ref": "Mach. Learn.: Sci. Technol. 2, 035004 (2021)", "doi": "10.1088/2632-2153/abec22", "report-no": null, "categories": "quant-ph cs.LG cs.SY eess.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Control of the stochastic dynamics of a quantum system is indispensable in\nfields such as quantum information processing and metrology. However, there is\nno general ready-made approach to the design of efficient control strategies.\nHere, we propose a framework for the automated design of control schemes based\non differentiable programming ($\\partial \\mathrm{P}$). We apply this approach\nto the state preparation and stabilization of a qubit subjected to homodyne\ndetection. To this end, we formulate the control task as an optimization\nproblem where the loss function quantifies the distance from the target state,\nand we employ neural networks (NNs) as controllers. The system's time evolution\nis governed by a stochastic differential equation (SDE). To implement efficient\ntraining, we backpropagate the gradient information from the loss function\nthrough the SDE solver using adjoint sensitivity methods. As a first example,\nwe feed the quantum state to the controller and focus on different methods of\nobtaining gradients. As a second example, we directly feed the homodyne\ndetection signal to the controller. The instantaneous value of the homodyne\ncurrent contains only very limited information on the actual state of the\nsystem, masked by unavoidable photon-number fluctuations. Despite the resulting\npoor signal-to-noise ratio, we can train our controller to prepare and\nstabilize the qubit to a target state with a mean fidelity of around 85%. We\nalso compare the solutions found by the NN to a hand-crafted control strategy.\n", "versions": [{"version": "v1", "created": "Mon, 4 Jan 2021 19:00:03 GMT"}, {"version": "v2", "created": "Fri, 23 Apr 2021 16:23:36 GMT"}], "update_date": "2021-04-26", "authors_parsed": [["Sch\u00e4fer", "Frank", ""], ["Sekatski", "Pavel", ""], ["Koppenh\u00f6fer", "Martin", ""], ["Bruder", "Christoph", ""], ["Kloc", "Michal", ""]]}, {"id": "2101.01204", "submitter": "Lawrence Thul", "authors": "Lawrence Thul, Warren Powell", "title": "Stochastic Optimization for Vaccine and Testing Kit Allocation for the\n  COVID-19 Pandemic", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  The pandemic caused by the SARS-CoV-2 virus has exposed many flaws in the\ndecision-making strategies used to distribute resources to combat global health\ncrises. In this paper, we leverage reinforcement learning and optimization to\nimprove upon the allocation strategies for various resources. In particular, we\nconsider a problem where a central controller must decide where to send testing\nkits to learn about the uncertain states of the world (active learning); then,\nuse the new information to construct beliefs about the states and decide where\nto allocate resources. We propose a general model coupled with a tunable\nlookahead policy for making vaccine allocation decisions without perfect\nknowledge about the state of the world. The lookahead policy is compared to a\npopulation-based myopic policy which is more likely to be similar to the\npresent strategies in practice. Each vaccine allocation policy works in\nconjunction with a testing kit allocation policy to perform active learning.\nOur simulation results demonstrate that an optimization-based lookahead\ndecision making strategy will outperform the presented myopic policy.\n", "versions": [{"version": "v1", "created": "Mon, 4 Jan 2021 19:08:32 GMT"}], "update_date": "2021-01-06", "authors_parsed": [["Thul", "Lawrence", ""], ["Powell", "Warren", ""]]}, {"id": "2101.01207", "submitter": "Peter He", "authors": "Peter He, Raksha Jain, J\\'er\\^ome Chambost, C\\'eline Jacques, Cristina\n  Hickman", "title": "Semantic Video Segmentation for Intracytoplasmic Sperm Injection\n  Procedures", "comments": "Accepted at the 'Medical Imaging meets NeurIPS Workshop' at the 34th\n  Conference on Neural Information Processing Systems", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We present the first deep learning model for the analysis of intracytoplasmic\nsperm injection (ICSI) procedures. Using a dataset of ICSI procedure videos, we\ntrain a deep neural network to segment key objects in the videos achieving a\nmean IoU of 0.962, and to localize the needle tip achieving a mean pixel error\nof 3.793 pixels at 14 FPS on a single GPU. We further analyze the variation\nbetween the dataset's human annotators and find the model's performance to be\ncomparable to human experts.\n", "versions": [{"version": "v1", "created": "Mon, 4 Jan 2021 19:33:12 GMT"}], "update_date": "2021-01-06", "authors_parsed": [["He", "Peter", ""], ["Jain", "Raksha", ""], ["Chambost", "J\u00e9r\u00f4me", ""], ["Jacques", "C\u00e9line", ""], ["Hickman", "Cristina", ""]]}, {"id": "2101.01229", "submitter": "Claudio Daniel Ten\\'orio De Barros", "authors": "Claudio D. T. Barros, Matheus R. F. Mendon\\c{c}a, Alex B. Vieira,\n  Artur Ziviani", "title": "A Survey on Embedding Dynamic Graphs", "comments": "41 pages, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Embedding static graphs in low-dimensional vector spaces plays a key role in\nnetwork analytics and inference, supporting applications like node\nclassification, link prediction, and graph visualization. However, many\nreal-world networks present dynamic behavior, including topological evolution,\nfeature evolution, and diffusion. Therefore, several methods for embedding\ndynamic graphs have been proposed to learn network representations over time,\nfacing novel challenges, such as time-domain modeling, temporal features to be\ncaptured, and the temporal granularity to be embedded. In this survey, we\noverview dynamic graph embedding, discussing its fundamentals and the recent\nadvances developed so far. We introduce the formal definition of dynamic graph\nembedding, focusing on the problem setting and introducing a novel taxonomy for\ndynamic graph embedding input and output. We further explore different dynamic\nbehaviors that may be encompassed by embeddings, classifying by topological\nevolution, feature evolution, and processes on networks. Afterward, we describe\nexisting techniques and propose a taxonomy for dynamic graph embedding\ntechniques based on algorithmic approaches, from matrix and tensor\nfactorization to deep learning, random walks, and temporal point processes. We\nalso elucidate main applications, including dynamic link prediction, anomaly\ndetection, and diffusion prediction, and we further state some promising\nresearch directions in the area.\n", "versions": [{"version": "v1", "created": "Mon, 4 Jan 2021 20:35:26 GMT"}, {"version": "v2", "created": "Thu, 22 Jul 2021 02:47:17 GMT"}], "update_date": "2021-07-23", "authors_parsed": [["Barros", "Claudio D. T.", ""], ["Mendon\u00e7a", "Matheus R. F.", ""], ["Vieira", "Alex B.", ""], ["Ziviani", "Artur", ""]]}, {"id": "2101.01251", "submitter": "Mostafa Hussein", "authors": "Mostafa Hussein, Brendan Crowe, Marek Petrik and Momotaz Begum", "title": "Robust Maximum Entropy Behavior Cloning", "comments": "NeurIPS 2020 3rd Robot Learning Workshop: Grounding Machine Learning\n  Development in the Real World", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Imitation learning (IL) algorithms use expert demonstrations to learn a\nspecific task. Most of the existing approaches assume that all expert\ndemonstrations are reliable and trustworthy, but what if there exist some\nadversarial demonstrations among the given data-set? This may result in poor\ndecision-making performance. We propose a novel general frame-work to directly\ngenerate a policy from demonstrations that autonomously detect the adversarial\ndemonstrations and exclude them from the data set. At the same time, it's\nsample, time-efficient, and does not require a simulator. To model such\nadversarial demonstration we propose a min-max problem that leverages the\nentropy of the model to assign weights for each demonstration. This allows us\nto learn the behavior using only the correct demonstrations or a mixture of\ncorrect demonstrations.\n", "versions": [{"version": "v1", "created": "Mon, 4 Jan 2021 22:08:46 GMT"}], "update_date": "2021-01-06", "authors_parsed": [["Hussein", "Mostafa", ""], ["Crowe", "Brendan", ""], ["Petrik", "Marek", ""], ["Begum", "Momotaz", ""]]}, {"id": "2101.01266", "submitter": "Zhiyan Chen", "authors": "Zhiyan Chen, Murat Simsek, Burak Kantarci", "title": "Federated Learning-Based Risk-Aware Decision toMitigate Fake Task\n  Impacts on CrowdsensingPlatforms", "comments": "This work has been submitted to the IEEE for possible publication.\n  Copyright may be transferred without notice, after which this version may no\n  longer be accessible", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Mobile crowdsensing (MCS) leverages distributed and non-dedicated sensing\nconcepts by utilizing sensors imbedded in a large number of mobile smart\ndevices. However, the openness and distributed nature of MCS leads to various\nvulnerabilities and consequent challenges to address. A malicious user\nsubmitting fake sensing tasks to an MCS platform may be attempting to consume\nresources from any number of participants' devices; as well as attempting to\nclog the MCS server. In this paper, a novel approach that is based on\nhorizontal federated learning is proposed to identify fake tasks that contain a\nnumber of independent detection devices and an aggregation entity. Detection\ndevices are deployed to operate in parallel with each device equipped with a\nmachine learning (ML) module, and an associated training dataset. Furthermore,\nthe aggregation module collects the prediction results from individual devices\nand determines the final decision with the objective of minimizing the\nprediction loss. Loss measurement considers the lost task values with respect\nto misclassification, where the final decision utilizes a risk-aware approach\nwhere the risk is formulated as a function of the utility loss. Experimental\nresults demonstrate that using federated learning-driven illegitimate task\ndetection with a risk aware aggregation function improves the detection\nperformance of the traditional centralized framework. Furthermore, the higher\nperformance of detection and lower loss of utility can be achieved by the\nproposed framework. This scheme can even achieve 100%detection accuracy using\nsmall training datasets distributed across devices, while achieving slightly\nover an 8% increase in detection improvement over traditional approaches.\n", "versions": [{"version": "v1", "created": "Mon, 4 Jan 2021 22:43:24 GMT"}], "update_date": "2021-01-06", "authors_parsed": [["Chen", "Zhiyan", ""], ["Simsek", "Murat", ""], ["Kantarci", "Burak", ""]]}, {"id": "2101.01271", "submitter": "Wandong Zhang", "authors": "Wandong Zhang (1 and 2), QM Jonathan Wu (1), Yimin Yang (2 and 3), WG\n  Will Zhao (2 and 4), Tianlei Wang (5), and Hui Zhang (6) ((1) University of\n  Windsor, (2) Lakehead University, (3) Vector Institute for Artificial\n  Intelligence, (4) CEGEP de Ste Foy, (5) Hangzhou Dianzi University, (6) Hunan\n  University)", "title": "Multi-Model Least Squares-Based Recomputation Framework for Large Data\n  Analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most multilayer least squares (LS)-based neural networks are structured with\ntwo separate stages: unsupervised feature encoding and supervised pattern\nclassification. Once the unsupervised learning is finished, the latent encoding\nwould be fixed without supervised fine-tuning. However, in complex tasks such\nas handling the ImageNet dataset, there are often many more clues that can be\ndirectly encoded, while the unsupervised learning, by definition cannot know\nexactly what is useful for a certain task. This serves as the motivation to\nretrain the latent space representations to learn some clues that unsupervised\nlearning has not yet learned. In particular, the error matrix from the output\nlayer is pulled back to each hidden layer, and the parameters of the hidden\nlayer are recalculated with Moore-Penrose (MP) inverse for more generalized\nrepresentations. In this paper, a recomputation-based multilayer network using\nMP inverse (RML-MP) is developed. A sparse RML-MP (SRML-MP) model to boost the\nperformance of RML-MP is then proposed. The experimental results with varying\ntraining samples (from 3 K to 1.8 M) show that the proposed models provide\nbetter generalization performance than most representation learning algorithms.\n", "versions": [{"version": "v1", "created": "Mon, 4 Jan 2021 23:01:30 GMT"}, {"version": "v2", "created": "Wed, 6 Jan 2021 23:44:40 GMT"}, {"version": "v3", "created": "Sun, 10 Jan 2021 00:25:30 GMT"}, {"version": "v4", "created": "Wed, 3 Mar 2021 16:31:59 GMT"}], "update_date": "2021-03-04", "authors_parsed": [["Zhang", "Wandong", "", "1 and 2"], ["Wu", "QM Jonathan", "", "2 and 3"], ["Yang", "Yimin", "", "2 and 3"], ["Zhao", "WG Will", "", "2 and 4"], ["Wang", "Tianlei", ""], ["Zhang", "Hui", ""]]}, {"id": "2101.01292", "submitter": "Maximilian Schleich", "authors": "Maximilian Schleich, Zixuan Geng, Yihong Zhang, Dan Suciu", "title": "GeCo: Quality Counterfactual Explanations in Real Time", "comments": "16 pages, 12 figures, 3 tables, 3 algorithms", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Machine learning is increasingly applied in high-stakes decision making that\ndirectly affect people's lives, and this leads to an increased demand for\nsystems to explain their decisions. Explanations often take the form of\ncounterfactuals, which consists of conveying to the end user what she/he needs\nto change in order to improve the outcome. Computing counterfactual\nexplanations is challenging, because of the inherent tension between a rich\nsemantics of the domain, and the need for real time response. In this paper we\npresent GeCo, the first system that can compute plausible and feasible\ncounterfactual explanations in real time. At its core, GeCo relies on a genetic\nalgorithm, which is customized to favor searching counterfactual explanations\nwith the smallest number of changes. To achieve real-time performance, we\nintroduce two novel optimizations: $\\Delta$-representation of candidate\ncounterfactuals, and partial evaluation of the classifier. We compare\nempirically GeCo against five other systems described in the literature, and\nshow that it is the only system that can achieve both high quality explanations\nand real time answers.\n", "versions": [{"version": "v1", "created": "Tue, 5 Jan 2021 00:23:58 GMT"}, {"version": "v2", "created": "Wed, 17 Mar 2021 04:23:30 GMT"}, {"version": "v3", "created": "Wed, 19 May 2021 00:02:49 GMT"}], "update_date": "2021-05-20", "authors_parsed": [["Schleich", "Maximilian", ""], ["Geng", "Zixuan", ""], ["Zhang", "Yihong", ""], ["Suciu", "Dan", ""]]}, {"id": "2101.01294", "submitter": "Daniel Cauchi", "authors": "Daniel Cauchi, Adrian Muscat", "title": "One vs Previous and Similar Classes Learning -- A Comparative Study", "comments": "10 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  When dealing with multi-class classification problems, it is common practice\nto build a model consisting of a series of binary classifiers using a learning\nparadigm which dictates how the classifiers are built and combined to\ndiscriminate between the individual classes. As new data enters the system and\nthe model needs updating, these models would often need to be retrained from\nscratch. This work proposes three learning paradigms which allow trained models\nto be updated without the need of retraining from scratch. A comparative\nanalysis is performed to evaluate them against a baseline. Results show that\nthe proposed paradigms are faster than the baseline at updating, with two of\nthem being faster at training from scratch as well, especially on larger\ndatasets, while retaining a comparable classification performance.\n", "versions": [{"version": "v1", "created": "Tue, 5 Jan 2021 00:28:38 GMT"}], "update_date": "2021-01-06", "authors_parsed": [["Cauchi", "Daniel", ""], ["Muscat", "Adrian", ""]]}, {"id": "2101.01300", "submitter": "Waheed Bajwa", "authors": "Arpita Gang and Waheed U. Bajwa", "title": "A Linearly Convergent Algorithm for Distributed Principal Component\n  Analysis", "comments": "33 pages; 15 figures; preprint of a journal paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DC cs.MA eess.SP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Principal Component Analysis (PCA) is the workhorse tool for dimensionality\nreduction in this era of big data. While often overlooked, the purpose of PCA\nis not only to reduce data dimensionality, but also to yield features that are\nuncorrelated. Furthermore, the ever-increasing volume of data in the modern\nworld often requires storage of data samples across multiple machines, which\nprecludes the use of centralized PCA algorithms. This paper focuses on the dual\nobjective of PCA, namely, dimensionality reduction and decorrelation of\nfeatures, but in a distributed setting. This requires estimating the\neigenvectors of the data covariance matrix, as opposed to only estimating the\nsubspace spanned by the eigenvectors, when data is distributed across a network\nof machines. Although a few distributed solutions to the PCA problem have been\nproposed recently, convergence guarantees and/or communications overhead of\nthese solutions remain a concern. With an eye towards communications\nefficiency, this paper introduces a feedforward neural network-based one\ntime-scale distributed PCA algorithm termed Distributed Sanger's Algorithm\n(DSA) that estimates the eigenvectors of the data covariance matrix when data\nis distributed across an undirected and arbitrarily connected network of\nmachines. Furthermore, the proposed algorithm is shown to converge linearly to\na neighborhood of the true solution. Numerical results are also provided to\ndemonstrate the efficacy of the proposed solution.\n", "versions": [{"version": "v1", "created": "Tue, 5 Jan 2021 00:51:14 GMT"}, {"version": "v2", "created": "Fri, 28 May 2021 20:04:10 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Gang", "Arpita", ""], ["Bajwa", "Waheed U.", ""]]}, {"id": "2101.01301", "submitter": "Yining Wang", "authors": "Xi Chen and Yanjun Han and Yining Wang", "title": "Adversarial Combinatorial Bandits with General Non-linear Reward\n  Functions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper we study the adversarial combinatorial bandit with a known\nnon-linear reward function, extending existing work on adversarial linear\ncombinatorial bandit. {The adversarial combinatorial bandit with general\nnon-linear reward is an important open problem in bandit literature, and it is\nstill unclear whether there is a significant gap from the case of linear\nreward, stochastic bandit, or semi-bandit feedback.} We show that, with $N$\narms and subsets of $K$ arms being chosen at each of $T$ time periods, the\nminimax optimal regret is $\\widetilde\\Theta_{d}(\\sqrt{N^d T})$ if the reward\nfunction is a $d$-degree polynomial with $d< K$, and $\\Theta_K(\\sqrt{N^K T})$\nif the reward function is not a low-degree polynomial. {Both bounds are\nsignificantly different from the bound $O(\\sqrt{\\mathrm{poly}(N,K)T})$ for the\nlinear case, which suggests that there is a fundamental gap between the linear\nand non-linear reward structures.} Our result also finds applications to\nadversarial assortment optimization problem in online recommendation. We show\nthat in the worst-case of adversarial assortment problem, the optimal algorithm\nmust treat each individual $\\binom{N}{K}$ assortment as independent.\n", "versions": [{"version": "v1", "created": "Tue, 5 Jan 2021 00:56:27 GMT"}], "update_date": "2021-01-06", "authors_parsed": [["Chen", "Xi", ""], ["Han", "Yanjun", ""], ["Wang", "Yining", ""]]}, {"id": "2101.01329", "submitter": "Trista Chen", "authors": "Davide Burba, Trista Chen", "title": "A Trainable Reconciliation Method for Hierarchical Time-Series", "comments": "Accepted paper to ITISE 2021 (7th International Conference on Time\n  Series and Forecasting). 12 pages, 3 figures, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In numerous applications, it is required to produce forecasts for multiple\ntime-series at different hierarchy levels. An obvious example is given by the\nsupply chain in which demand forecasting may be needed at a store, city, or\ncountry level. The independent forecasts typically do not add up properly\nbecause of the hierarchical constraints, so a reconciliation step is needed. In\nthis paper, we propose a new general, flexible, and easy-to-implement\nreconciliation strategy based on an encoder-decoder neural network. By testing\nour method on four real-world datasets, we show that it can consistently reach\nor surpass the performance of existing methods in the reconciliation setting.\n", "versions": [{"version": "v1", "created": "Tue, 5 Jan 2021 03:21:07 GMT"}], "update_date": "2021-01-06", "authors_parsed": [["Burba", "Davide", ""], ["Chen", "Trista", ""]]}, {"id": "2101.01336", "submitter": "Qiyu Hu", "authors": "Qiyu Hu, Yanzhen Liu, Yunlong Cai, Guanding Yu, and Zhi Ding", "title": "Joint Deep Reinforcement Learning and Unfolding: Beam Selection and\n  Precoding for mmWave Multiuser MIMO with Lens Arrays", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.LG eess.SP math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The millimeter wave (mmWave) multiuser multiple-input multiple-output\n(MU-MIMO) systems with discrete lens arrays (DLA) have received great attention\ndue to their simple hardware implementation and excellent performance. In this\nwork, we investigate the joint design of beam selection and digital precoding\nmatrices for mmWave MU-MIMO systems with DLA to maximize the sum-rate subject\nto the transmit power constraint and the constraints of the selection matrix\nstructure. The investigated non-convex problem with discrete variables and\ncoupled constraints is challenging to solve and an efficient framework of joint\nneural network (NN) design is proposed to tackle it. Specifically, the proposed\nframework consists of a deep reinforcement learning (DRL)-based NN and a\ndeep-unfolding NN, which are employed to optimize the beam selection and\ndigital precoding matrices, respectively. As for the DRL-based NN, we formulate\nthe beam selection problem as a Markov decision process and a double deep\nQ-network algorithm is developed to solve it. The base station is considered to\nbe an agent, where the state, action, and reward function are carefully\ndesigned. Regarding the design of the digital precoding matrix, we develop an\niterative weighted minimum mean-square error algorithm induced deep-unfolding\nNN, which unfolds this algorithm into a layerwise structure with introduced\ntrainable parameters. Simulation results verify that this jointly trained NN\nremarkably outperforms the existing iterative algorithms with reduced\ncomplexity and stronger robustness.\n", "versions": [{"version": "v1", "created": "Tue, 5 Jan 2021 03:55:04 GMT"}], "update_date": "2021-01-06", "authors_parsed": [["Hu", "Qiyu", ""], ["Liu", "Yanzhen", ""], ["Cai", "Yunlong", ""], ["Yu", "Guanding", ""], ["Ding", "Zhi", ""]]}, {"id": "2101.01337", "submitter": "Mohammed Alawad", "authors": "Mohammed Alawad, Shang Gao, Mayanka Chandra Shekar, S.M.Shamimul\n  Hasan, J. Blair Christian, Xiao-Cheng Wu, Eric B. Durbin, Jennifer Doherty,\n  Antoinette Stroup, Linda Coyle, Lynne Penberthy, Georgia Tourassi", "title": "Integration of Domain Knowledge using Medical Knowledge Graph Deep\n  Learning for Cancer Phenotyping", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A key component of deep learning (DL) for natural language processing (NLP)\nis word embeddings. Word embeddings that effectively capture the meaning and\ncontext of the word that they represent can significantly improve the\nperformance of downstream DL models for various NLP tasks. Many existing word\nembeddings techniques capture the context of words based on word co-occurrence\nin documents and text; however, they often cannot capture broader\ndomain-specific relationships between concepts that may be crucial for the NLP\ntask at hand. In this paper, we propose a method to integrate external\nknowledge from medical terminology ontologies into the context captured by word\nembeddings. Specifically, we use a medical knowledge graph, such as the unified\nmedical language system (UMLS), to find connections between clinical terms in\ncancer pathology reports. This approach aims to minimize the distance between\nconnected clinical concepts. We evaluate the proposed approach using a\nMultitask Convolutional Neural Network (MT-CNN) to extract six cancer\ncharacteristics -- site, subsite, laterality, behavior, histology, and grade --\nfrom a dataset of ~900K cancer pathology reports. The results show that the\nMT-CNN model which uses our domain informed embeddings outperforms the same\nMT-CNN using standard word2vec embeddings across all tasks, with an improvement\nin the overall micro- and macro-F1 scores by 4.97\\%and 22.5\\%, respectively.\n", "versions": [{"version": "v1", "created": "Tue, 5 Jan 2021 03:59:43 GMT"}], "update_date": "2021-01-06", "authors_parsed": [["Alawad", "Mohammed", ""], ["Gao", "Shang", ""], ["Shekar", "Mayanka Chandra", ""], ["Hasan", "S. M. Shamimul", ""], ["Christian", "J. Blair", ""], ["Wu", "Xiao-Cheng", ""], ["Durbin", "Eric B.", ""], ["Doherty", "Jennifer", ""], ["Stroup", "Antoinette", ""], ["Coyle", "Linda", ""], ["Penberthy", "Lynne", ""], ["Tourassi", "Georgia", ""]]}, {"id": "2101.01341", "submitter": "Bo Hui", "authors": "Bo Hui, Yuchen Yang, Haolin Yuan, Philippe Burlina, Neil Zhenqiang\n  Gong and Yinzhi Cao", "title": "Practical Blind Membership Inference Attack via Differential Comparisons", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Membership inference (MI) attacks affect user privacy by inferring whether\ngiven data samples have been used to train a target learning model, e.g., a\ndeep neural network. There are two types of MI attacks in the literature, i.e.,\nthese with and without shadow models. The success of the former heavily depends\non the quality of the shadow model, i.e., the transferability between the\nshadow and the target; the latter, given only blackbox probing access to the\ntarget model, cannot make an effective inference of unknowns, compared with MI\nattacks using shadow models, due to the insufficient number of qualified\nsamples labeled with ground truth membership information.\n  In this paper, we propose an MI attack, called BlindMI, which probes the\ntarget model and extracts membership semantics via a novel approach, called\ndifferential comparison. The high-level idea is that BlindMI first generates a\ndataset with nonmembers via transforming existing samples into new samples, and\nthen differentially moves samples from a target dataset to the generated,\nnon-member set in an iterative manner. If the differential move of a sample\nincreases the set distance, BlindMI considers the sample as non-member and vice\nversa.\n  BlindMI was evaluated by comparing it with state-of-the-art MI attack\nalgorithms. Our evaluation shows that BlindMI improves F1-score by nearly 20%\nwhen compared to state-of-the-art on some datasets, such as Purchase-50 and\nBirds-200, in the blind setting where the adversary does not know the target\nmodel's architecture and the target dataset's ground truth labels. We also show\nthat BlindMI can defeat state-of-the-art defenses.\n", "versions": [{"version": "v1", "created": "Tue, 5 Jan 2021 04:07:15 GMT"}, {"version": "v2", "created": "Thu, 7 Jan 2021 02:24:04 GMT"}], "update_date": "2021-01-08", "authors_parsed": [["Hui", "Bo", ""], ["Yang", "Yuchen", ""], ["Yuan", "Haolin", ""], ["Burlina", "Philippe", ""], ["Gong", "Neil Zhenqiang", ""], ["Cao", "Yinzhi", ""]]}, {"id": "2101.01350", "submitter": "Xiangyu Yang", "authors": "Xiangyu Yang, Jiashan Wang, and Hao Wang", "title": "Towards an efficient approach for the nonconvex $\\ell_p$-ball\n  projection: algorithm and analysis", "comments": "This work has been submitted and may be published. Copyright may be\n  transferred without notice, after which this version may no longer be\n  accessible", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper primarily focuses on computing the Euclidean projection of a\nvector onto the $\\ell_{p}$ ball in which $p\\in(0,1)$. Such a problem emerges as\nthe core building block in statistical machine learning and signal processing\ntasks because of its ability to promote sparsity. However, efficient numerical\nalgorithms for finding the projections are still not available, particularly in\nlarge-scale optimization. To meet this challenge, we first derive the\nfirst-order necessary optimality conditions of this problem using Fr\\'echet\nnormal cone. Based on this characterization, we develop a novel numerical\napproach for computing the stationary point through solving a sequence of\nprojections onto the reweighted $\\ell_{1}$-balls. This method is practically\nsimple to implement and computationally efficient. Moreover, the proposed\nalgorithm is shown to converge uniquely under mild conditions and has a\nworst-case $O(1/\\sqrt{k})$ convergence rate. Numerical experiments demonstrate\nthe efficiency of our proposed algorithm.\n", "versions": [{"version": "v1", "created": "Tue, 5 Jan 2021 04:51:12 GMT"}, {"version": "v2", "created": "Mon, 22 Feb 2021 12:33:46 GMT"}, {"version": "v3", "created": "Fri, 11 Jun 2021 02:46:55 GMT"}, {"version": "v4", "created": "Wed, 23 Jun 2021 15:12:43 GMT"}], "update_date": "2021-06-24", "authors_parsed": [["Yang", "Xiangyu", ""], ["Wang", "Jiashan", ""], ["Wang", "Hao", ""]]}, {"id": "2101.01356", "submitter": "Anugunj Naman", "authors": "Anugunj Naman, Liliana Mancini", "title": "Fixed-MAML for Few Shot Classification in Multilingual Speech Emotion\n  Recognition", "comments": "Code at https://github.com/AnugunjNaman/Fixed-MAML", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.LG eess.AS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper, we analyze the feasibility of applying few-shot learning to\nspeech emotion recognition task (SER). The current speech emotion recognition\nmodels work exceptionally well but fail when then input is multilingual.\nMoreover, when training such models, the models' performance is suitable only\nwhen the training corpus is vast. This availability of a big training corpus is\na significant problem when choosing a language that is not much popular or\nobscure. We attempt to solve this challenge of multilingualism and lack of\navailable data by turning this problem into a few-shot learning problem. We\nsuggest relaxing the assumption that all N classes in an N-way K-shot problem\nbe new and define an N+F way problem where N and F are the number of emotion\nclasses and predefined fixed classes, respectively. We propose this\nmodification to the Model-Agnostic MetaLearning (MAML) algorithm to solve the\nproblem and call this new model F-MAML. This modification performs better than\nthe original MAML and outperforms on EmoFilm dataset.\n", "versions": [{"version": "v1", "created": "Tue, 5 Jan 2021 05:51:50 GMT"}], "update_date": "2021-01-06", "authors_parsed": [["Naman", "Anugunj", ""], ["Mancini", "Liliana", ""]]}, {"id": "2101.01366", "submitter": "Nontawat Charoenphakdee", "authors": "Nontawat Charoenphakdee, Jongyeong Lee, Masashi Sugiyama", "title": "A Symmetric Loss Perspective of Reliable Machine Learning", "comments": "Preprint of an Invited Review Article", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When minimizing the empirical risk in binary classification, it is a common\npractice to replace the zero-one loss with a surrogate loss to make the\nlearning objective feasible to optimize. Examples of well-known surrogate\nlosses for binary classification include the logistic loss, hinge loss, and\nsigmoid loss. It is known that the choice of a surrogate loss can highly\ninfluence the performance of the trained classifier and therefore it should be\ncarefully chosen. Recently, surrogate losses that satisfy a certain symmetric\ncondition (aka., symmetric losses) have demonstrated their usefulness in\nlearning from corrupted labels. In this article, we provide an overview of\nsymmetric losses and their applications. First, we review how a symmetric loss\ncan yield robust classification from corrupted labels in balanced error rate\n(BER) minimization and area under the receiver operating characteristic curve\n(AUC) maximization. Then, we demonstrate how the robust AUC maximization method\ncan benefit natural language processing in the problem where we want to learn\nonly from relevant keywords and unlabeled documents. Finally, we conclude this\narticle by discussing future directions, including potential applications of\nsymmetric losses for reliable machine learning and the design of non-symmetric\nlosses that can benefit from the symmetric condition.\n", "versions": [{"version": "v1", "created": "Tue, 5 Jan 2021 06:25:47 GMT"}], "update_date": "2021-01-06", "authors_parsed": [["Charoenphakdee", "Nontawat", ""], ["Lee", "Jongyeong", ""], ["Sugiyama", "Masashi", ""]]}, {"id": "2101.01385", "submitter": "Ruimeng Hu", "authors": "Jiequn Han, Ruimeng Hu", "title": "Recurrent Neural Networks for Stochastic Control Problems with Delay", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.LG q-fin.CP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Stochastic control problems with delay are challenging due to the\npath-dependent feature of the system and thus its intrinsic high dimensions. In\nthis paper, we propose and systematically study deep neural networks-based\nalgorithms to solve stochastic control problems with delay features.\nSpecifically, we employ neural networks for sequence modeling (\\emph{e.g.},\nrecurrent neural networks such as long short-term memory) to parameterize the\npolicy and optimize the objective function. The proposed algorithms are tested\non three benchmark examples: a linear-quadratic problem, optimal consumption\nwith fixed finite delay, and portfolio optimization with complete memory.\nParticularly, we notice that the architecture of recurrent neural networks\nnaturally captures the path-dependent feature with much flexibility and yields\nbetter performance with more efficient and stable training of the network\ncompared to feedforward networks. The superiority is even evident in the case\nof portfolio optimization with complete memory, which features infinite delay.\n", "versions": [{"version": "v1", "created": "Tue, 5 Jan 2021 07:18:47 GMT"}, {"version": "v2", "created": "Wed, 16 Jun 2021 22:44:49 GMT"}], "update_date": "2021-06-18", "authors_parsed": [["Han", "Jiequn", ""], ["Hu", "Ruimeng", ""]]}, {"id": "2101.01386", "submitter": "Shuyue Guan", "authors": "Shuyue Guan, Murray Loew", "title": "Understanding the Ability of Deep Neural Networks to Count Connected\n  Components in Images", "comments": "7 pages, 12 figures. Accepted by IEEE AIPR 2020 (Oral)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Humans can count very fast by subitizing, but slow substantially as the\nnumber of objects increases. Previous studies have shown a trained deep neural\nnetwork (DNN) detector can count the number of objects in an amount of time\nthat increases slowly with the number of objects. Such a phenomenon suggests\nthe subitizing ability of DNNs, and unlike humans, it works equally well for\nlarge numbers. Many existing studies have successfully applied DNNs to object\ncounting, but few studies have studied the subitizing ability of DNNs and its\ninterpretation. In this paper, we found DNNs do not have the ability to\ngenerally count connected components. We provided experiments to support our\nconclusions and explanations to understand the results and phenomena of these\nexperiments. We proposed three ML-learnable characteristics to verify learnable\nproblems for ML models, such as DNNs, and explain why DNNs work for specific\ncounting problems but cannot generally count connected components.\n", "versions": [{"version": "v1", "created": "Tue, 5 Jan 2021 07:28:34 GMT"}], "update_date": "2021-01-06", "authors_parsed": [["Guan", "Shuyue", ""], ["Loew", "Murray", ""]]}, {"id": "2101.01400", "submitter": "Chongxuan Li", "authors": "Qijun Luo, Zhili Liu, Lanqing Hong, Chongxuan Li, Kuo Yang, Liyuan\n  Wang, Fengwei Zhou, Guilin Li, Zhenguo Li, Jun Zhu", "title": "Relaxed Conditional Image Transfer for Semi-supervised Domain Adaptation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Semi-supervised domain adaptation (SSDA), which aims to learn models in a\npartially labeled target domain with the assistance of the fully labeled source\ndomain, attracts increasing attention in recent years. To explicitly leverage\nthe labeled data in both domains, we naturally introduce a conditional GAN\nframework to transfer images without changing the semantics in SSDA. However,\nwe identify a label-domination problem in such an approach. In fact, the\ngenerator tends to overlook the input source image and only memorizes\nprototypes of each class, which results in unsatisfactory adaptation\nperformance. To this end, we propose a simple yet effective Relaxed conditional\nGAN (Relaxed cGAN) framework. Specifically, we feed the image without its label\nto our generator. In this way, the generator has to infer the semantic\ninformation of input data. We formally prove that its equilibrium is desirable\nand empirically validate its practical convergence and effectiveness in image\ntransfer. Additionally, we propose several techniques to make use of unlabeled\ndata in the target domain, enhancing the model in SSDA settings. We validate\nour method on the well-adopted datasets: Digits, DomainNet, and Office-Home. We\nachieve state-of-the-art performance on DomainNet, Office-Home and most digit\nbenchmarks in low-resource and high-resource settings.\n", "versions": [{"version": "v1", "created": "Tue, 5 Jan 2021 08:15:48 GMT"}], "update_date": "2021-01-06", "authors_parsed": [["Luo", "Qijun", ""], ["Liu", "Zhili", ""], ["Hong", "Lanqing", ""], ["Li", "Chongxuan", ""], ["Yang", "Kuo", ""], ["Wang", "Liyuan", ""], ["Zhou", "Fengwei", ""], ["Li", "Guilin", ""], ["Li", "Zhenguo", ""], ["Zhu", "Jun", ""]]}, {"id": "2101.01407", "submitter": "Wouter Verbeke", "authors": "Diego Olaya, Wouter Verbeke, Jente Van Belle, Marie-Anne Guerry", "title": "To do or not to do: cost-sensitive causal decision-making", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Causal classification models are adopted across a variety of operational\nbusiness processes to predict the effect of a treatment on a categorical\nbusiness outcome of interest depending on the process instance characteristics.\nThis allows optimizing operational decision-making and selecting the optimal\ntreatment to apply in each specific instance, with the aim of maximizing the\npositive outcome rate. While various powerful approaches have been presented in\nthe literature for learning causal classification models, no formal framework\nhas been elaborated for optimal decision-making based on the estimated\nindividual treatment effects, given the cost of the various treatments and the\nbenefit of the potential outcomes.\n  In this article, we therefore extend upon the expected value framework and\nformally introduce a cost-sensitive decision boundary for double binary causal\nclassification, which is a linear function of the estimated individual\ntreatment effect, the positive outcome probability and the cost and benefit\nparameters of the problem setting. The boundary allows causally classifying\ninstances in the positive and negative treatment class to maximize the expected\ncausal profit, which is introduced as the objective at hand in cost-sensitive\ncausal classification. We introduce the expected causal profit ranker which\nranks instances for maximizing the expected causal profit at each possible\nthreshold for causally classifying instances and differs from the conventional\nranking approach based on the individual treatment effect. The proposed ranking\napproach is experimentally evaluated on synthetic and marketing campaign data\nsets. The results indicate that the presented ranking method effectively\noutperforms the cost-insensitive ranking approach and allows boosting\nprofitability.\n", "versions": [{"version": "v1", "created": "Tue, 5 Jan 2021 08:36:01 GMT"}], "update_date": "2021-01-06", "authors_parsed": [["Olaya", "Diego", ""], ["Verbeke", "Wouter", ""], ["Van Belle", "Jente", ""], ["Guerry", "Marie-Anne", ""]]}, {"id": "2101.01418", "submitter": "Petros Spachos", "authors": "Lili Zhu, Petros Spachos", "title": "Support Vector Machine and YOLO for a Mobile Food Grading System", "comments": null, "journal-ref": null, "doi": "10.1016/j.iot.2021.100359", "report-no": null, "categories": "cs.CV cs.AI cs.LG cs.SY eess.SY", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Food quality and safety are of great concern to society since it is an\nessential guarantee not only for human health but also for social development,\nand stability. Ensuring food quality and safety is a complex process. All food\nprocessing stages should be considered, from cultivating, harvesting and\nstorage to preparation and consumption. Grading is one of the essential\nprocesses to control food quality. This paper proposed a mobile visual-based\nsystem to evaluate food grading. Specifically, the proposed system acquires\nimages of bananas when they are on moving conveyors. A two-layer image\nprocessing system based on machine learning is used to grade bananas, and these\ntwo layers are allocated on edge devices and cloud servers, respectively.\nSupport Vector Machine (SVM) is the first layer to classify bananas based on an\nextracted feature vector composed of color and texture features. Then, the a\nYou Only Look Once (YOLO) v3 model further locating the peel's defected area\nand determining if the inputs belong to the mid-ripened or well-ripened class.\nAccording to experimental results, the first layer's performance achieved an\naccuracy of 98.5% while the accuracy of the second layer is 85.7%, and the\noverall accuracy is 96.4%.\n", "versions": [{"version": "v1", "created": "Tue, 5 Jan 2021 09:01:06 GMT"}], "update_date": "2021-01-29", "authors_parsed": [["Zhu", "Lili", ""], ["Spachos", "Petros", ""]]}, {"id": "2101.01423", "submitter": "Moritz Weber", "authors": "Moritz Weber, Marian Turowski, H\\\"useyin K. \\c{C}akmak, Ralf Mikut,\n  Uwe K\\\"uhnapfel, Veit Hagenmeyer", "title": "Data-Driven Copy-Paste Imputation for Energy Time Series", "comments": "8 pages, 7 figures, submitted to IEEE Transactions on Smart Grid, the\n  first two authors equally contributed to this work", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG physics.data-an", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A cornerstone of the worldwide transition to smart grids are smart meters.\nSmart meters typically collect and provide energy time series that are vital\nfor various applications, such as grid simulations, fault-detection, load\nforecasting, load analysis, and load management. Unfortunately, these time\nseries are often characterized by missing values that must be handled before\nthe data can be used. A common approach to handle missing values in time series\nis imputation. However, existing imputation methods are designed for power time\nseries and do not take into account the total energy of gaps, resulting in\njumps or constant shifts when imputing energy time series. In order to overcome\nthese issues, the present paper introduces the new Copy-Paste Imputation (CPI)\nmethod for energy time series. The CPI method copies data blocks with similar\nproperties and pastes them into gaps of the time series while preserving the\ntotal energy of each gap. The new method is evaluated on a real-world dataset\nthat contains six shares of artificially inserted missing values between 1 and\n30%. It outperforms by far the three benchmark imputation methods selected for\ncomparison. The comparison furthermore shows that the CPI method uses matching\npatterns and preserves the total energy of each gap while requiring only a\nmoderate run-time.\n", "versions": [{"version": "v1", "created": "Tue, 5 Jan 2021 09:26:18 GMT"}], "update_date": "2021-01-06", "authors_parsed": [["Weber", "Moritz", ""], ["Turowski", "Marian", ""], ["\u00c7akmak", "H\u00fcseyin K.", ""], ["Mikut", "Ralf", ""], ["K\u00fchnapfel", "Uwe", ""], ["Hagenmeyer", "Veit", ""]]}, {"id": "2101.01425", "submitter": "Elena Casiraghi Prof.", "authors": "Giorgio Valentini and Elena Casiraghi and Luca Cappelletti and Vida\n  Ravanmehr and Tommaso Fontana and Justin Reese and Peter Robinson", "title": "Het-node2vec: second order random walk sampling for heterogeneous\n  multigraphs embedding", "comments": "20 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.SI physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a set of algorithms (Het-node2vec) that extend the original\nnode2vec node-neighborhood sampling method to heterogeneous multigraphs, i.e.\nnetworks characterized by multiple types of nodes and edges. The resulting\nrandom walk samples capture both the structural characteristics of the graph\nand the semantics of the different types of nodes and edges. The proposed\nalgorithms can focus their attention on specific node or edge types, allowing\naccurate representations also for underrepresented types of nodes/edges that\nare of interest for the prediction problem under investigation. These rich and\nwell-focused representations can boost unsupervised and supervised learning on\nheterogeneous graphs.\n", "versions": [{"version": "v1", "created": "Tue, 5 Jan 2021 09:38:02 GMT"}], "update_date": "2021-01-06", "authors_parsed": [["Valentini", "Giorgio", ""], ["Casiraghi", "Elena", ""], ["Cappelletti", "Luca", ""], ["Ravanmehr", "Vida", ""], ["Fontana", "Tommaso", ""], ["Reese", "Justin", ""], ["Robinson", "Peter", ""]]}, {"id": "2101.01429", "submitter": "Minh Ha Quang", "authors": "Minh Ha Quang", "title": "Convergence and finite sample approximations of entropic regularized\n  Wasserstein distances in Gaussian and RKHS settings", "comments": "51 pages, minor revision, references added", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work studies the convergence and finite sample approximations of\nentropic regularized Wasserstein distances in the Hilbert space setting. Our\nfirst main result is that for Gaussian measures on an infinite-dimensional\nHilbert space, convergence in the 2-Sinkhorn divergence is {\\it strictly\nweaker} than convergence in the exact 2-Wasserstein distance. Specifically, a\nsequence of centered Gaussian measures converges in the 2-Sinkhorn divergence\nif the corresponding covariance operators converge in the Hilbert-Schmidt norm.\nThis is in contrast to the previous known result that a sequence of centered\nGaussian measures converges in the exact 2-Wasserstein distance if and only if\nthe covariance operators converge in the trace class norm. In the reproducing\nkernel Hilbert space (RKHS) setting, the {\\it kernel Gaussian-Sinkhorn\ndivergence}, which is the Sinkhorn divergence between Gaussian measures defined\non an RKHS, defines a semi-metric on the set of Borel probability measures on a\nPolish space, given a characteristic kernel on that space. With the\nHilbert-Schmidt norm convergence, we obtain {\\it dimension-independent}\nconvergence rates for finite sample approximations of the kernel\nGaussian-Sinkhorn divergence, with the same order as the Maximum Mean\nDiscrepancy. These convergence rates apply in particular to Sinkhorn divergence\nbetween Gaussian measures on Euclidean and infinite-dimensional Hilbert spaces.\nThe sample complexity for the 2-Wasserstein distance between Gaussian measures\non Euclidean space, while dimension-dependent and larger than that of the\nSinkhorn divergence, is exponentially faster than the worst case scenario in\nthe literature.\n", "versions": [{"version": "v1", "created": "Tue, 5 Jan 2021 09:46:58 GMT"}, {"version": "v2", "created": "Mon, 15 Feb 2021 09:44:14 GMT"}], "update_date": "2021-02-16", "authors_parsed": [["Quang", "Minh Ha", ""]]}, {"id": "2101.01431", "submitter": "Jiamou Sun", "authors": "Jiamou Sun, Zhenchang Xing, Hao Guo, Deheng Ye, Xiaohong Li, Xiwei Xu,\n  Liming Zhu", "title": "Generating Informative CVE Description From ExploitDB Posts by\n  Extractive Summarization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  ExploitDB is one of the important public websites, which contributes a large\nnumber of vulnerabilities to official CVE database. Over 60\\% of these\nvulnerabilities have high- or critical-security risks. Unfortunately, over 73\\%\nof exploits appear publicly earlier than the corresponding CVEs, and about 40\\%\nof exploits do not even have CVEs. To assist in documenting CVEs for the\nExploitDB posts, we propose an open information method to extract 9 key\nvulnerability aspects (vulnerable product/version/component, vulnerability\ntype, vendor, attacker type, root cause, attack vector and impact) from the\nverbose and noisy ExploitDB posts. The extracted aspects from an ExploitDB post\nare then composed into a CVE description according to the suggested CVE\ndescription templates, which is must-provided information for requesting new\nCVEs. Through the evaluation on 13,017 manually labeled sentences and the\nstatistically sampling of 3,456 extracted aspects, we confirm the high accuracy\nof our extraction method. Compared with 27,230 reference CVE descriptions. Our\ncomposed CVE descriptions achieve high ROUGH-L (0.38), a longest common\nsubsequence based metric for evaluating text summarization methods.\n", "versions": [{"version": "v1", "created": "Tue, 5 Jan 2021 09:52:05 GMT"}], "update_date": "2021-01-06", "authors_parsed": [["Sun", "Jiamou", ""], ["Xing", "Zhenchang", ""], ["Guo", "Hao", ""], ["Ye", "Deheng", ""], ["Li", "Xiaohong", ""], ["Xu", "Xiwei", ""], ["Zhu", "Liming", ""]]}, {"id": "2101.01441", "submitter": "Sangkyun Lee", "authors": "Hyeongmin Cho, Sangkyun Lee", "title": "Data Quality Measures and Efficient Evaluation Algorithms for\n  Large-Scale High-Dimensional Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Machine learning has been proven to be effective in various application\nareas, such as object and speech recognition on mobile systems. Since a\ncritical key to machine learning success is the availability of large training\ndata, many datasets are being disclosed and published online. From a data\nconsumer or manager point of view, measuring data quality is an important first\nstep in the learning process. We need to determine which datasets to use,\nupdate, and maintain. However, not many practical ways to measure data quality\nare available today, especially when it comes to large-scale high-dimensional\ndata, such as images and videos. This paper proposes two data quality measures\nthat can compute class separability and in-class variability, the two important\naspects of data quality, for a given dataset. Classical data quality measures\ntend to focus only on class separability; however, we suggest that in-class\nvariability is another important data quality factor. We provide efficient\nalgorithms to compute our quality measures based on random projections and\nbootstrapping with statistical benefits on large-scale high-dimensional data.\nIn experiments, we show that our measures are compatible with classical\nmeasures on small-scale data and can be computed much more efficiently on\nlarge-scale high-dimensional datasets.\n", "versions": [{"version": "v1", "created": "Tue, 5 Jan 2021 10:23:08 GMT"}], "update_date": "2021-01-06", "authors_parsed": [["Cho", "Hyeongmin", ""], ["Lee", "Sangkyun", ""]]}, {"id": "2101.01456", "submitter": "Bojia Zi", "authors": "Bojia Zi, Minghao Chang, Jingjing Chen, Xingjun Ma, Yu-Gang Jiang", "title": "WildDeepfake: A Challenging Real-World Dataset for Deepfake Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  In recent years, the abuse of a face swap technique called deepfake Deepfake\nhas raised enormous public concerns. So far, a large number of deepfake videos\n(known as \"deepfakes\") have been crafted and uploaded to the internet, calling\nfor effective countermeasures. One promising countermeasure against deepfakes\nis deepfake detection. Several deepfake datasets have been released to support\nthe training and testing of deepfake detectors, such as DeepfakeDetection and\nFaceForensics++. While this has greatly advanced deepfake detection, most of\nthe real videos in these datasets are filmed with a few volunteer actors in\nlimited scenes, and the fake videos are crafted by researchers using a few\npopular deepfake softwares. Detectors developed on these datasets may become\nless effective against real-world deepfakes on the internet. To better support\ndetection against real-world deepfakes, in this paper, we introduce a new\ndataset WildDeepfake, which consists of 7,314 face sequences extracted from 707\ndeepfake videos collected completely from the internet. WildDeepfake is a small\ndataset that can be used, in addition to existing datasets, to develop and test\nthe effectiveness of deepfake detectors against real-world deepfakes. We\nconduct a systematic evaluation of a set of baseline detection networks on both\nexisting and our WildDeepfake datasets, and show that WildDeepfake is indeed a\nmore challenging dataset, where the detection performance can decrease\ndrastically. We also propose two (eg. 2D and 3D) Attention-based Deepfake\nDetection Networks (ADDNets) to leverage the attention masks on real/fake faces\nfor improved detection. We empirically verify the effectiveness of ADDNets on\nboth existing datasets and WildDeepfake. The dataset is available\nat:https://github.com/deepfakeinthewild/deepfake-in-the-wild.\n", "versions": [{"version": "v1", "created": "Tue, 5 Jan 2021 11:10:32 GMT"}], "update_date": "2021-01-06", "authors_parsed": [["Zi", "Bojia", ""], ["Chang", "Minghao", ""], ["Chen", "Jingjing", ""], ["Ma", "Xingjun", ""], ["Jiang", "Yu-Gang", ""]]}, {"id": "2101.01473", "submitter": "Tsuyoshi Kato", "authors": "Kenya Tajima, Takahiko Henmi, Kohei Tsuchida, Esmeraldo Ronnie R.\n  Zara, and Tsuyoshi Kato", "title": "Learning Sign-Constrained Support Vector Machines", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Domain knowledge is useful to improve the generalization performance of\nlearning machines. Sign constraints are a handy representation to combine\ndomain knowledge with learning machine. In this paper, we consider constraining\nthe signs of the weight coefficients in learning the linear support vector\nmachine, and develop two optimization algorithms for minimizing the empirical\nrisk under the sign constraints. One of the two algorithms is based on the\nprojected gradient method, in which each iteration of the projected gradient\nmethod takes $O(nd)$ computational cost and the sublinear convergence of the\nobjective error is guaranteed. The second algorithm is based on the Frank-Wolfe\nmethod that also converges sublinearly and possesses a clear termination\ncriterion. We show that each iteration of the Frank-Wolfe also requires $O(nd)$\ncost. Furthermore, we derive the explicit expression for the minimal iteration\nnumber to ensure an $\\epsilon$-accurate solution by analyzing the curvature of\nthe objective function. Finally, we empirically demonstrate that the sign\nconstraints are a promising technique when similarities to the training\nexamples compose the feature vector.\n", "versions": [{"version": "v1", "created": "Tue, 5 Jan 2021 12:08:17 GMT"}], "update_date": "2021-01-06", "authors_parsed": [["Tajima", "Kenya", ""], ["Henmi", "Takahiko", ""], ["Tsuchida", "Kohei", ""], ["Zara", "Esmeraldo Ronnie R.", ""], ["Kato", "Tsuyoshi", ""]]}, {"id": "2101.01494", "submitter": "Wouter Verbeke", "authors": "Jakob Raymaekers, Wouter Verbeke, Tim Verdonck", "title": "Weight-of-evidence 2.0 with shrinkage and spline-binning", "comments": "New version: duplicate paragraph omitted", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In many practical applications, such as fraud detection, credit risk modeling\nor medical decision making, classification models for assigning instances to a\npredefined set of classes are required to be both precise as well as\ninterpretable. Linear modeling methods such as logistic regression are often\nadopted, since they offer an acceptable balance between precision and\ninterpretability. Linear methods, however, are not well equipped to handle\ncategorical predictors with high-cardinality or to exploit non-linear relations\nin the data. As a solution, data preprocessing methods such as\nweight-of-evidence are typically used for transforming the predictors. The\nbinning procedure that underlies the weight-of-evidence approach, however, has\nbeen little researched and typically relies on ad-hoc or expert driven\nprocedures. The objective in this paper, therefore, is to propose a formalized,\ndata-driven and powerful method.\n  To this end, we explore the discretization of continuous variables through\nthe binning of spline functions, which allows for capturing non-linear effects\nin the predictor variables and yields highly interpretable predictors taking\nonly a small number of discrete values. Moreover, we extend upon the\nweight-of-evidence approach and propose to estimate the proportions using\nshrinkage estimators. Together, this offers an improved ability to exploit both\nnon-linear and categorical predictors for achieving increased classification\nprecision, while maintaining interpretability of the resulting model and\ndecreasing the risk of overfitting.\n  We present the results of a series of experiments in a fraud detection\nsetting, which illustrate the effectiveness of the presented approach. We\nfacilitate reproduction of the presented results and adoption of the proposed\napproaches by providing both the dataset and the code for implementing the\nexperiments and the presented approach.\n", "versions": [{"version": "v1", "created": "Tue, 5 Jan 2021 13:13:16 GMT"}, {"version": "v2", "created": "Tue, 2 Feb 2021 08:02:49 GMT"}], "update_date": "2021-02-03", "authors_parsed": [["Raymaekers", "Jakob", ""], ["Verbeke", "Wouter", ""], ["Verdonck", "Tim", ""]]}, {"id": "2101.01502", "submitter": "Ichiro Hasuo", "authors": "Ichiro Hasuo, Yuichiro Oyabu, Clovis Eberhart, Kohei Suenaga, Kenta\n  Cho, Shin-ya Katsumata", "title": "Control-Data Separation and Logical Condition Propagation for Efficient\n  Inference on Probabilistic Programs", "comments": "11 pages with appendices", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.PL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We introduce a novel sampling algorithm for Bayesian inference on imperative\nprobabilistic programs. It features a hierarchical architecture that separates\ncontrol flows from data: the top-level samples a control flow, and the bottom\nlevel samples data values along the control flow picked by the top level. This\nseparation allows us to plug various language-based analysis techniques in\nprobabilistic program sampling; specifically, we use logical backward\npropagation of observations for sampling efficiency. We implemented our\nalgorithm on top of Anglican. The experimental results demonstrate our\nalgorithm's efficiency, especially for programs with while loops and rare\nobservations.\n", "versions": [{"version": "v1", "created": "Tue, 5 Jan 2021 13:40:59 GMT"}, {"version": "v2", "created": "Thu, 28 Jan 2021 14:43:18 GMT"}], "update_date": "2021-01-29", "authors_parsed": [["Hasuo", "Ichiro", ""], ["Oyabu", "Yuichiro", ""], ["Eberhart", "Clovis", ""], ["Suenaga", "Kohei", ""], ["Cho", "Kenta", ""], ["Katsumata", "Shin-ya", ""]]}, {"id": "2101.01505", "submitter": "Xiang Li", "authors": "Xiang Li, Zhihua Zhang", "title": "Delayed Projection Techniques for Linearly Constrained Problems:\n  Convergence Rates, Acceleration, and Applications", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we study a novel class of projection-based algorithms for\nlinearly constrained problems (LCPs) which have a lot of applications in\nstatistics, optimization, and machine learning. Conventional primal\ngradient-based methods for LCPs call a projection after each (stochastic)\ngradient descent, resulting in that the required number of projections equals\nthat of gradient descents (or total iterations). Motivated by the recent\nprogress in distributed optimization, we propose the delayed projection\ntechnique that calls a projection once for a while, lowering the projection\nfrequency and improving the projection efficiency. Accordingly, we devise a\nseries of stochastic methods for LCPs using the technique, including a variance\nreduced method and an accelerated one. We theoretically show that it is\nfeasible to improve projection efficiency in both strongly convex and generally\nconvex cases. Our analysis is simple and unified and can be easily extended to\nother methods using delayed projections. When applying our new algorithms to\nfederated optimization, a newfangled and privacy-preserving subfield in\ndistributed optimization, we obtain not only a variance reduced federated\nalgorithm with convergence rates better than previous works, but also the first\naccelerated method able to handle data heterogeneity inherent in federated\noptimization.\n", "versions": [{"version": "v1", "created": "Tue, 5 Jan 2021 13:42:41 GMT"}], "update_date": "2021-01-06", "authors_parsed": [["Li", "Xiang", ""], ["Zhang", "Zhihua", ""]]}, {"id": "2101.01506", "submitter": "Marcus Haywood-Alexander", "authors": "Marcus Haywood-Alexander, Nikolaos Dervilis, Keith Worden, Elizabeth\n  J. Cross, Robin S. Mills, Timothy J. Rogers", "title": "Structured Machine Learning Tools for Modelling Characteristics of\n  Guided Waves", "comments": "33 pages, 11 figures", "journal-ref": null, "doi": "10.1016/j.ymssp.2021.107628", "report-no": null, "categories": "stat.ML cs.LG", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  The use of ultrasonic guided waves to probe the materials/structures for\ndamage continues to increase in popularity for non-destructive evaluation (NDE)\nand structural health monitoring (SHM). The use of high-frequency waves such as\nthese offers an advantage over low-frequency methods from their ability to\ndetect damage on a smaller scale. However, in order to assess damage in a\nstructure, and implement any NDE or SHM tool, knowledge of the behaviour of a\nguided wave throughout the material/structure is important (especially when\ndesigning sensor placement for SHM systems). Determining this behaviour is\nextremely diffcult in complex materials, such as fibre-matrix composites, where\nunique phenomena such as continuous mode conversion takes place. This paper\nintroduces a novel method for modelling the feature-space of guided waves in a\ncomposite material. This technique is based on a data-driven model, where prior\nphysical knowledge can be used to create structured machine learning tools;\nwhere constraints are applied to provide said structure. The method shown makes\nuse of Gaussian processes, a full Bayesian analysis tool, and in this paper it\nis shown how physical knowledge of the guided waves can be utilised in\nmodelling using an ML tool. This paper shows that through careful consideration\nwhen applying machine learning techniques, more robust models can be generated\nwhich offer advantages such as extrapolation ability and physical\ninterpretation.\n", "versions": [{"version": "v1", "created": "Tue, 5 Jan 2021 13:42:50 GMT"}], "update_date": "2021-02-15", "authors_parsed": [["Haywood-Alexander", "Marcus", ""], ["Dervilis", "Nikolaos", ""], ["Worden", "Keith", ""], ["Cross", "Elizabeth J.", ""], ["Mills", "Robin S.", ""], ["Rogers", "Timothy J.", ""]]}, {"id": "2101.01509", "submitter": "Stefan Tiegel", "authors": "David Steurer, Stefan Tiegel", "title": "SoS Degree Reduction with Applications to Clustering and Robust Moment\n  Estimation", "comments": "32 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop a general framework to significantly reduce the degree of\nsum-of-squares proofs by introducing new variables. To illustrate the power of\nthis framework, we use it to speed up previous algorithms based on\nsum-of-squares for two important estimation problems, clustering and robust\nmoment estimation. The resulting algorithms offer the same statistical\nguarantees as the previous best algorithms but have significantly faster\nrunning times. Roughly speaking, given a sample of $n$ points in dimension $d$,\nour algorithms can exploit order-$\\ell$ moments in time $d^{O(\\ell)}\\cdot\nn^{O(1)}$, whereas a naive implementation requires time $(d\\cdot n)^{O(\\ell)}$.\nSince for the aforementioned applications, the typical sample size is\n$d^{\\Theta(\\ell)}$, our framework improves running times from $d^{O(\\ell^2)}$\nto $d^{O(\\ell)}$.\n", "versions": [{"version": "v1", "created": "Tue, 5 Jan 2021 13:49:59 GMT"}], "update_date": "2021-01-06", "authors_parsed": [["Steurer", "David", ""], ["Tiegel", "Stefan", ""]]}, {"id": "2101.01519", "submitter": "Zoltan Szabo", "authors": "Pierre-Cyril Aubin-Frankowski, Zoltan Szabo", "title": "Handling Hard Affine SDP Shape Constraints in RKHSs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Shape constraints, such as non-negativity, monotonicity, convexity or\nsupermodularity, play a key role in various applications of machine learning\nand statistics. However, incorporating this side information into predictive\nmodels in a hard way (for example at all points of an interval) for rich\nfunction classes is a notoriously challenging problem. We propose a unified and\nmodular convex optimization framework, relying on second-order cone (SOC)\ntightening, to encode hard affine SDP constraints on function derivatives, for\nmodels belonging to vector-valued reproducing kernel Hilbert spaces (vRKHSs).\nThe modular nature of the proposed approach allows to simultaneously handle\nmultiple shape constraints, and to tighten an infinite number of constraints\ninto finitely many. We prove the consistency of the proposed scheme and that of\nits adaptive variant, leveraging geometric properties of vRKHSs. The efficiency\nof the approach is illustrated in the context of shape optimization,\nsafety-critical control and econometrics.\n", "versions": [{"version": "v1", "created": "Tue, 5 Jan 2021 14:08:58 GMT"}], "update_date": "2021-01-06", "authors_parsed": [["Aubin-Frankowski", "Pierre-Cyril", ""], ["Szabo", "Zoltan", ""]]}, {"id": "2101.01524", "submitter": "Dakuo Wang", "authors": "Dakuo Wang and Liuping Wang and Zhan Zhang and Ding Wang and Haiyi Zhu\n  and Yvonne Gao and Xiangmin Fan and Feng Tian", "title": "\"Brilliant AI Doctor\" in Rural China: Tensions and Challenges in\n  AI-Powered CDSS Deployment", "comments": null, "journal-ref": null, "doi": "10.1145/3411764.3445432", "report-no": null, "categories": "cs.HC cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Artificial intelligence (AI) technology has been increasingly used in the\nimplementation of advanced Clinical Decision Support Systems (CDSS). Research\ndemonstrated the potential usefulness of AI-powered CDSS (AI-CDSS) in clinical\ndecision making scenarios. However, post-adoption user perception and\nexperience remain understudied, especially in developing countries. Through\nobservations and interviews with 22 clinicians from 6 rural clinics in China,\nthis paper reports the various tensions between the design of an AI-CDSS system\n(\"Brilliant Doctor\") and the rural clinical context, such as the misalignment\nwith local context and workflow, the technical limitations and usability\nbarriers, as well as issues related to transparency and trustworthiness of\nAI-CDSS. Despite these tensions, all participants expressed positive attitudes\ntoward the future of AI-CDSS, especially acting as \"a doctor's AI assistant\" to\nrealize a Human-AI Collaboration future in clinical settings. Finally we draw\non our findings to discuss implications for designing AI-CDSS interventions for\nrural clinical contexts in developing countries.\n", "versions": [{"version": "v1", "created": "Mon, 4 Jan 2021 05:32:48 GMT"}, {"version": "v2", "created": "Tue, 12 Jan 2021 22:44:26 GMT"}], "update_date": "2021-01-14", "authors_parsed": [["Wang", "Dakuo", ""], ["Wang", "Liuping", ""], ["Zhang", "Zhan", ""], ["Wang", "Ding", ""], ["Zhu", "Haiyi", ""], ["Gao", "Yvonne", ""], ["Fan", "Xiangmin", ""], ["Tian", "Feng", ""]]}, {"id": "2101.01533", "submitter": "John Tsotsos", "authors": "John K. Tsotsos, Omar Abid, Iuliia Kotseruba, Markus D. Solbach", "title": "On the Control of Attentional Processes in Vision", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CC cs.CV cs.LG q-bio.NC", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  The study of attentional processing in vision has a long and deep history.\nRecently, several papers have presented insightful perspectives into how the\ncoordination of multiple attentional functions in the brain might occur. These\nbegin with experimental observations and the authors propose structures,\nprocesses, and computations that might explain those observations. Here, we\nconsider a perspective that past works have not, as a complementary approach to\nthe experimentally-grounded ones. We approach the same problem as past authors\nbut from the other end of the computational spectrum, from the problem nature,\nas Marr's Computational Level would prescribe. What problem must the brain\nsolve when orchestrating attentional processes in order to successfully\ncomplete one of the myriad possible visuospatial tasks at which we as humans\nexcel? The hope, of course, is for the approaches to eventually meet and thus\nform a complete theory, but this is likely not soon. We make the first steps\ntowards this by addressing the necessity of attentional control, examining the\nbreadth and computational difficulty of the visuospatial and attentional tasks\nseen in human behavior, and suggesting a sketch of how attentional control\nmight arise in the brain. The key conclusions of this paper are that an\nexecutive controller is necessary for human attentional function in vision, and\nthat there is a 'first principles' computational approach to its understanding\nthat is complementary to the previous approaches that focus on modelling or\nlearning from experimental observations directly.\n", "versions": [{"version": "v1", "created": "Tue, 5 Jan 2021 14:24:20 GMT"}], "update_date": "2021-01-06", "authors_parsed": [["Tsotsos", "John K.", ""], ["Abid", "Omar", ""], ["Kotseruba", "Iuliia", ""], ["Solbach", "Markus D.", ""]]}, {"id": "2101.01568", "submitter": "C\\'esar Quilodr\\'an-Casas", "authors": "C\\'esar Quilodr\\'an-Casas, Rossella Arcucci, Christopher Pain, Yike\n  Guo", "title": "Adversarially trained LSTMs on reduced order models of urban air\n  pollution simulations", "comments": "6 pages, Third workshop on Machine LEarning and the Physical Sciences\n  at NeurIPS 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG physics.flu-dyn", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents an approach to improve computational fluid dynamics\nsimulations forecasts of air pollution using deep learning. Our method, which\nintegrates Principal Components Analysis (PCA) and adversarial training, is a\nway to improve the forecast skill of reduced order models obtained from the\noriginal model solution. Once the reduced-order model (ROM) is obtained via\nPCA, a Long Short-Term Memory network (LSTM) is adversarially trained on the\nROM to make forecasts. Once trained, the adversarially trained LSTM outperforms\na LSTM trained in a classical way. The study area is in London, including\nvelocities and a concentration tracer that replicates a busy traffic junction.\nThis adversarially trained LSTM-based approach is used on the ROM in order to\nproduce faster forecasts of the air pollution tracer.\n", "versions": [{"version": "v1", "created": "Tue, 5 Jan 2021 15:02:18 GMT"}], "update_date": "2021-01-06", "authors_parsed": [["Quilodr\u00e1n-Casas", "C\u00e9sar", ""], ["Arcucci", "Rossella", ""], ["Pain", "Christopher", ""], ["Guo", "Yike", ""]]}, {"id": "2101.01570", "submitter": "Zaccharie Ramzi", "authors": "Zaccharie Ramzi, Jean-Luc Starck, Philippe Ciuciu", "title": "Density Compensated Unrolled Networks for Non-Cartesian MRI\n  Reconstruction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG physics.med-ph stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Deep neural networks have recently been thoroughly investigated as a powerful\ntool for MRI reconstruction. There is a lack of research, however, regarding\ntheir use for a specific setting of MRI, namely non-Cartesian acquisitions. In\nthis work, we introduce a novel kind of deep neural networks to tackle this\nproblem, namely density compensated unrolled neural networks, which rely on\nDensity Compensation to correct the uneven weighting of the k-space. We assess\ntheir efficiency on the publicly available fastMRI dataset, and perform a small\nablation study. Our results show that the density-compensated unrolled neural\nnetworks outperform the different baselines, and that all parts of the design\nare needed. We also open source our code, in particular a Non-Uniform Fast\nFourier transform for TensorFlow.\n", "versions": [{"version": "v1", "created": "Tue, 5 Jan 2021 15:03:38 GMT"}, {"version": "v2", "created": "Mon, 8 Feb 2021 10:52:24 GMT"}], "update_date": "2021-02-09", "authors_parsed": [["Ramzi", "Zaccharie", ""], ["Starck", "Jean-Luc", ""], ["Ciuciu", "Philippe", ""]]}, {"id": "2101.01571", "submitter": "Burak \\c{C}akmak", "authors": "Burak \\c{C}akmak and Manfred Opper", "title": "Exact solution to the random sequential dynamics of a message passing\n  algorithm", "comments": "Accepted for publication in Physical Review E Letter", "journal-ref": "Phys. Rev. E 103, 030101 (2021)", "doi": "10.1103/PhysRevE.103.L030101", "report-no": null, "categories": "cond-mat.dis-nn cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We analyze the random sequential dynamics of a message passing algorithm for\nIsing models with random interactions in the large system limit. We derive\nexact results for the two-time correlation functions and the speed of\nconvergence. The {\\em de Almedia-Thouless} stability criterion of the static\nproblem is found to be necessary and sufficient for the global convergence of\nthe random sequential dynamics.\n", "versions": [{"version": "v1", "created": "Tue, 5 Jan 2021 15:03:54 GMT"}, {"version": "v2", "created": "Tue, 2 Mar 2021 20:09:56 GMT"}], "update_date": "2021-03-10", "authors_parsed": [["\u00c7akmak", "Burak", ""], ["Opper", "Manfred", ""]]}, {"id": "2101.01572", "submitter": "Anshuka Rangi", "authors": "Anshuka Rangi, Massimo Franceschetti and Long Tran-Thanh", "title": "Sequential Choice Bandits with Feedback for Personalizing users'\n  experience", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we study sequential choice bandits with feedback. We propose\nbandit algorithms for a platform that personalizes users' experience to\nmaximize its rewards. For each action directed to a given user, the platform is\ngiven a positive reward, which is a non-decreasing function of the action, if\nthis action is below the user's threshold. Users are equipped with a patience\nbudget, and actions that are above the threshold decrease the user's patience.\nWhen all patience is lost, the user abandons the platform. The platform\nattempts to learn the thresholds of the users in order to maximize its rewards,\nbased on two different feedback models describing the information pattern\navailable to the platform at each action. We define a notion of regret by\ndetermining the best action to be taken when the platform knows that the user's\nthreshold is in a given interval. We then propose bandit algorithms for the two\nfeedback models and show that upper and lower bounds on the regret are of the\norder of $\\tilde{O}(N^{2/3})$ and $\\tilde\\Omega(N^{2/3})$, respectively, where\n$N$ is the total number of users. Finally, we show that the waiting time of any\nuser before receiving a personalized experience is uniform in $N$.\n", "versions": [{"version": "v1", "created": "Tue, 5 Jan 2021 15:04:10 GMT"}], "update_date": "2021-01-06", "authors_parsed": [["Rangi", "Anshuka", ""], ["Franceschetti", "Massimo", ""], ["Tran-Thanh", "Long", ""]]}, {"id": "2101.01576", "submitter": "Lucas Murtinho", "authors": "Eduardo Laber, Lucas Murtinho", "title": "On the price of explainability for some clustering problems", "comments": "23 pages, 2 figures; general revision of text + added section on\n  practical algorithm (with experimental results) for the $k$-means problem", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  The price of explainability for a clustering task can be defined as the\nunavoidable loss,in terms of the objective function, if we force the final\npartition to be explainable.\n  Here, we study this price for the following clustering problems: $k$-means,\n$k$-medians, $k$-centers and maximum-spacing. We provide upper and lower bounds\nfor a natural model where explainability is achieved via decision trees. For\nthe $k$-means and $k$-medians problems our upper bounds improve those obtained\nby [Moshkovitz et. al, ICML 20] for low dimensions.\n  Another contribution is a simple and efficient algorithm for building\nexplainable clusterings for the $k$-means problem. We provide empirical\nevidence that its performance is better than the current state of the art for\ndecision-tree based explainable clustering.\n", "versions": [{"version": "v1", "created": "Tue, 5 Jan 2021 15:08:25 GMT"}, {"version": "v2", "created": "Sat, 13 Feb 2021 14:39:39 GMT"}], "update_date": "2021-02-16", "authors_parsed": [["Laber", "Eduardo", ""], ["Murtinho", "Lucas", ""]]}, {"id": "2101.01583", "submitter": "Dakuo Wang", "authors": "Liuping Wang and Dakuo Wang and Feng Tian and Zhenhui Peng and\n  Xiangmin Fan and Zhan Zhang and Shuai Ma and Mo Yu and Xiaojuan Ma and Hongan\n  Wang", "title": "CASS: Towards Building a Social-Support Chatbot for Online Health\n  Community", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Chatbots systems, despite their popularity in today's HCI and CSCW research,\nfall short for one of the two reasons: 1) many of the systems use a rule-based\ndialog flow, thus they can only respond to a limited number of pre-defined\ninputs with pre-scripted responses; or 2) they are designed with a focus on\nsingle-user scenarios, thus it is unclear how these systems may affect other\nusers or the community. In this paper, we develop a generalizable chatbot\narchitecture (CASS) to provide social support for community members in an\nonline health community. The CASS architecture is based on advanced neural\nnetwork algorithms, thus it can handle new inputs from users and generate a\nvariety of responses to them. CASS is also generalizable as it can be easily\nmigrate to other online communities. With a follow-up field experiment, CASS is\nproven useful in supporting individual members who seek emotional support. Our\nwork also contributes to fill the research gap on how a chatbot may influence\nthe whole community's engagement.\n", "versions": [{"version": "v1", "created": "Mon, 4 Jan 2021 05:52:03 GMT"}, {"version": "v2", "created": "Wed, 13 Jan 2021 08:42:24 GMT"}, {"version": "v3", "created": "Thu, 4 Feb 2021 08:01:03 GMT"}], "update_date": "2021-02-05", "authors_parsed": [["Wang", "Liuping", ""], ["Wang", "Dakuo", ""], ["Tian", "Feng", ""], ["Peng", "Zhenhui", ""], ["Fan", "Xiangmin", ""], ["Zhang", "Zhan", ""], ["Ma", "Shuai", ""], ["Yu", "Mo", ""], ["Ma", "Xiaojuan", ""], ["Wang", "Hongan", ""]]}, {"id": "2101.01600", "submitter": "Didac Sur\\'is Coll-Vinent", "authors": "D\\'idac Sur\\'is, Ruoshi Liu, Carl Vondrick", "title": "Learning the Predictability of the Future", "comments": "Website: https://hyperfuture.cs.columbia.edu", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a framework for learning from unlabeled video what is\npredictable in the future. Instead of committing up front to features to\npredict, our approach learns from data which features are predictable. Based on\nthe observation that hyperbolic geometry naturally and compactly encodes\nhierarchical structure, we propose a predictive model in hyperbolic space. When\nthe model is most confident, it will predict at a concrete level of the\nhierarchy, but when the model is not confident, it learns to automatically\nselect a higher level of abstraction. Experiments on two established datasets\nshow the key role of hierarchical representations for action prediction.\nAlthough our representation is trained with unlabeled video, visualizations\nshow that action hierarchies emerge in the representation.\n", "versions": [{"version": "v1", "created": "Fri, 1 Jan 2021 18:58:36 GMT"}], "update_date": "2021-01-06", "authors_parsed": [["Sur\u00eds", "D\u00eddac", ""], ["Liu", "Ruoshi", ""], ["Vondrick", "Carl", ""]]}, {"id": "2101.01618", "submitter": "Robin Winter", "authors": "Robin Winter, Frank No\\'e, Djork-Arn\\'e Clevert", "title": "Auto-Encoding Molecular Conformations", "comments": "6 pages, 2 figures, presented at Machine Learning for Molecules\n  Workshop at NeurIPS 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG physics.chem-ph q-bio.QM", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this work we introduce an Autoencoder for molecular conformations. Our\nproposed model converts the discrete spatial arrangements of atoms in a given\nmolecular graph (conformation) into and from a continuous fixed-sized latent\nrepresentation. We demonstrate that in this latent representation, similar\nconformations cluster together while distinct conformations split apart.\nMoreover, by training a probabilistic model on a large dataset of molecular\nconformations, we demonstrate how our model can be used to generate diverse\nsets of energetically favorable conformations for a given molecule. Finally, we\nshow that the continuous representation allows us to utilize optimization\nmethods to find molecules that have conformations with favourable spatial\nproperties.\n", "versions": [{"version": "v1", "created": "Tue, 5 Jan 2021 16:09:10 GMT"}], "update_date": "2021-01-06", "authors_parsed": [["Winter", "Robin", ""], ["No\u00e9", "Frank", ""], ["Clevert", "Djork-Arn\u00e9", ""]]}, {"id": "2101.01628", "submitter": "David Noever", "authors": "David Noever, Josh Kalin, Matt Ciolino, Dom Hambrick, and Gerry Dozier", "title": "Local Translation Services for Neglected Languages", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Taking advantage of computationally lightweight, but high-quality translators\nprompt consideration of new applications that address neglected languages.\nLocally run translators for less popular languages may assist data projects\nwith protected or personal data that may require specific compliance checks\nbefore posting to a public translation API, but which could render reasonable,\ncost-effective solutions if done with an army of local, small-scale pair\ntranslators. Like handling a specialist's dialect, this research illustrates\ntranslating two historically interesting, but obfuscated languages: 1)\nhacker-speak (\"l33t\") and 2) reverse (or \"mirror\") writing as practiced by\nLeonardo da Vinci. The work generalizes a deep learning architecture to\ntranslatable variants of hacker-speak with lite, medium, and hard vocabularies.\nThe original contribution highlights a fluent translator of hacker-speak in\nunder 50 megabytes and demonstrates a generator for augmenting future datasets\nwith greater than a million bilingual sentence pairs. The long short-term\nmemory, recurrent neural network (LSTM-RNN) extends previous work demonstrating\nan English-to-foreign translation service built from as little as 10,000\nbilingual sentence pairs. This work further solves the equivalent translation\nproblem in twenty-six additional (non-obfuscated) languages and rank orders\nthose models and their proficiency quantitatively with Italian as the most\nsuccessful and Mandarin Chinese as the most challenging. For neglected\nlanguages, the method prototypes novel services for smaller niche translations\nsuch as Kabyle (Algerian dialect) which covers between 5-7 million speakers but\none which for most enterprise translators, has not yet reached development. One\nanticipates the extension of this approach to other important dialects, such as\ntranslating technical (medical or legal) jargon and processing health records.\n", "versions": [{"version": "v1", "created": "Tue, 5 Jan 2021 16:25:51 GMT"}, {"version": "v2", "created": "Wed, 13 Jan 2021 20:09:07 GMT"}], "update_date": "2021-01-15", "authors_parsed": [["Noever", "David", ""], ["Kalin", "Josh", ""], ["Ciolino", "Matt", ""], ["Hambrick", "Dom", ""], ["Dozier", "Gerry", ""]]}, {"id": "2101.01637", "submitter": "Chao Zhang", "authors": "Chao Zhang, Joaquin Vanschoren, Arlette van Wissen, Daniel Lakens,\n  Boris de Ruyter, and Wijnand A. IJsselsteijn", "title": "Theory-based Habit Modeling for Enhancing Behavior Prediction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.HC cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Psychological theories of habit posit that when a strong habit is formed\nthrough behavioral repetition, it can trigger behavior automatically in the\nsame environment. Given the reciprocal relationship between habit and behavior,\nchanging lifestyle behaviors (e.g., toothbrushing) is largely a task of\nbreaking old habits and creating new and healthy ones. Thus, representing\nusers' habit strengths can be very useful for behavior change support systems\n(BCSS), for example, to predict behavior or to decide when an intervention\nreaches its intended effect. However, habit strength is not directly observable\nand existing self-report measures are taxing for users. In this paper, built on\nrecent computational models of habit formation, we propose a method to enable\nintelligent systems to compute habit strength based on observable behavior. The\nhypothesized advantage of using computed habit strength for behavior prediction\nwas tested using data from two intervention studies, where we trained\nparticipants to brush their teeth twice a day for three weeks and monitored\ntheir behaviors using accelerometers. Through hierarchical cross-validation, we\nfound that for the task of predicting future brushing behavior, computed habit\nstrength clearly outperformed self-reported habit strength (in both studies)\nand was also superior to models based on past behavior frequency (in the larger\nsecond study). Our findings provide initial support for our theory-based\napproach of modeling user habits and encourages the use of habit computation to\ndeliver personalized and adaptive interventions.\n", "versions": [{"version": "v1", "created": "Tue, 5 Jan 2021 16:42:59 GMT"}], "update_date": "2021-01-06", "authors_parsed": [["Zhang", "Chao", ""], ["Vanschoren", "Joaquin", ""], ["van Wissen", "Arlette", ""], ["Lakens", "Daniel", ""], ["de Ruyter", "Boris", ""], ["IJsselsteijn", "Wijnand A.", ""]]}, {"id": "2101.01666", "submitter": "Muhammad Uzair Zahid", "authors": "Muhammad Uzair Zahid, Serkan Kiranyaz, Turker Ince, Ozer Can\n  Devecioglu, Muhammad E. H. Chowdhury, Amith Khandakar, Anas Tahir and Moncef\n  Gabbouj", "title": "Robust R-Peak Detection in Low-Quality Holter ECGs using 1D\n  Convolutional Neural Network", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SP cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Noise and low quality of ECG signals acquired from Holter or wearable devices\ndeteriorate the accuracy and robustness of R-peak detection algorithms. This\npaper presents a generic and robust system for R-peak detection in Holter ECG\nsignals. While many proposed algorithms have successfully addressed the problem\nof ECG R-peak detection, there is still a notable gap in the performance of\nthese detectors on such low-quality ECG records. Therefore, in this study, a\nnovel implementation of the 1D Convolutional Neural Network (CNN) is used\nintegrated with a verification model to reduce the number of false alarms. This\nCNN architecture consists of an encoder block and a corresponding decoder block\nfollowed by a sample-wise classification layer to construct the 1D segmentation\nmap of R- peaks from the input ECG signal. Once the proposed model has been\ntrained, it can solely be used to detect R-peaks possibly in a single channel\nECG data stream quickly and accurately, or alternatively, such a solution can\nbe conveniently employed for real-time monitoring on a lightweight portable\ndevice. The model is tested on two open-access ECG databases: The China\nPhysiological Signal Challenge (2020) database (CPSC-DB) with more than one\nmillion beats, and the commonly used MIT-BIH Arrhythmia Database (MIT-DB).\nExperimental results demonstrate that the proposed systematic approach achieves\n99.30% F1-score, 99.69% recall, and 98.91% precision in CPSC-DB, which is the\nbest R-peak detection performance ever achieved. Compared to all competing\nmethods, the proposed approach can reduce the false-positives and\nfalse-negatives in Holter ECG signals by more than 54% and 82%, respectively.\nResults also demonstrate similar or better performance than most competing\nalgorithms on MIT-DB with 99.83% F1-score, 99.85% recall, and 99.82% precision.\n", "versions": [{"version": "v1", "created": "Tue, 29 Dec 2020 21:10:54 GMT"}, {"version": "v2", "created": "Tue, 8 Jun 2021 14:04:19 GMT"}], "update_date": "2021-06-09", "authors_parsed": [["Zahid", "Muhammad Uzair", ""], ["Kiranyaz", "Serkan", ""], ["Ince", "Turker", ""], ["Devecioglu", "Ozer Can", ""], ["Chowdhury", "Muhammad E. H.", ""], ["Khandakar", "Amith", ""], ["Tahir", "Anas", ""], ["Gabbouj", "Moncef", ""]]}, {"id": "2101.01667", "submitter": "Zhiyuan Chen Dr", "authors": "Le Dinh Van Khoa and Zhiyuan Chen", "title": "Incremental learning with online SVMs on LiDAR sensory data", "comments": "This paper has been published at the International Conference on\n  Digital Image and Signal Processing (DISP 2019)At: Oxford, United Kingdom", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SP cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The pipelines transmission system is one of the growing aspects, which has\nexisted for a long time in the energy industry. The cost of in-pipe exploration\nfor maintaining service always draws lots of attention in this industry.\nNormally exploration methods (e.g. Magnetic flux leakage and eddy current) will\nestablish the sensors stationary for each pipe milestone or carry sensors to\ntravel inside the pipe. It makes the maintenance process very difficult due to\nthe massive amount of sensors. One of the solutions is to implement machine\nlearning techniques for the analysis of sensory data. Although SVMs can resolve\nthis issue with kernel trick, the problem is that computing the kernel depends\non the data size too. It is because the process can be exaggerated quickly if\nthe number of support vectors becomes really large. Particularly LiDAR spins\nwith an extremely rapid rate and the flow of input data might eventually lead\nto massive expansion. In our proposed approach, each sample is learned in an\ninstant way and the supported kernel is computed simultaneously. In this\nresearch, incremental learning approach with online support vector machines\n(SVMs) is presented, which aims to deal with LiDAR sensory data only.\n", "versions": [{"version": "v1", "created": "Wed, 30 Dec 2020 03:54:35 GMT"}], "update_date": "2021-01-06", "authors_parsed": [["Van Khoa", "Le Dinh", ""], ["Chen", "Zhiyuan", ""]]}, {"id": "2101.01668", "submitter": "Guanxiong Shen", "authors": "Guanxiong Shen, Junqing Zhang, Alan Marshall, Linning Peng, and\n  Xianbin Wang", "title": "Radio Frequency Fingerprint Identification for LoRa Using Spectrogram\n  and CNN", "comments": "Accepted for publication in IEEE INFOCOM 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SP cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Radio frequency fingerprint identification (RFFI) is an emerging device\nauthentication technique that relies on intrinsic hardware characteristics of\nwireless devices. We designed an RFFI scheme for Long Range (LoRa) systems\nbased on spectrogram and convolutional neural network (CNN). Specifically, we\nused spectrogram to represent the fine-grained time-frequency characteristics\nof LoRa signals. In addition, we revealed that the instantaneous carrier\nfrequency offset (CFO) is drifting, which will result in misclassification and\nsignificantly compromise the system stability; we demonstrated CFO compensation\nis an effective mitigation. Finally, we designed a hybrid classifier that can\nadjust CNN outputs with the estimated CFO. The mean value of CFO remains\nrelatively stable, hence it can be used to rule out CNN predictions whose\nestimated CFO falls out of the range. We performed experiments in real wireless\nenvironments using 20 LoRa devices under test (DUTs) and a Universal Software\nRadio Peripheral (USRP) N210 receiver. By comparing with the IQ-based and\nFFT-based RFFI schemes, our spectrogram-based scheme can reach the best\nclassification accuracy, i.e., 97.61% for 20 LoRa DUTs.\n", "versions": [{"version": "v1", "created": "Wed, 30 Dec 2020 17:17:47 GMT"}], "update_date": "2021-01-06", "authors_parsed": [["Shen", "Guanxiong", ""], ["Zhang", "Junqing", ""], ["Marshall", "Alan", ""], ["Peng", "Linning", ""], ["Wang", "Xianbin", ""]]}, {"id": "2101.01669", "submitter": "Zhizhi Yu", "authors": "Di Jin, Zhizhi Yu, Pengfei Jiao, Shirui Pan, Philip S. Yu, Weixiong\n  Zhang", "title": "A Survey of Community Detection Approaches: From Statistical Modeling to\n  Deep Representation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.AI cs.LG physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Community detection, a fundamental task for network analysis, aims to\npartition a network into multiple sub-structures to help reveal their latent\nfunctions. Community detection has been extensively studied in and broadly\napplied to many real-world network problems. Classical approaches to community\ndetection typically utilize probabilistic graphical models and adopt a variety\nof prior knowledge to infer community structures. As the problems that network\nmethods try to solve and the network data to be analyzed become increasingly\nmore sophisticated, new approaches have also been proposed and developed,\nparticularly those that utilize deep learning and convert networked data into\nlow dimensional representation. Despite all the recent advancement, there is\nstill a lack of insightful understanding of the theoretical and methodological\nunderpinning of community detection, which will be critically important for\nfuture development of the area of network analysis. In this paper, we develop\nand present a unified architecture of network community-finding methods to\ncharacterize the state-of-the-art of the field of community detection.\nSpecifically, we provide a comprehensive review of the existing community\ndetection methods and introduce a new taxonomy that divides the existing\nmethods into two categories, namely probabilistic graphical model and deep\nlearning. We then discuss in detail the main idea behind each method in the two\ncategories. Furthermore, to promote future development of community detection,\nwe release several benchmark datasets from several problem domains and\nhighlight their applications to various network analysis tasks. We conclude\nwith discussions of the challenges of the field and suggestions of possible\ndirections for future research.\n", "versions": [{"version": "v1", "created": "Sun, 3 Jan 2021 02:32:45 GMT"}, {"version": "v2", "created": "Wed, 26 May 2021 06:58:54 GMT"}], "update_date": "2021-05-27", "authors_parsed": [["Jin", "Di", ""], ["Yu", "Zhizhi", ""], ["Jiao", "Pengfei", ""], ["Pan", "Shirui", ""], ["Yu", "Philip S.", ""], ["Zhang", "Weixiong", ""]]}, {"id": "2101.01673", "submitter": "Avijit Ghosh", "authors": "Avijit Ghosh, Lea Genuit, Mary Reagan", "title": "Characterizing Intersectional Group Fairness with Worst-Case Comparisons", "comments": "Accepted to the 2nd Affinity Group Workshop on Diversity in\n  Artificial Intelligence: Diversity, Belonging, Equity, and Inclusion (AIDBEI)\n  at AAAI 2021 - camera ready version", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Machine Learning or Artificial Intelligence algorithms have gained\nconsiderable scrutiny in recent times owing to their propensity towards\nimitating and amplifying existing prejudices in society. This has led to a\nniche but growing body of work that identifies and attempts to fix these\nbiases. A first step towards making these algorithms more fair is designing\nmetrics that measure unfairness. Most existing work in this field deals with\neither a binary view of fairness (protected vs. unprotected groups) or\npolitically defined categories (race or gender). Such categorization misses the\nimportant nuance of intersectionality - biases can often be amplified in\nsubgroups that combine membership from different categories, especially if such\na subgroup is particularly underrepresented in historical platforms of\nopportunity.\n  In this paper, we discuss why fairness metrics need to be looked at under the\nlens of intersectionality, identify existing work in intersectional fairness,\nsuggest a simple worst case comparison method to expand the definitions of\nexisting group fairness metrics to incorporate intersectionality, and finally\nconclude with the social, legal and political framework to handle\nintersectional fairness in the modern context.\n", "versions": [{"version": "v1", "created": "Tue, 5 Jan 2021 17:44:33 GMT"}, {"version": "v2", "created": "Mon, 18 Jan 2021 17:53:13 GMT"}, {"version": "v3", "created": "Mon, 1 Feb 2021 03:10:01 GMT"}], "update_date": "2021-02-02", "authors_parsed": [["Ghosh", "Avijit", ""], ["Genuit", "Lea", ""], ["Reagan", "Mary", ""]]}, {"id": "2101.01689", "submitter": "Hongda Shen", "authors": "Hongda Shen, Eren Kursun", "title": "Label Augmentation via Time-based Knowledge Distillation for Financial\n  Anomaly Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Detecting anomalies has become increasingly critical to the financial service\nindustry. Anomalous events are often indicative of illegal activities such as\nfraud, identity theft, network intrusion, account takeover, and money\nlaundering. Financial anomaly detection use cases face serious challenges due\nto the dynamic nature of the underlying patterns especially in adversarial\nenvironments such as constantly changing fraud tactics. While retraining the\nmodels with the new patterns is absolutely essential; keeping up with the rapid\nchanges introduces other challenges as it moves the model away from older\npatterns or continuously grows the size of the training data. The resulting\ndata growth is hard to manage and it reduces the agility of the models'\nresponse to the latest attacks. Due to the data size limitations and the need\nto track the latest patterns, older time periods are often dropped in practice,\nwhich in turn, causes vulnerabilities. In this study, we propose a label\naugmentation approach to utilize the learning from older models to boost the\nlatest. Experimental results show that the proposed approach provides a\nsignificant reduction in training time, while providing potential performance\nimprovement.\n", "versions": [{"version": "v1", "created": "Tue, 5 Jan 2021 18:24:13 GMT"}], "update_date": "2021-01-06", "authors_parsed": [["Shen", "Hongda", ""], ["Kursun", "Eren", ""]]}, {"id": "2101.01697", "submitter": "Devendra Swami", "authors": "Devendra Swami, Yash Phogat, Aadiraj Batlaw, Ashwin Goyal", "title": "Analyzing movies to predict their commercial viability for producers", "comments": "6 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Upon film premiere, a major form of speculation concerns the relative success\nof the film. This relativity is in particular regards to the film's original\nbudget, as many a time have big-budget blockbusters been met with exceptional\nsuccess as met with abject failure. So how does one predict the success of an\nupcoming film? In this paper, we explored a vast array of film data in an\nattempt to develop a model that could predict the expected return of an\nupcoming film. The approach to this development is as follows: First, we began\nwith the MovieLens dataset having common movie attributes along with genome\ntags per each film. Genome tags give insight into what particular\ncharacteristics of the film are most salient. We then included additional\nfeatures regarding film content, cast/crew, audience perception, budget, and\nearnings from TMDB, IMDB, and Metacritic websites. Next, we performed\nexploratory data analysis and engineered a wide range of new features capturing\nhistorical information for the available features. Thereafter, we used singular\nvalue decomposition (SVD) for dimensionality reduction of the high dimensional\nfeatures (ex. genome tags). Finally, we built a Random Forest Classifier and\nperformed hyper-parameter tuning to optimize for model accuracy. A future\napplication of our model could be seen in the film industry, allowing\nproduction companies to better predict the expected return of their projects\nbased on their envisioned outline for their production procedure, thereby\nallowing them to revise their plan in an attempt to achieve optimal returns.\n", "versions": [{"version": "v1", "created": "Tue, 5 Jan 2021 18:42:38 GMT"}], "update_date": "2021-01-06", "authors_parsed": [["Swami", "Devendra", ""], ["Phogat", "Yash", ""], ["Batlaw", "Aadiraj", ""], ["Goyal", "Ashwin", ""]]}, {"id": "2101.01708", "submitter": "Yulong Lu", "authors": "Jianfeng Lu, Yulong Lu, Min Wang", "title": "A Priori Generalization Analysis of the Deep Ritz Method for Solving\n  High Dimensional Elliptic Equations", "comments": "Revised the definition of Barron space and updated the proofs induced\n  by the changes", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.NA cs.LG cs.NA math.AP math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper concerns the a priori generalization analysis of the Deep Ritz\nMethod (DRM) [W. E and B. Yu, 2017], a popular neural-network-based method for\nsolving high dimensional partial differential equations. We derive the\ngeneralization error bounds of two-layer neural networks in the framework of\nthe DRM for solving two prototype elliptic PDEs: Poisson equation and static\nSchr\\\"odinger equation on the $d$-dimensional unit hypercube. Specifically, we\nprove that the convergence rates of generalization errors are independent of\nthe dimension $d$, under the a priori assumption that the exact solutions of\nthe PDEs lie in a suitable low-complexity space called spectral Barron space.\nMoreover, we give sufficient conditions on the forcing term and the potential\nfunction which guarantee that the solutions are spectral Barron functions. We\nachieve this by developing a new solution theory for the PDEs on the spectral\nBarron space, which can be viewed as an analog of the classical Sobolev\nregularity theory for PDEs.\n", "versions": [{"version": "v1", "created": "Tue, 5 Jan 2021 18:50:59 GMT"}, {"version": "v2", "created": "Mon, 22 Mar 2021 14:58:28 GMT"}], "update_date": "2021-03-23", "authors_parsed": [["Lu", "Jianfeng", ""], ["Lu", "Yulong", ""], ["Wang", "Min", ""]]}, {"id": "2101.01739", "submitter": "Aaron Roth", "authors": "Varun Gupta, Christopher Jung, Georgy Noarov, Mallesh M. Pai, Aaron\n  Roth", "title": "Online Multivalid Learning: Means, Moments, and Prediction Intervals", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS cs.GT econ.EM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a general, efficient technique for providing contextual\npredictions that are \"multivalid\" in various senses, against an online sequence\nof adversarially chosen examples $(x,y)$. This means that the resulting\nestimates correctly predict various statistics of the labels $y$ not just\nmarginally -- as averaged over the sequence of examples -- but also\nconditionally on $x \\in G$ for any $G$ belonging to an arbitrary intersecting\ncollection of groups $\\mathcal{G}$.\n  We provide three instantiations of this framework. The first is mean\nprediction, which corresponds to an online algorithm satisfying the notion of\nmulticalibration from Hebert-Johnson et al. The second is variance and higher\nmoment prediction, which corresponds to an online algorithm satisfying the\nnotion of mean-conditioned moment multicalibration from Jung et al. Finally, we\ndefine a new notion of prediction interval multivalidity, and give an algorithm\nfor finding prediction intervals which satisfy it. Because our algorithms\nhandle adversarially chosen examples, they can equally well be used to predict\nstatistics of the residuals of arbitrary point prediction methods, giving rise\nto very general techniques for quantifying the uncertainty of predictions of\nblack box algorithms, even in an online adversarial setting. When instantiated\nfor prediction intervals, this solves a similar problem as conformal\nprediction, but in an adversarial environment and with multivalidity guarantees\nstronger than simple marginal coverage guarantees.\n", "versions": [{"version": "v1", "created": "Tue, 5 Jan 2021 19:08:11 GMT"}], "update_date": "2021-01-07", "authors_parsed": [["Gupta", "Varun", ""], ["Jung", "Christopher", ""], ["Noarov", "Georgy", ""], ["Pai", "Mallesh M.", ""], ["Roth", "Aaron", ""]]}, {"id": "2101.01761", "submitter": "Hieu Pham", "authors": "Hieu Pham, Quoc V. Le", "title": "AutoDropout: Learning Dropout Patterns to Regularize Deep Networks", "comments": "Accepted to AAAI 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CL cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Neural networks are often over-parameterized and hence benefit from\naggressive regularization. Conventional regularization methods, such as Dropout\nor weight decay, do not leverage the structures of the network's inputs and\nhidden states. As a result, these conventional methods are less effective than\nmethods that leverage the structures, such as SpatialDropout and DropBlock,\nwhich randomly drop the values at certain contiguous areas in the hidden states\nand setting them to zero. Although the locations of dropout areas random, the\npatterns of SpatialDropout and DropBlock are manually designed and fixed. Here\nwe propose to learn the dropout patterns. In our method, a controller learns to\ngenerate a dropout pattern at every channel and layer of a target network, such\nas a ConvNet or a Transformer. The target network is then trained with the\ndropout pattern, and its resulting validation performance is used as a signal\nfor the controller to learn from. We show that this method works well for both\nimage recognition on CIFAR-10 and ImageNet, as well as language modeling on\nPenn Treebank and WikiText-2. The learned dropout patterns also transfers to\ndifferent tasks and datasets, such as from language model on Penn Treebank to\nEngligh-French translation on WMT 2014. Our code will be available.\n", "versions": [{"version": "v1", "created": "Tue, 5 Jan 2021 19:54:22 GMT"}], "update_date": "2021-01-07", "authors_parsed": [["Pham", "Hieu", ""], ["Le", "Quoc V.", ""]]}, {"id": "2101.01765", "submitter": "Cemre Zor", "authors": "Cemre Zor and Terry Windeatt", "title": "A unifying approach on bias and variance analysis for classification", "comments": "17 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Standard bias and variance (B&V) terminologies were originally defined for\nthe regression setting and their extensions to classification have led to\nseveral different models / definitions in the literature. In this paper, we aim\nto provide the link between the commonly used frameworks of Tumer & Ghosh (T&G)\nand James. By unifying the two approaches, we relate the B&V defined for the\n0/1 loss to the standard B&V of the boundary distributions given for the\nsquared error loss. The closed form relationships provide a deeper\nunderstanding of classification performance, and their use is demonstrated in\ntwo case studies.\n", "versions": [{"version": "v1", "created": "Tue, 5 Jan 2021 20:00:12 GMT"}, {"version": "v2", "created": "Tue, 12 Jan 2021 19:28:01 GMT"}], "update_date": "2021-01-14", "authors_parsed": [["Zor", "Cemre", ""], ["Windeatt", "Terry", ""]]}, {"id": "2101.01774", "submitter": "Shubham Shrivastava", "authors": "Kaushik Balakrishnan, Punarjay Chakravarty, Shubham Shrivastava", "title": "An A* Curriculum Approach to Reinforcement Learning for RGBD Indoor\n  Robot Navigation", "comments": "8 pages, 6 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Training robots to navigate diverse environments is a challenging problem as\nit involves the confluence of several different perception tasks such as\nmapping and localization, followed by optimal path-planning and control.\nRecently released photo-realistic simulators such as Habitat allow for the\ntraining of networks that output control actions directly from perception:\nagents use Deep Reinforcement Learning (DRL) to regress directly from the\ncamera image to a control output in an end-to-end fashion. This is\ndata-inefficient and can take several days to train on a GPU. Our paper tries\nto overcome this problem by separating the training of the perception and\ncontrol neural nets and increasing the path complexity gradually using a\ncurriculum approach. Specifically, a pre-trained twin Variational AutoEncoder\n(VAE) is used to compress RGBD (RGB & depth) sensing from an environment into a\nlatent embedding, which is then used to train a DRL-based control policy. A*, a\ntraditional path-planner is used as a guide for the policy and the distance\nbetween start and target locations is incrementally increased along the A*\nroute, as training progresses. We demonstrate the efficacy of the proposed\napproach, both in terms of increased performance and decreased training times\nfor the PointNav task in the Habitat simulation environment. This strategy of\nimproving the training of direct-perception based DRL navigation policies is\nexpected to hasten the deployment of robots of particular interest to industry\nsuch as co-bots on the factory floor and last-mile delivery robots.\n", "versions": [{"version": "v1", "created": "Tue, 5 Jan 2021 20:35:14 GMT"}], "update_date": "2021-01-07", "authors_parsed": [["Balakrishnan", "Kaushik", ""], ["Chakravarty", "Punarjay", ""], ["Shrivastava", "Shubham", ""]]}, {"id": "2101.01781", "submitter": "Kuo Yang", "authors": "Kuo Yang, Emad A. Mohammed", "title": "A Review of Artificial Intelligence Technologies for Early Prediction of\n  Alzheimer's Disease", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Alzheimer's Disease (AD) is a severe brain disorder, destroying memories and\nbrain functions. AD causes chronically, progressively, and irreversibly\ncognitive declination and brain damages. The reliable and effective evaluation\nof early dementia has become essential research with medical imaging\ntechnologies and computer-aided algorithms. This trend has moved to modern\nArtificial Intelligence (AI) technologies motivated by deeplearning success in\nimage classification and natural language processing. The purpose of this\nreview is to provide an overview of the latest research involving deep-learning\nalgorithms in evaluating the process of dementia, diagnosing the early stage of\nAD, and discussing an outlook for this research. This review introduces various\napplications of modern AI algorithms in AD diagnosis, including Convolutional\nNeural Network (CNN), Recurrent Neural Network (RNN), Automatic Image\nSegmentation, Autoencoder, Graph CNN (GCN), Ensemble Learning, and Transfer\nLearning. The advantages and disadvantages of the proposed methods and their\nperformance are discussed. The conclusion section summarizes the primary\ncontributions and medical imaging preprocessing techniques applied in the\nreviewed research. Finally, we discuss the limitations and future outlooks.\n", "versions": [{"version": "v1", "created": "Tue, 22 Dec 2020 01:05:34 GMT"}], "update_date": "2021-01-07", "authors_parsed": [["Yang", "Kuo", ""], ["Mohammed", "Emad A.", ""]]}, {"id": "2101.01787", "submitter": "Christopher Griffin", "authors": "Nishanth Nakshatri and Arjun Menon and C. Lee Giles and Sarah\n  Rajtmajer and Christopher Griffin", "title": "Design and Analysis of a Synthetic Prediction Market using Dynamic\n  Convex Sets", "comments": "17 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CE cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a synthetic prediction market whose agent purchase logic is\ndefined using a sigmoid transformation of a convex semi-algebraic set defined\nin feature space. Asset prices are determined by a logarithmic scoring market\nrule. Time varying asset prices affect the structure of the semi-algebraic sets\nleading to time-varying agent purchase rules. We show that under certain\nassumptions on the underlying geometry, the resulting synthetic prediction\nmarket can be used to arbitrarily closely approximate a binary function defined\non a set of input data. We also provide sufficient conditions for market\nconvergence and show that under certain instances markets can exhibit limit\ncycles in asset spot price. We provide an evolutionary algorithm for training\nagent parameters to allow a market to model the distribution of a given data\nset and illustrate the market approximation using two open source data sets.\nResults are compared to standard machine learning methods.\n", "versions": [{"version": "v1", "created": "Tue, 5 Jan 2021 21:11:13 GMT"}], "update_date": "2021-01-07", "authors_parsed": [["Nakshatri", "Nishanth", ""], ["Menon", "Arjun", ""], ["Giles", "C. Lee", ""], ["Rajtmajer", "Sarah", ""], ["Griffin", "Christopher", ""]]}, {"id": "2101.01792", "submitter": "Kilian Fatras", "authors": "Kilian Fatras, Younes Zine, Szymon Majewski, R\\'emi Flamary, R\\'emi\n  Gribonval, Nicolas Courty", "title": "Minibatch optimal transport distances; analysis and applications", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Optimal transport distances have become a classic tool to compare probability\ndistributions and have found many applications in machine learning. Yet,\ndespite recent algorithmic developments, their complexity prevents their direct\nuse on large scale datasets. To overcome this challenge, a common workaround is\nto compute these distances on minibatches i.e. to average the outcome of\nseveral smaller optimal transport problems. We propose in this paper an\nextended analysis of this practice, which effects were previously studied in\nrestricted cases. We first consider a large variety of Optimal Transport\nkernels. We notably argue that the minibatch strategy comes with appealing\nproperties such as unbiased estimators, gradients and a concentration bound\naround the expectation, but also with limits: the minibatch OT is not a\ndistance. To recover some of the lost distance axioms, we introduce a debiased\nminibatch OT function and study its statistical and optimisation properties.\nAlong with this theoretical analysis, we also conduct empirical experiments on\ngradient flows, generative adversarial networks (GANs) or color transfer that\nhighlight the practical interest of this strategy.\n", "versions": [{"version": "v1", "created": "Tue, 5 Jan 2021 21:29:31 GMT"}], "update_date": "2021-01-07", "authors_parsed": [["Fatras", "Kilian", ""], ["Zine", "Younes", ""], ["Majewski", "Szymon", ""], ["Flamary", "R\u00e9mi", ""], ["Gribonval", "R\u00e9mi", ""], ["Courty", "Nicolas", ""]]}, {"id": "2101.01815", "submitter": "Michael Everett", "authors": "Michael Everett, Golnaz Habibi, Jonathan P. How", "title": "Efficient Reachability Analysis of Closed-Loop Systems with Neural\n  Network Controllers", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SY cs.LG cs.RO cs.SY", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Neural Networks (NNs) can provide major empirical performance improvements\nfor robotic systems, but they also introduce challenges in formally analyzing\nthose systems' safety properties. In particular, this work focuses on\nestimating the forward reachable set of closed-loop systems with NN\ncontrollers. Recent work provides bounds on these reachable sets, yet the\ncomputationally efficient approaches provide overly conservative bounds (thus\ncannot be used to verify useful properties), whereas tighter methods are too\nintensive for online computation. This work bridges the gap by formulating a\nconvex optimization problem for reachability analysis for closed-loop systems\nwith NN controllers. While the solutions are less tight than prior semidefinite\nprogram-based methods, they are substantially faster to compute, and some of\nthe available computation time can be used to refine the bounds through input\nset partitioning, which more than overcomes the tightness gap. The proposed\nframework further considers systems with measurement and process noise, thus\nbeing applicable to realistic systems with uncertainty. Finally, numerical\ncomparisons show $10\\times$ reduction in conservatism in $\\frac{1}{2}$ of the\ncomputation time compared to the state-of-the-art, and the ability to handle\nvarious sources of uncertainty is highlighted on a quadrotor model.\n", "versions": [{"version": "v1", "created": "Tue, 5 Jan 2021 22:30:39 GMT"}, {"version": "v2", "created": "Mon, 24 May 2021 18:23:46 GMT"}], "update_date": "2021-05-26", "authors_parsed": [["Everett", "Michael", ""], ["Habibi", "Golnaz", ""], ["How", "Jonathan P.", ""]]}, {"id": "2101.01832", "submitter": "M. Ali Vosoughi", "authors": "M. Ali Vosoughi and Axel Wismuller", "title": "Large-Scale Extended Granger Causality for Classification of Marijuana\n  Users From Functional MRI", "comments": "13 pages, conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CE eess.IV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  It has been shown in the literature that marijuana use is associated with\nchanges in brain network connectivity. We propose large-scale Extended Granger\nCausality (lsXGC) and investigate whether it can capture such changes using\nresting-state fMRI. This method combines dimension reduction with source\ntime-series augmentation and uses predictive time-series modeling for\nestimating directed causal relationships among fMRI time-series. It is a\nmultivariate approach, since it is capable of identifying the interdependence\nof time-series in the presence of all other time-series of the underlying\ndynamic system. Here, we investigate whether this model can serve as a\nbiomarker for classifying marijuana users from typical controls using 126 adult\nsubjects with a childhood diagnosis of ADHD from the Addiction Connectome\nPreprocessed Initiative (ACPI) database. We use brain connections estimated by\nlsXGC as features for classification. After feature extraction, we perform\nfeature selection by Kendall's-tau rank correlation coefficient followed by\nclassification using a support vector machine. As a reference method, we\ncompare our results with cross-correlation, which is typically used in the\nliterature as a standard measure of functional connectivity. Within a\ncross-validation scheme of 100 different training/test (90%/10%) data splits,\nwe obtain a mean accuracy range of [0.714, 0.985] and a mean Area Under the\nreceiver operating characteristic Curve (AUC) range of [0.779, 0.999] across\nall tested numbers of features for lsXGC, which is significantly better than\nresults obtained with cross-correlation, namely mean accuracy of [0.728, 0.912]\nand mean AUC of [0.825, 0.969]. Our results suggest the applicability of lsXGC\nas a potential biomarker for marijuana use.\n", "versions": [{"version": "v1", "created": "Wed, 6 Jan 2021 00:40:47 GMT"}], "update_date": "2021-01-07", "authors_parsed": [["Vosoughi", "M. Ali", ""], ["Wismuller", "Axel", ""]]}, {"id": "2101.01835", "submitter": "Blanca Vazquez", "authors": "Blanca Vazquez, Gibran Fuentes, Fabian Garcia, Gabriela Borrayo, Juan\n  Prohias", "title": "Risk markers by sex and age group for in-hospital mortality in patients\n  with STEMI or NSTEMI: an approach based on machine learning", "comments": "20 pages, 6 figures, submitted to BMC Medical Informatics and\n  Decision Making", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Machine learning (ML) has demonstrated promising results in the\nidentification of clinical markers for Acute Coronary Syndrome (ACS) from\nelectronic health records (EHR). In the past, the ACS was perceived as a health\nproblem mainly for men and women were under-represented in clinical trials,\nwhich led to both sexes receiving the same clinical attention. Although some\napproaches have emphasized the importance of distinguishing markers, these\ndistinctions remain unclear. This study aims at exploiting ML methods for\nidentifying in-hospital mortality markers by sex and age-group for patients\nwith ST-elevation myocardial infarction (STEMI) and the Non-ST-elevation\nmyocardial infarction (NSTEMI) from EHR. From the MIMIC-III database, we\nextracted 1,299 patients with STEMI and 2,820 patients with NSTEMI. We trained\nand validated mortality prediction models with different hyperparameters,\nclinical sets, and ML methods. Using the best performing model and a\ngame-theoretic approach to interpret predictions, we identified risk markers\nfor patients with STEMI and NSTEMI separately. The models based on Extreme\nGradient Boosting achieved the highest performance: AUC=0.92 (95\\%\nCI:0.87-0.98) for STEMI and AUC=0.87 (95\\% CI:0.80-0.93) for NSTEMI. For STEMI,\nthe top markers for both sexes are the presence of hyponatremia, and metabolic\nacidosis. More specific markers for women are acute kidney failure, and age>75\nyears, while for men are chronic kidney failure, and age>70 years. In contrast,\nfor NSTEMI, the top markers for both sexes are advanced age, and intubation\nprocedures. The specific markers for women are low creatinine levels and age>60\nyears, whilst, for men are damage to the left atrium and age>70 years. We\nconsider that distinguishing markers for sexes could lead to more appropriate\ntreatment strategies, thus improving clinical outcomes.\n", "versions": [{"version": "v1", "created": "Wed, 6 Jan 2021 00:56:54 GMT"}], "update_date": "2021-01-07", "authors_parsed": [["Vazquez", "Blanca", ""], ["Fuentes", "Gibran", ""], ["Garcia", "Fabian", ""], ["Borrayo", "Gabriela", ""], ["Prohias", "Juan", ""]]}, {"id": "2101.01844", "submitter": "Qiaojun Feng", "authors": "Qiaojun Feng, Nikolay Atanasov", "title": "Mesh Reconstruction from Aerial Images for Outdoor Terrain Mapping Using\n  Joint 2D-3D Learning", "comments": "7 pages, 7 figures. Accepted at ICRA 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper addresses outdoor terrain mapping using overhead images obtained\nfrom an unmanned aerial vehicle. Dense depth estimation from aerial images\nduring flight is challenging. While feature-based localization and mapping\ntechniques can deliver real-time odometry and sparse points reconstruction, a\ndense environment model is generally recovered offline with significant\ncomputation and storage. This paper develops a joint 2D-3D learning approach to\nreconstruct local meshes at each camera keyframe, which can be assembled into a\nglobal environment model. Each local mesh is initialized from sparse depth\nmeasurements. We associate image features with the mesh vertices through camera\nprojection and apply graph convolution to refine the mesh vertices based on\njoint 2-D reprojected depth and 3-D mesh supervision. Quantitative and\nqualitative evaluations using real aerial images show the potential of our\nmethod to support environmental monitoring and surveillance applications.\n", "versions": [{"version": "v1", "created": "Wed, 6 Jan 2021 02:09:03 GMT"}, {"version": "v2", "created": "Tue, 13 Apr 2021 20:45:33 GMT"}], "update_date": "2021-04-15", "authors_parsed": [["Feng", "Qiaojun", ""], ["Atanasov", "Nikolay", ""]]}, {"id": "2101.01847", "submitter": "Tugba Erpek", "authors": "Tarun S. Cousik, Vijay K. Shah, Tugba Erpek, Yalin E. Sagduyu, Jeffrey\n  H. Reed", "title": "Deep Learning for Fast and Reliable Initial Access in AI-Driven 6G\n  mmWave Networks", "comments": "arXiv admin note: substantial text overlap with arXiv:2006.12653", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI cs.LG eess.SP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present DeepIA, a deep neural network (DNN) framework for enabling fast\nand reliable initial access for AI-driven beyond 5G and 6G millimeter (mmWave)\nnetworks. DeepIA reduces the beam sweep time compared to a conventional\nexhaustive search-based IA process by utilizing only a subset of the available\nbeams. DeepIA maps received signal strengths (RSSs) obtained from a subset of\nbeams to the beam that is best oriented to the receiver. In both line of sight\n(LoS) and non-line of sight (NLoS) conditions, DeepIA reduces the IA time and\noutperforms the conventional IA's beam prediction accuracy. We show that the\nbeam prediction accuracy of DeepIA saturates with the number of beams used for\nIA and depends on the particular selection of the beams. In LoS conditions, the\nselection of the beams is consequential and improves the accuracy by up to 70%.\nIn NLoS situations, it improves accuracy by up to 35%. We find that, averaging\nmultiple RSS snapshots further reduces the number of beams needed and achieves\nmore than 95% accuracy in both LoS and NLoS conditions. Finally, we evaluate\nthe beam prediction time of DeepIA through embedded hardware implementation and\nshow the improvement over the conventional beam sweeping.\n", "versions": [{"version": "v1", "created": "Wed, 6 Jan 2021 02:59:49 GMT"}], "update_date": "2021-01-07", "authors_parsed": [["Cousik", "Tarun S.", ""], ["Shah", "Vijay K.", ""], ["Erpek", "Tugba", ""], ["Sagduyu", "Yalin E.", ""], ["Reed", "Jeffrey H.", ""]]}, {"id": "2101.01849", "submitter": "Hao Yuan", "authors": "Hao Yuan, Shuiwang Ji", "title": "Node2Seq: Towards Trainable Convolutions in Graph Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Investigating graph feature learning becomes essentially important with the\nemergence of graph data in many real-world applications. Several graph neural\nnetwork approaches are proposed for node feature learning and they generally\nfollow a neighboring information aggregation scheme to learn node features.\nWhile great performance has been achieved, the weights learning for different\nneighboring nodes is still less explored. In this work, we propose a novel\ngraph network layer, known as Node2Seq, to learn node embeddings with\nexplicitly trainable weights for different neighboring nodes. For a target\nnode, our method sorts its neighboring nodes via attention mechanism and then\nemploys 1D convolutional neural networks (CNNs) to enable explicit weights for\ninformation aggregation. In addition, we propose to incorporate non-local\ninformation for feature learning in an adaptive manner based on the attention\nscores. Experimental results demonstrate the effectiveness of our proposed\nNode2Seq layer and show that the proposed adaptively non-local information\nlearning can improve the performance of feature learning.\n", "versions": [{"version": "v1", "created": "Wed, 6 Jan 2021 03:05:37 GMT"}], "update_date": "2021-01-07", "authors_parsed": [["Yuan", "Hao", ""], ["Ji", "Shuiwang", ""]]}, {"id": "2101.01857", "submitter": "Wenling Shang", "authors": "Wenling Shang, Xiaofei Wang, Aravind Srinivas, Aravind Rajeswaran,\n  Yang Gao, Pieter Abbeel, Michael Laskin", "title": "Reinforcement Learning with Latent Flow", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Temporal information is essential to learning effective policies with\nReinforcement Learning (RL). However, current state-of-the-art RL algorithms\neither assume that such information is given as part of the state space or,\nwhen learning from pixels, use the simple heuristic of frame-stacking to\nimplicitly capture temporal information present in the image observations. This\nheuristic is in contrast to the current paradigm in video classification\narchitectures, which utilize explicit encodings of temporal information through\nmethods such as optical flow and two-stream architectures to achieve\nstate-of-the-art performance. Inspired by leading video classification\narchitectures, we introduce the Flow of Latents for Reinforcement Learning\n(Flare), a network architecture for RL that explicitly encodes temporal\ninformation through latent vector differences. We show that Flare (i) recovers\noptimal performance in state-based RL without explicit access to the state\nvelocity, solely with positional state information, (ii) achieves\nstate-of-the-art performance on pixel-based challenging continuous control\ntasks within the DeepMind control benchmark suite, namely quadruped walk,\nhopper hop, finger turn hard, pendulum swing, and walker run, and is the most\nsample efficient model-free pixel-based RL algorithm, outperforming the prior\nmodel-free state-of-the-art by 1.9X and 1.5X on the 500k and 1M step\nbenchmarks, respectively, and (iv), when augmented over rainbow DQN,\noutperforms this state-of-the-art level baseline on 5 of 8 challenging Atari\ngames at 100M time step benchmark.\n", "versions": [{"version": "v1", "created": "Wed, 6 Jan 2021 03:50:50 GMT"}], "update_date": "2021-01-07", "authors_parsed": [["Shang", "Wenling", ""], ["Wang", "Xiaofei", ""], ["Srinivas", "Aravind", ""], ["Rajeswaran", "Aravind", ""], ["Gao", "Yang", ""], ["Abbeel", "Pieter", ""], ["Laskin", "Michael", ""]]}, {"id": "2101.01861", "submitter": "JIe Zhang", "authors": "Jie Zhang", "title": "TGCN: Time Domain Graph Convolutional Network for Multiple Objects\n  Tracking", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multiple object tracking is to give each object an id in the video. The\ndifficulty is how to match the predicted objects and detected objects in same\nframes. Matching features include appearance features, location features, etc.\nThese features of the predicted object are basically based on some previous\nframes. However, few papers describe the relationship in the time domain\nbetween the previous frame features and the current frame features.In this\npaper, we proposed a time domain graph convolutional network for multiple\nobjects tracking.The model is mainly divided into two parts, we first use\nconvolutional neural network (CNN) to extract pedestrian appearance feature,\nwhich is a normal operation processing image in deep learning, then we use GCN\nto model some past frames' appearance feature to get the prediction appearance\nfeature of the current frame. Due to this extension, we can get the pose\nfeatures of the current frame according to the relationship between some frames\nin the past. Experimental evaluation shows that our extensions improve the MOTA\nby 1.3 on the MOT16, achieving overall competitive performance at high frame\nrates.\n", "versions": [{"version": "v1", "created": "Wed, 6 Jan 2021 04:11:25 GMT"}], "update_date": "2021-01-07", "authors_parsed": [["Zhang", "Jie", ""]]}, {"id": "2101.01863", "submitter": "Jae-Wook Ahn", "authors": "Chunheng Jiang, Jae-wook Ahn, Nirmit Desai", "title": "Environment Transfer for Distributed Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.LG eess.AS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Collecting sufficient amount of data that can represent various acoustic\nenvironmental attributes is a critical problem for distributed acoustic machine\nlearning. Several audio data augmentation techniques have been introduced to\naddress this problem but they tend to remain in simple manipulation of existing\ndata and are insufficient to cover the variability of the environments. We\npropose a method to extend a technique that has been used for transferring\nacoustic style textures between audio data. The method transfers audio\nsignatures between environments for distributed acoustic data augmentation.\nThis paper devises metrics to evaluate the generated acoustic data, based on\nclassification accuracy and content preservation. A series of experiments were\nconducted using UrbanSound8K dataset and the results show that the proposed\nmethod generates better audio data with transferred environmental features\nwhile preserving content features.\n", "versions": [{"version": "v1", "created": "Wed, 6 Jan 2021 04:27:24 GMT"}], "update_date": "2021-01-07", "authors_parsed": [["Jiang", "Chunheng", ""], ["Ahn", "Jae-wook", ""], ["Desai", "Nirmit", ""]]}, {"id": "2101.01864", "submitter": "Aaron Tuor", "authors": "Elliott Skomski, Soumya Vasisht, Colby Wight, Aaron Tuor, Jan Drgona,\n  Draguna Vrabie", "title": "Constrained Block Nonlinear Neural Dynamical Models", "comments": "10 pages. Submitted to American Control Conference ACC 2020. Under\n  review", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.DS cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural network modules conditioned by known priors can be effectively trained\nand combined to represent systems with nonlinear dynamics. This work explores a\nnovel formulation for data-efficient learning of deep control-oriented\nnonlinear dynamical models by embedding local model structure and constraints.\nThe proposed method consists of neural network blocks that represent input,\nstate, and output dynamics with constraints placed on the network weights and\nsystem variables. For handling partially observable dynamical systems, we\nutilize a state observer neural network to estimate the states of the system's\nlatent dynamics. We evaluate the performance of the proposed architecture and\ntraining methods on system identification tasks for three nonlinear systems: a\ncontinuous stirred tank reactor, a two tank interacting system, and an\naerodynamics body. Models optimized with a few thousand system state\nobservations accurately represent system dynamics in open loop simulation over\nthousands of time steps from a single set of initial conditions. Experimental\nresults demonstrate an order of magnitude reduction in open-loop simulation\nmean squared error for our constrained, block-structured neural models when\ncompared to traditional unstructured and unconstrained neural network models.\n", "versions": [{"version": "v1", "created": "Wed, 6 Jan 2021 04:27:54 GMT"}], "update_date": "2021-01-07", "authors_parsed": [["Skomski", "Elliott", ""], ["Vasisht", "Soumya", ""], ["Wight", "Colby", ""], ["Tuor", "Aaron", ""], ["Drgona", "Jan", ""], ["Vrabie", "Draguna", ""]]}, {"id": "2101.01866", "submitter": "Hassan Raji", "authors": "Hassan Raji, Muhammad Tayyab, Jianye Sui, Seyed Reza Mahmoodi, Mehdi\n  Javanmard", "title": "Biosensors and Machine Learning for Enhanced Detection, Stratification,\n  and Classification of Cells: A Review", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SP cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Biological cells, by definition, are the basic units which contain the\nfundamental molecules of life of which all living things are composed.\nUnderstanding how they function and differentiating cells from one another\ntherefore is of paramount importance for disease diagnostics as well as\ntherapeutics. Sensors focusing on the detection and stratification of cells\nhave gained popularity as technological advancements have allowed for the\nminiaturization of various components inching us closer to Point-of-Care (POC)\nsolutions with each passing day. Furthermore, Machine Learning has allowed for\nenhancement in analytical capabilities of these various biosensing modalities,\nespecially the challenging task of classification of cells into various\ncategories using a data-driven approach rather than physics-driven. In this\nreview, we provide an account of how Machine Learning has been applied\nexplicitly to sensors that detect and classify cells. We also provide a\ncomparison of how different sensing modalities and algorithms affect the\nclassifier accuracy and the dataset size required.\n", "versions": [{"version": "v1", "created": "Wed, 6 Jan 2021 04:32:30 GMT"}], "update_date": "2021-01-07", "authors_parsed": [["Raji", "Hassan", ""], ["Tayyab", "Muhammad", ""], ["Sui", "Jianye", ""], ["Mahmoodi", "Seyed Reza", ""], ["Javanmard", "Mehdi", ""]]}, {"id": "2101.01867", "submitter": "Neha R. Gupta", "authors": "Neha R. Gupta (1), Vittorio Orlandi (1), Chia-Rui Chang (2), Tianyu\n  Wang (1), Marco Morucci (1), Pritam Dey (1), Thomas J. Howell (1), Xian Sun\n  (1), Angikar Ghosal (1), Sudeepa Roy (1), Cynthia Rudin (1), Alexander\n  Volfovsky (1) ((1) Duke University, (2) Harvard University)", "title": "dame-flame: A Python Library Providing Fast Interpretable Matching for\n  Causal Inference", "comments": "5 pages, 1 figure; Reference and discussion of CEM corrected", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.MS", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  dame-flame is a Python package for performing matching for observational\ncausal inference on datasets containing discrete covariates. This package\nimplements the Dynamic Almost Matching Exactly (DAME) and Fast Large-Scale\nAlmost Matching Exactly (FLAME) algorithms, which match treatment and control\nunits on subsets of the covariates. The resulting matched groups are\ninterpretable, because the matches are made on covariates (rather than, for\ninstance, propensity scores), and high-quality, because machine learning is\nused to determine which covariates are important to match on. DAME solves an\noptimization problem that matches units on as many covariates as possible,\nprioritizing matches on important covariates. FLAME approximates the solution\nfound by DAME via a much faster backward feature selection procedure. The\npackage provides several adjustable parameters to adapt the algorithms to\nspecific applications, and can calculate treatment effects after matching.\nDescriptions of these parameters, details on estimating treatment effects, and\nfurther examples, can be found in the documentation at\nhttps://almost-matching-exactly.github.io/DAME-FLAME-Python-Package/\n", "versions": [{"version": "v1", "created": "Wed, 6 Jan 2021 04:38:57 GMT"}, {"version": "v2", "created": "Thu, 14 Jan 2021 18:21:44 GMT"}], "update_date": "2021-01-15", "authors_parsed": [["Gupta", "Neha R.", "", "Duke University"], ["Orlandi", "Vittorio", "", "Duke University"], ["Chang", "Chia-Rui", "", "Harvard University"], ["Wang", "Tianyu", "", "Duke University"], ["Morucci", "Marco", "", "Duke University"], ["Dey", "Pritam", "", "Duke University"], ["Howell", "Thomas J.", "", "Duke University"], ["Sun", "Xian", "", "Duke University"], ["Ghosal", "Angikar", "", "Duke University"], ["Roy", "Sudeepa", "", "Duke University"], ["Rudin", "Cynthia", "", "Duke University"], ["Volfovsky", "Alexander", "", "Duke University"]]}, {"id": "2101.01876", "submitter": "Chaopeng Shen", "authors": "Kuai Fang, Daniel Kifer, Kathryn Lawson, Dapeng Feng, Chaopeng Shen", "title": "The data synergy effects of time-series deep learning models in\n  hydrology", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When fitting statistical models to variables in geoscientific disciplines\nsuch as hydrology, it is a customary practice to regionalize - to divide a\nlarge spatial domain into multiple regions and study each region separately -\ninstead of fitting a single model on the entire data (also known as\nunification). Traditional wisdom in these fields suggests that models built for\neach region separately will have higher performance because of homogeneity\nwithin each region. However, by partitioning the training data, each model has\naccess to fewer data points and cannot learn from commonalities between\nregions. Here, through two hydrologic examples (soil moisture and streamflow),\nwe argue that unification can often significantly outperform regionalization in\nthe era of big data and deep learning (DL). Common DL architectures, even\nwithout bespoke customization, can automatically build models that benefit from\nregional commonality while accurately learning region-specific differences. We\nhighlight an effect we call data synergy, where the results of the DL models\nimproved when data were pooled together from characteristically different\nregions. In fact, the performance of the DL models benefited from more diverse\nrather than more homogeneous training data. We hypothesize that DL models\nautomatically adjust their internal representations to identify commonalities\nwhile also providing sufficient discriminatory information to the model. The\nresults here advocate for pooling together larger datasets, and suggest the\nacademic community should place greater emphasis on data sharing and\ncompilation.\n", "versions": [{"version": "v1", "created": "Wed, 6 Jan 2021 05:24:45 GMT"}], "update_date": "2021-01-07", "authors_parsed": [["Fang", "Kuai", ""], ["Kifer", "Daniel", ""], ["Lawson", "Kathryn", ""], ["Feng", "Dapeng", ""], ["Shen", "Chaopeng", ""]]}, {"id": "2101.01877", "submitter": "Tryambak Gangopadhyay", "authors": "Tryambak Gangopadhyay, Vikram Ramanan, Adedotun Akintayo, Paige K\n  Boor, Soumalya Sarkar, Satyanarayanan R Chakravarthy, Soumik Sarkar", "title": "3D Convolutional Selective Autoencoder For Instability Detection in\n  Combustion Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.SY eess.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While analytical solutions of critical (phase) transitions in physical\nsystems are abundant for simple nonlinear systems, such analysis remains\nintractable for real-life dynamical systems. A key example of such a physical\nsystem is thermoacoustic instability in combustion, where prediction or early\ndetection of an onset of instability is a hard technical challenge, which needs\nto be addressed to build safer and more energy-efficient gas turbine engines\npowering aerospace and energy industries. The instabilities arising in\ncombustion chambers of engines are mathematically too complex to model. To\naddress this issue in a data-driven manner instead, we propose a novel deep\nlearning architecture called 3D convolutional selective autoencoder (3D-CSAE)\nto detect the evolution of self-excited oscillations using spatiotemporal data,\ni.e., hi-speed videos taken from a swirl-stabilized combustor (laboratory\nsurrogate of gas turbine engine combustor). 3D-CSAE consists of filters to\nlearn, in a hierarchical fashion, the complex visual and dynamic features\nrelated to combustion instability. We train the 3D-CSAE on frames of videos\nobtained from a limited set of operating conditions. We select the 3D-CSAE\nhyper-parameters that are effective for characterizing hierarchical and\nmultiscale instability structure evolution by utilizing the dynamic information\navailable in the video. The proposed model clearly shows performance\nimprovement in detecting the precursors of instability. The machine\nlearning-driven results are verified with physics-based off-line measures.\nAdvanced active control mechanisms can directly leverage the proposed online\ndetection capability of 3D-CSAE to mitigate the adverse effects of combustion\ninstabilities on the engine operating under various stringent requirements and\nconditions.\n", "versions": [{"version": "v1", "created": "Wed, 6 Jan 2021 05:29:00 GMT"}], "update_date": "2021-01-07", "authors_parsed": [["Gangopadhyay", "Tryambak", ""], ["Ramanan", "Vikram", ""], ["Akintayo", "Adedotun", ""], ["Boor", "Paige K", ""], ["Sarkar", "Soumalya", ""], ["Chakravarthy", "Satyanarayanan R", ""], ["Sarkar", "Soumik", ""]]}, {"id": "2101.01881", "submitter": "Woojeong Jin", "authors": "Woojeong Jin, Maziar Sanjabi, Shaoliang Nie, Liang Tan, Xiang Ren,\n  Hamed Firooz", "title": "Modality-specific Distillation", "comments": "Preprint", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Large neural networks are impractical to deploy on mobile devices due to\ntheir heavy computational cost and slow inference. Knowledge distillation (KD)\nis a technique to reduce the model size while retaining performance by\ntransferring knowledge from a large \"teacher\" model to a smaller \"student\"\nmodel. However, KD on multimodal datasets such as vision-language datasets is\nrelatively unexplored and digesting such multimodal information is challenging\nsince different modalities present different types of information. In this\npaper, we propose modality-specific distillation (MSD) to effectively transfer\nknowledge from a teacher on multimodal datasets. Existing KD approaches can be\napplied to multimodal setup, but a student doesn't have access to\nmodality-specific predictions. Our idea aims at mimicking a teacher's\nmodality-specific predictions by introducing an auxiliary loss term for each\nmodality. Because each modality has different importance for predictions, we\nalso propose weighting approaches for the auxiliary losses; a meta-learning\napproach to learn the optimal weights on these loss terms. In our experiments,\nwe demonstrate the effectiveness of our MSD and the weighting scheme and show\nthat it achieves better performance than KD.\n", "versions": [{"version": "v1", "created": "Wed, 6 Jan 2021 05:45:07 GMT"}], "update_date": "2021-01-07", "authors_parsed": [["Jin", "Woojeong", ""], ["Sanjabi", "Maziar", ""], ["Nie", "Shaoliang", ""], ["Tan", "Liang", ""], ["Ren", "Xiang", ""], ["Firooz", "Hamed", ""]]}, {"id": "2101.01885", "submitter": "Peter Attia", "authors": "Peter M. Attia, Kristen A. Severson, Jeremy D. Witmer", "title": "Statistical learning for accurate and interpretable battery lifetime\n  prediction", "comments": "Submitted to the Journal of the Electrochemical Society", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cond-mat.mtrl-sci stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Data-driven methods for battery lifetime prediction are attracting increasing\nattention for applications in which the degradation mechanisms are poorly\nunderstood and suitable training sets are available. However, while advanced\nmachine learning and deep learning methods promise high performance with\nminimal data preprocessing, simpler linear models with engineered features\noften achieve comparable performance, especially for small training sets, while\nalso providing physical and statistical interpretability. In this work, we use\na previously published dataset to develop simple, accurate, and interpretable\ndata-driven models for battery lifetime prediction. We first present the\n\"capacity matrix\" concept as a compact representation of battery\nelectrochemical cycling data, along with a series of feature representations.\nWe then create a number of univariate and multivariate models, many of which\nachieve comparable performance to the highest-performing models previously\npublished for this dataset. These models also provide insights into the\ndegradation of these cells. Our approaches can be used both to quickly train\nmodels for a new dataset and to benchmark the performance of more advanced\nmachine learning methods.\n", "versions": [{"version": "v1", "created": "Wed, 6 Jan 2021 06:05:24 GMT"}, {"version": "v2", "created": "Sat, 24 Apr 2021 19:53:56 GMT"}], "update_date": "2021-04-27", "authors_parsed": [["Attia", "Peter M.", ""], ["Severson", "Kristen A.", ""], ["Witmer", "Jeremy D.", ""]]}, {"id": "2101.01901", "submitter": "Dimitris Chatzopoulos", "authors": "Christodoulos Pappas, Dimitris Chatzopoulos, Spyros Lalis, Manolis\n  Vavalis", "title": "IPLS : A Framework for Decentralized Federated Learning", "comments": "6 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI cs.DC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The proliferation of resourceful mobile devices that store rich,\nmultidimensional and privacy-sensitive user data motivate the design of\nfederated learning (FL), a machine-learning (ML) paradigm that enables mobile\ndevices to produce an ML model without sharing their data. However, the\nmajority of the existing FL frameworks rely on centralized entities. In this\nwork, we introduce IPLS, a fully decentralized federated learning framework\nthat is partially based on the interplanetary file system (IPFS). By using IPLS\nand connecting into the corresponding private IPFS network, any party can\ninitiate the training process of an ML model or join an ongoing training\nprocess that has already been started by another party. IPLS scales with the\nnumber of participants, is robust against intermittent connectivity and dynamic\nparticipant departures/arrivals, requires minimal resources, and guarantees\nthat the accuracy of the trained model quickly converges to that of a\ncentralized FL framework with an accuracy drop of less than one per thousand.\n", "versions": [{"version": "v1", "created": "Wed, 6 Jan 2021 07:44:51 GMT"}], "update_date": "2021-01-07", "authors_parsed": [["Pappas", "Christodoulos", ""], ["Chatzopoulos", "Dimitris", ""], ["Lalis", "Spyros", ""], ["Vavalis", "Manolis", ""]]}, {"id": "2101.01902", "submitter": "Chandan Karadagur Ananda Reddy", "authors": "Chandan K A Reddy, Harishchandra Dubey, Kazuhito Koishida, Arun Nair,\n  Vishak Gopal, Ross Cutler, Sebastian Braun, Hannes Gamper, Robert Aichner,\n  Sriram Srinivasan", "title": "Interspeech 2021 Deep Noise Suppression Challenge", "comments": "arXiv admin note: substantial text overlap with arXiv:2009.06122", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.LG eess.AS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The Deep Noise Suppression (DNS) challenge is designed to foster innovation\nin the area of noise suppression to achieve superior perceptual speech quality.\nWe recently organized a DNS challenge special session at INTERSPEECH and ICASSP\n2020. We open-sourced training and test datasets for the wideband scenario. We\nalso open-sourced a subjective evaluation framework based on ITU-T standard\nP.808, which was also used to evaluate participants of the challenge. Many\nresearchers from academia and industry made significant contributions to push\nthe field forward, yet even the best noise suppressor was far from achieving\nsuperior speech quality in challenging scenarios. In this version of the\nchallenge organized at INTERSPEECH 2021, we are expanding both our training and\ntest datasets to accommodate full band scenarios. The two tracks in this\nchallenge will focus on real-time denoising for (i) wide band, and(ii) full\nband scenarios. We are also making available a reliable non-intrusive objective\nspeech quality metric called DNSMOS for the participants to use during their\ndevelopment phase.\n", "versions": [{"version": "v1", "created": "Wed, 6 Jan 2021 07:46:25 GMT"}, {"version": "v2", "created": "Sun, 10 Jan 2021 22:27:23 GMT"}, {"version": "v3", "created": "Mon, 5 Apr 2021 01:19:31 GMT"}], "update_date": "2021-04-06", "authors_parsed": [["Reddy", "Chandan K A", ""], ["Dubey", "Harishchandra", ""], ["Koishida", "Kazuhito", ""], ["Nair", "Arun", ""], ["Gopal", "Vishak", ""], ["Cutler", "Ross", ""], ["Braun", "Sebastian", ""], ["Gamper", "Hannes", ""], ["Aichner", "Robert", ""], ["Srinivasan", "Sriram", ""]]}, {"id": "2101.01904", "submitter": "Rohan Saha", "authors": "Rohan Saha", "title": "Comparing Classification Models on Kepler Data", "comments": null, "journal-ref": null, "doi": "10.13140/RG.2.2.11232.43523", "report-no": null, "categories": "astro-ph.EP astro-ph.IM cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Even though the original Kepler mission ended due to mechanical failures, the\nKepler satellite continues to collect data. Using classification models, we can\nunderstand the features exoplanets possess and then use those features to\ninvestigate further for any more information on the candidate planet. Based on\nthe classification model, the idea is to find out the probability of the planet\nunder observation being a candidate for an exoplanet or a false positive. If\nthe model predicts that the observation is a candidate for being an exoplanet,\nthen the further investigation can be conducted. From the model, we can narrow\ndown the features that might explain the difference between a candidate and a\nfalse-positive which ultimately helps us to increase the efficiency of any\nmodel and fine-tune the model and ultimately the process of searching for any\nfuture exoplanets. The model comparison is supported by McNemar's test for\nchecking significance.\n", "versions": [{"version": "v1", "created": "Wed, 6 Jan 2021 07:48:52 GMT"}, {"version": "v2", "created": "Thu, 7 Jan 2021 04:11:53 GMT"}], "update_date": "2021-01-08", "authors_parsed": [["Saha", "Rohan", ""]]}, {"id": "2101.01918", "submitter": "Oussama Dhifallah", "authors": "Oussama Dhifallah and Yue M. Lu", "title": "Phase Transitions in Transfer Learning for High-Dimensional Perceptrons", "comments": null, "journal-ref": null, "doi": "10.3390/e23040400", "report-no": null, "categories": "cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Transfer learning seeks to improve the generalization performance of a target\ntask by exploiting the knowledge learned from a related source task. Central\nquestions include deciding what information one should transfer and when\ntransfer can be beneficial. The latter question is related to the so-called\nnegative transfer phenomenon, where the transferred source information actually\nreduces the generalization performance of the target task. This happens when\nthe two tasks are sufficiently dissimilar. In this paper, we present a\ntheoretical analysis of transfer learning by studying a pair of related\nperceptron learning tasks. Despite the simplicity of our model, it reproduces\nseveral key phenomena observed in practice. Specifically, our asymptotic\nanalysis reveals a phase transition from negative transfer to positive transfer\nas the similarity of the two tasks moves past a well-defined threshold.\n", "versions": [{"version": "v1", "created": "Wed, 6 Jan 2021 08:29:22 GMT"}], "update_date": "2021-04-07", "authors_parsed": [["Dhifallah", "Oussama", ""], ["Lu", "Yue M.", ""]]}, {"id": "2101.01926", "submitter": "Xuekai Li", "authors": "Tongtong Wu, Xuekai Li, Yuan-Fang Li, Reza Haffari, Guilin Qi, Yujin\n  Zhu and Guoqiang Xu", "title": "Curriculum-Meta Learning for Order-Robust Continual Relation Extraction", "comments": "Accepted by AAAI2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Continual relation extraction is an important task that focuses on extracting\nnew facts incrementally from unstructured text. Given the sequential arrival\norder of the relations, this task is prone to two serious challenges, namely\ncatastrophic forgetting and order-sensitivity. We propose a novel\ncurriculum-meta learning method to tackle the above two challenges in continual\nrelation extraction. We combine meta learning and curriculum learning to\nquickly adapt model parameters to a new task and to reduce interference of\npreviously seen tasks on the current task. We design a novel relation\nrepresentation learning method through the distribution of domain and range\ntypes of relations. Such representations are utilized to quantify the\ndifficulty of tasks for the construction of curricula. Moreover, we also\npresent novel difficulty-based metrics to quantitatively measure the extent of\norder-sensitivity of a given model, suggesting new ways to evaluate model\nrobustness. Our comprehensive experiments on three benchmark datasets show that\nour proposed method outperforms the state-of-the-art techniques. The code is\navailable at the anonymous GitHub repository:\nhttps://github.com/wutong8023/AAAI_CML.\n", "versions": [{"version": "v1", "created": "Wed, 6 Jan 2021 08:52:34 GMT"}, {"version": "v2", "created": "Thu, 7 Jan 2021 02:25:10 GMT"}, {"version": "v3", "created": "Fri, 8 Jan 2021 10:06:40 GMT"}], "update_date": "2021-01-11", "authors_parsed": [["Wu", "Tongtong", ""], ["Li", "Xuekai", ""], ["Li", "Yuan-Fang", ""], ["Haffari", "Reza", ""], ["Qi", "Guilin", ""], ["Zhu", "Yujin", ""], ["Xu", "Guoqiang", ""]]}, {"id": "2101.01975", "submitter": "Suwei Yang", "authors": "Suwei Yang, Massimo Lupascu, Kuldeep S. Meel", "title": "Predicting Forest Fire Using Remote Sensing Data And Machine Learning", "comments": "8 pages, 3 figures, to be published in the Thirty-Fifth AAAI\n  Conference on Artificial Intelligence (AAAI-21)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Over the last few decades, deforestation and climate change have caused\nincreasing number of forest fires. In Southeast Asia, Indonesia has been the\nmost affected country by tropical peatland forest fires. These fires have a\nsignificant impact on the climate resulting in extensive health, social and\neconomic issues. Existing forest fire prediction systems, such as the Canadian\nForest Fire Danger Rating System, are based on handcrafted features and require\ninstallation and maintenance of expensive instruments on the ground, which can\nbe a challenge for developing countries such as Indonesia. We propose a novel,\ncost-effective, machine-learning based approach that uses remote sensing data\nto predict forest fires in Indonesia. Our prediction model achieves more than\n0.81 area under the receiver operator characteristic (ROC) curve, performing\nsignificantly better than the baseline approach which never exceeds 0.70 area\nunder ROC curve on the same tasks. Our model's performance remained above 0.81\narea under ROC curve even when evaluated with reduced data. The results support\nour claim that machine-learning based approaches can lead to reliable and\ncost-effective forest fire prediction systems.\n", "versions": [{"version": "v1", "created": "Wed, 6 Jan 2021 11:22:55 GMT"}], "update_date": "2021-01-07", "authors_parsed": [["Yang", "Suwei", ""], ["Lupascu", "Massimo", ""], ["Meel", "Kuldeep S.", ""]]}, {"id": "2101.01990", "submitter": "Ansgar Steland", "authors": "Ansgar Steland and Bart E. Pieters", "title": "Cross-Validation and Uncertainty Determination for Randomized Neural\n  Networks with Applications to Mobile Sensors", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG eess.SP", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Randomized artificial neural networks such as extreme learning machines\nprovide an attractive and efficient method for supervised learning under\nlimited computing ressources and green machine learning. This especially\napplies when equipping mobile devices (sensors) with weak artificial\nintelligence. Results are discussed about supervised learning with such\nnetworks and regression methods in terms of consistency and bounds for the\ngeneralization and prediction error. Especially, some recent results are\nreviewed addressing learning with data sampled by moving sensors leading to\nnon-stationary and dependent samples.\n  As randomized networks lead to random out-of-sample performance measures, we\nstudy a cross-validation approach to handle the randomness and make use of it\nto improve out-of-sample performance. Additionally, a computationally efficient\napproach to determine the resulting uncertainty in terms of a confidence\ninterval for the mean out-of-sample prediction error is discussed based on\ntwo-stage estimation. The approach is applied to a prediction problem arising\nin vehicle integrated photovoltaics.\n", "versions": [{"version": "v1", "created": "Wed, 6 Jan 2021 12:28:06 GMT"}], "update_date": "2021-01-07", "authors_parsed": [["Steland", "Ansgar", ""], ["Pieters", "Bart E.", ""]]}, {"id": "2101.02006", "submitter": "Abdallah Moubayed", "authors": "Abdallah Moubayed, MohammadNoor Injadat, Abdallah Shami, Hanan\n  Lutfiyya", "title": "Relationship between Student Engagement and Performance in e-Learning\n  Environment Using Association Rules", "comments": "1 Table, 1 Figure, published in 2018 IEEE World Engineering Education\n  Conference (EDUNINE)", "journal-ref": "2018 IEEE World Engineering Education Conference (EDUNINE), 2018,\n  pp. 1-6", "doi": "10.1109/EDUNINE.2018.8451005", "report-no": null, "categories": "cs.CY cs.HC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The field of e-learning has emerged as a topic of interest in academia due to\nthe increased ease of accessing the Internet using using smart-phones and\nwireless devices. One of the challenges facing e-learning platforms is how to\nkeep students motivated and engaged. Moreover, it is also crucial to identify\nthe students that might need help in order to make sure their academic\nperformance doesn't suffer. To that end, this paper tries to investigate the\nrelationship between student engagement and their academic performance. Apriori\nassociation rules algorithm is used to derive a set of rules that relate\nstudent engagement to academic performance. Experimental results' analysis done\nusing confidence and lift metrics show that a positive correlation exists\nbetween students' engagement level and their academic performance in a blended\ne-learning environment. In particular, it is shown that higher engagement often\nleads to better academic performance. This cements the previous work that\nlinked engagement and academic performance in traditional classrooms.\n", "versions": [{"version": "v1", "created": "Fri, 25 Dec 2020 17:00:23 GMT"}], "update_date": "2021-01-07", "authors_parsed": [["Moubayed", "Abdallah", ""], ["Injadat", "MohammadNoor", ""], ["Shami", "Abdallah", ""], ["Lutfiyya", "Hanan", ""]]}, {"id": "2101.02017", "submitter": "Mihir Parmar", "authors": "Mihir Parmar, Ashwin Karthik Ambalavanan, Hong Guan, Rishab Banerjee,\n  Jitesh Pabla and Murthy Devarakonda", "title": "COVID-19: Comparative Analysis of Methods for Identifying Articles\n  Related to Therapeutics and Vaccines without Using Labeled Data", "comments": "6 pages, 3 Tables, Appendix", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Here we proposed an approach to analyze text classification methods based on\nthe presence or absence of task-specific terms (and their synonyms) in the\ntext. We applied this approach to study six different transfer-learning and\nunsupervised methods for screening articles relevant to COVID-19 vaccines and\ntherapeutics. The analysis revealed that while a BERT model trained on\nsearch-engine results generally performed well, it miss-classified relevant\nabstracts that did not contain task-specific terms. We used this insight to\ncreate a more effective unsupervised ensemble.\n", "versions": [{"version": "v1", "created": "Tue, 5 Jan 2021 08:40:04 GMT"}], "update_date": "2021-01-07", "authors_parsed": [["Parmar", "Mihir", ""], ["Ambalavanan", "Ashwin Karthik", ""], ["Guan", "Hong", ""], ["Banerjee", "Rishab", ""], ["Pabla", "Jitesh", ""], ["Devarakonda", "Murthy", ""]]}, {"id": "2101.02028", "submitter": "Ye Tian", "authors": "Ye Tian", "title": "A Multilayer Correlated Topic Model", "comments": "11 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG stat.CO stat.ME stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We proposed a novel multilayer correlated topic model (MCTM) to analyze how\nthe main ideas inherit and vary between a document and its different segments,\nwhich helps understand an article's structure. The variational\nexpectation-maximization (EM) algorithm was derived to estimate the posterior\nand parameters in MCTM. We introduced two potential applications of MCTM,\nincluding the paragraph-level document analysis and market basket data\nanalysis. The effectiveness of MCTM in understanding the document structure has\nbeen verified by the great predictive performance on held-out documents and\nintuitive visualization. We also showed that MCTM could successfully capture\ncustomers' popular shopping patterns in the market basket analysis.\n", "versions": [{"version": "v1", "created": "Sat, 2 Jan 2021 21:50:36 GMT"}], "update_date": "2021-01-07", "authors_parsed": [["Tian", "Ye", ""]]}, {"id": "2101.02033", "submitter": "Yurio Windiatmoko", "authors": "Malik Abdul Aziz, Fahmi Nurrahim, Prastyo Eko Susanto, Yurio\n  Windiatmoko", "title": "Boarding House Renting Price Prediction Using Deep Neural Network\n  Regression on Mobile Apps", "comments": "10 pages, 14 figures, 3 tables, and pre-print paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.LG", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Boarding house is the most important requirement, especially for college\nstudents who live far away from the city, place of his origin or house.\nHowever, the problem we see now is the uneven distribution of study places in\nIndonesia which 75% of the best top educational institutions come from the\nisland of Java. So, students who are looking for boarding houses rent requires\nmore effort in comparing the various aspects desired. They need to survey one\nby one to the boarding house they want, even though they can survey online, it\nstill requires more effort to pay attention to the desired facilities one by\none. Therefore, we then created an Mobile Application that can predict prices\nbased on student needs by comparing several variables, namely city, area, type\nof boarding house, and facilities. So, students can easily estimate the ideal\nprice. The results of this study prove that we have succeeded in predicting\nprices for boarding houses rent well based on the variables we have determined,\nand modeling that variables using Deep Neural Network Regression.\n", "versions": [{"version": "v1", "created": "Thu, 31 Dec 2020 15:12:10 GMT"}], "update_date": "2021-01-07", "authors_parsed": [["Aziz", "Malik Abdul", ""], ["Nurrahim", "Fahmi", ""], ["Susanto", "Prastyo Eko", ""], ["Windiatmoko", "Yurio", ""]]}, {"id": "2101.02034", "submitter": "Qing Wei", "authors": "Qing Wei, Hailan Ma, Chunlin Chen, Daoyi Dong", "title": "Deep Reinforcement Learning with Quantum-inspired Experience Replay", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI quant-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, a novel training paradigm inspired by quantum computation is\nproposed for deep reinforcement learning (DRL) with experience replay. In\ncontrast to traditional experience replay mechanism in DRL, the proposed deep\nreinforcement learning with quantum-inspired experience replay (DRL-QER)\nadaptively chooses experiences from the replay buffer according to the\ncomplexity and the replayed times of each experience (also called transition),\nto achieve a balance between exploration and exploitation. In DRL-QER,\ntransitions are first formulated in quantum representations, and then the\npreparation operation and the depreciation operation are performed on the\ntransitions. In this progress, the preparation operation reflects the\nrelationship between the temporal difference errors (TD-errors) and the\nimportance of the experiences, while the depreciation operation is taken into\naccount to ensure the diversity of the transitions. The experimental results on\nAtari 2600 games show that DRL-QER outperforms state-of-the-art algorithms such\nas DRL-PER and DCRL on most of these games with improved training efficiency,\nand is also applicable to such memory-based DRL approaches as double network\nand dueling network.\n", "versions": [{"version": "v1", "created": "Wed, 6 Jan 2021 13:52:04 GMT"}], "update_date": "2021-01-07", "authors_parsed": [["Wei", "Qing", ""], ["Ma", "Hailan", ""], ["Chen", "Chunlin", ""], ["Dong", "Daoyi", ""]]}, {"id": "2101.02055", "submitter": "Zhaohan Guo", "authors": "Zhaohan Daniel Guo, Mohammad Gheshlaghi Azar, Alaa Saade, Shantanu\n  Thakoor, Bilal Piot, Bernardo Avila Pires, Michal Valko, Thomas Mesnard, Tor\n  Lattimore, R\\'emi Munos", "title": "Geometric Entropic Exploration", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Exploration is essential for solving complex Reinforcement Learning (RL)\ntasks. Maximum State-Visitation Entropy (MSVE) formulates the exploration\nproblem as a well-defined policy optimization problem whose solution aims at\nvisiting all states as uniformly as possible. This is in contrast to standard\nuncertainty-based approaches where exploration is transient and eventually\nvanishes. However, existing approaches to MSVE are theoretically justified only\nfor discrete state-spaces as they are oblivious to the geometry of continuous\ndomains. We address this challenge by introducing Geometric Entropy\nMaximisation (GEM), a new algorithm that maximises the geometry-aware Shannon\nentropy of state-visits in both discrete and continuous domains. Our key\ntheoretical contribution is casting geometry-aware MSVE exploration as a\ntractable problem of optimising a simple and novel noise-contrastive objective\nfunction. In our experiments, we show the efficiency of GEM in solving several\nRL problems with sparse rewards, compared against other deep RL exploration\napproaches.\n", "versions": [{"version": "v1", "created": "Wed, 6 Jan 2021 14:15:07 GMT"}, {"version": "v2", "created": "Thu, 7 Jan 2021 12:57:27 GMT"}], "update_date": "2021-01-08", "authors_parsed": [["Guo", "Zhaohan Daniel", ""], ["Azar", "Mohammad Gheshlaghi", ""], ["Saade", "Alaa", ""], ["Thakoor", "Shantanu", ""], ["Piot", "Bilal", ""], ["Pires", "Bernardo Avila", ""], ["Valko", "Michal", ""], ["Mesnard", "Thomas", ""], ["Lattimore", "Tor", ""], ["Munos", "R\u00e9mi", ""]]}, {"id": "2101.02069", "submitter": "Hailong Hu", "authors": "Hailong Hu, Jun Pang", "title": "Model Extraction and Defenses on Generative Adversarial Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Model extraction attacks aim to duplicate a machine learning model through\nquery access to a target model. Early studies mainly focus on discriminative\nmodels. Despite the success, model extraction attacks against generative models\nare less well explored. In this paper, we systematically study the feasibility\nof model extraction attacks against generative adversarial networks (GANs).\nSpecifically, we first define accuracy and fidelity on model extraction attacks\nagainst GANs. Then we study model extraction attacks against GANs from the\nperspective of accuracy extraction and fidelity extraction, according to the\nadversary's goals and background knowledge. We further conduct a case study\nwhere an adversary can transfer knowledge of the extracted model which steals a\nstate-of-the-art GAN trained with more than 3 million images to new domains to\nbroaden the scope of applications of model extraction attacks. Finally, we\npropose effective defense techniques to safeguard GANs, considering a trade-off\nbetween the utility and security of GAN models.\n", "versions": [{"version": "v1", "created": "Wed, 6 Jan 2021 14:36:21 GMT"}], "update_date": "2021-01-07", "authors_parsed": [["Hu", "Hailong", ""], ["Pang", "Jun", ""]]}, {"id": "2101.02083", "submitter": "Hiroaki Sasaki", "authors": "Hiroaki Sasaki and Takashi Takenouchi", "title": "A unified view for unsupervised representation learning with density\n  ratio estimation: Maximization of mutual information, nonlinear ICA and\n  nonlinear subspace estimation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG eess.SP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Unsupervised representation learning is one of the most important problems in\nmachine learning. Recent promising methods are based on contrastive learning.\nHowever, contrastive learning often relies on heuristic ideas, and therefore it\nis not easy to understand what contrastive learning is doing. This paper\nemphasizes that density ratio estimation is a promising goal for unsupervised\nrepresentation learning, and promotes understanding to contrastive learning.\nOur primal contribution is to theoretically show that density ratio estimation\nunifies three frameworks for unsupervised representation learning: Maximization\nof mutual information (MI), nonlinear independent component analysis (ICA) and\na novel framework for estimation of a lower-dimensional nonlinear subspace\nproposed in this paper. This unified view clarifies under what conditions\ncontrastive learning can be regarded as maximizing MI, performing nonlinear ICA\nor estimating the lower-dimensional nonlinear subspace in the proposed\nframework. Furthermore, we also make theoretical contributions in each of the\nthree frameworks: We show that MI can be maximized through density ratio\nestimation under certain conditions, while our analysis for nonlinear ICA\nreveals a novel insight for recovery of the latent source components, which is\nclearly supported by numerical experiments. In addition, some theoretical\nconditions are also established to estimate a nonlinear subspace in the\nproposed framework. Based on the unified view, we propose two practical methods\nfor unsupervised representation learning through density ratio estimation: The\nfirst method is an outlier-robust method for representation learning, while the\nsecond one is a sample-efficient nonlinear ICA method. Finally, we numerically\ndemonstrate usefulness of the proposed methods in nonlinear ICA and through\napplication to a downstream task for classification.\n", "versions": [{"version": "v1", "created": "Wed, 6 Jan 2021 15:08:54 GMT"}], "update_date": "2021-01-07", "authors_parsed": [["Sasaki", "Hiroaki", ""], ["Takenouchi", "Takashi", ""]]}, {"id": "2101.02084", "submitter": "Silvia Chiappa", "authors": "Silvia Chiappa and Aldo Pacchiano", "title": "Fairness with Continuous Optimal Transport", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Whilst optimal transport (OT) is increasingly being recognized as a powerful\nand flexible approach for dealing with fairness issues, current OT fairness\nmethods are confined to the use of discrete OT. In this paper, we leverage\nrecent advances from the OT literature to introduce a stochastic-gradient\nfairness method based on a dual formulation of continuous OT. We show that this\nmethod gives superior performance to discrete OT methods when little data is\navailable to solve the OT problem, and similar performance otherwise. We also\nshow that both continuous and discrete OT methods are able to continually\nadjust the model parameters to adapt to different levels of unfairness that\nmight occur in real-world applications of ML systems.\n", "versions": [{"version": "v1", "created": "Wed, 6 Jan 2021 15:10:10 GMT"}], "update_date": "2021-01-07", "authors_parsed": [["Chiappa", "Silvia", ""], ["Pacchiano", "Aldo", ""]]}, {"id": "2101.02113", "submitter": "Francesca Tang", "authors": "Francesca Tang, Yang Feng, Hamza Chiheb, Jianqing Fan", "title": "The Interplay of Demographic Variables and Social Distancing Scores in\n  Deep Prediction of U.S. COVID-19 Cases", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.LG physics.soc-ph stat.AP stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  With the severity of the COVID-19 outbreak, we characterize the nature of the\ngrowth trajectories of counties in the United States using a novel combination\nof spectral clustering and the correlation matrix. As the U.S. and the rest of\nthe world are experiencing a severe second wave of infections, the importance\nof assigning growth membership to counties and understanding the determinants\nof the growth are increasingly evident. Subsequently, we select the demographic\nfeatures that are most statistically significant in distinguishing the\ncommunities. Lastly, we effectively predict the future growth of a given county\nwith an LSTM using three social distancing scores. This comprehensive study\ncaptures the nature of counties' growth in cases at a very micro-level using\ngrowth communities, demographic factors, and social distancing performance to\nhelp government agencies utilize known information to make appropriate\ndecisions regarding which potential counties to target resources and funding\nto.\n", "versions": [{"version": "v1", "created": "Wed, 6 Jan 2021 16:12:29 GMT"}], "update_date": "2021-01-07", "authors_parsed": [["Tang", "Francesca", ""], ["Feng", "Yang", ""], ["Chiheb", "Hamza", ""], ["Fan", "Jianqing", ""]]}, {"id": "2101.02115", "submitter": "Ruben Ohana", "authors": "Alessandro Cappelli, Ruben Ohana, Julien Launay, Laurent Meunier,\n  Iacopo Poli, Florent Krzakala", "title": "Adversarial Robustness by Design through Analog Computing and Synthetic\n  Gradients", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new defense mechanism against adversarial attacks inspired by an\noptical co-processor, providing robustness without compromising natural\naccuracy in both white-box and black-box settings. This hardware co-processor\nperforms a nonlinear fixed random transformation, where the parameters are\nunknown and impossible to retrieve with sufficient precision for large enough\ndimensions. In the white-box setting, our defense works by obfuscating the\nparameters of the random projection. Unlike other defenses relying on\nobfuscated gradients, we find we are unable to build a reliable backward\ndifferentiable approximation for obfuscated parameters. Moreover, while our\nmodel reaches a good natural accuracy with a hybrid backpropagation - synthetic\ngradient method, the same approach is suboptimal if employed to generate\nadversarial examples. We find the combination of a random projection and\nbinarization in the optical system also improves robustness against various\ntypes of black-box attacks. Finally, our hybrid training method builds robust\nfeatures against transfer attacks. We demonstrate our approach on a VGG-like\narchitecture, placing the defense on top of the convolutional features, on\nCIFAR-10 and CIFAR-100. Code is available at\nhttps://github.com/lightonai/adversarial-robustness-by-design.\n", "versions": [{"version": "v1", "created": "Wed, 6 Jan 2021 16:15:29 GMT"}], "update_date": "2021-01-07", "authors_parsed": [["Cappelli", "Alessandro", ""], ["Ohana", "Ruben", ""], ["Launay", "Julien", ""], ["Meunier", "Laurent", ""], ["Poli", "Iacopo", ""], ["Krzakala", "Florent", ""]]}, {"id": "2101.02118", "submitter": "Daniela Thyssens", "authors": "Shereen Elsayed, Daniela Thyssens, Ahmed Rashed, Lars Schmidt-Thieme\n  and Hadi Samer Jomaa", "title": "Do We Really Need Deep Learning Models for Time Series Forecasting?", "comments": "14 pages with appendix, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Time series forecasting is a crucial task in machine learning, as it has a\nwide range of applications including but not limited to forecasting electricity\nconsumption, traffic, and air quality. Traditional forecasting models relied on\nrolling averages, vector auto-regression and auto-regressive integrated moving\naverages. On the other hand, deep learning and matrix factorization models have\nbeen recently proposed to tackle the same problem with more competitive\nperformance. However, one major drawback of such models is that they tend to be\noverly complex in comparison to traditional techniques. In this paper, we try\nto answer whether these highly complex deep learning models are without\nalternative. We aim to enrich the pool of simple but powerful baselines by\nrevisiting the gradient boosting regression trees for time series forecasting.\nSpecifically, we reconfigure the way time series data is handled by Gradient\nTree Boosting models in a windowed fashion that is similar to the deep learning\nmodels. For each training window, the target values are concatenated with\nexternal features, and then flattened to form one input instance for a\nmulti-output gradient boosting regression tree model. We conducted a\ncomparative study on nine datasets for eight state-of-the-art deep-learning\nmodels that were presented at top-level conferences in the last years. The\nresults demonstrated that the proposed approach outperforms all of the\nstate-of-the-art models.\n", "versions": [{"version": "v1", "created": "Wed, 6 Jan 2021 16:18:04 GMT"}], "update_date": "2021-01-07", "authors_parsed": [["Elsayed", "Shereen", ""], ["Thyssens", "Daniela", ""], ["Rashed", "Ahmed", ""], ["Schmidt-Thieme", "Lars", ""], ["Jomaa", "Hadi Samer", ""]]}, {"id": "2101.02121", "submitter": "Julian Mack", "authors": "Julian Mack, Rossella Arcucci, Miguel Molina-Solana and Yi-Ke Guo", "title": "Attention-based Convolutional Autoencoders for 3D-Variational Data\n  Assimilation", "comments": "Published in Computer Methods in Applied Mechanics and Engineering in\n  Dec 2020", "journal-ref": "Computer Methods in Applied Mechanics and Engineering 372 (2020)\n  113291", "doi": "10.1016/j.cma.2020.113291", "report-no": null, "categories": "cs.LG cs.CE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new 'Bi-Reduced Space' approach to solving 3D Variational Data\nAssimilation using Convolutional Autoencoders. We prove that our approach has\nthe same solution as previous methods but has significantly lower computational\ncomplexity; in other words, we reduce the computational cost without affecting\nthe data assimilation accuracy. We tested the new method with data from a\nreal-world application: a pollution model of a site in Elephant and Castle,\nLondon and found that we could reduce the size of the background covariance\nmatrix representation by O(10^3) and, at the same time, increase our data\nassimilation accuracy with respect to existing reduced space methods.\n", "versions": [{"version": "v1", "created": "Wed, 6 Jan 2021 16:23:58 GMT"}], "update_date": "2021-01-07", "authors_parsed": [["Mack", "Julian", ""], ["Arcucci", "Rossella", ""], ["Molina-Solana", "Miguel", ""], ["Guo", "Yi-Ke", ""]]}, {"id": "2101.02137", "submitter": "Nithia Vijayan", "authors": "Nithia Vijayan and Prashanth L. A", "title": "Smoothed functional-based gradient algorithms for off-policy\n  reinforcement learning: A non-asymptotic viewpoint", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose two policy gradient algorithms for solving the problem of control\nin an off-policy reinforcement learning (RL) context. Both algorithms\nincorporate a smoothed functional (SF) based gradient estimation scheme. The\nfirst algorithm is a straightforward combination of importance sampling-based\noff-policy evaluation with SF-based gradient estimation. The second algorithm,\ninspired by the stochastic variance-reduced gradient (SVRG) algorithm,\nincorporates variance reduction in the update iteration. For both algorithms,\nwe derive non-asymptotic bounds that establish convergence to an approximate\nstationary point. From these results, we infer that the first algorithm\nconverges at a rate that is comparable to the well-known REINFORCE algorithm in\nan off-policy RL context, while the second algorithm exhibits an improved rate\nof convergence.\n", "versions": [{"version": "v1", "created": "Wed, 6 Jan 2021 17:06:42 GMT"}, {"version": "v2", "created": "Mon, 22 Feb 2021 02:54:39 GMT"}, {"version": "v3", "created": "Thu, 27 May 2021 05:15:26 GMT"}, {"version": "v4", "created": "Wed, 14 Jul 2021 05:57:40 GMT"}], "update_date": "2021-07-15", "authors_parsed": [["Vijayan", "Nithia", ""], ["A", "Prashanth L.", ""]]}, {"id": "2101.02138", "submitter": "Zoe Holmes", "authors": "Zo\\\"e Holmes, Kunal Sharma, M. Cerezo, Patrick J. Coles", "title": "Connecting ansatz expressibility to gradient magnitudes and barren\n  plateaus", "comments": "Main text: 10 pages, 4 figures. Appendices: 10 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": "LA-UR-21-20034", "categories": "quant-ph cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Parameterized quantum circuits serve as ans\\\"{a}tze for solving variational\nproblems and provide a flexible paradigm for programming near-term quantum\ncomputers. Ideally, such ans\\\"{a}tze should be highly expressive so that a\nclose approximation of the desired solution can be accessed. On the other hand,\nthe ansatz must also have sufficiently large gradients to allow for training.\nHere, we derive a fundamental relationship between these two essential\nproperties: expressibility and trainability. This is done by extending the well\nestablished barren plateau phenomenon, which holds for ans\\\"{a}tze that form\nexact 2-designs, to arbitrary ans\\\"{a}tze. Specifically, we calculate the\nvariance in the cost gradient in terms of the expressibility of the ansatz, as\nmeasured by its distance from being a 2-design. Our resulting bounds indicate\nthat highly expressive ans\\\"{a}tze exhibit flatter cost landscapes and\ntherefore will be harder to train. Furthermore, we provide numerics\nillustrating the effect of expressiblity on gradient scalings, and we discuss\nthe implications for designing strategies to avoid barren plateaus.\n", "versions": [{"version": "v1", "created": "Wed, 6 Jan 2021 17:09:37 GMT"}], "update_date": "2021-01-07", "authors_parsed": [["Holmes", "Zo\u00eb", ""], ["Sharma", "Kunal", ""], ["Cerezo", "M.", ""], ["Coles", "Patrick J.", ""]]}, {"id": "2101.02149", "submitter": "Linh Tran", "authors": "Linh Tran, Maja Pantic, Marc Peter Deisenroth", "title": "Cauchy-Schwarz Regularized Autoencoder", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Recent work in unsupervised learning has focused on efficient inference and\nlearning in latent variables models. Training these models by maximizing the\nevidence (marginal likelihood) is typically intractable. Thus, a common\napproximation is to maximize the Evidence Lower BOund (ELBO) instead.\nVariational autoencoders (VAE) are a powerful and widely-used class of\ngenerative models that optimize the ELBO efficiently for large datasets.\nHowever, the VAE's default Gaussian choice for the prior imposes a strong\nconstraint on its ability to represent the true posterior, thereby degrading\noverall performance. A Gaussian mixture model (GMM) would be a richer prior,\nbut cannot be handled efficiently within the VAE framework because of the\nintractability of the Kullback-Leibler divergence for GMMs. We deviate from the\ncommon VAE framework in favor of one with an analytical solution for Gaussian\nmixture prior. To perform efficient inference for GMM priors, we introduce a\nnew constrained objective based on the Cauchy-Schwarz divergence, which can be\ncomputed analytically for GMMs. This new objective allows us to incorporate\nricher, multi-modal priors into the autoencoding framework. We provide\nempirical studies on a range of datasets and show that our objective improves\nupon variational auto-encoding models in density estimation, unsupervised\nclustering, semi-supervised learning, and face analysis.\n", "versions": [{"version": "v1", "created": "Wed, 6 Jan 2021 17:36:26 GMT"}, {"version": "v2", "created": "Fri, 12 Feb 2021 18:47:39 GMT"}], "update_date": "2021-02-15", "authors_parsed": [["Tran", "Linh", ""], ["Pantic", "Maja", ""], ["Deisenroth", "Marc Peter", ""]]}, {"id": "2101.02153", "submitter": "Benedek Rozemberczki", "authors": "Benedek Rozemberczki and Rik Sarkar", "title": "The Shapley Value of Classifiers in Ensemble Games", "comments": "Source code is available here:\n  https://github.com/benedekrozemberczki/shapley", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.DS cs.GT cs.NE", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  What is the value of an individual model in an ensemble of binary\nclassifiers? We answer this question by introducing a class of transferable\nutility cooperative games called \\textit{ensemble games}. In machine learning\nensembles, pre-trained models cooperate to make classification decisions. To\nquantify the importance of models in these ensemble games, we define\n\\textit{Troupe} -- an efficient algorithm which allocates payoffs based on\napproximate Shapley values of the classifiers. We argue that the Shapley value\nof models in these games is an effective decision metric for choosing a high\nperforming subset of models from the ensemble. Our analytical findings prove\nthat our Shapley value estimation scheme is precise and scalable; its\nperformance increases with size of the dataset and ensemble. Empirical results\non real world graph classification tasks demonstrate that our algorithm\nproduces high quality estimates of the Shapley value. We find that Shapley\nvalues can be utilized for ensemble pruning, and that adversarial models\nreceive a low valuation. Complex classifiers are frequently found to be\nresponsible for both correct and incorrect classification decisions.\n", "versions": [{"version": "v1", "created": "Wed, 6 Jan 2021 17:40:23 GMT"}, {"version": "v2", "created": "Thu, 10 Jun 2021 20:38:54 GMT"}], "update_date": "2021-06-14", "authors_parsed": [["Rozemberczki", "Benedek", ""], ["Sarkar", "Rik", ""]]}, {"id": "2101.02180", "submitter": "David Wu", "authors": "David Wu, David R. Palmer, Daryl R. Deford", "title": "Bayesian Inference of Random Dot Product Graphs via Conic Programming", "comments": "submitted for publication in SIAM Journal on Optimization (SIOPT)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.OC stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We present a convex cone program to infer the latent probability matrix of a\nrandom dot product graph (RDPG). The optimization problem maximizes the\nBernoulli maximum likelihood function with an added nuclear norm regularization\nterm. The dual problem has a particularly nice form, related to the well-known\nsemidefinite program relaxation of the MaxCut problem. Using the primal-dual\noptimality conditions, we bound the entries and rank of the primal and dual\nsolutions. Furthermore, we bound the optimal objective value and prove\nasymptotic consistency of the probability estimates of a slightly modified\nmodel under mild technical assumptions. Our experiments on synthetic RDPGs not\nonly recover natural clusters, but also reveal the underlying low-dimensional\ngeometry of the original data. We also demonstrate that the method recovers\nlatent structure in the Karate Club Graph and synthetic U.S. Senate vote graphs\nand is scalable to graphs with up to a few hundred nodes.\n", "versions": [{"version": "v1", "created": "Wed, 6 Jan 2021 18:29:37 GMT"}], "update_date": "2021-01-07", "authors_parsed": [["Wu", "David", ""], ["Palmer", "David R.", ""], ["Deford", "Daryl R.", ""]]}, {"id": "2101.02185", "submitter": "Seyed Sajjadi", "authors": "Volkan Ustun, Rajay Kumar, Adam Reilly, Seyed Sajjadi, Andrew Miller", "title": "Adaptive Synthetic Characters for Military Training", "comments": null, "journal-ref": "2020 Interservice/Industry Training, Simulation, and Education\n  Conference (I/ITSEC)", "doi": null, "report-no": null, "categories": "cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Behaviors of the synthetic characters in current military simulations are\nlimited since they are generally generated by rule-based and reactive\ncomputational models with minimal intelligence. Such computational models\ncannot adapt to reflect the experience of the characters, resulting in brittle\nintelligence for even the most effective behavior models devised via costly and\nlabor-intensive processes. Observation-based behavior model adaptation that\nleverages machine learning and the experience of synthetic entities in\ncombination with appropriate prior knowledge can address the issues in the\nexisting computational behavior models to create a better training experience\nin military training simulations. In this paper, we introduce a framework that\naims to create autonomous synthetic characters that can perform coherent\nsequences of believable behavior while being aware of human trainees and their\nneeds within a training simulation. This framework brings together three\nmutually complementary components. The first component is a Unity-based\nsimulation environment - Rapid Integration and Development Environment (RIDE) -\nsupporting One World Terrain (OWT) models and capable of running and supporting\nmachine learning experiments. The second is Shiva, a novel multi-agent\nreinforcement and imitation learning framework that can interface with a\nvariety of simulation environments, and that can additionally utilize a variety\nof learning algorithms. The final component is the Sigma Cognitive Architecture\nthat will augment the behavior models with symbolic and probabilistic reasoning\ncapabilities. We have successfully created proof-of-concept behavior models\nleveraging this framework on realistic terrain as an essential step towards\nbringing machine learning into military simulations.\n", "versions": [{"version": "v1", "created": "Wed, 6 Jan 2021 18:45:48 GMT"}], "update_date": "2021-01-07", "authors_parsed": [["Ustun", "Volkan", ""], ["Kumar", "Rajay", ""], ["Reilly", "Adam", ""], ["Sajjadi", "Seyed", ""], ["Miller", "Andrew", ""]]}, {"id": "2101.02188", "submitter": "Brian Mac Namee", "authors": "Cathal Ryan and Christophe Gu\\'eret and Donagh Berry and Medb Corcoran\n  and Mark T. Keane and Brian Mac Namee", "title": "Predicting Illness for a Sustainable Dairy Agriculture: Predicting and\n  Explaining the Onset of Mastitis in Dairy Cows", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Mastitis is a billion dollar health problem for the modern dairy industry,\nwith implications for antibiotic resistance. The use of AI techniques to\nidentify the early onset of this disease, thus has significant implications for\nthe sustainability of this agricultural sector. Current approaches to treating\nmastitis involve antibiotics and this practice is coming under ever increasing\nscrutiny. Using machine learning models to identify cows at risk of developing\nmastitis and applying targeted treatment regimes to only those animals promotes\na more sustainable approach. Incorrect predictions from such models, however,\ncan lead to monetary losses, unnecessary use of antibiotics, and even the\npremature death of animals, so it is important to generate compelling\nexplanations for predictions to build trust with users and to better support\ntheir decision making. In this paper we demonstrate a system developed to\npredict mastitis infections in cows and provide explanations of these\npredictions using counterfactuals. We demonstrate the system and describe the\nengagement with farmers undertaken to build it.\n", "versions": [{"version": "v1", "created": "Wed, 6 Jan 2021 18:48:26 GMT"}, {"version": "v2", "created": "Thu, 7 Jan 2021 14:16:13 GMT"}], "update_date": "2021-01-08", "authors_parsed": [["Ryan", "Cathal", ""], ["Gu\u00e9ret", "Christophe", ""], ["Berry", "Donagh", ""], ["Corcoran", "Medb", ""], ["Keane", "Mark T.", ""], ["Mac Namee", "Brian", ""]]}, {"id": "2101.02195", "submitter": "Quanquan Gu", "authors": "Tianhao Wang and Dongruo Zhou and Quanquan Gu", "title": "Provably Efficient Reinforcement Learning with Linear Function\n  Approximation Under Adaptivity Constraints", "comments": "17 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study reinforcement learning (RL) with linear function approximation under\nthe adaptivity constraint. We consider two popular limited adaptivity models:\nbatch learning model and rare policy switch model, and propose two efficient\nonline RL algorithms for linear Markov decision processes. In specific, for the\nbatch learning model, our proposed LSVI-UCB-Batch algorithm achieves an $\\tilde\nO(\\sqrt{d^3H^3T} + dHT/B)$ regret, where $d$ is the dimension of the feature\nmapping, $H$ is the episode length, $T$ is the number of interactions and $B$\nis the number of batches. Our result suggests that it suffices to use only\n$\\sqrt{T/dH}$ batches to obtain $\\tilde O(\\sqrt{d^3H^3T})$ regret. For the rare\npolicy switch model, our proposed LSVI-UCB-RareSwitch algorithm enjoys an\n$\\tilde O(\\sqrt{d^3H^3T[1+T/(dH)]^{dH/B}})$ regret, which implies that $dH\\log\nT$ policy switches suffice to obtain the $\\tilde O(\\sqrt{d^3H^3T})$ regret. Our\nalgorithms achieve the same regret as the LSVI-UCB algorithm (Jin et al.,\n2019), yet with a substantially smaller amount of adaptivity.\n", "versions": [{"version": "v1", "created": "Wed, 6 Jan 2021 18:56:07 GMT"}], "update_date": "2021-01-07", "authors_parsed": [["Wang", "Tianhao", ""], ["Zhou", "Dongruo", ""], ["Gu", "Quanquan", ""]]}, {"id": "2101.02198", "submitter": "Cong Shen", "authors": "Xizixiang Wei and Cong Shen", "title": "Federated Learning over Noisy Channels: Convergence Analysis and Design\n  Examples", "comments": "30 pages, 9 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.LG cs.NI eess.SP math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Does Federated Learning (FL) work when both uplink and downlink\ncommunications have errors? How much communication noise can FL handle and what\nis its impact to the learning performance? This work is devoted to answering\nthese practically important questions by explicitly incorporating both uplink\nand downlink noisy channels in the FL pipeline. We present several novel\nconvergence analyses of FL over simultaneous uplink and downlink noisy\ncommunication channels, which encompass full and partial clients participation,\ndirect model and model differential transmissions, and non-independent and\nidentically distributed (IID) local datasets. These analyses characterize the\nsufficient conditions for FL over noisy channels to have the same convergence\nbehavior as the ideal case of no communication error. More specifically, in\norder to maintain the O(1/T) convergence rate of FedAvg with perfect\ncommunications, the uplink and downlink signal-to-noise ratio (SNR) for direct\nmodel transmissions should be controlled such that they scale as O(t^2) where t\nis the index of communication rounds, but can stay constant for model\ndifferential transmissions. The key insight of these theoretical results is a\n\"flying under the radar\" principle - stochastic gradient descent (SGD) is an\ninherent noisy process and uplink/downlink communication noises can be\ntolerated as long as they do not dominate the time-varying SGD noise. We\nexemplify these theoretical findings with two widely adopted communication\ntechniques - transmit power control and diversity combining - and further\nvalidating their performance advantages over the standard methods via extensive\nnumerical experiments using several real-world FL tasks.\n", "versions": [{"version": "v1", "created": "Wed, 6 Jan 2021 18:57:39 GMT"}], "update_date": "2021-01-07", "authors_parsed": [["Wei", "Xizixiang", ""], ["Shen", "Cong", ""]]}, {"id": "2101.02230", "submitter": "Kaige Yang Mr", "authors": "Kaige Yang", "title": "Learn Dynamic-Aware State Embedding for Transfer Learning", "comments": "11 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Transfer reinforcement learning aims to improve the sample efficiency of\nsolving unseen new tasks by leveraging experiences obtained from previous\ntasks. We consider the setting where all tasks (MDPs) share the same\nenvironment dynamic except reward function. In this setting, the MDP dynamic is\na good knowledge to transfer, which can be inferred by uniformly random policy.\nHowever, trajectories generated by uniform random policy are not useful for\npolicy improvement, which impairs the sample efficiency severely. Instead, we\nobserve that the binary MDP dynamic can be inferred from trajectories of any\npolicy which avoids the need of uniform random policy. As the binary MDP\ndynamic contains the state structure shared over all tasks we believe it is\nsuitable to transfer. Built on this observation, we introduce a method to infer\nthe binary MDP dynamic on-line and at the same time utilize it to guide state\nembedding learning, which is then transferred to new tasks. We keep state\nembedding learning and policy learning separately. As a result, the learned\nstate embedding is task and policy agnostic which makes it ideal for transfer\nlearning. In addition, to facilitate the exploration over the state space, we\npropose a novel intrinsic reward based on the inferred binary MDP dynamic. Our\nmethod can be used out-of-box in combination with model-free RL algorithms. We\nshow two instances on the basis of \\algo{DQN} and \\algo{A2C}. Empirical results\nof intensive experiments show the advantage of our proposed method in various\ntransfer learning tasks.\n", "versions": [{"version": "v1", "created": "Wed, 6 Jan 2021 19:07:31 GMT"}], "update_date": "2021-01-08", "authors_parsed": [["Yang", "Kaige", ""]]}, {"id": "2101.02231", "submitter": "Seyed Sajjadi", "authors": "Volkan Ustun, Paul S. Rosenbloom, Seyed Sajjadi, Jeremy Nuttal", "title": "Controlling Synthetic Characters in Simulations: A Case for Cognitive\n  Architectures and Sigma", "comments": null, "journal-ref": "Interservice/Industry Training, Simulation, and Education\n  Conference (I/ITSEC) 2018", "doi": null, "report-no": null, "categories": "cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Simulations, along with other similar applications like virtual worlds and\nvideo games, require computational models of intelligence that generate\nrealistic and credible behavior for the participating synthetic characters.\nCognitive architectures, which are models of the fixed structure underlying\nintelligent behavior in both natural and artificial systems, provide a\nconceptually valid common basis, as evidenced by the current efforts towards a\nstandard model of the mind, to generate human-like intelligent behavior for\nthese synthetic characters. Sigma is a cognitive architecture and system that\nstrives to combine what has been learned from four decades of independent work\non symbolic cognitive architectures, probabilistic graphical models, and more\nrecently neural models, under its graphical architecture hypothesis. Sigma\nleverages an extended form of factor graphs towards a uniform grand unification\nof not only traditional cognitive capabilities but also key non-cognitive\naspects, creating unique opportunities for the construction of new kinds of\ncognitive models that possess a Theory-of-Mind and that are perceptual,\nautonomous, interactive, affective, and adaptive. In this paper, we will\nintroduce Sigma along with its diverse capabilities and then use three distinct\nproof-of-concept Sigma models to highlight combinations of these capabilities:\n(1) Distributional reinforcement learning models in; (2) A pair of adaptive and\ninteractive agent models that demonstrate rule-based, probabilistic, and social\nreasoning; and (3) A knowledge-free exploration model in which an agent\nleverages only architectural appraisal variables, namely attention and\ncuriosity, to locate an item while building up a map in a Unity environment.\n", "versions": [{"version": "v1", "created": "Wed, 6 Jan 2021 19:07:36 GMT"}], "update_date": "2021-01-08", "authors_parsed": [["Ustun", "Volkan", ""], ["Rosenbloom", "Paul S.", ""], ["Sajjadi", "Seyed", ""], ["Nuttal", "Jeremy", ""]]}, {"id": "2101.02244", "submitter": "Anamaria Crisan", "authors": "Anamaria Crisan, Michael Correll", "title": "User Ex Machina : Simulation as a Design Probe in Human-in-the-Loop Text\n  Analytics", "comments": "16 Pages, 9 Figures, CHI 2021 Conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.CL cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Topic models are widely used analysis techniques for clustering documents and\nsurfacing thematic elements of text corpora. These models remain challenging to\noptimize and often require a \"human-in-the-loop\" approach where domain experts\nuse their knowledge to steer and adjust. However, the fragility,\nincompleteness, and opacity of these models means even minor changes could\ninduce large and potentially undesirable changes in resulting model. In this\npaper we conduct a simulation-based analysis of human-centered interactions\nwith topic models, with the objective of measuring the sensitivity of topic\nmodels to common classes of user actions. We find that user interactions have\nimpacts that differ in magnitude but often negatively affect the quality of the\nresulting modelling in a way that can be difficult for the user to evaluate. We\nsuggest the incorporation of sensitivity and \"multiverse\" analyses to topic\nmodel interfaces to surface and overcome these deficiencies.\n", "versions": [{"version": "v1", "created": "Wed, 6 Jan 2021 19:44:11 GMT"}], "update_date": "2021-01-08", "authors_parsed": [["Crisan", "Anamaria", ""], ["Correll", "Michael", ""]]}, {"id": "2101.02264", "submitter": "Clemente Rubio-Manzano", "authors": "Clemente Rubio-Manzano, Tomas Lermanda, CLaudia Martinez, Alejandra\n  Segura, Christian Vidal", "title": "Teach me to play, gamer! Imitative learning in computer games via\n  linguistic description of complex phenomena and decision tree", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this article, we present a new machine learning model by imitation based\non the linguistic description of complex phenomena. The idea consists of,\nfirst, capturing the behaviour of human players by creating a computational\nperception network based on the execution traces of the games and, second,\nrepresenting it using fuzzy logic (linguistic variables and if-then rules).\nFrom this knowledge, a set of data (dataset) is automatically created to\ngenerate a learning model based on decision trees. This model will be used\nlater to automatically control the movements of a bot. The result is an\nartificial agent that mimics the human player. We have implemented, tested and\nevaluated this technology. The results obtained are interesting and promising,\nshowing that this method can be a good alternative to design and implement the\nbehaviour of intelligent agents in video game development.\n", "versions": [{"version": "v1", "created": "Wed, 6 Jan 2021 21:14:10 GMT"}], "update_date": "2021-01-08", "authors_parsed": [["Rubio-Manzano", "Clemente", ""], ["Lermanda", "Tomas", ""], ["Martinez", "CLaudia", ""], ["Segura", "Alejandra", ""], ["Vidal", "Christian", ""]]}, {"id": "2101.02284", "submitter": "Ashwinkumar Badanidiyuru", "authors": "Ashwinkumar Badanidiyuru, Andrew Evdokimov, Vinodh Krishnan, Pan Li,\n  Wynn Vonnegut, Jayden Wang", "title": "Handling many conversions per click in modeling delayed feedback", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Predicting the expected value or number of post-click conversions (purchases\nor other events) is a key task in performance-based digital advertising. In\ntraining a conversion optimizer model, one of the most crucial aspects is\nhandling delayed feedback with respect to conversions, which can happen\nmultiple times with varying delay. This task is difficult, as the delay\ndistribution is different for each advertiser, is long-tailed, often does not\nfollow any particular class of parametric distributions, and can change over\ntime. We tackle these challenges using an unbiased estimation model based on\nthree core ideas. The first idea is to split the label as a sum of labels with\ndifferent delay buckets, each of which trains only on mature label, the second\nis to use thermometer encoding to increase accuracy and reduce inference cost,\nand the third is to use auxiliary information to increase the stability of the\nmodel and to handle drift in the distribution.\n", "versions": [{"version": "v1", "created": "Wed, 6 Jan 2021 21:57:51 GMT"}], "update_date": "2021-01-08", "authors_parsed": [["Badanidiyuru", "Ashwinkumar", ""], ["Evdokimov", "Andrew", ""], ["Krishnan", "Vinodh", ""], ["Li", "Pan", ""], ["Vonnegut", "Wynn", ""], ["Wang", "Jayden", ""]]}, {"id": "2101.02287", "submitter": "Arash Mohammadi", "authors": "Farnoush Ronaghi, Mohammad Salimibeni, Farnoosh Naderkhani, and Arash\n  Mohammadi", "title": "COVID19-HPSMP: COVID-19 Adopted Hybrid and Parallel Deep Information\n  Fusion Framework for Stock Price Movement Prediction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.ST cs.LG eess.SP q-fin.RM", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The novel of coronavirus (COVID-19) has suddenly and abruptly changed the\nworld as we knew at the start of the 3rd decade of the 21st century.\nParticularly, COVID-19 pandemic has negatively affected financial econometrics\nand stock markets across the globe. Artificial Intelligence (AI) and Machine\nLearning (ML)-based prediction models, especially Deep Neural Network (DNN)\narchitectures, have the potential to act as a key enabling factor to reduce the\nadverse effects of the COVID-19 pandemic and future possible ones on financial\nmarkets. In this regard, first, a unique COVID-19 related PRIce MOvement\nprediction (COVID19 PRIMO) dataset is introduced in this paper, which\nincorporates effects of social media trends related to COVID-19 on stock market\nprice movements. Afterwards, a novel hybrid and parallel DNN-based framework is\nproposed that integrates different and diversified learning architectures.\nReferred to as the COVID-19 adopted Hybrid and Parallel deep fusion framework\nfor Stock price Movement Prediction (COVID19-HPSMP), innovative fusion\nstrategies are used to combine scattered social media news related to COVID-19\nwith historical mark data. The proposed COVID19-HPSMP consists of two parallel\npaths (hence hybrid), one based on Convolutional Neural Network (CNN) with\nLocal/Global Attention modules, and one integrated CNN and Bi-directional Long\nShort term Memory (BLSTM) path. The two parallel paths are followed by a\nmultilayer fusion layer acting as a fusion centre that combines localized\nfeatures. Performance evaluations are performed based on the introduced COVID19\nPRIMO dataset illustrating superior performance of the proposed framework.\n", "versions": [{"version": "v1", "created": "Sat, 2 Jan 2021 15:55:19 GMT"}, {"version": "v2", "created": "Thu, 8 Jul 2021 17:59:17 GMT"}], "update_date": "2021-07-09", "authors_parsed": [["Ronaghi", "Farnoush", ""], ["Salimibeni", "Mohammad", ""], ["Naderkhani", "Farnoosh", ""], ["Mohammadi", "Arash", ""]]}, {"id": "2101.02289", "submitter": "Joaquin Vanschoren", "authors": "Jeroen van Hoof, Joaquin Vanschoren", "title": "Hyperboost: Hyperparameter Optimization by Gradient Boosting surrogate\n  models", "comments": "ECMLPKDD 2019 Workshop on Automating Data Science", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Bayesian Optimization is a popular tool for tuning algorithms in automatic\nmachine learning (AutoML) systems. Current state-of-the-art methods leverage\nRandom Forests or Gaussian processes to build a surrogate model that predicts\nalgorithm performance given a certain set of hyperparameter settings. In this\npaper, we propose a new surrogate model based on gradient boosting, where we\nuse quantile regression to provide optimistic estimates of the performance of\nan unobserved hyperparameter setting, and combine this with a distance metric\nbetween unobserved and observed hyperparameter settings to help regulate\nexploration. We demonstrate empirically that the new method is able to\noutperform some state-of-the art techniques across a reasonable sized set of\nclassification problems.\n", "versions": [{"version": "v1", "created": "Wed, 6 Jan 2021 22:07:19 GMT"}], "update_date": "2021-01-08", "authors_parsed": [["van Hoof", "Jeroen", ""], ["Vanschoren", "Joaquin", ""]]}, {"id": "2101.02305", "submitter": "Maryam Motamedi", "authors": "Maryam Motamedi, Na Li, Douglas G. Down and Nancy M. Heddle", "title": "Demand Forecasting for Platelet Usage: from Univariate Time Series to\n  Multivariate Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.AP stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Platelet products are both expensive and have very short shelf lives. As\nusage rates for platelets are highly variable, the effective management of\nplatelet demand and supply is very important yet challenging. The primary goal\nof this paper is to present an efficient forecasting model for platelet demand\nat Canadian Blood Services (CBS). To accomplish this goal, four different\ndemand forecasting methods, ARIMA (Auto Regressive Moving Average), Prophet,\nlasso regression (least absolute shrinkage and selection operator) and LSTM\n(Long Short-Term Memory) networks are utilized and evaluated. We use a large\nclinical dataset for a centralized blood distribution centre for four hospitals\nin Hamilton, Ontario, spanning from 2010 to 2018 and consisting of daily\nplatelet transfusions along with information such as the product\nspecifications, the recipients' characteristics, and the recipients' laboratory\ntest results. This study is the first to utilize different methods from\nstatistical time series models to data-driven regression and a machine learning\ntechnique for platelet transfusion using clinical predictors and with different\namounts of data. We find that the multivariate approaches have the highest\naccuracy in general, however, if sufficient data are available, a simpler time\nseries approach such as ARIMA appears to be sufficient. We also comment on the\napproach to choose clinical indicators (inputs) for the multivariate models.\n", "versions": [{"version": "v1", "created": "Wed, 6 Jan 2021 23:54:10 GMT"}], "update_date": "2021-01-12", "authors_parsed": [["Motamedi", "Maryam", ""], ["Li", "Na", ""], ["Down", "Douglas G.", ""], ["Heddle", "Nancy M.", ""]]}, {"id": "2101.02307", "submitter": "Huan Qing", "authors": "Huan Qing and Jingli Wang", "title": "Bipartite mixed membership stochastic blockmodel", "comments": "24 pages, 5 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mixed membership problem for undirected network has been well studied in\nnetwork analysis recent years. However, the more general case of mixed\nmembership for directed network remains a challenge. Here, we propose an\ninterpretable model: bipartite mixed membership stochastic blockmodel (BiMMSB\nfor short) for directed mixed membership networks. BiMMSB allows that row nodes\nand column nodes of the adjacency matrix can be different and these nodes may\nhave distinct community structure in a directed network. We also develop an\nefficient spectral algorithm called BiMPCA to estimate the mixed memberships\nfor both row nodes and column nodes in a directed network. We show that the\napproach is asymptotically consistent under BiMMSB. We demonstrate the\nadvantages of BiMMSB with applications to a small-scale simulation study, the\ndirected Political blogs network and the Papers Citations network.\n", "versions": [{"version": "v1", "created": "Thu, 7 Jan 2021 00:21:50 GMT"}], "update_date": "2021-01-08", "authors_parsed": [["Qing", "Huan", ""], ["Wang", "Jingli", ""]]}, {"id": "2101.02308", "submitter": "Baoqian Wang", "authors": "Baoqian Wang, Junfei Xie, Nikolay Atanasov", "title": "Coding for Distributed Multi-Agent Reinforcement Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper aims to mitigate straggler effects in synchronous distributed\nlearning for multi-agent reinforcement learning (MARL) problems. Stragglers\narise frequently in a distributed learning system, due to the existence of\nvarious system disturbances such as slow-downs or failures of compute nodes and\ncommunication bottlenecks. To resolve this issue, we propose a coded\ndistributed learning framework, which speeds up the training of MARL algorithms\nin the presence of stragglers, while maintaining the same accuracy as the\ncentralized approach. As an illustration, a coded distributed version of the\nmulti-agent deep deterministic policy gradient(MADDPG) algorithm is developed\nand evaluated. Different coding schemes, including maximum distance separable\n(MDS)code, random sparse code, replication-based code, and regular low density\nparity check (LDPC) code are also investigated. Simulations in several\nmulti-robot problems demonstrate the promising performance of the proposed\nframework.\n", "versions": [{"version": "v1", "created": "Thu, 7 Jan 2021 00:22:34 GMT"}], "update_date": "2021-01-08", "authors_parsed": [["Wang", "Baoqian", ""], ["Xie", "Junfei", ""], ["Atanasov", "Nikolay", ""]]}, {"id": "2101.02313", "submitter": "Daniel Clouse", "authors": "Daniel J. Clouse", "title": "A Note on Rough Set Algebra and Core Regular Double Stone Algebras", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.RA cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Given an approximation space $\\langle U,\\theta \\rangle$, assume that $E$ is\nthe indexing set for the equivalence classes of $\\theta$ and let $R_\\theta$\ndenote the collection of rough sets of the form\n$\\langle\\underline{X},\\overline{X}\\rangle$ as a regular double Stone algebra\nand what I. Dunstch referred to as a Katrinak algebra.[7],[8] We give an\nalternate proof from the one given in [1] of the fact that if $|\\theta_u| > 1\\\n\\forall\\ u \\in U$ then $R_\\theta$ is a core regular double Stone algebra.\nFurther let $C_3$ denote the 3 element chain as a core regular double Stone\nalgebra and $TP_U$ denote the collection of ternary partitions over the set\n$U$. In our Main Theorem we show $R_\\theta$ with $|\\theta_u| > 1\\ \\forall\\ u\n\\in U$ to be isomorphic to $TP_E$ and $C_3^E$, with $E$ is an indexing set for\n$\\theta$, and that the three CRDSA's are complete and atomic. We feel this\ncould be very useful when dealing with a specific $R_\\theta$ in an application.\nIn our Main Corollary we show explicitly how we can embed such $R_\\theta$ in\n$TP_U$, $C_3^U$, respectively, $\\phi\\circ \\alpha_r:R_\\theta\\hookrightarrow\nTP_U\\hookrightarrow C_3^U$, and hence identify it with its specific images.\nFollowing in the footsteps of Theorem 3. and Corollary 2.4 of [7], we show\n$C_3^J \\cong R_\\theta$ for $\\langle U,\\theta \\rangle$ the approximation space\ngiven by $U = J \\times \\{0,1\\}$, $\\theta = \\{(j0),(j1)\\} : j \\in J\\}$ and every\nCRDSA is isomorphic to a subalgebra of a principal rough set algebra,\n$R_\\theta$, for some approximation space $\\langle U,\\theta \\rangle$. Finally,\nwe demonstrate this and our Main Theorem by expanding an example from [1].\nFurther, we know a little more about the subalgebras of $TP_U$ and $C_3^U$ in\ngeneral as they must exist for every $E$ that is an indexing set for the\nequivalence classes of any equivalence relation $\\theta$ on $U$ satisfying\n$|\\theta_u| > 1\\ \\forall\\ u \\in U$.\n", "versions": [{"version": "v1", "created": "Thu, 7 Jan 2021 00:32:03 GMT"}, {"version": "v2", "created": "Mon, 18 Jan 2021 18:57:26 GMT"}], "update_date": "2021-01-19", "authors_parsed": [["Clouse", "Daniel J.", ""]]}, {"id": "2101.02316", "submitter": "Jonathan Jiang", "authors": "Zhihui Kong, Jonathan H. Jiang, Zong-Hong Zhu, Kristen A. Fahy, Remo\n  Burn", "title": "Analyzing the Stability of Non-coplanar Circumbinary Planets using\n  Machine Learning", "comments": "This manuscript has been rejected by the journal", "journal-ref": null, "doi": null, "report-no": null, "categories": "astro-ph.EP cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Exoplanet detection in the past decade by efforts including NASA's Kepler and\nTESS missions has discovered many worlds that differ substantially from planets\nin our own Solar system, including more than 400 exoplanets orbiting binary or\nmulti-star systems. This not only broadens our understanding of the diversity\nof exoplanets, but also promotes our study of exoplanets in the complex binary\nand multi-star systems and provides motivation to explore their habitability.\nIn this study, we analyze orbital stability of exoplanets in non-coplanar\ncircumbinary systems using a numerical simulation method, with which a large\nnumber of circumbinary planet samples are generated in order to quantify the\neffects of various orbital parameters on orbital stability. We also train a\nmachine learning model that can quickly determine the stability of the\ncircumbinary planetary systems. Our results indicate that larger inclinations\nof the planet tend to increase the stability of its orbit, but change in the\nplanet's mass range between Earth and Jupiter has little effect on the\nstability of the system. In addition, we find that Deep Neural Networks (DNNs)\nhave higher accuracy and precision than other machine learning algorithms.\n", "versions": [{"version": "v1", "created": "Thu, 7 Jan 2021 00:59:31 GMT"}, {"version": "v2", "created": "Mon, 28 Jun 2021 21:13:42 GMT"}], "update_date": "2021-06-30", "authors_parsed": [["Kong", "Zhihui", ""], ["Jiang", "Jonathan H.", ""], ["Zhu", "Zong-Hong", ""], ["Fahy", "Kristen A.", ""], ["Burn", "Remo", ""]]}, {"id": "2101.02326", "submitter": "Tian Xie", "authors": "Tian Xie, Bin Wang, C.-C. Jay Kuo", "title": "GraphHop: An Enhanced Label Propagation Method for Node Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  A scalable semi-supervised node classification method on graph-structured\ndata, called GraphHop, is proposed in this work. The graph contains attributes\nof all nodes but labels of a few nodes. The classical label propagation (LP)\nmethod and the emerging graph convolutional network (GCN) are two popular\nsemi-supervised solutions to this problem. The LP method is not effective in\nmodeling node attributes and labels jointly or facing a slow convergence rate\non large-scale graphs. GraphHop is proposed to its shortcoming. With proper\ninitial label vector embeddings, each iteration of GraphHop contains two steps:\n1) label aggregation and 2) label update. In Step 1, each node aggregates its\nneighbors' label vectors obtained in the previous iteration. In Step 2, a new\nlabel vector is predicted for each node based on the label of the node itself\nand the aggregated label information obtained in Step 1. This iterative\nprocedure exploits the neighborhood information and enables GraphHop to perform\nwell in an extremely small label rate setting and scale well for very large\ngraphs. Experimental results show that GraphHop outperforms state-of-the-art\ngraph learning methods on a wide range of tasks (e.g., multi-label and\nmulti-class classification on citation networks, social graphs, and commodity\nconsumption graphs) in graphs of various sizes. Our codes are publicly\navailable on GitHub (https://github.com/TianXieUSC/GraphHop).\n", "versions": [{"version": "v1", "created": "Thu, 7 Jan 2021 02:10:20 GMT"}], "update_date": "2021-01-08", "authors_parsed": [["Xie", "Tian", ""], ["Wang", "Bin", ""], ["Kuo", "C. -C. Jay", ""]]}, {"id": "2101.02327", "submitter": "Baogang Hu", "authors": "Bao-Gang Hu and Wei-Ming Dong", "title": "A design of human-like robust AI machines in object identification", "comments": "6 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This is a perspective paper inspired from the study of Turing Test proposed\nby A.M. Turing (23 June 1912 - 7 June 1954) in 1950. Following one important\nimplication of Turing Test for enabling a machine with a human-like behavior or\nperformance, we define human-like robustness (HLR) for AI machines. The\nobjective of the new definition aims to enforce AI machines with HLR, including\nto evaluate them in terms of HLR. A specific task is discussed only on object\nidentification, because it is the most common task for every person in daily\nlife. Similar to the perspective, or design, position by Turing, we provide a\nsolution of how to achieve HLR AI machines without constructing them and\nconducting real experiments. The solution should consists of three important\nfeatures in the machines. The first feature of HLR machines is to utilize\ncommon sense from humans for realizing a causal inference. The second feature\nis to make a decision from a semantic space for having interpretations to the\ndecision. The third feature is to include a \"human-in-the-loop\" setting for\nadvancing HLR machines. We show an \"identification game\" using proposed design\nof HLR machines. The present paper shows an attempt to learn and explore\nfurther from Turing Test towards the design of human-like AI machines.\n", "versions": [{"version": "v1", "created": "Thu, 7 Jan 2021 02:11:45 GMT"}], "update_date": "2021-01-08", "authors_parsed": [["Hu", "Bao-Gang", ""], ["Dong", "Wei-Ming", ""]]}, {"id": "2101.02330", "submitter": "Matthew Davidow", "authors": "Matthew Davidow, David Matteson", "title": "Copula Quadrant Similarity for Anomaly Scores", "comments": "17 pages, 11 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Practical anomaly detection requires applying numerous approaches due to the\ninherent difficulty of unsupervised learning. Direct comparison between complex\nor opaque anomaly detection algorithms is intractable; we instead propose a\nframework for associating the scores of multiple methods. Our aim is to answer\nthe question: how should one measure the similarity between anomaly scores\ngenerated by different methods? The scoring crux is the extremes, which\nidentify the most anomalous observations. A pair of algorithms are defined here\nto be similar if they assign their highest scores to roughly the same small\nfraction of observations. To formalize this, we propose a measure based on\nextremal similarity in scoring distributions through a novel upper quadrant\nmodeling approach, and contrast it with tail and other dependence measures. We\nillustrate our method with simulated and real experiments, applying spectral\nmethods to cluster multiple anomaly detection methods and to contrast our\nsimilarity measure with others. We demonstrate that our method is able to\ndetect the clusters of anomaly detection algorithms to achieve an accurate and\nrobust ensemble algorithm.\n", "versions": [{"version": "v1", "created": "Thu, 7 Jan 2021 02:19:36 GMT"}], "update_date": "2021-01-08", "authors_parsed": [["Davidow", "Matthew", ""], ["Matteson", "David", ""]]}, {"id": "2101.02332", "submitter": "Boris Hayete", "authors": "Boris Hayete, Fred Gruber, Anna Decker, Raymond Yan", "title": "Identification of Latent Variables From Graphical Model Residuals", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG q-bio.QM", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Graph-based causal discovery methods aim to capture conditional\nindependencies consistent with the observed data and differentiate causal\nrelationships from indirect or induced ones. Successful construction of\ngraphical models of data depends on the assumption of causal sufficiency: that\nis, that all confounding variables are measured. When this assumption is not\nmet, learned graphical structures may become arbitrarily incorrect and effects\nimplied by such models may be wrongly attributed, carry the wrong magnitude, or\nmis-represent direction of correlation. Wide application of graphical models to\nincreasingly less curated \"big data\" draws renewed attention to the unobserved\nconfounder problem.\n  We present a novel method that aims to control for the latent space when\nestimating a DAG by iteratively deriving proxies for the latent space from the\nresiduals of the inferred model. Under mild assumptions, our method improves\nstructural inference of Gaussian graphical models and enhances identifiability\nof the causal effect. In addition, when the model is being used to predict\noutcomes, it un-confounds the coefficients on the parents of the outcomes and\nleads to improved predictive performance when out-of-sample regime is very\ndifferent from the training data. We show that any improvement of prediction of\nan outcome is intrinsically capped and cannot rise beyond a certain limit as\ncompared to the confounded model. We extend our methodology beyond GGMs to\nordinal variables and nonlinear cases. Our R package provides both PCA and\nautoencoder implementations of the methodology, suitable for GGMs with some\nguarantees and for better performance in general cases but without such\nguarantees.\n", "versions": [{"version": "v1", "created": "Thu, 7 Jan 2021 02:28:49 GMT"}], "update_date": "2021-01-08", "authors_parsed": [["Hayete", "Boris", ""], ["Gruber", "Fred", ""], ["Decker", "Anna", ""], ["Yan", "Raymond", ""]]}, {"id": "2101.02333", "submitter": "Er-Dong Guo", "authors": "Erdong Guo and David Draper", "title": "Infinitely Wide Tensor Networks as Gaussian Process", "comments": "20 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Gaussian Process is a non-parametric prior which can be understood as a\ndistribution on the function space intuitively. It is known that by introducing\nappropriate prior to the weights of the neural networks, Gaussian Process can\nbe obtained by taking the infinite-width limit of the Bayesian neural networks\nfrom a Bayesian perspective. In this paper, we explore the infinitely wide\nTensor Networks and show the equivalence of the infinitely wide Tensor Networks\nand the Gaussian Process. We study the pure Tensor Network and another two\nextended Tensor Network structures: Neural Kernel Tensor Network and Tensor\nNetwork hidden layer Neural Network and prove that each one will converge to\nthe Gaussian Process as the width of each model goes to infinity. (We note here\nthat Gaussian Process can also be obtained by taking the infinite limit of at\nleast one of the bond dimensions $\\alpha_{i}$ in the product of tensor nodes,\nand the proofs can be done with the same ideas in the proofs of the\ninfinite-width cases.) We calculate the mean function (mean vector) and the\ncovariance function (covariance matrix) of the finite dimensional distribution\nof the induced Gaussian Process by the infinite-width tensor network with a\ngeneral set-up. We study the properties of the covariance function and derive\nthe approximation of the covariance function when the integral in the\nexpectation operator is intractable. In the numerical experiments, we implement\nthe Gaussian Process corresponding to the infinite limit tensor networks and\nplot the sample paths of these models. We study the hyperparameters and plot\nthe sample path families in the induced Gaussian Process by varying the\nstandard deviations of the prior distributions. As expected, the parameters in\nthe prior distribution namely the hyper-parameters in the induced Gaussian\nProcess controls the characteristic lengthscales of the Gaussian Process.\n", "versions": [{"version": "v1", "created": "Thu, 7 Jan 2021 02:29:15 GMT"}], "update_date": "2021-01-08", "authors_parsed": [["Guo", "Erdong", ""], ["Draper", "David", ""]]}, {"id": "2101.02337", "submitter": "Dave Epstein", "authors": "Dave Epstein, Jiajun Wu, Cordelia Schmid, Chen Sun", "title": "Learning Temporal Dynamics from Cycles in Narrated Video", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning to model how the world changes as time elapses has proven a\nchallenging problem for the computer vision community. We propose a\nself-supervised solution to this problem using temporal cycle consistency\njointly in vision and language, training on narrated video. Our model learns\nmodality-agnostic functions to predict forward and backward in time, which must\nundo each other when composed. This constraint leads to the discovery of\nhigh-level transitions between moments in time, since such transitions are\neasily inverted and shared across modalities. We justify the design of our\nmodel with an ablation study on different configurations of the cycle\nconsistency problem. We then show qualitatively and quantitatively that our\napproach yields a meaningful, high-level model of the future and past. We apply\nthe learned dynamics model without further training to various tasks, such as\npredicting future action and temporally ordering sets of images.\n", "versions": [{"version": "v1", "created": "Thu, 7 Jan 2021 02:41:32 GMT"}], "update_date": "2021-01-08", "authors_parsed": [["Epstein", "Dave", ""], ["Wu", "Jiajun", ""], ["Schmid", "Cordelia", ""], ["Sun", "Chen", ""]]}, {"id": "2101.02338", "submitter": "Haoran You", "authors": "Randall Balestriero, Haoran You, Zhihan Lu, Yutong Kou, Huihong Shi,\n  Yingyan Lin, Richard Baraniuk", "title": "Max-Affine Spline Insights Into Deep Network Pruning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper, we study the importance of pruning in Deep Networks (DNs) and\nthe yin & yang relationship between (1) pruning highly overparametrized DNs\nthat have been trained from random initialization and (2) training small DNs\nthat have been \"cleverly\" initialized. As in most cases practitioners can only\nresort to random initialization, there is a strong need to develop a grounded\nunderstanding of DN pruning. Current literature remains largely empirical,\nlacking a theoretical understanding of how pruning affects DNs' decision\nboundary, how to interpret pruning, and how to design corresponding principled\npruning techniques. To tackle those questions, we propose to employ recent\nadvances in the theoretical analysis of Continuous Piecewise Affine (CPA) DNs.\nFrom this perspective, we will be able to detect the early-bird (EB) ticket\nphenomenon, provide interpretability into current pruning techniques, and\ndevelop a principled pruning strategy. In each step of our study, we conduct\nextensive experiments supporting our claims and results; while our main goal is\nto enhance the current understanding towards DN pruning instead of developing a\nnew pruning method, our spline pruning criteria in terms of layerwise and\nglobal pruning is on par with or even outperforms state-of-the-art pruning\nmethods.\n", "versions": [{"version": "v1", "created": "Thu, 7 Jan 2021 02:42:16 GMT"}, {"version": "v2", "created": "Sat, 13 Feb 2021 01:21:40 GMT"}], "update_date": "2021-02-16", "authors_parsed": [["Balestriero", "Randall", ""], ["You", "Haoran", ""], ["Lu", "Zhihan", ""], ["Kou", "Yutong", ""], ["Shi", "Huihong", ""], ["Lin", "Yingyan", ""], ["Baraniuk", "Richard", ""]]}, {"id": "2101.02342", "submitter": "Zhabiz Gharibshah", "authors": "Zhabiz Gharibshah, Xingquan Zhu", "title": "User Response Prediction in Online Advertising", "comments": "ACM Computing Surveys (CSUR), 2021, preprint", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.GL cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Online advertising, as the vast market, has gained significant attention in\nvarious platforms ranging from search engines, third-party websites, social\nmedia, and mobile apps. The prosperity of online campaigns is a challenge in\nonline marketing and is usually evaluated by user response through different\nmetrics, such as clicks on advertisement (ad) creatives, subscriptions to\nproducts, purchases of items, or explicit user feedback through online surveys.\nRecent years have witnessed a significant increase in the number of studies\nusing computational approaches, including machine learning methods, for user\nresponse prediction. However, existing literature mainly focuses on\nalgorithmic-driven designs to solve specific challenges, and no comprehensive\nreview exists to answer many important questions. What are the parties involved\nin the online digital advertising eco-systems? What type of data are available\nfor user response prediction? How to predict user response in a reliable and/or\ntransparent way? In this survey, we provide a comprehensive review of user\nresponse prediction in online advertising and related recommender applications.\nOur essential goal is to provide a thorough understanding of online advertising\nplatforms, stakeholders, data availability, and typical ways of user response\nprediction. We propose a taxonomy to categorize state-of-the-art user response\nprediction methods, primarily focus on the current progress of machine learning\nmethods used in different online platforms. In addition, we also review\napplications of user response prediction, benchmark datasets, and open-source\ncodes in the field.\n", "versions": [{"version": "v1", "created": "Thu, 7 Jan 2021 03:00:44 GMT"}, {"version": "v2", "created": "Tue, 23 Feb 2021 00:50:09 GMT"}], "update_date": "2021-02-24", "authors_parsed": [["Gharibshah", "Zhabiz", ""], ["Zhu", "Xingquan", ""]]}, {"id": "2101.02344", "submitter": "Yufang Huang Dr.", "authors": "Yufang Huang, Kelly M. Axsom, John Lee, Lakshminarayanan Subramanian\n  and Yiye Zhang", "title": "DICE: Deep Significance Clustering for Outcome-Aware Stratification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present deep significance clustering (DICE), a framework for jointly\nperforming representation learning and clustering for \"outcome-aware\"\nstratification. DICE is intended to generate cluster membership that may be\nused to categorize a population by individual risk level for a targeted\noutcome. Following the representation learning and clustering steps, we embed\nthe objective function in DICE with a constraint which requires a statistically\nsignificant association between the outcome and cluster membership of learned\nrepresentations. DICE further includes a neural architecture search step to\nmaximize both the likelihood of representation learning and outcome\nclassification accuracy with cluster membership as the predictor. To\ndemonstrate its utility in medicine for patient risk-stratification, the\nperformance of DICE was evaluated using two datasets with different outcome\nratios extracted from real-world electronic health records. Outcomes are\ndefined as acute kidney injury (30.4\\%) among a cohort of COVID-19 patients,\nand discharge disposition (36.8\\%) among a cohort of heart failure patients,\nrespectively. Extensive results demonstrate that DICE has superior performance\nas measured by the difference in outcome distribution across clusters,\nSilhouette score, Calinski-Harabasz index, and Davies-Bouldin index for\nclustering, and Area under the ROC Curve (AUC) for outcome classification\ncompared to several baseline approaches.\n", "versions": [{"version": "v1", "created": "Thu, 7 Jan 2021 03:06:52 GMT"}], "update_date": "2021-01-08", "authors_parsed": [["Huang", "Yufang", ""], ["Axsom", "Kelly M.", ""], ["Lee", "John", ""], ["Subramanian", "Lakshminarayanan", ""], ["Zhang", "Yiye", ""]]}, {"id": "2101.02358", "submitter": "SungKwon An", "authors": "Sungkwon An, Jeonghoon Kim, Myungjoo Kang, Shahbaz Razaei and Xin Liu", "title": "OAAE: Adversarial Autoencoders for Novelty Detection in Multi-modal\n  Normality Case via Orthogonalized Latent Space", "comments": "Accepted to AAAI 2021 Workshop: Towards Robust, Secure and Efficient\n  Machine Learning", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Novelty detection using deep generative models such as autoencoder,\ngenerative adversarial networks mostly takes image reconstruction error as\nnovelty score function. However, image data, high dimensional as it is,\ncontains a lot of different features other than class information which makes\nmodels hard to detect novelty data. The problem gets harder in multi-modal\nnormality case. To address this challenge, we propose a new way of measuring\nnovelty score in multi-modal normality cases using orthogonalized latent space.\nSpecifically, we employ orthogonal low-rank embedding in the latent space to\ndisentangle the features in the latent space using mutual class information.\nWith the orthogonalized latent space, novelty score is defined by the change of\neach latent vector. Proposed algorithm was compared to state-of-the-art novelty\ndetection algorithms using GAN such as RaPP and OCGAN, and experimental results\nshow that ours outperforms those algorithms.\n", "versions": [{"version": "v1", "created": "Thu, 7 Jan 2021 03:59:47 GMT"}], "update_date": "2021-01-08", "authors_parsed": [["An", "Sungkwon", ""], ["Kim", "Jeonghoon", ""], ["Kang", "Myungjoo", ""], ["Razaei", "Shahbaz", ""], ["Liu", "Xin", ""]]}, {"id": "2101.02359", "submitter": "Xiangyang Li", "authors": "Xiangyang Li, Yu Xia, Xiang Long, Zheng Li, Sujian Li", "title": "Exploring Text-transformers in AAAI 2021 Shared Task: COVID-19 Fake News\n  Detection in English", "comments": "3rd solution of 'Constraint@AAAI2021 - COVID19 Fake News Detection in\n  English'", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper, we describe our system for the AAAI 2021 shared task of\nCOVID-19 Fake News Detection in English, where we achieved the 3rd position\nwith the weighted F1 score of 0.9859 on the test set. Specifically, we proposed\nan ensemble method of different pre-trained language models such as BERT,\nRoberta, Ernie, etc. with various training strategies including\nwarm-up,learning rate schedule and k-fold cross-validation. We also conduct an\nextensive analysis of the samples that are not correctly classified. The code\nis available\nat:https://github.com/archersama/3rd-solution-COVID19-Fake-News-Detection-in-English.\n", "versions": [{"version": "v1", "created": "Thu, 7 Jan 2021 04:01:13 GMT"}], "update_date": "2021-01-08", "authors_parsed": [["Li", "Xiangyang", ""], ["Xia", "Yu", ""], ["Long", "Xiang", ""], ["Li", "Zheng", ""], ["Li", "Sujian", ""]]}, {"id": "2101.02373", "submitter": "Sin Kit Lo", "authors": "Sin Kit Lo, Qinghua Lu, Liming Zhu, Hye-young Paik, Xiwei Xu, Chen\n  Wang", "title": "Architectural Patterns for the Design of Federated Learning Systems", "comments": "Resubmitted after minor revision to Elsevier's Journal of Systems and\n  Software, Special issue on Software Architecture and Artificial Intelligence", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DC cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Federated learning has received fast-growing interests from academia and\nindustry to tackle the challenges of data hungriness and privacy in machine\nlearning. A federated learning system can be viewed as a large-scale\ndistributed system with different components and stakeholders as numerous\nclient devices participate in federated learning. Designing a federated\nlearning system requires software system design thinking apart from machine\nlearning knowledge. Although much effort has been put into federated learning\nfrom the machine learning technique aspects, the software architecture design\nconcerns in building federated learning systems have been largely ignored.\nTherefore, in this paper, we present a collection of architectural patterns to\ndeal with the design challenges of federated learning systems. Architectural\npatterns present reusable solutions to a commonly occurring problem within a\ngiven context during software architecture design. The presented patterns are\nbased on the results of a systematic literature review and include three client\nmanagement patterns, four model management patterns, three model training\npatterns, and four model aggregation patterns. The patterns are associated to\nthe particular state transitions in a federated learning model lifecycle,\nserving as a guidance for effective use of the patterns in the design of\nfederated learning systems.\n", "versions": [{"version": "v1", "created": "Thu, 7 Jan 2021 05:11:09 GMT"}, {"version": "v2", "created": "Fri, 28 May 2021 05:10:21 GMT"}, {"version": "v3", "created": "Fri, 18 Jun 2021 05:09:15 GMT"}], "update_date": "2021-06-21", "authors_parsed": [["Lo", "Sin Kit", ""], ["Lu", "Qinghua", ""], ["Zhu", "Liming", ""], ["Paik", "Hye-young", ""], ["Xu", "Xiwei", ""], ["Wang", "Chen", ""]]}, {"id": "2101.02377", "submitter": "Naoto Yanai", "authors": "Nami Ashizawa, Naoto Yanai, Jason Paul Cruz, Shingo Okamura", "title": "Eth2Vec: Learning Contract-Wide Code Representations for Vulnerability\n  Detection on Ethereum Smart Contracts", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.LG cs.SE", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Ethereum smart contracts are programs that run on the Ethereum blockchain,\nand many smart contract vulnerabilities have been discovered in the past\ndecade. Many security analysis tools have been created to detect such\nvulnerabilities, but their performance decreases drastically when codes to be\nanalyzed are being rewritten. In this paper, we propose Eth2Vec, a\nmachine-learning-based static analysis tool for vulnerability detection, with\nrobustness against code rewrites in smart contracts. Existing\nmachine-learning-based static analysis tools for vulnerability detection need\nfeatures, which analysts create manually, as inputs. In contrast, Eth2Vec\nautomatically learns features of vulnerable Ethereum Virtual Machine (EVM)\nbytecodes with tacit knowledge through a neural network for language\nprocessing. Therefore, Eth2Vec can detect vulnerabilities in smart contracts by\ncomparing the code similarity between target EVM bytecodes and the EVM\nbytecodes it already learned. We conducted experiments with existing open\ndatabases, such as Etherscan, and our results show that Eth2Vec outperforms the\nexisting work in terms of well-known metrics, i.e., precision, recall, and\nF1-score. Moreover, Eth2Vec can detect vulnerabilities even in rewritten codes.\n", "versions": [{"version": "v1", "created": "Thu, 7 Jan 2021 05:28:26 GMT"}, {"version": "v2", "created": "Fri, 8 Jan 2021 09:57:47 GMT"}], "update_date": "2021-01-11", "authors_parsed": [["Ashizawa", "Nami", ""], ["Yanai", "Naoto", ""], ["Cruz", "Jason Paul", ""], ["Okamura", "Shingo", ""]]}, {"id": "2101.02385", "submitter": "Sergio Casas", "authors": "Katie Luo, Sergio Casas, Renjie Liao, Xinchen Yan, Yuwen Xiong,\n  Wenyuan Zeng, Raquel Urtasun", "title": "Safety-Oriented Pedestrian Motion and Scene Occupancy Forecasting", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we address the important problem in self-driving of\nforecasting multi-pedestrian motion and their shared scene occupancy map,\ncritical for safe navigation. Our contributions are two-fold. First, we\nadvocate for predicting both the individual motions as well as the scene\noccupancy map in order to effectively deal with missing detections caused by\npostprocessing, e.g., confidence thresholding and non-maximum suppression.\nSecond, we propose a Scene-Actor Graph Neural Network (SA-GNN) which preserves\nthe relative spatial information of pedestrians via 2D convolution, and\ncaptures the interactions among pedestrians within the same scene, including\nthose that have not been detected, via message passing. On two large-scale\nreal-world datasets, nuScenes and ATG4D, we showcase that our scene-occupancy\npredictions are more accurate and better calibrated than those from\nstate-of-the-art motion forecasting methods, while also matching their\nperformance in pedestrian motion forecasting metrics.\n", "versions": [{"version": "v1", "created": "Thu, 7 Jan 2021 06:08:21 GMT"}], "update_date": "2021-01-08", "authors_parsed": [["Luo", "Katie", ""], ["Casas", "Sergio", ""], ["Liao", "Renjie", ""], ["Yan", "Xinchen", ""], ["Xiong", "Yuwen", ""], ["Zeng", "Wenyuan", ""], ["Urtasun", "Raquel", ""]]}, {"id": "2101.02388", "submitter": "Troy Luhman", "authors": "Eric Luhman, Troy Luhman", "title": "Knowledge Distillation in Iterative Generative Models for Improved\n  Sampling Speed", "comments": "20 pages, 13 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Iterative generative models, such as noise conditional score networks and\ndenoising diffusion probabilistic models, produce high quality samples by\ngradually denoising an initial noise vector. However, their denoising process\nhas many steps, making them 2-3 orders of magnitude slower than other\ngenerative models such as GANs and VAEs. In this paper, we establish a novel\nconnection between knowledge distillation and image generation with a technique\nthat distills a multi-step denoising process into a single step, resulting in a\nsampling speed similar to other single-step generative models. Our Denoising\nStudent generates high quality samples comparable to GANs on the CIFAR-10 and\nCelebA datasets, without adversarial training. We demonstrate that our method\nscales to higher resolutions through experiments on 256 x 256 LSUN. Code and\ncheckpoints are available at https://github.com/tcl9876/Denoising_Student\n", "versions": [{"version": "v1", "created": "Thu, 7 Jan 2021 06:12:28 GMT"}], "update_date": "2021-01-08", "authors_parsed": [["Luhman", "Eric", ""], ["Luhman", "Troy", ""]]}, {"id": "2101.02392", "submitter": "Yicheng Guo", "authors": "Yicheng Guo, Yujin Wen, Congwei Jiang, Yixin Lian, Yi Wan", "title": "Detecting Log Anomalies with Multi-Head Attention (LAMA)", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Anomaly detection is a crucial and challenging subject that has been studied\nwithin diverse research areas. In this work, we explore the task of log anomaly\ndetection (especially computer system logs and user behavior logs) by analyzing\nlogs' sequential information. We propose LAMA, a multi-head attention based\nsequential model to process log streams as template activity (event) sequences.\n  A next event prediction task is applied to train the model for anomaly\ndetection. Extensive empirical studies demonstrate that our new model\noutperforms existing log anomaly detection methods including statistical and\ndeep learning methodologies, which validate the effectiveness of our proposed\nmethod in learning sequence patterns of log data.\n", "versions": [{"version": "v1", "created": "Thu, 7 Jan 2021 06:15:59 GMT"}], "update_date": "2021-01-08", "authors_parsed": [["Guo", "Yicheng", ""], ["Wen", "Yujin", ""], ["Jiang", "Congwei", ""], ["Lian", "Yixin", ""], ["Wan", "Yi", ""]]}, {"id": "2101.02397", "submitter": "Kaustubh Yadav", "authors": "Kaustubh Yadav", "title": "A Comprehensive Study on Optimization Strategies for Gradient Descent In\n  Deep Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  One of the most important parts of Artificial Neural Networks is minimizing\nthe loss functions which tells us how good or bad our model is. To minimize\nthese losses we need to tune the weights and biases. Also to calculate the\nminimum value of a function we need gradient. And to update our weights we need\ngradient descent. But there are some problems with regular gradient descent ie.\nit is quite slow and not that accurate. This article aims to give an\nintroduction to optimization strategies to gradient descent. In addition, we\nshall also discuss the architecture of these algorithms and further\noptimization of Neural Networks in general\n", "versions": [{"version": "v1", "created": "Thu, 7 Jan 2021 06:24:55 GMT"}], "update_date": "2021-01-08", "authors_parsed": [["Yadav", "Kaustubh", ""]]}, {"id": "2101.02398", "submitter": "Rohan Saha", "authors": "Rohan Saha", "title": "Homonym Identification using BERT -- Using a Clustering Approach", "comments": null, "journal-ref": null, "doi": "10.13140/RG.2.2.29120.07681", "report-no": null, "categories": "cs.CL cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Homonym identification is important for WSD that require coarse-grained\npartitions of senses. The goal of this project is to determine whether\ncontextual information is sufficient for identifying a homonymous word. To\ncapture the context, BERT embeddings are used as opposed to Word2Vec, which\nconflates senses into one vector. SemCor is leveraged to retrieve the\nembeddings. Various clustering algorithms are applied to the embeddings.\nFinally, the embeddings are visualized in a lower-dimensional space to\nunderstand the feasibility of the clustering process.\n", "versions": [{"version": "v1", "created": "Thu, 7 Jan 2021 06:26:59 GMT"}], "update_date": "2021-01-08", "authors_parsed": [["Saha", "Rohan", ""]]}, {"id": "2101.02404", "submitter": "Mitchell Krock", "authors": "Mitchell Krock, William Kleiber, Dorit Hammerling, and Stephen Becker", "title": "Modeling massive highly-multivariate nonstationary spatial data with the\n  basis graphical lasso", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We propose a new modeling framework for highly-multivariate spatial processes\nthat synthesizes ideas from recent multiscale and spectral approaches with\ngraphical models. The basis graphical lasso writes a univariate Gaussian\nprocess as a linear combination of basis functions weighted with entries of a\nGaussian graphical vector whose graph is estimated from optimizing an $\\ell_1$\npenalized likelihood. This paper extends the setting to a multivariate Gaussian\nprocess where the basis functions are weighted with Gaussian graphical vectors.\nWe motivate a model where the basis functions represent different levels of\nresolution and the graphical vectors for each level are assumed to be\nindependent. Using an orthogonal basis grants linear complexity and memory\nusage in the number of spatial locations, the number of basis functions, and\nthe number of realizations. An additional fusion penalty encourages a\nparsimonious conditional independence structure in the multilevel graphical\nmodel. We illustrate our method on a large climate ensemble from the National\nCenter for Atmospheric Research's Community Atmosphere Model that involves 40\nspatial processes.\n", "versions": [{"version": "v1", "created": "Thu, 7 Jan 2021 07:01:54 GMT"}, {"version": "v2", "created": "Wed, 9 Jun 2021 05:15:53 GMT"}], "update_date": "2021-06-10", "authors_parsed": [["Krock", "Mitchell", ""], ["Kleiber", "William", ""], ["Hammerling", "Dorit", ""], ["Becker", "Stephen", ""]]}, {"id": "2101.02409", "submitter": "Ignacio Rodr\\'iguez Dr.", "authors": "Ignacio Rodriguez", "title": "On the Management of Type 1 Diabetes Mellitus with IoT Devices and ML\n  Techniques", "comments": "6 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  The purpose of this Conference is to present the main lines of base projects\nthat are founded on research already begun in previous years. In this sense,\nthis manuscript will present the main lines of research in Diabetes Mellitus\ntype 1 and Machine Learning techniques in an Internet of Things environment, so\nthat we can summarize the future lines to be developed as follows: data\ncollection through biosensors, massive data processing in the cloud,\ninterconnection of biodevices, local computing vs. cloud computing, and\npossibilities of machine learning techniques to predict blood glucose values,\nincluding both variable selection algorithms and predictive techniques.\n", "versions": [{"version": "v1", "created": "Thu, 7 Jan 2021 07:31:32 GMT"}], "update_date": "2021-01-08", "authors_parsed": [["Rodriguez", "Ignacio", ""]]}, {"id": "2101.02415", "submitter": "Ying Sheng", "authors": "Yichao Zhou, Ying Sheng, Nguyen Vo, Nick Edmonds, Sandeep Tata", "title": "Simplified DOM Trees for Transferable Attribute Extraction from the Web", "comments": "10 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  There has been a steady need to precisely extract structured knowledge from\nthe web (i.e. HTML documents). Given a web page, extracting a structured object\nalong with various attributes of interest (e.g. price, publisher, author, and\ngenre for a book) can facilitate a variety of downstream applications such as\nlarge-scale knowledge base construction, e-commerce product search, and\npersonalized recommendation. Considering each web page is rendered from an HTML\nDOM tree, existing approaches formulate the problem as a DOM tree node tagging\ntask. However, they either rely on computationally expensive visual feature\nengineering or are incapable of modeling the relationship among the tree nodes.\nIn this paper, we propose a novel transferable method, Simplified DOM Trees for\nAttribute Extraction (SimpDOM), to tackle the problem by efficiently retrieving\nuseful context for each node by leveraging the tree structure. We study two\nchallenging experimental settings: (i) intra-vertical few-shot extraction, and\n(ii) cross-vertical fewshot extraction with out-of-domain knowledge, to\nevaluate our approach. Extensive experiments on the SWDE public dataset show\nthat SimpDOM outperforms the state-of-the-art (SOTA) method by 1.44% on the F1\nscore. We also find that utilizing knowledge from a different vertical\n(cross-vertical extraction) is surprisingly useful and helps beat the SOTA by a\nfurther 1.37%.\n", "versions": [{"version": "v1", "created": "Thu, 7 Jan 2021 07:41:55 GMT"}], "update_date": "2021-01-08", "authors_parsed": [["Zhou", "Yichao", ""], ["Sheng", "Ying", ""], ["Vo", "Nguyen", ""], ["Edmonds", "Nick", ""], ["Tata", "Sandeep", ""]]}, {"id": "2101.02420", "submitter": "Ke He", "authors": "Le He, Ke He, Lisheng Fan, Xianfu Lei, Arumugam Nallanathan and George\n  K. Karagiannidis", "title": "Towards Optimally Efficient Tree Search with Deep Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.IT math.IT", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper investigates the classical integer least-squares problem which\nestimates integer signals from linear models. The problem is NP-hard and often\narises in diverse applications such as signal processing, bioinformatics,\ncommunications and machine learning, to name a few. Since the existing optimal\nsearch strategies involve prohibitive complexities, they are hard to be adopted\nin large-scale problems. To address this issue, we propose a general\nhyper-accelerated tree search (HATS) algorithm by employing a deep neural\nnetwork to estimate the optimal heuristic for the underlying simplified\nmemory-bounded A* algorithm, and the proposed algorithm can be easily\ngeneralized with other heuristic search algorithms. Inspired by the temporal\ndifference learning, we further propose a training strategy which enables the\nnetwork to approach the optimal heuristic precisely and consistently, thus the\nproposed algorithm can reach nearly the optimal efficiency when the estimation\nerror is small enough. Experiments show that the proposed algorithm can reach\nalmost the optimal maximum likelihood estimate performance in large-scale\nproblems, with a very low complexity in both time and space. The code of this\npaper is avaliable at https://github.com/skypitcher/hats.\n", "versions": [{"version": "v1", "created": "Thu, 7 Jan 2021 08:00:02 GMT"}, {"version": "v2", "created": "Thu, 21 Jan 2021 04:43:55 GMT"}, {"version": "v3", "created": "Thu, 11 Mar 2021 16:42:23 GMT"}, {"version": "v4", "created": "Mon, 15 Mar 2021 08:26:14 GMT"}], "update_date": "2021-03-16", "authors_parsed": [["He", "Le", ""], ["He", "Ke", ""], ["Fan", "Lisheng", ""], ["Lei", "Xianfu", ""], ["Nallanathan", "Arumugam", ""], ["Karagiannidis", "George K.", ""]]}, {"id": "2101.02424", "submitter": "Kristiaan Pelckmans", "authors": "Kristiaan Pelckmans, Moustafa Aboushady, Andreas Brosemyr", "title": "Detecting Suspicious Events in Fast Information Flows", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We describe a computational feather-light and intuitive, yet provably\nefficient algorithm, named HALFADO. HALFADO is designed for detecting\nsuspicious events in a high-frequency stream of complex entries, based on a\nrelatively small number of examples of human judgement. Operating a\nsufficiently accurate detection system is vital for {\\em assisting} teams of\nhuman experts in many different areas of the modern digital society. These\nsystems have intrinsically a far-reaching normative effect, and public\nknowledge of the workings of such technology should be a human right.\n  On a conceptual level, the present approach extends one of the most classical\nlearning algorithms for classification, inheriting its theoretical properties.\nIt however works in a semi-supervised way integrating human and computational\nintelligence. On a practical level, this algorithm transcends existing\napproaches (expert systems) by managing and boosting their performance into a\nsingle global detector.\n  We illustrate HALFADO's efficacy on two challenging applications: (1) for\ndetecting {\\em hate speech} messages in a flow of text messages gathered from a\nsocial media platform, and (2) for a Transaction Monitoring System (TMS) in\nFinTech detecting fraudulent transactions in a stream of financial\ntransactions.\n  This algorithm illustrates that - contrary to popular belief - advanced\nmethods of machine learning need not require neither advanced levels of\ncomputation power nor expensive annotation efforts.\n", "versions": [{"version": "v1", "created": "Thu, 7 Jan 2021 08:19:25 GMT"}], "update_date": "2021-01-08", "authors_parsed": [["Pelckmans", "Kristiaan", ""], ["Aboushady", "Moustafa", ""], ["Brosemyr", "Andreas", ""]]}, {"id": "2101.02429", "submitter": "Burak Bartan", "authors": "Burak Bartan, Mert Pilanci", "title": "Neural Spectrahedra and Semidefinite Lifts: Global Convex Optimization\n  of Polynomial Activation Neural Networks in Fully Polynomial-Time", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CC math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The training of two-layer neural networks with nonlinear activation functions\nis an important non-convex optimization problem with numerous applications and\npromising performance in layerwise deep learning. In this paper, we develop\nexact convex optimization formulations for two-layer neural networks with\nsecond degree polynomial activations based on semidefinite programming.\nRemarkably, we show that semidefinite lifting is always exact and therefore\ncomputational complexity for global optimization is polynomial in the input\ndimension and sample size for all input data. The developed convex formulations\nare proven to achieve the same global optimal solution set as their non-convex\ncounterparts. More specifically, the globally optimal two-layer neural network\nwith polynomial activations can be found by solving a semidefinite program\n(SDP) and decomposing the solution using a procedure we call Neural\nDecomposition. Moreover, the choice of regularizers plays a crucial role in the\ncomputational tractability of neural network training. We show that the\nstandard weight decay regularization formulation is NP-hard, whereas other\nsimple convex penalties render the problem tractable in polynomial time via\nconvex programming. We extend the results beyond the fully connected\narchitecture to different neural network architectures including networks with\nvector outputs and convolutional architectures with pooling. We provide\nextensive numerical simulations showing that the standard backpropagation\napproach often fails to achieve the global optimum of the training loss. The\nproposed approach is significantly faster to obtain better test accuracy\ncompared to the standard backpropagation procedure.\n", "versions": [{"version": "v1", "created": "Thu, 7 Jan 2021 08:43:01 GMT"}], "update_date": "2021-01-11", "authors_parsed": [["Bartan", "Burak", ""], ["Pilanci", "Mert", ""]]}, {"id": "2101.02442", "submitter": "Clement Leroy", "authors": "Cl\\'ement Leroy (INTUIDOC), Eric Anquetil (INTUIDOC), Nathalie Girard\n  (INTUIDOC)", "title": "Drift anticipation with forgetting to improve evolving fuzzy system", "comments": null, "journal-ref": "25th International Conference on Pattern Recognition (ICPR2020),\n  Jan 2021, Milan, Italy", "doi": null, "report-no": null, "categories": "cs.AI cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Working with a non-stationary stream of data requires for the analysis system\nto evolve its model (the parameters as well as the structure) over time. In\nparticular, concept drifts can occur, which makes it necessary to forget\nknowledge that has become obsolete. However, the forgetting is subjected to the\nstability-plasticity dilemma, that is, increasing forgetting improve reactivity\nof adapting to the new data while reducing the robustness of the system. Based\non a set of inference rules, Evolving Fuzzy Systems-EFS-have proven to be\neffective in solving the data stream learning problem. However tackling the\nstability-plasticity dilemma is still an open question. This paper proposes a\ncoherent method to integrate forgetting in Evolving Fuzzy System, based on the\nrecently introduced notion of concept drift anticipation. The forgetting is\napplied with two methods: an exponential forgetting of the premise part and a\ndeferred directional forgetting of the conclusion part of EFS to preserve the\ncoherence between both parts. The originality of the approach consists in\napplying the forgetting only in the anticipation module and in keeping the EFS\n(called principal system) learned without any forgetting. Then, when a drift is\ndetected in the stream, a selection mechanism is proposed to replace the\nobsolete parameters of the principal system with more suitable parameters of\nthe anticipation module. An evaluation of the proposed methods is carried out\non benchmark online datasets, with a comparison with state-of-the-art online\nclassifiers (Learn++.NSE, PENsemble, pclass) as well as with the original\nsystem using different forgetting strategies.\n", "versions": [{"version": "v1", "created": "Thu, 7 Jan 2021 09:21:27 GMT"}], "update_date": "2021-01-08", "authors_parsed": [["Leroy", "Cl\u00e9ment", "", "INTUIDOC"], ["Anquetil", "Eric", "", "INTUIDOC"], ["Girard", "Nathalie", "", "INTUIDOC"]]}, {"id": "2101.02452", "submitter": "Valentin Thorey", "authors": "Antoine Guillot and Valentin Thorey", "title": "RobustSleepNet: Transfer learning for automated sleep staging at scale", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG eess.SP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sleep disorder diagnosis relies on the analysis of polysomnography (PSG)\nrecords. As a preliminary step of this examination, sleep stages are\nsystematically determined. In practice, sleep stage classification relies on\nthe visual inspection of 30-second epochs of polysomnography signals. Numerous\nautomatic approaches have been developed to replace this tedious and expensive\ntask. Although these methods demonstrated better performance than human sleep\nexperts on specific datasets, they remain largely unused in sleep clinics. The\nmain reason is that each sleep clinic uses a specific PSG montage that most\nautomatic approaches cannot handle out-of-the-box. Moreover, even when the PSG\nmontage is compatible, publications have shown that automatic approaches\nperform poorly on unseen data with different demographics. To address these\nissues, we introduce RobustSleepNet, a deep learning model for automatic sleep\nstage classification able to handle arbitrary PSG montages. We trained and\nevaluated this model in a leave-one-out-dataset fashion on a large corpus of 8\nheterogeneous sleep staging datasets to make it robust to demographic changes.\nWhen evaluated on an unseen dataset, RobustSleepNet reaches 97% of the F1 of a\nmodel explicitly trained on this dataset. Hence, RobustSleepNet unlocks the\npossibility to perform high-quality out-of-the-box automatic sleep staging with\nany clinical setup. We further show that finetuning RobustSleepNet, using a\npart of the unseen dataset, increases the F1 by 2% when compared to a model\ntrained specifically for this dataset. Therefore, finetuning might be used to\nreach a state-of-the-art level of performance on a specific population.\n", "versions": [{"version": "v1", "created": "Thu, 7 Jan 2021 09:39:08 GMT"}, {"version": "v2", "created": "Fri, 18 Jun 2021 12:58:36 GMT"}], "update_date": "2021-06-21", "authors_parsed": [["Guillot", "Antoine", ""], ["Thorey", "Valentin", ""]]}, {"id": "2101.02453", "submitter": "Giuseppe Mangioni", "authors": "Marco Grassia, Manlio De Domenico, Giuseppe Mangioni", "title": "Machine learning dismantling and early-warning signals of disintegration\n  in complex systems", "comments": "18 pages, 5 figures. Supplementary materials: 35 pages, 10 figures, 4\n  tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.soc-ph cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  From physics to engineering, biology and social science, natural and\nartificial systems are characterized by interconnected topologies whose\nfeatures - e.g., heterogeneous connectivity, mesoscale organization, hierarchy\n- affect their robustness to external perturbations, such as targeted attacks\nto their units. Identifying the minimal set of units to attack to disintegrate\na complex network, i.e. network dismantling, is a computationally challenging\n(NP-hard) problem which is usually attacked with heuristics. Here, we show that\na machine trained to dismantle relatively small systems is able to identify\nhigher-order topological patterns, allowing to disintegrate large-scale social,\ninfrastructural and technological networks more efficiently than human-based\nheuristics. Remarkably, the machine assesses the probability that next attacks\nwill disintegrate the system, providing a quantitative method to quantify\nsystemic risk and detect early-warning signals of system's collapse. This\ndemonstrates that machine-assisted analysis can be effectively used for policy\nand decision making to better quantify the fragility of complex systems and\ntheir response to shocks.\n", "versions": [{"version": "v1", "created": "Thu, 7 Jan 2021 09:39:13 GMT"}], "update_date": "2021-01-08", "authors_parsed": [["Grassia", "Marco", ""], ["De Domenico", "Manlio", ""], ["Mangioni", "Giuseppe", ""]]}, {"id": "2101.02463", "submitter": "Gabriel Michau Dr.", "authors": "Gabriel Rodriguez Garcia, Gabriel Michau, Herbert H. Einstein and Olga\n  Fink", "title": "Decision Support System for an Intelligent Operator of Utility Tunnel\n  Boring Machines", "comments": "17 pages, 5 figures, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SY cs.LG cs.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In tunnel construction projects, delays induce high costs. Thus, tunnel\nboring machines (TBM) operators aim for fast advance rates, without safety\ncompromise, a difficult mission in uncertain ground environments. Finding the\noptimal control parameters based on the TBM sensors' measurements remains an\nopen research question with large practical relevance.\n  In this paper, we propose an intelligent decision support system developed in\nthree steps. First past projects performances are evaluated with an optimality\nscore, taking into account the advance rate and the working pressure safety.\nThen, a deep learning model learns the mapping between the TBM measurements and\nthis optimality score. Last, in real application, the model provides\nincremental recommendations to improve the optimality, taking into account the\ncurrent setting and measurements of the TBM.\n  The proposed approach is evaluated on real micro-tunnelling project and\ndemonstrates great promises for future projects.\n", "versions": [{"version": "v1", "created": "Thu, 7 Jan 2021 10:09:55 GMT"}, {"version": "v2", "created": "Fri, 8 Jan 2021 08:27:04 GMT"}], "update_date": "2021-01-11", "authors_parsed": [["Garcia", "Gabriel Rodriguez", ""], ["Michau", "Gabriel", ""], ["Einstein", "Herbert H.", ""], ["Fink", "Olga", ""]]}, {"id": "2101.02464", "submitter": "Hsin-Yuan Huang", "authors": "Hsin-Yuan Huang, Richard Kueng, John Preskill", "title": "Information-theoretic bounds on quantum advantage in machine learning", "comments": "6 pages, 2 figures + 28-page appendix", "journal-ref": "Phys. Rev. Lett. 126, 190505 (2021)", "doi": "10.1103/PhysRevLett.126.190505", "report-no": null, "categories": "quant-ph cs.IT cs.LG math.IT", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We study the performance of classical and quantum machine learning (ML)\nmodels in predicting outcomes of physical experiments. The experiments depend\non an input parameter $x$ and involve execution of a (possibly unknown) quantum\nprocess $\\mathcal{E}$. Our figure of merit is the number of runs of\n$\\mathcal{E}$ required to achieve a desired prediction performance. We consider\nclassical ML models that perform a measurement and record the classical outcome\nafter each run of $\\mathcal{E}$, and quantum ML models that can access\n$\\mathcal{E}$ coherently to acquire quantum data; the classical or quantum data\nis then used to predict outcomes of future experiments. We prove that for any\ninput distribution $\\mathcal{D}(x)$, a classical ML model can provide accurate\npredictions on average by accessing $\\mathcal{E}$ a number of times comparable\nto the optimal quantum ML model. In contrast, for achieving accurate prediction\non all inputs, we prove that exponential quantum advantage is possible. For\nexample, to predict expectations of all Pauli observables in an $n$-qubit\nsystem $\\rho$, classical ML models require $2^{\\Omega(n)}$ copies of $\\rho$,\nbut we present a quantum ML model using only $\\mathcal{O}(n)$ copies. Our\nresults clarify where quantum advantage is possible and highlight the potential\nfor classical ML models to address challenging quantum problems in physics and\nchemistry.\n", "versions": [{"version": "v1", "created": "Thu, 7 Jan 2021 10:10:09 GMT"}, {"version": "v2", "created": "Fri, 2 Apr 2021 00:26:59 GMT"}], "update_date": "2021-05-19", "authors_parsed": [["Huang", "Hsin-Yuan", ""], ["Kueng", "Richard", ""], ["Preskill", "John", ""]]}, {"id": "2101.02480", "submitter": "Tugdual Ceillier", "authors": "Alex Goupilleau, Tugdual Ceillier, Marie-Caroline Corbineau", "title": "Active learning for object detection in high-resolution satellite images", "comments": null, "journal-ref": "Conference on Artificial Intelligence for Defense, Dec 2020,\n  Rennes, France", "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.NE eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In machine learning, the term active learning regroups techniques that aim at\nselecting the most useful data to label from a large pool of unlabelled\nexamples. While supervised deep learning techniques have shown to be\nincreasingly efficient on many applications, they require a huge number of\nlabelled examples to reach operational performances. Therefore, the labelling\neffort linked to the creation of the datasets required is also increasing. When\nworking on defense-related remote sensing applications, labelling can be\nchallenging due to the large areas covered and often requires military experts\nwho are rare and whose time is primarily dedicated to operational needs.\nLimiting the labelling effort is thus of utmost importance. This study aims at\nreviewing the most relevant active learning techniques to be used for object\ndetection on very high resolution imagery and shows an example of the value of\nsuch techniques on a relevant operational use case: aircraft detection.\n", "versions": [{"version": "v1", "created": "Thu, 7 Jan 2021 10:57:38 GMT"}], "update_date": "2021-01-08", "authors_parsed": [["Goupilleau", "Alex", ""], ["Ceillier", "Tugdual", ""], ["Corbineau", "Marie-Caroline", ""]]}, {"id": "2101.02481", "submitter": "Marcello D'Orazio", "authors": "Marcello D'Orazio", "title": "Distances with mixed type variables some modified Gower's coefficients", "comments": "17 pages without figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Nearest neighbor methods have become popular in official statistics, mainly\nin imputation or in statistical matching problems; they play a key role in\nmachine learning too, where a high number of variants have been proposed. The\nchoice of the distance function depends mainly on the type of the selected\nvariables. Unfortunately, relatively few options permit to handle mixed type\nvariables, a situation frequently encountered in official statistics. The most\npopular distance for mixed type variables is derived as the complement of the\nGower's similarity coefficient; it is appealing because ranges between 0 and 1\nand allows to handle missing values. Unfortunately, the unweighted standard\nsetting the contribution of the single variables to the overall Gower's\ndistance is unbalanced because of the different nature of the variables\nthemselves. This article tries to address the main drawbacks that affect the\noverall unweighted Gower's distance by suggesting some modifications in\ncalculating the distance on the interval and ratio scaled variables. Simple\nmodifications try to attenuate the impact of outliers on the scaled Manhattan\ndistance; other modifications, relying on the kernel density estimation methods\nattempt to reduce the unbalanced contribution of the different types of\nvariables. The performance of the proposals is evaluated in simulations\nmimicking the imputation of missing values through nearest neighbor distance\nhotdeck method.\n", "versions": [{"version": "v1", "created": "Thu, 7 Jan 2021 11:00:57 GMT"}], "update_date": "2021-01-08", "authors_parsed": [["D'Orazio", "Marcello", ""]]}, {"id": "2101.02483", "submitter": "Rulin Shao", "authors": "Rulin Shao, Zhouxing Shi, Jinfeng Yi, Pin-Yu Chen, Cho-Jui Hsieh", "title": "Robust Text CAPTCHAs Using Adversarial Examples", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  CAPTCHA (Completely Automated Public Truing test to tell Computers and Humans\nApart) is a widely used technology to distinguish real users and automated\nusers such as bots. However, the advance of AI technologies weakens many\nCAPTCHA tests and can induce security concerns. In this paper, we propose a\nuser-friendly text-based CAPTCHA generation method named Robust Text CAPTCHA\n(RTC). At the first stage, the foregrounds and backgrounds are constructed with\nrandomly sampled font and background images, which are then synthesized into\nidentifiable pseudo adversarial CAPTCHAs. At the second stage, we design and\napply a highly transferable adversarial attack for text CAPTCHAs to better\nobstruct CAPTCHA solvers. Our experiments cover comprehensive models including\nshallow models such as KNN, SVM and random forest, various deep neural networks\nand OCR models. Experiments show that our CAPTCHAs have a failure rate lower\nthan one millionth in general and high usability. They are also robust against\nvarious defensive techniques that attackers may employ, including adversarial\ntraining, data pre-processing and manual tagging.\n", "versions": [{"version": "v1", "created": "Thu, 7 Jan 2021 11:03:07 GMT"}], "update_date": "2021-01-08", "authors_parsed": [["Shao", "Rulin", ""], ["Shi", "Zhouxing", ""], ["Yi", "Jinfeng", ""], ["Chen", "Pin-Yu", ""], ["Hsieh", "Cho-Jui", ""]]}, {"id": "2101.02494", "submitter": "Tinghui Ouyang", "authors": "Tinghui Ouyang, Vicent Sant Marco, Yoshinao Isobe, Hideki Asoh, Yutaka\n  Oiwa, Yoshiki Seo", "title": "Corner case data description and detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.SE", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  As the major factors affecting the safety of deep learning models, corner\ncases and related detection are crucial in AI quality assurance for\nconstructing safety- and security-critical systems. The generic corner case\nresearches involve two interesting topics. One is to enhance DL models\nrobustness to corner case data via the adjustment on parameters/structure. The\nother is to generate new corner cases for model retraining and improvement.\nHowever, the complex architecture and the huge amount of parameters make the\nrobust adjustment of DL models not easy, meanwhile it is not possible to\ngenerate all real-world corner cases for DL training. Therefore, this paper\nproposes to a simple and novel study aiming at corner case data detection via a\nspecific metric. This metric is developed on surprise adequacy (SA) which has\nadvantages on capture data behaviors. Furthermore, targeting at characteristics\nof corner case data, three modifications on distanced-based SA are developed\nfor classification applications in this paper. Consequently, through the\nexperiment analysis on MNIST data and industrial data, the feasibility and\nusefulness of the proposed method on corner case data detection are verified.\n", "versions": [{"version": "v1", "created": "Thu, 7 Jan 2021 11:26:20 GMT"}, {"version": "v2", "created": "Fri, 12 Mar 2021 01:05:19 GMT"}], "update_date": "2021-03-15", "authors_parsed": [["Ouyang", "Tinghui", ""], ["Marco", "Vicent Sant", ""], ["Isobe", "Yoshinao", ""], ["Asoh", "Hideki", ""], ["Oiwa", "Yutaka", ""], ["Seo", "Yoshiki", ""]]}, {"id": "2101.02505", "submitter": "Ana Belen Ruescas Orient", "authors": "Ana B. Ruescas, Martin Hieronymi, Sampsa Koponen, Kari Kallio and\n  Gustau Camps-Valls", "title": "Retrieval of Coloured Dissolved Organic Matter with Machine Learning\n  Methods", "comments": "7 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.geo-ph cs.LG", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  The coloured dissolved organic matter (CDOM) concentration is the standard\nmeasure of humic substance in natural waters. CDOM measurements by remote\nsensing is calculated using the absorption coefficient (a) at a certain\nwavelength (e.g. 440nm). This paper presents a comparison of four machine\nlearning methods for the retrieval of CDOM from remote sensing signals:\nregularized linear regression (RLR), random forest (RF), kernel ridge\nregression (KRR) and Gaussian process regression (GPR). Results are compared\nwith the established polynomial regression algorithms. RLR is revealed as the\nsimplest and most efficient method, followed closely by its nonlinear\ncounterpart KRR.\n", "versions": [{"version": "v1", "created": "Thu, 7 Jan 2021 11:50:38 GMT"}], "update_date": "2021-01-08", "authors_parsed": [["Ruescas", "Ana B.", ""], ["Hieronymi", "Martin", ""], ["Koponen", "Sampsa", ""], ["Kallio", "Kari", ""], ["Camps-Valls", "Gustau", ""]]}, {"id": "2101.02523", "submitter": "Mateusz Ochal", "authors": "Mateusz Ochal, Massimiliano Patacchiola, Amos Storkey, Jose Vazquez,\n  Sen Wang", "title": "Few-Shot Learning with Class Imbalance", "comments": "This work has been submitted to the IEEE for possible publication.\n  Copyright may be transferred without notice, after which this version may no\n  longer be accessible", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Few-Shot Learning (FSL) algorithms are commonly trained through Meta-Learning\n(ML), which exposes models to batches of tasks sampled from a meta-dataset to\nmimic tasks seen during evaluation. However, the standard training procedures\noverlook the real-world dynamics where classes commonly occur at different\nfrequencies. While it is generally understood that class imbalance harms the\nperformance of supervised methods, limited research examines the impact of\nimbalance on the FSL evaluation task. Our analysis compares 10 state-of-the-art\nmeta-learning and FSL methods on different imbalance distributions and\nrebalancing techniques. Our results reveal that 1) some FSL methods display a\nnatural disposition against imbalance while most other approaches produce a\nperformance drop by up to 17\\% compared to the balanced task without the\nappropriate mitigation; 2) contrary to popular belief, many meta-learning\nalgorithms will not automatically learn to balance from exposure to imbalanced\ntraining tasks; 3) classical rebalancing strategies, such as random\noversampling, can still be very effective, leading to state-of-the-art\nperformances and should not be overlooked; 4) FSL methods are more robust\nagainst meta-dataset imbalance than imbalance at the task-level with a similar\nimbalance ratio ($\\rho<20$), with the effect holding even in long-tail datasets\nunder a larger imbalance ($\\rho=65$).\n", "versions": [{"version": "v1", "created": "Thu, 7 Jan 2021 12:54:32 GMT"}, {"version": "v2", "created": "Mon, 14 Jun 2021 12:57:38 GMT"}], "update_date": "2021-06-15", "authors_parsed": [["Ochal", "Mateusz", ""], ["Patacchiola", "Massimiliano", ""], ["Storkey", "Amos", ""], ["Vazquez", "Jose", ""], ["Wang", "Sen", ""]]}, {"id": "2101.02524", "submitter": "Nicholas Baskerville", "authors": "Nicholas P Baskerville and Jonathan P Keating and Francesco Mezzadri\n  and Joseph Najnudel", "title": "A spin-glass model for the loss surfaces of generative adversarial\n  networks", "comments": "26 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math-ph cond-mat.dis-nn cond-mat.stat-mech cs.LG math.MP math.PR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a novel mathematical model that seeks to capture the key design\nfeature of generative adversarial networks (GANs). Our model consists of two\ninteracting spin glasses, and we conduct an extensive theoretical analysis of\nthe complexity of the model's critical points using techniques from Random\nMatrix Theory. The result is insights into the loss surfaces of large GANs that\nbuild upon prior insights for simpler networks, but also reveal new structure\nunique to this setting.\n", "versions": [{"version": "v1", "created": "Thu, 7 Jan 2021 12:56:15 GMT"}], "update_date": "2021-01-08", "authors_parsed": [["Baskerville", "Nicholas P", ""], ["Keating", "Jonathan P", ""], ["Mezzadri", "Francesco", ""], ["Najnudel", "Joseph", ""]]}, {"id": "2101.02530", "submitter": "Alexander Neergaard Olesen", "authors": "Alexander Neergaard Olesen, Poul Jennum, Emmanuel Mignot and Helge B.\n  D. Sorensen", "title": "MSED: a multi-modal sleep event detection model for clinical sleep\n  analysis", "comments": "20 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.SP stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Study objective: Clinical sleep analysis require manual analysis of sleep\npatterns for correct diagnosis of sleep disorders. Several studies show\nsignificant variability in scoring discrete sleep events. We wished to\ninvestigate, whether an automatic method could be used for detection of\narousals (Ar), leg movements (LM) and sleep disordered breathing (SDB) events,\nand if the joint detection of these events performed better than having three\nseparate models.\n  Methods: We designed a single deep neural network architecture to jointly\ndetect sleep events in a polysomnogram. We trained the model on 1653 recordings\nof individuals, and tested the optimized model on 1000 separate recordings. The\nperformance of the model was quantified by F1, precision, and recall scores,\nand by correlating index values to clinical values using Pearson's correlation\ncoefficient.\n  Results: F1 scores for the optimized model was 0.70, 0.63, and 0.62 for Ar,\nLM, and SDB, respectively. The performance was higher, when detecting events\njointly compared to corresponding single-event models. Index values computed\nfrom detected events correlated well with manual annotations ($r^2$ = 0.73,\n$r^2$ = 0.77, $r^2$ = 0.78, respectively).\n  Conclusion: Detecting arousals, leg movements and sleep disordered breathing\nevents jointly is possible, and the computed index values correlates well with\nhuman annotations.\n", "versions": [{"version": "v1", "created": "Thu, 7 Jan 2021 13:08:44 GMT"}], "update_date": "2021-01-08", "authors_parsed": [["Olesen", "Alexander Neergaard", ""], ["Jennum", "Poul", ""], ["Mignot", "Emmanuel", ""], ["Sorensen", "Helge B. D.", ""]]}, {"id": "2101.02533", "submitter": "Alon Brutzkus", "authors": "Roei Sarussi, Alon Brutzkus, Amir Globerson", "title": "Towards Understanding Learning in Neural Networks with Linear Teachers", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Can a neural network minimizing cross-entropy learn linearly separable data?\nDespite progress in the theory of deep learning, this question remains\nunsolved. Here we prove that SGD globally optimizes this learning problem for a\ntwo-layer network with Leaky ReLU activations. The learned network can in\nprinciple be very complex. However, empirical evidence suggests that it often\nturns out to be approximately linear. We provide theoretical support for this\nphenomenon by proving that if network weights converge to two weight clusters,\nthis will imply an approximately linear decision boundary. Finally, we show a\ncondition on the optimization that leads to weight clustering. We provide\nempirical results that validate our theoretical analysis.\n", "versions": [{"version": "v1", "created": "Thu, 7 Jan 2021 13:21:24 GMT"}, {"version": "v2", "created": "Wed, 28 Jul 2021 11:21:39 GMT"}], "update_date": "2021-07-29", "authors_parsed": [["Sarussi", "Roei", ""], ["Brutzkus", "Alon", ""], ["Globerson", "Amir", ""]]}, {"id": "2101.02538", "submitter": "Xue Jiang", "authors": "Xue Jiang", "title": "MRNet: a Multi-scale Residual Network for EEG-based Sleep Staging", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG eess.SP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Sleep staging based on electroencephalogram (EEG) plays an important role in\nthe clinical diagnosis and treatment of sleep disorders. In order to emancipate\nhuman experts from heavy labeling work, deep neural networks have been employed\nto formulate automated sleep staging systems recently. However, EEG signals\nlose considerable detailed information in network propagation, which affects\nthe representation of deep features. To address this problem, we propose a new\nframework, called MRNet, for data-driven sleep staging by integrating a\nmulti-scale feature fusion model and a Markov-based sequential correction\nalgorithm. The backbone of MRNet is a residual block-based network, which\nperforms as a feature extractor.Then the fusion model constructs a feature\npyramid by concatenating the outputs from the different depths of the backbone,\nwhich can help the network better comprehend the signals in different scales.\nThe Markov-based sequential correction algorithm is designed to reduce the\noutput jitters generated by the classifier. The algorithm depends on a prior\nstage distribution associated with the sleep stage transition rule and the\nMarkov chain. Experiment results demonstrate the competitive performance of our\nproposed approach on both accuracy and F1 score (e.g., 85.14% Acc and 78.91% F1\nscore on Sleep-EDFx, and 87.59% Acc and 79.62% F1 score on Sleep-EDF).\n", "versions": [{"version": "v1", "created": "Thu, 7 Jan 2021 13:48:30 GMT"}], "update_date": "2021-01-08", "authors_parsed": [["Jiang", "Xue", ""]]}, {"id": "2101.02547", "submitter": "Ole-Christoffer Granmo", "authors": "Lei Jiao, Xuan Zhang, Ole-Christoffer Granmo, K. Darshana Abeyrathna", "title": "On the Convergence of Tsetlin Machines for the XOR Operator", "comments": "31 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Tsetlin Machine (TM) is a novel machine learning algorithm with several\ndistinct properties, including transparent inference and learning using\nhardware-near building blocks. Although numerous papers explore the TM\nempirically, many of its properties have not yet been analyzed mathematically.\nIn this article, we analyze the convergence of the TM when input is\nnon-linearly related to output by the XOR-operator. Our analysis reveals that\nthe TM, with just two conjunctive clauses, can converge almost surely to\nreproducing XOR, learning from training data over an infinite time horizon.\nFurthermore, the analysis shows how the hyper-parameter T guides clause\nconstruction so that the clauses capture the distinct sub-patterns in the data.\nOur analysis of convergence for XOR thus lays the foundation for analyzing\nother more complex logical expressions. These analyses altogether, from a\nmathematical perspective, provide new insights on why TMs have obtained\nstate-of-the-art performance on several pattern recognition problems\n", "versions": [{"version": "v1", "created": "Thu, 7 Jan 2021 14:13:41 GMT"}], "update_date": "2021-01-08", "authors_parsed": [["Jiao", "Lei", ""], ["Zhang", "Xuan", ""], ["Granmo", "Ole-Christoffer", ""], ["Abeyrathna", "K. Darshana", ""]]}, {"id": "2101.02552", "submitter": "Sohail Ahmed Khan", "authors": "Sohail Ahmed Khan and Wasiq Khan and Abir Hussain", "title": "Phishing Attacks and Websites Classification Using Machine Learning and\n  Multiple Datasets (A Comparative Analysis)", "comments": null, "journal-ref": "In: Huang DS., Premaratne P. (eds) Intelligent Computing\n  Methodologies. ICIC 2020. Lecture Notes in Computer Science, vol 12465.\n  Springer, Cham", "doi": "10.1007/978-3-030-60796-8_26", "report-no": null, "categories": "cs.CR cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Phishing attacks are the most common type of cyber-attacks used to obtain\nsensitive information and have been affecting individuals as well as\norganisations across the globe. Various techniques have been proposed to\nidentify the phishing attacks specifically, deployment of machine intelligence\nin recent years. However, the deployed algorithms and discriminating factors\nare very diverse in existing works. In this study, we present a comprehensive\nanalysis of various machine learning algorithms to evaluate their performances\nover multiple datasets. We further investigate the most significant features\nwithin multiple datasets and compare the classification performance with the\nreduced dimensional datasets. The statistical results indicate that random\nforest and artificial neural network outperform other classification\nalgorithms, achieving over 97% accuracy using the identified features.\n", "versions": [{"version": "v1", "created": "Wed, 6 Jan 2021 08:23:43 GMT"}], "update_date": "2021-01-08", "authors_parsed": [["Khan", "Sohail Ahmed", ""], ["Khan", "Wasiq", ""], ["Hussain", "Abir", ""]]}, {"id": "2101.02553", "submitter": "Nikos Vlassis", "authors": "Nikos Vlassis, Fernando Amat Gil, Ashok Chandrashekar", "title": "Off-Policy Evaluation of Slate Policies under Bayes Risk", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of off-policy evaluation for slate bandits, for the\ntypical case in which the logging policy factorizes over the slots of the\nslate. We slightly depart from the existing literature by taking Bayes risk as\nthe criterion by which to evaluate estimators, and we analyze the family of\n'additive' estimators that includes the pseudoinverse (PI) estimator of\nSwaminathan et al.\\ (2017; arXiv:1605.04812). Using a control variate approach,\nwe identify a new estimator in this family that is guaranteed to have lower\nrisk than PI in the above class of problems. In particular, we show that the\nrisk improvement over PI grows linearly with the number of slots, and linearly\nwith the gap between the arithmetic and the harmonic mean of a set of\nslot-level divergences between the logging and the target policy. In the\ntypical case of a uniform logging policy and a deterministic target policy,\neach divergence corresponds to slot size, showing that maximal gains can be\nobtained for slate problems with diverse numbers of actions per slot.\n", "versions": [{"version": "v1", "created": "Tue, 5 Jan 2021 20:07:56 GMT"}], "update_date": "2021-01-08", "authors_parsed": [["Vlassis", "Nikos", ""], ["Gil", "Fernando Amat", ""], ["Chandrashekar", "Ashok", ""]]}, {"id": "2101.02555", "submitter": "Yehezkel Resheff", "authors": "Daniel Ben David, Yehezkel S. Resheff, Talia Tron", "title": "Explainable AI and Adoption of Financial Algorithmic Advisors: an\n  Experimental Study", "comments": "accepted: AIES '21", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.AI cs.CY cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We study whether receiving advice from either a human or algorithmic advisor,\naccompanied by five types of Local and Global explanation labelings, has an\neffect on the readiness to adopt, willingness to pay, and trust in a financial\nAI consultant. We compare the differences over time and in various key\nsituations using a unique experimental framework where participants play a\nweb-based game with real monetary consequences. We observed that accuracy-based\nexplanations of the model in initial phases leads to higher adoption rates.\nWhen the performance of the model is immaculate, there is less importance\nassociated with the kind of explanation for adoption. Using more elaborate\nfeature-based or accuracy-based explanations helps substantially in reducing\nthe adoption drop upon model failure. Furthermore, using an autopilot increases\nadoption significantly. Participants assigned to the AI-labeled advice with\nexplanations were willing to pay more for the advice than the AI-labeled advice\nwith a No-explanation alternative. These results add to the literature on the\nimportance of XAI for algorithmic adoption and trust.\n", "versions": [{"version": "v1", "created": "Tue, 5 Jan 2021 09:34:38 GMT"}, {"version": "v2", "created": "Tue, 8 Jun 2021 14:26:48 GMT"}, {"version": "v3", "created": "Wed, 9 Jun 2021 06:44:15 GMT"}], "update_date": "2021-06-10", "authors_parsed": [["David", "Daniel Ben", ""], ["Resheff", "Yehezkel S.", ""], ["Tron", "Talia", ""]]}, {"id": "2101.02557", "submitter": "Julia Ann Jose", "authors": "Julia Ann Jose, Trae Waggoner, Sudarsan Manikandan", "title": "Continuous Glucose Monitoring Prediction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.OT cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Diabetes is one of the deadliest diseases in the world and affects nearly 10\npercent of the global adult population. Fortunately, powerful new technologies\nallow for a consistent and reliable treatment plan for people with diabetes.\nOne major development is a system called continuous blood glucose monitoring\n(CGM). In this review, we look at three different continuous meal detection\nalgorithms that were developed using given CGM data from patients with\ndiabetes. From this analysis, an initial meal prediction algorithm was also\ndeveloped utilizing these methods.\n", "versions": [{"version": "v1", "created": "Mon, 4 Jan 2021 21:32:20 GMT"}], "update_date": "2021-01-08", "authors_parsed": [["Jose", "Julia Ann", ""], ["Waggoner", "Trae", ""], ["Manikandan", "Sudarsan", ""]]}, {"id": "2101.02558", "submitter": "Akira Horiguchi", "authors": "Akira Horiguchi and Thomas J. Santner and Ying Sun and Matthew T.\n  Pratola", "title": "Using BART for Multiobjective Optimization of Noisy Multiple Objectives", "comments": "45 pages, 12 figures, submitted to Industry 4.0 special issue of\n  Technometrics journal", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ME", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Techniques to reduce the energy burden of an Industry 4.0 ecosystem often\nrequire solving a multiobjective optimization problem. However, collecting\nexperimental data can often be either expensive or time-consuming. In such\ncases, statistical methods can be helpful. This article proposes Pareto Front\n(PF) and Pareto Set (PS) estimation methods using Bayesian Additive Regression\nTrees (BART), which is a non-parametric model whose assumptions are typically\nless restrictive than popular alternatives, such as Gaussian Processes. The\nperformance of our BART-based method is compared to a GP-based method using\nanalytic test functions, demonstrating convincing advantages. Finally, our\nBART-based methodology is applied to a motivating Industry 4.0 engineering\nproblem.\n", "versions": [{"version": "v1", "created": "Mon, 4 Jan 2021 18:58:37 GMT"}], "update_date": "2021-01-08", "authors_parsed": [["Horiguchi", "Akira", ""], ["Santner", "Thomas J.", ""], ["Sun", "Ying", ""], ["Pratola", "Matthew T.", ""]]}, {"id": "2101.02559", "submitter": "Lois Orosa", "authors": "Muhammad Shafique, Mahum Naseer, Theocharis Theocharides, Christos\n  Kyrkou, Onur Mutlu, Lois Orosa, Jungwook Choi", "title": "Robust Machine Learning Systems: Challenges, Current Trends,\n  Perspectives, and the Road Ahead", "comments": "Final version appears in https://ieeexplore.ieee.org/document/8979377", "journal-ref": "IEEE Design and Test (Volume: 37, Issue: 2, April 2020): 30-57", "doi": "10.1109/MDAT.2020.2971217", "report-no": null, "categories": "cs.CR cs.AI cs.AR cs.LG cs.SY eess.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Machine Learning (ML) techniques have been rapidly adopted by smart\nCyber-Physical Systems (CPS) and Internet-of-Things (IoT) due to their powerful\ndecision-making capabilities. However, they are vulnerable to various security\nand reliability threats, at both hardware and software levels, that compromise\ntheir accuracy. These threats get aggravated in emerging edge ML devices that\nhave stringent constraints in terms of resources (e.g., compute, memory,\npower/energy), and that therefore cannot employ costly security and reliability\nmeasures. Security, reliability, and vulnerability mitigation techniques span\nfrom network security measures to hardware protection, with an increased\ninterest towards formal verification of trained ML models.\n  This paper summarizes the prominent vulnerabilities of modern ML systems,\nhighlights successful defenses and mitigation techniques against these\nvulnerabilities, both at the cloud (i.e., during the ML training phase) and\nedge (i.e., during the ML inference stage), discusses the implications of a\nresource-constrained design on the reliability and security of the system,\nidentifies verification methodologies to ensure correct system behavior, and\ndescribes open research challenges for building secure and reliable ML systems\nat both the edge and the cloud.\n", "versions": [{"version": "v1", "created": "Mon, 4 Jan 2021 20:06:56 GMT"}], "update_date": "2021-01-08", "authors_parsed": [["Shafique", "Muhammad", ""], ["Naseer", "Mahum", ""], ["Theocharides", "Theocharis", ""], ["Kyrkou", "Christos", ""], ["Mutlu", "Onur", ""], ["Orosa", "Lois", ""], ["Choi", "Jungwook", ""]]}, {"id": "2101.02561", "submitter": "Yiming Xu", "authors": "Yiming Xu, Diego Klabjan", "title": "Open Set Domain Adaptation by Extreme Value Theory", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Common domain adaptation techniques assume that the source domain and the\ntarget domain share an identical label space, which is problematic since when\ntarget samples are unlabeled we have no knowledge on whether the two domains\nshare the same label space. When this is not the case, the existing methods\nfail to perform well because the additional unknown classes are also matched\nwith the source domain during adaptation. In this paper, we tackle the open set\ndomain adaptation problem under the assumption that the source and the target\nlabel spaces only partially overlap, and the task becomes when the unknown\nclasses exist, how to detect the target unknown classes and avoid aligning them\nwith the source domain. We propose to utilize an instance-level reweighting\nstrategy for domain adaptation where the weights indicate the likelihood of a\nsample belonging to known classes and to model the tail of the entropy\ndistribution with Extreme Value Theory for unknown class detection. Experiments\non conventional domain adaptation datasets show that the proposed method\noutperforms the state-of-the-art models.\n", "versions": [{"version": "v1", "created": "Tue, 22 Dec 2020 19:31:32 GMT"}], "update_date": "2021-01-08", "authors_parsed": [["Xu", "Yiming", ""], ["Klabjan", "Diego", ""]]}, {"id": "2101.02562", "submitter": "Zhanglongyuan Longyuan", "authors": "Jinyin Chen, Longyuan Zhang, Haibin Zheng, Xueke Wang and Zhaoyan Ming", "title": "DeepPoison: Feature Transfer Based Stealthy Poisoning Attack", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Deep neural networks are susceptible to poisoning attacks by purposely\npolluted training data with specific triggers. As existing episodes mainly\nfocused on attack success rate with patch-based samples, defense algorithms can\neasily detect these poisoning samples. We propose DeepPoison, a novel\nadversarial network of one generator and two discriminators, to address this\nproblem. Specifically, the generator automatically extracts the target class'\nhidden features and embeds them into benign training samples. One discriminator\ncontrols the ratio of the poisoning perturbation. The other discriminator works\nas the target model to testify the poisoning effects. The novelty of DeepPoison\nlies in that the generated poisoned training samples are indistinguishable from\nthe benign ones by both defensive methods and manual visual inspection, and\neven benign test samples can achieve the attack. Extensive experiments have\nshown that DeepPoison can achieve a state-of-the-art attack success rate, as\nhigh as 91.74%, with only 7% poisoned samples on publicly available datasets\nLFW and CASIA. Furthermore, we have experimented with high-performance defense\nalgorithms such as autodecoder defense and DBSCAN cluster detection and showed\nthe resilience of DeepPoison.\n", "versions": [{"version": "v1", "created": "Wed, 6 Jan 2021 15:45:36 GMT"}, {"version": "v2", "created": "Fri, 8 Jan 2021 04:07:19 GMT"}], "update_date": "2021-01-11", "authors_parsed": [["Chen", "Jinyin", ""], ["Zhang", "Longyuan", ""], ["Zheng", "Haibin", ""], ["Wang", "Xueke", ""], ["Ming", "Zhaoyan", ""]]}, {"id": "2101.02568", "submitter": "Jiawei Ren", "authors": "Jiawei Ren, Xiao Ma, Chen Xu, Haiyu Zhao, Shuai Yi", "title": "HAVANA: Hierarchical and Variation-Normalized Autoencoder for Person\n  Re-identification", "comments": "Manuscript", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Person Re-Identification (Re-ID) is of great importance to the many video\nsurveillance systems. Learning discriminative features for Re-ID remains a\nchallenge due to the large variations in the image space, e.g., continuously\nchanging human poses, illuminations and point of views. In this paper, we\npropose HAVANA, a novel extensible, light-weight HierArchical and\nVAriation-Normalized Autoencoder that learns features robust to intra-class\nvariations. In contrast to existing generative approaches that prune the\nvariations with heavy extra supervised signals, HAVANA suppresses the\nintra-class variations with a Variation-Normalized Autoencoder trained with no\nadditional supervision. We also introduce a novel Jensen-Shannon triplet loss\nfor contrastive distribution learning in Re-ID. In addition, we present\nHierarchical Variation Distiller, a hierarchical VAE to factorize the latent\nrepresentation and explicitly model the variations. To the best of our\nknowledge, HAVANA is the first VAE-based framework for person ReID.\n", "versions": [{"version": "v1", "created": "Wed, 6 Jan 2021 12:03:19 GMT"}, {"version": "v2", "created": "Sat, 9 Jan 2021 06:05:03 GMT"}], "update_date": "2021-01-12", "authors_parsed": [["Ren", "Jiawei", ""], ["Ma", "Xiao", ""], ["Xu", "Chen", ""], ["Zhao", "Haiyu", ""], ["Yi", "Shuai", ""]]}, {"id": "2101.02573", "submitter": "Hazem Soliman", "authors": "Hazem M. Soliman, Geoff Salmon, Du\\v{s}an Sovilj, Mohan Rao", "title": "RANK: AI-assisted End-to-End Architecture for Detecting Persistent\n  Attacks in Enterprise Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Advanced Persistent Threats (APTs) are sophisticated multi-step attacks,\nplanned and executed by skilled adversaries targeting modern government and\nenterprise networks. Intrusion Detection Systems (IDSs) and User and Entity\nBehavior Analytics (UEBA) are commonly employed to aid a security analyst in\nthe detection of APTs. The prolonged nature of APTs, combined with the granular\nfocus of UEBA and IDS, results in overwhelming the analyst with an increasingly\nimpractical number of alerts. Consequent to this abundance of data, and\ntogether with the crucial importance of the problem as well as the high cost of\nthe skilled personnel involved, the problem of APT detection becomes a perfect\ncandidate for automation through Artificial Intelligence (AI). In this paper,\nwe provide, up to our knowledge, the first study and implementation of an\nend-to-end AI-assisted architecture for detecting APTs -- RANK. The goal of the\nsystem is not to replace the analyst, rather, it is to automate the complete\npipeline from data sources to a final set of incidents for analyst review. The\narchitecture is composed of four consecutive steps: 1) alert templating and\nmerging, 2) alert graph construction, 3) alert graph partitioning into\nincidents, and 4) incident scoring and ordering. We evaluate our architecture\nagainst the 2000 DARPA Intrusion Detection dataset, as well as a read-world\nprivate dataset from a medium-scale enterprise. Extensive results are provided\nshowing a three order of magnitude reduction in the amount of data to be\nreviewed by the analyst, innovative extraction of incidents and security-wise\nscoring of extracted incidents.\n", "versions": [{"version": "v1", "created": "Wed, 6 Jan 2021 15:59:51 GMT"}], "update_date": "2021-01-08", "authors_parsed": [["Soliman", "Hazem M.", ""], ["Salmon", "Geoff", ""], ["Sovilj", "Du\u0161an", ""], ["Rao", "Mohan", ""]]}, {"id": "2101.02595", "submitter": "Stein Kristiansen", "authors": "Stein Kristiansen, Konstantinos Nikolaidis, Thomas Plagemann, Vera\n  Goebel, Gunn Marit Traaen, Britt {\\O}verland, Lars Aaker{\\o}y, Tove-Elizabeth\n  Hunt, Jan P{\\aa}l Loennechen, Sigurd Loe Steinshamn, Christina Holt Bendz,\n  Ole-Gunnar Anfinsen, Lars Gullestad, Harriet Akre", "title": "A Clinical Evaluation of a Low-Cost Strain Gauge Respiration Belt and\n  Machine Learning to Detect Sleep Apnea", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sleep apnea is a serious and severely under-diagnosed sleep-related\nrespiration disorder characterized by repeated disrupted breathing events\nduring sleep. It is diagnosed via polysomnography which is an expensive test\nconducted in a sleep lab requiring sleep experts to manually score the recorded\ndata. Since the symptoms of sleep apnea are often ambiguous, it is difficult\nfor a physician to decide whether to prescribe polysomnography. In this study,\nwe investigate whether helpful information can be obtained by collecting and\nautomatically analysing sleep data using a smartphone and an inexpensive strain\ngauge respiration belt. We evaluate how accurately we can detect sleep apnea\nwith wide variety of machine learning techniques with data from a clinical\nstudy with 49 overnight sleep recordings. With less than one hour of training,\nwe can distinguish between normal and apneic minutes with an accuracy,\nsensitivity, and specificity of 0.7609, 0.7833, and 0.7217, respectively. These\nresults can be achieved even if we train only on high-quality data from an\nentirely separate, clinically certified sensor, which has the potential to\nsubstantially reduce the cost of data collection. Data from a complete night\ncan be analyzed in about one second on a smartphone.\n", "versions": [{"version": "v1", "created": "Thu, 7 Jan 2021 15:38:28 GMT"}], "update_date": "2021-01-08", "authors_parsed": [["Kristiansen", "Stein", ""], ["Nikolaidis", "Konstantinos", ""], ["Plagemann", "Thomas", ""], ["Goebel", "Vera", ""], ["Traaen", "Gunn Marit", ""], ["\u00d8verland", "Britt", ""], ["Aaker\u00f8y", "Lars", ""], ["Hunt", "Tove-Elizabeth", ""], ["Loennechen", "Jan P\u00e5l", ""], ["Steinshamn", "Sigurd Loe", ""], ["Bendz", "Christina Holt", ""], ["Anfinsen", "Ole-Gunnar", ""], ["Gullestad", "Lars", ""], ["Akre", "Harriet", ""]]}, {"id": "2101.02609", "submitter": "Louis Falissard", "authors": "Louis Falissard, Karim Bounebache, Gr\\'egoire Rey", "title": "Learning a binary search with a recurrent neural network. A novel\n  approach to ordinal regression analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural networks are a family of computational models that are naturally\nsuited to the analysis of hierarchical data such as, for instance, sequential\ndata with the use of recurrent neural networks. In the other hand, ordinal\nregression is a well-known predictive modelling problem used in fields as\ndiverse as psychometry to deep neural network based voice modelling. Their\nspecificity lies in the properties of their outcome variable, typically\nconsidered as a categorical variable with natural ordering properties,\ntypically allowing comparisons between different states (\"a little\" is less\nthan \"somewhat\" which is itself less than \"a lot\", with transitivity allowed).\nThis article investigates the application of sequence-to-sequence learning\nmethods provided by the deep learning framework in ordinal regression, by\nformulating the ordinal regression problem as a sequential binary search. A\nmethod for visualizing the model's explanatory variables according to the\nordinal target variable is proposed, that bears some similarities to linear\ndiscriminant analysis. The method is compared to traditional ordinal regression\nmethods on a number of benchmark dataset, and is shown to have comparable or\nsignificantly better predictive power.\n", "versions": [{"version": "v1", "created": "Thu, 7 Jan 2021 16:16:43 GMT"}], "update_date": "2021-01-08", "authors_parsed": [["Falissard", "Louis", ""], ["Bounebache", "Karim", ""], ["Rey", "Gr\u00e9goire", ""]]}, {"id": "2101.02623", "submitter": "Lior Shamir", "authors": "Lior Shamir", "title": "Automatic identification of outliers in Hubble Space Telescope galaxy\n  images", "comments": "MNRAS, accepted", "journal-ref": null, "doi": "10.1093/mnras/staa4036", "report-no": null, "categories": "astro-ph.GA astro-ph.IM cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Rare extragalactic objects can carry substantial information about the past,\npresent, and future universe. Given the size of astronomical databases in the\ninformation era it can be assumed that very many outlier galaxies are included\nin existing and future astronomical databases. However, manual search for these\nobjects is impractical due to the required labor, and therefore the ability to\ndetect such objects largely depends on computer algorithms. This paper\ndescribes an unsupervised machine learning algorithm for automatic detection of\noutlier galaxy images, and its application to several Hubble Space Telescope\nfields. The algorithm does not require training, and therefore is not dependent\non the preparation of clean training sets. The application of the algorithm to\na large collection of galaxies detected a variety of outlier galaxy images. The\nalgorithm is not perfect in the sense that not all objects detected by the\nalgorithm are indeed considered outliers, but it reduces the dataset by two\norders of magnitude to allow practical manual identification. The catalogue\ncontains 147 objects that would be very difficult to identify without using\nautomation.\n", "versions": [{"version": "v1", "created": "Thu, 7 Jan 2021 16:52:10 GMT"}], "update_date": "2021-01-13", "authors_parsed": [["Shamir", "Lior", ""]]}, {"id": "2101.02625", "submitter": "Waheed Bajwa", "authors": "Rishabh Dixit and Waheed U. Bajwa", "title": "Boundary Conditions for Linear Exit Time Gradient Trajectories Around\n  Saddle Points: Analysis and Algorithm", "comments": "49 pages; 10 figures; preprint of a journal paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.LG cs.SY eess.SY math.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Gradient-related first-order methods have become the workhorse of large-scale\nnumerical optimization problems. Many of these problems involve nonconvex\nobjective functions with multiple saddle points, which necessitates an\nunderstanding of the behavior of discrete trajectories of first-order methods\nwithin the geometrical landscape of these functions. This paper concerns\nconvergence of first-order discrete methods to a local minimum of nonconvex\noptimization problems that comprise strict saddle points within the geometrical\nlandscape. To this end, it focuses on analysis of discrete gradient\ntrajectories around saddle neighborhoods, derives sufficient conditions under\nwhich these trajectories can escape strict-saddle neighborhoods in linear time,\nexplores the contractive and expansive dynamics of these trajectories in\nneighborhoods of strict-saddle points that are characterized by gradients of\nmoderate magnitude, characterizes the non-curving nature of these trajectories,\nand highlights the inability of these trajectories to re-enter the\nneighborhoods around strict-saddle points after exiting them. Based on these\ninsights and analyses, the paper then proposes a simple variant of the vanilla\ngradient descent algorithm, termed Curvature Conditioned Regularized Gradient\nDescent (CCRGD) algorithm, which utilizes a check for an initial boundary\ncondition to ensure its trajectories can escape strict-saddle neighborhoods in\nlinear time. Convergence analysis of the CCRGD algorithm, which includes its\nrate of convergence to a local minimum within a geometrical landscape that has\na maximum number of strict-saddle points, is also presented in the paper.\nNumerical experiments are then provided on a test function as well as a\nlow-rank matrix factorization problem to evaluate the efficacy of the proposed\nalgorithm.\n", "versions": [{"version": "v1", "created": "Thu, 7 Jan 2021 16:59:15 GMT"}], "update_date": "2021-01-08", "authors_parsed": [["Dixit", "Rishabh", ""], ["Bajwa", "Waheed U.", ""]]}, {"id": "2101.02628", "submitter": "Sandeep Ranjan Dr", "authors": "Sandeep Ranjan", "title": "Analyzing the response to TV serials retelecast during COVID19 lockdown\n  in India", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "econ.GN cs.LG cs.SI q-fin.EC", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  TV serials are a popular source of entertainment. The ongoing COVID19\nlockdown has a high probability of degrading the publics mental health. The\nGovernment of India started the retelecast of yesteryears popular TV serials on\npublic broadcaster Doordarshan from 28th March 2020 to 31st July 2020. Tweets\ncorresponding to the Doordarshan hashtag were mined to create a dataset. The\nexperiment aims to analyze the publics response to the retelecast of TV serials\nby calculating the sentiment score of the tweet dataset. Datasets mean\nsentiment score of 0.65 and high share 64.58% of positive tweets signifies the\nacceptance of Doordarshans retelecast decision. The sentiment analysis result\nalso reflects the positive state of mind of the public.\n", "versions": [{"version": "v1", "created": "Tue, 22 Dec 2020 07:05:32 GMT"}, {"version": "v2", "created": "Mon, 11 Jan 2021 04:58:05 GMT"}], "update_date": "2021-01-13", "authors_parsed": [["Ranjan", "Sandeep", ""]]}, {"id": "2101.02647", "submitter": "Juana Valeria Hurtado", "authors": "Juana Valeria Hurtado, Laura Londo\\~no, and Abhinav Valada", "title": "From Learning to Relearning: A Framework for Diminishing Bias in Social\n  Robot Navigation", "comments": null, "journal-ref": "Frontiers in Robotics and AI, 2021", "doi": "10.3389/frobt.2021.650325", "report-no": null, "categories": "cs.RO cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The exponentially increasing advances in robotics and machine learning are\nfacilitating the transition of robots from being confined to controlled\nindustrial spaces to performing novel everyday tasks in domestic and urban\nenvironments. In order to make the presence of robots safe as well as\ncomfortable for humans, and to facilitate their acceptance in public\nenvironments, they are often equipped with social abilities for navigation and\ninteraction. Socially compliant robot navigation is increasingly being learned\nfrom human observations or demonstrations. We argue that these techniques that\ntypically aim to mimic human behavior do not guarantee fair behavior. As a\nconsequence, social navigation models can replicate, promote, and amplify\nsocietal unfairness such as discrimination and segregation. In this work, we\ninvestigate a framework for diminishing bias in social robot navigation models\nso that robots are equipped with the capability to plan as well as adapt their\npaths based on both physical and social demands. Our proposed framework\nconsists of two components: \\textit{learning} which incorporates social context\ninto the learning process to account for safety and comfort, and\n\\textit{relearning} to detect and correct potentially harmful outcomes before\nthe onset. We provide both technological and societal analysis using three\ndiverse case studies in different social scenarios of interaction. Moreover, we\npresent ethical implications of deploying robots in social environments and\npropose potential solutions. Through this study, we highlight the importance\nand advocate for fairness in human-robot interactions in order to promote more\nequitable social relationships, roles, and dynamics and consequently positively\ninfluence our society.\n", "versions": [{"version": "v1", "created": "Thu, 7 Jan 2021 17:42:35 GMT"}, {"version": "v2", "created": "Wed, 3 Mar 2021 18:42:23 GMT"}], "update_date": "2021-03-04", "authors_parsed": [["Hurtado", "Juana Valeria", ""], ["Londo\u00f1o", "Laura", ""], ["Valada", "Abhinav", ""]]}, {"id": "2101.02649", "submitter": "Elmira Amirloo Abolfathi", "authors": "Elmira Amirloo Abolfathi, Jun Luo, Peyman Yadmellat, Kasra Rezaee", "title": "CoachNet: An Adversarial Sampling Approach for Reinforcement Learning", "comments": "NeurIPS2019 Workshop on Safety and Robustness in Decision Making", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite the recent successes of reinforcement learning in games and robotics,\nit is yet to become broadly practical. Sample efficiency and unreliable\nperformance in rare but challenging scenarios are two of the major obstacles.\nDrawing inspiration from the effectiveness of deliberate practice for achieving\nexpert-level human performance, we propose a new adversarial sampling approach\nguided by a failure predictor named \"CoachNet\". CoachNet is trained online\nalong with the agent to predict the probability of failure. This probability is\nthen used in a stochastic sampling process to guide the agent to more\nchallenging episodes. This way, instead of wasting time on scenarios that the\nagent has already mastered, training is focused on the agent's \"weak spots\". We\npresent the design of CoachNet, explain its underlying principles, and\nempirically demonstrate its effectiveness in improving sample efficiency and\ntest-time robustness in common continuous control tasks.\n", "versions": [{"version": "v1", "created": "Thu, 7 Jan 2021 17:45:18 GMT"}], "update_date": "2021-01-08", "authors_parsed": [["Abolfathi", "Elmira Amirloo", ""], ["Luo", "Jun", ""], ["Yadmellat", "Peyman", ""], ["Rezaee", "Kasra", ""]]}, {"id": "2101.02653", "submitter": "Opeoluwa Owoyele", "authors": "Opeoluwa Owoyele, Pinaki Pal, Alvaro Vidal Torreira, Daniel Probst,\n  Matthew Shaxted, Michael Wilde, Peter Kelly Senecal", "title": "Application of an automated machine learning-genetic algorithm\n  (AutoML-GA) coupled with computational fluid dynamics simulations for rapid\n  engine design optimization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, the use of machine learning-based surrogate models for\ncomputational fluid dynamics (CFD) simulations has emerged as a promising\ntechnique for reducing the computational cost associated with engine design\noptimization. However, such methods still suffer from drawbacks. One main\ndisadvantage of is that the default machine learning (ML) hyperparameters are\noften severely suboptimal for a given problem. This has often been addressed by\nmanually trying out different hyperparameter settings, but this solution is\nineffective in a high-dimensional hyperparameter space. Besides this problem,\nthe amount of data needed for training is also not known a priori. In response\nto these issues that need to be addressed, the present work describes and\nvalidates an automated active learning approach, AutoML-GA, for surrogate-based\noptimization of internal combustion engines. In this approach, a Bayesian\noptimization technique is used to find the best machine learning\nhyperparameters based on an initial dataset obtained from a small number of CFD\nsimulations. Subsequently, a genetic algorithm is employed to locate the design\noptimum on the ML surrogate surface. In the vicinity of the design optimum, the\nsolution is refined by repeatedly running CFD simulations at the projected\noptimum and adding the newly obtained data to the training dataset. It is\ndemonstrated that AutoML-GA leads to a better optimum with a lower number of\nCFD simulations, compared to the use of default hyperparameters. The proposed\nframework offers the advantage of being a more hands-off approach that can be\nreadily utilized by researchers and engineers in industry who do not have\nextensive machine learning expertise.\n", "versions": [{"version": "v1", "created": "Thu, 7 Jan 2021 17:50:52 GMT"}, {"version": "v2", "created": "Fri, 12 Feb 2021 20:06:24 GMT"}, {"version": "v3", "created": "Tue, 6 Apr 2021 15:10:23 GMT"}], "update_date": "2021-04-07", "authors_parsed": [["Owoyele", "Opeoluwa", ""], ["Pal", "Pinaki", ""], ["Torreira", "Alvaro Vidal", ""], ["Probst", "Daniel", ""], ["Shaxted", "Matthew", ""], ["Wilde", "Michael", ""], ["Senecal", "Peter Kelly", ""]]}, {"id": "2101.02655", "submitter": "Pawe{\\l} Zawistowski", "authors": "Bart{\\l}omiej Twardowski, Pawe{\\l} Zawistowski, Szymon Zaborowski", "title": "Metric Learning for Session-based Recommendations", "comments": "Accepted at European Conference On Information Retrieval (ECIR) 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Session-based recommenders, used for making predictions out of users'\nuninterrupted sequences of actions, are attractive for many applications. Here,\nfor this task we propose using metric learning, where a common embedding space\nfor sessions and items is created, and distance measures dissimilarity between\nthe provided sequence of users' events and the next action. We discuss and\ncompare metric learning approaches to commonly used learning-to-rank methods,\nwhere some synergies exist. We propose a simple architecture for problem\nanalysis and demonstrate that neither extensively big nor deep architectures\nare necessary in order to outperform existing methods. The experimental results\nagainst strong baselines on four datasets are provided with an ablation study.\n", "versions": [{"version": "v1", "created": "Thu, 7 Jan 2021 17:51:04 GMT"}], "update_date": "2021-01-08", "authors_parsed": [["Twardowski", "Bart\u0142omiej", ""], ["Zawistowski", "Pawe\u0142", ""], ["Zaborowski", "Szymon", ""]]}, {"id": "2101.02656", "submitter": "Tugba Erpek", "authors": "Yalin E. Sagduyu, Tugba Erpek, Yi Shi", "title": "Adversarial Machine Learning for 5G Communications Security", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Machine learning provides automated means to capture complex dynamics of\nwireless spectrum and support better understanding of spectrum resources and\ntheir efficient utilization. As communication systems become smarter with\ncognitive radio capabilities empowered by machine learning to perform critical\ntasks such as spectrum awareness and spectrum sharing, they also become\nsusceptible to new vulnerabilities due to the attacks that target the machine\nlearning applications. This paper identifies the emerging attack surface of\nadversarial machine learning and corresponding attacks launched against\nwireless communications in the context of 5G systems. The focus is on attacks\nagainst (i) spectrum sharing of 5G communications with incumbent users such as\nin the Citizens Broadband Radio Service (CBRS) band and (ii) physical layer\nauthentication of 5G User Equipment (UE) to support network slicing. For the\nfirst attack, the adversary transmits during data transmission or spectrum\nsensing periods to manipulate the signal-level inputs to the deep learning\nclassifier that is deployed at the Environmental Sensing Capability (ESC) to\nsupport the 5G system. For the second attack, the adversary spoofs wireless\nsignals with the generative adversarial network (GAN) to infiltrate the\nphysical layer authentication mechanism based on a deep learning classifier\nthat is deployed at the 5G base station. Results indicate major vulnerabilities\nof 5G systems to adversarial machine learning. To sustain the 5G system\noperations in the presence of adversaries, a defense mechanism is presented to\nincrease the uncertainty of the adversary in training the surrogate model used\nfor launching its subsequent attacks.\n", "versions": [{"version": "v1", "created": "Thu, 7 Jan 2021 17:52:17 GMT"}], "update_date": "2021-01-08", "authors_parsed": [["Sagduyu", "Yalin E.", ""], ["Erpek", "Tugba", ""], ["Shi", "Yi", ""]]}, {"id": "2101.02667", "submitter": "Seyed Abolfazl Ghasemzadeh", "authors": "Seyed Abolfazl Ghasemzadeh, Erfan Bank Tavakoli, Mehdi Kamal, Ali\n  Afzali-Kusha, Massoud Pedram", "title": "BRDS: An FPGA-based LSTM Accelerator with Row-Balanced Dual-Ratio\n  Sparsification", "comments": "8 pages, 9 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, first, a hardware-friendly pruning algorithm for reducing\nenergy consumption and improving the speed of Long Short-Term Memory (LSTM)\nneural network accelerators is presented. Next, an FPGA-based platform for\nefficient execution of the pruned networks based on the proposed algorithm is\nintroduced. By considering the sensitivity of two weight matrices of the LSTM\nmodels in pruning, different sparsity ratios (i.e., dual-ratio sparsity) are\napplied to these weight matrices. To reduce memory accesses, a row-wise\nsparsity pattern is adopted. The proposed hardware architecture makes use of\ncomputation overlapping and pipelining to achieve low-power and high-speed. The\neffectiveness of the proposed pruning algorithm and accelerator is assessed\nunder some benchmarks for natural language processing, binary sentiment\nclassification, and speech recognition. Results show that, e.g., compared to a\nrecently published work in this field, the proposed accelerator could provide\nup to 272% higher effective GOPS/W and the perplexity error is reduced by up to\n1.4% for the PTB dataset.\n", "versions": [{"version": "v1", "created": "Thu, 7 Jan 2021 18:23:48 GMT"}], "update_date": "2021-01-08", "authors_parsed": [["Ghasemzadeh", "Seyed Abolfazl", ""], ["Tavakoli", "Erfan Bank", ""], ["Kamal", "Mehdi", ""], ["Afzali-Kusha", "Ali", ""], ["Pedram", "Massoud", ""]]}, {"id": "2101.02672", "submitter": "Prarthana Bhattacharyya", "authors": "Prarthana Bhattacharyya, Chengjie Huang and Krzysztof Czarnecki", "title": "SA-Det3D: Self-Attention Based Context-Aware 3D Object Detection", "comments": "16 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Existing point-cloud based 3D object detectors use convolution-like operators\nto process information in a local neighbourhood with fixed-weight kernels and\naggregate global context hierarchically. However, non-local neural networks and\nself-attention for 2D vision have shown that explicitly modeling long-range\ninteractions can lead to more robust and competitive models. In this paper, we\npropose two variants of self-attention for contextual modeling in 3D object\ndetection by augmenting convolutional features with self-attention features. We\nfirst incorporate the pairwise self-attention mechanism into the current\nstate-of-the-art BEV, voxel and point-based detectors and show consistent\nimprovement over strong baseline models of up to 1.5 3D AP while simultaneously\nreducing their parameter footprint and computational cost by 15-80% and 30-50%,\nrespectively, on the KITTI validation set. We next propose a self-attention\nvariant that samples a subset of the most representative features by learning\ndeformations over randomly sampled locations. This not only allows us to scale\nexplicit global contextual modeling to larger point-clouds, but also leads to\nmore discriminative and informative feature descriptors. Our method can be\nflexibly applied to most state-of-the-art detectors with increased accuracy and\nparameter and compute efficiency. We show our proposed method improves 3D\nobject detection performance on KITTI, nuScenes and Waymo Open datasets. Code\nis available at https://github.com/AutoVision-cloud/SA-Det3D.\n", "versions": [{"version": "v1", "created": "Thu, 7 Jan 2021 18:30:32 GMT"}, {"version": "v2", "created": "Tue, 16 Mar 2021 16:53:42 GMT"}, {"version": "v3", "created": "Wed, 17 Mar 2021 17:35:41 GMT"}], "update_date": "2021-03-18", "authors_parsed": [["Bhattacharyya", "Prarthana", ""], ["Huang", "Chengjie", ""], ["Czarnecki", "Krzysztof", ""]]}, {"id": "2101.02689", "submitter": "Arno Blaas", "authors": "Arno Blaas, Stephen J. Roberts", "title": "The Effect of Prior Lipschitz Continuity on the Adversarial Robustness\n  of Bayesian Neural Networks", "comments": "4 pages, 2 tables, AAAI 2021 Workshop Towards Robust, Secure and\n  Efficient Machine Learning", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is desirable, and often a necessity, for machine learning models to be\nrobust against adversarial attacks. This is particularly true for Bayesian\nmodels, as they are well-suited for safety-critical applications, in which\nadversarial attacks can have catastrophic outcomes. In this work, we take a\ndeeper look at the adversarial robustness of Bayesian Neural Networks (BNNs).\nIn particular, we consider whether the adversarial robustness of a BNN can be\nincreased by model choices, particularly the Lipschitz continuity induced by\nthe prior. Conducting in-depth analysis on the case of i.i.d., zero-mean\nGaussian priors and posteriors approximated via mean-field variational\ninference, we find evidence that adversarial robustness is indeed sensitive to\nthe prior variance.\n", "versions": [{"version": "v1", "created": "Thu, 7 Jan 2021 18:51:05 GMT"}], "update_date": "2021-01-08", "authors_parsed": [["Blaas", "Arno", ""], ["Roberts", "Stephen J.", ""]]}, {"id": "2101.02696", "submitter": "Karan Chadha", "authors": "Karan Chadha, Gary Cheng, John C. Duchi", "title": "Accelerated, Optimal, and Parallel: Some Results on Model-Based\n  Stochastic Optimization", "comments": "24 pages, 17 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We extend the Approximate-Proximal Point (aProx) family of model-based\nmethods for solving stochastic convex optimization problems, including\nstochastic subgradient, proximal point, and bundle methods, to the minibatch\nand accelerated setting. To do so, we propose specific model-based algorithms\nand an acceleration scheme for which we provide non-asymptotic convergence\nguarantees, which are order-optimal in all problem-dependent constants and\nprovide linear speedup in minibatch size, while maintaining the desirable\nrobustness traits (e.g. to stepsize) of the aProx family. Additionally, we show\nimproved convergence rates and matching lower bounds identifying new\nfundamental constants for \"interpolation\" problems, whose importance in\nstatistical machine learning is growing; this, for example, gives a\nparallelization strategy for alternating projections. We corroborate our\ntheoretical results with empirical testing to demonstrate the gains accurate\nmodeling, acceleration, and minibatching provide.\n", "versions": [{"version": "v1", "created": "Thu, 7 Jan 2021 18:58:39 GMT"}], "update_date": "2021-01-08", "authors_parsed": [["Chadha", "Karan", ""], ["Cheng", "Gary", ""], ["Duchi", "John C.", ""]]}, {"id": "2101.02703", "submitter": "Anastasios Angelopoulos", "authors": "Stephen Bates and Anastasios Angelopoulos and Lihua Lei and Jitendra\n  Malik and Michael I. Jordan", "title": "Distribution-Free, Risk-Controlling Prediction Sets", "comments": "Project website available at\n  http://www.angelopoulos.ai/blog/posts/rcps/ and codebase available at\n  https://github.com/aangelopoulos/rcps", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While improving prediction accuracy has been the focus of machine learning in\nrecent years, this alone does not suffice for reliable decision-making.\nDeploying learning systems in consequential settings also requires calibrating\nand communicating the uncertainty of predictions. To convey instance-wise\nuncertainty for prediction tasks, we show how to generate set-valued\npredictions from a black-box predictor that control the expected loss on future\ntest points at a user-specified level. Our approach provides explicit\nfinite-sample guarantees for any dataset by using a holdout set to calibrate\nthe size of the prediction sets. This framework enables simple,\ndistribution-free, rigorous error control for many tasks, and we demonstrate it\nin five large-scale machine learning problems: (1) classification problems\nwhere some mistakes are more costly than others; (2) multi-label\nclassification, where each observation has multiple associated labels; (3)\nclassification problems where the labels have a hierarchical structure; (4)\nimage segmentation, where we wish to predict a set of pixels containing an\nobject of interest; and (5) protein structure prediction. Lastly, we discuss\nextensions to uncertainty quantification for ranking, metric learning and\ndistributionally robust learning.\n", "versions": [{"version": "v1", "created": "Thu, 7 Jan 2021 18:59:33 GMT"}, {"version": "v2", "created": "Sat, 30 Jan 2021 03:48:34 GMT"}], "update_date": "2021-02-02", "authors_parsed": [["Bates", "Stephen", ""], ["Angelopoulos", "Anastasios", ""], ["Lei", "Lihua", ""], ["Malik", "Jitendra", ""], ["Jordan", "Michael I.", ""]]}, {"id": "2101.02722", "submitter": "Rico Jonschkowski", "authors": "Austin Stone, Oscar Ramirez, Kurt Konolige, Rico Jonschkowski", "title": "The Distracting Control Suite -- A Challenging Benchmark for\n  Reinforcement Learning from Pixels", "comments": "Code available at\n  https://github.com/google-research/google-research/tree/master/distracting_control", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AI cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Robots have to face challenging perceptual settings, including changes in\nviewpoint, lighting, and background. Current simulated reinforcement learning\n(RL) benchmarks such as DM Control provide visual input without such\ncomplexity, which limits the transfer of well-performing methods to the real\nworld. In this paper, we extend DM Control with three kinds of visual\ndistractions (variations in background, color, and camera pose) to produce a\nnew challenging benchmark for vision-based control, and we analyze state of the\nart RL algorithms in these settings. Our experiments show that current RL\nmethods for vision-based control perform poorly under distractions, and that\ntheir performance decreases with increasing distraction complexity, showing\nthat new methods are needed to cope with the visual complexities of the real\nworld. We also find that combinations of multiple distraction types are more\ndifficult than a mere combination of their individual effects.\n", "versions": [{"version": "v1", "created": "Thu, 7 Jan 2021 19:03:34 GMT"}], "update_date": "2021-01-11", "authors_parsed": [["Stone", "Austin", ""], ["Ramirez", "Oscar", ""], ["Konolige", "Kurt", ""], ["Jonschkowski", "Rico", ""]]}, {"id": "2101.02726", "submitter": "Joachim Sicking", "authors": "Joachim Sicking, Maram Akila, Maximilian Pintz, Tim Wirtz, Asja\n  Fischer, Stefan Wrobel", "title": "A Novel Regression Loss for Non-Parametric Uncertainty Optimization", "comments": "Accepted at the 3rd Symposium on Advances in Approximate Bayesian\n  Inference (AABI), code is available on:\n  https://github.com/fraunhofer-iais/second-moment-loss. arXiv admin note:\n  substantial text overlap with arXiv:2012.12687", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Quantification of uncertainty is one of the most promising approaches to\nestablish safe machine learning. Despite its importance, it is far from being\ngenerally solved, especially for neural networks. One of the most commonly used\napproaches so far is Monte Carlo dropout, which is computationally cheap and\neasy to apply in practice. However, it can underestimate the uncertainty. We\npropose a new objective, referred to as second-moment loss (SML), to address\nthis issue. While the full network is encouraged to model the mean, the dropout\nnetworks are explicitly used to optimize the model variance. We intensively\nstudy the performance of the new objective on various UCI regression datasets.\nComparing to the state-of-the-art of deep ensembles, SML leads to comparable\nprediction accuracies and uncertainty estimates while only requiring a single\nmodel. Under distribution shift, we observe moderate improvements. As a side\nresult, we introduce an intuitive Wasserstein distance-based uncertainty\nmeasure that is non-saturating and thus allows to resolve quality differences\nbetween any two uncertainty estimates.\n", "versions": [{"version": "v1", "created": "Thu, 7 Jan 2021 19:12:06 GMT"}], "update_date": "2021-01-11", "authors_parsed": [["Sicking", "Joachim", ""], ["Akila", "Maram", ""], ["Pintz", "Maximilian", ""], ["Wirtz", "Tim", ""], ["Fischer", "Asja", ""], ["Wrobel", "Stefan", ""]]}, {"id": "2101.02729", "submitter": "Prabuddha Chakraborty", "authors": "Prabuddha Chakraborty and Swarup Bhunia", "title": "Neural Storage: A New Paradigm of Elastic Memory", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.AR cs.ET cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Storage and retrieval of data in a computer memory plays a major role in\nsystem performance. Traditionally, computer memory organization is static -\ni.e., they do not change based on the application-specific characteristics in\nmemory access behaviour during system operation. Specifically, the association\nof a data block with a search pattern (or cues) as well as the granularity of a\nstored data do not evolve. Such a static nature of computer memory, we observe,\nnot only limits the amount of data we can store in a given physical storage,\nbut it also misses the opportunity for dramatic performance improvement in\nvarious applications. On the contrary, human memory is characterized by\nseemingly infinite plasticity in storing and retrieving data - as well as\ndynamically creating/updating the associations between data and corresponding\ncues. In this paper, we introduce Neural Storage (NS), a brain-inspired\nlearning memory paradigm that organizes the memory as a flexible neural memory\nnetwork. In NS, the network structure, strength of associations, and\ngranularity of the data adjust continuously during system operation, providing\nunprecedented plasticity and performance benefits. We present the associated\nstorage/retrieval/retention algorithms in NS, which integrate a formalized\nlearning process. Using a full-blown operational model, we demonstrate that NS\nachieves an order of magnitude improvement in memory access performance for two\nrepresentative applications when compared to traditional content-based memory.\n", "versions": [{"version": "v1", "created": "Thu, 7 Jan 2021 19:19:25 GMT"}], "update_date": "2021-01-11", "authors_parsed": [["Chakraborty", "Prabuddha", ""], ["Bhunia", "Swarup", ""]]}, {"id": "2101.02737", "submitter": "Antonia Stern", "authors": "Antonia Stern, Lalith Sharan, Gabriele Romano, Sven Koehler, Matthias\n  Karck, Raffaele De Simone, Ivo Wolf, Sandy Engelhardt", "title": "Heatmap-based 2D Landmark Detection with a Varying Number of Landmarks", "comments": "accepted for BVM 2021, 6 pages, 2 figures, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mitral valve repair is a surgery to restore the function of the mitral valve.\nTo achieve this, a prosthetic ring is sewed onto the mitral annulus. Analyzing\nthe sutures, which are punctured through the annulus for ring implantation, can\nbe useful in surgical skill assessment, for quantitative surgery and for\npositioning a virtual prosthetic ring model in the scene via augmented reality.\nThis work presents a neural network approach which detects the sutures in\nendoscopic images of mitral valve repair and therefore solves a landmark\ndetection problem with varying amount of landmarks, as opposed to most other\nexisting deep learning-based landmark detection approaches. The neural network\nis trained separately on two data collections from different domains with the\nsame architecture and hyperparameter settings. The datasets consist of more\nthan 1,300 stereo frame pairs each, with a total over 60,000 annotated\nlandmarks. The proposed heatmap-based neural network achieves a mean positive\npredictive value (PPV) of 66.68$\\pm$4.67% and a mean true positive rate (TPR)\nof 24.45$\\pm$5.06% on the intraoperative test dataset and a mean PPV of\n81.50\\pm5.77\\% and a mean TPR of 61.60$\\pm$6.11% on a dataset recorded during\nsurgical simulation. The best detection results are achieved when the camera is\npositioned above the mitral valve with good illumination. A detection from a\nsideward view is also possible if the mitral valve is well perceptible.\n", "versions": [{"version": "v1", "created": "Thu, 7 Jan 2021 19:42:44 GMT"}], "update_date": "2021-01-11", "authors_parsed": [["Stern", "Antonia", ""], ["Sharan", "Lalith", ""], ["Romano", "Gabriele", ""], ["Koehler", "Sven", ""], ["Karck", "Matthias", ""], ["De Simone", "Raffaele", ""], ["Wolf", "Ivo", ""], ["Engelhardt", "Sandy", ""]]}, {"id": "2101.02744", "submitter": "Wei Chen", "authors": "Wei Chen and Arun Ramamurthy", "title": "Deep Generative Model for Efficient 3D Airfoil Parameterization and\n  Generation", "comments": null, "journal-ref": null, "doi": "10.2514/6.2021-1690", "report-no": null, "categories": "cs.LG cs.CE cs.CG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In aerodynamic shape optimization, the convergence and computational cost are\ngreatly affected by the representation capacity and compactness of the design\nspace. Previous research has demonstrated that using a deep generative model to\nparameterize two-dimensional (2D) airfoils achieves high representation\ncapacity/compactness, which significantly benefits shape optimization. In this\npaper, we propose a deep generative model, Free-Form Deformation Generative\nAdversarial Networks (FFD-GAN), that provides an efficient parameterization for\nthree-dimensional (3D) aerodynamic/hydrodynamic shapes like aircraft wings,\nturbine blades, car bodies, and hulls. The learned model maps a compact set of\ndesign variables to 3D surface points representing the shape. We ensure the\nsurface smoothness and continuity of generated geometries by incorporating an\nFFD layer into the generative model. We demonstrate FFD-GAN's performance using\na wing shape design example. The results show that FFD-GAN can generate\nrealistic designs and form a reasonable parameterization. We further\ndemonstrate FFD-GAN's high representation compactness and capacity by testing\nits design space coverage, the feasibility ratio of the design space, and its\nperformance in design optimization. We demonstrate that over 94% feasibility\nratio is achieved among wings randomly generated by the FFD-GAN, while FFD and\nB-spline only achieve less than 31%. We also show that the FFD-GAN leads to an\norder of magnitude faster convergence in a wing shape optimization problem,\ncompared to the FFD and the B-spline parameterizations.\n", "versions": [{"version": "v1", "created": "Thu, 7 Jan 2021 19:55:23 GMT"}], "update_date": "2021-01-11", "authors_parsed": [["Chen", "Wei", ""], ["Ramamurthy", "Arun", ""]]}, {"id": "2101.02757", "submitter": "Maciej A. Czyzewski", "authors": "Maciej A. Czyzewski", "title": "Transfer Learning Between Different Architectures Via Weights Injection", "comments": "6 pages; 7 figures; draft", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This work presents a naive algorithm for parameter transfer between different\narchitectures with a computationally cheap injection technique (which does not\nrequire data). The primary objective is to speed up the training of neural\nnetworks from scratch. It was found in this study that transferring knowledge\nfrom any architecture was superior to Kaiming and Xavier for initialization. In\nconclusion, the method presented is found to converge faster, which makes it a\ndrop-in replacement for classical methods. The method involves: 1) matching:\nthe layers of the pre-trained model with the targeted model; 2) injection: the\ntensor is transformed into a desired shape. This work provides a comparison of\nsimilarity between the current SOTA architectures (ImageNet), by utilising TLI\n(Transfer Learning by Injection) score.\n", "versions": [{"version": "v1", "created": "Thu, 7 Jan 2021 20:42:35 GMT"}], "update_date": "2021-01-11", "authors_parsed": [["Czyzewski", "Maciej A.", ""]]}, {"id": "2101.02766", "submitter": "Han Ching Ou", "authors": "Han-Ching Ou, Haipeng Chen, Shahin Jabbari and Milind Tambe", "title": "Active Screening for Recurrent Diseases: A Reinforcement Learning\n  Approach", "comments": "The short version of this paper appears in the proceedings of\n  AAMAS-21", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.MA", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Active screening is a common approach in controlling the spread of recurring\ninfectious diseases such as tuberculosis and influenza. In this approach,\nhealth workers periodically select a subset of population for screening.\nHowever, given the limited number of health workers, only a small subset of the\npopulation can be visited in any given time period. Given the recurrent nature\nof the disease and rapid spreading, the goal is to minimize the number of\ninfections over a long time horizon. Active screening can be formalized as a\nsequential combinatorial optimization over the network of people and their\nconnections. The main computational challenges in this formalization arise from\ni) the combinatorial nature of the problem, ii) the need of sequential planning\nand iii) the uncertainties in the infectiousness states of the population.\n  Previous works on active screening fail to scale to large time horizon while\nfully considering the future effect of current interventions. In this paper, we\npropose a novel reinforcement learning (RL) approach based on Deep Q-Networks\n(DQN), with several innovative adaptations that are designed to address the\nabove challenges. First, we use graph convolutional networks (GCNs) to\nrepresent the Q-function that exploit the node correlations of the underlying\ncontact network. Second, to avoid solving a combinatorial optimization problem\nin each time period, we decompose the node set selection as a sub-sequence of\ndecisions, and further design a two-level RL framework that solves the problem\nin a hierarchical way. Finally, to speed-up the slow convergence of RL which\narises from reward sparseness, we incorporate ideas from curriculum learning\ninto our hierarchical RL approach. We evaluate our RL algorithm on several\nreal-world networks.\n", "versions": [{"version": "v1", "created": "Thu, 7 Jan 2021 21:07:35 GMT"}, {"version": "v2", "created": "Wed, 27 Jan 2021 16:41:05 GMT"}, {"version": "v3", "created": "Mon, 19 Apr 2021 16:43:43 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Ou", "Han-Ching", ""], ["Chen", "Haipeng", ""], ["Jabbari", "Shahin", ""], ["Tambe", "Milind", ""]]}, {"id": "2101.02767", "submitter": "Joris Gu\\'erin", "authors": "Joris Guerin, Stephane Thiery, Eric Nyiri, Olivier Gibaru, Byron Boots", "title": "Combining pretrained CNN feature extractors to enhance clustering of\n  complex natural images", "comments": "21 pages, 16 figures, 10 tables, preprint of our paper published in\n  Neurocomputing", "journal-ref": "Guerin, J., Thiery, S., Nyiri, E., Gibaru, O., & Boots, B. (2021).\n  Combining pretrained CNN feature extractors to enhance clustering of complex\n  natural images. Neurocomputing, 423, 551-571", "doi": "10.1016/j.neucom.2020.10.068", "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, a common starting point for solving complex unsupervised image\nclassification tasks is to use generic features, extracted with deep\nConvolutional Neural Networks (CNN) pretrained on a large and versatile dataset\n(ImageNet). However, in most research, the CNN architecture for feature\nextraction is chosen arbitrarily, without justification. This paper aims at\nproviding insight on the use of pretrained CNN features for image clustering\n(IC). First, extensive experiments are conducted and show that, for a given\ndataset, the choice of the CNN architecture for feature extraction has a huge\nimpact on the final clustering. These experiments also demonstrate that proper\nextractor selection for a given IC task is difficult. To solve this issue, we\npropose to rephrase the IC problem as a multi-view clustering (MVC) problem\nthat considers features extracted from different architectures as different\n\"views\" of the same data. This approach is based on the assumption that\ninformation contained in the different CNN may be complementary, even when\npretrained on the same data. We then propose a multi-input neural network\narchitecture that is trained end-to-end to solve the MVC problem effectively.\nThis approach is tested on nine natural image datasets, and produces\nstate-of-the-art results for IC.\n", "versions": [{"version": "v1", "created": "Thu, 7 Jan 2021 21:23:04 GMT"}], "update_date": "2021-01-11", "authors_parsed": [["Guerin", "Joris", ""], ["Thiery", "Stephane", ""], ["Nyiri", "Eric", ""], ["Gibaru", "Olivier", ""], ["Boots", "Byron", ""]]}, {"id": "2101.02776", "submitter": "Peyman Mohajerin Esfahani", "authors": "Armin Eftekhari and Peyman Mohajerin Esfahani", "title": "The Nonconvex Geometry of Linear Inverse Problems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.LG math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The gauge function, closely related to the atomic norm, measures the\ncomplexity of a statistical model, and has found broad applications in machine\nlearning and statistical signal processing. In a high-dimensional learning\nproblem, the gauge function attempts to safeguard against overfitting by\npromoting a sparse (concise) representation within the learning alphabet.\n  In this work, within the context of linear inverse problems, we pinpoint the\nsource of its success, but also argue that the applicability of the gauge\nfunction is inherently limited by its convexity, and showcase several learning\nproblems where the classical gauge function theory fails. We then introduce a\nnew notion of statistical complexity, gauge$_p$ function, which overcomes the\nlimitations of the gauge function. The gauge$_p$ function is a simple\ngeneralization of the gauge function that can tightly control the sparsity of a\nstatistical model within the learning alphabet and, perhaps surprisingly, draws\nfurther inspiration from the Burer-Monteiro factorization in computational\nmathematics.\n  We also propose a new learning machine, with the building block of gauge$_p$\nfunction, and arm this machine with a number of statistical guarantees. The\npotential of the proposed gauge$_p$ function theory is then studied for two\nstylized applications. Finally, we discuss the computational aspects and, in\nparticular, suggest a tractable numerical algorithm for implementing the new\nlearning machine.\n", "versions": [{"version": "v1", "created": "Thu, 7 Jan 2021 21:55:08 GMT"}], "update_date": "2021-01-11", "authors_parsed": [["Eftekhari", "Armin", ""], ["Esfahani", "Peyman Mohajerin", ""]]}, {"id": "2101.02780", "submitter": "Tanujay Saha", "authors": "Tanujay Saha, Najwa Aaraj, Neel Ajjarapu, Niraj K. Jha", "title": "SHARKS: Smart Hacking Approaches for RisK Scanning in Internet-of-Things\n  and Cyber-Physical Systems based on Machine Learning", "comments": "This article has been accepted in IEEE Transactions on Emerging\n  Topics in Computing. 17 pages, 12 figures, IEEE copyright", "journal-ref": "IEEE Transactions on Emerging Topics in Computing, 2021", "doi": "10.1109/TETC.2021.3050733", "report-no": null, "categories": "cs.CR cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Cyber-physical systems (CPS) and Internet-of-Things (IoT) devices are\nincreasingly being deployed across multiple functionalities, ranging from\nhealthcare devices and wearables to critical infrastructures, e.g., nuclear\npower plants, autonomous vehicles, smart cities, and smart homes. These devices\nare inherently not secure across their comprehensive software, hardware, and\nnetwork stacks, thus presenting a large attack surface that can be exploited by\nhackers. In this article, we present an innovative technique for detecting\nunknown system vulnerabilities, managing these vulnerabilities, and improving\nincident response when such vulnerabilities are exploited. The novelty of this\napproach lies in extracting intelligence from known real-world CPS/IoT attacks,\nrepresenting them in the form of regular expressions, and employing machine\nlearning (ML) techniques on this ensemble of regular expressions to generate\nnew attack vectors and security vulnerabilities. Our results show that 10 new\nattack vectors and 122 new vulnerability exploits can be successfully generated\nthat have the potential to exploit a CPS or an IoT ecosystem. The ML\nmethodology achieves an accuracy of 97.4% and enables us to predict these\nattacks efficiently with an 87.2% reduction in the search space. We demonstrate\nthe application of our method to the hacking of the in-vehicle network of a\nconnected car. To defend against the known attacks and possible novel exploits,\nwe discuss a defense-in-depth mechanism for various classes of attacks and the\nclassification of data targeted by such attacks. This defense mechanism\noptimizes the cost of security measures based on the sensitivity of the\nprotected resource, thus incentivizing its adoption in real-world CPS/IoT by\ncybersecurity practitioners.\n", "versions": [{"version": "v1", "created": "Thu, 7 Jan 2021 22:01:30 GMT"}], "update_date": "2021-06-02", "authors_parsed": [["Saha", "Tanujay", ""], ["Aaraj", "Najwa", ""], ["Ajjarapu", "Neel", ""], ["Jha", "Niraj K.", ""]]}, {"id": "2101.02792", "submitter": "Hongjing Zhang", "authors": "Hongjing Zhang, Tianyang Zhan, Sugato Basu, Ian Davidson", "title": "A Framework for Deep Constrained Clustering", "comments": "Data Mining and Knowledge Discovery, 2021. arXiv admin note:\n  substantial text overlap with arXiv:1901.10061", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The area of constrained clustering has been extensively explored by\nresearchers and used by practitioners. Constrained clustering formulations\nexist for popular algorithms such as k-means, mixture models, and spectral\nclustering but have several limitations. A fundamental strength of deep\nlearning is its flexibility, and here we explore a deep learning framework for\nconstrained clustering and in particular explore how it can extend the field of\nconstrained clustering. We show that our framework can not only handle standard\ntogether/apart constraints (without the well documented negative effects\nreported earlier) generated from labeled side information but more complex\nconstraints generated from new types of side information such as continuous\nvalues and high-level domain knowledge. Furthermore, we propose an efficient\ntraining paradigm that is generally applicable to these four types of\nconstraints. We validate the effectiveness of our approach by empirical results\non both image and text datasets. We also study the robustness of our framework\nwhen learning with noisy constraints and show how different components of our\nframework contribute to the final performance. Our source code is available at\n$\\href{https://github.com/blueocean92/deep_constrained_clustering}{\\text{URL}}$.\n", "versions": [{"version": "v1", "created": "Thu, 7 Jan 2021 22:49:06 GMT"}], "update_date": "2021-01-11", "authors_parsed": [["Zhang", "Hongjing", ""], ["Zhan", "Tianyang", ""], ["Basu", "Sugato", ""], ["Davidson", "Ian", ""]]}, {"id": "2101.02797", "submitter": "Nisreen Ali", "authors": "Nisreen AbdAllah and Serestina Viriri", "title": "Off-Line Arabic Handwritten Words Segmentation using Morphological\n  Operators", "comments": "16 pages,27 figures", "journal-ref": "Signal & Image Processing: An International Journal (SIPIJ)\n  Vol.11, No.6, December 2020", "doi": "10.5121/sipij.2020.11602", "report-no": null, "categories": "cs.CV cs.CL cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The main aim of this study is the assessment and discussion of a model for\nhand-written Arabic through segmentation. The framework is proposed based on\nthree steps: pre-processing, segmentation, and evaluation. In the\npre-processing step, morphological operators are applied for Connecting Gaps\n(CGs) in written words. Gaps happen when pen lifting-off during writing,\nscanning documents, or while converting images to binary type. In the\nsegmentation step, first removed the small diacritics then bounded a connected\ncomponent to segment offline words. Huge data was utilized in the proposed\nmodel for applying a variety of handwriting styles so that to be more\ncompatible with real-life applications. Consequently, on the automatic\nevaluation stage, selected randomly 1,131 images from the IESK-ArDB database,\nand then segmented into sub-words. After small gaps been connected, the model\nperformance evaluation had been reached 88% against the standard ground truth\nof the database. The proposed model achieved the highest accuracy when compared\nwith the related works.\n", "versions": [{"version": "v1", "created": "Thu, 7 Jan 2021 23:38:53 GMT"}], "update_date": "2021-01-11", "authors_parsed": [["AbdAllah", "Nisreen", ""], ["Viriri", "Serestina", ""]]}, {"id": "2101.02808", "submitter": "Shangtong Zhang", "authors": "Shangtong Zhang, Yi Wan, Richard S. Sutton, Shimon Whiteson", "title": "Average-Reward Off-Policy Policy Evaluation with Function Approximation", "comments": "ICML 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider off-policy policy evaluation with function approximation (FA) in\naverage-reward MDPs, where the goal is to estimate both the reward rate and the\ndifferential value function. For this problem, bootstrapping is necessary and,\nalong with off-policy learning and FA, results in the deadly triad (Sutton &\nBarto, 2018). To address the deadly triad, we propose two novel algorithms,\nreproducing the celebrated success of Gradient TD algorithms in the\naverage-reward setting. In terms of estimating the differential value function,\nthe algorithms are the first convergent off-policy linear function\napproximation algorithms. In terms of estimating the reward rate, the\nalgorithms are the first convergent off-policy linear function approximation\nalgorithms that do not require estimating the density ratio. We demonstrate\nempirically the advantage of the proposed algorithms, as well as their\nnonlinear variants, over a competitive density-ratio-based approach, in a\nsimple domain as well as challenging robot simulation tasks.\n", "versions": [{"version": "v1", "created": "Fri, 8 Jan 2021 00:43:04 GMT"}, {"version": "v2", "created": "Thu, 27 May 2021 22:36:31 GMT"}], "update_date": "2021-05-31", "authors_parsed": [["Zhang", "Shangtong", ""], ["Wan", "Yi", ""], ["Sutton", "Richard S.", ""], ["Whiteson", "Shimon", ""]]}, {"id": "2101.02815", "submitter": "Prathamesh Deshpande", "authors": "Prathamesh Deshpande, Kamlesh Marathe, Abir De, Sunita Sarawagi", "title": "Long Horizon Forecasting With Temporal Point Processes", "comments": "9 pages, 4 figures", "journal-ref": null, "doi": "10.1145/3437963.3441740", "report-no": null, "categories": "cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  In recent years, marked temporal point processes (MTPPs) have emerged as a\npowerful modeling machinery to characterize asynchronous events in a wide\nvariety of applications. MTPPs have demonstrated significant potential in\npredicting event-timings, especially for events arriving in near future.\nHowever, due to current design choices, MTPPs often show poor predictive\nperformance at forecasting event arrivals in distant future. To ameliorate this\nlimitation, in this paper, we design DualTPP which is specifically well-suited\nto long horizon event forecasting. DualTPP has two components. The first\ncomponent is an intensity free MTPP model, which captures microscopic or\ngranular level signals of the event dynamics by modeling the time of future\nevents. The second component takes a different dual perspective of modeling\naggregated counts of events in a given time-window, thus encapsulating\nmacroscopic event dynamics. Then we develop a novel inference framework jointly\nover the two models % for efficiently forecasting long horizon events by\nsolving a sequence of constrained quadratic optimization problems. Experiments\nwith a diverse set of real datasets show that DualTPP outperforms existing MTPP\nmethods on long horizon forecasting by substantial margins, achieving almost an\norder of magnitude reduction in Wasserstein distance between actual events and\nforecasts.\n", "versions": [{"version": "v1", "created": "Fri, 8 Jan 2021 01:09:09 GMT"}, {"version": "v2", "created": "Sun, 7 Mar 2021 16:24:37 GMT"}], "update_date": "2021-03-09", "authors_parsed": [["Deshpande", "Prathamesh", ""], ["Marathe", "Kamlesh", ""], ["De", "Abir", ""], ["Sarawagi", "Sunita", ""]]}, {"id": "2101.02831", "submitter": "Becky Mashaido", "authors": "Becky Mashaido and Winston Moh Tangongho", "title": "A Tale of Fairness Revisited: Beyond Adversarial Learning for Deep\n  Neural Network Fairness", "comments": "6 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motivated by the need for fair algorithmic decision making in the age of\nautomation and artificially-intelligent technology, this technical report\nprovides a theoretical insight into adversarial training for fairness in deep\nlearning. We build upon previous work in adversarial fairness, show the\npersistent tradeoff between fair predictions and model performance, and explore\nfurther mechanisms that help in offsetting this tradeoff.\n", "versions": [{"version": "v1", "created": "Fri, 8 Jan 2021 03:13:44 GMT"}], "update_date": "2021-01-11", "authors_parsed": [["Mashaido", "Becky", ""], ["Tangongho", "Winston Moh", ""]]}, {"id": "2101.02833", "submitter": "Xueting Zhang", "authors": "Xueting Zhang, Debin Meng, Henry Gouk, Timothy Hospedales", "title": "Shallow Bayesian Meta Learning for Real-World Few-Shot Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Current state-of-the-art few-shot learners focus on developing effective\ntraining procedures for feature representations, before using simple, e.g.\nnearest centroid, classifiers. In this paper we take an orthogonal approach\nthat is agnostic to the features used, and focus exclusively on meta-learning\nthe actual classifier layer. Specifically, we introduce MetaQDA, a Bayesian\nmeta-learning generalisation of the classic quadratic discriminant analysis.\nThis setup has several benefits of interest to practitioners: meta-learning is\nfast and memory efficient, without the need to fine-tune features. It is\nagnostic to the off-the-shelf features chosen, and thus will continue to\nbenefit from advances in feature representations. Empirically, it leads to\nrobust performance in cross-domain few-shot learning and, crucially for\nreal-world applications, it leads to better uncertainty calibration in\npredictions.\n", "versions": [{"version": "v1", "created": "Fri, 8 Jan 2021 03:29:15 GMT"}], "update_date": "2021-01-11", "authors_parsed": [["Zhang", "Xueting", ""], ["Meng", "Debin", ""], ["Gouk", "Henry", ""], ["Hospedales", "Timothy", ""]]}, {"id": "2101.02839", "submitter": "Haojian Zhang", "authors": "Haojian Zhang, Yabin Zhang, Kui Jia, Lei Zhang", "title": "Unsupervised Domain Adaptation of Black-Box Source Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Unsupervised domain adaptation (UDA) aims to learn models for a target domain\nof unlabeled data by transferring knowledge from a labeled source domain. In\nthe traditional UDA setting, labeled source data are assumed to be available\nfor adaptation. Due to increasing concerns for data privacy, source-free UDA is\nhighly appreciated as a new UDA setting, where only a trained source model is\nassumed to be available, while labeled source data remain private. However,\ntrained source models may also be unavailable in practice since source models\nmay have commercial values and exposing source models brings risks to the\nsource domain, e.g., problems of model misuse and white-box attacks. In this\nwork, we study a subtly different setting, named Black-Box Unsupervised Domain\nAdaptation (B$^2$UDA), where only the application programming interface of\nsource model is accessible to the target domain; in other words, the source\nmodel itself is kept as a black-box one. To tackle B$^2$UDA, we propose a\nsimple yet effective method, termed Iterative Learning with Noisy Labels\n(IterLNL). With black-box models as tools of noisy labeling, IterLNL conducts\nnoisy labeling and learning with noisy labels (LNL), iteratively. To facilitate\nthe implementation of LNL in B$^2$UDA, we estimate the noise rate from model\npredictions of unlabeled target data and propose category-wise sampling to\ntackle the unbalanced label noise among categories. Experiments on benchmark\ndatasets show the efficacy of IterLNL. Given neither source data nor source\nmodels, IterLNL performs comparably with traditional UDA methods that make full\nuse of labeled source data.\n", "versions": [{"version": "v1", "created": "Fri, 8 Jan 2021 04:00:49 GMT"}, {"version": "v2", "created": "Sun, 28 Mar 2021 02:13:16 GMT"}], "update_date": "2021-03-30", "authors_parsed": [["Zhang", "Haojian", ""], ["Zhang", "Yabin", ""], ["Jia", "Kui", ""], ["Zhang", "Lei", ""]]}, {"id": "2101.02844", "submitter": "Xiaohan Li", "authors": "Xiaohan Li, Mengqi Zhang, Shu Wu, Zheng Liu, Liang Wang, Philip S. Yu", "title": "Dynamic Graph Collaborative Filtering", "comments": "ICDM 2020 Regular paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Dynamic recommendation is essential for modern recommender systems to provide\nreal-time predictions based on sequential data. In real-world scenarios, the\npopularity of items and interests of users change over time. Based on this\nassumption, many previous works focus on interaction sequences and learn\nevolutionary embeddings of users and items. However, we argue that\nsequence-based models are not able to capture collaborative information among\nusers and items directly. Here we propose Dynamic Graph Collaborative Filtering\n(DGCF), a novel framework leveraging dynamic graphs to capture collaborative\nand sequential relations of both items and users at the same time. We propose\nthree update mechanisms: zero-order 'inheritance', first-order 'propagation',\nand second-order 'aggregation', to represent the impact on a user or item when\na new interaction occurs. Based on them, we update related user and item\nembeddings simultaneously when interactions occur in turn, and then use the\nlatest embeddings to make recommendations. Extensive experiments conducted on\nthree public datasets show that DGCF significantly outperforms the\nstate-of-the-art dynamic recommendation methods up to 30. Our approach achieves\nhigher performance when the dataset contains less action repetition, indicating\nthe effectiveness of integrating dynamic collaborative information.\n", "versions": [{"version": "v1", "created": "Fri, 8 Jan 2021 04:16:24 GMT"}], "update_date": "2021-01-11", "authors_parsed": [["Li", "Xiaohan", ""], ["Zhang", "Mengqi", ""], ["Wu", "Shu", ""], ["Liu", "Zheng", ""], ["Wang", "Liang", ""], ["Yu", "Philip S.", ""]]}, {"id": "2101.02850", "submitter": "Tara Mina", "authors": "Tara Yasmin Mina and Grace Xingxin Gao", "title": "Designing Low-Correlation GPS Spreading Codes with a Natural Evolution\n  Strategy Machine Learning Algorithm", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SP cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the birth of the next-generation GPS III constellation and the upcoming\nlaunch of the Navigation Technology Satellite-3 (NTS-3) testing platform to\nexplore future technologies for GPS, we are indeed entering a new era of\nsatellite navigation. Correspondingly, it is time to revisit the design methods\nof the GPS spreading code families. In this work, we develop a natural\nevolution strategy (NES) machine learning algorithm with a Gaussian proposal\ndistribution which constructs high-quality families of spreading code\nsequences. We demonstrate the ability of our algorithm to achieve better\nmean-squared auto- and cross-correlation than well-chosen families of\nequal-length Gold codes and Weil codes, for sequences of up to length-1023 and\nlength-1031 bits and family sizes of up to 31 codes. Furthermore, we compare\nour algorithm with an analogous genetic algorithm implementation assigned the\nsame code evaluation metric. To the best of the authors' knowledge, this is the\nfirst work to explore using a machine learning approach for designing\nnavigation spreading code sequences.\n", "versions": [{"version": "v1", "created": "Fri, 8 Jan 2021 04:53:11 GMT"}, {"version": "v2", "created": "Fri, 23 Apr 2021 05:48:07 GMT"}], "update_date": "2021-04-26", "authors_parsed": [["Mina", "Tara Yasmin", ""], ["Gao", "Grace Xingxin", ""]]}, {"id": "2101.02860", "submitter": "Khaza Anuarul Hoque", "authors": "Ayesha Siddique, Kanad Basu, Khaza Anuarul Hoque", "title": "Exploring Fault-Energy Trade-offs in Approximate DNN Hardware\n  Accelerators", "comments": "Accepted for publication in the The 22nd International Symposium on\n  Quality Electronic Design (ISQED'21)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Systolic array-based deep neural network (DNN) accelerators have recently\ngained prominence for their low computational cost. However, their high energy\nconsumption poses a bottleneck to their deployment in energy-constrained\ndevices. To address this problem, approximate computing can be employed at the\ncost of some tolerable accuracy loss. However, such small accuracy variations\nmay increase the sensitivity of DNNs towards undesired subtle disturbances,\nsuch as permanent faults. The impact of permanent faults in accurate DNNs has\nbeen thoroughly investigated in the literature. Conversely, the impact of\npermanent faults in approximate DNN accelerators (AxDNNs) is yet\nunder-explored. The impact of such faults may vary with the fault bit\npositions, activation functions and approximation errors in AxDNN layers. Such\ndynamacity poses a considerable challenge to exploring the trade-off between\ntheir energy efficiency and fault resilience in AxDNNs. Towards this, we\npresent an extensive layer-wise and bit-wise fault resilience and energy\nanalysis of different AxDNNs, using the state-of-the-art Evoapprox8b signed\nmultipliers. In particular, we vary the stuck-at-0, stuck-at-1 fault-bit\npositions, and activation functions to study their impact using the most widely\nused MNIST and Fashion-MNIST datasets. Our quantitative analysis shows that the\npermanent faults exacerbate the accuracy loss in AxDNNs when compared to the\naccurate DNN accelerators. For instance, a permanent fault in AxDNNs can lead\nup to 66\\% accuracy loss, whereas the same faulty bit can lead to only 9\\%\naccuracy loss in an accurate DNN accelerator. Our results demonstrate that the\nfault resilience in AxDNNs is orthogonal to the energy efficiency.\n", "versions": [{"version": "v1", "created": "Fri, 8 Jan 2021 05:52:12 GMT"}], "update_date": "2021-01-11", "authors_parsed": [["Siddique", "Ayesha", ""], ["Basu", "Kanad", ""], ["Hoque", "Khaza Anuarul", ""]]}, {"id": "2101.02870", "submitter": "Vishnu Sampathkumar", "authors": "Vishnu Ram Sampathkumar", "title": "ADiag: Graph Neural Network Based Diagnosis of Alzheimer's Disease", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV eess.IV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Alzheimer's Disease (AD) is the most widespread neurodegenerative disease,\naffecting over 50 million people across the world. While its progression cannot\nbe stopped, early and accurate diagnostic testing can drastically improve\nquality of life in patients. Currently, only qualitative means of testing are\nemployed in the form of scoring performance on a battery of cognitive tests.\nThe inherent disadvantage of this method is that the burden of an accurate\ndiagnosis falls on the clinician's competence. Quantitative methods like MRI\nscan assessment are inaccurate at best,due to the elusive nature of visually\nobservable changes in the brain. In lieu of these disadvantages to extant\nmethods of AD diagnosis, we have developed ADiag, a novel quantitative method\nto diagnose AD through GraphSAGE Network and Dense Differentiable Pooling (DDP)\nanalysis of large graphs based on thickness difference between different\nstructural regions of the cortex. Preliminary tests of ADiag have revealed a\nrobust accuracy of 83%, vastly outperforming other qualitative and quantitative\ndiagnostic techniques.\n", "versions": [{"version": "v1", "created": "Fri, 8 Jan 2021 06:23:30 GMT"}], "update_date": "2021-01-11", "authors_parsed": [["Sampathkumar", "Vishnu Ram", ""]]}, {"id": "2101.02874", "submitter": "Jose-Luis Blanco-Claraco", "authors": "Jos\\'e-Luis Blanco-Claraco, Antonio Leanza, Giulio Reina", "title": "A general framework for modeling and dynamic simulation of multibody\n  systems using factor graphs", "comments": "23 pages", "journal-ref": "Nonlinear Dynamics, 2021", "doi": "10.1007/s11071-021-06731-6", "report-no": null, "categories": "cs.RO cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  In this paper, we present a novel general framework grounded in the factor\ngraph theory to solve kinematic and dynamic problems for multi-body systems.\nAlthough the motion of multi-body systems is considered to be a well-studied\nproblem and various methods have been proposed for its solution, a unified\napproach providing an intuitive interpretation is still pursued. We describe\nhow to build factor graphs to model and simulate multibody systems using both,\nindependent and dependent coordinates. Then, batch optimization or a\nfixed-lag-smoother can be applied to solve the underlying optimization problem\nthat results in a highly-sparse nonlinear minimization problem. The proposed\nframework has been tested in extensive simulations and validated against a\ncommercial multibody software. We release a reference implementation as an\nopen-source C++ library, based on the GTSAM framework, a well-known estimation\nlibrary. Simulations of forward and inverse dynamics are presented, showing\ncomparable accuracy with classical approaches. The proposed factor graph-based\nframework has the potential to be integrated into applications related with\nmotion estimation and parameter identification of complex mechanical systems,\nranging from mechanisms to vehicles, or robot manipulators.\n", "versions": [{"version": "v1", "created": "Fri, 8 Jan 2021 06:45:45 GMT"}], "update_date": "2021-07-27", "authors_parsed": [["Blanco-Claraco", "Jos\u00e9-Luis", ""], ["Leanza", "Antonio", ""], ["Reina", "Giulio", ""]]}, {"id": "2101.02876", "submitter": "Syed Anwar", "authors": "Ali Nawaz, Syed Muhammad Anwar, Rehan Liaqat, Javid Iqbal, Ulas Bagci,\n  Muhammad Majid", "title": "Deep Convolutional Neural Network based Classification of Alzheimer's\n  Disease using MRI data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Alzheimer's disease (AD) is a progressive and incurable neurodegenerative\ndisease which destroys brain cells and causes loss to patient's memory. An\nearly detection can prevent the patient from further damage of the brain cells\nand hence avoid permanent memory loss. In past few years, various automatic\ntools and techniques have been proposed for diagnosis of AD. Several methods\nfocus on fast, accurate and early detection of the disease to minimize the loss\nto patients mental health. Although machine learning and deep learning\ntechniques have significantly improved medical imaging systems for AD by\nproviding diagnostic performance close to human level. But the main problem\nfaced during multi-class classification is the presence of highly correlated\nfeatures in the brain structure. In this paper, we have proposed a smart and\naccurate way of diagnosing AD based on a two-dimensional deep convolutional\nneural network (2D-DCNN) using imbalanced three-dimensional MRI dataset.\nExperimental results on Alzheimer Disease Neuroimaging Initiative magnetic\nresonance imaging (MRI) dataset confirms that the proposed 2D-DCNN model is\nsuperior in terms of accuracy, efficiency, and robustness. The model classifies\nMRI into three categories: AD, mild cognitive impairment, and normal control:\nand has achieved 99.89% classification accuracy with imbalanced classes. The\nproposed model exhibits noticeable improvement in accuracy as compared to the\nstate-fo-the-art methods.\n", "versions": [{"version": "v1", "created": "Fri, 8 Jan 2021 06:51:08 GMT"}], "update_date": "2021-01-11", "authors_parsed": [["Nawaz", "Ali", ""], ["Anwar", "Syed Muhammad", ""], ["Liaqat", "Rehan", ""], ["Iqbal", "Javid", ""], ["Bagci", "Ulas", ""], ["Majid", "Muhammad", ""]]}, {"id": "2101.02888", "submitter": "Junaid Rahim", "authors": "Priyansi, Biswaroop Bhattacharjee, Junaid Rahim", "title": "Predicting Semen Motility using three-dimensional Convolutional Neural\n  Networks", "comments": "Corrected typos. Made slight changes as per the comments", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Manual and computer aided methods to perform semen analysis are\ntime-consuming, requires extensive training and prone to human error. The use\nof classical machine learning and deep learning based methods using videos to\nperform semen analysis have yielded good results. The state-of-the-art method\nuses regular convolutional neural networks to perform quality assessments on a\nvideo of the provided sample. In this paper we propose an improved deep\nlearning based approach using three-dimensional convolutional neural networks\nto predict sperm motility from microscopic videos of the semen sample. We make\nuse of the VISEM dataset that consists of video and tabular data of semen\nsamples collected from 85 participants. We were able to achieve good results\nfrom significantly less data points. Our models indicate that deep learning\nbased automatic semen analysis may become a valuable and effective tool in\nfertility and IVF labs.\n", "versions": [{"version": "v1", "created": "Fri, 8 Jan 2021 07:38:52 GMT"}, {"version": "v2", "created": "Thu, 14 Jan 2021 05:35:09 GMT"}], "update_date": "2021-01-15", "authors_parsed": [["Priyansi", "", ""], ["Bhattacharjee", "Biswaroop", ""], ["Rahim", "Junaid", ""]]}, {"id": "2101.02897", "submitter": "Itai Arieli", "authors": "Itai Arieli, Yakov Babichenko, Manuel Mueller-Frank", "title": "Sequential Naive Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.GT", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We analyze boundedly rational updating from aggregate statistics in a model\nwith binary actions and binary states. Agents each take an irreversible action\nin sequence after observing the unordered set of previous actions. Each agent\nfirst forms her prior based on the aggregate statistic, then incorporates her\nsignal with the prior based on Bayes rule, and finally applies a decision rule\nthat assigns a (mixed) action to each belief. If priors are formed according to\na discretized DeGroot rule, then actions converge to the state (in\nprobability), i.e., \\emph{asymptotic learning}, in any informative information\nstructure if and only if the decision rule satisfies probability matching. This\nresult generalizes to unspecified information settings where information\nstructures differ across agents and agents know only the information structure\ngenerating their own signal. Also, the main result extends to the case of $n$\nstates and $n$ actions.\n", "versions": [{"version": "v1", "created": "Fri, 8 Jan 2021 08:10:13 GMT"}], "update_date": "2021-01-11", "authors_parsed": [["Arieli", "Itai", ""], ["Babichenko", "Yakov", ""], ["Mueller-Frank", "Manuel", ""]]}, {"id": "2101.02899", "submitter": "Josh Harguess", "authors": "Marissa Dotter, Sherry Xie, Keith Manville, Josh Harguess, Colin\n  Busho, Mikel Rodriguez", "title": "Adversarial Attack Attribution: Discovering Attributable Signals in\n  Adversarial ML Attacks", "comments": "Accepted to RSEML Workshop at AAAI 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Machine Learning (ML) models are known to be vulnerable to adversarial inputs\nand researchers have demonstrated that even production systems, such as\nself-driving cars and ML-as-a-service offerings, are susceptible. These systems\nrepresent a target for bad actors. Their disruption can cause real physical and\neconomic harm. When attacks on production ML systems occur, the ability to\nattribute the attack to the responsible threat group is a critical step in\nformulating a response and holding the attackers accountable. We pose the\nfollowing question: can adversarially perturbed inputs be attributed to the\nparticular methods used to generate the attack? In other words, is there a way\nto find a signal in these attacks that exposes the attack algorithm, model\narchitecture, or hyperparameters used in the attack? We introduce the concept\nof adversarial attack attribution and create a simple supervised learning\nexperimental framework to examine the feasibility of discovering attributable\nsignals in adversarial attacks. We find that it is possible to differentiate\nattacks generated with different attack algorithms, models, and hyperparameters\non both the CIFAR-10 and MNIST datasets.\n", "versions": [{"version": "v1", "created": "Fri, 8 Jan 2021 08:16:41 GMT"}], "update_date": "2021-01-11", "authors_parsed": [["Dotter", "Marissa", ""], ["Xie", "Sherry", ""], ["Manville", "Keith", ""], ["Harguess", "Josh", ""], ["Busho", "Colin", ""], ["Rodriguez", "Mikel", ""]]}, {"id": "2101.02906", "submitter": "Anwar Alnawas", "authors": "Anwar Alnawas and Nursal ARICI", "title": "Effect of Word Embedding Variable Parameters on Arabic Sentiment\n  Analysis Performance", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Social media such as Twitter, Facebook, etc. has led to a generated growing\nnumber of comments that contains users opinions. Sentiment analysis research\ndeals with these comments to extract opinions which are positive or negative.\nArabic language is a rich morphological language; thus, classical techniques of\nEnglish sentiment analysis cannot be used for Arabic. Word embedding technique\ncan be considered as one of successful methods to gaping the morphological\nproblem of Arabic. Many works have been done for Arabic sentiment analysis\nbased on word embedding, but there is no study focused on variable parameters.\nThis study will discuss three parameters (Window size, Dimension of vector and\nNegative Sample) for Arabic sentiment analysis using DBOW and DMPV\narchitectures. A large corpus of previous works generated to learn word\nrepresentations and extract features. Four binary classifiers (Logistic\nRegression, Decision Tree, Support Vector Machine and Naive Bayes) are used to\ndetect sentiment. The performance of classifiers evaluated based on; Precision,\nRecall and F1-score.\n", "versions": [{"version": "v1", "created": "Fri, 8 Jan 2021 08:31:00 GMT"}], "update_date": "2021-01-11", "authors_parsed": [["Alnawas", "Anwar", ""], ["ARICI", "Nursal", ""]]}, {"id": "2101.02908", "submitter": "Weijun Li", "authors": "Liang Xu, Liying Zheng, Weijun Li, Zhenbo Chen, Weishun Song, Yue\n  Deng, Yongzhe Chang, Jing Xiao, Bo Yuan", "title": "NVAE-GAN Based Approach for Unsupervised Time Series Anomaly Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent studies, Lots of work has been done to solve time series anomaly\ndetection by applying Variational Auto-Encoders (VAEs). Time series anomaly\ndetection is a very common but challenging task in many industries, which plays\nan important role in network monitoring, facility maintenance, information\nsecurity, and so on. However, it is very difficult to detect anomalies in time\nseries with high accuracy, due to noisy data collected from real world, and\ncomplicated abnormal patterns. From recent studies, we are inspired by Nouveau\nVAE (NVAE) and propose our anomaly detection model: Time series to Image VAE\n(T2IVAE), an unsupervised model based on NVAE for univariate series,\ntransforming 1D time series to 2D image as input, and adopting the\nreconstruction error to detect anomalies. Besides, we also apply the Generative\nAdversarial Networks based techniques to T2IVAE training strategy, aiming to\nreduce the overfitting. We evaluate our model performance on three datasets,\nand compare it with other several popular models using F1 score. T2IVAE\nachieves 0.639 on Numenta Anomaly Benchmark, 0.651 on public dataset from NASA,\nand 0.504 on our dataset collected from real-world scenario, outperforms other\ncomparison models.\n", "versions": [{"version": "v1", "created": "Fri, 8 Jan 2021 08:35:15 GMT"}], "update_date": "2021-01-11", "authors_parsed": [["Xu", "Liang", ""], ["Zheng", "Liying", ""], ["Li", "Weijun", ""], ["Chen", "Zhenbo", ""], ["Song", "Weishun", ""], ["Deng", "Yue", ""], ["Chang", "Yongzhe", ""], ["Xiao", "Jing", ""], ["Yuan", "Bo", ""]]}, {"id": "2101.02916", "submitter": "Mingyang Yi", "authors": "Mingyang Yi, Qi Meng, Wei Chen, Zhi-Ming Ma", "title": "Towards Accelerating Training of Batch Normalization: A Manifold\n  Perspective", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Batch normalization (BN) has become a crucial component across diverse deep\nneural networks. The network with BN is invariant to positively linear\nre-scaling of weights, which makes there exist infinite functionally equivalent\nnetworks with various scales of weights. However, optimizing these equivalent\nnetworks with the first-order method such as stochastic gradient descent will\nconverge to different local optima owing to different gradients across\ntraining. To alleviate this, we propose a quotient manifold \\emph{PSI\nmanifold}, in which all the equivalent weights of the network with BN are\nregarded as the same one element. Then, gradient descent and stochastic\ngradient descent on the PSI manifold are also constructed. The two algorithms\nguarantee that every group of equivalent weights (caused by positively\nre-scaling) converge to the equivalent optima. Besides that, we give the\nconvergence rate of the proposed algorithms on PSI manifold and justify that\nthey accelerate training compared with the algorithms on the Euclidean weight\nspace. Empirical studies show that our algorithms can consistently achieve\nbetter performances over various experimental settings.\n", "versions": [{"version": "v1", "created": "Fri, 8 Jan 2021 08:53:07 GMT"}], "update_date": "2021-01-11", "authors_parsed": [["Yi", "Mingyang", ""], ["Meng", "Qi", ""], ["Chen", "Wei", ""], ["Ma", "Zhi-Ming", ""]]}, {"id": "2101.02930", "submitter": "Zun Wang", "authors": "Zun Wang, Chong Wang, Sibo Zhao, Shiqiao Du, Yong Xu, Bing-Lin Gu,\n  Wenhui Duan", "title": "Symmetry-adapted graph neural networks for constructing molecular\n  dynamics force fields", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.comp-ph cond-mat.dis-nn cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Molecular dynamics is a powerful simulation tool to explore material\nproperties. Most of the realistic material systems are too large to be\nsimulated with first-principles molecular dynamics. Classical molecular\ndynamics has lower computational cost but requires accurate force fields to\nachieve chemical accuracy. In this work, we develop a symmetry-adapted graph\nneural networks framework, named molecular dynamics graph neural networks\n(MDGNN), to construct force fields automatically for molecular dynamics\nsimulations for both molecules and crystals. This architecture consistently\npreserves the translation, rotation and permutation invariance in the\nsimulations. We propose a new feature engineering method including higher order\ncontributions and show that MDGNN accurately reproduces the results of both\nclassical and first-principles molecular dynamics. We also demonstrate that\nforce fields constructed by the model has good transferability. Therefore,\nMDGNN provides an efficient and promising option for molecular dynamics\nsimulations of large scale systems with high accuracy.\n", "versions": [{"version": "v1", "created": "Fri, 8 Jan 2021 09:32:24 GMT"}], "update_date": "2021-01-11", "authors_parsed": [["Wang", "Zun", ""], ["Wang", "Chong", ""], ["Zhao", "Sibo", ""], ["Du", "Shiqiao", ""], ["Xu", "Yong", ""], ["Gu", "Bing-Lin", ""], ["Duan", "Wenhui", ""]]}, {"id": "2101.02931", "submitter": "Paris Giampouras", "authors": "Paris V. Giampouras, Athanasios A. Rontogiannis, Eleftherios Kofidis", "title": "Block-Term Tensor Decomposition Model Selection and Computation: The\n  Bayesian Way", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.LG cs.NA math.NA", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The so-called block-term decomposition (BTD) tensor model, especially in its\nrank-$(L_r,L_r,1)$ version, has been recently receiving increasing attention\ndue to its enhanced ability of representing systems and signals that are\ncomposed of \\emph{blocks} of rank higher than one, a scenario encountered in\nnumerous and diverse applications. Uniqueness conditions and fitting methods\nhave thus been thoroughly studied. Nevertheless, the challenging problem of\nestimating the BTD model structure, namely the number of block terms, $R$, and\ntheir individual ranks, $L_r$, has only recently started to attract significant\nattention, mainly through regularization-based approaches which entail the need\nto tune the regularization parameter(s). In this work, we build on ideas of\nsparse Bayesian learning (SBL) and put forward a fully automated Bayesian\napproach. Through a suitably crafted multi-level \\emph{hierarchical}\nprobabilistic model, which gives rise to heavy-tailed prior distributions for\nthe BTD factors, structured sparsity is \\emph{jointly} imposed. Ranks are then\nestimated from the numbers of blocks ($R$) and columns ($L_r$) of\nnon-negligible energy. Approximate posterior inference is implemented, within\nthe variational inference framework. The resulting iterative algorithm\ncompletely avoids hyperparameter tuning, which is a significant defect of\nregularization-based methods. Alternative probabilistic models are also\nexplored and the connections with their regularization-based counterparts are\nbrought to light with the aid of the associated maximum a-posteriori (MAP)\nestimators. We report simulation results with both synthetic and real-word\ndata, which demonstrate the merits of the proposed method in terms of both rank\nestimation and model fitting as compared to state-of-the-art relevant methods.\n", "versions": [{"version": "v1", "created": "Fri, 8 Jan 2021 09:37:21 GMT"}, {"version": "v2", "created": "Mon, 5 Jul 2021 13:16:19 GMT"}], "update_date": "2021-07-06", "authors_parsed": [["Giampouras", "Paris V.", ""], ["Rontogiannis", "Athanasios A.", ""], ["Kofidis", "Eleftherios", ""]]}, {"id": "2101.02944", "submitter": "Mingyang Yi", "authors": "Mingyang Yi, Huishuai Zhang, Wei Chen, Zhi-Ming Ma, Tie-Yan Liu", "title": "BN-invariant sharpness regularizes the training model to better\n  generalization", "comments": null, "journal-ref": "Published in IJCAI2019", "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is arguably believed that flatter minima can generalize better. However,\nit has been pointed out that the usual definitions of sharpness, which consider\neither the maxima or the integral of loss over a $\\delta$ ball of parameters\naround minima, cannot give consistent measurement for scale invariant neural\nnetworks, e.g., networks with batch normalization layer. In this paper, we\nfirst propose a measure of sharpness, BN-Sharpness, which gives consistent\nvalue for equivalent networks under BN. It achieves the property of scale\ninvariance by connecting the integral diameter with the scale of parameter.\nThen we present a computation-efficient way to calculate the BN-sharpness\napproximately i.e., one dimensional integral along the \"sharpest\" direction.\nFurthermore, we use the BN-sharpness to regularize the training and design an\nalgorithm to minimize the new regularized objective. Our algorithm achieves\nconsiderably better performance than vanilla SGD over various experiment\nsettings.\n", "versions": [{"version": "v1", "created": "Fri, 8 Jan 2021 10:23:24 GMT"}], "update_date": "2021-01-11", "authors_parsed": [["Yi", "Mingyang", ""], ["Zhang", "Huishuai", ""], ["Chen", "Wei", ""], ["Ma", "Zhi-Ming", ""], ["Liu", "Tie-Yan", ""]]}, {"id": "2101.02966", "submitter": "Florian Stelzer", "authors": "Florian Stelzer (1, 2 and 3), Serhiy Yanchuk (1) ((1) Institute of\n  Mathematics, Technische Universit\\\"at Berlin, Germany, (2) Department of\n  Mathematics, Humboldt-Universit\\\"at zu Berlin, Germany, (3) Institute of\n  Computer Science, University of Tartu, Estonia)", "title": "Infinite-dimensional Folded-in-time Deep Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE math.DS math.FA math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The method recently introduced in arXiv:2011.10115 realizes a deep neural\nnetwork with just a single nonlinear element and delayed feedback. It is\napplicable for the description of physically implemented neural networks. In\nthis work, we present an infinite-dimensional generalization, which allows for\na more rigorous mathematical analysis and a higher flexibility in choosing the\nweight functions. Precisely speaking, the weights are described by Lebesgue\nintegrable functions instead of step functions. We also provide a functional\nback-propagation algorithm, which enables gradient descent training of the\nweights. In addition, with a slight modification, our concept realizes\nrecurrent neural networks.\n", "versions": [{"version": "v1", "created": "Fri, 8 Jan 2021 11:30:50 GMT"}, {"version": "v2", "created": "Fri, 11 Jun 2021 12:57:35 GMT"}], "update_date": "2021-06-14", "authors_parsed": [["Stelzer", "Florian", "", "1, 2 and 3"], ["Yanchuk", "Serhiy", ""]]}, {"id": "2101.02969", "submitter": "Hui Luo", "authors": "Hui Luo, Jingbo Zhou, Zhifeng Bao, Shuangli Li, J. Shane Culpepper,\n  Haochao Ying, Hao Liu, Hui Xiong", "title": "Spatial Object Recommendation with Hints: When Spatial Granularity\n  Matters", "comments": null, "journal-ref": "SIGIR Conference (2020) 781-790", "doi": "10.1145/3397271.3401090", "report-no": null, "categories": "cs.IR cs.DB cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Existing spatial object recommendation algorithms generally treat objects\nidentically when ranking them. However, spatial objects often cover different\nlevels of spatial granularity and thereby are heterogeneous. For example, one\nuser may prefer to be recommended a region (say Manhattan), while another user\nmight prefer a venue (say a restaurant). Even for the same user, preferences\ncan change at different stages of data exploration. In this paper, we study how\nto support top-k spatial object recommendations at varying levels of spatial\ngranularity, enabling spatial objects at varying granularity, such as a city,\nsuburb, or building, as a Point of Interest (POI). To solve this problem, we\npropose the use of a POI tree, which captures spatial containment relationships\nbetween POIs. We design a novel multi-task learning model called MPR (short for\nMulti-level POI Recommendation), where each task aims to return the top-k POIs\nat a certain spatial granularity level. Each task consists of two subtasks: (i)\nattribute-based representation learning; (ii) interaction-based representation\nlearning. The first subtask learns the feature representations for both users\nand POIs, capturing attributes directly from their profiles. The second subtask\nincorporates user-POI interactions into the model. Additionally, MPR can\nprovide insights into why certain recommendations are being made to a user\nbased on three types of hints: user-aspect, POI-aspect, and interaction-aspect.\nWe empirically validate our approach using two real-life datasets, and show\npromising performance improvements over several state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Fri, 8 Jan 2021 11:39:51 GMT"}], "update_date": "2021-01-11", "authors_parsed": [["Luo", "Hui", ""], ["Zhou", "Jingbo", ""], ["Bao", "Zhifeng", ""], ["Li", "Shuangli", ""], ["Culpepper", "J. Shane", ""], ["Ying", "Haochao", ""], ["Liu", "Hao", ""], ["Xiong", "Hui", ""]]}, {"id": "2101.02971", "submitter": "Fabian K\\\"uppers", "authors": "Franziska Schwaiger, Maximilian Henne, Fabian K\\\"uppers, Felippe\n  Schmoeller Roza, Karsten Roscher, Anselm Haselhoff", "title": "From Black-box to White-box: Examining Confidence Calibration under\n  different Conditions", "comments": "Accepted on SafeAI 2021 workshop on AAAI 2021 conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Confidence calibration is a major concern when applying artificial neural\nnetworks in safety-critical applications. Since most research in this area has\nfocused on classification in the past, confidence calibration in the scope of\nobject detection has gained more attention only recently. Based on previous\nwork, we study the miscalibration of object detection models with respect to\nimage location and box scale. Our main contribution is to additionally consider\nthe impact of box selection methods like non-maximum suppression to\ncalibration. We investigate the default intrinsic calibration of object\ndetection models and how it is affected by these post-processing techniques.\nFor this purpose, we distinguish between black-box calibration with non-maximum\nsuppression and white-box calibration with raw network outputs. Our experiments\nreveal that post-processing highly affects confidence calibration. We show that\nnon-maximum suppression has the potential to degrade initially well-calibrated\npredictions, leading to overconfident and thus miscalibrated models.\n", "versions": [{"version": "v1", "created": "Fri, 8 Jan 2021 11:45:30 GMT"}], "update_date": "2021-01-11", "authors_parsed": [["Schwaiger", "Franziska", ""], ["Henne", "Maximilian", ""], ["K\u00fcppers", "Fabian", ""], ["Roza", "Felippe Schmoeller", ""], ["Roscher", "Karsten", ""], ["Haselhoff", "Anselm", ""]]}, {"id": "2101.02974", "submitter": "Joachim Sicking", "authors": "Joachim Sicking, Alexander Kister, Matthias Fahrland, Stefan Eickeler,\n  Fabian H\\\"uger, Stefan R\\\"uping, Peter Schlicht, Tim Wirtz", "title": "Approaching Neural Network Uncertainty Realism", "comments": "Accepted at the NeurIPS 2019 Workshop on Machine Learning for\n  Autonomous Driving (ML4AD)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Statistical models are inherently uncertain. Quantifying or at least\nupper-bounding their uncertainties is vital for safety-critical systems such as\nautonomous vehicles. While standard neural networks do not report this\ninformation, several approaches exist to integrate uncertainty estimates into\nthem. Assessing the quality of these uncertainty estimates is not\nstraightforward, as no direct ground truth labels are available. Instead,\nimplicit statistical assessments are required. For regression, we propose to\nevaluate uncertainty realism -- a strict quality criterion -- with a\nMahalanobis distance-based statistical test. An empirical evaluation reveals\nthe need for uncertainty measures that are appropriate to upper-bound\nheavy-tailed empirical errors. Alongside, we transfer the variational U-Net\nclassification architecture to standard supervised image-to-image tasks. We\nadopt it to the automotive domain and show that it significantly improves\nuncertainty realism compared to a plain encoder-decoder model.\n", "versions": [{"version": "v1", "created": "Fri, 8 Jan 2021 11:56:12 GMT"}], "update_date": "2021-01-11", "authors_parsed": [["Sicking", "Joachim", ""], ["Kister", "Alexander", ""], ["Fahrland", "Matthias", ""], ["Eickeler", "Stefan", ""], ["H\u00fcger", "Fabian", ""], ["R\u00fcping", "Stefan", ""], ["Schlicht", "Peter", ""], ["Wirtz", "Tim", ""]]}, {"id": "2101.02997", "submitter": "Constance Beguier", "authors": "Constance Beguier, Jean Ogier du Terrail, Iqraa Meah, Mathieu Andreux,\n  Eric W. Tramel", "title": "Differentially Private Federated Learning for Cancer Prediction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Since 2014, the NIH funded iDASH (integrating Data for Analysis,\nAnonymization, SHaring) National Center for Biomedical Computing has hosted\nyearly competitions on the topic of private computing for genomic data. For one\ntrack of the 2020 iteration of this competition, participants were challenged\nto produce an approach to federated learning (FL) training of genomic cancer\nprediction models using differential privacy (DP), with submissions ranked\naccording to held-out test accuracy for a given set of DP budgets. More\nprecisely, in this track, we are tasked with training a supervised model for\nthe prediction of breast cancer occurrence from genomic data split between two\nvirtual centers while ensuring data privacy with respect to model transfer via\nDP. In this article, we present our 3rd place submission to this competition.\nDuring the competition, we encountered two main challenges discussed in this\narticle: i) ensuring correctness of the privacy budget evaluation and ii)\nachieving an acceptable trade-off between prediction performance and privacy\nbudget.\n", "versions": [{"version": "v1", "created": "Fri, 8 Jan 2021 13:12:40 GMT"}, {"version": "v2", "created": "Wed, 24 Mar 2021 12:47:01 GMT"}], "update_date": "2021-03-25", "authors_parsed": [["Beguier", "Constance", ""], ["Terrail", "Jean Ogier du", ""], ["Meah", "Iqraa", ""], ["Andreux", "Mathieu", ""], ["Tramel", "Eric W.", ""]]}, {"id": "2101.03000", "submitter": "Timm Faulwasser", "authors": "Timm Faulwasser and Arne-Jens Hempel and Stefan Streif", "title": "On the Turnpike to Design of Deep Neural Nets: Explicit Depth Bounds", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.SY eess.SY math.OC", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  It is well-known that the training of Deep Neural Networks (DNN) can be\nformalized in the language of optimal control. In this context, this paper\nleverages classical turnpike properties of optimal control problems to attempt\na quantifiable answer to the question of how many layers should be considered\nin a DNN. The underlying assumption is that the number of neurons per layer --\ni.e., the width of the DNN -- is kept constant. Pursuing a different route than\nthe classical analysis of approximation properties of sigmoidal functions, we\nprove explicit bounds on the required depths of DNNs based on asymptotic\nreachability assumptions and a dissipativity-inducing choice of the\nregularization terms in the training problem. Numerical results obtained for\nthe two spiral task data set for classification indicate that the proposed\nestimates can provide non-conservative depth bounds.\n", "versions": [{"version": "v1", "created": "Fri, 8 Jan 2021 13:23:37 GMT"}], "update_date": "2021-01-11", "authors_parsed": [["Faulwasser", "Timm", ""], ["Hempel", "Arne-Jens", ""], ["Streif", "Stefan", ""]]}, {"id": "2101.03013", "submitter": "Iknoor Singh", "authors": "Iknoor Singh, Carolina Scarton, Kalina Bontcheva", "title": "Multistage BiCross Encoder: Team GATE Entry for MLIA Multilingual\n  Semantic Search Task 2", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Coronavirus (COVID-19) pandemic has led to a rapidly growing `infodemic'\nonline. Thus, the accurate retrieval of reliable relevant data from millions of\ndocuments about COVID-19 has become urgently needed for the general public as\nwell as for other stakeholders. The COVID-19 Multilingual Information Access\n(MLIA) initiative is a joint effort to ameliorate exchange of COVID-19 related\ninformation by developing applications and services through research and\ncommunity participation. In this work, we present a search system called\nMultistage BiCross Encoder, developed by team GATE for the MLIA task 2\nMultilingual Semantic Search. Multistage BiCross-Encoder is a sequential three\nstage pipeline which uses the Okapi BM25 algorithm and a transformer based\nbi-encoder and cross-encoder to effectively rank the documents with respect to\nthe query. The results of round 1 show that our models achieve state-of-the-art\nperformance for all ranking metrics for both monolingual and bilingual runs.\n", "versions": [{"version": "v1", "created": "Fri, 8 Jan 2021 13:59:26 GMT"}, {"version": "v2", "created": "Fri, 15 Jan 2021 20:38:23 GMT"}], "update_date": "2021-01-19", "authors_parsed": [["Singh", "Iknoor", ""], ["Scarton", "Carolina", ""], ["Bontcheva", "Kalina", ""]]}, {"id": "2101.03020", "submitter": "Camille Chapdelaine", "authors": "Cyril Cappi, Camille Chapdelaine, Laurent Gardes, Eric Jenn, Baptiste\n  Lefevre, Sylvaine Picard, Thomas Soumarmon", "title": "Dataset Definition Standard (DDS)", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This document gives a set of recommendations to build and manipulate the\ndatasets used to develop and/or validate machine learning models such as deep\nneural networks. This document is one of the 3 documents defined in [1] to\nensure the quality of datasets. This is a work in progress as good practices\nevolve along with our understanding of machine learning. The document is\ndivided into three main parts. Section 2 addresses the data collection\nactivity. Section 3 gives recommendations about the annotation process.\nFinally, Section 4 gives recommendations concerning the breakdown between\ntrain, validation, and test datasets. In each part, we first define the desired\nproperties at stake, then we explain the objectives targeted to meet the\nproperties, finally we state the recommendations to reach these objectives.\n", "versions": [{"version": "v1", "created": "Thu, 7 Jan 2021 10:11:03 GMT"}], "update_date": "2021-01-11", "authors_parsed": [["Cappi", "Cyril", ""], ["Chapdelaine", "Camille", ""], ["Gardes", "Laurent", ""], ["Jenn", "Eric", ""], ["Lefevre", "Baptiste", ""], ["Picard", "Sylvaine", ""], ["Soumarmon", "Thomas", ""]]}, {"id": "2101.03024", "submitter": "Sourav Ghosh", "authors": "Sonal Kumari, Vibhav Agarwal, Bharath Challa, Kranti Chalamalasetti,\n  Sourav Ghosh, Harshavardhana, Barath Raj Kandur Raja", "title": "LiteMuL: A Lightweight On-Device Sequence Tagger using Multi-task\n  Learning", "comments": "Published in 2021 IEEE 15th International Conference on Semantic\n  Computing (ICSC); Candidate for Best Paper Award", "journal-ref": "2021 IEEE 15th International Conference on Semantic Computing\n  (ICSC), Laguna Hills, CA, USA, 2021, pp. 1-8", "doi": "10.1109/ICSC50631.2021.00007", "report-no": null, "categories": "cs.CL cs.LG", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Named entity detection and Parts-of-speech tagging are the key tasks for many\nNLP applications. Although the current state of the art methods achieved near\nperfection for long, formal, structured text there are hindrances in deploying\nthese models on memory-constrained devices such as mobile phones. Furthermore,\nthe performance of these models is degraded when they encounter short,\ninformal, and casual conversations. To overcome these difficulties, we present\nLiteMuL - a lightweight on-device sequence tagger that can efficiently process\nthe user conversations using a Multi-Task Learning (MTL) approach. To the best\nof our knowledge, the proposed model is the first on-device MTL neural model\nfor sequence tagging. Our LiteMuL model is about 2.39 MB in size and achieved\nan accuracy of 0.9433 (for NER), 0.9090 (for POS) on the CoNLL 2003 dataset.\nThe proposed LiteMuL not only outperforms the current state of the art results\nbut also surpasses the results of our proposed on-device task-specific models,\nwith accuracy gains of up to 11% and model-size reduction by 50%-56%. Our model\nis competitive with other MTL approaches for NER and POS tasks while outshines\nthem with a low memory footprint. We also evaluated our model on custom-curated\nuser conversations and observed impressive results.\n", "versions": [{"version": "v1", "created": "Tue, 15 Dec 2020 19:15:54 GMT"}, {"version": "v2", "created": "Mon, 29 Mar 2021 14:31:19 GMT"}], "update_date": "2021-03-30", "authors_parsed": [["Kumari", "Sonal", ""], ["Agarwal", "Vibhav", ""], ["Challa", "Bharath", ""], ["Chalamalasetti", "Kranti", ""], ["Ghosh", "Sourav", ""], ["Harshavardhana", "", ""], ["Raja", "Barath Raj Kandur", ""]]}, {"id": "2101.03025", "submitter": "Sourav Ghosh", "authors": "Vibhav Agarwal, Sourav Ghosh, Kranti Chalamalasetti, Bharath Challa,\n  Sonal Kumari, Harshavardhana, Barath Raj Kandur Raja", "title": "EmpLite: A Lightweight Sequence Labeling Model for Emphasis Selection of\n  Short Texts", "comments": "Accepted for publication in ICON 2020: 17th International Conference\n  on Natural Language Processing", "journal-ref": "17th International Conference on Natural Language Processing\n  (ICON), Patna, India, December 18 - 21, 2020, pages 19-26, ACL Anthology:\n  2020.icon-1.3", "doi": null, "report-no": "2020.icon-1.3 (ACL Anthology)", "categories": "cs.CL cs.LG", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Word emphasis in textual content aims at conveying the desired intention by\nchanging the size, color, typeface, style (bold, italic, etc.), and other\ntypographical features. The emphasized words are extremely helpful in drawing\nthe readers' attention to specific information that the authors wish to\nemphasize. However, performing such emphasis using a soft keyboard for social\nmedia interactions is time-consuming and has an associated learning curve. In\nthis paper, we propose a novel approach to automate the emphasis word detection\non short written texts. To the best of our knowledge, this work presents the\nfirst lightweight deep learning approach for smartphone deployment of emphasis\nselection. Experimental results show that our approach achieves comparable\naccuracy at a much lower model size than existing models. Our best lightweight\nmodel has a memory footprint of 2.82 MB with a matching score of 0.716 on\nSemEval-2020 public benchmark dataset.\n", "versions": [{"version": "v1", "created": "Tue, 15 Dec 2020 19:00:44 GMT"}], "update_date": "2021-05-25", "authors_parsed": [["Agarwal", "Vibhav", ""], ["Ghosh", "Sourav", ""], ["Chalamalasetti", "Kranti", ""], ["Challa", "Bharath", ""], ["Kumari", "Sonal", ""], ["Harshavardhana", "", ""], ["Raja", "Barath Raj Kandur", ""]]}, {"id": "2101.03029", "submitter": "Mansooreh Karami", "authors": "Mansooreh Karami, Ahmadreza Mosallanezhad, Michelle V Mancenido, Huan\n  Liu", "title": "\"Let's Eat Grandma\": When Punctuation Matters in Sentence Representation\n  for Sentiment Analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Neural network-based embeddings have been the mainstream approach for\ncreating a vector representation of the text to capture lexical and semantic\nsimilarities and dissimilarities. In general, existing encoding methods dismiss\nthe punctuation as insignificant information; consequently, they are routinely\neliminated in the pre-processing phase as they are shown to improve task\nperformance. In this paper, we hypothesize that punctuation could play a\nsignificant role in sentiment analysis and propose a novel representation model\nto improve syntactic and contextual performance. We corroborate our findings by\nconducting experiments on publicly available datasets and verify that our model\ncan identify the sentiments more accurately over other state-of-the-art\nbaseline methods.\n", "versions": [{"version": "v1", "created": "Thu, 10 Dec 2020 19:07:31 GMT"}], "update_date": "2021-06-30", "authors_parsed": [["Karami", "Mansooreh", ""], ["Mosallanezhad", "Ahmadreza", ""], ["Mancenido", "Michelle V", ""], ["Liu", "Huan", ""]]}, {"id": "2101.03037", "submitter": "Bobak Kiani", "authors": "Bobak Toussi Kiani, Giacomo De Palma, Milad Marvian, Zi-Wen Liu, Seth\n  Lloyd", "title": "Quantum Earth Mover's Distance: A New Approach to Learning Quantum Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "quant-ph cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Quantifying how far the output of a learning algorithm is from its target is\nan essential task in machine learning. However, in quantum settings, the loss\nlandscapes of commonly used distance metrics often produce undesirable outcomes\nsuch as poor local minima and exponentially decaying gradients. As a new\napproach, we consider here the quantum earth mover's (EM) or Wasserstein-1\ndistance, recently proposed in [De Palma et al., arXiv:2009.04469] as a quantum\nanalog to the classical EM distance. We show that the quantum EM distance\npossesses unique properties, not found in other commonly used quantum distance\nmetrics, that make quantum learning more stable and efficient. We propose a\nquantum Wasserstein generative adversarial network (qWGAN) which takes\nadvantage of the quantum EM distance and provides an efficient means of\nperforming learning on quantum data. Our qWGAN requires resources polynomial in\nthe number of qubits, and our numerical experiments demonstrate that it is\ncapable of learning a diverse set of quantum data.\n", "versions": [{"version": "v1", "created": "Fri, 8 Jan 2021 14:33:19 GMT"}], "update_date": "2021-01-11", "authors_parsed": [["Kiani", "Bobak Toussi", ""], ["De Palma", "Giacomo", ""], ["Marvian", "Milad", ""], ["Liu", "Zi-Wen", ""], ["Lloyd", "Seth", ""]]}, {"id": "2101.03042", "submitter": "Pulei Xiong", "authors": "Pulei Xiong, Scott Buffett, Shahrear Iqbal, Philippe Lamontagne,\n  Mohammad Mamun, and Heather Molyneaux", "title": "Towards a Robust and Trustworthy Machine Learning System Development", "comments": "40 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Machine Learning (ML) technologies have been widely adopted in many mission\ncritical fields, such as cyber security, autonomous vehicle control,\nhealthcare, etc. to support intelligent decision-making. While ML has\ndemonstrated impressive performance over conventional methods in these\napplications, concerns arose with respect to system resilience against\nML-specific security attacks and privacy breaches as well as the trust that\nusers have in these systems. In this article, firstly we present our recent\nsystematic and comprehensive survey on the state-of-the-art ML robustness and\ntrustworthiness technologies from a security engineering perspective, which\ncovers all aspects of secure ML system development including threat modeling,\ncommon offensive and defensive technologies, privacy-preserving machine\nlearning, user trust in the context of machine learning, and empirical\nevaluation for ML model robustness. Secondly, we then push our studies forward\nabove and beyond a survey by describing a metamodel we created that represents\nthe body of knowledge in a standard and visualized way for ML practitioners. We\nfurther illustrate how to leverage the metamodel to guide a systematic threat\nanalysis and security design process in a context of generic ML system\ndevelopment, which extends and scales up the classic process. Thirdly, we\npropose future research directions motivated by our findings to advance the\ndevelopment of robust and trustworthy ML systems. Our work differs from\nexisting surveys in this area in that, to the best of our knowledge, it is the\nfirst of its kind of engineering effort to (i) explore the fundamental\nprinciples and best practices to support robust and trustworthy ML system\ndevelopment; and (ii) study the interplay of robustness and user trust in the\ncontext of ML systems.\n", "versions": [{"version": "v1", "created": "Fri, 8 Jan 2021 14:43:58 GMT"}], "update_date": "2021-01-11", "authors_parsed": [["Xiong", "Pulei", ""], ["Buffett", "Scott", ""], ["Iqbal", "Shahrear", ""], ["Lamontagne", "Philippe", ""], ["Mamun", "Mohammad", ""], ["Molyneaux", "Heather", ""]]}, {"id": "2101.03054", "submitter": "Serguei Mokhov", "authors": "Yuhao Mao, Serguei A. Mokhov, Sudhir P. Mudur", "title": "Application of Knowledge Graphs to Provide Side Information for Improved\n  Recommendation Accuracy", "comments": "27 pages, 16 figures, 5 tables, 2 algorithms; Submitted to Science of\n  Computer Programming", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Personalized recommendations are popular in these days of Internet driven\nactivities, specifically shopping. Recommendation methods can be grouped into\nthree major categories, content based filtering, collaborative filtering and\nmachine learning enhanced. Information about products and preferences of\ndifferent users are primarily used to infer preferences for a specific user.\nInadequate information can obviously cause these methods to fail or perform\npoorly. The more information we provide to these methods, the more likely it is\nthat the methods perform better. Knowledge graphs represent the current trend\nin recording information in the form of relations between entities, and can\nprovide additional (side) information about products and users. Such\ninformation can be used to improve nearest neighbour search, clustering users\nand products, or train the neural network, when one is used. In this work, we\npresent a new generic recommendation systems framework, that integrates\nknowledge graphs into the recommendation pipeline. We describe its software\ndesign and implementation, and then show through experiments, how such a\nframework can be specialized for a domain, say movie recommendations, and the\nimprovements in recommendation results possible due to side information\nobtained from knowledge graphs representation of such information. Our\nframework supports different knowledge graph representation formats, and\nfacilitates format conversion, merging and information extraction needed for\ntraining recommendation methods.\n", "versions": [{"version": "v1", "created": "Thu, 7 Jan 2021 16:52:05 GMT"}], "update_date": "2021-01-11", "authors_parsed": [["Mao", "Yuhao", ""], ["Mokhov", "Serguei A.", ""], ["Mudur", "Sudhir P.", ""]]}, {"id": "2101.03057", "submitter": "Sebastian Palacio", "authors": "Sebastian Palacio, Philipp Engler, J\\\"orn Hees, Andreas Dengel", "title": "Contextual Classification Using Self-Supervised Auxiliary Models for\n  Deep Neural Networks", "comments": "Accepted for publication at the International Conference of Pattern\n  Recognition (ICPR) 2020", "journal-ref": null, "doi": "10.1109/ICPR48806.2021.9412175", "report-no": null, "categories": "cs.LG cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Classification problems solved with deep neural networks (DNNs) typically\nrely on a closed world paradigm, and optimize over a single objective (e.g.,\nminimization of the cross-entropy loss). This setup dismisses all kinds of\nsupporting signals that can be used to reinforce the existence or absence of a\nparticular pattern. The increasing need for models that are interpretable by\ndesign makes the inclusion of said contextual signals a crucial necessity. To\nthis end, we introduce the notion of Self-Supervised Autogenous Learning (SSAL)\nmodels. A SSAL objective is realized through one or more additional targets\nthat are derived from the original supervised classification task, following\narchitectural principles found in multi-task learning. SSAL branches impose\nlow-level priors into the optimization process (e.g., grouping). The ability of\nusing SSAL branches during inference, allow models to converge faster, focusing\non a richer set of class-relevant features. We show that SSAL models\nconsistently outperform the state-of-the-art while also providing structured\npredictions that are more interpretable.\n", "versions": [{"version": "v1", "created": "Thu, 7 Jan 2021 18:41:16 GMT"}], "update_date": "2021-05-27", "authors_parsed": [["Palacio", "Sebastian", ""], ["Engler", "Philipp", ""], ["Hees", "J\u00f6rn", ""], ["Dengel", "Andreas", ""]]}, {"id": "2101.03064", "submitter": "Poojan Oza", "authors": "Pramuditha Perera, Poojan Oza, Vishal M. Patel", "title": "One-Class Classification: A Survey", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One-Class Classification (OCC) is a special case of multi-class\nclassification, where data observed during training is from a single positive\nclass. The goal of OCC is to learn a representation and/or a classifier that\nenables recognition of positively labeled queries during inference. This topic\nhas received considerable amount of interest in the computer vision, machine\nlearning and biometrics communities in recent years. In this article, we\nprovide a survey of classical statistical and recent deep learning-based OCC\nmethods for visual recognition. We discuss the merits and drawbacks of existing\nOCC approaches and identify promising avenues for research in this field. In\naddition, we present a discussion of commonly used datasets and evaluation\nmetrics for OCC.\n", "versions": [{"version": "v1", "created": "Fri, 8 Jan 2021 15:30:29 GMT"}], "update_date": "2021-01-11", "authors_parsed": [["Perera", "Pramuditha", ""], ["Oza", "Poojan", ""], ["Patel", "Vishal M.", ""]]}, {"id": "2101.03087", "submitter": "Racine Ly", "authors": "Racine Ly, Fousseini Traore, Khadim Dia", "title": "Forecasting Commodity Prices Using Long Short-Term Memory Neural\n  Networks", "comments": "13 pages, 8 figures, 7 tables, 27 references", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.ST cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  This paper applies a recurrent neural network (RNN) method to forecast cotton\nand oil prices. We show how these new tools from machine learning, particularly\nLong-Short Term Memory (LSTM) models, complement traditional methods. Our\nresults show that machine learning methods fit reasonably well the data but do\nnot outperform systematically classical methods such as Autoregressive\nIntegrated Moving Average (ARIMA) models in terms of out of sample forecasts.\nHowever, averaging the forecasts from the two type of models provide better\nresults compared to either method. Compared to the ARIMA and the LSTM, the Root\nMean Squared Error (RMSE) of the average forecast was 0.21 and 21.49 percent\nlower respectively for cotton. For oil, the forecast averaging does not provide\nimprovements in terms of RMSE. We suggest using a forecast averaging method and\nextending our analysis to a wide range of commodity prices.\n", "versions": [{"version": "v1", "created": "Fri, 8 Jan 2021 16:28:19 GMT"}, {"version": "v2", "created": "Fri, 15 Jan 2021 11:13:11 GMT"}], "update_date": "2021-01-18", "authors_parsed": [["Ly", "Racine", ""], ["Traore", "Fousseini", ""], ["Dia", "Khadim", ""]]}, {"id": "2101.03091", "submitter": "Benedek Rozemberczki", "authors": "Benedek Rozemberczki and Rik Sarkar", "title": "Twitch Gamers: a Dataset for Evaluating Proximity Preserving and\n  Structural Role-based Node Embeddings", "comments": "The dataset is available at\n  https://github.com/benedekrozemberczki/datasets", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Proximity preserving and structural role-based node embeddings have become a\nprime workhorse of applied graph mining. Novel node embedding techniques are\noften tested on a restricted set of benchmark datasets. In this paper, we\npropose a new diverse social network dataset called Twitch Gamers with multiple\npotential target attributes. Our analysis of the social network and node\nclassification experiments illustrate that Twitch Gamers is suitable for\nassessing the predictive performance of novel proximity preserving and\nstructural role-based node embedding algorithms.\n", "versions": [{"version": "v1", "created": "Fri, 8 Jan 2021 16:40:37 GMT"}, {"version": "v2", "created": "Tue, 16 Feb 2021 22:17:58 GMT"}], "update_date": "2021-02-18", "authors_parsed": [["Rozemberczki", "Benedek", ""], ["Sarkar", "Rik", ""]]}, {"id": "2101.03093", "submitter": "Ricardo Baptista", "authors": "Ricardo Baptista, Youssef Marzouk, Rebecca E. Morrison, Olivier Zahm", "title": "Learning non-Gaussian graphical models via Hessian scores and triangular\n  transport", "comments": "40 pages, 12 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.CO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Undirected probabilistic graphical models represent the conditional\ndependencies, or Markov properties, of a collection of random variables.\nKnowing the sparsity of such a graphical model is valuable for modeling\nmultivariate distributions and for efficiently performing inference. While the\nproblem of learning graph structure from data has been studied extensively for\ncertain parametric families of distributions, most existing methods fail to\nconsistently recover the graph structure for non-Gaussian data. Here we propose\nan algorithm for learning the Markov structure of continuous and non-Gaussian\ndistributions. To characterize conditional independence, we introduce a score\nbased on integrated Hessian information from the joint log-density, and we\nprove that this score upper bounds the conditional mutual information for a\ngeneral class of distributions. To compute the score, our algorithm SING\nestimates the density using a deterministic coupling, induced by a triangular\ntransport map, and iteratively exploits sparse structure in the map to reveal\nsparsity in the graph. For certain non-Gaussian datasets, we show that our\nalgorithm recovers the graph structure even with a biased approximation to the\ndensity. Among other examples, we apply sing to learn the dependencies between\nthe states of a chaotic dynamical system with local interactions.\n", "versions": [{"version": "v1", "created": "Fri, 8 Jan 2021 16:42:42 GMT"}], "update_date": "2021-01-11", "authors_parsed": [["Baptista", "Ricardo", ""], ["Marzouk", "Youssef", ""], ["Morrison", "Rebecca E.", ""], ["Zahm", "Olivier", ""]]}, {"id": "2101.03118", "submitter": "Fabio Massimo Zennaro", "authors": "Laszlo Erdodi, {\\AA}vald {\\AA}slaugson Sommervoll, Fabio Massimo\n  Zennaro", "title": "Simulating SQL Injection Vulnerability Exploitation Using Q-Learning\n  Reinforcement Learning Agents", "comments": "20 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a formalization of the process of exploitation of\nSQL injection vulnerabilities. We consider a simplification of the dynamics of\nSQL injection attacks by casting this problem as a security capture-the-flag\nchallenge. We model it as a Markov decision process, and we implement it as a\nreinforcement learning problem. We then deploy reinforcement learning agents\ntasked with learning an effective policy to perform SQL injection; we design\nour training in such a way that the agent learns not just a specific strategy\nto solve an individual challenge but a more generic policy that may be applied\nto perform SQL injection attacks against any system instantiated randomly by\nour problem generator. We analyze the results in terms of the quality of the\nlearned policy and in terms of convergence time as a function of the complexity\nof the challenge and the learning agent's complexity. Our work fits in the\nwider research on the development of intelligent agents for autonomous\npenetration testing and white-hat hacking, and our results aim to contribute to\nunderstanding the potential and the limits of reinforcement learning in a\nsecurity environment.\n", "versions": [{"version": "v1", "created": "Fri, 8 Jan 2021 17:19:21 GMT"}, {"version": "v2", "created": "Sat, 22 May 2021 09:23:19 GMT"}], "update_date": "2021-05-25", "authors_parsed": [["Erdodi", "Laszlo", ""], ["Sommervoll", "\u00c5vald \u00c5slaugson", ""], ["Zennaro", "Fabio Massimo", ""]]}, {"id": "2101.03126", "submitter": "Zaheer Ullah Khan", "authors": "Zaheer Ullah Khan, Dechang Pi, Izhar Ahmed Khan, Asif Nawaz, Jamil\n  Ahmad, Mushtaq Hussain", "title": "piSAAC: Extended notion of SAAC feature selection novel method for\n  discrimination of Enzymes model using different machine learning algorithm", "comments": "3 Figures, 5 Tables, 6 Pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.BM cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Enzymes and proteins are live driven biochemicals, which has a dramatic\nimpact over the environment, in which it is active. So, therefore, it is highly\nlooked-for to build such a robust and highly accurate automatic and\ncomputational model to accurately predict enzymes nature. In this study, a\nnovel split amino acid composition model named piSAAC is proposed. In this\nmodel, protein sequence is discretized in equal and balanced terminus to fully\nevaluate the intrinsic correlation properties of the sequence. Several\nstate-of-the-art algorithms have been employed to evaluate the proposed model.\nA 10-folds cross-validation evaluation is used for finding out the authenticity\nand robust-ness of the model using different statistical measures e.g.\nAccuracy, sensitivity, specificity, F-measure and area un-der ROC curve. The\nexperimental results show that, probabilistic neural network algorithm with\npiSAAC feature extraction yields an accuracy of 98.01%, sensitivity of 97.12%,\nspecificity of 95.87%, f-measure of 0.9812and AUC 0.95812, over dataset S1,\naccuracy of 97.85%, sensitivity of 97.54%, specificity of 96.24%, f-measure of\n0.9774 and AUC 0.9803 over dataset S2. Evident from these excellent empirical\nresults, the proposed model would be a very useful tool for academic research\nand drug designing related application areas.\n", "versions": [{"version": "v1", "created": "Wed, 16 Dec 2020 03:45:21 GMT"}], "update_date": "2021-01-11", "authors_parsed": [["Khan", "Zaheer Ullah", ""], ["Pi", "Dechang", ""], ["Khan", "Izhar Ahmed", ""], ["Nawaz", "Asif", ""], ["Ahmad", "Jamil", ""], ["Hussain", "Mushtaq", ""]]}, {"id": "2101.03127", "submitter": "Filippo Neri", "authors": "Filippo Neri", "title": "How to Identify Investor's types in real financial markets by means of\n  agent based simulation", "comments": "18 pages, in press", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.TR cs.CE cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The paper proposes a computational adaptation of the principles underlying\nprincipal component analysis with agent based simulation in order to produce a\nnovel modeling methodology for financial time series and financial markets.\nGoal of the proposed methodology is to find a reduced set of investor s models\n(agents) which is able to approximate or explain a target financial time\nseries. As computational testbed for the study, we choose the learning system L\nFABS which combines simulated annealing with agent based simulation for\napproximating financial time series. We will also comment on how L FABS s\narchitecture could exploit parallel computation to scale when dealing with\nmassive agent simulations. Two experimental case studies showing the efficacy\nof the proposed methodology are reported.\n", "versions": [{"version": "v1", "created": "Thu, 31 Dec 2020 16:22:30 GMT"}], "update_date": "2021-01-11", "authors_parsed": [["Neri", "Filippo", ""]]}, {"id": "2101.03128", "submitter": "Alexandre Miot", "authors": "Alexandre Miot", "title": "Adversarial trading", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.TR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Adversarial samples have drawn a lot of attention from the Machine Learning\ncommunity in the past few years. An adverse sample is an artificial data point\ncoming from an imperceptible modification of a sample point aiming at\nmisleading. Surprisingly, in financial research, little has been done in\nrelation to this topic from a concrete trading point of view. We show that\nthose adversarial samples can be implemented in a trading environment and have\na negative impact on certain market participants. This could have far reaching\nimplications for financial markets either from a trading or a regulatory point\nof view.\n", "versions": [{"version": "v1", "created": "Wed, 16 Dec 2020 16:08:22 GMT"}], "update_date": "2021-01-11", "authors_parsed": [["Miot", "Alexandre", ""]]}, {"id": "2101.03134", "submitter": "Sarah Walker", "authors": "Sarah Walker, Joshua Peeples, Jeff Dale, James Keller, Alina Zare", "title": "Explainable Systematic Analysis for Synthetic Aperture Sonar Imagery", "comments": "IGARSS 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we present an in-depth and systematic analysis using tools such\nas local interpretable model-agnostic explanations (LIME) (arXiv:1602.04938)\nand divergence measures to analyze what changes lead to improvement in\nperformance in fine tuned models for synthetic aperture sonar (SAS) data. We\nexamine the sensitivity to factors in the fine tuning process such as class\nimbalance. Our findings show not only an improvement in seafloor texture\nclassification, but also provide greater insight into what features play\ncritical roles in improving performance as well as a knowledge of the\nimportance of balanced data for fine tuning deep learning models for seafloor\nclassification in SAS imagery.\n", "versions": [{"version": "v1", "created": "Wed, 6 Jan 2021 16:33:27 GMT"}, {"version": "v2", "created": "Thu, 21 Jan 2021 16:12:43 GMT"}, {"version": "v3", "created": "Tue, 16 Mar 2021 20:33:45 GMT"}], "update_date": "2021-03-18", "authors_parsed": [["Walker", "Sarah", ""], ["Peeples", "Joshua", ""], ["Dale", "Jeff", ""], ["Keller", "James", ""], ["Zare", "Alina", ""]]}, {"id": "2101.03135", "submitter": "Nader Tavaf", "authors": "Nader Tavaf, Amirsina Torfi, Kamil Ugurbil, Pierre-Francois Van de\n  Moortele", "title": "GRAPPA-GANs for Parallel MRI Reconstruction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG physics.med-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  k-space undersampling is a standard technique to accelerate MR image\nacquisitions. Reconstruction techniques including GeneRalized Autocalibrating\nPartial Parallel Acquisition(GRAPPA) and its variants are utilized extensively\nin clinical and research settings. A reconstruction model combining GRAPPA with\na conditional generative adversarial network (GAN) was developed and tested on\nmulti-coil human brain images from the fastMRI dataset. For various\nacceleration rates, GAN and GRAPPA reconstructions were compared in terms of\npeak signal-to-noise ratio (PSNR) and structural similarity (SSIM). For an\nacceleration rate of R=4, PSNR improved from 33.88 using regularized GRAPPA to\n37.65 using GAN. GAN consistently outperformed GRAPPA for various acceleration\nrates.\n", "versions": [{"version": "v1", "created": "Tue, 5 Jan 2021 15:47:20 GMT"}, {"version": "v2", "created": "Mon, 15 Feb 2021 19:48:12 GMT"}], "update_date": "2021-02-17", "authors_parsed": [["Tavaf", "Nader", ""], ["Torfi", "Amirsina", ""], ["Ugurbil", "Kamil", ""], ["Van de Moortele", "Pierre-Francois", ""]]}, {"id": "2101.03138", "submitter": "Tae Wan Kim", "authors": "Tae Wan Kim, Matloob Khushi", "title": "Portfolio Optimization with 2D Relative-Attentional Gated Transformer", "comments": "Accepted to be published in the Proceedings of the IEEE Asia Pacific\n  Conference on Computer Science and Data Engineering 2020 (CSDE 2020)", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.PM cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Portfolio optimization is one of the most attentive fields that have been\nresearched with machine learning approaches. Many researchers attempted to\nsolve this problem using deep reinforcement learning due to its efficient\ninherence that can handle the property of financial markets. However, most of\nthem can hardly be applicable to real-world trading since they ignore or\nextremely simplify the realistic constraints of transaction costs. These\nconstraints have a significantly negative impact on portfolio profitability. In\nour research, a conservative level of transaction fees and slippage are\nconsidered for the realistic experiment. To enhance the performance under those\nconstraints, we propose a novel Deterministic Policy Gradient with 2D\nRelative-attentional Gated Transformer (DPGRGT) model. Applying learnable\nrelative positional embeddings for the time and assets axes, the model better\nunderstands the peculiar structure of the financial data in the portfolio\noptimization domain. Also, gating layers and layer reordering are employed for\nstable convergence of Transformers in reinforcement learning. In our experiment\nusing U.S. stock market data of 20 years, our model outperformed baseline\nmodels and demonstrated its effectiveness.\n", "versions": [{"version": "v1", "created": "Sun, 27 Dec 2020 14:08:26 GMT"}], "update_date": "2021-01-11", "authors_parsed": [["Kim", "Tae Wan", ""], ["Khushi", "Matloob", ""]]}, {"id": "2101.03140", "submitter": "Iqbal H. Sarker", "authors": "Md. Zubair, MD.Asif Iqbal, Avijeet Shil, Enamul Haque, Mohammed\n  Moshiul Hoque and Iqbal H. Sarker", "title": "An Efficient K-means Clustering Algorithm for Analysing COVID-19", "comments": "10 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  COVID-19 hits the world like a storm by arising pandemic situations for most\nof the countries around the world. The whole world is trying to overcome this\npandemic situation. A better health care quality may help a country to tackle\nthe pandemic. Making clusters of countries with similar types of health care\nquality provides an insight into the quality of health care in different\ncountries. In the area of machine learning and data science, the K-means\nclustering algorithm is typically used to create clusters based on similarity.\nIn this paper, we propose an efficient K-means clustering method that\ndetermines the initial centroids of the clusters efficiently. Based on this\nproposed method, we have determined health care quality clusters of countries\nutilizing the COVID-19 datasets. Experimental results show that our proposed\nmethod reduces the number of iterations and execution time to analyze COVID-19\nwhile comparing with the traditional k-means clustering algorithm.\n", "versions": [{"version": "v1", "created": "Mon, 21 Dec 2020 04:06:30 GMT"}], "update_date": "2021-01-11", "authors_parsed": [["Zubair", "Md.", ""], ["Iqbal", "MD. Asif", ""], ["Shil", "Avijeet", ""], ["Haque", "Enamul", ""], ["Hoque", "Mohammed Moshiul", ""], ["Sarker", "Iqbal H.", ""]]}, {"id": "2101.03141", "submitter": "Iqbal H. Sarker", "authors": "Rony Chowdhury Ripan, Iqbal H. Sarker, Md Musfique Anwar, Md. Hasan\n  Furhad, Fazle Rahat, Mohammed Moshiul Hoque and Muhammad Sarfraz", "title": "An Isolation Forest Learning Based Outlier Detection Approach for\n  Effectively Classifying Cyber Anomalies", "comments": "10 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Cybersecurity has recently gained considerable interest in today's security\nissues because of the popularity of the Internet-of-Things (IoT), the\nconsiderable growth of mobile networks, and many related apps. Therefore,\ndetecting numerous cyber-attacks in a network and creating an effective\nintrusion detection system plays a vital role in today's security. In this\npaper, we present an Isolation Forest Learning-Based Outlier Detection Model\nfor effectively classifying cyber anomalies. In order to evaluate the efficacy\nof the resulting Outlier Detection model, we also use several conventional\nmachine learning approaches, such as Logistic Regression (LR), Support Vector\nMachine (SVM), AdaBoost Classifier (ABC), Naive Bayes (NB), and K-Nearest\nNeighbor (KNN). The effectiveness of our proposed Outlier Detection model is\nevaluated by conducting experiments on Network Intrusion Dataset with\nevaluation metrics such as precision, recall, F1-score, and accuracy.\nExperimental results show that the classification accuracy of cyber anomalies\nhas been improved after removing outliers.\n", "versions": [{"version": "v1", "created": "Wed, 9 Dec 2020 05:09:52 GMT"}], "update_date": "2021-01-11", "authors_parsed": [["Ripan", "Rony Chowdhury", ""], ["Sarker", "Iqbal H.", ""], ["Anwar", "Md Musfique", ""], ["Furhad", "Md. Hasan", ""], ["Rahat", "Fazle", ""], ["Hoque", "Mohammed Moshiul", ""], ["Sarfraz", "Muhammad", ""]]}, {"id": "2101.03154", "submitter": "Fanjie Kong", "authors": "Fanjie Kong, Xiao-yang Liu, Ricardo Henao", "title": "Quantum Tensor Network in Machine Learning: An Application to Tiny\n  Object Classification", "comments": "8 pages, 7 figures", "journal-ref": "https://tensorworkshop.github.io/NeurIPS2020/CFP.html", "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Tiny object classification problem exists in many machine learning\napplications like medical imaging or remote sensing, where the object of\ninterest usually occupies a small region of the whole image. It is challenging\nto design an efficient machine learning model with respect to tiny object of\ninterest. Current neural network structures are unable to deal with tiny object\nefficiently because they are mainly developed for images featured by large\nscale objects. However, in quantum physics, there is a great theoretical\nfoundation guiding us to analyze the target function for image classification\nregarding to specific objects size ratio. In our work, we apply Tensor Networks\nto solve this arising tough machine learning problem. First, we summarize the\nprevious work that connects quantum spin model to image classification and\nbring the theory into the scenario of tiny object classification. Second, we\npropose using 2D multi-scale entanglement renormalization ansatz (MERA) to\nclassify tiny objects in image. In the end, our experimental results indicate\nthat tensor network models are effective for tiny object classification problem\nand potentially will beat state-of-the-art. Our codes will be available online\nhttps://github.com/timqqt/MERA_Image_Classification.\n", "versions": [{"version": "v1", "created": "Fri, 8 Jan 2021 18:33:52 GMT"}], "update_date": "2021-01-11", "authors_parsed": [["Kong", "Fanjie", ""], ["Liu", "Xiao-yang", ""], ["Henao", "Ricardo", ""]]}, {"id": "2101.03164", "submitter": "Simon Lutz Batzner", "authors": "Simon Batzner, Albert Musaelian, Lixin Sun, Mario Geiger, Jonathan P.\n  Mailoa, Mordechai Kornbluth, Nicola Molinari, Tess E. Smidt, and Boris\n  Kozinsky", "title": "SE(3)-Equivariant Graph Neural Networks for Data-Efficient and Accurate\n  Interatomic Potentials", "comments": "17 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.comp-ph cond-mat.mtrl-sci cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work presents Neural Equivariant Interatomic Potentials (NequIP), a\nSE(3)-equivariant neural network approach for learning interatomic potentials\nfrom ab-initio calculations for molecular dynamics simulations. While most\ncontemporary symmetry-aware models use invariant convolutions and only act on\nscalars, NequIP employs SE(3)-equivariant convolutions for interactions of\ngeometric tensors, resulting in a more information-rich and faithful\nrepresentation of atomic environments. The method achieves state-of-the-art\naccuracy on a challenging set of diverse molecules and materials while\nexhibiting remarkable data efficiency. NequIP outperforms existing models with\nup to three orders of magnitude fewer training data, challenging the widely\nheld belief that deep neural networks require massive training sets. The high\ndata efficiency of the method allows for the construction of accurate\npotentials using high-order quantum chemical level of theory as reference and\nenables high-fidelity molecular dynamics simulations over long time scales.\n", "versions": [{"version": "v1", "created": "Fri, 8 Jan 2021 18:49:10 GMT"}, {"version": "v2", "created": "Fri, 2 Jul 2021 15:29:11 GMT"}], "update_date": "2021-07-05", "authors_parsed": [["Batzner", "Simon", ""], ["Musaelian", "Albert", ""], ["Sun", "Lixin", ""], ["Geiger", "Mario", ""], ["Mailoa", "Jonathan P.", ""], ["Kornbluth", "Mordechai", ""], ["Molinari", "Nicola", ""], ["Smidt", "Tess E.", ""], ["Kozinsky", "Boris", ""]]}, {"id": "2101.03169", "submitter": "Wen Liu", "authors": "Maohan Liang, Ryan Wen Liu, Shichen Li, Zhe Xiao, Xin Liu, Feng Lu", "title": "An Unsupervised Learning Method with Convolutional Auto-Encoder for\n  Vessel Trajectory Similarity Computation", "comments": "22 pages, 16 figures", "journal-ref": "Ocean Engineering, 2021", "doi": "10.1016/j.oceaneng.2021.108803", "report-no": null, "categories": "cs.LG cs.AI cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  To achieve reliable mining results for massive vessel trajectories, one of\nthe most important challenges is how to efficiently compute the similarities\nbetween different vessel trajectories. The computation of vessel trajectory\nsimilarity has recently attracted increasing attention in the maritime data\nmining research community. However, traditional shape- and warping-based\nmethods often suffer from several drawbacks such as high computational cost and\nsensitivity to unwanted artifacts and non-uniform sampling rates, etc. To\neliminate these drawbacks, we propose an unsupervised learning method which\nautomatically extracts low-dimensional features through a convolutional\nauto-encoder (CAE). In particular, we first generate the informative trajectory\nimages by remapping the raw vessel trajectories into two-dimensional matrices\nwhile maintaining the spatio-temporal properties. Based on the massive vessel\ntrajectories collected, the CAE can learn the low-dimensional representations\nof informative trajectory images in an unsupervised manner. The trajectory\nsimilarity is finally equivalent to efficiently computing the similarities\nbetween the learned low-dimensional features, which strongly correlate with the\nraw vessel trajectories. Comprehensive experiments on realistic data sets have\ndemonstrated that the proposed method largely outperforms traditional\ntrajectory similarity computation methods in terms of efficiency and\neffectiveness. The high-quality trajectory clustering performance could also be\nguaranteed according to the CAE-based trajectory similarity computation\nresults.\n", "versions": [{"version": "v1", "created": "Sun, 10 Jan 2021 04:42:11 GMT"}], "update_date": "2021-06-11", "authors_parsed": [["Liang", "Maohan", ""], ["Liu", "Ryan Wen", ""], ["Li", "Shichen", ""], ["Xiao", "Zhe", ""], ["Liu", "Xin", ""], ["Lu", "Feng", ""]]}, {"id": "2101.03170", "submitter": "Dai Feng", "authors": "Dai Feng and Lili Zhao", "title": "BDNNSurv: Bayesian deep neural networks for survival analysis using\n  pseudo values", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.LG", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  There has been increasing interest in modeling survival data using deep\nlearning methods in medical research. In this paper, we proposed a Bayesian\nhierarchical deep neural networks model for modeling and prediction of survival\ndata. Compared with previously studied methods, the new proposal can provide\nnot only point estimate of survival probability but also quantification of the\ncorresponding uncertainty, which can be of crucial importance in predictive\nmodeling and subsequent decision making. The favorable statistical properties\nof point and uncertainty estimates were demonstrated by simulation studies and\nreal data analysis. The Python code implementing the proposed approach was\nprovided.\n", "versions": [{"version": "v1", "created": "Thu, 7 Jan 2021 20:18:43 GMT"}], "update_date": "2021-01-12", "authors_parsed": [["Feng", "Dai", ""], ["Zhao", "Lili", ""]]}, {"id": "2101.03184", "submitter": "Iqbal H. Sarker", "authors": "Uwaise Ibna Islam, Iqbal H. Sarker, Enamul Haque and Mohammed Moshiul\n  Hoque", "title": "Predicting Individual Substance Abuse Vulnerability using Machine\n  Learning Techniques", "comments": "10 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Substance abuse is the unrestrained and detrimental use of psychoactive\nchemical substances, unauthorized drugs, and alcohol. Continuous use of these\nsubstances can ultimately lead a human to disastrous consequences. As patients\ndisplay a high rate of relapse, prevention at an early stage can be an\neffective restraint. We therefore propose a binary classifier to identify any\nindividual's present vulnerability towards substance abuse by analyzing\nsubjects' socio-economic environment. We have collected data by a questionnaire\nwhich is created after carefully assessing the commonly involved factors behind\nsubstance abuse. Pearson's chi-squared test of independence is used to identify\nkey feature variables influencing substance abuse. Later we build the\npredictive classifiers using machine learning classification algorithms on\nthose variables. Logistic regression classifier trained with 18 features can\npredict individual vulnerability with the best accuracy.\n", "versions": [{"version": "v1", "created": "Wed, 9 Dec 2020 05:21:05 GMT"}], "update_date": "2021-01-12", "authors_parsed": [["Islam", "Uwaise Ibna", ""], ["Sarker", "Iqbal H.", ""], ["Haque", "Enamul", ""], ["Hoque", "Mohammed Moshiul", ""]]}, {"id": "2101.03196", "submitter": "Bei Wang", "authors": "Mingzhe Li, Sourabh Palande, Lin Yan, Bei Wang", "title": "Sketching Merge Trees for Scientific Data Visualization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CG cs.DS cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Merge trees are a type of topological descriptors that record the\nconnectivity among the sublevel sets of scalar fields. They are among the most\nwidely used topological tools in visualization. In this paper, we are\ninterested in sketching a set of merge trees. That is, given a large set T of\nmerge trees, we would like to find a much smaller basis set S such that each\ntree in T can be approximately reconstructed from a linear combination of merge\ntrees in S. A set of high-dimensional vectors can be sketched via matrix\nsketching techniques such as principal component analysis and column subset\nselection. However, up until now, topological descriptors such as merge trees\nhave not been known to be sketchable. We develop a framework for sketching a\nset of merge trees that combines the Gromov-Wasserstein probabilistic matching\nwith techniques from matrix sketching. We demonstrate the applications of our\nframework in sketching merge trees that arise from time-varying scientific\nsimulations. Specifically, our framework obtains a much smaller representation\nof a large set of merge trees for downstream analysis and visualization. It is\nshown to be useful in identifying good representatives and outliers with\nrespect to a chosen basis. Finally, our work shows a promising direction of\nutilizing randomized linear algebra within scientific visualization.\n", "versions": [{"version": "v1", "created": "Fri, 8 Jan 2021 19:38:46 GMT"}, {"version": "v2", "created": "Sun, 30 May 2021 23:39:30 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Li", "Mingzhe", ""], ["Palande", "Sourabh", ""], ["Yan", "Lin", ""], ["Wang", "Bei", ""]]}, {"id": "2101.03197", "submitter": "Abiy Tasissa", "authors": "Abiy Tasissa, Duc Nguyen, James Murphy", "title": "Deep Diffusion Processes for Active Learning of Hyperspectral Images", "comments": "5 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  A method for active learning of hyperspectral images (HSI) is proposed, which\ncombines deep learning with diffusion processes on graphs. A deep variational\nautoencoder extracts smoothed, denoised features from a high-dimensional HSI,\nwhich are then used to make labeling queries based on graph diffusion\nprocesses. The proposed method combines the robust representations of deep\nlearning with the mathematical tractability of diffusion geometry, and leads to\nstrong performance on real HSI.\n", "versions": [{"version": "v1", "created": "Fri, 8 Jan 2021 19:38:54 GMT"}], "update_date": "2021-01-12", "authors_parsed": [["Tasissa", "Abiy", ""], ["Nguyen", "Duc", ""], ["Murphy", "James", ""]]}, {"id": "2101.03198", "submitter": "Badri Narayanan", "authors": "Badri Narayanan, Mohamed Saadeldin, Paul Albert, Kevin McGuinness, and\n  Brian Mac Namee", "title": "Extracting Pasture Phenotype and Biomass Percentages using Weakly\n  Supervised Multi-target Deep Learning on a Small Dataset", "comments": null, "journal-ref": "Irish Machine Vision and Image Processing Conference (2020) 21-28", "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The dairy industry uses clover and grass as fodder for cows. Accurate\nestimation of grass and clover biomass yield enables smart decisions in\noptimizing fertilization and seeding density, resulting in increased\nproductivity and positive environmental impact. Grass and clover are usually\nplanted together, since clover is a nitrogen-fixing plant that brings nutrients\nto the soil. Adjusting the right percentages of clover and grass in a field\nreduces the need for external fertilization. Existing approaches for estimating\nthe grass-clover composition of a field are expensive and time consuming -\nrandom samples of the pasture are clipped and then the components are\nphysically separated to weigh and calculate percentages of dry grass, clover\nand weeds in each sample. There is growing interest in developing novel deep\nlearning based approaches to non-destructively extract pasture phenotype\nindicators and biomass yield predictions of different plant species from\nagricultural imagery collected from the field. Providing these indicators and\npredictions from images alone remains a significant challenge. Heavy occlusions\nin the dense mixture of grass, clover and weeds make it difficult to estimate\neach component accurately. Moreover, although supervised deep learning models\nperform well with large datasets, it is tedious to acquire large and diverse\ncollections of field images with precise ground truth for different biomass\nyields. In this paper, we demonstrate that applying data augmentation and\ntransfer learning is effective in predicting multi-target biomass percentages\nof different plant species, even with a small training dataset. The scheme\nproposed in this paper used a training set of only 261 images and provided\npredictions of biomass percentages of grass, clover, white clover, red clover,\nand weeds with mean absolute error of 6.77%, 6.92%, 6.21%, 6.89%, and 4.80%\nrespectively.\n", "versions": [{"version": "v1", "created": "Fri, 8 Jan 2021 19:41:46 GMT"}], "update_date": "2021-01-12", "authors_parsed": [["Narayanan", "Badri", ""], ["Saadeldin", "Mohamed", ""], ["Albert", "Paul", ""], ["McGuinness", "Kevin", ""], ["Mac Namee", "Brian", ""]]}, {"id": "2101.03207", "submitter": "Sayar Ghosh Roy", "authors": "Sayar Ghosh Roy, Ujwal Narayan, Tathagata Raha, Zubair Abid, Vasudeva\n  Varma", "title": "Leveraging Multilingual Transformers for Hate Speech Detection", "comments": "To be published in: FIRE (Working Notes) 2020, Hate Speech and\n  Offensive Content Identification in Indo-European Languages, HASOC 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.CY cs.IR cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Detecting and classifying instances of hate in social media text has been a\nproblem of interest in Natural Language Processing in the recent years. Our\nwork leverages state of the art Transformer language models to identify hate\nspeech in a multilingual setting. Capturing the intent of a post or a comment\non social media involves careful evaluation of the language style, semantic\ncontent and additional pointers such as hashtags and emojis. In this paper, we\nlook at the problem of identifying whether a Twitter post is hateful and\noffensive or not. We further discriminate the detected toxic content into one\nof the following three classes: (a) Hate Speech (HATE), (b) Offensive (OFFN)\nand (c) Profane (PRFN). With a pre-trained multilingual Transformer-based text\nencoder at the base, we are able to successfully identify and classify hate\nspeech from multiple languages. On the provided testing corpora, we achieve\nMacro F1 scores of 90.29, 81.87 and 75.40 for English, German and Hindi\nrespectively while performing hate speech detection and of 60.70, 53.28 and\n49.74 during fine-grained classification. In our experiments, we show the\nefficacy of Perspective API features for hate speech classification and the\neffects of exploiting a multilingual training scheme. A feature selection study\nis provided to illustrate impacts of specific features upon the architecture's\nclassification head.\n", "versions": [{"version": "v1", "created": "Fri, 8 Jan 2021 20:23:50 GMT"}], "update_date": "2021-01-12", "authors_parsed": [["Roy", "Sayar Ghosh", ""], ["Narayan", "Ujwal", ""], ["Raha", "Tathagata", ""], ["Abid", "Zubair", ""], ["Varma", "Vasudeva", ""]]}, {"id": "2101.03218", "submitter": "Olakunle Ibitoye", "authors": "Olakunle Ibitoye, M. Omair Shafiq, Ashraf Matrawy", "title": "DiPSeN: Differentially Private Self-normalizing Neural Networks For\n  Adversarial Robustness in Federated Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CY cs.NI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The need for robust, secure and private machine learning is an important goal\nfor realizing the full potential of the Internet of Things (IoT). Federated\nlearning has proven to help protect against privacy violations and information\nleakage. However, it introduces new risk vectors which make machine learning\nmodels more difficult to defend against adversarial samples. In this study, we\nexamine the role of differential privacy and self-normalization in mitigating\nthe risk of adversarial samples specifically in a federated learning\nenvironment. We introduce DiPSeN, a Differentially Private Self-normalizing\nNeural Network which combines elements of differential privacy noise with\nself-normalizing techniques. Our empirical results on three publicly available\ndatasets show that DiPSeN successfully improves the adversarial robustness of a\ndeep learning classifier in a federated learning environment based on several\nevaluation metrics.\n", "versions": [{"version": "v1", "created": "Fri, 8 Jan 2021 20:49:56 GMT"}], "update_date": "2021-01-12", "authors_parsed": [["Ibitoye", "Olakunle", ""], ["Shafiq", "M. Omair", ""], ["Matrawy", "Ashraf", ""]]}, {"id": "2101.03219", "submitter": "Hugues Nelson Iradukunda", "authors": "Zeyu Ning, Hugues Nelson Iradukunda, Qingquan Zhang, Ting Zhu", "title": "Benchmarking Machine Learning: How Fast Can Your Algorithms Go?", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper is focused on evaluating the effect of some different techniques\nin machine learning speed-up, including vector caches, parallel execution, and\nso on. The following content will include some review of the previous\napproaches and our own experimental results.\n", "versions": [{"version": "v1", "created": "Fri, 8 Jan 2021 20:50:33 GMT"}], "update_date": "2021-01-12", "authors_parsed": [["Ning", "Zeyu", ""], ["Iradukunda", "Hugues Nelson", ""], ["Zhang", "Qingquan", ""], ["Zhu", "Ting", ""]]}, {"id": "2101.03221", "submitter": "Stefano Martina", "authors": "Stefano Martina, Stefano Gherardini, Filippo Caruso", "title": "Machine learning approach for quantum non-Markovian noise classification", "comments": "14 pages, 3 figures, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "quant-ph cond-mat.dis-nn cs.AI cs.LG cs.NE", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  In this paper, machine learning and artificial neural network models are\nproposed for quantum noise classification in stochastic quantum dynamics. For\nthis purpose, we train and then validate support vector machine, multi-layer\nperceptron and recurrent neural network, models with different complexity and\naccuracy, to solve supervised binary classification problems. By exploiting the\nquantum random walk formalism, we demonstrate the high efficacy of such tools\nin classifying noisy quantum dynamics using data sets collected in a single\nrealisation of the quantum system evolution. In addition, we also show that for\na successful classification one just needs to measure, in a sequence of\ndiscrete time instants, the probabilities that the analysed quantum system is\nin one of the allowed positions or energy configurations, without any external\ndriving. Thus, neither measurements of quantum coherences nor sequences of\ncontrol pulses are required. Since in principle the training of the machine\nlearning models can be performed a-priori on synthetic data, our approach is\nexpected to find direct application in a vast number of experimental schemes\nand also for the noise benchmarking of the already available noisy\nintermediate-scale quantum devices.\n", "versions": [{"version": "v1", "created": "Fri, 8 Jan 2021 20:56:56 GMT"}], "update_date": "2021-01-12", "authors_parsed": [["Martina", "Stefano", ""], ["Gherardini", "Stefano", ""], ["Caruso", "Filippo", ""]]}, {"id": "2101.03236", "submitter": "Ping Yu", "authors": "Ping Yu, Ruiyi Zhang, Yang Zhao, Yizhe Zhang, Chunyuan Li, Changyou\n  Chen", "title": "SDA: Improving Text Generation with Self Data Augmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Data augmentation has been widely used to improve deep neural networks in\nmany research fields, such as computer vision. However, less work has been done\nin the context of text, partially due to its discrete nature and the complexity\nof natural languages. In this paper, we propose to improve the standard maximum\nlikelihood estimation (MLE) paradigm by incorporating a self-imitation-learning\nphase for automatic data augmentation. Unlike most existing sentence-level\naugmentation strategies, which are only applied to specific models, our method\nis more general and could be easily adapted to any MLE-based training\nprocedure. In addition, our framework allows task-specific evaluation metrics\nto be designed to flexibly control the generated sentences, for example, in\nterms of controlling vocabulary usage and avoiding nontrivial repetitions.\nExtensive experimental results demonstrate the superiority of our method on two\nsynthetic and several standard real datasets, significantly improving related\nbaselines.\n", "versions": [{"version": "v1", "created": "Sat, 2 Jan 2021 01:15:57 GMT"}], "update_date": "2021-01-12", "authors_parsed": [["Yu", "Ping", ""], ["Zhang", "Ruiyi", ""], ["Zhao", "Yang", ""], ["Zhang", "Yizhe", ""], ["Li", "Chunyuan", ""], ["Chen", "Changyou", ""]]}, {"id": "2101.03238", "submitter": "Yichen Yang", "authors": "Jeevana Priya Inala, Yichen Yang, James Paulos, Yewen Pu, Osbert\n  Bastani, Vijay Kumar, Martin Rinard, Armando Solar-Lezama", "title": "Neurosymbolic Transformers for Multi-Agent Communication", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MA cs.LG cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of inferring communication structures that can solve\ncooperative multi-agent planning problems while minimizing the amount of\ncommunication. We quantify the amount of communication as the maximum degree of\nthe communication graph; this metric captures settings where agents have\nlimited bandwidth. Minimizing communication is challenging due to the\ncombinatorial nature of both the decision space and the objective; for\ninstance, we cannot solve this problem by training neural networks using\ngradient descent. We propose a novel algorithm that synthesizes a control\npolicy that combines a programmatic communication policy used to generate the\ncommunication graph with a transformer policy network used to choose actions.\nOur algorithm first trains the transformer policy, which implicitly generates a\n\"soft\" communication graph; then, it synthesizes a programmatic communication\npolicy that \"hardens\" this graph, forming a neurosymbolic transformer. Our\nexperiments demonstrate how our approach can synthesize policies that generate\nlow-degree communication graphs while maintaining near-optimal performance.\n", "versions": [{"version": "v1", "created": "Tue, 5 Jan 2021 04:13:57 GMT"}], "update_date": "2021-01-12", "authors_parsed": [["Inala", "Jeevana Priya", ""], ["Yang", "Yichen", ""], ["Paulos", "James", ""], ["Pu", "Yewen", ""], ["Bastani", "Osbert", ""], ["Kumar", "Vijay", ""], ["Rinard", "Martin", ""], ["Solar-Lezama", "Armando", ""]]}, {"id": "2101.03247", "submitter": "AmirAbbas Davari", "authors": "Michael Holzmann, Amirabbas Davari, Thorsten Seehaus, Matthias Braun,\n  Andreas Maier, Vincent Christlein", "title": "Glacier Calving Front Segmentation Using Attention U-Net", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  An essential climate variable to determine the tidewater glacier status is\nthe location of the calving front position and the separation of seasonal\nvariability from long-term trends. Previous studies have proposed deep\nlearning-based methods to semi-automatically delineate the calving fronts of\ntidewater glaciers. They used U-Net to segment the ice and non-ice regions and\nextracted the calving fronts in a post-processing step. In this work, we show a\nmethod to segment the glacier calving fronts from SAR images in an end-to-end\nfashion using Attention U-Net. The main objective is to investigate the\nattention mechanism in this application. Adding attention modules to the\nstate-of-the-art U-Net network lets us analyze the learning process by\nextracting its attention maps. We use these maps as a tool to search for proper\nhyperparameters and loss functions in order to generate higher qualitative\nresults. Our proposed attention U-Net performs comparably to the standard U-Net\nwhile providing additional insight into those regions on which the network\nlearned to focus more. In the best case, the attention U-Net achieves a 1.5%\nbetter Dice score compared to the canonical U-Net with a glacier front line\nprediction certainty of up to 237.12 meters.\n", "versions": [{"version": "v1", "created": "Fri, 8 Jan 2021 23:06:21 GMT"}], "update_date": "2021-01-12", "authors_parsed": [["Holzmann", "Michael", ""], ["Davari", "Amirabbas", ""], ["Seehaus", "Thorsten", ""], ["Braun", "Matthias", ""], ["Maier", "Andreas", ""], ["Christlein", "Vincent", ""]]}, {"id": "2101.03249", "submitter": "AmirAbbas Davari", "authors": "Andreas Hartmann, Amirabbas Davari, Thorsten Seehaus, Matthias Braun,\n  Andreas Maier, Vincent Christlein", "title": "Bayesian U-Net for Segmenting Glaciers in SAR Imagery", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Fluctuations of the glacier calving front have an important influence over\nthe ice flow of whole glacier systems. It is therefore important to precisely\nmonitor the position of the calving front. However, the manual delineation of\nSAR images is a difficult, laborious and subjective task. Convolutional neural\nnetworks have previously shown promising results in automating the glacier\nsegmentation in SAR images, making them desirable for further exploration of\ntheir possibilities. In this work, we propose to compute uncertainty and use it\nin an Uncertainty Optimization regime as a novel two-stage process. By using\ndropout as a random sampling layer in a U-Net architecture, we create a\nprobabilistic Bayesian Neural Network. With several forward passes, we create a\nsampling distribution, which can estimate the model uncertainty for each pixel\nin the segmentation mask. The additional uncertainty map information can serve\nas a guideline for the experts in the manual annotation of the data.\nFurthermore, feeding the uncertainty map to the network leads to 95.24% Dice\nsimilarity, which is an overall improvement in the segmentation performance\ncompared to the state-of-the-art deterministic U-Net-based glacier segmentation\npipelines.\n", "versions": [{"version": "v1", "created": "Fri, 8 Jan 2021 23:17:49 GMT"}, {"version": "v2", "created": "Tue, 4 May 2021 08:39:17 GMT"}], "update_date": "2021-05-05", "authors_parsed": [["Hartmann", "Andreas", ""], ["Davari", "Amirabbas", ""], ["Seehaus", "Thorsten", ""], ["Braun", "Matthias", ""], ["Maier", "Andreas", ""], ["Christlein", "Vincent", ""]]}, {"id": "2101.03251", "submitter": "Babak Taati", "authors": "Siavash Rezaei, Abhishek Moturu, Shun Zhao, Kenneth M. Prkachin,\n  Thomas Hadjistavropoulos, and Babak Taati", "title": "Unobtrusive Pain Monitoring in Older Adults with Dementia using Pairwise\n  and Contrastive Training", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Although pain is frequent in old age, older adults are often undertreated for\npain. This is especially the case for long-term care residents with moderate to\nsevere dementia who cannot report their pain because of cognitive impairments\nthat accompany dementia. Nursing staff acknowledge the challenges of\neffectively recognizing and managing pain in long-term care facilities due to\nlack of human resources and, sometimes, expertise to use validated pain\nassessment approaches on a regular basis. Vision-based ambient monitoring will\nallow for frequent automated assessments so care staff could be automatically\nnotified when signs of pain are displayed. However, existing computer vision\ntechniques for pain detection are not validated on faces of older adults or\npeople with dementia, and this population is not represented in existing facial\nexpression datasets of pain. We present the first fully automated vision-based\ntechnique validated on a dementia cohort. Our contributions are threefold.\nFirst, we develop a deep learning-based computer vision system for detecting\npainful facial expressions on a video dataset that is collected unobtrusively\nfrom older adult participants with and without dementia. Second, we introduce a\npairwise comparative inference method that calibrates to each person and is\nsensitive to changes in facial expression while using training data more\nefficiently than sequence models. Third, we introduce a fast contrastive\ntraining method that improves cross-dataset performance. Our pain estimation\nmodel outperforms baselines by a wide margin, especially when evaluated on\nfaces of people with dementia. Pre-trained model and demo code available at\nhttps://github.com/TaatiTeam/pain_detection_demo\n", "versions": [{"version": "v1", "created": "Fri, 8 Jan 2021 23:28:30 GMT"}], "update_date": "2021-01-12", "authors_parsed": [["Rezaei", "Siavash", ""], ["Moturu", "Abhishek", ""], ["Zhao", "Shun", ""], ["Prkachin", "Kenneth M.", ""], ["Hadjistavropoulos", "Thomas", ""], ["Taati", "Babak", ""]]}, {"id": "2101.03252", "submitter": "AmirAbbas Davari", "authors": "Rosanna Dietrich-Sussner, Amirabbas Davari, Thorsten Seehaus, Matthias\n  Braun, Vincent Christlein, Andreas Maier, Christian Riess", "title": "Synthetic Glacier SAR Image Generation from Arbitrary Masks Using\n  Pix2Pix Algorithm", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Supervised machine learning requires a large amount of labeled data to\nachieve proper test results. However, generating accurately labeled\nsegmentation maps on remote sensing imagery, including images from synthetic\naperture radar (SAR), is tedious and highly subjective. In this work, we\npropose to alleviate the issue of limited training data by generating synthetic\nSAR images with the pix2pix algorithm. This algorithm uses conditional\nGenerative Adversarial Networks (cGANs) to generate an artificial image while\npreserving the structure of the input. In our case, the input is a segmentation\nmask, from which a corresponding synthetic SAR image is generated. We present\ndifferent models, perform a comparative study and demonstrate that this\napproach synthesizes convincing glaciers in SAR images with promising\nqualitative and quantitative results.\n", "versions": [{"version": "v1", "created": "Fri, 8 Jan 2021 23:30:00 GMT"}, {"version": "v2", "created": "Thu, 14 Jan 2021 22:07:48 GMT"}], "update_date": "2021-01-18", "authors_parsed": [["Dietrich-Sussner", "Rosanna", ""], ["Davari", "Amirabbas", ""], ["Seehaus", "Thorsten", ""], ["Braun", "Matthias", ""], ["Christlein", "Vincent", ""], ["Maier", "Andreas", ""], ["Riess", "Christian", ""]]}, {"id": "2101.03255", "submitter": "Haoyu Ma", "authors": "Haoyu Ma, Tianlong Chen, Ting-Kuei Hu, Chenyu You, Xiaohui Xie,\n  Zhangyang Wang", "title": "Good Students Play Big Lottery Better", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Lottery ticket hypothesis suggests that a dense neural network contains a\nsparse sub-network that can match the test accuracy of the original dense net\nwhen trained in isolation from (the same) random initialization. However, the\nhypothesis failed to generalize to larger dense networks such as ResNet-50. As\na remedy, recent studies demonstrate that a sparse sub-network can still be\nobtained by using a rewinding technique, which is to re-train it from\nearly-phase training weights or learning rates of the dense model, rather than\nfrom random initialization.\n  Is rewinding the only or the best way to scale up lottery tickets? This paper\nproposes a new, simpler and yet powerful technique for re-training the\nsub-network, called \"Knowledge Distillation ticket\" (KD ticket). Rewinding\nexploits the value of inheriting knowledge from the early training phase to\nimprove lottery tickets in large networks. In comparison, KD ticket addresses a\ncomplementary possibility - inheriting useful knowledge from the late training\nphase of the dense model. It is achieved by leveraging the soft labels\ngenerated by the trained dense model to re-train the sub-network, instead of\nthe hard labels. Extensive experiments are conducted using several large deep\nnetworks (e.g ResNet-50 and ResNet-110) on CIFAR-10 and ImageNet datasets.\nWithout bells and whistles, when applied by itself, KD ticket performs on par\nor better than rewinding, while being nearly free of hyperparameters or ad-hoc\nselection. KD ticket can be further applied together with rewinding, yielding\nstate-of-the-art results for large-scale lottery tickets.\n", "versions": [{"version": "v1", "created": "Fri, 8 Jan 2021 23:33:53 GMT"}, {"version": "v2", "created": "Mon, 18 Jan 2021 07:25:16 GMT"}], "update_date": "2021-01-19", "authors_parsed": [["Ma", "Haoyu", ""], ["Chen", "Tianlong", ""], ["Hu", "Ting-Kuei", ""], ["You", "Chenyu", ""], ["Xie", "Xiaohui", ""], ["Wang", "Zhangyang", ""]]}, {"id": "2101.03263", "submitter": "Matthew Sotoudeh", "authors": "Matthew Sotoudeh and Aditya V. Thakur", "title": "SyReNN: A Tool for Analyzing Deep Neural Networks", "comments": "Accepted paper at TACAS 2021. Tool is available at\n  https://github.com/95616ARG/SyReNN", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep Neural Networks (DNNs) are rapidly gaining popularity in a variety of\nimportant domains. Formally, DNNs are complicated vector-valued functions which\ncome in a variety of sizes and applications. Unfortunately, modern DNNs have\nbeen shown to be vulnerable to a variety of attacks and buggy behavior. This\nhas motivated recent work in formally analyzing the properties of such DNNs.\nThis paper introduces SyReNN, a tool for understanding and analyzing a DNN by\ncomputing its symbolic representation. The key insight is to decompose the DNN\ninto linear functions. Our tool is designed for analyses using low-dimensional\nsubsets of the input space, a unique design point in the space of DNN analysis\ntools. We describe the tool and the underlying theory, then evaluate its use\nand performance on three case studies: computing Integrated Gradients,\nvisualizing a DNN's decision boundaries, and patching a DNN.\n", "versions": [{"version": "v1", "created": "Sat, 9 Jan 2021 00:27:23 GMT"}], "update_date": "2021-01-12", "authors_parsed": [["Sotoudeh", "Matthew", ""], ["Thakur", "Aditya V.", ""]]}, {"id": "2101.03271", "submitter": "Jonathan Daniel Smith", "authors": "Jonathan D. Smith, Zachary E. Ross, Kamyar Azizzadenesheli, Jack B.\n  Muir", "title": "HypoSVI: Hypocenter inversion with Stein variational inference and\n  Physics Informed Neural Networks", "comments": "Changing the figures to rasterized form to reduce file size", "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.geo-ph cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a scheme for probabilistic hypocenter inversion with Stein\nvariational inference. Our approach uses a differentiable forward model in the\nform of a physics-informed neural network, which we train to solve the Eikonal\nequation. This allows for rapid approximation of the posterior by iteratively\noptimizing a collection of particles against a kernelized Stein discrepancy. We\nshow that the method is well-equipped to handle highly non-convex posterior\ndistributions, which are common in hypocentral inverse problems. A suite of\nexperiments is performed to examine the influence of the various\nhyperparameters. Once trained, the method is valid for any network geometry\nwithin the study area without the need to build travel time tables. We show\nthat the computational demands scale efficiently with the number of\ndifferential times, making it ideal for large-N sensing technologies like\nDistributed Acoustic Sensing.\n", "versions": [{"version": "v1", "created": "Sat, 9 Jan 2021 01:56:48 GMT"}, {"version": "v2", "created": "Tue, 12 Jan 2021 20:49:19 GMT"}], "update_date": "2021-01-14", "authors_parsed": [["Smith", "Jonathan D.", ""], ["Ross", "Zachary E.", ""], ["Azizzadenesheli", "Kamyar", ""], ["Muir", "Jack B.", ""]]}, {"id": "2101.03273", "submitter": "Saeed Kaviani", "authors": "Saeed Kaviani, Bo Ryu, Ejaz Ahmed, Kevin A. Larson, Anh Le, Alex\n  Yahja, Jae H. Kim", "title": "Robust and Scalable Routing with Multi-Agent Deep Reinforcement Learning\n  for MANETs", "comments": "14 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI cs.AI cs.LG cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Highly dynamic mobile ad-hoc networks (MANETs) are continuing to serve as one\nof the most challenging environments to develop and deploy robust, efficient,\nand scalable routing protocols. In this paper, we present DeepCQ+ routing\nwhich, in a novel manner, integrates emerging multi-agent deep reinforcement\nlearning (MADRL) techniques into existing Q-learning-based routing protocols\nand their variants, and achieves persistently higher performance across a wide\nrange of MANET configurations while training only on a limited range of network\nparameters and conditions. Quantitatively, DeepCQ+ shows consistently higher\nend-to-end throughput with lower overhead compared to its Q-learning-based\ncounterparts with the overall gain of 10-15% in its efficiency. Qualitatively\nand more significantly, DeepCQ+ maintains remarkably similar performance gains\nunder many scenarios that it was not trained for in terms of network sizes,\nmobility conditions, and traffic dynamics. To the best of our knowledge, this\nis the first successful demonstration of MADRL for the MANET routing problem\nthat achieves and maintains a high degree of scalability and robustness even in\nthe environments that are outside the trained range of scenarios. This implies\nthat the proposed hybrid design approach of DeepCQ+ that combines MADRL and\nQ-learning significantly increases its practicality and explainability because\nthe real-world MANET environment will likely vary outside the trained range of\nMANET scenarios.\n", "versions": [{"version": "v1", "created": "Sat, 9 Jan 2021 02:26:14 GMT"}, {"version": "v2", "created": "Mon, 29 Mar 2021 02:53:58 GMT"}], "update_date": "2021-03-30", "authors_parsed": [["Kaviani", "Saeed", ""], ["Ryu", "Bo", ""], ["Ahmed", "Ejaz", ""], ["Larson", "Kevin A.", ""], ["Le", "Anh", ""], ["Yahja", "Alex", ""], ["Kim", "Jae H.", ""]]}, {"id": "2101.03279", "submitter": "Fang-Chieh Chou", "authors": "Abhishek Mohta, Fang-Chieh Chou, Brian C. Becker, Carlos\n  Vallespi-Gonzalez, Nemanja Djuric", "title": "Investigating the Effect of Sensor Modalities in Multi-Sensor\n  Detection-Prediction Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Detection of surrounding objects and their motion prediction are critical\ncomponents of a self-driving system. Recently proposed models that jointly\naddress these tasks rely on a number of sensors to achieve state-of-the-art\nperformance. However, this increases system complexity and may result in a\nbrittle model that overfits to any single sensor modality while ignoring\nothers, leading to reduced generalization. We focus on this important problem\nand analyze the contribution of sensor modalities towards the model\nperformance. In addition, we investigate the use of sensor dropout to mitigate\nthe above-mentioned issues, leading to a more robust, better-performing model\non real-world driving data.\n", "versions": [{"version": "v1", "created": "Sat, 9 Jan 2021 03:21:36 GMT"}], "update_date": "2021-01-12", "authors_parsed": [["Mohta", "Abhishek", ""], ["Chou", "Fang-Chieh", ""], ["Becker", "Brian C.", ""], ["Vallespi-Gonzalez", "Carlos", ""], ["Djuric", "Nemanja", ""]]}, {"id": "2101.03285", "submitter": "Yu Tian", "authors": "Yu Tian, Leonardo Zorron Cheng Tao Pu, Yuyuan Liu, Gabriel Maicas,\n  Johan W. Verjans, Alastair D. Burt, Seon Ho Shin, Rajvinder Singh, Gustavo\n  Carneiro", "title": "Detecting, Localising and Classifying Polyps from Colonoscopy Videos\n  using Deep Learning", "comments": "Preprint to submit to IEEE journals", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose and analyse a system that can automatically detect,\nlocalise and classify polyps from colonoscopy videos. The detection of frames\nwith polyps is formulated as a few-shot anomaly classification problem, where\nthe training set is highly imbalanced with the large majority of frames\nconsisting of normal images and a small minority comprising frames with polyps.\nColonoscopy videos may contain blurry images and frames displaying feces and\nwater jet sprays to clean the colon -- such frames can mistakenly be detected\nas anomalies, so we have implemented a classifier to reject these two types of\nframes before polyp detection takes place. Next, given a frame containing a\npolyp, our method localises (with a bounding box around the polyp) and\nclassifies it into five different classes. Furthermore, we study a method to\nimprove the reliability and interpretability of the classification result using\nuncertainty estimation and classification calibration. Classification\nuncertainty and calibration not only help improve classification accuracy by\nrejecting low-confidence and high-uncertain results, but can be used by doctors\nto decide how to decide on the classification of a polyp. All the proposed\ndetection, localisation and classification methods are tested using large data\nsets and compared with relevant baseline approaches.\n", "versions": [{"version": "v1", "created": "Sat, 9 Jan 2021 04:25:34 GMT"}], "update_date": "2021-01-12", "authors_parsed": [["Tian", "Yu", ""], ["Pu", "Leonardo Zorron Cheng Tao", ""], ["Liu", "Yuyuan", ""], ["Maicas", "Gabriel", ""], ["Verjans", "Johan W.", ""], ["Burt", "Alastair D.", ""], ["Shin", "Seon Ho", ""], ["Singh", "Rajvinder", ""], ["Carneiro", "Gustavo", ""]]}, {"id": "2101.03288", "submitter": "Yang Song", "authors": "Yang Song and Diederik P. Kingma", "title": "How to Train Your Energy-Based Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Energy-Based Models (EBMs), also known as non-normalized probabilistic\nmodels, specify probability density or mass functions up to an unknown\nnormalizing constant. Unlike most other probabilistic models, EBMs do not place\na restriction on the tractability of the normalizing constant, thus are more\nflexible to parameterize and can model a more expressive family of probability\ndistributions. However, the unknown normalizing constant of EBMs makes training\nparticularly difficult. Our goal is to provide a friendly introduction to\nmodern approaches for EBM training. We start by explaining maximum likelihood\ntraining with Markov chain Monte Carlo (MCMC), and proceed to elaborate on\nMCMC-free approaches, including Score Matching (SM) and Noise Constrastive\nEstimation (NCE). We highlight theoretical connections among these three\napproaches, and end with a brief survey on alternative training methods, which\nare still under active research. Our tutorial is targeted at an audience with\nbasic understanding of generative models who want to apply EBMs or start a\nresearch project in this direction.\n", "versions": [{"version": "v1", "created": "Sat, 9 Jan 2021 04:51:31 GMT"}, {"version": "v2", "created": "Wed, 17 Feb 2021 19:20:09 GMT"}], "update_date": "2021-02-19", "authors_parsed": [["Song", "Yang", ""], ["Kingma", "Diederik P.", ""]]}, {"id": "2101.03295", "submitter": "Bahareh Najafi", "authors": "Bahareh Najafi, Saeedeh Parsaeefard, Alberto Leon-Garcia", "title": "Estimation of Missing Data in Intelligent Transportation System", "comments": "presented at the 2020 92nd IEEE conference on vehicular technology,\n  18 Nov.-16 Dec 2020 6 pages, 5 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Missing data is a challenge in many applications, including intelligent\ntransportation systems (ITS). In this paper, we study traffic speed and travel\ntime estimations in ITS, where portions of the collected data are missing due\nto sensor instability and communication errors at collection points. These\npractical issues can be remediated by missing data analysis, which are mainly\ncategorized as either statistical or machine learning(ML)-based approaches.\nStatistical methods require the prior probability distribution of the data\nwhich is unknown in our application. Therefore, we focus on an ML-based\napproach, Multi-Directional Recurrent Neural Network (M-RNN). M-RNN utilizes\nboth temporal and spatial characteristics of the data. We evaluate the\neffectiveness of this approach on a TomTom dataset containing spatio-temporal\nmeasurements of average vehicle speed and travel time in the Greater Toronto\nArea (GTA). We evaluate the method under various conditions, where the results\ndemonstrate that M-RNN outperforms existing solutions,e.g., spline\ninterpolation and matrix completion, by up to 58% decreases in Root Mean Square\nError (RMSE).\n", "versions": [{"version": "v1", "created": "Sat, 9 Jan 2021 05:42:31 GMT"}], "update_date": "2021-01-12", "authors_parsed": [["Najafi", "Bahareh", ""], ["Parsaeefard", "Saeedeh", ""], ["Leon-Garcia", "Alberto", ""]]}, {"id": "2101.03298", "submitter": "Bolin Ding", "authors": "Shuyuan Yan, Bolin Ding, Wei Guo, Jingren Zhou, Zhewei Wei, Xiaowei\n  Jiang, and Sheng Xu", "title": "FlashP: An Analytical Pipeline for Real-time Forecasting of Time-Series\n  Relational Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Interactive response time is important in analytical pipelines for users to\nexplore a sufficient number of possibilities and make informed business\ndecisions. We consider a forecasting pipeline with large volumes of\nhigh-dimensional time series data. Real-time forecasting can be conducted in\ntwo steps. First, we specify the part of data to be focused on and the measure\nto be predicted by slicing, dicing, and aggregating the data. Second, a\nforecasting model is trained on the aggregated results to predict the trend of\nthe specified measure. While there are a number of forecasting models\navailable, the first step is the performance bottleneck. A natural idea is to\nutilize sampling to obtain approximate aggregations in real time as the input\nto train the forecasting model. Our scalable real-time forecasting system\nFlashP (Flash Prediction) is built based on this idea, with two major\nchallenges to be resolved in this paper: first, we need to figure out how\napproximate aggregations affect the fitting of forecasting models, and\nforecasting results; and second, accordingly, what sampling algorithms we\nshould use to obtain these approximate aggregations and how large the samples\nare. We introduce a new sampling scheme, called GSW sampling, and analyze error\nbounds for estimating aggregations using GSW samples. We introduce how to\nconstruct compact GSW samples with the existence of multiple measures to be\nanalyzed. We conduct experiments to evaluate our solution and compare it with\nalternatives on real data.\n", "versions": [{"version": "v1", "created": "Sat, 9 Jan 2021 06:23:13 GMT"}, {"version": "v2", "created": "Sat, 16 Jan 2021 00:46:04 GMT"}], "update_date": "2021-01-19", "authors_parsed": [["Yan", "Shuyuan", ""], ["Ding", "Bolin", ""], ["Guo", "Wei", ""], ["Zhou", "Jingren", ""], ["Wei", "Zhewei", ""], ["Jiang", "Xiaowei", ""], ["Xu", "Sheng", ""]]}, {"id": "2101.03300", "submitter": "Hang Chen", "authors": "Hang Chen, Syed Ali Asif, Jihong Park, Chien-Chung Shen, Mehdi Bennis", "title": "Robust Blockchained Federated Learning with Model Validation and\n  Proof-of-Stake Inspired Consensus", "comments": "8 pages, 7 figures, AAAI 2021 Workshop - Towards Robust, Secure and\n  Efficient Machine Learning", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CR cs.DC cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Federated learning (FL) is a promising distributed learning solution that\nonly exchanges model parameters without revealing raw data. However, the\ncentralized architecture of FL is vulnerable to the single point of failure. In\naddition, FL does not examine the legitimacy of local models, so even a small\nfraction of malicious devices can disrupt global training. To resolve these\nrobustness issues of FL, in this paper, we propose a blockchain-based\ndecentralized FL framework, termed VBFL, by exploiting two mechanisms in a\nblockchained architecture. First, we introduced a novel decentralized\nvalidation mechanism such that the legitimacy of local model updates is\nexamined by individual validators. Second, we designed a dedicated\nproof-of-stake consensus mechanism where stake is more frequently rewarded to\nhonest devices, which protects the legitimate local model updates by increasing\ntheir chances of dictating the blocks appended to the blockchain. Together,\nthese solutions promote more federation within legitimate devices, enabling\nrobust FL. Our emulation results of the MNIST classification corroborate that\nwith 15% of malicious devices, VBFL achieves 87% accuracy, which is 7.4x higher\nthan Vanilla FL.\n", "versions": [{"version": "v1", "created": "Sat, 9 Jan 2021 06:30:38 GMT"}], "update_date": "2021-01-12", "authors_parsed": [["Chen", "Hang", ""], ["Asif", "Syed Ali", ""], ["Park", "Jihong", ""], ["Shen", "Chien-Chung", ""], ["Bennis", "Mehdi", ""]]}, {"id": "2101.03303", "submitter": "Anurag Roy", "authors": "Anurag Roy, Shalmoli Ghosh, Kripabandhu Ghosh, Saptarshi Ghosh", "title": "An Unsupervised Normalization Algorithm for Noisy Text: A Case Study for\n  Information Retrieval and Stance Detection", "comments": "Will be appearing in the ACM Journal of Data and Information Quality.\n  Implementation available at https://github.com/ranarag/UnsupClean", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.AI cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A large fraction of textual data available today contains various types of\n'noise', such as OCR noise in digitized documents, noise due to informal\nwriting style of users on microblogging sites, and so on. To enable tasks such\nas search/retrieval and classification over all the available data, we need\nrobust algorithms for text normalization, i.e., for cleaning different kinds of\nnoise in the text. There have been several efforts towards cleaning or\nnormalizing noisy text; however, many of the existing text normalization\nmethods are supervised and require language-dependent resources or large\namounts of training data that is difficult to obtain. We propose an\nunsupervised algorithm for text normalization that does not need any training\ndata / human intervention. The proposed algorithm is applicable to text over\ndifferent languages, and can handle both machine-generated and human-generated\nnoise. Experiments over several standard datasets show that text normalization\nthrough the proposed algorithm enables better retrieval and stance detection,\nas compared to that using several baseline text normalization methods.\nImplementation of our algorithm can be found at\nhttps://github.com/ranarag/UnsupClean.\n", "versions": [{"version": "v1", "created": "Sat, 9 Jan 2021 06:57:09 GMT"}], "update_date": "2021-01-12", "authors_parsed": [["Roy", "Anurag", ""], ["Ghosh", "Shalmoli", ""], ["Ghosh", "Kripabandhu", ""], ["Ghosh", "Saptarshi", ""]]}, {"id": "2101.03305", "submitter": "Ting Jiang", "authors": "Ting Jiang, Deqing Wang, Leilei Sun, Huayi Yang, Zhengyang Zhao,\n  Fuzhen Zhuang", "title": "LightXML: Transformer with Dynamic Negative Sampling for\n  High-Performance Extreme Multi-label Text Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Extreme Multi-label text Classification (XMC) is a task of finding the most\nrelevant labels from a large label set. Nowadays deep learning-based methods\nhave shown significant success in XMC. However, the existing methods (e.g.,\nAttentionXML and X-Transformer etc) still suffer from 1) combining several\nmodels to train and predict for one dataset, and 2) sampling negative labels\nstatically during the process of training label ranking model, which reduces\nboth the efficiency and accuracy of the model. To address the above problems,\nwe proposed LightXML, which adopts end-to-end training and dynamic negative\nlabels sampling. In LightXML, we use generative cooperative networks to recall\nand rank labels, in which label recalling part generates negative and positive\nlabels, and label ranking part distinguishes positive labels from these labels.\nThrough these networks, negative labels are sampled dynamically during label\nranking part training by feeding with the same text representation. Extensive\nexperiments show that LightXML outperforms state-of-the-art methods in five\nextreme multi-label datasets with much smaller model size and lower\ncomputational complexity. In particular, on the Amazon dataset with 670K\nlabels, LightXML can reduce the model size up to 72% compared to AttentionXML.\n", "versions": [{"version": "v1", "created": "Sat, 9 Jan 2021 07:04:18 GMT"}], "update_date": "2021-01-12", "authors_parsed": [["Jiang", "Ting", ""], ["Wang", "Deqing", ""], ["Sun", "Leilei", ""], ["Yang", "Huayi", ""], ["Zhao", "Zhengyang", ""], ["Zhuang", "Fuzhen", ""]]}, {"id": "2101.03308", "submitter": "Ruibing Song", "authors": "Ruibing Song, Kejie Huang, Zongsheng Wang, Haibin Shen", "title": "An Ultra Fast Low Power Convolutional Neural Network Image Sensor with\n  Pixel-level Computing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The separation of the data capture and analysis in modern vision systems has\nled to a massive amount of data transfer between the end devices and cloud\ncomputers, resulting in long latency, slow response, and high power\nconsumption. Efficient hardware architectures are under focused development to\nenable Artificial Intelligence (AI) at the resource-limited end sensing\ndevices. This paper proposes a Processing-In-Pixel (PIP) CMOS sensor\narchitecture, which allows convolution operation before the column readout\ncircuit to significantly improve the image reading speed with much lower power\nconsumption. The simulation results show that the proposed architecture enables\nconvolution operation (kernel size=3*3, stride=2, input channel=3, output\nchannel=64) in a 1080P image sensor array with only 22.62 mW power consumption.\nIn other words, the computational efficiency is 4.75 TOPS/w, which is about 3.6\ntimes as higher as the state-of-the-art.\n", "versions": [{"version": "v1", "created": "Sat, 9 Jan 2021 07:10:03 GMT"}], "update_date": "2021-01-12", "authors_parsed": [["Song", "Ruibing", ""], ["Huang", "Kejie", ""], ["Wang", "Zongsheng", ""], ["Shen", "Haibin", ""]]}, {"id": "2101.03309", "submitter": "Jianzhun Du", "authors": "Kristine Zhang, Yuanheng Wang, Jianzhun Du, Brian Chu, Leo Anthony\n  Celi, Ryan Kindle, Finale Doshi-Velez", "title": "Identifying Decision Points for Safe and Interpretable Reinforcement\n  Learning in Hypotension Treatment", "comments": "NeurIPS 2020 Machine Learning for Health (ML4H) Workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Many batch RL health applications first discretize time into fixed intervals.\nHowever, this discretization both loses resolution and forces a policy\ncomputation at each (potentially fine) interval. In this work, we develop a\nnovel framework to compress continuous trajectories into a few, interpretable\ndecision points --places where the batch data support multiple alternatives. We\napply our approach to create recommendations from a cohort of hypotensive\npatients dataset. Our reduced state space results in faster planning and allows\neasy inspection by a clinical expert.\n", "versions": [{"version": "v1", "created": "Sat, 9 Jan 2021 07:15:33 GMT"}], "update_date": "2021-01-12", "authors_parsed": [["Zhang", "Kristine", ""], ["Wang", "Yuanheng", ""], ["Du", "Jianzhun", ""], ["Chu", "Brian", ""], ["Celi", "Leo Anthony", ""], ["Kindle", "Ryan", ""], ["Doshi-Velez", "Finale", ""]]}, {"id": "2101.03323", "submitter": "Jingwei Liu", "authors": "Jingwei Liu", "title": "SARS-Cov-2 RNA Sequence Classification Based on Territory Information", "comments": "7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.QM cs.LG stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  CovID-19 genetics analysis is critical to determine virus type,virus variant\nand evaluate vaccines. In this paper, SARS-Cov-2 RNA sequence analysis relative\nto region or territory is investigated. A uniform framework of sequence SVM\nmodel with various genetics length from short to long and mixed-bases is\ndeveloped by projecting SARS-Cov-2 RNA sequence to different dimensional space,\nthen scoring it according to the output probability of pre-trained SVM models\nto explore the territory or origin information of SARS-Cov-2. Different sample\nsize ratio of training set and test set is also discussed in the data analysis.\nTwo SARS-Cov-2 RNA classification tasks are constructed based on GISAID\ndatabase, one is for mainland, Hongkong and Taiwan of China, and the other is a\n6-class classification task (Africa, Asia, Europe, North American, South\nAmerican\\& Central American, Ocean) of 7 continents. For 3-class classification\nof China, the Top-1 accuracy rate can reach 82.45\\% (train 60\\%, test=40\\%);\nFor 2-class classification of China, the Top-1 accuracy rate can reach 97.35\\%\n(train 80\\%, test 20\\%); For 6-class classification task of world, when the\nratio of training set and test set is 20\\% : 80\\% , the Top-1 accuracy rate can\nachieve 30.30\\%. And, some Top-N results are also given.\n", "versions": [{"version": "v1", "created": "Sat, 9 Jan 2021 09:12:27 GMT"}], "update_date": "2021-01-12", "authors_parsed": [["Liu", "Jingwei", ""]]}, {"id": "2101.03336", "submitter": "Robin Gubela", "authors": "Robin M. Gubela and Stefan Lessmann", "title": "Interpretable Multiple Treatment Revenue Uplift Modeling", "comments": null, "journal-ref": "Proceedings of the 26th Americas Conference on Information Systems\n  (AMCIS 2020)", "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Big data and business analytics are critical drivers of business and societal\ntransformations. Uplift models support a firm's decision-making by predicting\nthe change of a customer's behavior due to a treatment. Prior work examines\nmodels for single treatments and binary customer responses. The paper extends\ncorresponding approaches by developing uplift models for multiple treatments\nand continuous outcomes. This facilitates selecting an optimal treatment from a\nset of alternatives and estimating treatment effects in the form of business\noutcomes of continuous scale. Another contribution emerges from an evaluation\nof an uplift model's interpretability, whereas prior studies focus almost\nexclusively on predictive performance. To achieve these goals, the paper\ndevelops revenue uplift models for multiple treatments based on a recently\nintroduced algorithm for causal machine learning, the causal forest. Empirical\nexperimentation using two real-world marketing data sets demonstrates the\nadvantages of the proposed modeling approach over benchmarks and standard\nmarketing practices.\n", "versions": [{"version": "v1", "created": "Sat, 9 Jan 2021 11:29:00 GMT"}], "update_date": "2021-01-12", "authors_parsed": [["Gubela", "Robin M.", ""], ["Lessmann", "Stefan", ""]]}, {"id": "2101.03367", "submitter": "Stefano Savazzi", "authors": "Stefano Savazzi, Monica Nicoli, Mehdi Bennis, Sanaz Kianoush, Luca\n  Barbieri", "title": "Opportunities of Federated Learning in Connected, Cooperative and\n  Automated Industrial Systems", "comments": "The paper has been accepted for publication in the IEEE\n  Communications Magazine. The current arXiv contains an additional Appendix\n  that describes the dataset for the setup of Fig. 5", "journal-ref": "IEEE Communications Magazine, vol. 59, no. 2, pp. 16-21, February\n  2021", "doi": "10.1109/MCOM.001.2000200", "report-no": null, "categories": "cs.LG cs.DC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Next-generation autonomous and networked industrial systems (i.e., robots,\nvehicles, drones) have driven advances in ultra-reliable, low latency\ncommunications (URLLC) and computing. These networked multi-agent systems\nrequire fast, communication-efficient and distributed machine learning (ML) to\nprovide mission critical control functionalities. Distributed ML techniques,\nincluding federated learning (FL), represent a mushrooming multidisciplinary\nresearch area weaving in sensing, communication and learning. FL enables\ncontinual model training in distributed wireless systems: rather than fusing\nraw data samples at a centralized server, FL leverages a cooperative fusion\napproach where networked agents, connected via URLLC, act as distributed\nlearners that periodically exchange their locally trained model parameters.\nThis article explores emerging opportunities of FL for the next-generation\nnetworked industrial systems. Open problems are discussed, focusing on\ncooperative driving in connected automated vehicles and collaborative robotics\nin smart manufacturing.\n", "versions": [{"version": "v1", "created": "Sat, 9 Jan 2021 14:27:52 GMT"}, {"version": "v2", "created": "Tue, 12 Jan 2021 22:42:24 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Savazzi", "Stefano", ""], ["Nicoli", "Monica", ""], ["Bennis", "Mehdi", ""], ["Kianoush", "Sanaz", ""], ["Barbieri", "Luca", ""]]}, {"id": "2101.03382", "submitter": "Sayar Ghosh Roy", "authors": "Tathagata Raha, Sayar Ghosh Roy, Ujwal Narayan, Zubair Abid, Vasudeva\n  Varma", "title": "Task Adaptive Pretraining of Transformers for Hostility Detection", "comments": "To be published in: Proceedings of the First Workshop on Combating\n  Online Hostile Posts in Regional Languages during Emergency Situation\n  (CONSTRAINT) at AAAI 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Identifying adverse and hostile content on the web and more particularly, on\nsocial media, has become a problem of paramount interest in recent years. With\ntheir ever increasing popularity, fine-tuning of pretrained Transformer-based\nencoder models with a classifier head are gradually becoming the new baseline\nfor natural language classification tasks. In our work, we explore the gains\nattributed to Task Adaptive Pretraining (TAPT) prior to fine-tuning of\nTransformer-based architectures. We specifically study two problems, namely,\n(a) Coarse binary classification of Hindi Tweets into Hostile or Not, and (b)\nFine-grained multi-label classification of Tweets into four categories: hate,\nfake, offensive, and defamation. Building up on an architecture which takes\nemojis and segmented hashtags into consideration for classification, we are\nable to experimentally showcase the performance upgrades due to TAPT. Our\nsystem (with team name 'iREL IIIT') ranked first in the 'Hostile Post Detection\nin Hindi' shared task with an F1 score of 97.16% for coarse-grained detection\nand a weighted F1 score of 62.96% for fine-grained multi-label classification\non the provided blind test corpora.\n", "versions": [{"version": "v1", "created": "Sat, 9 Jan 2021 15:45:26 GMT"}], "update_date": "2021-01-12", "authors_parsed": [["Raha", "Tathagata", ""], ["Roy", "Sayar Ghosh", ""], ["Narayan", "Ujwal", ""], ["Abid", "Zubair", ""], ["Varma", "Vasudeva", ""]]}, {"id": "2101.03409", "submitter": "Andr\\'e Fusioka", "authors": "Gabriel Henrique de Almeida Pereira and Andr\\'e Minoro Fusioka and\n  Bogdan Tomoyuki Nassu and Rodrigo Minetto", "title": "Active Fire Detection in Landsat-8 Imagery: a Large-Scale Dataset and a\n  Deep-Learning Study", "comments": "23 pages, 17 figures", "journal-ref": null, "doi": "10.1016/j.isprsjprs.2021.06.002", "report-no": null, "categories": "cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Active fire detection in satellite imagery is of critical importance to the\nmanagement of environmental conservation policies, supporting decision-making\nand law enforcement. This is a well established field, with many techniques\nbeing proposed over the years, usually based on pixel or region-level\ncomparisons involving sensor-specific thresholds and neighborhood statistics.\nIn this paper, we address the problem of active fire detection using deep\nlearning techniques. In recent years, deep learning techniques have been\nenjoying an enormous success in many fields, but their use for active fire\ndetection is relatively new, with open questions and demand for datasets and\narchitectures for evaluation. This paper addresses these issues by introducing\na new large-scale dataset for active fire detection, with over 150,000 image\npatches (more than 200 GB of data) extracted from Landsat-8 images captured\naround the world in August and September 2020, containing wildfires in several\nlocations. The dataset was split in two parts, and contains 10-band spectral\nimages with associated outputs, produced by three well known handcrafted\nalgorithms for active fire detection in the first part, and manually annotated\nmasks in the second part. We also present a study on how different\nconvolutional neural network architectures can be used to approximate these\nhandcrafted algorithms, and how models trained on automatically segmented\npatches can be combined to achieve better performance than the original\nalgorithms - with the best combination having 87.2% precision and 92.4% recall\non our manually annotated dataset. The proposed dataset, source codes and\ntrained models are available on Github\n(https://github.com/pereira-gha/activefire), creating opportunities for further\nadvances in the field\n", "versions": [{"version": "v1", "created": "Sat, 9 Jan 2021 19:05:03 GMT"}, {"version": "v2", "created": "Fri, 2 Jul 2021 16:50:53 GMT"}], "update_date": "2021-07-05", "authors_parsed": [["Pereira", "Gabriel Henrique de Almeida", ""], ["Fusioka", "Andr\u00e9 Minoro", ""], ["Nassu", "Bogdan Tomoyuki", ""], ["Minetto", "Rodrigo", ""]]}, {"id": "2101.03418", "submitter": "Sophia Gu", "authors": "Sophia Gu", "title": "Deep Reinforcement Learning with Function Properties in Mean Reversion\n  Strategies", "comments": "11 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.MF cs.LG q-fin.CP", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  With the recent advancement in Deep Reinforcement Learning in the gaming\nindustry, we are curious if the same technology would work as well for common\nquantitative financial problems. In this paper, we will investigate if an\noff-the-shelf library developed by OpenAI can be easily adapted to mean\nreversion strategy. Moreover, we will design and test to see if we can get\nbetter performance by narrowing the function space that the agent needs to\nsearch for. We achieve this through augmenting the reward function by a\ncarefully picked penalty term.\n", "versions": [{"version": "v1", "created": "Sat, 9 Jan 2021 19:41:29 GMT"}, {"version": "v2", "created": "Tue, 12 Jan 2021 03:09:48 GMT"}], "update_date": "2021-01-13", "authors_parsed": [["Gu", "Sophia", ""]]}, {"id": "2101.03419", "submitter": "Shiyu Duan", "authors": "Shiyu Duan and Jose C. Principe", "title": "Training Deep Architectures Without End-to-End Backpropagation: A Brief\n  Survey", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This tutorial paper surveys training alternatives to end-to-end\nbackpropagation (E2EBP) -- the de facto standard for training deep\narchitectures. Modular training refers to strictly local training without both\nthe forward and the backward pass, i.e., dividing a deep architecture into\nseveral nonoverlapping modules and training them separately without any\nend-to-end operation. Between the fully global E2EBP and the strictly local\nmodular training, there are \"weakly modular\" hybrids performing training\nwithout the backward pass only. These alternatives can match or surpass the\nperformance of E2EBP on challenging datasets such as ImageNet, and are gaining\nincreased attention primarily because they offer practical advantages over\nE2EBP, which will be enumerated herein. In particular, they allow for greater\nmodularity and transparency in deep learning workflows, aligning deep learning\nwith the mainstream computer science engineering that heavily exploits\nmodularization for scalability. Modular training has also revealed novel\ninsights about learning and has further implications on other important\nresearch domains. Specifically, it induces natural and effective solutions to\nsome important practical problems such as data efficiency and transferability\nestimation.\n", "versions": [{"version": "v1", "created": "Sat, 9 Jan 2021 19:56:22 GMT"}, {"version": "v2", "created": "Mon, 14 Jun 2021 03:36:00 GMT"}], "update_date": "2021-06-15", "authors_parsed": [["Duan", "Shiyu", ""], ["Principe", "Jose C.", ""]]}, {"id": "2101.03438", "submitter": "Junde Li", "authors": "Junde Li, Rasit Topaloglu, Swaroop Ghosh", "title": "Quantum Generative Models for Small Molecule Drug Discovery", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.ET cs.LG quant-ph", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Existing drug discovery pipelines take 5-10 years and cost billions of\ndollars. Computational approaches aim to sample from regions of the whole\nmolecular and solid-state compounds called chemical space which could be on the\norder of 1060 . Deep generative models can model the underlying probability\ndistribution of both the physical structures and property of drugs and relate\nthem nonlinearly. By exploiting patterns in massive datasets, these models can\ndistill salient features that characterize the molecules. Generative\nAdversarial Networks (GANs) discover drug candidates by generating molecular\nstructures that obey chemical and physical properties and show affinity towards\nbinding with the receptor for a target disease. However, classical GANs cannot\nexplore certain regions of the chemical space and suffer from\ncurse-of-dimensionality. A full quantum GAN may require more than 90 qubits\neven to generate QM9-like small molecules. We propose a qubit-efficient quantum\nGAN with a hybrid generator (QGAN-HG) to learn richer representation of\nmolecules via searching exponentially large chemical space with few qubits more\nefficiently than classical GAN. The QGANHG model is composed of a hybrid\nquantum generator that supports various number of qubits and quantum circuit\nlayers, and, a classical discriminator. QGAN-HG with only 14.93% retained\nparameters can learn molecular distribution as efficiently as classical\ncounterpart. The QGAN-HG variation with patched circuits considerably\naccelerates our standard QGANHG training process and avoids potential gradient\nvanishing issue of deep neural networks. Code is available on GitHub\nhttps://github.com/jundeli/quantum-gan.\n", "versions": [{"version": "v1", "created": "Sat, 9 Jan 2021 22:33:16 GMT"}], "update_date": "2021-01-12", "authors_parsed": [["Li", "Junde", ""], ["Topaloglu", "Rasit", ""], ["Ghosh", "Swaroop", ""]]}, {"id": "2101.03464", "submitter": "Yiding Yang", "authors": "Yiding Yang, Xinchao Wang, Mingli Song, Junsong Yuan, Dacheng Tao", "title": "SPAGAN: Shortest Path Graph Attention Network", "comments": "Accepted by IJCAI 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Graph convolutional networks (GCN) have recently demonstrated their potential\nin analyzing non-grid structure data that can be represented as graphs. The\ncore idea is to encode the local topology of a graph, via convolutions, into\nthe feature of a center node. In this paper, we propose a novel GCN model,\nwhich we term as Shortest Path Graph Attention Network (SPAGAN). Unlike\nconventional GCN models that carry out node-based attentions within each layer,\nthe proposed SPAGAN conducts path-based attention that explicitly accounts for\nthe influence of a sequence of nodes yielding the minimum cost, or shortest\npath, between the center node and its higher-order neighbors. SPAGAN therefore\nallows for a more informative and intact exploration of the graph structure and\nfurther {a} more effective aggregation of information from distant neighbors\ninto the center node, as compared to node-based GCN methods. We test SPAGAN on\nthe downstream classification task on several standard datasets, and achieve\nperformances superior to the state of the art. Code is publicly available at\nhttps://github.com/ihollywhy/SPAGAN.\n", "versions": [{"version": "v1", "created": "Sun, 10 Jan 2021 03:18:34 GMT"}], "update_date": "2021-01-12", "authors_parsed": [["Yang", "Yiding", ""], ["Wang", "Xinchao", ""], ["Song", "Mingli", ""], ["Yuan", "Junsong", ""], ["Tao", "Dacheng", ""]]}, {"id": "2101.03497", "submitter": "Weixin Wang", "authors": "Weixin Wang", "title": "Joint Prediction of Remaining Useful Life and Failure Type of Train\n  Wheelsets: A Multi-task Learning Approach", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.OC", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  The failures of train wheels account for disruptions of train operations and\neven a large portion of train derailments. Remaining useful life (RUL) of a\nwheelset measures the how soon the next failure will arrive, and the failure\ntype reveals how severe the failure will be. RUL prediction is a regression\ntask, whereas failure type is a classification task. In this paper, we propose\na multi-task learning approach to jointly accomplish these two tasks by using a\ncommon input space to achieve more desirable results. We develop a convex\noptimization formulation to integrate both least square loss and the negative\nmaximum likelihood of logistic regression, and model the joint sparsity as the\nL2/L1 norm of the model parameters to couple feature selection across tasks.\nThe experiment results show that our method outperforms the single task\nlearning method by 3% in prediction accuracy.\n", "versions": [{"version": "v1", "created": "Sun, 10 Jan 2021 08:20:21 GMT"}, {"version": "v2", "created": "Wed, 20 Jan 2021 03:41:49 GMT"}], "update_date": "2021-01-21", "authors_parsed": [["Wang", "Weixin", ""]]}, {"id": "2101.03498", "submitter": "Jun Zhao", "authors": "Quoc-Viet Pham, Thien Huynh-The, Mamoun Alazab, Jun Zhao, Won-Joo\n  Hwang", "title": "Sum-Rate Maximization for UAV-assisted Visible Light Communications\n  using NOMA: Swarm Intelligence meets Machine Learning", "comments": "Published in IEEE Internet of Things Journal (IoTJ) 2020", "journal-ref": null, "doi": "10.1109/JIOT.2020.2988930", "report-no": null, "categories": "cs.LG cs.SY eess.SY", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  As the integration of unmanned aerial vehicles (UAVs) into visible light\ncommunications (VLC) can offer many benefits for massive-connectivity\napplications and services in 5G and beyond, this work considers a UAV-assisted\nVLC using non-orthogonal multiple-access. More specifically, we formulate a\njoint problem of power allocation and UAV's placement to maximize the sum rate\nof all users, subject to constraints on power allocation, quality of service of\nusers, and UAV's position. Since the problem is non-convex and NP-hard in\ngeneral, it is difficult to be solved optimally. Moreover, the problem is not\neasy to be solved by conventional approaches, e.g., coordinate descent\nalgorithms, due to channel modeling in VLC. Therefore, we propose using harris\nhawks optimization (HHO) algorithm to solve the formulated problem and obtain\nan efficient solution. We then use the HHO algorithm together with artificial\nneural networks to propose a design which can be used in real-time applications\nand avoid falling into the \"local minima\" trap in conventional trainers.\nNumerical results are provided to verify the effectiveness of the proposed\nalgorithm and further demonstrate that the proposed algorithm/HHO trainer is\nsuperior to several alternative schemes and existing metaheuristic algorithms.\n", "versions": [{"version": "v1", "created": "Sun, 10 Jan 2021 08:21:49 GMT"}], "update_date": "2021-01-12", "authors_parsed": [["Pham", "Quoc-Viet", ""], ["Huynh-The", "Thien", ""], ["Alazab", "Mamoun", ""], ["Zhao", "Jun", ""], ["Hwang", "Won-Joo", ""]]}, {"id": "2101.03499", "submitter": "Adrian Prochaska", "authors": "Adrian Prochaska, Julien Pillas and Bernard B\\\"aker", "title": "Improved active output selection strategy for noisy environments", "comments": "This work has been submitted to IFAC for possible publication at\n  SysID 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The test bench time needed for model-based calibration can be reduced with\nactive learning methods for test design. This paper presents an improved\nstrategy for active output selection. This is the task of learning multiple\nmodels in the same input dimensions and suits the needs of calibration tasks.\nCompared to an existing strategy, we take into account the noise estimate,\nwhich is inherent to Gaussian processes. The method is validated on three\ndifferent toy examples. The performance compared to the existing best strategy\nis the same or better in each example. In a best case scenario, the new\nstrategy needs at least 10% less measurements compared to all other active or\npassive strategies. Further efforts will evaluate the strategy on a real-world\napplication. Moreover, the implementation of more sophisticated active-learning\nstrategies for the query placement will be realized.\n", "versions": [{"version": "v1", "created": "Sun, 10 Jan 2021 08:27:30 GMT"}], "update_date": "2021-01-12", "authors_parsed": [["Prochaska", "Adrian", ""], ["Pillas", "Julien", ""], ["B\u00e4ker", "Bernard", ""]]}, {"id": "2101.03501", "submitter": "Spencer Compton", "authors": "Spencer Compton, Murat Kocaoglu, Kristjan Greenewald, Dmitriy Katz", "title": "Entropic Causal Inference: Identifiability and Finite Sample Results", "comments": "In Proceedings of NeurIPS 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.IT cs.LG math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Entropic causal inference is a framework for inferring the causal direction\nbetween two categorical variables from observational data. The central\nassumption is that the amount of unobserved randomness in the system is not too\nlarge. This unobserved randomness is measured by the entropy of the exogenous\nvariable in the underlying structural causal model, which governs the causal\nrelation between the observed variables. Kocaoglu et al. conjectured that the\ncausal direction is identifiable when the entropy of the exogenous variable is\nnot too large. In this paper, we prove a variant of their conjecture. Namely,\nwe show that for almost all causal models where the exogenous variable has\nentropy that does not scale with the number of states of the observed\nvariables, the causal direction is identifiable from observational data. We\nalso consider the minimum entropy coupling-based algorithmic approach presented\nby Kocaoglu et al., and for the first time demonstrate algorithmic\nidentifiability guarantees using a finite number of samples. We conduct\nextensive experiments to evaluate the robustness of the method to relaxing some\nof the assumptions in our theory and demonstrate that both the constant-entropy\nexogenous variable and the no latent confounder assumptions can be relaxed in\npractice. We also empirically characterize the number of observational samples\nneeded for causal identification. Finally, we apply the algorithm on Tuebingen\ncause-effect pairs dataset.\n", "versions": [{"version": "v1", "created": "Sun, 10 Jan 2021 08:37:54 GMT"}], "update_date": "2021-01-12", "authors_parsed": [["Compton", "Spencer", ""], ["Kocaoglu", "Murat", ""], ["Greenewald", "Kristjan", ""], ["Katz", "Dmitriy", ""]]}, {"id": "2101.03525", "submitter": "Hsueh-Cheng Wang", "authors": "Jui-Te Huang, Chen-Lung Lu, Po-Kai Chang, Ching-I Huang, Chao-Chun\n  Hsu, Zu Lin Ewe, Po-Jui Huang and Hsueh-Cheng Wang", "title": "Cross-Modal Contrastive Learning of Representations for Navigation using\n  Lightweight, Low-Cost Millimeter Wave Radar for Adverse Environmental\n  Conditions", "comments": "For further details, please visit\n  https://arg-nctu.github.io/projects/deeprl-mmWave.html", "journal-ref": "IEEE Robotics and Automation Letters, 2021", "doi": "10.1109/LRA.2021.3062011", "report-no": null, "categories": "cs.RO cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Deep reinforcement learning (RL), where the agent learns from mistakes, has\nbeen successfully applied to a variety of tasks. With the aim of learning\ncollision-free policies for unmanned vehicles, deep RL has been used for\ntraining with various types of data, such as colored images, depth images, and\nLiDAR point clouds, without the use of classic map--localize--plan approaches.\nHowever, existing methods are limited by their reliance on cameras and LiDAR\ndevices, which have degraded sensing under adverse environmental conditions\n(e.g., smoky environments). In response, we propose the use of single-chip\nmillimeter-wave (mmWave) radar, which is lightweight and inexpensive, for\nlearning-based autonomous navigation. However, because mmWave radar signals are\noften noisy and sparse, we propose a cross-modal contrastive learning for\nrepresentation (CM-CLR) method that maximizes the agreement between mmWave\nradar data and LiDAR data in the training stage. We evaluated our method in\nreal-world robot compared with 1) a method with two separate networks using\ncross-modal generative reconstruction and an RL policy and 2) a baseline RL\npolicy without cross-modal representation. Our proposed end-to-end deep RL\npolicy with contrastive learning successfully navigated the robot through\nsmoke-filled maze environments and achieved better performance compared with\ngenerative reconstruction methods, in which noisy artifact walls or obstacles\nwere produced. All pretrained models and hardware settings are open access for\nreproducing this study and can be obtained at\nhttps://arg-nctu.github.io/projects/deeprl-mmWave.html\n", "versions": [{"version": "v1", "created": "Sun, 10 Jan 2021 11:21:17 GMT"}], "update_date": "2021-03-02", "authors_parsed": [["Huang", "Jui-Te", ""], ["Lu", "Chen-Lung", ""], ["Chang", "Po-Kai", ""], ["Huang", "Ching-I", ""], ["Hsu", "Chao-Chun", ""], ["Ewe", "Zu Lin", ""], ["Huang", "Po-Jui", ""], ["Wang", "Hsueh-Cheng", ""]]}, {"id": "2101.03541", "submitter": "Fabian Amherd", "authors": "Fabian Amherd, Elias Rodriguez", "title": "Heatmap-based Object Detection and Tracking with a Fully Convolutional\n  Neural Network", "comments": "30 pages, 29 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  The main topic of this paper is a brief overview of the field of Artificial\nIntelligence. The core of this paper is a practical implementation of an\nalgorithm for object detection and tracking. The ability to detect and track\nfast-moving objects is crucial for various applications of Artificial\nIntelligence like autonomous driving, ball tracking in sports, robotics or\nobject counting. As part of this paper the Fully Convolutional Neural Network\n\"CueNet\" was developed. It detects and tracks the cueball on a labyrinth game\nrobustly and reliably. While CueNet V1 has a single input image, the approach\nwith CueNet V2 was to take three consecutive 240 x 180-pixel images as an input\nand transform them into a probability heatmap for the cueball's location. The\nnetwork was tested with a separate video that contained all sorts of\ndistractions to test its robustness. When confronted with our testing data,\nCueNet V1 predicted the correct cueball location in 99.6% of all frames, while\nCueNet V2 had 99.8% accuracy.\n", "versions": [{"version": "v1", "created": "Sun, 10 Jan 2021 13:13:38 GMT"}], "update_date": "2021-01-12", "authors_parsed": [["Amherd", "Fabian", ""], ["Rodriguez", "Elias", ""]]}, {"id": "2101.03545", "submitter": "Saikat Dutta", "authors": "Sourya Dipta Das, Ayan Basak and Saikat Dutta", "title": "A Heuristic-driven Ensemble Framework for COVID-19 Fake News Detection", "comments": "Accepted to CONSTRAINT Workshop, AAAI'21", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The significance of social media has increased manifold in the past few\ndecades as it helps people from even the most remote corners of the world stay\nconnected. With the COVID-19 pandemic raging, social media has become more\nrelevant and widely used than ever before, and along with this, there has been\na resurgence in the circulation of fake news and tweets that demand immediate\nattention. In this paper, we describe our Fake News Detection system that\nautomatically identifies whether a tweet related to COVID-19 is \"real\" or\n\"fake\", as a part of CONSTRAINT COVID19 Fake News Detection in English\nchallenge. We have used an ensemble model consisting of pre-trained models that\nhas helped us achieve a joint 8th position on the leader board. We have\nachieved an F1-score of 0.9831 against a top score of 0.9869. Post completion\nof the competition, we have been able to drastically improve our system by\nincorporating a novel heuristic algorithm based on username handles and link\ndomains in tweets fetching an F1-score of 0.9883 and achieving state-of-the art\nresults on the given dataset.\n", "versions": [{"version": "v1", "created": "Sun, 10 Jan 2021 13:21:08 GMT"}], "update_date": "2021-01-12", "authors_parsed": [["Das", "Sourya Dipta", ""], ["Basak", "Ayan", ""], ["Dutta", "Saikat", ""]]}, {"id": "2101.03549", "submitter": "Koby Bibas", "authors": "Koby Bibas, Gili Weiss-Dicker, Dana Cohen, Noa Cahan, Hayit Greenspan", "title": "Learning Rotation Invariant Features for Cryogenic Electron Microscopy\n  Image Reconstruction", "comments": "Accepted IEEE-ISBI 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Cryo-Electron Microscopy (Cryo-EM) is a Nobel prize-winning technology for\ndetermining the 3D structure of particles at near-atomic resolution. A\nfundamental step in the recovering of the 3D single-particle structure is to\nalign its 2D projections; thus, the construction of a canonical representation\nwith a fixed rotation angle is required. Most approaches use discrete\nclustering which fails to capture the continuous nature of image rotation,\nothers suffer from low-quality image reconstruction. We propose a novel method\nthat leverages the recent development in the generative adversarial networks.\nWe introduce an encoder-decoder with a rotation angle classifier. In addition,\nwe utilize a discriminator on the decoder output to minimize the reconstruction\nerror. We demonstrate our approach with the Cryo-EM 5HDB and the rotated MNIST\ndatasets showing substantial improvement over recent methods.\n", "versions": [{"version": "v1", "created": "Sun, 10 Jan 2021 13:38:16 GMT"}], "update_date": "2021-01-12", "authors_parsed": [["Bibas", "Koby", ""], ["Weiss-Dicker", "Gili", ""], ["Cohen", "Dana", ""], ["Cahan", "Noa", ""], ["Greenspan", "Hayit", ""]]}, {"id": "2101.03552", "submitter": "Andreas Kirsch", "authors": "Andreas Kirsch, Yarin Gal", "title": "PowerEvaluationBALD: Efficient Evaluation-Oriented Deep (Bayesian)\n  Active Learning with Stochastic Acquisition Functions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop BatchEvaluationBALD, a new acquisition function for deep Bayesian\nactive learning, as an expansion of BatchBALD that takes into account an\nevaluation set of unlabeled data, for example, the pool set. We also develop a\nvariant for the non-Bayesian setting, which we call Evaluation Information\nGain. To reduce computational requirements and allow these methods to scale to\nlarger acquisition batch sizes, we introduce stochastic acquisition functions\nthat use importance sampling of tempered acquisition scores. We call this\nmethod PowerEvaluationBALD. We show in a few initial experiments that\nPowerEvaluationBALD works on par with BatchEvaluationBALD, which outperforms\nBatchBALD on Repeated MNIST (MNISTx2), while massively reducing the\ncomputational requirements compared to BatchBALD or BatchEvaluationBALD.\n", "versions": [{"version": "v1", "created": "Sun, 10 Jan 2021 13:46:45 GMT"}, {"version": "v2", "created": "Mon, 10 May 2021 19:27:20 GMT"}], "update_date": "2021-05-12", "authors_parsed": [["Kirsch", "Andreas", ""], ["Gal", "Yarin", ""]]}, {"id": "2101.03553", "submitter": "Sayar Ghosh Roy", "authors": "Sayar Ghosh Roy, Nikhil Pinnaparaju, Risubh Jain, Manish Gupta,\n  Vasudeva Varma", "title": "Summaformers @ LaySumm 20, LongSumm 20", "comments": "Proceedings of the First Workshop on Scholarly Document Processing\n  (SDP) at EMNLP 2020", "journal-ref": "In Proceedings of the First Workshop on Scholarly Document\n  Processing, pages 336 - 343, 2020, Online. Association for Computational\n  Linguistics", "doi": "10.18653/v1/2020.sdp-1.39", "report-no": "IIIT/TR/2020/75", "categories": "cs.CL cs.AI cs.IR cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Automatic text summarization has been widely studied as an important task in\nnatural language processing. Traditionally, various feature engineering and\nmachine learning based systems have been proposed for extractive as well as\nabstractive text summarization. Recently, deep learning based, specifically\nTransformer-based systems have been immensely popular. Summarization is a\ncognitively challenging task - extracting summary worthy sentences is\nlaborious, and expressing semantics in brief when doing abstractive\nsummarization is complicated. In this paper, we specifically look at the\nproblem of summarizing scientific research papers from multiple domains. We\ndifferentiate between two types of summaries, namely, (a) LaySumm: A very short\nsummary that captures the essence of the research paper in layman terms\nrestricting overtly specific technical jargon and (b) LongSumm: A much longer\ndetailed summary aimed at providing specific insights into various ideas\ntouched upon in the paper. While leveraging latest Transformer-based models,\nour systems are simple, intuitive and based on how specific paper sections\ncontribute to human summaries of the two types described above. Evaluations\nagainst gold standard summaries using ROUGE metrics prove the effectiveness of\nour approach. On blind test corpora, our system ranks first and third for the\nLongSumm and LaySumm tasks respectively.\n", "versions": [{"version": "v1", "created": "Sun, 10 Jan 2021 13:48:12 GMT"}], "update_date": "2021-01-12", "authors_parsed": [["Roy", "Sayar Ghosh", ""], ["Pinnaparaju", "Nikhil", ""], ["Jain", "Risubh", ""], ["Gupta", "Manish", ""], ["Varma", "Vasudeva", ""]]}, {"id": "2101.03558", "submitter": "Manjish Pal", "authors": "Manjish Pal. Subham Pokhriyal", "title": "Learning from Satisfying Assignments Using Risk Minimization", "comments": "Accepted for Publication at 28th FRUCT 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper we consider the problem of Learning from Satisfying Assignments\nintroduced by \\cite{1} of finding a distribution that is a close approximation\nto the uniform distribution over the satisfying assignments of a low complexity\nBoolean function $f$. In a later work \\cite{2} consider the same problem but\nwith the knowledge of some continuous distribution $D$ and the objective being\nto estimate $D_f$, which is $D$ restricted to the satisfying assignments of an\nunknown Boolean function $f$. We consider these problems from the point of view\nof parameter estimation techniques in statistical machine learning and prove\nsimilar results that are based on standard optimization algorithms for Risk\nMinimization.\n", "versions": [{"version": "v1", "created": "Sun, 10 Jan 2021 14:10:03 GMT"}], "update_date": "2021-01-12", "authors_parsed": [["Pokhriyal", "Manjish Pal. Subham", ""]]}, {"id": "2101.03581", "submitter": "Zheming Zuo", "authors": "Zheming Zuo, Jie Li, Noura Al Moubayed", "title": "Curvature-based Feature Selection with Application in Classifying\n  Electronic Health Records", "comments": "12 pages, 5 figures, 3 tables, 4 data sets, source code available", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Electronic Health Records (EHRs) are widely applied in healthcare facilities\nnowadays. Due to the inherent heterogeneity, unbalanced, incompleteness, and\nhigh-dimensional nature of EHRs, it is a challenging task to employ machine\nlearning algorithms to analyse such EHRs for prediction and diagnostics within\nthe scope of precision medicine. Dimensionality reduction is an efficient data\npreprocessing technique for the analysis of high dimensional data that reduces\nthe number of features while improving the performance of the data analysis,\ne.g. classification. In this paper, we propose an efficient curvature-based\nfeature selection method for supporting more precise diagnosis. The proposed\nmethod is a filter-based feature selection method, which directly utilises the\nMenger Curvature for ranking all the attributes in the given data set. We\nevaluate the performance of our method against conventional PCA and recent ones\nincluding BPCM, GSAM, WCNN, BLS II, VIBES, 2L-MJFA, RFGA, and VAF. Our method\nachieves state-of-the-art performance on four benchmark healthcare data sets\nincluding CCRFDS, BCCDS, BTDS, and DRDDS with impressive 24.73% and 13.93%\nimprovements respectively on BTDS and CCRFDS, 7.97% improvement on BCCDS, and\n3.63% improvement on DRDDS. Our CFS source code is publicly available at\nhttps://github.com/zhemingzuo/CFS.\n", "versions": [{"version": "v1", "created": "Sun, 10 Jan 2021 16:55:40 GMT"}], "update_date": "2021-01-12", "authors_parsed": [["Zuo", "Zheming", ""], ["Li", "Jie", ""], ["Moubayed", "Noura Al", ""]]}, {"id": "2101.03583", "submitter": "Changxin Qiu", "authors": "Changxin Qiu, Aaron Bendickson, Joshua Kalyanapu and Jue Yan", "title": "Accuracy and Architecture Studies of Residual Neural Network solving\n  Ordinary Differential Equations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.NA cs.LG cs.NA math.DS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper we consider utilizing a residual neural network (ResNet) to\nsolve ordinary differential equations. Stochastic gradient descent method is\napplied to obtain the optimal parameter set of weights and biases of the\nnetwork. We apply forward Euler, Runge-Kutta2 and Runge-Kutta4 finite\ndifference methods to generate three sets of targets training the ResNet and\ncarry out the target study. The well trained ResNet behaves just as its\ncounterpart of the corresponding one-step finite difference method. In\nparticular, we carry out (1) the architecture study in terms of number of\nhidden layers and neurons per layer to find the optimal ResNet structure; (2)\nthe target study to verify the ResNet solver behaves as accurate as its finite\ndifference method counterpart; (3) solution trajectory simulation. Even the\nResNet solver looks like and is implemented in a way similar to forward Euler\nscheme, its accuracy can be as high as any one step method. A sequence of\nnumerical examples are presented to demonstrate the performance of the ResNet\nsolver.\n", "versions": [{"version": "v1", "created": "Sun, 10 Jan 2021 17:34:10 GMT"}], "update_date": "2021-01-12", "authors_parsed": [["Qiu", "Changxin", ""], ["Bendickson", "Aaron", ""], ["Kalyanapu", "Joshua", ""], ["Yan", "Jue", ""]]}, {"id": "2101.03606", "submitter": "Wessel Bruinsma", "authors": "Wessel P. Bruinsma and James Requeima and Andrew Y. K. Foong and\n  Jonathan Gordon and Richard E. Turner", "title": "The Gaussian Neural Process", "comments": "34 pages; includes supplementary material; to appear in AABI 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural Processes (NPs; Garnelo et al., 2018a,b) are a rich class of models\nfor meta-learning that map data sets directly to predictive stochastic\nprocesses. We provide a rigorous analysis of the standard maximum-likelihood\nobjective used to train conditional NPs. Moreover, we propose a new member to\nthe Neural Process family called the Gaussian Neural Process (GNP), which\nmodels predictive correlations, incorporates translation equivariance, provides\nuniversal approximation guarantees, and demonstrates encouraging performance.\n", "versions": [{"version": "v1", "created": "Sun, 10 Jan 2021 19:15:27 GMT"}], "update_date": "2021-01-12", "authors_parsed": [["Bruinsma", "Wessel P.", ""], ["Requeima", "James", ""], ["Foong", "Andrew Y. K.", ""], ["Gordon", "Jonathan", ""], ["Turner", "Richard E.", ""]]}, {"id": "2101.03613", "submitter": "Ekram Hossain", "authors": "F. Hussain, R. Hussain, and E. Hossain", "title": "Explainable Artificial Intelligence (XAI): An Engineering Perspective", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  The remarkable advancements in Deep Learning (DL) algorithms have fueled\nenthusiasm for using Artificial Intelligence (AI) technologies in almost every\ndomain; however, the opaqueness of these algorithms put a question mark on\ntheir applications in safety-critical systems. In this regard, the\n`explainability' dimension is not only essential to both explain the inner\nworkings of black-box algorithms, but it also adds accountability and\ntransparency dimensions that are of prime importance for regulators, consumers,\nand service providers. eXplainable Artificial Intelligence (XAI) is the set of\ntechniques and methods to convert the so-called black-box AI algorithms to\nwhite-box algorithms, where the results achieved by these algorithms and the\nvariables, parameters, and steps taken by the algorithm to reach the obtained\nresults, are transparent and explainable. To complement the existing literature\non XAI, in this paper, we take an `engineering' approach to illustrate the\nconcepts of XAI. We discuss the stakeholders in XAI and describe the\nmathematical contours of XAI from engineering perspective. Then we take the\nautonomous car as a use-case and discuss the applications of XAI for its\ndifferent components such as object detection, perception, control, action\ndecision, and so on. This work is an exploratory study to identify new avenues\nof research in the field of XAI.\n", "versions": [{"version": "v1", "created": "Sun, 10 Jan 2021 19:49:12 GMT"}], "update_date": "2021-01-12", "authors_parsed": [["Hussain", "F.", ""], ["Hussain", "R.", ""], ["Hossain", "E.", ""]]}, {"id": "2101.03616", "submitter": "Mohammadhossein Toutiaee", "authors": "Mohammadhossein Toutiaee", "title": "Occupancy Detection in Room Using Sensor Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the advent of Internet of Thing (IoT), and ubiquitous data collected\nevery moment by either portable (smart phone) or fixed (sensor) devices, it is\nimportant to gain insights and meaningful information from the sensor data in\ncontext-aware computing environments. Many researches have been implemented by\nscientists in different fields, to analyze such data for the purpose of\nsecurity, energy efficiency, building reliability and smart environments. One\nstudy, that many researchers are interested in, is to utilize Machine Learning\ntechniques for occupancy detection where the aforementioned sensors gather\ninformation about the environment. This paper provides a solution to detect\noccupancy using sensor data by using and testing several variables.\nAdditionally we show the analysis performed over the gathered data using\nMachine Learning and pattern recognition mechanisms is possible to determine\nthe occupancy of indoor environments. Seven famous algorithms in Machine\nLearning, namely as Decision Tree, Random Forest, Gradient Boosting Machine,\nLogistic Regression, Naive Bayes, Kernelized SVM and K-Nearest Neighbors are\ntested and compared in this study.\n", "versions": [{"version": "v1", "created": "Sun, 10 Jan 2021 19:53:57 GMT"}], "update_date": "2021-01-12", "authors_parsed": [["Toutiaee", "Mohammadhossein", ""]]}, {"id": "2101.03627", "submitter": "Jie Xu", "authors": "Jie Xu, Heqiang Wang, Lixing Chen", "title": "Bandwidth Allocation for Multiple Federated Learning Services in\n  Wireless Edge Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper studies a federated learning (FL) system, where \\textit{multiple}\nFL services co-exist in a wireless network and share common wireless resources.\nIt fills the void of wireless resource allocation for multiple simultaneous FL\nservices in the existing literature. Our method designs a two-level resource\nallocation framework comprising \\emph{intra-service} resource allocation and\n\\emph{inter-service} resource allocation. The intra-service resource allocation\nproblem aims to minimize the length of FL rounds by optimizing the bandwidth\nallocation among the clients of each FL service. Based on this, an\ninter-service resource allocation problem is further considered, which\ndistributes bandwidth resources among multiple simultaneous FL services. We\nconsider both cooperative and selfish providers of the FL services. For\ncooperative FL service providers, we design a distributed bandwidth allocation\nalgorithm to optimize the overall performance of multiple FL services,\nmeanwhile cater to the fairness among FL services and the privacy of clients.\nFor selfish FL service providers, a new auction scheme is designed with the FL\nservice owners as the bidders and the network provider as the auctioneer. The\ndesigned auction scheme strikes a balance between the overall FL performance\nand fairness. Our simulation results show that the proposed algorithms\noutperform other benchmarks under various network conditions.\n", "versions": [{"version": "v1", "created": "Sun, 10 Jan 2021 20:57:13 GMT"}], "update_date": "2021-01-12", "authors_parsed": [["Xu", "Jie", ""], ["Wang", "Heqiang", ""], ["Chen", "Lixing", ""]]}, {"id": "2101.03641", "submitter": "Jian Li", "authors": "Guojun Xiong, Rahul Singh, Jian Li", "title": "Learning Augmented Index Policy for Optimal Service Placement at the\n  Network Edge", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of service placement at the network edge, in which a\ndecision maker has to choose between $N$ services to host at the edge to\nsatisfy the demands of customers. Our goal is to design adaptive algorithms to\nminimize the average service delivery latency for customers. We pose the\nproblem as a Markov decision process (MDP) in which the system state is given\nby describing, for each service, the number of customers that are currently\nwaiting at the edge to obtain the service. However, solving this $N$-services\nMDP is computationally expensive due to the curse of dimensionality. To\novercome this challenge, we show that the optimal policy for a single-service\nMDP has an appealing threshold structure, and derive explicitly the Whittle\nindices for each service as a function of the number of requests from customers\nbased on the theory of Whittle index policy.\n  Since request arrival and service delivery rates are usually unknown and\npossibly time-varying, we then develop efficient learning augmented algorithms\nthat fully utilize the structure of optimal policies with a low learning\nregret. The first of these is UCB-Whittle, and relies upon the principle of\noptimism in the face of uncertainty. The second algorithm, Q-learning-Whittle,\nutilizes Q-learning iterations for each service by using a two time scale\nstochastic approximation. We characterize the non-asymptotic performance of\nUCB-Whittle by analyzing its learning regret, and also analyze the convergence\nproperties of Q-learning-Whittle. Simulation results show that the proposed\npolicies yield excellent empirical performance.\n", "versions": [{"version": "v1", "created": "Sun, 10 Jan 2021 23:54:59 GMT"}, {"version": "v2", "created": "Thu, 14 Jan 2021 04:01:37 GMT"}], "update_date": "2021-01-15", "authors_parsed": [["Xiong", "Guojun", ""], ["Singh", "Rahul", ""], ["Li", "Jian", ""]]}, {"id": "2101.03655", "submitter": "MohammadNoor Injadat", "authors": "MohammadNoor Injadat, Abdallah Moubayed, Ali Bou Nassif, Abdallah\n  Shami", "title": "Machine Learning Towards Intelligent Systems: Applications, Challenges,\n  and Opportunities", "comments": "46 pages, 7 figures, 5 tables, journal", "journal-ref": null, "doi": "10.1007/s10462-020-09948-w", "report-no": null, "categories": "cs.LG cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The emergence and continued reliance on the Internet and related technologies\nhas resulted in the generation of large amounts of data that can be made\navailable for analyses. However, humans do not possess the cognitive\ncapabilities to understand such large amounts of data. Machine learning (ML)\nprovides a mechanism for humans to process large amounts of data, gain insights\nabout the behavior of the data, and make more informed decision based on the\nresulting analysis. ML has applications in various fields. This review focuses\non some of the fields and applications such as education, healthcare, network\nsecurity, banking and finance, and social media. Within these fields, there are\nmultiple unique challenges that exist. However, ML can provide solutions to\nthese challenges, as well as create further research opportunities.\nAccordingly, this work surveys some of the challenges facing the aforementioned\nfields and presents some of the previous literature works that tackled them.\nMoreover, it suggests several research opportunities that benefit from the use\nof ML to address these challenges.\n", "versions": [{"version": "v1", "created": "Mon, 11 Jan 2021 01:32:15 GMT"}], "update_date": "2021-01-12", "authors_parsed": [["Injadat", "MohammadNoor", ""], ["Moubayed", "Abdallah", ""], ["Nassif", "Ali Bou", ""], ["Shami", "Abdallah", ""]]}, {"id": "2101.03678", "submitter": "Yan Qin", "authors": "Xuewen Zhang, Yan Qin, Chau Yuen (Fellow IEEE), Lahiru Jayasinghe, and\n  Xiang Liu", "title": "Time-Series Regeneration with Convolutional Recurrent Generative\n  Adversarial Network for Remaining Useful Life Estimation", "comments": null, "journal-ref": "This paper has been accetped by IEEE Transactions on Industrial\n  Informatics in Dec. 2020", "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  For health prognostic task, ever-increasing efforts have been focused on\nmachine learning-based methods, which are capable of yielding accurate\nremaining useful life (RUL) estimation for industrial equipment or components\nwithout exploring the degradation mechanism. A prerequisite ensuring the\nsuccess of these methods depends on a wealth of run-to-failure data, however,\nrun-to-failure data may be insufficient in practice. That is, conducting a\nsubstantial amount of destructive experiments not only is high costs, but also\nmay cause catastrophic consequences. Out of this consideration, an enhanced RUL\nframework focusing on data self-generation is put forward for both non-cyclic\nand cyclic degradation patterns for the first time. It is designed to enrich\ndata from a data-driven way, generating realistic-like time-series to enhance\ncurrent RUL methods. First, high-quality data generation is ensured through the\nproposed convolutional recurrent generative adversarial network (CR-GAN), which\nadopts a two-channel fusion convolutional recurrent neural network. Next, a\nhierarchical framework is proposed to combine generated data into current RUL\nestimation methods. Finally, the efficacy of the proposed method is verified\nthrough both non-cyclic and cyclic degradation systems. With the enhanced RUL\nframework, an aero-engine system following non-cyclic degradation has been\ntested using three typical RUL models. State-of-art RUL estimation results are\nachieved by enhancing capsule network with generated time-series. Specifically,\nestimation errors evaluated by the index score function have been reduced by\n21.77%, and 32.67% for the two employed operating conditions, respectively.\nBesides, the estimation error is reduced to zero for the Lithium-ion battery\nsystem, which presents cyclic degradation.\n", "versions": [{"version": "v1", "created": "Mon, 11 Jan 2021 02:44:34 GMT"}], "update_date": "2021-01-13", "authors_parsed": [["Zhang", "Xuewen", "", "Fellow IEEE"], ["Qin", "Yan", "", "Fellow IEEE"], ["Yuen", "Chau", "", "Fellow IEEE"], ["Jayasinghe", "Lahiru", ""], ["Liu", "Xiang", ""]]}, {"id": "2101.03690", "submitter": "Limon Barua", "authors": "Limon Barua, Bo Zou, Yan (Joann) Zhou, Yulin Liu", "title": "Modeling Household Online Shopping Demand in the U.S.: A Machine\n  Learning Approach and Comparative Investigation between 2009 and 2017", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite the rapid growth of online shopping and research interest in the\nrelationship between online and in-store shopping, national-level modeling and\ninvestigation of the demand for online shopping with a prediction focus remain\nlimited in the literature. This paper differs from prior work and leverages two\nrecent releases of the U.S. National Household Travel Survey (NHTS) data for\n2009 and 2017 to develop machine learning (ML) models, specifically gradient\nboosting machine (GBM), for predicting household-level online shopping\npurchases. The NHTS data allow for not only conducting nationwide investigation\nbut also at the level of households, which is more appropriate than at the\nindividual level given the connected consumption and shopping needs of members\nin a household. We follow a systematic procedure for model development\nincluding employing Recursive Feature Elimination algorithm to select input\nvariables (features) in order to reduce the risk of model overfitting and\nincrease model explainability. Extensive post-modeling investigation is\nconducted in a comparative manner between 2009 and 2017, including quantifying\nthe importance of each input variable in predicting online shopping demand, and\ncharacterizing value-dependent relationships between demand and the input\nvariables. In doing so, two latest advances in machine learning techniques,\nnamely Shapley value-based feature importance and Accumulated Local Effects\nplots, are adopted to overcome inherent drawbacks of the popular techniques in\ncurrent ML modeling. The modeling and investigation are performed both at the\nnational level and for three of the largest cities (New York, Los Angeles, and\nHouston). The models developed and insights gained can be used for online\nshopping-related freight demand generation and may also be considered for\nevaluating the potential impact of relevant policies on online shopping demand.\n", "versions": [{"version": "v1", "created": "Mon, 11 Jan 2021 03:45:53 GMT"}], "update_date": "2021-01-12", "authors_parsed": [["Barua", "Limon", "", "Joann"], ["Zou", "Bo", "", "Joann"], ["Yan", "", "", "Joann"], ["Zhou", "", ""], ["Liu", "Yulin", ""]]}, {"id": "2101.03697", "submitter": "Xiaohan Ding", "authors": "Xiaohan Ding, Xiangyu Zhang, Ningning Ma, Jungong Han, Guiguang Ding,\n  Jian Sun", "title": "RepVGG: Making VGG-style ConvNets Great Again", "comments": "CVPR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a simple but powerful architecture of convolutional neural\nnetwork, which has a VGG-like inference-time body composed of nothing but a\nstack of 3x3 convolution and ReLU, while the training-time model has a\nmulti-branch topology. Such decoupling of the training-time and inference-time\narchitecture is realized by a structural re-parameterization technique so that\nthe model is named RepVGG. On ImageNet, RepVGG reaches over 80% top-1 accuracy,\nwhich is the first time for a plain model, to the best of our knowledge. On\nNVIDIA 1080Ti GPU, RepVGG models run 83% faster than ResNet-50 or 101% faster\nthan ResNet-101 with higher accuracy and show favorable accuracy-speed\ntrade-off compared to the state-of-the-art models like EfficientNet and RegNet.\nThe code and trained models are available at\nhttps://github.com/megvii-model/RepVGG.\n", "versions": [{"version": "v1", "created": "Mon, 11 Jan 2021 04:46:11 GMT"}, {"version": "v2", "created": "Fri, 26 Mar 2021 15:02:08 GMT"}, {"version": "v3", "created": "Mon, 29 Mar 2021 13:02:36 GMT"}], "update_date": "2021-03-30", "authors_parsed": [["Ding", "Xiaohan", ""], ["Zhang", "Xiangyu", ""], ["Ma", "Ningning", ""], ["Han", "Jungong", ""], ["Ding", "Guiguang", ""], ["Sun", "Jian", ""]]}, {"id": "2101.03701", "submitter": "Zhiming Zhang", "authors": "Zhiming Zhang, Jin Yan, Liangding Li, Hong Pan, and Chuanzhi Dong", "title": "Condition Assessment of Stay Cables through Enhanced Time Series\n  Classification Using a Deep Learning Approach", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  This study proposes a data-driven method that detects cable damage from\nmeasured cable forces by recognizing biased patterns from the intact\nconditions. The proposed method solves the pattern recognition problem for\ncable damage detection through time series classification (TSC) in deep\nlearning, considering that the cable's behavior can be implicitly represented\nby the measured cable force series. A deep learning model, long short term\nmemory fully convolutional network (LSTM-FCN), is leveraged by assigning\nappropriate inputs and representative class labels for the TSC problem, First,\na TSC classifier is trained and validated using the data collected under intact\nconditions of stay cables, setting the segmented data series as input and the\ncable (or cable pair) ID as class labels. Subsequently, the classifier is\ntested using the data collected under possible damaged conditions. Finally, the\ncable or cable pair corresponding to the least classification accuracy is\nrecommended as the most probable damaged cable or cable pair. The proposed\nmethod was tested on an in-service cable-stayed bridge with damaged stay\ncables. Two scenarios in the proposed TSC scheme were investigated: 1) raw time\nseries of cable forces were fed into the classifiers; and 2) cable force ratios\nwere inputted in the classifiers considering the possible variation of force\ndistribution between cable pairs due to cable damage. Combining the results of\nTSC testing in these two scenarios, the cable with rupture was correctly\nidentified. This study proposes a data-driven methodology for cable damage\ndetection that requires the least data preprocessing and feature engineering,\nwhich enables fast and convenient early detection in real applications.\n", "versions": [{"version": "v1", "created": "Mon, 11 Jan 2021 05:08:19 GMT"}], "update_date": "2021-01-12", "authors_parsed": [["Zhang", "Zhiming", ""], ["Yan", "Jin", ""], ["Li", "Liangding", ""], ["Pan", "Hong", ""], ["Dong", "Chuanzhi", ""]]}, {"id": "2101.03704", "submitter": "Yan Qin", "authors": "Yan Qin, Stefan Adams, and Chau Yuen", "title": "A Transfer Learning-based State of Charge Estimation for Lithium-Ion\n  Battery at Varying Ambient Temperatures", "comments": "This paper has been accepted by IEEE Transaction on Industrial\n  Informatics on Jan. 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Accurate and reliable state of charge (SoC) estimation becomes increasingly\nimportant to provide a stable and efficient environment for Lithium-ion\nbatteries (LiBs) powered devices. Most data-driven SoC models are built for a\nfixed ambient temperature, which neglect the high sensitivity of LiBs to\ntemperature and may cause severe prediction errors. Nevertheless, a systematic\nevaluation of the impact of temperature on SoC estimation and ways for a prompt\nadjustment of the estimation model to new temperatures using limited data have\nbeen hardly discussed. To solve these challenges, a novel SoC estimation method\nis proposed by exploiting temporal dynamics of measurements and transferring\nconsistent estimation ability among different temperatures. First, temporal\ndynamics, which is presented by correlations between the past fluctuation and\nthe future motion, is extracted using canonical variate analysis. Next, two\nmodels, including a reference SoC estimation model and an estimation ability\nmonitoring model, are developed with temporal dynamics. The monitoring model\nprovides a path to quantitatively evaluate the influences of temperature on SoC\nestimation ability. After that, once the inability of the reference SoC\nestimation model is detected, consistent temporal dynamics between temperatures\nare selected for transfer learning. Finally, the efficacy of the proposed\nmethod is verified through a benchmark. Our proposed method not only reduces\nprediction errors at fixed temperatures (e.g., reduced by 24.35% at -20{\\deg}C,\n49.82% at 25{\\deg}C) but also improves prediction accuracies at new\ntemperatures.\n", "versions": [{"version": "v1", "created": "Mon, 11 Jan 2021 05:26:37 GMT"}], "update_date": "2021-01-12", "authors_parsed": [["Qin", "Yan", ""], ["Adams", "Stefan", ""], ["Yuen", "Chau", ""]]}, {"id": "2101.03705", "submitter": "Ahmed Imteaj", "authors": "Ahmed Imteaj and M. Hadi Amini", "title": "FedAR: Activity and Resource-Aware Federated Learning Model for\n  Distributed Mobile Robots", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Smartphones, autonomous vehicles, and the Internet-of-things (IoT) devices\nare considered the primary data source for a distributed network. Due to a\nrevolutionary breakthrough in internet availability and continuous improvement\nof the IoT devices capabilities, it is desirable to store data locally and\nperform computation at the edge, as opposed to share all local information with\na centralized computation agent. A recently proposed Machine Learning (ML)\nalgorithm called Federated Learning (FL) paves the path towards preserving data\nprivacy, performing distributed learning, and reducing communication overhead\nin large-scale machine learning (ML) problems. This paper proposes an FL model\nby monitoring client activities and leveraging available local computing\nresources, particularly for resource-constrained IoT devices (e.g., mobile\nrobots), to accelerate the learning process. We assign a trust score to each FL\nclient, which is updated based on the client's activities. We consider a\ndistributed mobile robot as an FL client with resource limitations either in\nmemory, bandwidth, processor, or battery life. We consider such mobile robots\nas FL clients to understand their resource-constrained behavior in a real-world\nsetting. We consider an FL client to be untrustworthy if the client infuses\nincorrect models or repeatedly gives slow responses during the FL process.\nAfter disregarding the ineffective and unreliable client, we perform local\ntraining on the selected FL clients. To further reduce the straggler issue, we\nenable an asynchronous FL mechanism by performing aggregation on the FL server\nwithout waiting for a long period to receive a particular client's response.\n", "versions": [{"version": "v1", "created": "Mon, 11 Jan 2021 05:27:37 GMT"}], "update_date": "2021-01-12", "authors_parsed": [["Imteaj", "Ahmed", ""], ["Amini", "M. Hadi", ""]]}, {"id": "2101.03709", "submitter": "Ali Siahkoohi", "authors": "Ali Siahkoohi and Gabrio Rizzuti and Mathias Louboutin and Philipp A.\n  Witte and Felix J. Herrmann", "title": "Preconditioned training of normalizing flows for variational inference\n  in inverse problems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG physics.geo-ph", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Obtaining samples from the posterior distribution of inverse problems with\nexpensive forward operators is challenging especially when the unknowns involve\nthe strongly heterogeneous Earth. To meet these challenges, we propose a\npreconditioning scheme involving a conditional normalizing flow (NF) capable of\nsampling from a low-fidelity posterior distribution directly. This conditional\nNF is used to speed up the training of the high-fidelity objective involving\nminimization of the Kullback-Leibler divergence between the predicted and the\ndesired high-fidelity posterior density for indirect measurements at hand. To\nminimize costs associated with the forward operator, we initialize the\nhigh-fidelity NF with the weights of the pretrained low-fidelity NF, which is\ntrained beforehand on available model and data pairs. Our numerical\nexperiments, including a 2D toy and a seismic compressed sensing example,\ndemonstrate that thanks to the preconditioning considerable speed-ups are\nachievable compared to training NFs from scratch.\n", "versions": [{"version": "v1", "created": "Mon, 11 Jan 2021 05:35:36 GMT"}], "update_date": "2021-01-12", "authors_parsed": [["Siahkoohi", "Ali", ""], ["Rizzuti", "Gabrio", ""], ["Louboutin", "Mathias", ""], ["Witte", "Philipp A.", ""], ["Herrmann", "Felix J.", ""]]}, {"id": "2101.03724", "submitter": "Takumi Watanabe", "authors": "Takumi Watanabe, Hiroki Takahashi, Goh Sato, Yusuke Iwasawa, Yutaka\n  Matsuo, Ikuko Eguchi Yairi", "title": "Wheelchair Behavior Recognition for Visualizing Sidewalk Accessibility\n  by Deep Neural Networks", "comments": "15 pages, 6 figures, and 1 table; accepted at 2ND International\n  Workshop on Deep Learning for Human Activity Recognition, held in conjunction\n  with IJCAI-PRICAI 2020, January 2021; will be published at Springer\n  Communications in Computer and Information Science (CCIS) proceedings", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces our methodology to estimate sidewalk accessibilities\nfrom wheelchair behavior via a triaxial accelerometer in a smartphone installed\nunder a wheelchair seat. Our method recognizes sidewalk accessibilities from\nenvironmental factors, e.g. gradient, curbs, and gaps, which influence\nwheelchair bodies and become a burden for people with mobility difficulties.\nThis paper developed and evaluated a prototype system that visualizes sidewalk\naccessibility information by extracting knowledge from wheelchair acceleration\nusing deep neural networks. Firstly, we created a supervised convolutional\nneural network model to classify road surface conditions using wheelchair\nacceleration data. Secondly, we applied a weakly supervised method to extract\nrepresentations of road surface conditions without manual annotations. Finally,\nwe developed a self-supervised variational autoencoder to assess sidewalk\nbarriers for wheelchair users. The results show that the proposed method\nestimates sidewalk accessibilities from wheelchair accelerations and extracts\nknowledge of accessibilities by weakly supervised and self-supervised\napproaches.\n", "versions": [{"version": "v1", "created": "Mon, 11 Jan 2021 06:41:42 GMT"}], "update_date": "2021-01-12", "authors_parsed": [["Watanabe", "Takumi", ""], ["Takahashi", "Hiroki", ""], ["Sato", "Goh", ""], ["Iwasawa", "Yusuke", ""], ["Matsuo", "Yutaka", ""], ["Yairi", "Ikuko Eguchi", ""]]}, {"id": "2101.03735", "submitter": "Wei Xie", "authors": "Bo Wang, Wei Xie, Tugce Martagan, Alp Akcay, Bram van Ravenstein", "title": "Optimizing Biomanufacturing Harvesting Decisions under Limited\n  Historical Data", "comments": "37 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In biopharmaceutical manufacturing, fermentation processes play a critical\nrole on productivity and profit. A fermentation process uses living cells with\ncomplex biological mechanisms, and this leads to high variability in the\nprocess outputs. By building on the biological mechanisms of protein and\nimpurity growth, we introduce a stochastic model to characterize the\naccumulation of the protein and impurity levels in the fermentation process.\nHowever, a common challenge in industry is the availability of only very\nlimited amount of data especially in the development and early stage of\nproduction. This adds an additional layer of uncertainty, referred to as model\nrisk, due to the difficulty of estimating the model parameters with limited\ndata. In this paper, we study the harvesting decision for a fermentation\nprocess under model risk. In particular, we adopt a Bayesian approach to update\nthe unknown parameters of the growth-rate distributions, and use the resulting\nposterior distributions to characterize the impact of model risk on\nfermentation output variability. The harvesting problem is formulated as a\nMarkov decision process model with knowledge states that summarize the\nposterior distributions and hence incorporate the model risk in\ndecision-making. The resulting model is solved by using a reinforcement\nlearning algorithm based on Bayesian sparse sampling. We provide analytical\nresults on the structure of the optimal policy and its objective function, and\nexplicitly study the impact of model risk on harvesting decisions. Our case\nstudies at MSD Animal Health demonstrate that the proposed model and solution\napproach improve the harvesting decisions in real life by achieving\nsubstantially higher average output from a fermentation batch along with lower\nbatch-to-batch variability.\n", "versions": [{"version": "v1", "created": "Mon, 11 Jan 2021 07:47:25 GMT"}, {"version": "v2", "created": "Mon, 7 Jun 2021 21:35:42 GMT"}, {"version": "v3", "created": "Tue, 15 Jun 2021 19:09:42 GMT"}], "update_date": "2021-06-17", "authors_parsed": [["Wang", "Bo", ""], ["Xie", "Wei", ""], ["Martagan", "Tugce", ""], ["Akcay", "Alp", ""], ["van Ravenstein", "Bram", ""]]}, {"id": "2101.03742", "submitter": "Soma Bandyopadhyay", "authors": "Soma Bandyopadhyay, Anish Datta and Arpan Pal (TCS Research, TATA\n  Consultancy Services, Kolkata, India)", "title": "Hierarchical Clustering using Auto-encoded Compact Representation for\n  Time-series Analysis", "comments": "6 figures, 8 pages , 6 tables, accepted and presented conference\n  IJCAI-PRICAI LDRC Learning Data Representation for Clustering (LDRC) workshop\n  2020 https://ldrcworkshop.github.io/LDRC2020/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Getting a robust time-series clustering with best choice of distance measure\nand appropriate representation is always a challenge. We propose a novel\nmechanism to identify the clusters combining learned compact representation of\ntime-series, Auto Encoded Compact Sequence (AECS) and hierarchical clustering\napproach. Proposed algorithm aims to address the large computing time issue of\nhierarchical clustering as learned latent representation AECS has a length much\nless than the original length of time-series and at the same time want to\nenhance its performance.Our algorithm exploits Recurrent Neural Network (RNN)\nbased under complete Sequence to Sequence(seq2seq) autoencoder and\nagglomerative hierarchical clustering with a choice of best distance measure to\nrecommend the best clustering. Our scheme selects the best distance measure and\ncorresponding clustering for both univariate and multivariate time-series. We\nhave experimented with real-world time-series from UCR and UCI archive taken\nfrom diverse application domains like health, smart-city, manufacturing etc.\nExperimental results show that proposed method not only produce close to\nbenchmark results but also in some cases outperform the benchmark.\n", "versions": [{"version": "v1", "created": "Mon, 11 Jan 2021 08:03:57 GMT"}], "update_date": "2021-01-12", "authors_parsed": [["Bandyopadhyay", "Soma", "", "TCS Research, TATA\n  Consultancy Services, Kolkata, India"], ["Datta", "Anish", "", "TCS Research, TATA\n  Consultancy Services, Kolkata, India"], ["Pal", "Arpan", "", "TCS Research, TATA\n  Consultancy Services, Kolkata, India"]]}, {"id": "2101.03747", "submitter": "Yuan Yuan Ding", "authors": "Yuanyuan Ding and Junchi Yan and Guoqiang Hu and Jun Zhu", "title": "Cognitive Visual Inspection Service for LCD Manufacturing Industry", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the rapid growth of display devices, quality inspection via machine\nvision technology has become increasingly important for flat-panel displays\n(FPD) industry. This paper discloses a novel visual inspection system for\nliquid crystal display (LCD), which is currently a dominant type in the FPD\nindustry. The system is based on two cornerstones: robust/high-performance\ndefect recognition model and cognitive visual inspection service architecture.\nA hybrid application of conventional computer vision technique and the latest\ndeep convolutional neural network (DCNN) leads to an integrated defect\ndetection, classfication and impact evaluation model that can be economically\ntrained with only image-level class annotations to achieve a high inspection\naccuracy. In addition, the properly trained model is robust to the variation of\nthe image qulity, significantly alleviating the dependency between the model\nprediction performance and the image aquisition environment. This in turn\njustifies the decoupling of the defect recognition functions from the front-end\ndevice to the back-end serivce, motivating the design and realization of the\ncognitive visual inspection service architecture. Empirical case study is\nperformed on a large-scale real-world LCD dataset from a manufacturing line\nwith different layers and products, which shows the promising utility of our\nsystem, which has been deployed in a real-world LCD manufacturing line from a\nmajor player in the world.\n", "versions": [{"version": "v1", "created": "Mon, 11 Jan 2021 08:14:35 GMT"}], "update_date": "2021-01-12", "authors_parsed": [["Ding", "Yuanyuan", ""], ["Yan", "Junchi", ""], ["Hu", "Guoqiang", ""], ["Zhu", "Jun", ""]]}, {"id": "2101.03778", "submitter": "Ekaterina Artemova", "authors": "Alexander Podolskiy and Dmitry Lipin and Andrey Bout and Ekaterina\n  Artemova and Irina Piontkovskaya", "title": "Revisiting Mahalanobis Distance for Transformer-Based Out-of-Domain\n  Detection", "comments": "to appear in AAAI 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Real-life applications, heavily relying on machine learning, such as dialog\nsystems, demand out-of-domain detection methods. Intent classification models\nshould be equipped with a mechanism to distinguish seen intents from unseen\nones so that the dialog agent is capable of rejecting the latter and avoiding\nundesired behavior. However, despite increasing attention paid to the task, the\nbest practices for out-of-domain intent detection have not yet been fully\nestablished.\n  This paper conducts a thorough comparison of out-of-domain intent detection\nmethods. We prioritize the methods, not requiring access to out-of-domain data\nduring training, gathering of which is extremely time- and labor-consuming due\nto lexical and stylistic variation of user utterances. We evaluate multiple\ncontextual encoders and methods, proven to be efficient, on three standard\ndatasets for intent classification, expanded with out-of-domain utterances. Our\nmain findings show that fine-tuning Transformer-based encoders on in-domain\ndata leads to superior results. Mahalanobis distance, together with utterance\nrepresentations, derived from Transformer-based encoders, outperforms other\nmethods by a wide margin and establishes new state-of-the-art results for all\ndatasets.\n  The broader analysis shows that the reason for success lies in the fact that\nthe fine-tuned Transformer is capable of constructing homogeneous\nrepresentations of in-domain utterances, revealing geometrical disparity to out\nof domain utterances. In turn, the Mahalanobis distance captures this disparity\neasily.\n", "versions": [{"version": "v1", "created": "Mon, 11 Jan 2021 09:10:58 GMT"}], "update_date": "2021-01-12", "authors_parsed": [["Podolskiy", "Alexander", ""], ["Lipin", "Dmitry", ""], ["Bout", "Andrey", ""], ["Artemova", "Ekaterina", ""], ["Piontkovskaya", "Irina", ""]]}, {"id": "2101.03785", "submitter": "Sayed Erfan Arefin", "authors": "Sayed Erfan Arefin, Tasnia Ashrafi Heya, Dr Moinul Zaber", "title": "Predictive Analysis of Chikungunya", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Chikungunya is an emerging threat for health security all over the world\nwhich is spreading very fast. Researches for proper forecasting of the\nincidence rate of chikungunya has been going on in many places in which DARPA\nhas done a very extensive summarized result from 2014 to 2017 with the data of\nsuspected cases, confirmed cases, deaths, population and incidence rate in\ndifferent countries. In this project, we have analysed the dataset from DARPA\nand extended it to predict the incidence rate using different features of\nweather like temperature, humidity, dewiness, wind and pressure along with the\nlatitude and longitude of every country. We had to use different APIs to find\nout these extra features from 2014-2016. After creating a pure dataset, we have\nused Linear Regression to predict the incidence rate and calculated the\naccuracy and error rate.\n", "versions": [{"version": "v1", "created": "Mon, 11 Jan 2021 09:40:41 GMT"}], "update_date": "2021-01-12", "authors_parsed": [["Arefin", "Sayed Erfan", ""], ["Heya", "Tasnia Ashrafi", ""], ["Zaber", "Dr Moinul", ""]]}, {"id": "2101.03788", "submitter": "Sayed Erfan Arefin", "authors": "Sayed Erfan Arefin", "title": "Second Hand Price Prediction for Tesla Vehicles", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The Tesla vehicles became very popular in the car industry as it was\naffordable in the consumer market and it left no carbon footprint. Due to the\nlarge decline in the stock prices of Tesla Inc. at the beginning of 2019, Tesla\nowners started selling their vehicles in the used car market. These used car\nprices depended on attributes such as the model of the vehicle, year of\nproduction, miles driven, and the battery used for the vehicle. Prices were\ndifferent for a specific vehicle in different months. In this paper, it is\ndiscussed how a machine learning technique is being implemented in order to\ndevelop a second-hand Teslavehicle price prediction system. To reach this goal,\ndifferent machine learning techniques such as decision trees, support vector\nmachine (SVM), random forest, and deep learning were investigated and finally\nwas implemented with boosted decision tree regression. I the future, it is\nintended to use a more sophisticated algorithm for better accuracy.\n", "versions": [{"version": "v1", "created": "Mon, 11 Jan 2021 09:54:13 GMT"}], "update_date": "2021-01-12", "authors_parsed": [["Arefin", "Sayed Erfan", ""]]}, {"id": "2101.03814", "submitter": "Sten Hanke", "authors": "Josef Steppan and Sten Hanke", "title": "Analysis of skin lesion images with deep learning", "comments": "for source code see: http://github.com/j05t/lesion-analysis", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Skin cancer is the most common cancer worldwide, with melanoma being the\ndeadliest form. Dermoscopy is a skin imaging modality that has shown an\nimprovement in the diagnosis of skin cancer compared to visual examination\nwithout support. We evaluate the current state of the art in the classification\nof dermoscopic images based on the ISIC-2019 Challenge for the classification\nof skin lesions and current literature. Various deep neural network\narchitectures pre-trained on the ImageNet data set are adapted to a combined\ntraining data set comprised of publicly available dermoscopic and clinical\nimages of skin lesions using transfer learning and model fine-tuning. The\nperformance and applicability of these models for the detection of eight\nclasses of skin lesions are examined. Real-time data augmentation, which uses\nrandom rotation, translation, shear, and zoom within specified bounds is used\nto increase the number of available training samples. Model predictions are\nmultiplied by inverse class frequencies and normalized to better approximate\nactual probability distributions. Overall prediction accuracy is further\nincreased by using the arithmetic mean of the predictions of several\nindependently trained models. The best single model has been published as a web\nservice.\n", "versions": [{"version": "v1", "created": "Mon, 11 Jan 2021 10:58:36 GMT"}], "update_date": "2021-01-12", "authors_parsed": [["Steppan", "Josef", ""], ["Hanke", "Sten", ""]]}, {"id": "2101.03842", "submitter": "Alexander Shknevsky", "authors": "Alexander Shknevsky, Yuval Shahar, Robert Moskovitch", "title": "The Semantic Adjacency Criterion in Time Intervals Mining", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Frequent temporal patterns discovered in time-interval-based multivariate\ndata, although syntactically correct, might be non-transparent: For some\npattern instances, there might exist intervals for the same entity that\ncontradict the pattern's usual meaning. We conjecture that non-transparent\npatterns are also less useful as classification or prediction features. We\npropose a new pruning constraint during a frequent temporal-pattern discovery\nprocess, the Semantic Adjacency Criterion [SAC], which exploits domain\nknowledge to filter out patterns that contain potentially semantically\ncontradictory components. We have defined three SAC versions, and tested their\neffect in three medical domains. We embedded these criteria in a\nfrequent-temporal-pattern discovery framework. Previously, we had informally\npresented the SAC principle and showed that using it to prune patterns enhances\nthe repeatability of their discovery in the same clinical domain. Here, we\ndefine formally the semantics of three SAC variations, and compare the use of\nthe set of pruned patterns to the use of the complete set of discovered\npatterns, as features for classification and prediction tasks in three\ndifferent medical domains. We induced four classifiers for each task, using\nfour machine-learning methods: Random Forests, Naive Bayes, SVM, and Logistic\nRegression. The features were frequent temporal patterns discovered in each\ndata set. SAC-based temporal pattern-discovery reduced by up to 97% the number\nof discovered patterns and by up to 98% the discovery runtime. But the\nclassification and prediction performance of the reduced SAC-based\npattern-based features set, was as good as when using the complete set. Using\nSAC can significantly reduce the number of discovered frequent interval-based\ntemporal patterns, and the corresponding computational effort, without losing\nclassification or prediction performance.\n", "versions": [{"version": "v1", "created": "Mon, 11 Jan 2021 12:23:49 GMT"}], "update_date": "2021-01-12", "authors_parsed": [["Shknevsky", "Alexander", ""], ["Shahar", "Yuval", ""], ["Moskovitch", "Robert", ""]]}, {"id": "2101.03850", "submitter": "Antoine Garcon AntoineGarcon", "authors": "Antoine Garcon, Julian Vexler, Dmitry Budker, Stefan Kramer", "title": "Deep Neural Networks to Recover Unknown Physical Parameters from\n  Oscillating Time Series", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NA math.NA physics.data-an", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural networks (DNNs) are widely used in pattern-recognition tasks for\nwhich a human comprehensible, quantitative description of the data-generating\nprocess, e.g., in the form of equations, cannot be achieved. While doing so,\nDNNs often produce an abstract (entangled and non-interpretable) representation\nof the data-generating process. This is one of the reasons why DNNs are not\nextensively used in physics-signal processing: physicists generally require\ntheir analyses to yield quantitative information about the studied systems. In\nthis article we use DNNs to disentangle components of oscillating time series,\nand recover meaningful information. We show that, because DNNs can find useful\nabstract feature representations, they can be used when prior knowledge about\nthe signal-generating process exists, but is not complete, as it is\nparticularly the case in \"new-physics\" searches. To this aim, we train our DNN\non synthetic oscillating time series to perform two tasks: a regression of the\nsignal latent parameters and signal denoising by an Autoencoder-like\narchitecture. We show that the regression and denoising performance is similar\nto those of least-square curve fittings (LS-fit) with true latent parameters'\ninitial guesses, in spite of the DNN needing no initial guesses at all. We then\nexplore applications in which we believe our architecture could prove useful\nfor time-series processing in physics, when prior knowledge is incomplete. As\nan example, we employ DNNs as a tool to inform LS-fits when initial guesses are\nunknown. We show that the regression can be performed on some latent\nparameters, while ignoring the existence of others. Because the Autoencoder\nneeds no prior information about the physical model, the remaining unknown\nlatent parameters can still be captured, thus making use of partial prior\nknowledge, while leaving space for data exploration and discoveries.\n", "versions": [{"version": "v1", "created": "Mon, 11 Jan 2021 12:39:50 GMT"}], "update_date": "2021-01-12", "authors_parsed": [["Garcon", "Antoine", ""], ["Vexler", "Julian", ""], ["Budker", "Dmitry", ""], ["Kramer", "Stefan", ""]]}, {"id": "2101.03864", "submitter": "Luisa Zintgraf", "authors": "Luisa Zintgraf, Sam Devlin, Kamil Ciosek, Shimon Whiteson, Katja\n  Hofmann", "title": "Deep Interactive Bayesian Reinforcement Learning via Meta-Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Agents that interact with other agents often do not know a priori what the\nother agents' strategies are, but have to maximise their own online return\nwhile interacting with and learning about others. The optimal adaptive\nbehaviour under uncertainty over the other agents' strategies w.r.t. some prior\ncan in principle be computed using the Interactive Bayesian Reinforcement\nLearning framework. Unfortunately, doing so is intractable in most settings,\nand existing approximation methods are restricted to small tasks. To overcome\nthis, we propose to meta-learn approximate belief inference and Bayes-optimal\nbehaviour for a given prior. To model beliefs over other agents, we combine\nsequential and hierarchical Variational Auto-Encoders, and meta-train this\ninference model alongside the policy. We show empirically that our approach\noutperforms existing methods that use a model-free approach, sample from the\napproximate posterior, maintain memory-free models of others, or do not fully\nutilise the known structure of the environment.\n", "versions": [{"version": "v1", "created": "Mon, 11 Jan 2021 13:25:13 GMT"}], "update_date": "2021-01-12", "authors_parsed": [["Zintgraf", "Luisa", ""], ["Devlin", "Sam", ""], ["Ciosek", "Kamil", ""], ["Whiteson", "Shimon", ""], ["Hofmann", "Katja", ""]]}, {"id": "2101.03867", "submitter": "Ahmad Asadi", "authors": "Mehran Taghian, Ahmad Asadi, Reza Safabakhsh", "title": "A Reinforcement Learning Based Encoder-Decoder Framework for Learning\n  Stock Trading Rules", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.ST cs.AI cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A wide variety of deep reinforcement learning (DRL) models have recently been\nproposed to learn profitable investment strategies. The rules learned by these\nmodels outperform the previous strategies specially in high frequency trading\nenvironments. However, it is shown that the quality of the extracted features\nfrom a long-term sequence of raw prices of the instruments greatly affects the\nperformance of the trading rules learned by these models. Employing a neural\nencoder-decoder structure to extract informative features from complex input\ntime-series has proved very effective in other popular tasks like neural\nmachine translation and video captioning in which the models face a similar\nproblem. The encoder-decoder framework extracts highly informative features\nfrom a long sequence of prices along with learning how to generate outputs\nbased on the extracted features. In this paper, a novel end-to-end model based\non the neural encoder-decoder framework combined with DRL is proposed to learn\nsingle instrument trading strategies from a long sequence of raw prices of the\ninstrument. The proposed model consists of an encoder which is a neural\nstructure responsible for learning informative features from the input\nsequence, and a decoder which is a DRL model responsible for learning\nprofitable strategies based on the features extracted by the encoder. The\nparameters of the encoder and the decoder structures are learned jointly, which\nenables the encoder to extract features fitted to the task of the decoder DRL.\nIn addition, the effects of different structures for the encoder and various\nforms of the input sequences on the performance of the learned strategies are\ninvestigated. Experimental results showed that the proposed model outperforms\nother state-of-the-art models in highly dynamic environments.\n", "versions": [{"version": "v1", "created": "Fri, 8 Jan 2021 13:19:01 GMT"}], "update_date": "2021-01-12", "authors_parsed": [["Taghian", "Mehran", ""], ["Asadi", "Ahmad", ""], ["Safabakhsh", "Reza", ""]]}, {"id": "2101.03885", "submitter": "Rayyan Ahmad Khan", "authors": "Rayyan Ahmad Khan, Muhammad Umer Anwaar, Omran Kaddah and Martin\n  Kleinsteuber", "title": "Variational Embeddings for Community Detection and Node Representation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we study how to simultaneously learn two highly correlated\ntasks of graph analysis, i.e., community detection and node representation\nlearning. We propose an efficient generative model called VECoDeR for jointly\nlearning Variational Embeddings for Community Detection and node\nRepresentation. VECoDeR assumes that every node can be a member of one or more\ncommunities. The node embeddings are learned in such a way that connected nodes\nare not only \"closer\" to each other but also share similar community\nassignments. A joint learning framework leverages community-aware node\nembeddings for better community detection. We demonstrate on several graph\ndatasets that VECoDeR effectively out-performs many competitive baselines on\nall three tasks i.e. node classification, overlapping community detection and\nnon-overlapping community detection. We also show that VECoDeR is\ncomputationally efficient and has quite robust performance with varying\nhyperparameters.\n", "versions": [{"version": "v1", "created": "Mon, 11 Jan 2021 13:36:29 GMT"}], "update_date": "2021-01-12", "authors_parsed": [["Khan", "Rayyan Ahmad", ""], ["Anwaar", "Muhammad Umer", ""], ["Kaddah", "Omran", ""], ["Kleinsteuber", "Martin", ""]]}, {"id": "2101.03895", "submitter": "Xiang Lan", "authors": "Zhaowei Zhu, Xiang Lan, Tingting Zhao, Yangming Guo, Pipin Kojodjojo,\n  Zhuoyang Xu, Zhuo Liu, Siqi Liu, Han Wang, Xingzhi Sun, Mengling Feng", "title": "Identification of 27 abnormalities from multi-lead ECG signals: An\n  ensembled Se-ResNet framework with Sign Loss function", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SP cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cardiovascular disease is a major threat to health and one of the primary\ncauses of death globally. The 12-lead ECG is a cheap and commonly accessible\ntool to identify cardiac abnormalities. Early and accurate diagnosis will allow\nearly treatment and intervention to prevent severe complications of\ncardiovascular disease. In the PhysioNet/Computing in Cardiology Challenge\n2020, our objective is to develop an algorithm that automatically identifies 27\nECG abnormalities from 12-lead ECG recordings.\n", "versions": [{"version": "v1", "created": "Sat, 12 Dec 2020 17:16:02 GMT"}, {"version": "v2", "created": "Tue, 12 Jan 2021 04:46:56 GMT"}], "update_date": "2021-01-13", "authors_parsed": [["Zhu", "Zhaowei", ""], ["Lan", "Xiang", ""], ["Zhao", "Tingting", ""], ["Guo", "Yangming", ""], ["Kojodjojo", "Pipin", ""], ["Xu", "Zhuoyang", ""], ["Liu", "Zhuo", ""], ["Liu", "Siqi", ""], ["Wang", "Han", ""], ["Sun", "Xingzhi", ""], ["Feng", "Mengling", ""]]}, {"id": "2101.03909", "submitter": "Mingyu Yang", "authors": "Mingyu Yang, Chenghong Bian, and Hun-Seok Kim", "title": "Deep Joint Source Channel Coding for WirelessImage Transmission with\n  OFDM", "comments": "Accepted to ICC 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SP cs.IT cs.LG math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a deep learning based joint source channel coding (JSCC) scheme\nfor wireless image transmission over multipath fading channels with non-linear\nsignal clipping. The proposed encoder and decoder use convolutional neural\nnetworks (CNN) and directly map the source images to complex-valued baseband\nsamples for orthogonal frequency division multiplexing (OFDM) transmission. The\nproposed model-driven machine learning approach eliminates the need for\nseparate source and channel coding while integrating an OFDM datapath to cope\nwith multipath fading channels. The end-to-end JSCC communication system\ncombines trainable CNN layers with non-trainable but differentiable layers\nrepresenting the multipath channel model and OFDM signal processing blocks. Our\nresults show that injecting domain expert knowledge by incorporating OFDM\nbaseband processing blocks into the machine learning framework significantly\nenhances the overall performance compared to an unstructured CNN. Our method\noutperforms conventional schemes that employ state-of-the-art but separate\nsource and channel coding such as BPG and LDPC with OFDM. Moreover, our method\nis shown to be robust against non-linear signal clipping in OFDM for various\nchannel conditions that do not match the model parameter used during the\ntraining.\n", "versions": [{"version": "v1", "created": "Tue, 5 Jan 2021 22:27:20 GMT"}, {"version": "v2", "created": "Mon, 24 May 2021 05:37:39 GMT"}], "update_date": "2021-05-25", "authors_parsed": [["Yang", "Mingyu", ""], ["Bian", "Chenghong", ""], ["Kim", "Hun-Seok", ""]]}, {"id": "2101.03921", "submitter": "Anugrah Akbar Praramadhan", "authors": "Anugrah Akbar Praramadhan and Guntur Eka Saputra", "title": "Cycle Generative Adversarial Networks Algorithm With Style Transfer For\n  Image Generation", "comments": "in Indonesian language", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The biggest challenge faced by a Machine Learning Engineer is the lack of\ndata they have, especially for 2-dimensional images. The image is processed to\nbe trained into a Machine Learning model so that it can recognize patterns in\nthe data and provide predictions. This research is intended to create a\nsolution using the Cycle Generative Adversarial Networks (GANs) algorithm in\novercoming the problem of lack of data. Then use Style Transfer to be able to\ngenerate a new image based on the given style. Based on the results of testing\nthe resulting model has been carried out several improvements, previously the\nloss value of the photo generator: 3.1267, monet style generator: 3.2026, photo\ndiscriminator: 0.6325, and monet style discriminator: 0.6931 to photo\ngenerator: 2.3792, monet style generator: 2.7291, photo discriminator: 0.5956,\nand monet style discriminator: 0.4940. It is hoped that the research will make\nthe application of this solution useful in the fields of Education, Arts,\nInformation Technology, Medicine, Astronomy, Automotive and other important\nfields.\n", "versions": [{"version": "v1", "created": "Mon, 11 Jan 2021 14:37:25 GMT"}], "update_date": "2021-01-12", "authors_parsed": [["Praramadhan", "Anugrah Akbar", ""], ["Saputra", "Guntur Eka", ""]]}, {"id": "2101.03940", "submitter": "Emma Rocheteau", "authors": "Emma Rocheteau, Catherine Tong, Petar Veli\\v{c}kovi\\'c, Nicholas Lane,\n  Pietro Li\\`o", "title": "Predicting Patient Outcomes with Graph Representation Learning", "comments": "Accepted at the W3PHIAI-21 and DLG-AAAI'21 workshops at AAAI 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Recent work on predicting patient outcomes in the Intensive Care Unit (ICU)\nhas focused heavily on the physiological time series data, largely ignoring\nsparse data such as diagnoses and medications. When they are included, they are\nusually concatenated in the late stages of a model, which may struggle to learn\nfrom rarer disease patterns. Instead, we propose a strategy to exploit\ndiagnoses as relational information by connecting similar patients in a graph.\nTo this end, we propose LSTM-GNN for patient outcome prediction tasks: a hybrid\nmodel combining Long Short-Term Memory networks (LSTMs) for extracting temporal\nfeatures and Graph Neural Networks (GNNs) for extracting the patient\nneighbourhood information. We demonstrate that LSTM-GNNs outperform the\nLSTM-only baseline on length of stay prediction tasks on the eICU database.\nMore generally, our results indicate that exploiting information from\nneighbouring patient cases using graph neural networks is a promising research\ndirection, yielding tangible returns in supervised learning performance on\nElectronic Health Records.\n", "versions": [{"version": "v1", "created": "Mon, 11 Jan 2021 15:04:07 GMT"}], "update_date": "2021-01-12", "authors_parsed": [["Rocheteau", "Emma", ""], ["Tong", "Catherine", ""], ["Veli\u010dkovi\u0107", "Petar", ""], ["Lane", "Nicholas", ""], ["Li\u00f2", "Pietro", ""]]}, {"id": "2101.03944", "submitter": "Rajarshi Banerjee", "authors": "Haonan Wu, Rajarshi Banerjee, Indhumathi Venkatachalam, Daniel\n  Percy-Hughes and Praveen Chougale", "title": "Impact of Interventional Policies Including Vaccine on Covid-19\n  Propagation and Socio-Economic Factors", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  A novel coronavirus disease has emerged (later named COVID-19) and caused the\nworld to enter a new reality, with many direct and indirect factors influencing\nit. Some are human-controllable (e.g. interventional policies, mobility and the\nvaccine); some are not (e.g. the weather). We have sought to test how a change\nin these human-controllable factors might influence two measures: the number of\ndaily cases against economic impact. If applied at the right level and with\nup-to-date data to measure, policymakers would be able to make targeted\ninterventions and measure their cost. This study aims to provide a predictive\nanalytics framework to model, predict and simulate COVID-19 propagation and the\nsocio-economic impact of interventions intended to reduce the spread of the\ndisease such as policy and/or vaccine. It allows policymakers, government\nrepresentatives and business leaders to make better-informed decisions about\nthe potential effect of various interventions with forward-looking views via\nscenario planning. We have leveraged a recently launched open-source COVID-19\nbig data platform and used published research to find potentially relevant\nvariables (features) and leveraged in-depth data quality checks and analytics\nfor feature selection and predictions. An advanced machine learning pipeline\nhas been developed armed with a self-evolving model, deployed on a modern\nmachine learning architecture. It has high accuracy for trend prediction\n(back-tested with r-squared) and is augmented with interpretability for deeper\ninsights.\n", "versions": [{"version": "v1", "created": "Mon, 11 Jan 2021 15:08:07 GMT"}], "update_date": "2021-01-12", "authors_parsed": [["Wu", "Haonan", ""], ["Banerjee", "Rajarshi", ""], ["Venkatachalam", "Indhumathi", ""], ["Percy-Hughes", "Daniel", ""], ["Chougale", "Praveen", ""]]}, {"id": "2101.03958", "submitter": "John Co-Reyes", "authors": "John D. Co-Reyes, Yingjie Miao, Daiyi Peng, Esteban Real, Sergey\n  Levine, Quoc V. Le, Honglak Lee, Aleksandra Faust", "title": "Evolving Reinforcement Learning Algorithms", "comments": "ICLR 2021 Oral. See project website at\n  https://sites.google.com/view/evolvingrl", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a method for meta-learning reinforcement learning algorithms by\nsearching over the space of computational graphs which compute the loss\nfunction for a value-based model-free RL agent to optimize. The learned\nalgorithms are domain-agnostic and can generalize to new environments not seen\nduring training. Our method can both learn from scratch and bootstrap off known\nexisting algorithms, like DQN, enabling interpretable modifications which\nimprove performance. Learning from scratch on simple classical control and\ngridworld tasks, our method rediscovers the temporal-difference (TD) algorithm.\nBootstrapped from DQN, we highlight two learned algorithms which obtain good\ngeneralization performance over other classical control tasks, gridworld type\ntasks, and Atari games. The analysis of the learned algorithm behavior shows\nresemblance to recently proposed RL algorithms that address overestimation in\nvalue-based methods.\n", "versions": [{"version": "v1", "created": "Fri, 8 Jan 2021 18:55:07 GMT"}, {"version": "v2", "created": "Mon, 15 Mar 2021 19:41:47 GMT"}, {"version": "v3", "created": "Fri, 26 Mar 2021 22:53:58 GMT"}, {"version": "v4", "created": "Mon, 3 May 2021 16:35:06 GMT"}], "update_date": "2021-05-04", "authors_parsed": [["Co-Reyes", "John D.", ""], ["Miao", "Yingjie", ""], ["Peng", "Daiyi", ""], ["Real", "Esteban", ""], ["Levine", "Sergey", ""], ["Le", "Quoc V.", ""], ["Lee", "Honglak", ""], ["Faust", "Aleksandra", ""]]}, {"id": "2101.03961", "submitter": "William Fedus", "authors": "William Fedus, Barret Zoph, Noam Shazeer", "title": "Switch Transformers: Scaling to Trillion Parameter Models with Simple\n  and Efficient Sparsity", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In deep learning, models typically reuse the same parameters for all inputs.\nMixture of Experts (MoE) defies this and instead selects different parameters\nfor each incoming example. The result is a sparsely-activated model -- with\noutrageous numbers of parameters -- but a constant computational cost. However,\ndespite several notable successes of MoE, widespread adoption has been hindered\nby complexity, communication costs and training instability -- we address these\nwith the Switch Transformer. We simplify the MoE routing algorithm and design\nintuitive improved models with reduced communication and computational costs.\nOur proposed training techniques help wrangle the instabilities and we show\nlarge sparse models may be trained, for the first time, with lower precision\n(bfloat16) formats. We design models based off T5-Base and T5-Large to obtain\nup to 7x increases in pre-training speed with the same computational resources.\nThese improvements extend into multilingual settings where we measure gains\nover the mT5-Base version across all 101 languages. Finally, we advance the\ncurrent scale of language models by pre-training up to trillion parameter\nmodels on the \"Colossal Clean Crawled Corpus\" and achieve a 4x speedup over the\nT5-XXL model.\n", "versions": [{"version": "v1", "created": "Mon, 11 Jan 2021 16:11:52 GMT"}], "update_date": "2021-01-12", "authors_parsed": [["Fedus", "William", ""], ["Zoph", "Barret", ""], ["Shazeer", "Noam", ""]]}, {"id": "2101.03970", "submitter": "Dakuo Wang", "authors": "Dakuo Wang and Q. Vera Liao and Yunfeng Zhang and Udayan Khurana and\n  Horst Samulowitz and Soya Park and Michael Muller and Lisa Amini", "title": "How Much Automation Does a Data Scientist Want?", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data science and machine learning (DS/ML) are at the heart of the recent\nadvancements of many Artificial Intelligence (AI) applications. There is an\nactive research thread in AI, \\autoai, that aims to develop systems for\nautomating end-to-end the DS/ML Lifecycle. However, do DS and ML workers really\nwant to automate their DS/ML workflow? To answer this question, we first\nsynthesize a human-centered AutoML framework with 6 User Role/Personas, 10\nStages and 43 Sub-Tasks, 5 Levels of Automation, and 5 Types of Explanation,\nthrough reviewing research literature and marketing reports. Secondly, we use\nthe framework to guide the design of an online survey study with 217 DS/ML\nworkers who had varying degrees of experience, and different user roles\n\"matching\" to our 6 roles/personas. We found that different user personas\nparticipated in distinct stages of the lifecycle -- but not all stages. Their\ndesired levels of automation and types of explanation for AutoML also varied\nsignificantly depending on the DS/ML stage and the user persona. Based on the\nsurvey results, we argue there is no rationale from user needs for complete\nautomation of the end-to-end DS/ML lifecycle. We propose new next steps for\nuser-controlled DS/ML automation.\n", "versions": [{"version": "v1", "created": "Thu, 7 Jan 2021 04:09:08 GMT"}], "update_date": "2021-01-12", "authors_parsed": [["Wang", "Dakuo", ""], ["Liao", "Q. Vera", ""], ["Zhang", "Yunfeng", ""], ["Khurana", "Udayan", ""], ["Samulowitz", "Horst", ""], ["Park", "Soya", ""], ["Muller", "Michael", ""], ["Amini", "Lisa", ""]]}, {"id": "2101.03973", "submitter": "Pascal Van Hentenryck", "authors": "Terrence W.K. Mak and Ferdinando Fioretto and Pascal VanHentenryck", "title": "Load Embeddings for Scalable AC-OPF Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SY cs.LG cs.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  AC Optimal Power Flow (AC-OPF) is a fundamental building block in power\nsystem optimization. It is often solved repeatedly, especially in regions with\nlarge penetration of renewable generation, to avoid violating operational\nlimits. Recent work has shown that deep learning can be effective in providing\nhighly accurate approximations of AC-OPF. However, deep learning approaches may\nsuffer from scalability issues, especially when applied to large realistic\ngrids. This paper addresses these scalability limitations and proposes a load\nembedding scheme using a 3-step approach. The first step formulates the load\nembedding problem as a bilevel optimization model that can be solved using a\npenalty method. The second step learns the encoding optimization to quickly\nproduce load embeddings for new OPF instances. The third step is a deep\nlearning model that uses load embeddings to produce accurate AC-OPF\napproximations. The approach is evaluated experimentally on large-scale test\ncases from the NESTA library. The results demonstrate that the proposed\napproach produces an order of magnitude improvements in training convergence\nand prediction accuracy.\n", "versions": [{"version": "v1", "created": "Mon, 11 Jan 2021 15:28:38 GMT"}], "update_date": "2021-01-12", "authors_parsed": [["Mak", "Terrence W. K.", ""], ["Fioretto", "Ferdinando", ""], ["VanHentenryck", "Pascal", ""]]}, {"id": "2101.03988", "submitter": "Bla\\v{z} \\v{S}krlj", "authors": "Boshko Koloski, Timen Stepi\\v{s}nik Perdih, Senja Pollak and Bla\\v{z}\n  \\v{S}krlj", "title": "Identification of COVID-19 related Fake News via Neural Stacking", "comments": "Published at CONSTRAIN 2021 (AAAI)", "journal-ref": null, "doi": "10.1007/978-3-030-73696-5_17", "report-no": null, "categories": "cs.CL cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Identification of Fake News plays a prominent role in the ongoing pandemic,\nimpacting multiple aspects of day-to-day life. In this work we present a\nsolution to the shared task titled COVID19 Fake News Detection in English,\nscoring the 50th place amongst 168 submissions. The solution was within 1.5% of\nthe best performing solution. The proposed solution employs a heterogeneous\nrepresentation ensemble, adapted for the classification task via an additional\nneural classification head comprised of multiple hidden layers. The paper\nconsists of detailed ablation studies further displaying the proposed method's\nbehavior and possible implications. The solution is freely available.\n\\url{https://gitlab.com/boshko.koloski/covid19-fake-news}\n", "versions": [{"version": "v1", "created": "Mon, 11 Jan 2021 15:52:37 GMT"}, {"version": "v2", "created": "Wed, 30 Jun 2021 18:35:03 GMT"}], "update_date": "2021-07-02", "authors_parsed": [["Koloski", "Boshko", ""], ["Perdih", "Timen Stepi\u0161nik", ""], ["Pollak", "Senja", ""], ["\u0160krlj", "Bla\u017e", ""]]}, {"id": "2101.03989", "submitter": "Alexander Lavin", "authors": "Alexander Lavin, Ciar\\'an M. Gilligan-Lee, Alessya Visnjic, Siddha\n  Ganju, Dava Newman, Sujoy Ganguly, Danny Lange, At{\\i}l{\\i}m G\\\"une\\c{s}\n  Baydin, Amit Sharma, Adam Gibson, Yarin Gal, Eric P. Xing, Chris Mattmann,\n  James Parr", "title": "Technology Readiness Levels for Machine Learning Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The development and deployment of machine learning (ML) systems can be\nexecuted easily with modern tools, but the process is typically rushed and\nmeans-to-an-end. The lack of diligence can lead to technical debt, scope creep\nand misaligned objectives, model misuse and failures, and expensive\nconsequences. Engineering systems, on the other hand, follow well-defined\nprocesses and testing standards to streamline development for high-quality,\nreliable results. The extreme is spacecraft systems, where mission critical\nmeasures and robustness are ingrained in the development process. Drawing on\nexperience in both spacecraft engineering and ML (from research through product\nacross domain areas), we have developed a proven systems engineering approach\nfor machine learning development and deployment. Our \"Machine Learning\nTechnology Readiness Levels\" (MLTRL) framework defines a principled process to\nensure robust, reliable, and responsible systems while being streamlined for ML\nworkflows, including key distinctions from traditional software engineering.\nEven more, MLTRL defines a lingua franca for people across teams and\norganizations to work collaboratively on artificial intelligence and machine\nlearning technologies. Here we describe the framework and elucidate it with\nseveral real world use-cases of developing ML methods from basic research\nthrough productization and deployment, in areas such as medical diagnostics,\nconsumer computer vision, satellite imagery, and particle physics.\n", "versions": [{"version": "v1", "created": "Mon, 11 Jan 2021 15:54:48 GMT"}], "update_date": "2021-01-12", "authors_parsed": [["Lavin", "Alexander", ""], ["Gilligan-Lee", "Ciar\u00e1n M.", ""], ["Visnjic", "Alessya", ""], ["Ganju", "Siddha", ""], ["Newman", "Dava", ""], ["Ganguly", "Sujoy", ""], ["Lange", "Danny", ""], ["Baydin", "At\u0131l\u0131m G\u00fcne\u015f", ""], ["Sharma", "Amit", ""], ["Gibson", "Adam", ""], ["Gal", "Yarin", ""], ["Xing", "Eric P.", ""], ["Mattmann", "Chris", ""], ["Parr", "James", ""]]}, {"id": "2101.03996", "submitter": "Baichuan Mo", "authors": "Baichuan Mo, Zhan Zhao, Haris N. Koutsopoulos, Jinhua Zhao", "title": "Individual Mobility Prediction: An Interpretable Activity-based Hidden\n  Markov Approach", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Individual mobility is driven by demand for activities with diverse\nspatiotemporal patterns, but existing methods for mobility prediction often\noverlook the underlying activity patterns. To address this issue, this study\ndevelops an activity-based modeling framework for individual mobility\nprediction. Specifically, an input-output hidden Markov model (IOHMM) framework\nis proposed to simultaneously predict the (continuous) time and (discrete)\nlocation of an individual's next trip using transit smart card data. The\nprediction task can be transformed into predicting the hidden activity duration\nand end location. Based on a case study of Hong Kong's metro system, we show\nthat the proposed model can achieve similar prediction performance as the\nstate-of-the-art long short-term memory (LSTM) model. Unlike LSTM, the proposed\nIOHMM model can also be used to analyze hidden activity patterns, which\nprovides meaningful behavioral interpretation for why an individual makes a\ncertain trip. Therefore, the activity-based prediction framework offers a way\nto preserve the predictive power of advanced machine learning methods while\nenhancing our ability to generate insightful behavioral explanations, which is\nuseful for enhancing situational awareness in user-centric transportation\napplications such as personalized traveler information.\n", "versions": [{"version": "v1", "created": "Mon, 11 Jan 2021 16:11:27 GMT"}], "update_date": "2021-01-12", "authors_parsed": [["Mo", "Baichuan", ""], ["Zhao", "Zhan", ""], ["Koutsopoulos", "Haris N.", ""], ["Zhao", "Jinhua", ""]]}, {"id": "2101.03999", "submitter": "Aakash Bansal", "authors": "Aakash Bansal, Zachary Eberhart, Lingfei Wu, Collin McMillan", "title": "A Neural Question Answering System for Basic Questions about Subroutines", "comments": "12 pages, 5 figures . To be published at the proceeding of IEEE\n  International Conference on Software Analysis, Evolution and Reengineering\n  (SANER) 2021, Honolulu, Hawaii, USA", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SE cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A question answering (QA) system is a type of conversational AI that\ngenerates natural language answers to questions posed by human users. QA\nsystems often form the backbone of interactive dialogue systems, and have been\nstudied extensively for a wide variety of tasks ranging from restaurant\nrecommendations to medical diagnostics. Dramatic progress has been made in\nrecent years, especially from the use of encoder-decoder neural architectures\ntrained with big data input. In this paper, we take initial steps to bringing\nstate-of-the-art neural QA technologies to Software Engineering applications by\ndesigning a context-based QA system for basic questions about subroutines. We\ncurate a training dataset of 10.9 million question/context/answer tuples based\non rules we extract from recent empirical studies. Then, we train a custom\nneural QA model with this dataset and evaluate the model in a study with\nprofessional programmers. We demonstrate the strengths and weaknesses of the\nsystem, and lay the groundwork for its use in eventual dialogue systems for\nsoftware engineering.\n", "versions": [{"version": "v1", "created": "Mon, 11 Jan 2021 16:18:52 GMT"}], "update_date": "2021-01-12", "authors_parsed": [["Bansal", "Aakash", ""], ["Eberhart", "Zachary", ""], ["Wu", "Lingfei", ""], ["McMillan", "Collin", ""]]}, {"id": "2101.04012", "submitter": "Raviraj Joshi", "authors": "Apurva Wani, Isha Joshi, Snehal Khandve, Vedangi Wagh, Raviraj Joshi", "title": "Evaluating Deep Learning Approaches for Covid19 Fake News Detection", "comments": "Accepted at Contraint@AAAI 2021", "journal-ref": null, "doi": "10.1007/978-3-030-73696-5_15", "report-no": null, "categories": "cs.LG cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Social media platforms like Facebook, Twitter, and Instagram have enabled\nconnection and communication on a large scale. It has revolutionized the rate\nat which information is shared and enhanced its reach. However, another side of\nthe coin dictates an alarming story. These platforms have led to an increase in\nthe creation and spread of fake news. The fake news has not only influenced\npeople in the wrong direction but also claimed human lives. During these\ncritical times of the Covid19 pandemic, it is easy to mislead people and make\nthem believe in fatal information. Therefore it is important to curb fake news\nat source and prevent it from spreading to a larger audience. We look at\nautomated techniques for fake news detection from a data mining perspective. We\nevaluate different supervised text classification algorithms on Contraint@AAAI\n2021 Covid-19 Fake news detection dataset. The classification algorithms are\nbased on Convolutional Neural Networks (CNN), Long Short Term Memory (LSTM),\nand Bidirectional Encoder Representations from Transformers (BERT). We also\nevaluate the importance of unsupervised learning in the form of language model\npre-training and distributed word representations using unlabelled covid tweets\ncorpus. We report the best accuracy of 98.41\\% on the Covid-19 Fake news\ndetection dataset.\n", "versions": [{"version": "v1", "created": "Mon, 11 Jan 2021 16:39:03 GMT"}, {"version": "v2", "created": "Wed, 13 Jan 2021 17:18:02 GMT"}], "update_date": "2021-04-13", "authors_parsed": [["Wani", "Apurva", ""], ["Joshi", "Isha", ""], ["Khandve", "Snehal", ""], ["Wagh", "Vedangi", ""], ["Joshi", "Raviraj", ""]]}, {"id": "2101.04013", "submitter": "Tingyi Wanyan", "authors": "Tingyi Wanyan, Hossein Honarvar, Suraj K. Jaladanki, Chengxi Zang,\n  Nidhi Naik, Sulaiman Somani, Jessica K. De Freitas, Ishan Paranjpe, Akhil\n  Vaid, Riccardo Miotto, Girish N. Nadkarni, Marinka Zitnik, ArifulAzad, Fei\n  Wang, Ying Ding, Benjamin S. Glicksberg", "title": "Contrastive Learning Improves Critical Event Prediction in COVID-19\n  Patients", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Machine Learning (ML) models typically require large-scale, balanced training\ndata to be robust, generalizable, and effective in the context of healthcare.\nThis has been a major issue for developing ML models for the\ncoronavirus-disease 2019 (COVID-19) pandemic where data is highly imbalanced,\nparticularly within electronic health records (EHR) research. Conventional\napproaches in ML use cross-entropy loss (CEL) that often suffers from poor\nmargin classification. For the first time, we show that contrastive loss (CL)\nimproves the performance of CEL especially for imbalanced EHR data and the\nrelated COVID-19 analyses. This study has been approved by the Institutional\nReview Board at the Icahn School of Medicine at Mount Sinai. We use EHR data\nfrom five hospitals within the Mount Sinai Health System (MSHS) to predict\nmortality, intubation, and intensive care unit (ICU) transfer in hospitalized\nCOVID-19 patients over 24 and 48 hour time windows. We train two sequential\narchitectures (RNN and RETAIN) using two loss functions (CEL and CL). Models\nare tested on full sample data set which contain all available data and\nrestricted data set to emulate higher class imbalance.CL models consistently\noutperform CEL models with the restricted data set on these tasks with\ndifferences ranging from 0.04 to 0.15 for AUPRC and 0.05 to 0.1 for AUROC. For\nthe restricted sample, only the CL model maintains proper clustering and is\nable to identify important features, such as pulse oximetry. CL outperforms CEL\nin instances of severe class imbalance, on three EHR outcomes with respect to\nthree performance metrics: predictive power, clustering, and feature\nimportance. We believe that the developed CL framework can be expanded and used\nfor EHR ML work in general.\n", "versions": [{"version": "v1", "created": "Mon, 11 Jan 2021 16:41:13 GMT"}], "update_date": "2021-01-12", "authors_parsed": [["Wanyan", "Tingyi", ""], ["Honarvar", "Hossein", ""], ["Jaladanki", "Suraj K.", ""], ["Zang", "Chengxi", ""], ["Naik", "Nidhi", ""], ["Somani", "Sulaiman", ""], ["De Freitas", "Jessica K.", ""], ["Paranjpe", "Ishan", ""], ["Vaid", "Akhil", ""], ["Miotto", "Riccardo", ""], ["Nadkarni", "Girish N.", ""], ["Zitnik", "Marinka", ""], ["ArifulAzad", "", ""], ["Wang", "Fei", ""], ["Ding", "Ying", ""], ["Glicksberg", "Benjamin S.", ""]]}, {"id": "2101.04025", "submitter": "Malte S. Kurz", "authors": "Malte S. Kurz", "title": "Distributed Double Machine Learning with a Serverless Architecture", "comments": null, "journal-ref": "In Companion of the ACM/SPEC International Conference on\n  Performance Engineering (ICPE '21), 2021, Association for Computing\n  Machinery, New York, NY, USA, 27-33", "doi": "10.1145/3447545.3451181", "report-no": null, "categories": "cs.DC cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper explores serverless cloud computing for double machine learning.\nBeing based on repeated cross-fitting, double machine learning is particularly\nwell suited to exploit the high level of parallelism achievable with serverless\ncomputing. It allows to get fast on-demand estimations without additional cloud\nmaintenance effort. We provide a prototype Python implementation\n\\texttt{DoubleML-Serverless} for the estimation of double machine learning\nmodels with the serverless computing platform AWS Lambda and demonstrate its\nutility with a case study analyzing estimation times and costs.\n", "versions": [{"version": "v1", "created": "Mon, 11 Jan 2021 16:58:30 GMT"}, {"version": "v2", "created": "Wed, 24 Feb 2021 12:13:03 GMT"}], "update_date": "2021-04-21", "authors_parsed": [["Kurz", "Malte S.", ""]]}, {"id": "2101.04035", "submitter": "Jesse Josua Benjamin", "authors": "Jesse Josua Benjamin, Arne Berger, Nick Merrill, James Pierce", "title": "Machine Learning Uncertainty as a Design Material: A\n  Post-Phenomenological Inquiry", "comments": "Accepted to ACM 2021 CHI Conference on Human Factors in Computing\n  Systems (CHI 2021)", "journal-ref": null, "doi": "10.1145/3411764.3445481", "report-no": null, "categories": "cs.HC cs.CY cs.LG", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Design research is important for understanding and interrogating how emerging\ntechnologies shape human experience. However, design research with Machine\nLearning (ML) is relatively underdeveloped. Crucially, designers have not found\na grasp on ML uncertainty as a design opportunity rather than an obstacle. The\ntechnical literature points to data and model uncertainties as two main\nproperties of ML. Through post-phenomenology, we position uncertainty as one\ndefining material attribute of ML processes which mediate human experience. To\nunderstand ML uncertainty as a design material, we investigate four design\nresearch case studies involving ML. We derive three provocative concepts:\nthingly uncertainty: ML-driven artefacts have uncertain, variable relations to\ntheir environments; pattern leakage: ML uncertainty can lead to patterns\nshaping the world they are meant to represent; and futures creep: ML\ntechnologies texture human relations to time with uncertainty. Finally, we\noutline design research trajectories and sketch a post-phenomenological\napproach to human-ML relations.\n", "versions": [{"version": "v1", "created": "Mon, 11 Jan 2021 17:11:19 GMT"}], "update_date": "2021-01-12", "authors_parsed": [["Benjamin", "Jesse Josua", ""], ["Berger", "Arne", ""], ["Merrill", "Nick", ""], ["Pierce", "James", ""]]}, {"id": "2101.04041", "submitter": "Rapha\\\"el Dang-Nhu", "authors": "Rapha\\\"el Dang-Nhu and Angelika Steger", "title": "Evaluating Disentanglement of Structured Latent Representations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We design the first multi-layer disentanglement metric operating at all\nhierarchy levels of a structured latent representation, and derive its\ntheoretical properties. Applied to object-centric representations, our metric\nunifies the evaluation of both object separation between latent slots and\ninternal slot disentanglement into a common mathematical framework. It also\naddresses the problematic dependence on segmentation mask sharpness of previous\npixel-level segmentation metrics such as ARI. Perhaps surprisingly, our\nexperimental results show that good ARI values do not guarantee a disentangled\nrepresentation, and that the exclusive focus on this metric has led to\ncounterproductive choices in some previous evaluations. As an additional\ntechnical contribution, we present a new algorithm for obtaining feature\nimportances that handles slot permutation invariance in the representation.\n", "versions": [{"version": "v1", "created": "Mon, 11 Jan 2021 17:24:01 GMT"}], "update_date": "2021-01-12", "authors_parsed": [["Dang-Nhu", "Rapha\u00ebl", ""], ["Steger", "Angelika", ""]]}, {"id": "2101.04047", "submitter": "Linda Boedi", "authors": "Linda H. Boedi and Helmut Grabner", "title": "Learning to Ignore: Fair and Task Independent Representations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Training fair machine learning models, aiming for their interpretability and\nsolving the problem of domain shift has gained a lot of interest in the last\nyears. There is a vast amount of work addressing these topics, mostly in\nseparation. In this work we show that they can be seen as a common framework of\nlearning invariant representations. The representations should allow to predict\nthe target while at the same time being invariant to sensitive attributes which\nsplit the dataset into subgroups. Our approach is based on the simple\nobservation that it is impossible for any learning algorithm to differentiate\nsamples if they have the same feature representation. This is formulated as an\nadditional loss (regularizer) enforcing a common feature representation across\nsubgroups. We apply it to learn fair models and interpret the influence of the\nsensitive attribute. Furthermore it can be used for domain adaptation,\ntransferring knowledge and learning effectively from very few examples. In all\napplications it is essential not only to learn to predict the target, but also\nto learn what to ignore.\n", "versions": [{"version": "v1", "created": "Mon, 11 Jan 2021 17:33:18 GMT"}, {"version": "v2", "created": "Thu, 21 Jan 2021 09:01:04 GMT"}], "update_date": "2021-01-22", "authors_parsed": [["Boedi", "Linda H.", ""], ["Grabner", "Helmut", ""]]}, {"id": "2101.04053", "submitter": "Tomer Meirman", "authors": "Tomer Meirman, Roni Stern, Gilad Katz", "title": "Anomaly Detection for Aggregated Data Using Multi-Graph Autoencoder", "comments": "Mandatory changes for the thesis, post thesis-defense presentation.\n  Changing of title, fixing grammar and vocabulary mistakes, removing\n  unnecessary parts", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In data systems, activities or events are continuously collected in the field\nto trace their proper executions. Logging, which means recording sequences of\nevents, can be used for analyzing system failures and malfunctions, and\nidentifying the causes and locations of such issues. In our research we focus\non creating an Anomaly detection models for system logs. The task of anomaly\ndetection is identifying unexpected events in dataset, which differ from the\nnormal behavior. Anomaly detection models also assist in data systems analysis\ntasks.\n  Modern systems may produce such a large amount of events monitoring every\nindividual event is not feasible. In such cases, the events are often\naggregated over a fixed period of time, reporting the number of times every\nevent has occurred in that time period. This aggregation facilitates scaling,\nbut requires a different approach for anomaly detection. In this research, we\npresent a thorough analysis of the aggregated data and the relationships\nbetween aggregated events. Based on the initial phase of our research we\npresent graphs representations of our aggregated dataset, which represent the\ndifferent relationships between aggregated instances in the same context.\n  Using the graph representation, we propose Multiple-graphs autoencoder MGAE,\na novel convolutional graphs-autoencoder model which exploits the relationships\nof the aggregated instances in our unique dataset. MGAE outperforms standard\ngraph-autoencoder models and the different experiments. With our novel MGAE we\npresent 60% decrease in reconstruction error in comparison to standard graph\nautoencoder, which is expressed in reconstructing high-degree relationships.\n", "versions": [{"version": "v1", "created": "Mon, 11 Jan 2021 17:38:42 GMT"}, {"version": "v2", "created": "Mon, 10 May 2021 08:37:39 GMT"}, {"version": "v3", "created": "Sun, 27 Jun 2021 09:41:00 GMT"}], "update_date": "2021-06-29", "authors_parsed": [["Meirman", "Tomer", ""], ["Stern", "Roni", ""], ["Katz", "Gilad", ""]]}, {"id": "2101.04073", "submitter": "Anush Sankaran", "authors": "Anush Sankaran, Olivier Mastropietro, Ehsan Saboori, Yasser Idris,\n  Davis Sawyer, MohammadHossein AskariHemmat, Ghouthi Boukli Hacene", "title": "Deeplite Neutrino: An End-to-End Framework for Constrained Deep Learning\n  Model Optimization", "comments": "\"IAAI Deployed Application Award\", IAAI 2021 @ AAAI 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Designing deep learning-based solutions is becoming a race for training\ndeeper models with a greater number of layers. While a large-size deeper model\ncould provide competitive accuracy, it creates a lot of logistical challenges\nand unreasonable resource requirements during development and deployment. This\nhas been one of the key reasons for deep learning models not being excessively\nused in various production environments, especially in edge devices. There is\nan immediate requirement for optimizing and compressing these deep learning\nmodels, to enable on-device intelligence. In this research, we introduce a\nblack-box framework, Deeplite Neutrino for production-ready optimization of\ndeep learning models. The framework provides an easy mechanism for the\nend-users to provide constraints such as a tolerable drop in accuracy or target\nsize of the optimized models, to guide the whole optimization process. The\nframework is easy to include in an existing production pipeline and is\navailable as a Python Package, supporting PyTorch and Tensorflow libraries. The\noptimization performance of the framework is shown across multiple benchmark\ndatasets and popular deep learning models. Further, the framework is currently\nused in production and the results and testimonials from several clients are\nsummarized.\n", "versions": [{"version": "v1", "created": "Mon, 11 Jan 2021 18:07:45 GMT"}, {"version": "v2", "created": "Wed, 13 Jan 2021 14:57:12 GMT"}], "update_date": "2021-01-14", "authors_parsed": [["Sankaran", "Anush", ""], ["Mastropietro", "Olivier", ""], ["Saboori", "Ehsan", ""], ["Idris", "Yasser", ""], ["Sawyer", "Davis", ""], ["AskariHemmat", "MohammadHossein", ""], ["Hacene", "Ghouthi Boukli", ""]]}, {"id": "2101.04076", "submitter": "Melanie Laffin", "authors": "Shwetha Bharadwaj, Melanie Laffin", "title": "Automating the Compilation of Potential Core-Outcomes for Clinical\n  Trials", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Due to increased access to clinical trial outcomes and analysis, researchers\nand scientists are able to iterate or improve upon relevant approaches more\neffectively. However, the metrics and related results of clinical trials\ntypically do not follow any standardization in their reports, making it more\ndifficult for researchers to parse the results of different trials. The\nobjective of this paper is to describe an automated method utilizing natural\nlanguage processing in order to describe the probable core outcomes of clinical\ntrials, in order to alleviate the issues around disparate clinical trial\noutcomes. As the nature of this process is domain specific, BioBERT was\nemployed in order to conduct a multi-class entity normalization task. In\naddition to BioBERT, an unsupervised feature-based approach making use of only\nthe encoder output embedding representations for the outcomes and labels was\nutilized. Finally, cosine similarity was calculated across the vectors to\nobtain the semantic similarity. This method was able to both harness the\ndomain-specific context of each of the tokens from the learned embeddings of\nthe BioBERT model as well as a more stable metric of sentence similarity. Some\ncommon outcomes identified using the Jaccard similarity in each of the\nclassifications were compiled, and while some are untenable, a pipeline for\nwhich this automation process could be conducted was established.\n", "versions": [{"version": "v1", "created": "Mon, 11 Jan 2021 18:14:49 GMT"}], "update_date": "2021-01-12", "authors_parsed": [["Bharadwaj", "Shwetha", ""], ["Laffin", "Melanie", ""]]}, {"id": "2101.04086", "submitter": "An Nguyen", "authors": "An Nguyen, Stefan Foerstel, Thomas Kittler, Andrey Kurzyukov, Leo\n  Schwinn, Dario Zanca, Tobias Hipp, Da Jun Sun, Michael Schrapp, Eva Rothgang,\n  Bjoern Eskofier", "title": "System Design for a Data-driven and Explainable Customer Sentiment\n  Monitor", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The most important goal of customer services is to keep the customer\nsatisfied. However, service resources are always limited and must be\nprioritized. Therefore, it is important to identify customers who potentially\nbecome unsatisfied and might lead to escalations. Today this prioritization of\ncustomers is often done manually. Data science on IoT data (esp. log data) for\nmachine health monitoring, as well as analytics on enterprise data for customer\nrelationship management (CRM) have mainly been researched and applied\nindependently. In this paper, we present a framework for a data-driven decision\nsupport system which combines IoT and enterprise data to model customer\nsentiment. Such decision support systems can help to prioritize customers and\nservice resources to effectively troubleshoot problems or even avoid them. The\nframework is applied in a real-world case study with a major medical device\nmanufacturer. This includes a fully automated and interpretable machine\nlearning pipeline designed to meet the requirements defined with domain experts\nand end users. The overall framework is currently deployed, learns and\nevaluates predictive models from terabytes of IoT and enterprise data to\nactively monitor the customer sentiment for a fleet of thousands of high-end\nmedical devices. Furthermore, we provide an anonymized industrial benchmark\ndataset for the research community.\n", "versions": [{"version": "v1", "created": "Mon, 11 Jan 2021 18:29:50 GMT"}], "update_date": "2021-01-12", "authors_parsed": [["Nguyen", "An", ""], ["Foerstel", "Stefan", ""], ["Kittler", "Thomas", ""], ["Kurzyukov", "Andrey", ""], ["Schwinn", "Leo", ""], ["Zanca", "Dario", ""], ["Hipp", "Tobias", ""], ["Sun", "Da Jun", ""], ["Schrapp", "Michael", ""], ["Rothgang", "Eva", ""], ["Eskofier", "Bjoern", ""]]}, {"id": "2101.04097", "submitter": "Adri\\`a Garriga-Alonso", "authors": "Adri\\`a Garriga-Alonso, Mark van der Wilk", "title": "Correlated Weights in Infinite Limits of Deep Convolutional Neural\n  Networks", "comments": "Accepted for the 37th Conference on Uncertainty in Artificial\n  Intelligence (UAI 2021)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Infinite width limits of deep neural networks often have tractable forms.\nThey have been used to analyse the behaviour of finite networks, as well as\nbeing useful methods in their own right. When investigating infinitely wide\nconvolutional neural networks (CNNs), it was observed that the correlations\narising from spatial weight sharing disappear in the infinite limit. This is\nundesirable, as spatial correlation is the main motivation behind CNNs. We show\nthat the loss of this property is not a consequence of the infinite limit, but\nrather of choosing an independent weight prior. Correlating the weights\nmaintains the correlations in the activations. Varying the amount of\ncorrelation interpolates between independent-weight limits and mean-pooling.\nEmpirical evaluation of the infinitely wide network shows that optimal\nperformance is achieved between the extremes, indicating that correlations can\nbe useful.\n", "versions": [{"version": "v1", "created": "Mon, 11 Jan 2021 18:43:40 GMT"}, {"version": "v2", "created": "Sun, 13 Jun 2021 12:16:59 GMT"}], "update_date": "2021-06-15", "authors_parsed": [["Garriga-Alonso", "Adri\u00e0", ""], ["van der Wilk", "Mark", ""]]}, {"id": "2101.04108", "submitter": "Umang Gupta", "authors": "Umang Gupta and Aaron M Ferber and Bistra Dilkina and Greg Ver Steeg", "title": "Controllable Guarantees for Fair Outcomes via Contrastive Information\n  Estimation", "comments": "This version fixes an error in Theorem 2 of the original manuscript\n  that appeared at the Proceedings of the 35th AAAI Conference on Artificial\n  Intelligence (AAAI-21). Code is available at\n  https://github.com/umgupta/fairness-via-contrastive-estimation", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Controlling bias in training datasets is vital for ensuring equal treatment,\nor parity, between different groups in downstream applications. A naive\nsolution is to transform the data so that it is statistically independent of\ngroup membership, but this may throw away too much information when a\nreasonable compromise between fairness and accuracy is desired. Another common\napproach is to limit the ability of a particular adversary who seeks to\nmaximize parity. Unfortunately, representations produced by adversarial\napproaches may still retain biases as their efficacy is tied to the complexity\nof the adversary used during training. To this end, we theoretically establish\nthat by limiting the mutual information between representations and protected\nattributes, we can assuredly control the parity of any downstream classifier.\nWe demonstrate an effective method for controlling parity through mutual\ninformation based on contrastive information estimators and show that they\noutperform approaches that rely on variational bounds based on complex\ngenerative models. We test our approach on UCI Adult and Heritage Health\ndatasets and demonstrate that our approach provides more informative\nrepresentations across a range of desired parity thresholds while providing\nstrong theoretical guarantees on the parity of any downstream algorithm.\n", "versions": [{"version": "v1", "created": "Mon, 11 Jan 2021 18:57:33 GMT"}, {"version": "v2", "created": "Thu, 8 Apr 2021 17:54:43 GMT"}, {"version": "v3", "created": "Thu, 3 Jun 2021 17:21:06 GMT"}], "update_date": "2021-06-04", "authors_parsed": [["Gupta", "Umang", ""], ["Ferber", "Aaron M", ""], ["Dilkina", "Bistra", ""], ["Steeg", "Greg Ver", ""]]}, {"id": "2101.04109", "submitter": "Zijian Zhang", "authors": "Zijian Zhang, Koustav Rudra, Avishek Anand", "title": "Explain and Predict, and then Predict Again", "comments": "Accepted in the WSDM 2021", "journal-ref": null, "doi": "10.1145/3437963.3441758", "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  A desirable property of learning systems is to be both effective and\ninterpretable. Towards this goal, recent models have been proposed that first\ngenerate an extractive explanation from the input text and then generate a\nprediction on just the explanation called explain-then-predict models. These\nmodels primarily consider the task input as a supervision signal in learning an\nextractive explanation and do not effectively integrate rationales data as an\nadditional inductive bias to improve task performance. We propose a novel yet\nsimple approach ExPred, that uses multi-task learning in the explanation\ngeneration phase effectively trading-off explanation and prediction losses. And\nthen we use another prediction network on just the extracted explanations for\noptimizing the task performance. We conduct an extensive evaluation of our\napproach on three diverse language datasets -- fact verification, sentiment\nclassification, and QA -- and find that we substantially outperform existing\napproaches.\n", "versions": [{"version": "v1", "created": "Mon, 11 Jan 2021 19:36:52 GMT"}, {"version": "v2", "created": "Thu, 4 Feb 2021 05:19:23 GMT"}], "update_date": "2021-02-05", "authors_parsed": [["Zhang", "Zijian", ""], ["Rudra", "Koustav", ""], ["Anand", "Avishek", ""]]}, {"id": "2101.04117", "submitter": "Miles Cranmer", "authors": "Miles Cranmer, Daniel Tamayo, Hanno Rein, Peter Battaglia, Samuel\n  Hadden, Philip J. Armitage, Shirley Ho, David N. Spergel", "title": "A Bayesian neural network predicts the dissolution of compact planetary\n  systems", "comments": "8 content pages, 7 appendix and references. 8 figures. Source code\n  at: https://github.com/MilesCranmer/bnn_chaos_model inference code at\n  https://github.com/dtamayo/spock", "journal-ref": null, "doi": null, "report-no": null, "categories": "astro-ph.EP astro-ph.IM cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite over three hundred years of effort, no solutions exist for predicting\nwhen a general planetary configuration will become unstable. We introduce a\ndeep learning architecture to push forward this problem for compact systems.\nWhile current machine learning algorithms in this area rely on\nscientist-derived instability metrics, our new technique learns its own metrics\nfrom scratch, enabled by a novel internal structure inspired from dynamics\ntheory. Our Bayesian neural network model can accurately predict not only if,\nbut also when a compact planetary system with three or more planets will go\nunstable. Our model, trained directly from short N-body time series of raw\norbital elements, is more than two orders of magnitude more accurate at\npredicting instability times than analytical estimators, while also reducing\nthe bias of existing machine learning algorithms by nearly a factor of three.\nDespite being trained on compact resonant and near-resonant three-planet\nconfigurations, the model demonstrates robust generalization to both\nnon-resonant and higher multiplicity configurations, in the latter case\noutperforming models fit to that specific set of integrations. The model\ncomputes instability estimates up to five orders of magnitude faster than a\nnumerical integrator, and unlike previous efforts provides confidence intervals\non its predictions. Our inference model is publicly available in the SPOCK\npackage, with training code open-sourced.\n", "versions": [{"version": "v1", "created": "Mon, 11 Jan 2021 19:00:00 GMT"}], "update_date": "2021-01-21", "authors_parsed": [["Cranmer", "Miles", ""], ["Tamayo", "Daniel", ""], ["Rein", "Hanno", ""], ["Battaglia", "Peter", ""], ["Hadden", "Samuel", ""], ["Armitage", "Philip J.", ""], ["Ho", "Shirley", ""], ["Spergel", "David N.", ""]]}, {"id": "2101.04141", "submitter": "Henrik Hoeiness", "authors": "Henrik Hoeiness and Axel Harstad and Gerald Friedland", "title": "From Tinkering to Engineering: Measurements in Tensorflow Playground", "comments": "3 pages, 3 figures, ICPR 2020", "journal-ref": "Peer-reviewed by and presented at ICPR 2020", "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this article, we present an extension of the Tensorflow Playground, called\nTensorflow Meter (short TFMeter). TFMeter is an interactive neural network\narchitecting tool that allows the visual creation of different architectures of\nneural networks. In addition to its ancestor, the playground, our tool shows\ninformation-theoretic measurements while constructing, training, and testing\nthe network. As a result, each change results in a change in at least one of\nthe measurements, providing for a better engineering intuition of what\ndifferent architectures are able to learn. The measurements are derived from\nvarious places in the literature. In this demo, we describe our web application\nthat is available online at http://tfmeter.icsi.berkeley.edu/ and argue that in\nthe same way that the original Playground is meant to build an intuition about\nneural networks, our extension educates users on available measurements, which\nwe hope will ultimately improve experimental design and reproducibility in the\nfield.\n", "versions": [{"version": "v1", "created": "Mon, 11 Jan 2021 19:06:44 GMT"}], "update_date": "2021-01-14", "authors_parsed": [["Hoeiness", "Henrik", ""], ["Harstad", "Axel", ""], ["Friedland", "Gerald", ""]]}, {"id": "2101.04144", "submitter": "Raviraj Joshi", "authors": "Ramchandra Joshi, Rushabh Karnavat, Kaustubh Jirapure, Raviraj Joshi", "title": "Evaluation of Deep Learning Models for Hostility Detection in Hindi Text", "comments": "Accepted at IEEE I2CT 2021", "journal-ref": null, "doi": "10.1109/I2CT51068.2021.9418073", "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The social media platform is a convenient medium to express personal thoughts\nand share useful information. It is fast, concise, and has the ability to reach\nmillions. It is an effective place to archive thoughts, share artistic content,\nreceive feedback, promote products, etc. Despite having numerous advantages\nthese platforms have given a boost to hostile posts. Hate speech and derogatory\nremarks are being posted for personal satisfaction or political gain. The\nhostile posts can have a bullying effect rendering the entire platform\nexperience hostile. Therefore detection of hostile posts is important to\nmaintain social media hygiene. The problem is more pronounced languages like\nHindi which are low in resources. In this work, we present approaches for\nhostile text detection in the Hindi language. The proposed approaches are\nevaluated on the Constraint@AAAI 2021 Hindi hostility detection dataset. The\ndataset consists of hostile and non-hostile texts collected from social media\nplatforms. The hostile posts are further segregated into overlapping classes of\nfake, offensive, hate, and defamation. We evaluate a host of deep learning\napproaches based on CNN, LSTM, and BERT for this multi-label classification\nproblem. The pre-trained Hindi fast text word embeddings by IndicNLP and\nFacebook are used in conjunction with CNN and LSTM models. Two variations of\npre-trained multilingual transformer language models mBERT and IndicBERT are\nused. We show that the performance of BERT based models is best. Moreover, CNN\nand LSTM models also perform competitively with BERT based models.\n", "versions": [{"version": "v1", "created": "Mon, 11 Jan 2021 19:10:57 GMT"}, {"version": "v2", "created": "Wed, 13 Jan 2021 14:25:06 GMT"}, {"version": "v3", "created": "Tue, 9 Mar 2021 16:27:40 GMT"}, {"version": "v4", "created": "Wed, 7 Apr 2021 06:44:47 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Joshi", "Ramchandra", ""], ["Karnavat", "Rushabh", ""], ["Jirapure", "Kaustubh", ""], ["Joshi", "Raviraj", ""]]}, {"id": "2101.04163", "submitter": "Yipeng Zhou", "authors": "Yao Fu, Yipeng Zhou, Di Wu, Shui Yu, Yonggang Wen, Chao Li", "title": "On the Practicality of Differential Privacy in Federated Learning by\n  Tuning Iteration Times", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.AI cs.DC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In spite that Federated Learning (FL) is well known for its privacy\nprotection when training machine learning models among distributed clients\ncollaboratively, recent studies have pointed out that the naive FL is\nsusceptible to gradient leakage attacks. In the meanwhile, Differential Privacy\n(DP) emerges as a promising countermeasure to defend against gradient leakage\nattacks. However, the adoption of DP by clients in FL may significantly\njeopardize the model accuracy. It is still an open problem to understand the\npracticality of DP from a theoretic perspective. In this paper, we make the\nfirst attempt to understand the practicality of DP in FL through tuning the\nnumber of conducted iterations. Based on the FedAvg algorithm, we formally\nderive the convergence rate with DP noises in FL. Then, we theoretically\nderive: 1) the conditions for the DP based FedAvg to converge as the number of\nglobal iterations (GI) approaches infinity; 2) the method to set the number of\nlocal iterations (LI) to minimize the negative influence of DP noises. By\nfurther substituting the Laplace and Gaussian mechanisms into the derived\nconvergence rate respectively, we show that: 3) The DP based FedAvg with the\nLaplace mechanism cannot converge, but the divergence rate can be effectively\nprohibited by setting the number of LIs with our method; 4) The learning error\nof the DP based FedAvg with the Gaussian mechanism can converge to a constant\nnumber finally if we use a fixed number of LIs per GI. To verify our\ntheoretical findings, we conduct extensive experiments using two real-world\ndatasets. The results not only validate our analysis results, but also provide\nuseful guidelines on how to optimize model accuracy when incorporating DP into\nFL\n", "versions": [{"version": "v1", "created": "Mon, 11 Jan 2021 19:43:12 GMT"}], "update_date": "2021-01-13", "authors_parsed": [["Fu", "Yao", ""], ["Zhou", "Yipeng", ""], ["Wu", "Di", ""], ["Yu", "Shui", ""], ["Wen", "Yonggang", ""], ["Li", "Chao", ""]]}, {"id": "2101.04167", "submitter": "Ruiyang Xu", "authors": "Ruiyang Xu, Prashank Kadam, Karl Lieberherr", "title": "First-Order Problem Solving through Neural MCTS based Reinforcement\n  Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The formal semantics of an interpreted first-order logic (FOL) statement can\nbe given in Tarskian Semantics or a basically equivalent Game Semantics. The\nlatter maps the statement and the interpretation into a two-player semantic\ngame. Many combinatorial problems can be described using interpreted FOL\nstatements and can be mapped into a semantic game. Therefore, learning to play\na semantic game perfectly leads to the solution of a specific instance of a\ncombinatorial problem. We adapt the AlphaZero algorithm so that it becomes\nbetter at learning to play semantic games that have different characteristics\nthan Go and Chess. We propose a general framework, Persephone, to map the FOL\ndescription of a combinatorial problem to a semantic game so that it can be\nsolved through a neural MCTS based reinforcement learning algorithm. Our goal\nfor Persephone is to make it tabula-rasa, mapping a problem stated in\ninterpreted FOL to a solution without human intervention.\n", "versions": [{"version": "v1", "created": "Mon, 11 Jan 2021 19:54:06 GMT"}], "update_date": "2021-01-13", "authors_parsed": [["Xu", "Ruiyang", ""], ["Kadam", "Prashank", ""], ["Lieberherr", "Karl", ""]]}, {"id": "2101.04176", "submitter": "Michela Meister", "authors": "Michela Meister and Sloan Nietert", "title": "Learning with Comparison Feedback: Online Estimation of Sample\n  Statistics", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study an online version of the noisy binary search problem where feedback\nis generated by a non-stochastic adversary rather than perturbed by random\nnoise. We reframe this as maintaining an accurate estimate for the median of an\nadversarial sequence of integers, $x_1, x_2, \\dots$, in a model where each\nnumber $x_t$ can only be accessed through a single threshold query of the form\n${1(x_t \\leq q_t)}$. In this online comparison feedback model, we explore\nestimation of general sample statistics, providing robust algorithms for\nmedian, CDF, and mean estimation with nearly matching lower bounds. We conclude\nwith several high-dimensional generalizations.\n", "versions": [{"version": "v1", "created": "Mon, 11 Jan 2021 20:28:32 GMT"}], "update_date": "2021-01-13", "authors_parsed": [["Meister", "Michela", ""], ["Nietert", "Sloan", ""]]}, {"id": "2101.04178", "submitter": "Ondrej Biza", "authors": "Ondrej Biza, Dian Wang, Robert Platt, Jan-Willem van de Meent and\n  Lawson L. S. Wong", "title": "Action Priors for Large Action Spaces in Robotics", "comments": "13 pages, 9 figures", "journal-ref": "Proceedings of the 20th International Conference on Autonomous\n  Agents and MultiAgent Systems (AAMAS '21). 2021. 205 - 213", "doi": null, "report-no": null, "categories": "cs.RO cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In robotics, it is often not possible to learn useful policies using pure\nmodel-free reinforcement learning without significant reward shaping or\ncurriculum learning. As a consequence, many researchers rely on expert\ndemonstrations to guide learning. However, acquiring expert demonstrations can\nbe expensive. This paper proposes an alternative approach where the solutions\nof previously solved tasks are used to produce an action prior that can\nfacilitate exploration in future tasks. The action prior is a probability\ndistribution over actions that summarizes the set of policies found solving\nprevious tasks. Our results indicate that this approach can be used to solve\nrobotic manipulation problems that would otherwise be infeasible without expert\ndemonstrations. Source code is available at\n\\url{https://github.com/ondrejba/action_priors}.\n", "versions": [{"version": "v1", "created": "Mon, 11 Jan 2021 20:30:54 GMT"}, {"version": "v2", "created": "Mon, 15 Feb 2021 23:45:25 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Biza", "Ondrej", ""], ["Wang", "Dian", ""], ["Platt", "Robert", ""], ["van de Meent", "Jan-Willem", ""], ["Wong", "Lawson L. S.", ""]]}, {"id": "2101.04185", "submitter": "Ariel Keller Rorabaugh", "authors": "Ariel Keller Rorabaugh (1), Silvina Ca\\'ino-Lores (1), Michael R.\n  Wyatt II (1), Travis Johnston (2), Michela Taufer (1) ((1) University of\n  Tennessee, Knoxville, USA, (2) Oak Ridge National Lab, Oak Ridge, USA)", "title": "PEng4NN: An Accurate Performance Estimation Engine for Efficient\n  Automated Neural Network Architecture Search", "comments": "10 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural network (NN) models are increasingly used in scientific simulations,\nAI, and other high performance computing (HPC) fields to extract knowledge from\ndatasets. Each dataset requires tailored NN model architecture, but designing\nstructures by hand is a time-consuming and error-prone process. Neural\narchitecture search (NAS) automates the design of NN architectures. NAS\nattempts to find well-performing NN models for specialized datsets, where\nperformance is measured by key metrics that capture the NN capabilities (e.g.,\naccuracy of classification of samples in a dataset). Existing NAS methods are\nresource intensive, especially when searching for highly accurate models for\nlarger and larger datasets.\n  To address this problem, we propose a performance estimation strategy that\nreduces the resources for training NNs and increases NAS throughput without\njeopardizing accuracy. We implement our strategy via an engine called PEng4NN\nthat plugs into existing NAS methods; in doing so, PEng4NN predicts the final\naccuracy of NNs early in the training process, informs the NAS of NN\nperformance, and thus enables the NAS to terminate training NNs early. We\nassess our engine on three diverse datasets (i.e., CIFAR-100, Fashion MNIST,\nand SVHN). By reducing the training epochs needed, our engine achieves\nsubstantial throughput gain; on average, our engine saves $61\\%$ to $82\\%$ of\ntraining epochs, increasing throughput by a factor of 2.5 to 5 compared to a\nstate-of-the-art NAS method. We achieve this gain without compromising\naccuracy, as we demonstrate with two key outcomes. First, across all our tests,\nbetween $74\\%$ and $97\\%$ of the ground truth best models lie in our set of\npredicted best models. Second, the accuracy distributions of the ground truth\nbest models and our predicted best models are comparable, with the mean\naccuracy values differing by at most .7 percentage points across all tests.\n", "versions": [{"version": "v1", "created": "Mon, 11 Jan 2021 20:49:55 GMT"}], "update_date": "2021-01-13", "authors_parsed": [["Rorabaugh", "Ariel Keller", ""], ["Ca\u00edno-Lores", "Silvina", ""], ["Wyatt", "Michael R.", "II"], ["Johnston", "Travis", ""], ["Taufer", "Michela", ""]]}, {"id": "2101.04199", "submitter": "Behzad Javaheri", "authors": "Behzad Javaheri", "title": "Where you live matters: a spatial analysis of COVID-19 mortality", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The COVID-19 pandemic has caused ~ 2 million fatalities. Significant progress\nhas been made in advancing our understanding of the disease process, one of the\nunanswered questions, however, is the anomaly in the case/mortality ratio with\nMexico as a clear example. Herein, this anomaly is explored by spatial analysis\nand whether mortality varies locally according to local factors. To address\nthis, hexagonal cartogram maps (hexbin) used to spatially map COVID-19\nmortality and visualise association with patient-level data on demographics and\npre-existing health conditions. This was further interrogated at local Mexico\nCity level by choropleth mapping. Our data show that the use of hexagonal\ncartograms is a better approach for spatial mapping of COVID-19 data in Mexico\nas it addresses bias in area size and population. We report sex/age-related\nspatial relationship with mortality amongst the Mexican states and a trend\nbetween health conditions and mortality at the state level. Within Mexico City,\nthere is a clear south, north divide with higher mortality in the northern\nmunicipalities. Deceased patients in these northern municipalities have the\nhighest pre-existing health conditions. Taken together, this study provides an\nimproved presentation of COVID-19 mapping in Mexico and demonstrates spatial\ndivergence of the mortality in Mexico.\n", "versions": [{"version": "v1", "created": "Mon, 11 Jan 2021 21:25:13 GMT"}], "update_date": "2021-01-13", "authors_parsed": [["Javaheri", "Behzad", ""]]}, {"id": "2101.04200", "submitter": "Maged Eljazzar", "authors": "Ali M. Alagrami, Maged M. Eljazzar", "title": "Smartajweed Automatic Recognition of Arabic Quranic Recitation Rules", "comments": "8 pages, the paper already published in airccse", "journal-ref": "airccse 2020", "doi": null, "report-no": null, "categories": "eess.AS cs.CL cs.LG cs.SD", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Tajweed is a set of rules to read the Quran in a correct Pronunciation of the\nletters with all its Qualities, while Reciting the Quran. which means you have\nto give every letter in the Quran its due of characteristics and apply it to\nthis particular letter in this specific situation while reading, which may\ndiffer in other times. These characteristics include melodic rules, like where\nto stop and for how long, when to merge two letters in pronunciation or when to\nstretch some, or even when to put more strength on some letters over other.\nMost of the papers focus mainly on the main recitation rules and the\npronunciation but not (Ahkam AL Tajweed) which give different rhythm and\ndifferent melody to the pronunciation with every different rule of (Tajweed).\nWhich is also considered very important and essential in Reading the Quran as\nit can give different meanings to the words. In this paper we discuss in detail\nfull system for automatic recognition of Quran Recitation Rules (Tajweed) by\nusing support vector machine and threshold scoring system\n", "versions": [{"version": "v1", "created": "Sat, 26 Dec 2020 11:24:03 GMT"}], "update_date": "2021-01-13", "authors_parsed": [["Alagrami", "Ali M.", ""], ["Eljazzar", "Maged M.", ""]]}, {"id": "2101.04209", "submitter": "Yue Zhao", "authors": "Yue Zhao, Zhi Qiao, Cao Xiao, Lucas Glass, Jimeng Sun", "title": "PyHealth: A Python Library for Health Predictive Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Despite the explosion of interest in healthcare AI research, the\nreproducibility and benchmarking of those research works are often limited due\nto the lack of standard benchmark datasets and diverse evaluation metrics. To\naddress this reproducibility challenge, we develop PyHealth, an open-source\nPython toolbox for developing various predictive models on healthcare data.\n  PyHealth consists of data preprocessing module, predictive modeling module,\nand evaluation module. The target users of PyHealth are both computer science\nresearchers and healthcare data scientists. With PyHealth, they can conduct\ncomplex machine learning pipelines on healthcare datasets with fewer than ten\nlines of code. The data preprocessing module enables the transformation of\ncomplex healthcare datasets such as longitudinal electronic health records,\nmedical images, continuous signals (e.g., electrocardiogram), and clinical\nnotes into machine learning friendly formats. The predictive modeling module\nprovides more than 30 machine learning models, including established ensemble\ntrees and deep neural network-based approaches, via a unified but extendable\nAPI designed for both researchers and practitioners. The evaluation module\nprovides various evaluation strategies (e.g., cross-validation and\ntrain-validation-test split) and predictive model metrics.\n  With robustness and scalability in mind, best practices such as unit testing,\ncontinuous integration, code coverage, and interactive examples are introduced\nin the library's development. PyHealth can be installed through the Python\nPackage Index (PyPI) or https://github.com/yzhao062/PyHealth .\n", "versions": [{"version": "v1", "created": "Mon, 11 Jan 2021 22:02:08 GMT"}], "update_date": "2021-01-13", "authors_parsed": [["Zhao", "Yue", ""], ["Qiao", "Zhi", ""], ["Xiao", "Cao", ""], ["Glass", "Lucas", ""], ["Sun", "Jimeng", ""]]}, {"id": "2101.04210", "submitter": "Kare Kamila", "authors": "Kare Kamila", "title": "General Hannan and Quinn Criterion for Common Time Series", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.LG stat.ML stat.TH", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  This paper aims to study data driven model selection criteria for a large\nclass of time series, which includes ARMA or AR($\\infty$) processes, as well as\nGARCH or ARCH($\\infty$), APARCH and many others processes. We tackled the\nchallenging issue of designing adaptive criteria which enjoys the strong\nconsistency property. When the observations are generated from one of the\naforementioned models, the new criteria, select the true model almost surely\nasymptotically. The proposed criteria are based on the minimization of a\npenalized contrast akin to the Hannan and Quinn's criterion and then involved a\nterm which is known for most classical time series models and for more complex\nmodels, this term can be data driven calibrated. Monte-Carlo experiments and an\nillustrative example on the CAC 40 index are performed to highlight the\nobtained results.\n", "versions": [{"version": "v1", "created": "Mon, 11 Jan 2021 22:03:02 GMT"}], "update_date": "2021-01-13", "authors_parsed": [["Kamila", "Kare", ""]]}, {"id": "2101.04223", "submitter": "Luca Manneschi", "authors": "Luca Manneschi, Matthew O. A. Ellis, Guido Gigante, Andrew C. Lin,\n  Paolo Del Giudice, Eleni Vasilaki", "title": "Exploiting Multiple Timescales in Hierarchical Echo State Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Echo state networks (ESNs) are a powerful form of reservoir computing that\nonly require training of linear output weights whilst the internal reservoir is\nformed of fixed randomly connected neurons. With a correctly scaled\nconnectivity matrix, the neurons' activity exhibits the echo-state property and\nresponds to the input dynamics with certain timescales. Tuning the timescales\nof the network can be necessary for treating certain tasks, and some\nenvironments require multiple timescales for an efficient representation. Here\nwe explore the timescales in hierarchical ESNs, where the reservoir is\npartitioned into two smaller linked reservoirs with distinct properties. Over\nthree different tasks (NARMA10, a reconstruction task in a volatile\nenvironment, and psMNIST), we show that by selecting the hyper-parameters of\neach partition such that they focus on different timescales, we achieve a\nsignificant performance improvement over a single ESN. Through a linear\nanalysis, and under the assumption that the timescales of the first partition\nare much shorter than the second's (typically corresponding to optimal\noperating conditions), we interpret the feedforward coupling of the partitions\nin terms of an effective representation of the input signal, provided by the\nfirst partition to the second, whereby the instantaneous input signal is\nexpanded into a weighted combination of its time derivatives. Furthermore, we\npropose a data-driven approach to optimise the hyper-parameters through a\ngradient descent optimisation method that is an online approximation of\nbackpropagation through time. We demonstrate the application of the online\nlearning rule across all the tasks considered.\n", "versions": [{"version": "v1", "created": "Mon, 11 Jan 2021 22:33:17 GMT"}, {"version": "v2", "created": "Thu, 14 Jan 2021 17:39:59 GMT"}], "update_date": "2021-01-15", "authors_parsed": [["Manneschi", "Luca", ""], ["Ellis", "Matthew O. A.", ""], ["Gigante", "Guido", ""], ["Lin", "Andrew C.", ""], ["Del Giudice", "Paolo", ""], ["Vasilaki", "Eleni", ""]]}, {"id": "2101.04224", "submitter": "Shruti Jadon", "authors": "Shruti Jadon, Jan Kanty Milczek, Ajit Patankar", "title": "Challenges and approaches to time-series forecasting in data center\n  telemetry: A Survey", "comments": "13 Pages, 6 Figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.NI", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Time-series forecasting has been an important research domain for so many\nyears. Its applications include ECG predictions, sales forecasting, weather\nconditions, even COVID-19 spread predictions. These applications have motivated\nmany researchers to figure out an optimal forecasting approach, but the\nmodeling approach also changes as the application domain changes. This work has\nfocused on reviewing different forecasting approaches for telemetry data\npredictions collected at data centers. Forecasting of telemetry data is a\ncritical feature of network and data center management products. However, there\nare multiple options of forecasting approaches that range from a simple linear\nstatistical model to high capacity deep learning architectures. In this paper,\nwe attempted to summarize and evaluate the performance of well known time\nseries forecasting techniques. We hope that this evaluation provides a\ncomprehensive summary to innovate in forecasting approaches for telemetry data.\n", "versions": [{"version": "v1", "created": "Mon, 11 Jan 2021 22:36:21 GMT"}, {"version": "v2", "created": "Thu, 11 Feb 2021 21:55:24 GMT"}], "update_date": "2021-02-15", "authors_parsed": [["Jadon", "Shruti", ""], ["Milczek", "Jan Kanty", ""], ["Patankar", "Ajit", ""]]}, {"id": "2101.04227", "submitter": "Kalyana Babu Nakshatrala", "authors": "N. V. Jagtap, M. K. Mudunuru, and K. B. Nakshatrala", "title": "A deep learning modeling framework to capture mixing patterns in\n  reactive-transport systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NA math.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Prediction and control of chemical mixing are vital for many scientific areas\nsuch as subsurface reactive transport, climate modeling, combustion,\nepidemiology, and pharmacology. Due to the complex nature of mixing in\nheterogeneous and anisotropic media, the mathematical models related to this\nphenomenon are not analytically tractable. Numerical simulations often provide\na viable route to predict chemical mixing accurately. However, contemporary\nmodeling approaches for mixing cannot utilize available spatial-temporal data\nto improve the accuracy of the future prediction and can be compute-intensive,\nespecially when the spatial domain is large and for long-term temporal\npredictions. To address this knowledge gap, we will present in this paper a\ndeep-learning (DL) modeling framework applied to predict the progress of\nchemical mixing under fast bimolecular reactions. This framework uses\nconvolutional neural networks (CNN) for capturing spatial patterns and long\nshort-term memory (LSTM) networks for forecasting temporal variations in\nmixing. By careful design of the framework -- placement of non-negative\nconstraint on the weights of the CNN and the selection of activation function,\nthe framework ensures non-negativity of the chemical species at all spatial\npoints and for all times. Our DL-based framework is fast, accurate, and\nrequires minimal data for training.\n", "versions": [{"version": "v1", "created": "Mon, 11 Jan 2021 22:55:24 GMT"}], "update_date": "2021-01-13", "authors_parsed": [["Jagtap", "N. V.", ""], ["Mudunuru", "M. K.", ""], ["Nakshatrala", "K. B.", ""]]}, {"id": "2101.04233", "submitter": "Noah Golowich", "authors": "Constantinos Daskalakis, Dylan J. Foster, Noah Golowich", "title": "Independent Policy Gradient Methods for Competitive Reinforcement\n  Learning", "comments": "Appeared at NeurIPS 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We obtain global, non-asymptotic convergence guarantees for independent\nlearning algorithms in competitive reinforcement learning settings with two\nagents (i.e., zero-sum stochastic games). We consider an episodic setting where\nin each episode, each player independently selects a policy and observes only\ntheir own actions and rewards, along with the state. We show that if both\nplayers run policy gradient methods in tandem, their policies will converge to\na min-max equilibrium of the game, as long as their learning rates follow a\ntwo-timescale rule (which is necessary). To the best of our knowledge, this\nconstitutes the first finite-sample convergence result for independent policy\ngradient methods in competitive RL; prior work has largely focused on\ncentralized, coordinated procedures for equilibrium computation.\n", "versions": [{"version": "v1", "created": "Mon, 11 Jan 2021 23:20:42 GMT"}], "update_date": "2021-01-13", "authors_parsed": [["Daskalakis", "Constantinos", ""], ["Foster", "Dylan J.", ""], ["Golowich", "Noah", ""]]}, {"id": "2101.04237", "submitter": "Samuel Sokota", "authors": "Samuel Sokota, Edward Lockhart, Finbarr Timbers, Elnaz Davoodi, Ryan\n  D'Orazio, Neil Burch, Martin Schmid, Michael Bowling, Marc Lanctot", "title": "Solving Common-Payoff Games with Approximate Policy Iteration", "comments": "AAAI 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For artificially intelligent learning systems to have widespread\napplicability in real-world settings, it is important that they be able to\noperate decentrally. Unfortunately, decentralized control is difficult --\ncomputing even an epsilon-optimal joint policy is a NEXP complete problem.\nNevertheless, a recently rediscovered insight -- that a team of agents can\ncoordinate via common knowledge -- has given rise to algorithms capable of\nfinding optimal joint policies in small common-payoff games. The Bayesian\naction decoder (BAD) leverages this insight and deep reinforcement learning to\nscale to games as large as two-player Hanabi. However, the approximations it\nuses to do so prevent it from discovering optimal joint policies even in games\nsmall enough to brute force optimal solutions. This work proposes CAPI, a novel\nalgorithm which, like BAD, combines common knowledge with deep reinforcement\nlearning. However, unlike BAD, CAPI prioritizes the propensity to discover\noptimal joint policies over scalability. While this choice precludes CAPI from\nscaling to games as large as Hanabi, empirical results demonstrate that, on the\ngames to which CAPI does scale, it is capable of discovering optimal joint\npolicies even when other modern multi-agent reinforcement learning algorithms\nare unable to do so. Code is available at https://github.com/ssokota/capi .\n", "versions": [{"version": "v1", "created": "Mon, 11 Jan 2021 23:42:02 GMT"}], "update_date": "2021-01-13", "authors_parsed": [["Sokota", "Samuel", ""], ["Lockhart", "Edward", ""], ["Timbers", "Finbarr", ""], ["Davoodi", "Elnaz", ""], ["D'Orazio", "Ryan", ""], ["Burch", "Neil", ""], ["Schmid", "Martin", ""], ["Bowling", "Michael", ""], ["Lanctot", "Marc", ""]]}, {"id": "2101.04243", "submitter": "Asaf Noy", "authors": "Asaf Noy, Yi Xu, Yonathan Aflalo, Lihi Zelnik-Manor, Rong Jin", "title": "A Convergence Theory Towards Practical Over-parameterized Deep Neural\n  Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural networks' remarkable ability to correctly fit training data when\noptimized by gradient-based algorithms is yet to be fully understood. Recent\ntheoretical results explain the convergence for ReLU networks that are wider\nthan those used in practice by orders of magnitude. In this work, we take a\nstep towards closing the gap between theory and practice by significantly\nimproving the known theoretical bounds on both the network width and the\nconvergence time. We show that convergence to a global minimum is guaranteed\nfor networks with widths quadratic in the sample size and linear in their depth\nat a time logarithmic in both. Our analysis and convergence bounds are derived\nvia the construction of a surrogate network with fixed activation patterns that\ncan be transformed at any time to an equivalent ReLU network of a reasonable\nsize. This construction can be viewed as a novel technique to accelerate\ntraining, while its tight finite-width equivalence to Neural Tangent Kernel\n(NTK) suggests it can be utilized to study generalization as well.\n", "versions": [{"version": "v1", "created": "Tue, 12 Jan 2021 00:40:45 GMT"}, {"version": "v2", "created": "Mon, 8 Feb 2021 11:38:39 GMT"}], "update_date": "2021-02-09", "authors_parsed": [["Noy", "Asaf", ""], ["Xu", "Yi", ""], ["Aflalo", "Yonathan", ""], ["Zelnik-Manor", "Lihi", ""], ["Jin", "Rong", ""]]}, {"id": "2101.04264", "submitter": "Ling Chen", "authors": "Jiahui Xu, Ling Chen, Mingqi Lv, Chaoqun Zhan, Sanjian Chen, Jian\n  Chang", "title": "HighAir: A Hierarchical Graph Neural Network-Based Air Quality\n  Forecasting Method", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Accurately forecasting air quality is critical to protecting general public\nfrom lung and heart diseases. This is a challenging task due to the complicated\ninteractions among distinct pollution sources and various other influencing\nfactors. Existing air quality forecasting methods cannot effectively model the\ndiffusion processes of air pollutants between cities and monitoring stations,\nwhich may suddenly deteriorate the air quality of a region. In this paper, we\npropose HighAir, i.e., a hierarchical graph neural network-based air quality\nforecasting method, which adopts an encoder-decoder architecture and considers\ncomplex air quality influencing factors, e.g., weather and land usage.\nSpecifically, we construct a city-level graph and station-level graphs from a\nhierarchical perspective, which can consider city-level and station-level\npatterns, respectively. We design two strategies, i.e., upper delivery and\nlower updating, to implement the inter-level interactions, and introduce\nmessage passing mechanism to implement the intra-level interactions. We\ndynamically adjust edge weights based on wind direction to model the\ncorrelations between dynamic factors and air quality. We compare HighAir with\nthe state-of-the-art air quality forecasting methods on the dataset of Yangtze\nRiver Delta city group, which covers 10 major cities within 61,500 km2. The\nexperimental results show that HighAir significantly outperforms other methods.\n", "versions": [{"version": "v1", "created": "Tue, 12 Jan 2021 02:31:14 GMT"}], "update_date": "2021-01-13", "authors_parsed": [["Xu", "Jiahui", ""], ["Chen", "Ling", ""], ["Lv", "Mingqi", ""], ["Zhan", "Chaoqun", ""], ["Chen", "Sanjian", ""], ["Chang", "Jian", ""]]}, {"id": "2101.04266", "submitter": "Yi Liu", "authors": "Yi Liu, Shuiwang Ji", "title": "CleftNet: Augmented Deep Learning for Synaptic Cleft Detection from\n  Brain Electron Microscopy", "comments": "10 pages, 3 figures, 6 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Detecting synaptic clefts is a crucial step to investigate the biological\nfunction of synapses. The volume electron microscopy (EM) allows the\nidentification of synaptic clefts by photoing EM images with high resolution\nand fine details. Machine learning approaches have been employed to\nautomatically predict synaptic clefts from EM images. In this work, we propose\na novel and augmented deep learning model, known as CleftNet, for improving\nsynaptic cleft detection from brain EM images. We first propose two novel\nnetwork components, known as the feature augmentor and the label augmentor, for\naugmenting features and labels to improve cleft representations. The feature\naugmentor can fuse global information from inputs and learn common\nmorphological patterns in clefts, leading to augmented cleft features. In\naddition, it can generate outputs with varying dimensions, making it flexible\nto be integrated in any deep network. The proposed label augmentor augments the\nlabel of each voxel from a value to a vector, which contains both the\nsegmentation label and boundary label. This allows the network to learn\nimportant shape information and to produce more informative cleft\nrepresentations. Based on the proposed feature augmentor and label augmentor,\nWe build the CleftNet as a U-Net like network. The effectiveness of our methods\nis evaluated on both online and offline tasks. Our CleftNet currently ranks \\#1\non the online task of the CREMI open challenge. In addition, both quantitative\nand qualitative results in the offline tasks show that our method outperforms\nthe baseline approaches significantly.\n", "versions": [{"version": "v1", "created": "Tue, 12 Jan 2021 02:45:53 GMT"}], "update_date": "2021-01-13", "authors_parsed": [["Liu", "Yi", ""], ["Ji", "Shuiwang", ""]]}, {"id": "2101.04283", "submitter": "Huimin Peng", "authors": "Huimin Peng", "title": "A Brief Survey of Associations Between Meta-Learning and General AI", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper briefly reviews the history of meta-learning and describes its\ncontribution to general AI. Meta-learning improves model generalization\ncapacity and devises general algorithms applicable to both in-distribution and\nout-of-distribution tasks potentially. General AI replaces task-specific models\nwith general algorithmic systems introducing higher level of automation in\nsolving diverse tasks using AI. We summarize main contributions of\nmeta-learning to the developments in general AI, including memory module,\nmeta-learner, coevolution, curiosity, forgetting and AI-generating algorithm.\nWe present connections between meta-learning and general AI and discuss how\nmeta-learning can be used to formulate general AI algorithms.\n", "versions": [{"version": "v1", "created": "Tue, 12 Jan 2021 03:57:16 GMT"}], "update_date": "2021-01-13", "authors_parsed": [["Peng", "Huimin", ""]]}, {"id": "2101.04285", "submitter": "Wei Min", "authors": "Wei Min, Weiming Liang, Hang Yin, Zhurong Wang, Mei Li, Alok Lal", "title": "Explainable Deep Behavioral Sequence Clustering for Transaction Fraud\n  Detection", "comments": "Accepted by AAAI2021 KDF Workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  In e-commerce industry, user behavior sequence data has been widely used in\nmany business units such as search and merchandising to improve their products.\nHowever, it is rarely used in financial services not only due to its 3V\ncharacteristics - i.e. Volume, Velocity and Variety - but also due to its\nunstructured nature. In this paper, we propose a Financial Service scenario\nDeep learning based Behavior data representation method for Clustering\n(FinDeepBehaviorCluster) to detect fraudulent transactions. To utilize the\nbehavior sequence data, we treat click stream data as event sequence, use time\nattention based Bi-LSTM to learn the sequence embedding in an unsupervised\nfashion, and combine them with intuitive features generated by risk experts to\nform a hybrid feature representation. We also propose a GPU powered HDBSCAN\n(pHDBSCAN) algorithm, which is an engineering optimization for the original\nHDBSCAN algorithm based on FAISS project, so that clustering can be carried out\non hundreds of millions of transactions within a few minutes. The computation\nefficiency of the algorithm has increased 500 times compared with the original\nimplementation, which makes flash fraud pattern detection feasible. Our\nexperimental results show that the proposed FinDeepBehaviorCluster framework is\nable to catch missed fraudulent transactions with considerable business values.\nIn addition, rule extraction method is applied to extract patterns from risky\nclusters using intuitive features, so that narrative descriptions can be\nattached to the risky clusters for case investigation, and unknown risk\npatterns can be mined for real-time fraud detection. In summary,\nFinDeepBehaviorCluster as a complementary risk management strategy to the\nexisting real-time fraud detection engine, can further increase our fraud\ndetection and proactive risk defense capabilities.\n", "versions": [{"version": "v1", "created": "Tue, 12 Jan 2021 04:12:18 GMT"}], "update_date": "2021-01-13", "authors_parsed": [["Min", "Wei", ""], ["Liang", "Weiming", ""], ["Yin", "Hang", ""], ["Wang", "Zhurong", ""], ["Li", "Mei", ""], ["Lal", "Alok", ""]]}, {"id": "2101.04292", "submitter": "Li Wang", "authors": "Li Wang and Lei-Hong Zhang and Ren-Cang Li", "title": "Trace Ratio Optimization with an Application to Multi-view Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A trace ratio optimization problem over the Stiefel manifold is investigated\nfrom the perspectives of both theory and numerical computations. At least three\nspecial cases of the problem have arisen from Fisher linear discriminant\nanalysis, canonical correlation analysis, and unbalanced Procrustes problem,\nrespectively. Necessary conditions in the form of nonlinear eigenvalue problem\nwith eigenvector dependency are established and a numerical method based on the\nself-consistent field (SCF) iteration is designed and proved to be always\nconvergent. As an application to multi-view subspace learning, a new framework\nand its instantiated concrete models are proposed and demonstrated on real\nworld data sets. Numerical results show that the efficiency of the proposed\nnumerical methods and effectiveness of the new multi-view subspace learning\nmodels.\n", "versions": [{"version": "v1", "created": "Tue, 12 Jan 2021 04:38:09 GMT"}], "update_date": "2021-01-13", "authors_parsed": [["Wang", "Li", ""], ["Zhang", "Lei-Hong", ""], ["Li", "Ren-Cang", ""]]}, {"id": "2101.04296", "submitter": "Anamaria Crisan", "authors": "Anamaria Crisan, Brittany Fiore-Gartland", "title": "Fits and Starts: Enterprise Use of AutoML and the Role of Humans in the\n  Loop", "comments": "CHI 2021 Conference, 15 pages, 3 figures, 1 Table", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.CY cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  AutoML systems can speed up routine data science work and make machine\nlearning available to those without expertise in statistics and computer\nscience. These systems have gained traction in enterprise settings where pools\nof skilled data workers are limited. In this study, we conduct interviews with\n29 individuals from organizations of different sizes to characterize how they\ncurrently use, or intend to use, AutoML systems in their data science work. Our\ninvestigation also captures how data visualization is used in conjunction with\nAutoML systems. Our findings identify three usage scenarios for AutoML that\nresulted in a framework summarizing the level of automation desired by data\nworkers with different levels of expertise. We surfaced the tension between\nspeed and human oversight and found that data visualization can do a poor job\nbalancing the two. Our findings have implications for the design and\nimplementation of human-in-the-loop visual analytics approaches.\n", "versions": [{"version": "v1", "created": "Tue, 12 Jan 2021 04:52:48 GMT"}], "update_date": "2021-01-13", "authors_parsed": [["Crisan", "Anamaria", ""], ["Fiore-Gartland", "Brittany", ""]]}, {"id": "2101.04315", "submitter": "Tsubasa Ochiai", "authors": "Tsubasa Ochiai, Marc Delcroix, Tomohiro Nakatani, Rintaro Ikeshita,\n  Keisuke Kinoshita, Shoko Araki", "title": "Neural Network-based Virtual Microphone Estimator", "comments": "5 pages, 2 figures, submitted to ICASSP 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.AS cs.LG cs.SD", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Developing microphone array technologies for a small number of microphones is\nimportant due to the constraints of many devices. One direction to address this\nsituation consists of virtually augmenting the number of microphone signals,\ne.g., based on several physical model assumptions. However, such assumptions\nare not necessarily met in realistic conditions. In this paper, as an\nalternative approach, we propose a neural network-based virtual microphone\nestimator (NN-VME). The NN-VME estimates virtual microphone signals directly in\nthe time domain, by utilizing the precise estimation capability of the recent\ntime-domain neural networks. We adopt a fully supervised learning framework\nthat uses actual observations at the locations of the virtual microphones at\ntraining time. Consequently, the NN-VME can be trained using only multi-channel\nobservations and thus directly on real recordings, avoiding the need for\nunrealistic physical model-based assumptions. Experiments on the CHiME-4 corpus\nshow that the proposed NN-VME achieves high virtual microphone estimation\nperformance even for real recordings and that a beamformer augmented with the\nNN-VME improves both the speech enhancement and recognition performance.\n", "versions": [{"version": "v1", "created": "Tue, 12 Jan 2021 06:30:24 GMT"}], "update_date": "2021-01-13", "authors_parsed": [["Ochiai", "Tsubasa", ""], ["Delcroix", "Marc", ""], ["Nakatani", "Tomohiro", ""], ["Ikeshita", "Rintaro", ""], ["Kinoshita", "Keisuke", ""], ["Araki", "Shoko", ""]]}, {"id": "2101.04319", "submitter": "Alsharif Abuadbba Dr", "authors": "Alsharif Abuadbba, Hyoungshick Kim, Surya Nepal", "title": "DeepiSign: Invisible Fragile Watermark to Protect the Integrityand\n  Authenticity of CNN", "comments": "The 36th ACM SIGAPP Symposium on Applied Computing (ACM SAC)", "journal-ref": null, "doi": "10.1145/3412841.3441970", "report-no": null, "categories": "cs.CR cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Convolutional Neural Networks (CNNs) deployed in real-life applications such\nas autonomous vehicles have shown to be vulnerable to manipulation attacks,\nsuch as poisoning attacks and fine-tuning. Hence, it is essential to ensure the\nintegrity and authenticity of CNNs because compromised models can produce\nincorrect outputs and behave maliciously. In this paper, we propose a\nself-contained tamper-proofing method, called DeepiSign, to ensure the\nintegrity and authenticity of CNN models against such manipulation attacks.\nDeepiSign applies the idea of fragile invisible watermarking to securely embed\na secret and its hash value into a CNN model. To verify the integrity and\nauthenticity of the model, we retrieve the secret from the model, compute the\nhash value of the secret, and compare it with the embedded hash value. To\nminimize the effects of the embedded secret on the CNN model, we use a\nwavelet-based technique to transform weights into the frequency domain and\nembed the secret into less significant coefficients. Our theoretical analysis\nshows that DeepiSign can hide up to 1KB secret in each layer with minimal loss\nof the model's accuracy. To evaluate the security and performance of DeepiSign,\nwe performed experiments on four pre-trained models (ResNet18, VGG16, AlexNet,\nand MobileNet) using three datasets (MNIST, CIFAR-10, and Imagenet) against\nthree types of manipulation attacks (targeted input poisoning, output\npoisoning, and fine-tuning). The results demonstrate that DeepiSign is\nverifiable without degrading the classification accuracy, and robust against\nrepresentative CNN manipulation attacks.\n", "versions": [{"version": "v1", "created": "Tue, 12 Jan 2021 06:42:45 GMT"}], "update_date": "2021-01-13", "authors_parsed": [["Abuadbba", "Alsharif", ""], ["Kim", "Hyoungshick", ""], ["Nepal", "Surya", ""]]}, {"id": "2101.04333", "submitter": "Yunhe Feng", "authors": "Yunhe Feng and Wenjun Zhou", "title": "Seed Stocking Via Multi-Task Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sellers of crop seeds need to plan for the variety and quantity of seeds to\nstock at least a year in advance. There are a large number of seed varieties of\none crop, and each can perform best under different growing conditions. Given\nthe unpredictability of weather, farmers need to make decisions that balance\nhigh yield and low risk. A seed vendor needs to be able to anticipate the needs\nof farmers and have them ready. In this study, we propose an analytical\nframework for estimating seed demand with three major steps. First, we will\nestimate the yield and risk of each variety as if they were planted at each\nlocation. Since past experiments performed with different seed varieties are\nhighly unbalanced across varieties, and the combination of growing conditions\nis sparse, we employ multi-task learning to borrow information from similar\nvarieties. Second, we will determine the best mix of seeds for each location by\nseeking a tradeoff between yield and risk. Third, we will aggregate such mix\nand pick the top five varieties to re-balance the yield and risk for each\ngrowing location. We find that multi-task learning provides a viable solution\nfor yield prediction, and our overall analytical framework has resulted in a\ngood performance.\n", "versions": [{"version": "v1", "created": "Tue, 12 Jan 2021 07:26:38 GMT"}], "update_date": "2021-01-13", "authors_parsed": [["Feng", "Yunhe", ""], ["Zhou", "Wenjun", ""]]}, {"id": "2101.04337", "submitter": "Jafar Norolahi", "authors": "Jafar Norolahi, Paeiz Azmi", "title": "Blind Modulation Classification via Combined Machine Learning and Signal\n  Feature Extraction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG eess.SP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this study, an algorithm to blind and automatic modulation classification\nhas been proposed. It well benefits combined machine leaning and signal feature\nextraction to recognize diverse range of modulation in low signal power to\nnoise ratio (SNR). The presented algorithm contains four. First, it advantages\nspectrum analyzing to branching modulated signal based on regular and irregular\nspectrum character. Seconds, a nonlinear soft margin support vector (NS SVM)\nproblem is applied to received signal, and its symbols are classified to\ncorrect and incorrect (support vectors) symbols. The NS SVM employment leads to\ndiscounting in physical layer noise effect on modulated signal. After that, a\nk-center clustering can find center of each class. finally, in correlation\nfunction estimation of scatter diagram is correlated with pre-saved ideal\nscatter diagram of modulations. The correlation outcome is classification\nresult. For more evaluation, success rate, performance, and complexity in\ncompare to many published methods are provided. The simulation prove that the\nproposed algorithm can classified the modulated signal in less SNR. For\nexample, it can recognize 4-QAM in SNR=-4.2 dB, and 4-FSK in SNR=2.1 dB with\n%99 success rate. Moreover, due to using of kernel function in dual problem of\nNS SVM and feature base function, the proposed algorithm has low complexity and\nsimple implementation in practical issues.\n", "versions": [{"version": "v1", "created": "Tue, 12 Jan 2021 07:58:33 GMT"}], "update_date": "2021-01-13", "authors_parsed": [["Norolahi", "Jafar", ""], ["Azmi", "Paeiz", ""]]}, {"id": "2101.04347", "submitter": "Konstantin Klemmer", "authors": "Tejumade Afonja, Konstantin Klemmer, Aya Salama, Paula Rodriguez Diaz,\n  Niveditha Kalavakonda, Oluwafemi Azeez", "title": "Proceedings of the NeurIPS 2020 Workshop on Machine Learning for the\n  Developing World: Improving Resilience", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  These are the proceedings of the 4th workshop on Machine Learning for the\nDeveloping World (ML4D), held as part of the Thirty-fourth Conference on Neural\nInformation Processing Systems (NeurIPS) on Saturday, December 12th 2020.\n", "versions": [{"version": "v1", "created": "Tue, 12 Jan 2021 08:35:54 GMT"}], "update_date": "2021-01-13", "authors_parsed": [["Afonja", "Tejumade", ""], ["Klemmer", "Konstantin", ""], ["Salama", "Aya", ""], ["Diaz", "Paula Rodriguez", ""], ["Kalavakonda", "Niveditha", ""], ["Azeez", "Oluwafemi", ""]]}, {"id": "2101.04348", "submitter": "Chang-Jen Wang", "authors": "Chang-Jen Wang, Chao-Kai Wen, Shang-Ho (Lawrence) Tsai, Shi Jin,\n  Geoffrey Ye Li", "title": "Phase Retrieval using Expectation Consistent Signal Recovery Algorithm\n  based on Hypernetwork", "comments": "13 pages, 9 figures, submitted to IEEE SP", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.IT math.IT", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Phase retrieval (PR) is an important component in modern computational\nimaging systems. Many algorithms have been developed over the past half\ncentury. Recent advances in deep learning have opened up a new possibility for\nrobust and fast PR. An emerging technique, called deep unfolding, provides a\nsystematic connection between conventional model-based iterative algorithms and\nmodern data-based deep learning. Unfolded algorithms, powered by data learning,\nhave shown remarkable performance and convergence speed improvement over the\noriginal algorithms. Despite their potential, most existing unfolded algorithms\nare strictly confined to a fixed number of iterations when employing\nlayer-dependent parameters. In this study, we develop a novel framework for\ndeep unfolding to overcome the existing limitations. Even if our framework can\nbe widely applied to general inverse problems, we take PR as an example in the\npaper. Our development is based on an unfolded generalized expectation\nconsistent signal recovery (GEC-SR) algorithm, wherein damping factors are left\nfor data-driven learning. In particular, we introduce a hypernetwork to\ngenerate the damping factors for GEC-SR. Instead of directly learning a set of\noptimal damping factors, the hypernetwork learns how to generate the optimal\ndamping factors according to the clinical settings, thus ensuring its\nadaptivity to different scenarios. To make the hypernetwork work adapt to\nvarying layer numbers, we use a recurrent architecture to develop a dynamic\nhypernetwork, which generates a damping factor that can vary online across\nlayers. We also exploit a self-attention mechanism to enhance the robustness of\nthe hypernetwork. Extensive experiments show that the proposed algorithm\noutperforms existing ones in convergence speed and accuracy, and still works\nwell under very harsh settings, that many classical PR algorithms unstable or\neven fail.\n", "versions": [{"version": "v1", "created": "Tue, 12 Jan 2021 08:36:23 GMT"}], "update_date": "2021-01-13", "authors_parsed": [["Wang", "Chang-Jen", "", "Lawrence"], ["Wen", "Chao-Kai", "", "Lawrence"], ["Shang-Ho", "", "", "Lawrence"], ["Tsai", "", ""], ["Jin", "Shi", ""], ["Li", "Geoffrey Ye", ""]]}, {"id": "2101.04354", "submitter": "Yeshwanth Venkatesha", "authors": "Karina Vasquez, Yeshwanth Venkatesha, Abhiroop Bhattacharjee, Abhishek\n  Moitra, Priyadarshini Panda", "title": "Activation Density based Mixed-Precision Quantization for Energy\n  Efficient Neural Networks", "comments": "Published in Design, Automation and Test in Europe (DATE) conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As neural networks gain widespread adoption in embedded devices, there is a\nneed for model compression techniques to facilitate deployment in\nresource-constrained environments. Quantization is one of the go-to methods\nyielding state-of-the-art model compression. Most approaches take a fully\ntrained model, apply different heuristics to determine the optimal\nbit-precision for different layers of the network, and retrain the network to\nregain any drop in accuracy. Based on Activation Density (AD)-the proportion of\nnon-zero activations in a layer-we propose an in-training quantization method.\nOur method calculates bit-width for each layer during training yielding a mixed\nprecision model with competitive accuracy. Since we train lower precision\nmodels during training, our approach yields the final quantized model at lower\ntraining complexity and also eliminates the need for re-training. We run\nexperiments on benchmark datasets like CIFAR-10, CIFAR-100, TinyImagenet on\nVGG19/ResNet18 architectures and report the accuracy and energy estimates for\nthe same. We achieve ~4.5x benefit in terms of estimated\nmultiply-and-accumulate (MAC) reduction while reducing the training complexity\nby 50% in our experiments. To further evaluate the energy benefits of our\nproposed method, we develop a mixed-precision scalable Process In Memory (PIM)\nhardware accelerator platform. The hardware platform incorporates shift-add\nfunctionality for handling multi-bit precision neural network models.\nEvaluating the quantized models obtained with our proposed method on the PIM\nplatform yields ~5x energy reduction compared to 16-bit models. Additionally,\nwe find that integrating AD based quantization with AD based pruning (both\nconducted during training) yields up to ~198x and ~44x energy reductions for\nVGG19 and ResNet18 architectures respectively on PIM platform compared to\nbaseline 16-bit precision, unpruned models.\n", "versions": [{"version": "v1", "created": "Tue, 12 Jan 2021 09:01:44 GMT"}], "update_date": "2021-01-13", "authors_parsed": [["Vasquez", "Karina", ""], ["Venkatesha", "Yeshwanth", ""], ["Bhattacharjee", "Abhiroop", ""], ["Moitra", "Abhishek", ""], ["Panda", "Priyadarshini", ""]]}, {"id": "2101.04356", "submitter": "Gustavo Penha", "authors": "Gustavo Penha and Claudia Hauff", "title": "On the Calibration and Uncertainty of Neural Learning to Rank Models", "comments": "Accepted for publication in the 16th conference of the European\n  Chapter of the Association for Computational Linguistics (EACL'21)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  According to the Probability Ranking Principle (PRP), ranking documents in\ndecreasing order of their probability of relevance leads to an optimal document\nranking for ad-hoc retrieval. The PRP holds when two conditions are met: [C1]\nthe models are well calibrated, and, [C2] the probabilities of relevance are\nreported with certainty. We know however that deep neural networks (DNNs) are\noften not well calibrated and have several sources of uncertainty, and thus\n[C1] and [C2] might not be satisfied by neural rankers. Given the success of\nneural Learning to Rank (L2R) approaches-and here, especially BERT-based\napproaches-we first analyze under which circumstances deterministic, i.e.\noutputs point estimates, neural rankers are calibrated. Then, motivated by our\nfindings we use two techniques to model the uncertainty of neural rankers\nleading to the proposed stochastic rankers, which output a predictive\ndistribution of relevance as opposed to point estimates. Our experimental\nresults on the ad-hoc retrieval task of conversation response ranking reveal\nthat (i) BERT-based rankers are not robustly calibrated and that stochastic\nBERT-based rankers yield better calibration; and (ii) uncertainty estimation is\nbeneficial for both risk-aware neural ranking, i.e.taking into account the\nuncertainty when ranking documents, and for predicting unanswerable\nconversational contexts.\n", "versions": [{"version": "v1", "created": "Tue, 12 Jan 2021 09:05:46 GMT"}], "update_date": "2021-01-13", "authors_parsed": [["Penha", "Gustavo", ""], ["Hauff", "Claudia", ""]]}, {"id": "2101.04365", "submitter": "Thulitha Theekshana Senevirathna", "authors": "Thulitha Senevirathna, Bathiya Thennakoon, Tharindu Sankalpa, Chatura\n  Seneviratne, Samad Ali and Nandana Rajatheva", "title": "Event-Driven Source Traffic Prediction in Machine-Type Communications\n  Using LSTM Networks", "comments": "6 pages, 6 figures, IEEE Global Communications Conference 2020,\n  https://globecom2020.ieee-globecom.org/program/symposia-tuesday", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.LG eess.SP math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Source traffic prediction is one of the main challenges of enabling\npredictive resource allocation in machine type communications (MTC). In this\npaper, a Long Short-Term Memory (LSTM) based deep learning approach is proposed\nfor event-driven source traffic prediction. The source traffic prediction\nproblem can be formulated as a sequence generation task where the main focus is\npredicting the transmission states of machine-type devices (MTDs) based on\ntheir past transmission data. This is done by restructuring the transmission\ndata in a way that the LSTM network can identify the causal relationship\nbetween the devices. Knowledge of such a causal relationship can enable\nevent-driven traffic prediction. The performance of the proposed approach is\nstudied using data regarding events from MTDs with different ranges of entropy.\nOur model outperforms existing baseline solutions in saving resources and\naccuracy with a margin of around 9%. Reduction in Random Access (RA) requests\nby our model is also analyzed to demonstrate the low amount of signaling\nrequired as a result of our proposed LSTM based source traffic prediction\napproach.\n", "versions": [{"version": "v1", "created": "Tue, 12 Jan 2021 09:31:18 GMT"}], "update_date": "2021-01-13", "authors_parsed": [["Senevirathna", "Thulitha", ""], ["Thennakoon", "Bathiya", ""], ["Sankalpa", "Tharindu", ""], ["Seneviratne", "Chatura", ""], ["Ali", "Samad", ""], ["Rajatheva", "Nandana", ""]]}, {"id": "2101.04371", "submitter": "Z Yan", "authors": "Li Wang and Zhenya Yan", "title": "Data-driven peakon and periodic peakon travelling wave solutions of some\n  nonlinear dispersive equations via deep learning", "comments": "20 pages, 11 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "nlin.PS cs.LG math-ph math.MP nlin.SI physics.flu-dyn", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In the field of mathematical physics, there exist many physically interesting\nnonlinear dispersive equations with peakon solutions, which are solitary waves\nwith discontinuous first-order derivative at the wave peak. In this paper, we\napply the multi-layer physics-informed neural networks (PINNs) deep learning to\nsuccessfully study the data-driven peakon and periodic peakon solutions of some\nwell-known nonlinear dispersion equations with initial-boundary value\nconditions such as the Camassa-Holm (CH) equation, Degasperis-Procesi equation,\nmodified CH equation with cubic nonlinearity, Novikov equation with cubic\nnonlinearity, mCH-Novikov equation, b-family equation with quartic\nnonlinearity, generalized modified CH equation with quintic nonlinearity, and\netc. These results will be useful to further study the peakon solutions and\ncorresponding experimental design of nonlinear dispersive equations.\n", "versions": [{"version": "v1", "created": "Tue, 12 Jan 2021 09:50:28 GMT"}], "update_date": "2021-01-13", "authors_parsed": [["Wang", "Li", ""], ["Yan", "Zhenya", ""]]}, {"id": "2101.04383", "submitter": "Petar Todorovic", "authors": "Hitarth Choubisa (1), Petar Todorovi\\'c (1), Joao M. Pina (1), Darshan\n  H. Parmar (1), Ziliang Li (1), Oleksandr Voznyy (4), Isaac Tamblyn (2,3),\n  Edward Sargent (1) ((1) Department of Electrical and Computer Engineering,\n  University of Toronto, Toronto, ON, Canada, (2) National Research Council of\n  Canada, Ottawa, ON, Canada, (3) Vector Institute for Artificial Intelligence,\n  Toronto, ON, Canada, (4) Department of Physical and Environmental Sciences,\n  University of Toronto, Scarborough, ON, Canada)", "title": "Interpretable discovery of new semiconductors with machine learning", "comments": "25 pages, 4 figures, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "cond-mat.mtrl-sci cs.LG", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Machine learning models of materials$^{1-5}$ accelerate discovery compared to\nab initio methods: deep learning models now reproduce density functional theory\n(DFT)-calculated results at one hundred thousandths of the cost of DFT$^{6}$.\nTo provide guidance in experimental materials synthesis, these need to be\ncoupled with an accurate yet effective search algorithm and training data\nconsistent with experimental observations. Here we report an evolutionary\nalgorithm powered search which uses machine-learned surrogate models trained on\nhigh-throughput hybrid functional DFT data benchmarked against experimental\nbandgaps: Deep Adaptive Regressive Weighted Intelligent Network (DARWIN). The\nstrategy enables efficient search over the materials space of ~10$^8$ ternaries\nand 10$^{11}$ quaternaries$^{7}$ for candidates with target properties. It\nprovides interpretable design rules, such as our finding that the difference in\nthe electronegativity between the halide and B-site cation being a strong\npredictor of ternary structural stability. As an example, when we seek UV\nemission, DARWIN predicts K$_2$CuX$_3$ (X = Cl, Br) as a promising materials\nfamily, based on its electronegativity difference. We synthesized and found\nthese materials to be stable, direct bandgap UV emitters. The approach also\nallows knowledge distillation for use by humans.\n", "versions": [{"version": "v1", "created": "Tue, 12 Jan 2021 10:23:16 GMT"}], "update_date": "2021-01-13", "authors_parsed": [["Choubisa", "Hitarth", ""], ["Todorovi\u0107", "Petar", ""], ["Pina", "Joao M.", ""], ["Parmar", "Darshan H.", ""], ["Li", "Ziliang", ""], ["Voznyy", "Oleksandr", ""], ["Tamblyn", "Isaac", ""], ["Sargent", "Edward", ""]]}, {"id": "2101.04386", "submitter": "Ishaan Bhat", "authors": "Ishaan Bhat, Hugo J. Kuijf, Veronika Cheplygina and Josien P.W. Pluim", "title": "Using uncertainty estimation to reduce false positives in liver lesion\n  detection", "comments": "Accepted at IEEE ISBI 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Despite the successes of deep learning techniques at detecting objects in\nmedical images, false positive detections occur which may hinder an accurate\ndiagnosis. We propose a technique to reduce false positive detections made by a\nneural network using an SVM classifier trained with features derived from the\nuncertainty map of the neural network prediction. We demonstrate the\neffectiveness of this method for the detection of liver lesions on a dataset of\nabdominal MR images. We find that the use of a dropout rate of 0.5 produces the\nleast number of false positives in the neural network predictions and the\ntrained classifier filters out approximately 90% of these false positives\ndetections in the test-set.\n", "versions": [{"version": "v1", "created": "Tue, 12 Jan 2021 10:26:41 GMT"}, {"version": "v2", "created": "Wed, 13 Jan 2021 12:30:54 GMT"}, {"version": "v3", "created": "Tue, 26 Jan 2021 11:02:34 GMT"}], "update_date": "2021-01-27", "authors_parsed": [["Bhat", "Ishaan", ""], ["Kuijf", "Hugo J.", ""], ["Cheplygina", "Veronika", ""], ["Pluim", "Josien P. W.", ""]]}, {"id": "2101.04401", "submitter": "Yujin Huang", "authors": "Yujin Huang, Han Hu, Chunyang Chen", "title": "Robustness of on-device Models: Adversarial Attack to Deep Learning\n  Models on Android Apps", "comments": "Accepted to the 43rd International Conference on Software\n  Engineering, Software Engineering in Practice Track. This is a preprint\n  version, the copyright belongs to The Institute of Electrical and Electronics\n  Engineers", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning has shown its power in many applications, including object\ndetection in images, natural-language understanding, and speech recognition. To\nmake it more accessible to end users, many deep learning models are now\nembedded in mobile apps. Compared to offloading deep learning from smartphones\nto the cloud, performing machine learning on-device can help improve latency,\nconnectivity, and power consumption. However, most deep learning models within\nAndroid apps can easily be obtained via mature reverse engineering, while the\nmodels' exposure may invite adversarial attacks. In this study, we propose a\nsimple but effective approach to hacking deep learning models using adversarial\nattacks by identifying highly similar pre-trained models from TensorFlow Hub.\nAll 10 real-world Android apps in the experiment are successfully attacked by\nour approach. Apart from the feasibility of the model attack, we also carry out\nan empirical study that investigates the characteristics of deep learning\nmodels used by hundreds of Android apps on Google Play. The results show that\nmany of them are similar to each other and widely use fine-tuning techniques to\npre-trained models on the Internet.\n", "versions": [{"version": "v1", "created": "Tue, 12 Jan 2021 10:49:30 GMT"}, {"version": "v2", "created": "Thu, 4 Feb 2021 13:39:00 GMT"}], "update_date": "2021-02-05", "authors_parsed": [["Huang", "Yujin", ""], ["Hu", "Han", ""], ["Chen", "Chunyang", ""]]}, {"id": "2101.04414", "submitter": "Leonardo Andr\\'es Espinosa Leal", "authors": "Emmanuel Raj, Magnus Westerlund, Leonardo Espinosa-Leal", "title": "Reliable Fleet Analytics for Edge IoT Solutions", "comments": "8 pages, 6 figures, 5 tables, conference paper", "journal-ref": null, "doi": null, "report-no": "CLOUD COMPUTING 2020, 55", "categories": "cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  In recent years we have witnessed a boom in Internet of Things (IoT) device\ndeployments, which has resulted in big data and demand for low-latency\ncommunication. This shift in the demand for infrastructure is also enabling\nreal-time decision making using artificial intelligence for IoT applications.\nArtificial Intelligence of Things (AIoT) is the combination of Artificial\nIntelligence (AI) technologies and the IoT infrastructure to provide robust and\nefficient operations and decision making. Edge computing is emerging to enable\nAIoT applications. Edge computing enables generating insights and making\ndecisions at or near the data source, reducing the amount of data sent to the\ncloud or a central repository. In this paper, we propose a framework for\nfacilitating machine learning at the edge for AIoT applications, to enable\ncontinuous delivery, deployment, and monitoring of machine learning models at\nthe edge (Edge MLOps). The contribution is an architecture that includes\nservices, tools, and methods for delivering fleet analytics at scale. We\npresent a preliminary validation of the framework by performing experiments\nwith IoT devices on a university campus's rooms. For the machine learning\nexperiments, we forecast multivariate time series for predicting air quality in\nthe respective rooms by using the models deployed in respective edge devices.\nBy these experiments, we validate the proposed fleet analytics framework for\nefficiency and robustness.\n", "versions": [{"version": "v1", "created": "Tue, 12 Jan 2021 11:28:43 GMT"}], "update_date": "2021-01-13", "authors_parsed": [["Raj", "Emmanuel", ""], ["Westerlund", "Magnus", ""], ["Espinosa-Leal", "Leonardo", ""]]}, {"id": "2101.04422", "submitter": "Quirin G\\\"ottl", "authors": "Quirin G\\\"ottl, Dominik G. Grimm, Jakob Burger", "title": "Automated Synthesis of Steady-State Continuous Processes using\n  Reinforcement Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CE cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Automated flowsheet synthesis is an important field in computer-aided process\nengineering. The present work demonstrates how reinforcement learning can be\nused for automated flowsheet synthesis without any heuristics of prior\nknowledge of conceptual design. The environment consists of a steady-state\nflowsheet simulator that contains all physical knowledge. An agent is trained\nto take discrete actions and sequentially built up flowsheets that solve a\ngiven process problem. A novel method named SynGameZero is developed to ensure\ngood exploration schemes in the complex problem. Therein, flowsheet synthesis\nis modelled as a game of two competing players. The agent plays this game\nagainst itself during training and consists of an artificial neural network and\na tree search for forward planning. The method is applied successfully to a\nreaction-distillation process in a quaternary system.\n", "versions": [{"version": "v1", "created": "Tue, 12 Jan 2021 11:49:34 GMT"}, {"version": "v2", "created": "Mon, 15 Mar 2021 09:42:10 GMT"}], "update_date": "2021-03-16", "authors_parsed": [["G\u00f6ttl", "Quirin", ""], ["Grimm", "Dominik G.", ""], ["Burger", "Jakob", ""]]}, {"id": "2101.04423", "submitter": "Chaopeng Shen", "authors": "Wenyu Ouyang, Kathryn Lawson, Dapeng Feng, Lei Ye, Chi Zhang, Chaopeng\n  Shen", "title": "Continental-scale streamflow modeling of basins with reservoirs: towards\n  a coherent deep-learning-based strategy", "comments": null, "journal-ref": "Journal of Hydrology, 2021", "doi": "10.1016/j.jhydrol.2021.126455", "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A large fraction of major waterways have dams influencing streamflow, which\nmust be accounted for in large-scale hydrologic modeling. However, daily\nstreamflow prediction for basins with dams is challenging for various modeling\napproaches, especially at large scales. Here we examined which types of dammed\nbasins could be well represented by long short-term memory (LSTM) models using\nreadily-available information, and delineated the remaining challenges. We\nanalyzed data from 3557 basins (83% dammed) over the contiguous United States\nand noted strong impacts of reservoir purposes, degree of regulation (dor), and\ndiversion on streamflow modeling. While a model trained on a widely-used\nreference-basin dataset performed poorly for non-reference basins, the model\ntrained on the whole dataset presented a median Nash-Sutcliffe efficiency\ncoefficient (NSE) of 0.74. The zero-dor, small-dor (with storage of\napproximately a month of average streamflow or less), and large-dor basins were\nfound to have distinct behaviors, so migrating models between categories\nyielded catastrophic results, which means we must not treat small-dor basins as\nreference ones. However, training with pooled data from different sets yielded\noptimal median NSEs of 0.72, 0.79, and 0.64 for these respective groups,\nnoticeably stronger than existing models. These results support a coherent\nmodeling strategy where smaller dams (storing about a month of average\nstreamflow or less) are modeled implicitly as part of basin rainfall-runoff\nprocesses; then, large-dor reservoirs of certain types can be represented\nexplicitly. However, dammed basins must be present in the training dataset.\nFuture work should examine separate modeling of large reservoirs for fire\nprotection and irrigation, hydroelectric power generation, and flood control.\n", "versions": [{"version": "v1", "created": "Tue, 12 Jan 2021 11:49:54 GMT"}, {"version": "v2", "created": "Wed, 12 May 2021 21:12:04 GMT"}], "update_date": "2021-06-09", "authors_parsed": [["Ouyang", "Wenyu", ""], ["Lawson", "Kathryn", ""], ["Feng", "Dapeng", ""], ["Ye", "Lei", ""], ["Zhang", "Chi", ""], ["Shen", "Chaopeng", ""]]}, {"id": "2101.04434", "submitter": "Michael Allen", "authors": "Michael Allen, Kerry Pearn and Tom Monks", "title": "Developing an OpenAI Gym-compatible framework and simulation environment\n  for testing Deep Reinforcement Learning agents solving the Ambulance Location\n  Problem", "comments": "Fig 1 updated since first version (corrected panel 3 which previously\n  replicated panel 2)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Background and motivation: Deep Reinforcement Learning (Deep RL) is a rapidly\ndeveloping field. Historically most application has been made to games (such as\nchess, Atari games, and go). Deep RL is now reaching the stage where it may\noffer value in real world problems, including optimisation of healthcare\nsystems. One such problem is where to locate ambulances between calls in order\nto minimise time from emergency call to ambulance on-scene. This is known as\nthe Ambulance Location problem.\n  Aim: To develop an OpenAI Gym-compatible framework and simulation environment\nfor testing Deep RL agents.\n  Methods: A custom ambulance dispatch simulation environment was developed\nusing OpenAI Gym and SimPy. Deep RL agents were built using PyTorch. The\nenvironment is a simplification of the real world, but allows control over the\nnumber of clusters of incident locations, number of possible dispatch\nlocations, number of hospitals, and creating incidents that occur at different\nlocations throughout each day.\n  Results: A range of Deep RL agents based on Deep Q networks were tested in\nthis custom environment. All reduced time to respond to emergency calls\ncompared with random allocation to dispatch points. Bagging Noisy Duelling Deep\nQ networks gave the most consistence performance. All methods had a tendency to\nlose performance if trained for too long, and so agents were saved at their\noptimal performance (and tested on independent simulation runs).\n  Conclusions: Deep RL agents, developed using simulated environments, have the\npotential to offer a novel approach to optimise the Ambulance Location problem.\nCreating open simulation environments should allow more rapid progress in this\nfield.\n", "versions": [{"version": "v1", "created": "Tue, 12 Jan 2021 12:10:52 GMT"}, {"version": "v2", "created": "Wed, 13 Jan 2021 09:07:43 GMT"}], "update_date": "2021-01-14", "authors_parsed": [["Allen", "Michael", ""], ["Pearn", "Kerry", ""], ["Monks", "Tom", ""]]}, {"id": "2101.04446", "submitter": "Gianmarco Cerutti", "authors": "Gianmarco Cerutti, Renzo Andri, Lukas Cavigelli, Michele Magno,\n  Elisabetta Farella, Luca Benini", "title": "Sound Event Detection with Binary Neural Networks on Tightly\n  Power-Constrained IoT Devices", "comments": "6 pages conference", "journal-ref": null, "doi": "10.1145/3370748.3406588", "report-no": null, "categories": "cs.LG eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sound event detection (SED) is a hot topic in consumer and smart city\napplications. Existing approaches based on Deep Neural Networks are very\neffective, but highly demanding in terms of memory, power, and throughput when\ntargeting ultra-low power always-on devices.\n  Latency, availability, cost, and privacy requirements are pushing recent IoT\nsystems to process the data on the node, close to the sensor, with a very\nlimited energy supply, and tight constraints on the memory size and processing\ncapabilities precluding to run state-of-the-art DNNs.\n  In this paper, we explore the combination of extreme quantization to a\nsmall-footprint binary neural network (BNN) with the highly energy-efficient,\nRISC-V-based (8+1)-core GAP8 microcontroller. Starting from an existing CNN for\nSED whose footprint (815 kB) exceeds the 512 kB of memory available on our\nplatform, we retrain the network using binary filters and activations to match\nthese memory constraints. (Fully) binary neural networks come with a natural\ndrop in accuracy of 12-18% on the challenging ImageNet object recognition\nchallenge compared to their equivalent full-precision baselines. This BNN\nreaches a 77.9% accuracy, just 7% lower than the full-precision version, with\n58 kB (7.2 times less) for the weights and 262 kB (2.4 times less) memory in\ntotal. With our BNN implementation, we reach a peak throughput of 4.6 GMAC/s\nand 1.5 GMAC/s over the full network, including preprocessing with Mel bins,\nwhich corresponds to an efficiency of 67.1 GMAC/s/W and 31.3 GMAC/s/W,\nrespectively. Compared to the performance of an ARM Cortex-M4 implementation,\nour system has a 10.3 times faster execution time and a 51.1 times higher\nenergy-efficiency.\n", "versions": [{"version": "v1", "created": "Tue, 12 Jan 2021 12:38:23 GMT"}], "update_date": "2021-01-13", "authors_parsed": [["Cerutti", "Gianmarco", ""], ["Andri", "Renzo", ""], ["Cavigelli", "Lukas", ""], ["Magno", "Michele", ""], ["Farella", "Elisabetta", ""], ["Benini", "Luca", ""]]}, {"id": "2101.04454", "submitter": "Sahand Rezaei-Shoshtari Mr.", "authors": "Sahand Rezaei-Shoshtari, Francois Robert Hogan, Michael Jenkin, David\n  Meger, Gregory Dudek", "title": "Learning Intuitive Physics with Multimodal Generative Models", "comments": "AAAI 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Predicting the future interaction of objects when they come into contact with\ntheir environment is key for autonomous agents to take intelligent and\nanticipatory actions. This paper presents a perception framework that fuses\nvisual and tactile feedback to make predictions about the expected motion of\nobjects in dynamic scenes. Visual information captures object properties such\nas 3D shape and location, while tactile information provides critical cues\nabout interaction forces and resulting object motion when it makes contact with\nthe environment. Utilizing a novel See-Through-your-Skin (STS) sensor that\nprovides high resolution multimodal sensing of contact surfaces, our system\ncaptures both the visual appearance and the tactile properties of objects. We\ninterpret the dual stream signals from the sensor using a Multimodal\nVariational Autoencoder (MVAE), allowing us to capture both modalities of\ncontacting objects and to develop a mapping from visual to tactile interaction\nand vice-versa. Additionally, the perceptual system can be used to infer the\noutcome of future physical interactions, which we validate through simulated\nand real-world experiments in which the resting state of an object is predicted\nfrom given initial conditions.\n", "versions": [{"version": "v1", "created": "Tue, 12 Jan 2021 12:55:53 GMT"}, {"version": "v2", "created": "Tue, 19 Jan 2021 21:57:48 GMT"}], "update_date": "2021-01-21", "authors_parsed": [["Rezaei-Shoshtari", "Sahand", ""], ["Hogan", "Francois Robert", ""], ["Jenkin", "Michael", ""], ["Meger", "David", ""], ["Dudek", "Gregory", ""]]}, {"id": "2101.04470", "submitter": "Amir M. Mir", "authors": "Amir M. Mir, Evaldas Latoskinas, Sebastian Proksch, Georgios Gousios", "title": "Type4Py: Deep Similarity Learning-Based Type Inference for Python", "comments": "Type4Py's source code and dataset can be retrieved here:\n  https://github.com/mir-am/type4py-paper The second version of the paper is\n  published in Jul. 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.PL cs.SE", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Dynamic languages, such as Python and Javascript, trade static typing for\ndeveloper flexibility and productivity. Lack of static typing can cause\nrun-time exceptions and is a major factor for weak IDE support. To alleviate\nthese issues, PEP 484 introduced optional type annotations for Python. As\nretrofitting types to existing codebases is error-prone and laborious,\nlearning-based approaches have been proposed to enable automatic type\nannotations based on existing, partially annotated codebases. However, it is\nstill quite challenging for learning-based approaches to give a relevant\nprediction in the first suggestion or the first few ones. In this paper, we\npresent Type4Py, a deep similarity learning-based hierarchical neural network\nmodel that learns to discriminate between types of the same kind and dissimilar\ntypes in a high-dimensional space, which results in clusters of types. Nearest\nneighbor search suggests a list of likely types for arguments, variables, and\nfunctions' return. The results of the quantitative and qualitative evaluation\nindicate that Type4Py significantly outperforms state-of-the-art approaches at\nthe type prediction task. Considering the Top-1 prediction, Type4Py obtains a\nMean Reciprocal Rank of 72.5%, which is 10.87% and 16.45% higher than that of\nTypilus and TypeWriter, respectively.\n", "versions": [{"version": "v1", "created": "Tue, 12 Jan 2021 13:32:53 GMT"}, {"version": "v2", "created": "Thu, 22 Jul 2021 16:10:37 GMT"}], "update_date": "2021-07-23", "authors_parsed": [["Mir", "Amir M.", ""], ["Latoskinas", "Evaldas", ""], ["Proksch", "Sebastian", ""], ["Gousios", "Georgios", ""]]}, {"id": "2101.04480", "submitter": "Sridhar Ravula", "authors": "Sridhar Ravula", "title": "Text analysis in financial disclosures", "comments": "24 pages, 1 figure, Text analysis in financial disclosure analysis\n  survey", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.TR cs.LG q-fin.GN", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Financial disclosure analysis and Knowledge extraction is an important\nfinancial analysis problem. Prevailing methods depend predominantly on\nquantitative ratios and techniques, which suffer from limitations like window\ndressing and past focus. Most of the information in a firm's financial\ndisclosures is in unstructured text and contains valuable information about its\nhealth. Humans and machines fail to analyze it satisfactorily due to the\nenormous volume and unstructured nature, respectively. Researchers have started\nanalyzing text content in disclosures recently. This paper covers the previous\nwork in unstructured data analysis in Finance and Accounting. It also explores\nthe state of art methods in computational linguistics and reviews the current\nmethodologies in Natural Language Processing (NLP). Specifically, it focuses on\nresearch related to text source, linguistic attributes, firm attributes, and\nmathematical models employed in the text analysis approach. This work\ncontributes to disclosure analysis methods by highlighting the limitations of\nthe current focus on sentiment metrics and highlighting broader future research\nareas\n", "versions": [{"version": "v1", "created": "Wed, 6 Jan 2021 17:45:40 GMT"}], "update_date": "2021-01-13", "authors_parsed": [["Ravula", "Sridhar", ""]]}, {"id": "2101.04493", "submitter": "Kseniya Cherenkova", "authors": "Kseniya Cherenkova, Djamila Aouada, Gleb Gusev", "title": "PvDeConv: Point-Voxel Deconvolution for Autoencoding CAD Construction in\n  3D", "comments": "2020 IEEE International Conference on Image Processing (ICIP)", "journal-ref": "2020 IEEE International Conference on Image Processing (ICIP),\n  2020, pp. 2741-2745", "doi": "10.1109/ICIP40778.2020.9191095", "report-no": null, "categories": "cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  We propose a Point-Voxel DeConvolution (PVDeConv) module for 3D data\nautoencoder. To demonstrate its efficiency we learn to synthesize\nhigh-resolution point clouds of 10k points that densely describe the underlying\ngeometry of Computer Aided Design (CAD) models. Scanning artifacts, such as\nprotrusions, missing parts, smoothed edges and holes, inevitably appear in real\n3D scans of fabricated CAD objects. Learning the original CAD model\nconstruction from a 3D scan requires a ground truth to be available together\nwith the corresponding 3D scan of an object. To solve the gap, we introduce a\nnew dedicated dataset, the CC3D, containing 50k+ pairs of CAD models and their\ncorresponding 3D meshes. This dataset is used to learn a convolutional\nautoencoder for point clouds sampled from the pairs of 3D scans - CAD models.\nThe challenges of this new dataset are demonstrated in comparison with other\ngenerative point cloud sampling models trained on ShapeNet. The CC3D\nautoencoder is efficient with respect to memory consumption and training time\nas compared to stateof-the-art models for 3D data generation.\n", "versions": [{"version": "v1", "created": "Tue, 12 Jan 2021 14:14:13 GMT"}], "update_date": "2021-01-13", "authors_parsed": [["Cherenkova", "Kseniya", ""], ["Aouada", "Djamila", ""], ["Gusev", "Gleb", ""]]}, {"id": "2101.04516", "submitter": "Monica Arul", "authors": "Monica Arul and Ahsan Kareem", "title": "Machine learning based automated identification of thunderstorms from\n  anemometric records using shapelet transform", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.geo-ph cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Detection of thunderstorms is important to the wind hazard community to\nbetter understand extreme winds field characteristics and associated wind\ninduced load effects on structures. This paper contributes to this effort by\nproposing a new course of research that uses machine learning techniques,\nindependent of wind statistics based parameters, to autonomously identify and\nseparate thunderstorms from large databases containing high frequency sampled\ncontinuous wind speed measurements. In this context, the use of Shapelet\ntransform is proposed to identify key individual attributes distinctive to\nextreme wind events based on similarity of shape of their time series. This\nnovel shape based representation when combined with machine learning algorithms\nyields a practical event detection procedure with minimal domain expertise. In\nthis paper, the shapelet transform along with Random Forest classifier is\nemployed for the identification of thunderstorms from 1 year of data from 14\nultrasonic anemometers that are a part of an extensive in situ wind monitoring\nnetwork in the Northern Mediterranean ports. A collective total of 235\nnon-stationary records associated with thunderstorms were identified using this\nmethod. The results lead to enhancing the pool of thunderstorm data for more\ncomprehensive understanding of a wide variety of thunderstorms that have not\nbeen previously detected using conventional gust factor-based methods.\n", "versions": [{"version": "v1", "created": "Sun, 10 Jan 2021 11:06:41 GMT"}], "update_date": "2021-01-13", "authors_parsed": [["Arul", "Monica", ""], ["Kareem", "Ahsan", ""]]}, {"id": "2101.04520", "submitter": "Morteza Haghir Chehreghani", "authors": "Victor Eberstein, Jonas Sj\\\"oblom, Nikolce Murgovski, Morteza Haghir\n  Chehreghani", "title": "A Unified Framework for Online Trip Destination Prediction", "comments": "18 pages, 12 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Trip destination prediction is an area of increasing importance in many\napplications such as trip planning, autonomous driving and electric vehicles.\nEven though this problem could be naturally addressed in an online learning\nparadigm where data is arriving in a sequential fashion, the majority of\nresearch has rather considered the offline setting. In this paper, we present a\nunified framework for trip destination prediction in an online setting, which\nis suitable for both online training and online prediction. For this purpose,\nwe develop two clustering algorithms and integrate them within two online\nprediction models for this problem.\n  We investigate the different configurations of clustering algorithms and\nprediction models on a real-world dataset. By using traditional clustering\nmetrics and accuracy, we demonstrate that both the clustering and the entire\nframework yield consistent results compared to the offline setting. Finally, we\npropose a novel regret metric for evaluating the entire online framework in\ncomparison to its offline counterpart. This metric makes it possible to relate\nthe source of erroneous predictions to either the clustering or the prediction\nmodel. Using this metric, we show that the proposed methods converge to a\nprobability distribution resembling the true underlying distribution and enjoy\na lower regret than all of the baselines.\n", "versions": [{"version": "v1", "created": "Tue, 12 Jan 2021 14:45:27 GMT"}], "update_date": "2021-01-13", "authors_parsed": [["Eberstein", "Victor", ""], ["Sj\u00f6blom", "Jonas", ""], ["Murgovski", "Nikolce", ""], ["Chehreghani", "Morteza Haghir", ""]]}, {"id": "2101.04526", "submitter": "Yoni Halpern", "authors": "Sirui Yao and Yoni Halpern and Nithum Thain and Xuezhi Wang and Kang\n  Lee and Flavien Prost and Ed H. Chi and Jilin Chen and Alex Beutel", "title": "Measuring Recommender System Effects with Simulated Users", "comments": "Presented at Second Workshop on Fairness, Accountability,\n  Transparency, Ethics and Society on the Web (FATES 2020) with the title\n  \"Beyond Next Step Bias: Trajectory Simulation for Understanding Recommender\n  System Behavior\"", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CY cs.IR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Imagine a food recommender system -- how would we check if it is\n\\emph{causing} and fostering unhealthy eating habits or merely reflecting\nusers' interests? How much of a user's experience over time with a recommender\nis caused by the recommender system's choices and biases, and how much is based\non the user's preferences and biases? Popularity bias and filter bubbles are\ntwo of the most well-studied recommender system biases, but most of the prior\nresearch has focused on understanding the system behavior in a single\nrecommendation step. How do these biases interplay with user behavior, and what\ntypes of user experiences are created from repeated interactions?\n  In this work, we offer a simulation framework for measuring the impact of a\nrecommender system under different types of user behavior. Using this\nsimulation framework, we can (a) isolate the effect of the recommender system\nfrom the user preferences, and (b) examine how the system performs not just on\naverage for an \"average user\" but also the extreme experiences under atypical\nuser behavior. As part of the simulation framework, we propose a set of\nevaluation metrics over the simulations to understand the recommender system's\nbehavior. Finally, we present two empirical case studies -- one on traditional\ncollaborative filtering in MovieLens and one on a large-scale production\nrecommender system -- to understand how popularity bias manifests over time.\n", "versions": [{"version": "v1", "created": "Tue, 12 Jan 2021 14:51:11 GMT"}], "update_date": "2021-01-13", "authors_parsed": [["Yao", "Sirui", ""], ["Halpern", "Yoni", ""], ["Thain", "Nithum", ""], ["Wang", "Xuezhi", ""], ["Lee", "Kang", ""], ["Prost", "Flavien", ""], ["Chi", "Ed H.", ""], ["Chen", "Jilin", ""], ["Beutel", "Alex", ""]]}, {"id": "2101.04530", "submitter": "Thomas Daniel", "authors": "Thomas Daniel, Fabien Casenave, Nissrine Akkari, David Ryckelynck", "title": "Data augmentation and feature selection for automatic model\n  recommendation in computational physics", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG physics.comp-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Classification algorithms have recently found applications in computational\nphysics for the selection of numerical methods or models adapted to the\nenvironment and the state of the physical system. For such classification\ntasks, labeled training data come from numerical simulations and generally\ncorrespond to physical fields discretized on a mesh. Three challenging\ndifficulties arise: the lack of training data, their high dimensionality, and\nthe non-applicability of common data augmentation techniques to physics data.\nThis article introduces two algorithms to address these issues, one for\ndimensionality reduction via feature selection, and one for data augmentation.\nThese algorithms are combined with a wide variety of classifiers for their\nevaluation. When combined with a stacking ensemble made of six multilayer\nperceptrons and a ridge logistic regression, they enable reaching an accuracy\nof 90% on our classification problem for nonlinear structural mechanics.\n", "versions": [{"version": "v1", "created": "Tue, 12 Jan 2021 15:09:11 GMT"}], "update_date": "2021-01-13", "authors_parsed": [["Daniel", "Thomas", ""], ["Casenave", "Fabien", ""], ["Akkari", "Nissrine", ""], ["Ryckelynck", "David", ""]]}, {"id": "2101.04535", "submitter": "Milad Nasr", "authors": "Milad Nasr, Shuang Song, Abhradeep Thakurta, Nicolas Papernot and\n  Nicholas Carlini", "title": "Adversary Instantiation: Lower Bounds for Differentially Private Machine\n  Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Differentially private (DP) machine learning allows us to train models on\nprivate data while limiting data leakage. DP formalizes this data leakage\nthrough a cryptographic game, where an adversary must predict if a model was\ntrained on a dataset D, or a dataset D' that differs in just one example.If\nobserving the training algorithm does not meaningfully increase the adversary's\nodds of successfully guessing which dataset the model was trained on, then the\nalgorithm is said to be differentially private. Hence, the purpose of privacy\nanalysis is to upper bound the probability that any adversary could\nsuccessfully guess which dataset the model was trained on.In our paper, we\ninstantiate this hypothetical adversary in order to establish lower bounds on\nthe probability that this distinguishing game can be won. We use this adversary\nto evaluate the importance of the adversary capabilities allowed in the privacy\nanalysis of DP training algorithms.For DP-SGD, the most common method for\ntraining neural networks with differential privacy, our lower bounds are tight\nand match the theoretical upper bound. This implies that in order to prove\nbetter upper bounds, it will be necessary to make use of additional\nassumptions. Fortunately, we find that our attacks are significantly weaker\nwhen additional (realistic)restrictions are put in place on the adversary's\ncapabilities.Thus, in the practical setting common to many real-world\ndeployments, there is a gap between our lower bounds and the upper bounds\nprovided by the analysis: differential privacy is conservative and adversaries\nmay not be able to leak as much information as suggested by the theoretical\nbound.\n", "versions": [{"version": "v1", "created": "Mon, 11 Jan 2021 18:47:11 GMT"}], "update_date": "2021-01-13", "authors_parsed": [["Nasr", "Milad", ""], ["Song", "Shuang", ""], ["Thakurta", "Abhradeep", ""], ["Papernot", "Nicolas", ""], ["Carlini", "Nicholas", ""]]}, {"id": "2101.04562", "submitter": "Wei Peng", "authors": "Wei Peng, Tuomas Varanka, Abdelrahman Mostafa, Henglin Shi, Guoying\n  Zhao", "title": "Hyperbolic Deep Neural Networks: A Survey", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Recently, there has been a rising surge of momentum for deep representation\nlearning in hyperbolic spaces due to theirhigh capacity of modeling data like\nknowledge graphs or synonym hierarchies, possessing hierarchical structure. We\nrefer to the model as hyperbolic deep neural network in this paper. Such a\nhyperbolic neural architecture potentially leads to drastically compact model\nwithmuch more physical interpretability than its counterpart in Euclidean\nspace. To stimulate future research, this paper presents acoherent and\ncomprehensive review of the literature around the neural components in the\nconstruction of hyperbolic deep neuralnetworks, as well as the generalization\nof the leading deep approaches to the Hyperbolic space. It also presents\ncurrent applicationsaround various machine learning tasks on several publicly\navailable datasets, together with insightful observations and identifying\nopenquestions and promising future directions.\n", "versions": [{"version": "v1", "created": "Tue, 12 Jan 2021 15:55:16 GMT"}, {"version": "v2", "created": "Thu, 14 Jan 2021 10:03:17 GMT"}, {"version": "v3", "created": "Wed, 17 Feb 2021 14:59:23 GMT"}], "update_date": "2021-02-18", "authors_parsed": [["Peng", "Wei", ""], ["Varanka", "Tuomas", ""], ["Mostafa", "Abdelrahman", ""], ["Shi", "Henglin", ""], ["Zhao", "Guoying", ""]]}, {"id": "2101.04595", "submitter": "Roland Pulch", "authors": "Roland Pulch and Maha Youssef", "title": "Machine Learning for Initial Value Problems of Parameter-Dependent\n  Dynamical Systems", "comments": "11 pages, 5 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NA math.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider initial value problems of nonlinear dynamical systems, which\ninclude physical parameters. A quantity of interest depending on the solution\nis observed. A discretisation yields the trajectories of the quantity of\ninterest in many time points. We examine the mapping from the set of parameters\nto the discrete values of the trajectories. An evaluation of this mapping\nrequires to solve an initial value problem. Alternatively, we determine an\napproximation, where the evaluation requires low computation work, using a\nconcept of machine learning. We employ feedforward neural networks, which are\nfitted to data from samples of the trajectories. Results of numerical\ncomputations are presented for a test example modelling an electric circuit.\n", "versions": [{"version": "v1", "created": "Tue, 12 Jan 2021 16:50:58 GMT"}], "update_date": "2021-01-13", "authors_parsed": [["Pulch", "Roland", ""], ["Youssef", "Maha", ""]]}, {"id": "2101.04615", "submitter": "Chau-Wai Wong", "authors": "Jiele Wu, Chau-Wai Wong, Xinyan Zhao, Xianpeng Liu", "title": "Toward Effective Automated Content Analysis via Crowdsourcing", "comments": "Corrected minor typos. Camera-ready version for the 2021 IEEE\n  International Conference on Multimedia and Expo (ICME)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Many computer scientists use the aggregated answers of online workers to\nrepresent ground truth. Prior work has shown that aggregation methods such as\nmajority voting are effective for measuring relatively objective features. For\nsubjective features such as semantic connotation, online workers, known for\noptimizing their hourly earnings, tend to deteriorate in the quality of their\nresponses as they work longer. In this paper, we aim to address this issue by\nproposing a quality-aware semantic data annotation system. We observe that with\ntimely feedback on workers' performance quantified by quality scores, better\ninformed online workers can maintain the quality of their labeling throughout\nan extended period of time. We validate the effectiveness of the proposed\nannotation system through i) evaluating performance based on an expert-labeled\ndataset, and ii) demonstrating machine learning tasks that can lead to\nconsistent learning behavior with 70%-80% accuracy. Our results suggest that\nwith our system, researchers can collect high-quality answers of subjective\nsemantic features at a large scale.\n", "versions": [{"version": "v1", "created": "Tue, 12 Jan 2021 17:14:18 GMT"}, {"version": "v2", "created": "Sun, 4 Apr 2021 23:59:01 GMT"}], "update_date": "2021-04-06", "authors_parsed": [["Wu", "Jiele", ""], ["Wong", "Chau-Wai", ""], ["Zhao", "Xinyan", ""], ["Liu", "Xianpeng", ""]]}, {"id": "2101.04617", "submitter": "Zhi Hong", "authors": "Zhi Hong, J. Gregory Pauloski, Logan Ward, Kyle Chard, Ben Blaiszik,\n  and Ian Foster", "title": "AI- and HPC-enabled Lead Generation for SARS-CoV-2: Models and Processes\n  to Extract Druglike Molecules Contained in Natural Language Text", "comments": "17 single-column pages, 6 figures, and 6 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Researchers worldwide are seeking to repurpose existing drugs or discover new\ndrugs to counter the disease caused by severe acute respiratory syndrome\ncoronavirus 2 (SARS-CoV-2). A promising source of candidates for such studies\nis molecules that have been reported in the scientific literature to be\ndrug-like in the context of coronavirus research. We report here on a project\nthat leverages both human and artificial intelligence to detect references to\ndrug-like molecules in free text. We engage non-expert humans to create a\ncorpus of labeled text, use this labeled corpus to train a named entity\nrecognition model, and employ the trained model to extract 10912 drug-like\nmolecules from the COVID-19 Open Research Dataset Challenge (CORD-19) corpus of\n198875 papers. Performance analyses show that our automated extraction model\ncan achieve performance on par with that of non-expert humans.\n", "versions": [{"version": "v1", "created": "Tue, 12 Jan 2021 17:15:43 GMT"}], "update_date": "2021-01-13", "authors_parsed": [["Hong", "Zhi", ""], ["Pauloski", "J. Gregory", ""], ["Ward", "Logan", ""], ["Chard", "Kyle", ""], ["Blaiszik", "Ben", ""], ["Foster", "Ian", ""]]}, {"id": "2101.04627", "submitter": "Majid Raeis", "authors": "Majid Raeis, Ali Tizghadam, Alberto Leon-Garcia", "title": "Queue-Learning: A Reinforcement Learning Approach for Providing Quality\n  of Service", "comments": "8 pages, Accepted to AAAI 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.NI cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  End-to-end delay is a critical attribute of quality of service (QoS) in\napplication domains such as cloud computing and computer networks. This metric\nis particularly important in tandem service systems, where the end-to-end\nservice is provided through a chain of services. Service-rate control is a\ncommon mechanism for providing QoS guarantees in service systems. In this\npaper, we introduce a reinforcement learning-based (RL-based) service-rate\ncontroller that provides probabilistic upper-bounds on the end-to-end delay of\nthe system, while preventing the overuse of service resources. In order to have\na general framework, we use queueing theory to model the service systems.\nHowever, we adopt an RL-based approach to avoid the limitations of\nqueueing-theoretic methods. In particular, we use Deep Deterministic Policy\nGradient (DDPG) to learn the service rates (action) as a function of the queue\nlengths (state) in tandem service systems. In contrast to existing RL-based\nmethods that quantify their performance by the achieved overall reward, which\ncould be hard to interpret or even misleading, our proposed controller provides\nexplicit probabilistic guarantees on the end-to-end delay of the system. The\nevaluations are presented for a tandem queueing system with non-exponential\ninter-arrival and service times, the results of which validate our controller's\ncapability in meeting QoS constraints.\n", "versions": [{"version": "v1", "created": "Tue, 12 Jan 2021 17:28:57 GMT"}], "update_date": "2021-01-13", "authors_parsed": [["Raeis", "Majid", ""], ["Tizghadam", "Ali", ""], ["Leon-Garcia", "Alberto", ""]]}, {"id": "2101.04632", "submitter": "Fares Ben Slimane", "authors": "Fares Ben Slimane and Mohamed Bouguessa", "title": "Context Matters: Self-Attention for Sign Language Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes an attentional network for the task of Continuous Sign\nLanguage Recognition. The proposed approach exploits co-independent streams of\ndata to model the sign language modalities. These different channels of\ninformation can share a complex temporal structure between each other. For that\nreason, we apply attention to synchronize and help capture entangled\ndependencies between the different sign language components. Even though Sign\nLanguage is multi-channel, handshapes represent the central entities in sign\ninterpretation. Seeing handshapes in their correct context defines the meaning\nof a sign. Taking that into account, we utilize the attention mechanism to\nefficiently aggregate the hand features with their appropriate spatio-temporal\ncontext for better sign recognition. We found that by doing so the model is\nable to identify the essential Sign Language components that revolve around the\ndominant hand and the face areas. We test our model on the benchmark dataset\nRWTH-PHOENIX-Weather 2014, yielding competitive results.\n", "versions": [{"version": "v1", "created": "Tue, 12 Jan 2021 17:40:19 GMT"}], "update_date": "2021-01-13", "authors_parsed": [["Slimane", "Fares Ben", ""], ["Bouguessa", "Mohamed", ""]]}, {"id": "2101.04635", "submitter": "Thijs-Enagnon Nassi", "authors": "Thijs E Nassi, Wolfgang Ganglberger, Haoqi Sun, Abigail A Bucklin,\n  Siddharth Biswal, Michel J A M van Putten, Robert J Thomas, M Brandon\n  Westover", "title": "Automated Respiratory Event Detection Using Deep Neural Networks", "comments": "11 pages, 6 figures, 6 tables, \\c{opyright}2020 IEEE", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG q-bio.QM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The gold standard to assess respiration during sleep is polysomnography; a\ntechnique that is burdensome, expensive (both in analysis time and measurement\ncosts), and difficult to repeat. Automation of respiratory analysis can improve\ntest efficiency and enable accessible implementation opportunities worldwide.\nUsing 9,656 polysomnography recordings from the Massachusetts General Hospital\n(MGH), we trained a neural network (WaveNet) based on a single respiratory\neffort belt to detect obstructive apnea, central apnea, hypopnea and\nrespiratory-effort related arousals. Performance evaluation included\nevent-based and recording-based metrics - using an apnea-hypopnea index\nanalysis. The model was further evaluated on a public dataset, the\nSleep-Heart-Health-Study-1, containing 8,455 polysomnographic recordings. For\nbinary apnea event detection in the MGH dataset, the neural network obtained an\naccuracy of 95%, an apnea-hypopnea index $r^2$ of 0.89 and area under the curve\nfor the receiver operating characteristics curve and precision-recall curve of\n0.93 and 0.74, respectively. For the multiclass task, we obtained varying\nperformances: 81% of all labeled central apneas were correctly classified,\nwhereas this metric was 46% for obstructive apneas, 29% for respiratory effort\nrelated arousals and 16% for hypopneas. The majority of false predictions were\nmisclassifications as another type of respiratory event. Our fully automated\nmethod can detect respiratory events and assess the apnea-hypopnea index with\nsufficient accuracy for clinical utilization. Differentiation of event types is\nmore difficult and may reflect in part the complexity of human respiratory\noutput and some degree of arbitrariness in the clinical thresholds and criteria\nused during manual annotation.\n", "versions": [{"version": "v1", "created": "Tue, 12 Jan 2021 17:43:17 GMT"}], "update_date": "2021-01-13", "authors_parsed": [["Nassi", "Thijs E", ""], ["Ganglberger", "Wolfgang", ""], ["Sun", "Haoqi", ""], ["Bucklin", "Abigail A", ""], ["Biswal", "Siddharth", ""], ["van Putten", "Michel J A M", ""], ["Thomas", "Robert J", ""], ["Westover", "M Brandon", ""]]}, {"id": "2101.04645", "submitter": "Jan-Philipp Schulze", "authors": "J.-P. Schulze, P. Sperl, K. B\\\"ottinger", "title": "Double-Adversarial Activation Anomaly Detection: Adversarial\n  Autoencoders are Anomaly Generators", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Anomaly detection is a challenging task for machine learning algorithms due\nto the inherent class imbalance. It is costly and time-demanding to manually\nanalyse the observed data, thus usually only few known anomalies if any are\navailable. Inspired by generative models and the analysis of the hidden\nactivations of neural networks, we introduce a novel unsupervised anomaly\ndetection method called DA3D. Here, we use adversarial autoencoders to generate\nanomalous counterexamples based on the normal data only. These artificial\nanomalies used during training allow the detection of real, yet unseen\nanomalies. With our novel generative approach, we transform the unsupervised\ntask of anomaly detection to a supervised one, which is more tractable by\nmachine learning and especially deep learning methods. DA3D surpasses the\nperformance of state-of-the-art anomaly detection methods in a purely\ndata-driven way, where no domain knowledge is required.\n", "versions": [{"version": "v1", "created": "Tue, 12 Jan 2021 18:07:34 GMT"}, {"version": "v2", "created": "Sun, 4 Apr 2021 17:05:36 GMT"}], "update_date": "2021-04-06", "authors_parsed": [["Schulze", "J. -P.", ""], ["Sperl", "P.", ""], ["B\u00f6ttinger", "K.", ""]]}, {"id": "2101.04653", "submitter": "Jan-Matthis Lueckmann", "authors": "Jan-Matthis Lueckmann, Jan Boelts, David S. Greenberg, Pedro J.\n  Gon\\c{c}alves, Jakob H. Macke", "title": "Benchmarking Simulation-Based Inference", "comments": "In AISTATS 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent advances in probabilistic modelling have led to a large number of\nsimulation-based inference algorithms which do not require numerical evaluation\nof likelihoods. However, a public benchmark with appropriate performance\nmetrics for such 'likelihood-free' algorithms has been lacking. This has made\nit difficult to compare algorithms and identify their strengths and weaknesses.\nWe set out to fill this gap: We provide a benchmark with inference tasks and\nsuitable performance metrics, with an initial selection of algorithms including\nrecent approaches employing neural networks and classical Approximate Bayesian\nComputation methods. We found that the choice of performance metric is\ncritical, that even state-of-the-art algorithms have substantial room for\nimprovement, and that sequential estimation improves sample efficiency. Neural\nnetwork-based approaches generally exhibit better performance, but there is no\nuniformly best algorithm. We provide practical advice and highlight the\npotential of the benchmark to diagnose problems and improve algorithms. The\nresults can be explored interactively on a companion website. All code is open\nsource, making it possible to contribute further benchmark tasks and inference\nalgorithms.\n", "versions": [{"version": "v1", "created": "Tue, 12 Jan 2021 18:31:22 GMT"}, {"version": "v2", "created": "Fri, 9 Apr 2021 15:07:54 GMT"}], "update_date": "2021-04-12", "authors_parsed": [["Lueckmann", "Jan-Matthis", ""], ["Boelts", "Jan", ""], ["Greenberg", "David S.", ""], ["Gon\u00e7alves", "Pedro J.", ""], ["Macke", "Jakob H.", ""]]}, {"id": "2101.04667", "submitter": "Emmanouil Vasileios Vlatakis Gkaragkounis", "authors": "Angeliki Giannou, Emmanouil-Vasileios Vlatakis-Gkaragkounis, Panayotis\n  Mertikopoulos", "title": "Survival of the strictest: Stable and unstable equilibria under\n  regularized learning with partial information", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GT cs.LG cs.MA math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we examine the Nash equilibrium convergence properties of\nno-regret learning in general N-player games. For concreteness, we focus on the\narchetypal follow the regularized leader (FTRL) family of algorithms, and we\nconsider the full spectrum of uncertainty that the players may encounter - from\nnoisy, oracle-based feedback, to bandit, payoff-based information. In this\ngeneral context, we establish a comprehensive equivalence between the stability\nof a Nash equilibrium and its support: a Nash equilibrium is stable and\nattracting with arbitrarily high probability if and only if it is strict (i.e.,\neach equilibrium strategy has a unique best response). This equivalence extends\nexisting continuous-time versions of the folk theorem of evolutionary game\ntheory to a bona fide algorithmic learning setting, and it provides a clear\nrefinement criterion for the prediction of the day-to-day behavior of no-regret\nlearning in games\n", "versions": [{"version": "v1", "created": "Tue, 12 Jan 2021 18:55:11 GMT"}, {"version": "v2", "created": "Thu, 4 Feb 2021 14:45:34 GMT"}], "update_date": "2021-02-05", "authors_parsed": [["Giannou", "Angeliki", ""], ["Vlatakis-Gkaragkounis", "Emmanouil-Vasileios", ""], ["Mertikopoulos", "Panayotis", ""]]}, {"id": "2101.04699", "submitter": "Daniel Osaku", "authors": "D. Osaku, J.F. Gomes, A.X. Falc\\~ao", "title": "Convolutional Neural Network Simplification with Progressive Retraining", "comments": "7 pages, 4 figures. This paper was submitted to Pattern Recognition\n  Letters", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Kernel pruning methods have been proposed to speed up, simplify, and improve\nexplanation of convolutional neural network (CNN) models. However, the\neffectiveness of a simplified model is often below the original one. In this\nletter, we present new methods based on objective and subjective relevance\ncriteria for kernel elimination in a layer-by-layer fashion. During the\nprocess, a CNN model is retrained only when the current layer is entirely\nsimplified, by adjusting the weights from the next layer to the first one and\npreserving weights of subsequent layers not involved in the process. We call\nthis strategy \\emph{progressive retraining}, differently from kernel pruning\nmethods that usually retrain the entire model after each simplification action\n-- e.g., the elimination of one or a few kernels. Our subjective relevance\ncriterion exploits the ability of humans in recognizing visual patterns and\nimproves the designer's understanding of the simplification process. The\ncombination of suitable relevance criteria and progressive retraining shows\nthat our methods can increase effectiveness with considerable model\nsimplification. We also demonstrate that our methods can provide better results\nthan two popular ones and another one from the state-of-the-art using four\nchallenging image datasets.\n", "versions": [{"version": "v1", "created": "Tue, 12 Jan 2021 19:05:42 GMT"}], "update_date": "2021-01-15", "authors_parsed": [["Osaku", "D.", ""], ["Gomes", "J. F.", ""], ["Falc\u00e3o", "A. X.", ""]]}, {"id": "2101.04713", "submitter": "David Torpey", "authors": "David Torpey and Richard Klein", "title": "Explicit homography estimation improves contrastive self-supervised\n  learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The typical contrastive self-supervised algorithm uses a similarity measure\nin latent space as the supervision signal by contrasting positive and negative\nimages directly or indirectly. Although the utility of self-supervised\nalgorithms has improved recently, there are still bottlenecks hindering their\nwidespread use, such as the compute needed. In this paper, we propose a module\nthat serves as an additional objective in the self-supervised contrastive\nlearning paradigm. We show how the inclusion of this module to regress the\nparameters of an affine transformation or homography, in addition to the\noriginal contrastive objective, improves both performance and learning speed.\nImportantly, we ensure that this module does not enforce invariance to the\nvarious components of the affine transform, as this is not always ideal. We\ndemonstrate the effectiveness of the additional objective on two recent,\npopular self-supervised algorithms. We perform an extensive experimental\nanalysis of the proposed method and show an improvement in performance for all\nconsidered datasets. Further, we find that although both the general homography\nand affine transformation are sufficient to improve performance and\nconvergence, the affine transformation performs better in all cases.\n", "versions": [{"version": "v1", "created": "Tue, 12 Jan 2021 19:33:37 GMT"}], "update_date": "2021-01-14", "authors_parsed": [["Torpey", "David", ""], ["Klein", "Richard", ""]]}, {"id": "2101.04724", "submitter": "Davide Piras", "authors": "Davide Piras, Alessio Spurio Mancini, Benjamin Joachimi, Michael P.\n  Hobson", "title": "Towards fast machine-learning-assisted Bayesian posterior inference of\n  realistic microseismic events", "comments": "13 pages, 11 figures, 2 tables. Under review", "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.geo-ph cs.LG physics.data-an", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Bayesian inference applied to microseismic activity monitoring allows for\nprincipled estimation of the coordinates of microseismic events from recorded\nseismograms, and their associated uncertainties. However, forward modelling of\nthese microseismic events, necessary to perform Bayesian source inversion, can\nbe prohibitively expensive in terms of computational resources. A viable\nsolution is to train a surrogate model based on machine learning techniques, to\nemulate the forward model and thus accelerate Bayesian inference. In this\npaper, we improve on previous work, which considered only sources with\nisotropic moment tensor. We train a machine learning algorithm on the power\nspectrum of the recorded pressure wave and show that the trained emulator\nallows for the complete and fast retrieval of the event coordinates for\n$\\textit{any}$ source mechanism. Moreover, we show that our approach is\ncomputationally inexpensive, as it can be run in less than 1 hour on a\ncommercial laptop, while yielding accurate results using less than $10^4$\ntraining seismograms. We additionally demonstrate how the trained emulators can\nbe used to identify the source mechanism through the estimation of the Bayesian\nevidence. This work lays the foundations for the efficient localisation and\ncharacterisation of any recorded seismogram, thus helping to quantify human\nimpact on seismic activity and mitigate seismic hazard.\n", "versions": [{"version": "v1", "created": "Tue, 12 Jan 2021 19:51:32 GMT"}], "update_date": "2021-01-14", "authors_parsed": [["Piras", "Davide", ""], ["Mancini", "Alessio Spurio", ""], ["Joachimi", "Benjamin", ""], ["Hobson", "Michael P.", ""]]}, {"id": "2101.04737", "submitter": "Jaehyuk Park", "authors": "Jaehyuk Park, Bogdan State, Monica Bhole, Michael C. Bailey, and\n  Yong-Yeol Ahn", "title": "People, Places, and Ties: Landscape of social places and their social\n  network structures", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Due to their essential role as places for socialization, \"third places\" -\nsocial places where people casually visit and communicate with friends and\nneighbors - have been studied by a wide range of fields including network\nscience, sociology, geography, urban planning, and regional studies. However,\nthe lack of a large-scale census on third places kept researchers from\nsystematic investigations. Here we provide a systematic nationwide\ninvestigation of third places and their social networks, by using Facebook\npages. Our analysis reveals a large degree of geographic heterogeneity in the\ndistribution of the types of third places, which is highly correlated with\nbaseline demographics and county characteristics. Certain types of pages like\n\"Places of Worship\" demonstrate a large degree of clustering suggesting\ncommunity preference or potential complementarities to concentration. We also\nfound that the social networks of different types of social place differ in\nimportant ways: The social networks of 'Restaurants' and 'Indoor Recreation'\npages are more likely to be tight-knit communities of pre-existing friendships\nwhereas 'Places of Worship' and 'Community Amenities' page categories are more\nlikely to bridge new friendship ties. We believe that this study can serve as\nan important milestone for future studies on the systematic comparative study\nof social spaces and their social relationships.\n", "versions": [{"version": "v1", "created": "Tue, 12 Jan 2021 20:21:42 GMT"}], "update_date": "2021-01-14", "authors_parsed": [["Park", "Jaehyuk", ""], ["State", "Bogdan", ""], ["Bhole", "Monica", ""], ["Bailey", "Michael C.", ""], ["Ahn", "Yong-Yeol", ""]]}, {"id": "2101.04750", "submitter": "Matt Peng", "authors": "Matt Peng, Banghua Zhu, Jiantao Jiao", "title": "Linear Representation Meta-Reinforcement Learning for Instant Adaptation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces Fast Linearized Adaptive Policy (FLAP), a new\nmeta-reinforcement learning (meta-RL) method that is able to extrapolate well\nto out-of-distribution tasks without the need to reuse data from training, and\nadapt almost instantaneously with the need of only a few samples during\ntesting. FLAP builds upon the idea of learning a shared linear representation\nof the policy so that when adapting to a new task, it suffices to predict a set\nof linear weights. A separate adapter network is trained simultaneously with\nthe policy such that during adaptation, we can directly use the adapter network\nto predict these linear weights instead of updating a meta-policy via gradient\ndescent, such as in prior meta-RL methods like MAML, to obtain the new policy.\nThe application of the separate feed-forward network not only speeds up the\nadaptation run-time significantly, but also generalizes extremely well to very\ndifferent tasks that prior Meta-RL methods fail to generalize to. Experiments\non standard continuous-control meta-RL benchmarks show FLAP presenting\nsignificantly stronger performance on out-of-distribution tasks with up to\ndouble the average return and up to 8X faster adaptation run-time speeds when\ncompared to prior methods.\n", "versions": [{"version": "v1", "created": "Tue, 12 Jan 2021 20:56:34 GMT"}], "update_date": "2021-01-14", "authors_parsed": [["Peng", "Matt", ""], ["Zhu", "Banghua", ""], ["Jiao", "Jiantao", ""]]}, {"id": "2101.04757", "submitter": "Yuyang Wang", "authors": "Yuyang Wang, Kenji Shimada, Amir Barati Farimani", "title": "Airfoil GAN: Encoding and Synthesizing Airfoils forAerodynamic-aware\n  Shape Optimization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CE cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The current design of aerodynamic shapes, like airfoils, involves\ncomputationally intensive simulations to explore the possible design space.\nUsually, such design relies on the prior definition of design parameters and\nplaces restrictions on synthesizing novel shapes. In this work, we propose a\ndata-driven shape encoding and generating method, which automatically learns\nrepresentations from existing airfoils and uses the learned representations to\ngenerate new airfoils. The representations are then used in the optimization of\nsynthesized airfoil shapes based on their aerodynamic performance. Our model is\nbuilt upon VAEGAN, a neural network that combines Variational Autoencoder with\nGenerative Adversarial Network and is trained by the gradient-based technique.\nOur model can (1) encode the existing airfoil into a latent vector and\nreconstruct the airfoil from that, (2) generate novel airfoils by randomly\nsampling the latent vectors and mapping the vectors to the airfoil coordinate\ndomain, and (3) synthesize airfoils with desired aerodynamic properties by\noptimizing learned features via a genetic algorithm. Our experiments show that\nthe learned features encode shape information thoroughly and comprehensively\nwithout predefined design parameters. By interpolating/extrapolating feature\nvectors or sampling from Gaussian noises, the model can automatically\nsynthesize novel airfoil shapes, some of which possess competitive or even\nbetter aerodynamic properties comparing with training airfoils. By optimizing\nshape on learned features via a genetic algorithm, synthesized airfoils can\nevolve to have specific aerodynamic properties, which can guide designing\naerodynamic products effectively and efficiently.\n", "versions": [{"version": "v1", "created": "Tue, 12 Jan 2021 21:25:45 GMT"}], "update_date": "2021-01-14", "authors_parsed": [["Wang", "Yuyang", ""], ["Shimada", "Kenji", ""], ["Farimani", "Amir Barati", ""]]}, {"id": "2101.04770", "submitter": "Ignacio Rodr\\'iguez Dr.", "authors": "Ignacio Rodriguez", "title": "Forecasting blood sugar levels in Diabetes with univariate algorithms", "comments": "8 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  AI procedures joined with wearable gadgets can convey exact transient blood\nglucose level forecast models. Also, such models can learn customized\nglucose-insulin elements dependent on the sensor information gathered by\nobserving a few parts of the physiological condition and every day movement of\na person. Up to this point, the predominant methodology for creating\ninformation driven forecast models was to gather \"however much information as\ncould be expected\" to help doctors and patients ideally change treatment. The\ngoal of this work was to examine the base information assortment, volume, and\nspeed needed to accomplish exact individual driven diminutive term expectation\nmodels. We built up a progression of these models utilizing distinctive AI time\narrangement guaging strategies that are appropriate for execution inside a\nwearable processor. We completed a broad aloof patient checking concentrate in\ngenuine conditions to fabricate a strong informational collection. The\nexamination included a subset of type-1 diabetic subjects wearing a glimmer\nglucose checking framework. We directed a relative quantitative assessment of\nthe presentation of the created information driven expectation models and\ncomparing AI methods. Our outcomes show that precise momentary forecast can be\naccomplished by just checking interstitial glucose information over a brief\ntimeframe and utilizing a low examining recurrence. The models created can\nanticipate glucose levels inside a 15-minute skyline with a normal mistake as\nlow as 15.43 mg/dL utilizing just 24 memorable qualities gathered inside a time\nof 6 hours, and by expanding the inspecting recurrence to incorporate 72\nqualities, the normal blunder is limited to 10.15 mg/dL. Our forecast models\nare reasonable for execution inside a wearable gadget, requiring the base\nequipment necessities while simultaneously accomplishing high expectation\nprecision.\n", "versions": [{"version": "v1", "created": "Tue, 12 Jan 2021 21:52:57 GMT"}, {"version": "v2", "created": "Thu, 21 Jan 2021 05:35:55 GMT"}], "update_date": "2021-01-22", "authors_parsed": [["Rodriguez", "Ignacio", ""]]}, {"id": "2101.04785", "submitter": "Korneel Van Den Broek", "authors": "Korneel van den Broek", "title": "MP3net: coherent, minute-long music generation from raw audio with a\n  simple convolutional GAN", "comments": "11 pages, 8 figures, samples and source code available on\n  https://korneelvdbroek.github.io/mp3net", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.LG eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a deep convolutional GAN which leverages techniques from\nMP3/Vorbis audio compression to produce long, high-quality audio samples with\nlong-range coherence. The model uses a Modified Discrete Cosine Transform\n(MDCT) data representation, which includes all phase information. Phase\ngeneration is hence integral part of the model. We leverage the auditory\nmasking and psychoacoustic perception limit of the human ear to widen the true\ndistribution and stabilize the training process. The model architecture is a\ndeep 2D convolutional network, where each subsequent generator model block\nincreases the resolution along the time axis and adds a higher octave along the\nfrequency axis. The deeper layers are connected with all parts of the output\nand have the context of the full track. This enables generation of samples\nwhich exhibit long-range coherence. We use MP3net to create 95s stereo tracks\nwith a 22kHz sample rate after training for 250h on a single Cloud TPUv2. An\nadditional benefit of the CNN-based model architecture is that generation of\nnew songs is almost instantaneous.\n", "versions": [{"version": "v1", "created": "Tue, 12 Jan 2021 22:37:21 GMT"}], "update_date": "2021-01-14", "authors_parsed": [["Broek", "Korneel van den", ""]]}, {"id": "2101.04789", "submitter": "Mounia Hamidouche", "authors": "Mounia Hamidouche, Carlos Lassance, Yuqing Hu, Lucas Drumetz, Bastien\n  Pasdeloup, Vincent Gripon", "title": "Improving Classification Accuracy with Graph Filtering", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In machine learning, classifiers are typically susceptible to noise in the\ntraining data. In this work, we aim at reducing intra-class noise with the help\nof graph filtering to improve the classification performance. Considered graphs\nare obtained by connecting samples of the training set that belong to a same\nclass depending on the similarity of their representation in a latent space. We\nshow that the proposed graph filtering methodology has the effect of\nasymptotically reducing intra-class variance, while maintaining the mean. While\nour approach applies to all classification problems in general, it is\nparticularly useful in few-shot settings, where intra-class noise can have a\nhuge impact due to the small sample selection. Using standardized benchmarks in\nthe field of vision, we empirically demonstrate the ability of the proposed\nmethod to slightly improve state-of-the-art results in both cases of few-shot\nand standard classification.\n", "versions": [{"version": "v1", "created": "Tue, 12 Jan 2021 22:51:55 GMT"}, {"version": "v2", "created": "Mon, 25 Jan 2021 18:24:06 GMT"}], "update_date": "2021-01-26", "authors_parsed": [["Hamidouche", "Mounia", ""], ["Lassance", "Carlos", ""], ["Hu", "Yuqing", ""], ["Drumetz", "Lucas", ""], ["Pasdeloup", "Bastien", ""], ["Gripon", "Vincent", ""]]}, {"id": "2101.04792", "submitter": "Nikolay Mikhaylovskiy", "authors": "Roman Vygon, Nikolay Mikhaylovskiy", "title": "Learning Efficient Representations for Keyword Spotting with Triplet\n  Loss", "comments": "Submitted to SPECOM 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.AS cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the past few years, triplet loss-based metric embeddings have become a\nde-facto standard for several important computer vision problems, most\nno-tably, person reidentification. On the other hand, in the area of speech\nrecognition the metric embeddings generated by the triplet loss are rarely used\neven for classification problems. We fill this gap showing that a combination\nof two representation learning techniques: a triplet loss-based embedding and a\nvariant of kNN for classification instead of cross-entropy loss significantly\n(by 26% to 38%) improves the classification accuracy for convolutional networks\non a LibriSpeech-derived LibriWords datasets. To do so, we propose a novel\nphonetic similarity based triplet mining approach. We also improve the current\nbest published SOTA for Google Speech Commands dataset V1 10+2 -class\nclassification by about 34%, achieving 98.55% accuracy, V2 10+2-class\nclassification by about 20%, achieving 98.37% accuracy, and V2 35-class\nclassification by over 50%, achieving 97.0% accuracy.\n", "versions": [{"version": "v1", "created": "Tue, 12 Jan 2021 22:55:17 GMT"}, {"version": "v2", "created": "Sat, 30 Jan 2021 16:48:16 GMT"}, {"version": "v3", "created": "Fri, 16 Apr 2021 21:11:36 GMT"}, {"version": "v4", "created": "Fri, 4 Jun 2021 22:20:46 GMT"}], "update_date": "2021-06-08", "authors_parsed": [["Vygon", "Roman", ""], ["Mikhaylovskiy", "Nikolay", ""]]}, {"id": "2101.04799", "submitter": "Ananda Samajdar", "authors": "Ananda Samajdar, Michael Pellauer, Tushar Krishna", "title": "Self-Adaptive Reconfigurable Arrays (SARA): Using ML to Assist Scaling\n  GEMM Acceleration", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With increasing diversity in Deep Neural Network(DNN) models in terms of\nlayer shapes and sizes, the research community has been investigating\nflexible/reconfigurable accelerator substrates. This line of research has\nopened up two challenges. The first is to determine the appropriate amount of\nflexibility within an accelerator array that that can trade-off the performance\nbenefits versus the area overheads of the reconfigurability. The second is\nbeing able to determine the right configuration of the array for the current\nDNN model and/or layer and reconfigure the accelerator at runtime. This work\nintroduces a new class of accelerators that we call Self Adaptive\nReconfigurable Array (SARA). SARA architectures comprise of both a\nreconfigurable array and a hardware unit capable of determining an optimized\nconfiguration for the array at runtime. We demonstrate an instance of SARA with\nan accelerator we call SAGAR, which introduces a novel reconfigurable systolic\narray that can be configured to work as a distributed collection of smaller\narrays of various sizes or as a single array with flexible aspect ratios. We\nalso develop a novel recommendation neural network called ADAPTNET which\nrecommends an array configuration and dataflow for the current layer\nparameters. ADAPTNET runs on an integrated custom hardware ADAPTNETX that runs\nADAPTNET at runtime and reconfigures the array, making the entire accelerator\nself-sufficient. SAGAR is capable of providing the same mapping flexibility as\na collection of 10244x4 arrays working as a distributed system while achieving\n3.5x more power efficiency and 3.2x higher compute density Furthermore, the\nruntime achieved on the recommended parameters from ADAPTNET is 99.93% of the\nbest achievable runtime.\n", "versions": [{"version": "v1", "created": "Tue, 12 Jan 2021 23:20:23 GMT"}], "update_date": "2021-01-14", "authors_parsed": [["Samajdar", "Ananda", ""], ["Pellauer", "Michael", ""], ["Krishna", "Tushar", ""]]}, {"id": "2101.04800", "submitter": "Nicolas Tobis", "authors": "Ognjen Rudovic, Nicolas Tobis, Sebastian Kaltwang, Bj\\\"orn Schuller,\n  Daniel Rueckert, Jeffrey F. Cohn and Rosalind W. Picard", "title": "Personalized Federated Deep Learning for Pain Estimation From Face\n  Images", "comments": "12 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Standard machine learning approaches require centralizing the users' data in\none computer or a shared database, which raises data privacy and\nconfidentiality concerns. Therefore, limiting central access is important,\nespecially in healthcare settings, where data regulations are strict. A\npotential approach to tackling this is Federated Learning (FL), which enables\nmultiple parties to collaboratively learn a shared prediction model by using\nparameters of locally trained models while keeping raw training data locally.\nIn the context of AI-assisted pain-monitoring, we wish to enable\nconfidentiality-preserving and unobtrusive pain estimation for long-term\npain-monitoring and reduce the burden on the nursing staff who perform frequent\nroutine check-ups. To this end, we propose a novel Personalized Federated Deep\nLearning (PFDL) approach for pain estimation from face images. PFDL performs\ncollaborative training of a deep model, implemented using a lightweight CNN\narchitecture, across different clients (i.e., subjects) without sharing their\nface images. Instead of sharing all parameters of the model, as in standard FL,\nPFDL retains the last layer locally (used to personalize the pain estimates).\nThis (i) adds another layer of data confidentiality, making it difficult for an\nadversary to infer pain levels of the target subject, while (ii) personalizing\nthe pain estimation to each subject through local parameter tuning. We show\nusing a publicly available dataset of face videos of pain (UNBC-McMaster\nShoulder Pain Database), that PFDL performs comparably or better than the\nstandard centralized and FL algorithms, while further enhancing data privacy.\nThis, has the potential to improve traditional pain monitoring by making it\nmore secure, computationally efficient, and scalable to a large number of\nindividuals (e.g., for in-home pain monitoring), providing timely and\nunobtrusive pain measurement.\n", "versions": [{"version": "v1", "created": "Tue, 12 Jan 2021 23:21:25 GMT"}], "update_date": "2021-01-14", "authors_parsed": [["Rudovic", "Ognjen", ""], ["Tobis", "Nicolas", ""], ["Kaltwang", "Sebastian", ""], ["Schuller", "Bj\u00f6rn", ""], ["Rueckert", "Daniel", ""], ["Cohn", "Jeffrey F.", ""], ["Picard", "Rosalind W.", ""]]}, {"id": "2101.04808", "submitter": "Mircea Trofin", "authors": "Mircea Trofin (1), Yundi Qian (1), Eugene Brevdo (1), Zinan Lin (2),\n  Krzysztof Choromanski (1), David Li (1) ((1) Google, Inc., (2) Carnegie\n  Mellon University)", "title": "MLGO: a Machine Learning Guided Compiler Optimizations Framework", "comments": "First two authors are equal contributors", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Leveraging machine-learning (ML) techniques for compiler optimizations has\nbeen widely studied and explored in academia. However, the adoption of ML in\ngeneral-purpose, industry strength compilers has yet to happen. We propose\nMLGO, a framework for integrating ML techniques systematically in an industrial\ncompiler -- LLVM. As a case study, we present the details and results of\nreplacing the heuristics-based inlining-for-size optimization in LLVM with\nmachine learned models. To the best of our knowledge, this work is the first\nfull integration of ML in a complex compiler pass in a real-world setting. It\nis available in the main LLVM repository. We use two different ML algorithms:\nPolicy Gradient and Evolution Strategies, to train the inlining-for-size model,\nand achieve up to 7\\% size reduction, when compared to state of the art LLVM\n-Oz. The same model, trained on one corpus, generalizes well to a diversity of\nreal-world targets, as well as to the same set of targets after months of\nactive development. This property of the trained models is beneficial to deploy\nML techniques in real-world settings.\n", "versions": [{"version": "v1", "created": "Wed, 13 Jan 2021 00:02:49 GMT"}], "update_date": "2021-01-14", "authors_parsed": [["Trofin", "Mircea", ""], ["Qian", "Yundi", ""], ["Brevdo", "Eugene", ""], ["Lin", "Zinan", ""], ["Choromanski", "Krzysztof", ""], ["Li", "David", ""]]}, {"id": "2101.04812", "submitter": "Casey Handmer", "authors": "Casey Handmer", "title": "Digital Elevation Model enhancement using Deep Learning", "comments": "11 pages, 13 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV astro-ph.EP cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We demonstrate high fidelity enhancement of planetary digital elevation\nmodels (DEMs) using optical images and deep learning with convolutional neural\nnetworks. Enhancement can be applied recursively to the limit of available\noptical data, representing a 90x resolution improvement in global Mars DEMs.\nDeep learning-based photoclinometry robustly recovers features obscured by\nnon-ideal lighting conditions. Method can be automated at global scale.\nAnalysis shows enhanced DEM slope errors are comparable with high resolution\nmaps using conventional, labor intensive methods.\n", "versions": [{"version": "v1", "created": "Wed, 13 Jan 2021 00:07:57 GMT"}], "update_date": "2021-01-14", "authors_parsed": [["Handmer", "Casey", ""]]}, {"id": "2101.04817", "submitter": "Yongfeng Zhang", "authors": "Yunqi Li, Shuyuan Xu, Bo Liu, Zuohui Fu, Shuchang Liu, Xu Chen,\n  Yongfeng Zhang", "title": "Discrete Knowledge Graph Embedding based on Discrete Optimization", "comments": "Accepted at the AAAI-20 Workshop on Knowledge Discovery from\n  Unstructured Data in Financial Services", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.AI cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a discrete knowledge graph (KG) embedding (DKGE) method,\nwhich projects KG entities and relations into the Hamming space based on a\ncomputationally tractable discrete optimization algorithm, to solve the\nformidable storage and computation cost challenges in traditional continuous\ngraph embedding methods. The convergence of DKGE can be guaranteed\ntheoretically. Extensive experiments demonstrate that DKGE achieves superior\naccuracy than classical hashing functions that map the effective continuous\nembeddings into discrete codes. Besides, DKGE reaches comparable accuracy with\nmuch lower computational complexity and storage compared to many continuous\ngraph embedding methods.\n", "versions": [{"version": "v1", "created": "Wed, 13 Jan 2021 00:23:07 GMT"}], "update_date": "2021-01-14", "authors_parsed": [["Li", "Yunqi", ""], ["Xu", "Shuyuan", ""], ["Liu", "Bo", ""], ["Fu", "Zuohui", ""], ["Liu", "Shuchang", ""], ["Chen", "Xu", ""], ["Zhang", "Yongfeng", ""]]}, {"id": "2101.04824", "submitter": "Rodrigo de Lamare", "authors": "A. Danaee, R. C. de Lamare and V. H. Nascimento", "title": "Energy-Efficient Distributed Learning Algorithms for Coarsely Quantized\n  Signals", "comments": "5 pages, 4 figures. arXiv admin note: substantial text overlap with\n  arXiv:2012.10939", "journal-ref": null, "doi": "10.1109/LSP.2021.3051522", "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we present an energy-efficient distributed learning framework\nusing low-resolution ADCs and coarsely quantized signals for Internet of Things\n(IoT) networks. In particular, we develop a distributed quantization-aware\nleast-mean square (DQA-LMS) algorithm that can learn parameters in an\nenergy-efficient fashion using signals quantized with few bits while requiring\na low computational cost. We also carry out a statistical analysis of the\nproposed DQA-LMS algorithm that includes a stability condition. Simulations\nassess the DQA-LMS algorithm against existing techniques for a distributed\nparameter estimation task where IoT devices operate in a peer-to-peer mode and\ndemonstrate the effectiveness of the DQA-LMS algorithm.\n", "versions": [{"version": "v1", "created": "Wed, 13 Jan 2021 01:13:47 GMT"}], "update_date": "2021-02-24", "authors_parsed": [["Danaee", "A.", ""], ["de Lamare", "R. C.", ""], ["Nascimento", "V. H.", ""]]}, {"id": "2101.04829", "submitter": "Junyoung Byun", "authors": "Junyoung Byun, Hyojun Go, Changick Kim", "title": "Small Input Noise is Enough to Defend Against Query-based Black-box\n  Attacks", "comments": "14 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While deep neural networks show unprecedented performance in various tasks,\nthe vulnerability to adversarial examples hinders their deployment in\nsafety-critical systems. Many studies have shown that attacks are also possible\neven in a black-box setting where an adversary cannot access the target model's\ninternal information. Most black-box attacks are based on queries, each of\nwhich obtains the target model's output for an input, and many recent studies\nfocus on reducing the number of required queries. In this paper, we pay\nattention to an implicit assumption of these attacks that the target model's\noutput exactly corresponds to the query input. If some randomness is introduced\ninto the model to break this assumption, query-based attacks may have\ntremendous difficulty in both gradient estimation and local search, which are\nthe core of their attack process. From this motivation, we observe even a small\nadditive input noise can neutralize most query-based attacks and name this\nsimple yet effective approach Small Noise Defense (SND). We analyze how SND can\ndefend against query-based black-box attacks and demonstrate its effectiveness\nagainst eight different state-of-the-art attacks with CIFAR-10 and ImageNet\ndatasets. Even with strong defense ability, SND almost maintains the original\nclean accuracy and computational speed. SND is readily applicable to\npre-trained models by adding only one line of code at the inference stage, so\nwe hope that it will be used as a baseline of defense against query-based\nblack-box attacks in the future.\n", "versions": [{"version": "v1", "created": "Wed, 13 Jan 2021 01:45:59 GMT"}], "update_date": "2021-01-14", "authors_parsed": [["Byun", "Junyoung", ""], ["Go", "Hyojun", ""], ["Kim", "Changick", ""]]}, {"id": "2101.04834", "submitter": "Doris Xin", "authors": "Doris Xin, Eva Yiwei Wu, Doris Jung-Lin Lee, Niloufar Salehi, Aditya\n  Parameswaran", "title": "Whither AutoML? Understanding the Role of Automation in Machine Learning\n  Workflows", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Efforts to make machine learning more widely accessible have led to a rapid\nincrease in Auto-ML tools that aim to automate the process of training and\ndeploying machine learning. To understand how Auto-ML tools are used in\npractice today, we performed a qualitative study with participants ranging from\nnovice hobbyists to industry researchers who use Auto-ML tools. We present\ninsights into the benefits and deficiencies of existing tools, as well as the\nrespective roles of the human and automation in ML workflows. Finally, we\ndiscuss design implications for the future of Auto-ML tool development. We\nargue that instead of full automation being the ultimate goal of Auto-ML,\ndesigners of these tools should focus on supporting a partnership between the\nuser and the Auto-ML tool. This means that a range of Auto-ML tools will need\nto be developed to support varying user goals such as simplicity,\nreproducibility, and reliability.\n", "versions": [{"version": "v1", "created": "Wed, 13 Jan 2021 02:12:46 GMT"}], "update_date": "2021-01-14", "authors_parsed": [["Xin", "Doris", ""], ["Wu", "Eva Yiwei", ""], ["Lee", "Doris Jung-Lin", ""], ["Salehi", "Niloufar", ""], ["Parameswaran", "Aditya", ""]]}, {"id": "2101.04840", "submitter": "Nazneen Fatema Rajani", "authors": "Karan Goel, Nazneen Rajani, Jesse Vig, Samson Tan, Jason Wu, Stephan\n  Zheng, Caiming Xiong, Mohit Bansal, Christopher R\\'e", "title": "Robustness Gym: Unifying the NLP Evaluation Landscape", "comments": "34 pages, 8 figures, 6 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite impressive performance on standard benchmarks, deep neural networks\nare often brittle when deployed in real-world systems. Consequently, recent\nresearch has focused on testing the robustness of such models, resulting in a\ndiverse set of evaluation methodologies ranging from adversarial attacks to\nrule-based data transformations. In this work, we identify challenges with\nevaluating NLP systems and propose a solution in the form of Robustness Gym\n(RG), a simple and extensible evaluation toolkit that unifies 4 standard\nevaluation paradigms: subpopulations, transformations, evaluation sets, and\nadversarial attacks. By providing a common platform for evaluation, Robustness\nGym enables practitioners to compare results from all 4 evaluation paradigms\nwith just a few clicks, and to easily develop and share novel evaluation\nmethods using a built-in set of abstractions. To validate Robustness Gym's\nutility to practitioners, we conducted a real-world case study with a\nsentiment-modeling team, revealing performance degradations of 18%+. To verify\nthat Robustness Gym can aid novel research analyses, we perform the first study\nof state-of-the-art commercial and academic named entity linking (NEL) systems,\nas well as a fine-grained analysis of state-of-the-art summarization models.\nFor NEL, commercial systems struggle to link rare entities and lag their\nacademic counterparts by 10%+, while state-of-the-art summarization models\nstruggle on examples that require abstraction and distillation, degrading by\n9%+. Robustness Gym can be found at https://robustnessgym.com/\n", "versions": [{"version": "v1", "created": "Wed, 13 Jan 2021 02:37:54 GMT"}], "update_date": "2021-01-14", "authors_parsed": [["Goel", "Karan", ""], ["Rajani", "Nazneen", ""], ["Vig", "Jesse", ""], ["Tan", "Samson", ""], ["Wu", "Jason", ""], ["Zheng", "Stephan", ""], ["Xiong", "Caiming", ""], ["Bansal", "Mohit", ""], ["R\u00e9", "Christopher", ""]]}, {"id": "2101.04844", "submitter": "Senwei Liang", "authors": "Senwei Liang and Liyao Lyu and Chunmei Wang and Haizhao Yang", "title": "Reproducing Activation Function for Deep Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.NA math.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose reproducing activation functions (RAFs) to improve deep learning\naccuracy for various applications ranging from computer vision to scientific\ncomputing. The idea is to employ several basic functions and their learnable\nlinear combination to construct neuron-wise data-driven activation functions\nfor each neuron. Armed with RAFs, neural networks (NNs) can reproduce\ntraditional approximation tools and, therefore, approximate target functions\nwith a smaller number of parameters than traditional NNs. In NN training, RAFs\ncan generate neural tangent kernels (NTKs) with a better condition number than\ntraditional activation functions lessening the spectral bias of deep learning.\nAs demonstrated by extensive numerical tests, the proposed RAFs can facilitate\nthe convergence of deep learning optimization for a solution with higher\naccuracy than existing deep learning solvers for audio/image/video\nreconstruction, PDEs, and eigenvalue problems. With RAFs, the errors of\naudio/video reconstruction, PDEs, and eigenvalue problems are decreased by over\n14%, 73%, 99%, respectively, compared with baseline, while the performance of\nimage reconstruction increases by 58%.\n", "versions": [{"version": "v1", "created": "Wed, 13 Jan 2021 02:53:22 GMT"}, {"version": "v2", "created": "Sun, 21 Feb 2021 04:29:58 GMT"}], "update_date": "2021-02-23", "authors_parsed": [["Liang", "Senwei", ""], ["Lyu", "Liyao", ""], ["Wang", "Chunmei", ""], ["Yang", "Haizhao", ""]]}, {"id": "2101.04849", "submitter": "Chen Ma", "authors": "Chen Ma, Liheng Ma, Yingxue Zhang, Ruiming Tang, Xue Liu and Mark\n  Coates", "title": "Probabilistic Metric Learning with Adaptive Margin for Top-K\n  Recommendation", "comments": "Accepted by the 26th ACM SIGKDD Conference on Knowledge Discovery and\n  Data Mining (KDD 2020 Research Track)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Personalized recommender systems are playing an increasingly important role\nas more content and services become available and users struggle to identify\nwhat might interest them. Although matrix factorization and deep learning based\nmethods have proved effective in user preference modeling, they violate the\ntriangle inequality and fail to capture fine-grained preference information. To\ntackle this, we develop a distance-based recommendation model with several\nnovel aspects: (i) each user and item are parameterized by Gaussian\ndistributions to capture the learning uncertainties; (ii) an adaptive margin\ngeneration scheme is proposed to generate the margins regarding different\ntraining triplets; (iii) explicit user-user/item-item similarity modeling is\nincorporated in the objective function. The Wasserstein distance is employed to\ndetermine preferences because it obeys the triangle inequality and can measure\nthe distance between probabilistic distributions. Via a comparison using five\nreal-world datasets with state-of-the-art methods, the proposed model\noutperforms the best existing models by 4-22% in terms of recall@K on Top-K\nrecommendation.\n", "versions": [{"version": "v1", "created": "Wed, 13 Jan 2021 03:11:04 GMT"}], "update_date": "2021-01-14", "authors_parsed": [["Ma", "Chen", ""], ["Ma", "Liheng", ""], ["Zhang", "Yingxue", ""], ["Tang", "Ruiming", ""], ["Liu", "Xue", ""], ["Coates", "Mark", ""]]}, {"id": "2101.04850", "submitter": "Ziyang Liu", "authors": "Ziyang Liu, Zhaomeng Cheng, Yunjiang Jiang, Yue Shang, Wei Xiong,\n  Sulong Xu, Bo Long, Di Jin", "title": "Heterogeneous Network Embedding for Deep Semantic Relevance Match in\n  E-commerce Search", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Result relevance prediction is an essential task of e-commerce search engines\nto boost the utility of search engines and ensure smooth user experience. The\nlast few years eyewitnessed a flurry of research on the use of\nTransformer-style models and deep text-match models to improve relevance.\nHowever, these two types of models ignored the inherent bipartite network\nstructures that are ubiquitous in e-commerce search logs, making these models\nineffective. We propose in this paper a novel Second-order Relevance, which is\nfundamentally different from the previous First-order Relevance, to improve\nresult relevance prediction. We design, for the first time, an end-to-end\nFirst-and-Second-order Relevance prediction model for e-commerce item\nrelevance. The model is augmented by the neighborhood structures of bipartite\nnetworks that are built using the information of user behavioral feedback,\nincluding clicks and purchases. To ensure that edges accurately encode\nrelevance information, we introduce external knowledge generated from BERT to\nrefine the network of user behaviors. This allows the new model to integrate\ninformation from neighboring items and queries, which are highly relevant to\nthe focus query-item pair under consideration. Results of offline experiments\nshowed that the new model significantly improved the prediction accuracy in\nterms of human relevance judgment. An ablation study showed that the\nFirst-and-Second-order model gained a 4.3% average gain over the First-order\nmodel. Results of an online A/B test revealed that the new model derived more\ncommercial benefits compared to the base model.\n", "versions": [{"version": "v1", "created": "Wed, 13 Jan 2021 03:12:53 GMT"}], "update_date": "2021-01-14", "authors_parsed": [["Liu", "Ziyang", ""], ["Cheng", "Zhaomeng", ""], ["Jiang", "Yunjiang", ""], ["Shang", "Yue", ""], ["Xiong", "Wei", ""], ["Xu", "Sulong", ""], ["Long", "Bo", ""], ["Jin", "Di", ""]]}, {"id": "2101.04852", "submitter": "Chen Ma", "authors": "Chen Ma, Liheng Ma, Yingxue Zhang, Haolun Wu, Xue Liu and Mark Coates", "title": "Knowledge-Enhanced Top-K Recommendation in Poincar\\'e Ball", "comments": "Accepted by the 35th AAAI Conference on Artificial Intelligence (AAAI\n  2021)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Personalized recommender systems are increasingly important as more content\nand services become available and users struggle to identify what might\ninterest them. Thanks to the ability for providing rich information, knowledge\ngraphs (KGs) are being incorporated to enhance the recommendation performance\nand interpretability. To effectively make use of the knowledge graph, we\npropose a recommendation model in the hyperbolic space, which facilitates the\nlearning of the hierarchical structure of knowledge graphs. Furthermore, a\nhyperbolic attention network is employed to determine the relative importances\nof neighboring entities of a certain item. In addition, we propose an adaptive\nand fine-grained regularization mechanism to adaptively regularize items and\ntheir neighboring representations. Via a comparison using three real-world\ndatasets with state-of-the-art methods, we show that the proposed model\noutperforms the best existing models by 2-16% in terms of NDCG@K on Top-K\nrecommendation.\n", "versions": [{"version": "v1", "created": "Wed, 13 Jan 2021 03:16:50 GMT"}, {"version": "v2", "created": "Fri, 29 Jan 2021 20:41:23 GMT"}], "update_date": "2021-02-02", "authors_parsed": [["Ma", "Chen", ""], ["Ma", "Liheng", ""], ["Zhang", "Yingxue", ""], ["Wu", "Haolun", ""], ["Liu", "Xue", ""], ["Coates", "Mark", ""]]}, {"id": "2101.04853", "submitter": "Yiqin Yu", "authors": "Yiqin Yu, Pin-Yu Chen, Yuan Zhou, Jing Mei", "title": "Adversarial Sample Enhanced Domain Adaptation: A Case Study on\n  Predictive Modeling with Electronic Health Records", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  With the successful adoption of machine learning on electronic health records\n(EHRs), numerous computational models have been deployed to address a variety\nof clinical problems. However, due to the heterogeneity of EHRs, models trained\non different patient groups suffer from poor generalizability. How to mitigate\ndomain shifts between the source patient group where the model is built upon\nand the target one where the model will be deployed becomes a critical issue.\nIn this paper, we propose a data augmentation method to facilitate domain\nadaptation, which leverages knowledge from the source patient group when\ntraining model on the target one. Specifically, adversarially generated samples\nare used during domain adaptation to fill the generalization gap between the\ntwo patient groups. The proposed method is evaluated by a case study on\ndifferent predictive modeling tasks on MIMIC-III EHR dataset. Results confirm\nthe effectiveness of our method and the generality on different tasks.\n", "versions": [{"version": "v1", "created": "Wed, 13 Jan 2021 03:20:20 GMT"}], "update_date": "2021-01-14", "authors_parsed": [["Yu", "Yiqin", ""], ["Chen", "Pin-Yu", ""], ["Zhou", "Yuan", ""], ["Mei", "Jing", ""]]}, {"id": "2101.04856", "submitter": "Maxwell Emerson", "authors": "Maxwell Emerson, James M. Ferguson, Tayfun Efe Ertop, Margaret Rox,\n  Josephine Granna, Michael Lester, Fabien Maldonado, Erin A. Gillaspie, Ron\n  Alterovitz, Robert J. Webster III., and Alan Kuntz", "title": "A Recurrent Neural Network Approach to Roll Estimation for Needle\n  Steering", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.LG cs.SY eess.SP eess.SY", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Steerable needles are a promising technology for delivering targeted\ntherapies in the body in a minimally-invasive fashion, as they can curve around\nanatomical obstacles and hone in on anatomical targets. In order to accurately\nsteer them, controllers must have full knowledge of the needle tip's\norientation. However, current sensors either do not provide full orientation\ninformation or interfere with the needle's ability to deliver therapy. Further,\ntorsional dynamics can vary and depend on many parameters making steerable\nneedles difficult to accurately model, limiting the effectiveness of\ntraditional observer methods. To overcome these limitations, we propose a\nmodel-free, learned-method that leverages LSTM neural networks to estimate the\nneedle tip's orientation online. We validate our method by integrating it into\na sliding-mode controller and steering the needle to targets in gelatin and ex\nvivo ovine brain tissue. We compare our method's performance against an\nExtended Kalman Filter, a model-based observer, achieving significantly lower\ntargeting errors.\n", "versions": [{"version": "v1", "created": "Wed, 13 Jan 2021 03:40:00 GMT"}], "update_date": "2021-01-18", "authors_parsed": [["Emerson", "Maxwell", ""], ["Ferguson", "James M.", ""], ["Ertop", "Tayfun Efe", ""], ["Rox", "Margaret", ""], ["Granna", "Josephine", ""], ["Lester", "Michael", ""], ["Maldonado", "Fabien", ""], ["Gillaspie", "Erin A.", ""], ["Alterovitz", "Ron", ""], ["III.", "Robert J. Webster", ""], ["Kuntz", "Alan", ""]]}, {"id": "2101.04859", "submitter": "Arun Raja", "authors": "Govind Narasimman, Kangkang Lu, Arun Raja, Chuan Sheng Foo, Mohamed\n  Sabry Aly, Jie Lin, Vijay Chandrasekhar", "title": "A*HAR: A New Benchmark towards Semi-supervised learning for\n  Class-imbalanced Human Activity Recognition", "comments": "5 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG eess.SP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Despite the vast literature on Human Activity Recognition (HAR) with wearable\ninertial sensor data, it is perhaps surprising that there are few studies\ninvestigating semisupervised learning for HAR, particularly in a challenging\nscenario with class imbalance problem. In this work, we present a new\nbenchmark, called A*HAR, towards semisupervised learning for class-imbalanced\nHAR. We evaluate state-of-the-art semi-supervised learning method on A*HAR, by\ncombining Mean Teacher and Convolutional Neural Network. Interestingly, we find\nthat Mean Teacher boosts the overall performance when training the classifier\nwith fewer labelled samples and a large amount of unlabeled samples, but the\nclassifier falls short in handling unbalanced activities. These findings lead\nto an interesting open problem, i.e., development of semi-supervised HAR\nalgorithms that are class-imbalance aware without any prior knowledge on the\nclass distribution for unlabeled samples. The dataset and benchmark evaluation\nare released at https://github.com/I2RDL2/ASTAR-HAR for future research.\n", "versions": [{"version": "v1", "created": "Wed, 13 Jan 2021 03:56:49 GMT"}], "update_date": "2021-01-14", "authors_parsed": [["Narasimman", "Govind", ""], ["Lu", "Kangkang", ""], ["Raja", "Arun", ""], ["Foo", "Chuan Sheng", ""], ["Aly", "Mohamed Sabry", ""], ["Lin", "Jie", ""], ["Chandrasekhar", "Vijay", ""]]}, {"id": "2101.04866", "submitter": "Dian Shi", "authors": "Dian Shi, Liang Li, Rui Chen, Pavana Prakash, Miao Pan, Yuguang Fang", "title": "Towards Energy Efficient Federated Learning over 5G+ Mobile Devices", "comments": "submitted to IEEE Wireless Communications", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The continuous convergence of machine learning algorithms, 5G and beyond\n(5G+) wireless communications, and artificial intelligence (AI) hardware\nimplementation hastens the birth of federated learning (FL) over 5G+ mobile\ndevices, which pushes AI functions to mobile devices and initiates a new era of\non-device AI applications. Despite the remarkable progress made in FL, huge\nenergy consumption is one of the most significant obstacles restricting the\ndevelopment of FL over battery-constrained 5G+ mobile devices. To address this\nissue, in this paper, we investigate how to develop energy efficient FL over\n5G+ mobile devices by making a trade-off between energy consumption for\n\"working\" (i.e., local computing) and that for \"talking\" (i.e., wireless\ncommunications) in order to boost the overall energy efficiency. Specifically,\nwe first examine energy consumption models for graphics processing unit (GPU)\ncomputation and wireless transmissions. Then, we overview the state of the art\nof integrating FL procedure with energy-efficient learning techniques (e.g.,\ngradient sparsification, weight quantization, pruning, etc.). Finally, we\npresent several potential future research directions for FL over 5G+ mobile\ndevices from the perspective of energy efficiency.\n", "versions": [{"version": "v1", "created": "Wed, 13 Jan 2021 04:13:54 GMT"}], "update_date": "2021-01-14", "authors_parsed": [["Shi", "Dian", ""], ["Li", "Liang", ""], ["Chen", "Rui", ""], ["Prakash", "Pavana", ""], ["Pan", "Miao", ""], ["Fang", "Yuguang", ""]]}, {"id": "2101.04869", "submitter": "Shengli Jiang", "authors": "Shengli Jiang and Victor M. Zavala", "title": "Convolutional Neural Nets in Chemical Engineering: Foundations,\n  Computations, and Applications", "comments": null, "journal-ref": "AIChE J. 2021; e17282", "doi": "10.1002/aic.17282", "report-no": null, "categories": "cs.LG cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper we review the mathematical foundations of convolutional neural\nnets (CNNs) with the goals of: i) highlighting connections with techniques from\nstatistics, signal processing, linear algebra, differential equations, and\noptimization, ii) demystifying underlying computations, and iii) identifying\nnew types of applications. CNNs are powerful machine learning models that\nhighlight features from grid data to make predictions (regression and\nclassification). The grid data object can be represented as vectors (in 1D),\nmatrices (in 2D), or tensors (in 3D or higher dimensions) and can incorporate\nmultiple channels (thus providing high flexibility in the input data\nrepresentation). CNNs highlight features from the grid data by performing\nconvolution operations with different types of operators. The operators\nhighlight different types of features (e.g., patterns, gradients, geometrical\nfeatures) and are learned by using optimization techniques. In other words,\nCNNs seek to identify optimal operators that best map the input data to the\noutput data. A common misconception is that CNNs are only capable of processing\nimage or video data but their application scope is much wider; specifically,\ndatasets encountered in diverse applications can be expressed as grid data.\nHere, we show how to apply CNNs to new types of applications such as optimal\ncontrol, flow cytometry, multivariate process monitoring, and molecular\nsimulations.\n", "versions": [{"version": "v1", "created": "Wed, 13 Jan 2021 04:20:42 GMT"}, {"version": "v2", "created": "Wed, 7 Jul 2021 14:06:33 GMT"}], "update_date": "2021-07-08", "authors_parsed": [["Jiang", "Shengli", ""], ["Zavala", "Victor M.", ""]]}, {"id": "2101.04879", "submitter": "Xiaoxuan Zhang", "authors": "Xiaoxuan Zhang, Krishna Garikipati", "title": "Bayesian neural networks for weak solution of PDEs with uncertainty\n  quantification", "comments": "37 pages, 25 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CE cs.LG physics.comp-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Solving partial differential equations (PDEs) is the canonical approach for\nunderstanding the behavior of physical systems. However, large scale solutions\nof PDEs using state of the art discretization techniques remains an expensive\nproposition. In this work, a new physics-constrained neural network (NN)\napproach is proposed to solve PDEs without labels, with a view to enabling\nhigh-throughput solutions in support of design and decision-making. Distinct\nfrom existing physics-informed NN approaches, where the strong form or weak\nform of PDEs are used to construct the loss function, we write the loss\nfunction of NNs based on the discretized residual of PDEs through an efficient,\nconvolutional operator-based, and vectorized implementation. We explore an\nencoder-decoder NN structure for both deterministic and probabilistic models,\nwith Bayesian NNs (BNNs) for the latter, which allow us to quantify both\nepistemic uncertainty from model parameters and aleatoric uncertainty from\nnoise in the data. For BNNs, the discretized residual is used to construct the\nlikelihood function. In our approach, both deterministic and probabilistic\nconvolutional layers are used to learn the applied boundary conditions (BCs)\nand to detect the problem domain. As both Dirichlet and Neumann BCs are\nspecified as inputs to NNs, a single NN can solve for similar physics, but with\ndifferent BCs and on a number of problem domains. The trained surrogate PDE\nsolvers can also make interpolating and extrapolating (to a certain extent)\npredictions for BCs that they were not exposed to during training. Such\nsurrogate models are of particular importance for problems, where similar types\nof PDEs need to be repeatedly solved for many times with slight variations. We\ndemonstrate the capability and performance of the proposed framework by\napplying it to steady-state diffusion, linear elasticity, and nonlinear\nelasticity.\n", "versions": [{"version": "v1", "created": "Wed, 13 Jan 2021 04:57:51 GMT"}], "update_date": "2021-01-14", "authors_parsed": [["Zhang", "Xiaoxuan", ""], ["Garikipati", "Krishna", ""]]}, {"id": "2101.04882", "submitter": "Lilian Weng", "authors": "OpenAI OpenAI, Matthias Plappert, Raul Sampedro, Tao Xu, Ilge Akkaya,\n  Vineet Kosaraju, Peter Welinder, Ruben D'Sa, Arthur Petron, Henrique P. d.O.\n  Pinto, Alex Paino, Hyeonwoo Noh, Lilian Weng, Qiming Yuan, Casey Chu,\n  Wojciech Zaremba", "title": "Asymmetric self-play for automatic goal discovery in robotic\n  manipulation", "comments": "Videos are shown at https://robotics-self-play.github.io", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We train a single, goal-conditioned policy that can solve many robotic\nmanipulation tasks, including tasks with previously unseen goals and objects.\nWe rely on asymmetric self-play for goal discovery, where two agents, Alice and\nBob, play a game. Alice is asked to propose challenging goals and Bob aims to\nsolve them. We show that this method can discover highly diverse and complex\ngoals without any human priors. Bob can be trained with only sparse rewards,\nbecause the interaction between Alice and Bob results in a natural curriculum\nand Bob can learn from Alice's trajectory when relabeled as a goal-conditioned\ndemonstration. Finally, our method scales, resulting in a single policy that\ncan generalize to many unseen tasks such as setting a table, stacking blocks,\nand solving simple puzzles. Videos of a learned policy is available at\nhttps://robotics-self-play.github.io.\n", "versions": [{"version": "v1", "created": "Wed, 13 Jan 2021 05:20:20 GMT"}], "update_date": "2021-03-09", "authors_parsed": [["OpenAI", "OpenAI", ""], ["Plappert", "Matthias", ""], ["Sampedro", "Raul", ""], ["Xu", "Tao", ""], ["Akkaya", "Ilge", ""], ["Kosaraju", "Vineet", ""], ["Welinder", "Peter", ""], ["D'Sa", "Ruben", ""], ["Petron", "Arthur", ""], ["Pinto", "Henrique P. d. O.", ""], ["Paino", "Alex", ""], ["Noh", "Hyeonwoo", ""], ["Weng", "Lilian", ""], ["Yuan", "Qiming", ""], ["Chu", "Casey", ""], ["Zaremba", "Wojciech", ""]]}, {"id": "2101.04884", "submitter": "Paritosh Parmar", "authors": "Paritosh Parmar, Jaiden Reddy, Brendan Morris", "title": "Piano Skills Assessment", "comments": "Dataset is available from:\n  https://github.com/ParitoshParmar/Piano-Skills-Assessment", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.MM cs.SD eess.AS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Can a computer determine a piano player's skill level? Is it preferable to\nbase this assessment on visual analysis of the player's performance or should\nwe trust our ears over our eyes? Since current CNNs have difficulty processing\nlong video videos, how can shorter clips be sampled to best reflect the players\nskill level? In this work, we collect and release a first-of-its-kind dataset\nfor multimodal skill assessment focusing on assessing piano player's skill\nlevel, answer the asked questions, initiate work in automated evaluation of\npiano playing skills and provide baselines for future work. Dataset is\navailable from: https://github.com/ParitoshParmar/Piano-Skills-Assessment.\n", "versions": [{"version": "v1", "created": "Wed, 13 Jan 2021 05:26:29 GMT"}, {"version": "v2", "created": "Mon, 21 Jun 2021 01:57:59 GMT"}], "update_date": "2021-06-22", "authors_parsed": [["Parmar", "Paritosh", ""], ["Reddy", "Jaiden", ""], ["Morris", "Brendan", ""]]}, {"id": "2101.04889", "submitter": "Yuzhou Lin", "authors": "Yuzhou Lin, Xiaolin Chang", "title": "Towards Interpretable Ensemble Learning for Image-based Malware\n  Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning (DL) models for image-based malware detection have exhibited\ntheir capability in producing high prediction accuracy. But model\ninterpretability is posing challenges to their widespread application in\nsecurity and safety-critical application domains. This paper aims for designing\nan Interpretable Ensemble learning approach for image-based Malware Detection\n(IEMD). We first propose a Selective Deep Ensemble Learning-based (SDEL)\ndetector and then design an Ensemble Deep Taylor Decomposition (EDTD) approach,\nwhich can give the pixel-level explanation to SDEL detector outputs.\nFurthermore, we develop formulas for calculating fidelity, robustness and\nexpressiveness on pixel-level heatmaps in order to assess the quality of EDTD\nexplanation. With EDTD explanation, we develop a novel Interpretable Dropout\napproach (IDrop), which establishes IEMD by training SDEL detector. Experiment\nresults exhibit the better explanation of our EDTD than the previous\nexplanation methods for image-based malware detection. Besides, experiment\nresults indicate that IEMD achieves a higher detection accuracy up to 99.87%\nwhile exhibiting interpretability with high quality of prediction results.\nMoreover, experiment results indicate that IEMD interpretability increases with\nthe increasing detection accuracy during the construction of IEMD. This\nconsistency suggests that IDrop can mitigate the tradeoff between model\ninterpretability and detection accuracy.\n", "versions": [{"version": "v1", "created": "Wed, 13 Jan 2021 05:46:44 GMT"}], "update_date": "2021-01-14", "authors_parsed": [["Lin", "Yuzhou", ""], ["Chang", "Xiaolin", ""]]}, {"id": "2101.04898", "submitter": "Hanxun Huang", "authors": "Hanxun Huang, Xingjun Ma, Sarah Monazam Erfani, James Bailey, Yisen\n  Wang", "title": "Unlearnable Examples: Making Personal Data Unexploitable", "comments": "ICLR2021, In International Conference on Learning Representations", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR cs.CV stat.ML", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  The volume of \"free\" data on the internet has been key to the current success\nof deep learning. However, it also raises privacy concerns about the\nunauthorized exploitation of personal data for training commercial models. It\nis thus crucial to develop methods to prevent unauthorized data exploitation.\nThis paper raises the question: \\emph{can data be made unlearnable for deep\nlearning models?} We present a type of \\emph{error-minimizing} noise that can\nindeed make training examples unlearnable. Error-minimizing noise is\nintentionally generated to reduce the error of one or more of the training\nexample(s) close to zero, which can trick the model into believing there is\n\"nothing\" to learn from these example(s). The noise is restricted to be\nimperceptible to human eyes, and thus does not affect normal data utility. We\nempirically verify the effectiveness of error-minimizing noise in both\nsample-wise and class-wise forms. We also demonstrate its flexibility under\nextensive experimental settings and practicability in a case study of face\nrecognition. Our work establishes an important first step towards making\npersonal data unexploitable to deep learning models.\n", "versions": [{"version": "v1", "created": "Wed, 13 Jan 2021 06:15:56 GMT"}, {"version": "v2", "created": "Wed, 24 Feb 2021 22:53:31 GMT"}], "update_date": "2021-02-26", "authors_parsed": [["Huang", "Hanxun", ""], ["Ma", "Xingjun", ""], ["Erfani", "Sarah Monazam", ""], ["Bailey", "James", ""], ["Wang", "Yisen", ""]]}, {"id": "2101.04899", "submitter": "Raviraj Joshi", "authors": "Atharva Kulkarni, Meet Mandhane, Manali Likhitkar, Gayatri Kshirsagar,\n  Jayashree Jagdale, Raviraj Joshi", "title": "Experimental Evaluation of Deep Learning models for Marathi Text\n  Classification", "comments": "Accepted at ICMISC 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Marathi language is one of the prominent languages used in India. It is\npredominantly spoken by the people of Maharashtra. Over the past decade, the\nusage of language on online platforms has tremendously increased. However,\nresearch on Natural Language Processing (NLP) approaches for Marathi text has\nnot received much attention. Marathi is a morphologically rich language and\nuses a variant of the Devanagari script in the written form. This works aims to\nprovide a comprehensive overview of available resources and models for Marathi\ntext classification. We evaluate CNN, LSTM, ULMFiT, and BERT based models on\ntwo publicly available Marathi text classification datasets and present a\ncomparative analysis. The pre-trained Marathi fast text word embeddings by\nFacebook and IndicNLP are used in conjunction with word-based models. We show\nthat basic single layer models based on CNN and LSTM coupled with FastText\nembeddings perform on par with the BERT based models on the available datasets.\nWe hope our paper aids focused research and experiments in the area of Marathi\nNLP.\n", "versions": [{"version": "v1", "created": "Wed, 13 Jan 2021 06:21:27 GMT"}, {"version": "v2", "created": "Thu, 14 Jan 2021 13:08:27 GMT"}], "update_date": "2021-01-15", "authors_parsed": [["Kulkarni", "Atharva", ""], ["Mandhane", "Meet", ""], ["Likhitkar", "Manali", ""], ["Kshirsagar", "Gayatri", ""], ["Jagdale", "Jayashree", ""], ["Joshi", "Raviraj", ""]]}, {"id": "2101.04904", "submitter": "Ali Ayub", "authors": "Ali Ayub, Alan R. Wagner", "title": "EEC: Learning to Encode and Regenerate Images for Continual Learning", "comments": "Added link to the code in the paper. A preliminary version of this\n  work was presented at ICML 2020 Workshop on Lifelong Machine Learning:\n  arXiv:2007.06637", "journal-ref": "International Conference on Learning Representations (ICLR) 2021", "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The two main impediments to continual learning are catastrophic forgetting\nand memory limitations on the storage of data. To cope with these challenges,\nwe propose a novel, cognitively-inspired approach which trains autoencoders\nwith Neural Style Transfer to encode and store images. During training on a new\ntask, reconstructed images from encoded episodes are replayed in order to avoid\ncatastrophic forgetting. The loss function for the reconstructed images is\nweighted to reduce its effect during classifier training to cope with image\ndegradation. When the system runs out of memory the encoded episodes are\nconverted into centroids and covariance matrices, which are used to generate\npseudo-images during classifier training, keeping classifier performance stable\nwhile using less memory. Our approach increases classification accuracy by\n13-17% over state-of-the-art methods on benchmark datasets, while requiring 78%\nless storage space.\n", "versions": [{"version": "v1", "created": "Wed, 13 Jan 2021 06:43:10 GMT"}, {"version": "v2", "created": "Thu, 14 Jan 2021 09:16:24 GMT"}, {"version": "v3", "created": "Mon, 5 Apr 2021 05:05:05 GMT"}, {"version": "v4", "created": "Sun, 2 May 2021 05:45:03 GMT"}], "update_date": "2021-05-04", "authors_parsed": [["Ayub", "Ali", ""], ["Wagner", "Alan R.", ""]]}, {"id": "2101.04909", "submitter": "Anuroop Sriram", "authors": "Anuroop Sriram, Matthew Muckley, Koustuv Sinha, Farah Shamout, Joelle\n  Pineau, Krzysztof J. Geras, Lea Azour, Yindalon Aphinyanaphongs, Nafissa\n  Yakubova, William Moore", "title": "COVID-19 Prognosis via Self-Supervised Representation Learning and\n  Multi-Image Prediction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The rapid spread of COVID-19 cases in recent months has strained hospital\nresources, making rapid and accurate triage of patients presenting to emergency\ndepartments a necessity. Machine learning techniques using clinical data such\nas chest X-rays have been used to predict which patients are most at risk of\ndeterioration. We consider the task of predicting two types of patient\ndeterioration based on chest X-rays: adverse event deterioration (i.e.,\ntransfer to the intensive care unit, intubation, or mortality) and increased\noxygen requirements beyond 6 L per day. Due to the relative scarcity of\nCOVID-19 patient data, existing solutions leverage supervised pretraining on\nrelated non-COVID images, but this is limited by the differences between the\npretraining data and the target COVID-19 patient data. In this paper, we use\nself-supervised learning based on the momentum contrast (MoCo) method in the\npretraining phase to learn more general image representations to use for\ndownstream tasks. We present three results. The first is deterioration\nprediction from a single image, where our model achieves an area under receiver\noperating characteristic curve (AUC) of 0.742 for predicting an adverse event\nwithin 96 hours (compared to 0.703 with supervised pretraining) and an AUC of\n0.765 for predicting oxygen requirements greater than 6 L a day at 24 hours\n(compared to 0.749 with supervised pretraining). We then propose a new\ntransformer-based architecture that can process sequences of multiple images\nfor prediction and show that this model can achieve an improved AUC of 0.786\nfor predicting an adverse event at 96 hours and an AUC of 0.848 for predicting\nmortalities at 96 hours. A small pilot clinical study suggested that the\nprediction accuracy of our model is comparable to that of experienced\nradiologists analyzing the same information.\n", "versions": [{"version": "v1", "created": "Wed, 13 Jan 2021 07:03:17 GMT"}, {"version": "v2", "created": "Mon, 25 Jan 2021 04:52:53 GMT"}], "update_date": "2021-01-26", "authors_parsed": [["Sriram", "Anuroop", ""], ["Muckley", "Matthew", ""], ["Sinha", "Koustuv", ""], ["Shamout", "Farah", ""], ["Pineau", "Joelle", ""], ["Geras", "Krzysztof J.", ""], ["Azour", "Lea", ""], ["Aphinyanaphongs", "Yindalon", ""], ["Yakubova", "Nafissa", ""], ["Moore", "William", ""]]}, {"id": "2101.04921", "submitter": "Segwang Kim", "authors": "Segwang Kim, Hyoungwook Nam, Joonyoung Kim, Kyomin Jung", "title": "Neural Sequence-to-grid Module for Learning Symbolic Rules", "comments": "9 pages, 9 figures, AAAI 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Logical reasoning tasks over symbols, such as learning arithmetic operations\nand computer program evaluations, have become challenges to deep learning. In\nparticular, even state-of-the-art neural networks fail to achieve\n\\textit{out-of-distribution} (OOD) generalization of symbolic reasoning tasks,\nwhereas humans can easily extend learned symbolic rules. To resolve this\ndifficulty, we propose a neural sequence-to-grid (seq2grid) module, an input\npreprocessor that automatically segments and aligns an input sequence into a\ngrid. As our module outputs a grid via a novel differentiable mapping, any\nneural network structure taking a grid input, such as ResNet or TextCNN, can be\njointly trained with our module in an end-to-end fashion. Extensive experiments\nshow that neural networks having our module as an input preprocessor achieve\nOOD generalization on various arithmetic and algorithmic problems including\nnumber sequence prediction problems, algebraic word problems, and computer\nprogram evaluation problems while other state-of-the-art sequence transduction\nmodels cannot. Moreover, we verify that our module enhances TextCNN to solve\nthe bAbI QA tasks without external memory.\n", "versions": [{"version": "v1", "created": "Wed, 13 Jan 2021 07:53:14 GMT"}, {"version": "v2", "created": "Mon, 26 Apr 2021 23:10:25 GMT"}], "update_date": "2021-04-28", "authors_parsed": [["Kim", "Segwang", ""], ["Nam", "Hyoungwook", ""], ["Kim", "Joonyoung", ""], ["Jung", "Kyomin", ""]]}, {"id": "2101.04932", "submitter": "Erez Shmueli Dr.", "authors": "Marcelo Bacher, Irad Ben-Gal, Erez Shmueli", "title": "A Non-Parametric Subspace Analysis Approach with Application to Anomaly\n  Detection Ensembles", "comments": "41 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Identifying anomalies in multi-dimensional datasets is an important task in\nmany real-world applications. A special case arises when anomalies are occluded\nin a small set of attributes, typically referred to as a subspace, and not\nnecessarily over the entire data space. In this paper, we propose a new\nsubspace analysis approach named Agglomerative Attribute Grouping (AAG) that\naims to address this challenge by searching for subspaces that are comprised of\nhighly correlative attributes. Such correlations among attributes represent a\nsystematic interaction among the attributes that can better reflect the\nbehavior of normal observations and hence can be used to improve the\nidentification of two particularly interesting types of abnormal data samples:\nanomalies that are occluded in relatively small subsets of the attributes and\nanomalies that represent a new data class. AAG relies on a novel\nmulti-attribute measure, which is derived from information theory measures of\npartitions, for evaluating the \"information distance\" between groups of data\nattributes. To determine the set of subspaces to use, AAG applies a variation\nof the well-known agglomerative clustering algorithm with the proposed\nmulti-attribute measure as the underlying distance function. Finally, the set\nof subspaces is used in an ensemble for anomaly detection. Extensive evaluation\ndemonstrates that, in the vast majority of cases, the proposed AAG method (i)\noutperforms classical and state-of-the-art subspace analysis methods when used\nin anomaly detection ensembles, and (ii) generates fewer subspaces with a fewer\nnumber of attributes each (on average), thus resulting in a faster training\ntime for the anomaly detection ensemble. Furthermore, in contrast to existing\nmethods, the proposed AAG method does not require any tuning of parameters.\n", "versions": [{"version": "v1", "created": "Wed, 13 Jan 2021 08:23:01 GMT"}], "update_date": "2021-01-14", "authors_parsed": [["Bacher", "Marcelo", ""], ["Ben-Gal", "Irad", ""], ["Shmueli", "Erez", ""]]}, {"id": "2101.04938", "submitter": "Markus L\\\"oning", "authors": "Franz J. Kir\\'aly, Markus L\\\"oning, Anthony Blaom, Ahmed Guecioueur,\n  Raphael Sonabend", "title": "Designing Machine Learning Toolboxes: Concepts, Principles and Patterns", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SE cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Machine learning (ML) and AI toolboxes such as scikit-learn or Weka are\nworkhorses of contemporary data scientific practice -- their central role being\nenabled by usable yet powerful designs that allow to easily specify, train and\nvalidate complex modeling pipelines. However, despite their universal success,\nthe key design principles in their construction have never been fully analyzed.\nIn this paper, we attempt to provide an overview of key patterns in the design\nof AI modeling toolboxes, taking inspiration, in equal parts, from the field of\nsoftware engineering, implementation patterns found in contemporary toolboxes,\nand our own experience from developing ML toolboxes. In particular, we develop\na conceptual model for the AI/ML domain, with a new type system, called\nscientific types, at its core. Scientific types capture the scientific meaning\nof common elements in ML workflows based on the set of operations that we\nusually perform with them (i.e. their interface) and their statistical\nproperties. From our conceptual analysis, we derive a set of design principles\nand patterns. We illustrate that our analysis can not only explain the design\nof existing toolboxes, but also guide the development of new ones. We intend\nour contribution to be a state-of-art reference for future toolbox engineers, a\nsummary of best practices, a collection of ML design patterns which may become\nuseful for future research, and, potentially, the first steps towards a\nhigher-level programming paradigm for constructing AI.\n", "versions": [{"version": "v1", "created": "Wed, 13 Jan 2021 08:55:15 GMT"}], "update_date": "2021-01-14", "authors_parsed": [["Kir\u00e1ly", "Franz J.", ""], ["L\u00f6ning", "Markus", ""], ["Blaom", "Anthony", ""], ["Guecioueur", "Ahmed", ""], ["Sonabend", "Raphael", ""]]}, {"id": "2101.04948", "submitter": "Foozhan Ataiefard", "authors": "Mohammad Jafar Mashhadi, Foozhan Ataiefard, Hadi Hemmati and Niel\n  Walkinshaw", "title": "Behavioral Model Inference of Black-box Software using Deep Neural\n  Networks", "comments": "16 pages,8 figures. arXiv admin note: text overlap with\n  arXiv:2008.11856", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.SE", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Many software engineering tasks, such as testing, and anomaly detection can\nbenefit from the ability to infer a behavioral model of the software.Most\nexisting inference approaches assume access to code to collect execution\nsequences. In this paper, we investigate a black-box scenario, where the system\nunder analysis cannot be instrumented, in this granular fashion.This scenario\nis particularly prevalent with control systems' log analysis in the form of\ncontinuous signals. In this situation, an execution trace amounts to a\nmultivariate time-series of input and output signals, where different states of\nthe system correspond to different `phases` in the time-series. The main\nchallenge is to detect when these phase changes take place. Unfortunately, most\nexisting solutions are either univariate, make assumptions on the data\ndistribution, or have limited learning power.Therefore, we propose a hybrid\ndeep neural network that accepts as input a multivariate time series and\napplies a set of convolutional and recurrent layers to learn the non-linear\ncorrelations between signals and the patterns over time.We show how this\napproach can be used to accurately detect state changes, and how the inferred\nmodels can be successfully applied to transfer-learning scenarios, to\naccurately process traces from different products with similar execution\ncharacteristics. Our experimental results on two UAV autopilot case studies\nindicate that our approach is highly accurate (over 90% F1 score for state\nclassification) and significantly improves baselines (by up to 102% for change\npoint detection).Using transfer learning we also show that up to 90% of the\nmaximum achievable F1 scores in the open-source case study can be achieved by\nreusing the trained models from the industrial case and only fine tuning them\nusing as low as 5 labeled samples, which reduces the manual labeling effort by\n98%.\n", "versions": [{"version": "v1", "created": "Wed, 13 Jan 2021 09:23:37 GMT"}], "update_date": "2021-01-14", "authors_parsed": [["Mashhadi", "Mohammad Jafar", ""], ["Ataiefard", "Foozhan", ""], ["Hemmati", "Hadi", ""], ["Walkinshaw", "Niel", ""]]}, {"id": "2101.04968", "submitter": "Dominic Richards", "authors": "Dominic Richards, Mike Rabbat", "title": "Learning with Gradient Descent and Weakly Convex Losses", "comments": "Updated References", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.ST stat.TH", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  We study the learning performance of gradient descent when the empirical risk\nis weakly convex, namely, the smallest negative eigenvalue of the empirical\nrisk's Hessian is bounded in magnitude. By showing that this eigenvalue can\ncontrol the stability of gradient descent, generalisation error bounds are\nproven that hold under a wider range of step sizes compared to previous work.\nOut of sample guarantees are then achieved by decomposing the test error into\ngeneralisation, optimisation and approximation errors, each of which can be\nbounded and traded off with respect to algorithmic parameters, sample size and\nmagnitude of this eigenvalue. In the case of a two layer neural network, we\ndemonstrate that the empirical risk can satisfy a notion of local weak\nconvexity, specifically, the Hessian's smallest eigenvalue during training can\nbe controlled by the normalisation of the layers, i.e., network scaling. This\nallows test error guarantees to then be achieved when the population risk\nminimiser satisfies a complexity assumption. By trading off the network\ncomplexity and scaling, insights are gained into the implicit bias of neural\nnetwork scaling, which are further supported by experimental findings.\n", "versions": [{"version": "v1", "created": "Wed, 13 Jan 2021 09:58:06 GMT"}, {"version": "v2", "created": "Tue, 1 Jun 2021 14:43:44 GMT"}], "update_date": "2021-06-02", "authors_parsed": [["Richards", "Dominic", ""], ["Rabbat", "Mike", ""]]}, {"id": "2101.04977", "submitter": "Sasho Nedelkoski", "authors": "Jasmin Bogatinovski and Sasho Nedelkoski", "title": "Multi-Source Anomaly Detection in Distributed IT Systems", "comments": "12 pages. Presented at AIOPS 2020 workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DC cs.SC cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The multi-source data generated by distributed systems, provide a holistic\ndescription of the system. Harnessing the joint distribution of the different\nmodalities by a learning model can be beneficial for critical applications for\nmaintenance of the distributed systems. One such important task is the task of\nanomaly detection where we are interested in detecting the deviation of the\ncurrent behaviour of the system from the theoretically expected. In this work,\nwe utilize the joint representation from the distributed traces and system log\ndata for the task of anomaly detection in distributed systems. We demonstrate\nthat the joint utilization of traces and logs produced better results compared\nto the single modality anomaly detection methods. Furthermore, we formalize a\nlearning task - next template prediction NTP, that is used as a generalization\nfor anomaly detection for both logs and distributed trace. Finally, we\ndemonstrate that this formalization allows for the learning of template\nembedding for both the traces and logs. The joint embeddings can be reused in\nother applications as good initialization for spans and logs.\n", "versions": [{"version": "v1", "created": "Wed, 13 Jan 2021 10:11:32 GMT"}], "update_date": "2021-01-14", "authors_parsed": [["Bogatinovski", "Jasmin", ""], ["Nedelkoski", "Sasho", ""]]}, {"id": "2101.04989", "submitter": "Nati Daniel", "authors": "Tomer Czyzewski, Nati Daniel, Mark Rochman, Julie M. Caldwell, Garrett\n  A. Osswald, Margaret H. Collins, Marc E. Rothenberg, and Yonatan Savir", "title": "Machine learning approach for biopsy-based identification of\n  eosinophilic esophagitis reveals importance of global features", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV q-bio.QM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Goal: Eosinophilic esophagitis (EoE) is an allergic inflammatory condition\ncharacterized by eosinophil accumulation in the esophageal mucosa. EoE\ndiagnosis includes a manual assessment of eosinophil levels in mucosal biopsies\n- a time-consuming, laborious task that is difficult to standardize. One of the\nmain challenges in automating this process, like many other biopsy-based\ndiagnostics, is detecting features that are small relative to the size of the\nbiopsy. Results: In this work, we utilized hematoxylin- and eosin-stained\nslides from esophageal biopsies from patients with active EoE and control\nsubjects to develop a platform based on a deep convolutional neural network\n(DCNN) that can classify esophageal biopsies with an accuracy of 85%,\nsensitivity of 82.5%, and specificity of 87%. Moreover, by combining several\ndownscaling and cropping strategies, we show that some of the features\ncontributing to the correct classification are global rather than specific,\nlocal features. Conclusions: We report the ability of artificial intelligence\nto identify EoE using computer vision analysis of esophageal biopsy slides.\nFurther, the DCNN features associated with EoE are based on not only local\neosinophils but also global histologic changes. Our approach can be used for\nother conditions that rely on biopsy-based histologic diagnostics.\n", "versions": [{"version": "v1", "created": "Wed, 13 Jan 2021 10:38:46 GMT"}], "update_date": "2021-01-14", "authors_parsed": [["Czyzewski", "Tomer", ""], ["Daniel", "Nati", ""], ["Rochman", "Mark", ""], ["Caldwell", "Julie M.", ""], ["Osswald", "Garrett A.", ""], ["Collins", "Margaret H.", ""], ["Rothenberg", "Marc E.", ""], ["Savir", "Yonatan", ""]]}, {"id": "2101.04997", "submitter": "Soumya Chatterjee", "authors": "Soumya Chatterjee, Ayush Maheshwari, Ganesh Ramakrishnan, Saketha Nath\n  Jagaralpudi", "title": "Joint Learning of Hyperbolic Label Embeddings for Hierarchical\n  Multi-label Classification", "comments": "10 pages, 2 figures. To appear at EACL 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  We consider the problem of multi-label classification where the labels lie in\na hierarchy. However, unlike most existing works in hierarchical multi-label\nclassification, we do not assume that the label-hierarchy is known. Encouraged\nby the recent success of hyperbolic embeddings in capturing hierarchical\nrelations, we propose to jointly learn the classifier parameters as well as the\nlabel embeddings. Such a joint learning is expected to provide a twofold\nadvantage: i) the classifier generalizes better as it leverages the prior\nknowledge of existence of a hierarchy over the labels, and ii) in addition to\nthe label co-occurrence information, the label-embedding may benefit from the\nmanifold structure of the input datapoints, leading to embeddings that are more\nfaithful to the label hierarchy. We propose a novel formulation for the joint\nlearning and empirically evaluate its efficacy. The results show that the joint\nlearning improves over the baseline that employs label co-occurrence based\npre-trained hyperbolic embeddings. Moreover, the proposed classifiers achieve\nstate-of-the-art generalization on standard benchmarks. We also present\nevaluation of the hyperbolic embeddings obtained by joint learning and show\nthat they represent the hierarchy more accurately than the other alternatives.\n", "versions": [{"version": "v1", "created": "Wed, 13 Jan 2021 10:58:54 GMT"}], "update_date": "2021-01-14", "authors_parsed": [["Chatterjee", "Soumya", ""], ["Maheshwari", "Ayush", ""], ["Ramakrishnan", "Ganesh", ""], ["Jagaralpudi", "Saketha Nath", ""]]}, {"id": "2101.05003", "submitter": "Maximilian Ernst Tschuchnig", "authors": "Maximilian Ernst Tschuchnig and Cornelia Ferner and Stefan Wegenkittl", "title": "Sequential IoT Data Augmentation using Generative Adversarial Networks", "comments": null, "journal-ref": null, "doi": "10.1109/ICASSP40776.2020.9053949", "report-no": null, "categories": "cs.LG", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Sequential data in industrial applications can be used to train and evaluate\nmachine learning models (e.g. classifiers). Since gathering representative\namounts of data is difficult and time consuming, there is an incentive to\ngenerate it from a small ground truth. Data augmentation is a common method to\ngenerate more data through a priori knowledge with one specific method, so\ncalled generative adversarial networks (GANs), enabling data generation from\nnoise. This paper investigates the possibility of using GANs in order to\naugment sequential Internet of Things (IoT) data, with an example\nimplementation that generates household energy consumption data with and\nwithout swimming pools. The results of the example implementation seem\nsubjectively similar to the original data. Additionally to this subjective\nevaluation, the paper also introduces a quantitative evaluation technique for\nGANs if labels are provided. The positive results from the evaluation support\nthe initial assumption that generating sequential data from a small ground\ntruth is possible. This means that tedious data acquisition of sequential data\ncan be shortened. In the future, the results of this paper may be included as a\ntool in machine learning, tackling the small data challenge.\n", "versions": [{"version": "v1", "created": "Wed, 13 Jan 2021 11:08:07 GMT"}], "update_date": "2021-01-14", "authors_parsed": [["Tschuchnig", "Maximilian Ernst", ""], ["Ferner", "Cornelia", ""], ["Wegenkittl", "Stefan", ""]]}, {"id": "2101.05014", "submitter": "Max W. Y. Lam", "authors": "Max W. Y. Lam, Jun Wang, Dan Su, Dong Yu", "title": "Effective Low-Cost Time-Domain Audio Separation Using Globally Attentive\n  Locally Recurrent Networks", "comments": "Accepted in IEEE SLT 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.AS cs.AI cs.LG cs.SD eess.SP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent research on the time-domain audio separation networks (TasNets) has\nbrought great success to speech separation. Nevertheless, conventional TasNets\nstruggle to satisfy the memory and latency constraints in industrial\napplications. In this regard, we design a low-cost high-performance\narchitecture, namely, globally attentive locally recurrent (GALR) network.\nAlike the dual-path RNN (DPRNN), we first split a feature sequence into 2D\nsegments and then process the sequence along both the intra- and inter-segment\ndimensions. Our main innovation lies in that, on top of features recurrently\nprocessed along the inter-segment dimensions, GALR applies a self-attention\nmechanism to the sequence along the inter-segment dimension, which aggregates\ncontext-aware information and also enables parallelization. Our experiments\nsuggest that GALR is a notably more effective network than the prior work. On\none hand, with only 1.5M parameters, it has achieved comparable separation\nperformance at a much lower cost with 36.1% less runtime memory and 49.4% fewer\ncomputational operations, relative to the DPRNN. On the other hand, in a\ncomparable model size with DPRNN, GALR has consistently outperformed DPRNN in\nthree datasets, in particular, with a substantial margin of 2.4dB absolute\nimprovement of SI-SNRi in the benchmark WSJ0-2mix task.\n", "versions": [{"version": "v1", "created": "Wed, 13 Jan 2021 11:30:14 GMT"}], "update_date": "2021-01-14", "authors_parsed": [["Lam", "Max W. Y.", ""], ["Wang", "Jun", ""], ["Su", "Dan", ""], ["Yu", "Dong", ""]]}, {"id": "2101.05056", "submitter": "Manav Kaushik", "authors": "Manav Kaushik, Van Tung Pham, Eng Siong Chng", "title": "End-to-End Speaker Height and age estimation using Attention Mechanism\n  with LSTM-RNN", "comments": "5 Pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.CL cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Automatic height and age estimation of speakers using acoustic features is\nwidely used for the purpose of human-computer interaction, forensics, etc. In\nthis work, we propose a novel approach of using attention mechanism to build an\nend-to-end architecture for height and age estimation. The attention mechanism\nis combined with Long Short-Term Memory(LSTM) encoder which is able to capture\nlong-term dependencies in the input acoustic features. We modify the\nconventionally used Attention -- which calculates context vectors the sum of\nattention only across timeframes -- by introducing a modified context vector\nwhich takes into account total attention across encoder units as well, giving\nus a new cross-attention mechanism. Apart from this, we also investigate a\nmulti-task learning approach for jointly estimating speaker height and age. We\ntrain and test our model on the TIMIT corpus. Our model outperforms several\napproaches in the literature. We achieve a root mean square error (RMSE) of\n6.92cm and6.34cm for male and female heights respectively and RMSE of 7.85years\nand 8.75years for male and females ages respectively. By tracking the attention\nweights allocated to different phones, we find that Vowel phones are most\nimportant whistlestop phones are least important for the estimation task.\n", "versions": [{"version": "v1", "created": "Wed, 13 Jan 2021 13:41:18 GMT"}], "update_date": "2021-01-14", "authors_parsed": [["Kaushik", "Manav", ""], ["Pham", "Van Tung", ""], ["Chng", "Eng Siong", ""]]}, {"id": "2101.05069", "submitter": "Tomas Langer", "authors": "Tomas Langer, Natalia Fedorova, Ron Hagensieker", "title": "Formatting the Landscape: Spatial conditional GAN for varying population\n  in satellite imagery", "comments": "Presented as a poster at Tackling Climate Change with Machine\n  Learning workshop at NeurIPS 2020. Code:\n  https://github.com/LendelTheGreat/SCALAE/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Climate change is expected to reshuffle the settlement landscape: forcing\npeople in affected areas to migrate, to change their lifeways, and continuing\nto affect demographic change throughout the world. Changes to the geographic\ndistribution of population will have dramatic impacts on land use and land\ncover and thus constitute one of the major challenges of planning for climate\nchange scenarios. In this paper, we explore a generative model framework for\ngenerating satellite imagery conditional on gridded population distributions.\nWe make additions to the existing ALAE architecture, creating a spatially\nconditional version: SCALAE. This method allows us to explicitly disentangle\npopulation from the model's latent space and thus input custom population\nforecasts into the generated imagery. We postulate that such imagery could then\nbe directly used for land cover and land use change estimation using existing\nframeworks, as well as for realistic visualisation of expected local change. We\nevaluate the model by comparing pixel and semantic reconstructions, as well as\ncalculate the standard FID metric. The results suggest the model captures\npopulation distributions accurately and delivers a controllable method to\ngenerate realistic satellite imagery.\n", "versions": [{"version": "v1", "created": "Tue, 8 Dec 2020 13:31:49 GMT"}], "update_date": "2021-01-14", "authors_parsed": [["Langer", "Tomas", ""], ["Fedorova", "Natalia", ""], ["Hagensieker", "Ron", ""]]}, {"id": "2101.05091", "submitter": "Vasudevan Lakshminarayanan", "authors": "Darwin Castillo, Vasudevan Lakshminarayanan, Maria J.\n  Rodriguez-Alvarez", "title": "MRI Images, Brain Lesions and Deep Learning", "comments": "Submitted to: Computer Programs and Methods in Biomedicine update\n  (2021)", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG q-bio.NC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Medical brain image analysis is a necessary step in Computer Assisted /Aided\nDiagnosis (CAD) systems. Advancements in both hardware and software in the past\nfew years have led to improved segmentation and classification of various\ndiseases. In the present work, we review the published literature on systems\nand algorithms that allow for classification, identification, and detection of\nWhite Matter Hyperintensities (WMHs) of brain MRI images specifically in cases\nof ischemic stroke and demyelinating diseases. For the selection criteria, we\nused the bibliometric networks. Out of a total of 140 documents we selected 38\narticles that deal with the main objectives of this study. Based on the\nanalysis and discussion of the revised documents, there is constant growth in\nthe research and proposal of new models of deep learning to achieve the highest\naccuracy and reliability of the segmentation of ischemic and demyelinating\nlesions. Models with indicators (Dice Score, DSC: 0.99) were found, however\nwith little practical application due to the uses of small datasets and lack of\nreproducibility. Therefore, the main conclusion is to establish\nmultidisciplinary research groups to overcome the gap between CAD developments\nand their complete utilization in the clinical environment.\n", "versions": [{"version": "v1", "created": "Wed, 13 Jan 2021 14:18:48 GMT"}, {"version": "v2", "created": "Thu, 14 Jan 2021 15:30:59 GMT"}], "update_date": "2021-01-15", "authors_parsed": [["Castillo", "Darwin", ""], ["Lakshminarayanan", "Vasudevan", ""], ["Rodriguez-Alvarez", "Maria J.", ""]]}, {"id": "2101.05108", "submitter": "Thea Aarrestad", "authors": "Thea Aarrestad, Vladimir Loncar, Nicol\\`o Ghielmetti, Maurizio\n  Pierini, Sioni Summers, Jennifer Ngadiuba, Christoffer Petersson, Hampus\n  Linander, Yutaro Iiyama, Giuseppe Di Guglielmo, Javier Duarte, Philip Harris,\n  Dylan Rankin, Sergo Jindariani, Kevin Pedro, Nhan Tran, Mia Liu, Edward\n  Kreinar, Zhenbin Wu, and Duc Hoang", "title": "Fast convolutional neural networks on FPGAs with hls4ml", "comments": "18 pages, 18 figures, 4 tables", "journal-ref": "Mach. Learn.: Sci. Technol. 2 045015 (2021)", "doi": "10.1088/2632-2153/ac0ea1", "report-no": null, "categories": "cs.LG cs.CV hep-ex physics.ins-det stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce an automated tool for deploying ultra low-latency, low-power\ndeep neural networks with convolutional layers on FPGAs. By extending the\nhls4ml library, we demonstrate an inference latency of $5\\,\\mu$s using\nconvolutional architectures, targeting microsecond latency applications like\nthose at the CERN Large Hadron Collider. Considering benchmark models trained\non the Street View House Numbers Dataset, we demonstrate various methods for\nmodel compression in order to fit the computational constraints of a typical\nFPGA device used in trigger and data acquisition systems of particle detectors.\nIn particular, we discuss pruning and quantization-aware training, and\ndemonstrate how resource utilization can be significantly reduced with little\nto no loss in model accuracy. We show that the FPGA critical resource\nconsumption can be reduced by 97% with zero loss in model accuracy, and by 99%\nwhen tolerating a 6% accuracy degradation.\n", "versions": [{"version": "v1", "created": "Wed, 13 Jan 2021 14:47:11 GMT"}, {"version": "v2", "created": "Thu, 29 Apr 2021 11:30:02 GMT"}], "update_date": "2021-07-19", "authors_parsed": [["Aarrestad", "Thea", ""], ["Loncar", "Vladimir", ""], ["Ghielmetti", "Nicol\u00f2", ""], ["Pierini", "Maurizio", ""], ["Summers", "Sioni", ""], ["Ngadiuba", "Jennifer", ""], ["Petersson", "Christoffer", ""], ["Linander", "Hampus", ""], ["Iiyama", "Yutaro", ""], ["Di Guglielmo", "Giuseppe", ""], ["Duarte", "Javier", ""], ["Harris", "Philip", ""], ["Rankin", "Dylan", ""], ["Jindariani", "Sergo", ""], ["Pedro", "Kevin", ""], ["Tran", "Nhan", ""], ["Liu", "Mia", ""], ["Kreinar", "Edward", ""], ["Wu", "Zhenbin", ""], ["Hoang", "Duc", ""]]}, {"id": "2101.05119", "submitter": "Stefano Vigogna", "authors": "Wenjing Liao, Mauro Maggioni and Stefano Vigogna", "title": "Multiscale regression on unknown manifolds", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the regression problem of estimating functions on $\\mathbb{R}^D$\nbut supported on a $d$-dimensional manifold $ \\mathcal{M} \\subset \\mathbb{R}^D\n$ with $ d \\ll D $. Drawing ideas from multi-resolution analysis and nonlinear\napproximation, we construct low-dimensional coordinates on $\\mathcal{M}$ at\nmultiple scales, and perform multiscale regression by local polynomial fitting.\nWe propose a data-driven wavelet thresholding scheme that automatically adapts\nto the unknown regularity of the function, allowing for efficient estimation of\nfunctions exhibiting nonuniform regularity at different locations and scales.\nWe analyze the generalization error of our method by proving finite sample\nbounds in high probability on rich classes of priors. Our estimator attains\noptimal learning rates (up to logarithmic factors) as if the function was\ndefined on a known Euclidean domain of dimension $d$, instead of an unknown\nmanifold embedded in $\\mathbb{R}^D$. The implemented algorithm has quasilinear\ncomplexity in the sample size, with constants linear in $D$ and exponential in\n$d$. Our work therefore establishes a new framework for regression on\nlow-dimensional sets embedded in high dimensions, with fast implementation and\nstrong theoretical guarantees.\n", "versions": [{"version": "v1", "created": "Wed, 13 Jan 2021 15:14:31 GMT"}], "update_date": "2021-01-14", "authors_parsed": [["Liao", "Wenjing", ""], ["Maggioni", "Mauro", ""], ["Vigogna", "Stefano", ""]]}, {"id": "2101.05130", "submitter": "Jasjeet Dhaliwal", "authors": "Jasjeet Dhaliwal, Kyle Hambrook", "title": "DAEs for Linear Inverse Problems: Improved Recovery with Provable\n  Guarantees", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generative priors have been shown to provide improved results over sparsity\npriors in linear inverse problems. However, current state of the art methods\nsuffer from one or more of the following drawbacks: (a) speed of recovery is\nslow; (b) reconstruction quality is deficient; (c) reconstruction quality is\ncontingent on a computationally expensive process of tuning hyperparameters. In\nthis work, we address these issues by utilizing Denoising Auto Encoders (DAEs)\nas priors and a projected gradient descent algorithm for recovering the\noriginal signal. We provide rigorous theoretical guarantees for our method and\nexperimentally demonstrate its superiority over existing state of the art\nmethods in compressive sensing, inpainting, and super-resolution. We find that\nour algorithm speeds up recovery by two orders of magnitude (over 100x),\nimproves quality of reconstruction by an order of magnitude (over 10x), and\ndoes not require tuning hyperparameters.\n", "versions": [{"version": "v1", "created": "Wed, 13 Jan 2021 15:24:37 GMT"}], "update_date": "2021-01-14", "authors_parsed": [["Dhaliwal", "Jasjeet", ""], ["Hambrook", "Kyle", ""]]}, {"id": "2101.05136", "submitter": "Sara Mohammad Taheri Mrs", "authors": "Jeremy Zucker, Kaushal Paneri, Sara Mohammad-Taheri, Somya Bhargava,\n  Pallavi Kolambkar, Craig Bakker, Jeremy Teuton, Charles Tapley Hoyt, Kristie\n  Oxford, Robert Ness and Olga Vitek", "title": "Leveraging Structured Biological Knowledge for Counterfactual Inference:\n  a Case Study of Viral Pathogenesis", "comments": "In proceeding of IEEE, Transactions on Big Data", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.QM cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Counterfactual inference is a useful tool for comparing outcomes of\ninterventions on complex systems. It requires us to represent the system in\nform of a structural causal model, complete with a causal diagram,\nprobabilistic assumptions on exogenous variables, and functional assignments.\nSpecifying such models can be extremely difficult in practice. The process\nrequires substantial domain expertise, and does not scale easily to large\nsystems, multiple systems, or novel system modifications. At the same time,\nmany application domains, such as molecular biology, are rich in structured\ncausal knowledge that is qualitative in nature. This manuscript proposes a\ngeneral approach for querying a causal biological knowledge graph, and\nconverting the qualitative result into a quantitative structural causal model\nthat can learn from data to answer the question. We demonstrate the\nfeasibility, accuracy and versatility of this approach using two case studies\nin systems biology. The first demonstrates the appropriateness of the\nunderlying assumptions and the accuracy of the results. The second demonstrates\nthe versatility of the approach by querying a knowledge base for the molecular\ndeterminants of a severe acute respiratory syndrome coronavirus 2\n(SARS-CoV-2)-induced cytokine storm, and performing counterfactual inference to\nestimate the causal effect of medical countermeasures for severely ill\npatients.\n", "versions": [{"version": "v1", "created": "Wed, 13 Jan 2021 15:32:17 GMT"}], "update_date": "2021-01-14", "authors_parsed": [["Zucker", "Jeremy", ""], ["Paneri", "Kaushal", ""], ["Mohammad-Taheri", "Sara", ""], ["Bhargava", "Somya", ""], ["Kolambkar", "Pallavi", ""], ["Bakker", "Craig", ""], ["Teuton", "Jeremy", ""], ["Hoyt", "Charles Tapley", ""], ["Oxford", "Kristie", ""], ["Ness", "Robert", ""], ["Vitek", "Olga", ""]]}, {"id": "2101.05137", "submitter": "Yan Shuhan", "authors": "Shuhan Yan, Yuting Jia, Xinbing Wang", "title": "Overlapping Community Detection in Temporal Text Networks", "comments": "13 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Analyzing the groups in the network based on same attributes, functions or\nconnections between nodes is a way to understand network information. The task\nof discovering a series of node groups is called community detection.\nGenerally, two types of information can be utilized to fulfill this task, i.e.,\nthe link structures and the node attributes. The temporal text network is a\nspecial kind of network that contains both sources of information. Typical\nrepresentatives include online blog networks, the World Wide Web (WWW) and\nacademic citation networks. In this paper, we study the problem of overlapping\ncommunity detection in temporal text network. By examining 32 large temporal\ntext networks, we find a lot of edges connecting two nodes with no common\ncommunity and discover that nodes in the same community share similar textual\ncontents. This scenario cannot be quantitatively modeled by practically all\nexisting community detection methods. Motivated by these empirical\nobservations, we propose MAGIC (Model Affiliation Graph with Interacting\nCommunities), a generative model which captures community interactions and\nconsiders the information from both link structures and node attributes. Our\nexperiments on 3 types of datasets show that MAGIC achieves large improvements\nover 4 state-of-the-art methods in terms of 4 widely-used metrics.\n", "versions": [{"version": "v1", "created": "Wed, 13 Jan 2021 15:32:39 GMT"}], "update_date": "2021-01-14", "authors_parsed": [["Yan", "Shuhan", ""], ["Jia", "Yuting", ""], ["Wang", "Xinbing", ""]]}, {"id": "2101.05145", "submitter": "Rohit Jena", "authors": "Rohit Jena, Sumedha Singla, Kayhan Batmanghelich", "title": "Self-Supervised Vessel Enhancement Using Flow-Based Consistencies", "comments": "Early accept at MICCAI 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Vessel segmentation is an essential task in many clinical applications.\nAlthough supervised methods have achieved state-of-art performance, acquiring\nexpert annotation is laborious and mostly limited for two-dimensional datasets\nwith a small sample size. On the contrary, unsupervised methods rely on\nhandcrafted features to detect tube-like structures such as vessels. However,\nthose methods require complex pipelines involving several hyper-parameters and\ndesign choices rendering the procedure sensitive, dataset-specific, and not\ngeneralizable. We propose a self-supervised method with a limited number of\nhyper-parameters that is generalizable across modalities. Our method uses\ntube-like structure properties, such as connectivity, profile consistency, and\nbifurcation, to introduce inductive bias into a learning algorithm. To model\nthose properties, we generate a vector field that we refer to as a flow. Our\nexperiments on various public datasets in 2D and 3D show that our method\nperforms better than unsupervised methods while learning useful transferable\nfeatures from unlabeled data. Unlike generic self-supervised methods, the\nlearned features learn vessel-relevant features that are transferable for\nsupervised approaches, which is essential when the number of annotated data is\nlimited.\n", "versions": [{"version": "v1", "created": "Wed, 13 Jan 2021 15:38:23 GMT"}, {"version": "v2", "created": "Tue, 9 Mar 2021 16:24:25 GMT"}, {"version": "v3", "created": "Thu, 22 Jul 2021 14:52:23 GMT"}], "update_date": "2021-07-23", "authors_parsed": [["Jena", "Rohit", ""], ["Singla", "Sumedha", ""], ["Batmanghelich", "Kayhan", ""]]}, {"id": "2101.05147", "submitter": "Niv Nayman", "authors": "Jian Tan, Niv Nayman, Mengchang Wang, Feifei Li, Rong Jin", "title": "CobBO: Coordinate Backoff Bayesian Optimization", "comments": "Jian Tan and Niv Nayman contributed equally. An implementation of\n  CobBO is available at: https://github.com/Alibaba-MIIL/CobBO", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bayesian optimization is a popular method for optimizing expensive black-box\nfunctions. The objective functions of hard real world problems are oftentimes\ncharacterized by a fluctuated landscape of many local optima. Bayesian\noptimization risks in over-exploiting such traps, remaining with insufficient\nquery budget for exploring the global landscape. We introduce Coordinate\nBackoff Bayesian Optimization (CobBO) to alleviate those challenges. CobBO\ncaptures a smooth approximation of the global landscape by interpolating the\nvalues of queried points projected to randomly selected promising subspaces.\nThus also a smaller query budget is required for the Gaussian process\nregressions applied over the lower dimensional subspaces. This approach can be\nviewed as a variant of coordinate ascent, tailored for Bayesian optimization,\nusing a stopping rule for backing off from a certain subspace and switching to\nanother coordinate subset. Extensive evaluations show that CobBO finds\nsolutions comparable to or better than other state-of-the-art methods for\ndimensions ranging from tens to hundreds, while reducing the trial complexity.\n", "versions": [{"version": "v1", "created": "Wed, 13 Jan 2021 15:39:32 GMT"}, {"version": "v2", "created": "Tue, 16 Feb 2021 10:18:57 GMT"}], "update_date": "2021-02-17", "authors_parsed": [["Tan", "Jian", ""], ["Nayman", "Niv", ""], ["Wang", "Mengchang", ""], ["Li", "Feifei", ""], ["Jin", "Rong", ""]]}, {"id": "2101.05151", "submitter": "Zhen Han", "authors": "Zifeng Ding, Zhen Han, Yunpu Ma, Volker Tresp", "title": "Temporal Knowledge Graph Forecasting with Neural ODE", "comments": "11 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning node representation on dynamically-evolving, multi-relational graph\ndata has gained great research interest. However, most of the existing models\nfor temporal knowledge graph forecasting use Recurrent Neural Network (RNN)\nwith discrete depth to capture temporal information, while time is a continuous\nvariable. Inspired by Neural Ordinary Differential Equation (NODE), we extend\nthe idea of continuum-depth models to time-evolving multi-relational graph\ndata, and propose a novel Temporal Knowledge Graph Forecasting model with NODE.\nOur model captures temporal information through NODE and structural information\nthrough a Graph Neural Network (GNN). Thus, our graph ODE model achieves a\ncontinuous model in time and efficiently learns node representation for future\nprediction. We evaluate our model on six temporal knowledge graph datasets by\nperforming link forecasting. Experiment results show the superiority of our\nmodel.\n", "versions": [{"version": "v1", "created": "Wed, 13 Jan 2021 15:49:48 GMT"}], "update_date": "2021-01-14", "authors_parsed": [["Ding", "Zifeng", ""], ["Han", "Zhen", ""], ["Ma", "Yunpu", ""], ["Tresp", "Volker", ""]]}, {"id": "2101.05186", "submitter": "Pieter-Jan Hoedt", "authors": "Pieter-Jan Hoedt, Frederik Kratzert, Daniel Klotz, Christina Halmich,\n  Markus Holzleitner, Grey Nearing, Sepp Hochreiter and G\\\"unter Klambauer", "title": "MC-LSTM: Mass-Conserving LSTM", "comments": "13 pages (8.5 without references) + 17 pages appendix", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The success of Convolutional Neural Networks (CNNs) in computer vision is\nmainly driven by their strong inductive bias, which is strong enough to allow\nCNNs to solve vision-related tasks with random weights, meaning without\nlearning. Similarly, Long Short-Term Memory (LSTM) has a strong inductive bias\ntowards storing information over time. However, many real-world systems are\ngoverned by conservation laws, which lead to the redistribution of particular\nquantities -- e.g. in physical and economical systems. Our novel\nMass-Conserving LSTM (MC-LSTM) adheres to these conservation laws by extending\nthe inductive bias of LSTM to model the redistribution of those stored\nquantities. MC-LSTMs set a new state-of-the-art for neural arithmetic units at\nlearning arithmetic operations, such as addition tasks, which have a strong\nconservation law, as the sum is constant over time. Further, MC-LSTM is applied\nto traffic forecasting, modelling a pendulum, and a large benchmark dataset in\nhydrology, where it sets a new state-of-the-art for predicting peak flows. In\nthe hydrology example, we show that MC-LSTM states correlate with real-world\nprocesses and are therefore interpretable.\n", "versions": [{"version": "v1", "created": "Wed, 13 Jan 2021 16:40:48 GMT"}, {"version": "v2", "created": "Mon, 8 Feb 2021 14:24:37 GMT"}, {"version": "v3", "created": "Thu, 10 Jun 2021 15:33:23 GMT"}], "update_date": "2021-06-11", "authors_parsed": [["Hoedt", "Pieter-Jan", ""], ["Kratzert", "Frederik", ""], ["Klotz", "Daniel", ""], ["Halmich", "Christina", ""], ["Holzleitner", "Markus", ""], ["Nearing", "Grey", ""], ["Hochreiter", "Sepp", ""], ["Klambauer", "G\u00fcnter", ""]]}, {"id": "2101.05201", "submitter": "Ka Man Yim", "authors": "Ka Man Yim, Jacob Leygonie", "title": "Optimisation of Spectral Wavelets for Persistence-based Graph\n  Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SP cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  A graph's spectral wavelet signature determines a filtration, and\nconsequently an associated set of extended persistence diagrams. We propose a\nframework that optimises the choice of wavelet for a dataset of graphs, such\nthat their associated persistence diagrams capture features of the graphs that\nare best suited to a given data science problem. Since the spectral wavelet\nsignature of a graph is derived from its Laplacian, our framework encodes\ngeometric properties of graphs in their associated persistence diagrams and can\nbe applied to graphs without a priori node attributes. We apply our framework\nto graph classification problems and obtain performances competitive with other\npersistence-based architectures. To provide the underlying theoretical\nfoundations, we extend the differentiability result for ordinary persistent\nhomology to extended persistent homology.\n", "versions": [{"version": "v1", "created": "Sun, 10 Jan 2021 18:34:27 GMT"}, {"version": "v2", "created": "Mon, 1 Mar 2021 16:43:12 GMT"}], "update_date": "2021-03-02", "authors_parsed": [["Yim", "Ka Man", ""], ["Leygonie", "Jacob", ""]]}, {"id": "2101.05206", "submitter": "Ke Ma", "authors": "Ke Ma, Dongxuan He, Hancun Sun, Zhaocheng Wang, Sheng Chen", "title": "Deep Learning Assisted Calibrated Beam Training for Millimeter-Wave\n  Communication Systems", "comments": "Accepted by IEEE Transactions on Communications", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SP cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Huge overhead of beam training imposes a significant challenge in\nmillimeter-wave (mmWave) wireless communications. To address this issue, in\nthis paper, we propose a wide beam based training approach to calibrate the\nnarrow beam direction according to the channel power leakage. To handle the\ncomplex nonlinear properties of the channel power leakage, deep learning is\nutilized to predict the optimal narrow beam directly. Specifically, three deep\nlearning assisted calibrated beam training schemes are proposed. The first\nscheme adopts convolution neural network to implement the prediction based on\nthe instantaneous received signals of wide beam training. We also perform the\nadditional narrow beam training based on the predicted probabilities for\nfurther beam direction calibrations. However, the first scheme only depends on\none wide beam training, which lacks the robustness to noise. To tackle this\nproblem, the second scheme adopts long-short term memory (LSTM) network for\ntracking the movement of users and calibrating the beam direction according to\nthe received signals of prior beam training, in order to enhance the robustness\nto noise. To further reduce the overhead of wide beam training, our third\nscheme, an adaptive beam training strategy, selects partial wide beams to be\ntrained based on the prior received signals. Two criteria, namely, optimal\nneighboring criterion and maximum probability criterion, are designed for the\nselection. Furthermore, to handle mobile scenarios, auxiliary LSTM is\nintroduced to calibrate the directions of the selected wide beams more\nprecisely. Simulation results demonstrate that our proposed schemes achieve\nsignificantly higher beamforming gain with smaller beam training overhead\ncompared with the conventional and existing deep-learning based counterparts.\n", "versions": [{"version": "v1", "created": "Fri, 8 Jan 2021 04:02:34 GMT"}, {"version": "v2", "created": "Fri, 22 Jan 2021 00:53:33 GMT"}, {"version": "v3", "created": "Tue, 20 Jul 2021 03:36:54 GMT"}], "update_date": "2021-07-21", "authors_parsed": [["Ma", "Ke", ""], ["He", "Dongxuan", ""], ["Sun", "Hancun", ""], ["Wang", "Zhaocheng", ""], ["Chen", "Sheng", ""]]}, {"id": "2101.05207", "submitter": "Mengfei Xu", "authors": "Mengfei Xu, Shufang Song, Xuxiang Sun, Weiwei Zhang", "title": "UCNN: A Convolutional Strategy on Unstructured Mesh", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.flu-dyn cs.LG physics.comp-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In machine learning for fluid mechanics, fully-connected neural network (FNN)\nonly uses the local features for modelling, while the convolutional neural\nnetwork (CNN) cannot be applied to data on structured/unstructured mesh. In\norder to overcome the limitations of FNN and CNN, the unstructured\nconvolutional neural network (UCNN) is proposed, which aggregates and\neffectively exploits the features of neighbour nodes through the weight\nfunction. Adjoint vector modelling is taken as the task to study the\nperformance of UCNN. The mapping function from flow-field features to adjoint\nvector is constructed through efficient parallel implementation on GPU. The\nmodelling capability of UCNN is compared with that of FNN on validation set and\nin aerodynamic shape optimization at test case. The influence of mesh changing\non the modelling capability of UCNN is further studied. The results indicate\nthat UCNN is more accurate in modelling process.\n", "versions": [{"version": "v1", "created": "Tue, 12 Jan 2021 10:48:25 GMT"}], "update_date": "2021-01-14", "authors_parsed": [["Xu", "Mengfei", ""], ["Song", "Shufang", ""], ["Sun", "Xuxiang", ""], ["Zhang", "Weiwei", ""]]}, {"id": "2101.05216", "submitter": "Souvik Kundu", "authors": "Souvik Kundu, Sairam Sundaresan", "title": "AttentionLite: Towards Efficient Self-Attention Models for Vision", "comments": "5 pages, 3 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel framework for producing a class of parameter and compute\nefficient models called AttentionLitesuitable for resource-constrained\napplications. Prior work has primarily focused on optimizing models either via\nknowledge distillation or pruning. In addition to fusing these two mechanisms,\nour joint optimization framework also leverages recent advances in\nself-attention as a substitute for convolutions. We can simultaneously distill\nknowledge from a compute-heavy teacher while also pruning the student model in\na single pass of training thereby reducing training and fine-tuning times\nconsiderably. We evaluate the merits of our proposed approach on the CIFAR-10,\nCIFAR-100, and Tiny-ImageNet datasets. Not only do our AttentionLite models\nsignificantly outperform their unoptimized counterparts in accuracy, we find\nthat in some cases, that they perform almost as well as their compute-heavy\nteachers while consuming only a fraction of the parameters and FLOPs.\nConcretely, AttentionLite models can achieve upto30x parameter efficiency and\n2x computation efficiency with no significant accuracy drop compared to their\nteacher.\n", "versions": [{"version": "v1", "created": "Mon, 21 Dec 2020 17:54:09 GMT"}], "update_date": "2021-01-14", "authors_parsed": [["Kundu", "Souvik", ""], ["Sundaresan", "Sairam", ""]]}, {"id": "2101.05217", "submitter": "Luc Le Magoarou", "authors": "Luc Le Magoarou (IRT b-com, Hypermedia)", "title": "Similarity-based prediction for channel mapping and user positioning", "comments": "IEEE Communications Letters, Institute of Electrical and Electronics\n  Engineers, In press", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SP cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In a wireless network, gathering information at the base station about mobile\nusers based only on uplink channel measurements is an interesting challenge.\nIndeed, accessing the users locations and predicting their downlink channels\nwould be particularly useful in order to optimize the network efficiency. In\nthis paper, a supervised machine learning approach addressing these tasks in an\nunified way is proposed. It relies on a labeled database that can be acquired\nin a simple way by the base station while operating. The proposed regression\nmethod can be seen as a computationally efficient two layers neural network\ninitialized with a non-parametric estimator. It is illustrated on realistic\nchannel data, both for the positioning and channel mapping tasks, achieving\nbetter results than previously proposed approaches, at a lower cost.\n", "versions": [{"version": "v1", "created": "Wed, 2 Dec 2020 11:30:51 GMT"}, {"version": "v2", "created": "Thu, 14 Jan 2021 07:35:25 GMT"}], "update_date": "2021-01-15", "authors_parsed": [["Magoarou", "Luc Le", "", "IRT b-com, Hypermedia"]]}, {"id": "2101.05219", "submitter": "Jonathan Helland", "authors": "Jonathan Helland, Nathan VanHoudnos", "title": "On the human-recognizability phenomenon of adversarially trained deep\n  image classifiers", "comments": null, "journal-ref": "In JSM Proceedings, Statistical Computing Section. Alexandria, VA:\n  American Statistical Association. 1121-1131 (2020)", "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we investigate the phenomenon that robust image classifiers\nhave human-recognizable features -- often referred to as interpretability -- as\nrevealed through the input gradients of their score functions and their\nsubsequent adversarial perturbations. In particular, we demonstrate that\nstate-of-the-art methods for adversarial training incorporate two terms -- one\nthat orients the decision boundary via minimizing the expected loss, and\nanother that induces smoothness of the classifier's decision surface by\npenalizing the local Lipschitz constant. Through this demonstration, we provide\na unified discussion of gradient and Jacobian-based regularizers that have been\nused to encourage adversarial robustness in prior works. Following this\ndiscussion, we give qualitative evidence that the coupling of smoothness and\norientation of the decision boundary is sufficient to induce the aforementioned\nhuman-recognizability phenomenon.\n", "versions": [{"version": "v1", "created": "Fri, 18 Dec 2020 17:33:52 GMT"}], "update_date": "2021-01-14", "authors_parsed": [["Helland", "Jonathan", ""], ["VanHoudnos", "Nathan", ""]]}, {"id": "2101.05224", "submitter": "Shekoofeh Azizi", "authors": "Shekoofeh Azizi, Basil Mustafa, Fiona Ryan, Zachary Beaver, Jan\n  Freyberg, Jonathan Deaton, Aaron Loh, Alan Karthikesalingam, Simon Kornblith,\n  Ting Chen, Vivek Natarajan, Mohammad Norouzi", "title": "Big Self-Supervised Models Advance Medical Image Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Self-supervised pretraining followed by supervised fine-tuning has seen\nsuccess in image recognition, especially when labeled examples are scarce, but\nhas received limited attention in medical image analysis. This paper studies\nthe effectiveness of self-supervised learning as a pretraining strategy for\nmedical image classification. We conduct experiments on two distinct tasks:\ndermatology skin condition classification from digital camera images and\nmulti-label chest X-ray classification, and demonstrate that self-supervised\nlearning on ImageNet, followed by additional self-supervised learning on\nunlabeled domain-specific medical images significantly improves the accuracy of\nmedical image classifiers. We introduce a novel Multi-Instance Contrastive\nLearning (MICLe) method that uses multiple images of the underlying pathology\nper patient case, when available, to construct more informative positive pairs\nfor self-supervised learning. Combining our contributions, we achieve an\nimprovement of 6.7% in top-1 accuracy and an improvement of 1.1% in mean AUC on\ndermatology and chest X-ray classification respectively, outperforming strong\nsupervised baselines pretrained on ImageNet. In addition, we show that big\nself-supervised models are robust to distribution shift and can learn\nefficiently with a small number of labeled medical images.\n", "versions": [{"version": "v1", "created": "Wed, 13 Jan 2021 17:36:31 GMT"}, {"version": "v2", "created": "Thu, 1 Apr 2021 17:43:59 GMT"}], "update_date": "2021-04-02", "authors_parsed": [["Azizi", "Shekoofeh", ""], ["Mustafa", "Basil", ""], ["Ryan", "Fiona", ""], ["Beaver", "Zachary", ""], ["Freyberg", "Jan", ""], ["Deaton", "Jonathan", ""], ["Loh", "Aaron", ""], ["Karthikesalingam", "Alan", ""], ["Kornblith", "Simon", ""], ["Chen", "Ting", ""], ["Natarajan", "Vivek", ""], ["Norouzi", "Mohammad", ""]]}, {"id": "2101.05231", "submitter": "Keaton Hamm", "authors": "HanQin Cai, Keaton Hamm, Longxiu Huang, Deanna Needell", "title": "Robust CUR Decomposition: Theory and Imaging Applications", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper considers the use of Robust PCA in a CUR decomposition framework\nand applications thereof. Our main algorithms produce a robust version of\ncolumn-row factorizations of matrices $\\mathbf{D}=\\mathbf{L}+\\mathbf{S}$ where\n$\\mathbf{L}$ is low-rank and $\\mathbf{S}$ contains sparse outliers. These\nmethods yield interpretable factorizations at low computational cost, and\nprovide new CUR decompositions that are robust to sparse outliers, in contrast\nto previous methods. We consider two key imaging applications of Robust PCA:\nvideo foreground-background separation and face modeling. This paper examines\nthe qualitative behavior of our Robust CUR decompositions on the benchmark\nvideos and face datasets, and find that our method works as well as standard\nRobust PCA while being significantly faster. Additionally, we consider hybrid\nrandomized and deterministic sampling methods which produce a compact CUR\ndecomposition of a given matrix, and apply this to video sequences to produce\ncanonical frames thereof.\n", "versions": [{"version": "v1", "created": "Tue, 5 Jan 2021 17:58:15 GMT"}], "update_date": "2021-01-14", "authors_parsed": [["Cai", "HanQin", ""], ["Hamm", "Keaton", ""], ["Huang", "Longxiu", ""], ["Needell", "Deanna", ""]]}, {"id": "2101.05234", "submitter": "Annie Marsden", "authors": "Annie Marsden, John Duchi, Gregory Valiant", "title": "On Misspecification in Prediction Problems and Robustness via Improper\n  Learning", "comments": "28 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study probabilistic prediction games when the underlying model is\nmisspecified, investigating the consequences of predicting using an incorrect\nparametric model. We show that for a broad class of loss functions and\nparametric families of distributions, the regret of playing a \"proper\"\npredictor -- one from the putative model class -- relative to the best\npredictor in the same model class has lower bound scaling at least as\n$\\sqrt{\\gamma n}$, where $\\gamma$ is a measure of the model misspecification to\nthe true distribution in terms of total variation distance. In contrast, using\nan aggregation-based (improper) learner, one can obtain regret $d \\log n$ for\nany underlying generating distribution, where $d$ is the dimension of the\nparameter; we exhibit instances in which this is unimprovable even over the\nfamily of all learners that may play distributions in the convex hull of the\nparametric family. These results suggest that simple strategies for aggregating\nmultiple learners together should be more robust, and several experiments\nconform to this hypothesis.\n", "versions": [{"version": "v1", "created": "Wed, 13 Jan 2021 17:54:08 GMT"}, {"version": "v2", "created": "Fri, 29 Jan 2021 21:34:04 GMT"}], "update_date": "2021-02-02", "authors_parsed": [["Marsden", "Annie", ""], ["Duchi", "John", ""], ["Valiant", "Gregory", ""]]}, {"id": "2101.05239", "submitter": "Yermek Kapushev", "authors": "Tsimboy Olga, Yermek Kapushev, Evgeny Burnaev, Ivan Oseledets", "title": "Denoising Score Matching with Random Fourier Features", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The density estimation is one of the core problems in statistics. Despite\nthis, existing techniques like maximum likelihood estimation are\ncomputationally inefficient due to the intractability of the normalizing\nconstant. For this reason an interest to score matching has increased being\nindependent on the normalizing constant. However, such estimator is consistent\nonly for distributions with the full space support. One of the approaches to\nmake it consistent is to add noise to the input data which is called Denoising\nScore Matching. In this work we derive analytical expression for the Denoising\nScore matching using the Kernel Exponential Family as a model distribution. The\nusage of the kernel exponential family is motivated by the richness of this\nclass of densities. To tackle the computational complexity we use Random\nFourier Features based approximation of the kernel function. The analytical\nexpression allows to drop additional regularization terms based on the\nhigher-order derivatives as they are already implicitly included. Moreover, the\nobtained expression explicitly depends on the noise variance, so the validation\nloss can be straightforwardly used to tune the noise level. Along with\nbenchmark experiments, the model was tested on various synthetic distributions\nto study the behaviour of the model in different cases. The empirical study\nshows comparable quality to the competing approaches, while the proposed method\nbeing computationally faster. The latter one enables scaling up to complex\nhigh-dimensional data.\n", "versions": [{"version": "v1", "created": "Wed, 13 Jan 2021 18:02:39 GMT"}], "update_date": "2021-01-14", "authors_parsed": [["Olga", "Tsimboy", ""], ["Kapushev", "Yermek", ""], ["Burnaev", "Evgeny", ""], ["Oseledets", "Ivan", ""]]}, {"id": "2101.05248", "submitter": "Emmanouil Vasileios Vlatakis Gkaragkounis", "authors": "Lampros Flokas, Emmanouil-Vasileios Vlatakis-Gkaragkounis, Georgios\n  Piliouras", "title": "Solving Min-Max Optimization with Hidden Structure via Gradient Descent\n  Ascent", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.GT cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Many recent AI architectures are inspired by zero-sum games, however, the\nbehavior of their dynamics is still not well understood. Inspired by this, we\nstudy standard gradient descent ascent (GDA) dynamics in a specific class of\nnon-convex non-concave zero-sum games, that we call hidden zero-sum games. In\nthis class, players control the inputs of smooth but possibly non-linear\nfunctions whose outputs are being applied as inputs to a convex-concave game.\nUnlike general zero-sum games, these games have a well-defined notion of\nsolution; outcomes that implement the von-Neumann equilibrium of the \"hidden\"\nconvex-concave game. We prove that if the hidden game is strictly\nconvex-concave then vanilla GDA converges not merely to local Nash, but\ntypically to the von-Neumann solution. If the game lacks strict convexity\nproperties, GDA may fail to converge to any equilibrium, however, by applying\nstandard regularization techniques we can prove convergence to a von-Neumann\nsolution of a slightly perturbed zero-sum game. Our convergence guarantees are\nnon-local, which as far as we know is a first-of-its-kind type of result in\nnon-convex non-concave games. Finally, we discuss connections of our framework\nwith generative adversarial networks.\n", "versions": [{"version": "v1", "created": "Wed, 13 Jan 2021 18:13:49 GMT"}], "update_date": "2021-01-14", "authors_parsed": [["Flokas", "Lampros", ""], ["Vlatakis-Gkaragkounis", "Emmanouil-Vasileios", ""], ["Piliouras", "Georgios", ""]]}, {"id": "2101.05254", "submitter": "Rangeet Mitra", "authors": "Rangeet Mitra, Georges Kaddoum", "title": "Random Fourier Feature Based Deep Learning for Wireless Communications", "comments": "12 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.LG eess.SP math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep-learning (DL) has emerged as a powerful machine-learning technique for\nseveral classic problems encountered in generic wireless communications.\nSpecifically, random Fourier Features (RFF) based deep-learning has emerged as\nan attractive solution for several machine-learning problems; yet there is a\nlacuna of rigorous results to justify the viability of RFF based DL-algorithms\nin general. To address this gap, we attempt to analytically quantify the\nviability of RFF based DL. Precisely, in this paper, analytical proofs are\npresented demonstrating that RFF based DL architectures have lower\napproximation-error and probability of misclassification as compared to\nclassical DL architectures. In addition, a new distribution-dependent RFF is\nproposed to facilitate DL architectures with low training-complexity. Through\ncomputer simulations, the practical application of the presented analytical\nresults and the proposed distribution-dependent RFF, are depicted for various\nmachine-learning problems encountered in next-generation communication systems\nsuch as: a) line of sight (LOS)/non-line of sight (NLOS) classification, and b)\nmessage-passing based detection of low-density parity check codes (LDPC) codes\nover nonlinear visible light communication (VLC) channels. Especially in the\nlow training-data regime, the presented simulations show that significant\nperformance gains are achieved when utilizing RFF maps of observations. Lastly,\nin all the presented simulations, it is observed that the proposed\ndistribution-dependent RFFs significantly outperform RFFs, which make them\nuseful for potential machine-learning/DL based applications in the context of\nnext-generation communication systems.\n", "versions": [{"version": "v1", "created": "Wed, 13 Jan 2021 18:39:36 GMT"}], "update_date": "2021-01-14", "authors_parsed": [["Mitra", "Rangeet", ""], ["Kaddoum", "Georges", ""]]}, {"id": "2101.05265", "submitter": "Rishabh Agarwal", "authors": "Rishabh Agarwal, Marlos C. Machado, Pablo Samuel Castro, Marc G.\n  Bellemare", "title": "Contrastive Behavioral Similarity Embeddings for Generalization in\n  Reinforcement Learning", "comments": "ICLR 2021 (Spotlight). Website: https://agarwl.github.io/pse", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Reinforcement learning methods trained on few environments rarely learn\npolicies that generalize to unseen environments. To improve generalization, we\nincorporate the inherent sequential structure in reinforcement learning into\nthe representation learning process. This approach is orthogonal to recent\napproaches, which rarely exploit this structure explicitly. Specifically, we\nintroduce a theoretically motivated policy similarity metric (PSM) for\nmeasuring behavioral similarity between states. PSM assigns high similarity to\nstates for which the optimal policies in those states as well as in future\nstates are similar. We also present a contrastive representation learning\nprocedure to embed any state similarity metric, which we instantiate with PSM\nto obtain policy similarity embeddings (PSEs). We demonstrate that PSEs improve\ngeneralization on diverse benchmarks, including LQR with spurious correlations,\na jumping task from pixels, and Distracting DM Control Suite.\n", "versions": [{"version": "v1", "created": "Wed, 13 Jan 2021 18:55:43 GMT"}, {"version": "v2", "created": "Thu, 18 Mar 2021 13:58:01 GMT"}], "update_date": "2021-03-19", "authors_parsed": [["Agarwal", "Rishabh", ""], ["Machado", "Marlos C.", ""], ["Castro", "Pablo Samuel", ""], ["Bellemare", "Marc G.", ""]]}, {"id": "2101.05272", "submitter": "Lisa-Marie Vortmann", "authors": "Lisa-Marie Vortmann, Leonid Schwenke, Felix Putze", "title": "Real or Virtual? Using Brain Activity Patterns to differentiate Attended\n  Targets during Augmented Reality Scenarios", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Augmented Reality is the fusion of virtual components and our real\nsurroundings. The simultaneous visibility of generated and natural objects\noften requires users to direct their selective attention to a specific target\nthat is either real or virtual. In this study, we investigated whether this\ntarget is real or virtual by using machine learning techniques to classify\nelectroencephalographic (EEG) data collected in Augmented Reality scenarios. A\nshallow convolutional neural net classified 3 second data windows from 20\nparticipants in a person-dependent manner with an average accuracy above 70\\%\nif the testing data and training data came from different trials.\nPerson-independent classification was possible above chance level for 6 out of\n20 participants. Thus, the reliability of such a Brain-Computer Interface is\nhigh enough for it to be treated as a useful input mechanism for Augmented\nReality applications.\n", "versions": [{"version": "v1", "created": "Tue, 12 Jan 2021 19:08:39 GMT"}], "update_date": "2021-01-15", "authors_parsed": [["Vortmann", "Lisa-Marie", ""], ["Schwenke", "Leonid", ""], ["Putze", "Felix", ""]]}, {"id": "2101.05273", "submitter": "Dakuo Wang", "authors": "Dakuo Wang, Josh Andres, Justin Weisz, Erick Oduor, Casey Dugan", "title": "AutoDS: Towards Human-Centered Automation of Data Science", "comments": null, "journal-ref": null, "doi": "10.1145/3411764.3445526", "report-no": null, "categories": "cs.HC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data science (DS) projects often follow a lifecycle that consists of\nlaborious tasks for data scientists and domain experts (e.g., data exploration,\nmodel training, etc.). Only till recently, machine learning(ML) researchers\nhave developed promising automation techniques to aid data workers in these\ntasks. This paper introduces AutoDS, an automated machine learning (AutoML)\nsystem that aims to leverage the latest ML automation techniques to support\ndata science projects. Data workers only need to upload their dataset, then the\nsystem can automatically suggest ML configurations, preprocess data, select\nalgorithm, and train the model. These suggestions are presented to the user via\na web-based graphical user interface and a notebook-based programming user\ninterface.\n  We studied AutoDS with 30 professional data scientists, where one group used\nAutoDS, and the other did not, to complete a data science project. As expected,\nAutoDS improves productivity; Yet surprisingly, we find that the models\nproduced by the AutoDS group have higher quality and less errors, but lower\nhuman confidence scores. We reflect on the findings by presenting design\nimplications for incorporating automation techniques into human work in the\ndata science lifecycle.\n", "versions": [{"version": "v1", "created": "Wed, 13 Jan 2021 08:35:14 GMT"}], "update_date": "2021-01-15", "authors_parsed": [["Wang", "Dakuo", ""], ["Andres", "Josh", ""], ["Weisz", "Justin", ""], ["Oduor", "Erick", ""], ["Dugan", "Casey", ""]]}, {"id": "2101.05303", "submitter": "Han Liu", "authors": "Han Liu, Vivian Lai, Chenhao Tan", "title": "Understanding the Effect of Out-of-distribution Examples and Interactive\n  Explanations on Human-AI Decision Making", "comments": "43 pages, 24 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CY cs.HC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Although AI holds promise for improving human decision making in societally\ncritical domains, it remains an open question how human-AI teams can reliably\noutperform AI alone and human alone in challenging prediction tasks (also known\nas complementary performance). We explore two directions to understand the gaps\nin achieving complementary performance. First, we argue that the typical\nexperimental setup limits the potential of human-AI teams. To account for lower\nAI performance out-of-distribution than in-distribution because of distribution\nshift, we design experiments with different distribution types and investigate\nhuman performance for both in-distribution and out-of-distribution examples.\nSecond, we develop novel interfaces to support interactive explanations so that\nhumans can actively engage with AI assistance. Using virtual pilot studies and\nlarge-scale randomized experiments across three tasks, we demonstrate a clear\ndifference between in-distribution and out-of-distribution, and observe mixed\nresults for interactive explanations: while interactive explanations improve\nhuman perception of AI assistance's usefulness, they may reinforce human biases\nand lead to limited performance improvement. Overall, our work points out\ncritical challenges and future directions towards enhancing human performance\nwith AI assistance.\n", "versions": [{"version": "v1", "created": "Wed, 13 Jan 2021 19:01:32 GMT"}, {"version": "v2", "created": "Wed, 27 Jan 2021 19:02:32 GMT"}, {"version": "v3", "created": "Tue, 20 Jul 2021 18:00:05 GMT"}], "update_date": "2021-07-22", "authors_parsed": [["Liu", "Han", ""], ["Lai", "Vivian", ""], ["Tan", "Chenhao", ""]]}, {"id": "2101.05304", "submitter": "Ravid Shwartz Ziv", "authors": "Ravid Shwartz-Ziv, Itamar Ben Ari and Amitai Armon", "title": "Spatial-Temporal Convolutional Network for Spread Prediction of COVID-19", "comments": "IEEE BigData 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this work we present a spatial-temporal convolutional neural network for\npredicting future COVID-19 related symptoms severity among a population, per\nregion, given its past reported symptoms. This can help approximate the number\nof future Covid-19 patients in each region, thus enabling a faster response,\ne.g., preparing the local hospital or declaring a local lockdown where\nnecessary. Our model is based on a national symptom survey distributed in\nIsrael and can predict symptoms severity for different regions daily. The model\nincludes two main parts - (1) learned region-based survey responders profiles\nused for aggregating questionnaires data into features (2) Spatial-Temporal 3D\nconvolutional neural network which uses the above features to predict symptoms\nprogression.\n", "versions": [{"version": "v1", "created": "Sun, 27 Dec 2020 09:00:17 GMT"}], "update_date": "2021-01-15", "authors_parsed": [["Shwartz-Ziv", "Ravid", ""], ["Ari", "Itamar Ben", ""], ["Armon", "Amitai", ""]]}, {"id": "2101.05307", "submitter": "Eloi Zablocki", "authors": "\\'Eloi Zablocki, H\\'edi Ben-Younes, Patrick P\\'erez, Matthieu Cord", "title": "Explainability of vision-based autonomous driving systems: Review and\n  challenges", "comments": "submitted to IJCV", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG cs.RO", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  This survey reviews explainability methods for vision-based self-driving\nsystems. The concept of explainability has several facets and the need for\nexplainability is strong in driving, a safety-critical application. Gathering\ncontributions from several research fields, namely computer vision, deep\nlearning, autonomous driving, explainable AI (X-AI), this survey tackles\nseveral points. First, it discusses definitions, context, and motivation for\ngaining more interpretability and explainability from self-driving systems.\nSecond, major recent state-of-the-art approaches to develop self-driving\nsystems are quickly presented. Third, methods providing explanations to a\nblack-box self-driving system in a post-hoc fashion are comprehensively\norganized and detailed. Fourth, approaches from the literature that aim at\nbuilding more interpretable self-driving systems by design are presented and\ndiscussed in detail. Finally, remaining open-challenges and potential future\nresearch directions are identified and examined.\n", "versions": [{"version": "v1", "created": "Wed, 13 Jan 2021 19:09:38 GMT"}], "update_date": "2021-01-15", "authors_parsed": [["Zablocki", "\u00c9loi", ""], ["Ben-Younes", "H\u00e9di", ""], ["P\u00e9rez", "Patrick", ""], ["Cord", "Matthieu", ""]]}, {"id": "2101.05313", "submitter": "Tobias Bleisch", "authors": "Qiong Hu, Tobias Bleisch, Petko Petkov, Tuomo Raitio, Erik Marchi,\n  Varun Lakshminarasimhan", "title": "Whispered and Lombard Neural Speech Synthesis", "comments": "To appear in SLT 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.AS cs.CL cs.LG cs.SD", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is desirable for a text-to-speech system to take into account the\nenvironment where synthetic speech is presented, and provide appropriate\ncontext-dependent output to the user. In this paper, we present and compare\nvarious approaches for generating different speaking styles, namely, normal,\nLombard, and whisper speech, using only limited data. The following systems are\nproposed and assessed: 1) Pre-training and fine-tuning a model for each style.\n2) Lombard and whisper speech conversion through a signal processing based\napproach. 3) Multi-style generation using a single model based on a speaker\nverification model. Our mean opinion score and AB preference listening tests\nshow that 1) we can generate high quality speech through the\npre-training/fine-tuning approach for all speaking styles. 2) Although our\nspeaker verification (SV) model is not explicitly trained to discriminate\ndifferent speaking styles, and no Lombard and whisper voice is used for\npre-training this system, the SV model can be used as a style encoder for\ngenerating different style embeddings as input for the Tacotron system. We also\nshow that the resulting synthetic Lombard speech has a significant positive\nimpact on intelligibility gain.\n", "versions": [{"version": "v1", "created": "Wed, 13 Jan 2021 19:22:11 GMT"}], "update_date": "2021-01-15", "authors_parsed": [["Hu", "Qiong", ""], ["Bleisch", "Tobias", ""], ["Petkov", "Petko", ""], ["Raitio", "Tuomo", ""], ["Marchi", "Erik", ""], ["Lakshminarasimhan", "Varun", ""]]}, {"id": "2101.05317", "submitter": "Yan Du", "authors": "Renke Huang, Yujiao Chen, Tianzhixi Yin, Qiuhua Huang, Jie Tan, Wenhao\n  Yu, Xinya Li, Ang Li, Yan Du", "title": "Learning and Fast Adaptation for Grid Emergency Control via Deep Meta\n  Reinforcement Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.SY eess.SY", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  As power systems are undergoing a significant transformation with more\nuncertainties, less inertia and closer to operation limits, there is increasing\nrisk of large outages. Thus, there is an imperative need to enhance grid\nemergency control to maintain system reliability and security. Towards this\nend, great progress has been made in developing deep reinforcement learning\n(DRL) based grid control solutions in recent years. However, existing DRL-based\nsolutions have two main limitations: 1) they cannot handle well with a wide\nrange of grid operation conditions, system parameters, and contingencies; 2)\nthey generally lack the ability to fast adapt to new grid operation conditions,\nsystem parameters, and contingencies, limiting their applicability for\nreal-world applications. In this paper, we mitigate these limitations by\ndeveloping a novel deep meta reinforcement learning (DMRL) algorithm. The DMRL\ncombines the meta strategy optimization together with DRL, and trains policies\nmodulated by a latent space that can quickly adapt to new scenarios. We test\nthe developed DMRL algorithm on the IEEE 300-bus system. We demonstrate fast\nadaptation of the meta-trained DRL polices with latent variables to new\noperating conditions and scenarios using the proposed method and achieve\nsuperior performance compared to the state-of-the-art DRL and model predictive\ncontrol (MPC) methods.\n", "versions": [{"version": "v1", "created": "Wed, 13 Jan 2021 19:45:59 GMT"}], "update_date": "2021-01-15", "authors_parsed": [["Huang", "Renke", ""], ["Chen", "Yujiao", ""], ["Yin", "Tianzhixi", ""], ["Huang", "Qiuhua", ""], ["Tan", "Jie", ""], ["Yu", "Wenhao", ""], ["Li", "Xinya", ""], ["Li", "Ang", ""], ["Du", "Yan", ""]]}, {"id": "2101.05328", "submitter": "Armin Lederer", "authors": "Armin Lederer, Jonas Umlauft, Sandra Hirche", "title": "Uniform Error and Posterior Variance Bounds for Gaussian Process\n  Regression with Application to Safe Control", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.SY eess.SY stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In application areas where data generation is expensive, Gaussian processes\nare a preferred supervised learning model due to their high data-efficiency.\nParticularly in model-based control, Gaussian processes allow the derivation of\nperformance guarantees using probabilistic model error bounds. To make these\napproaches applicable in practice, two open challenges must be solved i)\nExisting error bounds rely on prior knowledge, which might not be available for\nmany real-world tasks. (ii) The relationship between training data and the\nposterior variance, which mainly drives the error bound, is not well understood\nand prevents the asymptotic analysis. This article addresses these issues by\npresenting a novel uniform error bound using Lipschitz continuity and an\nanalysis of the posterior variance function for a large class of kernels.\nAdditionally, we show how these results can be used to guarantee safe control\nof an unknown dynamical system and provide numerical illustration examples.\n", "versions": [{"version": "v1", "created": "Wed, 13 Jan 2021 20:06:30 GMT"}], "update_date": "2021-01-15", "authors_parsed": [["Lederer", "Armin", ""], ["Umlauft", "Jonas", ""], ["Hirche", "Sandra", ""]]}, {"id": "2101.05339", "submitter": "Tian Xie", "authors": "Tian Xie, Arthur France-Lanord, Yanming Wang, Jeffrey Lopez, Michael\n  Austin Stolberg, Megan Hill, Graham Michael Leverick, Rafael\n  Gomez-Bombarelli, Jeremiah A. Johnson, Yang Shao-Horn, Jeffrey C. Grossman", "title": "Accelerating the screening of amorphous polymer electrolytes by learning\n  to reduce random and systematic errors in molecular dynamics simulations", "comments": "25 pages, 5 figures + supplementary information", "journal-ref": null, "doi": null, "report-no": null, "categories": "cond-mat.mtrl-sci cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Machine learning has been widely adopted to accelerate the screening of\nmaterials. Most existing studies implicitly assume that the training data are\ngenerated through a deterministic, unbiased process, but this assumption might\nnot hold for the simulation of some complex materials. In this work, we aim to\nscreen amorphous polymer electrolytes which are promising candidates for the\nnext generation lithium-ion battery technology but extremely expensive to\nsimulate due to their structural complexity. We demonstrate that a multi-task\ngraph neural network can learn from a large amount of noisy, biased data and a\nsmall number of unbiased data and reduce both random and systematic errors in\npredicting the transport properties of polymer electrolytes. This observation\nallows us to achieve accurate predictions on the properties of complex\nmaterials by learning to reduce errors in the training data, instead of running\nrepetitive, expensive simulations which is conventionally used to reduce\nsimulation errors. With this approach, we screen a space of 6247 polymer\nelectrolytes, orders of magnitude larger than previous computational studies.\nWe also find a good extrapolation performance to the top polymers from a larger\nspace of 53362 polymers and 31 experimentally-realized polymers. The strategy\nemployed in this work may be applicable to a broad class of material discovery\nproblems that involve the simulation of complex, amorphous materials.\n", "versions": [{"version": "v1", "created": "Wed, 13 Jan 2021 20:46:24 GMT"}], "update_date": "2021-01-15", "authors_parsed": [["Xie", "Tian", ""], ["France-Lanord", "Arthur", ""], ["Wang", "Yanming", ""], ["Lopez", "Jeffrey", ""], ["Stolberg", "Michael Austin", ""], ["Hill", "Megan", ""], ["Leverick", "Graham Michael", ""], ["Gomez-Bombarelli", "Rafael", ""], ["Johnson", "Jeremiah A.", ""], ["Shao-Horn", "Yang", ""], ["Grossman", "Jeffrey C.", ""]]}, {"id": "2101.05346", "submitter": "Xintian Han", "authors": "Mark Goldstein, Xintian Han, Aahlad Puli, Adler J. Perotte and Rajesh\n  Ranganath", "title": "X-CAL: Explicit Calibration for Survival Analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Survival analysis models the distribution of time until an event of interest,\nsuch as discharge from the hospital or admission to the ICU. When a model's\npredicted number of events within any time interval is similar to the observed\nnumber, it is called well-calibrated. A survival model's calibration can be\nmeasured using, for instance, distributional calibration (D-CALIBRATION)\n[Haider et al., 2020] which computes the squared difference between the\nobserved and predicted number of events within different time intervals.\nClassically, calibration is addressed in post-training analysis. We develop\nexplicit calibration (X-CAL), which turns D-CALIBRATION into a differentiable\nobjective that can be used in survival modeling alongside maximum likelihood\nestimation and other objectives. X-CAL allows practitioners to directly\noptimize calibration and strike a desired balance between predictive power and\ncalibration. In our experiments, we fit a variety of shallow and deep models on\nsimulated data, a survival dataset based on MNIST, on length-of-stay prediction\nusing MIMIC-III data, and on brain cancer data from The Cancer Genome Atlas. We\nshow that the models we study can be miscalibrated. We give experimental\nevidence on these datasets that X-CAL improves D-CALIBRATION without a large\ndecrease in concordance or likelihood.\n", "versions": [{"version": "v1", "created": "Wed, 13 Jan 2021 21:00:23 GMT"}], "update_date": "2021-01-15", "authors_parsed": [["Goldstein", "Mark", ""], ["Han", "Xintian", ""], ["Puli", "Aahlad", ""], ["Perotte", "Adler J.", ""], ["Ranganath", "Rajesh", ""]]}, {"id": "2101.05348", "submitter": "Hang Yin", "authors": "Hang Yin, Xinyue Liu, Xiangnan Kong", "title": "Gaussian Mixture Graphical Lasso with Application to Edge Detection in\n  Brain Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sparse inverse covariance estimation (i.e., edge de-tection) is an important\nresearch problem in recent years, wherethe goal is to discover the direct\nconnections between a set ofnodes in a networked system based upon the observed\nnodeactivities. Existing works mainly focus on unimodal distributions,where it\nis usually assumed that the observed activities aregenerated from\nasingleGaussian distribution (i.e., one graph).However, this assumption is too\nstrong for many real-worldapplications. In many real-world applications (e.g.,\nbrain net-works), the node activities usually exhibit much more complexpatterns\nthat are difficult to be captured by one single Gaussiandistribution. In this\nwork, we are inspired by Latent DirichletAllocation (LDA) [4] and consider\nmodeling the edge detectionproblem as estimating a mixture ofmultipleGaussian\ndistribu-tions, where each corresponds to a separate sub-network. Toaddress\nthis problem, we propose a novel model called GaussianMixture Graphical Lasso\n(MGL). It learns the proportionsof signals generated by each mixture component\nand theirparameters iteratively via an EM framework. To obtain\nmoreinterpretable networks, MGL imposes a special regularization,called Mutual\nExclusivity Regularization (MER), to minimize theoverlap between different\nsub-networks. MER also addresses thecommon issues in read-world data sets,i.e.,\nnoisy observationsand small sample size. Through the extensive experiments\nonsynthetic and real brain data sets, the results demonstrate thatMGL can\neffectively discover multiple connectivity structuresfrom the observed node\nactivities\n", "versions": [{"version": "v1", "created": "Wed, 13 Jan 2021 21:15:30 GMT"}], "update_date": "2021-01-15", "authors_parsed": [["Yin", "Hang", ""], ["Liu", "Xinyue", ""], ["Kong", "Xiangnan", ""]]}, {"id": "2101.05357", "submitter": "Mehrshad Zandigohar", "authors": "Mehrshad Zandigohar, Mo Han, Deniz Erdogmus, and Gunar Schirner", "title": "Towards Creating a Deployable Grasp Type Probability Estimator for a\n  Prosthetic Hand", "comments": null, "journal-ref": "CyPhy 2019, WESE 2019. Lecture Notes in Computer Science, vol\n  11971. Springer, Cham", "doi": "10.1007/978-3-030-41131-2_3", "report-no": null, "categories": "cs.LG cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For lower arm amputees, prosthetic hands promise to restore most of physical\ninteraction capabilities. This requires to accurately predict hand gestures\ncapable of grabbing varying objects and execute them timely as intended by the\nuser. Current approaches often rely on physiological signal inputs such as\nElectromyography (EMG) signal from residual limb muscles to infer the intended\nmotion. However, limited signal quality, user diversity and high variability\nadversely affect the system robustness. Instead of solely relying on EMG\nsignals, our work enables augmenting EMG intent inference with physical state\nprobability through machine learning and computer vision method. To this end,\nwe: (1) study state-of-the-art deep neural network architectures to select a\nperformant source of knowledge transfer for the prosthetic hand, (2) use a\ndataset containing object images and probability distribution of grasp types as\na new form of labeling where instead of using absolute values of zero and one\nas the conventional classification labels, our labels are a set of\nprobabilities whose sum is 1. The proposed method generates probabilistic\npredictions which could be fused with EMG prediction of probabilities over\ngrasps by using the visual information from the palm camera of a prosthetic\nhand. Our results demonstrate that InceptionV3 achieves highest accuracy with\n0.95 angular similarity followed by 1.4 MobileNetV2 with 0.93 at ~20% the\namount of operations.\n", "versions": [{"version": "v1", "created": "Wed, 13 Jan 2021 21:39:41 GMT"}], "update_date": "2021-01-15", "authors_parsed": [["Zandigohar", "Mehrshad", ""], ["Han", "Mo", ""], ["Erdogmus", "Deniz", ""], ["Schirner", "Gunar", ""]]}, {"id": "2101.05360", "submitter": "Melanie F. Pradier", "authors": "Melanie F. Pradier, Javier Zazo, Sonali Parbhoo, Roy H. Perlis,\n  Maurizio Zazzi, Finale Doshi-Velez", "title": "Preferential Mixture-of-Experts: Interpretable Models that Rely on Human\n  Expertise as much as Possible", "comments": "10 pages, 5 figures, 4 tables, AMIA 2021 Virtual Informatics Summit", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We propose Preferential MoE, a novel human-ML mixture-of-experts model that\naugments human expertise in decision making with a data-based classifier only\nwhen necessary for predictive performance. Our model exhibits an interpretable\ngating function that provides information on when human rules should be\nfollowed or avoided. The gating function is maximized for using human-based\nrules, and classification errors are minimized. We propose solving a coupled\nmulti-objective problem with convex subproblems. We develop approximate\nalgorithms and study their performance and convergence. Finally, we demonstrate\nthe utility of Preferential MoE on two clinical applications for the treatment\nof Human Immunodeficiency Virus (HIV) and management of Major Depressive\nDisorder (MDD).\n", "versions": [{"version": "v1", "created": "Wed, 13 Jan 2021 21:57:00 GMT"}], "update_date": "2021-01-15", "authors_parsed": [["Pradier", "Melanie F.", ""], ["Zazo", "Javier", ""], ["Parbhoo", "Sonali", ""], ["Perlis", "Roy H.", ""], ["Zazzi", "Maurizio", ""], ["Doshi-Velez", "Finale", ""]]}, {"id": "2101.05363", "submitter": "Mehrshad Zandigohar", "authors": "Mehrshad Zandigohar, Deniz Erdogmus, Gunar Schirner", "title": "NetCut: Real-Time DNN Inference Using Layer Removal", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep Learning plays a significant role in assisting humans in many aspects of\ntheir lives. As these networks tend to get deeper over time, they extract more\nfeatures to increase accuracy at the cost of additional inference latency. This\naccuracy-performance trade-off makes it more challenging for Embedded Systems,\nas resource-constrained processors with strict deadlines, to deploy them\nefficiently. This can lead to selection of networks that can prematurely meet a\nspecified deadline with excess slack time that could have potentially\ncontributed to increased accuracy.\n  In this work, we propose: (i) the concept of layer removal as a means of\nconstructing TRimmed Networks (TRNs) that are based on removing\nproblem-specific features of a pretrained network used in transfer learning,\nand (ii) NetCut, a methodology based on an empirical or an analytical latency\nestimator, which only proposes and retrains TRNs that can meet the\napplication's deadline, hence reducing the exploration time significantly.\n  We demonstrate that TRNs can expand the Pareto frontier that trades off\nlatency and accuracy to provide networks that can meet arbitrary deadlines with\npotential accuracy improvement over off-the-shelf networks. Our experimental\nresults show that such utilization of TRNs, while transferring to a simpler\ndataset, in combination with NetCut, can lead to the proposal of networks that\ncan achieve relative accuracy improvement of up to 10.43% among existing\noff-the-shelf neural architectures while meeting a specific deadline, and 27x\nspeedup in exploration time.\n", "versions": [{"version": "v1", "created": "Wed, 13 Jan 2021 22:02:43 GMT"}], "update_date": "2021-01-15", "authors_parsed": [["Zandigohar", "Mehrshad", ""], ["Erdogmus", "Deniz", ""], ["Schirner", "Gunar", ""]]}, {"id": "2101.05371", "submitter": "Lukas Daniel Klausner", "authors": "Sebastian Eresheim, Lukas Daniel Klausner, Patrick Kochberger", "title": "Anomaly Detection Support Using Process Classification", "comments": "14 pages, 6 figures", "journal-ref": "Proceedings of the 5th International Conference on Software\n  Security and Assurance (ICSSA 2019), 2019, 27-40", "doi": "10.1109/ICSSA48308.2019.00011", "report-no": null, "categories": "cs.LG cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Anomaly detection systems need to consider a lot of information when scanning\nfor anomalies. One example is the context of the process in which an anomaly\nmight occur, because anomalies for one process might not be anomalies for a\ndifferent one. Therefore data -- such as system events -- need to be assigned\nto the program they originate from. This paper investigates whether it is\npossible to infer from a list of system events the program whose behavior\ncaused the occurrence of these system events. To that end, we model transition\nprobabilities between non-equivalent events and apply the $k$-nearest neighbors\nalgorithm. This system is evaluated on non-malicious, real-world data using\nfour different evaluation scores. Our results suggest that the approach\nproposed in this paper is capable of correctly inferring program names from\nsystem events.\n", "versions": [{"version": "v1", "created": "Wed, 13 Jan 2021 22:22:03 GMT"}], "update_date": "2021-01-18", "authors_parsed": [["Eresheim", "Sebastian", ""], ["Klausner", "Lukas Daniel", ""], ["Kochberger", "Patrick", ""]]}, {"id": "2101.05388", "submitter": "Paul Garnier", "authors": "Paul Garnier, Th\\'eophane Gregoir", "title": "Evaluating Soccer Player: from Live Camera to Deep Reinforcement\n  Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.GT", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Scientifically evaluating soccer players represents a challenging Machine\nLearning problem. Unfortunately, most existing answers have very opaque\nalgorithm training procedures; relevant data are scarcely accessible and almost\nimpossible to generate. In this paper, we will introduce a two-part solution:\nan open-source Player Tracking model and a new approach to evaluate these\nplayers based solely on Deep Reinforcement Learning, without human data\ntraining nor guidance. Our tracking model was trained in a supervised fashion\non datasets we will also release, and our Evaluation Model relies only on\nsimulations of virtual soccer games. Combining those two architectures allows\none to evaluate Soccer Players directly from a live camera without large\ndatasets constraints. We term our new approach Expected Discounted Goal (EDG),\nas it represents the number of goals a team can score or concede from a\nparticular state. This approach leads to more meaningful results than the\nexisting ones that are based on real-world data, and could easily be extended\nto other sports.\n", "versions": [{"version": "v1", "created": "Wed, 13 Jan 2021 23:26:17 GMT"}], "update_date": "2021-01-15", "authors_parsed": [["Garnier", "Paul", ""], ["Gregoir", "Th\u00e9ophane", ""]]}, {"id": "2101.05390", "submitter": "Anastasis Kratsios", "authors": "Anastasis Kratsios, Leonie Papon", "title": "Universal Approximation Theorems for Differentiable Geometric Deep\n  Learning", "comments": "Keywords: Geometric Deep Learning, Symmetric Positive-Definite\n  Matrices, Hyperbolic neural networks, Deep Kalman Filter, Shape Space,\n  Riemannian Manifolds, Curse of Dimensionality. Additional Information: 30\n  Pages + 30 Pages Appendix, 2 Tables, 7 Figures;", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.FA math.GN math.GT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper addresses the growing need to process non-Euclidean data, by\nintroducing a geometric deep learning (GDL) framework for building universal\nfeedforward-type models compatible with differentiable manifold geometries. We\nshow that our GDL models can approximate any continuous target function\nuniformly on compacts of a controlled maximum diameter. We obtain curvature\ndependant lower-bounds on this maximum diameter and upper-bounds on the depth\nof our approximating GDL models. Conversely, we find that there is always a\ncontinuous function between any two non-degenerate compact manifolds that any\n\"locally-defined\" GDL model cannot uniformly approximate. Our last main result\nidentifies data-dependent conditions guaranteeing that the GDL model\nimplementing our approximation breaks \"the curse of dimensionality.\" We find\nthat any \"real-world\" (i.e. finite) dataset always satisfies our condition and,\nconversely, any dataset satisfies our requirement if the target function is\nsmooth. As applications, we confirm the universal approximation capabilities of\nthe following GDL models: Ganea et al. (2018)'s hyperbolic feedforward\nnetworks, the architecture implementing Krishnan et al. (2015)'s deep\nKalman-Filter, and deep softmax classifiers. We build universal\nextensions/variants of: the SPD-matrix regressor of Meyer et al. (2011), and\nFletcher et al. (2009)'s Procrustean regressor. In the Euclidean setting, our\nresults imply a quantitative version of Kidger and Lyons (2020)'s approximation\ntheorem and a data-dependent version of Yarotsky and Zhevnerchuk (2020)'s\nuncursed approximation rates.\n", "versions": [{"version": "v1", "created": "Wed, 13 Jan 2021 23:29:40 GMT"}, {"version": "v2", "created": "Wed, 27 Jan 2021 08:38:02 GMT"}, {"version": "v3", "created": "Tue, 29 Jun 2021 14:47:15 GMT"}], "update_date": "2021-06-30", "authors_parsed": [["Kratsios", "Anastasis", ""], ["Papon", "Leonie", ""]]}, {"id": "2101.05397", "submitter": "Xixin Wu", "authors": "Xixin Wu and Mark Gales", "title": "Should Ensemble Members Be Calibrated?", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Underlying the use of statistical approaches for a wide range of applications\nis the assumption that the probabilities obtained from a statistical model are\nrepresentative of the \"true\" probability that event, or outcome, will occur.\nUnfortunately, for modern deep neural networks this is not the case, they are\noften observed to be poorly calibrated. Additionally, these deep learning\napproaches make use of large numbers of model parameters, motivating the use of\nBayesian, or ensemble approximation, approaches to handle issues with parameter\nestimation. This paper explores the application of calibration schemes to deep\nensembles from both a theoretical perspective and empirically on a standard\nimage classification task, CIFAR-100. The underlying theoretical requirements\nfor calibration, and associated calibration criteria, are first described. It\nis shown that well calibrated ensemble members will not necessarily yield a\nwell calibrated ensemble prediction, and if the ensemble prediction is well\ncalibrated its performance cannot exceed that of the average performance of the\ncalibrated ensemble members. On CIFAR-100 the impact of calibration for\nensemble prediction, and associated calibration is evaluated. Additionally the\nsituation where multiple different topologies are combined together is\ndiscussed.\n", "versions": [{"version": "v1", "created": "Wed, 13 Jan 2021 23:59:00 GMT"}], "update_date": "2021-01-15", "authors_parsed": [["Wu", "Xixin", ""], ["Gales", "Mark", ""]]}, {"id": "2101.05400", "submitter": "Manuel Ciosici", "authors": "Manuel R. Ciosici, Joseph Cummings, Mitchell DeHaven, Alex Hedges,\n  Yash Kankanampati, Dong-Ho Lee, Ralph Weischedel, Marjorie Freedman", "title": "Machine-Assisted Script Curation", "comments": "Identical to the NAACL 2021 Demo version", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  We describe Machine-Aided Script Curator (MASC), a system for human-machine\ncollaborative script authoring. Scripts produced with MASC include (1) English\ndescriptions of sub-events that comprise a larger, complex event; (2) event\ntypes for each of those events; (3) a record of entities expected to\nparticipate in multiple sub-events; and (4) temporal sequencing between the\nsub-events. MASC automates portions of the script creation process with\nsuggestions for event types, links to Wikidata, and sub-events that may have\nbeen forgotten. We illustrate how these automations are useful to the script\nwriter with a few case-study scripts.\n", "versions": [{"version": "v1", "created": "Thu, 14 Jan 2021 00:19:21 GMT"}, {"version": "v2", "created": "Tue, 4 May 2021 16:33:01 GMT"}], "update_date": "2021-05-05", "authors_parsed": [["Ciosici", "Manuel R.", ""], ["Cummings", "Joseph", ""], ["DeHaven", "Mitchell", ""], ["Hedges", "Alex", ""], ["Kankanampati", "Yash", ""], ["Lee", "Dong-Ho", ""], ["Weischedel", "Ralph", ""], ["Freedman", "Marjorie", ""]]}, {"id": "2101.05402", "submitter": "Anderson Ye Zhang", "authors": "Xin Chen, Anderson Y. Zhang", "title": "Optimal Clustering in Anisotropic Gaussian Mixture Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.LG stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the clustering task under anisotropic Gaussian Mixture Models where\nthe covariance matrices from different clusters are unknown and are not\nnecessarily the identical matrix. We characterize the dependence of\nsignal-to-noise ratios on the cluster centers and covariance matrices and\nobtain the minimax lower bound for the clustering problem. In addition, we\npropose a computationally feasible procedure and prove it achieves the optimal\nrate within a few iterations. The proposed procedure is a hard EM type\nalgorithm, and it can also be seen as a variant of the Lloyd's algorithm that\nis adjusted to the anisotropic covariance matrices.\n", "versions": [{"version": "v1", "created": "Thu, 14 Jan 2021 00:31:52 GMT"}, {"version": "v2", "created": "Mon, 18 Jan 2021 04:24:38 GMT"}], "update_date": "2021-01-19", "authors_parsed": [["Chen", "Xin", ""], ["Zhang", "Anderson Y.", ""]]}, {"id": "2101.05403", "submitter": "Jianzhong Wang", "authors": "Yanni Zhang, Yiming Liu, Qiang Li, Miao Qi, Dahong Xu, Jun Kong,\n  Jianzhong Wang", "title": "Image deblurring based on lightweight multi-information fusion network", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Recently, deep learning based image deblurring has been well developed.\nHowever, exploiting the detailed image features in a deep learning framework\nalways requires a mass of parameters, which inevitably makes the network suffer\nfrom high computational burden. To solve this problem, we propose a lightweight\nmultiinformation fusion network (LMFN) for image deblurring. The proposed LMFN\nis designed as an encoder-decoder architecture. In the encoding stage, the\nimage feature is reduced to various smallscale spaces for multi-scale\ninformation extraction and fusion without a large amount of information loss.\nThen, a distillation network is used in the decoding stage, which allows the\nnetwork benefit the most from residual learning while remaining sufficiently\nlightweight. Meanwhile, an information fusion strategy between distillation\nmodules and feature channels is also carried out by attention mechanism.\nThrough fusing different information in the proposed approach, our network can\nachieve state-of-the-art image deblurring result with smaller number of\nparameters and outperforms existing methods in model complexity.\n", "versions": [{"version": "v1", "created": "Thu, 14 Jan 2021 00:37:37 GMT"}], "update_date": "2021-01-15", "authors_parsed": [["Zhang", "Yanni", ""], ["Liu", "Yiming", ""], ["Li", "Qiang", ""], ["Qi", "Miao", ""], ["Xu", "Dahong", ""], ["Kong", "Jun", ""], ["Wang", "Jianzhong", ""]]}, {"id": "2101.05404", "submitter": "Justyna P. Zwolak", "authors": "Shangjie Guo, Amilson R. Fritsch, Craig Greenberg, I. B. Spielman,\n  Justyna P. Zwolak", "title": "Machine-learning enhanced dark soliton detection in Bose-Einstein\n  condensates", "comments": "17 pages, 5 figures", "journal-ref": "Mach. Learn.: Sci. Technol. 2: 035020 (2021)", "doi": "10.1088/2632-2153/abed1e", "report-no": null, "categories": "cond-mat.quant-gas cs.CV cs.LG quant-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most data in cold-atom experiments comes from images, the analysis of which\nis limited by our preconceptions of the patterns that could be present in the\ndata. We focus on the well-defined case of detecting dark solitons -- appearing\nas local density depletions in a Bose-Einstein condensate (BEC) -- using a\nmethodology that is extensible to the general task of pattern recognition in\nimages of cold atoms. Studying soliton dynamics over a wide range of parameters\nrequires the analysis of large datasets, making the existing\nhuman-inspection-based methodology a significant bottleneck. Here we describe\nan automated classification and positioning system for identifying localized\nexcitations in atomic BECs utilizing deep convolutional neural networks to\neliminate the need for human image examination. Furthermore, we openly publish\nour labeled dataset of dark solitons, the first of its kind, for further\nmachine learning research.\n", "versions": [{"version": "v1", "created": "Thu, 14 Jan 2021 00:44:56 GMT"}, {"version": "v2", "created": "Thu, 17 Jun 2021 17:41:14 GMT"}], "update_date": "2021-06-18", "authors_parsed": [["Guo", "Shangjie", ""], ["Fritsch", "Amilson R.", ""], ["Greenberg", "Craig", ""], ["Spielman", "I. B.", ""], ["Zwolak", "Justyna P.", ""]]}, {"id": "2101.05405", "submitter": "Huseyin Inan", "authors": "Huseyin A. Inan, Osman Ramadan, Lukas Wutschitz, Daniel Jones, Victor\n  R\\\"uhle, James Withers, Robert Sim", "title": "Training Data Leakage Analysis in Language Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent advances in neural network based language models lead to successful\ndeployments of such models, improving user experience in various applications.\nIt has been demonstrated that strong performance of language models comes along\nwith the ability to memorize rare training samples, which poses serious privacy\nthreats in case the model is trained on confidential user content. In this\nwork, we introduce a methodology that investigates identifying the user content\nin the training data that could be leaked under a strong and realistic threat\nmodel. We propose two metrics to quantify user-level data leakage by measuring\na model's ability to produce unique sentence fragments within training data.\nOur metrics further enable comparing different models trained on the same data\nin terms of privacy. We demonstrate our approach through extensive numerical\nstudies on both RNN and Transformer based models. We further illustrate how the\nproposed metrics can be utilized to investigate the efficacy of mitigations\nlike differentially private training or API hardening.\n", "versions": [{"version": "v1", "created": "Thu, 14 Jan 2021 00:57:32 GMT"}, {"version": "v2", "created": "Mon, 22 Feb 2021 23:53:08 GMT"}], "update_date": "2021-02-24", "authors_parsed": [["Inan", "Huseyin A.", ""], ["Ramadan", "Osman", ""], ["Wutschitz", "Lukas", ""], ["Jones", "Daniel", ""], ["R\u00fchle", "Victor", ""], ["Withers", "James", ""], ["Sim", "Robert", ""]]}, {"id": "2101.05415", "submitter": "EPTCS", "authors": "Tommaso Dreossi (Amazon Search), Giorgio Ballardin (Amazon Search),\n  Parth Gupta (Amazon Search), Jan Bakus (Amazon Search), Yu-Hsiang Lin (Amazon\n  Search), Vamsi Salaka (Amazon Search)", "title": "Analysis of E-commerce Ranking Signals via Signal Temporal Logic", "comments": "In Proceedings SNR 2020, arXiv:2101.05256", "journal-ref": "EPTCS 331, 2021, pp. 33-42", "doi": "10.4204/EPTCS.331.3", "report-no": null, "categories": "cs.LO cs.FL cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The timed position of documents retrieved by learning to rank models can be\nseen as signals. Signals carry useful information such as drop or rise of\ndocuments over time or user behaviors. In this work, we propose to use the\nlogic formalism called Signal Temporal Logic (STL) to characterize document\nbehaviors in ranking accordingly to the specified formulas. Our analysis shows\nthat interesting document behaviors can be easily formalized and detected\nthanks to STL formulas. We validate our idea on a dataset of 100K product\nsignals. Through the presented framework, we uncover interesting patterns, such\nas cold start, warm start, spikes, and inspect how they affect our learning to\nranks models.\n", "versions": [{"version": "v1", "created": "Thu, 14 Jan 2021 01:54:31 GMT"}], "update_date": "2021-01-15", "authors_parsed": [["Dreossi", "Tommaso", "", "Amazon Search"], ["Ballardin", "Giorgio", "", "Amazon Search"], ["Gupta", "Parth", "", "Amazon Search"], ["Bakus", "Jan", "", "Amazon Search"], ["Lin", "Yu-Hsiang", "", "Amazon\n  Search"], ["Salaka", "Vamsi", "", "Amazon Search"]]}, {"id": "2101.05428", "submitter": "Priyanka Mary Mammen", "authors": "Priyanka Mary Mammen", "title": "Federated Learning: Opportunities and Challenges", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Federated Learning (FL) is a concept first introduced by Google in 2016, in\nwhich multiple devices collaboratively learn a machine learning model without\nsharing their private data under the supervision of a central server. This\noffers ample opportunities in critical domains such as healthcare, finance etc,\nwhere it is risky to share private user information to other organisations or\ndevices. While FL appears to be a promising Machine Learning (ML) technique to\nkeep the local data private, it is also vulnerable to attacks like other ML\nmodels. Given the growing interest in the FL domain, this report discusses the\nopportunities and challenges in federated learning.\n", "versions": [{"version": "v1", "created": "Thu, 14 Jan 2021 02:44:28 GMT"}], "update_date": "2021-01-15", "authors_parsed": [["Mammen", "Priyanka Mary", ""]]}, {"id": "2101.05442", "submitter": "Xin He", "authors": "Xin He, Shihao Wang, Xiaowen Chu, Shaohuai Shi, Jiangping Tang, Xin\n  Liu, Chenggang Yan, Jiyong Zhang, Guiguang Ding", "title": "Automated Model Design and Benchmarking of 3D Deep Learning Models for\n  COVID-19 Detection with Chest CT Scans", "comments": "Accepted by AAAI 2021, COVID-19, Neural Architecture Search, AutoML", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The COVID-19 pandemic has spread globally for several months. Because its\ntransmissibility and high pathogenicity seriously threaten people's lives, it\nis crucial to accurately and quickly detect COVID-19 infection. Many recent\nstudies have shown that deep learning (DL) based solutions can help detect\nCOVID-19 based on chest CT scans. However, most existing work focuses on 2D\ndatasets, which may result in low quality models as the real CT scans are 3D\nimages. Besides, the reported results span a broad spectrum on different\ndatasets with a relatively unfair comparison. In this paper, we first use three\nstate-of-the-art 3D models (ResNet3D101, DenseNet3D121, and MC3\\_18) to\nestablish the baseline performance on the three publicly available chest CT\nscan datasets. Then we propose a differentiable neural architecture search\n(DNAS) framework to automatically search for the 3D DL models for 3D chest CT\nscans classification with the Gumbel Softmax technique to improve the searching\nefficiency. We further exploit the Class Activation Mapping (CAM) technique on\nour models to provide the interpretability of the results. The experimental\nresults show that our automatically searched models (CovidNet3D) outperform the\nbaseline human-designed models on the three datasets with tens of times smaller\nmodel size and higher accuracy. Furthermore, the results also verify that CAM\ncan be well applied in CovidNet3D for COVID-19 datasets to provide\ninterpretability for medical diagnosis.\n", "versions": [{"version": "v1", "created": "Thu, 14 Jan 2021 03:45:01 GMT"}, {"version": "v2", "created": "Fri, 12 Feb 2021 05:02:43 GMT"}], "update_date": "2021-02-15", "authors_parsed": [["He", "Xin", ""], ["Wang", "Shihao", ""], ["Chu", "Xiaowen", ""], ["Shi", "Shaohuai", ""], ["Tang", "Jiangping", ""], ["Liu", "Xin", ""], ["Yan", "Chenggang", ""], ["Zhang", "Jiyong", ""], ["Ding", "Guiguang", ""]]}, {"id": "2101.05453", "submitter": "Jian Li", "authors": "Jian Li, Raziel Alvarez", "title": "On the quantization of recurrent neural networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Integer quantization of neural networks can be defined as the approximation\nof the high precision computation of the canonical neural network formulation,\nusing reduced integer precision. It plays a significant role in the efficient\ndeployment and execution of machine learning (ML) systems, reducing memory\nconsumption and leveraging typically faster computations. In this work, we\npresent an integer-only quantization strategy for Long Short-Term Memory (LSTM)\nneural network topologies, which themselves are the foundation of many\nproduction ML systems. Our quantization strategy is accurate (e.g. works well\nwith quantization post-training), efficient and fast to execute (utilizing 8\nbit integer weights and mostly 8 bit activations), and is able to target a\nvariety of hardware (by leveraging instructions sets available in common CPU\narchitectures, as well as available neural accelerators).\n", "versions": [{"version": "v1", "created": "Thu, 14 Jan 2021 04:25:08 GMT"}], "update_date": "2021-01-15", "authors_parsed": [["Li", "Jian", ""], ["Alvarez", "Raziel", ""]]}, {"id": "2101.05457", "submitter": "Ka-Hou Chan", "authors": "Ka-Hou Chan, Sio-Kei Im and Wei Ke", "title": "A Multiple Classifier Approach for Concatenate-Designed Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This article introduces a multiple classifier method to improve the\nperformance of concatenate-designed neural networks, such as ResNet and\nDenseNet, with the purpose to alleviate the pressure on the final classifier.\nWe give the design of the classifiers, which collects the features produced\nbetween the network sets, and present the constituent layers and the activation\nfunction for the classifiers, to calculate the classification score of each\nclassifier. We use the L2 normalization method to obtain the classifier score\ninstead of the Softmax normalization. We also determine the conditions that can\nenhance convergence. As a result, the proposed classifiers are able to improve\nthe accuracy in the experimental cases significantly, and show that the method\nnot only has better performance than the original models, but also produces\nfaster convergence. Moreover, our classifiers are general and can be applied to\nall classification related concatenate-designed network models.\n", "versions": [{"version": "v1", "created": "Thu, 14 Jan 2021 04:32:40 GMT"}], "update_date": "2021-01-15", "authors_parsed": [["Chan", "Ka-Hou", ""], ["Im", "Sio-Kei", ""], ["Ke", "Wei", ""]]}, {"id": "2101.05467", "submitter": "Qizhou Wang", "authors": "Qizhou Wang, Bo Han, Tongliang Liu, Gang Niu, Jian Yang, Chen Gong", "title": "Tackling Instance-Dependent Label Noise via a Universal Probabilistic\n  Model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The drastic increase of data quantity often brings the severe decrease of\ndata quality, such as incorrect label annotations, which poses a great\nchallenge for robustly training Deep Neural Networks (DNNs). Existing learning\n\\mbox{methods} with label noise either employ ad-hoc heuristics or restrict to\nspecific noise assumptions. However, more general situations, such as\ninstance-dependent label noise, have not been fully explored, as scarce studies\nfocus on their label corruption process. By categorizing instances into\nconfusing and unconfusing instances, this paper proposes a simple yet universal\nprobabilistic model, which explicitly relates noisy labels to their instances.\nThe resultant model can be realized by DNNs, where the training procedure is\naccomplished by employing an alternating optimization algorithm. Experiments on\ndatasets with both synthetic and real-world label noise verify that the\nproposed method yields significant improvements on robustness over\nstate-of-the-art counterparts.\n", "versions": [{"version": "v1", "created": "Thu, 14 Jan 2021 05:43:51 GMT"}], "update_date": "2021-01-15", "authors_parsed": [["Wang", "Qizhou", ""], ["Han", "Bo", ""], ["Liu", "Tongliang", ""], ["Niu", "Gang", ""], ["Yang", "Jian", ""], ["Gong", "Chen", ""]]}, {"id": "2101.05471", "submitter": "Li Shen", "authors": "Congliang Chen, Li Shen, Fangyu Zou, Wei Liu", "title": "Towards Practical Adam: Non-Convexity, Convergence Theory, and\n  Mini-Batch Acceleration", "comments": "44 Pages. arXiv admin note: substantial text overlap with\n  arXiv:1811.09358", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DC math.OC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Adam is one of the most influential adaptive stochastic algorithms for\ntraining deep neural networks, which has been pointed out to be divergent even\nin the simple convex setting via a few simple counterexamples. Many attempts,\nsuch as decreasing an adaptive learning rate, adopting a big batch size,\nincorporating a temporal decorrelation technique, seeking an analogous\nsurrogate, \\textit{etc.}, have been tried to promote Adam-type algorithms to\nconverge. In contrast with existing approaches, we introduce an alternative\neasy-to-check sufficient condition, which merely depends on the parameters of\nthe base learning rate and combinations of historical second-order moments, to\nguarantee the global convergence of generic Adam for solving large-scale\nnon-convex stochastic optimization. This observation coupled with this\nsufficient condition gives much deeper interpretations on the divergence of\nAdam. On the other hand, in practice, mini-Adam and distributed-Adam are widely\nused without theoretical guarantee, we further give an analysis on how will the\nbatch size or the number of nodes in the distributed system will affect the\nconvergence of Adam, which theoretically shows that mini-batch and distributed\nAdam can be linearly accelerated by using a larger mini-batch size or more\nnumber of nodes. At last, we apply the generic Adam and mini-batch Adam with a\nsufficient condition for solving the counterexample and training several\ndifferent neural networks on various real-world datasets. Experimental results\nare exactly in accord with our theoretical analysis.\n", "versions": [{"version": "v1", "created": "Thu, 14 Jan 2021 06:42:29 GMT"}], "update_date": "2021-01-15", "authors_parsed": [["Chen", "Congliang", ""], ["Shen", "Li", ""], ["Zou", "Fangyu", ""], ["Liu", "Wei", ""]]}, {"id": "2101.05477", "submitter": "Yi Yu", "authors": "Yi Yu, Oscar Hernan Madrid Padilla, Daren Wang and Alessandro Rinaldo", "title": "Optimal network online change point localisation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.LG stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of online network change point detection. In this\nsetting, a collection of independent Bernoulli networks is collected\nsequentially, and the underlying distributions change when a change point\noccurs. The goal is to detect the change point as quickly as possible, if it\nexists, subject to a constraint on the number or probability of false alarms.\nIn this paper, on the detection delay, we establish a minimax lower bound and\ntwo upper bounds based on NP-hard algorithms and polynomial-time algorithms,\ni.e., \\[ \\mbox{detection delay} \\begin{cases} \\gtrsim \\log(1/\\alpha)\n\\frac{\\max\\{r^2/n, \\, 1\\}}{\\kappa_0^2 n \\rho},\\\\ \\lesssim \\log(\\Delta/\\alpha)\n\\frac{\\max\\{r^2/n, \\, \\log(r)\\}}{\\kappa_0^2 n \\rho}, & \\mbox{with NP-hard\nalgorithms},\\\\ \\lesssim \\log(\\Delta/\\alpha) \\frac{r}{\\kappa_0^2 n \\rho}, &\n\\mbox{with polynomial-time algorithms}, \\end{cases} \\] where $\\kappa_0, n,\n\\rho, r$ and $\\alpha$ are the normalised jump size, network size, entrywise\nsparsity, rank sparsity and the overall Type-I error upper bound. All the model\nparameters are allowed to vary as $\\Delta$, the location of the change point,\ndiverges. The polynomial-time algorithms are novel procedures that we propose\nin this paper, designed for quick detection under two different forms of Type-I\nerror control. The first is based on controlling the overall probability of a\nfalse alarm when there are no change points, and the second is based on\nspecifying a lower bound on the expected time of the first false alarm.\nExtensive experiments show that, under different scenarios and the\naforementioned forms of Type-I error control, our proposed approaches\noutperform state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Thu, 14 Jan 2021 07:24:39 GMT"}], "update_date": "2021-01-15", "authors_parsed": [["Yu", "Yi", ""], ["Padilla", "Oscar Hernan Madrid", ""], ["Wang", "Daren", ""], ["Rinaldo", "Alessandro", ""]]}, {"id": "2101.05479", "submitter": "Sharanya Chakravarthy", "authors": "Vinay Damodaran, Sharanya Chakravarthy, Akshay Kumar, Anjana Umapathy,\n  Teruko Mitamura, Yuta Nakashima, Noa Garcia, Chenhui Chu", "title": "Understanding the Role of Scene Graphs in Visual Question Answering", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Visual Question Answering (VQA) is of tremendous interest to the research\ncommunity with important applications such as aiding visually impaired users\nand image-based search. In this work, we explore the use of scene graphs for\nsolving the VQA task. We conduct experiments on the GQA dataset which presents\na challenging set of questions requiring counting, compositionality and\nadvanced reasoning capability, and provides scene graphs for a large number of\nimages. We adopt image + question architectures for use with scene graphs,\nevaluate various scene graph generation techniques for unseen images, propose a\ntraining curriculum to leverage human-annotated and auto-generated scene\ngraphs, and build late fusion architectures to learn from multiple image\nrepresentations. We present a multi-faceted study into the use of scene graphs\nfor VQA, making this work the first of its kind.\n", "versions": [{"version": "v1", "created": "Thu, 14 Jan 2021 07:27:37 GMT"}, {"version": "v2", "created": "Sun, 17 Jan 2021 04:17:07 GMT"}], "update_date": "2021-01-19", "authors_parsed": [["Damodaran", "Vinay", ""], ["Chakravarthy", "Sharanya", ""], ["Kumar", "Akshay", ""], ["Umapathy", "Anjana", ""], ["Mitamura", "Teruko", ""], ["Nakashima", "Yuta", ""], ["Garcia", "Noa", ""], ["Chu", "Chenhui", ""]]}, {"id": "2101.05484", "submitter": "Guowen Xiao", "authors": "Guowen Xiao, Mengwen Ye, Bowen Xu, Zhendi Chen, Quansheng Ren", "title": "4D Attention-based Neural Network for EEG Emotion Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Electroencephalograph (EEG) emotion recognition is a significant task in the\nbrain-computer interface field. Although many deep learning methods are\nproposed recently, it is still challenging to make full use of the information\ncontained in different domains of EEG signals. In this paper, we present a\nnovel method, called four-dimensional attention-based neural network (4D-aNN)\nfor EEG emotion recognition. First, raw EEG signals are transformed into 4D\nspatial-spectral-temporal representations. Then, the proposed 4D-aNN adopts\nspectral and spatial attention mechanisms to adaptively assign the weights of\ndifferent brain regions and frequency bands, and a convolutional neural network\n(CNN) is utilized to deal with the spectral and spatial information of the 4D\nrepresentations. Moreover, a temporal attention mechanism is integrated into a\nbidirectional Long Short-Term Memory (LSTM) to explore temporal dependencies of\nthe 4D representations. Our model achieves state-of-the-art performance on the\nSEED dataset under intra-subject splitting. The experimental results have shown\nthe effectiveness of the attention mechanisms in different domains for EEG\nemotion recognition.\n", "versions": [{"version": "v1", "created": "Thu, 14 Jan 2021 07:41:48 GMT"}], "update_date": "2021-01-15", "authors_parsed": [["Xiao", "Guowen", ""], ["Ye", "Mengwen", ""], ["Xu", "Bowen", ""], ["Chen", "Zhendi", ""], ["Ren", "Quansheng", ""]]}, {"id": "2101.05486", "submitter": "Yuxiang Ren", "authors": "Yuxiang Ren, Jiyang Bai, and Jiawei Zhang", "title": "Label Contrastive Coding based Graph Neural Network for Graph\n  Classification", "comments": "Accept by DASFAA'21", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Graph classification is a critical research problem in many applications from\ndifferent domains. In order to learn a graph classification model, the most\nwidely used supervision component is an output layer together with\nclassification loss (e.g.,cross-entropy loss together with softmax or margin\nloss). In fact, the discriminative information among instances are more\nfine-grained, which can benefit graph classification tasks. In this paper, we\npropose the novel Label Contrastive Coding based Graph Neural Network (LCGNN)\nto utilize label information more effectively and comprehensively. LCGNN still\nuses the classification loss to ensure the discriminability of classes.\nMeanwhile, LCGNN leverages the proposed Label Contrastive Loss derived from\nself-supervised learning to encourage instance-level intra-class compactness\nand inter-class separability. To power the contrastive learning, LCGNN\nintroduces a dynamic label memory bank and a momentum updated encoder. Our\nextensive evaluations with eight benchmark graph datasets demonstrate that\nLCGNN can outperform state-of-the-art graph classification models. Experimental\nresults also verify that LCGNN can achieve competitive performance with less\ntraining data because LCGNN exploits label information comprehensively.\n", "versions": [{"version": "v1", "created": "Thu, 14 Jan 2021 07:45:55 GMT"}], "update_date": "2021-01-15", "authors_parsed": [["Ren", "Yuxiang", ""], ["Bai", "Jiyang", ""], ["Zhang", "Jiawei", ""]]}, {"id": "2101.05490", "submitter": "Fengxiang He", "authors": "Fengxiang He, Shiye Lei, Jianmin Ji, Dacheng Tao", "title": "Neural networks behave as hash encoders: An empirical study", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The input space of a neural network with ReLU-like activations is partitioned\ninto multiple linear regions, each corresponding to a specific activation\npattern of the included ReLU-like activations. We demonstrate that this\npartition exhibits the following encoding properties across a variety of deep\nlearning models: (1) {\\it determinism}: almost every linear region contains at\nmost one training example. We can therefore represent almost every training\nexample by a unique activation pattern, which is parameterized by a {\\it neural\ncode}; and (2) {\\it categorization}: according to the neural code, simple\nalgorithms, such as $K$-Means, $K$-NN, and logistic regression, can achieve\nfairly good performance on both training and test data. These encoding\nproperties surprisingly suggest that {\\it normal neural networks well-trained\nfor classification behave as hash encoders without any extra efforts.} In\naddition, the encoding properties exhibit variability in different scenarios.\n{Further experiments demonstrate that {\\it model size}, {\\it training time},\n{\\it training sample size}, {\\it regularization}, and {\\it label noise}\ncontribute in shaping the encoding properties, while the impacts of the first\nthree are dominant.} We then define an {\\it activation hash phase chart} to\nrepresent the space expanded by {model size}, training time, training sample\nsize, and the encoding properties, which is divided into three canonical\nregions: {\\it under-expressive regime}, {\\it critically-expressive regime}, and\n{\\it sufficiently-expressive regime}. The source code package is available at\n\\url{https://github.com/LeavesLei/activation-code}.\n", "versions": [{"version": "v1", "created": "Thu, 14 Jan 2021 07:50:40 GMT"}], "update_date": "2021-01-15", "authors_parsed": [["He", "Fengxiang", ""], ["Lei", "Shiye", ""], ["Ji", "Jianmin", ""], ["Tao", "Dacheng", ""]]}, {"id": "2101.05499", "submitter": "Zeyd Boukhers", "authors": "Ipek Baris and Zeyd Boukhers", "title": "ECOL: Early Detection of COVID Lies Using Content, Prior Knowledge and\n  Source Information", "comments": "to be published in Constraint-2021 Workshop @ AAAI", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Social media platforms are vulnerable to fake news dissemination, which\ncauses negative consequences such as panic and wrong medication in the\nhealthcare domain. Therefore, it is important to automatically detect fake news\nin an early stage before they get widely spread. This paper analyzes the impact\nof incorporating content information, prior knowledge, and credibility of\nsources into models for the early detection of fake news. We propose a\nframework modeling those features by using BERT language model and external\nsources, namely Simple English Wikipedia and source reliability tags. The\nconducted experiments on CONSTRAINT datasets demonstrated the benefit of\nintegrating these features for the early detection of fake news in the\nhealthcare domain.\n", "versions": [{"version": "v1", "created": "Thu, 14 Jan 2021 08:39:50 GMT"}], "update_date": "2021-01-15", "authors_parsed": [["Baris", "Ipek", ""], ["Boukhers", "Zeyd", ""]]}, {"id": "2101.05500", "submitter": "Hao Cheng", "authors": "Yanjun Li, Bihan Wen, Hao Cheng and Yoram Bresler", "title": "Joint Dimensionality Reduction for Separable Embedding Estimation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Low-dimensional embeddings for data from disparate sources play critical\nroles in multi-modal machine learning, multimedia information retrieval, and\nbioinformatics. In this paper, we propose a supervised dimensionality reduction\nmethod that learns linear embeddings jointly for two feature vectors\nrepresenting data of different modalities or data from distinct types of\nentities. We also propose an efficient feature selection method that\ncomplements, and can be applied prior to, our joint dimensionality reduction\nmethod. Assuming that there exist true linear embeddings for these features,\nour analysis of the error in the learned linear embeddings provides theoretical\nguarantees that the dimensionality reduction method accurately estimates the\ntrue embeddings when certain technical conditions are satisfied and the number\nof samples is sufficiently large. The derived sample complexity results are\nechoed by numerical experiments. We apply the proposed dimensionality reduction\nmethod to gene-disease association, and predict unknown associations using\nkernel regression on the dimension-reduced feature vectors. Our approach\ncompares favorably against other dimensionality reduction methods, and against\na state-of-the-art method of bilinear regression for predicting gene-disease\nassociations.\n", "versions": [{"version": "v1", "created": "Thu, 14 Jan 2021 08:48:37 GMT"}], "update_date": "2021-01-15", "authors_parsed": [["Li", "Yanjun", ""], ["Wen", "Bihan", ""], ["Cheng", "Hao", ""], ["Bresler", "Yoram", ""]]}, {"id": "2101.05504", "submitter": "Kennedy Edemacu", "authors": "Kennedy Edemacu, Beakcheol Jang, Jong Wook Kim", "title": "Reliability Check via Weight Similarity in Privacy-Preserving\n  Multi-Party Machine Learning", "comments": null, "journal-ref": "Information Sciences, Volume 574, October 2021, Pages 51-65", "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Multi-party machine learning is a paradigm in which multiple participants\ncollaboratively train a machine learning model to achieve a common learning\nobjective without sharing their privately owned data. The paradigm has recently\nreceived a lot of attention from the research community aimed at addressing its\nassociated privacy concerns. In this work, we focus on addressing the concerns\nof data privacy, model privacy, and data quality associated with\nprivacy-preserving multi-party machine learning, i.e., we present a scheme for\nprivacy-preserving collaborative learning that checks the participants' data\nquality while guaranteeing data and model privacy. In particular, we propose a\nnovel metric called weight similarity that is securely computed and used to\ncheck whether a participant can be categorized as a reliable participant (holds\ngood quality data) or not. The problems of model and data privacy are tackled\nby integrating homomorphic encryption in our scheme and uploading encrypted\nweights, which prevent leakages to the server and malicious participants,\nrespectively. The analytical and experimental evaluations of our scheme\ndemonstrate that it is accurate and ensures data and model privacy.\n", "versions": [{"version": "v1", "created": "Thu, 14 Jan 2021 08:55:42 GMT"}], "update_date": "2021-07-26", "authors_parsed": [["Edemacu", "Kennedy", ""], ["Jang", "Beakcheol", ""], ["Kim", "Jong Wook", ""]]}, {"id": "2101.05507", "submitter": "Paul Knott PhD MPhys BSc", "authors": "Paul Knott, Micah Carroll, Sam Devlin, Kamil Ciosek, Katja Hofmann, A.\n  D. Dragan and Rohin Shah", "title": "Evaluating the Robustness of Collaborative Agents", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.HC cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In order for agents trained by deep reinforcement learning to work alongside\nhumans in realistic settings, we will need to ensure that the agents are\n\\emph{robust}. Since the real world is very diverse, and human behavior often\nchanges in response to agent deployment, the agent will likely encounter novel\nsituations that have never been seen during training. This results in an\nevaluation challenge: if we cannot rely on the average training or validation\nreward as a metric, then how can we effectively evaluate robustness? We take\ninspiration from the practice of \\emph{unit testing} in software engineering.\nSpecifically, we suggest that when designing AI agents that collaborate with\nhumans, designers should search for potential edge cases in \\emph{possible\npartner behavior} and \\emph{possible states encountered}, and write tests which\ncheck that the behavior of the agent in these edge cases is reasonable. We\napply this methodology to build a suite of unit tests for the Overcooked-AI\nenvironment, and use this test suite to evaluate three proposals for improving\nrobustness. We find that the test suite provides significant insight into the\neffects of these proposals that were generally not revealed by looking solely\nat the average validation reward.\n", "versions": [{"version": "v1", "created": "Thu, 14 Jan 2021 09:02:45 GMT"}], "update_date": "2021-01-15", "authors_parsed": [["Knott", "Paul", ""], ["Carroll", "Micah", ""], ["Devlin", "Sam", ""], ["Ciosek", "Kamil", ""], ["Hofmann", "Katja", ""], ["Dragan", "A. D.", ""], ["Shah", "Rohin", ""]]}, {"id": "2101.05510", "submitter": "Michael Schaub", "authors": "Michael T. Schaub and Yu Zhu and Jean-Baptiste Seby and T. Mitchell\n  Roddenberry and Santiago Segarra", "title": "Signal Processing on Higher-Order Networks: Livin' on the Edge ... and\n  Beyond", "comments": "41 pages; 8 figures", "journal-ref": "Signal Processing, Volume 187, 2021, 108149, ISSN 0165-1684", "doi": "10.1016/j.sigpro.2021.108149", "report-no": null, "categories": "cs.SI cs.LG physics.soc-ph stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this tutorial, we provide a didactic treatment of the emerging topic of\nsignal processing on higher-order networks. Drawing analogies from discrete and\ngraph signal processing, we introduce the building blocks for processing data\non simplicial complexes and hypergraphs, two common higher-order network\nabstractions that can incorporate polyadic relationships. We provide brief\nintroductions to simplicial complexes and hypergraphs, with a special emphasis\non the concepts needed for the processing of signals supported on these\nstructures. Specifically, we discuss Fourier analysis, signal denoising, signal\ninterpolation, node embeddings, and nonlinear processing through neural\nnetworks, using these two higher-order network models. In the context of\nsimplicial complexes, we specifically focus on signal processing using the\nHodge Laplacian matrix, a multi-relational operator that leverages the special\nstructure of simplicial complexes and generalizes desirable properties of the\nLaplacian matrix in graph signal processing. For hypergraphs, we present both\nmatrix and tensor representations, and discuss the trade-offs in adopting one\nor the other. We also highlight limitations and potential research avenues,\nboth to inform practitioners and to motivate the contribution of new\nresearchers to the area.\n", "versions": [{"version": "v1", "created": "Thu, 14 Jan 2021 09:08:26 GMT"}, {"version": "v2", "created": "Tue, 13 Apr 2021 17:24:25 GMT"}, {"version": "v3", "created": "Fri, 11 Jun 2021 10:16:00 GMT"}], "update_date": "2021-06-14", "authors_parsed": [["Schaub", "Michael T.", ""], ["Zhu", "Yu", ""], ["Seby", "Jean-Baptiste", ""], ["Roddenberry", "T. Mitchell", ""], ["Segarra", "Santiago", ""]]}, {"id": "2101.05514", "submitter": "Riikka Huusari", "authors": "Riikka Huusari, Hachem Kadri", "title": "Entangled Kernels -- Beyond Separability", "comments": null, "journal-ref": "Journal of Machine Learning Research 22 (2021) 1-40", "doi": null, "report-no": null, "categories": "cs.LG quant-ph stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of operator-valued kernel learning and investigate\nthe possibility of going beyond the well-known separable kernels. Borrowing\ntools and concepts from the field of quantum computing, such as partial trace\nand entanglement, we propose a new view on operator-valued kernels and define a\ngeneral family of kernels that encompasses previously known operator-valued\nkernels, including separable and transformable kernels. Within this framework,\nwe introduce another novel class of operator-valued kernels called entangled\nkernels that are not separable. We propose an efficient two-step algorithm for\nthis framework, where the entangled kernel is learned based on a novel\nextension of kernel alignment to operator-valued kernels. We illustrate our\nalgorithm with an application to supervised dimensionality reduction, and\ndemonstrate its effectiveness with both artificial and real data for\nmulti-output regression.\n", "versions": [{"version": "v1", "created": "Thu, 14 Jan 2021 09:18:02 GMT"}], "update_date": "2021-01-18", "authors_parsed": [["Huusari", "Riikka", ""], ["Kadri", "Hachem", ""]]}, {"id": "2101.05519", "submitter": "Zhixian Chen", "authors": "Zhixian Chen, Tengfei Ma, Zhihua Jin, Yangqiu Song, Yang Wang", "title": "BiGCN: A Bi-directional Low-Pass Filtering Graph Neural Network", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Graph convolutional networks have achieved great success on graph-structured\ndata. Many graph convolutional networks can be regarded as low-pass filters for\ngraph signals. In this paper, we propose a new model, BiGCN, which represents a\ngraph neural network as a bi-directional low-pass filter. Specifically, we not\nonly consider the original graph structure information but also the latent\ncorrelation between features, thus BiGCN can filter the signals along with both\nthe original graph and a latent feature-connection graph. Our model outperforms\nprevious graph neural networks in the tasks of node classification and link\nprediction on most of the benchmark datasets, especially when we add noise to\nthe node features.\n", "versions": [{"version": "v1", "created": "Thu, 14 Jan 2021 09:41:00 GMT"}], "update_date": "2021-01-15", "authors_parsed": [["Chen", "Zhixian", ""], ["Ma", "Tengfei", ""], ["Jin", "Zhihua", ""], ["Song", "Yangqiu", ""], ["Wang", "Yang", ""]]}, {"id": "2101.05536", "submitter": "Axel Laborieux", "authors": "Axel Laborieux, Maxence Ernoult, Benjamin Scellier, Yoshua Bengio,\n  Julie Grollier and Damien Querlioz", "title": "Scaling Equilibrium Propagation to Deep ConvNets by Drastically Reducing\n  its Gradient Estimator Bias", "comments": "NeurIPS 2020 Workshop : \"Beyond Backpropagation Novel Ideas for\n  Training Neural Architectures\". arXiv admin note: substantial text overlap\n  with arXiv:2006.03824", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Equilibrium Propagation (EP) is a biologically-inspired counterpart of\nBackpropagation Through Time (BPTT) which, owing to its strong theoretical\nguarantees and the locality in space of its learning rule, fosters the design\nof energy-efficient hardware dedicated to learning. In practice, however, EP\ndoes not scale to visual tasks harder than MNIST. In this work, we show that a\nbias in the gradient estimate of EP, inherent in the use of finite nudging, is\nresponsible for this phenomenon and that cancelling it allows training deep\nConvNets by EP, including architectures with distinct forward and backward\nconnections. These results highlight EP as a scalable approach to compute error\ngradients in deep neural networks, thereby motivating its hardware\nimplementation.\n", "versions": [{"version": "v1", "created": "Thu, 14 Jan 2021 10:23:40 GMT"}], "update_date": "2021-01-15", "authors_parsed": [["Laborieux", "Axel", ""], ["Ernoult", "Maxence", ""], ["Scellier", "Benjamin", ""], ["Bengio", "Yoshua", ""], ["Grollier", "Julie", ""], ["Querlioz", "Damien", ""]]}, {"id": "2101.05537", "submitter": "Stefano Massaroli", "authors": "Stefano Massaroli, Michael Poli, Federico Califano, Jinkyoo Park,\n  Atsushi Yamashita and Hajime Asama", "title": "Optimal Energy Shaping via Neural Approximators", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SY cs.AI cs.LG cs.NE cs.SY math.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce optimal energy shaping as an enhancement of classical\npassivity-based control methods. A promising feature of passivity theory,\nalongside stability, has traditionally been claimed to be intuitive performance\ntuning along the execution of a given task. However, a systematic approach to\nadjust performance within a passive control framework has yet to be developed,\nas each method relies on few and problem-specific practical insights. Here, we\ncast the classic energy-shaping control design process in an optimal control\nframework; once a task-dependent performance metric is defined, an optimal\nsolution is systematically obtained through an iterative procedure relying on\nneural networks and gradient-based optimization. The proposed method is\nvalidated on state-regulation tasks.\n", "versions": [{"version": "v1", "created": "Thu, 14 Jan 2021 10:25:58 GMT"}], "update_date": "2021-01-15", "authors_parsed": [["Massaroli", "Stefano", ""], ["Poli", "Michael", ""], ["Califano", "Federico", ""], ["Park", "Jinkyoo", ""], ["Yamashita", "Atsushi", ""], ["Asama", "Hajime", ""]]}, {"id": "2101.05544", "submitter": "Alexandre Rame", "authors": "Alexandre Rame and Matthieu Cord", "title": "DICE: Diversity in Deep Ensembles via Conditional Redundancy Adversarial\n  Estimation", "comments": "Published as a conference paper at ICLR 2021. 9 main pages, 13\n  figures, 12 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.IT math.IT", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Deep ensembles perform better than a single network thanks to the diversity\namong their members. Recent approaches regularize predictions to increase\ndiversity; however, they also drastically decrease individual members'\nperformances. In this paper, we argue that learning strategies for deep\nensembles need to tackle the trade-off between ensemble diversity and\nindividual accuracies. Motivated by arguments from information theory and\nleveraging recent advances in neural estimation of conditional mutual\ninformation, we introduce a novel training criterion called DICE: it increases\ndiversity by reducing spurious correlations among features. The main idea is\nthat features extracted from pairs of members should only share information\nuseful for target class prediction without being conditionally redundant.\nTherefore, besides the classification loss with information bottleneck, we\nadversarially prevent features from being conditionally predictable from each\nother. We manage to reduce simultaneous errors while protecting class\ninformation. We obtain state-of-the-art accuracy results on CIFAR-10/100: for\nexample, an ensemble of 5 networks trained with DICE matches an ensemble of 7\nnetworks trained independently. We further analyze the consequences on\ncalibration, uncertainty estimation, out-of-distribution detection and online\nco-distillation.\n", "versions": [{"version": "v1", "created": "Thu, 14 Jan 2021 10:53:26 GMT"}], "update_date": "2021-01-15", "authors_parsed": [["Rame", "Alexandre", ""], ["Cord", "Matthieu", ""]]}, {"id": "2101.05546", "submitter": "Sylvia N\\\"urnberg", "authors": "Alexander Denker, Anastasia Steshina, Theresa Grooss, Frank Ueckert,\n  Sylvia N\\\"urnberg", "title": "Feature reduction for machine learning on molecular features: The\n  GeneScore", "comments": "11 pages, 9 figures, 4 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.GN cs.LG", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  We present the GeneScore, a concept of feature reduction for Machine Learning\nanalysis of biomedical data. Using expert knowledge, the GeneScore integrates\ndifferent molecular data types into a single score. We show that the GeneScore\nis superior to a binary matrix in the classification of cancer entities from\nSNV, Indel, CNV, gene fusion and gene expression data. The GeneScore is a\nstraightforward way to facilitate state-of-the-art analysis, while making use\nof the available scientific knowledge on the nature of molecular data features\nused.\n", "versions": [{"version": "v1", "created": "Thu, 14 Jan 2021 10:58:39 GMT"}], "update_date": "2021-01-15", "authors_parsed": [["Denker", "Alexander", ""], ["Steshina", "Anastasia", ""], ["Grooss", "Theresa", ""], ["Ueckert", "Frank", ""], ["N\u00fcrnberg", "Sylvia", ""]]}, {"id": "2101.05555", "submitter": "Stefanos Nikolopoulos", "authors": "Stefanos Nikolopoulos, Ioannis Kalogeris, Vissarion Papadopoulos", "title": "Non-intrusive Surrogate Modeling for Parametrized Time-dependent PDEs\n  using Convolutional Autoencoders", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.NA cs.AI cs.LG cs.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work presents a non-intrusive surrogate modeling scheme based on machine\nlearning technology for predictive modeling of complex systems, described by\nparametrized time-dependent PDEs. For these problems, typical finite element\napproaches involve the spatiotemporal discretization of the PDE and the\nsolution of the corresponding linear system of equations at each time step.\nInstead, the proposed method utilizes a convolutional autoencoder in\nconjunction with a feed forward neural network to establish a low-cost and\naccurate mapping from the problem's parametric space to its solution space. For\nthis purpose, time history response data are collected by solving the\nhigh-fidelity model via FEM for a reduced set of parameter values. Then, by\napplying the convolutional autoencoder to this data set, a low-dimensional\nrepresentation of the high-dimensional solution matrices is provided by the\nencoder, while the reconstruction map is obtained by the decoder. Using the\nlatent representation given by the encoder, a feed-forward neural network is\nefficiently trained to map points from the problem's parametric space to the\ncompressed version of the respective solution matrices. This way, the encoded\nresponse of the system at new parameter values is given by the neural network,\nwhile the entire response is delivered by the decoder. This approach\neffectively bypasses the need to serially formulate and solve the system's\ngoverning equations at each time increment, thus resulting in a significant\ncost reduction and rendering the method ideal for problems requiring repeated\nmodel evaluations or 'real-time' computations. The elaborated methodology is\ndemonstrated on the stochastic analysis of time-dependent PDEs solved with the\nMonte Carlo method, however, it can be straightforwardly applied to other\nsimilar-type problems, such as sensitivity analysis, design optimization, etc.\n", "versions": [{"version": "v1", "created": "Thu, 14 Jan 2021 11:34:58 GMT"}, {"version": "v2", "created": "Fri, 23 Apr 2021 09:53:13 GMT"}], "update_date": "2021-04-26", "authors_parsed": [["Nikolopoulos", "Stefanos", ""], ["Kalogeris", "Ioannis", ""], ["Papadopoulos", "Vissarion", ""]]}, {"id": "2101.05564", "submitter": "M. F. Mridha", "authors": "Abu Quwsar Ohi, M. F. Mridha, Md. Abdul Hamid, Muhammad Mostafa\n  Monowar, Faris A Kateb", "title": "FabricNet: A Fiber Recognition Architecture Using Ensemble ConvNets", "comments": "Accepted in IEEE Access", "journal-ref": null, "doi": "10.1109/ACCESS.2021.3051980", "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fabric is a planar material composed of textile fibers. Textile fibers are\ngenerated from many natural sources; including plants, animals, minerals, and\neven, it can be synthetic. A particular fabric may contain different types of\nfibers that pass through a complex production process. Fiber identification is\nusually carried out through chemical tests and microscopic tests. However,\nthese testing processes are complicated as well as time-consuming. We propose\nFabricNet, a pioneering approach for the image-based textile fiber recognition\nsystem, which may have a revolutionary impact from individual to the industrial\nfiber recognition process. The FabricNet can recognize a large scale of fibers\nby only utilizing a surface image of fabric. The recognition system is\nconstructed using a distinct category of class-based ensemble convolutional\nneural network (CNN) architecture. The experiment is conducted on recognizing\n50 different types of textile fibers. This experiment includes a significantly\nlarge number of unique textile fibers than previous research endeavors to the\nbest of our knowledge. We experiment with popular CNN architectures that\ninclude Inception, ResNet, VGG, MobileNet, DenseNet, and Xception. Finally, the\nexperimental results demonstrate that FabricNet outperforms the\nstate-of-the-art popular CNN architectures by reaching an accuracy of 84% and\nF1-score of 90%.\n", "versions": [{"version": "v1", "created": "Thu, 14 Jan 2021 12:11:23 GMT"}], "update_date": "2021-01-15", "authors_parsed": [["Ohi", "Abu Quwsar", ""], ["Mridha", "M. F.", ""], ["Hamid", "Md. Abdul", ""], ["Monowar", "Muhammad Mostafa", ""], ["Kateb", "Faris A", ""]]}, {"id": "2101.05593", "submitter": "Renato Stoffalette Joao", "authors": "Renato Stoffalette Joao", "title": "On the Temporality of Priors in Entity Linking", "comments": null, "journal-ref": "2020 European Conference on Information Retrieval", "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Entity linking is a fundamental task in natural language processing which\ndeals with the lexical ambiguity in texts. An important component in entity\nlinking approaches is the mention-to-entity prior probability. Even though\nthere is a large number of works in entity linking, the existing approaches do\nnot explicitly consider the time aspect, specifically the temporality of an\nentity's prior probability. We posit that this prior probability is temporal in\nnature and affects the performance of entity linking systems. In this paper we\nsystematically study the effect of the prior on the entity linking performance\nover the temporal validity of both texts and KBs.\n", "versions": [{"version": "v1", "created": "Thu, 14 Jan 2021 13:58:31 GMT"}], "update_date": "2021-01-15", "authors_parsed": [["Joao", "Renato Stoffalette", ""]]}, {"id": "2101.05605", "submitter": "Rui Liu", "authors": "Rui Liu and Sen Liu and Xiaoli Zhang", "title": "A Physics-Informed Machine Learning Model for Porosity Analysis in Laser\n  Powder Bed Fusion Additive Manufacturing", "comments": "14 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To control part quality, it is critical to analyze pore generation\nmechanisms, laying theoretical foundation for future porosity control. Current\nporosity analysis models use machine setting parameters, such as laser angle\nand part pose. However, these setting-based models are machine dependent, hence\nthey often do not transfer to analysis of porosity for a different machine. To\naddress the first problem, a physics-informed, data-driven model (PIM), which\ninstead of directly using machine setting parameters to predict porosity levels\nof printed parts, it first interprets machine settings into physical effects,\nsuch as laser energy density and laser radiation pressure. Then, these\nphysical, machine independent effects are used to predict porosity levels\naccording to pass, flag, fail categories instead of focusing on quantitative\npore size prediction. With six learning methods evaluation, PIM proved to\nachieve good performances with prediction error of 10$\\sim$26%. Finally,\npore-encouraging influence and pore-suppressing influence were analyzed for\nquality analysis.\n", "versions": [{"version": "v1", "created": "Wed, 13 Jan 2021 01:29:01 GMT"}], "update_date": "2021-01-15", "authors_parsed": [["Liu", "Rui", ""], ["Liu", "Sen", ""], ["Zhang", "Xiaoli", ""]]}, {"id": "2101.05608", "submitter": "Lasitha Vidyaratne", "authors": "Lasitha Vidyaratne, Mahbubul Alam, Alexander Glandon, Anna Shabalina,\n  Christopher Tennant, and Khan Iftekharuddin", "title": "Deep Cellular Recurrent Network for Efficient Analysis of Time-Series\n  Data with Spatial Information", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV eess.SP", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Efficient processing of large-scale time series data is an intricate problem\nin machine learning. Conventional sensor signal processing pipelines with hand\nengineered feature extraction often involve huge computational cost with high\ndimensional data. Deep recurrent neural networks have shown promise in\nautomated feature learning for improved time-series processing. However,\ngeneric deep recurrent models grow in scale and depth with increased complexity\nof the data. This is particularly challenging in presence of high dimensional\ndata with temporal and spatial characteristics. Consequently, this work\nproposes a novel deep cellular recurrent neural network (DCRNN) architecture to\nefficiently process complex multi-dimensional time series data with spatial\ninformation. The cellular recurrent architecture in the proposed model allows\nfor location-aware synchronous processing of time series data from spatially\ndistributed sensor signal sources. Extensive trainable parameter sharing due to\ncellularity in the proposed architecture ensures efficiency in the use of\nrecurrent processing units with high-dimensional inputs. This study also\ninvestigates the versatility of the proposed DCRNN model for classification of\nmulti-class time series data from different application domains. Consequently,\nthe proposed DCRNN architecture is evaluated using two time-series datasets: a\nmultichannel scalp EEG dataset for seizure detection, and a machine fault\ndetection dataset obtained in-house. The results suggest that the proposed\narchitecture achieves state-of-the-art performance while utilizing\nsubstantially less trainable parameters when compared to comparable methods in\nthe literature.\n", "versions": [{"version": "v1", "created": "Tue, 12 Jan 2021 20:08:18 GMT"}], "update_date": "2021-01-15", "authors_parsed": [["Vidyaratne", "Lasitha", ""], ["Alam", "Mahbubul", ""], ["Glandon", "Alexander", ""], ["Shabalina", "Anna", ""], ["Tennant", "Christopher", ""], ["Iftekharuddin", "Khan", ""]]}, {"id": "2101.05611", "submitter": "Guangneng Hu", "authors": "Guangneng Hu, Qiang Yang", "title": "TrNews: Heterogeneous User-Interest Transfer Learning for News\n  Recommendation", "comments": "EACL 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.AI cs.CL cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We investigate how to solve the cross-corpus news recommendation for unseen\nusers in the future. This is a problem where traditional content-based\nrecommendation techniques often fail. Luckily, in real-world recommendation\nservices, some publisher (e.g., Daily news) may have accumulated a large corpus\nwith lots of consumers which can be used for a newly deployed publisher (e.g.,\nPolitical news). To take advantage of the existing corpus, we propose a\ntransfer learning model (dubbed as TrNews) for news recommendation to transfer\nthe knowledge from a source corpus to a target corpus. To tackle the\nheterogeneity of different user interests and of different word distributions\nacross corpora, we design a translator-based transfer-learning strategy to\nlearn a representation mapping between source and target corpora. The learned\ntranslator can be used to generate representations for unseen users in the\nfuture. We show through experiments on real-world datasets that TrNews is\nbetter than various baselines in terms of four metrics. We also show that our\ntranslator is effective among existing transfer strategies.\n", "versions": [{"version": "v1", "created": "Tue, 12 Jan 2021 13:52:53 GMT"}, {"version": "v2", "created": "Wed, 27 Jan 2021 06:31:04 GMT"}], "update_date": "2021-01-28", "authors_parsed": [["Hu", "Guangneng", ""], ["Yang", "Qiang", ""]]}, {"id": "2101.05612", "submitter": "Shaosheng Xu", "authors": "Shaosheng Xu, Jinde Cao, Yichao Cao, Tong Wang", "title": "A SOM-based Gradient-Free Deep Learning Method with Convergence Analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  As gradient descent method in deep learning causes a series of questions,\nthis paper proposes a novel gradient-free deep learning structure. By adding a\nnew module into traditional Self-Organizing Map and introducing residual into\nthe map, a Deep Valued Self-Organizing Map network is constructed. And analysis\nabout the convergence performance of such a deep Valued Self-Organizing Map\nnetwork is proved in this paper, which gives an inequality about the designed\nparameters with the dimension of inputs and the loss of prediction.\n", "versions": [{"version": "v1", "created": "Tue, 12 Jan 2021 09:09:42 GMT"}], "update_date": "2021-01-15", "authors_parsed": [["Xu", "Shaosheng", ""], ["Cao", "Jinde", ""], ["Cao", "Yichao", ""], ["Wang", "Tong", ""]]}, {"id": "2101.05615", "submitter": "Daya Khudia", "authors": "Daya Khudia, Jianyu Huang, Protonu Basu, Summer Deng, Haixin Liu,\n  Jongsoo Park, Mikhail Smelyanskiy", "title": "FBGEMM: Enabling High-Performance Low-Precision Deep Learning Inference", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning models typically use single-precision (FP32) floating point\ndata types for representing activations and weights, but a slew of recent\nresearch work has shown that computations with reduced-precision data types\n(FP16, 16-bit integers, 8-bit integers or even 4- or 2-bit integers) are enough\nto achieve same accuracy as FP32 and are much more efficient. Therefore, we\ndesigned fbgemm, a high-performance kernel library, from ground up to perform\nhigh-performance quantized inference on current generation CPUs. fbgemm\nachieves efficiency by fusing common quantization operations with a\nhigh-performance gemm implementation and by shape- and size-specific kernel\ncode generation at runtime. The library has been deployed at Facebook, where it\ndelivers greater than 2x performance gains with respect to our current\nproduction baseline.\n", "versions": [{"version": "v1", "created": "Wed, 13 Jan 2021 00:34:04 GMT"}], "update_date": "2021-01-15", "authors_parsed": [["Khudia", "Daya", ""], ["Huang", "Jianyu", ""], ["Basu", "Protonu", ""], ["Deng", "Summer", ""], ["Liu", "Haixin", ""], ["Park", "Jongsoo", ""], ["Smelyanskiy", "Mikhail", ""]]}, {"id": "2101.05620", "submitter": "Yan Jia", "authors": "Yan Jia, Tom Lawton, John McDermid, Eric Rojas, Ibrahim Habli", "title": "A Framework for Assurance of Medication Safety using Machine Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CY", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Medication errors continue to be the leading cause of avoidable patient harm\nin hospitals. This paper sets out a framework to assure medication safety that\ncombines machine learning and safety engineering methods. It uses safety\nanalysis to proactively identify potential causes of medication error, based on\nexpert opinion. As healthcare is now data rich, it is possible to augment\nsafety analysis with machine learning to discover actual causes of medication\nerror from the data, and to identify where they deviate from what was predicted\nin the safety analysis. Combining these two views has the potential to enable\nthe risk of medication errors to be managed proactively and dynamically. We\napply the framework to a case study involving thoracic surgery, e.g.\noesophagectomy, where errors in giving beta-blockers can be critical to control\natrial fibrillation. This case study combines a HAZOP-based safety analysis\nmethod known as SHARD with Bayesian network structure learning and process\nmining to produce the analysis results, showing the potential of the framework\nfor ensuring patient safety, and for transforming the way that safety is\nmanaged in complex healthcare environments.\n", "versions": [{"version": "v1", "created": "Mon, 11 Jan 2021 12:32:15 GMT"}], "update_date": "2021-01-15", "authors_parsed": [["Jia", "Yan", ""], ["Lawton", "Tom", ""], ["McDermid", "John", ""], ["Rojas", "Eric", ""], ["Habli", "Ibrahim", ""]]}, {"id": "2101.05623", "submitter": "David Pardo", "authors": "M. Shahriari, A. Hazra, D. Pardo", "title": "Design of borehole resistivity measurement acquisition systems using\n  deep learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NA eess.SP math.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Borehole resistivity measurements recorded with logging-while-drilling (LWD)\ninstruments are widely used for characterizing the earth's subsurface\nproperties. They facilitate the extraction of natural resources such as oil and\ngas. LWD instruments require real-time inversions of electromagnetic\nmeasurements to estimate the electrical properties of the earth's subsurface\nnear the well and possibly correct the well trajectory. Deep Neural Network\n(DNN)-based methods are suitable for the rapid inversion of borehole\nresistivity measurements as they approximate the forward and inverse problem\noffline during the training phase and they only require a fraction of a second\nfor the evaluation (aka prediction). However, the inverse problem generally\nadmits multiple solutions. DNNs with traditional loss functions based on data\nmisfit are ill-equipped for solving an inverse problem. This can be partially\novercome by adding regularization terms to a loss function specifically\ndesigned for encoder-decoder architectures. But adding regularization seriously\nlimits the number of possible solutions to a set of a priori desirable physical\nsolutions. To avoid this, we use a two-step loss function without any\nregularization. In addition, to guarantee an inverse solution, we need a\ncarefully selected measurement acquisition system with a sufficient number of\nmeasurements. In this work, we propose a DNN-based iterative algorithm for\ndesigning such a measurement acquisition system. We illustrate our DNN-based\niterative algorithm via several synthetic examples. Numerical results show that\nthe obtained measurement acquisition system is sufficient to identify and\ncharacterize both resistive and conductive layers above and below the logging\ninstrument. Numerical results are promising, although further improvements are\nrequired to make our method amenable for industrial purposes.\n", "versions": [{"version": "v1", "created": "Tue, 12 Jan 2021 12:49:44 GMT"}], "update_date": "2021-01-15", "authors_parsed": [["Shahriari", "M.", ""], ["Hazra", "A.", ""], ["Pardo", "D.", ""]]}, {"id": "2101.05624", "submitter": "Yao Qiang", "authors": "Yao Qiang, Supriya Tumkur Suresh Kumar, Marco Brocanelli and Dongxiao\n  Zhu", "title": "Adversarially Robust and Explainable Model Compression with On-Device\n  Personalization for Text Classification", "comments": "8 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  On-device Deep Neural Networks (DNNs) have recently gained more attention due\nto the increasing computing power of the mobile devices and the number of\napplications in Computer Vision (CV), Natural Language Processing (NLP), and\nInternet of Things (IoTs). Unfortunately, the existing efficient convolutional\nneural network (CNN) architectures designed for CV tasks are not directly\napplicable to NLP tasks and the tiny Recurrent Neural Network (RNN)\narchitectures have been designed primarily for IoT applications. In NLP\napplications, although model compression has seen initial success in on-device\ntext classification, there are at least three major challenges yet to be\naddressed: adversarial robustness, explainability, and personalization. Here we\nattempt to tackle these challenges by designing a new training scheme for model\ncompression and adversarial robustness, including the optimization of an\nexplainable feature mapping objective, a knowledge distillation objective, and\nan adversarially robustness objective. The resulting compressed model is\npersonalized using on-device private training data via fine-tuning. We perform\nextensive experiments to compare our approach with both compact RNN (e.g.,\nFastGRNN) and compressed RNN (e.g., PRADO) architectures in both natural and\nadversarial NLP test settings.\n", "versions": [{"version": "v1", "created": "Sun, 10 Jan 2021 15:06:55 GMT"}, {"version": "v2", "created": "Fri, 15 Jan 2021 04:38:50 GMT"}, {"version": "v3", "created": "Wed, 20 Jan 2021 13:18:25 GMT"}], "update_date": "2021-01-21", "authors_parsed": [["Qiang", "Yao", ""], ["Kumar", "Supriya Tumkur Suresh", ""], ["Brocanelli", "Marco", ""], ["Zhu", "Dongxiao", ""]]}, {"id": "2101.05625", "submitter": "Shalini Pandey", "authors": "Shalini Pandey, Andrew Lan, George Karypis, Jaideep Srivastava", "title": "Learning Student Interest Trajectory for MOOCThread Recommendation", "comments": "Accepted at IEEE ICDM Workshop on Continual Learning and Adaptation\n  for Time Evolving Data, 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CY cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, Massive Open Online Courses (MOOCs) have witnessed immense\ngrowth in popularity. Now, due to the recent Covid19 pandemic situation, it is\nimportant to push the limits of online education. Discussion forums are primary\nmeans of interaction among learners and instructors. However, with growing\nclass size, students face the challenge of finding useful and informative\ndiscussion forums. This problem can be solved by matching the interest of\nstudents with thread contents. The fundamental challenge is that the student\ninterests drift as they progress through the course, and forum contents evolve\nas students or instructors update them. In our paper, we propose to predict\nfuture interest trajectories of students. Our model consists of two key\noperations: 1) Update operation and 2) Projection operation. Update operation\nmodels the inter-dependency between the evolution of student and thread using\ncoupled Recurrent Neural Networks when the student posts on the thread. The\nprojection operation learns to estimate future embedding of students and\nthreads. For students, the projection operation learns the drift in their\ninterests caused by the change in the course topic they study. The projection\noperation for threads exploits how different posts induce varying interest\nlevels in a student according to the thread structure. Extensive\nexperimentation on three real-world MOOC datasets shows that our model\nsignificantly outperforms other baselines for thread recommendation.\n", "versions": [{"version": "v1", "created": "Sun, 10 Jan 2021 10:23:11 GMT"}, {"version": "v2", "created": "Sat, 16 Jan 2021 05:01:58 GMT"}], "update_date": "2021-01-19", "authors_parsed": [["Pandey", "Shalini", ""], ["Lan", "Andrew", ""], ["Karypis", "George", ""], ["Srivastava", "Jaideep", ""]]}, {"id": "2101.05626", "submitter": "Eisa Alanazi", "authors": "Sarah Alqurashi, Btool Hamoui, Abdulaziz Alashaikh, Ahmad Alhindi,\n  Eisa Alanazi", "title": "Eating Garlic Prevents COVID-19 Infection: Detecting Misinformation on\n  the Arabic Content of Twitter", "comments": "18 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CY cs.LG cs.SI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The rapid growth of social media content during the current pandemic provides\nuseful tools for disseminating information which has also become a root for\nmisinformation. Therefore, there is an urgent need for fact-checking and\neffective techniques for detecting misinformation in social media. In this\nwork, we study the misinformation in the Arabic content of Twitter. We\nconstruct a large Arabic dataset related to COVID-19 misinformation and\ngold-annotate the tweets into two categories: misinformation or not. Then, we\napply eight different traditional and deep machine learning models, with\ndifferent features including word embeddings and word frequency. The word\nembedding models (\\textsc{FastText} and word2vec) exploit more than two million\nArabic tweets related to COVID-19. Experiments show that optimizing the area\nunder the curve (AUC) improves the models' performance and the Extreme Gradient\nBoosting (XGBoost) presents the highest accuracy in detecting COVID-19\nmisinformation online.\n", "versions": [{"version": "v1", "created": "Sat, 9 Jan 2021 22:52:21 GMT"}], "update_date": "2021-01-15", "authors_parsed": [["Alqurashi", "Sarah", ""], ["Hamoui", "Btool", ""], ["Alashaikh", "Abdulaziz", ""], ["Alhindi", "Ahmad", ""], ["Alanazi", "Eisa", ""]]}, {"id": "2101.05631", "submitter": "Mohamad Alissa", "authors": "Mohamad Alissa", "title": "Parkinson's Disease Diagnosis Using Deep Learning", "comments": "Master Research Project", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG eess.IV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Parkinson's Disease (PD) is a chronic, degenerative disorder which leads to a\nrange of motor and cognitive symptoms. PD diagnosis is a challenging task since\nits symptoms are very similar to other diseases such as normal ageing and\nessential tremor. Much research has been applied to diagnosing this disease.\nThis project aims to automate the PD diagnosis process using deep learning,\nRecursive Neural Networks (RNN) and Convolutional Neural Networks (CNN), to\ndifferentiate between healthy and PD patients. Besides that, since different\ndatasets may capture different aspects of this disease, this project aims to\nexplore which PD test is more effective in the discrimination process by\nanalysing different imaging and movement datasets (notably cube and spiral\npentagon datasets). In addition, this project evaluates which dataset type,\nimaging or time series, is more effective in diagnosing PD.\n", "versions": [{"version": "v1", "created": "Sun, 3 Jan 2021 18:39:25 GMT"}], "update_date": "2021-01-15", "authors_parsed": [["Alissa", "Mohamad", ""]]}, {"id": "2101.05633", "submitter": "Byungryul Choi", "authors": "Byungryul Choi", "title": "Enhanced Audit Techniques Empowered by the Reinforcement Learning\n  Pertaining to IFRS 16 Lease", "comments": "for codes, please refer to\n  https://github.com/BryanBYChoi/Reinforcement_Learning_IFRS16_Lease", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The purpose of accounting audit is to have clear understanding on the\nfinancial activities of a company, which can be enhanced by machine learning or\nreinforcement learning as numeric analysis better than manual analysis can be\nmade. For the purpose of assessment on the relevance, completeness and accuracy\nof the information produced by entity pertaining to the newly implemented\nInternational Financial Reporting Standard 16 Lease (IFRS 16) is one of such\ncandidates as its characteristic of requiring the understanding on the nature\nof contracts and its complete analysis from listing up without omission, which\ncan be enhanced by the digitalization of contracts for the purpose of creating\nthe lists, still leaving the need of auditing cash flows of companies for the\npossible omission due to the potential error at the stage of data collection,\nespecially for entities with various short or middle term business sites and\nrelated leases, such as construction entities.\n  The implementation of the reinforcement learning and its well-known code is\nto be made for the purpose of drawing the possibility and utilizability of\ninterpreters from domain knowledge to numerical system, also can be called\n'gamification interpreter' or 'numericalization interpreter' which can be\nreferred or compared to the extrapolation with nondimensional numbers, such as\nFroude Number, in physics, which was a source of inspiration at this study.\nStudies on the interpreters can be able to empower the utilizability of\nartificial general intelligence in domain and commercial area.\n", "versions": [{"version": "v1", "created": "Tue, 5 Jan 2021 14:44:37 GMT"}], "update_date": "2021-01-15", "authors_parsed": [["Choi", "Byungryul", ""]]}, {"id": "2101.05634", "submitter": "Renato Stoffalette Joao", "authors": "Renato Stoffalette Jo\\~ao and Pavlos Fafalios and Stefan Dietze", "title": "Better Together -- An Ensemble Learner for Combining the Results of\n  Ready-made Entity Linking Systems", "comments": "SAC '20: Proceedings of the 35th Annual ACM Symposium on Applied\n  Computing", "journal-ref": null, "doi": "10.1145/3341105.3373883", "report-no": null, "categories": "cs.CL cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Entity linking (EL) is the task of automatically identifying entity mentions\nin text and resolving them to a corresponding entity in a reference knowledge\nbase like Wikipedia. Throughout the past decade, a plethora of EL systems and\npipelines have become available, where performance of individual systems varies\nheavily across corpora, languages or domains. Linking performance varies even\nbetween different mentions in the same text corpus, where, for instance, some\nEL approaches are better able to deal with short surface forms while others may\nperform better when more context information is available. To this end, we\nargue that performance may be optimised by exploiting results from distinct EL\nsystems on the same corpus, thereby leveraging their individual strengths on a\nper-mention basis. In this paper, we introduce a supervised approach which\nexploits the output of multiple ready-made EL systems by predicting the correct\nlink on a per-mention basis. Experimental results obtained on existing ground\ntruth datasets and exploiting three state-of-the-art EL systems show the\neffectiveness of our approach and its capacity to significantly outperform the\nindividual EL systems as well as a set of baseline methods.\n", "versions": [{"version": "v1", "created": "Thu, 14 Jan 2021 14:42:57 GMT"}], "update_date": "2021-01-15", "authors_parsed": [["Jo\u00e3o", "Renato Stoffalette", ""], ["Fafalios", "Pavlos", ""], ["Dietze", "Stefan", ""]]}, {"id": "2101.05639", "submitter": "Pradeep Rathore", "authors": "Pradeep Rathore, Arghya Basak, Sri Harsha Nistala, Venkataramana\n  Runkana", "title": "Untargeted, Targeted and Universal Adversarial Attacks and Defenses on\n  Time Series", "comments": "Published at IJCNN 2020", "journal-ref": null, "doi": "10.1109/IJCNN48605.2020.9207272", "report-no": null, "categories": "cs.LG", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Deep learning based models are vulnerable to adversarial attacks. These\nattacks can be much more harmful in case of targeted attacks, where an attacker\ntries not only to fool the deep learning model, but also to misguide the model\nto predict a specific class. Such targeted and untargeted attacks are\nspecifically tailored for an individual sample and require addition of an\nimperceptible noise to the sample. In contrast, universal adversarial attack\ncalculates a special imperceptible noise which can be added to any sample of\nthe given dataset so that, the deep learning model is forced to predict a wrong\nclass. To the best of our knowledge these targeted and universal attacks on\ntime series data have not been studied in any of the previous works. In this\nwork, we have performed untargeted, targeted and universal adversarial attacks\non UCR time series datasets. Our results show that deep learning based time\nseries classification models are vulnerable to these attacks. We also show that\nuniversal adversarial attacks have good generalization property as it need only\na fraction of the training data. We have also performed adversarial training\nbased adversarial defense. Our results show that models trained adversarially\nusing Fast gradient sign method (FGSM), a single step attack, are able to\ndefend against FGSM as well as Basic iterative method (BIM), a popular\niterative attack.\n", "versions": [{"version": "v1", "created": "Wed, 13 Jan 2021 13:00:51 GMT"}], "update_date": "2021-01-15", "authors_parsed": [["Rathore", "Pradeep", ""], ["Basak", "Arghya", ""], ["Nistala", "Sri Harsha", ""], ["Runkana", "Venkataramana", ""]]}, {"id": "2101.05640", "submitter": "Junya Ikemoto", "authors": "Junya Ikemoto and Toshimitsu Ushio", "title": "Continuous Deep Q-Learning with Simulator for Stabilization of Uncertain\n  Discrete-Time Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.SY eess.SY stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Applications of reinforcement learning (RL) to stabilization problems of real\nsystems are restricted since an agent needs many experiences to learn an\noptimal policy and may determine dangerous actions during its exploration. If\nwe know a mathematical model of a real system, a simulator is useful because it\npredicates behaviors of the real system using the mathematical model with a\ngiven system parameter vector. We can collect many experiences more efficiently\nthan interactions with the real system. However, it is difficult to identify\nthe system parameter vector accurately. If we have an identification error,\nexperiences obtained by the simulator may degrade the performance of the\nlearned policy. Thus, we propose a practical RL algorithm that consists of two\nstages. At the first stage, we choose multiple system parameter vectors. Then,\nwe have a mathematical model for each system parameter vector, which is called\na virtual system. We obtain optimal Q-functions for multiple virtual systems\nusing the continuous deep Q-learning algorithm. At the second stage, we\nrepresent a Q-function for the real system by a linear approximated function\nwhose basis functions are optimal Q-functions learned at the first stage. The\nagent learns the Q-function through interactions with the real system online.\nBy numerical simulations, we show the usefulness of our proposed method.\n", "versions": [{"version": "v1", "created": "Wed, 13 Jan 2021 10:21:12 GMT"}, {"version": "v2", "created": "Mon, 19 Apr 2021 04:08:45 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Ikemoto", "Junya", ""], ["Ushio", "Toshimitsu", ""]]}, {"id": "2101.05641", "submitter": "Jialiang Han", "authors": "Jialiang Han, Yun Ma", "title": "$C^3DRec$: Cloud-Client Cooperative Deep Learning for Temporal\n  Recommendation in the Post-GDPR Era", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mobile devices enable users to retrieve information at any time and any\nplace. Considering the occasional requirements and fragmentation usage pattern\nof mobile users, temporal recommendation techniques are proposed to improve the\nefficiency of information retrieval on mobile devices by means of accurately\nrecommending items via learning temporal interests with short-term user\ninteraction behaviors. However, the enforcement of privacy-preserving laws and\nregulations, such as GDPR, may overshadow the successful practice of temporal\nrecommendation. The reason is that state-of-the-art recommendation systems\nrequire to gather and process the user data in centralized servers but the\ninteraction behaviors data used for temporal recommendation are usually\nnon-transactional data that are not allowed to gather without the explicit\npermission of users according to GDPR. As a result, if users do not permit\nservices to gather their interaction behaviors data, the temporal\nrecommendation fails to work. To realize the temporal recommendation in the\npost-GDPR era, this paper proposes $C^3DRec$, a cloud-client cooperative deep\nlearning framework of mining interaction behaviors for recommendation while\npreserving user privacy. $C^3DRec$ constructs a global recommendation model on\ncentralized servers using data collected before GDPR and fine-tunes the model\ndirectly on individual local devices using data collected after GDPR. We design\ntwo modes to accomplish the recommendation, i.e. pull mode where candidate\nitems are pulled down onto the devices and fed into the local model to get\nrecommended items, and push mode where the output of the local model is pushed\nonto the server and combined with candidate items to get recommended ones.\nEvaluation results show that $C^3DRec$ achieves comparable recommendation\naccuracy to the centralized approaches, with minimal privacy concern.\n", "versions": [{"version": "v1", "created": "Wed, 13 Jan 2021 12:49:34 GMT"}], "update_date": "2021-01-15", "authors_parsed": [["Han", "Jialiang", ""], ["Ma", "Yun", ""]]}, {"id": "2101.05646", "submitter": "Cengiz Acart\\\"urk", "authors": "Cengiz Acarturk, Melih Sirlanci, Pinar Gurkan Balikcioglu, Deniz\n  Demirci, Nazenin Sahin, Ozge Acar Kucuk", "title": "Malicious Code Detection: Run Trace Output Analysis by LSTM", "comments": "11 pages, 5 figures, 5 tables, accepted to IEEE Access", "journal-ref": null, "doi": "10.1109/ACCESS.2021.3049200", "report-no": null, "categories": "cs.CR cs.CL cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Malicious software threats and their detection have been gaining importance\nas a subdomain of information security due to the expansion of ICT applications\nin daily settings. A major challenge in designing and developing anti-malware\nsystems is the coverage of the detection, particularly the development of\ndynamic analysis methods that can detect polymorphic and metamorphic malware\nefficiently. In the present study, we propose a methodological framework for\ndetecting malicious code by analyzing run trace outputs by Long Short-Term\nMemory (LSTM). We developed models of run traces of malicious and benign\nPortable Executable (PE) files. We created our dataset from run trace outputs\nobtained from dynamic analysis of PE files. The obtained dataset was in the\ninstruction format as a sequence and was called Instruction as a Sequence Model\n(ISM). By splitting the first dataset into basic blocks, we obtained the second\none called Basic Block as a Sequence Model (BSM). The experiments showed that\nthe ISM achieved an accuracy of 87.51% and a false positive rate of 18.34%,\nwhile BSM achieved an accuracy of 99.26% and a false positive rate of 2.62%.\n", "versions": [{"version": "v1", "created": "Thu, 14 Jan 2021 15:00:42 GMT"}], "update_date": "2021-01-15", "authors_parsed": [["Acarturk", "Cengiz", ""], ["Sirlanci", "Melih", ""], ["Balikcioglu", "Pinar Gurkan", ""], ["Demirci", "Deniz", ""], ["Sahin", "Nazenin", ""], ["Kucuk", "Ozge Acar", ""]]}, {"id": "2101.05652", "submitter": "Gustavo de Rosa", "authors": "Gustavo H. de Rosa, Jo\\~ao Paulo Papa, Xin-She Yang", "title": "A Nature-Inspired Feature Selection Approach based on Hypercomplex\n  Information", "comments": "17 pages, 7 figures", "journal-ref": "APPLIED SOFT COMPUTING; v. 94, SEP 2020", "doi": "10.1016/j.asoc.2020.106453", "report-no": null, "categories": "cs.NE cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Feature selection for a given model can be transformed into an optimization\ntask. The essential idea behind it is to find the most suitable subset of\nfeatures according to some criterion. Nature-inspired optimization can mitigate\nthis problem by producing compelling yet straightforward solutions when dealing\nwith complicated fitness functions. Additionally, new mathematical\nrepresentations, such as quaternions and octonions, are being used to handle\nhigher-dimensional spaces. In this context, we are introducing a meta-heuristic\noptimization framework in a hypercomplex-based feature selection, where\nhypercomplex numbers are mapped to real-valued solutions and then transferred\nonto a boolean hypercube by a sigmoid function. The intended hypercomplex\nfeature selection is tested for several meta-heuristic algorithms and\nhypercomplex representations, achieving results comparable to some\nstate-of-the-art approaches. The good results achieved by the proposed approach\nmake it a promising tool amongst feature selection research.\n", "versions": [{"version": "v1", "created": "Thu, 14 Jan 2021 15:05:13 GMT"}], "update_date": "2021-01-15", "authors_parsed": [["de Rosa", "Gustavo H.", ""], ["Papa", "Jo\u00e3o Paulo", ""], ["Yang", "Xin-She", ""]]}, {"id": "2101.05656", "submitter": "Renato Stoffalette Joao", "authors": "Renato Stoffalette Jo\\~ao", "title": "On Informative Tweet Identification For Tracking Mass Events", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Twitter has been heavily used as an important channel for communicating and\ndiscussing about events in real-time. In such major events, many uninformative\ntweets are also published rapidly by many users, making it hard to follow the\nevents. In this paper, we address this problem by investigating machine\nlearning methods for automatically identifying informative tweets among those\nthat are relevant to a target event. We examine both traditional approaches\nwith a rich set of handcrafted features and state of the art approaches with\nautomatically learned features. We further propose a hybrid model that\nleverages both the handcrafted features and the automatically learned ones. Our\nexperiments on several large datasets of real-world events show that the latter\napproaches significantly outperform the former and our proposed model performs\nthe best, suggesting highly effective mechanisms for tracking mass events.\n", "versions": [{"version": "v1", "created": "Thu, 14 Jan 2021 15:10:42 GMT"}], "update_date": "2021-01-15", "authors_parsed": [["Jo\u00e3o", "Renato Stoffalette", ""]]}, {"id": "2101.05657", "submitter": "Ankur Moitra", "authors": "Linus Hamilton, Ankur Moitra", "title": "No-go Theorem for Acceleration in the Hyperbolic Plane", "comments": "12 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.DS cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years there has been significant effort to adapt the key tools and\nideas in convex optimization to the Riemannian setting. One key challenge has\nremained: Is there a Nesterov-like accelerated gradient method for geodesically\nconvex functions on a Riemannian manifold? Recent work has given partial\nanswers and the hope was that this ought to be possible.\n  Here we dash these hopes. We prove that in a noisy setting, there is no\nanalogue of accelerated gradient descent for geodesically convex functions on\nthe hyperbolic plane. Our results apply even when the noise is exponentially\nsmall. The key intuition behind our proof is short and simple: In negatively\ncurved spaces, the volume of a ball grows so fast that information about the\npast gradients is not useful in the future.\n", "versions": [{"version": "v1", "created": "Thu, 14 Jan 2021 15:11:24 GMT"}, {"version": "v2", "created": "Sun, 17 Jan 2021 03:01:27 GMT"}], "update_date": "2021-01-19", "authors_parsed": [["Hamilton", "Linus", ""], ["Moitra", "Ankur", ""]]}, {"id": "2101.05661", "submitter": "Carson Schubert", "authors": "Carson Schubert, Kevin Black, Daniel Fonseka, Abhimanyu Dhir, Jacob\n  Deutsch, Nihal Dhamani, Gavin Martin, Maruthi Akella", "title": "A Pipeline for Vision-Based On-Orbit Proximity Operations Using Deep\n  Learning and Synthetic Imagery", "comments": "Accepted to IEEE Aerospace Conference 2021. 14 pages, 11 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning has become the gold standard for image processing over the past\ndecade. Simultaneously, we have seen growing interest in orbital activities\nsuch as satellite servicing and debris removal that depend on proximity\noperations between spacecraft. However, two key challenges currently pose a\nmajor barrier to the use of deep learning for vision-based on-orbit proximity\noperations. Firstly, efficient implementation of these techniques relies on an\neffective system for model development that streamlines data curation,\ntraining, and evaluation. Secondly, a scarcity of labeled training data (images\nof a target spacecraft) hinders creation of robust deep learning models. This\npaper presents an open-source deep learning pipeline, developed specifically\nfor on-orbit visual navigation applications, that addresses these challenges.\nThe core of our work consists of two custom software tools built on top of a\ncloud architecture that interconnects all stages of the model development\nprocess. The first tool leverages Blender, an open-source 3D graphics toolset,\nto generate labeled synthetic training data with configurable model poses\n(positions and orientations), lighting conditions, backgrounds, and commonly\nobserved in-space image aberrations. The second tool is a plugin-based\nframework for effective dataset curation and model training; it provides common\nfunctionality like metadata generation and remote storage access to all\nprojects while giving complete independence to project-specific code.\nTime-consuming, graphics-intensive processes such as synthetic image generation\nand model training run on cloud-based computational resources which scale to\nany scope and budget and allow development of even the largest datasets and\nmodels from any machine. The presented system has been used in the Texas\nSpacecraft Laboratory with marked benefits in development speed and quality.\n", "versions": [{"version": "v1", "created": "Thu, 14 Jan 2021 15:17:54 GMT"}], "update_date": "2021-01-15", "authors_parsed": [["Schubert", "Carson", ""], ["Black", "Kevin", ""], ["Fonseka", "Daniel", ""], ["Dhir", "Abhimanyu", ""], ["Deutsch", "Jacob", ""], ["Dhamani", "Nihal", ""], ["Martin", "Gavin", ""], ["Akella", "Maruthi", ""]]}, {"id": "2101.05673", "submitter": "Anton Khritankov", "authors": "Anton Khritankov", "title": "Analysis of hidden feedback loops in continuous machine learning systems", "comments": "7 pages, 9 figures; added more experiments, minor stylistic fixes and\n  typos", "journal-ref": "Soft. Qual.: Fut. Persp. on Soft. Eng. Q. SWQD 2021. LNBIP, V. 404", "doi": "10.1007/978-3-030-65854-0_5", "report-no": null, "categories": "cs.LG cs.SE", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  In this concept paper, we discuss intricacies of specifying and verifying the\nquality of continuous and lifelong learning artificial intelligence systems as\nthey interact with and influence their environment causing a so-called concept\ndrift. We signify a problem of implicit feedback loops, demonstrate how they\nintervene with user behavior on an exemplary housing prices prediction system.\nBased on a preliminary model, we highlight conditions when such feedback loops\narise and discuss possible solution approaches.\n", "versions": [{"version": "v1", "created": "Thu, 14 Jan 2021 15:43:18 GMT"}, {"version": "v2", "created": "Sun, 17 Jan 2021 18:38:51 GMT"}], "update_date": "2021-01-19", "authors_parsed": [["Khritankov", "Anton", ""]]}, {"id": "2101.05679", "submitter": "Aratrika Mustafi", "authors": "Aratrika Mustafi", "title": "Convex Smoothed Autoencoder-Optimal Transport model", "comments": "26 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generative modelling is a key tool in unsupervised machine learning which has\nachieved stellar success in recent years. Despite this huge success, even the\nbest generative models such as Generative Adversarial Networks (GANs) and\nVariational Autoencoders (VAEs) come with their own shortcomings, mode collapse\nand mode mixture being the two most prominent problems. In this paper we\ndevelop a new generative model capable of generating samples which resemble the\nobserved data, and is free from mode collapse and mode mixture. Our model is\ninspired by the recently proposed Autoencoder-Optimal Transport (AE-OT) model\nand tries to improve on it by addressing the problems faced by the AE-OT model\nitself, specifically with respect to the sample generation algorithm.\nTheoretical results concerning the bound on the error in approximating the\nnon-smooth Brenier potential by its smoothed estimate, and approximating the\ndiscontinuous optimal transport map by a smoothed optimal transport map\nestimate have also been established in this paper.\n", "versions": [{"version": "v1", "created": "Thu, 14 Jan 2021 15:55:20 GMT"}], "update_date": "2021-01-15", "authors_parsed": [["Mustafi", "Aratrika", ""]]}, {"id": "2101.05684", "submitter": "Gustav Eje Henter", "authors": "Simon Alexanderson, \\'Eva Sz\\'ekely, Gustav Eje Henter, Taras\n  Kucherenko, Jonas Beskow", "title": "Generating coherent spontaneous speech and gesture from text", "comments": "3 pages, 2 figures, published at the ACM International Conference on\n  Intelligent Virtual Agents (IVA) 2020", "journal-ref": "Proceedings of the 20th ACM International Conference on\n  Intelligent Virtual Agents (IVA '20), 2020, 3 pages", "doi": "10.1145/3383652.3423874", "report-no": null, "categories": "cs.LG cs.GR cs.SD eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Embodied human communication encompasses both verbal (speech) and non-verbal\ninformation (e.g., gesture and head movements). Recent advances in machine\nlearning have substantially improved the technologies for generating synthetic\nversions of both of these types of data: On the speech side, text-to-speech\nsystems are now able to generate highly convincing, spontaneous-sounding speech\nusing unscripted speech audio as the source material. On the motion side,\nprobabilistic motion-generation methods can now synthesise vivid and lifelike\nspeech-driven 3D gesticulation. In this paper, we put these two\nstate-of-the-art technologies together in a coherent fashion for the first\ntime. Concretely, we demonstrate a proof-of-concept system trained on a\nsingle-speaker audio and motion-capture dataset, that is able to generate both\nspeech and full-body gestures together from text input. In contrast to previous\napproaches for joint speech-and-gesture generation, we generate full-body\ngestures from speech synthesis trained on recordings of spontaneous speech from\nthe same person as the motion-capture data. We illustrate our results by\nvisualising gesture spaces and text-speech-gesture alignments, and through a\ndemonstration video at https://simonalexanderson.github.io/IVA2020 .\n", "versions": [{"version": "v1", "created": "Thu, 14 Jan 2021 16:02:21 GMT"}], "update_date": "2021-01-15", "authors_parsed": [["Alexanderson", "Simon", ""], ["Sz\u00e9kely", "\u00c9va", ""], ["Henter", "Gustav Eje", ""], ["Kucherenko", "Taras", ""], ["Beskow", "Jonas", ""]]}, {"id": "2101.05775", "submitter": "Leandro Passos", "authors": "Leandro Aparecido Passos, Danilo Samuel Jodas, Luiz C. F. Ribeiro,\n  Thierry Pinheiro, Jo\\~ao P. Papa", "title": "$\\text{O}^2$PF: Oversampling via Optimum-Path Forest for Breast Cancer\n  Detection", "comments": "6 pages, 3 figures. 2020 IEEE 33rd International Symposium on\n  Computer-Based Medical Systems (CBMS)", "journal-ref": null, "doi": "10.1109/CBMS49503.2020.00100", "report-no": null, "categories": "cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Breast cancer is among the most deadly diseases, distressing mostly women\nworldwide. Although traditional methods for detection have presented themselves\nas valid for the task, they still commonly present low accuracies and demand\nconsiderable time and effort from professionals. Therefore, a computer-aided\ndiagnosis (CAD) system capable of providing early detection becomes hugely\ndesirable. In the last decade, machine learning-based techniques have been of\nparamount importance in this context, since they are capable of extracting\nessential information from data and reasoning about it. However, such\napproaches still suffer from imbalanced data, specifically on medical issues,\nwhere the number of healthy people samples is, in general, considerably higher\nthan the number of patients. Therefore this paper proposes the $\\text{O}^2$PF,\na data oversampling method based on the unsupervised Optimum-Path Forest\nAlgorithm. Experiments conducted over the full oversampling scenario state the\nrobustness of the model, which is compared against three well-established\noversampling methods considering three breast cancer and three general-purpose\ntasks for medical issues datasets.\n", "versions": [{"version": "v1", "created": "Thu, 14 Jan 2021 18:26:13 GMT"}], "update_date": "2021-01-15", "authors_parsed": [["Passos", "Leandro Aparecido", ""], ["Jodas", "Danilo Samuel", ""], ["Ribeiro", "Luiz C. F.", ""], ["Pinheiro", "Thierry", ""], ["Papa", "Jo\u00e3o P.", ""]]}, {"id": "2101.05778", "submitter": "Benjamin Filippenko", "authors": "Ephy R. Love, Benjamin Filippenko, Vasileios Maroulas, Gunnar Carlsson", "title": "Topological Deep Learning", "comments": "28 pages, 14 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work introduces the Topological CNN (TCNN), which encompasses several\ntopologically defined convolutional methods. Manifolds with important\nrelationships to the natural image space are used to parameterize image filters\nwhich are used as convolutional weights in a TCNN. These manifolds also\nparameterize slices in layers of a TCNN across which the weights are localized.\nWe show evidence that TCNNs learn faster, on less data, with fewer learned\nparameters, and with greater generalizability and interpretability than\nconventional CNNs. We introduce and explore TCNN layers for both image and\nvideo data. We propose extensions to 3D images and 3D video.\n", "versions": [{"version": "v1", "created": "Thu, 14 Jan 2021 18:32:11 GMT"}, {"version": "v2", "created": "Thu, 4 Mar 2021 11:30:12 GMT"}], "update_date": "2021-03-05", "authors_parsed": [["Love", "Ephy R.", ""], ["Filippenko", "Benjamin", ""], ["Maroulas", "Vasileios", ""], ["Carlsson", "Gunnar", ""]]}, {"id": "2101.05779", "submitter": "Giovanni Paolini", "authors": "Giovanni Paolini, Ben Athiwaratkun, Jason Krone, Jie Ma, Alessandro\n  Achille, Rishita Anubhai, Cicero Nogueira dos Santos, Bing Xiang, Stefano\n  Soatto", "title": "Structured Prediction as Translation between Augmented Natural Languages", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new framework, Translation between Augmented Natural Languages\n(TANL), to solve many structured prediction language tasks including joint\nentity and relation extraction, nested named entity recognition, relation\nclassification, semantic role labeling, event extraction, coreference\nresolution, and dialogue state tracking. Instead of tackling the problem by\ntraining task-specific discriminative classifiers, we frame it as a translation\ntask between augmented natural languages, from which the task-relevant\ninformation can be easily extracted. Our approach can match or outperform\ntask-specific models on all tasks, and in particular, achieves new\nstate-of-the-art results on joint entity and relation extraction (CoNLL04, ADE,\nNYT, and ACE2005 datasets), relation classification (FewRel and TACRED), and\nsemantic role labeling (CoNLL-2005 and CoNLL-2012). We accomplish this while\nusing the same architecture and hyperparameters for all tasks and even when\ntraining a single model to solve all tasks at the same time (multi-task\nlearning). Finally, we show that our framework can also significantly improve\nthe performance in a low-resource regime, thanks to better use of label\nsemantics.\n", "versions": [{"version": "v1", "created": "Thu, 14 Jan 2021 18:32:21 GMT"}, {"version": "v2", "created": "Thu, 28 Jan 2021 22:08:48 GMT"}], "update_date": "2021-02-01", "authors_parsed": [["Paolini", "Giovanni", ""], ["Athiwaratkun", "Ben", ""], ["Krone", "Jason", ""], ["Ma", "Jie", ""], ["Achille", "Alessandro", ""], ["Anubhai", "Rishita", ""], ["Santos", "Cicero Nogueira dos", ""], ["Xiang", "Bing", ""], ["Soatto", "Stefano", ""]]}, {"id": "2101.05781", "submitter": "Deborah Blevins", "authors": "Deborah H. Blevins (1), Pablo Moriano (2), Robert A. Bridges (2), Miki\n  E. Verma (2), Michael D. Iannacone (2), Samuel C Hollifield (2) ((1)\n  University of Kentucky, (2) Oak Ridge National Laboratory)", "title": "Time-Based CAN Intrusion Detection Benchmark", "comments": "7 pages, 2 figures", "journal-ref": "Workshop on Automotive and Autonomous Vehicle Security (AutoSec)\n  2021", "doi": "10.14722/autosec.2021.23013", "report-no": null, "categories": "cs.CR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modern vehicles are complex cyber-physical systems made of hundreds of\nelectronic control units (ECUs) that communicate over controller area networks\n(CANs). This inherited complexity has expanded the CAN attack surface which is\nvulnerable to message injection attacks. These injections change the overall\ntiming characteristics of messages on the bus, and thus, to detect these\nmalicious messages, time-based intrusion detection systems (IDSs) have been\nproposed. However, time-based IDSs are usually trained and tested on\nlow-fidelity datasets with unrealistic, labeled attacks. This makes difficult\nthe task of evaluating, comparing, and validating IDSs. Here we detail and\nbenchmark four time-based IDSs against the newly published ROAD dataset, the\nfirst open CAN IDS dataset with real (non-simulated) stealthy attacks with\nphysically verified effects. We found that methods that perform hypothesis\ntesting by explicitly estimating message timing distributions have lower\nperformance than methods that seek anomalies in a distribution-related\nstatistic. In particular, these \"distribution-agnostic\" based methods\noutperform \"distribution-based\" methods by at least 55% in area under the\nprecision-recall curve (AUC-PR). Our results expand the body of knowledge of\nCAN time-based IDSs by providing details of these methods and reporting their\nresults when tested on datasets with real advanced attacks. Finally, we develop\nan after-market plug-in detector using lightweight hardware, which can be used\nto deploy the best performing IDS method on nearly any vehicle.\n", "versions": [{"version": "v1", "created": "Thu, 14 Jan 2021 18:33:19 GMT"}], "update_date": "2021-02-08", "authors_parsed": [["Blevins", "Deborah H.", ""], ["Moriano", "Pablo", ""], ["Bridges", "Robert A.", ""], ["Verma", "Miki E.", ""], ["Iannacone", "Michael D.", ""], ["Hollifield", "Samuel C", ""]]}, {"id": "2101.05783", "submitter": "Abubakar Abid", "authors": "Abubakar Abid, Maheen Farooqi, James Zou", "title": "Persistent Anti-Muslim Bias in Large Language Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  It has been observed that large-scale language models capture undesirable\nsocietal biases, e.g. relating to race and gender; yet religious bias has been\nrelatively unexplored. We demonstrate that GPT-3, a state-of-the-art contextual\nlanguage model, captures persistent Muslim-violence bias. We probe GPT-3 in\nvarious ways, including prompt completion, analogical reasoning, and story\ngeneration, to understand this anti-Muslim bias, demonstrating that it appears\nconsistently and creatively in different uses of the model and that it is\nsevere even compared to biases about other religious groups. For instance,\n\"Muslim\" is analogized to \"terrorist\" in 23% of test cases, while \"Jewish\" is\nmapped to \"money\" in 5% of test cases. We quantify the positive distraction\nneeded to overcome this bias with adversarial text prompts, and find that use\nof the most positive 6 adjectives reduces violent completions for \"Muslims\"\nfrom 66% to 20%, but which is still higher than for other religious groups.\n", "versions": [{"version": "v1", "created": "Thu, 14 Jan 2021 18:41:55 GMT"}, {"version": "v2", "created": "Mon, 18 Jan 2021 17:02:28 GMT"}], "update_date": "2021-01-19", "authors_parsed": [["Abid", "Abubakar", ""], ["Farooqi", "Maheen", ""], ["Zou", "James", ""]]}, {"id": "2101.05795", "submitter": "Leandro Passos", "authors": "Leandro Aparecido Passos, Jo\\~ao Paulo Papa", "title": "A Metaheuristic-Driven Approach to Fine-Tune Deep Boltzmann Machines", "comments": "30 pages, 7 figures", "journal-ref": "Applied Soft Computing 97 (2020): 105717", "doi": "10.1016/j.asoc.2019.105717", "report-no": null, "categories": "cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Deep learning techniques, such as Deep Boltzmann Machines (DBMs), have\nreceived considerable attention over the past years due to the outstanding\nresults concerning a variable range of domains. One of the main shortcomings of\nthese techniques involves the choice of their hyperparameters, since they have\na significant impact on the final results. This work addresses the issue of\nfine-tuning hyperparameters of Deep Boltzmann Machines using metaheuristic\noptimization techniques with different backgrounds, such as swarm intelligence,\nmemory- and evolutionary-based approaches. Experiments conducted in three\npublic datasets for binary image reconstruction showed that metaheuristic\ntechniques can obtain reasonable results.\n", "versions": [{"version": "v1", "created": "Thu, 14 Jan 2021 18:57:29 GMT"}], "update_date": "2021-01-15", "authors_parsed": [["Passos", "Leandro Aparecido", ""], ["Papa", "Jo\u00e3o Paulo", ""]]}, {"id": "2101.05796", "submitter": "Valentin Wolf", "authors": "Valentin Wolf, Andreas Lugmayr, Martin Danelljan, Luc Van Gool, Radu\n  Timofte", "title": "DeFlow: Learning Complex Image Degradations from Unpaired Data with\n  Conditional Flows", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The difficulty of obtaining paired data remains a major bottleneck for\nlearning image restoration and enhancement models for real-world applications.\nCurrent strategies aim to synthesize realistic training data by modeling noise\nand degradations that appear in real-world settings. We propose DeFlow, a\nmethod for learning stochastic image degradations from unpaired data. Our\napproach is based on a novel unpaired learning formulation for conditional\nnormalizing flows. We model the degradation process in the latent space of a\nshared flow encoder-decoder network. This allows us to learn the conditional\ndistribution of a noisy image given the clean input by solely minimizing the\nnegative log-likelihood of the marginal distributions. We validate our DeFlow\nformulation on the task of joint image restoration and super-resolution. The\nmodels trained with the synthetic data generated by DeFlow outperform previous\nlearnable approaches on all three datasets.\n", "versions": [{"version": "v1", "created": "Thu, 14 Jan 2021 18:58:01 GMT"}], "update_date": "2021-01-15", "authors_parsed": [["Wolf", "Valentin", ""], ["Lugmayr", "Andreas", ""], ["Danelljan", "Martin", ""], ["Van Gool", "Luc", ""], ["Timofte", "Radu", ""]]}, {"id": "2101.05808", "submitter": "Adam Spannaus", "authors": "Adam Spannaus, Kody J. H. Law, Piotr Luszczek, Farzana Nasrin, Cassie\n  Putman Micucci, Peter K. Liaw, Louis J. Santodonato, David J. Keffer,\n  Vasileios Maroulas", "title": "Materials Fingerprinting Classification", "comments": null, "journal-ref": null, "doi": "10.1016/j.cpc.2021.108019", "report-no": null, "categories": "cond-mat.mtrl-sci cs.LG stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Significant progress in many classes of materials could be made with the\navailability of experimentally-derived large datasets composed of atomic\nidentities and three-dimensional coordinates. Methods for visualizing the local\natomic structure, such as atom probe tomography (APT), which routinely generate\ndatasets comprised of millions of atoms, are an important step in realizing\nthis goal. However, state-of-the-art APT instruments generate noisy and sparse\ndatasets that provide information about elemental type, but obscure atomic\nstructures, thus limiting their subsequent value for materials discovery. The\napplication of a materials fingerprinting process, a machine learning algorithm\ncoupled with topological data analysis, provides an avenue by which\nhere-to-fore unprecedented structural information can be extracted from an APT\ndataset. As a proof of concept, the material fingerprint is applied to\nhigh-entropy alloy APT datasets containing body-centered cubic (BCC) and\nface-centered cubic (FCC) crystal structures. A local atomic configuration\ncentered on an arbitrary atom is assigned a topological descriptor, with which\nit can be characterized as a BCC or FCC lattice with near perfect accuracy,\ndespite the inherent noise in the dataset. This successful identification of a\nfingerprint is a crucial first step in the development of algorithms which can\nextract more nuanced information, such as chemical ordering, from existing\ndatasets of complex materials.\n", "versions": [{"version": "v1", "created": "Thu, 14 Jan 2021 20:32:38 GMT"}], "update_date": "2021-06-30", "authors_parsed": [["Spannaus", "Adam", ""], ["Law", "Kody J. H.", ""], ["Luszczek", "Piotr", ""], ["Nasrin", "Farzana", ""], ["Micucci", "Cassie Putman", ""], ["Liaw", "Peter K.", ""], ["Santodonato", "Louis J.", ""], ["Keffer", "David J.", ""], ["Maroulas", "Vasileios", ""]]}, {"id": "2101.05834", "submitter": "Sebastian Kaltenbach", "authors": "Sebastian Kaltenbach, Phaedon-Stelios Koutsourelakis", "title": "Physics-aware, probabilistic model order reduction with guaranteed\n  stability", "comments": "ICLR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG physics.comp-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given (small amounts of) time-series' data from a high-dimensional,\nfine-grained, multiscale dynamical system, we propose a generative framework\nfor learning an effective, lower-dimensional, coarse-grained dynamical model\nthat is predictive of the fine-grained system's long-term evolution but also of\nits behavior under different initial conditions. We target fine-grained models\nas they arise in physical applications (e.g. molecular dynamics, agent-based\nmodels), the dynamics of which are strongly non-stationary but their transition\nto equilibrium is governed by unknown slow processes which are largely\ninaccessible by brute-force simulations. Approaches based on domain knowledge\nheavily rely on physical insight in identifying temporally slow features and\nfail to enforce the long-term stability of the learned dynamics. On the other\nhand, purely statistical frameworks lack interpretability and rely on large\namounts of expensive simulation data (long and multiple trajectories) as they\ncannot infuse domain knowledge. The generative framework proposed achieves the\naforementioned desiderata by employing a flexible prior on the complex plane\nfor the latent, slow processes, and an intermediate layer of physics-motivated\nlatent variables that reduces reliance on data and imbues inductive bias. In\ncontrast to existing schemes, it does not require the a priori definition of\nprojection operators from the fine-grained description and addresses\nsimultaneously the tasks of dimensionality reduction and model estimation. We\ndemonstrate its efficacy and accuracy in multiscale physical systems of\nparticle dynamics where probabilistic, long-term predictions of phenomena not\ncontained in the training data are produced.\n", "versions": [{"version": "v1", "created": "Thu, 14 Jan 2021 19:16:51 GMT"}], "update_date": "2021-01-18", "authors_parsed": [["Kaltenbach", "Sebastian", ""], ["Koutsourelakis", "Phaedon-Stelios", ""]]}, {"id": "2101.05840", "submitter": "Rhema Linder", "authors": "Oleg Bezrukavnikov and Rhema Linder", "title": "A Neophyte With AutoML: Evaluating the Promises of Automatic Machine\n  Learning Tools", "comments": "10 pages, 3 tables, 3 figures. First author is a high school senior", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.HC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper discusses modern Auto Machine Learning (AutoML) tools from the\nperspective of a person with little prior experience in Machine Learning (ML).\nThere are many AutoML tools both ready-to-use and under development, which are\ncreated to simplify and democratize usage of ML technologies in everyday life.\nOur position is that ML should be easy to use and available to a greater number\nof people. Prior research has identified the need for intuitive AutoML tools.\nThis work seeks to understand how well AutoML tools have achieved that goal in\npractice. We evaluate three AutoML Tools to evaluate the end-user experience\nand system performance. We evaluate the tools by having them create models from\na competition dataset on banking data. We report on their performance and the\ndetails of our experience. This process provides a unique understanding of the\nstate of the art of AutoML tools. Finally, we use these experiences to inform a\ndiscussion on how future AutoML tools can improve the user experience for\nneophytes of Machine Learning.\n", "versions": [{"version": "v1", "created": "Thu, 14 Jan 2021 19:28:57 GMT"}], "update_date": "2021-01-18", "authors_parsed": [["Bezrukavnikov", "Oleg", ""], ["Linder", "Rhema", ""]]}, {"id": "2101.05844", "submitter": "Alessandro De Palma", "authors": "Alessandro De Palma, Harkirat Singh Behl, Rudy Bunel, Philip H.S.\n  Torr, M. Pawan Kumar", "title": "Scaling the Convex Barrier with Sparse Dual Algorithms", "comments": "Previous version: published at ICLR 2021. This extended version:\n  submitted to JMLR", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Tight and efficient neural network bounding is crucial to the scaling of\nneural network verification systems. Many efficient bounding algorithms have\nbeen presented recently, but they are often too loose to verify more\nchallenging properties. This is due to the weakness of the employed relaxation,\nwhich is usually a linear program of size linear in the number of neurons.\nWhile a tighter linear relaxation for piecewise-linear activations exists, it\ncomes at the cost of exponentially many constraints and currently lacks an\nefficient customized solver. We alleviate this deficiency by presenting two\nnovel dual algorithms: one operates a subgradient method on a small active set\nof dual variables, the other exploits the sparsity of Frank-Wolfe type\noptimizers to incur only a linear memory cost. Both methods recover the\nstrengths of the new relaxation: tightness and a linear separation oracle. At\nthe same time, they share the benefits of previous dual approaches for weaker\nrelaxations: massive parallelism, GPU implementation, low cost per iteration\nand valid bounds at any time. As a consequence, we can obtain better bounds\nthan off-the-shelf solvers in only a fraction of their running time, attaining\nsignificant formal verification speed-ups.\n", "versions": [{"version": "v1", "created": "Thu, 14 Jan 2021 19:45:17 GMT"}, {"version": "v2", "created": "Tue, 26 Jan 2021 14:36:42 GMT"}], "update_date": "2021-01-27", "authors_parsed": [["De Palma", "Alessandro", ""], ["Behl", "Harkirat Singh", ""], ["Bunel", "Rudy", ""], ["Torr", "Philip H. S.", ""], ["Kumar", "M. Pawan", ""]]}, {"id": "2101.05846", "submitter": "Josef Lorenz Rumberger", "authors": "Josef Lorenz Rumberger, Xiaoyan Yu, Peter Hirsch, Melanie Dohmen,\n  Vanessa Emanuela Guarino, Ashkan Mokarian, Lisa Mais, Jan Funke, Dagmar\n  Kainmueller", "title": "How Shift Equivariance Impacts Metric Learning for Instance Segmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Metric learning has received conflicting assessments concerning its\nsuitability for solving instance segmentation tasks. It has been dismissed as\ntheoretically flawed due to the shift equivariance of the employed CNNs and\ntheir respective inability to distinguish same-looking objects. Yet it has been\nshown to yield state of the art results for a variety of tasks, and practical\nissues have mainly been reported in the context of tile-and-stitch approaches,\nwhere discontinuities at tile boundaries have been observed. To date, neither\nof the reported issues have undergone thorough formal analysis. In our work, we\ncontribute a comprehensive formal analysis of the shift equivariance properties\nof encoder-decoder-style CNNs, which yields a clear picture of what can and\ncannot be achieved with metric learning in the face of same-looking objects. In\nparticular, we prove that a standard encoder-decoder network that takes\n$d$-dimensional images as input, with $l$ pooling layers and pooling factor\n$f$, has the capacity to distinguish at most $f^{dl}$ same-looking objects, and\nwe show that this upper limit can be reached. Furthermore, we show that to\navoid discontinuities in a tile-and-stitch approach, assuming standard batch\nsize 1, it is necessary to employ valid convolutions in combination with a\ntraining output window size strictly greater than $f^l$, while at test-time it\nis necessary to crop tiles to size $n\\cdot f^l$ before stitching, with $n\\geq\n1$. We complement these theoretical findings by discussing a number of\ninsightful special cases for which we show empirical results on synthetic data.\n", "versions": [{"version": "v1", "created": "Thu, 14 Jan 2021 19:48:24 GMT"}], "update_date": "2021-01-18", "authors_parsed": [["Rumberger", "Josef Lorenz", ""], ["Yu", "Xiaoyan", ""], ["Hirsch", "Peter", ""], ["Dohmen", "Melanie", ""], ["Guarino", "Vanessa Emanuela", ""], ["Mokarian", "Ashkan", ""], ["Mais", "Lisa", ""], ["Funke", "Jan", ""], ["Kainmueller", "Dagmar", ""]]}, {"id": "2101.05848", "submitter": "Guillermo Barrios Morales", "authors": "Guillermo B. Morales, Claudio R. Mirasso and Miguel C. Soriano", "title": "Unveiling the role of plasticity rules in reservoir computing", "comments": null, "journal-ref": null, "doi": "10.1016/j.neucom.2020.05.127", "report-no": null, "categories": "nlin.AO cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Reservoir Computing (RC) is an appealing approach in Machine Learning that\ncombines the high computational capabilities of Recurrent Neural Networks with\na fast and easy training method. Likewise, successful implementation of\nneuro-inspired plasticity rules into RC artificial networks has boosted the\nperformance of the original models. In this manuscript, we analyze the role\nthat plasticity rules play on the changes that lead to a better performance of\nRC. To this end, we implement synaptic and non-synaptic plasticity rules in a\nparadigmatic example of RC model: the Echo State Network. Testing on nonlinear\ntime series prediction tasks, we show evidence that improved performance in all\nplastic models are linked to a decrease of the pair-wise correlations in the\nreservoir, as well as a significant increase of individual neurons ability to\nseparate similar inputs in their activity space. Here we provide new insights\non this observed improvement through the study of different stages on the\nplastic learning. From the perspective of the reservoir dynamics, optimal\nperformance is found to occur close to the so-called edge of instability. Our\nresults also show that it is possible to combine different forms of plasticity\n(namely synaptic and non-synaptic rules) to further improve the performance on\nprediction tasks, obtaining better results than those achieved with\nsingle-plasticity models.\n", "versions": [{"version": "v1", "created": "Thu, 14 Jan 2021 19:55:30 GMT"}], "update_date": "2021-07-13", "authors_parsed": [["Morales", "Guillermo B.", ""], ["Mirasso", "Claudio R.", ""], ["Soriano", "Miguel C.", ""]]}, {"id": "2101.05850", "submitter": "Angel Daruna", "authors": "Angel Daruna, Mehul Gupta, Mohan Sridharan, and Sonia Chernova", "title": "Continual Learning of Knowledge Graph Embeddings", "comments": "8 pages, 4 figures. Accepted for publication in IEEE Robotics and\n  Automation Letters (RA-L)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, there has been a resurgence in methods that use distributed\n(neural) representations to represent and reason about semantic knowledge for\nrobotics applications. However, while robots often observe previously unknown\nconcepts, these representations typically assume that all concepts are known a\npriori, and incorporating new information requires all concepts to be learned\nafresh. Our work relaxes this limiting assumption of existing representations\nand tackles the incremental knowledge graph embedding problem by leveraging the\nprinciples of a range of continual learning methods. Through an experimental\nevaluation with several knowledge graphs and embedding representations, we\nprovide insights about trade-offs for practitioners to match a semantics-driven\nrobotics applications to a suitable continual knowledge graph embedding method.\n", "versions": [{"version": "v1", "created": "Thu, 14 Jan 2021 19:59:57 GMT"}, {"version": "v2", "created": "Mon, 10 May 2021 15:37:10 GMT"}], "update_date": "2021-05-11", "authors_parsed": [["Daruna", "Angel", ""], ["Gupta", "Mehul", ""], ["Sridharan", "Mohan", ""], ["Chernova", "Sonia", ""]]}, {"id": "2101.05853", "submitter": "Manish Raghavan", "authors": "Jon Kleinberg, Manish Raghavan", "title": "Algorithmic Monoculture and Social Welfare", "comments": "A version of this paper appears in Proceedings of the National\n  Academy of Sciences at https://www.pnas.org/content/118/22/e2018340118", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GT cs.CY cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  As algorithms are increasingly applied to screen applicants for high-stakes\ndecisions in employment, lending, and other domains, concerns have been raised\nabout the effects of algorithmic monoculture, in which many decision-makers all\nrely on the same algorithm. This concern invokes analogies to agriculture,\nwhere a monocultural system runs the risk of severe harm from unexpected\nshocks. Here we show that the dangers of algorithmic monoculture run much\ndeeper, in that monocultural convergence on a single algorithm by a group of\ndecision-making agents, even when the algorithm is more accurate for any one\nagent in isolation, can reduce the overall quality of the decisions being made\nby the full collection of agents. Unexpected shocks are therefore not needed to\nexpose the risks of monoculture; it can hurt accuracy even under \"normal\"\noperations, and even for algorithms that are more accurate when used by only a\nsingle decision-maker. Our results rely on minimal assumptions, and involve the\ndevelopment of a probabilistic framework for analyzing systems that use\nmultiple noisy estimates of a set of alternatives.\n", "versions": [{"version": "v1", "created": "Thu, 14 Jan 2021 20:18:23 GMT"}, {"version": "v2", "created": "Tue, 1 Jun 2021 14:46:26 GMT"}], "update_date": "2021-06-02", "authors_parsed": [["Kleinberg", "Jon", ""], ["Raghavan", "Manish", ""]]}, {"id": "2101.05855", "submitter": "Akanksha Atrey", "authors": "Akanksha Atrey, Prashant Shenoy, David Jensen", "title": "Preserving Privacy in Personalized Models for Distributed Mobile\n  Services", "comments": "Published at ICDCS 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The ubiquity of mobile devices has led to the proliferation of mobile\nservices that provide personalized and context-aware content to their users.\nModern mobile services are distributed between end-devices, such as\nsmartphones, and remote servers that reside in the cloud. Such services thrive\non their ability to predict future contexts to pre-fetch content or make\ncontext-specific recommendations. An increasingly common method to predict\nfuture contexts, such as location, is via machine learning (ML) models. Recent\nwork in context prediction has focused on ML model personalization where a\npersonalized model is learned for each individual user in order to tailor\npredictions or recommendations to a user's mobile behavior. While the use of\npersonalized models increases efficacy of the mobile service, we argue that it\nincreases privacy risk since a personalized model encodes contextual behavior\nunique to each user. To demonstrate these privacy risks, we present several\nattribute inference-based privacy attacks and show that such attacks can leak\nprivacy with up to 78% efficacy for top-3 predictions. We present Pelican, a\nprivacy-preserving personalization system for context-aware mobile services\nthat leverages both device and cloud resources to personalize ML models while\nminimizing the risk of privacy leakage for users. We evaluate Pelican using\nreal world traces for location-aware mobile services and show that Pelican can\nsubstantially reduce privacy leakage by up to 75%.\n", "versions": [{"version": "v1", "created": "Thu, 14 Jan 2021 20:26:54 GMT"}, {"version": "v2", "created": "Wed, 21 Apr 2021 23:18:05 GMT"}], "update_date": "2021-04-23", "authors_parsed": [["Atrey", "Akanksha", ""], ["Shenoy", "Prashant", ""], ["Jensen", "David", ""]]}, {"id": "2101.05866", "submitter": "Feichen Shen PhD", "authors": "David Oniani, Chen Wang, Yiqing Zhao, Andrew Wen, Hongfang Liu,\n  Feichen Shen", "title": "Comparisons of Graph Neural Networks on Cancer Classification Leveraging\n  a Joint of Phenotypic and Genetic Features", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG q-bio.QM", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Cancer is responsible for millions of deaths worldwide every year. Although\nsignificant progress hasbeen achieved in cancer medicine, many issues remain to\nbe addressed for improving cancer therapy.Appropriate cancer patient\nstratification is the prerequisite for selecting appropriate treatment plan,\nascancer patients are of known heterogeneous genetic make-ups and phenotypic\ndifferences. In thisstudy, built upon deep phenotypic characterizations\nextractable from Mayo Clinic electronic healthrecords (EHRs) and genetic test\nreports for a collection of cancer patients, we evaluated variousgraph neural\nnetworks (GNNs) leveraging a joint of phenotypic and genetic features for\ncancer typeclassification. Models were applied and fine-tuned on the Mayo\nClinic cancer disease dataset. Theassessment was done through the reported\naccuracy, precision, recall, and F1 values as well as throughF1 scores based on\nthe disease class. Per our evaluation results, GNNs on average outperformed\nthebaseline models with mean statistics always being higher that those of the\nbaseline models (0.849 vs0.772 for accuracy, 0.858 vs 0.794 for precision,\n0.843 vs 0.759 for recall, and 0.843 vs 0.855 for F1score). Among GNNs,\nChebNet, GraphSAGE, and TAGCN showed the best performance, while GATshowed the\nworst. We applied and compared eight GNN models including AGNN, ChebNet,\nGAT,GCN, GIN, GraphSAGE, SGC, and TAGCN on the Mayo Clinic cancer disease\ndataset and assessedtheir performance as well as compared them with each other\nand with more conventional machinelearning models such as decision tree,\ngradient boosting, multi-layer perceptron, naive bayes, andrandom forest which\nwe used as the baselines.\n", "versions": [{"version": "v1", "created": "Thu, 14 Jan 2021 20:53:49 GMT"}], "update_date": "2021-01-18", "authors_parsed": [["Oniani", "David", ""], ["Wang", "Chen", ""], ["Zhao", "Yiqing", ""], ["Wen", "Andrew", ""], ["Liu", "Hongfang", ""], ["Shen", "Feichen", ""]]}, {"id": "2101.05880", "submitter": "Shenghui Li", "authors": "Shenghui Li, Edith Ngai, Fanghua Ye, and Thiemo Voigt", "title": "Auto-weighted Robust Federated Learning with Corrupted Data Sources", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Federated learning provides a communication-efficient and privacy-preserving\ntraining process by enabling learning statistical models with massive\nparticipants while keeping their data in local clients. However, standard\nfederated learning techniques that naively minimize an average loss function\nare vulnerable to data corruptions from outliers, systematic mislabeling, or\neven adversaries. In addition, it is often prohibited for service providers to\nverify the quality of data samples due to the increasing concern of user data\nprivacy. In this paper, we address this challenge by proposing Auto-weighted\nRobust Federated Learning (arfl), a novel approach that jointly learns the\nglobal model and the weights of local updates to provide robustness against\ncorrupted data sources. We prove a learning bound on the expected risk with\nrespect to the predictor and the weights of clients, which guides the\ndefinition of the objective for robust federated learning. The weights are\nallocated by comparing the empirical loss of a client with the average loss of\nthe best p clients (p-average), thus we can downweight the clients with\nsignificantly high losses, thereby lower their contributions to the global\nmodel. We show that this approach achieves robustness when the data of\ncorrupted clients is distributed differently from benign ones. To optimize the\nobjective function, we propose a communication-efficient algorithm based on the\nblockwise minimization paradigm. We conduct experiments on multiple benchmark\ndatasets, including CIFAR-10, FEMNIST and Shakespeare, considering different\ndeep neural network models. The results show that our solution is robust\nagainst different scenarios including label shuffling, label flipping and noisy\nfeatures, and outperforms the state-of-the-art methods in most scenarios.\n", "versions": [{"version": "v1", "created": "Thu, 14 Jan 2021 21:54:55 GMT"}, {"version": "v2", "created": "Wed, 31 Mar 2021 16:34:49 GMT"}], "update_date": "2021-04-01", "authors_parsed": [["Li", "Shenghui", ""], ["Ngai", "Edith", ""], ["Ye", "Fanghua", ""], ["Voigt", "Thiemo", ""]]}, {"id": "2101.05885", "submitter": "Tongyu Zong", "authors": "Tongyu Zong, Chen Li, Yuanyuan Lei, Guangyu Li, Houwei Cao, Yong Liu", "title": "Cocktail Edge Caching: Ride Dynamic Trends of Content Popularity with\n  Ensemble Learning", "comments": "INFOCOM 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Edge caching will play a critical role in facilitating the emerging\ncontent-rich applications. However, it faces many new challenges, in\nparticular, the highly dynamic content popularity and the heterogeneous caching\nconfigurations. In this paper, we propose Cocktail Edge Caching, that tackles\nthe dynamic popularity and heterogeneity through ensemble learning. Instead of\ntrying to find a single dominating caching policy for all the caching\nscenarios, we employ an ensemble of constituent caching policies and adaptively\nselect the best-performing policy to control the cache. Towards this goal, we\nfirst show through formal analysis and experiments that different variations of\nthe LFU and LRU policies have complementary performance in different caching\nscenarios. We further develop a novel caching algorithm that enhances LFU/LRU\nwith deep recurrent neural network (LSTM) based time-series analysis. Finally,\nwe develop a deep reinforcement learning agent that adaptively combines base\ncaching policies according to their virtual hit ratios on parallel virtual\ncaches. Through extensive experiments driven by real content requests from two\nlarge video streaming platforms, we demonstrate that CEC not only consistently\noutperforms all single policies, but also improves the robustness of them. CEC\ncan be well generalized to different caching scenarios with low computation\noverheads for deployment.\n", "versions": [{"version": "v1", "created": "Thu, 14 Jan 2021 21:59:04 GMT"}], "update_date": "2021-01-18", "authors_parsed": [["Zong", "Tongyu", ""], ["Li", "Chen", ""], ["Lei", "Yuanyuan", ""], ["Li", "Guangyu", ""], ["Cao", "Houwei", ""], ["Liu", "Yong", ""]]}, {"id": "2101.05891", "submitter": "Sajila Wickramaratne", "authors": "Sajila D. Wickramaratne and Md Shaad Mahmud", "title": "A Deep Learning Based Ternary Task Classification System Using Gramian\n  Angular Summation Field in fNIRS Neuroimaging Data", "comments": "4 pages, 4 Figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Functional near-infrared spectroscopy (fNIRS) is a non-invasive, economical\nmethod used to study its blood flow pattern. These patterns can be used to\nclassify tasks a subject is performing. Currently, most of the classification\nsystems use simple machine learning solutions for the classification of tasks.\nThese conventional machine learning methods, which are easier to implement and\ninterpret, usually suffer from low accuracy and undergo a complex preprocessing\nphase before network training. The proposed method converts the raw fNIRS time\nseries data into an image using Gramian Angular Summation Field. A Deep\nConvolutional Neural Network (CNN) based architecture is then used for task\nclassification, including mental arithmetic, motor imagery, and idle state.\nFurther, this method can eliminate the feature selection stage, which affects\nthe traditional classifiers' performance. This system obtained 87.14% average\nclassification accuracy higher than any other method for the dataset.\n", "versions": [{"version": "v1", "created": "Thu, 14 Jan 2021 22:09:35 GMT"}], "update_date": "2021-01-18", "authors_parsed": [["Wickramaratne", "Sajila D.", ""], ["Mahmud", "Md Shaad", ""]]}, {"id": "2101.05892", "submitter": "Sajila Wickramaratne", "authors": "Sajila D. Wickramaratne and MD Shaad Mahmud", "title": "A Ternary Bi-Directional LSTM Classification for Brain Activation\n  Pattern Recognition Using fNIRS", "comments": "6 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Functional near-infrared spectroscopy (fNIRS) is a non-invasive, low-cost\nmethod used to study the brain's blood flow pattern. Such patterns can enable\nus to classify performed by a subject. In recent research, most classification\nsystems use traditional machine learning algorithms for the classification of\ntasks. These methods, which are easier to implement, usually suffer from low\naccuracy. Further, a complex pre-processing phase is required for data\npreparation before implementing traditional machine learning methods. The\nproposed system uses a Bi-Directional LSTM based deep learning architecture for\ntask classification, including mental arithmetic, motor imagery, and idle state\nusing fNIRS data. Further, this system will require less pre-processing than\nthe traditional approach, saving time and computational resources while\nobtaining an accuracy of 81.48\\%, which is considerably higher than the\naccuracy obtained using conventional machine learning algorithms for the same\ndata set.\n", "versions": [{"version": "v1", "created": "Thu, 14 Jan 2021 22:21:15 GMT"}], "update_date": "2021-01-18", "authors_parsed": [["Wickramaratne", "Sajila D.", ""], ["Mahmud", "MD Shaad", ""]]}, {"id": "2101.05916", "submitter": "Jason Choi", "authors": "Sylvia Herbert, Jason J. Choi, Suvansh Sanjeev, Marsalis Gibson,\n  Koushil Sreenath, Claire J. Tomlin", "title": "Scalable Learning of Safety Guarantees for Autonomous Systems using\n  Hamilton-Jacobi Reachability", "comments": "The first two authors are co-first authors. ICRA 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.LG cs.SY eess.SY", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Autonomous systems like aircraft and assistive robots often operate in\nscenarios where guaranteeing safety is critical. Methods like Hamilton-Jacobi\nreachability can provide guaranteed safe sets and controllers for such systems.\nHowever, often these same scenarios have unknown or uncertain environments,\nsystem dynamics, or predictions of other agents. As the system is operating, it\nmay learn new knowledge about these uncertainties and should therefore update\nits safety analysis accordingly. However, work to learn and update safety\nanalysis is limited to small systems of about two dimensions due to the\ncomputational complexity of the analysis. In this paper we synthesize several\ntechniques to speed up computation: decomposition, warm-starting, and adaptive\ngrids. Using this new framework we can update safe sets by one or more orders\nof magnitude faster than prior work, making this technique practical for many\nrealistic systems. We demonstrate our results on simulated 2D and 10D\nnear-hover quadcopters operating in a windy environment.\n", "versions": [{"version": "v1", "created": "Fri, 15 Jan 2021 00:13:01 GMT"}, {"version": "v2", "created": "Fri, 2 Apr 2021 22:47:23 GMT"}], "update_date": "2021-04-06", "authors_parsed": [["Herbert", "Sylvia", ""], ["Choi", "Jason J.", ""], ["Sanjeev", "Suvansh", ""], ["Gibson", "Marsalis", ""], ["Sreenath", "Koushil", ""], ["Tomlin", "Claire J.", ""]]}, {"id": "2101.05917", "submitter": "Pingchuan Ma", "authors": "Tao Du, Kui Wu, Pingchuan Ma, Sebastien Wah, Andrew Spielberg, Daniela\n  Rus, Wojciech Matusik", "title": "DiffPD: Differentiable Projective Dynamics", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a novel, fast differentiable simulator for soft-body learning and\ncontrol applications. Existing differentiable soft-body simulators can be\nclassified into two categories based on their time integration methods:\nSimulators using explicit time-stepping scheme require tiny time steps to avoid\nnumerical instabilities in gradient computation, and simulators using implicit\ntime integration typically compute gradients by employing the adjoint method\nand solving the expensive linearized dynamics. Inspired by Projective Dynamics\n(PD), we present Differentiable Projective Dynamics (DiffPD), an efficient\ndifferentiable soft-body simulator based on PD with implicit time integration.\nThe key idea in DiffPD is to speed up backpropagation by exploiting the\nprefactorized Cholesky decomposition in forward PD simulation. In terms of\ncontact handling, DiffPD supports two types of contacts: a penalty-based model\ndescribing contact and friction forces and a complementarity-based model\nenforcing non-penetration conditions and static friction. We evaluate the\nperformance of DiffPD and observe it is 4-19 times faster compared to the\nstandard Newton's method in various applications including system\nidentification, inverse design problems, trajectory optimization, and\nclosed-loop control. We also apply DiffPD in a real-to-sim example with contact\nand collisions and show its capability of reconstructing a digital twin of\nreal-world scenes.\n", "versions": [{"version": "v1", "created": "Fri, 15 Jan 2021 00:13:33 GMT"}, {"version": "v2", "created": "Wed, 9 Jun 2021 16:57:33 GMT"}], "update_date": "2021-06-10", "authors_parsed": [["Du", "Tao", ""], ["Wu", "Kui", ""], ["Ma", "Pingchuan", ""], ["Wah", "Sebastien", ""], ["Spielberg", "Andrew", ""], ["Rus", "Daniela", ""], ["Matusik", "Wojciech", ""]]}, {"id": "2101.05924", "submitter": "Shomik Jain", "authors": "Shomik Jain, Davide Proserpio, Giovanni Quattrone, Daniele Quercia", "title": "Nowcasting Gentrification Using Airbnb Data", "comments": "To appear in the proceedings of the ACM Conference on\n  Computer-Supported Cooperative Work and Social Computing (CSCW 2021)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There is a rumbling debate over the impact of gentrification: presumed\ngentrifiers have been the target of protests and attacks in some cities, while\nthey have been welcome as generators of new jobs and taxes in others. Census\ndata fails to measure neighborhood change in real-time since it is usually\nupdated every ten years. This work shows that Airbnb data can be used to\nquantify and track neighborhood changes. Specifically, we consider both\nstructured data (e.g. number of listings, number of reviews, listing\ninformation) and unstructured data (e.g. user-generated reviews processed with\nnatural language processing and machine learning algorithms) for three major\ncities, New York City (US), Los Angeles (US), and Greater London (UK). We find\nthat Airbnb data (especially its unstructured part) appears to nowcast\nneighborhood gentrification, measured as changes in housing affordability and\ndemographics. Overall, our results suggest that user-generated data from online\nplatforms can be used to create socioeconomic indices to complement traditional\nmeasures that are less granular, not in real-time, and more costly to obtain.\n", "versions": [{"version": "v1", "created": "Fri, 15 Jan 2021 01:08:47 GMT"}, {"version": "v2", "created": "Mon, 18 Jan 2021 16:47:17 GMT"}], "update_date": "2021-01-19", "authors_parsed": [["Jain", "Shomik", ""], ["Proserpio", "Davide", ""], ["Quattrone", "Giovanni", ""], ["Quercia", "Daniele", ""]]}, {"id": "2101.05930", "submitter": "Yige Li", "authors": "Yige Li, Xixiang Lyu, Nodens Koren, Lingjuan Lyu, Bo Li, Xingjun Ma", "title": "Neural Attention Distillation: Erasing Backdoor Triggers from Deep\n  Neural Networks", "comments": "19 pages, 14 figures, ICLR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural networks (DNNs) are known vulnerable to backdoor attacks, a\ntraining time attack that injects a trigger pattern into a small proportion of\ntraining data so as to control the model's prediction at the test time.\nBackdoor attacks are notably dangerous since they do not affect the model's\nperformance on clean examples, yet can fool the model to make incorrect\nprediction whenever the trigger pattern appears during testing. In this paper,\nwe propose a novel defense framework Neural Attention Distillation (NAD) to\nerase backdoor triggers from backdoored DNNs. NAD utilizes a teacher network to\nguide the finetuning of the backdoored student network on a small clean subset\nof data such that the intermediate-layer attention of the student network\naligns with that of the teacher network. The teacher network can be obtained by\nan independent finetuning process on the same clean subset. We empirically\nshow, against 6 state-of-the-art backdoor attacks, NAD can effectively erase\nthe backdoor triggers using only 5\\% clean training data without causing\nobvious performance degradation on clean examples. Code is available in\nhttps://github.com/bboylyg/NAD.\n", "versions": [{"version": "v1", "created": "Fri, 15 Jan 2021 01:35:22 GMT"}, {"version": "v2", "created": "Wed, 27 Jan 2021 06:23:25 GMT"}], "update_date": "2021-01-28", "authors_parsed": [["Li", "Yige", ""], ["Lyu", "Xixiang", ""], ["Koren", "Nodens", ""], ["Lyu", "Lingjuan", ""], ["Li", "Bo", ""], ["Ma", "Xingjun", ""]]}, {"id": "2101.05938", "submitter": "Jing Jin", "authors": "Jing Jin, Cai Liang, Tiancheng Wu, Liqin Zou, Zhiliang Gan", "title": "KDLSQ-BERT: A Quantized Bert Combining Knowledge Distillation with\n  Learned Step Size Quantization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Recently, transformer-based language models such as BERT have shown\ntremendous performance improvement for a range of natural language processing\ntasks. However, these language models usually are computation expensive and\nmemory intensive during inference. As a result, it is difficult to deploy them\non resource-restricted devices. To improve the inference performance, as well\nas reduce the model size while maintaining the model accuracy, we propose a\nnovel quantization method named KDLSQ-BERT that combines knowledge distillation\n(KD) with learned step size quantization (LSQ) for language model quantization.\nThe main idea of our method is that the KD technique is leveraged to transfer\nthe knowledge from a \"teacher\" model to a \"student\" model when exploiting LSQ\nto quantize that \"student\" model during the quantization training process.\nExtensive experiment results on GLUE benchmark and SQuAD demonstrate that our\nproposed KDLSQ-BERT not only performs effectively when doing different bit\n(e.g. 2-bit $\\sim$ 8-bit) quantization, but also outperforms the existing BERT\nquantization methods, and even achieves comparable performance as the\nfull-precision base-line model while obtaining 14.9x compression ratio. Our\ncode will be public available.\n", "versions": [{"version": "v1", "created": "Fri, 15 Jan 2021 02:21:28 GMT"}], "update_date": "2021-01-18", "authors_parsed": [["Jin", "Jing", ""], ["Liang", "Cai", ""], ["Wu", "Tiancheng", ""], ["Zou", "Liqin", ""], ["Gan", "Zhiliang", ""]]}, {"id": "2101.05950", "submitter": "Xiaoyang Wang", "authors": "Xiaoyang Wang, Bo Li, Yibo Zhang, Bhavya Kailkhura, Klara Nahrstedt", "title": "Robusta: Robust AutoML for Feature Selection via Reinforcement Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Several AutoML approaches have been proposed to automate the machine learning\n(ML) process, such as searching for the ML model architectures and\nhyper-parameters. However, these AutoML pipelines only focus on improving the\nlearning accuracy of benign samples while ignoring the ML model robustness\nunder adversarial attacks. As ML systems are increasingly being used in a\nvariety of mission-critical applications, improving the robustness of ML\nsystems has become of utmost importance. In this paper, we propose the first\nrobust AutoML framework, Robusta--based on reinforcement learning (RL)--to\nperform feature selection, aiming to select features that lead to both accurate\nand robust ML systems. We show that a variation of the 0-1 robust loss can be\ndirectly optimized via an RL-based combinatorial search in the feature\nselection scenario. In addition, we employ heuristics to accelerate the search\nprocedure based on feature scoring metrics, which are mutual information\nscores, tree-based classifiers feature importance scores, F scores, and\nIntegrated Gradient (IG) scores, as well as their combinations. We conduct\nextensive experiments and show that the proposed framework is able to improve\nthe model robustness by up to 22% while maintaining competitive accuracy on\nbenign samples compared with other feature selection methods.\n", "versions": [{"version": "v1", "created": "Fri, 15 Jan 2021 03:12:29 GMT"}], "update_date": "2021-01-18", "authors_parsed": [["Wang", "Xiaoyang", ""], ["Li", "Bo", ""], ["Zhang", "Yibo", ""], ["Kailkhura", "Bhavya", ""], ["Nahrstedt", "Klara", ""]]}, {"id": "2101.05952", "submitter": "Beibei Zhang", "authors": "Beibei Zhang, Tian Xiang, Hongxuan Zhang, Te Li, Shiqiang Zhu, Jianjun\n  Gu", "title": "Dynamic DNN Decomposition for Lossless Synergistic Inference", "comments": "11 pages, 13 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.LG cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural networks (DNNs) sustain high performance in today's data\nprocessing applications. DNN inference is resource-intensive thus is difficult\nto fit into a mobile device. An alternative is to offload the DNN inference to\na cloud server. However, such an approach requires heavy raw data transmission\nbetween the mobile device and the cloud server, which is not suitable for\nmission-critical and privacy-sensitive applications such as autopilot. To solve\nthis problem, recent advances unleash DNN services using the edge computing\nparadigm. The existing approaches split a DNN into two parts and deploy the two\npartitions to computation nodes at two edge computing tiers. Nonetheless, these\nmethods overlook collaborative device-edge-cloud computation resources.\nBesides, previous algorithms demand the whole DNN re-partitioning to adapt to\ncomputation resource changes and network dynamics. Moreover, for\nresource-demanding convolutional layers, prior works do not give a parallel\nprocessing strategy without loss of accuracy at the edge side. To tackle these\nissues, we propose D3, a dynamic DNN decomposition system for synergistic\ninference without precision loss. The proposed system introduces a heuristic\nalgorithm named horizontal partition algorithm to split a DNN into three parts.\nThe algorithm can partially adjust the partitions at run time according to\nprocessing time and network conditions. At the edge side, a vertical separation\nmodule separates feature maps into tiles that can be independently run on\ndifferent edge nodes in parallel. Extensive quantitative evaluation of five\npopular DNNs illustrates that D3 outperforms the state-of-the-art counterparts\nup to 3.4 times in end-to-end DNN inference time and reduces backbone network\ncommunication overhead up to 3.68 times.\n", "versions": [{"version": "v1", "created": "Fri, 15 Jan 2021 03:18:53 GMT"}], "update_date": "2021-01-18", "authors_parsed": [["Zhang", "Beibei", ""], ["Xiang", "Tian", ""], ["Zhang", "Hongxuan", ""], ["Li", "Te", ""], ["Zhu", "Shiqiang", ""], ["Gu", "Jianjun", ""]]}, {"id": "2101.05953", "submitter": "Sundeep Teki", "authors": "Ayush Gupta, Rohan Sukumaran, Kevin John, Sundeep Teki", "title": "Hostility Detection and Covid-19 Fake News Detection in Social Media", "comments": "13 pages, 3 figures, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Withtheadventofsocialmedia,therehasbeenanextremely rapid increase in the\ncontent shared online. Consequently, the propagation of fake news and hostile\nmessages on social media platforms has also skyrocketed. In this paper, we\naddress the problem of detecting hostile and fake content in the Devanagari\n(Hindi) script as a multi-class, multi-label problem. Using NLP techniques, we\nbuild a model that makes use of an abusive language detector coupled with\nfeatures extracted via Hindi BERT and Hindi FastText models and metadata. Our\nmodel achieves a 0.97 F1 score on coarse grain evaluation on Hostility\ndetection task. Additionally, we built models to identify fake news related to\nCovid-19 in English tweets. We leverage entity information extracted from the\ntweets along with textual representations learned from word embeddings and\nachieve a 0.93 F1 score on the English fake news detection task.\n", "versions": [{"version": "v1", "created": "Fri, 15 Jan 2021 03:24:36 GMT"}], "update_date": "2021-01-18", "authors_parsed": [["Gupta", "Ayush", ""], ["Sukumaran", "Rohan", ""], ["John", "Kevin", ""], ["Teki", "Sundeep", ""]]}, {"id": "2101.05958", "submitter": "Ahmed Attia", "authors": "Ahmed Attia and Sven Leyffer and Todd Munson", "title": "Stochastic Learning Approach to Binary Optimization for Optimal Design\n  of Experiments", "comments": "34 pages, 12 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a novel stochastic approach to binary optimization for optimal\nexperimental design (OED) for Bayesian inverse problems governed by\nmathematical models such as partial differential equations. The OED utility\nfunction, namely, the regularized optimality criterion, is cast into a\nstochastic objective function in the form of an expectation over a multivariate\nBernoulli distribution. The probabilistic objective is then solved by using a\nstochastic optimization routine to find an optimal observational policy. The\nproposed approach is analyzed from an optimization perspective and also from a\nmachine learning perspective with correspondence to policy gradient\nreinforcement learning. The approach is demonstrated numerically by using an\nidealized two-dimensional Bayesian linear inverse problem, and validated by\nextensive numerical experiments carried out for sensor placement in a parameter\nidentification setup.\n", "versions": [{"version": "v1", "created": "Fri, 15 Jan 2021 03:54:12 GMT"}], "update_date": "2021-01-18", "authors_parsed": [["Attia", "Ahmed", ""], ["Leyffer", "Sven", ""], ["Munson", "Todd", ""]]}, {"id": "2101.05960", "submitter": "Yash Narayan", "authors": "Yash Narayan", "title": "DeepWaste: Applying Deep Learning to Waste Classification for a\n  Sustainable Planet", "comments": null, "journal-ref": "Tackling Climate Change with Machine Learning at NeurIPS 2020", "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Accurate waste disposal, at the point of disposal, is crucial to fighting\nclimate change. When materials that could be recycled or composted get diverted\ninto landfills, they cause the emission of potent greenhouse gases such as\nmethane. Current attempts to reduce erroneous waste disposal are expensive,\ninaccurate, and confusing. In this work, we propose DeepWaste, an easy-to-use\nmobile app, that utilizes highly optimized deep learning techniques to provide\nusers instantaneous waste classification into trash, recycling, and compost. We\nexperiment with several convolution neural network architectures to detect and\nclassify waste items. Our best model, a deep learning residual neural network\nwith 50 layers, achieves an average precision of 0.881 on the test set. We\ndemonstrate the performance and efficiency of our app on a set of real-world\nimages.\n", "versions": [{"version": "v1", "created": "Fri, 15 Jan 2021 04:06:25 GMT"}], "update_date": "2021-01-18", "authors_parsed": [["Narayan", "Yash", ""]]}, {"id": "2101.05961", "submitter": "Manish Shetty Molahalli", "authors": "Manish Shetty, Chetan Bansal, Sumit Kumar, Nikitha Rao, Nachiappan\n  Nagappan", "title": "SoftNER: Mining Knowledge Graphs From Cloud Incidents", "comments": "arXiv admin note: substantial text overlap with arXiv:2007.05505", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SE cs.AI cs.DC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The move from boxed products to services and the widespread adoption of cloud\ncomputing has had a huge impact on the software development life cycle and\nDevOps processes. Particularly, incident management has become critical for\ndeveloping and operating large-scale services. Prior work on incident\nmanagement has heavily focused on the challenges with incident triaging and\nde-duplication. In this work, we address the fundamental problem of structured\nknowledge extraction from service incidents. We have built SoftNER, a framework\nfor mining Knowledge Graphs from incident reports. First, we build a novel\nmulti-task learning based BiLSTM-CRF model which leverages not just the\nsemantic context but also the data-types for extracting factual information in\nthe form of named entities. Next, we present an approach to mine relations\nbetween the named entities for automatically constructing knowledge graphs. We\nhave deployed SoftNER at Microsoft, a major cloud service provider and have\nevaluated it on more than 2 months of cloud incidents. We show that the\nunsupervised machine learning pipeline has a high precision of 0.96. Our\nmulti-task learning based deep learning model also outperforms the\nstate-of-the-art NER models. Lastly, using the knowledge extracted by SoftNER,\nwe are able to build accurate models for applications such as incident triaging\nand recommending entities based on their relevance to incident titles.\n", "versions": [{"version": "v1", "created": "Fri, 15 Jan 2021 04:15:26 GMT"}, {"version": "v2", "created": "Wed, 23 Jun 2021 08:35:29 GMT"}], "update_date": "2021-06-24", "authors_parsed": [["Shetty", "Manish", ""], ["Bansal", "Chetan", ""], ["Kumar", "Sumit", ""], ["Rao", "Nikitha", ""], ["Nagappan", "Nachiappan", ""]]}, {"id": "2101.05967", "submitter": "Steven Whang", "authors": "Steven Euijong Whang, Ki Hyun Tae, Yuji Roh, Geon Heo", "title": "Responsible AI Challenges in End-to-end Machine Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Responsible AI is becoming critical as AI is widely used in our everyday\nlives. Many companies that deploy AI publicly state that when training a model,\nwe not only need to improve its accuracy, but also need to guarantee that the\nmodel does not discriminate against users (fairness), is resilient to noisy or\npoisoned data (robustness), is explainable, and more. In addition, these\nobjectives are not only relevant to model training, but to all steps of\nend-to-end machine learning, which include data collection, data cleaning and\nvalidation, model training, model evaluation, and model management and serving.\nFinally, responsible AI is conceptually challenging, and supporting all the\nobjectives must be as easy as possible. We thus propose three key research\ndirections towards this vision - depth, breadth, and usability - to measure\nprogress and introduce our ongoing research. First, responsible AI must be\ndeeply supported where multiple objectives like fairness and robust must be\nhandled together. To this end, we propose FR-Train, a holistic framework for\nfair and robust model training in the presence of data bias and poisoning.\nSecond, responsible AI must be broadly supported, preferably in all steps of\nmachine learning. Currently we focus on the data pre-processing steps and\npropose Slice Tuner, a selective data acquisition framework for training fair\nand accurate models, and MLClean, a data cleaning framework that also improves\nfairness and robustness. Finally, responsible AI must be usable where the\ntechniques must be easy to deploy and actionable. We propose FairBatch, a batch\nselection approach for fairness that is effective and simple to use, and Slice\nFinder, a model evaluation tool that automatically finds problematic slices. We\nbelieve we scratched the surface of responsible AI for end-to-end machine\nlearning and suggest research challenges moving forward.\n", "versions": [{"version": "v1", "created": "Fri, 15 Jan 2021 04:55:03 GMT"}], "update_date": "2021-01-18", "authors_parsed": [["Whang", "Steven Euijong", ""], ["Tae", "Ki Hyun", ""], ["Roh", "Yuji", ""], ["Heo", "Geon", ""]]}, {"id": "2101.05970", "submitter": "Tanmay Agarwal", "authors": "Tanmay Agarwal, Hitesh Arora, Jeff Schneider", "title": "Affordance-based Reinforcement Learning for Urban Driving", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.RO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Traditional autonomous vehicle pipelines that follow a modular approach have\nbeen very successful in the past both in academia and industry, which has led\nto autonomy deployed on road. Though this approach provides ease of\ninterpretation, its generalizability to unseen environments is limited and\nhand-engineering of numerous parameters is required, especially in the\nprediction and planning systems. Recently, deep reinforcement learning has been\nshown to learn complex strategic games and perform challenging robotic tasks,\nwhich provides an appealing framework for learning to drive. In this work, we\npropose a deep reinforcement learning framework to learn optimal control policy\nusing waypoints and low-dimensional visual representations, also known as\naffordances. We demonstrate that our agents when trained from scratch learn the\ntasks of lane-following, driving around inter-sections as well as stopping in\nfront of other actors or traffic lights even in the dense traffic setting. We\nnote that our method achieves comparable or better performance than the\nbaseline methods on the original and NoCrash benchmarks on the CARLA simulator.\n", "versions": [{"version": "v1", "created": "Fri, 15 Jan 2021 05:21:25 GMT"}], "update_date": "2021-01-18", "authors_parsed": [["Agarwal", "Tanmay", ""], ["Arora", "Hitesh", ""], ["Schneider", "Jeff", ""]]}, {"id": "2101.05974", "submitter": "Yanbang Wang", "authors": "Yanbang Wang, Yen-Yu Chang, Yunyu Liu, Jure Leskovec, Pan Li", "title": "Inductive Representation Learning in Temporal Networks via Causal\n  Anonymous Walks", "comments": "Accepted at ICLR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.SI stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Temporal networks serve as abstractions of many real-world dynamic systems.\nThese networks typically evolve according to certain laws, such as the law of\ntriadic closure, which is universal in social networks. Inductive\nrepresentation learning of temporal networks should be able to capture such\nlaws and further be applied to systems that follow the same laws but have not\nbeen unseen during the training stage. Previous works in this area depend on\neither network node identities or rich edge attributes and typically fail to\nextract these laws. Here, we propose Causal Anonymous Walks (CAWs) to\ninductively represent a temporal network. CAWs are extracted by temporal random\nwalks and work as automatic retrieval of temporal network motifs to represent\nnetwork dynamics while avoiding the time-consuming selection and counting of\nthose motifs. CAWs adopt a novel anonymization strategy that replaces node\nidentities with the hitting counts of the nodes based on a set of sampled walks\nto keep the method inductive, and simultaneously establish the correlation\nbetween motifs. We further propose a neural-network model CAW-N to encode CAWs,\nand pair it with a CAW sampling strategy with constant memory and time cost to\nsupport online training and inference. CAW-N is evaluated to predict links over\n6 real temporal networks and uniformly outperforms previous SOTA methods by\naveraged 10% AUC gain in the inductive setting. CAW-N also outperforms previous\nmethods in 4 out of the 6 networks in the transductive setting.\n", "versions": [{"version": "v1", "created": "Fri, 15 Jan 2021 05:47:26 GMT"}, {"version": "v2", "created": "Sat, 20 Feb 2021 05:29:12 GMT"}, {"version": "v3", "created": "Wed, 21 Apr 2021 03:38:47 GMT"}, {"version": "v4", "created": "Mon, 26 Apr 2021 04:26:19 GMT"}], "update_date": "2021-04-27", "authors_parsed": [["Wang", "Yanbang", ""], ["Chang", "Yen-Yu", ""], ["Liu", "Yunyu", ""], ["Leskovec", "Jure", ""], ["Li", "Pan", ""]]}, {"id": "2101.05982", "submitter": "Che Wang", "authors": "Xinyue Chen, Che Wang, Zijian Zhou, Keith Ross", "title": "Randomized Ensembled Double Q-Learning: Learning Fast Without a Model", "comments": "Published as a conference paper at ICLR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Using a high Update-To-Data (UTD) ratio, model-based methods have recently\nachieved much higher sample efficiency than previous model-free methods for\ncontinuous-action DRL benchmarks. In this paper, we introduce a simple\nmodel-free algorithm, Randomized Ensembled Double Q-Learning (REDQ), and show\nthat its performance is just as good as, if not better than, a state-of-the-art\nmodel-based algorithm for the MuJoCo benchmark. Moreover, REDQ can achieve this\nperformance using fewer parameters than the model-based method, and with less\nwall-clock run time. REDQ has three carefully integrated ingredients which\nallow it to achieve its high performance: (i) a UTD ratio >> 1; (ii) an\nensemble of Q functions; (iii) in-target minimization across a random subset of\nQ functions from the ensemble. Through carefully designed experiments, we\nprovide a detailed analysis of REDQ and related model-free algorithms. To our\nknowledge, REDQ is the first successful model-free DRL algorithm for\ncontinuous-action spaces using a UTD ratio >> 1.\n", "versions": [{"version": "v1", "created": "Fri, 15 Jan 2021 06:25:58 GMT"}, {"version": "v2", "created": "Thu, 18 Mar 2021 03:42:07 GMT"}], "update_date": "2021-03-19", "authors_parsed": [["Chen", "Xinyue", ""], ["Wang", "Che", ""], ["Zhou", "Zijian", ""], ["Ross", "Keith", ""]]}, {"id": "2101.05986", "submitter": "Haoyang Bi", "authors": "Haoyang Bi, Haiping Ma, Zhenya Huang, Yu Yin, Qi Liu, Enhong Chen, Yu\n  Su, Shijin Wang", "title": "Quality meets Diversity: A Model-Agnostic Framework for Computerized\n  Adaptive Testing", "comments": "Accepted by ICDM'2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CY cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Computerized Adaptive Testing (CAT) is emerging as a promising testing\napplication in many scenarios, such as education, game and recruitment, which\ntargets at diagnosing the knowledge mastery levels of examinees on required\nconcepts. It shows the advantage of tailoring a personalized testing procedure\nfor each examinee, which selects questions step by step, depending on her\nperformance. While there are many efforts on developing CAT systems, existing\nsolutions generally follow an inflexible model-specific fashion. That is, they\nneed to observe a specific cognitive model which can estimate examinee's\nknowledge levels and design the selection strategy according to the model\nestimation. In this paper, we study a novel model-agnostic CAT problem, where\nwe aim to propose a flexible framework that can adapt to different cognitive\nmodels. Meanwhile, this work also figures out CAT solution with addressing the\nproblem of how to generate both high-quality and diverse questions\nsimultaneously, which can give a comprehensive knowledge diagnosis for each\nexaminee. Inspired by Active Learning, we propose a novel framework, namely\nModel-Agnostic Adaptive Testing (MAAT) for CAT solution, where we design three\nsophisticated modules including Quality Module, Diversity Module and Importance\nModule. Extensive experimental results on two real-world datasets clearly\ndemonstrate that our MAAT can support CAT with guaranteeing both quality and\ndiversity perspectives.\n", "versions": [{"version": "v1", "created": "Fri, 15 Jan 2021 06:48:50 GMT"}], "update_date": "2021-01-18", "authors_parsed": [["Bi", "Haoyang", ""], ["Ma", "Haiping", ""], ["Huang", "Zhenya", ""], ["Yin", "Yu", ""], ["Liu", "Qi", ""], ["Chen", "Enhong", ""], ["Su", "Yu", ""], ["Wang", "Shijin", ""]]}, {"id": "2101.05993", "submitter": "Guangtao Wang", "authors": "Guangtao Wang, Qinbao Song and Xiaoyan Zhu", "title": "Ensemble Learning Based Classification Algorithm Recommendation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recommending appropriate algorithms to a classification problem is one of the\nmost challenging issues in the field of data mining. The existing algorithm\nrecommendation models are generally constructed on only one kind of\nmeta-features by single learners. Considering that i) ensemble learners usually\nshow better performance and ii) different kinds of meta-features characterize\nthe classification problems in different viewpoints independently, and further\nthe models constructed with different sets of meta-features will be\ncomplementary with each other and applicable for ensemble. This paper proposes\nan ensemble learning-based algorithm recommendation method. To evaluate the\nproposed recommendation method, extensive experiments with 13 well-known\ncandidate classification algorithms and five different kinds of meta-features\nare conducted on 1090 benchmark classification problems. The results show the\neffectiveness of the proposed ensemble learning based recommendation method.\n", "versions": [{"version": "v1", "created": "Fri, 15 Jan 2021 07:14:51 GMT"}], "update_date": "2021-06-08", "authors_parsed": [["Wang", "Guangtao", ""], ["Song", "Qinbao", ""], ["Zhu", "Xiaoyan", ""]]}, {"id": "2101.06006", "submitter": "Binxu Wang", "authors": "Binxu Wang, Carlos R. Ponce", "title": "The Geometry of Deep Generative Image Models and its Applications", "comments": "25 pages, 11 figures. Published as a conference paper at ICLR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NA cs.NE math.NA", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Generative adversarial networks (GANs) have emerged as a powerful\nunsupervised method to model the statistical patterns of real-world data sets,\nsuch as natural images. These networks are trained to map random inputs in\ntheir latent space to new samples representative of the learned data. However,\nthe structure of the latent space is hard to intuit due to its high\ndimensionality and the non-linearity of the generator, which limits the\nusefulness of the models. Understanding the latent space requires a way to\nidentify input codes for existing real-world images (inversion), and a way to\nidentify directions with known image transformations (interpretability). Here,\nwe use a geometric framework to address both issues simultaneously. We develop\nan architecture-agnostic method to compute the Riemannian metric of the image\nmanifold created by GANs. The eigen-decomposition of the metric isolates axes\nthat account for different levels of image variability. An empirical analysis\nof several pretrained GANs shows that image variation around each position is\nconcentrated along surprisingly few major axes (the space is highly\nanisotropic) and the directions that create this large variation are similar at\ndifferent positions in the space (the space is homogeneous). We show that many\nof the top eigenvectors correspond to interpretable transforms in the image\nspace, with a substantial part of eigenspace corresponding to minor transforms\nwhich could be compressed out. This geometric understanding unifies key\nprevious results related to GAN interpretability. We show that the use of this\nmetric allows for more efficient optimization in the latent space (e.g. GAN\ninversion) and facilitates unsupervised discovery of interpretable axes. Our\nresults illustrate that defining the geometry of the GAN image manifold can\nserve as a general framework for understanding GANs.\n", "versions": [{"version": "v1", "created": "Fri, 15 Jan 2021 07:57:33 GMT"}, {"version": "v2", "created": "Thu, 18 Mar 2021 08:24:26 GMT"}], "update_date": "2021-03-19", "authors_parsed": [["Wang", "Binxu", ""], ["Ponce", "Carlos R.", ""]]}, {"id": "2101.06013", "submitter": "Violetta Shevchenko", "authors": "Violetta Shevchenko, Damien Teney, Anthony Dick, Anton van den Hengel", "title": "Reasoning over Vision and Language: Exploring the Benefits of\n  Supplemental Knowledge", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The limits of applicability of vision-and-language models are defined by the\ncoverage of their training data. Tasks like vision question answering (VQA)\noften require commonsense and factual information beyond what can be learned\nfrom task-specific datasets. This paper investigates the injection of knowledge\nfrom general-purpose knowledge bases (KBs) into vision-and-language\ntransformers. We use an auxiliary training objective that encourages the\nlearned representations to align with graph embeddings of matching entities in\na KB. We empirically study the relevance of various KBs to multiple tasks and\nbenchmarks. The technique brings clear benefits to knowledge-demanding question\nanswering tasks (OK-VQA, FVQA) by capturing semantic and relational knowledge\nabsent from existing models. More surprisingly, the technique also benefits\nvisual reasoning tasks (NLVR2, SNLI-VE). We perform probing experiments and\nshow that the injection of additional knowledge regularizes the space of\nembeddings, which improves the representation of lexical and semantic\nsimilarities. The technique is model-agnostic and can expand the applicability\nof any vision-and-language transformer with minimal computational overhead.\n", "versions": [{"version": "v1", "created": "Fri, 15 Jan 2021 08:37:55 GMT"}], "update_date": "2021-01-18", "authors_parsed": [["Shevchenko", "Violetta", ""], ["Teney", "Damien", ""], ["Dick", "Anthony", ""], ["Hengel", "Anton van den", ""]]}, {"id": "2101.06040", "submitter": "Evangelos Mazomenos", "authors": "Patrick Brandao, Odysseas Zisimopoulos, Evangelos Mazomenos, Gastone\n  Ciuti, Jorge Bernal, Marco Visentini-Scarzanella, Arianna Menciassi, Paolo\n  Dario, Anastasios Koulaouzidis, Alberto Arezzo, David J Hawkes, Danail\n  Stoyanov", "title": "Towards a Computed-Aided Diagnosis System in Colonoscopy: Automatic\n  Polyp Segmentation Using Convolution Neural Networks", "comments": "10 pages, 6 figures", "journal-ref": "Journal of Medical Robotics Research, Volume 03, No. 02, 1840002\n  (2018) G", "doi": "10.1142/S2424905X18400020", "report-no": null, "categories": "cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Early diagnosis is essential for the successful treatment of bowel cancers\nincluding colorectal cancer (CRC) and capsule endoscopic imaging with robotic\nactuation can be a valuable diagnostic tool when combined with automated image\nanalysis. We present a deep learning rooted detection and segmentation\nframework for recognizing lesions in colonoscopy and capsule endoscopy images.\nWe restructure established convolution architectures, such as VGG and ResNets,\nby converting them into fully-connected convolution networks (FCNs), fine-tune\nthem and study their capabilities for polyp segmentation and detection. We\nadditionally use Shape from-Shading (SfS) to recover depth and provide a richer\nrepresentation of the tissue's structure in colonoscopy images. Depth is\nincorporated into our network models as an additional input channel to the RGB\ninformation and we demonstrate that the resulting network yields improved\nperformance. Our networks are tested on publicly available datasets and the\nmost accurate segmentation model achieved a mean segmentation IU of 47.78% and\n56.95% on the ETIS-Larib and CVC-Colon datasets, respectively. For polyp\ndetection, the top performing models we propose surpass the current state of\nthe art with detection recalls superior to 90% for all datasets tested. To our\nknowledge, we present the first work to use FCNs for polyp segmentation in\naddition to proposing a novel combination of SfS and RGB that boosts\nperformance\n", "versions": [{"version": "v1", "created": "Fri, 15 Jan 2021 10:08:53 GMT"}], "update_date": "2021-01-18", "authors_parsed": [["Brandao", "Patrick", ""], ["Zisimopoulos", "Odysseas", ""], ["Mazomenos", "Evangelos", ""], ["Ciuti", "Gastone", ""], ["Bernal", "Jorge", ""], ["Visentini-Scarzanella", "Marco", ""], ["Menciassi", "Arianna", ""], ["Dario", "Paolo", ""], ["Koulaouzidis", "Anastasios", ""], ["Arezzo", "Alberto", ""], ["Hawkes", "David J", ""], ["Stoyanov", "Danail", ""]]}, {"id": "2101.06044", "submitter": "Adyasha Mohanty", "authors": "Adyasha Mohanty, Shubh Gupta and Grace Xingxin Gao", "title": "A Particle Filtering Framework for Integrity Risk of GNSS-Camera Sensor\n  Fusion", "comments": null, "journal-ref": "Proceedings of the 33rd International Technical Meeting of the\n  Satellite Division of The Institute of Navigation (ION GNSS+ 2020)", "doi": "10.33012/2020.17660", "report-no": null, "categories": "cs.RO cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Adopting a joint approach towards state estimation and integrity monitoring\nresults in unbiased integrity monitoring unlike traditional approaches. So far,\na joint approach was used in Particle RAIM [l] for GNSS measurements only. In\nour work, we extend Particle RAIM to a GNSS-camera fused system for joint state\nestimation and integrity monitoring. To account for vision faults, we derive a\nprobability distribution over position from camera images using map-matching.\nWe formulate a Kullback-Leibler Divergence metric to assess the consistency of\nGNSS and camera measurements and mitigate faults during sensor fusion. The\nderived integrity risk upper bounds the probability of Hazardously Misleading\nInformation (HMI). Experimental validation on a real-world dataset shows that\nour algorithm produces less than 11 m position error and the integrity risk\nover bounds the probability of HMI with 0.11 failure rate for an 8 m Alert\nLimit in an urban scenario.\n", "versions": [{"version": "v1", "created": "Fri, 15 Jan 2021 10:15:59 GMT"}], "update_date": "2021-01-18", "authors_parsed": [["Mohanty", "Adyasha", ""], ["Gupta", "Shubh", ""], ["Gao", "Grace Xingxin", ""]]}, {"id": "2101.06046", "submitter": "Axel Sauer", "authors": "Axel Sauer, Andreas Geiger", "title": "Counterfactual Generative Networks", "comments": "Published at ICLR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural networks are prone to learning shortcuts -- they often model simple\ncorrelations, ignoring more complex ones that potentially generalize better.\nPrior works on image classification show that instead of learning a connection\nto object shape, deep classifiers tend to exploit spurious correlations with\nlow-level texture or the background for solving the classification task. In\nthis work, we take a step towards more robust and interpretable classifiers\nthat explicitly expose the task's causal structure. Building on current\nadvances in deep generative modeling, we propose to decompose the image\ngeneration process into independent causal mechanisms that we train without\ndirect supervision. By exploiting appropriate inductive biases, these\nmechanisms disentangle object shape, object texture, and background; hence,\nthey allow for generating counterfactual images. We demonstrate the ability of\nour model to generate such images on MNIST and ImageNet. Further, we show that\nthe counterfactual images can improve out-of-distribution robustness with a\nmarginal drop in performance on the original classification task, despite being\nsynthetic. Lastly, our generative model can be trained efficiently on a single\nGPU, exploiting common pre-trained models as inductive biases.\n", "versions": [{"version": "v1", "created": "Fri, 15 Jan 2021 10:23:12 GMT"}], "update_date": "2021-01-18", "authors_parsed": [["Sauer", "Axel", ""], ["Geiger", "Andreas", ""]]}, {"id": "2101.06054", "submitter": "Sasho Nedelkoski", "authors": "Jasmin Bogatinovski, Sasho Nedelkoski, Alexander Acker, Florian\n  Schmidt, Thorsten Wittkopp, Soeren Becker, Jorge Cardoso, and Odej Kao", "title": "Artificial Intelligence for IT Operations (AIOPS) Workshop White Paper", "comments": "8 pages, white paper for the AIOPS 2020 workshop at ICSOC 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Artificial Intelligence for IT Operations (AIOps) is an emerging\ninterdisciplinary field arising in the intersection between the research areas\nof machine learning, big data, streaming analytics, and the management of IT\noperations. AIOps, as a field, is a candidate to produce the future standard\nfor IT operation management. To that end, AIOps has several challenges. First,\nit needs to combine separate research branches from other research fields like\nsoftware reliability engineering. Second, novel modelling techniques are needed\nto understand the dynamics of different systems. Furthermore, it requires to\nlay out the basis for assessing: time horizons and uncertainty for imminent SLA\nviolations, the early detection of emerging problems, autonomous remediation,\ndecision making, support of various optimization objectives. Moreover, a good\nunderstanding and interpretability of these aiding models are important for\nbuilding trust between the employed tools and the domain experts. Finally, all\nthis will result in faster adoption of AIOps, further increase the interest in\nthis research field and contribute to bridging the gap towards fully-autonomous\noperating IT systems.\n  The main aim of the AIOPS workshop is to bring together researchers from both\nacademia and industry to present their experiences, results, and work in\nprogress in this field. The workshop aims to strengthen the community and unite\nit towards the goal of joining the efforts for solving the main challenges the\nfield is currently facing. A consensus and adoption of the principles of\nopenness and reproducibility will boost the research in this emerging area\nsignificantly.\n", "versions": [{"version": "v1", "created": "Fri, 15 Jan 2021 10:43:10 GMT"}], "update_date": "2021-01-18", "authors_parsed": [["Bogatinovski", "Jasmin", ""], ["Nedelkoski", "Sasho", ""], ["Acker", "Alexander", ""], ["Schmidt", "Florian", ""], ["Wittkopp", "Thorsten", ""], ["Becker", "Soeren", ""], ["Cardoso", "Jorge", ""], ["Kao", "Odej", ""]]}, {"id": "2101.06061", "submitter": "Bogdan Georgiev", "authors": "Bogdan Georgiev, Lukas Franken, Mayukh Mukherjee", "title": "Heating up decision boundaries: isocapacitory saturation, adversarial\n  scenarios and generalization bounds", "comments": "Accepted as conference paper at ICLR 2021. 36 pages, 16 figures,\n  comments welcome!", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.MG math.PR math.ST stat.ML stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In the present work we study classifiers' decision boundaries via Brownian\nmotion processes in ambient data space and associated probabilistic techniques.\nIntuitively, our ideas correspond to placing a heat source at the decision\nboundary and observing how effectively the sample points warm up. We are\nlargely motivated by the search for a soft measure that sheds further light on\nthe decision boundary's geometry. En route, we bridge aspects of potential\ntheory and geometric analysis (Mazya, 2011, Grigoryan-Saloff-Coste, 2002) with\nactive fields of ML research such as adversarial examples and generalization\nbounds. First, we focus on the geometric behavior of decision boundaries in the\nlight of adversarial attack/defense mechanisms. Experimentally, we observe a\ncertain capacitory trend over different adversarial defense strategies:\ndecision boundaries locally become flatter as measured by isoperimetric\ninequalities (Ford et al, 2019); however, our more sensitive heat-diffusion\nmetrics extend this analysis and further reveal that some non-trivial geometry\ninvisible to plain distance-based methods is still preserved. Intuitively, we\nprovide evidence that the decision boundaries nevertheless retain many\npersistent \"wiggly and fuzzy\" regions on a finer scale. Second, we show how\nBrownian hitting probabilities translate to soft generalization bounds which\nare in turn connected to compression and noise stability (Arora et al, 2018),\nand these bounds are significantly stronger if the decision boundary has\ncontrolled geometric features.\n", "versions": [{"version": "v1", "created": "Fri, 15 Jan 2021 11:15:51 GMT"}], "update_date": "2021-01-18", "authors_parsed": [["Georgiev", "Bogdan", ""], ["Franken", "Lukas", ""], ["Mukherjee", "Mayukh", ""]]}, {"id": "2101.06069", "submitter": "Gaurav Kumar Nayak", "authors": "Gaurav Kumar Nayak, Konda Reddy Mopuri, Saksham Jain, Anirban\n  Chakraborty", "title": "Mining Data Impressions from Deep Models as Substitute for the\n  Unavailable Training Data", "comments": "PAMI Submission (Under Review). arXiv admin note: text overlap with\n  arXiv:1905.08114", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Pretrained deep models hold their learnt knowledge in the form of model\nparameters. These parameters act as \"memory\" for the trained models and help\nthem generalize well on unseen data. However, in absence of training data, the\nutility of a trained model is merely limited to either inference or better\ninitialization towards a target task. In this paper, we go further and extract\nsynthetic data by leveraging the learnt model parameters. We dub them \"Data\nImpressions\", which act as proxy to the training data and can be used to\nrealize a variety of tasks. These are useful in scenarios where only the\npretrained models are available and the training data is not shared (e.g., due\nto privacy or sensitivity concerns). We show the applicability of data\nimpressions in solving several computer vision tasks such as unsupervised\ndomain adaptation, continual learning as well as knowledge distillation. We\nalso study the adversarial robustness of lightweight models trained via\nknowledge distillation using these data impressions. Further, we demonstrate\nthe efficacy of data impressions in generating data-free Universal Adversarial\nPerturbations (UAPs) with better fooling rates. Extensive experiments performed\non benchmark datasets demonstrate competitive performance achieved using data\nimpressions in absence of original training data.\n", "versions": [{"version": "v1", "created": "Fri, 15 Jan 2021 11:37:29 GMT"}, {"version": "v2", "created": "Fri, 23 Jul 2021 13:41:36 GMT"}], "update_date": "2021-07-26", "authors_parsed": [["Nayak", "Gaurav Kumar", ""], ["Mopuri", "Konda Reddy", ""], ["Jain", "Saksham", ""], ["Chakraborty", "Anirban", ""]]}, {"id": "2101.06070", "submitter": "Hang Ren", "authors": "Vincent Moens, Hang Ren, Alexandre Maraval, Rasul Tutunov, Jun Wang,\n  Haitham Ammar", "title": "Efficient Semi-Implicit Variational Inference", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose CI-VI an efficient and scalable solver for\nsemi-implicit variational inference (SIVI). Our method, first, maps SIVI's\nevidence lower bound (ELBO) to a form involving a nonlinear functional nesting\nof expected values and then develops a rigorous optimiser capable of correctly\nhandling bias inherent to nonlinear nested expectations using an\nextrapolation-smoothing mechanism coupled with gradient sketching. Our\ntheoretical results demonstrate convergence to a stationary point of the ELBO\nin general non-convex settings typically arising when using deep network models\nand an order of $O(t^{-\\frac{4}{5}})$ gradient-bias-vanishing rate. We believe\nthese results generalise beyond the specific nesting arising from SIVI to other\nforms. Finally, in a set of experiments, we demonstrate the effectiveness of\nour algorithm in approximating complex posteriors on various data-sets\nincluding those from natural language processing.\n", "versions": [{"version": "v1", "created": "Fri, 15 Jan 2021 11:39:09 GMT"}], "update_date": "2021-01-18", "authors_parsed": [["Moens", "Vincent", ""], ["Ren", "Hang", ""], ["Maraval", "Alexandre", ""], ["Tutunov", "Rasul", ""], ["Wang", "Jun", ""], ["Ammar", "Haitham", ""]]}, {"id": "2101.06072", "submitter": "Evlampios Apostolidis", "authors": "Evlampios Apostolidis, Eleni Adamantidou, Alexandros I. Metsai,\n  Vasileios Mezaris, Ioannis Patras", "title": "Video Summarization Using Deep Neural Networks: A Survey", "comments": "Journal paper; Under review", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Video summarization technologies aim to create a concise and complete\nsynopsis by selecting the most informative parts of the video content. Several\napproaches have been developed over the last couple of decades and the current\nstate of the art is represented by methods that rely on modern deep neural\nnetwork architectures. This work focuses on the recent advances in the area and\nprovides a comprehensive survey of the existing deep-learning-based methods for\ngeneric video summarization. After presenting the motivation behind the\ndevelopment of technologies for video summarization, we formulate the video\nsummarization task and discuss the main characteristics of a typical\ndeep-learning-based analysis pipeline. Then, we suggest a taxonomy of the\nexisting algorithms and provide a systematic review of the relevant literature\nthat shows the evolution of the deep-learning-based video summarization\ntechnologies and leads to suggestions for future developments. We then report\non protocols for the objective evaluation of video summarization algorithms and\nwe compare the performance of several deep-learning-based approaches. Based on\nthe outcomes of these comparisons, as well as some documented considerations\nabout the suitability of evaluation protocols, we indicate potential future\nresearch directions.\n", "versions": [{"version": "v1", "created": "Fri, 15 Jan 2021 11:41:29 GMT"}], "update_date": "2021-01-18", "authors_parsed": [["Apostolidis", "Evlampios", ""], ["Adamantidou", "Eleni", ""], ["Metsai", "Alexandros I.", ""], ["Mezaris", "Vasileios", ""], ["Patras", "Ioannis", ""]]}, {"id": "2101.06073", "submitter": "Chuan Liu", "authors": "Chuan Liu, Yi Gao, Jiancheng Lv", "title": "Dynamic Normalization", "comments": "9 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Batch Normalization has become one of the essential components in CNN. It\nallows the network to use a higher learning rate and speed up training. And the\nnetwork doesn't need to be initialized carefully. However, in our work, we find\nthat a simple extension of BN can increase the performance of the network.\nFirst, we extend BN to adaptively generate scale and shift parameters for each\nmini-batch data, called DN-C (Batch-shared and Channel-wise). We use the\nstatistical characteristics of mini-batch data ($E[X],\nStd[X]\\in\\mathbb{R}^{c}$) as the input of SC module. Then we extend BN to\nadaptively generate scale and shift parameters for each channel of each sample,\ncalled DN-B (Batch and Channel-wise). Our experiments show that DN-C model\ncan't train normally, but DN-B model has very good robustness. In\nclassification task, DN-B can improve the accuracy of the MobileNetV2 on\nImageNet-100 more than 2% with only 0.6% additional Mult-Adds. In detection\ntask, DN-B can improve the accuracy of the SSDLite on MS-COCO nearly 4% mAP\nwith the same settings. Compared with BN, DN-B has stable performance when\nusing higher learning rate or smaller batch size.\n", "versions": [{"version": "v1", "created": "Fri, 15 Jan 2021 11:41:41 GMT"}], "update_date": "2021-01-18", "authors_parsed": [["Liu", "Chuan", ""], ["Gao", "Yi", ""], ["Lv", "Jiancheng", ""]]}, {"id": "2101.06100", "submitter": "Tiago Ferreira", "authors": "Tiago A. E. Ferreira and Marios Mattheakis and Pavlos Protopapas", "title": "A New Artificial Neuron Proposal with Trainable Simultaneous Local and\n  Global Activation Function", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.LG", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  The activation function plays a fundamental role in the artificial neural\nnetwork learning process. However, there is no obvious choice or procedure to\ndetermine the best activation function, which depends on the problem. This\nstudy proposes a new artificial neuron, named global-local neuron, with a\ntrainable activation function composed of two components, a global and a local.\nThe global component term used here is relative to a mathematical function to\ndescribe a general feature present in all problem domain. The local component\nis a function that can represent a localized behavior, like a transient or a\nperturbation. This new neuron can define the importance of each activation\nfunction component in the learning phase. Depending on the problem, it results\nin a purely global, or purely local, or a mixed global and local activation\nfunction after the training phase. Here, the trigonometric sine function was\nemployed for the global component and the hyperbolic tangent for the local\ncomponent. The proposed neuron was tested for problems where the target was a\npurely global function, or purely local function, or a composition of two\nglobal and local functions. Two classes of test problems were investigated,\nregression problems and differential equations solving. The experimental tests\ndemonstrated the Global-Local Neuron network's superior performance, compared\nwith simple neural networks with sine or hyperbolic tangent activation\nfunction, and with a hybrid network that combines these two simple neural\nnetworks.\n", "versions": [{"version": "v1", "created": "Fri, 15 Jan 2021 13:34:49 GMT"}], "update_date": "2021-01-18", "authors_parsed": [["Ferreira", "Tiago A. E.", ""], ["Mattheakis", "Marios", ""], ["Protopapas", "Pavlos", ""]]}, {"id": "2101.06115", "submitter": "Ahmed Abdeljawad", "authors": "Ahmed Abdeljawad and Philipp Grohs", "title": "Approximations with deep neural networks in Sobolev time-space", "comments": "34 pages. This is the first version of the paper. It is expected that\n  some changes will be performed for the next version. arXiv admin note: text\n  overlap with arXiv:1902.07896", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.FA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Solutions of evolution equation generally lies in certain Bochner-Sobolev\nspaces, in which the solution may has regularity and integrability properties\nfor the time variable that can be different for the space variables. Therefore,\nin this paper, we develop a framework shows that deep neural networks can\napproximate Sobolev-regular functions with respect to Bochner-Sobolev spaces.\nIn our work we use the so-called Rectified Cubic Unit (ReCU) as an activation\nfunction in our networks, which allows us to deduce approximation results of\nthe neural networks while avoiding issues caused by the non regularity of the\nmost commonly used Rectivied Linear Unit (ReLU) activation function.\n", "versions": [{"version": "v1", "created": "Wed, 23 Dec 2020 22:21:05 GMT"}], "update_date": "2021-01-18", "authors_parsed": [["Abdeljawad", "Ahmed", ""], ["Grohs", "Philipp", ""]]}, {"id": "2101.06126", "submitter": "Daniel Obraczka", "authors": "Daniel Obraczka, Jonathan Schuchart, Erhard Rahm", "title": "EAGER: Embedding-Assisted Entity Resolution for Knowledge Graphs", "comments": "10 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DB", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Entity Resolution (ER) is a constitutional part for integrating different\nknowledge graphs in order to identify entities referring to the same real-world\nobject. A promising approach is the use of graph embeddings for ER in order to\ndetermine the similarity of entities based on the similarity of their graph\nneighborhood. The similarity computations for such embeddings translates to\ncalculating the distance between them in the embedding space which is\ncomparatively simple. However, previous work has shown that the use of graph\nembeddings alone is not sufficient to achieve high ER quality. We therefore\npropose a more comprehensive ER approach for knowledge graphs called EAGER\n(Embedding-Assisted Knowledge Graph Entity Resolution) to flexibly utilize both\nthe similarity of graph embeddings and attribute values within a supervised\nmachine learning approach. We evaluate our approach on 23 benchmark datasets\nwith differently sized and structured knowledge graphs and use hypothesis tests\nto ensure statistical significance of our results. Furthermore we compare our\napproach with state-of-the-art ER solutions, where our approach yields\ncompetitive results for table-oriented ER problems and shallow knowledge graphs\nbut much better results for deeper knowledge graphs.\n", "versions": [{"version": "v1", "created": "Fri, 15 Jan 2021 14:12:10 GMT"}], "update_date": "2021-01-18", "authors_parsed": [["Obraczka", "Daniel", ""], ["Schuchart", "Jonathan", ""], ["Rahm", "Erhard", ""]]}, {"id": "2101.06146", "submitter": "Niklas K\\\"uhl Dr", "authors": "Niklas K\\\"uhl and Gerhard Satzger", "title": "Needmining: Designing Digital Support to Elicit Needs from Social Media", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.LG", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Today's businesses face a high pressure to innovate in order to succeed in\nhighly competitive markets. Successful innovations, though, typically require\nthe identification and analysis of customer needs. While traditional,\nestablished need elicitation methods are time-proven and have demonstrated\ntheir capabilities to deliver valuable insights, they lack automation and\nscalability and, thus, are expensive and time-consuming. In this article, we\npropose an approach to automatically identify and quantify customer needs by\nutilizing a novel data source: Users voluntarily and publicly expose\ninformation about themselves via social media, as for instance Facebook or\nTwitter. These posts may contain valuable information about the needs, wants,\nand demands of their authors. We apply a Design Science Research (DSR)\nmethodology to add design knowledge and artifacts for the digitalization of\ninnovation processes, in particular to provide digital support for the\nelicitation of customer needs. We want to investigate whether automated,\nspeedy, and scalable need elicitation from social media is feasible. We\nconcentrate on Twitter as a data source and on e-mobility as an application\ndomain. In a first design cycle we conceive, implement and evaluate a method to\ndemonstrate the feasibility of identifying those social media posts that\nactually express customer needs. In a second cycle, we build on this artifact\nto additionally quantify the need information elicited, and prove its\nfeasibility. Third, we integrate both developed methods into an end-user\nsoftware artifact and test usability in an industrial use case. Thus, we add\nnew methods for need elicitation to the body of knowledge, and introduce\nconcrete tooling for innovation management in practice.\n", "versions": [{"version": "v1", "created": "Thu, 14 Jan 2021 14:49:19 GMT"}], "update_date": "2021-01-18", "authors_parsed": [["K\u00fchl", "Niklas", ""], ["Satzger", "Gerhard", ""]]}, {"id": "2101.06154", "submitter": "Kaifeng Bu", "authors": "Kaifeng Bu, Dax Enshan Koh, Lu Li, Qingxian Luo, Yaobo Zhang", "title": "On the statistical complexity of quantum circuits", "comments": "6+19 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "quant-ph cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In theoretical machine learning, the statistical complexity is a notion that\nmeasures the richness of a hypothesis space. In this work, we apply a\nparticular measure of statistical complexity, namely the Rademacher complexity,\nto the quantum circuit model in quantum computation and study how the\nstatistical complexity depends on various quantum circuit parameters. In\nparticular, we investigate the dependence of the statistical complexity on the\nresources, depth, width, and the number of input and output registers of a\nquantum circuit. To study how the statistical complexity scales with resources\nin the circuit, we introduce a resource measure of magic based on the $(p,q)$\ngroup norm, which quantifies the amount of magic in the quantum channels\nassociated with the circuit. These dependencies are investigated in the\nfollowing two settings: (i) where the entire quantum circuit is treated as a\nsingle quantum channel, and (ii) where each layer of the quantum circuit is\ntreated as a separate quantum channel. The bounds we obtain can be used to\nconstrain the capacity of quantum neural networks in terms of their depths and\nwidths as well as the resources in the network.\n", "versions": [{"version": "v1", "created": "Fri, 15 Jan 2021 14:55:55 GMT"}], "update_date": "2021-01-18", "authors_parsed": [["Bu", "Kaifeng", ""], ["Koh", "Dax Enshan", ""], ["Li", "Lu", ""], ["Luo", "Qingxian", ""], ["Zhang", "Yaobo", ""]]}, {"id": "2101.06162", "submitter": "Ghada Sokar", "authors": "Ghada Sokar, Decebal Constantin Mocanu, Mykola Pechenizkiy", "title": "Learning Invariant Representation for Continual Learning", "comments": "Accepted at the AAAI Meta-Learning for Computer Vision Workshop\n  (2021)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Continual learning aims to provide intelligent agents that are capable of\nlearning continually a sequence of tasks, building on previously learned\nknowledge. A key challenge in this learning paradigm is catastrophically\nforgetting previously learned tasks when the agent faces a new one. Current\nrehearsal-based methods show their success in mitigating the catastrophic\nforgetting problem by replaying samples from previous tasks during learning a\nnew one. However, these methods are infeasible when the data of previous tasks\nis not accessible. In this work, we propose a new pseudo-rehearsal-based\nmethod, named learning Invariant Representation for Continual Learning (IRCL),\nin which class-invariant representation is disentangled from a conditional\ngenerative model and jointly used with class-specific representation to learn\nthe sequential tasks. Disentangling the shared invariant representation helps\nto learn continually a sequence of tasks, while being more robust to forgetting\nand having better knowledge transfer. We focus on class incremental learning\nwhere there is no knowledge about task identity during inference. We\nempirically evaluate our proposed method on two well-known benchmarks for\ncontinual learning: split MNIST and split Fashion MNIST. The experimental\nresults show that our proposed method outperforms regularization-based methods\nby a big margin and is better than the state-of-the-art pseudo-rehearsal-based\nmethod. Finally, we analyze the role of the shared invariant representation in\nmitigating the forgetting problem especially when the number of replayed\nsamples for each previous task is small.\n", "versions": [{"version": "v1", "created": "Fri, 15 Jan 2021 15:12:51 GMT"}], "update_date": "2021-01-18", "authors_parsed": [["Sokar", "Ghada", ""], ["Mocanu", "Decebal Constantin", ""], ["Pechenizkiy", "Mykola", ""]]}, {"id": "2101.06166", "submitter": "Marcos Eduardo Valle", "authors": "Guilherme Vieira and Marcos Eduardo Valle", "title": "A General Framework for Hypercomplex-valued Extreme Learning Machines", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper aims to establish a framework for extreme learning machines (ELMs)\non general hypercomplex algebras. Hypercomplex neural networks are machine\nlearning models that feature higher-dimension numbers as parameters, inputs,\nand outputs. Firstly, we review broad hypercomplex algebras and show a\nframework to operate in these algebras through real-valued linear algebra\noperations in a robust manner. We proceed to explore a handful of well-known\nfour-dimensional examples. Then, we propose the hypercomplex-valued ELMs and\nderive their learning using a hypercomplex-valued least-squares problem.\nFinally, we compare real and hypercomplex-valued ELM models' performance in an\nexperiment on time-series prediction and another on color image auto-encoding.\nThe computational experiments highlight the excellent performance of\nhypercomplex-valued ELMs to treat high-dimensional data, including models based\non unusual hypercomplex algebras.\n", "versions": [{"version": "v1", "created": "Fri, 15 Jan 2021 15:22:05 GMT"}], "update_date": "2021-01-18", "authors_parsed": [["Vieira", "Guilherme", ""], ["Valle", "Marcos Eduardo", ""]]}, {"id": "2101.06171", "submitter": "Duc Thien Nguyen", "authors": "Duc Thien Nguyen, Shiau Hoong Lim, Laura Wynter and Desmond Cai", "title": "Probabilistic Inference for Learning from Untrusted Sources", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.DC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Federated learning brings potential benefits of faster learning, better\nsolutions, and a greater propensity to transfer when heterogeneous data from\ndifferent parties increases diversity. However, because federated learning\ntasks tend to be large and complex, and training times non-negligible, it is\nimportant for the aggregation algorithm to be robust to non-IID data and\ncorrupted parties. This robustness relies on the ability to identify, and\nappropriately weight, incompatible parties. Recent work assumes that a\n\\textit{reference dataset} is available through which to perform the\nidentification. We consider settings where no such reference dataset is\navailable; rather, the quality and suitability of the parties needs to be\n\\textit{inferred}. We do so by bringing ideas from crowdsourced predictions and\ncollaborative filtering, where one must infer an unknown ground truth given\nproposals from participants with unknown quality. We propose novel federated\nlearning aggregation algorithms based on Bayesian inference that adapt to the\nquality of the parties. Empirically, we show that the algorithms outperform\nstandard and robust aggregation in federated learning on both synthetic and\nreal data.\n", "versions": [{"version": "v1", "created": "Fri, 15 Jan 2021 15:30:06 GMT"}], "update_date": "2021-01-18", "authors_parsed": [["Nguyen", "Duc Thien", ""], ["Lim", "Shiau Hoong", ""], ["Wynter", "Laura", ""], ["Cai", "Desmond", ""]]}, {"id": "2101.06172", "submitter": "Yevgeniy Puzikov", "authors": "Yevgeniy Puzikov, Simoes Stanley, Iryna Gurevych and Immanuel\n  Schweizer", "title": "Empirical Evaluation of Supervision Signals for Style Transfer Models", "comments": "13 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Text style transfer has gained increasing attention from the research\ncommunity over the recent years. However, the proposed approaches vary in many\nways, which makes it hard to assess the individual contribution of the model\ncomponents. In style transfer, the most important component is the optimization\ntechnique used to guide the learning in the absence of parallel training data.\nIn this work we empirically compare the dominant optimization paradigms which\nprovide supervision signals during training: backtranslation, adversarial\ntraining and reinforcement learning. We find that backtranslation has\nmodel-specific limitations, which inhibits training style transfer models.\nReinforcement learning shows the best performance gains, while adversarial\ntraining, despite its popularity, does not offer an advantage over the latter\nalternative. In this work we also experiment with Minimum Risk Training, a\npopular technique in the machine translation community, which, to our\nknowledge, has not been empirically evaluated in the task of style transfer. We\nfill this research gap and empirically show its efficacy.\n", "versions": [{"version": "v1", "created": "Fri, 15 Jan 2021 15:33:30 GMT"}], "update_date": "2021-01-18", "authors_parsed": [["Puzikov", "Yevgeniy", ""], ["Stanley", "Simoes", ""], ["Gurevych", "Iryna", ""], ["Schweizer", "Immanuel", ""]]}, {"id": "2101.06178", "submitter": "Elchanan Mossel", "authors": "Ankur Moitra and Elchanan Mossel and Colin Sandon", "title": "Learning to Sample from Censored Markov Random Fields", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study learning Censor Markov Random Fields (abbreviated CMRFs). These are\nMarkov Random Fields where some of the nodes are censored (not observed). We\npresent an algorithm for learning high-temperature CMRFs within o(n)\ntransportation distance. Crucially our algorithm makes no assumption about the\nstructure of the graph or the number or location of the observed nodes. We\nobtain stronger results for high girth high-temperature CMRFs as well as\ncomputational lower bounds indicating that our results can not be qualitatively\nimproved.\n", "versions": [{"version": "v1", "created": "Fri, 15 Jan 2021 15:38:19 GMT"}], "update_date": "2021-01-18", "authors_parsed": [["Moitra", "Ankur", ""], ["Mossel", "Elchanan", ""], ["Sandon", "Colin", ""]]}, {"id": "2101.06182", "submitter": "Ivo Sbalzarini", "authors": "Suryanarayana Maddu, Dominik Sturm, Bevan L. Cheeseman, Christian L.\n  M\\\"uller, Ivo F. Sbalzarini", "title": "STENCIL-NET: Data-driven solution-adaptive discretization of partial\n  differential equations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.NA cs.LG cs.NA", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Numerical methods for approximately solving partial differential equations\n(PDE) are at the core of scientific computing. Often, this requires\nhigh-resolution or adaptive discretization grids to capture relevant\nspatio-temporal features in the PDE solution, e.g., in applications like\nturbulence, combustion, and shock propagation. Numerical approximation also\nrequires knowing the PDE in order to construct problem-specific\ndiscretizations. Systematically deriving such solution-adaptive discrete\noperators, however, is a current challenge. Here we present STENCIL-NET, an\nartificial neural network architecture for data-driven learning of problem- and\nresolution-specific local discretizations of nonlinear PDEs. STENCIL-NET\nachieves numerically stable discretization of the operators in an unknown\nnonlinear PDE by spatially and temporally adaptive parametric pooling on\nregular Cartesian grids, and by incorporating knowledge about discrete time\nintegration. Knowing the actual PDE is not necessary, as solution data is\nsufficient to train the network to learn the discrete operators. A once-trained\nSTENCIL-NET model can be used to predict solutions of the PDE on larger spatial\ndomains and for longer times than it was trained for, hence addressing the\nproblem of PDE-constrained extrapolation from data. To support this claim, we\npresent numerical experiments on long-term forecasting of chaotic PDE solutions\non coarse spatio-temporal grids. We also quantify the speed-up achieved by\nsubstituting base-line numerical methods with equation-free STENCIL-NET\npredictions on coarser grids with little compromise on accuracy.\n", "versions": [{"version": "v1", "created": "Fri, 15 Jan 2021 15:43:41 GMT"}, {"version": "v2", "created": "Mon, 18 Jan 2021 10:31:17 GMT"}], "update_date": "2021-01-19", "authors_parsed": [["Maddu", "Suryanarayana", ""], ["Sturm", "Dominik", ""], ["Cheeseman", "Bevan L.", ""], ["M\u00fcller", "Christian L.", ""], ["Sbalzarini", "Ivo F.", ""]]}, {"id": "2101.06189", "submitter": "Samuel Yen-Chi Chen", "authors": "Samuel Yen-Chi Chen, Tzu-Chieh Wei, Chao Zhang, Haiwang Yu, Shinjae\n  Yoo", "title": "Hybrid Quantum-Classical Graph Convolutional Network", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV hep-ex physics.data-an quant-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The high energy physics (HEP) community has a long history of dealing with\nlarge-scale datasets. To manage such voluminous data, classical machine\nlearning and deep learning techniques have been employed to accelerate physics\ndiscovery. Recent advances in quantum machine learning (QML) have indicated the\npotential of applying these techniques in HEP. However, there are only limited\nresults in QML applications currently available. In particular, the challenge\nof processing sparse data, common in HEP datasets, has not been extensively\nstudied in QML models. This research provides a hybrid quantum-classical graph\nconvolutional network (QGCNN) for learning HEP data. The proposed framework\ndemonstrates an advantage over classical multilayer perceptron and\nconvolutional neural networks in the aspect of number of parameters. Moreover,\nin terms of testing accuracy, the QGCNN shows comparable performance to a\nquantum convolutional neural network on the same HEP dataset while requiring\nless than $50\\%$ of the parameters. Based on numerical simulation results,\nstudying the application of graph convolutional operations and other QML models\nmay prove promising in advancing HEP research and other scientific fields.\n", "versions": [{"version": "v1", "created": "Fri, 15 Jan 2021 16:02:52 GMT"}], "update_date": "2021-01-18", "authors_parsed": [["Chen", "Samuel Yen-Chi", ""], ["Wei", "Tzu-Chieh", ""], ["Zhang", "Chao", ""], ["Yu", "Haiwang", ""], ["Yoo", "Shinjae", ""]]}, {"id": "2101.06197", "submitter": "Dilip Arumugam", "authors": "Dilip Arumugam and Benjamin Van Roy", "title": "Deciding What to Learn: A Rate-Distortion Approach", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.IT math.IT", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Agents that learn to select optimal actions represent a prominent focus of\nthe sequential decision-making literature. In the face of a complex environment\nor constraints on time and resources, however, aiming to synthesize such an\noptimal policy can become infeasible. These scenarios give rise to an important\ntrade-off between the information an agent must acquire to learn and the\nsub-optimality of the resulting policy. While an agent designer has a\npreference for how this trade-off is resolved, existing approaches further\nrequire that the designer translate these preferences into a fixed learning\ntarget for the agent. In this work, leveraging rate-distortion theory, we\nautomate this process such that the designer need only express their\npreferences via a single hyperparameter and the agent is endowed with the\nability to compute its own learning targets that best achieve the desired\ntrade-off. We establish a general bound on expected discounted regret for an\nagent that decides what to learn in this manner along with computational\nexperiments that illustrate the expressiveness of designer preferences and even\nshow improvements over Thompson sampling in identifying an optimal policy.\n", "versions": [{"version": "v1", "created": "Fri, 15 Jan 2021 16:22:49 GMT"}, {"version": "v2", "created": "Mon, 7 Jun 2021 23:12:42 GMT"}, {"version": "v3", "created": "Tue, 22 Jun 2021 03:41:39 GMT"}], "update_date": "2021-06-23", "authors_parsed": [["Arumugam", "Dilip", ""], ["Van Roy", "Benjamin", ""]]}, {"id": "2101.06203", "submitter": "Asia Biega", "authors": "Mich\\`ele Finck and Asia Biega", "title": "Reviving Purpose Limitation and Data Minimisation in Personalisation,\n  Profiling and Decision-Making Systems", "comments": "Max Planck Institute for Innovation & Competition Research Paper No.\n  21-04", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper determines, through an interdisciplinary law and computer science\nlens, whether data minimisation and purpose limitation can be meaningfully\nimplemented in data-driven algorithmic systems, including personalisation,\nprofiling and decision-making systems. Our analysis reveals that the two legal\nprinciples continue to play an important role in mitigating the risks of\npersonal data processing, allowing us to rebut claims that they have become\nobsolete. The paper goes beyond this finding, however. We highlight that even\nthough these principles are important safeguards in the systems under\nconsideration, there are important limits to their practical implementation,\nnamely, (i) the difficulties of measuring law and the resulting open\ncomputational research questions as well as a lack of concrete guidelines for\npractitioners; (ii) the unacknowledged trade-offs between various GDPR\nprinciples, notably between data minimisation on the one hand and accuracy or\nfairness on the other; (iii) the lack of practical means of removing personal\ndata from trained models in order to ensure legal compliance; and (iv) the\ninsufficient enforcement of data protection law.\n", "versions": [{"version": "v1", "created": "Fri, 15 Jan 2021 16:36:29 GMT"}], "update_date": "2021-01-19", "authors_parsed": [["Finck", "Mich\u00e8le", ""], ["Biega", "Asia", ""]]}, {"id": "2101.06213", "submitter": "Hsing-Chung Chen", "authors": "Hsing-Chung Chen, Karisma Trinanda Putra, Jerry Chun-WeiLin", "title": "A Novel Prediction Approach for Exploring PM2.5 Spatiotemporal\n  Propagation Based on Convolutional Recursive Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": "Report-no: HCC-2021-01", "categories": "cs.LG cs.NE", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The spread of PM2.5 pollutants that endanger health is difficult to predict\nbecause it involves many atmospheric variables. These micron particles can\nspread rapidly from their source to residential areas, increasing the risk of\nrespiratory disease if exposed for long periods. The prediction system of PM2.5\npropagation provides more detailed and accurate information as an early warning\nsystem to reduce health impacts on the community. According to the idea of\ntransformative computing, the approach we propose in this paper allows\ncomputation on the dataset obtained from massive-scale PM2.5 sensor nodes via\nwireless sensor network. In the scheme, the deep learning model is implemented\non the server nodes to extract spatiotemporal features on these datasets. This\nresearch was conducted by using dataset of air quality monitoring systems in\nTaiwan. This study presents a new model based on the convolutional recursive\nneural network to generate the prediction map. In general, the model is able to\nprovide accurate predictive results by considering the bonds among measurement\nnodes in both spatially and temporally. Therefore, the particulate pollutant\npropagation of PM2.5 could be precisely monitored by using the model we propose\nin this paper.\n", "versions": [{"version": "v1", "created": "Fri, 15 Jan 2021 17:00:04 GMT"}], "update_date": "2021-01-18", "authors_parsed": [["Chen", "Hsing-Chung", ""], ["Putra", "Karisma Trinanda", ""], ["Chun-WeiLin", "Jerry", ""]]}, {"id": "2101.06217", "submitter": "Prajwal Singh", "authors": "Aalok Gangopadhyay, Prajwal Singh, Shanmuganathan Raman", "title": "APEX-Net: Automatic Plot Extractor Network", "comments": "https://sites.google.com/view/apexnetpaper/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Automatic extraction of raw data from 2D line plot images is a problem of\ngreat importance having many real-world applications. Several algorithms have\nbeen proposed for solving this problem. However, these algorithms involve a\nsignificant amount of human intervention. To minimize this intervention, we\npropose APEX-Net, a deep learning based framework with novel loss functions for\nsolving the plot extraction problem. We introduce APEX-1M, a new large scale\ndataset which contains both the plot images and the raw data. We demonstrate\nthe performance of APEX-Net on the APEX-1M test set and show that it obtains\nimpressive accuracy. We also show visual results of our network on unseen plot\nimages and demonstrate that it extracts the shape of the plots to a great\nextent. Finally, we develop a GUI based software for plot extraction that can\nbenefit the community at large. For dataset and more information visit\nhttps://sites.google.com/view/apexnetpaper/.\n", "versions": [{"version": "v1", "created": "Fri, 15 Jan 2021 17:02:36 GMT"}, {"version": "v2", "created": "Mon, 25 Jan 2021 16:17:26 GMT"}, {"version": "v3", "created": "Thu, 11 Feb 2021 05:03:01 GMT"}], "update_date": "2021-02-12", "authors_parsed": [["Gangopadhyay", "Aalok", ""], ["Singh", "Prajwal", ""], ["Raman", "Shanmuganathan", ""]]}, {"id": "2101.06223", "submitter": "Yuhuai(Tony) Wu", "authors": "Yuhuai Wu, Markus Rabe, Wenda Li, Jimmy Ba, Roger Grosse, Christian\n  Szegedy", "title": "LIME: Learning Inductive Bias for Primitives of Mathematical Reasoning", "comments": "16 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.LO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  While designing inductive bias in neural architectures has been widely\nstudied, we hypothesize that transformer networks are flexible enough to learn\ninductive bias from suitable generic tasks. Here, we replace architecture\nengineering by encoding inductive bias in the form of datasets. Inspired by\nPeirce's view that deduction, induction, and abduction form an irreducible set\nof reasoning primitives, we design three synthetic tasks that are intended to\nrequire the model to have these three abilities. We specifically design these\nsynthetic tasks in a way that they are devoid of mathematical knowledge to\nensure that only the fundamental reasoning biases can be learned from these\ntasks. This defines a new pre-training methodology called \"LIME\" (Learning\nInductive bias for Mathematical rEasoning). Models trained with LIME\nsignificantly outperform vanilla transformers on three very different large\nmathematical reasoning benchmarks. Unlike dominating the computation cost as\ntraditional pre-training approaches, LIME requires only a small fraction of the\ncomputation cost of the typical downstream task.\n", "versions": [{"version": "v1", "created": "Fri, 15 Jan 2021 17:15:24 GMT"}], "update_date": "2021-01-18", "authors_parsed": [["Wu", "Yuhuai", ""], ["Rabe", "Markus", ""], ["Li", "Wenda", ""], ["Ba", "Jimmy", ""], ["Grosse", "Roger", ""], ["Szegedy", "Christian", ""]]}, {"id": "2101.06227", "submitter": "Wouter Caarls", "authors": "Franklin Carde\\~noso Fernandez and Wouter Caarls", "title": "Deep Reinforcement Learning for Haptic Shared Control in Unknown Tasks", "comments": "This work has been submitted to the IEEE for possible publication.\n  Copyright may be transferred without notice, after which this version may no\n  longer be accessible", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent years have shown a growing interest in using haptic shared control\n(HSC) in teleoperated systems. In HSC, the application of virtual guiding\nforces decreases the user's control effort and improves execution time in\nvarious tasks, presenting a good alternative in comparison with direct\nteleoperation. HSC, despite demonstrating good performance, opens a new gap:\nhow to design the guiding forces. For this reason, the challenge lies in\ndeveloping controllers to provide the optimal guiding forces for the tasks that\nare being performed. This work addresses this challenge by designing a\ncontroller based on the deep deterministic policy gradient (DDPG) algorithm to\nprovide the assistance, and a convolutional neural network (CNN) to perform the\ntask detection, called TAHSC (Task Agnostic Haptic Shared Controller). The\nagent learns to minimize the time it takes the human to execute the desired\ntask, while simultaneously minimizing their resistance to the provided\nfeedback. This resistance thus provides the learning algorithm with information\nabout which direction the human is trying to follow, in this case, the\npick-and-place task. Diverse results demonstrate the successful application of\nthe proposed approach by learning custom policies for each user who was asked\nto test the system. It exhibits stable convergence and aids the user in\ncompleting the task with the least amount of time possible.\n", "versions": [{"version": "v1", "created": "Fri, 15 Jan 2021 17:27:38 GMT"}], "update_date": "2021-01-18", "authors_parsed": [["Fernandez", "Franklin Carde\u00f1oso", ""], ["Caarls", "Wouter", ""]]}, {"id": "2101.06232", "submitter": "Yuzhou Lin", "authors": "Yuzhou Lin, Xiaolin Chang", "title": "Towards interpreting ML-based automated malware detection models: a\n  survey", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Malware is being increasingly threatening and malware detectors based on\ntraditional signature-based analysis are no longer suitable for current malware\ndetection. Recently, the models based on machine learning (ML) are developed\nfor predicting unknown malware variants and saving human strength. However,\nmost of the existing ML models are black-box, which made their pre-diction\nresults undependable, and therefore need further interpretation in order to be\neffectively deployed in the wild. This paper aims to examine and categorize the\nexisting researches on ML-based malware detector interpretability. We first\ngive a detailed comparison over the previous work on common ML model\ninter-pretability in groups after introducing the principles, attributes,\nevaluation indi-cators and taxonomy of common ML interpretability. Then we\ninvestigate the interpretation methods towards malware detection, by addressing\nthe importance of interpreting malware detectors, challenges faced by this\nfield, solutions for migitating these challenges, and a new taxonomy for\nclassifying all the state-of-the-art malware detection interpretability work in\nrecent years. The highlight of our survey is providing a new taxonomy towards\nmalware detection interpreta-tion methods based on the common taxonomy\nsummarized by previous re-searches in the common field. In addition, we are the\nfirst to evaluate the state-of-the-art approaches by interpretation method\nattributes to generate the final score so as to give insight to quantifying the\ninterpretability. By concluding the results of the recent researches, we hope\nour work can provide suggestions for researchers who are interested in the\ninterpretability on ML-based malware de-tection models.\n", "versions": [{"version": "v1", "created": "Fri, 15 Jan 2021 17:34:40 GMT"}], "update_date": "2021-01-18", "authors_parsed": [["Lin", "Yuzhou", ""], ["Chang", "Xiaolin", ""]]}, {"id": "2101.06233", "submitter": "Tomoya Sakai", "authors": "Tomoya Sakai, Naoto Ohsaka", "title": "Predictive Optimization with Zero-Shot Domain Adaptation", "comments": "SDM2021. Full version including appendix", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Prediction in a new domain without any training sample, called zero-shot\ndomain adaptation (ZSDA), is an important task in domain adaptation. While\nprediction in a new domain has gained much attention in recent years, in this\npaper, we investigate another potential of ZSDA. Specifically, instead of\npredicting responses in a new domain, we find a description of a new domain\ngiven a prediction. The task is regarded as predictive optimization, but\nexisting predictive optimization methods have not been extended to handling\nmultiple domains. We propose a simple framework for predictive optimization\nwith ZSDA and analyze the condition in which the optimization problem becomes\nconvex optimization. We also discuss how to handle the interaction of\ncharacteristics of a domain in predictive optimization. Through numerical\nexperiments, we demonstrate the potential usefulness of our proposed framework.\n", "versions": [{"version": "v1", "created": "Fri, 15 Jan 2021 17:35:12 GMT"}], "update_date": "2021-01-18", "authors_parsed": [["Sakai", "Tomoya", ""], ["Ohsaka", "Naoto", ""]]}, {"id": "2101.06255", "submitter": "Daniel Moyer", "authors": "Daniel Moyer and Polina Golland", "title": "Harmonization and the Worst Scanner Syndrome", "comments": "Med-NeurIPS 2020 Workshop Paper, updated 4/2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV eess.IV q-bio.QM stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We show that for a wide class of harmonization/domain-invariance schemes\nseveral undesirable properties are unavoidable. If a predictive machine is made\ninvariant to a set of domains, the accuracy of the output predictions (as\nmeasured by mutual information) is limited by the domain with the least amount\nof information to begin with. If a real label value is highly informative about\nthe source domain, it cannot be accurately predicted by an invariant predictor.\nThese results are simple and intuitive, but we believe that it is beneficial to\nstate them for medical imaging harmonization.\n", "versions": [{"version": "v1", "created": "Fri, 15 Jan 2021 18:41:41 GMT"}, {"version": "v2", "created": "Wed, 21 Apr 2021 17:16:42 GMT"}], "update_date": "2021-04-22", "authors_parsed": [["Moyer", "Daniel", ""], ["Golland", "Polina", ""]]}, {"id": "2101.06262", "submitter": "Kyriakos Axiotis", "authors": "Kyriakos Axiotis and Maxim Sviridenko", "title": "Local Search Algorithms for Rank-Constrained Convex Optimization", "comments": "Accepted in ICLR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose greedy and local search algorithms for rank-constrained convex\noptimization, namely solving $\\underset{\\mathrm{rank}(A)\\leq r^*}{\\min}\\, R(A)$\ngiven a convex function $R:\\mathbb{R}^{m\\times n}\\rightarrow \\mathbb{R}$ and a\nparameter $r^*$. These algorithms consist of repeating two steps: (a) adding a\nnew rank-1 matrix to $A$ and (b) enforcing the rank constraint on $A$. We\nrefine and improve the theoretical analysis of Shalev-Shwartz et al. (2011),\nand show that if the rank-restricted condition number of $R$ is $\\kappa$, a\nsolution $A$ with rank $O(r^*\\cdot \\min\\{\\kappa \\log\n\\frac{R(\\mathbf{0})-R(A^*)}{\\epsilon}, \\kappa^2\\})$ and $R(A) \\leq R(A^*) +\n\\epsilon$ can be recovered, where $A^*$ is the optimal solution. This\nsignificantly generalizes associated results on sparse convex optimization, as\nwell as rank-constrained convex optimization for smooth functions. We then\nintroduce new practical variants of these algorithms that have superior runtime\nand recover better solutions in practice. We demonstrate the versatility of\nthese methods on a wide range of applications involving matrix completion and\nrobust principal component analysis.\n", "versions": [{"version": "v1", "created": "Fri, 15 Jan 2021 18:52:02 GMT"}], "update_date": "2021-01-18", "authors_parsed": [["Axiotis", "Kyriakos", ""], ["Sviridenko", "Maxim", ""]]}, {"id": "2101.06289", "submitter": "Manuel Eichenlaub", "authors": "Manuel M. Eichenlaub", "title": "On the relationship between a Gamma distributed precision parameter and\n  the associated standard deviation in the context of Bayesian parameter\n  inference", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.LG", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  In Bayesian inference, an unknown measurement uncertainty is often quantified\nin terms of a Gamma distributed precision parameter, which is impractical when\nprior information on the standard deviation of the measurement uncertainty\nshall be utilised during inference. This paper thus introduces a method for\ntransforming between a gamma distributed precision parameter and the\ndistribution of the associated standard deviation. The proposed method is based\non numerical optimisation and shows adequate results for a wide range of\nscenarios.\n", "versions": [{"version": "v1", "created": "Fri, 15 Jan 2021 20:07:12 GMT"}], "update_date": "2021-01-19", "authors_parsed": [["Eichenlaub", "Manuel M.", ""]]}, {"id": "2101.06296", "submitter": "Nathan Wycoff", "authors": "Nathan Wycoff, Micka\\\"el Binois, Robert B. Gramacy", "title": "Sensitivity Prewarping for Local Surrogate Modeling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In the continual effort to improve product quality and decrease operations\ncosts, computational modeling is increasingly being deployed to determine\nfeasibility of product designs or configurations. Surrogate modeling of these\ncomputer experiments via local models, which induce sparsity by only\nconsidering short range interactions, can tackle huge analyses of complicated\ninput-output relationships. However, narrowing focus to local scale means that\nglobal trends must be re-learned over and over again. In this article, we\npropose a framework for incorporating information from a global sensitivity\nanalysis into the surrogate model as an input rotation and rescaling\npreprocessing step. We discuss the relationship between several sensitivity\nanalysis methods based on kernel regression before describing how they give\nrise to a transformation of the input variables. Specifically, we perform an\ninput warping such that the \"warped simulator\" is equally sensitive to all\ninput directions, freeing local models to focus on local dynamics. Numerical\nexperiments on observational data and benchmark test functions, including a\nhigh-dimensional computer simulator from the automotive industry, provide\nempirical validation.\n", "versions": [{"version": "v1", "created": "Fri, 15 Jan 2021 20:42:32 GMT"}], "update_date": "2021-01-19", "authors_parsed": [["Wycoff", "Nathan", ""], ["Binois", "Micka\u00ebl", ""], ["Gramacy", "Robert B.", ""]]}, {"id": "2101.06309", "submitter": "Adel Javanmard", "authors": "Mohammad Mehrabi, Adel Javanmard, Ryan A. Rossi, Anup Rao and Tung Mai", "title": "Fundamental Tradeoffs in Distributionally Adversarial Training", "comments": "23 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Adversarial training is among the most effective techniques to improve the\nrobustness of models against adversarial perturbations. However, the full\neffect of this approach on models is not well understood. For example, while\nadversarial training can reduce the adversarial risk (prediction error against\nan adversary), it sometimes increase standard risk (generalization error when\nthere is no adversary). Even more, such behavior is impacted by various\nelements of the learning problem, including the size and quality of training\ndata, specific forms of adversarial perturbations in the input, model\noverparameterization, and adversary's power, among others. In this paper, we\nfocus on \\emph{distribution perturbing} adversary framework wherein the\nadversary can change the test distribution within a neighborhood of the\ntraining data distribution. The neighborhood is defined via Wasserstein\ndistance between distributions and the radius of the neighborhood is a measure\nof adversary's manipulative power. We study the tradeoff between standard risk\nand adversarial risk and derive the Pareto-optimal tradeoff, achievable over\nspecific classes of models, in the infinite data limit with features dimension\nkept fixed. We consider three learning settings: 1) Regression with the class\nof linear models; 2) Binary classification under the Gaussian mixtures data\nmodel, with the class of linear classifiers; 3) Regression with the class of\nrandom features model (which can be equivalently represented as two-layer\nneural network with random first-layer weights). We show that a tradeoff\nbetween standard and adversarial risk is manifested in all three settings. We\nfurther characterize the Pareto-optimal tradeoff curves and discuss how a\nvariety of factors, such as features correlation, adversary's power or the\nwidth of two-layer neural network would affect this tradeoff.\n", "versions": [{"version": "v1", "created": "Fri, 15 Jan 2021 21:59:18 GMT"}], "update_date": "2021-01-19", "authors_parsed": [["Mehrabi", "Mohammad", ""], ["Javanmard", "Adel", ""], ["Rossi", "Ryan A.", ""], ["Rao", "Anup", ""], ["Mai", "Tung", ""]]}, {"id": "2101.06310", "submitter": "Daniel Osaku", "authors": "D. Osaku, C. F. Cuba, Celso T.N. Suzuki, J.F. Gomes, A.X. Falc\\~ao", "title": "Automated Diagnosis of Intestinal Parasites: A new hybrid approach and\n  its benefits", "comments": "18 pages, 11 figures", "journal-ref": "Computers in Biology and Medicine, Volume 123, August 2020, 103917", "doi": "10.1016/j.compbiomed.2020.103917", "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Intestinal parasites are responsible for several diseases in human beings. In\norder to eliminate the error-prone visual analysis of optical microscopy\nslides, we have investigated automated, fast, and low-cost systems for the\ndiagnosis of human intestinal parasites. In this work, we present a hybrid\napproach that combines the opinion of two decision-making systems with\ncomplementary properties: ($DS_1$) a simpler system based on very fast\nhandcrafted image feature extraction and support vector machine classification\nand ($DS_2$) a more complex system based on a deep neural network, Vgg-16, for\nimage feature extraction and classification. $DS_1$ is much faster than $DS_2$,\nbut it is less accurate than $DS_2$. Fortunately, the errors of $DS_1$ are not\nthe same of $DS_2$. During training, we use a validation set to learn the\nprobabilities of misclassification by $DS_1$ on each class based on its\nconfidence values. When $DS_1$ quickly classifies all images from a microscopy\nslide, the method selects a number of images with higher chances of\nmisclassification for characterization and reclassification by $DS_2$. Our\nhybrid system can improve the overall effectiveness without compromising\nefficiency, being suitable for the clinical routine -- a strategy that might be\nsuitable for other real applications. As demonstrated on large datasets, the\nproposed system can achieve, on average, 94.9%, 87.8%, and 92.5% of Cohen's\nKappa on helminth eggs, helminth larvae, and protozoa cysts, respectively.\n", "versions": [{"version": "v1", "created": "Thu, 7 Jan 2021 05:11:01 GMT"}], "update_date": "2021-01-19", "authors_parsed": [["Osaku", "D.", ""], ["Cuba", "C. F.", ""], ["Suzuki", "Celso T. N.", ""], ["Gomes", "J. F.", ""], ["Falc\u00e3o", "A. X.", ""]]}, {"id": "2101.06317", "submitter": "Yang-Hui He", "authors": "Yang-Hui He", "title": "Machine-Learning Mathematical Structures", "comments": "32 pages. Based on various colloquia, seminars and conference talks\n  in 2020, this is a contribution to the launch of the journal \"Data Science in\n  the Mathematical Sciences.\"", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG hep-th math.HO physics.hist-ph", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We review, for a general audience, a variety of recent experiments on\nextracting structure from machine-learning mathematical data that have been\ncompiled over the years. Focusing on supervised machine-learning on labeled\ndata from different fields ranging from geometry to representation theory, from\ncombinatorics to number theory, we present a comparative study of the\naccuracies on different problems. The paradigm should be useful for conjecture\nformulation, finding more efficient methods of computation, as well as probing\ninto certain hierarchy of structures in mathematics.\n", "versions": [{"version": "v1", "created": "Fri, 15 Jan 2021 22:48:19 GMT"}, {"version": "v2", "created": "Thu, 8 Apr 2021 08:54:32 GMT"}], "update_date": "2021-04-09", "authors_parsed": [["He", "Yang-Hui", ""]]}, {"id": "2101.06323", "submitter": "Jason Zhu", "authors": "Jason Yue Zhu, Yanling Cui, Yuming Liu, Hao Sun, Xue Li, Markus\n  Pelger, Tianqi Yang, Liangjie Zhang, Ruofei Zhang, Huasha Zhao", "title": "TextGNN: Improving Text Encoder via Graph Neural Network in Sponsored\n  Search", "comments": "Jason Yue Zhu, Yanling Cui, Yuming Liu, Hao Sun, Xue Li, Markus\n  Pelger, Tianqi Yang, Liangjie Zhang, Ruofei Zhang, and Huasha Zhao. 2021.\n  TextGNN: Improving Text Encoder via Graph Neural Network in Sponsored Search.\n  In Proceedings of the Web Conference 2021 (WWW 21), April 19-23, 2021,\n  Ljubljana, Slovenia. ACM, New York, NY, USA, 10 pages. https:\n  //doi.org/10.1145/3442381.3449842", "journal-ref": null, "doi": "10.1145/3442381.3449842", "report-no": null, "categories": "cs.CL cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Text encoders based on C-DSSM or transformers have demonstrated strong\nperformance in many Natural Language Processing (NLP) tasks. Low latency\nvariants of these models have also been developed in recent years in order to\napply them in the field of sponsored search which has strict computational\nconstraints. However these models are not the panacea to solve all the Natural\nLanguage Understanding (NLU) challenges as the pure semantic information in the\ndata is not sufficient to fully identify the user intents. We propose the\nTextGNN model that naturally extends the strong twin tower structured encoders\nwith the complementary graph information from user historical behaviors, which\nserves as a natural guide to help us better understand the intents and hence\ngenerate better language representations. The model inherits all the benefits\nof twin tower models such as C-DSSM and TwinBERT so that it can still be used\nin the low latency environment while achieving a significant performance gain\nthan the strong encoder-only counterpart baseline models in both offline\nevaluations and online production system. In offline experiments, the model\nachieves a 0.14% overall increase in ROC-AUC with a 1% increased accuracy for\nlong-tail low-frequency Ads, and in the online A/B testing, the model shows a\n2.03% increase in Revenue Per Mille with a 2.32% decrease in Ad defect rate.\n", "versions": [{"version": "v1", "created": "Fri, 15 Jan 2021 23:12:47 GMT"}, {"version": "v2", "created": "Tue, 9 Feb 2021 17:29:32 GMT"}, {"version": "v3", "created": "Sat, 1 May 2021 04:23:20 GMT"}], "update_date": "2021-05-04", "authors_parsed": [["Zhu", "Jason Yue", ""], ["Cui", "Yanling", ""], ["Liu", "Yuming", ""], ["Sun", "Hao", ""], ["Li", "Xue", ""], ["Pelger", "Markus", ""], ["Yang", "Tianqi", ""], ["Zhang", "Liangjie", ""], ["Zhang", "Ruofei", ""], ["Zhao", "Huasha", ""]]}, {"id": "2101.06326", "submitter": "Alex John Quijano", "authors": "Alex John Quijano, Sam Nguyen, and Juanita Ordonez", "title": "Grid Search Hyperparameter Benchmarking of BERT, ALBERT, and LongFormer\n  on DuoRC", "comments": "9 pages, 2 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": "LLNL-TR-817729", "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The purpose of this project is to evaluate three language models named BERT,\nALBERT, and LongFormer on the Question Answering dataset called DuoRC. The\nlanguage model task has two inputs, a question, and a context. The context is a\nparagraph or an entire document while the output is the answer based on the\ncontext. The goal is to perform grid search hyperparameter fine-tuning using\nDuoRC. Pretrained weights of the models are taken from the Huggingface library.\nDifferent sets of hyperparameters are used to fine-tune the models using two\nversions of DuoRC which are the SelfRC and the ParaphraseRC. The results show\nthat the ALBERT (pretrained using the SQuAD1 dataset) has an F1 score of 76.4\nand an accuracy score of 68.52 after fine-tuning on the SelfRC dataset. The\nLongformer model (pretrained using the SQuAD and SelfRC datasets) has an F1\nscore of 52.58 and an accuracy score of 46.60 after fine-tuning on the\nParaphraseRC dataset. The current results outperformed the results from the\nprevious model by DuoRC.\n", "versions": [{"version": "v1", "created": "Fri, 15 Jan 2021 23:28:32 GMT"}, {"version": "v2", "created": "Mon, 29 Mar 2021 17:44:12 GMT"}], "update_date": "2021-03-30", "authors_parsed": [["Quijano", "Alex John", ""], ["Nguyen", "Sam", ""], ["Ordonez", "Juanita", ""]]}, {"id": "2101.06329", "submitter": "Mamshad Nayeem Rizve", "authors": "Mamshad Nayeem Rizve, Kevin Duarte, Yogesh S Rawat, Mubarak Shah", "title": "In Defense of Pseudo-Labeling: An Uncertainty-Aware Pseudo-label\n  Selection Framework for Semi-Supervised Learning", "comments": "ICLR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  The recent research in semi-supervised learning (SSL) is mostly dominated by\nconsistency regularization based methods which achieve strong performance.\nHowever, they heavily rely on domain-specific data augmentations, which are not\neasy to generate for all data modalities. Pseudo-labeling (PL) is a general SSL\napproach that does not have this constraint but performs relatively poorly in\nits original formulation. We argue that PL underperforms due to the erroneous\nhigh confidence predictions from poorly calibrated models; these predictions\ngenerate many incorrect pseudo-labels, leading to noisy training. We propose an\nuncertainty-aware pseudo-label selection (UPS) framework which improves pseudo\nlabeling accuracy by drastically reducing the amount of noise encountered in\nthe training process. Furthermore, UPS generalizes the pseudo-labeling process,\nallowing for the creation of negative pseudo-labels; these negative\npseudo-labels can be used for multi-label classification as well as negative\nlearning to improve the single-label classification. We achieve strong\nperformance when compared to recent SSL methods on the CIFAR-10 and CIFAR-100\ndatasets. Also, we demonstrate the versatility of our method on the video\ndataset UCF-101 and the multi-label dataset Pascal VOC.\n", "versions": [{"version": "v1", "created": "Fri, 15 Jan 2021 23:29:57 GMT"}, {"version": "v2", "created": "Thu, 18 Mar 2021 04:42:48 GMT"}, {"version": "v3", "created": "Mon, 19 Apr 2021 17:59:03 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Rizve", "Mamshad Nayeem", ""], ["Duarte", "Kevin", ""], ["Rawat", "Yogesh S", ""], ["Shah", "Mubarak", ""]]}, {"id": "2101.06348", "submitter": "Marcos Carreira", "authors": "Marcos Costa Santos Carreira", "title": "Exponential Kernels with Latency in Hawkes Processes: Applications in\n  Finance", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG q-fin.TR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Tick library allows researchers in market microstructure to simulate and\nlearn Hawkes process in high-frequency data, with optimized parametric and\nnon-parametric learners. But one challenge is to take into account the correct\ncausality of order book events considering latency: the only way one order book\nevent can influence another is if the time difference between them (by the\ncentral order book timestamps) is greater than the minimum amount of time for\nan event to be (i) published in the order book, (ii) reach the trader\nresponsible for the second event, (iii) influence the decision (processing time\nat the trader) and (iv) the 2nd event reach the order book and be processed.\nFor this we can use exponential kernels shifted to the right by the latency\namount. We derive the expression for the log-likelihood to be minimized for the\n1-D and the multidimensional cases, and test this method with simulated data\nand real data. On real data we find that, although not all decays are the same,\nthe latency itself will determine most of the decays. We also show how the\ndecays are related to the latency. Code is available on GitHub at\nhttps://github.com/MarcosCarreira/Hawkes-With-Latency.\n", "versions": [{"version": "v1", "created": "Sat, 16 Jan 2021 01:57:59 GMT"}], "update_date": "2021-01-19", "authors_parsed": [["Carreira", "Marcos Costa Santos", ""]]}, {"id": "2101.06353", "submitter": "Shoffan Saifullah", "authors": "Shoffan Saifullah, Yuli Fauziah, Agus Sasmito Aribowo", "title": "Comparison of Machine Learning for Sentiment Analysis in Detecting\n  Anxiety Based on Social Media Data", "comments": null, "journal-ref": "Jurnal Informatika,15(1), 2021, 45-55", "doi": "10.26555/jifo.v15i1.a20111", "report-no": null, "categories": "cs.CL cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  All groups of people felt the impact of the COVID-19 pandemic. This situation\ntriggers anxiety, which is bad for everyone. The government's role is very\ninfluential in solving these problems with its work program. It also has many\npros and cons that cause public anxiety. For that, it is necessary to detect\nanxiety to improve government programs that can increase public expectations.\nThis study applies machine learning to detecting anxiety based on social media\ncomments regarding government programs to deal with this pandemic. This concept\nwill adopt a sentiment analysis in detecting anxiety based on positive and\nnegative comments from netizens. The machine learning methods implemented\ninclude K-NN, Bernoulli, Decision Tree Classifier, Support Vector Classifier,\nRandom Forest, and XG-boost. The data sample used is the result of crawling\nYouTube comments. The data used amounted to 4862 comments consisting of\nnegative and positive data with 3211 and 1651. Negative data identify anxiety,\nwhile positive data identifies hope (not anxious). Machine learning is\nprocessed based on feature extraction of count-vectorization and TF-IDF. The\nresults showed that the sentiment data amounted to 3889 and 973 in testing, and\ntraining with the greatest accuracy was the random forest with feature\nextraction of vectorization count and TF-IDF of 84.99% and 82.63%,\nrespectively. The best precision test is K-NN, while the best recall is\nXG-Boost. Thus, Random Forest is the best accurate to detect someone's anxiety\nbased-on data from social media.\n", "versions": [{"version": "v1", "created": "Sat, 16 Jan 2021 02:47:14 GMT"}], "update_date": "2021-03-02", "authors_parsed": [["Saifullah", "Shoffan", ""], ["Fauziah", "Yuli", ""], ["Aribowo", "Agus Sasmito", ""]]}, {"id": "2101.06371", "submitter": "MyungJoo Ham", "authors": "MyungJoo Ham, Jijoong Moon, Geunsik Lim, Jaeyun Jung, Hyoungjoo Ahn,\n  Wook Song, Sangjung Woo, Parichay Kapoor, Dongju Chae, Gichan Jang, Yongjoo\n  Ahn, Jihoon Lee", "title": "NNStreamer: Efficient and Agile Development of On-Device AI Systems", "comments": "IEEE/ACM ICSE 2021 SEIP (preprint)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose NNStreamer, a software system that handles neural networks as\nfilters of stream pipelines, applying the stream processing paradigm to deep\nneural network applications. A new trend with the wide-spread of deep neural\nnetwork applications is on-device AI. It is to process neural networks on\nmobile devices or edge/IoT devices instead of cloud servers. Emerging privacy\nissues, data transmission costs, and operational costs signify the need for\non-device AI, especially if we deploy a massive number of devices. NNStreamer\nefficiently handles neural networks with complex data stream pipelines on\ndevices, significantly improving the overall performance with minimal efforts.\nBesides, NNStreamer simplifies implementations and allows reusing off-the-shelf\nmedia filters directly, which reduces developmental costs significantly. We are\nalready deploying NNStreamer for a wide range of products and platforms,\nincluding the Galaxy series and various consumer electronic devices. The\nexperimental results suggest a reduction in developmental costs and enhanced\nperformance of pipeline architectures and NNStreamer. It is an open-source\nproject incubated by Linux Foundation AI, available to the public and\napplicable to various hardware and software platforms.\n", "versions": [{"version": "v1", "created": "Sat, 16 Jan 2021 04:49:44 GMT"}], "update_date": "2021-01-19", "authors_parsed": [["Ham", "MyungJoo", ""], ["Moon", "Jijoong", ""], ["Lim", "Geunsik", ""], ["Jung", "Jaeyun", ""], ["Ahn", "Hyoungjoo", ""], ["Song", "Wook", ""], ["Woo", "Sangjung", ""], ["Kapoor", "Parichay", ""], ["Chae", "Dongju", ""], ["Jang", "Gichan", ""], ["Ahn", "Yongjoo", ""], ["Lee", "Jihoon", ""]]}, {"id": "2101.06381", "submitter": "Zhizhong Wang", "authors": "Zhizhong Wang, Lei Zhao, Haibo Chen, Zhiwen Zuo, Ailin Li, Wei Xing,\n  Dongming Lu", "title": "Diversified Patch-based Style Transfer with Shifted Style Normalization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Gram-based and patch-based approaches are two important research lines of\nimage style transfer. Recent diversified Gram-based methods have been able to\nproduce multiple and diverse reasonable solutions for the same content and\nstyle inputs. However, as another popular research interest, the diversity of\npatch-based methods remains challenging due to the stereotyped style swapping\nprocess based on nearest patch matching. To resolve this dilemma, in this\npaper, we dive into the core style swapping process of patch-based style\ntransfer and explore possible ways to diversify it. What stands out is an\noperation called shifted style normalization (SSN), the most effective and\nefficient way to empower existing patch-based methods to generate diverse\nresults for arbitrary styles. The key insight is to use an important intuition\nthat neural patches with higher activation values could contribute more to\ndiversity. Theoretical analyses and extensive experiments are conducted to\ndemonstrate the effectiveness of our method, and compared with other possible\noptions and state-of-the-art algorithms, it shows remarkable superiority in\nboth diversity and efficiency.\n", "versions": [{"version": "v1", "created": "Sat, 16 Jan 2021 06:34:15 GMT"}], "update_date": "2021-01-19", "authors_parsed": [["Wang", "Zhizhong", ""], ["Zhao", "Lei", ""], ["Chen", "Haibo", ""], ["Zuo", "Zhiwen", ""], ["Li", "Ailin", ""], ["Xing", "Wei", ""], ["Lu", "Dongming", ""]]}, {"id": "2101.06388", "submitter": "Ruizhong Miao", "authors": "Ruizhong Miao and Tianxi Li", "title": "Informative core identification in complex networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In network analysis, the core structure of modeling interest is usually\nhidden in a larger network in which most structures are not informative. The\nnoise and bias introduced by the non-informative component in networks can\nobscure the salient structure and limit many network modeling procedures'\neffectiveness. This paper introduces a novel core-periphery model for the\nnon-informative periphery structure of networks without imposing a specific\nform for the informative core structure. We propose spectral algorithms for\ncore identification as a data preprocessing step for general downstream network\nanalysis tasks based on the model. The algorithm enjoys a strong theoretical\nguarantee of accuracy and is scalable for large networks. We evaluate the\nproposed method by extensive simulation studies demonstrating various\nadvantages over many traditional core-periphery methods. The method is applied\nto extract the informative core structure from a citation network and give more\ninformative results in the downstream hierarchical community detection.\n", "versions": [{"version": "v1", "created": "Sat, 16 Jan 2021 07:19:21 GMT"}], "update_date": "2021-01-19", "authors_parsed": [["Miao", "Ruizhong", ""], ["Li", "Tianxi", ""]]}, {"id": "2101.06395", "submitter": "Shuo Yang", "authors": "Shuo Yang, Lu Liu, Min Xu", "title": "Free Lunch for Few-shot Learning: Distribution Calibration", "comments": "ICLR 2021", "journal-ref": "The 9th International Conference on Learning Representations (ICLR\n  2021)", "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning from a limited number of samples is challenging since the learned\nmodel can easily become overfitted based on the biased distribution formed by\nonly a few training examples. In this paper, we calibrate the distribution of\nthese few-sample classes by transferring statistics from the classes with\nsufficient examples, then an adequate number of examples can be sampled from\nthe calibrated distribution to expand the inputs to the classifier. We assume\nevery dimension in the feature representation follows a Gaussian distribution\nso that the mean and the variance of the distribution can borrow from that of\nsimilar classes whose statistics are better estimated with an adequate number\nof samples. Our method can be built on top of off-the-shelf pretrained feature\nextractors and classification models without extra parameters. We show that a\nsimple logistic regression classifier trained using the features sampled from\nour calibrated distribution can outperform the state-of-the-art accuracy on two\ndatasets (~5% improvement on miniImageNet compared to the next best). The\nvisualization of these generated features demonstrates that our calibrated\ndistribution is an accurate estimation.\n", "versions": [{"version": "v1", "created": "Sat, 16 Jan 2021 07:58:40 GMT"}, {"version": "v2", "created": "Mon, 15 Mar 2021 08:34:18 GMT"}], "update_date": "2021-03-16", "authors_parsed": [["Yang", "Shuo", ""], ["Liu", "Lu", ""], ["Xu", "Min", ""]]}, {"id": "2101.06396", "submitter": "Daniel Korzekwa", "authors": "Daniel Korzekwa, Jaime Lorenzo-Trueba, Szymon Zaporowski, Shira\n  Calamaro, Thomas Drugman, Bozena Kostek", "title": "Mispronunciation Detection in Non-native (L2) English with Uncertainty\n  Modeling", "comments": "Accepted to ICASSP 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.AS cs.LG cs.SD", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A common approach to the automatic detection of mispronunciation in language\nlearning is to recognize the phonemes produced by a student and compare it to\nthe expected pronunciation of a native speaker. This approach makes two\nsimplifying assumptions: a) phonemes can be recognized from speech with high\naccuracy, b) there is a single correct way for a sentence to be pronounced.\nThese assumptions do not always hold, which can result in a significant amount\nof false mispronunciation alarms. We propose a novel approach to overcome this\nproblem based on two principles: a) taking into account uncertainty in the\nautomatic phoneme recognition step, b) accounting for the fact that there may\nbe multiple valid pronunciations. We evaluate the model on non-native (L2)\nEnglish speech of German, Italian and Polish speakers, where it is shown to\nincrease the precision of detecting mispronunciations by up to 18% (relative)\ncompared to the common approach.\n", "versions": [{"version": "v1", "created": "Sat, 16 Jan 2021 08:03:51 GMT"}, {"version": "v2", "created": "Mon, 8 Feb 2021 20:16:47 GMT"}], "update_date": "2021-02-10", "authors_parsed": [["Korzekwa", "Daniel", ""], ["Lorenzo-Trueba", "Jaime", ""], ["Zaporowski", "Szymon", ""], ["Calamaro", "Shira", ""], ["Drugman", "Thomas", ""], ["Kostek", "Bozena", ""]]}, {"id": "2101.06417", "submitter": "Shaopeng Fu", "authors": "Shaopeng Fu, Fengxiang He, Yue Xu, Dacheng Tao", "title": "Bayesian Inference Forgetting", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The right to be forgotten has been legislated in many countries but the\nenforcement in machine learning would cause unbearable costs: companies may\nneed to delete whole models learned from massive resources due to single\nindividual requests. Existing works propose to remove the knowledge learned\nfrom the requested data via its influence function which is no longer naturally\nwell-defined in Bayesian inference. This paper proposes a {\\it Bayesian\ninference forgetting} (BIF) framework to realize the right to be forgotten in\nBayesian inference. In the BIF framework, we develop forgetting algorithms for\nvariational inference and Markov chain Monte Carlo. We show that our algorithms\ncan provably remove the influence of single datums on the learned models.\nTheoretical analysis demonstrates that our algorithms have guaranteed\ngeneralizability. Experiments of Gaussian mixture models on the synthetic\ndataset and Bayesian neural networks on the real-world data verify the\nfeasibility of our methods. The source code package is available at\n\\url{https://github.com/fshp971/BIF}.\n", "versions": [{"version": "v1", "created": "Sat, 16 Jan 2021 09:52:51 GMT"}, {"version": "v2", "created": "Thu, 18 Feb 2021 09:05:14 GMT"}], "update_date": "2021-02-19", "authors_parsed": [["Fu", "Shaopeng", ""], ["He", "Fengxiang", ""], ["Xu", "Yue", ""], ["Tao", "Dacheng", ""]]}, {"id": "2101.06427", "submitter": "Yuqing Zhu", "authors": "Mengying Guo, Tao Yi, Yuqing Zhu, Yungang Bao", "title": "JITuNE: Just-In-Time Hyperparameter Tuning for Network Embedding\n  Algorithms", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Network embedding (NE) can generate succinct node representations for\nmassive-scale networks and enable direct applications of common machine\nlearning methods to the network structure. Various NE algorithms have been\nproposed and used in a number of applications, such as node classification and\nlink prediction. NE algorithms typically contain hyperparameters that are key\nto performance, but the hyperparameter tuning process can be time consuming. It\nis desirable to have the hyperparameters tuned within a specified length of\ntime. Although AutoML methods have been applied to the hyperparameter tuning of\nNE algorithms, the problem of how to tune hyperparameters in a given period of\ntime is not studied for NE algorithms before. In this paper, we propose JITuNE,\na just-in-time hyperparameter tuning framework for NE algorithms. Our JITuNE\nframework enables the time-constrained hyperparameter tuning for NE algorithms\nby employing the tuning over hierarchical network synopses and transferring the\nknowledge obtained on synopses to the whole network. The hierarchical\ngeneration of synopsis and a time-constrained tuning method enable the\nconstraining of overall tuning time. Extensive experiments demonstrate that\nJITuNE can significantly improve performances of NE algorithms, outperforming\nstate-of-the-art methods within the same number of algorithm runs.\n", "versions": [{"version": "v1", "created": "Sat, 16 Jan 2021 10:57:17 GMT"}, {"version": "v2", "created": "Tue, 19 Jan 2021 03:54:35 GMT"}], "update_date": "2021-01-20", "authors_parsed": [["Guo", "Mengying", ""], ["Yi", "Tao", ""], ["Zhu", "Yuqing", ""], ["Bao", "Yungang", ""]]}, {"id": "2101.06428", "submitter": "Sabrina Amrouche", "authors": "Sabrina Amrouche, Moritz Kiehn, Tobias Golling, Andreas Salzburger", "title": "Hashing and metric learning for charged particle tracking", "comments": "Second Workshop on Machine Learning and the Physical Sciences\n  (NeurIPS 2019), Vancouver, Canada", "journal-ref": null, "doi": null, "report-no": null, "categories": "hep-ex cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel approach to charged particle tracking at high intensity\nparticle colliders based on Approximate Nearest Neighbors search. With hundreds\nof thousands of measurements per collision to be reconstructed e.g. at the High\nLuminosity Large Hadron Collider, the currently employed combinatorial track\nfinding approaches become inadequate. Here, we use hashing techniques to\nseparate measurements into buckets of 20-50 hits and increase their purity\nusing metric learning. Two different approaches are studied to further resolve\ntracks inside buckets: Local Fisher Discriminant Analysis and Neural Networks\nfor triplet similarity learning. We demonstrate the proposed approach on\nsimulated collisions and show significant speed improvement with bucket\ntracking efficiency of 96% and a fake rate of 8% on unseen particle events.\n", "versions": [{"version": "v1", "created": "Sat, 16 Jan 2021 10:57:25 GMT"}], "update_date": "2021-01-19", "authors_parsed": [["Amrouche", "Sabrina", ""], ["Kiehn", "Moritz", ""], ["Golling", "Tobias", ""], ["Salzburger", "Andreas", ""]]}, {"id": "2101.06443", "submitter": "Hyunjung Kwak", "authors": "Gloria Hyunjung Kwak, Christina Chen, Lowell Ling, Erina Ghosh, Leo\n  Anthony Celi, Pan Hui", "title": "Predicting Hyperkalemia in the ICU and Evaluation of Generalizability\n  and Interpretability", "comments": "6 pages, 3 figures, 3 tables", "journal-ref": "AAAI 2021 Workshop: Trustworthy AI for Healthcare", "doi": null, "report-no": null, "categories": "cs.LG q-bio.QM", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Hyperkalemia is a potentially life-threatening condition that can lead to\nfatal arrhythmias. Early identification of high risk patients can inform\nclinical care to mitigate the risk. While hyperkalemia is often a complication\nof acute kidney injury (AKI), it also occurs in the absence of AKI. We\ndeveloped predictive models to identify intensive care unit (ICU) patients at\nrisk of developing hyperkalemia by using the Medical Information Mart for\nIntensive Care (MIMIC) and the eICU Collaborative Research Database (eICU-CRD).\nOur methodology focused on building multiple models, optimizing for\ninterpretability through model selection, and simulating various clinical\nscenarios.\n  In order to determine if our models perform accurately on patients with and\nwithout AKI, we evaluated the following clinical cases: (i) predicting\nhyperkalemia after AKI within 14 days of ICU admission, (ii) predicting\nhyperkalemia within 14 days of ICU admission regardless of AKI status, and\ncompared different lead times for (i) and (ii). Both clinical scenarios were\nmodeled using logistic regression (LR), random forest (RF), and XGBoost.\n  Using observations from the first day in the ICU, our models were able to\npredict hyperkalemia with an AUC of (i) 0.79, 0.81, 0.81 and (ii) 0.81, 0.85,\n0.85 for LR, RF, and XGBoost respectively. We found that 4 out of the top 5\nfeatures were consistent across the models. AKI stage was significant in the\nmodels that included all patients with or without AKI, but not in the models\nwhich only included patients with AKI. This suggests that while AKI is\nimportant for hyperkalemia, the specific stage of AKI may not be as important.\nOur findings require further investigation and confirmation.\n", "versions": [{"version": "v1", "created": "Sat, 16 Jan 2021 12:35:27 GMT"}, {"version": "v2", "created": "Wed, 27 Jan 2021 13:51:06 GMT"}], "update_date": "2021-01-28", "authors_parsed": [["Kwak", "Gloria Hyunjung", ""], ["Chen", "Christina", ""], ["Ling", "Lowell", ""], ["Ghosh", "Erina", ""], ["Celi", "Leo Anthony", ""], ["Hui", "Pan", ""]]}, {"id": "2101.06458", "submitter": "Gian Maria Campedelli", "authors": "Gian Maria Campedelli and Maria Rita D'Orsogna", "title": "Temporal Clustering of Disorder Events During the COVID-19 Pandemic", "comments": "37 pages, 16 figures", "journal-ref": "PLOS ONE, 16(4), e0250433 (2021)", "doi": "10.1371/journal.pone.0250433", "report-no": null, "categories": "physics.soc-ph cs.LG econ.GN q-fin.EC stat.AP", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  The COVID-19 pandemic has unleashed multiple public health, socio-economic,\nand institutional crises. Measures taken to slow the spread of the virus have\nfostered significant strain between authorities and citizens, leading to waves\nof social unrest and anti-government demonstrations. We study the temporal\nnature of pandemic-related disorder events as tallied by the \"COVID-19 Disorder\nTracker\" initiative by focusing on the three countries with the largest number\nof incidents, India, Israel, and Mexico. By fitting Poisson and Hawkes\nprocesses to the stream of data, we find that disorder events are\ninter-dependent and self-excite in all three countries. Geographic clustering\nconfirms these features at the subnational level, indicating that nationwide\ndisorders emerge as the convergence of meso-scale patterns of self-excitation.\nConsiderable diversity is observed among countries when computing correlations\nof events between subnational clusters; these are discussed in the context of\nspecific political, societal and geographic characteristics. Israel, the most\nterritorially compact and where large scale protests were coordinated in\nresponse to government lockdowns, displays the largest reactivity and the\nshortest period of influence following an event, as well as the strongest\nnationwide synchrony. In Mexico, where complete lockdown orders were never\nmandated, reactivity and nationwide synchrony are lowest. Our work highlights\nthe need for authorities to promote local information campaigns to ensure that\nlivelihoods and virus containment policies are not perceived as mutually\nexclusive.\n", "versions": [{"version": "v1", "created": "Sat, 16 Jan 2021 15:34:42 GMT"}, {"version": "v2", "created": "Fri, 23 Apr 2021 09:58:09 GMT"}], "update_date": "2021-04-26", "authors_parsed": [["Campedelli", "Gian Maria", ""], ["D'Orsogna", "Maria Rita", ""]]}, {"id": "2101.06459", "submitter": "Subramanyam Natarajan", "authors": "Sumukh Aithal K, Dhruva Kashyap, Natarajan Subramanyam", "title": "Robustness to Augmentations as a Generalization metric", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generalization is the ability of a model to predict on unseen domains and is\na fundamental task in machine learning. Several generalization bounds, both\ntheoretical and empirical have been proposed but they do not provide tight\nbounds .In this work, we propose a simple yet effective method to predict the\ngeneralization performance of a model by using the concept that models that are\nrobust to augmentations are more generalizable than those which are not. We\nexperiment with several augmentations and composition of augmentations to check\nthe generalization capacity of a model. We also provide a detailed motivation\nbehind the proposed method. The proposed generalization metric is calculated\nbased on the change in the output of the model after augmenting the input. The\nproposed method was the first runner up solution for the NeurIPS competition on\nPredicting Generalization in Deep Learning.\n", "versions": [{"version": "v1", "created": "Sat, 16 Jan 2021 15:36:38 GMT"}], "update_date": "2021-01-19", "authors_parsed": [["K", "Sumukh Aithal", ""], ["Kashyap", "Dhruva", ""], ["Subramanyam", "Natarajan", ""]]}, {"id": "2101.06471", "submitter": "Likang Wu", "authors": "Likang Wu, Zhi Li, Hongke Zhao, Qi Liu, Jun Wang, Mengdi Zhang, Enhong\n  Chen", "title": "Learning the Implicit Semantic Representation on Graph-Structured Data", "comments": "16 pages, DASFAA 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Existing representation learning methods in graph convolutional networks are\nmainly designed by describing the neighborhood of each node as a perceptual\nwhole, while the implicit semantic associations behind highly complex\ninteractions of graphs are largely unexploited. In this paper, we propose a\nSemantic Graph Convolutional Networks (SGCN) that explores the implicit\nsemantics by learning latent semantic-paths in graphs. In previous work, there\nare explorations of graph semantics via meta-paths. However, these methods\nmainly rely on explicit heterogeneous information that is hard to be obtained\nin a large amount of graph-structured data. SGCN first breaks through this\nrestriction via leveraging the semantic-paths dynamically and automatically\nduring the node aggregating process. To evaluate our idea, we conduct\nsufficient experiments on several standard datasets, and the empirical results\nshow the superior performance of our model.\n", "versions": [{"version": "v1", "created": "Sat, 16 Jan 2021 16:18:43 GMT"}], "update_date": "2021-01-19", "authors_parsed": [["Wu", "Likang", ""], ["Li", "Zhi", ""], ["Zhao", "Hongke", ""], ["Liu", "Qi", ""], ["Wang", "Jun", ""], ["Zhang", "Mengdi", ""], ["Chen", "Enhong", ""]]}, {"id": "2101.06475", "submitter": "Maxwell Aladago", "authors": "Maxwell Mbabilla Aladago and Lorenzo Torresani", "title": "Slot Machines: Discovering Winning Combinations of Random Weights in\n  Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In contrast to traditional weight optimization in a continuous space, we\ndemonstrate the existence of effective random networks whose weights are never\nupdated. By selecting a weight among a fixed set of random values for each\nindividual connection, our method uncovers combinations of random weights that\nmatch the performance of traditionally-trained networks of the same capacity.\nWe refer to our networks as \"slot machines\" where each reel (connection)\ncontains a fixed set of symbols (random values). Our backpropagation algorithm\n\"spins\" the reels to seek \"winning\" combinations, i.e., selections of random\nweight values that minimize the given loss. Quite surprisingly, we find that\nallocating just a few random values to each connection (e.g., 8 values per\nconnection) yields highly competitive combinations despite being dramatically\nmore constrained compared to traditionally learned weights. Moreover,\nfinetuning these combinations often improves performance over the trained\nbaselines. A randomly initialized VGG-19 with 8 values per connection contains\na combination that achieves 91% test accuracy on CIFAR-10. Our method also\nachieves an impressive performance of 98.2% on MNIST for neural networks\ncontaining only random weights.\n", "versions": [{"version": "v1", "created": "Sat, 16 Jan 2021 16:56:48 GMT"}, {"version": "v2", "created": "Fri, 9 Apr 2021 05:27:33 GMT"}, {"version": "v3", "created": "Tue, 8 Jun 2021 15:10:26 GMT"}], "update_date": "2021-06-09", "authors_parsed": [["Aladago", "Maxwell Mbabilla", ""], ["Torresani", "Lorenzo", ""]]}, {"id": "2101.06476", "submitter": "Arunav Das", "authors": "Arunav Das", "title": "Visual Analytics approach for finding spatiotemporal patterns from\n  COVID19", "comments": "Coursework / concept paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG econ.GN q-fin.EC", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Bounce Back Loan is amongst a number of UK business financial support schemes\nlaunched by UK Government in 2020 amidst pandemic lockdown. Through these\nschemes, struggling businesses are provided financial support to weather\neconomic slowdown from pandemic lockdown. {\\pounds}43.5bn loan value has been\nprovided as of 17th Dec2020. However, with no major checks for granting these\nloans and looming prospect of loan losses from write-offs from failed\nbusinesses and fraud, this paper theorizes prospect of applying spatiotemporal\nmodelling technique to explore if geospatial patterns and temporal analysis\ncould aid design of loan grant criteria for schemes. Application of Clustering\nand Visual Analytics framework to business demographics, survival rate and\nSector concentration shows Inner and Outer London spatial patterns which\nhistoric business failures and reversal of the patterns under COVID-19 implying\nsector influence on spatial clusters. Combination of unsupervised clustering\ntechnique with multinomial logistic regression modelling on research datasets\ncomplimented by additional datasets on other support schemes, business\nstructure and financial crime, is recommended for modelling business\nvulnerability to certain types of financial market or economic condition. The\nlimitations of clustering technique for high dimensional is discussed along\nwith relevance of an applicable model for continuing the research through next\nsteps.\n", "versions": [{"version": "v1", "created": "Sat, 16 Jan 2021 16:57:16 GMT"}], "update_date": "2021-01-19", "authors_parsed": [["Das", "Arunav", ""]]}, {"id": "2101.06480", "submitter": "Byoungjip Kim", "authors": "Byoungjip Kim, Jinho Choo, Yeong-Dae Kwon, Seongho Joe, Seungjai Min,\n  Youngjune Gwon", "title": "SelfMatch: Combining Contrastive Self-Supervision and Consistency for\n  Semi-Supervised Learning", "comments": "4 pages, NeurIPS 2020 Workshop: Self-Supervised Learning - Theory and\n  Practice", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces SelfMatch, a semi-supervised learning method that\ncombines the power of contrastive self-supervised learning and consistency\nregularization. SelfMatch consists of two stages: (1) self-supervised\npre-training based on contrastive learning and (2) semi-supervised fine-tuning\nbased on augmentation consistency regularization. We empirically demonstrate\nthat SelfMatch achieves the state-of-the-art results on standard benchmark\ndatasets such as CIFAR-10 and SVHN. For example, for CIFAR-10 with 40 labeled\nexamples, SelfMatch achieves 93.19% accuracy that outperforms the strong\nprevious methods such as MixMatch (52.46%), UDA (70.95%), ReMixMatch (80.9%),\nand FixMatch (86.19%). We note that SelfMatch can close the gap between\nsupervised learning (95.87%) and semi-supervised learning (93.19%) by using\nonly a few labels for each class.\n", "versions": [{"version": "v1", "created": "Sat, 16 Jan 2021 17:03:53 GMT"}], "update_date": "2021-01-19", "authors_parsed": [["Kim", "Byoungjip", ""], ["Choo", "Jinho", ""], ["Kwon", "Yeong-Dae", ""], ["Joe", "Seongho", ""], ["Min", "Seungjai", ""], ["Gwon", "Youngjune", ""]]}, {"id": "2101.06482", "submitter": "Federica Ferretti", "authors": "Federica Ferretti, Victor Chard\\`es, Thierry Mora, Aleksandra M\n  Walczak, Irene Giardina", "title": "A Renormalization Group Approach to Connect Discrete- and\n  Continuous-Time Descriptions of Gaussian Processes", "comments": "5 pages, 2 figures; 14 pages - Supplemental Material", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cond-mat.stat-mech cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Identifying correct discretization schemes of continuous stochastic processes\nis an important task, which is needed to infer model parameters from\nexperimental observations. Motivated by the observation that consistent\ndiscretizations of continuous models should be invariant under temporal coarse\ngraining, we derive an explicit Renormalization Group transformation on linear\nstochastic time series and show that the Renormalization Group fixed points\ncorrespond to discretizations of naturally occuring physical dynamics. Our\nfixed point analysis explains why standard embedding procedures do not allow\nfor reconstructing hidden Markov dynamics, and why the Euler-Maruyama scheme\napplied to underdamped Langevin equations works for numerical integration, but\nnot to derive the likelihood of a partially observed process in the context of\nparametric inference.\n", "versions": [{"version": "v1", "created": "Sat, 16 Jan 2021 17:11:02 GMT"}, {"version": "v2", "created": "Wed, 20 Jan 2021 11:10:06 GMT"}, {"version": "v3", "created": "Fri, 28 May 2021 10:34:15 GMT"}], "update_date": "2021-05-31", "authors_parsed": [["Ferretti", "Federica", ""], ["Chard\u00e8s", "Victor", ""], ["Mora", "Thierry", ""], ["Walczak", "Aleksandra M", ""], ["Giardina", "Irene", ""]]}, {"id": "2101.06492", "submitter": "Lars Lindemann", "authors": "Alexander Robey, Lars Lindemann, Stephen Tu, and Nikolai Matni", "title": "Learning Robust Hybrid Control Barrier Functions for Uncertain Systems", "comments": "17 pages, 7th IFAC Conference on Analysis and Design of Hybrid\n  Systems (accepted). arXiv admin note: text overlap with arXiv:2011.04112", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SY cs.LG cs.SY math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The need for robust control laws is especially important in safety-critical\napplications. We propose robust hybrid control barrier functions as a means to\nsynthesize control laws that ensure robust safety. Based on this notion, we\nformulate an optimization problem for learning robust hybrid control barrier\nfunctions from data. We identify sufficient conditions on the data such that\nfeasibility of the optimization problem ensures correctness of the learned\nrobust hybrid control barrier functions. Our techniques allow us to safely\nexpand the region of attraction of a compass gait walker that is subject to\nmodel uncertainty.\n", "versions": [{"version": "v1", "created": "Sat, 16 Jan 2021 17:53:35 GMT"}, {"version": "v2", "created": "Thu, 13 May 2021 02:20:12 GMT"}], "update_date": "2021-05-14", "authors_parsed": [["Robey", "Alexander", ""], ["Lindemann", "Lars", ""], ["Tu", "Stephen", ""], ["Matni", "Nikolai", ""]]}, {"id": "2101.06507", "submitter": "Jia Liu", "authors": "Jia Liu and Yaochu Jin", "title": "Multi-objective Search of Robust Neural Architectures against Multiple\n  Types of Adversarial Attacks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many existing deep learning models are vulnerable to adversarial examples\nthat are imperceptible to humans. To address this issue, various methods have\nbeen proposed to design network architectures that are robust to one particular\ntype of adversarial attacks. It is practically impossible, however, to predict\nbeforehand which type of attacks a machine learn model may suffer from. To\naddress this challenge, we propose to search for deep neural architectures that\nare robust to five types of well-known adversarial attacks using a\nmulti-objective evolutionary algorithm. To reduce the computational cost, a\nnormalized error rate of a randomly chosen attack is calculated as the\nrobustness for each newly generated neural architecture at each generation. All\nnon-dominated network architectures obtained by the proposed method are then\nfully trained against randomly chosen adversarial attacks and tested on two\nwidely used datasets. Our experimental results demonstrate the superiority of\noptimized neural architectures found by the proposed approach over\nstate-of-the-art networks that are widely used in the literature in terms of\nthe classification accuracy under different adversarial attacks.\n", "versions": [{"version": "v1", "created": "Sat, 16 Jan 2021 19:38:16 GMT"}], "update_date": "2021-01-19", "authors_parsed": [["Liu", "Jia", ""], ["Jin", "Yaochu", ""]]}, {"id": "2101.06509", "submitter": "Yuhai Tu", "authors": "Yu Feng and Yuhai Tu", "title": "Phases of learning dynamics in artificial neural networks: with or\n  without mislabeled data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cond-mat.stat-mech physics.data-an", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Despite tremendous success of deep neural network in machine learning, the\nunderlying reason for its superior learning capability remains unclear. Here,\nwe present a framework based on statistical physics to study dynamics of\nstochastic gradient descent (SGD) that drives learning in neural networks. By\nusing the minibatch gradient ensemble, we construct order parameters to\ncharacterize dynamics of weight updates in SGD. Without mislabeled data, we\nfind that the SGD learning dynamics transitions from a fast learning phase to a\nslow exploration phase, which is associated with large changes in order\nparameters that characterize the alignment of SGD gradients and their mean\namplitude. In the case with randomly mislabeled samples, SGD learning dynamics\nfalls into four distinct phases. The system first finds solutions for the\ncorrectly labeled samples in phase I, it then wanders around these solutions in\nphase II until it finds a direction to learn the mislabeled samples during\nphase III, after which it finds solutions that satisfy all training samples\nduring phase IV. Correspondingly, the test error decreases during phase I and\nremains low during phase II; however, it increases during phase III and reaches\na high plateau during phase IV. The transitions between different phases can be\nunderstood by changes of order parameters that characterize the alignment of\nmean gradients for the correctly and incorrectly labeled samples and their\n(relative) strength during learning. We find that individual sample losses for\nthe two datasets are most separated during phase II, which leads to a cleaning\nprocess to eliminate mislabeled samples for improving generalization.\n", "versions": [{"version": "v1", "created": "Sat, 16 Jan 2021 19:44:27 GMT"}], "update_date": "2021-01-19", "authors_parsed": [["Feng", "Yu", ""], ["Tu", "Yuhai", ""]]}, {"id": "2101.06511", "submitter": "Yigit Alparslan", "authors": "Yigit Alparslan, Ethan Jacob Moyer, Isamu Mclean Isozaki, Daniel\n  Schwartz, Adam Dunlop, Shesh Dave, Edward Kim", "title": "Towards Searching Efficient and Accurate Neural Network Architectures in\n  Binary Classification Problems", "comments": "8 pages, 11 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  In recent years, deep neural networks have had great success in machine\nlearning and pattern recognition. Architecture size for a neural network\ncontributes significantly to the success of any neural network. In this study,\nwe optimize the selection process by investigating different search algorithms\nto find a neural network architecture size that yields the highest accuracy. We\napply binary search on a very well-defined binary classification network search\nspace and compare the results to those of linear search. We also propose how to\nrelax some of the assumptions regarding the dataset so that our solution can be\ngeneralized to any binary classification problem. We report a 100-fold running\ntime improvement over the naive linear search when we apply the binary search\nmethod to our datasets in order to find the best architecture candidate. By\nfinding the optimal architecture size for any binary classification problem\nquickly, we hope that our research contributes to discovering intelligent\nalgorithms for optimizing architecture size selection in machine learning.\n", "versions": [{"version": "v1", "created": "Sat, 16 Jan 2021 20:00:38 GMT"}], "update_date": "2021-01-19", "authors_parsed": [["Alparslan", "Yigit", ""], ["Moyer", "Ethan Jacob", ""], ["Isozaki", "Isamu Mclean", ""], ["Schwartz", "Daniel", ""], ["Dunlop", "Adam", ""], ["Dave", "Shesh", ""], ["Kim", "Edward", ""]]}, {"id": "2101.06518", "submitter": "Yigit Alparslan", "authors": "Yigit Alparslan, Ethan Jacob Moyer, Edward Kim", "title": "Evaluating Online and Offline Accuracy Traversal Algorithms for\n  k-Complete Neural Network Architectures", "comments": "8 pages, 18 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Architecture sizes for neural networks have been studied widely and several\nsearch methods have been offered to find the best architecture size in the\nshortest amount of time possible. In this paper, we study compact neural\nnetwork architectures for binary classification and investigate improvements in\nspeed and accuracy when favoring overcomplete architecture candidates that have\na very high-dimensional representation of the input. We hypothesize that an\novercomplete model architecture that creates a relatively high-dimensional\nrepresentation of the input will be not only be more accurate but would also be\neasier and faster to find. In an NxM search space, we propose an online\ntraversal algorithm that finds the best architecture candidate in O(1) time for\nbest case and O(N) amortized time for average case for any compact binary\nclassification problem by using k-completeness as heuristics in our search. The\ntwo other offline search algorithms we implement are brute force traversal and\ndiagonal traversal, which both find the best architecture candidate in O(NxM)\ntime. We compare our new algorithm to brute force and diagonal searching as a\nbaseline and report search time improvement of 52.1% over brute force and of\n15.4% over diagonal search to find the most accurate neural network\narchitecture when given the same dataset. In all cases discussed in the paper,\nour online traversal algorithm can find an accurate, if not better,\narchitecture in significantly shorter amount of time.\n", "versions": [{"version": "v1", "created": "Sat, 16 Jan 2021 20:37:29 GMT"}], "update_date": "2021-01-19", "authors_parsed": [["Alparslan", "Yigit", ""], ["Moyer", "Ethan Jacob", ""], ["Kim", "Edward", ""]]}, {"id": "2101.06521", "submitter": "Jesse Zhang", "authors": "Jesse Zhang, Haonan Yu, Wei Xu", "title": "Hierarchical Reinforcement Learning By Discovering Intrinsic Options", "comments": "ICLR 2021. 19 pages, 9 figures. Code at\n  https://www.github.com/jesbu1/hidio", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a hierarchical reinforcement learning method, HIDIO, that can\nlearn task-agnostic options in a self-supervised manner while jointly learning\nto utilize them to solve sparse-reward tasks. Unlike current hierarchical RL\napproaches that tend to formulate goal-reaching low-level tasks or pre-define\nad hoc lower-level policies, HIDIO encourages lower-level option learning that\nis independent of the task at hand, requiring few assumptions or little\nknowledge about the task structure. These options are learned through an\nintrinsic entropy minimization objective conditioned on the option\nsub-trajectories. The learned options are diverse and task-agnostic. In\nexperiments on sparse-reward robotic manipulation and navigation tasks, HIDIO\nachieves higher success rates with greater sample efficiency than regular RL\nbaselines and two state-of-the-art hierarchical RL methods.\n", "versions": [{"version": "v1", "created": "Sat, 16 Jan 2021 20:54:31 GMT"}, {"version": "v2", "created": "Thu, 11 Mar 2021 00:31:24 GMT"}], "update_date": "2021-03-12", "authors_parsed": [["Zhang", "Jesse", ""], ["Yu", "Haonan", ""], ["Xu", "Wei", ""]]}, {"id": "2101.06524", "submitter": "Atousa Zarindast", "authors": "Atousa Zarindast, Anuj Sharma", "title": "Big Data application in congestion detection and classification using\n  Apache spark", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the era of big data, an explosive amount of information is now\navailable. This enormous increase of Big Data in both academia and industry\nrequires large-scale data processing systems. A large body of research is\nbehind optimizing Spark's performance to make it state of the art, a fast and\ngeneral data processing system. Many science and engineering fields have\nadvanced with Big Data analytics, such as Biology, finance, and transportation.\nIntelligent transportation systems (ITS) gain popularity and direct benefit\nfrom the richness of information. The objective is to improve the safety and\nmanagement of transportation networks by reducing congestion and incidents. The\nfirst step toward the goal is better understanding, modeling, and detecting\ncongestion across a network efficiently and effectively. In this study, we\nintroduce an efficient congestion detection model. The underlying network\nconsists of 3017 segments in I-35, I-80, I-29, and I-380 freeways with an\noverall length of 1570 miles and averaged (0.4-0.6) miles per segment. The\nresult of congestion detection shows the proposed method is 90% accurate while\nhas reduced computation time by 99.88%.\n", "versions": [{"version": "v1", "created": "Sat, 16 Jan 2021 21:26:11 GMT"}], "update_date": "2021-01-19", "authors_parsed": [["Zarindast", "Atousa", ""], ["Sharma", "Anuj", ""]]}, {"id": "2101.06536", "submitter": "Chirag Nagpal", "authors": "Chirag Nagpal, Steve Yadlowsky, Negar Rostamzadeh and Katherine Heller", "title": "Deep Cox Mixtures for Survival Regression", "comments": "Machine Learning for Healthcare Conference, 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Survival analysis is a challenging variation of regression modeling because\nof the presence of censoring, where the outcome measurement is only partially\nknown, due to, for example, loss to follow up. Such problems come up frequently\nin medical applications, making survival analysis a key endeavor in\nbiostatistics and machine learning for healthcare, with Cox regression models\nbeing amongst the most commonly employed models. We describe a new approach for\nsurvival analysis regression models, based on learning mixtures of Cox\nregressions to model individual survival distributions. We propose an\napproximation to the Expectation Maximization algorithm for this model that\ndoes hard assignments to mixture groups to make optimization efficient. In each\ngroup assignment, we fit the hazard ratios within each group using deep neural\nnetworks, and the baseline hazard for each mixture component\nnon-parametrically.\n  We perform experiments on multiple real world datasets, and look at the\nmortality rates of patients across ethnicity and gender. We emphasize the\nimportance of calibration in healthcare settings and demonstrate that our\napproach outperforms classical and modern survival analysis baselines, both in\nterms of discriminative performance and calibration, with large gains in\nperformance on the minority demographics.\n", "versions": [{"version": "v1", "created": "Sat, 16 Jan 2021 22:41:22 GMT"}, {"version": "v2", "created": "Mon, 15 Mar 2021 15:28:29 GMT"}, {"version": "v3", "created": "Thu, 22 Jul 2021 11:11:32 GMT"}], "update_date": "2021-07-23", "authors_parsed": [["Nagpal", "Chirag", ""], ["Yadlowsky", "Steve", ""], ["Rostamzadeh", "Negar", ""], ["Heller", "Katherine", ""]]}, {"id": "2101.06541", "submitter": "Kelvin Wong", "authors": "Shuhan Tan, Kelvin Wong, Shenlong Wang, Sivabalan Manivasagam, Mengye\n  Ren, Raquel Urtasun", "title": "SceneGen: Learning to Generate Realistic Traffic Scenes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG cs.RO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We consider the problem of generating realistic traffic scenes automatically.\nExisting methods typically insert actors into the scene according to a set of\nhand-crafted heuristics and are limited in their ability to model the true\ncomplexity and diversity of real traffic scenes, thus inducing a content gap\nbetween synthesized traffic scenes versus real ones. As a result, existing\nsimulators lack the fidelity necessary to train and test self-driving vehicles.\nTo address this limitation, we present SceneGen, a neural autoregressive model\nof traffic scenes that eschews the need for rules and heuristics. In\nparticular, given the ego-vehicle state and a high definition map of\nsurrounding area, SceneGen inserts actors of various classes into the scene and\nsynthesizes their sizes, orientations, and velocities. We demonstrate on two\nlarge-scale datasets SceneGen's ability to faithfully model distributions of\nreal traffic scenes. Moreover, we show that SceneGen coupled with sensor\nsimulation can be used to train perception models that generalize to the real\nworld.\n", "versions": [{"version": "v1", "created": "Sat, 16 Jan 2021 22:51:43 GMT"}], "update_date": "2021-01-19", "authors_parsed": [["Tan", "Shuhan", ""], ["Wong", "Kelvin", ""], ["Wang", "Shenlong", ""], ["Manivasagam", "Sivabalan", ""], ["Ren", "Mengye", ""], ["Urtasun", "Raquel", ""]]}, {"id": "2101.06542", "submitter": "Chandra Maddila", "authors": "Chandra Maddila, Nachiappan Nagappan, Christian Bird, Georgios\n  Gousios, Arie van Deursen", "title": "ConE: A Concurrent Edit Detection Tool for Large ScaleSoftware\n  Development", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SE cs.LG cs.PL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Modern, complex software systems are being continuously extended and\nadjusted. The developers responsible for this may come from different teams or\norganizations, and may be distributed over the world. This may make it\ndifficult to keep track of what other developers are doing, which may result in\nmultiple developers concurrently editing the same code areas. This, in turn,\nmay lead to hard-to-merge changes or even merge conflicts, logical bugs that\nare difficult to detect, duplication of work, and wasted developer\nproductivity. To address this, we explore the extent of this problem in the\npull request based software development model. We study half a year of changes\nmade to six large repositories in Microsoft in which at least 1,000 pull\nrequests are created each month. We find that files concurrently edited in\ndifferent pull requests are more likely to introduce bugs. Motivated by these\nfindings, we design, implement, and deploy a service named ConE (Concurrent\nEdit Detector) that proactively detects pull requests containing concurrent\nedits, to help mitigate the problems caused by them. ConE has been designed to\nscale, and to minimize false alarms while still flagging relevant concurrently\nedited files. Key concepts of ConE include the detection of the Extent of\nOverlap between pull requests, and the identification of Rarely Concurrently\nEdited Files. To evaluate ConE, we report on its operational deployment on 234\nrepositories inside Microsoft. ConE assessed 26,000 pull requests and made 775\nrecommendations about conflicting changes, which were rated as useful in over\n70% (554) of the cases. From interviews with 48 users we learned that they\nbelieved ConE would save time in conflict resolution and avoiding duplicate\nwork, and that over 90% intend to keep using the service on a daily basis.\n", "versions": [{"version": "v1", "created": "Sat, 16 Jan 2021 22:55:44 GMT"}, {"version": "v2", "created": "Fri, 21 May 2021 22:05:52 GMT"}], "update_date": "2021-05-25", "authors_parsed": [["Maddila", "Chandra", ""], ["Nagappan", "Nachiappan", ""], ["Bird", "Christian", ""], ["Gousios", "Georgios", ""], ["van Deursen", "Arie", ""]]}, {"id": "2101.06547", "submitter": "Sergio Casas", "authors": "Alexander Cui, Sergio Casas, Abbas Sadat, Renjie Liao, Raquel Urtasun", "title": "LookOut: Diverse Multi-Future Prediction and Planning for Self-Driving", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AI cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present LookOut, a novel autonomy system that perceives the\nenvironment, predicts a diverse set of futures of how the scene might unroll\nand estimates the trajectory of the SDV by optimizing a set of contingency\nplans over these future realizations. In particular, we learn a diverse joint\ndistribution over multi-agent future trajectories in a traffic scene that\ncovers a wide range of future modes with high sample efficiency while\nleveraging the expressive power of generative models. Unlike previous work in\ndiverse motion forecasting, our diversity objective explicitly rewards sampling\nfuture scenarios that require distinct reactions from the self-driving vehicle\nfor improved safety. Our contingency planner then finds comfortable and\nnon-conservative trajectories that ensure safe reactions to a wide range of\nfuture scenarios. Through extensive evaluations, we show that our model\ndemonstrates significantly more diverse and sample-efficient motion forecasting\nin a large-scale self-driving dataset as well as safer and less-conservative\nmotion plans in long-term closed-loop simulations when compared to current\nstate-of-the-art models.\n", "versions": [{"version": "v1", "created": "Sat, 16 Jan 2021 23:19:22 GMT"}, {"version": "v2", "created": "Thu, 6 May 2021 16:30:19 GMT"}, {"version": "v3", "created": "Fri, 7 May 2021 18:47:54 GMT"}], "update_date": "2021-05-11", "authors_parsed": [["Cui", "Alexander", ""], ["Casas", "Sergio", ""], ["Sadat", "Abbas", ""], ["Liao", "Renjie", ""], ["Urtasun", "Raquel", ""]]}, {"id": "2101.06549", "submitter": "Jingkang Wang", "authors": "Jingkang Wang, Ava Pun, James Tu, Sivabalan Manivasagam, Abbas Sadat,\n  Sergio Casas, Mengye Ren, Raquel Urtasun", "title": "AdvSim: Generating Safety-Critical Scenarios for Self-Driving Vehicles", "comments": "CVPR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AI cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As self-driving systems become better, simulating scenarios where the\nautonomy stack may fail becomes more important. Traditionally, those scenarios\nare generated for a few scenes with respect to the planning module that takes\nground-truth actor states as input. This does not scale and cannot identify all\npossible autonomy failures, such as perception failures due to occlusion. In\nthis paper, we propose AdvSim, an adversarial framework to generate\nsafety-critical scenarios for any LiDAR-based autonomy system. Given an initial\ntraffic scenario, AdvSim modifies the actors' trajectories in a physically\nplausible manner and updates the LiDAR sensor data to match the perturbed\nworld. Importantly, by simulating directly from sensor data, we obtain\nadversarial scenarios that are safety-critical for the full autonomy stack. Our\nexperiments show that our approach is general and can identify thousands of\nsemantically meaningful safety-critical scenarios for a wide range of modern\nself-driving systems. Furthermore, we show that the robustness and safety of\nthese systems can be further improved by training them with scenarios generated\nby AdvSim.\n", "versions": [{"version": "v1", "created": "Sat, 16 Jan 2021 23:23:12 GMT"}, {"version": "v2", "created": "Sun, 4 Apr 2021 03:42:18 GMT"}], "update_date": "2021-04-06", "authors_parsed": [["Wang", "Jingkang", ""], ["Pun", "Ava", ""], ["Tu", "James", ""], ["Manivasagam", "Sivabalan", ""], ["Sadat", "Abbas", ""], ["Casas", "Sergio", ""], ["Ren", "Mengye", ""], ["Urtasun", "Raquel", ""]]}, {"id": "2101.06551", "submitter": "Isa Inuwa-Dutse", "authors": "Isa Inuwa-Dutse, Mark Liptrott, Yannis Korkontzelos", "title": "A multilevel clustering technique for community detection", "comments": "32 pages, 8 figures, journal article", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.SI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  A network is a composition of many communities, i.e., sets of nodes and edges\nwith stronger relationships, with distinct and overlapping properties.\nCommunity detection is crucial for various reasons, such as serving as a\nfunctional unit of a network that captures local interactions among nodes.\nCommunities come in various forms and types, ranging from biologically to\ntechnology-induced ones. As technology-induced communities, social media\nnetworks such as Twitter and Facebook connect a myriad of diverse users,\nleading to a highly connected and dynamic ecosystem. Although many algorithms\nhave been proposed for detecting socially cohesive communities on Twitter,\nmining and related tasks remain challenging. This study presents a novel\ndetection method based on a scalable framework to identify related communities\nin a network. We propose a multilevel clustering technique (MCT) that leverages\nstructural and textual information to identify local communities termed\nmicrocosms. Experimental evaluation on benchmark models and datasets\ndemonstrate the efficacy of the approach. This study contributes a new\ndimension for the detection of cohesive communities in social networks. The\napproach offers a better understanding and clarity toward describing how\nlow-level communities evolve and behave on Twitter. From an application point\nof view, identifying such communities can better inform recommendation, among\nother benefits.\n", "versions": [{"version": "v1", "created": "Sat, 16 Jan 2021 23:26:44 GMT"}], "update_date": "2021-01-19", "authors_parsed": [["Inuwa-Dutse", "Isa", ""], ["Liptrott", "Mark", ""], ["Korkontzelos", "Yannis", ""]]}, {"id": "2101.06553", "submitter": "Yuwen Xiong", "authors": "Yuwen Xiong, Mengye Ren, Wenyuan Zeng, Raquel Urtasun", "title": "Self-Supervised Representation Learning from Flow Equivariance", "comments": "tech report", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Self-supervised representation learning is able to learn semantically\nmeaningful features; however, much of its recent success relies on multiple\ncrops of an image with very few objects. Instead of learning view-invariant\nrepresentation from simple images, humans learn representations in a complex\nworld with changing scenes by observing object movement, deformation, pose\nvariation, and ego motion. Motivated by this ability, we present a new\nself-supervised learning representation framework that can be directly deployed\non a video stream of complex scenes with many moving objects. Our framework\nfeatures a simple flow equivariance objective that encourages the network to\npredict the features of another frame by applying a flow transformation to the\nfeatures of the current frame. Our representations, learned from\nhigh-resolution raw video, can be readily used for downstream tasks on static\nimages. Readout experiments on challenging semantic segmentation, instance\nsegmentation, and object detection benchmarks show that we are able to\noutperform representations obtained from previous state-of-the-art methods\nincluding SimCLR and BYOL.\n", "versions": [{"version": "v1", "created": "Sat, 16 Jan 2021 23:44:09 GMT"}], "update_date": "2021-01-19", "authors_parsed": [["Xiong", "Yuwen", ""], ["Ren", "Mengye", ""], ["Zeng", "Wenyuan", ""], ["Urtasun", "Raquel", ""]]}, {"id": "2101.06554", "submitter": "Abbas Sadat", "authors": "Abbas Sadat, Sean Segal, Sergio Casas, James Tu, Bin Yang, Raquel\n  Urtasun, Ersin Yumer", "title": "Diverse Complexity Measures for Dataset Curation in Self-driving", "comments": "13 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modern self-driving autonomy systems heavily rely on deep learning. As a\nconsequence, their performance is influenced significantly by the quality and\nrichness of the training data. Data collecting platforms can generate many\nhours of raw data in a daily basis, however, it is not feasible to label\neverything. It is thus of key importance to have a mechanism to identify \"what\nto label\". Active learning approaches identify examples to label, but their\ninterestingness is tied to a fixed model performing a particular task. These\nassumptions are not valid in self-driving, where we have to solve a diverse set\nof tasks (i.e., perception, and motion forecasting) and our models evolve over\ntime frequently. In this paper we introduce a novel approach and propose a new\ndata selection method that exploits a diverse set of criteria that quantize\ninterestingness of traffic scenes. Our experiments on a wide range of tasks and\nmodels show that the proposed curation pipeline is able to select datasets that\nlead to better generalization and higher performance.\n", "versions": [{"version": "v1", "created": "Sat, 16 Jan 2021 23:45:02 GMT"}], "update_date": "2021-01-19", "authors_parsed": [["Sadat", "Abbas", ""], ["Segal", "Sean", ""], ["Casas", "Sergio", ""], ["Tu", "James", ""], ["Yang", "Bin", ""], ["Urtasun", "Raquel", ""], ["Yumer", "Ersin", ""]]}, {"id": "2101.06557", "submitter": "Simon Suo", "authors": "Simon Suo, Sebastian Regalado, Sergio Casas, Raquel Urtasun", "title": "TrafficSim: Learning to Simulate Realistic Multi-Agent Behaviors", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AI cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Simulation has the potential to massively scale evaluation of self-driving\nsystems enabling rapid development as well as safe deployment. To close the gap\nbetween simulation and the real world, we need to simulate realistic\nmulti-agent behaviors. Existing simulation environments rely on heuristic-based\nmodels that directly encode traffic rules, which cannot capture irregular\nmaneuvers (e.g., nudging, U-turns) and complex interactions (e.g., yielding,\nmerging). In contrast, we leverage real-world data to learn directly from human\ndemonstration and thus capture a more diverse set of actor behaviors. To this\nend, we propose TrafficSim, a multi-agent behavior model for realistic traffic\nsimulation. In particular, we leverage an implicit latent variable model to\nparameterize a joint actor policy that generates socially-consistent plans for\nall actors in the scene jointly. To learn a robust policy amenable for long\nhorizon simulation, we unroll the policy in training and optimize through the\nfully differentiable simulation across time. Our learning objective\nincorporates both human demonstrations as well as common sense. We show\nTrafficSim generates significantly more realistic and diverse traffic scenarios\nas compared to a diverse set of baselines. Notably, we can exploit trajectories\ngenerated by TrafficSim as effective data augmentation for training better\nmotion planner.\n", "versions": [{"version": "v1", "created": "Sun, 17 Jan 2021 00:29:30 GMT"}], "update_date": "2021-01-19", "authors_parsed": [["Suo", "Simon", ""], ["Regalado", "Sebastian", ""], ["Casas", "Sergio", ""], ["Urtasun", "Raquel", ""]]}, {"id": "2101.06558", "submitter": "Rahul Paropkari", "authors": "Rahul Arun Paropkari, Anurag Thantharate, Cory Beard", "title": "Deep-Mobility: A Deep Learning Approach for an Efficient and Reliable 5G\n  Handover", "comments": "This paper was accepted at the 29th ICCCN 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DC cs.NE", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  5G cellular networks are being deployed all over the world and this\narchitecture supports ultra-dense network (UDN) deployment. Small cells have a\nvery important role in providing 5G connectivity to the end users. Exponential\nincreases in devices, data and network demands make it mandatory for the\nservice providers to manage handovers better, to cater to the services that a\nuser desire. In contrast to any traditional handover improvement scheme, we\ndevelop a 'Deep-Mobility' model by implementing a deep learning neural network\n(DLNN) to manage network mobility, utilizing in-network deep learning and\nprediction. We use network key performance indicators (KPIs) to train our model\nto analyze network traffic and handover requirements. In this method, RF signal\nconditions are continuously observed and tracked using deep learning neural\nnetworks such as the Recurrent neural network (RNN) or Long Short-Term Memory\nnetwork (LSTM) and system level inputs are also considered in conjunction, to\ntake a collective decision for a handover. We can study multiple parameters and\ninteractions between system events along with the user mobility, which would\nthen trigger a handoff in any given scenario. Here, we show the fundamental\nmodeling approach and demonstrate usefulness of our model while investigating\nimpacts and sensitivities of certain KPIs from the user equipment (UE) and\nnetwork side.\n", "versions": [{"version": "v1", "created": "Sun, 17 Jan 2021 00:31:37 GMT"}, {"version": "v2", "created": "Tue, 19 Jan 2021 01:19:11 GMT"}], "update_date": "2021-01-20", "authors_parsed": [["Paropkari", "Rahul Arun", ""], ["Thantharate", "Anurag", ""], ["Beard", "Cory", ""]]}, {"id": "2101.06560", "submitter": "James Tu", "authors": "James Tu, Tsunhsuan Wang, Jingkang Wang, Sivabalan Manivasagam, Mengye\n  Ren, Raquel Urtasun", "title": "Adversarial Attacks On Multi-Agent Communication", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Growing at a very fast pace, modern autonomous systems will soon be deployed\nat scale, opening up the possibility for cooperative multi-agent systems. By\nsharing information and distributing workloads, autonomous agents can better\nperform their tasks and enjoy improved computation efficiency. However, such\nadvantages rely heavily on communication channels which have been shown to be\nvulnerable to security breaches. Thus, communication can be compromised to\nexecute adversarial attacks on deep learning models which are widely employed\nin modern systems. In this paper, we explore such adversarial attacks in a\nnovel multi-agent setting where agents communicate by sharing learned\nintermediate representations. We observe that an indistinguishable adversarial\nmessage can severely degrade performance, but becomes weaker as the number of\nbenign agents increase. Furthermore, we show that transfer attacks are more\ndifficult in this setting when compared to directly perturbing the inputs, as\nit is necessary to align the distribution of communication messages with domain\nadaptation. Finally, we show that low-budget online attacks can be achieved by\nexploiting the temporal consistency of streaming sensory inputs.\n", "versions": [{"version": "v1", "created": "Sun, 17 Jan 2021 00:35:26 GMT"}], "update_date": "2021-01-19", "authors_parsed": [["Tu", "James", ""], ["Wang", "Tsunhsuan", ""], ["Wang", "Jingkang", ""], ["Manivasagam", "Sivabalan", ""], ["Ren", "Mengye", ""], ["Urtasun", "Raquel", ""]]}, {"id": "2101.06564", "submitter": "Sharare Zehtabian", "authors": "Sharare Zehtabian, Siavash Khodadadeh, Ladislau B\\\"ol\\\"oni and Damla\n  Turgut", "title": "Privacy-Preserving Learning of Human Activity Predictors in Smart\n  Environments", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The daily activities performed by a disabled or elderly person can be\nmonitored by a smart environment, and the acquired data can be used to learn a\npredictive model of user behavior. To speed up the learning, several\nresearchers designed collaborative learning systems that use data from multiple\nusers. However, disclosing the daily activities of an elderly or disabled user\nraises privacy concerns. In this paper, we use state-of-the-art deep neural\nnetwork-based techniques to learn predictive human activity models in the\nlocal, centralized, and federated learning settings. A novel aspect of our work\nis that we carefully track the temporal evolution of the data available to the\nlearner and the data shared by the user. In contrast to previous work where\nusers shared all their data with the centralized learner, we consider users\nthat aim to preserve their privacy. Thus, they choose between approaches in\norder to achieve their goals of predictive accuracy while minimizing the shared\ndata. To help users make decisions before disclosing any data, we use machine\nlearning to predict the degree to which a user would benefit from collaborative\nlearning. We validate our approaches on real-world data.\n", "versions": [{"version": "v1", "created": "Sun, 17 Jan 2021 01:04:53 GMT"}], "update_date": "2021-01-19", "authors_parsed": [["Zehtabian", "Sharare", ""], ["Khodadadeh", "Siavash", ""], ["B\u00f6l\u00f6ni", "Ladislau", ""], ["Turgut", "Damla", ""]]}, {"id": "2101.06570", "submitter": "Iyiola E. Olatunji", "authors": "Iyiola E. Olatunji, Wolfgang Nejdl and Megha Khosla", "title": "Membership Inference Attack on Graph Neural Networks", "comments": "Relaxation of assumptions, reasons why models are robust to MI", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Graph Neural Networks (GNNs), which generalize traditional deep neural\nnetworks or graph data, have achieved state-of-the-art performance on several\ngraph analytical tasks like node classification, link prediction, or graph\nclassification. We focus on how trained GNN models could leak information about\nthe \\emph{member} nodes that they were trained on. We introduce two realistic\ninductive settings for carrying out a membership inference (MI) attack on GNNs.\nWhile choosing the simplest possible attack model that utilizes the posteriors\nof the trained model, we thoroughly analyze the properties of GNNs which\ndictate the differences in their robustness towards MI attack. The surprising\nand worrying fact is that the attack is successful even if the target model\ngeneralizes well. While in traditional machine learning models, overfitting is\nconsidered the main cause of such leakage, we show that in GNNs the additional\nstructural information is the major contributing factor. We support our\nfindings by extensive experiments on four representative GNN models. On a\npositive note, we identify properties of certain models which make them less\nvulnerable to MI attacks than others.\n", "versions": [{"version": "v1", "created": "Sun, 17 Jan 2021 02:12:35 GMT"}, {"version": "v2", "created": "Mon, 22 Mar 2021 21:12:16 GMT"}], "update_date": "2021-03-24", "authors_parsed": [["Olatunji", "Iyiola E.", ""], ["Nejdl", "Wolfgang", ""], ["Khosla", "Megha", ""]]}, {"id": "2101.06580", "submitter": "Zhaobin Mo", "authors": "Rongye Shi, Zhaobin Mo, Kuang Huang, Xuan Di, Qiang Du", "title": "Physics-Informed Deep Learning for Traffic State Estimation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Traffic state estimation (TSE), which reconstructs the traffic variables\n(e.g., density) on road segments using partially observed data, plays an\nimportant role on efficient traffic control and operation that intelligent\ntransportation systems (ITS) need to provide to people. Over decades, TSE\napproaches bifurcate into two main categories, model-driven approaches and\ndata-driven approaches. However, each of them has limitations: the former\nhighly relies on existing physical traffic flow models, such as\nLighthill-Whitham-Richards (LWR) models, which may only capture limited\ndynamics of real-world traffic, resulting in low-quality estimation, while the\nlatter requires massive data in order to perform accurate and generalizable\nestimation. To mitigate the limitations, this paper introduces a\nphysics-informed deep learning (PIDL) framework to efficiently conduct\nhigh-quality TSE with small amounts of observed data. PIDL contains both\nmodel-driven and data-driven components, making possible the integration of the\nstrong points of both approaches while overcoming the shortcomings of either.\nThis paper focuses on highway TSE with observed data from loop detectors, using\ntraffic density as the traffic variables. We demonstrate the use of PIDL to\nsolve (with data from loop detectors) two popular physical traffic flow models,\ni.e., Greenshields-based LWR and three-parameter-based LWR, and discover the\nmodel parameters. We then evaluate the PIDL-based highway TSE using the Next\nGeneration SIMulation (NGSIM) dataset. The experimental results show the\nadvantages of the PIDL-based approach in terms of estimation accuracy and data\nefficiency over advanced baseline TSE methods.\n", "versions": [{"version": "v1", "created": "Sun, 17 Jan 2021 03:28:32 GMT"}], "update_date": "2021-01-19", "authors_parsed": [["Shi", "Rongye", ""], ["Mo", "Zhaobin", ""], ["Huang", "Kuang", ""], ["Di", "Xuan", ""], ["Du", "Qiang", ""]]}, {"id": "2101.06589", "submitter": "Juntao Huang", "authors": "Juntao Huang and Yizhou Zhou and Wen-An Yong", "title": "Data-driven discovery of multiscale chemical reactions governed by the\n  law of mass action", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.chem-ph cs.LG cs.NA math.NA math.OC physics.comp-ph", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper, we propose a data-driven method to discover multiscale\nchemical reactions governed by the law of mass action. First, we use a single\nmatrix to represent the stoichiometric coefficients for both the reactants and\nproducts in a system without catalysis reactions. The negative entries in the\nmatrix denote the stoichiometric coefficients for the reactants and the\npositive ones for the products. Second, we find that the conventional\noptimization methods usually get stuck in the local minima and could not find\nthe true solution in learning the multiscale chemical reactions. To overcome\nthis difficulty, we propose a partial-parameters-freezing (PPF) technique to\nprogressively determine the network parameters by using the fact that the\nstoichiometric coefficients are integers. With such a technique, the dimension\nof the searching space is gradually reduced in the training process and the\nglobal mimina can be eventually obtained. Several numerical experiments\nincluding the classical Michaelis-Menten kinetics and the hydrogen oxidation\nreactions verify the good performance of our algorithm in learning the\nmultiscale chemical reactions. The code is available at\n\\url{https://github.com/JuntaoHuang/multiscale-chemical-reaction}.\n", "versions": [{"version": "v1", "created": "Sun, 17 Jan 2021 04:53:30 GMT"}, {"version": "v2", "created": "Tue, 2 Feb 2021 03:33:30 GMT"}], "update_date": "2021-02-03", "authors_parsed": [["Huang", "Juntao", ""], ["Zhou", "Yizhou", ""], ["Yong", "Wen-An", ""]]}, {"id": "2101.06590", "submitter": "Jingkang Wang", "authors": "Jingkang Wang, Mengye Ren, Ilija Bogunovic, Yuwen Xiong, Raquel\n  Urtasun", "title": "Cost-Efficient Online Hyperparameter Optimization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent work on hyperparameters optimization (HPO) has shown the possibility\nof training certain hyperparameters together with regular parameters. However,\nthese online HPO algorithms still require running evaluation on a set of\nvalidation examples at each training step, steeply increasing the training\ncost. To decide when to query the validation loss, we model online HPO as a\ntime-varying Bayesian optimization problem, on top of which we propose a novel\n\\textit{costly feedback} setting to capture the concept of the query cost.\nUnder this setting, standard algorithms are cost-inefficient as they evaluate\non the validation set at every round. In contrast, the cost-efficient GP-UCB\nalgorithm proposed in this paper queries the unknown function only when the\nmodel is less confident about current decisions. We evaluate our proposed\nalgorithm by tuning hyperparameters online for VGG and ResNet on CIFAR-10 and\nImageNet100. Our proposed online HPO algorithm reaches human expert-level\nperformance within a single run of the experiment, while incurring only modest\ncomputational overhead compared to regular training.\n", "versions": [{"version": "v1", "created": "Sun, 17 Jan 2021 04:55:30 GMT"}], "update_date": "2021-01-19", "authors_parsed": [["Wang", "Jingkang", ""], ["Ren", "Mengye", ""], ["Bogunovic", "Ilija", ""], ["Xiong", "Yuwen", ""], ["Urtasun", "Raquel", ""]]}, {"id": "2101.06592", "submitter": "Simon Mak", "authors": "Simon Mak, Yuanshuo Zhou, Lavonne Hoang, C. F. Jeff Wu", "title": "TSEC: a framework for online experimentation under experimental\n  constraints", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Thompson sampling is a popular algorithm for solving multi-armed bandit\nproblems, and has been applied in a wide range of applications, from website\ndesign to portfolio optimization. In such applications, however, the number of\nchoices (or arms) $N$ can be large, and the data needed to make adaptive\ndecisions require expensive experimentation. One is then faced with the\nconstraint of experimenting on only a small subset of $K \\ll N$ arms within\neach time period, which poses a problem for traditional Thompson sampling. We\npropose a new Thompson Sampling under Experimental Constraints (TSEC) method,\nwhich addresses this so-called \"arm budget constraint\". TSEC makes use of a\nBayesian interaction model with effect hierarchy priors, to model correlations\nbetween rewards on different arms. This fitted model is then integrated within\nThompson sampling, to jointly identify a good subset of arms for\nexperimentation and to allocate resources over these arms. We demonstrate the\neffectiveness of TSEC in two problems with arm budget constraints. The first is\na simulated website optimization study, where TSEC shows noticeable\nimprovements over industry benchmarks. The second is a portfolio optimization\napplication on industry-based exchange-traded funds, where TSEC provides more\nconsistent and greater wealth accumulation over standard investment strategies.\n", "versions": [{"version": "v1", "created": "Sun, 17 Jan 2021 05:04:12 GMT"}], "update_date": "2021-01-19", "authors_parsed": [["Mak", "Simon", ""], ["Zhou", "Yuanshuo", ""], ["Hoang", "Lavonne", ""], ["Wu", "C. F. Jeff", ""]]}, {"id": "2101.06605", "submitter": "Jiahui Huang", "authors": "Jiahui Huang, He Wang, Tolga Birdal, Minhyuk Sung, Federica Arrigoni,\n  Shi-Min Hu, Leonidas Guibas", "title": "MultiBodySync: Multi-Body Segmentation and Motion Estimation via 3D Scan\n  Synchronization", "comments": "Contact: huang-jh18<at>mails<dot>tsinghua<dot>edu<dot>cn", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We present MultiBodySync, a novel, end-to-end trainable multi-body motion\nsegmentation and rigid registration framework for multiple input 3D point\nclouds. The two non-trivial challenges posed by this multi-scan multibody\nsetting that we investigate are: (i) guaranteeing correspondence and\nsegmentation consistency across multiple input point clouds capturing different\nspatial arrangements of bodies or body parts; and (ii) obtaining robust\nmotion-based rigid body segmentation applicable to novel object categories. We\npropose an approach to address these issues that incorporates spectral\nsynchronization into an iterative deep declarative network, so as to\nsimultaneously recover consistent correspondences as well as motion\nsegmentation. At the same time, by explicitly disentangling the correspondence\nand motion segmentation estimation modules, we achieve strong generalizability\nacross different object categories. Our extensive evaluations demonstrate that\nour method is effective on various datasets ranging from rigid parts in\narticulated objects to individually moving objects in a 3D scene, be it\nsingle-view or full point clouds.\n", "versions": [{"version": "v1", "created": "Sun, 17 Jan 2021 06:36:28 GMT"}, {"version": "v2", "created": "Fri, 26 Mar 2021 15:12:13 GMT"}, {"version": "v3", "created": "Mon, 29 Mar 2021 01:41:07 GMT"}], "update_date": "2021-03-30", "authors_parsed": [["Huang", "Jiahui", ""], ["Wang", "He", ""], ["Birdal", "Tolga", ""], ["Sung", "Minhyuk", ""], ["Arrigoni", "Federica", ""], ["Hu", "Shi-Min", ""], ["Guibas", "Leonidas", ""]]}, {"id": "2101.06614", "submitter": "Anqi Liu", "authors": "Anqi Liu, Hao Liu, Tongxin Li, Saeed Karimi-Bidhendi, Yisong Yue,\n  Anima Anandkumar", "title": "Disentangling Observed Causal Effects from Latent Confounders using\n  Method of Moments", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Discovering the complete set of causal relations among a group of variables\nis a challenging unsupervised learning problem. Often, this challenge is\ncompounded by the fact that there are latent or hidden confounders. When only\nobservational data is available, the problem is ill-posed, i.e. the causal\nrelationships are non-identifiable unless strong modeling assumptions are made.\nWhen interventions are available, we provide guarantees on identifiability and\nlearnability under mild assumptions. We assume a linear structural equation\nmodel (SEM) with independent latent factors and directed acyclic graph (DAG)\nrelationships among the observables. Since the latent variable inference is\nbased on independent component analysis (ICA), we call this model SEM-ICA. We\nuse the method of moments principle to establish model identifiability. We\ndevelop efficient algorithms based on coupled tensor decomposition with linear\nconstraints to obtain scalable and guaranteed solutions. Thus, we provide a\nprincipled approach to tackling the joint problem of causal discovery and\nlatent variable inference.\n", "versions": [{"version": "v1", "created": "Sun, 17 Jan 2021 07:48:45 GMT"}], "update_date": "2021-01-19", "authors_parsed": [["Liu", "Anqi", ""], ["Liu", "Hao", ""], ["Li", "Tongxin", ""], ["Karimi-Bidhendi", "Saeed", ""], ["Yue", "Yisong", ""], ["Anandkumar", "Anima", ""]]}, {"id": "2101.06619", "submitter": "Ruiyang Xu", "authors": "Ruiyang Xu, Karl Lieberherr", "title": "Solving QSAT problems with neural MCTS", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Recent achievements from AlphaZero using self-play has shown remarkable\nperformance on several board games. It is plausible to think that self-play,\nstarting from zero knowledge, can gradually approximate a winning strategy for\ncertain two-player games after an amount of training. In this paper, we try to\nleverage the computational power of neural Monte Carlo Tree Search (neural\nMCTS), the core algorithm from AlphaZero, to solve Quantified Boolean Formula\nSatisfaction (QSAT) problems, which are PSPACE complete. Knowing that every\nQSAT problem is equivalent to a QSAT game, the game outcome can be used to\nderive the solutions of the original QSAT problems. We propose a way to encode\nQuantified Boolean Formulas (QBFs) as graphs and apply a graph neural network\n(GNN) to embed the QBFs into the neural MCTS. After training, an off-the-shelf\nQSAT solver is used to evaluate the performance of the algorithm. Our result\nshows that, for problems within a limited size, the algorithm learns to solve\nthe problem correctly merely from self-play.\n", "versions": [{"version": "v1", "created": "Sun, 17 Jan 2021 08:20:07 GMT"}], "update_date": "2021-01-19", "authors_parsed": [["Xu", "Ruiyang", ""], ["Lieberherr", "Karl", ""]]}, {"id": "2101.06634", "submitter": "Ardhendu Behera", "authors": "Ardhendu Behera, Zachary Wharton, Morteza Ghahremani, Swagat Kumar,\n  Nik Bessis", "title": "Regional Attention Network (RAN) for Head Pose and Fine-grained Gesture\n  Recognition", "comments": "This manuscript is the accepted version of the published paper in\n  IEEE Transaction on Affective Computing", "journal-ref": "IEEE Transaction on Affective Computing 2020", "doi": "10.1109/TAFFC.2020.3031841", "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Affect is often expressed via non-verbal body language such as\nactions/gestures, which are vital indicators for human behaviors. Recent\nstudies on recognition of fine-grained actions/gestures in monocular images\nhave mainly focused on modeling spatial configuration of body parts\nrepresenting body pose, human-objects interactions and variations in local\nappearance. The results show that this is a brittle approach since it relies on\naccurate body parts/objects detection. In this work, we argue that there exist\nlocal discriminative semantic regions, whose \"informativeness\" can be evaluated\nby the attention mechanism for inferring fine-grained gestures/actions. To this\nend, we propose a novel end-to-end \\textbf{Regional Attention Network (RAN)},\nwhich is a fully Convolutional Neural Network (CNN) to combine multiple\ncontextual regions through attention mechanism, focusing on parts of the images\nthat are most relevant to a given task. Our regions consist of one or more\nconsecutive cells and are adapted from the strategies used in computing HOG\n(Histogram of Oriented Gradient) descriptor. The model is extensively evaluated\non ten datasets belonging to 3 different scenarios: 1) head pose recognition,\n2) drivers state recognition, and 3) human action and facial expression\nrecognition. The proposed approach outperforms the state-of-the-art by a\nconsiderable margin in different metrics.\n", "versions": [{"version": "v1", "created": "Sun, 17 Jan 2021 10:14:28 GMT"}], "update_date": "2021-01-19", "authors_parsed": [["Behera", "Ardhendu", ""], ["Wharton", "Zachary", ""], ["Ghahremani", "Morteza", ""], ["Kumar", "Swagat", ""], ["Bessis", "Nik", ""]]}, {"id": "2101.06635", "submitter": "Ardhendu Behera", "authors": "Ardhendu Behera, Zachary Wharton, Pradeep Hewage, Asish Bera", "title": "Context-aware Attentional Pooling (CAP) for Fine-grained Visual\n  Classification", "comments": "Extended version of the accepted paper in 35th AAAI Conference on\n  Artificial Intelligence 2021", "journal-ref": "35th AAAI Conference on Artificial Intelligence 2021", "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Deep convolutional neural networks (CNNs) have shown a strong ability in\nmining discriminative object pose and parts information for image recognition.\nFor fine-grained recognition, context-aware rich feature representation of\nobject/scene plays a key role since it exhibits a significant variance in the\nsame subcategory and subtle variance among different subcategories. Finding the\nsubtle variance that fully characterizes the object/scene is not\nstraightforward. To address this, we propose a novel context-aware attentional\npooling (CAP) that effectively captures subtle changes via sub-pixel gradients,\nand learns to attend informative integral regions and their importance in\ndiscriminating different subcategories without requiring the bounding-box\nand/or distinguishable part annotations. We also introduce a novel feature\nencoding by considering the intrinsic consistency between the informativeness\nof the integral regions and their spatial structures to capture the semantic\ncorrelation among them. Our approach is simple yet extremely effective and can\nbe easily applied on top of a standard classification backbone network. We\nevaluate our approach using six state-of-the-art (SotA) backbone networks and\neight benchmark datasets. Our method significantly outperforms the SotA\napproaches on six datasets and is very competitive with the remaining two.\n", "versions": [{"version": "v1", "created": "Sun, 17 Jan 2021 10:15:02 GMT"}], "update_date": "2021-01-19", "authors_parsed": [["Behera", "Ardhendu", ""], ["Wharton", "Zachary", ""], ["Hewage", "Pradeep", ""], ["Bera", "Asish", ""]]}, {"id": "2101.06636", "submitter": "Ardhendu Behera", "authors": "Zachary Wharton, Ardhendu Behera, Yonghuai Liu, Nik Bessis", "title": "Coarse Temporal Attention Network (CTA-Net) for Driver's Activity\n  Recognition", "comments": "Extended version of the accepted WACV 2021", "journal-ref": "Winter Conference on Applications of Computer Vision (WACV 2021)", "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  There is significant progress in recognizing traditional human activities\nfrom videos focusing on highly distinctive actions involving discriminative\nbody movements, body-object and/or human-human interactions. Driver's\nactivities are different since they are executed by the same subject with\nsimilar body parts movements, resulting in subtle changes. To address this, we\npropose a novel framework by exploiting the spatiotemporal attention to model\nthe subtle changes. Our model is named Coarse Temporal Attention Network\n(CTA-Net), in which coarse temporal branches are introduced in a trainable\nglimpse network. The goal is to allow the glimpse to capture high-level\ntemporal relationships, such as 'during', 'before' and 'after' by focusing on a\nspecific part of a video. These branches also respect the topology of the\ntemporal dynamics in the video, ensuring that different branches learn\nmeaningful spatial and temporal changes. The model then uses an innovative\nattention mechanism to generate high-level action specific contextual\ninformation for activity recognition by exploring the hidden states of an LSTM.\nThe attention mechanism helps in learning to decide the importance of each\nhidden state for the recognition task by weighing them when constructing the\nrepresentation of the video. Our approach is evaluated on four publicly\naccessible datasets and significantly outperforms the state-of-the-art by a\nconsiderable margin with only RGB video as input.\n", "versions": [{"version": "v1", "created": "Sun, 17 Jan 2021 10:15:37 GMT"}], "update_date": "2021-01-19", "authors_parsed": [["Wharton", "Zachary", ""], ["Behera", "Ardhendu", ""], ["Liu", "Yonghuai", ""], ["Bessis", "Nik", ""]]}, {"id": "2101.06639", "submitter": "Saehyung Lee", "authors": "Saehyung Lee, Changhwa Park, Hyungyu Lee, Jihun Yi, Jonghyun Lee,\n  Sungroh Yoon", "title": "Removing Undesirable Feature Contributions Using Out-of-Distribution\n  Data", "comments": "Published as a conference paper at ICLR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Several data augmentation methods deploy unlabeled-in-distribution (UID) data\nto bridge the gap between the training and inference of neural networks.\nHowever, these methods have clear limitations in terms of availability of UID\ndata and dependence of algorithms on pseudo-labels. Herein, we propose a data\naugmentation method to improve generalization in both adversarial and standard\nlearning by using out-of-distribution (OOD) data that are devoid of the\nabovementioned issues. We show how to improve generalization theoretically\nusing OOD data in each learning scenario and complement our theoretical\nanalysis with experiments on CIFAR-10, CIFAR-100, and a subset of ImageNet. The\nresults indicate that undesirable features are shared even among image data\nthat seem to have little correlation from a human point of view. We also\npresent the advantages of the proposed method through comparison with other\ndata augmentation methods, which can be used in the absence of UID data.\nFurthermore, we demonstrate that the proposed method can further improve the\nexisting state-of-the-art adversarial training.\n", "versions": [{"version": "v1", "created": "Sun, 17 Jan 2021 10:26:34 GMT"}, {"version": "v2", "created": "Wed, 3 Mar 2021 05:40:51 GMT"}], "update_date": "2021-03-04", "authors_parsed": [["Lee", "Saehyung", ""], ["Park", "Changhwa", ""], ["Lee", "Hyungyu", ""], ["Yi", "Jihun", ""], ["Lee", "Jonghyun", ""], ["Yoon", "Sungroh", ""]]}, {"id": "2101.06640", "submitter": "Hrayr Harutyunyan", "authors": "Hrayr Harutyunyan, Alessandro Achille, Giovanni Paolini, Orchid\n  Majumder, Avinash Ravichandran, Rahul Bhotika, Stefano Soatto", "title": "Estimating informativeness of samples with Smooth Unique Information", "comments": "ICLR 2021, 22 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We define a notion of information that an individual sample provides to the\ntraining of a neural network, and we specialize it to measure both how much a\nsample informs the final weights and how much it informs the function computed\nby the weights. Though related, we show that these quantities have a\nqualitatively different behavior. We give efficient approximations of these\nquantities using a linearized network and demonstrate empirically that the\napproximation is accurate for real-world architectures, such as pre-trained\nResNets. We apply these measures to several problems, such as dataset\nsummarization, analysis of under-sampled classes, comparison of informativeness\nof different data sources, and detection of adversarial and corrupted examples.\nOur work generalizes existing frameworks but enjoys better computational\nproperties for heavily over-parametrized models, which makes it possible to\napply it to real-world networks.\n", "versions": [{"version": "v1", "created": "Sun, 17 Jan 2021 10:29:29 GMT"}, {"version": "v2", "created": "Sun, 28 Mar 2021 08:24:40 GMT"}], "update_date": "2021-03-30", "authors_parsed": [["Harutyunyan", "Hrayr", ""], ["Achille", "Alessandro", ""], ["Paolini", "Giovanni", ""], ["Majumder", "Orchid", ""], ["Ravichandran", "Avinash", ""], ["Bhotika", "Rahul", ""], ["Soatto", "Stefano", ""]]}, {"id": "2101.06650", "submitter": "Liang Liao", "authors": "Liang Liao, Xuechun Zhang, Xinqiang Wang, Sen Lin, Xin Liu", "title": "Generalized Image Reconstruction over T-Algebra", "comments": "6 pages, 4 figures, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.MM math.AC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Principal Component Analysis (PCA) is well known for its capability of\ndimension reduction and data compression. However, when using PCA for\ncompressing/reconstructing images, images need to be recast to vectors. The\nvectorization of images makes some correlation constraints of neighboring\npixels and spatial information lost. To deal with the drawbacks of the\nvectorizations adopted by PCA, we used small neighborhoods of each pixel to\nform compounded pixels and use a tensorial version of PCA, called TPCA\n(Tensorial Principal Component Analysis), to compress and reconstruct a\ncompounded image of compounded pixels. Our experiments on public data show that\nTPCA compares favorably with PCA in compressing and reconstructing images. We\nalso show in our experiments that the performance of TPCA increases when the\norder of compounded pixels increases.\n", "versions": [{"version": "v1", "created": "Sun, 17 Jan 2021 11:44:50 GMT"}, {"version": "v2", "created": "Wed, 20 Jan 2021 01:42:02 GMT"}, {"version": "v3", "created": "Sun, 2 May 2021 14:51:12 GMT"}], "update_date": "2021-05-04", "authors_parsed": [["Liao", "Liang", ""], ["Zhang", "Xuechun", ""], ["Wang", "Xinqiang", ""], ["Lin", "Sen", ""], ["Liu", "Xin", ""]]}, {"id": "2101.06658", "submitter": "Zhiwu Huang", "authors": "Yan Wu, Zhiwu Huang, Suryansh Kumar, Rhea Sanjay Sukthanker, Radu\n  Timofte, Luc Van Gool", "title": "Trilevel Neural Architecture Search for Efficient Single Image\n  Super-Resolution", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modern solutions to the single image super-resolution (SISR) problem using\ndeep neural networks aim not only at better performance accuracy but also at a\nlighter and computationally efficient model. To that end, recently, neural\narchitecture search (NAS) approaches have shown some tremendous potential.\nFollowing the same underlying, in this paper, we suggest a novel trilevel NAS\nmethod that provides a better balance between different efficiency metrics and\nperformance to solve SISR. Unlike available NAS, our search is more complete,\nand therefore it leads to an efficient, optimized, and compressed architecture.\nWe innovatively introduce a trilevel search space modeling, i.e., hierarchical\nmodeling on network-, cell-, and kernel-level structures. To make the search on\ntrilevel spaces differentiable and efficient, we exploit a new sparsestmax\ntechnique that is excellent at generating sparse distributions of individual\nneural architecture candidates so that they can be better disentangled for the\nfinal selection from the enlarged search space. We further introduce the\nsorting technique to the sparsestmax relaxation for better network-level\ncompression. The proposed NAS optimization additionally facilitates\nsimultaneous search and training in a single phase, reducing search time and\ntrain time. Comprehensive evaluations on the benchmark datasets show our\nmethod's clear superiority over the state-of-the-art NAS in terms of a good\ntrade-off between model size, performance, and efficiency.\n", "versions": [{"version": "v1", "created": "Sun, 17 Jan 2021 12:19:49 GMT"}, {"version": "v2", "created": "Fri, 23 Apr 2021 15:50:09 GMT"}], "update_date": "2021-04-26", "authors_parsed": [["Wu", "Yan", ""], ["Huang", "Zhiwu", ""], ["Kumar", "Suryansh", ""], ["Sukthanker", "Rhea Sanjay", ""], ["Timofte", "Radu", ""], ["Van Gool", "Luc", ""]]}, {"id": "2101.06662", "submitter": "Pengzhou (Abel) Wu", "authors": "Pengzhou Wu and Kenji Fukumizu", "title": "Intact-VAE: Estimating Treatment Effects under Unobserved Confounding", "comments": "A major update of the ICLR submission. About 80% of the paper is\n  rewritten, and the theoretical part is totally new. For detailed notes on the\n  update, see https://openreview.net/forum?id=D3TNqCspFpM", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As an important problem of causal inference, we discuss the identification\nand estimation of treatment effects under unobserved confounding. Representing\nthe confounder as a latent variable, we propose Intact-VAE, a new variant of\nvariational autoencoder (VAE), motivated by the prognostic score that is\nsufficient for identifying treatment effects. We theoretically show that, under\ncertain settings, treatment effects are identified by our model, and further,\nbased on the identifiability of our model (i.e., determinacy of\nrepresentation), our VAE is a consistent estimator with representation balanced\nfor treatment groups. Experiments on (semi-)synthetic datasets show\nstate-of-the-art performance under diverse settings.\n", "versions": [{"version": "v1", "created": "Sun, 17 Jan 2021 13:03:44 GMT"}, {"version": "v2", "created": "Wed, 17 Feb 2021 01:12:31 GMT"}], "update_date": "2021-02-18", "authors_parsed": [["Wu", "Pengzhou", ""], ["Fukumizu", "Kenji", ""]]}, {"id": "2101.06686", "submitter": "Po Hsiang Yu", "authors": "Po-Hsiang Yu, Sih-Sian Wu and Liang-Gee Chen", "title": "KCP: Kernel Cluster Pruning for Dense Labeling Neural Networks", "comments": "17 pages, 16 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Pruning has become a promising technique used to compress and accelerate\nneural networks. Existing methods are mainly evaluated on spare labeling\napplications. However, dense labeling applications are those closer to real\nworld problems that require real-time processing on resource-constrained mobile\ndevices. Pruning for dense labeling applications is still a largely unexplored\nfield. The prevailing filter channel pruning method removes the entire filter\nchannel. Accordingly, the interaction between each kernel in one filter channel\nis ignored.\n  In this study, we proposed kernel cluster pruning (KCP) to prune dense\nlabeling networks. We developed a clustering technique to identify the least\nrepresentational kernels in each layer. By iteratively removing those kernels,\nthe parameter that can better represent the entire network is preserved; thus,\nwe achieve better accuracy with a decent model size and computation reduction.\nWhen evaluated on stereo matching and semantic segmentation neural networks,\nour method can reduce more than 70% of FLOPs with less than 1% of accuracy\ndrop. Moreover, for ResNet-50 on ILSVRC-2012, our KCP can reduce more than 50%\nof FLOPs reduction with 0.13% Top-1 accuracy gain. Therefore, KCP achieves\nstate-of-the-art pruning results.\n", "versions": [{"version": "v1", "created": "Sun, 17 Jan 2021 14:59:00 GMT"}], "update_date": "2021-01-19", "authors_parsed": [["Yu", "Po-Hsiang", ""], ["Wu", "Sih-Sian", ""], ["Chen", "Liang-Gee", ""]]}, {"id": "2101.06704", "submitter": "Xingjun Ma", "authors": "Nodens Koren, Qiuhong Ke, Yisen Wang, James Bailey, Xingjun Ma", "title": "Adversarial Interaction Attack: Fooling AI to Misinterpret Human\n  Intentions", "comments": "Preprint", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CR cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Understanding the actions of both humans and artificial intelligence (AI)\nagents is important before modern AI systems can be fully integrated into our\ndaily life. In this paper, we show that, despite their current huge success,\ndeep learning based AI systems can be easily fooled by subtle adversarial noise\nto misinterpret the intention of an action in interaction scenarios. Based on a\ncase study of skeleton-based human interactions, we propose a novel adversarial\nattack on interactions, and demonstrate how DNN-based interaction models can be\ntricked to predict the participants' reactions in unexpected ways. From a\nbroader perspective, the scope of our proposed attack method is not confined to\nproblems related to skeleton data but can also be extended to any type of\nproblems involving sequential regressions. Our study highlights potential risks\nin the interaction loop with AI and humans, which need to be carefully\naddressed when deploying AI systems in safety-critical applications.\n", "versions": [{"version": "v1", "created": "Sun, 17 Jan 2021 16:23:20 GMT"}], "update_date": "2021-01-19", "authors_parsed": [["Koren", "Nodens", ""], ["Ke", "Qiuhong", ""], ["Wang", "Yisen", ""], ["Bailey", "James", ""], ["Ma", "Xingjun", ""]]}, {"id": "2101.06720", "submitter": "Julieta Martinez", "authors": "John Phillips, Julieta Martinez, Ioan Andrei B\\^arsan, Sergio Casas,\n  Abbas Sadat, Raquel Urtasun", "title": "Deep Multi-Task Learning for Joint Localization, Perception, and\n  Prediction", "comments": "CVPR 21", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Over the last few years, we have witnessed tremendous progress on many\nsubtasks of autonomous driving, including perception, motion forecasting, and\nmotion planning. However, these systems often assume that the car is accurately\nlocalized against a high-definition map. In this paper we question this\nassumption, and investigate the issues that arise in state-of-the-art autonomy\nstacks under localization error. Based on our observations, we design a system\nthat jointly performs perception, prediction, and localization. Our\narchitecture is able to reuse computation between both tasks, and is thus able\nto correct localization errors efficiently. We show experiments on a\nlarge-scale autonomy dataset, demonstrating the efficiency and accuracy of our\nproposed approach.\n", "versions": [{"version": "v1", "created": "Sun, 17 Jan 2021 17:20:31 GMT"}, {"version": "v2", "created": "Tue, 19 Jan 2021 03:17:34 GMT"}, {"version": "v3", "created": "Sat, 10 Apr 2021 23:31:27 GMT"}], "update_date": "2021-04-13", "authors_parsed": [["Phillips", "John", ""], ["Martinez", "Julieta", ""], ["B\u00e2rsan", "Ioan Andrei", ""], ["Casas", "Sergio", ""], ["Sadat", "Abbas", ""], ["Urtasun", "Raquel", ""]]}, {"id": "2101.06741", "submitter": "Mateus Roder", "authors": "Mateus Roder, Gustavo H. de Rosa, Victor Hugo C. de Albuquerque,\n  Andr\\'e L. D. Rossi, Jo\\~ao P. Papa", "title": "Energy-based Dropout in Restricted Boltzmann Machines: Why not go random", "comments": null, "journal-ref": null, "doi": "10.1109/TETCI.2020.3043764", "report-no": null, "categories": "cs.LG cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Deep learning architectures have been widely fostered throughout the last\nyears, being used in a wide range of applications, such as object recognition,\nimage reconstruction, and signal processing. Nevertheless, such models suffer\nfrom a common problem known as overfitting, which limits the network from\npredicting unseen data effectively. Regularization approaches arise in an\nattempt to address such a shortcoming. Among them, one can refer to the\nwell-known Dropout, which tackles the problem by randomly shutting down a set\nof neurons and their connections according to a certain probability. Therefore,\nthis approach does not consider any additional knowledge to decide which units\nshould be disconnected. In this paper, we propose an energy-based Dropout\n(E-Dropout) that makes conscious decisions whether a neuron should be dropped\nor not. Specifically, we design this regularization method by correlating\nneurons and the model's energy as an importance level for further applying it\nto energy-based models, such as Restricted Boltzmann Machines (RBMs). The\nexperimental results over several benchmark datasets revealed the proposed\napproach's suitability compared to the traditional Dropout and the standard\nRBMs.\n", "versions": [{"version": "v1", "created": "Sun, 17 Jan 2021 18:21:05 GMT"}], "update_date": "2021-01-19", "authors_parsed": [["Roder", "Mateus", ""], ["de Rosa", "Gustavo H.", ""], ["de Albuquerque", "Victor Hugo C.", ""], ["Rossi", "Andr\u00e9 L. D.", ""], ["Papa", "Jo\u00e3o P.", ""]]}, {"id": "2101.06742", "submitter": "Simon Suo", "authors": "Shenlong Wang, Simon Suo, Wei-Chiu Ma, Andrei Pokrovsky, Raquel\n  Urtasun", "title": "Deep Parametric Continuous Convolutional Neural Networks", "comments": "Accepted by CVPR 2018", "journal-ref": null, "doi": "10.1109/CVPR.2018.00274", "report-no": null, "categories": "cs.CV cs.AI cs.LG cs.RO stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Standard convolutional neural networks assume a grid structured input is\navailable and exploit discrete convolutions as their fundamental building\nblocks. This limits their applicability to many real-world applications. In\nthis paper we propose Parametric Continuous Convolution, a new learnable\noperator that operates over non-grid structured data. The key idea is to\nexploit parameterized kernel functions that span the full continuous vector\nspace. This generalization allows us to learn over arbitrary data structures as\nlong as their support relationship is computable. Our experiments show\nsignificant improvement over the state-of-the-art in point cloud segmentation\nof indoor and outdoor scenes, and lidar motion estimation of driving scenes.\n", "versions": [{"version": "v1", "created": "Sun, 17 Jan 2021 18:28:23 GMT"}], "update_date": "2021-01-19", "authors_parsed": [["Wang", "Shenlong", ""], ["Suo", "Simon", ""], ["Ma", "Wei-Chiu", ""], ["Pokrovsky", "Andrei", ""], ["Urtasun", "Raquel", ""]]}, {"id": "2101.06749", "submitter": "Mateus Roder", "authors": "Mateus Roder, Leandro A. Passos, Luiz Carlos Felix Ribeiro, Clayton\n  Pereira, Jo\\~ao Paulo Papa", "title": "A Layer-Wise Information Reinforcement Approach to Improve Learning in\n  Deep Belief Networks", "comments": null, "journal-ref": null, "doi": "10.1007/978-3-030-61401-0_22", "report-no": null, "categories": "cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  With the advent of deep learning, the number of works proposing new methods\nor improving existent ones has grown exponentially in the last years. In this\nscenario, \"very deep\" models were emerging, once they were expected to extract\nmore intrinsic and abstract features while supporting a better performance.\nHowever, such models suffer from the gradient vanishing problem, i.e.,\nbackpropagation values become too close to zero in their shallower layers,\nultimately causing learning to stagnate. Such an issue was overcome in the\ncontext of convolution neural networks by creating \"shortcut connections\"\nbetween layers, in a so-called deep residual learning framework. Nonetheless, a\nvery popular deep learning technique called Deep Belief Network still suffers\nfrom gradient vanishing when dealing with discriminative tasks. Therefore, this\npaper proposes the Residual Deep Belief Network, which considers the\ninformation reinforcement layer-by-layer to improve the feature extraction and\nknowledge retaining, that support better discriminative performance.\nExperiments conducted over three public datasets demonstrate its robustness\nconcerning the task of binary image classification.\n", "versions": [{"version": "v1", "created": "Sun, 17 Jan 2021 18:53:18 GMT"}], "update_date": "2021-01-19", "authors_parsed": [["Roder", "Mateus", ""], ["Passos", "Leandro A.", ""], ["Ribeiro", "Luiz Carlos Felix", ""], ["Pereira", "Clayton", ""], ["Papa", "Jo\u00e3o Paulo", ""]]}, {"id": "2101.06763", "submitter": "Theodoulos Rodosthenous", "authors": "Theodoulos Rodosthenous, Vahid Shahrezaei and Marina Evangelou", "title": "Multi-view Data Visualisation via Manifold Learning", "comments": "27 pages, 12 figures, 6 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.CO", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Non-linear dimensionality reduction can be performed by \\textit{manifold\nlearning} approaches, such as Stochastic Neighbour Embedding (SNE), Locally\nLinear Embedding (LLE) and Isometric Feature Mapping (ISOMAP). These methods\naim to produce two or three latent embeddings, primarily to visualise the data\nin intelligible representations. This manuscript proposes extensions of\nStudent's t-distributed SNE (t-SNE), LLE and ISOMAP, for dimensionality\nreduction and visualisation of multi-view data. Multi-view data refers to\nmultiple types of data generated from the same samples. The proposed multi-view\napproaches provide more comprehensible projections of the samples compared to\nthe ones obtained by visualising each data-view separately. Commonly\nvisualisation is used for identifying underlying patterns within the samples.\nBy incorporating the obtained low-dimensional embeddings from the multi-view\nmanifold approaches into the K-means clustering algorithm, it is shown that\nclusters of the samples are accurately identified. Through the analysis of real\nand synthetic data the proposed multi-SNE approach is found to have the best\nperformance. We further illustrate the applicability of the multi-SNE approach\nfor the analysis of multi-omics single-cell data, where the aim is to visualise\nand identify cell heterogeneity and cell types in biological tissues relevant\nto health and disease.\n", "versions": [{"version": "v1", "created": "Sun, 17 Jan 2021 19:54:36 GMT"}, {"version": "v2", "created": "Thu, 13 May 2021 11:22:41 GMT"}], "update_date": "2021-05-14", "authors_parsed": [["Rodosthenous", "Theodoulos", ""], ["Shahrezaei", "Vahid", ""], ["Evangelou", "Marina", ""]]}, {"id": "2101.06768", "submitter": "Pascal Van Hentenryck", "authors": "Minas Chatzos and Terrence W.K. Mak and Pascal Van Hentenryck", "title": "Spatial Network Decomposition for Fast and Scalable AC-OPF Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.SY eess.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a novel machine-learning approach for predicting AC-OPF\nsolutions that features a fast and scalable training. It is motivated by the\ntwo critical considerations: (1) the fact that topology optimization and the\nstochasticity induced by renewable energy sources may lead to fundamentally\ndifferent AC-OPF instances; and (2) the significant training time needed by\nexisting machine-learning approaches for predicting AC-OPF. The proposed\napproach is a 2-stage methodology that exploits a spatial decomposition of the\npower network that is viewed as a set of regions. The first stage learns to\npredict the flows and voltages on the buses and lines coupling the regions, and\nthe second stage trains, in parallel, the machine-learning models for each\nregion. Experimental results on the French transmission system (up to 6,700\nbuses and 9,000 lines) demonstrate the potential of the approach. Within a\nshort training time, the approach predicts AC-OPF solutions with very high\nfidelity and minor constraint violations, producing significant improvements\nover the state-of-the-art. The results also show that the predictions can seed\na load flow optimization to return a feasible solution within 0.03% of the\nAC-OPF objective, while reducing running times significantly.\n", "versions": [{"version": "v1", "created": "Sun, 17 Jan 2021 20:09:11 GMT"}], "update_date": "2021-01-19", "authors_parsed": [["Chatzos", "Minas", ""], ["Mak", "Terrence W. K.", ""], ["Van Hentenryck", "Pascal", ""]]}, {"id": "2101.06772", "submitter": "Christopher Vogelsanger", "authors": "Christopher Vogelsanger and Christian Federau", "title": "Latent Space Analysis of VAE and Intro-VAE applied to 3-dimensional MR\n  Brain Volumes of Multiple Sclerosis, Leukoencephalopathy, and Healthy\n  Patients", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Multiple Sclerosis (MS) and microvascular leukoencephalopathy are two\ndistinct neurological conditions, the first caused by focal autoimmune\ninflammation in the central nervous system, the second caused by chronic white\nmatter damage from atherosclerotic microvascular disease. Both conditions lead\nto signal anomalies on Fluid Attenuated Inversion Recovery (FLAIR) magnetic\nresonance (MR) images, which can be distinguished by an expert\nneuroradiologist, but which can look very similar to the untrained eye as well\nas in the early stage of both diseases. In this paper, we attempt to train a\n3-dimensional deep neural network to learn the specific features of both\ndiseases in an unsupervised manner. For this manner, in a first step we train a\ngenerative neural network to create artificial MR images of both conditions\nwith approximate explicit density, using a mixed dataset of multiple sclerosis,\nleukoencephalopathy and healthy patients containing in total 5404 volumes of\n3096 patients. In a second step, we distinguish features between the different\ndiseases in the latent space of this network, and use them to classify new\ndata.\n", "versions": [{"version": "v1", "created": "Sun, 17 Jan 2021 20:31:22 GMT"}], "update_date": "2021-01-19", "authors_parsed": [["Vogelsanger", "Christopher", ""], ["Federau", "Christian", ""]]}, {"id": "2101.06774", "submitter": "Sara Mesquita", "authors": "Sara Mesquita, Cl\\'audio Haupt Vieira, L\\'ilia Perfeito and Joana\n  Gon\\c{c}alves-S\\'a", "title": "Learning from pandemics: using extraordinary events can improve disease\n  now-casting models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Online searches have been used to study different health-related behaviours,\nincluding monitoring disease outbreaks. An obvious caveat is that several\nreasons can motivate individuals to seek online information and models that are\nblind to people's motivations are of limited use and can even mislead. This is\nparticularly true during extraordinary public health crisis, such as the\nongoing pandemic, when fear, curiosity and many other reasons can lead\nindividuals to search for health-related information, masking the\ndisease-driven searches. However, health crisis can also offer an opportunity\nto disentangle between different drivers and learn about human behavior. Here,\nwe focus on the two pandemics of the 21st century (2009-H1N1 flu and Covid-19)\nand propose a methodology to discriminate between search patterns linked to\ngeneral information seeking (media driven) and search patterns possibly more\nassociated with actual infection (disease driven). We show that by learning\nfrom such pandemic periods, with high anxiety and media hype, it is possible to\nselect online searches and improve model performance both in pandemic and\nseasonal settings. Moreover, and despite the common claim that more data is\nalways better, our results indicate that lower volume of the right data can be\nbetter than including large volumes of apparently similar data, especially in\nthe long run. Our work provides a general framework that can be applied beyond\nspecific events and diseases, and argues that algorithms can be improved simply\nby using less (better) data. This has important consequences, for example, to\nsolve the accuracy-explainability trade-off in machine-learning.\n", "versions": [{"version": "v1", "created": "Sun, 17 Jan 2021 20:36:19 GMT"}], "update_date": "2021-01-19", "authors_parsed": [["Mesquita", "Sara", ""], ["Vieira", "Cl\u00e1udio Haupt", ""], ["Perfeito", "L\u00edlia", ""], ["Gon\u00e7alves-S\u00e1", "Joana", ""]]}, {"id": "2101.06784", "submitter": "James Tu", "authors": "James Tu, Huichen Li, Xinchen Yan, Mengye Ren, Yun Chen, Ming Liang,\n  Eilyan Bitar, Ersin Yumer, Raquel Urtasun", "title": "Exploring Adversarial Robustness of Multi-Sensor Perception Systems in\n  Self Driving", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Modern self-driving perception systems have been shown to improve upon\nprocessing complementary inputs such as LiDAR with images. In isolation, 2D\nimages have been found to be extremely vulnerable to adversarial attacks. Yet,\nthere have been limited studies on the adversarial robustness of multi-modal\nmodels that fuse LiDAR features with image features. Furthermore, existing\nworks do not consider physically realizable perturbations that are consistent\nacross the input modalities. In this paper, we showcase practical\nsusceptibilities of multi-sensor detection by placing an adversarial object on\ntop of a host vehicle. We focus on physically realizable and input-agnostic\nattacks as they are feasible to execute in practice, and show that a single\nuniversal adversary can hide different host vehicles from state-of-the-art\nmulti-modal detectors. Our experiments demonstrate that successful attacks are\nprimarily caused by easily corrupted image features. Furthermore, we find that\nin modern sensor fusion methods which project image features into 3D,\nadversarial attacks can exploit the projection process to generate false\npositives across distant regions in 3D. Towards more robust multi-modal\nperception systems, we show that adversarial training with feature denoising\ncan boost robustness to such attacks significantly. However, we find that\nstandard adversarial defenses still struggle to prevent false positives which\nare also caused by inaccurate associations between 3D LiDAR points and 2D\npixels.\n", "versions": [{"version": "v1", "created": "Sun, 17 Jan 2021 21:15:34 GMT"}, {"version": "v2", "created": "Tue, 26 Jan 2021 00:40:48 GMT"}], "update_date": "2021-01-27", "authors_parsed": [["Tu", "James", ""], ["Li", "Huichen", ""], ["Yan", "Xinchen", ""], ["Ren", "Mengye", ""], ["Chen", "Yun", ""], ["Liang", "Ming", ""], ["Bitar", "Eilyan", ""], ["Yumer", "Ersin", ""], ["Urtasun", "Raquel", ""]]}, {"id": "2101.06798", "submitter": "Ahmed Qureshi", "authors": "Linjun Li, Yinglong Miao, Ahmed H. Qureshi, and Michael C. Yip", "title": "MPC-MPNet: Model-Predictive Motion Planning Networks for Fast,\n  Near-Optimal Planning under Kinodynamic Constraints", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Kinodynamic Motion Planning (KMP) is to find a robot motion subject to\nconcurrent kinematics and dynamics constraints. To date, quite a few methods\nsolve KMP problems and those that exist struggle to find near-optimal solutions\nand exhibit high computational complexity as the planning space dimensionality\nincreases. To address these challenges, we present a scalable, imitation\nlearning-based, Model-Predictive Motion Planning Networks framework that\nquickly finds near-optimal path solutions with worst-case theoretical\nguarantees under kinodynamic constraints for practical underactuated systems.\nOur framework introduces two algorithms built on a neural generator,\ndiscriminator, and a parallelizable Model Predictive Controller (MPC). The\ngenerator outputs various informed states towards the given target, and the\ndiscriminator selects the best possible subset from them for the extension. The\nMPC locally connects the selected informed states while satisfying the given\nconstraints leading to feasible, near-optimal solutions. We evaluate our\nalgorithms on a range of cluttered, kinodynamically constrained, and\nunderactuated planning problems with results indicating significant\nimprovements in computation times, path qualities, and success rates over\nexisting methods.\n", "versions": [{"version": "v1", "created": "Sun, 17 Jan 2021 23:07:04 GMT"}], "update_date": "2021-01-19", "authors_parsed": [["Li", "Linjun", ""], ["Miao", "Yinglong", ""], ["Qureshi", "Ahmed H.", ""], ["Yip", "Michael C.", ""]]}, {"id": "2101.06800", "submitter": "Zheng Liu", "authors": "Zheng Liu, Xiaohan Li, Hao Peng, Lifang He, Philip S. Yu", "title": "Heterogeneous Similarity Graph Neural Network on Electronic Health\n  Records", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Mining Electronic Health Records (EHRs) becomes a promising topic because of\nthe rich information they contain. By learning from EHRs, machine learning\nmodels can be built to help human experts to make medical decisions and thus\nimprove healthcare quality. Recently, many models based on sequential or graph\nmodels are proposed to achieve this goal. EHRs contain multiple entities and\nrelations and can be viewed as a heterogeneous graph. However, previous studies\nignore the heterogeneity in EHRs. On the other hand, current heterogeneous\ngraph neural networks cannot be simply used on an EHR graph because of the\nexistence of hub nodes in it. To address this issue, we propose Heterogeneous\nSimilarity Graph Neural Network (HSGNN) analyze EHRs with a novel heterogeneous\nGNN. Our framework consists of two parts: one is a preprocessing method and the\nother is an end-to-end GNN. The preprocessing method normalizes edges and\nsplits the EHR graph into multiple homogeneous graphs while each homogeneous\ngraph contains partial information of the original EHR graph. The GNN takes all\nhomogeneous graphs as input and fuses all of them into one graph to make a\nprediction. Experimental results show that HSGNN outperforms other baselines in\nthe diagnosis prediction task.\n", "versions": [{"version": "v1", "created": "Sun, 17 Jan 2021 23:14:29 GMT"}], "update_date": "2021-01-19", "authors_parsed": [["Liu", "Zheng", ""], ["Li", "Xiaohan", ""], ["Peng", "Hao", ""], ["He", "Lifang", ""], ["Yu", "Philip S.", ""]]}, {"id": "2101.06802", "submitter": "Liu Yang", "authors": "Liu Yang, Tingwei Meng, George Em Karniadakis", "title": "Measure-conditional Discriminator with Stationary Optimum for GANs and\n  Statistical Distance Surrogates", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We propose a simple but effective modification of the discriminators, namely\nmeasure-conditional discriminators, as a plug-and-play module for different\nGANs. By taking the generated distributions as part of input so that the target\noptimum for the discriminator is stationary, the proposed discriminator is more\nrobust than the vanilla one. A variant of the measure-conditional discriminator\ncan also handle multiple target distributions, or act as a surrogate model of\nstatistical distances such as KL divergence with applications to transfer\nlearning.\n", "versions": [{"version": "v1", "created": "Sun, 17 Jan 2021 23:18:10 GMT"}], "update_date": "2021-01-19", "authors_parsed": [["Yang", "Liu", ""], ["Meng", "Tingwei", ""], ["Karniadakis", "George Em", ""]]}, {"id": "2101.06806", "submitter": "Sergio Casas", "authors": "Sergio Casas, Abbas Sadat, Raquel Urtasun", "title": "MP3: A Unified Model to Map, Perceive, Predict and Plan", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AI cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  High-definition maps (HD maps) are a key component of most modern\nself-driving systems due to their valuable semantic and geometric information.\nUnfortunately, building HD maps has proven hard to scale due to their cost as\nwell as the requirements they impose in the localization system that has to\nwork everywhere with centimeter-level accuracy. Being able to drive without an\nHD map would be very beneficial to scale self-driving solutions as well as to\nincrease the failure tolerance of existing ones (e.g., if localization fails or\nthe map is not up-to-date). Towards this goal, we propose MP3, an end-to-end\napproach to mapless driving where the input is raw sensor data and a high-level\ncommand (e.g., turn left at the intersection). MP3 predicts intermediate\nrepresentations in the form of an online map and the current and future state\nof dynamic agents, and exploits them in a novel neural motion planner to make\ninterpretable decisions taking into account uncertainty. We show that our\napproach is significantly safer, more comfortable, and can follow commands\nbetter than the baselines in challenging long-term closed-loop simulations, as\nwell as when compared to an expert driver in a large-scale real-world dataset.\n", "versions": [{"version": "v1", "created": "Mon, 18 Jan 2021 00:09:30 GMT"}], "update_date": "2021-01-19", "authors_parsed": [["Casas", "Sergio", ""], ["Sadat", "Abbas", ""], ["Urtasun", "Raquel", ""]]}, {"id": "2101.06811", "submitter": "Farhad Farokhi", "authors": "Farhad Farokhi", "title": "Optimal Pre-Processing to Achieve Fairness and Its Relationship with\n  Total Variation Barycenter", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.LG math.IT math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We use disparate impact, i.e., the extent that the probability of observing\nan output depends on protected attributes such as race and gender, to measure\nfairness. We prove that disparate impact is upper bounded by the total\nvariation distance between the distribution of the inputs given the protected\nattributes. We then use pre-processing, also known as data repair, to enforce\nfairness. We show that utility degradation, i.e., the extent that the success\nof a forecasting model changes by pre-processing the data, is upper bounded by\nthe total variation distance between the distribution of the data before and\nafter pre-processing. Hence, the problem of finding the optimal pre-processing\nregiment for enforcing fairness can be cast as minimizing total variations\ndistance between the distribution of the data before and after pre-processing\nsubject to a constraint on the total variation distance between the\ndistribution of the inputs given protected attributes. This problem is a linear\nprogram that can be efficiently solved. We show that this problem is intimately\nrelated to finding the barycenter (i.e., center of mass) of two distributions\nwhen distances in the probability space are measured by total variation\ndistance. We also investigate the effect of differential privacy on fairness\nusing the proposed the total variation distances. We demonstrate the results\nusing numerical experimentation with a practice dataset.\n", "versions": [{"version": "v1", "created": "Mon, 18 Jan 2021 00:20:32 GMT"}], "update_date": "2021-01-19", "authors_parsed": [["Farokhi", "Farhad", ""]]}, {"id": "2101.06813", "submitter": "Zhengchun Liu", "authors": "Jiali Wang, Zhengchun Liu, Ian Foster, Won Chang, Rajkumar Kettimuthu,\n  Rao Kotamarthi", "title": "Fast and accurate learned multiresolution dynamical downscaling for\n  precipitation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This study develops a neural network-based approach for emulating\nhigh-resolution modeled precipitation data with comparable statistical\nproperties but at greatly reduced computational cost. The key idea is to use\ncombination of low- and high- resolution simulations to train a neural network\nto map from the former to the latter. Specifically, we define two types of\nCNNs, one that stacks variables directly and one that encodes each variable\nbefore stacking, and we train each CNN type both with a conventional loss\nfunction, such as mean square error (MSE), and with a conditional generative\nadversarial network (CGAN), for a total of four CNN variants. We compare the\nfour new CNN-derived high-resolution precipitation results with precipitation\ngenerated from original high resolution simulations, a bilinear interpolater\nand the state-of-the-art CNN-based super-resolution (SR) technique. Results\nshow that the SR technique produces results similar to those of the bilinear\ninterpolator with smoother spatial and temporal distributions and smaller data\nvariabilities and extremes than the original high resolution simulations. While\nthe new CNNs trained by MSE generate better results over some regions than the\ninterpolator and SR technique do, their predictions are still not as close as\nthe original high resolution simulations. The CNNs trained by CGAN generate\nmore realistic and physically reasonable results, better capturing not only\ndata variability in time and space but also extremes such as intense and\nlong-lasting storms. The new proposed CNN-based downscaling approach can\ndownscale precipitation from 50~km to 12~km in 14~min for 30~years once the\nnetwork is trained (training takes 4~hours using 1~GPU), while the conventional\ndynamical downscaling would take 1~month using 600 CPU cores to generate\nsimulations at the resolution of 12~km over contiguous United States.\n", "versions": [{"version": "v1", "created": "Mon, 18 Jan 2021 00:25:04 GMT"}], "update_date": "2021-01-19", "authors_parsed": [["Wang", "Jiali", ""], ["Liu", "Zhengchun", ""], ["Foster", "Ian", ""], ["Chang", "Won", ""], ["Kettimuthu", "Rajkumar", ""], ["Kotamarthi", "Rao", ""]]}, {"id": "2101.06821", "submitter": "Hung Du", "authors": "Yong-Bin Kang, Hung Du, Abdur Rahim Mohammad Forkan, Prem Prakash\n  Jayaraman, Amir Aryani, Timos Sellis (Fellow, IEEE)", "title": "ExpFinder: An Ensemble Expert Finding Model Integrating $N$-gram Vector\n  Space Model and $\\mu$CO-HITS", "comments": "15 pages, 18 figures, \"for source code on Github, see\n  https://github.com/Yongbinkang/ExpFinder\", \"Submitted to IEEE Transactions on\n  Knowledge and Data Engineering\"", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.AI cs.LG cs.SI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Finding an expert plays a crucial role in driving successful collaborations\nand speeding up high-quality research development and innovations. However, the\nrapid growth of scientific publications and digital expertise data makes\nidentifying the right experts a challenging problem. Existing approaches for\nfinding experts given a topic can be categorised into information retrieval\ntechniques based on vector space models, document language models, and\ngraph-based models. In this paper, we propose $\\textit{ExpFinder}$, a new\nensemble model for expert finding, that integrates a novel $N$-gram vector\nspace model, denoted as $n$VSM, and a graph-based model, denoted as\n$\\textit{$\\mu$CO-HITS}$, that is a proposed variation of the CO-HITS algorithm.\nThe key of $n$VSM is to exploit recent inverse document frequency weighting\nmethod for $N$-gram words and $\\textit{ExpFinder}$ incorporates $n$VSM into\n$\\textit{$\\mu$CO-HITS}$ to achieve expert finding. We comprehensively evaluate\n$\\textit{ExpFinder}$ on four different datasets from the academic domains in\ncomparison with six different expert finding models. The evaluation results\nshow that $\\textit{ExpFinder}$ is a highly effective model for expert finding,\nsubstantially outperforming all the compared models in 19% to 160.2%.\n", "versions": [{"version": "v1", "created": "Mon, 18 Jan 2021 00:44:21 GMT"}], "update_date": "2021-01-19", "authors_parsed": [["Kang", "Yong-Bin", "", "Fellow, IEEE"], ["Du", "Hung", "", "Fellow, IEEE"], ["Forkan", "Abdur Rahim Mohammad", "", "Fellow, IEEE"], ["Jayaraman", "Prem Prakash", "", "Fellow, IEEE"], ["Aryani", "Amir", "", "Fellow, IEEE"], ["Sellis", "Timos", "", "Fellow, IEEE"]]}, {"id": "2101.06823", "submitter": "Yizhen Xu", "authors": "Yizhen Xu, Joseph W. Hogan, Michael J. Daniels, Rami Kantor, Ann\n  Mwangi", "title": "Inference for BART with Multinomial Outcomes", "comments": "23 pages, 12 tables, 6 figures, with appendix, 49 pages total", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  The multinomial probit Bayesian additive regression trees (MPBART) framework\nwas proposed by Kindo et al. (KD), approximating the latent utilities in the\nmultinomial probit (MNP) model with BART (Chipman et al. 2010). Compared to\nmultinomial logistic models, MNP does not assume independent alternatives and\nthe correlation structure among alternatives can be specified through\nmultivariate Gaussian distributed latent utilities. We introduce two new\nalgorithms for fitting the MPBART and show that the theoretical mixing rates of\nour proposals are equal or superior to the existing algorithm in KD. Through\nsimulations, we explore the robustness of the methods to the choice of\nreference level, imbalance in outcome frequencies, and the specifications of\nprior hyperparameters for the utility error term. The work is motivated by the\napplication of generating posterior predictive distributions for mortality and\nengagement in care among HIV-positive patients based on electronic health\nrecords (EHRs) from the Academic Model Providing Access to Healthcare (AMPATH)\nin Kenya. In both the application and simulations, we observe better\nperformance using our proposals as compared to KD in terms of MCMC convergence\nrate and posterior predictive accuracy.\n", "versions": [{"version": "v1", "created": "Mon, 18 Jan 2021 00:59:03 GMT"}], "update_date": "2021-01-19", "authors_parsed": [["Xu", "Yizhen", ""], ["Hogan", "Joseph W.", ""], ["Daniels", "Michael J.", ""], ["Kantor", "Rami", ""], ["Mwangi", "Ann", ""]]}, {"id": "2101.06827", "submitter": "Quanying Liu", "authors": "Wanguang Yin, Zhengming Ma, Quanying Liu", "title": "HyperNTF: A Hypergraph Regularized Nonnegative Tensor Factorization for\n  Dimensionality Reduction", "comments": "12 pages, 6 figures, 9 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG eess.IV eess.SP", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Most methods for dimensionality reduction are based on either tensor\nrepresentation or local geometry learning. However, the tensor-based methods\nseverely rely on the assumption of global and multilinear structures in\nhigh-dimensional data; and the manifold learning methods suffer from the\nout-of-sample problem. In this paper, bridging the tensor decomposition and\nmanifold learning, we propose a novel method, called Hypergraph Regularized\nNonnegative Tensor Factorization (HyperNTF). HyperNTF can preserve\nnonnegativity in tensor factorization, and uncover the higher-order\nrelationship among the nearest neighborhoods. Clustering analysis with HyperNTF\nhas low computation and storage costs. The experiments on four synthetic data\nshow a desirable property of hypergraph in uncovering the high-order\ncorrelation to unfold the curved manifolds. Moreover, the numerical experiments\non six real datasets suggest that HyperNTF robustly outperforms\nstate-of-the-art algorithms in clustering analysis.\n", "versions": [{"version": "v1", "created": "Mon, 18 Jan 2021 01:38:47 GMT"}, {"version": "v2", "created": "Tue, 26 Jan 2021 16:08:29 GMT"}], "update_date": "2021-01-27", "authors_parsed": [["Yin", "Wanguang", ""], ["Ma", "Zhengming", ""], ["Liu", "Quanying", ""]]}, {"id": "2101.06832", "submitter": "Jerry Liu", "authors": "Jerry Liu, Wenyuan Zeng, Raquel Urtasun, Ersin Yumer", "title": "Deep Structured Reactive Planning", "comments": "ICRA 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An intelligent agent operating in the real-world must balance achieving its\ngoal with maintaining the safety and comfort of not only itself, but also other\nparticipants within the surrounding scene. This requires jointly reasoning\nabout the behavior of other actors while deciding its own actions as these two\nprocesses are inherently intertwined - a vehicle will yield to us if we decide\nto proceed first at the intersection but will proceed first if we decide to\nyield. However, this is not captured in most self-driving pipelines, where\nplanning follows prediction. In this paper we propose a novel data-driven,\nreactive planning objective which allows a self-driving vehicle to jointly\nreason about its own plans as well as how other actors will react to them. We\nformulate the problem as an energy-based deep structured model that is learned\nfrom observational data and encodes both the planning and prediction problems.\nThrough simulations based on both real-world driving and synthetically\ngenerated dense traffic, we demonstrate that our reactive model outperforms a\nnon-reactive variant in successfully completing highly complex maneuvers (lane\nmerges/turns in traffic) faster, without trading off collision rate.\n", "versions": [{"version": "v1", "created": "Mon, 18 Jan 2021 01:43:36 GMT"}, {"version": "v2", "created": "Thu, 29 Apr 2021 06:26:56 GMT"}], "update_date": "2021-04-30", "authors_parsed": [["Liu", "Jerry", ""], ["Zeng", "Wenyuan", ""], ["Urtasun", "Raquel", ""], ["Yumer", "Ersin", ""]]}, {"id": "2101.06840", "submitter": "Shuangyan Yang", "authors": "Jie Ren, Samyam Rajbhandari, Reza Yazdani Aminabadi, Olatunji Ruwase,\n  Shuangyan Yang, Minjia Zhang, Dong Li, Yuxiong He", "title": "ZeRO-Offload: Democratizing Billion-Scale Model Training", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Large-scale model training has been a playing ground for a limited few\nrequiring complex model refactoring and access to prohibitively expensive GPU\nclusters. ZeRO-Offload changes the large model training landscape by making\nlarge model training accessible to nearly everyone. It can train models with\nover 13 billion parameters on a single GPU, a 10x increase in size compared to\npopular framework such as PyTorch, and it does so without requiring any model\nchange from the data scientists or sacrificing computational efficiency.\nZeRO-Offload enables large model training by offloading data and compute to\nCPU. To preserve compute efficiency, it is designed to minimize the data\nmovement to/from GPU, and reduce CPU compute time while maximizing memory\nsavings on GPU. As a result, ZeRO-Offload can achieve 40 TFlops/GPU on a single\nNVIDIA V100 GPU for 10B parameter model compared to 30TF using PyTorch alone\nfor a 1.4B parameter model, the largest that can be trained without running out\nof memory. ZeRO-Offload is also designed to scale on multiple-GPUs when\navailable, offering near linear speedup on up to 128 GPUs. Additionally, it can\nwork together with model parallelism to train models with over 70 billion\nparameters on a single DGX-2 box, a 4.5x increase in model size compared to\nusing model parallelism alone. By combining compute and memory efficiency with\nease-of-use, ZeRO-Offload democratizes large-scale model training making it\naccessible to even data scientists with access to just a single GPU.\n", "versions": [{"version": "v1", "created": "Mon, 18 Jan 2021 02:11:25 GMT"}], "update_date": "2021-01-19", "authors_parsed": [["Ren", "Jie", ""], ["Rajbhandari", "Samyam", ""], ["Aminabadi", "Reza Yazdani", ""], ["Ruwase", "Olatunji", ""], ["Yang", "Shuangyan", ""], ["Zhang", "Minjia", ""], ["Li", "Dong", ""], ["He", "Yuxiong", ""]]}, {"id": "2101.06842", "submitter": "Naoya Takahashi", "authors": "Naoya Takahashi, Mayank Kumar Singh, Yuki Mitsufuji", "title": "Hierarchical disentangled representation learning for singing voice\n  conversion", "comments": "accepted at IJCNN 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.LG eess.AS", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Conventional singing voice conversion (SVC) methods often suffer from\noperating in high-resolution audio owing to a high dimensionality of data. In\nthis paper, we propose a hierarchical representation learning that enables the\nlearning of disentangled representations with multiple resolutions\nindependently. With the learned disentangled representations, the proposed\nmethod progressively performs SVC from low to high resolutions. Experimental\nresults show that the proposed method outperforms baselines that operate with a\nsingle resolution in terms of mean opinion score (MOS), similarity score, and\npitch accuracy.\n", "versions": [{"version": "v1", "created": "Mon, 18 Jan 2021 02:17:24 GMT"}, {"version": "v2", "created": "Mon, 26 Apr 2021 00:55:26 GMT"}], "update_date": "2021-04-27", "authors_parsed": [["Takahashi", "Naoya", ""], ["Singh", "Mayank Kumar", ""], ["Mitsufuji", "Yuki", ""]]}, {"id": "2101.06847", "submitter": "Thulasi Tholeti", "authors": "Thulasi Tholeti, Sheetal Kalyani", "title": "On the Differentially Private Nature of Perturbed Gradient Descent", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of empirical risk minimization given a database,\nusing the gradient descent algorithm. We note that the function to be optimized\nmay be non-convex, consisting of saddle points which impede the convergence of\nthe algorithm. A perturbed gradient descent algorithm is typically employed to\nescape these saddle points. We show that this algorithm, that perturbs the\ngradient, inherently preserves the privacy of the data. We then employ the\ndifferential privacy framework to quantify the privacy hence achieved. We also\nanalyze the change in privacy with varying parameters such as problem dimension\nand the distance between the databases.\n", "versions": [{"version": "v1", "created": "Mon, 18 Jan 2021 02:29:37 GMT"}], "update_date": "2021-01-19", "authors_parsed": [["Tholeti", "Thulasi", ""], ["Kalyani", "Sheetal", ""]]}, {"id": "2101.06850", "submitter": "Md Fazle Rabby", "authors": "Md Fazle Rabby, Yazhou Tu, Md Imran Hossen, Insup Le, Anthony S Maida,\n  Xiali Hei", "title": "Stacked LSTM Based Deep Recurrent Neural Network with Kalman Smoothing\n  for Blood Glucose Prediction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Blood glucose (BG) management is crucial for type-1 diabetes patients\nresulting in the necessity of reliable artificial pancreas or insulin infusion\nsystems. In recent years, deep learning techniques have been utilized for a\nmore accurate BG level prediction system. However, continuous glucose\nmonitoring (CGM) readings are susceptible to sensor errors. As a result,\ninaccurate CGM readings would affect BG prediction and make it unreliable, even\nif the most optimal machine learning model is used. In this work, we propose a\nnovel approach to predicting blood glucose level with a stacked Long short-term\nmemory (LSTM) based deep recurrent neural network (RNN) model considering\nsensor fault. We use the Kalman smoothing technique for the correction of the\ninaccurate CGM readings due to sensor error. For the OhioT1DM dataset,\ncontaining eight weeks' data from six different patients, we achieve an average\nRMSE of 6.45 and 17.24 mg/dl for 30 minutes and 60 minutes of prediction\nhorizon (PH), respectively. To the best of our knowledge, this is the leading\naverage prediction accuracy for the ohioT1DM dataset. Different physiological\ninformation, e.g., Kalman smoothed CGM data, carbohydrates from the meal, bolus\ninsulin, and cumulative step counts in a fixed time interval, are crafted to\nrepresent meaningful features used as input to the model. The goal of our\napproach is to lower the difference between the predicted CGM values and the\nfingerstick blood glucose readings - the ground truth. Our results indicate\nthat the proposed approach is feasible for more reliable BG forecasting that\nmight improve the performance of the artificial pancreas and insulin infusion\nsystem for T1D diabetes management.\n", "versions": [{"version": "v1", "created": "Mon, 18 Jan 2021 02:31:38 GMT"}], "update_date": "2021-01-19", "authors_parsed": [["Rabby", "Md Fazle", ""], ["Tu", "Yazhou", ""], ["Hossen", "Md Imran", ""], ["Le", "Insup", ""], ["Maida", "Anthony S", ""], ["Hei", "Xiali", ""]]}, {"id": "2101.06855", "submitter": "Dunjie Zhang", "authors": "Jinyin Chen, Dunjie Zhang, Zhaoyan Ming and Kejie Huang", "title": "GraphAttacker: A General Multi-Task GraphAttack Framework", "comments": "13 pages,8 figeures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Graph Neural Networks (GNNs) have been successfully exploited in graph\nanalysis tasks in many real-world applications. However, GNNs have been shown\nto have potential security issues imposed by adversarial samples generated by\nattackers, which achieved great attack performance with almost imperceptible\nperturbations. What limit the wide application of these attackers are their\nmethods' specificity on a certain graph analysis task, such as node\nclassification or link prediction. We thus propose GraphAttacker, a novel\ngeneric graph attack framework that can flexibly adjust the structures and the\nattack strategies according to the graph analysis tasks. Based on the\nGenerative Adversarial Network (GAN), GraphAttacker generates adversarial\nsamples through alternate training on three key components, the Multi-strategy\nAttack Generator (MAG), the Similarity Discriminator (SD), and the Attack\nDiscriminator(AD). Furthermore, to achieve attackers within perturbation\nbudget, we propose a novel Similarity Modification Rate (SMR) to quantify the\nsimilarity between nodes thus constrain the attack budget. We carry out\nextensive experiments and the results show that GraphAttacker can achieve\nstate-of-the-art attack performance on graph analysis tasks of node\nclassification, graph classification, and link prediction. Besides, we also\nanalyze the unique characteristics of each task and their specific response in\nthe unified attack framework. We will release GraphAttacker as an open-source\nsimulation platform for future attack researches.\n", "versions": [{"version": "v1", "created": "Mon, 18 Jan 2021 03:06:41 GMT"}], "update_date": "2021-01-19", "authors_parsed": [["Chen", "Jinyin", ""], ["Zhang", "Dunjie", ""], ["Ming", "Zhaoyan", ""], ["Huang", "Kejie", ""]]}, {"id": "2101.06861", "submitter": "Jie Chen", "authors": "Chao Shang, Jie Chen, Jinbo Bi", "title": "Discrete Graph Structure Learning for Forecasting Multiple Time Series", "comments": "ICLR 2021. Code is available at https://github.com/chaoshangcs/GTS", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Time series forecasting is an extensively studied subject in statistics,\neconomics, and computer science. Exploration of the correlation and causation\namong the variables in a multivariate time series shows promise in enhancing\nthe performance of a time series model. When using deep neural networks as\nforecasting models, we hypothesize that exploiting the pairwise information\namong multiple (multivariate) time series also improves their forecast. If an\nexplicit graph structure is known, graph neural networks (GNNs) have been\ndemonstrated as powerful tools to exploit the structure. In this work, we\npropose learning the structure simultaneously with the GNN if the graph is\nunknown. We cast the problem as learning a probabilistic graph model through\noptimizing the mean performance over the graph distribution. The distribution\nis parameterized by a neural network so that discrete graphs can be sampled\ndifferentiably through reparameterization. Empirical evaluations show that our\nmethod is simpler, more efficient, and better performing than a recently\nproposed bilevel learning approach for graph structure learning, as well as a\nbroad array of forecasting models, either deep or non-deep learning based, and\ngraph or non-graph based.\n", "versions": [{"version": "v1", "created": "Mon, 18 Jan 2021 03:36:33 GMT"}, {"version": "v2", "created": "Mon, 15 Feb 2021 20:21:28 GMT"}, {"version": "v3", "created": "Wed, 21 Apr 2021 03:42:14 GMT"}], "update_date": "2021-04-22", "authors_parsed": [["Shang", "Chao", ""], ["Chen", "Jie", ""], ["Bi", "Jinbo", ""]]}, {"id": "2101.06871", "submitter": "Pranav Rajpurkar", "authors": "Alexander Ke, William Ellsworth, Oishi Banerjee, Andrew Y. Ng, Pranav\n  Rajpurkar", "title": "CheXtransfer: Performance and Parameter Efficiency of ImageNet Models\n  for Chest X-Ray Interpretation", "comments": null, "journal-ref": null, "doi": "10.1145/3450439.3451867", "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Deep learning methods for chest X-ray interpretation typically rely on\npretrained models developed for ImageNet. This paradigm assumes that better\nImageNet architectures perform better on chest X-ray tasks and that\nImageNet-pretrained weights provide a performance boost over random\ninitialization. In this work, we compare the transfer performance and parameter\nefficiency of 16 popular convolutional architectures on a large chest X-ray\ndataset (CheXpert) to investigate these assumptions. First, we find no\nrelationship between ImageNet performance and CheXpert performance for both\nmodels without pretraining and models with pretraining. Second, we find that,\nfor models without pretraining, the choice of model family influences\nperformance more than size within a family for medical imaging tasks. Third, we\nobserve that ImageNet pretraining yields a statistically significant boost in\nperformance across architectures, with a higher boost for smaller\narchitectures. Fourth, we examine whether ImageNet architectures are\nunnecessarily large for CheXpert by truncating final blocks from pretrained\nmodels, and find that we can make models 3.25x more parameter-efficient on\naverage without a statistically significant drop in performance. Our work\ncontributes new experimental evidence about the relation of ImageNet to chest\nx-ray interpretation performance.\n", "versions": [{"version": "v1", "created": "Mon, 18 Jan 2021 04:48:24 GMT"}, {"version": "v2", "created": "Sun, 21 Feb 2021 02:06:43 GMT"}], "update_date": "2021-02-23", "authors_parsed": [["Ke", "Alexander", ""], ["Ellsworth", "William", ""], ["Banerjee", "Oishi", ""], ["Ng", "Andrew Y.", ""], ["Rajpurkar", "Pranav", ""]]}, {"id": "2101.06884", "submitter": "Milan Pape\\v{z}", "authors": "Milan Pape\\v{z}, Anthony Quinn", "title": "Transferring model structure in Bayesian transfer learning for Gaussian\n  process regression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bayesian transfer learning (BTL) is defined in this paper as the task of\nconditioning a target probability distribution on a transferred source\ndistribution. The target globally models the interaction between the source and\ntarget, and conditions on a probabilistic data predictor made available by an\nindependent local source modeller. Fully probabilistic design is adopted to\nsolve this optimal decision-making problem in the target. By successfully\ntransferring higher moments of the source, the target can reject unreliable\nsource knowledge (i.e. it achieves robust transfer). This dual-modeller\nframework means that the source's local processing of raw data into a\ntransferred predictive distribution -- with compressive possibilities -- is\nenriched by (the possible expertise of) the local source model. In addition,\nthe introduction of the global target modeller allows correlation between the\nsource and target tasks -- if known to the target -- to be accounted for.\nImportant consequences emerge. Firstly, the new scheme attains the performance\nof fully modelled (i.e. conventional) multitask learning schemes in (those\nrare) cases where target model misspecification is avoided. Secondly, and more\nimportantly, the new dual-modeller framework is robust to the model\nmisspecification that undermines conventional multitask learning. We thoroughly\nexplore these issues in the key context of interacting Gaussian process\nregression tasks. Experimental evidence from both synthetic and real data\nsettings validates our technical findings: that the proposed BTL framework\nenjoys robustness in transfer while also being robust to model\nmisspecification.\n", "versions": [{"version": "v1", "created": "Mon, 18 Jan 2021 05:28:02 GMT"}], "update_date": "2021-01-19", "authors_parsed": [["Pape\u017e", "Milan", ""], ["Quinn", "Anthony", ""]]}, {"id": "2101.06886", "submitter": "Shunyao Wu", "authors": "Shunyao Wu, Muhammad Alrabeiah, Andrew Hredzak, Chaitali Chakrabarti,\n  and Ahmed Alkhateeb", "title": "Deep Learning for Moving Blockage Prediction using Real Millimeter Wave\n  Measurements", "comments": "6 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG eess.SP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Millimeter wave (mmWave) communication is a key component of 5G and beyond.\nHarvesting the gains of the large bandwidth and low latency at mmWave systems,\nhowever, is challenged by the sensitivity of mmWave signals to blockages; a\nsudden blockage in the line of sight (LOS) link leads to abrupt disconnection,\nwhich affects the reliability of the network. In addition, searching for an\nalternative base station to re-establish the link could result in needless\nlatency overhead. In this paper, we address these challenges collectively by\nutilizing machine learning to anticipate dynamic blockages proactively. The\nproposed approach sees a machine learning algorithm learning to predict future\nblockages by observing what we refer to as the pre-blockage signature. To\nevaluate our proposed approach, we build a mmWave communication setup with a\nmoving blockage and collect a dataset of received power sequences. Simulation\nresults on a real dataset show that blockage occurrence could be predicted with\nmore than 85% accuracy and the exact time instance of blockage occurrence can\nbe obtained with low error. This highlights the potential of the proposed\nsolution for dynamic blockage prediction and proactive hand-off, which enhances\nthe reliability and latency of future wireless networks.\n", "versions": [{"version": "v1", "created": "Mon, 18 Jan 2021 05:34:37 GMT"}, {"version": "v2", "created": "Thu, 21 Jan 2021 09:39:55 GMT"}, {"version": "v3", "created": "Mon, 8 Feb 2021 06:38:33 GMT"}], "update_date": "2021-02-09", "authors_parsed": [["Wu", "Shunyao", ""], ["Alrabeiah", "Muhammad", ""], ["Hredzak", "Andrew", ""], ["Chakrabarti", "Chaitali", ""], ["Alkhateeb", "Ahmed", ""]]}, {"id": "2101.06887", "submitter": "Dmitry Krotov", "authors": "Yuchen Liang, Chaitanya K. Ryali, Benjamin Hoover, Leopold Grinberg,\n  Saket Navlakha, Mohammed J. Zaki, Dmitry Krotov", "title": "Can a Fruit Fly Learn Word Embeddings?", "comments": "Accepted for publication at ICLR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG cs.NE q-bio.NC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The mushroom body of the fruit fly brain is one of the best studied systems\nin neuroscience. At its core it consists of a population of Kenyon cells, which\nreceive inputs from multiple sensory modalities. These cells are inhibited by\nthe anterior paired lateral neuron, thus creating a sparse high dimensional\nrepresentation of the inputs. In this work we study a mathematical\nformalization of this network motif and apply it to learning the correlational\nstructure between words and their context in a corpus of unstructured text, a\ncommon natural language processing (NLP) task. We show that this network can\nlearn semantic representations of words and can generate both static and\ncontext-dependent word embeddings. Unlike conventional methods (e.g., BERT,\nGloVe) that use dense representations for word embedding, our algorithm encodes\nsemantic meaning of words and their context in the form of sparse binary hash\ncodes. The quality of the learned representations is evaluated on word\nsimilarity analysis, word-sense disambiguation, and document classification. It\nis shown that not only can the fruit fly network motif achieve performance\ncomparable to existing methods in NLP, but, additionally, it uses only a\nfraction of the computational resources (shorter training time and smaller\nmemory footprint).\n", "versions": [{"version": "v1", "created": "Mon, 18 Jan 2021 05:41:50 GMT"}, {"version": "v2", "created": "Sun, 14 Mar 2021 19:50:25 GMT"}], "update_date": "2021-03-16", "authors_parsed": [["Liang", "Yuchen", ""], ["Ryali", "Chaitanya K.", ""], ["Hoover", "Benjamin", ""], ["Grinberg", "Leopold", ""], ["Navlakha", "Saket", ""], ["Zaki", "Mohammed J.", ""], ["Krotov", "Dmitry", ""]]}, {"id": "2101.06890", "submitter": "Heechang Ryu", "authors": "Heechang Ryu, Hayong Shin, Jinkyoo Park", "title": "Cooperative and Competitive Biases for Multi-Agent Reinforcement\n  Learning", "comments": "Accepted as a full paper at the Twentieth International Conference on\n  Autonomous Agents and Multiagent Systems (AAMAS-21), Virtual Conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.MA", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Training a multi-agent reinforcement learning (MARL) algorithm is more\nchallenging than training a single-agent reinforcement learning algorithm,\nbecause the result of a multi-agent task strongly depends on the complex\ninteractions among agents and their interactions with a stochastic and dynamic\nenvironment. We propose an algorithm that boosts MARL training using the biased\naction information of other agents based on a friend-or-foe concept. For a\ncooperative and competitive environment, there are generally two groups of\nagents: cooperative-agents and competitive-agents. In the proposed algorithm,\neach agent updates its value function using its own action and the biased\naction information of other agents in the two groups. The biased joint action\nof cooperative agents is computed as the sum of their actual joint action and\nthe imaginary cooperative joint action, by assuming all the cooperative agents\njointly maximize the target agent's value function. The biased joint action of\ncompetitive agents can be computed similarly. Each agent then updates its own\nvalue function using the biased action information, resulting in a biased value\nfunction and corresponding biased policy. Subsequently, the biased policy of\neach agent is inevitably subjected to recommend an action to cooperate and\ncompete with other agents, thereby introducing more active interactions among\nagents and enhancing the MARL policy learning. We empirically demonstrate that\nour algorithm outperforms existing algorithms in various mixed\ncooperative-competitive environments. Furthermore, the introduced biases\ngradually decrease as the training proceeds and the correction based on the\nimaginary assumption vanishes.\n", "versions": [{"version": "v1", "created": "Mon, 18 Jan 2021 05:52:22 GMT"}], "update_date": "2021-01-19", "authors_parsed": [["Ryu", "Heechang", ""], ["Shin", "Hayong", ""], ["Park", "Jinkyoo", ""]]}, {"id": "2101.06891", "submitter": "Jesse Geneson", "authors": "Jesse Geneson", "title": "A note on the price of bandit feedback for mistake-bounded online\n  learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DM cs.LG math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The standard model and the bandit model are two generalizations of the\nmistake-bound model to online multiclass classification. In both models the\nlearner guesses a classification in each round, but in the standard model the\nlearner recieves the correct classification after each guess, while in the\nbandit model the learner is only told whether or not their guess is correct in\neach round. For any set $F$ of multiclass classifiers, define $opt_{std}(F)$\nand $opt_{bandit}(F)$ to be the optimal worst-case number of prediction\nmistakes in the standard and bandit models respectively.\n  Long (Theoretical Computer Science, 2020) claimed that for all $M > 2$ and\ninfinitely many $k$, there exists a set $F$ of functions from a set $X$ to a\nset $Y$ of size $k$ such that $opt_{std}(F) = M$ and $opt_{bandit}(F) \\ge (1 -\no(1))(|Y|\\ln{|Y|})opt_{std}(F)$. The proof of this result depended on the\nfollowing lemma, which is false e.g. for all prime $p \\ge 5$, $s = \\mathbf{1}$\n(the all $1$ vector), $t = \\mathbf{2}$ (the all $2$ vector), and all $z$.\n  Lemma: Fix $n \\ge 2$ and prime $p$, and let $u$ be chosen uniformly at random\nfrom $\\left\\{0, \\dots, p-1\\right\\}^n$. For any $s, t \\in \\left\\{1, \\dots,\np-1\\right\\}^n$ with $s \\neq t$ and for any $z \\in \\left\\{0, \\dots,\np-1\\right\\}$, we have $\\Pr(t \\cdot u = z \\mod p \\text{ } | \\text{ } s \\cdot u =\nz \\mod p) = \\frac{1}{p}$.\n  We show that this lemma is false precisely when $s$ and $t$ are multiples of\neach other mod $p$. Then using a new lemma, we fix Long's proof.\n", "versions": [{"version": "v1", "created": "Mon, 18 Jan 2021 06:00:30 GMT"}, {"version": "v2", "created": "Sun, 31 Jan 2021 21:08:43 GMT"}], "update_date": "2021-02-02", "authors_parsed": [["Geneson", "Jesse", ""]]}, {"id": "2101.06897", "submitter": "Abhijeet Sahu", "authors": "Abhijeet Sahu and Zeyu Mao and Patrick Wlazlo and Hao Huang and\n  Katherine Davis and Ana Goulart and Saman Zonouz", "title": "Multi-Source Data Fusion for Cyberattack Detection in Power Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.SY eess.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cyberattacks can cause a severe impact on power systems unless detected\nearly. However, accurate and timely detection in critical infrastructure\nsystems presents challenges, e.g., due to zero-day vulnerability exploitations\nand the cyber-physical nature of the system coupled with the need for high\nreliability and resilience of the physical system. Conventional rule-based and\nanomaly-based intrusion detection system (IDS) tools are insufficient for\ndetecting zero-day cyber intrusions in the industrial control system (ICS)\nnetworks. Hence, in this work, we show that fusing information from multiple\ndata sources can help identify cyber-induced incidents and reduce false\npositives. Specifically, we present how to recognize and address the barriers\nthat can prevent the accurate use of multiple data sources for fusion-based\ndetection. We perform multi-source data fusion for training IDS in a\ncyber-physical power system testbed where we collect cyber and physical side\ndata from multiple sensors emulating real-world data sources that would be\nfound in a utility and synthesizes these into features for algorithms to detect\nintrusions. Results are presented using the proposed data fusion application to\ninfer False Data and Command injection-based Man-in- The-Middle (MiTM) attacks.\nPost collection, the data fusion application uses time-synchronized merge and\nextracts features followed by pre-processing such as imputation and encoding\nbefore training supervised, semi-supervised, and unsupervised learning models\nto evaluate the performance of the IDS. A major finding is the improvement of\ndetection accuracy by fusion of features from cyber, security, and physical\ndomains. Additionally, we observed the co-training technique performs at par\nwith supervised learning methods when fed with our features.\n", "versions": [{"version": "v1", "created": "Mon, 18 Jan 2021 06:34:45 GMT"}], "update_date": "2021-01-19", "authors_parsed": [["Sahu", "Abhijeet", ""], ["Mao", "Zeyu", ""], ["Wlazlo", "Patrick", ""], ["Huang", "Hao", ""], ["Davis", "Katherine", ""], ["Goulart", "Ana", ""], ["Zonouz", "Saman", ""]]}, {"id": "2101.06905", "submitter": "Yumeng Shao", "authors": "Jun Li, Yumeng Shao, Kang Wei, Ming Ding, Chuan Ma, Long Shi, Zhu Han,\n  and H. Vincent Poor", "title": "Blockchain Assisted Decentralized Federated Learning (BLADE-FL):\n  Performance Analysis and Resource Allocation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Federated learning (FL), as a distributed machine learning paradigm, promotes\npersonal privacy by local data processing at each client. However, relying on a\ncentralized server for model aggregation, standard FL is vulnerable to server\nmalfunctions, untrustworthy server, and external attacks. To address this\nissue, we propose a decentralized FL framework by integrating blockchain into\nFL, namely, blockchain assisted decentralized federated learning (BLADE-FL). In\na round of the proposed BLADE-FL, each client broadcasts the trained model to\nother clients, aggregates its own model with received ones, and then competes\nto generate a block before its local training of the next round. We evaluate\nthe learning performance of BLADE-FL, and develop an upper bound on the global\nloss function. Then we verify that this bound is convex with respect to the\nnumber of overall aggregation rounds K, and optimize the computing resource\nallocation for minimizing the upper bound. We also note that there is a\ncritical problem of training deficiency, caused by lazy clients who plagiarize\nothers' trained models and add artificial noises to disguise their cheating\nbehaviors. Focusing on this problem, we explore the impact of lazy clients on\nthe learning performance of BLADE-FL, and characterize the relationship among\nthe optimal K, the learning parameters, and the proportion of lazy clients.\nBased on MNIST and Fashion-MNIST datasets, we show that the experimental\nresults are consistent with the analytical ones. To be specific, the gap\nbetween the developed upper bound and experimental results is lower than 5%,\nand the optimized K based on the upper bound can effectively minimize the loss\nfunction.\n", "versions": [{"version": "v1", "created": "Mon, 18 Jan 2021 07:19:08 GMT"}, {"version": "v2", "created": "Tue, 25 May 2021 13:41:18 GMT"}], "update_date": "2021-05-26", "authors_parsed": [["Li", "Jun", ""], ["Shao", "Yumeng", ""], ["Wei", "Kang", ""], ["Ding", "Ming", ""], ["Ma", "Chuan", ""], ["Shi", "Long", ""], ["Han", "Zhu", ""], ["Poor", "H. Vincent", ""]]}, {"id": "2101.06906", "submitter": "Kanata Suzuki", "authors": "Kanata Suzuki, Tetsuya Ogata", "title": "Stable deep reinforcement learning method by predicting uncertainty in\n  rewards as a subtask", "comments": "Published as a conference paper at ICONIP 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, a variety of tasks have been accomplished by deep\nreinforcement learning (DRL). However, when applying DRL to tasks in a\nreal-world environment, designing an appropriate reward is difficult. Rewards\nobtained via actual hardware sensors may include noise, misinterpretation, or\nfailed observations. The learning instability caused by these unstable signals\nis a problem that remains to be solved in DRL. In this work, we propose an\napproach that extends existing DRL models by adding a subtask to directly\nestimate the variance contained in the reward signal. The model then takes the\nfeature map learned by the subtask in a critic network and sends it to the\nactor network. This enables stable learning that is robust to the effects of\npotential noise. The results of experiments in the Atari game domain with\nunstable reward signals show that our method stabilizes training convergence.\nWe also discuss the extensibility of the model by visualizing feature maps.\nThis approach has the potential to make DRL more practical for use in noisy,\nreal-world scenarios.\n", "versions": [{"version": "v1", "created": "Mon, 18 Jan 2021 07:19:14 GMT"}], "update_date": "2021-01-19", "authors_parsed": [["Suzuki", "Kanata", ""], ["Ogata", "Tetsuya", ""]]}, {"id": "2101.06915", "submitter": "Praveen Damacharla", "authors": "Praveen Damacharla, Achuth Rao M. V., Jordan Ringenberg, and Ahmad Y\n  Javaid", "title": "TLU-Net: A Deep Learning Approach for Automatic Steel Surface Defect\n  Detection", "comments": null, "journal-ref": "International Conference on Applied Artificial Intelligence\n  (ICAPAI 2021), Halden, Norway, May 19-21, 2021", "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG eess.IV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Visual steel surface defect detection is an essential step in steel sheet\nmanufacturing. Several machine learning-based automated visual inspection (AVI)\nmethods have been studied in recent years. However, most steel manufacturing\nindustries still use manual visual inspection due to training time and\ninaccuracies involved with AVI methods. Automatic steel defect detection\nmethods could be useful in less expensive and faster quality control and\nfeedback. But preparing the annotated training data for segmentation and\nclassification could be a costly process. In this work, we propose to use the\nTransfer Learning-based U-Net (TLU-Net) framework for steel surface defect\ndetection. We use a U-Net architecture as the base and explore two kinds of\nencoders: ResNet and DenseNet. We compare these nets' performance using random\ninitialization and the pre-trained networks trained using the ImageNet data\nset. The experiments are performed using Severstal data. The results\ndemonstrate that the transfer learning performs 5% (absolute) better than that\nof the random initialization in defect classification. We found that the\ntransfer learning performs 26% (relative) better than that of the random\ninitialization in defect segmentation. We also found the gain of transfer\nlearning increases as the training data decreases, and the convergence rate\nwith transfer learning is better than that of the random initialization.\n", "versions": [{"version": "v1", "created": "Mon, 18 Jan 2021 07:53:20 GMT"}], "update_date": "2021-01-19", "authors_parsed": [["Damacharla", "Praveen", ""], ["V.", "Achuth Rao M.", ""], ["Ringenberg", "Jordan", ""], ["Javaid", "Ahmad Y", ""]]}, {"id": "2101.06917", "submitter": "Xiaoxiao Wu", "authors": "Sissi Xiaoxiao Wu, Gangqiang Li, Shengli Zhang, and Xiaohui Lin", "title": "Detection of Insider Attacks in Distributed Projected Subgradient\n  Algorithms", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.IT math.IT", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The gossip-based distributed algorithms are widely used to solve\ndecentralized optimization problems in various multi-agent applications, while\nthey are generally vulnerable to data injection attacks by internal malicious\nagents as each agent locally estimates its decent direction without an\nauthorized supervision. In this work, we explore the application of artificial\nintelligence (AI) technologies to detect internal attacks. We show that a\ngeneral neural network is particularly suitable for detecting and localizing\nthe malicious agents, as they can effectively explore nonlinear relationship\nunderlying the collected data. Moreover, we propose to adopt one of the\nstate-of-art approaches in federated learning, i.e., a collaborative\npeer-to-peer machine learning protocol, to facilitate training our neural\nnetwork models by gossip exchanges. This advanced approach is expected to make\nour model more robust to challenges with insufficient training data, or\nmismatched test data. In our simulations, a least-squared problem is considered\nto verify the feasibility and effectiveness of AI-based methods. Simulation\nresults demonstrate that the proposed AI-based methods are beneficial to\nimprove performance of detecting and localizing malicious agents over\nscore-based methods, and the peer-to-peer neural network model is indeed robust\nto target issues.\n", "versions": [{"version": "v1", "created": "Mon, 18 Jan 2021 08:01:06 GMT"}], "update_date": "2021-01-19", "authors_parsed": [["Wu", "Sissi Xiaoxiao", ""], ["Li", "Gangqiang", ""], ["Zhang", "Shengli", ""], ["Lin", "Xiaohui", ""]]}, {"id": "2101.06930", "submitter": "Fan Yang", "authors": "Fan Yang, Ninghao Liu, Mengnan Du, Xia Hu", "title": "Generative Counterfactuals for Neural Networks via Attribute-Informed\n  Perturbation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  With the wide use of deep neural networks (DNN), model interpretability has\nbecome a critical concern, since explainable decisions are preferred in\nhigh-stake scenarios. Current interpretation techniques mainly focus on the\nfeature attribution perspective, which are limited in indicating why and how\nparticular explanations are related to the prediction. To this end, an\nintriguing class of explanations, named counterfactuals, has been developed to\nfurther explore the \"what-if\" circumstances for interpretation, and enables the\nreasoning capability on black-box models. However, generating counterfactuals\nfor raw data instances (i.e., text and image) is still in the early stage due\nto its challenges on high data dimensionality and unsemantic raw features. In\nthis paper, we design a framework to generate counterfactuals specifically for\nraw data instances with the proposed Attribute-Informed Perturbation (AIP). By\nutilizing generative models conditioned with different attributes,\ncounterfactuals with desired labels can be obtained effectively and\nefficiently. Instead of directly modifying instances in the data space, we\niteratively optimize the constructed attribute-informed latent space, where\nfeatures are more robust and semantic. Experimental results on real-world texts\nand images demonstrate the effectiveness, sample quality as well as efficiency\nof our designed framework, and show the superiority over other alternatives.\nBesides, we also introduce some practical applications based on our framework,\nindicating its potential beyond the model interpretability aspect.\n", "versions": [{"version": "v1", "created": "Mon, 18 Jan 2021 08:37:13 GMT"}], "update_date": "2021-01-19", "authors_parsed": [["Yang", "Fan", ""], ["Liu", "Ninghao", ""], ["Du", "Mengnan", ""], ["Hu", "Xia", ""]]}, {"id": "2101.06940", "submitter": "Shih-Shuo Tung", "authors": "Wen-Liang Hwang, Shih-Shuo Tung", "title": "Learning DNN networks using un-rectifying ReLU with compressed sensing\n  application", "comments": "35 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG eess.SP math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The un-rectifying technique expresses a non-linear point-wise activation\nfunction as a data-dependent variable, which means that the activation variable\nalong with its input and output can all be employed in optimization. The ReLU\nnetwork in this study was un-rectified means that the activation functions\ncould be replaced with data-dependent activation variables in the form of\nequations and constraints. The discrete nature of activation variables\nassociated with un-rectifying ReLUs allows the reformulation of deep learning\nproblems as problems of combinatorial optimization. However, we demonstrate\nthat the optimal solution to a combinatorial optimization problem can be\npreserved by relaxing the discrete domains of activation variables to closed\nintervals. This makes it easier to learn a network using methods developed for\nreal-domain constrained optimization. We also demonstrate that by introducing\ndata-dependent slack variables as constraints, it is possible to optimize a\nnetwork based on the augmented Lagrangian approach. This means that our method\ncould theoretically achieve global convergence and all limit points are\ncritical points of the learning problem. In experiments, our novel approach to\nsolving the compressed sensing recovery problem achieved state-of-the-art\nperformance when applied to the MNIST database and natural images.\n", "versions": [{"version": "v1", "created": "Mon, 18 Jan 2021 09:04:37 GMT"}], "update_date": "2021-01-19", "authors_parsed": [["Hwang", "Wen-Liang", ""], ["Tung", "Shih-Shuo", ""]]}, {"id": "2101.06954", "submitter": "Qiang Zhou", "authors": "Qiang Zhou, Jingjing Gu, Xinjiang Lu, Fuzhen Zhuang, Yanchao Zhao,\n  Qiuhong Wang, Xiao Zhang", "title": "Modeling Heterogeneous Relations across Multiple Modes for Potential\n  Crowd Flow Prediction", "comments": "Accepted by the 35th AAAI Conference on Artificial Intelligence (AAAI\n  2021)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Potential crowd flow prediction for new planned transportation sites is a\nfundamental task for urban planners and administrators. Intuitively, the\npotential crowd flow of the new coming site can be implied by exploring the\nnearby sites. However, the transportation modes of nearby sites (e.g. bus\nstations, bicycle stations) might be different from the target site (e.g.\nsubway station), which results in severe data scarcity issues. To this end, we\npropose a data driven approach, named MOHER, to predict the potential crowd\nflow in a certain mode for a new planned site. Specifically, we first identify\nthe neighbor regions of the target site by examining the geographical proximity\nas well as the urban function similarity. Then, to aggregate these\nheterogeneous relations, we devise a cross-mode relational GCN, a novel\nrelation-specific transformation model, which can learn not only the\ncorrelations but also the differences between different transportation modes.\nAfterward, we design an aggregator for inductive potential flow representation.\nFinally, an LTSM module is used for sequential flow prediction. Extensive\nexperiments on real-world data sets demonstrate the superiority of the MOHER\nframework compared with the state-of-the-art algorithms.\n", "versions": [{"version": "v1", "created": "Mon, 18 Jan 2021 09:31:30 GMT"}], "update_date": "2021-01-19", "authors_parsed": [["Zhou", "Qiang", ""], ["Gu", "Jingjing", ""], ["Lu", "Xinjiang", ""], ["Zhuang", "Fuzhen", ""], ["Zhao", "Yanchao", ""], ["Wang", "Qiuhong", ""], ["Zhang", "Xiao", ""]]}, {"id": "2101.06967", "submitter": "Alexandre Thiery", "authors": "Atin Ghosh and Alexandre H. Thiery", "title": "On Data-Augmentation and Consistency-Based Semi-Supervised Learning", "comments": null, "journal-ref": "ICLR 2021", "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Recently proposed consistency-based Semi-Supervised Learning (SSL) methods\nsuch as the $\\Pi$-model, temporal ensembling, the mean teacher, or the virtual\nadversarial training, have advanced the state of the art in several SSL tasks.\nThese methods can typically reach performances that are comparable to their\nfully supervised counterparts while using only a fraction of labelled examples.\nDespite these methodological advances, the understanding of these methods is\nstill relatively limited. In this text, we analyse (variations of) the\n$\\Pi$-model in settings where analytically tractable results can be obtained.\nWe establish links with Manifold Tangent Classifiers and demonstrate that the\nquality of the perturbations is key to obtaining reasonable SSL performances.\nImportantly, we propose a simple extension of the Hidden Manifold Model that\nnaturally incorporates data-augmentation schemes and offers a framework for\nunderstanding and experimenting with SSL methods.\n", "versions": [{"version": "v1", "created": "Mon, 18 Jan 2021 10:12:31 GMT"}], "update_date": "2021-01-19", "authors_parsed": [["Ghosh", "Atin", ""], ["Thiery", "Alexandre H.", ""]]}, {"id": "2101.06982", "submitter": "Clarice Poon", "authors": "Jingwei Liang and Clarice Poon", "title": "Screening for Sparse Online Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sparsity promoting regularizers are widely used to impose low-complexity\nstructure (e.g. l1-norm for sparsity) to the regression coefficients of\nsupervised learning. In the realm of deterministic optimization, the sequence\ngenerated by iterative algorithms (such as proximal gradient descent) exhibit\n\"finite activity identification\", namely, they can identify the low-complexity\nstructure in a finite number of iterations. However, most online algorithms\n(such as proximal stochastic gradient descent) do not have the property owing\nto the vanishing step-size and non-vanishing variance. In this paper, by\ncombining with a screening rule, we show how to eliminate useless features of\nthe iterates generated by online algorithms, and thereby enforce finite\nactivity identification. One consequence is that when combined with any\nconvergent online algorithm, sparsity properties imposed by the regularizer can\nbe exploited for computational gains. Numerically, significant acceleration can\nbe obtained.\n", "versions": [{"version": "v1", "created": "Mon, 18 Jan 2021 10:40:47 GMT"}], "update_date": "2021-01-19", "authors_parsed": [["Liang", "Jingwei", ""], ["Poon", "Clarice", ""]]}, {"id": "2101.06983", "submitter": "Luyu Gao", "authors": "Luyu Gao, Yunyi Zhang, Jiawei Han, Jamie Callan", "title": "Scaling Deep Contrastive Learning Batch Size under Memory Limited Setup", "comments": "RepL4NLP 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Contrastive learning has been applied successfully to learn vector\nrepresentations of text. Previous research demonstrated that learning\nhigh-quality representations benefits from batch-wise contrastive loss with a\nlarge number of negatives. In practice, the technique of in-batch negative is\nused, where for each example in a batch, other batch examples' positives will\nbe taken as its negatives, avoiding encoding extra negatives. This, however,\nstill conditions each example's loss on all batch examples and requires fitting\nthe entire large batch into GPU memory. This paper introduces a gradient\ncaching technique that decouples backpropagation between contrastive loss and\nthe encoder, removing encoder backward pass data dependency along the batch\ndimension. As a result, gradients can be computed for one subset of the batch\nat a time, leading to almost constant memory usage.\n", "versions": [{"version": "v1", "created": "Mon, 18 Jan 2021 10:42:34 GMT"}, {"version": "v2", "created": "Mon, 14 Jun 2021 16:37:28 GMT"}], "update_date": "2021-06-16", "authors_parsed": [["Gao", "Luyu", ""], ["Zhang", "Yunyi", ""], ["Han", "Jiawei", ""], ["Callan", "Jamie", ""]]}, {"id": "2101.06986", "submitter": "Katarina Domijan PhD", "authors": "Catherine B. Hurley, Mark O'Connell, Katarina Domijan", "title": "Interactive slice visualization for exploring machine learning models", "comments": "35 pages, 14 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Machine learning models fit complex algorithms to arbitrarily large datasets.\nThese algorithms are well-known to be high on performance and low on\ninterpretability. We use interactive visualization of slices of predictor space\nto address the interpretability deficit; in effect opening up the black-box of\nmachine learning algorithms, for the purpose of interrogating, explaining,\nvalidating and comparing model fits. Slices are specified directly through\ninteraction, or using various touring algorithms designed to visit\nhigh-occupancy sections or regions where the model fits have interesting\nproperties. The methods presented here are implemented in the R package\n\\pkg{condvis2}.\n", "versions": [{"version": "v1", "created": "Mon, 18 Jan 2021 10:47:53 GMT"}], "update_date": "2021-01-19", "authors_parsed": [["Hurley", "Catherine B.", ""], ["O'Connell", "Mark", ""], ["Domijan", "Katarina", ""]]}, {"id": "2101.06993", "submitter": "Mingxuan Li", "authors": "Mingxuan Li, Yuanxun Shao", "title": "Deep Compression of Neural Networks for Fault Detection on Tennessee\n  Eastman Chemical Processes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Artificial neural network has achieved the state-of-art performance in fault\ndetection on the Tennessee Eastman process, but it often requires enormous\nmemory to fund its massive parameters. In order to implement online real-time\nfault detection, three deep compression techniques (pruning, clustering, and\nquantization) are applied to reduce the computational burden. We have\nextensively studied 7 different combinations of compression techniques, all\nmethods achieve high model compression rates over 64% while maintain high fault\ndetection accuracy. The best result is applying all three techniques, which\nreduces the model sizes by 91.5% and remains a high accuracy over 94%. This\nresult leads to a smaller storage requirement in production environments, and\nmakes the deployment smoother in real world.\n", "versions": [{"version": "v1", "created": "Mon, 18 Jan 2021 10:53:12 GMT"}], "update_date": "2021-01-19", "authors_parsed": [["Li", "Mingxuan", ""], ["Shao", "Yuanxun", ""]]}, {"id": "2101.07012", "submitter": "Hisham Husain", "authors": "Hisham Husain and Kamil Ciosek and Ryota Tomioka", "title": "Regularized Policies are Reward Robust", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Entropic regularization of policies in Reinforcement Learning (RL) is a\ncommonly used heuristic to ensure that the learned policy explores the\nstate-space sufficiently before overfitting to a local optimal policy. The\nprimary motivation for using entropy is for exploration and disambiguating\noptimal policies; however, the theoretical effects are not entirely understood.\nIn this work, we study the more general regularized RL objective and using\nFenchel duality; we derive the dual problem which takes the form of an\nadversarial reward problem. In particular, we find that the optimal policy\nfound by a regularized objective is precisely an optimal policy of a\nreinforcement learning problem under a worst-case adversarial reward. Our\nresult allows us to reinterpret the popular entropic regularization scheme as a\nform of robustification. Furthermore, due to the generality of our results, we\napply to other existing regularization schemes. Our results thus give insights\ninto the effects of regularization of policies and deepen our understanding of\nexploration through robust rewards at large.\n", "versions": [{"version": "v1", "created": "Mon, 18 Jan 2021 11:38:47 GMT"}], "update_date": "2021-01-19", "authors_parsed": [["Husain", "Hisham", ""], ["Ciosek", "Kamil", ""], ["Tomioka", "Ryota", ""]]}, {"id": "2101.07023", "submitter": "Laura Scarabosio", "authors": "Laura Scarabosio", "title": "Deep neural network surrogates for non-smooth quantities of interest in\n  shape uncertainty quantification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.NA cs.LG cs.NA stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the point evaluation of the solution to interface problems with\ngeometric uncertainties, where the uncertainty in the obstacle is described by\na high-dimensional parameter $\\boldsymbol{y}\\in[-1,1]^d$, $d\\in\\mathbb{N}$. We\nfocus in particular on an elliptic interface problem and a Helmholtz\ntransmission problem. Point values of the solution in the physical domain\ndepend in general non-smoothly on the high-dimensional parameter, posing a\nchallenge when one is interested in building surrogates. Indeed, high-order\nmethods show poor convergence rates, while methods which are able to track\ndiscontinuities usually suffer from the so-called curse of dimensionality. For\nthis reason, in this work we propose to build surrogates for point evaluation\nusing deep neural networks. We provide a theoretical justification for why we\nexpect neural networks to provide good surrogates. Furthermore, we present\nextensive numerical experiments showing their good performance in practice. We\nobserve in particular that neural networks do not suffer from the curse of\ndimensionality, and we study the dependence of the error on the number of point\nevaluations (that is, the number of discontinuities in the parameter space), as\nwell as on several modeling parameters, such as the contrast between the two\nmaterials and, for the Helmholtz transmission problem, the wavenumber.\n", "versions": [{"version": "v1", "created": "Mon, 18 Jan 2021 12:02:57 GMT"}], "update_date": "2021-01-19", "authors_parsed": [["Scarabosio", "Laura", ""]]}, {"id": "2101.07033", "submitter": "Anastasios Gounaris", "authors": "Petros Petsinis, Athanasios Naskos and Anastasios Gounaris", "title": "Analysis of key flavors of event-driven predictive maintenance using\n  logs of phenomena described by Weibull distributions", "comments": "13 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This work explores two approaches to event-driven predictive maintenance in\nIndustry 4.0 that cast the problem at hand as a classification or a regression\none, respectively, using as a starting point two state-of-the-art solutions.\nFor each of the two approaches, we examine different data preprocessing\ntechniques, different prediction algorithms and the impact of ensemble and\nsampling methods. Through systematic experiments regarding the aspectsmentioned\nabove,we aimto understand the strengths of the alternatives, and more\nimportantly, shed light on how to navigate through the vast number of such\nalternatives in an informed manner. Our work constitutes a key step towards\nunderstanding the true potential of this type of data-driven predictive\nmaintenance as of to date, and assist practitioners in focusing on the aspects\nthat have the greatest impact.\n", "versions": [{"version": "v1", "created": "Mon, 18 Jan 2021 12:15:35 GMT"}], "update_date": "2021-01-19", "authors_parsed": [["Petsinis", "Petros", ""], ["Naskos", "Athanasios", ""], ["Gounaris", "Anastasios", ""]]}, {"id": "2101.07043", "submitter": "Samrat Mukhopadhyay", "authors": "Samrat Mukhopadhyay, Abhishek Sinha", "title": "Online Caching with Optimal Switching Regret", "comments": "11 pages, 3 figures, to be submitted to ISIT, 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.LG cs.PF math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the classical uncoded caching problem from an online learning\npoint-of-view. A cache of limited storage capacity can hold $C$ files at a time\nfrom a large catalog. A user requests an arbitrary file from the catalog at\neach time slot. Before the file request from the user arrives, a caching policy\npopulates the cache with any $C$ files of its choice. In the case of a\ncache-hit, the policy receives a unit reward and zero rewards otherwise. In\naddition to that, there is a cost associated with fetching files to the cache,\nwhich we refer to as the switching cost. The objective is to design a caching\npolicy that incurs minimal regret while considering both the rewards due to\ncache-hits and the switching cost due to the file fetches. The main\ncontribution of this paper is the switching regret analysis of a Follow the\nPerturbed Leader-based anytime caching policy, which is shown to have an order\noptimal switching regret. In this pursuit, we improve the best-known switching\nregret bound for this problem by a factor of $\\Theta(\\sqrt{C}).$ We conclude\nthe paper by comparing the performance of different popular caching policies\nusing a publicly available trace from a commercial CDN server.\n", "versions": [{"version": "v1", "created": "Mon, 18 Jan 2021 12:47:22 GMT"}], "update_date": "2021-01-19", "authors_parsed": [["Mukhopadhyay", "Samrat", ""], ["Sinha", "Abhishek", ""]]}, {"id": "2101.07046", "submitter": "Maximilian Soelch", "authors": "Justin Bayer, Maximilian Soelch, Atanas Mirchev, Baris Kayalibay,\n  Patrick van der Smagt", "title": "Mind the Gap when Conditioning Amortised Inference in Sequential\n  Latent-Variable Models", "comments": "Published as a conference paper at ICLR 2021 (Poster)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Amortised inference enables scalable learning of sequential latent-variable\nmodels (LVMs) with the evidence lower bound (ELBO). In this setting,\nvariational posteriors are often only partially conditioned. While the true\nposteriors depend, e.g., on the entire sequence of observations, approximate\nposteriors are only informed by past observations. This mimics the Bayesian\nfilter -- a mixture of smoothing posteriors. Yet, we show that the ELBO\nobjective forces partially-conditioned amortised posteriors to approximate\nproducts of smoothing posteriors instead. Consequently, the learned generative\nmodel is compromised. We demonstrate these theoretical findings in three\nscenarios: traffic flow, handwritten digits, and aerial vehicle dynamics. Using\nfully-conditioned approximate posteriors, performance improves in terms of\ngenerative modelling and multi-step prediction.\n", "versions": [{"version": "v1", "created": "Mon, 18 Jan 2021 12:53:39 GMT"}, {"version": "v2", "created": "Wed, 17 Mar 2021 16:52:01 GMT"}], "update_date": "2021-03-18", "authors_parsed": [["Bayer", "Justin", ""], ["Soelch", "Maximilian", ""], ["Mirchev", "Atanas", ""], ["Kayalibay", "Baris", ""], ["van der Smagt", "Patrick", ""]]}, {"id": "2101.07053", "submitter": "Iman Saberi", "authors": "Iman Saberi, Fathiyeh Faghih, Farzad Sobhi Bavil", "title": "A Passive Online Technique for Learning Hybrid Automata from\n  Input/Output Traces", "comments": "10 pages, 15 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.FL cs.LG cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Specification synthesis is the process of deriving a model from the\ninput-output traces of a system. It is used extensively in test design, reverse\nengineering, and system identification. One type of the resulting artifact of\nthis process for cyber-physical systems is hybrid automata. They are intuitive,\nprecise, tool independent, and at a high level of abstraction, and can model\nsystems with both discrete and continuous variables. In this paper, we propose\na new technique for synthesizing hybrid automaton from the input-output traces\nof a non-linear cyber-physical system. Similarity detection in non-linear\nbehaviors is the main challenge for extracting such models. We address this\nproblem by utilizing the Dynamic Time Warping technique. Our approach is\npassive, meaning that it does not need interaction with the system during\nautomata synthesis from the logged traces; and online, which means that each\ninput/output trace is used only once in the procedure. In other words, each new\ntrace can be used to improve the already synthesized automaton. We evaluated\nour algorithm in two industrial and simulated case studies. The accuracy of the\nderived automata show promising results.\n", "versions": [{"version": "v1", "created": "Mon, 18 Jan 2021 13:08:14 GMT"}], "update_date": "2021-01-19", "authors_parsed": [["Saberi", "Iman", ""], ["Faghih", "Fathiyeh", ""], ["Bavil", "Farzad Sobhi", ""]]}, {"id": "2101.07061", "submitter": "Rooholla Khorrambakht", "authors": "Rooholla Khorrambakht, Chris Xiaoxuan Lu, Hamed Damirchi, Zhenghua\n  Chen, Zhengguo Li", "title": "Deep Inertial Odometry with Accurate IMU Preintegration", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Inertial Measurement Units (IMUs) are interceptive modalities that provide\nego-motion measurements independent of the environmental factors. They are\nwidely adopted in various autonomous systems. Motivated by the limitations in\nprocessing the noisy measurements from these sensors using their mathematical\nmodels, researchers have recently proposed various deep learning architectures\nto estimate inertial odometry in an end-to-end manner. Nevertheless, the\nhigh-frequency and redundant measurements from IMUs lead to long raw sequences\nto be processed. In this study, we aim to investigate the efficacy of accurate\npreintegration as a more realistic solution to the IMU motion model for deep\ninertial odometry (DIO) and the resultant DIO is a fusion of model-driven and\ndata-driven approaches. The accurate IMU preintegration has the potential to\noutperform numerical approximation of the continuous IMU model used in the\nexisting DIOs. Experimental results validate the proposed DIO.\n", "versions": [{"version": "v1", "created": "Mon, 18 Jan 2021 13:16:04 GMT"}], "update_date": "2021-01-19", "authors_parsed": [["Khorrambakht", "Rooholla", ""], ["Lu", "Chris Xiaoxuan", ""], ["Damirchi", "Hamed", ""], ["Chen", "Zhenghua", ""], ["Li", "Zhengguo", ""]]}, {"id": "2101.07069", "submitter": "Jong-Seok Lee", "authors": "Seong-Eun Moon, Chun-Jui Chen, Cho-Jui Hsieh, Jane-Ling Wang,\n  Jong-Seok Lee", "title": "Emotional EEG Classification using Connectivity Features and\n  Convolutional Neural Networks", "comments": null, "journal-ref": "Neural Networks, vol. 132, pp. 96-107, Dec. 2020", "doi": "10.1016/j.neunet.2020.08.009", "report-no": null, "categories": "cs.LG cs.HC", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Convolutional neural networks (CNNs) are widely used to recognize the user's\nstate through electroencephalography (EEG) signals. In the previous studies,\nthe EEG signals are usually fed into the CNNs in the form of high-dimensional\nraw data. However, this approach makes it difficult to exploit the brain\nconnectivity information that can be effective in describing the functional\nbrain network and estimating the perceptual state of the user. We introduce a\nnew classification system that utilizes brain connectivity with a CNN and\nvalidate its effectiveness via the emotional video classification by using\nthree different types of connectivity measures. Furthermore, two data-driven\nmethods to construct the connectivity matrix are proposed to maximize\nclassification performance. Further analysis reveals that the level of\nconcentration of the brain connectivity related to the emotional property of\nthe target video is correlated with classification performance.\n", "versions": [{"version": "v1", "created": "Mon, 18 Jan 2021 13:28:08 GMT"}], "update_date": "2021-01-19", "authors_parsed": [["Moon", "Seong-Eun", ""], ["Chen", "Chun-Jui", ""], ["Hsieh", "Cho-Jui", ""], ["Wang", "Jane-Ling", ""], ["Lee", "Jong-Seok", ""]]}, {"id": "2101.07077", "submitter": "Jinxiong Zhang", "authors": "Jinxiong Zhang", "title": "Yet Another Representation of Binary Decision Trees: A Mathematical\n  Demonstration", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  A decision tree looks like a simple computational graph without cycles, where\nonly the leaf nodes specify the output values and the non-terminals specify\ntheir tests or split conditions. From the numerical perspective, we express\ndecision trees in the language of computational graph. We explicitly\nparameterize the test phase, traversal phase and prediction phase of decision\ntrees based on the bitvectors of non-terminal nodes. As shown later, the\ndecision tree is a shallow binary network in some sense. Especially, we\nintroduce the bitvector matrix to implement the tree traversal in numerical\napproach, where the core is to convert the logical `AND' operation to\narithmetic operations. And we apply this numerical representation to extend and\nunify diverse decision trees in concept.\n", "versions": [{"version": "v1", "created": "Mon, 18 Jan 2021 13:50:14 GMT"}, {"version": "v2", "created": "Tue, 19 Jan 2021 12:04:47 GMT"}, {"version": "v3", "created": "Thu, 21 Jan 2021 14:20:33 GMT"}], "update_date": "2021-01-22", "authors_parsed": [["Zhang", "Jinxiong", ""]]}, {"id": "2101.07100", "submitter": "Kenenbek Arzymatov", "authors": "Kenenbek Arzymatov, Mikhail Hushchyn, Andrey Sapronov, Vladislav\n  Belavin, Leonid Gremyachikh, Maksim Karpov and Andrey Ustyuzhanin", "title": "Online detection of failures generated by storage simulator", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Modern large-scale data-farms consist of hundreds of thousands of storage\ndevices that span distributed infrastructure. Devices used in modern data\ncenters (such as controllers, links, SSD- and HDD-disks) can fail due to\nhardware as well as software problems. Such failures or anomalies can be\ndetected by monitoring the activity of components using machine learning\ntechniques. In order to use these techniques, researchers need plenty of\nhistorical data of devices in normal and failure mode for training algorithms.\nIn this work, we challenge two problems: 1) lack of storage data in the methods\nabove by creating a simulator and 2) applying existing online algorithms that\ncan faster detect a failure occurred in one of the components.\n  We created a Go-based (golang) package for simulating the behavior of modern\nstorage infrastructure. The software is based on the discrete-event modeling\nparadigm and captures the structure and dynamics of high-level storage system\nbuilding blocks. The package's flexible structure allows us to create a model\nof a real-world storage system with a configurable number of components. The\nprimary area of interest is exploring the storage machine's behavior under\nstress testing or exploitation in the medium- or long-term for observing\nfailures of its components.\n  To discover failures in the time series distribution generated by the\nsimulator, we modified a change point detection algorithm that works in online\nmode. The goal of the change-point detection is to discover differences in time\nseries distribution. This work describes an approach for failure detection in\ntime series data based on direct density ratio estimation via binary\nclassifiers.\n", "versions": [{"version": "v1", "created": "Mon, 18 Jan 2021 14:56:53 GMT"}], "update_date": "2021-01-19", "authors_parsed": [["Arzymatov", "Kenenbek", ""], ["Hushchyn", "Mikhail", ""], ["Sapronov", "Andrey", ""], ["Belavin", "Vladislav", ""], ["Gremyachikh", "Leonid", ""], ["Karpov", "Maksim", ""], ["Ustyuzhanin", "Andrey", ""]]}, {"id": "2101.07107", "submitter": "Jeremy Turiel", "authors": "Antonio Briola, Jeremy Turiel, Riccardo Marcaccioli, Tomaso Aste", "title": "Deep Reinforcement Learning for Active High Frequency Trading", "comments": "9 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.MA q-fin.TR", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  We introduce the first end-to-end Deep Reinforcement Learning (DRL) based\nframework for active high frequency trading. We train DRL agents to trade one\nunit of Intel Corporation stock by employing the Proximal Policy Optimization\nalgorithm. The training is performed on three contiguous months of high\nfrequency Limit Order Book data, of which the last month constitutes the\nvalidation data. In order to maximise the signal to noise ratio in the training\ndata, we compose the latter by only selecting training samples with largest\nprice changes. The test is then carried out on the following month of data.\nHyperparameters are tuned using the Sequential Model Based Optimization\ntechnique. We consider three different state characterizations, which differ in\ntheir LOB-based meta-features. Analysing the agents' performances on test data,\nwe argue that the agents are able to create a dynamic representation of the\nunderlying environment. They identify occasional regularities present in the\ndata and exploit them to create long-term profitable trading strategies.\nIndeed, agents learn trading strategies able to produce stable positive returns\nin spite of the highly stochastic and non-stationary environment.\n", "versions": [{"version": "v1", "created": "Mon, 18 Jan 2021 15:09:28 GMT"}, {"version": "v2", "created": "Thu, 4 Feb 2021 11:46:03 GMT"}], "update_date": "2021-02-05", "authors_parsed": [["Briola", "Antonio", ""], ["Turiel", "Jeremy", ""], ["Marcaccioli", "Riccardo", ""], ["Aste", "Tomaso", ""]]}, {"id": "2101.07120", "submitter": "Aravindh Gowtham Bommisetty", "authors": "Mohan Bharath B, Aravindh Gowtham B, Akhil M", "title": "Neural Abstractive Text Summarizer for Telugu Language", "comments": "11 pages, 2 figures. Presented the paper at Third International\n  Conference on Soft Computing and Signal Processing (ICSCSP 2020) and is\n  currently in production. It will soon be published in springer Advances in\n  Intelligent Systems and Computing (AISC) series", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Abstractive Text Summarization is the process of constructing semantically\nrelevant shorter sentences which captures the essence of the overall meaning of\nthe source text. It is actually difficult and very time consuming for humans to\nsummarize manually large documents of text. Much of work in abstractive text\nsummarization is being done in English and almost no significant work has been\nreported in Telugu abstractive text summarization. So, we would like to propose\nan abstractive text summarization approach for Telugu language using Deep\nlearning. In this paper we are proposing an abstractive text summarization Deep\nlearning model for Telugu language. The proposed architecture is based on\nencoder-decoder sequential models with attention mechanism. We have applied\nthis model on manually created dataset to generate a one sentence summary of\nthe source text and have got good results measured qualitatively.\n", "versions": [{"version": "v1", "created": "Mon, 18 Jan 2021 15:22:50 GMT"}], "update_date": "2021-01-19", "authors_parsed": [["B", "Mohan Bharath", ""], ["B", "Aravindh Gowtham", ""], ["M", "Akhil", ""]]}, {"id": "2101.07123", "submitter": "L\\'eonard Blier", "authors": "L\\'eonard Blier, Corentin Tallec, Yann Ollivier", "title": "Learning Successor States and Goal-Dependent Values: A Mathematical\n  Viewpoint", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In reinforcement learning, temporal difference-based algorithms can be\nsample-inefficient: for instance, with sparse rewards, no learning occurs until\na reward is observed. This can be remedied by learning richer objects, such as\na model of the environment, or successor states. Successor states model the\nexpected future state occupancy from any given state for a given policy and are\nrelated to goal-dependent value functions, which learn how to reach arbitrary\nstates. We formally derive the temporal difference algorithm for successor\nstate and goal-dependent value function learning, either for discrete or for\ncontinuous environments with function approximation. Especially, we provide\nfinite-variance estimators even in continuous environments, where the reward\nfor exactly reaching a goal state becomes infinitely sparse. Successor states\nsatisfy more than just the Bellman equation: a backward Bellman operator and a\nBellman-Newton (BN) operator encode path compositionality in the environment.\nThe BN operator is akin to second-order gradient descent methods and provides\nthe true update of the value function when acquiring more observations, with\nexplicit tabular bounds. In the tabular case and with infinitesimal learning\nrates, mixing the usual and backward Bellman operators provably improves\neigenvalues for asymptotic convergence, and the asymptotic convergence of the\nBN operator is provably better than TD, with a rate independent from the\nenvironment. However, the BN method is more complex and less robust to sampling\nnoise. Finally, a forward-backward (FB) finite-rank parameterization of\nsuccessor states enjoys reduced variance and improved samplability, provides a\ndirect model of the value function, has fully understood fixed points\ncorresponding to long-range dependencies, approximates the BN method, and\nprovides two canonical representations of states as a byproduct.\n", "versions": [{"version": "v1", "created": "Mon, 18 Jan 2021 15:33:26 GMT"}], "update_date": "2021-01-19", "authors_parsed": [["Blier", "L\u00e9onard", ""], ["Tallec", "Corentin", ""], ["Ollivier", "Yann", ""]]}, {"id": "2101.07126", "submitter": "Asaf Amrami", "authors": "Asaf Amrami and Yoav Goldberg", "title": "A simple geometric proof for the benefit of depth in ReLU networks", "comments": "9 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We present a simple proof for the benefit of depth in multi-layer feedforward\nnetwork with rectified activation (\"depth separation\"). Specifically we present\na sequence of classification problems indexed by $m$ such that (a) for any\nfixed depth rectified network there exist an $m$ above which classifying\nproblem $m$ correctly requires exponential number of parameters (in $m$); and\n(b) for any problem in the sequence, we present a concrete neural network with\nlinear depth (in $m$) and small constant width ($\\leq 4$) that classifies the\nproblem with zero error.\n  The constructive proof is based on geometric arguments and a space folding\nconstruction.\n  While stronger bounds and results exist, our proof uses substantially simpler\ntools and techniques, and should be accessible to undergraduate students in\ncomputer science and people with similar backgrounds.\n", "versions": [{"version": "v1", "created": "Mon, 18 Jan 2021 15:40:27 GMT"}], "update_date": "2021-01-19", "authors_parsed": [["Amrami", "Asaf", ""], ["Goldberg", "Yoav", ""]]}, {"id": "2101.07128", "submitter": "Talha Siddiue", "authors": "Talha Siddique and Md Shaad Mahmud", "title": "Classification of fNIRS Data Under Uncertainty: A Bayesian Neural\n  Network Approach", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Functional Near-Infrared Spectroscopy (fNIRS) is a non-invasive form of\nBrain-Computer Interface (BCI). It is used for the imaging of brain\nhemodynamics and has gained popularity due to the certain pros it poses over\nother similar technologies. The overall functionalities encompass the capture,\nprocessing and classification of brain signals. Since hemodynamic responses are\ncontaminated by physiological noises, several methods have been implemented in\nthe past literature to classify the responses in focus from the unwanted ones.\nHowever, the methods, thus far does not take into consideration the uncertainty\nin the data or model parameters. In this paper, we use a Bayesian Neural\nNetwork (BNN) to carry out a binary classification on an open-access dataset,\nconsisting of unilateral finger tapping (left- and right-hand finger tapping).\nA BNN uses Bayesian statistics to assign a probability distribution to the\nnetwork weights instead of a point estimate. In this way, it takes data and\nmodel uncertainty into consideration while carrying out the classification. We\nused Variational Inference (VI) to train our model. Our model produced an\noverall classification accuracy of 86.44% over 30 volunteers. We illustrated\nhow the evidence lower bound (ELBO) function of the model converges over\niterations. We further illustrated the uncertainty that is inherent during the\nsampling of the posterior distribution of the weights. We also generated a ROC\ncurve for our BNN classifier using test data from a single volunteer and our\nmodel has an AUC score of 0.855.\n", "versions": [{"version": "v1", "created": "Mon, 18 Jan 2021 15:43:59 GMT"}], "update_date": "2021-01-19", "authors_parsed": [["Siddique", "Talha", ""], ["Mahmud", "Md Shaad", ""]]}, {"id": "2101.07138", "submitter": "Yannis Papanikolaou", "authors": "Yannis Papanikolaou", "title": "Teach me how to Label: Labeling Functions from Natural Language with\n  Text-to-text Transformers", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Annotated data has become the most important bottleneck in training accurate\nmachine learning models, especially for areas that require domain expertise. A\nrecent approach to deal with the above issue proposes using natural language\nexplanations instead of labeling individual data points, thereby increasing\nhuman annotators' efficiency as well as decreasing costs substantially. This\npaper focuses on the task of turning these natural language descriptions into\nPython labeling functions by following a novel approach to semantic parsing\nwith pre-trained text-to-text Transformers. In a series of experiments our\napproach achieves a new state of the art on the semantic parsing benchmark\nCoNaLa, surpassing the previous best approach by 3.7 BLEU points. Furthermore,\non a manually constructed dataset of natural language descriptions-labeling\nfunctions pairs we achieve a BLEU of 0.39. Our approach can be regarded as a\nstepping stone towards models that are taught how to label in natural language,\ninstead of being provided specific labeled samples. Our code, constructed\ndataset and models are available at\nhttps://github.com/ypapanik/t5-for-code-generation.\n", "versions": [{"version": "v1", "created": "Mon, 18 Jan 2021 16:04:15 GMT"}], "update_date": "2021-01-19", "authors_parsed": [["Papanikolaou", "Yannis", ""]]}, {"id": "2101.07140", "submitter": "Pradyumna Tambwekar", "authors": "Pradyumna Tambwekar, Andrew Silva, Nakul Gopalan, Matthew Gombolay", "title": "Interpretable Policy Specification and Synthesis through Natural\n  Language and RL", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CL cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Policy specification is a process by which a human can initialize a robot's\nbehaviour and, in turn, warm-start policy optimization via Reinforcement\nLearning (RL). While policy specification/design is inherently a collaborative\nprocess, modern methods based on Learning from Demonstration or Deep RL lack\nthe model interpretability and accessibility to be classified as such. Current\nstate-of-the-art methods for policy specification rely on black-box models,\nwhich are an insufficient means of collaboration for non-expert users: These\nmodels provide no means of inspecting policies learnt by the agent and are not\nfocused on creating a usable modality for teaching robot behaviour. In this\npaper, we propose a novel machine learning framework that enables humans to 1)\nspecify, through natural language, interpretable policies in the form of\neasy-to-understand decision trees, 2) leverage these policies to warm-start\nreinforcement learning and 3) outperform baselines that lack our natural\nlanguage initialization mechanism. We train our approach by collecting a\nfirst-of-its-kind corpus mapping free-form natural language policy descriptions\nto decision tree-based policies. We show that our novel framework translates\nnatural language to decision trees with a 96% and 97% accuracy on a held-out\ncorpus across two domains, respectively. Finally, we validate that policies\ninitialized with natural language commands are able to significantly outperform\nrelevant baselines (p < 0.001) that do not benefit from our natural\nlanguage-based warm-start technique.\n", "versions": [{"version": "v1", "created": "Mon, 18 Jan 2021 16:07:00 GMT"}], "update_date": "2021-01-19", "authors_parsed": [["Tambwekar", "Pradyumna", ""], ["Silva", "Andrew", ""], ["Gopalan", "Nakul", ""], ["Gombolay", "Matthew", ""]]}, {"id": "2101.07150", "submitter": "Michael Rauchensteiner", "authors": "Christian Fiedler, Massimo Fornasier, Timo Klock, and Michael\n  Rauchensteiner", "title": "Stable Recovery of Entangled Weights: Towards Robust Identification of\n  Deep Neural Networks from Minimal Samples", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we approach the problem of unique and stable identifiability of\ngeneric deep artificial neural networks with pyramidal shape and smooth\nactivation functions from a finite number of input-output samples. More\nspecifically we introduce the so-called entangled weights, which compose\nweights of successive layers intertwined with suitable diagonal and invertible\nmatrices depending on the activation functions and their shifts. We prove that\nentangled weights are completely and stably approximated by an efficient and\nrobust algorithm as soon as $\\mathcal O(D^2 \\times m)$ nonadaptive input-output\nsamples of the network are collected, where $D$ is the input dimension and $m$\nis the number of neurons of the network. Moreover, we empirically observe that\nthe approach applies to networks with up to $\\mathcal O(D \\times m_L)$ neurons,\nwhere $m_L$ is the number of output neurons at layer $L$. Provided knowledge of\nlayer assignments of entangled weights and of remaining scaling and shift\nparameters, which may be further heuristically obtained by least squares, the\nentangled weights identify the network completely and uniquely. To highlight\nthe relevance of the theoretical result of stable recovery of entangled\nweights, we present numerical experiments, which demonstrate that multilayered\nnetworks with generic weights can be robustly identified and therefore\nuniformly approximated by the presented algorithmic pipeline. In contrast\nbackpropagation cannot generalize stably very well in this setting, being\nalways limited by relatively large uniform error. In terms of practical impact,\nour study shows that we can relate input-output information uniquely and stably\nto network parameters, providing a form of explainability. Moreover, our method\npaves the way for compression of overparametrized networks and for the training\nof minimal complexity networks.\n", "versions": [{"version": "v1", "created": "Mon, 18 Jan 2021 16:31:19 GMT"}], "update_date": "2021-01-19", "authors_parsed": [["Fiedler", "Christian", ""], ["Fornasier", "Massimo", ""], ["Klock", "Timo", ""], ["Rauchensteiner", "Michael", ""]]}, {"id": "2101.07152", "submitter": "Ilie Sarpe", "authors": "Ilie Sarpe, Fabio Vandin", "title": "PRESTO: Simple and Scalable Sampling Techniques for the Rigorous\n  Approximation of Temporal Motif Counts", "comments": "19 pages, 5 figures, to appear in SDM 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.DS cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The identification and counting of small graph patterns, called network\nmotifs, is a fundamental primitive in the analysis of networks, with\napplication in various domains, from social networks to neuroscience. Several\ntechniques have been designed to count the occurrences of motifs in static\nnetworks, with recent work focusing on the computational challenges provided by\nlarge networks. Modern networked datasets contain rich information, such as the\ntime at which the events modeled by the networks edges happened, which can\nprovide useful insights into the process modeled by the network. The analysis\nof motifs in temporal networks, called temporal motifs, is becoming an\nimportant component in the analysis of modern networked datasets. Several\nmethods have been recently designed to count the number of instances of\ntemporal motifs in temporal networks, which is even more challenging than its\ncounterpart for static networks. Such methods are either exact, and not\napplicable to large networks, or approximate, but provide only weak guarantees\non the estimates they produce and do not scale to very large networks. In this\nwork we present an efficient and scalable algorithm to obtain rigorous\napproximations of the count of temporal motifs. Our algorithm is based on a\nsimple but effective sampling approach, which renders our algorithm practical\nfor very large datasets. Our extensive experimental evaluation shows that our\nalgorithm provides estimates of temporal motif counts which are more accurate\nthan the state-of-the-art sampling algorithms, with significantly lower running\ntime than exact approaches, enabling the study of temporal motifs, of size\nlarger than the ones considered in previous works, on billion edges networks.\n", "versions": [{"version": "v1", "created": "Mon, 18 Jan 2021 16:35:12 GMT"}], "update_date": "2021-01-19", "authors_parsed": [["Sarpe", "Ilie", ""], ["Vandin", "Fabio", ""]]}, {"id": "2101.07157", "submitter": "Grigorios Loukides", "authors": "Leqian Zheng and Hau Chan and Grigorios Loukides and Minming Li", "title": "Maximizing approximately k-submodular functions", "comments": "To be published in SIAM International Conference on Data Mining (SDM)\n  2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We introduce the problem of maximizing approximately $k$-submodular functions\nsubject to size constraints. In this problem, one seeks to select $k$-disjoint\nsubsets of a ground set with bounded total size or individual sizes, and\nmaximum utility, given by a function that is \"close\" to being $k$-submodular.\nThe problem finds applications in tasks such as sensor placement, where one\nwishes to install $k$ types of sensors whose measurements are noisy, and\ninfluence maximization, where one seeks to advertise $k$ topics to users of a\nsocial network whose level of influence is uncertain. To deal with the problem,\nwe first provide two natural definitions for approximately $k$-submodular\nfunctions and establish a hierarchical relationship between them. Next, we show\nthat simple greedy algorithms offer approximation guarantees for different\ntypes of size constraints. Last, we demonstrate experimentally that the greedy\nalgorithms are effective in sensor placement and influence maximization\nproblems.\n", "versions": [{"version": "v1", "created": "Mon, 18 Jan 2021 16:48:40 GMT"}], "update_date": "2021-01-19", "authors_parsed": [["Zheng", "Leqian", ""], ["Chan", "Hau", ""], ["Loukides", "Grigorios", ""], ["Li", "Minming", ""]]}, {"id": "2101.07175", "submitter": "Wouter Caarls", "authors": "Wouter Caarls", "title": "Deep Reinforcement Learning with Embedded LQR Controllers", "comments": "This work has been accepted to IFAC for publication under a Creative\n  Commons Licence CC-BY-NC-ND", "journal-ref": null, "doi": "10.1016/j.ifacol.2020.12.2261", "report-no": null, "categories": "cs.RO cs.LG cs.SY eess.SY", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Reinforcement learning is a model-free optimal control method that optimizes\na control policy through direct interaction with the environment. For reaching\ntasks that end in regulation, popular discrete-action methods are not well\nsuited due to chattering in the goal state. We compare three different ways to\nsolve this problem through combining reinforcement learning with classical LQR\ncontrol. In particular, we introduce a method that integrates LQR control into\nthe action set, allowing generalization and avoiding fixing the computed\ncontrol in the replay memory if it is based on learned dynamics. We also embed\nLQR control into a continuous-action method. In all cases, we show that adding\nLQR control can improve performance, although the effect is more profound if it\ncan be used to augment a discrete action set.\n", "versions": [{"version": "v1", "created": "Mon, 18 Jan 2021 17:28:48 GMT"}], "update_date": "2021-06-23", "authors_parsed": [["Caarls", "Wouter", ""]]}, {"id": "2101.07190", "submitter": "Sadegh Bolouki", "authors": "Mohammad-Mehdi Keramati, Elnaz Azizi, Hamidreza Momeni, Sadegh Bolouki", "title": "Incorporating Coincidental Water Data into Non-intrusive Load Monitoring", "comments": "13 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SY cs.LG cs.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Non-intrusive load monitoring (NILM) as the process of extracting the usage\npattern of appliances from the aggregated power signal is among successful\napproaches aiding residential energy management. In recent years, high volume\ndatasets on power profiles have become available, which has helped make\nclassification methods employed for the NILM purpose more effective and more\naccurate. However, the presence of multi-mode appliances and appliances with\nclose power values have remained influential in worsening the computational\ncomplexity and diminishing the accuracy of these algorithms. To tackle these\nchallenges, we propose an event-based classification process, in the first\nphase of which the $K$-nearest neighbors method, as a fast classification\ntechnique, is employed to extract power signals of appliances with exclusive\nnon-overlapping power values. Then, two deep learning models, which consider\nthe water consumption of some appliances as a novel signature in the network,\nare utilized to distinguish between appliances with overlapping power values.\nIn addition to power disaggregation, the proposed process as well extracts the\nwater consumption profiles of specific appliances. To illustrate the proposed\nprocess and validate its efficiency, seven appliances of the AMPds are\nconsidered, with the numerical classification results showing marked\nimprovement with respect to the existing classification-based NILM techniques.\n", "versions": [{"version": "v1", "created": "Mon, 18 Jan 2021 17:49:39 GMT"}], "update_date": "2021-01-19", "authors_parsed": [["Keramati", "Mohammad-Mehdi", ""], ["Azizi", "Elnaz", ""], ["Momeni", "Hamidreza", ""], ["Bolouki", "Sadegh", ""]]}, {"id": "2101.07191", "submitter": "Sadegh Bolouki", "authors": "Elnaz Azizi, Mohammad T H Beheshti, Sadegh Bolouki", "title": "Quantification of Disaggregation Difficulty with Respect to the Number\n  of Meters", "comments": "13 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SY cs.LG cs.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A promising approach toward efficient energy management is non-intrusive load\nmonitoring (NILM), that is to extract the consumption profiles of appliances\nwithin a residence by analyzing the aggregated consumption signal. Among\nefficient NILM methods are event-based algorithms in which events of the\naggregated signal are detected and classified in accordance with the appliances\ncausing them. The large number of appliances and the presence of appliances\nwith close consumption values are known to limit the performance of event-based\nNILM methods. To tackle these challenges, one could enhance the feature space\nwhich in turn results in extra hardware costs, installation complexity, and\nconcerns regarding the consumer's comfort and privacy. This has led to the\nemergence of an alternative approach, namely semi-intrusive load monitoring\n(SILM), where appliances are partitioned into blocks and the consumption of\neach block is monitored via separate power meters.\n  While a greater number of meters can result in more accurate disaggregation,\nit increases the monetary cost of load monitoring, indicating a trade-off that\nrepresents an important gap in this field. In this paper, we take a\ncomprehensive approach to close this gap by establishing a so-called notion of\n\"disaggregation difficulty metric (DDM),\" which quantifies how difficult it is\nto monitor the events of any given group of appliances based on both their\npower values and the consumer's usage behavior. Thus, DDM in essence quantifies\nhow much is expected to be gained in terms of disaggregation accuracy of a\ngeneric event-based algorithm by installing meters on the blocks of any\npartition of the appliances. Experimental results based on the REDD dataset\nillustrate the practicality of the proposed approach in addressing the\naforementioned trade-off.\n", "versions": [{"version": "v1", "created": "Mon, 18 Jan 2021 17:50:48 GMT"}], "update_date": "2021-01-19", "authors_parsed": [["Azizi", "Elnaz", ""], ["Beheshti", "Mohammad T H", ""], ["Bolouki", "Sadegh", ""]]}, {"id": "2101.07202", "submitter": "Christoph Weinhuber", "authors": "Pranav Ashok, Mathias Jackermeier, Jan K\\v{r}et\\'insk\\'y, Christoph\n  Weinhuber, Maximilian Weininger, Mayank Yadav", "title": "dtControl 2.0: Explainable Strategy Representation via Decision Tree\n  Learning Steered by Experts", "comments": null, "journal-ref": "TACAS (2) (pp. 326-345). Springer. 2021", "doi": "10.1007/978-3-030-72013-1_17", "report-no": null, "categories": "cs.AI cs.FL cs.LG cs.LO cs.SY eess.SY", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Recent advances have shown how decision trees are apt data structures for\nconcisely representing strategies (or controllers) satisfying various\nobjectives. Moreover, they also make the strategy more explainable. The recent\ntool dtControl had provided pipelines with tools supporting strategy synthesis\nfor hybrid systems, such as SCOTS and Uppaal Stratego. We present dtControl\n2.0, a new version with several fundamentally novel features. Most importantly,\nthe user can now provide domain knowledge to be exploited in the decision tree\nlearning process and can also interactively steer the process based on the\ndynamically provided information. To this end, we also provide a graphical user\ninterface. It allows for inspection and re-computation of parts of the result,\nsuggesting as well as receiving advice on predicates, and visual simulation of\nthe decision-making process. Besides, we interface model checkers of\nprobabilistic systems, namely Storm and PRISM and provide dedicated support for\ncategorical enumeration-type state variables. Consequently, the controllers are\nmore explainable and smaller.\n", "versions": [{"version": "v1", "created": "Fri, 15 Jan 2021 11:22:49 GMT"}, {"version": "v2", "created": "Tue, 4 May 2021 10:10:43 GMT"}], "update_date": "2021-05-05", "authors_parsed": [["Ashok", "Pranav", ""], ["Jackermeier", "Mathias", ""], ["K\u0159et\u00ednsk\u00fd", "Jan", ""], ["Weinhuber", "Christoph", ""], ["Weininger", "Maximilian", ""], ["Yadav", "Mayank", ""]]}, {"id": "2101.07206", "submitter": "Daniel Shea", "authors": "Craig R. Gin, Daniel E. Shea, Steven L. Brunton, J. Nathan Kutz", "title": "DeepGreen: Deep Learning of Green's Functions for Nonlinear Boundary\n  Value Problems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.NA cs.LG cs.NA physics.comp-ph", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Boundary value problems (BVPs) play a central role in the mathematical\nanalysis of constrained physical systems subjected to external forces.\nConsequently, BVPs frequently emerge in nearly every engineering discipline and\nspan problem domains including fluid mechanics, electromagnetics, quantum\nmechanics, and elasticity. The fundamental solution, or Green's function, is a\nleading method for solving linear BVPs that enables facile computation of new\nsolutions to systems under any external forcing. However, fundamental Green's\nfunction solutions for nonlinear BVPs are not feasible since linear\nsuperposition no longer holds. In this work, we propose a flexible deep\nlearning approach to solve nonlinear BVPs using a dual-autoencoder\narchitecture. The autoencoders discover an invertible coordinate transform that\nlinearizes the nonlinear BVP and identifies both a linear operator $L$ and\nGreen's function $G$ which can be used to solve new nonlinear BVPs. We find\nthat the method succeeds on a variety of nonlinear systems including nonlinear\nHelmholtz and Sturm--Liouville problems, nonlinear elasticity, and a 2D\nnonlinear Poisson equation. The method merges the strengths of the universal\napproximation capabilities of deep learning with the physics knowledge of\nGreen's functions to yield a flexible tool for identifying fundamental\nsolutions to a variety of nonlinear systems.\n", "versions": [{"version": "v1", "created": "Thu, 31 Dec 2020 21:31:56 GMT"}], "update_date": "2021-01-19", "authors_parsed": [["Gin", "Craig R.", ""], ["Shea", "Daniel E.", ""], ["Brunton", "Steven L.", ""], ["Kutz", "J. Nathan", ""]]}, {"id": "2101.07209", "submitter": "Luis Souza Jr.", "authors": "Luis A. de Souza Jr., Luis C. S. Afonso, Alanna Ebigbo, Andreas\n  Probst, Helmut Messmann, Robert Mendel, Christoph Palm and Jo\\~ao P. Papa", "title": "Learning Visual Representations with Optimum-Path Forest and its\n  Applications to Barrett's Esophagus and Adenocarcinoma Diagnosis", "comments": null, "journal-ref": null, "doi": "10.1007/s00521-018-03982-0", "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we introduce the unsupervised Optimum-Path Forest (OPF)\nclassifier for learning visual dictionaries in the context of Barrett's\nesophagus (BE) and automatic adenocarcinoma diagnosis. The proposed approach\nwas validated in two datasets (MICCAI 2015 and Augsburg) using three different\nfeature extractors (SIFT, SURF, and the not yet applied to the BE context\nA-KAZE), as well as five supervised classifiers, including two variants of the\nOPF, Support Vector Machines with Radial Basis Function and Linear kernels, and\na Bayesian classifier. Concerning MICCAI 2015 dataset, the best results were\nobtained using unsupervised OPF for dictionary generation using supervised OPF\nfor classification purposes and using SURF feature extractor with accuracy\nnearly to 78% for distinguishing BE patients from adenocarcinoma ones.\nRegarding the Augsburg dataset, the most accurate results were also obtained\nusing both OPF classifiers but with A-KAZE as the feature extractor with\naccuracy close to 73%. The combination of feature extraction and\nbag-of-visual-words techniques showed results that outperformed others obtained\nrecently in the literature, as well as we highlight new advances in the related\nresearch area. Reinforcing the significance of this work, to the best of our\nknowledge, this is the first one that aimed at addressing computer-aided BE\nidentification using bag-of-visual-words and OPF classifiers, being this\napplication of unsupervised technique in the BE feature calculation the major\ncontribution of this work. It is also proposed a new BE and adenocarcinoma\ndescription using the A-KAZE features, not yet applied in the literature.\n", "versions": [{"version": "v1", "created": "Mon, 18 Jan 2021 18:15:26 GMT"}, {"version": "v2", "created": "Tue, 19 Jan 2021 20:41:20 GMT"}], "update_date": "2021-01-21", "authors_parsed": [["Souza", "Luis A. de", "Jr."], ["Afonso", "Luis C. S.", ""], ["Ebigbo", "Alanna", ""], ["Probst", "Andreas", ""], ["Messmann", "Helmut", ""], ["Mendel", "Robert", ""], ["Palm", "Christoph", ""], ["Papa", "Jo\u00e3o P.", ""]]}, {"id": "2101.07215", "submitter": "Samarth Bhatia", "authors": "Yukti Makhija (1), Samarth Bhatia (1), Shalendra Singh (2), Sneha\n  Kumar Jayaswal (1), Prabhat Singh Malik (3), Pallavi Gupta (4), Shreyas N.\n  Samaga (1), Shreya Johri (1), Sri Krishna Venigalla (2), Rabi Narayan Hota\n  (2), Surinder Singh Bhatia (5), Ishaan Gupta (1) ((1) Indian Institute of\n  Technology Delhi, (2) Armed forces Medical College Pune, (3) All India\n  Institute of Medical Sciences Delhi, (4) Indian institute of Science\n  Education and Research Bhopal, (5) DGAFMS office Ministry of Defence Delhi)", "title": "Challenges in the application of a mortality prediction model for\n  COVID-19 patients on an Indian cohort", "comments": "8 pages, 1 figure, 1 table Study designed by: IG, SB, YM, SJ. Data\n  collected and curated by: SKJ, PG, SNS, RNH, SSB, PSM, SKV and SS. Data\n  analysis performed by: SB, YM. Manuscript was written by: IG, SS, SB, YM .\n  All authors read and approved the final manuscript. The first two authors\n  have contributed equally", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Many countries are now experiencing the third wave of the COVID-19 pandemic\nstraining the healthcare resources with an acute shortage of hospital beds and\nventilators for the critically ill patients. This situation is especially worse\nin India with the second largest load of COVID-19 cases and a relatively\nresource-scarce medical infrastructure. Therefore, it becomes essential to\ntriage the patients based on the severity of their disease and devote resources\ntowards critically ill patients. Yan et al. 1 have published a very pertinent\nresearch that uses Machine learning (ML) methods to predict the outcome of\nCOVID-19 patients based on their clinical parameters at the day of admission.\nThey used the XGBoost algorithm, a type of ensemble model, to build the\nmortality prediction model. The final classifier is built through the\nsequential addition of multiple weak classifiers. The clinically operable\ndecision rule was obtained from a 'single-tree XGBoost' and used lactic\ndehydrogenase (LDH), lymphocyte and high-sensitivity C-reactive protein\n(hs-CRP) values. This decision tree achieved a 100% survival prediction and 81%\nmortality prediction. However, these models have several technical challenges\nand do not provide an out of the box solution that can be deployed for other\npopulations as has been reported in the \"Matters Arising\" section of Yan et al.\nHere, we show the limitations of this model by deploying it on one of the\nlargest datasets of COVID-19 patients containing detailed clinical parameters\ncollected from India.\n", "versions": [{"version": "v1", "created": "Fri, 15 Jan 2021 07:06:49 GMT"}], "update_date": "2021-01-19", "authors_parsed": [["Makhija", "Yukti", ""], ["Bhatia", "Samarth", ""], ["Singh", "Shalendra", ""], ["Jayaswal", "Sneha Kumar", ""], ["Malik", "Prabhat Singh", ""], ["Gupta", "Pallavi", ""], ["Samaga", "Shreyas N.", ""], ["Johri", "Shreya", ""], ["Venigalla", "Sri Krishna", ""], ["Hota", "Rabi Narayan", ""], ["Bhatia", "Surinder Singh", ""], ["Gupta", "Ishaan", ""]]}, {"id": "2101.07219", "submitter": "Francesco Sorrentino Dr.", "authors": "Chad Nathe, Enrico Del Frate, Thomas Carroll, Louis Pecora, Afroza\n  Shirin, Francesco Sorrentino", "title": "Reservoir Computers Modal Decomposition and Optimization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cond-mat.dis-nn cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The topology of a network associated with a reservoir computer is often taken\nso that the connectivity and the weights are chosen randomly. Optimization is\nhardly considered as the parameter space is typically too large. Here we\ninvestigate this problem for a class of reservoir computers for which we obtain\na decomposition of the reservoir dynamics into modes, which can be computed\nindependently of one another. Each mode depends on an eigenvalue of the network\nadjacency matrix. We then take a parametric approach in which the eigenvalues\nare parameters that can be appropriately designed and optimized. In addition,\nwe introduce the application of a time shift to each individual mode. We show\nthat manipulations of the individual modes, either in terms of the eigenvalues\nor the time shifts, can lead to dramatic reductions in the training error.\n", "versions": [{"version": "v1", "created": "Wed, 13 Jan 2021 23:30:21 GMT"}], "update_date": "2021-01-19", "authors_parsed": [["Nathe", "Chad", ""], ["Del Frate", "Enrico", ""], ["Carroll", "Thomas", ""], ["Pecora", "Louis", ""], ["Shirin", "Afroza", ""], ["Sorrentino", "Francesco", ""]]}, {"id": "2101.07223", "submitter": "Simon Pietro Romano", "authors": "Diego Antonelli, Roberta Cascella, Gaetano Perrone, Simon Pietro\n  Romano, Antonio Schiano", "title": "Leveraging AI to optimize website structure discovery during Penetration\n  Testing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Dirbusting is a technique used to brute force directories and file names on\nweb servers while monitoring HTTP responses, in order to enumerate server\ncontents. Such a technique uses lists of common words to discover the hidden\nstructure of the target website. Dirbusting typically relies on response codes\nas discovery conditions to find new pages. It is widely used in web application\npenetration testing, an activity that allows companies to detect websites\nvulnerabilities. Dirbusting techniques are both time and resource consuming and\ninnovative approaches have never been explored in this field. We hence propose\nan advanced technique to optimize the dirbusting process by leveraging\nArtificial Intelligence. More specifically, we use semantic clustering\ntechniques in order to organize wordlist items in different groups according to\ntheir semantic meaning. The created clusters are used in an ad-hoc implemented\nnext-word intelligent strategy. This paper demonstrates that the usage of\nclustering techniques outperforms the commonly used brute force methods.\nPerformance is evaluated by testing eight different web applications. Results\nshow a performance increase that is up to 50% for each of the conducted\nexperiments.\n", "versions": [{"version": "v1", "created": "Mon, 18 Jan 2021 18:21:42 GMT"}], "update_date": "2021-01-19", "authors_parsed": [["Antonelli", "Diego", ""], ["Cascella", "Roberta", ""], ["Perrone", "Gaetano", ""], ["Romano", "Simon Pietro", ""], ["Schiano", "Antonio", ""]]}, {"id": "2101.07226", "submitter": "Zeliang Liu", "authors": "Zeliang Liu", "title": "Cell division in deep material networks applied to multiscale strain\n  localization modeling", "comments": "38 pages, 21 figures", "journal-ref": null, "doi": "10.1016/j.cma.2021.113914", "report-no": null, "categories": "cs.CE cs.LG physics.comp-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite the increasing importance of strain localization modeling (e.g.,\nfailure analysis) in computer-aided engineering, there is a lack of effective\napproaches to capturing relevant material behaviors consistently across\nmultiple length scales. We aim to address this gap within the framework of deep\nmaterial networks (DMN) -- a machine learning model with embedded mechanics in\nthe building blocks. A new cell-division scheme is proposed to track the scale\ntransition through the network, and its consistency is ensured by the physics\nof fitting parameters. Essentially, each microscale node in the bottom layer is\ndescribed by an ellipsoidal cell with its dimensions back-propagated from the\nmacroscale material point. New crack surfaces in the cell are modeled by\nenriching cohesive layers, and failure algorithms are developed for crack\ninitiation and evolution in the implicit DMN analysis. Besides studies on a\nsingle material point, we apply the multiscale model to concurrent multiscale\nsimulations for the dynamic crush of a particle-reinforced composite tube and\nvarious tests on carbon fiber reinforced polymer composites. For the latter,\nexperimental validations on an off-axis tensile test specimen are also\nprovided.\n", "versions": [{"version": "v1", "created": "Mon, 18 Jan 2021 18:24:51 GMT"}, {"version": "v2", "created": "Wed, 26 May 2021 18:29:11 GMT"}], "update_date": "2021-06-16", "authors_parsed": [["Liu", "Zeliang", ""]]}, {"id": "2101.07235", "submitter": "Jean-Francois Rajotte", "authors": "Jean-Francois Rajotte, Sumit Mukherjee, Caleb Robinson, Anthony Ortiz,\n  Christopher West, Juan Lavista Ferres, Raymond T Ng", "title": "Reducing bias and increasing utility by federated generative modeling of\n  medical images using a centralized adversary", "comments": "10 pages, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.CV cs.DC cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We introduce FELICIA (FEderated LearnIng with a CentralIzed Adversary) a\ngenerative mechanism enabling collaborative learning. In particular, we show\nhow a data owner with limited and biased data could benefit from other data\nowners while keeping data from all the sources private. This is a common\nscenario in medical image analysis where privacy legislation prevents data from\nbeing shared outside local premises. FELICIA works for a large family of\nGenerative Adversarial Networks (GAN) architectures including vanilla and\nconditional GANs as demonstrated in this work. We show that by using the\nFELICIA mechanism, a data owner with limited image samples can generate\nhigh-quality synthetic images with high utility while neither data owners has\nto provide access to its data. The sharing happens solely through a central\ndiscriminator that has access limited to synthetic data. Here, utility is\ndefined as classification performance on a real test set. We demonstrate these\nbenefits on several realistic healthcare scenarions using benchmark image\ndatasets (MNIST, CIFAR-10) as well as on medical images for the task of skin\nlesion classification. With multiple experiments, we show that even in the\nworst cases, combining FELICIA with real data gracefully achieves performance\non par with real data while most results significantly improves the utility.\n", "versions": [{"version": "v1", "created": "Mon, 18 Jan 2021 18:40:46 GMT"}], "update_date": "2021-01-19", "authors_parsed": [["Rajotte", "Jean-Francois", ""], ["Mukherjee", "Sumit", ""], ["Robinson", "Caleb", ""], ["Ortiz", "Anthony", ""], ["West", "Christopher", ""], ["Ferres", "Juan Lavista", ""], ["Ng", "Raymond T", ""]]}, {"id": "2101.07240", "submitter": "Svetlana Kutuzova", "authors": "Svetlana Kutuzova, Oswin Krause, Douglas McCloskey, Mads Nielsen,\n  Christian Igel", "title": "Multimodal Variational Autoencoders for Semi-Supervised Learning: In\n  Defense of Product-of-Experts", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Multimodal generative models should be able to learn a meaningful latent\nrepresentation that enables a coherent joint generation of all modalities\n(e.g., images and text). Many applications also require the ability to\naccurately sample modalities conditioned on observations of a subset of the\nmodalities. Often not all modalities may be observed for all training data\npoints, so semi-supervised learning should be possible. In this study, we\nevaluate a family of product-of-experts (PoE) based variational autoencoders\nthat have these desired properties. We include a novel PoE based architecture\nand training procedure. An empirical evaluation shows that the PoE based models\ncan outperform an additive mixture-of-experts (MoE) approach. Our experiments\nsupport the intuition that PoE models are more suited for a conjunctive\ncombination of modalities while MoEs are more suited for a disjunctive fusion.\n", "versions": [{"version": "v1", "created": "Mon, 18 Jan 2021 18:47:43 GMT"}], "update_date": "2021-01-19", "authors_parsed": [["Kutuzova", "Svetlana", ""], ["Krause", "Oswin", ""], ["McCloskey", "Douglas", ""], ["Nielsen", "Mads", ""], ["Igel", "Christian", ""]]}, {"id": "2101.07241", "submitter": "Haoyu Xiong", "authors": "Haoyu Xiong, Quanzhou Li, Yun-Chun Chen, Homanga Bharadhwaj, Samarth\n  Sinha, Animesh Garg", "title": "Learning by Watching: Physical Imitation of Manipulation Skills from\n  Human Videos", "comments": "Project Website: https://www.pair.toronto.edu/lbw-kp/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV cs.LG", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  We present an approach for physical imitation from human videos for robot\nmanipulation tasks. The key idea of our method lies in explicitly exploiting\nthe kinematics and motion information embedded in the video to learn structured\nrepresentations that endow the robot with the ability to imagine how to perform\nmanipulation tasks in its own context. To achieve this, we design a perception\nmodule that learns to translate human videos to the robot domain followed by\nunsupervised keypoint detection. The resulting keypoint-based representations\nprovide semantically meaningful information that can be directly used for\nreward computing and policy learning. We evaluate the effectiveness of our\napproach on five robot manipulation tasks, including reaching, pushing,\nsliding, coffee making, and drawer closing. Detailed experimental evaluations\ndemonstrate that our method performs favorably against previous approaches.\n", "versions": [{"version": "v1", "created": "Mon, 18 Jan 2021 18:50:32 GMT"}], "update_date": "2021-01-19", "authors_parsed": [["Xiong", "Haoyu", ""], ["Li", "Quanzhou", ""], ["Chen", "Yun-Chun", ""], ["Bharadhwaj", "Homanga", ""], ["Sinha", "Samarth", ""], ["Garg", "Animesh", ""]]}, {"id": "2101.07243", "submitter": "Zhuo Chen", "authors": "Di Luo, Zhuo Chen, Kaiwen Hu, Zhizhen Zhao, Vera Mikyoung Hur, and\n  Bryan K. Clark", "title": "Gauge Invariant Autoregressive Neural Networks for Quantum Lattice\n  Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cond-mat.str-el cond-mat.dis-nn cs.LG hep-lat quant-ph", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Gauge invariance plays a crucial role in quantum mechanics from condensed\nmatter physics to high energy physics. We develop an approach to constructing\ngauge invariant autoregressive neural networks for quantum lattice models.\nThese networks can be efficiently sampled and explicitly obey gauge symmetries.\nWe variationally optimize our gauge invariant autoregressive neural networks\nfor ground states as well as real-time dynamics for a variety of models. We\nexactly represent the ground and excited states of the 2D and 3D toric codes,\nand the X-cube fracton model. We simulate the dynamics of the quantum link\nmodel of $\\text{U(1)}$ lattice gauge theory, obtain the phase diagram for the\n2D $\\mathbb{Z}_2$ gauge theory, determine the phase transition and the central\ncharge of the $\\text{SU(2)}_3$ anyonic chain, and also compute the ground state\nenergy of the $\\text{SU(2)}$ invariant Heisenberg spin chain. Our approach\nprovides powerful tools for exploring condensed matter physics, high energy\nphysics and quantum information science.\n", "versions": [{"version": "v1", "created": "Mon, 18 Jan 2021 18:55:21 GMT"}], "update_date": "2021-01-19", "authors_parsed": [["Luo", "Di", ""], ["Chen", "Zhuo", ""], ["Hu", "Kaiwen", ""], ["Zhao", "Zhizhen", ""], ["Hur", "Vera Mikyoung", ""], ["Clark", "Bryan K.", ""]]}, {"id": "2101.07251", "submitter": "Furkan Gursoy", "authors": "Furkan G\\\"ursoy, Mounir Haddad, C\\'ecile Bothorel", "title": "Alignment and stability of embeddings: measurement and inference\n  improvement", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Representation learning (RL) methods learn objects' latent embeddings where\ninformation is preserved by distances. Since distances are invariant to certain\nlinear transformations, one may obtain different embeddings while preserving\nthe same information. In dynamic systems, a temporal difference in embeddings\nmay be explained by the stability of the system or by the misalignment of\nembeddings due to arbitrary transformations. In the literature, embedding\nalignment has not been defined formally, explored theoretically, or analyzed\nempirically. Here, we explore the embedding alignment and its parts, provide\nthe first formal definitions, propose novel metrics to measure alignment and\nstability, and show their suitability through synthetic experiments. Real-world\nexperiments show that both static and dynamic RL methods are prone to produce\nmisaligned embeddings and such misalignment worsens the performance of dynamic\nnetwork inference tasks. By ensuring alignment, the prediction accuracy raises\nby up to 90% in static and by 40% in dynamic RL methods.\n", "versions": [{"version": "v1", "created": "Mon, 18 Jan 2021 18:58:59 GMT"}], "update_date": "2021-01-19", "authors_parsed": [["G\u00fcrsoy", "Furkan", ""], ["Haddad", "Mounir", ""], ["Bothorel", "C\u00e9cile", ""]]}, {"id": "2101.07256", "submitter": "David W. Hogg", "authors": "David W. Hogg (NYU) and Soledad Villar (JHU)", "title": "Fitting very flexible models: Linear regression with large numbers of\n  parameters", "comments": "all code used to make the figures is available at\n  https://github.com/davidwhogg/FlexibleLinearModels", "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.data-an astro-ph.IM cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There are many uses for linear fitting; the context here is interpolation and\ndenoising of data, as when you have calibration data and you want to fit a\nsmooth, flexible function to those data. Or you want to fit a flexible function\nto de-trend a time series or normalize a spectrum. In these contexts,\ninvestigators often choose a polynomial basis, or a Fourier basis, or wavelets,\nor something equally general. They also choose an order, or number of basis\nfunctions to fit, and (often) some kind of regularization. We discuss how this\nbasis-function fitting is done, with ordinary least squares and extensions\nthereof. We emphasize that it is often valuable to choose far more parameters\nthan data points, despite folk rules to the contrary: Suitably regularized\nmodels with enormous numbers of parameters generalize well and make good\npredictions for held-out data; over-fitting is not (mainly) a problem of having\ntoo many parameters. It is even possible to take the limit of infinite\nparameters, at which, if the basis and regularization are chosen correctly, the\nleast-squares fit becomes the mean of a Gaussian process. We recommend\ncross-validation as a good empirical method for model selection (for example,\nsetting the number of parameters and the form of the regularization), and\njackknife resampling as a good empirical method for estimating the\nuncertainties of the predictions made by the model. We also give advice for\nbuilding stable computational implementations.\n", "versions": [{"version": "v1", "created": "Fri, 15 Jan 2021 21:08:34 GMT"}], "update_date": "2021-01-20", "authors_parsed": [["Hogg", "David W.", "", "NYU"], ["Villar", "Soledad", "", "JHU"]]}, {"id": "2101.07259", "submitter": "Anuraganand Sharma Dr", "authors": "Anuraganand Sharma", "title": "Guided parallelized stochastic gradient descent for delay compensation", "comments": "This is a preprint version", "journal-ref": "Applied Soft Computing (2021)", "doi": "10.1016/j.asoc.2021.107084", "report-no": null, "categories": "cs.LG cs.NE", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Stochastic gradient descent (SGD) algorithm and its variations have been\neffectively used to optimize neural network models. However, with the rapid\ngrowth of big data and deep learning, SGD is no longer the most suitable choice\ndue to its natural behavior of sequential optimization of the error function.\nThis has led to the development of parallel SGD algorithms, such as\nasynchronous SGD (ASGD) and synchronous SGD (SSGD) to train deep neural\nnetworks. However, it introduces a high variance due to the delay in parameter\n(weight) update. We address this delay in our proposed algorithm and try to\nminimize its impact. We employed guided SGD (gSGD) that encourages consistent\nexamples to steer the convergence by compensating the unpredictable deviation\ncaused by the delay. Its convergence rate is also similar to A/SSGD, however,\nsome additional (parallel) processing is required to compensate for the delay.\nThe experimental results demonstrate that our proposed approach has been able\nto mitigate the impact of delay for the quality of classification accuracy. The\nguided approach with SSGD clearly outperforms sequential SGD and even achieves\nthe accuracy close to sequential SGD for some benchmark datasets.\n", "versions": [{"version": "v1", "created": "Sun, 17 Jan 2021 23:12:40 GMT"}], "update_date": "2021-01-20", "authors_parsed": [["Sharma", "Anuraganand", ""]]}, {"id": "2101.07295", "submitter": "Anh Thai", "authors": "Anh Thai, Stefan Stojanov, Isaac Rehg, James M. Rehg", "title": "Does Continual Learning = Catastrophic Forgetting?", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Continual learning is known for suffering from catastrophic forgetting, a\nphenomenon where earlier learned concepts are forgotten at the expense of more\nrecent samples. In this work, we challenge the assumption that continual\nlearning is inevitably associated with catastrophic forgetting by presenting a\nset of tasks that surprisingly do not suffer from catastrophic forgetting when\nlearned continually. The robustness of these tasks leads to the potential of\nhaving a proxy representation learning task for continual classification. We\nfurther introduce a novel yet simple algorithm, YASS that achieves\nstate-of-the-art performance in the class-incremental categorization learning\ntask and provide an insight into the benefit of learning the representation\ncontinuously. Finally, we present converging evidence on the forgetting\ndynamics of representation learning in continual models. The codebase, dataset,\nand pre-trained models released with this article can be found at\nhttps://github.com/rehg-lab/CLRec.\n", "versions": [{"version": "v1", "created": "Mon, 18 Jan 2021 19:29:12 GMT"}, {"version": "v2", "created": "Fri, 26 Mar 2021 03:42:31 GMT"}], "update_date": "2021-03-29", "authors_parsed": [["Thai", "Anh", ""], ["Stojanov", "Stefan", ""], ["Rehg", "Isaac", ""], ["Rehg", "James M.", ""]]}, {"id": "2101.07296", "submitter": "Stefan Stojanov", "authors": "Stefan Stojanov, Anh Thai, James M. Rehg", "title": "Using Shape to Categorize: Low-Shot Learning with an Explicit Shape Bias", "comments": "Accepted at CVPR2021. Project page, code and data available at\n  https://rehg-lab.github.io/publication-pages/lowshot-shapebias/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is widely accepted that reasoning about object shape is important for\nobject recognition. However, the most powerful object recognition methods today\ndo not explicitly make use of object shape during learning. In this work,\nmotivated by recent developments in low-shot learning, findings in\ndevelopmental psychology, and the increased use of synthetic data in computer\nvision research, we investigate how reasoning about 3D shape can be used to\nimprove low-shot learning methods' generalization performance. We propose a new\nway to improve existing low-shot learning approaches by learning a\ndiscriminative embedding space using 3D object shape, and using this embedding\nby learning how to map images into it. Our new approach improves the\nperformance of image-only low-shot learning approaches on multiple datasets. We\nalso introduce Toys4K, a 3D object dataset with the largest number of object\ncategories currently available, which supports low-shot learning.\n", "versions": [{"version": "v1", "created": "Mon, 18 Jan 2021 19:29:41 GMT"}, {"version": "v2", "created": "Sun, 20 Jun 2021 23:13:19 GMT"}], "update_date": "2021-06-22", "authors_parsed": [["Stojanov", "Stefan", ""], ["Thai", "Anh", ""], ["Rehg", "James M.", ""]]}, {"id": "2101.07312", "submitter": "Tobias Huber", "authors": "Tobias Huber, Benedikt Limmer, Elisabeth Andr\\'e", "title": "Benchmarking Perturbation-based Saliency Maps for Explaining Atari\n  Agents", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent years saw a plethora of work on explaining complex intelligent agents.\nOne example is the development of several algorithms that generate saliency\nmaps which show how much each pixel attributed to the agents' decision.\nHowever, most evaluations of such saliency maps focus on image classification\ntasks. As far as we know, there is no work that thoroughly compares different\nsaliency maps for Deep Reinforcement Learning agents. This paper compares four\nperturbation-based approaches to create saliency maps for Deep Reinforcement\nLearning agents trained on four different Atari 2600 games. All four approaches\nwork by perturbing parts of the input and measuring how much this affects the\nagent's output. The approaches are compared using three computational metrics:\ndependence on the learned parameters of the agent (sanity checks), faithfulness\nto the agent's reasoning (input degradation), and run-time. In particular,\nduring the sanity checks we find issues with two approaches and propose a\nsolution to fix one of those issues.\n", "versions": [{"version": "v1", "created": "Mon, 18 Jan 2021 19:57:52 GMT"}, {"version": "v2", "created": "Sat, 19 Jun 2021 09:02:25 GMT"}], "update_date": "2021-06-22", "authors_parsed": [["Huber", "Tobias", ""], ["Limmer", "Benedikt", ""], ["Andr\u00e9", "Elisabeth", ""]]}, {"id": "2101.07321", "submitter": "Krenare Pireva Nuci", "authors": "Vedat Apuk, Krenare Pireva Nu\\c{c}i", "title": "Classification of Pedagogical content using conventional machine\n  learning and deep learning model", "comments": "11 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.IR cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The advent of the Internet and a large number of digital technologies has\nbrought with it many different challenges. A large amount of data is found on\nthe web, which in most cases is unstructured and unorganized, and this\ncontributes to the fact that the use and manipulation of this data is quite a\ndifficult process. Due to this fact, the usage of different machine and deep\nlearning techniques for Text Classification has gained its importance, which\nimproved this discipline and made it more interesting for scientists and\nresearchers for further study. This paper aims to classify the pedagogical\ncontent using two different models, the K-Nearest Neighbor (KNN) from the\nconventional models and the Long short-term memory (LSTM) recurrent neural\nnetwork from the deep learning models. The result indicates that the accuracy\nof classifying the pedagogical content reaches 92.52 % using KNN model and\n87.71 % using LSTM model.\n", "versions": [{"version": "v1", "created": "Mon, 18 Jan 2021 20:29:34 GMT"}], "update_date": "2021-01-20", "authors_parsed": [["Apuk", "Vedat", ""], ["Nu\u00e7i", "Krenare Pireva", ""]]}, {"id": "2101.07342", "submitter": "Trevor Doherty Mr.", "authors": "Trevor Doherty, Susan McKeever, Nebras Al-Attar, Tiarnan Murphy,\n  Claudia Aura, Arman Rahman, Amanda O'Neill, Stephen P Finn, Elaine Kay,\n  William M. Gallagher, R. William G. Watson, Aoife Gowen and Patrick Jackman", "title": "Feature Fusion of Raman Chemical Imaging and Digital Histopathology\n  using Machine Learning for Prostate Cancer Detection", "comments": "19 pages, 8 tables, 18 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.LG q-bio.QM", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  The diagnosis of prostate cancer is challenging due to the heterogeneity of\nits presentations, leading to the over diagnosis and treatment of\nnon-clinically important disease. Accurate diagnosis can directly benefit a\npatient's quality of life and prognosis. Towards addressing this issue, we\npresent a learning model for the automatic identification of prostate cancer.\nWhile many prostate cancer studies have adopted Raman spectroscopy approaches,\nnone have utilised the combination of Raman Chemical Imaging (RCI) and other\nimaging modalities. This study uses multimodal images formed from stained\nDigital Histopathology (DP) and unstained RCI. The approach was developed and\ntested on a set of 178 clinical samples from 32 patients, containing a range of\nnon-cancerous, Gleason grade 3 (G3) and grade 4 (G4) tissue microarray samples.\nFor each histological sample, there is a pathologist labelled DP - RCI image\npair. The hypothesis tested was whether multimodal image models can outperform\nsingle modality baseline models in terms of diagnostic accuracy. Binary\nnon-cancer/cancer models and the more challenging G3/G4 differentiation were\ninvestigated. Regarding G3/G4 classification, the multimodal approach achieved\na sensitivity of 73.8% and specificity of 88.1% while the baseline DP model\nshowed a sensitivity and specificity of 54.1% and 84.7% respectively. The\nmultimodal approach demonstrated a statistically significant 12.7% AUC\nadvantage over the baseline with a value of 85.8% compared to 73.1%, also\noutperforming models based solely on RCI and median Raman spectra. Feature\nfusion of DP and RCI does not improve the more trivial task of tumour\nidentification but does deliver an observed advantage in G3/G4 discrimination.\nBuilding on these promising findings, future work could include the acquisition\nof larger datasets for enhanced model generalization.\n", "versions": [{"version": "v1", "created": "Mon, 18 Jan 2021 22:11:42 GMT"}], "update_date": "2021-01-20", "authors_parsed": [["Doherty", "Trevor", ""], ["McKeever", "Susan", ""], ["Al-Attar", "Nebras", ""], ["Murphy", "Tiarnan", ""], ["Aura", "Claudia", ""], ["Rahman", "Arman", ""], ["O'Neill", "Amanda", ""], ["Finn", "Stephen P", ""], ["Kay", "Elaine", ""], ["Gallagher", "William M.", ""], ["Watson", "R. William G.", ""], ["Gowen", "Aoife", ""], ["Jackman", "Patrick", ""]]}, {"id": "2101.07344", "submitter": "Arjun Balasubramanian", "authors": "Arjun Balasubramanian, Adarsh Kumar, Yuhan Liu, Han Cao, Shivaram\n  Venkataraman, Aditya Akella", "title": "Accelerating Deep Learning Inference via Learned Caches", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DC cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep Neural Networks (DNNs) are witnessing increased adoption in multiple\ndomains owing to their high accuracy in solving real-world problems. However,\nthis high accuracy has been achieved by building deeper networks, posing a\nfundamental challenge to the low latency inference desired by user-facing\napplications. Current low latency solutions trade-off on accuracy or fail to\nexploit the inherent temporal locality in prediction serving workloads.\n  We observe that caching hidden layer outputs of the DNN can introduce a form\nof late-binding where inference requests only consume the amount of computation\nneeded. This enables a mechanism for achieving low latencies, coupled with an\nability to exploit temporal locality. However, traditional caching approaches\nincur high memory overheads and lookup latencies, leading us to design learned\ncaches - caches that consist of simple ML models that are continuously updated.\nWe present the design of GATI, an end-to-end prediction serving system that\nincorporates learned caches for low-latency DNN inference. Results show that\nGATI can reduce inference latency by up to 7.69X on realistic workloads.\n", "versions": [{"version": "v1", "created": "Mon, 18 Jan 2021 22:13:08 GMT"}], "update_date": "2021-01-20", "authors_parsed": [["Balasubramanian", "Arjun", ""], ["Kumar", "Adarsh", ""], ["Liu", "Yuhan", ""], ["Cao", "Han", ""], ["Venkataraman", "Shivaram", ""], ["Akella", "Aditya", ""]]}, {"id": "2101.07354", "submitter": "Yichi Zhang", "authors": "Yichi Zhang, Minh Tang", "title": "Consistency of random-walk based network embedding algorithms", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Random-walk based network embedding algorithms like node2vec and DeepWalk are\nwidely used to obtain Euclidean representation of the nodes in a network prior\nto performing down-stream network inference tasks. Nevertheless, despite their\nimpressive empirical performance, there is a lack of theoretical results\nexplaining their behavior. In this paper we studied the node2vec and DeepWalk\nalgorithms through the perspective of matrix factorization. We analyze these\nalgorithms in the setting of community detection for stochastic blockmodel\ngraphs; in particular we established large-sample error bounds and prove\nconsistent community recovery of node2vec/DeepWalk embedding followed by\nk-means clustering. Our theoretical results indicate a subtle interplay between\nthe sparsity of the observed networks, the window sizes of the random walks,\nand the convergence rates of the node2vec/DeepWalk embedding toward the\nembedding of the true but unknown edge probabilities matrix. More specifically,\nas the network becomes sparser, our results suggest using larger window sizes,\nor equivalently, taking longer random walks, in order to attain better\nconvergence rate for the resulting embeddings. The paper includes numerical\nexperiments corroborating these observations.\n", "versions": [{"version": "v1", "created": "Mon, 18 Jan 2021 22:49:22 GMT"}], "update_date": "2021-01-20", "authors_parsed": [["Zhang", "Yichi", ""], ["Tang", "Minh", ""]]}, {"id": "2101.07357", "submitter": "David Lim", "authors": "David K. Lim, Naim U. Rashid, Junier B. Oliva, Joseph G. Ibrahim", "title": "Handling Non-ignorably Missing Features in Electronic Health Records\n  Data Using Importance-Weighted Autoencoders", "comments": "37 pages, 3 figures, 3 tables, under review (Journal of the American\n  Statistical Association)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Electronic Health Records (EHRs) are commonly used to investigate\nrelationships between patient health information and outcomes. Deep learning\nmethods are emerging as powerful tools to learn such relationships, given the\ncharacteristic high dimension and large sample size of EHR datasets. The\nPhysionet 2012 Challenge involves an EHR dataset pertaining to 12,000 ICU\npatients, where researchers investigated the relationships between clinical\nmeasurements, and in-hospital mortality. However, the prevalence and complexity\nof missing data in the Physionet data present significant challenges for the\napplication of deep learning methods, such as Variational Autoencoders (VAEs).\nAlthough a rich literature exists regarding the treatment of missing data in\ntraditional statistical models, it is unclear how this extends to deep learning\narchitectures. To address these issues, we propose a novel extension of VAEs\ncalled Importance-Weighted Autoencoders (IWAEs) to flexibly handle Missing Not\nAt Random (MNAR) patterns in the Physionet data. Our proposed method models the\nmissingness mechanism using an embedded neural network, eliminating the need to\nspecify the exact form of the missingness mechanism a priori. We show that the\nuse of our method leads to more realistic imputed values relative to the\nstate-of-the-art, as well as significant differences in fitted downstream\nmodels for mortality.\n", "versions": [{"version": "v1", "created": "Mon, 18 Jan 2021 22:53:29 GMT"}, {"version": "v2", "created": "Fri, 5 Feb 2021 20:05:41 GMT"}], "update_date": "2021-02-09", "authors_parsed": [["Lim", "David K.", ""], ["Rashid", "Naim U.", ""], ["Oliva", "Junier B.", ""], ["Ibrahim", "Joseph G.", ""]]}, {"id": "2101.07361", "submitter": "Maliha Islam", "authors": "Maliha Tashfia Islam, Anna Fariha, Alexandra Meliou", "title": "Through the Data Management Lens: Experimental Analysis and Evaluation\n  of Fair Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CY cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Classification, a heavily-studied data-driven machine learning task, drives\nan increasing number of prediction systems involving critical human decisions\nsuch as loan approval and criminal risk assessment. However, classifiers often\ndemonstrate discriminatory behavior, especially when presented with biased\ndata. Consequently, fairness in classification has emerged as a high-priority\nresearch area. Data management research is showing an increasing presence and\ninterest in topics related to data and algorithmic fairness, including the\ntopic of fair classification. The interdisciplinary efforts in fair\nclassification, with machine learning research having the largest presence,\nhave resulted in a large number of fairness notions and a wide range of\napproaches that have not been systematically evaluated and compared. In this\npaper, we contribute a broad analysis of 13 fair classification approaches and\nadditional variants, over their correctness, fairness, efficiency, scalability,\nand stability, using a variety of metrics and real-world datasets. Our analysis\nhighlights novel insights on the impact of different metrics and high-level\napproach characteristics on different aspects of performance. We also discuss\ngeneral principles for choosing approaches suitable for different practical\nsettings, and identify areas where data-management-centric solutions are likely\nto have the most impact.\n", "versions": [{"version": "v1", "created": "Mon, 18 Jan 2021 22:55:40 GMT"}], "update_date": "2021-01-25", "authors_parsed": [["Islam", "Maliha Tashfia", ""], ["Fariha", "Anna", ""], ["Meliou", "Alexandra", ""]]}, {"id": "2101.07365", "submitter": "Rafael Dowsley", "authors": "Amanda Resende, Davis Railsback, Rafael Dowsley, Anderson C. A.\n  Nascimento, Diego F. Aranha", "title": "Fast Privacy-Preserving Text Classification based on Secure Multiparty\n  Computation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a privacy-preserving Naive Bayes classifier and apply it to the\nproblem of private text classification. In this setting, a party (Alice) holds\na text message, while another party (Bob) holds a classifier. At the end of the\nprotocol, Alice will only learn the result of the classifier applied to her\ntext input and Bob learns nothing. Our solution is based on Secure Multiparty\nComputation (SMC). Our Rust implementation provides a fast and secure solution\nfor the classification of unstructured text. Applying our solution to the case\nof spam detection (the solution is generic, and can be used in any other\nscenario in which the Naive Bayes classifier can be employed), we can classify\nan SMS as spam or ham in less than 340ms in the case where the dictionary size\nof Bob's model includes all words (n = 5200) and Alice's SMS has at most m =\n160 unigrams. In the case with n = 369 and m = 8 (the average of a spam SMS in\nthe database), our solution takes only 21ms.\n", "versions": [{"version": "v1", "created": "Mon, 18 Jan 2021 23:08:12 GMT"}, {"version": "v2", "created": "Tue, 8 Jun 2021 06:26:21 GMT"}], "update_date": "2021-06-09", "authors_parsed": [["Resende", "Amanda", ""], ["Railsback", "Davis", ""], ["Dowsley", "Rafael", ""], ["Nascimento", "Anderson C. A.", ""], ["Aranha", "Diego F.", ""]]}, {"id": "2101.07367", "submitter": "Luke Metz", "authors": "Luke Metz, C. Daniel Freeman, Niru Maheswaranathan, Jascha\n  Sohl-Dickstein", "title": "Training Learned Optimizers with Randomly Initialized Learned Optimizers", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learned optimizers are increasingly effective, with performance exceeding\nthat of hand designed optimizers such as Adam~\\citep{kingma2014adam} on\nspecific tasks \\citep{metz2019understanding}. Despite the potential gains\navailable, in current work the meta-training (or `outer-training') of the\nlearned optimizer is performed by a hand-designed optimizer, or by an optimizer\ntrained by a hand-designed optimizer \\citep{metz2020tasks}. We show that a\npopulation of randomly initialized learned optimizers can be used to train\nthemselves from scratch in an online fashion, without resorting to a hand\ndesigned optimizer in any part of the process. A form of population based\ntraining is used to orchestrate this self-training. Although the randomly\ninitialized optimizers initially make slow progress, as they improve they\nexperience a positive feedback loop, and become rapidly more effective at\ntraining themselves. We believe feedback loops of this type, where an optimizer\nimproves itself, will be important and powerful in the future of machine\nlearning. These methods not only provide a path towards increased performance,\nbut more importantly relieve research and engineering effort.\n", "versions": [{"version": "v1", "created": "Thu, 14 Jan 2021 19:07:17 GMT"}], "update_date": "2021-01-20", "authors_parsed": [["Metz", "Luke", ""], ["Freeman", "C. Daniel", ""], ["Maheswaranathan", "Niru", ""], ["Sohl-Dickstein", "Jascha", ""]]}, {"id": "2101.07376", "submitter": "Khalid Alsamadony", "authors": "Khalid L. Alsamadony, Ertugrul U. Yildirim, Guenther Glatz, Umair bin\n  Waheed, Sherif M. Hanafy", "title": "Deep-Learning Driven Noise Reduction for Reduced Flux Computed\n  Tomography", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural networks have received considerable attention in clinical\nimaging, particularly with respect to the reduction of radiation risk. Lowering\nthe radiation dose by reducing the photon flux inevitably results in the\ndegradation of the scanned image quality. Thus, researchers have sought to\nexploit deep convolutional neural networks (DCNNs) to map low-quality, low-dose\nimages to higher-dose, higher-quality images thereby minimizing the associated\nradiation hazard. Conversely, computed tomography (CT) measurements of\ngeomaterials are not limited by the radiation dose. In contrast to the human\nbody, however, geomaterials may be comprised of high-density constituents\ncausing increased attenuation of the X-Rays. Consequently, higher dosage images\nare required to obtain an acceptable scan quality. The problem of prolonged\nacquisition times is particularly severe for micro-CT based scanning\ntechnologies. Depending on the sample size and exposure time settings, a single\nscan may require several hours to complete. This is of particular concern if\nphenomena with an exponential temperature dependency are to be elucidated. A\nprocess may happen too fast to be adequately captured by CT scanning. To\naddress the aforementioned issues, we apply DCNNs to improve the quality of\nrock CT images and reduce exposure times by more than 60\\%, simultaneously. We\nhighlight current results based on micro-CT derived datasets and apply transfer\nlearning to improve DCNN results without increasing training time. The approach\nis applicable to any computed tomography technology. Furthermore, we contrast\nthe performance of the DCNN trained by minimizing different loss functions such\nas mean squared error and structural similarity index.\n", "versions": [{"version": "v1", "created": "Mon, 18 Jan 2021 23:31:37 GMT"}], "update_date": "2021-01-20", "authors_parsed": [["Alsamadony", "Khalid L.", ""], ["Yildirim", "Ertugrul U.", ""], ["Glatz", "Guenther", ""], ["Waheed", "Umair bin", ""], ["Hanafy", "Sherif M.", ""]]}, {"id": "2101.07385", "submitter": "Maximilian Amsler", "authors": "Sebastian Ament, Maximilian Amsler, Duncan R. Sutherland, Ming-Chiang\n  Chang, Dan Guevarra, Aine B. Connolly, John M. Gregoire, Michael O. Thompson,\n  Carla P. Gomes, R. Bruce van Dover", "title": "Autonomous synthesis of metastable materials", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cond-mat.mtrl-sci cs.AI cs.LG cs.MA physics.comp-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Autonomous experimentation enabled by artificial intelligence (AI) offers a\nnew paradigm for accelerating scientific discovery. Non-equilibrium materials\nsynthesis is emblematic of complex, resource-intensive experimentation whose\nacceleration would be a watershed for materials discovery and development. The\nmapping of non-equilibrium synthesis phase diagrams has recently been\naccelerated via high throughput experimentation but still limits materials\nresearch because the parameter space is too vast to be exhaustively explored.\nWe demonstrate accelerated synthesis and exploration of metastable materials\nthrough hierarchical autonomous experimentation governed by the Scientific\nAutonomous Reasoning Agent (SARA). SARA integrates robotic materials synthesis\nand characterization along with a hierarchy of AI methods that efficiently\nreveal the structure of processing phase diagrams. SARA designs lateral\ngradient laser spike annealing (lg-LSA) experiments for parallel materials\nsynthesis and employs optical spectroscopy to rapidly identify phase\ntransitions. Efficient exploration of the multi-dimensional parameter space is\nachieved with nested active learning (AL) cycles built upon advanced machine\nlearning models that incorporate the underlying physics of the experiments as\nwell as end-to-end uncertainty quantification. With this, and the coordination\nof AL at multiple scales, SARA embodies AI harnessing of complex scientific\ntasks. We demonstrate its performance by autonomously mapping synthesis phase\nboundaries for the Bi$_2$O$_3$ system, leading to orders-of-magnitude\nacceleration in establishment of a synthesis phase diagram that includes\nconditions for kinetically stabilizing $\\delta$-Bi$_2$O$_3$ at room\ntemperature, a critical development for electrochemical technologies such as\nsolid oxide fuel cells.\n", "versions": [{"version": "v1", "created": "Tue, 19 Jan 2021 00:29:26 GMT"}], "update_date": "2021-01-20", "authors_parsed": [["Ament", "Sebastian", ""], ["Amsler", "Maximilian", ""], ["Sutherland", "Duncan R.", ""], ["Chang", "Ming-Chiang", ""], ["Guevarra", "Dan", ""], ["Connolly", "Aine B.", ""], ["Gregoire", "John M.", ""], ["Thompson", "Michael O.", ""], ["Gomes", "Carla P.", ""], ["van Dover", "R. Bruce", ""]]}, {"id": "2101.07393", "submitter": "Austin W. Hanjie", "authors": "Austin W. Hanjie, Victor Zhong, Karthik Narasimhan", "title": "Grounding Language to Entities and Dynamics for Generalization in\n  Reinforcement Learning", "comments": "Accepted to ICML 2021. Note author list and name changes from\n  previous version", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate the use of natural language to drive the generalization of\ncontrol policies and introduce the new multi-task environment Messenger with\nfree-form text manuals describing the environment dynamics. Unlike previous\nwork, Messenger does not assume prior knowledge connecting text and state\nobservations $-$ the control policy must simultaneously ground the game manual\nto entity symbols and dynamics in the environment. We develop a new model, EMMA\n(Entity Mapper with Multi-modal Attention) which uses an entity-conditioned\nattention module that allows for selective focus over relevant descriptions in\nthe manual for each entity in the environment. EMMA is end-to-end\ndifferentiable and learns a latent grounding of entities and dynamics from text\nto observations using only environment rewards. EMMA achieves successful\nzero-shot generalization to unseen games with new dynamics, obtaining a 40%\nhigher win rate compared to multiple baselines. However, win rate on the\nhardest stage of Messenger remains low (10%), demonstrating the need for\nadditional work in this direction.\n", "versions": [{"version": "v1", "created": "Tue, 19 Jan 2021 00:59:16 GMT"}, {"version": "v2", "created": "Fri, 11 Jun 2021 23:34:49 GMT"}], "update_date": "2021-06-15", "authors_parsed": [["Hanjie", "Austin W.", ""], ["Zhong", "Victor", ""], ["Narasimhan", "Karthik", ""]]}, {"id": "2101.07399", "submitter": "Yuyang Wang", "authors": "Yuyang Wang, Zhonglin Cao, Amir Barati Farimani", "title": "Deep Reinforcement Learning Optimizes Graphene Nanopores for Efficient\n  Desalination", "comments": "Yuyang Wang and Zhonglin Cao contributed equally to this work", "journal-ref": null, "doi": "10.1038/s41699-021-00246-9", "report-no": null, "categories": "cs.LG physics.chem-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Two-dimensional nanomaterials, such as graphene, have been extensively\nstudied because of their outstanding physical properties. Structure and\ngeometry optimization of nanopores on such materials is beneficial for their\nperformances in real-world engineering applications, like water desalination.\nHowever, the optimization process often involves very large number of\nexperiments or simulations which are expensive and time-consuming. In this\nwork, we propose a graphene nanopore optimization framework via the combination\nof deep reinforcement learning (DRL) and convolutional neural network (CNN) for\nefficient water desalination. The DRL agent controls the growth of nanopore by\ndetermining the atom to be removed at each timestep, while the CNN predicts the\nperformance of nanoporus graphene for water desalination: the water flux and\nion rejection at a certain external pressure. With the synchronous feedback\nfrom CNN-accelerated desalination performance prediction, our DRL agent can\noptimize the nanoporous graphene efficiently in an online manner. Molecular\ndynamics (MD) simulations on promising DRL-designed graphene nanopores show\nthat they have higher water flux while maintaining rival ion rejection rate\ncompared to the normal circular nanopores. Semi-oval shape with rough edges\ngeometry of DRL-designed pores is found to be the key factor for their high\nwater desalination performance. Ultimately, this study shows that DRL can be a\npowerful tool for material design.\n", "versions": [{"version": "v1", "created": "Tue, 19 Jan 2021 01:24:18 GMT"}, {"version": "v2", "created": "Tue, 9 Feb 2021 05:03:18 GMT"}], "update_date": "2021-07-13", "authors_parsed": [["Wang", "Yuyang", ""], ["Cao", "Zhonglin", ""], ["Farimani", "Amir Barati", ""]]}, {"id": "2101.07413", "submitter": "Junyuan Hong", "authors": "Junyuan Hong and Zhangyang Wang and Jiayu Zhou", "title": "On Dynamic Noise Influence in Differentially Private Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Protecting privacy in learning while maintaining the model performance has\nbecome increasingly critical in many applications that involve sensitive data.\nPrivate Gradient Descent (PGD) is a commonly used private learning framework,\nwhich noises gradients based on the Differential Privacy protocol. Recent\nstudies show that \\emph{dynamic privacy schedules} of decreasing noise\nmagnitudes can improve loss at the final iteration, and yet theoretical\nunderstandings of the effectiveness of such schedules and their connections to\noptimization algorithms remain limited. In this paper, we provide comprehensive\nanalysis of noise influence in dynamic privacy schedules to answer these\ncritical questions. We first present a dynamic noise schedule minimizing the\nutility upper bound of PGD, and show how the noise influence from each\noptimization step collectively impacts utility of the final model. Our study\nalso reveals how impacts from dynamic noise influence change when momentum is\nused. We empirically show the connection exists for general non-convex losses,\nand the influence is greatly impacted by the loss curvature.\n", "versions": [{"version": "v1", "created": "Tue, 19 Jan 2021 02:04:00 GMT"}], "update_date": "2021-01-20", "authors_parsed": [["Hong", "Junyuan", ""], ["Wang", "Zhangyang", ""], ["Zhou", "Jiayu", ""]]}, {"id": "2101.07415", "submitter": "Xingyou Song", "authors": "Xingyou Song, Krzysztof Choromanski, Jack Parker-Holder, Yunhao Tang,\n  Daiyi Peng, Deepali Jain, Wenbo Gao, Aldo Pacchiano, Tamas Sarlos, Yuxiang\n  Yang", "title": "ES-ENAS: Controller-Based Architecture Search for Evolutionary\n  Reinforcement Learning", "comments": "17 pages. This is an updated version of a previous submission which\n  can be found at arXiv:1907.06511. See\n  https://github.com/google-research/google-research/tree/master/es_enas for\n  associated code", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce ES-ENAS, a simple yet general evolutionary joint optimization\nprocedure by combining continuous optimization via Evolutionary Strategies (ES)\nand combinatorial optimization via Efficient NAS (ENAS) in a highly scalable\nand intuitive way. Our main insight is noticing that ES is already a highly\ndistributed algorithm involving hundreds of forward passes which can not only\nbe used for training neural network weights, but also for jointly training a\nNAS controller, both in a blackbox fashion. By doing so, we also bridge the gap\nfrom NAS research in supervised learning settings to the reinforcement learning\nscenario through this relatively simple marriage between two different yet\ncommon lines of research. We demonstrate the utility and effectiveness of our\nmethod over a large search space by training highly combinatorial neural\nnetwork architectures for RL problems in continuous control, via edge pruning\nand quantization. We also incorporate a wide variety of popular techniques from\nmodern NAS literature including multiobjective optimization along with various\ncontroller methods, to showcase their promise in the RL field and discuss\npossible extensions.\n", "versions": [{"version": "v1", "created": "Tue, 19 Jan 2021 02:19:05 GMT"}, {"version": "v2", "created": "Thu, 3 Jun 2021 23:48:45 GMT"}], "update_date": "2021-06-07", "authors_parsed": [["Song", "Xingyou", ""], ["Choromanski", "Krzysztof", ""], ["Parker-Holder", "Jack", ""], ["Tang", "Yunhao", ""], ["Peng", "Daiyi", ""], ["Jain", "Deepali", ""], ["Gao", "Wenbo", ""], ["Pacchiano", "Aldo", ""], ["Sarlos", "Tamas", ""], ["Yang", "Yuxiang", ""]]}, {"id": "2101.07423", "submitter": "G\\\"ozde \\\"Ozcan", "authors": "G\\\"ozde \\\"Ozcan, Armin Moharrer, Stratis Ioannidis", "title": "Submodular Maximization via Taylor Series Approximation", "comments": "15 pages, 2 figures, to be published in the SIAM International\n  Conference on Data Mining proceedings (SDM 2021)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We study submodular maximization problems with matroid constraints, in\nparticular, problems where the objective can be expressed via compositions of\nanalytic and multilinear functions. We show that for functions of this form,\nthe so-called continuous greedy algorithm attains a ratio arbitrarily close to\n$(1-1/e) \\approx 0.63$ using a deterministic estimation via Taylor series\napproximation. This drastically reduces execution time over prior art that uses\nsampling.\n", "versions": [{"version": "v1", "created": "Tue, 19 Jan 2021 02:41:45 GMT"}], "update_date": "2021-01-20", "authors_parsed": [["\u00d6zcan", "G\u00f6zde", ""], ["Moharrer", "Armin", ""], ["Ioannidis", "Stratis", ""]]}, {"id": "2101.07425", "submitter": "Jianguo Chen", "authors": "Jianguo Chen and Kenli Li and Keqin Li and Philip S. Yu and Zeng Zeng", "title": "Dynamic Planning of Bicycle Stations in Dockless Public Bicycle-sharing\n  System Using Gated Graph Neural Network", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Benefiting from convenient cycling and flexible parking locations, the\nDockless Public Bicycle-sharing (DL-PBS) network becomes increasingly popular\nin many countries. However, redundant and low-utility stations waste public\nurban space and maintenance costs of DL-PBS vendors. In this paper, we propose\na Bicycle Station Dynamic Planning (BSDP) system to dynamically provide the\noptimal bicycle station layout for the DL-PBS network. The BSDP system contains\nfour modules: bicycle drop-off location clustering, bicycle-station graph\nmodeling, bicycle-station location prediction, and bicycle-station layout\nrecommendation. In the bicycle drop-off location clustering module, candidate\nbicycle stations are clustered from each spatio-temporal subset of the\nlarge-scale cycling trajectory records. In the bicycle-station graph modeling\nmodule, a weighted digraph model is built based on the clustering results and\ninferior stations with low station revenue and utility are filtered. Then,\ngraph models across time periods are combined to create a graph sequence model.\nIn the bicycle-station location prediction module, the GGNN model is used to\ntrain the graph sequence data and dynamically predict bicycle stations in the\nnext period. In the bicycle-station layout recommendation module, the predicted\nbicycle stations are fine-tuned according to the government urban management\nplan, which ensures that the recommended station layout is conducive to city\nmanagement, vendor revenue, and user convenience. Experiments on actual DL-PBS\nnetworks verify the effectiveness, accuracy and feasibility of the proposed\nBSDP system.\n", "versions": [{"version": "v1", "created": "Tue, 19 Jan 2021 02:51:12 GMT"}], "update_date": "2021-01-20", "authors_parsed": [["Chen", "Jianguo", ""], ["Li", "Kenli", ""], ["Li", "Keqin", ""], ["Yu", "Philip S.", ""], ["Zeng", "Zeng", ""]]}, {"id": "2101.07433", "submitter": "Alexander Wong", "authors": "Hayden Gunraj, Ali Sabri, David Koff, and Alexander Wong", "title": "COVID-Net CT-2: Enhanced Deep Neural Networks for Detection of COVID-19\n  from Chest CT Images Through Bigger, More Diverse Learning", "comments": "15 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The COVID-19 pandemic continues to rage on, with multiple waves causing\nsubstantial harm to health and economies around the world. Motivated by the use\nof CT imaging at clinical institutes around the world as an effective\ncomplementary screening method to RT-PCR testing, we introduced COVID-Net CT, a\nneural network tailored for detection of COVID-19 cases from chest CT images as\npart of the open source COVID-Net initiative. However, one potential limiting\nfactor is restricted quantity and diversity given the single nation patient\ncohort used. In this study, we introduce COVID-Net CT-2, enhanced deep neural\nnetworks for COVID-19 detection from chest CT images trained on the largest\nquantity and diversity of multinational patient cases in research literature.\nWe introduce two new CT benchmark datasets, the largest comprising a\nmultinational cohort of 4,501 patients from at least 15 countries. We leverage\nexplainability to investigate the decision-making behaviour of COVID-Net CT-2,\nwith the results for select cases reviewed and reported on by two\nboard-certified radiologists with over 10 and 30 years of experience,\nrespectively. The COVID-Net CT-2 neural networks achieved accuracy, COVID-19\nsensitivity, PPV, specificity, and NPV of 98.1%/96.2%/96.7%/99%/98.8% and\n97.9%/95.7%/96.4%/98.9%/98.7%, respectively. Explainability-driven performance\nvalidation shows that COVID-Net CT-2's decision-making behaviour is consistent\nwith radiologist interpretation by leveraging correct, clinically relevant\ncritical factors. The results are promising and suggest the strong potential of\ndeep neural networks as an effective tool for computer-aided COVID-19\nassessment. While not a production-ready solution, we hope the open-source,\nopen-access release of COVID-Net CT-2 and benchmark datasets will continue to\nenable researchers, clinicians, and citizen data scientists alike to build upon\nthem.\n", "versions": [{"version": "v1", "created": "Tue, 19 Jan 2021 03:04:09 GMT"}, {"version": "v2", "created": "Tue, 26 Jan 2021 13:51:26 GMT"}], "update_date": "2021-01-27", "authors_parsed": [["Gunraj", "Hayden", ""], ["Sabri", "Ali", ""], ["Koff", "David", ""], ["Wong", "Alexander", ""]]}, {"id": "2101.07437", "submitter": "Jianguo Chen", "authors": "Jianguo Chen and Kenli Li and Keqin Li and Philip S. Yu and Zeng Zeng", "title": "Dynamic Bicycle Dispatching of Dockless Public Bicycle-sharing Systems\n  using Multi-objective Reinforcement Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As a new generation of Public Bicycle-sharing Systems (PBS), the dockless PBS\n(DL-PBS) is an important application of cyber-physical systems and intelligent\ntransportation. How to use AI to provide efficient bicycle dispatching\nsolutions based on dynamic bicycle rental demand is an essential issue for\nDL-PBS. In this paper, we propose a dynamic bicycle dispatching algorithm based\non multi-objective reinforcement learning (MORL-BD) to provide the optimal\nbicycle dispatching solution for DL-PBS. We model the DL-PBS system from the\nperspective of CPS and use deep learning to predict the layout of bicycle\nparking spots and the dynamic demand of bicycle dispatching. We define the\nmulti-route bicycle dispatching problem as a multi-objective optimization\nproblem by considering the optimization objectives of dispatching costs,\ndispatch truck's initial load, workload balance among the trucks, and the\ndynamic balance of bicycle supply and demand. On this basis, the collaborative\nmulti-route bicycle dispatching problem among multiple dispatch trucks is\nmodeled as a multi-agent MORL model. All dispatch paths between parking spots\nare defined as state spaces, and the reciprocal of dispatching costs is defined\nas a reward. Each dispatch truck is equipped with an agent to learn the optimal\ndispatch path in the dynamic DL-PBS network. We create an elite list to store\nthe Pareto optimal solutions of bicycle dispatch paths found in each action,\nand finally, get the Pareto frontier. Experimental results on the actual DL-PBS\nsystems show that compared with existing methods, MORL-BD can find a higher\nquality Pareto frontier with less execution time.\n", "versions": [{"version": "v1", "created": "Tue, 19 Jan 2021 03:09:51 GMT"}], "update_date": "2021-01-20", "authors_parsed": [["Chen", "Jianguo", ""], ["Li", "Kenli", ""], ["Li", "Keqin", ""], ["Yu", "Philip S.", ""], ["Zeng", "Zeng", ""]]}, {"id": "2101.07492", "submitter": "Taniya Seth", "authors": "Taniya Seth and Pranab K. Muhuri", "title": "Optimizing Hyperparameters in CNNs using Bilevel Programming in Time\n  Series Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Hyperparameter optimization has remained a central topic within the machine\nlearning community due to its ability to produce state-of-the-art results. With\nthe recent interest growing in the usage of CNNs for time series prediction, we\npropose the notion of optimizing Hyperparameters in CNNs for the purpose of\ntime series prediction. In this position paper, we give away the idea of\nmodeling the concerned hyperparameter optimization problem using bilevel\nprogramming.\n", "versions": [{"version": "v1", "created": "Tue, 19 Jan 2021 07:24:54 GMT"}], "update_date": "2021-01-20", "authors_parsed": [["Seth", "Taniya", ""], ["Muhuri", "Pranab K.", ""]]}, {"id": "2101.07496", "submitter": "Jun Han Mr", "authors": "Jun Han, Martin Renqiang Min, Ligong Han, Li Erran Li, Xuan Zhang", "title": "Disentangled Recurrent Wasserstein Autoencoder", "comments": "ICLR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning disentangled representations leads to interpretable models and\nfacilitates data generation with style transfer, which has been extensively\nstudied on static data such as images in an unsupervised learning framework.\nHowever, only a few works have explored unsupervised disentangled sequential\nrepresentation learning due to challenges of generating sequential data. In\nthis paper, we propose recurrent Wasserstein Autoencoder (R-WAE), a new\nframework for generative modeling of sequential data. R-WAE disentangles the\nrepresentation of an input sequence into static and dynamic factors (i.e.,\ntime-invariant and time-varying parts). Our theoretical analysis shows that,\nR-WAE minimizes an upper bound of a penalized form of the Wasserstein distance\nbetween model distribution and sequential data distribution, and simultaneously\nmaximizes the mutual information between input data and different disentangled\nlatent factors, respectively. This is superior to (recurrent) VAE which does\nnot explicitly enforce mutual information maximization between input data and\ndisentangled latent representations. When the number of actions in sequential\ndata is available as weak supervision information, R-WAE is extended to learn a\ncategorical latent representation of actions to improve its disentanglement.\nExperiments on a variety of datasets show that our models outperform other\nbaselines with the same settings in terms of disentanglement and unconditional\nvideo generation both quantitatively and qualitatively.\n", "versions": [{"version": "v1", "created": "Tue, 19 Jan 2021 07:43:25 GMT"}], "update_date": "2021-01-20", "authors_parsed": [["Han", "Jun", ""], ["Min", "Martin Renqiang", ""], ["Han", "Ligong", ""], ["Li", "Li Erran", ""], ["Zhang", "Xuan", ""]]}, {"id": "2101.07511", "submitter": "Adnan Qayyum", "authors": "Adnan Qayyum, Kashif Ahmad, Muhammad Ahtazaz Ahsan, Ala Al-Fuqaha, and\n  Junaid Qadir", "title": "Collaborative Federated Learning For Healthcare: Multi-Modal COVID-19\n  Diagnosis at the Edge", "comments": "preprint version", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Despite significant improvements over the last few years, cloud-based\nhealthcare applications continue to suffer from poor adoption due to their\nlimitations in meeting stringent security, privacy, and quality of service\nrequirements (such as low latency). The edge computing trend, along with\ntechniques for distributed machine learning such as federated learning, have\ngained popularity as a viable solution in such settings. In this paper, we\nleverage the capabilities of edge computing in medicine by analyzing and\nevaluating the potential of intelligent processing of clinical visual data at\nthe edge allowing the remote healthcare centers, lacking advanced diagnostic\nfacilities, to benefit from the multi-modal data securely. To this aim, we\nutilize the emerging concept of clustered federated learning (CFL) for an\nautomatic diagnosis of COVID-19. Such an automated system can help reduce the\nburden on healthcare systems across the world that has been under a lot of\nstress since the COVID-19 pandemic emerged in late 2019. We evaluate the\nperformance of the proposed framework under different experimental setups on\ntwo benchmark datasets. Promising results are obtained on both datasets\nresulting in comparable results against the central baseline where the\nspecialized models (i.e., each on a specific type of COVID-19 imagery) are\ntrained with central data, and improvements of 16\\% and 11\\% in overall\nF1-Scores have been achieved over the multi-modal model trained in the\nconventional Federated Learning setup on X-ray and Ultrasound datasets,\nrespectively. We also discuss in detail the associated challenges,\ntechnologies, tools, and techniques available for deploying ML at the edge in\nsuch privacy and delay-sensitive applications.\n", "versions": [{"version": "v1", "created": "Tue, 19 Jan 2021 08:40:59 GMT"}], "update_date": "2021-01-20", "authors_parsed": [["Qayyum", "Adnan", ""], ["Ahmad", "Kashif", ""], ["Ahsan", "Muhammad Ahtazaz", ""], ["Al-Fuqaha", "Ala", ""], ["Qadir", "Junaid", ""]]}, {"id": "2101.07524", "submitter": "Jiaheng Wei", "authors": "Jiaheng Wei, Minghao Liu, Jiahao Luo, Qiutong Li, James Davis, and\n  Yang Liu", "title": "PeerGAN: Generative Adversarial Networks with a Competing Peer\n  Discriminator", "comments": "Under Review", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we introduce PeerGAN, a generative adversarial network (GAN)\nsolution to improve the stability of the generated samples and to mitigate mode\ncollapse. Built upon the Vanilla GAN's two-player game between the\ndiscriminator $D_1$ and the generator $G$, we introduce a peer discriminator\n$D_2$ to the min-max game. Similar to previous work using two discriminators,\nthe first role of both $D_1$, $D_2$ is to distinguish between generated samples\nand real ones, while the generator tries to generate high-quality samples which\nare able to fool both discriminators. Different from existing methods, we\nintroduce another game between $D_1$ and $D_2$ to discourage their agreement\nand therefore increase the level of diversity of the generated samples. This\nproperty alleviates the issue of early mode collapse by preventing $D_1$ and\n$D_2$ from converging too fast. We provide theoretical analysis for the\nequilibrium of the min-max game formed among $G, D_1, D_2$. We offer\nconvergence behavior of PeerGAN as well as stability of the min-max game. It's\nworth mentioning that PeerGAN operates in the unsupervised setting, and the\nadditional game between $D_1$ and $D_2$ does not need any label supervision.\nExperiments results on a synthetic dataset and on real-world image datasets\n(MNIST, Fashion MNIST, CIFAR-10, STL-10, CelebA, VGG) demonstrate that PeerGAN\noutperforms competitive baseline work in generating diverse and high-quality\nsamples, while only introduces negligible computation cost.\n", "versions": [{"version": "v1", "created": "Tue, 19 Jan 2021 09:25:23 GMT"}, {"version": "v2", "created": "Sun, 6 Jun 2021 04:06:20 GMT"}], "update_date": "2021-06-08", "authors_parsed": [["Wei", "Jiaheng", ""], ["Liu", "Minghao", ""], ["Luo", "Jiahao", ""], ["Li", "Qiutong", ""], ["Davis", "James", ""], ["Liu", "Yang", ""]]}, {"id": "2101.07525", "submitter": "Zeming Li", "authors": "Zeming Li, Songtao Liu, Jian Sun", "title": "Momentum^2 Teacher: Momentum Teacher with Momentum Statistics for\n  Self-Supervised Learning", "comments": "11 pages, Tech report", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper, we present a novel approach, Momentum$^2$ Teacher, for\nstudent-teacher based self-supervised learning. The approach performs momentum\nupdate on both network weights and batch normalization (BN) statistics. The\nteacher's weight is a momentum update of the student, and the teacher's BN\nstatistics is a momentum update of those in history. The Momentum$^2$ Teacher\nis simple and efficient. It can achieve the state of the art results (74.5\\%)\nunder ImageNet linear evaluation protocol using small-batch size(\\eg, 128),\nwithout requiring large-batch training on special hardware like TPU or\ninefficient across GPU operation (\\eg, shuffling BN, synced BN). Our\nimplementation and pre-trained models will be given on\nGitHub\\footnote{https://github.com/zengarden/momentum2-teacher}.\n", "versions": [{"version": "v1", "created": "Tue, 19 Jan 2021 09:27:03 GMT"}], "update_date": "2021-01-20", "authors_parsed": [["Li", "Zeming", ""], ["Liu", "Songtao", ""], ["Sun", "Jian", ""]]}, {"id": "2101.07528", "submitter": "Edouard Oyallon", "authors": "Louis Thiry (DI-ENS), Michael Arbel (UCL), Eugene Belilovsky (MILA),\n  Edouard Oyallon (MLIA)", "title": "The Unreasonable Effectiveness of Patches in Deep Convolutional Kernels\n  Methods", "comments": null, "journal-ref": "International Conference on Learning Representation (ICLR 2021),\n  2021, Vienna (online), Austria", "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A recent line of work showed that various forms of convolutional kernel\nmethods can be competitive with standard supervised deep convolutional networks\non datasets like CIFAR-10, obtaining accuracies in the range of 87-90% while\nbeing more amenable to theoretical analysis. In this work, we highlight the\nimportance of a data-dependent feature extraction step that is key to the\nobtain good performance in convolutional kernel methods. This step typically\ncorresponds to a whitened dictionary of patches, and gives rise to a\ndata-driven convolutional kernel methods. We extensively study its effect,\ndemonstrating it is the key ingredient for high performance of these methods.\nSpecifically, we show that one of the simplest instances of such kernel\nmethods, based on a single layer of image patches followed by a linear\nclassifier is already obtaining classification accuracies on CIFAR-10 in the\nsame range as previous more sophisticated convolutional kernel methods. We\nscale this method to the challenging ImageNet dataset, showing such a simple\napproach can exceed all existing non-learned representation methods. This is a\nnew baseline for object recognition without representation learning methods,\nthat initiates the investigation of convolutional kernel models on ImageNet. We\nconduct experiments to analyze the dictionary that we used, our ablations\nshowing they exhibit low-dimensional properties.\n", "versions": [{"version": "v1", "created": "Tue, 19 Jan 2021 09:30:58 GMT"}], "update_date": "2021-01-20", "authors_parsed": [["Thiry", "Louis", "", "DI-ENS"], ["Arbel", "Michael", "", "UCL"], ["Belilovsky", "Eugene", "", "MILA"], ["Oyallon", "Edouard", "", "MLIA"]]}, {"id": "2101.07535", "submitter": "Dan Li", "authors": "Dacheng Chen, Dan Li, Xiuqin Xu, Ruizhi Yang, See-Kiong Ng", "title": "Electrocardiogram Classification and Visual Diagnosis of Atrial\n  Fibrillation with DenseECG", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SP cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Atrial Fibrillation (AF) is a common cardiac arrhythmia affecting a large\nnumber of people around the world. If left undetected, it will develop into\nchronic disability or even early mortality. However, patients who have this\nproblem can barely feel its presence, especially in its early stage. A\nnon-invasive, automatic, and effective detection method is therefore needed to\nhelp early detection so that medical intervention can be implemented in time to\nprevent its progression.\n  Electrocardiogram (ECG), which records the electrical activities of the\nheart, has been widely used for detecting the presence of AF. However, due to\nthe subtle patterns of AF, the performance of detection models have largely\ndepended on complicated data pre-processing and expertly engineered features.\nIn our work, we developed DenseECG, an end-to-end model based on 5 layers 1D\ndensely connected convolutional neural network. We trained our model using the\npublicly available dataset from 2017 PhysioNet Computing in Cardiology(CinC)\nChallenge containing 8528 single-lead ECG recordings of short-term heart\nrhythms (9-61s). Our trained model was able to outperform the other\nstate-of-the-art AF detection models on this dataset without complicated data\npre-processing and expert-supervised feature engineering.\n", "versions": [{"version": "v1", "created": "Tue, 19 Jan 2021 09:45:46 GMT"}], "update_date": "2021-01-20", "authors_parsed": [["Chen", "Dacheng", ""], ["Li", "Dan", ""], ["Xu", "Xiuqin", ""], ["Yang", "Ruizhi", ""], ["Ng", "See-Kiong", ""]]}, {"id": "2101.07561", "submitter": "Paul Novello", "authors": "Paul Novello (CEA, X, Inria), Ga\\\"el Po\\\"ette (CEA), David Lugato\n  (CEA), Pietro Congedo (X, Inria)", "title": "Variance Based Samples Weighting for Supervised Deep Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the context of supervised learning of a function by a Neural Network (NN),\nwe claim and empirically justify that a NN yields better results when the\ndistribution of the data set focuses on regions where the function to learn is\nsteeper. We first traduce this assumption in a mathematically workable way\nusing Taylor expansion. Then, theoretical derivations allow to construct a\nmethodology that we call Variance Based Samples Weighting (VBSW). VBSW uses\nlocal variance of the labels to weight the training points. This methodology is\ngeneral, scalable, cost effective, and significantly increases the performances\nof a large class of NNs for various classification and regression tasks on\nimage, text and multivariate data. We highlight its benefits with experiments\ninvolving NNs from shallow linear NN to Resnet or Bert.\n", "versions": [{"version": "v1", "created": "Tue, 19 Jan 2021 11:08:40 GMT"}, {"version": "v2", "created": "Thu, 28 Jan 2021 12:50:28 GMT"}], "update_date": "2021-01-29", "authors_parsed": [["Novello", "Paul", "", "CEA, X, Inria"], ["Po\u00ebtte", "Ga\u00ebl", "", "CEA"], ["Lugato", "David", "", "CEA"], ["Congedo", "Pietro", "", "X, Inria"]]}, {"id": "2101.07564", "submitter": "Luc Pronzato", "authors": "Luc Pronzato", "title": "Performance analysis of greedy algorithms for minimising a Maximum Mean\n  Discrepancy", "comments": "34 pages, 7 figures, preprint submitted to a journal", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.CO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We analyse the performance of several iterative algorithms for the\nquantisation of a probability measure $\\mu$, based on the minimisation of a\nMaximum Mean Discrepancy (MMD). Our analysis includes kernel herding, greedy\nMMD minimisation and Sequential Bayesian Quadrature (SBQ). We show that the\nfinite-sample-size approximation error, measured by the MMD, decreases as $1/n$\nfor SBQ and also for kernel herding and greedy MMD minimisation when using a\nsuitable step-size sequence. The upper bound on the approximation error is\nslightly better for SBQ, but the other methods are significantly faster, with a\ncomputational cost that increases only linearly with the number of points\nselected. This is illustrated by two numerical examples, with the target\nmeasure $\\mu$ being uniform (a space-filling design application) and with $\\mu$\na Gaussian mixture.\n", "versions": [{"version": "v1", "created": "Tue, 19 Jan 2021 11:18:51 GMT"}], "update_date": "2021-01-20", "authors_parsed": [["Pronzato", "Luc", ""]]}, {"id": "2101.07576", "submitter": "Rita Pucci", "authors": "Rita Pucci, Christian Micheloni, Niki Martinel", "title": "Collaboration among Image and Object Level Features for Image\n  Colourisation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image colourisation is an ill-posed problem, with multiple correct solutions\nwhich depend on the context and object instances present in the input datum.\nPrevious approaches attacked the problem either by requiring intense user\ninteractions or by exploiting the ability of convolutional neural networks\n(CNNs) in learning image level (context) features. However, obtaining human\nhints is not always feasible and CNNs alone are not able to learn object-level\nsemantics unless multiple models pretrained with supervision are considered. In\nthis work, we propose a single network, named UCapsNet, that separate\nimage-level features obtained through convolutions and object-level features\ncaptured by means of capsules. Then, by skip connections over different layers,\nwe enforce collaboration between such disentangling factors to produce high\nquality and plausible image colourisation. We pose the problem as a\nclassification task that can be addressed by a fully self-supervised approach,\nthus requires no human effort. Experimental results on three benchmark datasets\nshow that our approach outperforms existing methods on standard quality metrics\nand achieves a state of the art performances on image colourisation. A large\nscale user study shows that our method is preferred over existing solutions.\n", "versions": [{"version": "v1", "created": "Tue, 19 Jan 2021 11:48:12 GMT"}], "update_date": "2021-01-20", "authors_parsed": [["Pucci", "Rita", ""], ["Micheloni", "Christian", ""], ["Martinel", "Niki", ""]]}, {"id": "2101.07577", "submitter": "Chen Gao", "authors": "Siyi Liu, Chen Gao, Yihong Chen, Depeng Jin, Yong Li", "title": "Learnable Embedding Sizes for Recommender Systems", "comments": "International Conference on Learning Representations (ICLR), 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The embedding-based representation learning is commonly used in deep learning\nrecommendation models to map the raw sparse features to dense vectors. The\ntraditional embedding manner that assigns a uniform size to all features has\ntwo issues. First, the numerous features inevitably lead to a gigantic\nembedding table that causes a high memory usage cost. Second, it is likely to\ncause the over-fitting problem for those features that do not require too large\nrepresentation capacity. Existing works that try to address the problem always\ncause a significant drop in recommendation performance or suffers from the\nlimitation of unaffordable training time cost. In this paper, we proposed a\nnovel approach, named PEP (short for Plug-in Embedding Pruning), to reduce the\nsize of the embedding table while avoiding the drop of recommendation accuracy.\nPEP prunes embedding parameter where the pruning threshold(s) can be adaptively\nlearned from data. Therefore we can automatically obtain a mixed-dimension\nembedding-scheme by pruning redundant parameters for each feature. PEP is a\ngeneral framework that can plug in various base recommendation models.\nExtensive experiments demonstrate it can efficiently cut down embedding\nparameters and boost the base model's performance. Specifically, it achieves\nstrong recommendation performance while reducing 97-99% parameters. As for the\ncomputation cost, PEP only brings an additional 20-30% time cost compared with\nbase models. Codes are available at\nhttps://github.com/ssui-liu/learnable-embed-sizes-for-RecSys.\n", "versions": [{"version": "v1", "created": "Tue, 19 Jan 2021 11:50:33 GMT"}, {"version": "v2", "created": "Thu, 11 Mar 2021 10:38:59 GMT"}], "update_date": "2021-03-12", "authors_parsed": [["Liu", "Siyi", ""], ["Gao", "Chen", ""], ["Chen", "Yihong", ""], ["Jin", "Depeng", ""], ["Li", "Yong", ""]]}, {"id": "2101.07579", "submitter": "Panagiotis Tigas", "authors": "Panagiotis Tigas and Tyson Hosmer", "title": "Spatial Assembly: Generative Architecture With Reinforcement Learning,\n  Self Play and Tree Search", "comments": "Workshop on Machine Learning for Creativity and Design at the 34rd\n  Conference on Neural Information Processing Systems (NeurIPS 2020)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.GR cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  With this work, we investigate the use of Reinforcement Learning (RL) for the\ngeneration of spatial assemblies, by combining ideas from Procedural Generation\nalgorithms (Wave Function Collapse algorithm (WFC)) and RL for Game Solving.\nWFC is a Generative Design algorithm, inspired by Constraint Solving. In WFC,\none defines a set of tiles/blocks and constraints and the algorithm generates\nan assembly that satisfies these constraints. Casting the problem of generation\nof spatial assemblies as a Markov Decision Process whose states transitions are\ndefined by WFC, we propose an algorithm that uses Reinforcement Learning and\nSelf-Play to learn a policy that generates assemblies that maximize objectives\nset by the designer. Finally, we demonstrate the use of our Spatial Assembly\nalgorithm in Architecture Design.\n", "versions": [{"version": "v1", "created": "Tue, 19 Jan 2021 11:57:10 GMT"}], "update_date": "2021-01-20", "authors_parsed": [["Tigas", "Panagiotis", ""], ["Hosmer", "Tyson", ""]]}, {"id": "2101.07581", "submitter": "Jiacheng Liu", "authors": "Jiacheng Liu, Meghna Singh, Catherine ST.Hill, Vino Raj, Lisa\n  Kirkland, Jaideep Srivastava", "title": "Continual Deterioration Prediction for Hospitalized COVID-19 Patients", "comments": "8 pages, 11 figures. Submitted to JAMIA in OCT, 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Leading up to August 2020, COVID-19 has spread to almost every country in the\nworld, causing millions of infected and hundreds of thousands of deaths. In\nthis paper, we first verify the assumption that clinical variables could have\ntime-varying effects on COVID-19 outcomes. Then, we develop a temporal\nstratification approach to make daily predictions on patients' outcome at the\nend of hospital stay. Training data is segmented by the remaining length of\nstay, which is a proxy for the patient's overall condition. Based on this, a\nsequence of predictive models are built, one for each time segment. Thanks to\nthe publicly shared data, we were able to build and evaluate prototype models.\nPreliminary experiments show 0.98 AUROC, 0.91 F1 score and 0.97 AUPR on\ncontinuous deterioration prediction, encouraging further development of the\nmodel as well as validations on different datasets. We also verify the key\nassumption which motivates our method. Clinical variables could have\ntime-varying effects on COVID-19 outcomes. That is to say, the feature\nimportance of a variable in the predictive model varies at different disease\nstages.\n", "versions": [{"version": "v1", "created": "Tue, 19 Jan 2021 12:03:56 GMT"}], "update_date": "2021-01-20", "authors_parsed": [["Liu", "Jiacheng", ""], ["Singh", "Meghna", ""], ["Hill", "Catherine ST.", ""], ["Raj", "Vino", ""], ["Kirkland", "Lisa", ""], ["Srivastava", "Jaideep", ""]]}, {"id": "2101.07597", "submitter": "Chengyi Wang", "authors": "Chengyi Wang, Yu Wu, Yao Qian, Kenichi Kumatani, Shujie Liu, Furu Wei,\n  Michael Zeng and Xuedong Huang", "title": "UniSpeech: Unified Speech Representation Learning with Labeled and\n  Unlabeled Data", "comments": "accepted by ICML2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG cs.SD eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a unified pre-training approach called UniSpeech to\nlearn speech representations with both unlabeled and labeled data, in which\nsupervised phonetic CTC learning and phonetically-aware contrastive\nself-supervised learning are conducted in a multi-task learning manner. The\nresultant representations can capture information more correlated with phonetic\nstructures and improve the generalization across languages and domains. We\nevaluate the effectiveness of UniSpeech for cross-lingual representation\nlearning on public CommonVoice corpus. The results show that UniSpeech\noutperforms self-supervised pretraining and supervised transfer learning for\nspeech recognition by a maximum of 13.4% and 17.8% relative phone error rate\nreductions respectively (averaged over all testing languages). The\ntransferability of UniSpeech is also demonstrated on a domain-shift speech\nrecognition task, i.e., a relative word error rate reduction of 6% against the\nprevious approach.\n", "versions": [{"version": "v1", "created": "Tue, 19 Jan 2021 12:53:43 GMT"}, {"version": "v2", "created": "Thu, 10 Jun 2021 09:17:28 GMT"}], "update_date": "2021-06-11", "authors_parsed": [["Wang", "Chengyi", ""], ["Wu", "Yu", ""], ["Qian", "Yao", ""], ["Kumatani", "Kenichi", ""], ["Liu", "Shujie", ""], ["Wei", "Furu", ""], ["Zeng", "Michael", ""], ["Huang", "Xuedong", ""]]}, {"id": "2101.07598", "submitter": "Sergei Koltcov", "authors": "Sergei Koltcov, Vera Ignatenko, Maxim Terpilovskii, Paolo Rosso", "title": "Analysis and tuning of hierarchical topic models based on Renyi entropy\n  approach", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hierarchical topic modeling is a potentially powerful instrument for\ndetermining the topical structure of text collections that allows constructing\na topical hierarchy representing levels of topical abstraction. However, tuning\nof parameters of hierarchical models, including the number of topics on each\nhierarchical level, remains a challenging task and an open issue. In this\npaper, we propose a Renyi entropy-based approach for a partial solution to the\nabove problem. First, we propose a Renyi entropy-based metric of quality for\nhierarchical models. Second, we propose a practical concept of hierarchical\ntopic model tuning tested on datasets with human mark-up. In the numerical\nexperiments, we consider three different hierarchical models, namely,\nhierarchical latent Dirichlet allocation (hLDA) model, hierarchical Pachinko\nallocation model (hPAM), and hierarchical additive regularization of topic\nmodels (hARTM). We demonstrate that hLDA model possesses a significant level of\ninstability and, moreover, the derived numbers of topics are far away from the\ntrue numbers for labeled datasets. For hPAM model, the Renyi entropy approach\nallows us to determine only one level of the data structure. For hARTM model,\nthe proposed approach allows us to estimate the number of topics for two\nhierarchical levels.\n", "versions": [{"version": "v1", "created": "Tue, 19 Jan 2021 12:54:47 GMT"}], "update_date": "2021-01-20", "authors_parsed": [["Koltcov", "Sergei", ""], ["Ignatenko", "Vera", ""], ["Terpilovskii", "Maxim", ""], ["Rosso", "Paolo", ""]]}, {"id": "2101.07600", "submitter": "Ricards Marcinkevics", "authors": "Ri\\v{c}ards Marcinkevi\\v{c}s, Julia E. Vogt", "title": "Interpretable Models for Granger Causality Using Self-explaining Neural\n  Networks", "comments": "ICLR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Exploratory analysis of time series data can yield a better understanding of\ncomplex dynamical systems. Granger causality is a practical framework for\nanalysing interactions in sequential data, applied in a wide range of domains.\nIn this paper, we propose a novel framework for inferring multivariate Granger\ncausality under nonlinear dynamics based on an extension of self-explaining\nneural networks. This framework is more interpretable than other\nneural-network-based techniques for inferring Granger causality, since in\naddition to relational inference, it also allows detecting signs of\nGranger-causal effects and inspecting their variability over time. In\ncomprehensive experiments on simulated data, we show that our framework\nperforms on par with several powerful baseline methods at inferring Granger\ncausality and that it achieves better performance at inferring interaction\nsigns. The results suggest that our framework is a viable and more\ninterpretable alternative to sparse-input neural networks for inferring Granger\ncausality.\n", "versions": [{"version": "v1", "created": "Tue, 19 Jan 2021 12:59:00 GMT"}], "update_date": "2021-01-20", "authors_parsed": [["Marcinkevi\u010ds", "Ri\u010dards", ""], ["Vogt", "Julia E.", ""]]}, {"id": "2101.07606", "submitter": "Viraj Kulkarni", "authors": "Tanveer Gupte, Mrunmai Niljikar, Manish Gawali, Viraj Kulkarni, Amit\n  Kharat, Aniruddha Pant", "title": "Deep Learning Models for Calculation of Cardiothoracic Ratio from Chest\n  Radiographs for Assisted Diagnosis of Cardiomegaly", "comments": "6 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We propose an automated method based on deep learning to compute the\ncardiothoracic ratio and detect the presence of cardiomegaly from chest\nradiographs. We develop two separate models to demarcate the heart and chest\nregions in an X-ray image using bounding boxes and use their outputs to\ncalculate the cardiothoracic ratio. We obtain a sensitivity of 0.96 at a\nspecificity of 0.81 with a mean absolute error of 0.0209 on a held-out test\ndataset and a sensitivity of 0.84 at a specificity of 0.97 with a mean absolute\nerror of 0.018 on an independent dataset from a different hospital. We also\ncompare three different segmentation model architectures for the proposed\nmethod and observe that Attention U-Net yields better results than SE-Resnext\nU-Net and EfficientNet U-Net. By providing a numeric measurement of the\ncardiothoracic ratio, we hope to mitigate human subjectivity arising out of\nvisual assessment in the detection of cardiomegaly.\n", "versions": [{"version": "v1", "created": "Tue, 19 Jan 2021 13:09:29 GMT"}], "update_date": "2021-01-20", "authors_parsed": [["Gupte", "Tanveer", ""], ["Niljikar", "Mrunmai", ""], ["Gawali", "Manish", ""], ["Kulkarni", "Viraj", ""], ["Kharat", "Amit", ""], ["Pant", "Aniruddha", ""]]}, {"id": "2101.07612", "submitter": "Viraj Kulkarni", "authors": "Abhishek Shivdeo, Rohit Lokwani, Viraj Kulkarni, Amit Kharat,\n  Aniruddha Pant", "title": "Comparative Evaluation of 3D and 2D Deep Learning Techniques for\n  Semantic Segmentation in CT Scans", "comments": "9 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Image segmentation plays a pivotal role in several medical-imaging\napplications by assisting the segmentation of the regions of interest. Deep\nlearning-based approaches have been widely adopted for semantic segmentation of\nmedical data. In recent years, in addition to 2D deep learning architectures,\n3D architectures have been employed as the predictive algorithms for 3D medical\nimage data. In this paper, we propose a 3D stack-based deep learning technique\nfor segmenting manifestations of consolidation and ground-glass opacities in 3D\nComputed Tomography (CT) scans. We also present a comparison based on the\nsegmentation results, the contextual information retained, and the inference\ntime between this 3D technique and a traditional 2D deep learning technique. We\nalso define the area-plot, which represents the peculiar pattern observed in\nthe slice-wise areas of the pathology regions predicted by these deep learning\nmodels. In our exhaustive evaluation, 3D technique performs better than the 2D\ntechnique for the segmentation of CT scans. We get dice scores of 79% and 73%\nfor the 3D and the 2D techniques respectively. The 3D technique results in a 5X\nreduction in the inference time compared to the 2D technique. Results also show\nthat the area-plots predicted by the 3D model are more similar to the ground\ntruth than those predicted by the 2D model. We also show how increasing the\namount of contextual information retained during the training can improve the\n3D model's performance.\n", "versions": [{"version": "v1", "created": "Tue, 19 Jan 2021 13:23:43 GMT"}], "update_date": "2021-01-20", "authors_parsed": [["Shivdeo", "Abhishek", ""], ["Lokwani", "Rohit", ""], ["Kulkarni", "Viraj", ""], ["Kharat", "Amit", ""], ["Pant", "Aniruddha", ""]]}, {"id": "2101.07653", "submitter": "Sven Koehler", "authors": "Sven Koehler, Tarique Hussain, Zach Blair, Tyler Huffaker, Florian\n  Ritzmann, Animesh Tandon, Thomas Pickardt, Samir Sarikouch, Heiner Latus,\n  Gerald Greil, Ivo Wolf, Sandy Engelhardt", "title": "Unsupervised Domain Adaptation from Axial to Short-Axis Multi-Slice\n  Cardiac MR Images by Incorporating Pretrained Task Networks", "comments": "Accepted for IEEE Transaction on Medical Imaging (TMI) 2021 on\n  13.01.2021", "journal-ref": null, "doi": "10.1109/TMI.2021.3052972", "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Anisotropic multi-slice Cardiac Magnetic Resonance (CMR) Images are\nconventionally acquired in patient-specific short-axis (SAX) orientation. In\nspecific cardiovascular diseases that affect right ventricular (RV) morphology,\nacquisitions in standard axial (AX) orientation are preferred by some\ninvestigators, due to potential superiority in RV volume measurement for\ntreatment planning. Unfortunately, due to the rare occurrence of these\ndiseases, data in this domain is scarce. Recent research in deep learning-based\nmethods mainly focused on SAX CMR images and they had proven to be very\nsuccessful. In this work, we show that there is a considerable domain shift\nbetween AX and SAX images, and therefore, direct application of existing models\nyield sub-optimal results on AX samples. We propose a novel unsupervised domain\nadaptation approach, which uses task-related probabilities in an attention\nmechanism. Beyond that, cycle consistency is imposed on the learned\npatient-individual 3D rigid transformation to improve stability when\nautomatically re-sampling the AX images to SAX orientations. The network was\ntrained on 122 registered 3D AX-SAX CMR volume pairs from a multi-centric\npatient cohort. A mean 3D Dice of $0.86\\pm{0.06}$ for the left ventricle,\n$0.65\\pm{0.08}$ for the myocardium, and $0.77\\pm{0.10}$ for the right ventricle\ncould be achieved. This is an improvement of $25\\%$ in Dice for RV in\ncomparison to direct application on axial slices. To conclude, our pre-trained\ntask module has neither seen CMR images nor labels from the target domain, but\nis able to segment them after the domain gap is reduced. Code:\nhttps://github.com/Cardio-AI/3d-mri-domain-adaptation\n", "versions": [{"version": "v1", "created": "Tue, 19 Jan 2021 14:39:30 GMT"}, {"version": "v2", "created": "Wed, 20 Jan 2021 08:25:53 GMT"}], "update_date": "2021-01-21", "authors_parsed": [["Koehler", "Sven", ""], ["Hussain", "Tarique", ""], ["Blair", "Zach", ""], ["Huffaker", "Tyler", ""], ["Ritzmann", "Florian", ""], ["Tandon", "Animesh", ""], ["Pickardt", "Thomas", ""], ["Sarikouch", "Samir", ""], ["Latus", "Heiner", ""], ["Greil", "Gerald", ""], ["Wolf", "Ivo", ""], ["Engelhardt", "Sandy", ""]]}, {"id": "2101.07655", "submitter": "Clement Etienam", "authors": "Clement Etienam, Siying Shen, Edward J O'Dwyer and Joshua Sykes", "title": "A Novel Cluster Classify Regress Model Predictive Controller\n  Formulation; CCR-MPC", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SY cs.LG cs.SY", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this work, we develop a novel data-driven model predictive controller\nusing advanced techniques in the field of machine learning. The objective is to\nregulate control signals to adjust the desired internal room setpoint\ntemperature, affected indirectly by the external weather states. The\nmethodology involves developing a time-series machine learning model with\neither a Long Short Term Memory model (LSTM) or a Gradient Boosting Algorithm\n(XGboost), capable of forecasting this weather states for any desired time\nhorizon and concurrently optimising the control signals to the desired set\npoint. The supervised learning model for mapping the weather states together\nwith the control signals to the room temperature is constructed using a\npreviously developed methodology called Cluster Classify regress (CCR), which\nis similar in style but scales better to high dimensional dataset than the\nwell-known Mixture-of-Experts. The overall method called CCR-MPC involves a\ncombination of a time series model for weather states prediction, CCR for\nforwarding and any numerical optimisation method for solving the inverse\nproblem. Forward uncertainty quantification (Forward-UQ) leans towards the\nregression model in the CCR and is attainable using a Bayesian deep neural\nnetwork or a Gaussian process (GP). For this work, in the CCR modulation, we\nemploy K-means clustering for Clustering, XGboost classifier for Classification\nand 5th order polynomial regression for Regression. Inverse UQ can also be\nobtained by using an I-ES approach for solving the inverse problem or even the\nwell-known Markov chain Monte Carlo (MCMC) approach. The developed CCR-MPC is\nelegant, and as seen on the numerical experiments is able to optimise the\ncontroller to attain the desired setpoint temperature.\n", "versions": [{"version": "v1", "created": "Fri, 15 Jan 2021 12:14:54 GMT"}], "update_date": "2021-01-20", "authors_parsed": [["Etienam", "Clement", ""], ["Shen", "Siying", ""], ["O'Dwyer", "Edward J", ""], ["Sykes", "Joshua", ""]]}, {"id": "2101.07667", "submitter": "Martin Wistuba", "authors": "Martin Wistuba and Josif Grabocka", "title": "Few-Shot Bayesian Optimization with Deep Kernel Surrogates", "comments": "Published as a conference paper at ICLR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hyperparameter optimization (HPO) is a central pillar in the automation of\nmachine learning solutions and is mainly performed via Bayesian optimization,\nwhere a parametric surrogate is learned to approximate the black box response\nfunction (e.g. validation error). Unfortunately, evaluating the response\nfunction is computationally intensive. As a remedy, earlier work emphasizes the\nneed for transfer learning surrogates which learn to optimize hyperparameters\nfor an algorithm from other tasks. In contrast to previous work, we propose to\nrethink HPO as a few-shot learning problem in which we train a shared deep\nsurrogate model to quickly adapt (with few response evaluations) to the\nresponse function of a new task. We propose the use of a deep kernel network\nfor a Gaussian process surrogate that is meta-learned in an end-to-end fashion\nin order to jointly approximate the response functions of a collection of\ntraining data sets. As a result, the novel few-shot optimization of our deep\nkernel surrogate leads to new state-of-the-art results at HPO compared to\nseveral recent methods on diverse metadata sets.\n", "versions": [{"version": "v1", "created": "Tue, 19 Jan 2021 15:00:39 GMT"}], "update_date": "2021-01-20", "authors_parsed": [["Wistuba", "Martin", ""], ["Grabocka", "Josif", ""]]}, {"id": "2101.07669", "submitter": "Sebastian Garcia-Valencia", "authors": "Sebastian Garcia-Valencia, Alejandro Betancourt, Juan G.\n  Lalinde-Pulido", "title": "A framework to compare music generative models using automatic\n  evaluation metrics extended to rhythm", "comments": "arXiv admin note: substantial text overlap with arXiv:2012.01231", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.LG eess.AS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  To train a machine learning model is necessary to take numerous decisions\nabout many options for each process involved, in the field of sequence\ngeneration and more specifically of music composition, the nature of the\nproblem helps to narrow the options but at the same time, some other options\nappear for specific challenges. This paper takes the framework proposed in a\nprevious research that did not consider rhythm to make a series of design\ndecisions, then, rhythm support is added to evaluate the performance of two RNN\nmemory cells in the creation of monophonic music. The model considers the\nhandling of music transposition and the framework evaluates the quality of the\ngenerated pieces using automatic quantitative metrics based on geometry which\nhave rhythm support added as well.\n", "versions": [{"version": "v1", "created": "Tue, 19 Jan 2021 15:04:46 GMT"}], "update_date": "2021-01-20", "authors_parsed": [["Garcia-Valencia", "Sebastian", ""], ["Betancourt", "Alejandro", ""], ["Lalinde-Pulido", "Juan G.", ""]]}, {"id": "2101.07671", "submitter": "Jun Chen", "authors": "Jun Chen, Haopeng Chen", "title": "Edge-Featured Graph Attention Network", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Lots of neural network architectures have been proposed to deal with learning\ntasks on graph-structured data. However, most of these models concentrate on\nonly node features during the learning process. The edge features, which\nusually play a similarly important role as the nodes, are often ignored or\nsimplified by these models. In this paper, we present edge-featured graph\nattention networks, namely EGATs, to extend the use of graph neural networks to\nthose tasks learning on graphs with both node and edge features. These models\ncan be regarded as extensions of graph attention networks (GATs). By reforming\nthe model structure and the learning process, the new models can accept node\nand edge features as inputs, incorporate the edge information into feature\nrepresentations, and iterate both node and edge features in a parallel but\nmutual way. The results demonstrate that our work is highly competitive against\nother node classification approaches, and can be well applied in edge-featured\ngraph learning tasks.\n", "versions": [{"version": "v1", "created": "Tue, 19 Jan 2021 15:08:12 GMT"}], "update_date": "2021-01-20", "authors_parsed": [["Chen", "Jun", ""], ["Chen", "Haopeng", ""]]}, {"id": "2101.07683", "submitter": "Kui Yang", "authors": "Kui Yang, Wenjing Zhao, Constantinos Antoniou", "title": "Utilizing Import Vector Machines to Identify Dangerous Pro-active\n  Traffic Conditions", "comments": "6 pages, 3 figures, 2020 IEEE 23rd International Conference on\n  Intelligent Transportation Systems (ITSC)", "journal-ref": "In 2020 IEEE 23rd International Conference on Intelligent\n  Transportation Systems (ITSC) (pp. 1-6). IEEE", "doi": "10.1109/ITSC45102.2020.9294284", "report-no": null, "categories": "stat.ML cs.LG stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Traffic accidents have been a severe issue in metropolises with the\ndevelopment of traffic flow. This paper explores the theory and application of\na recently developed machine learning technique, namely Import Vector Machines\n(IVMs), in real-time crash risk analysis, which is a hot topic to reduce\ntraffic accidents. Historical crash data and corresponding traffic data from\nShanghai Urban Expressway System were employed and matched. Traffic conditions\nare labelled as dangerous (i.e. probably leading to a crash) and safe (i.e. a\nnormal traffic condition) based on 5-minute measurements of average speed,\nvolume and occupancy. The IVM algorithm is trained to build the classifier and\nits performance is compared to the popular and successfully applied technique\nof Support Vector Machines (SVMs). The main findings indicate that IVMs could\nsuccessfully be employed in real-time identification of dangerous pro-active\ntraffic conditions. Furthermore, similar to the \"support points\" of the SVM,\nthe IVM model uses only a fraction of the training data to index kernel basis\nfunctions, typically a much smaller fraction than the SVM, and its\nclassification rates are similar to those of SVMs. This gives the IVM a\ncomputational advantage over the SVM, especially when the size of the training\ndata set is large.\n", "versions": [{"version": "v1", "created": "Tue, 19 Jan 2021 15:22:23 GMT"}], "update_date": "2021-01-20", "authors_parsed": [["Yang", "Kui", ""], ["Zhao", "Wenjing", ""], ["Antoniou", "Constantinos", ""]]}, {"id": "2101.07685", "submitter": "Mattia Setzu", "authors": "Mattia Setzu, Riccardo Guidotti, Anna Monreale, Franco Turini, Dino\n  Pedreschi, Fosca Giannotti", "title": "GLocalX -- From Local to Global Explanations of Black Box AI Models", "comments": "27 pages, 2 figures, submitted to \"Special Issue on: Explainable AI\n  (XAI) for Web-based Information Processing\"", "journal-ref": "Journal of Artificial Intelligence, Volume 294, May 2021, 103457", "doi": "10.1016/j.artint.2021.103457", "report-no": null, "categories": "cs.LG cs.AI", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Artificial Intelligence (AI) has come to prominence as one of the major\ncomponents of our society, with applications in most aspects of our lives. In\nthis field, complex and highly nonlinear machine learning models such as\nensemble models, deep neural networks, and Support Vector Machines have\nconsistently shown remarkable accuracy in solving complex tasks. Although\naccurate, AI models often are \"black boxes\" which we are not able to\nunderstand. Relying on these models has a multifaceted impact and raises\nsignificant concerns about their transparency. Applications in sensitive and\ncritical domains are a strong motivational factor in trying to understand the\nbehavior of black boxes. We propose to address this issue by providing an\ninterpretable layer on top of black box models by aggregating \"local\"\nexplanations. We present GLocalX, a \"local-first\" model agnostic explanation\nmethod. Starting from local explanations expressed in form of local decision\nrules, GLocalX iteratively generalizes them into global explanations by\nhierarchically aggregating them. Our goal is to learn accurate yet simple\ninterpretable models to emulate the given black box, and, if possible, replace\nit entirely. We validate GLocalX in a set of experiments in standard and\nconstrained settings with limited or no access to either data or local\nexplanations. Experiments show that GLocalX is able to accurately emulate\nseveral models with simple and small models, reaching state-of-the-art\nperformance against natively global solutions. Our findings show how it is\noften possible to achieve a high level of both accuracy and comprehensibility\nof classification models, even in complex domains with high-dimensional data,\nwithout necessarily trading one property for the other. This is a key\nrequirement for a trustworthy AI, necessary for adoption in high-stakes\ndecision making applications.\n", "versions": [{"version": "v1", "created": "Tue, 19 Jan 2021 15:26:09 GMT"}, {"version": "v2", "created": "Tue, 26 Jan 2021 11:26:16 GMT"}], "update_date": "2021-01-29", "authors_parsed": [["Setzu", "Mattia", ""], ["Guidotti", "Riccardo", ""], ["Monreale", "Anna", ""], ["Turini", "Franco", ""], ["Pedreschi", "Dino", ""], ["Giannotti", "Fosca", ""]]}, {"id": "2101.07706", "submitter": "Peng Jiang", "authors": "Peng Jiang, Masuma Akter Rumi", "title": "Communication-Efficient Sampling for Distributed Training of Graph\n  Convolutional Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Training Graph Convolutional Networks (GCNs) is expensive as it needs to\naggregate data recursively from neighboring nodes. To reduce the computation\noverhead, previous works have proposed various neighbor sampling methods that\nestimate the aggregation result based on a small number of sampled neighbors.\nAlthough these methods have successfully accelerated the training, they mainly\nfocus on the single-machine setting. As real-world graphs are large, training\nGCNs in distributed systems is desirable. However, we found that the existing\nneighbor sampling methods do not work well in a distributed setting.\nSpecifically, a naive implementation may incur a huge amount of communication\nof feature vectors among different machines. To address this problem, we\npropose a communication-efficient neighbor sampling method in this work. Our\nmain idea is to assign higher sampling probabilities to the local nodes so that\nremote nodes are accessed less frequently. We present an algorithm that\ndetermines the local sampling probabilities and makes sure our skewed neighbor\nsampling does not affect much the convergence of the training. Our experiments\nwith node classification benchmarks show that our method significantly reduces\nthe communication overhead for distributed GCN training with little accuracy\nloss.\n", "versions": [{"version": "v1", "created": "Tue, 19 Jan 2021 16:12:44 GMT"}], "update_date": "2021-01-20", "authors_parsed": [["Jiang", "Peng", ""], ["Rumi", "Masuma Akter", ""]]}, {"id": "2101.07713", "submitter": "Rafael Pires", "authors": "Rafael G. Pires, Daniel F. S. Santos, Marcos C.S. Santana, Claudio\n  F.G. Santos, Joao P. Papa", "title": "Image Denoising using Attention-Residual Convolutional Neural Networks", "comments": "Published in: 2020 33rd SIBGRAPI Conference on Graphics, Patterns and\n  Images (SIBGRAPI)", "journal-ref": null, "doi": "10.1109/SIBGRAPI51738.2020.00022", "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  During the image acquisition process, noise is usually added to the data\nmainly due to physical limitations of the acquisition sensor, and also\nregarding imprecisions during the data transmission and manipulation. In that\nsense, the resultant image needs to be processed to attenuate its noise without\nlosing details. Non-learning-based strategies such as filter-based and noise\nprior modeling have been adopted to solve the image denoising problem.\nNowadays, learning-based denoising techniques showed to be much more effective\nand flexible approaches, such as Residual Convolutional Neural Networks. Here,\nwe propose a new learning-based non-blind denoising technique named Attention\nResidual Convolutional Neural Network (ARCNN), and its extension to blind\ndenoising named Flexible Attention Residual Convolutional Neural Network\n(FARCNN). The proposed methods try to learn the underlying noise expectation\nusing an Attention-Residual mechanism. Experiments on public datasets corrupted\nby different levels of Gaussian and Poisson noise support the effectiveness of\nthe proposed approaches against some state-of-the-art image denoising methods.\nARCNN achieved an overall average PSNR results of around 0.44dB and 0.96dB for\nGaussian and Poisson denoising, respectively FARCNN presented very consistent\nresults, even with slightly worsen performance compared to ARCNN.\n", "versions": [{"version": "v1", "created": "Tue, 19 Jan 2021 16:37:57 GMT"}], "update_date": "2021-01-20", "authors_parsed": [["Pires", "Rafael G.", ""], ["Santos", "Daniel F. S.", ""], ["Santana", "Marcos C. S.", ""], ["Santos", "Claudio F. G.", ""], ["Papa", "Joao P.", ""]]}, {"id": "2101.07715", "submitter": "David Bouget", "authors": "David Bouget, Andr\\'e Pedersen, Sayied Abdol Mohieb Hosainey, Ole\n  Solheim, Ingerid Reinertsen", "title": "Meningioma segmentation in T1-weighted MRI leveraging global context and\n  attention mechanisms", "comments": "16 pages, 5 figures, 3 tables. Submitted to Artificial Intelligence\n  in Medicine", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Meningiomas are the most common type of primary brain tumor, accounting for\napproximately 30% of all brain tumors. A substantial number of these tumors are\nnever surgically removed but rather monitored over time. Automatic and precise\nmeningioma segmentation is therefore beneficial to enable reliable growth\nestimation and patient-specific treatment planning. In this study, we propose\nthe inclusion of attention mechanisms over a U-Net architecture: (i)\nAttention-gated U-Net (AGUNet) and (ii) Dual Attention U-Net (DAUNet), using a\n3D MRI volume as input. Attention has the potential to leverage the global\ncontext and identify features' relationships across the entire volume. To limit\nspatial resolution degradation and loss of detail inherent to encoder-decoder\narchitectures, we studied the impact of multi-scale input and deep supervision\ncomponents. The proposed architectures are trainable end-to-end and each\nconcept can be seamlessly disabled for ablation studies. The validation studies\nwere performed using a 5-fold cross validation over 600 T1-weighted MRI volumes\nfrom St. Olavs University Hospital, Trondheim, Norway. For the best performing\narchitecture, an average Dice score of 81.6% was reached for an F1-score of\n95.6%. With an almost perfect precision of 98%, meningiomas smaller than 3ml\nwere occasionally missed hence reaching an overall recall of 93%. Leveraging\nglobal context from a 3D MRI volume provided the best performances, even if the\nnative volume resolution could not be processed directly. Overall, near-perfect\ndetection was achieved for meningiomas larger than 3ml which is relevant for\nclinical use. In the future, the use of multi-scale designs and refinement\nnetworks should be further investigated to improve the performance. A larger\nnumber of cases with meningiomas below 3ml might also be needed to improve the\nperformance for the smallest tumors.\n", "versions": [{"version": "v1", "created": "Tue, 19 Jan 2021 16:40:53 GMT"}], "update_date": "2021-01-20", "authors_parsed": [["Bouget", "David", ""], ["Pedersen", "Andr\u00e9", ""], ["Hosainey", "Sayied Abdol Mohieb", ""], ["Solheim", "Ole", ""], ["Reinertsen", "Ingerid", ""]]}, {"id": "2101.07717", "submitter": "S M Raju", "authors": "Sheikh Md Hanif Hossain, S M Raju and Amelia Ritahani Ismail", "title": "Predicting Pneumonia and Region Detection from X-Ray Images using Deep\n  Neural Network", "comments": "5 figures, 4 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Biomedical images are increasing drastically. Along the way, many machine\nlearning algorithms have been proposed to predict and identify various kinds of\ndiseases. One such disease is Pneumonia which is an infection caused by both\nbacteria and viruses through the inflammation of a person's lung air sacs. In\nthis paper, an algorithm was proposed that receives x-ray images as input and\nverifies whether this patient is infected by Pneumonia as well as specific\nregion of the lungs that the inflammation has occurred at. The algorithm is\nbased on the transfer learning mechanism where pre-trained ResNet-50\n(Convolutional Neural Network) was used followed by some custom layer for\nmaking the prediction. The model has achieved an accuracy of 90.6 percent which\nconfirms that the model is effective and can be implemented for the detection\nof Pneumonia in patients. Furthermore, a class activation map is used for the\ndetection of the infected region in the lungs. Also, PneuNet was developed so\nthat users can access more easily and use the services.\n", "versions": [{"version": "v1", "created": "Tue, 19 Jan 2021 16:43:05 GMT"}], "update_date": "2021-01-20", "authors_parsed": [["Hossain", "Sheikh Md Hanif", ""], ["Raju", "S M", ""], ["Ismail", "Amelia Ritahani", ""]]}, {"id": "2101.07721", "submitter": "Simon Bohlender", "authors": "Simon Bohlender, Ilkay Oksuz, Anirban Mukhopadhyay", "title": "A survey on shape-constraint deep learning for medical image\n  segmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Since the advent of U-Net, fully convolutional deep neural networks and its\nmany variants have completely changed the modern landscape of deep learning\nbased medical image segmentation. However, the over dependence of these methods\non pixel level classification and regression has been identified early on as a\nproblem. Especially when trained on medical databases with sparse available\nannotation, these methods are prone to generate segmentation artifacts such as\nfragmented structures, topological inconsistencies and islands of pixel. These\nartefacts are especially problematic in medical imaging since segmentation is\nalmost always a pre-processing step for some downstream evaluation. The range\nof possible downstream evaluations is rather big, for example surgical\nplanning, visualization, shape analysis, prognosis, treatment planning etc.\nHowever, one common thread across all these downstream tasks is the demand of\nanatomical consistency. To ensure the segmentation result is anatomically\nconsistent, approaches based on Markov/ Conditional Random Fields, Statistical\nShape Models are becoming increasingly popular over the past 5 years. In this\nreview paper, a broad overview of recent literature on bringing anatomical\nconstraints for medical image segmentation is given, the shortcomings and\nopportunities of the proposed methods are thoroughly discussed and potential\nfuture work is elaborated. We review the most relevant papers published until\nthe submission date. For quick access, important details such as the underlying\nmethod, datasets and performance are tabulated.\n", "versions": [{"version": "v1", "created": "Tue, 19 Jan 2021 16:52:10 GMT"}], "update_date": "2021-01-20", "authors_parsed": [["Bohlender", "Simon", ""], ["Oksuz", "Ilkay", ""], ["Mukhopadhyay", "Anirban", ""]]}, {"id": "2101.07730", "submitter": "Junteng Jia", "authors": "Junteng Jia and Austin R. Benson", "title": "A Unifying Generative Model for Graph Learning Algorithms: Label\n  Propagation, Graph Convolutions, and Combinations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Semi-supervised learning on graphs is a widely applicable problem in network\nscience and machine learning. Two standard algorithms -- label propagation and\ngraph neural networks -- both operate by repeatedly passing information along\nedges, the former by passing labels and the latter by passing node features,\nmodulated by neural networks. These two types of algorithms have largely\ndeveloped separately, and there is little understanding about the structure of\nnetwork data that would make one of these approaches work particularly well\ncompared to the other or when the approaches can be meaningfully combined.\nHere, we develop a Markov random field model for the data generation process of\nnode attributes, based on correlations of attributes on and between vertices,\nthat motivates and unifies these algorithmic approaches. We show that label\npropagation, a linearized graph convolutional network, and their combination\ncan all be derived as conditional expectations under our model, when\nconditioning on different attributes. In addition, the data model highlights\ndeficiencies in existing graph neural networks (while producing new algorithmic\nsolutions), serves as a rigorous statistical framework for understanding graph\nlearning issues such as over-smoothing, creates a testbed for evaluating\ninductive learning performance, and provides a way to sample graphs attributes\nthat resemble empirical data. We also find that a new algorithm derived from\nour data generation model, which we call a Linear Graph Convolution, performs\nextremely well in practice on empirical data, and provide theoretical\njustification for why this is the case.\n", "versions": [{"version": "v1", "created": "Tue, 19 Jan 2021 17:07:08 GMT"}, {"version": "v2", "created": "Sat, 30 Jan 2021 00:57:35 GMT"}], "update_date": "2021-02-02", "authors_parsed": [["Jia", "Junteng", ""], ["Benson", "Austin R.", ""]]}, {"id": "2101.07731", "submitter": "Daniel Shen", "authors": "Daniel Shen, Min Chi", "title": "TC-DTW: Accelerating Multivariate Dynamic Time Warping Through Triangle\n  Inequality and Point Clustering", "comments": null, "journal-ref": null, "doi": null, "report-no": "North Carolina State University TR-2021-2", "categories": "cs.LG cs.AI cs.DB", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Dynamic time warping (DTW) plays an important role in analytics on time\nseries. Despite the large body of research on speeding up univariate DTW, the\nmethod for multivariate DTW has not been improved much in the last two decades.\nThe most popular algorithm used today is still the one developed seventeen\nyears ago. This paper presents a solution that, as far as we know, for the\nfirst time consistently outperforms the classic multivariate DTW algorithm\nacross dataset sizes, series lengths, data dimensions, temporal window sizes,\nand machines. The new solution, named TC-DTW, introduces Triangle Inequality\nand Point Clustering into the algorithm design on lower bound calculations for\nmultivariate DTW. In experiments on DTW-based nearest neighbor finding, the new\nsolution avoids as much as 98% (60% average) DTW distance calculations and\nyields as much as 25X (7.5X average) speedups.\n", "versions": [{"version": "v1", "created": "Fri, 15 Jan 2021 16:38:28 GMT"}, {"version": "v2", "created": "Wed, 20 Jan 2021 02:55:15 GMT"}], "update_date": "2021-01-21", "authors_parsed": [["Shen", "Daniel", ""], ["Chi", "Min", ""]]}, {"id": "2101.07732", "submitter": "Ruocheng Guo", "authors": "Ruocheng Guo, Pengchuan Zhang, Hao Liu, Emre Kiciman", "title": "Out-of-distribution Prediction with Invariant Risk Minimization: The\n  Limitation and An Effective Fix", "comments": "22 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work considers the out-of-distribution (OOD) prediction problem where\n(1)~the training data are from multiple domains and (2)~the test domain is\nunseen in the training. DNNs fail in OOD prediction because they are prone to\npick up spurious correlations. Recently, Invariant Risk Minimization (IRM) is\nproposed to address this issue. Its effectiveness has been demonstrated in the\ncolored MNIST experiment. Nevertheless, we find that the performance of IRM can\nbe dramatically degraded under \\emph{strong $\\Lambda$ spuriousness} -- when the\nspurious correlation between the spurious features and the class label is\nstrong due to the strong causal influence of their common cause, the domain\nlabel, on both of them (see Fig. 1). In this work, we try to answer the\nquestions: why does IRM fail in the aforementioned setting? Why does IRM work\nfor the original colored MNIST dataset? How can we fix this problem of IRM?\nThen, we propose a simple and effective approach to fix the problem of IRM. We\ncombine IRM with conditional distribution matching to avoid a specific type of\nspurious correlation under strong $\\Lambda$ spuriousness. Empirically, we\ndesign a series of semi synthetic datasets -- the colored MNIST plus, which\nexposes the problems of IRM and demonstrates the efficacy of the proposed\nmethod.\n", "versions": [{"version": "v1", "created": "Sat, 16 Jan 2021 01:35:06 GMT"}, {"version": "v2", "created": "Mon, 22 Feb 2021 19:42:32 GMT"}], "update_date": "2021-02-24", "authors_parsed": [["Guo", "Ruocheng", ""], ["Zhang", "Pengchuan", ""], ["Liu", "Hao", ""], ["Kiciman", "Emre", ""]]}, {"id": "2101.07750", "submitter": "Hua Sun", "authors": "Yizhou Zhao, Hua Sun", "title": "Information Theoretic Secure Aggregation with User Dropouts", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.CR cs.LG math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the robust secure aggregation problem, a server wishes to learn and only\nlearn the sum of the inputs of a number of users while some users may drop out\n(i.e., may not respond). The identity of the dropped users is not known a\npriori and the server needs to securely recover the sum of the remaining\nsurviving users. We consider the following minimal two-round model of secure\naggregation. Over the first round, any set of no fewer than $U$ users out of\n$K$ users respond to the server and the server wants to learn the sum of the\ninputs of all responding users. The remaining users are viewed as dropped. Over\nthe second round, any set of no fewer than $U$ users of the surviving users\nrespond (i.e., dropouts are still possible over the second round) and from the\ninformation obtained from the surviving users over the two rounds, the server\ncan decode the desired sum. The security constraint is that even if the server\ncolludes with any $T$ users and the messages from the dropped users are\nreceived by the server (e.g., delayed packets), the server is not able to infer\nany additional information beyond the sum in the information theoretic sense.\nFor this information theoretic secure aggregation problem, we characterize the\noptimal communication cost. When $U \\leq T$, secure aggregation is not\nfeasible, and when $U > T$, to securely compute one symbol of the sum, the\nminimum number of symbols sent from each user to the server is $1$ over the\nfirst round, and $1/(U-T)$ over the second round.\n", "versions": [{"version": "v1", "created": "Tue, 19 Jan 2021 17:43:48 GMT"}], "update_date": "2021-01-20", "authors_parsed": [["Zhao", "Yizhou", ""], ["Sun", "Hua", ""]]}, {"id": "2101.07752", "submitter": "Asier Guti\\'errez-Fandi\\~no", "authors": "David P\\'erez-Fern\\'andez and Asier Guti\\'errez-Fandi\\~no and Jordi\n  Armengol-Estap\\'e and Marta Villegas", "title": "Characterizing and Measuring the Similarity of Neural Networks with\n  Persistent Homology", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.AT", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Characterizing the structural properties of neural networks is crucial yet\npoorly understood, and there are no well-established similarity measures\nbetween networks. In this work, we observe that neural networks can be\nrepresented as abstract simplicial complex and analyzed using their topological\n'fingerprints' via Persistent Homology (PH). We then describe a PH-based\nrepresentation proposed for characterizing and measuring similarity of neural\nnetworks. We empirically show the effectiveness of this representation as a\ndescriptor of different architectures in several datasets. This approach based\non Topological Data Analysis is a step towards better understanding neural\nnetworks and serves as a useful similarity measure.\n", "versions": [{"version": "v1", "created": "Tue, 19 Jan 2021 17:44:50 GMT"}, {"version": "v2", "created": "Thu, 21 Jan 2021 13:17:49 GMT"}, {"version": "v3", "created": "Mon, 31 May 2021 09:43:19 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["P\u00e9rez-Fern\u00e1ndez", "David", ""], ["Guti\u00e9rrez-Fandi\u00f1o", "Asier", ""], ["Armengol-Estap\u00e9", "Jordi", ""], ["Villegas", "Marta", ""]]}, {"id": "2101.07755", "submitter": "Tolga Birdal", "authors": "Tolga Birdal, Vladislav Golyanik, Christian Theobalt, Leonidas Guibas", "title": "Quantum Permutation Synchronization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "quant-ph cs.CV cs.ET cs.LG cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present QuantumSync, the first quantum algorithm for solving a\nsynchronization problem in the context of computer vision. In particular, we\nfocus on permutation synchronization which involves solving a non-convex\noptimization problem in discrete variables. We start by formulating\nsynchronization into a quadratic unconstrained binary optimization problem\n(QUBO). While such formulation respects the binary nature of the problem,\nensuring that the result is a set of permutations requires extra care. Hence,\nwe: (i) show how to insert permutation constraints into a QUBO problem and (ii)\nsolve the constrained QUBO problem on the current generation of the adiabatic\nquantum computers D-Wave. Thanks to the quantum annealing, we guarantee global\noptimality with high probability while sampling the energy landscape to yield\nconfidence estimates. Our proof-of-concepts realization on the adiabatic D-Wave\ncomputer demonstrates that quantum machines offer a promising way to solve the\nprevalent yet difficult synchronization problems.\n", "versions": [{"version": "v1", "created": "Tue, 19 Jan 2021 17:51:02 GMT"}], "update_date": "2021-01-20", "authors_parsed": [["Birdal", "Tolga", ""], ["Golyanik", "Vladislav", ""], ["Theobalt", "Christian", ""], ["Guibas", "Leonidas", ""]]}, {"id": "2101.07773", "submitter": "Balasubramaniam Srinivasan", "authors": "Balasubramaniam Srinivasan, Da Zheng, George Karypis", "title": "Learning over Families of Sets -- Hypergraph Representation Learning for\n  Higher Order Tasks", "comments": "Published as a conference paper at SIAM International Conference on\n  Data Mining(SDM 2021)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Graph representation learning has made major strides over the past decade.\nHowever, in many relational domains, the input data are not suited for simple\ngraph representations as the relationships between entities go beyond pairwise\ninteractions. In such cases, the relationships in the data are better\nrepresented as hyperedges (set of entities) of a non-uniform hypergraph. While\nthere have been works on principled methods for learning representations of\nnodes of a hypergraph, these approaches are limited in their applicability to\ntasks on non-uniform hypergraphs (hyperedges with different cardinalities). In\nthis work, we exploit the incidence structure to develop a hypergraph neural\nnetwork to learn provably expressive representations of variable sized\nhyperedges which preserve local-isomorphism in the line graph of the\nhypergraph, while also being invariant to permutations of its constituent\nvertices. Specifically, for a given vertex set, we propose frameworks for (1)\nhyperedge classification and (2) variable sized expansion of partially observed\nhyperedges which captures the higher order interactions among vertices and\nhyperedges. We evaluate performance on multiple real-world hypergraph datasets\nand demonstrate consistent, significant improvement in accuracy, over\nstate-of-the-art models.\n", "versions": [{"version": "v1", "created": "Tue, 19 Jan 2021 18:37:50 GMT"}], "update_date": "2021-01-20", "authors_parsed": [["Srinivasan", "Balasubramaniam", ""], ["Zheng", "Da", ""], ["Karypis", "George", ""]]}, {"id": "2101.07781", "submitter": "Cong Ma", "authors": "Cong Ma, Banghua Zhu, Jiantao Jiao, Martin J. Wainwright", "title": "Minimax Off-Policy Evaluation for Multi-Armed Bandits", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of off-policy evaluation in the multi-armed bandit model\nwith bounded rewards, and develop minimax rate-optimal procedures under three\nsettings. First, when the behavior policy is known, we show that the Switch\nestimator, a method that alternates between the plug-in and importance sampling\nestimators, is minimax rate-optimal for all sample sizes. Second, when the\nbehavior policy is unknown, we analyze performance in terms of the competitive\nratio, thereby revealing a fundamental gap between the settings of known and\nunknown behavior policies. When the behavior policy is unknown, any estimator\nmust have mean-squared error larger -- relative to the oracle estimator\nequipped with the knowledge of the behavior policy -- by a multiplicative\nfactor proportional to the support size of the target policy. Moreover, we\ndemonstrate that the plug-in approach achieves this worst-case competitive\nratio up to a logarithmic factor. Third, we initiate the study of the partial\nknowledge setting in which it is assumed that the minimum probability taken by\nthe behavior policy is known. We show that the plug-in estimator is optimal for\nrelatively large values of the minimum probability, but is sub-optimal when the\nminimum probability is low. In order to remedy this gap, we propose a new\nestimator based on approximation by Chebyshev polynomials that provably\nachieves the optimal estimation error. Numerical experiments on both simulated\nand real data corroborate our theoretical findings.\n", "versions": [{"version": "v1", "created": "Tue, 19 Jan 2021 18:55:29 GMT"}], "update_date": "2021-01-20", "authors_parsed": [["Ma", "Cong", ""], ["Zhu", "Banghua", ""], ["Jiao", "Jiantao", ""], ["Wainwright", "Martin J.", ""]]}, {"id": "2101.07824", "submitter": "Firuz Kamalov", "authors": "Firuz Kamalov, Aswani Cherukuri, Hana Sulieman, Fadi Thabtah, Akbar\n  Hossain", "title": "Machine learning applications for COVID-19: A state-of-the-art review", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The COVID-19 pandemic has galvanized the machine learning community to create\nnew solutions that can help in the fight against the virus. The body of\nliterature related to applications of machine learning and artificial\nintelligence to COVID-19 is constantly growing. The goal of this article is to\npresent the latest advances in machine learning research applied to COVID-19.\nWe cover four major areas of research: forecasting, medical diagnostics, drug\ndevelopment, and contact tracing. We review and analyze the most successful\nstate of the art studies. In contrast to other existing surveys on the subject,\nour article presents a high level overview of the current research that is\nsufficiently detailed to provide an informed insight.\n", "versions": [{"version": "v1", "created": "Tue, 19 Jan 2021 19:12:45 GMT"}], "update_date": "2021-01-21", "authors_parsed": [["Kamalov", "Firuz", ""], ["Cherukuri", "Aswani", ""], ["Sulieman", "Hana", ""], ["Thabtah", "Fadi", ""], ["Hossain", "Akbar", ""]]}, {"id": "2101.07825", "submitter": "Matteo Turchetta", "authors": "Christopher K\\\"onig, Matteo Turchetta, John Lygeros, Alisa Rupenyan,\n  Andreas Krause", "title": "Safe and Efficient Model-free Adaptive Control via Bayesian Optimization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SY cs.AI cs.LG cs.RO cs.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Adaptive control approaches yield high-performance controllers when a precise\nsystem model or suitable parametrizations of the controller are available.\nExisting data-driven approaches for adaptive control mostly augment standard\nmodel-based methods with additional information about uncertainties in the\ndynamics or about disturbances. In this work, we propose a purely data-driven,\nmodel-free approach for adaptive control. Tuning low-level controllers based\nsolely on system data raises concerns on the underlying algorithm safety and\ncomputational performance. Thus, our approach builds on GoOSE, an algorithm for\nsafe and sample-efficient Bayesian optimization. We introduce several\ncomputational and algorithmic modifications in GoOSE that enable its practical\nuse on a rotational motion system. We numerically demonstrate for several types\nof disturbances that our approach is sample efficient, outperforms constrained\nBayesian optimization in terms of safety, and achieves the performance optima\ncomputed by grid evaluation. We further demonstrate the proposed adaptive\ncontrol approach experimentally on a rotational motion system.\n", "versions": [{"version": "v1", "created": "Tue, 19 Jan 2021 19:15:00 GMT"}, {"version": "v2", "created": "Tue, 2 Mar 2021 13:26:34 GMT"}], "update_date": "2021-03-03", "authors_parsed": [["K\u00f6nig", "Christopher", ""], ["Turchetta", "Matteo", ""], ["Lygeros", "John", ""], ["Rupenyan", "Alisa", ""], ["Krause", "Andreas", ""]]}, {"id": "2101.07831", "submitter": "Flora Dellinger", "authors": "Flora Dellinger, Thomas Boulay, Diego Mendoza Barrenechea, Said\n  El-Hachimi, Isabelle Leang, Fabian B\\\"urger", "title": "Multi-Task Network Pruning and Embedded Optimization for Real-time\n  Deployment in ADAS", "comments": "Accepted at workshop on Machine Learning for Autonomous Driving\n  (NeurIPS 2020)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Camera-based Deep Learning algorithms are increasingly needed for perception\nin Automated Driving systems. However, constraints from the automotive industry\nchallenge the deployment of CNNs by imposing embedded systems with limited\ncomputational resources. In this paper, we propose an approach to embed a\nmulti-task CNN network under such conditions on a commercial prototype\nplatform, i.e. a low power System on Chip (SoC) processing four surround-view\nfisheye cameras at 10 FPS.\n  The first focus is on designing an efficient and compact multi-task network\narchitecture. Secondly, a pruning method is applied to compress the CNN,\nhelping to reduce the runtime and memory usage by a factor of 2 without\nlowering the performances significantly. Finally, several embedded optimization\ntechniques such as mixed-quantization format usage and efficient data transfers\nbetween different memory areas are proposed to ensure real-time execution and\navoid bandwidth bottlenecks. The approach is evaluated on the hardware\nplatform, considering embedded detection performances, runtime and memory\nbandwidth. Unlike most works from the literature that focus on classification\ntask, we aim here to study the effect of pruning and quantization on a compact\nmulti-task network with object detection, semantic segmentation and soiling\ndetection tasks.\n", "versions": [{"version": "v1", "created": "Tue, 19 Jan 2021 19:29:38 GMT"}], "update_date": "2021-01-22", "authors_parsed": [["Dellinger", "Flora", ""], ["Boulay", "Thomas", ""], ["Barrenechea", "Diego Mendoza", ""], ["El-Hachimi", "Said", ""], ["Leang", "Isabelle", ""], ["B\u00fcrger", "Fabian", ""]]}, {"id": "2101.07832", "submitter": "Xingyi Li", "authors": "Xingyi Li, Wenxuan Wu, Xiaoli Z. Fern, and Li Fuxin", "title": "The Devils in the Point Clouds: Studying the Robustness of Point Cloud\n  Convolutions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Recently, there has been a significant interest in performing convolution\nover irregularly sampled point clouds. Since point clouds are very different\nfrom regular raster images, it is imperative to study the generalization of the\nconvolution networks more closely, especially their robustness under variations\nin scale and rotations of the input data. This paper investigates different\nvariants of PointConv, a convolution network on point clouds, to examine their\nrobustness to input scale and rotation changes. Of the variants we explored,\ntwo are novel and generated significant improvements. The first is replacing\nthe multilayer perceptron based weight function with much simpler third degree\npolynomials, together with a Sobolev norm regularization. Secondly, for 3D\ndatasets, we derive a novel viewpoint-invariant descriptor by utilizing 3D\ngeometric properties as the input to PointConv, in addition to the regular 3D\ncoordinates. We have also explored choices of activation functions,\nneighborhood, and subsampling methods. Experiments are conducted on the 2D\nMNIST & CIFAR-10 datasets as well as the 3D SemanticKITTI & ScanNet datasets.\nResults reveal that on 2D, using third degree polynomials greatly improves\nPointConv's robustness to scale changes and rotations, even surpassing\ntraditional 2D CNNs for the MNIST dataset. On 3D datasets, the novel\nviewpoint-invariant descriptor significantly improves the performance as well\nas robustness of PointConv. We achieve the state-of-the-art semantic\nsegmentation performance on the SemanticKITTI dataset, as well as comparable\nperformance with the current highest framework on the ScanNet dataset among\npoint-based approaches.\n", "versions": [{"version": "v1", "created": "Tue, 19 Jan 2021 19:32:38 GMT"}, {"version": "v2", "created": "Thu, 28 Jan 2021 19:31:58 GMT"}], "update_date": "2021-02-01", "authors_parsed": [["Li", "Xingyi", ""], ["Wu", "Wenxuan", ""], ["Fern", "Xiaoli Z.", ""], ["Fuxin", "Li", ""]]}, {"id": "2101.07833", "submitter": "Melikasadat Emami", "authors": "Melikasadat Emami, Mojtaba Sahraee-Ardakan, Parthe Pandit, Sundeep\n  Rangan, Alyson K. Fletcher", "title": "Implicit Bias of Linear RNNs", "comments": "30 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE cs.SY eess.SY stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Contemporary wisdom based on empirical studies suggests that standard\nrecurrent neural networks (RNNs) do not perform well on tasks requiring\nlong-term memory. However, precise reasoning for this behavior is still\nunknown. This paper provides a rigorous explanation of this property in the\nspecial case of linear RNNs. Although this work is limited to linear RNNs, even\nthese systems have traditionally been difficult to analyze due to their\nnon-linear parameterization. Using recently-developed kernel regime analysis,\nour main result shows that linear RNNs learned from random initializations are\nfunctionally equivalent to a certain weighted 1D-convolutional network.\nImportantly, the weightings in the equivalent model cause an implicit bias to\nelements with smaller time lags in the convolution and hence, shorter memory.\nThe degree of this bias depends on the variance of the transition kernel matrix\nat initialization and is related to the classic exploding and vanishing\ngradients problem. The theory is validated in both synthetic and real data\nexperiments.\n", "versions": [{"version": "v1", "created": "Tue, 19 Jan 2021 19:39:28 GMT"}], "update_date": "2021-01-21", "authors_parsed": [["Emami", "Melikasadat", ""], ["Sahraee-Ardakan", "Mojtaba", ""], ["Pandit", "Parthe", ""], ["Rangan", "Sundeep", ""], ["Fletcher", "Alyson K.", ""]]}, {"id": "2101.07844", "submitter": "Timothy Verstraeten", "authors": "Timothy Verstraeten, Pieter-Jan Daems, Eugenio Bargiacchi, Diederik M.\n  Roijers, Pieter J.K. Libin, Jan Helsen", "title": "Scalable Optimization for Wind Farm Control using Coordination Graphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.SY eess.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Wind farms are a crucial driver toward the generation of ecological and\nrenewable energy. Due to their rapid increase in capacity, contemporary wind\nfarms need to adhere to strict constraints on power output to ensure stability\nof the electricity grid. Specifically, a wind farm controller is required to\nmatch the farm's power production with a power demand imposed by the grid\noperator. This is a non-trivial optimization problem, as complex dependencies\nexist between the wind turbines. State-of-the-art wind farm control typically\nrelies on physics-based heuristics that fail to capture the full load spectrum\nthat defines a turbine's health status. When this is not taken into account,\nthe long-term viability of the farm's turbines is put at risk. Given the\ncomplex dependencies that determine a turbine's lifetime, learning a flexible\nand optimal control strategy requires a data-driven approach. However, as wind\nfarms are large-scale multi-agent systems, optimizing control strategies over\nthe full joint action space is intractable. We propose a new learning method\nfor wind farm control that leverages the sparse wind farm structure to\nfactorize the optimization problem. Using a Bayesian approach, based on\nmulti-agent Thompson sampling, we explore the factored joint action space for\nconfigurations that match the demand, while considering the lifetime of\nturbines. We apply our method to a grid-like wind farm layout, and evaluate\nconfigurations using a state-of-the-art wind flow simulator. Our results are\ncompetitive with a physics-based heuristic approach in terms of demand error,\nwhile, contrary to the heuristic, our method prolongs the lifetime of high-risk\nturbines.\n", "versions": [{"version": "v1", "created": "Tue, 19 Jan 2021 20:12:30 GMT"}], "update_date": "2021-01-21", "authors_parsed": [["Verstraeten", "Timothy", ""], ["Daems", "Pieter-Jan", ""], ["Bargiacchi", "Eugenio", ""], ["Roijers", "Diederik M.", ""], ["Libin", "Pieter J. K.", ""], ["Helsen", "Jan", ""]]}, {"id": "2101.07852", "submitter": "Mikhail Mekhedkin Meskhi", "authors": "Mikhail M. Meskhi, Adriano Rivolli, Rafael G. Mantovani, Ricardo\n  Vilalta", "title": "Learning Abstract Task Representations", "comments": null, "journal-ref": "AAAI Workshop on Meta-Learning 2021", "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  A proper form of data characterization can guide the process of\nlearning-algorithm selection and model-performance estimation. The field of\nmeta-learning has provided a rich body of work describing effective forms of\ndata characterization using different families of meta-features (statistical,\nmodel-based, information-theoretic, topological, etc.). In this paper, we start\nwith the abundant set of existing meta-features and propose a method to induce\nnew abstract meta-features as latent variables in a deep neural network. We\ndiscuss the pitfalls of using traditional meta-features directly and argue for\nthe importance of learning high-level task properties. We demonstrate our\nmethodology using a deep neural network as a feature extractor. We demonstrate\nthat 1) induced meta-models mapping abstract meta-features to generalization\nperformance outperform other methods by ~18% on average, and 2) abstract\nmeta-features attain high feature-relevance scores.\n", "versions": [{"version": "v1", "created": "Tue, 19 Jan 2021 20:31:02 GMT"}, {"version": "v2", "created": "Tue, 26 Jan 2021 17:43:34 GMT"}, {"version": "v3", "created": "Thu, 28 Jan 2021 20:07:20 GMT"}], "update_date": "2021-02-01", "authors_parsed": [["Meskhi", "Mikhail M.", ""], ["Rivolli", "Adriano", ""], ["Mantovani", "Rafael G.", ""], ["Vilalta", "Ricardo", ""]]}, {"id": "2101.07864", "submitter": "Chaeun Lee", "authors": "Chaeun Lee, Seyoung Kim", "title": "SEMULATOR: Emulating the Dynamics of Crossbar Array-based Analog Neural\n  System with Regression Neural Networks", "comments": "10 pages, 7 figures, preprint", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.ET cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As deep neural networks require tremendous amount of computation and memory,\nanalog computing with emerging memory devices is a promising alternative to\ndigital computing for edge devices. However, because of the increasing\nsimulation time for analog computing system, it has not been explored. To\novercome this issue, analytically approximated simulators are developed, but\nthese models are inaccurate and narrow down the options for peripheral circuits\nfor multiply-accumulate operation (MAC). In this sense, we propose a\nmethodology, SEMULATOR (SiMULATOR by Emulating the analog computing block)\nwhich uses a deep neural network to emulate the behavior of crossbar-based\nanalog computing system. With the proposed neural architecture, we\nexperimentally and theoretically shows that it emulates a MAC unit for neural\ncomputation. In addition, the simulation time is incomparably reduced when it\ncompared to the circuit simulators such as SPICE.\n", "versions": [{"version": "v1", "created": "Tue, 19 Jan 2021 21:08:33 GMT"}], "update_date": "2021-01-21", "authors_parsed": [["Lee", "Chaeun", ""], ["Kim", "Seyoung", ""]]}, {"id": "2101.07866", "submitter": "Weihan Zhang", "authors": "Weihan Zhang, Bryan Pogorelsky, Mark Loveland, Trevor Wolf", "title": "Classification of COVID-19 X-ray Images Using a Combination of Deep and\n  Handcrafted Features", "comments": "5 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Coronavirus Disease 2019 (COVID-19) demonstrated the need for accurate and\nfast diagnosis methods for emergent viral diseases. Soon after the emergence of\nCOVID-19, medical practitioners used X-ray and computed tomography (CT) images\nof patients' lungs to detect COVID-19. Machine learning methods are capable of\nimproving the identification accuracy of COVID-19 in X-ray and CT images,\ndelivering near real-time results, while alleviating the burden on medical\npractitioners. In this work, we demonstrate the efficacy of a support vector\nmachine (SVM) classifier, trained with a combination of deep convolutional and\nhandcrafted features extracted from X-ray chest scans. We use this combination\nof features to discriminate between healthy, common pneumonia, and COVID-19\npatients. The performance of the combined feature approach is compared with a\nstandard convolutional neural network (CNN) and the SVM trained with\nhandcrafted features. We find that combining the features in our novel\nframework improves the performance of the classification task compared to the\nindependent application of convolutional and handcrafted features.\nSpecifically, we achieve an accuracy of 0.988 in the classification task with\nour combined approach compared to 0.963 and 0.983 accuracy for the handcrafted\nfeatures with SVM and CNN respectively.\n", "versions": [{"version": "v1", "created": "Tue, 19 Jan 2021 21:09:46 GMT"}, {"version": "v2", "created": "Thu, 21 Jan 2021 17:21:01 GMT"}], "update_date": "2021-01-22", "authors_parsed": [["Zhang", "Weihan", ""], ["Pogorelsky", "Bryan", ""], ["Loveland", "Mark", ""], ["Wolf", "Trevor", ""]]}, {"id": "2101.07868", "submitter": "Jacob Schrum", "authors": "Kirby Steckel and Jacob Schrum", "title": "Illuminating the Space of Beatable Lode Runner Levels Produced By\n  Various Generative Adversarial Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generative Adversarial Networks (GANs) are capable of generating convincing\nimitations of elements from a training set, but the distribution of elements in\nthe training set affects to difficulty of properly training the GAN and the\nquality of the outputs it produces. This paper looks at six different GANs\ntrained on different subsets of data from the game Lode Runner. The quality\ndiversity algorithm MAP-Elites was used to explore the set of quality levels\nthat could be produced by each GAN, where quality was defined as being beatable\nand having the longest solution path possible. Interestingly, a GAN trained on\nonly 20 levels generated the largest set of diverse beatable levels while a GAN\ntrained on 150 levels generated the smallest set of diverse beatable levels,\nthus challenging the notion that more is always better when training GANs.\n", "versions": [{"version": "v1", "created": "Tue, 19 Jan 2021 21:41:42 GMT"}], "update_date": "2021-01-21", "authors_parsed": [["Steckel", "Kirby", ""], ["Schrum", "Jacob", ""]]}, {"id": "2101.07899", "submitter": "Fupin Yao", "authors": "Fupin Yao", "title": "Cross-domain few-shot learning with unlabelled data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Few shot learning aims to solve the data scarcity problem. If there is a\ndomain shift between the test set and the training set, their performance will\ndecrease a lot. This setting is called Cross-domain few-shot learning. However,\nthis is very challenging because the target domain is unseen during training.\nThus we propose a new setting some unlabelled data from the target domain is\nprovided, which can bridge the gap between the source domain and the target\ndomain. A benchmark for this setting is constructed using DomainNet\n\\cite{peng2018oment}. We come up with a self-supervised learning method to\nfully utilize the knowledge in the labeled training set and the unlabelled set.\nExtensive experiments show that our methods outperforms several baseline\nmethods by a large margin. We also carefully design an episodic training\npipeline which yields a significant performance boost.\n", "versions": [{"version": "v1", "created": "Tue, 19 Jan 2021 23:41:57 GMT"}], "update_date": "2021-01-21", "authors_parsed": [["Yao", "Fupin", ""]]}, {"id": "2101.07907", "submitter": "Sergio Casas", "authors": "Sergio Casas, Wenjie Luo, Raquel Urtasun", "title": "IntentNet: Learning to Predict Intention from Raw Sensor Data", "comments": "CoRL 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AI cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In order to plan a safe maneuver, self-driving vehicles need to understand\nthe intent of other traffic participants. We define intent as a combination of\ndiscrete high-level behaviors as well as continuous trajectories describing\nfuture motion. In this paper, we develop a one-stage detector and forecaster\nthat exploits both 3D point clouds produced by a LiDAR sensor as well as\ndynamic maps of the environment. Our multi-task model achieves better accuracy\nthan the respective separate modules while saving computation, which is\ncritical to reducing reaction time in self-driving applications.\n", "versions": [{"version": "v1", "created": "Wed, 20 Jan 2021 00:31:52 GMT"}], "update_date": "2021-01-21", "authors_parsed": [["Casas", "Sergio", ""], ["Luo", "Wenjie", ""], ["Urtasun", "Raquel", ""]]}, {"id": "2101.07914", "submitter": "Jiang Wenqain", "authors": "Wenqian Jiang, Junyang Jin", "title": "Intelligent Icing Detection Model of Wind Turbine Blades Based on SCADA\n  data", "comments": "10 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Diagnosis of ice accretion on wind turbine blades is all the time a hard nut\nto crack in condition monitoring of wind farms. Existing methods focus on\nmechanism analysis of icing process, deviation degree analysis of feature\nengineering. However, there have not been deep researches of neural networks\napplied in this field at present. Supervisory control and data acquisition\n(SCADA) makes it possible to train networks through continuously providing not\nonly operation parameters and performance parameters of wind turbines but also\nenvironmental parameters and operation modes. This paper explores the\npossibility that using convolutional neural networks (CNNs), generative\nadversarial networks (GANs) and domain adaption learning to establish\nintelligent diagnosis frameworks under different training scenarios.\nSpecifically, PGANC and PGANT are proposed for sufficient and non-sufficient\ntarget wind turbine labeled data, respectively. The basic idea is that we\nconsider a two-stage training with parallel GANs, which are aimed at capturing\nintrinsic features for normal and icing samples, followed by classification CNN\nor domain adaption module in various training cases. Model validation on three\nwind turbine SCADA data shows that two-stage training can effectively improve\nthe model performance. Besides, if there is no sufficient labeled data for a\ntarget turbine, which is an extremely common phenomenon in real industrial\npractices, the addition of domain adaption learning makes the trained model\nshow better performance. Overall, our proposed intelligent diagnosis frameworks\ncan achieve more accurate detection on the same wind turbine and more\ngeneralized capability on a new wind turbine, compared with other machine\nlearning models and conventional CNNs.\n", "versions": [{"version": "v1", "created": "Wed, 20 Jan 2021 00:46:52 GMT"}], "update_date": "2021-01-21", "authors_parsed": [["Jiang", "Wenqian", ""], ["Jin", "Junyang", ""]]}, {"id": "2101.07918", "submitter": "HongChien Yu", "authors": "HongChien Yu, Zhuyun Dai, Jamie Callan", "title": "PGT: Pseudo Relevance Feedback Using a Graph-Based Transformer", "comments": "Accepted at ECIR 2021 (short paper track)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most research on pseudo relevance feedback (PRF) has been done in vector\nspace and probabilistic retrieval models. This paper shows that\nTransformer-based rerankers can also benefit from the extra context that PRF\nprovides. It presents PGT, a graph-based Transformer that sparsifies attention\nbetween graph nodes to enable PRF while avoiding the high computational\ncomplexity of most Transformer architectures. Experiments show that PGT\nimproves upon non-PRF Transformer reranker, and it is at least as accurate as\nTransformer PRF models that use full attention, but with lower computational\ncosts.\n", "versions": [{"version": "v1", "created": "Wed, 20 Jan 2021 01:07:47 GMT"}], "update_date": "2021-01-21", "authors_parsed": [["Yu", "HongChien", ""], ["Dai", "Zhuyun", ""], ["Callan", "Jamie", ""]]}, {"id": "2101.07922", "submitter": "Valeriia Cherepanova", "authors": "Valeriia Cherepanova, Micah Goldblum, Harrison Foley, Shiyuan Duan,\n  John Dickerson, Gavin Taylor, Tom Goldstein", "title": "LowKey: Leveraging Adversarial Attacks to Protect Social Media Users\n  from Facial Recognition", "comments": "Published as a conference paper at ICLR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Facial recognition systems are increasingly deployed by private corporations,\ngovernment agencies, and contractors for consumer services and mass\nsurveillance programs alike. These systems are typically built by scraping\nsocial media profiles for user images. Adversarial perturbations have been\nproposed for bypassing facial recognition systems. However, existing methods\nfail on full-scale systems and commercial APIs. We develop our own adversarial\nfilter that accounts for the entire image processing pipeline and is\ndemonstrably effective against industrial-grade pipelines that include face\ndetection and large scale databases. Additionally, we release an easy-to-use\nwebtool that significantly degrades the accuracy of Amazon Rekognition and the\nMicrosoft Azure Face Recognition API, reducing the accuracy of each to below\n1%.\n", "versions": [{"version": "v1", "created": "Wed, 20 Jan 2021 01:40:06 GMT"}, {"version": "v2", "created": "Mon, 25 Jan 2021 04:23:22 GMT"}], "update_date": "2021-01-26", "authors_parsed": [["Cherepanova", "Valeriia", ""], ["Goldblum", "Micah", ""], ["Foley", "Harrison", ""], ["Duan", "Shiyuan", ""], ["Dickerson", "John", ""], ["Taylor", "Gavin", ""], ["Goldstein", "Tom", ""]]}, {"id": "2101.07937", "submitter": "Woong-Hee Lee", "authors": "Woong-Hee Lee, Mustafa Ozger, Ursula Challita, and Ki Won Sung", "title": "Noise Learning Based Denoising Autoencoder", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI eess.SP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This letter introduces a new denoiser that modifies the structure of\ndenoising autoencoder (DAE), namely noise learning based DAE (nlDAE). The\nproposed nlDAE learns the noise of the input data. Then, the denoising is\nperformed by subtracting the regenerated noise from the noisy input. Hence,\nnlDAE is more effective than DAE when the noise is simpler to regenerate than\nthe original data. To validate the performance of nlDAE, we provide three case\nstudies: signal restoration, symbol demodulation, and precise localization.\nNumerical results suggest that nlDAE requires smaller latent space dimension\nand smaller training dataset compared to DAE.\n", "versions": [{"version": "v1", "created": "Wed, 20 Jan 2021 02:45:13 GMT"}, {"version": "v2", "created": "Mon, 21 Jun 2021 10:13:31 GMT"}], "update_date": "2021-06-22", "authors_parsed": [["Lee", "Woong-Hee", ""], ["Ozger", "Mustafa", ""], ["Challita", "Ursula", ""], ["Sung", "Ki Won", ""]]}, {"id": "2101.07948", "submitter": "Ziheng Wang", "authors": "Ziheng Wang", "title": "SparseDNN: Fast Sparse Deep Learning Inference on CPUs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The last few years have seen gigantic leaps in algorithms and systems to\nsupport efficient deep learning inference. Pruning and quantization algorithms\ncan now consistently compress neural networks by an order of magnitude. For a\ncompressed neural network, a multitude of inference frameworks have been\ndesigned to maximize the performance of the target hardware. While we find\nmature support for quantized neural networks in production frameworks such as\nOpenVINO and MNN, support for pruned sparse neural networks is still lacking.\nTo tackle this challenge, we present SparseDNN, a sparse deep learning\ninference engine targeting CPUs. We present both kernel-level optimizations\nwith a sparse code generator to accelerate sparse operators and novel\nnetwork-level optimizations catering to sparse networks. We show that our\nsparse code generator can achieve significant speedups over state-of-the-art\nsparse and dense libraries. On end-to-end benchmarks such as Huggingface\npruneBERT, SparseDNN achieves up to 5x throughput improvement over dense\ninference with state-of-the-art OpenVINO. Open source library at:\nhttps://github.com/marsupialtail/sparsednn.\n", "versions": [{"version": "v1", "created": "Wed, 20 Jan 2021 03:27:35 GMT"}, {"version": "v2", "created": "Thu, 28 Jan 2021 03:45:54 GMT"}, {"version": "v3", "created": "Thu, 1 Jul 2021 20:23:23 GMT"}, {"version": "v4", "created": "Wed, 21 Jul 2021 01:30:07 GMT"}], "update_date": "2021-07-22", "authors_parsed": [["Wang", "Ziheng", ""]]}, {"id": "2101.07950", "submitter": "Anna Vaughan", "authors": "Anna Vaughan, Will Tebbutt, J.Scott Hosking and Richard E. Turner", "title": "Convolutional conditional neural processes for local climate downscaling", "comments": "26 pages, 12 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG physics.ao-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A new model is presented for multisite statistical downscaling of temperature\nand precipitation using convolutional conditional neural processes (convCNPs).\nConvCNPs are a recently developed class of models that allow deep learning\ntechniques to be applied to off-the-grid spatio-temporal data. This model has a\nsubstantial advantage over existing downscaling methods in that the trained\nmodel can be used to generate multisite predictions at an arbitrary set of\nlocations, regardless of the availability of training data. The convCNP model\nis shown to outperform an ensemble of existing downscaling techniques over\nEurope for both temperature and precipitation taken from the VALUE\nintercomparison project. The model also outperforms an approach that uses\nGaussian processes to interpolate single-site downscaling models at unseen\nlocations. Importantly, substantial improvement is seen in the representation\nof extreme precipitation events. These results indicate that the convCNP is a\nrobust downscaling model suitable for generating localised projections for use\nin climate impact studies, and motivates further research into applications of\ndeep learning techniques in statistical downscaling.\n", "versions": [{"version": "v1", "created": "Wed, 20 Jan 2021 03:45:21 GMT"}], "update_date": "2021-01-21", "authors_parsed": [["Vaughan", "Anna", ""], ["Tebbutt", "Will", ""], ["Hosking", "J. Scott", ""], ["Turner", "Richard E.", ""]]}, {"id": "2101.07956", "submitter": "Seung Won Min", "authors": "Seung Won Min, Kun Wu, Sitao Huang, Mert Hidayeto\\u{g}lu, Jinjun\n  Xiong, Eiman Ebrahimi, Deming Chen, Wen-mei Hwu", "title": "PyTorch-Direct: Enabling GPU Centric Data Access for Very Large Graph\n  Neural Network Training with Irregular Accesses", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the increasing adoption of graph neural networks (GNNs) in the machine\nlearning community, GPUs have become an essential tool to accelerate GNN\ntraining. However, training GNNs on very large graphs that do not fit in GPU\nmemory is still a challenging task. Unlike conventional neural networks,\nmini-batching input samples in GNNs requires complicated tasks such as\ntraversing neighboring nodes and gathering their feature values. While this\nprocess accounts for a significant portion of the training time, we find\nexisting GNN implementations using popular deep neural network (DNN) libraries\nsuch as PyTorch are limited to a CPU-centric approach for the entire data\npreparation step. This \"all-in-CPU\" approach has negative impact on the overall\nGNN training performance as it over-utilizes CPU resources and hinders GPU\nacceleration of GNN training. To overcome such limitations, we introduce\nPyTorch-Direct, which enables a GPU-centric data accessing paradigm for GNN\ntraining. In PyTorch-Direct, GPUs are capable of efficiently accessing\ncomplicated data structures in host memory directly without CPU intervention.\nOur microbenchmark and end-to-end GNN training results show that PyTorch-Direct\nreduces data transfer time by 47.1% on average and speeds up GNN training by up\nto 1.6x. Furthermore, by reducing CPU utilization, PyTorch-Direct also saves\nsystem power by 12.4% to 17.5% during training. To minimize programmer effort,\nwe introduce a new \"unified tensor\" type along with necessary changes to the\nPyTorch memory allocator, dispatch logic, and placement rules. As a result,\nusers need to change at most two lines of their PyTorch GNN training code for\neach tensor object to take advantage of PyTorch-Direct.\n", "versions": [{"version": "v1", "created": "Wed, 20 Jan 2021 04:24:39 GMT"}], "update_date": "2021-01-21", "authors_parsed": [["Min", "Seung Won", ""], ["Wu", "Kun", ""], ["Huang", "Sitao", ""], ["Hidayeto\u011flu", "Mert", ""], ["Xiong", "Jinjun", ""], ["Ebrahimi", "Eiman", ""], ["Chen", "Deming", ""], ["Hwu", "Wen-mei", ""]]}, {"id": "2101.07957", "submitter": "Kei Takemura", "authors": "Kei Takemura, Shinji Ito, Daisuke Hatano, Hanna Sumita, Takuro\n  Fukunaga, Naonori Kakimura, Ken-ichi Kawarabayashi", "title": "Near-Optimal Regret Bounds for Contextual Combinatorial Semi-Bandits\n  with Linear Payoff Functions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The contextual combinatorial semi-bandit problem with linear payoff functions\nis a decision-making problem in which a learner chooses a set of arms with the\nfeature vectors in each round under given constraints so as to maximize the sum\nof rewards of arms. Several existing algorithms have regret bounds that are\noptimal with respect to the number of rounds $T$. However, there is a gap of\n$\\tilde{O}(\\max(\\sqrt{d}, \\sqrt{k}))$ between the current best upper and lower\nbounds, where $d$ is the dimension of the feature vectors, $k$ is the number of\nthe chosen arms in a round, and $\\tilde{O}(\\cdot)$ ignores the logarithmic\nfactors. The dependence of $k$ and $d$ is of practical importance because $k$\nmay be larger than $T$ in real-world applications such as recommender systems.\nIn this paper, we fill the gap by improving the upper and lower bounds. More\nprecisely, we show that the C${}^2$UCB algorithm proposed by Qin, Chen, and Zhu\n(2014) has the optimal regret bound $\\tilde{O}(d\\sqrt{kT} + dk)$ for the\npartition matroid constraints. For general constraints, we propose an algorithm\nthat modifies the reward estimates of arms in the C${}^2$UCB algorithm and\ndemonstrate that it enjoys the optimal regret bound for a more general problem\nthat can take into account other objectives simultaneously. We also show that\nour technique would be applicable to related problems. Numerical experiments\nsupport our theoretical results and considerations.\n", "versions": [{"version": "v1", "created": "Wed, 20 Jan 2021 04:29:18 GMT"}, {"version": "v2", "created": "Sat, 27 Feb 2021 11:22:30 GMT"}], "update_date": "2021-03-02", "authors_parsed": [["Takemura", "Kei", ""], ["Ito", "Shinji", ""], ["Hatano", "Daisuke", ""], ["Sumita", "Hanna", ""], ["Fukunaga", "Takuro", ""], ["Kakimura", "Naonori", ""], ["Kawarabayashi", "Ken-ichi", ""]]}, {"id": "2101.07965", "submitter": "Jie Chen", "authors": "Veronika Thost, Jie Chen", "title": "Directed Acyclic Graph Neural Networks", "comments": "ICLR 2021. Code is available at https://github.com/vthost/DAGNN", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Graph-structured data ubiquitously appears in science and engineering. Graph\nneural networks (GNNs) are designed to exploit the relational inductive bias\nexhibited in graphs; they have been shown to outperform other forms of neural\nnetworks in scenarios where structure information supplements node features.\nThe most common GNN architecture aggregates information from neighborhoods\nbased on message passing. Its generality has made it broadly applicable. In\nthis paper, we focus on a special, yet widely used, type of graphs -- DAGs --\nand inject a stronger inductive bias -- partial ordering -- into the neural\nnetwork design. We propose the \\emph{directed acyclic graph neural network},\nDAGNN, an architecture that processes information according to the flow defined\nby the partial order. DAGNN can be considered a framework that entails earlier\nworks as special cases (e.g., models for trees and models updating node\nrepresentations recurrently), but we identify several crucial components that\nprior architectures lack. We perform comprehensive experiments, including\nablation studies, on representative DAG datasets (i.e., source code, neural\narchitectures, and probabilistic graphical models) and demonstrate the\nsuperiority of DAGNN over simpler DAG architectures as well as general graph\narchitectures.\n", "versions": [{"version": "v1", "created": "Wed, 20 Jan 2021 04:50:16 GMT"}, {"version": "v2", "created": "Fri, 22 Jan 2021 18:18:07 GMT"}, {"version": "v3", "created": "Tue, 2 Feb 2021 18:45:44 GMT"}], "update_date": "2021-02-03", "authors_parsed": [["Thost", "Veronika", ""], ["Chen", "Jie", ""]]}, {"id": "2101.07968", "submitter": "Shangming Cai", "authors": "Shangming Cai, Dongsheng Wang, Haixia Wang, Yongqiang Lyu, Guangquan\n  Xu, Xi Zheng and Athanasios V. Vasilakos", "title": "DynaComm: Accelerating Distributed CNN Training between Edges and Clouds\n  through Dynamic Communication Scheduling", "comments": "12 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To reduce uploading bandwidth and address privacy concerns, deep learning at\nthe network edge has been an emerging topic. Typically, edge devices\ncollaboratively train a shared model using real-time generated data through the\nParameter Server framework. Although all the edge devices can share the\ncomputing workloads, the distributed training processes over edge networks are\nstill time-consuming due to the parameters and gradients transmission\nprocedures between parameter servers and edge devices. Focusing on accelerating\ndistributed Convolutional Neural Networks (CNNs) training at the network edge,\nwe present DynaComm, a novel scheduler that dynamically decomposes each\ntransmission procedure into several segments to achieve optimal communications\nand computations overlapping during run-time. Through experiments, we verify\nthat DynaComm manages to achieve optimal scheduling for all cases compared to\ncompeting strategies while the model accuracy remains untouched.\n", "versions": [{"version": "v1", "created": "Wed, 20 Jan 2021 05:09:41 GMT"}], "update_date": "2021-01-21", "authors_parsed": [["Cai", "Shangming", ""], ["Wang", "Dongsheng", ""], ["Wang", "Haixia", ""], ["Lyu", "Yongqiang", ""], ["Xu", "Guangquan", ""], ["Zheng", "Xi", ""], ["Vasilakos", "Athanasios V.", ""]]}, {"id": "2101.07969", "submitter": "Zheng Liu", "authors": "Zheng Liu, Po-Ling Loh", "title": "Robust W-GAN-Based Estimation Under Wasserstein Contamination", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.IT cs.LG math.IT stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Robust estimation is an important problem in statistics which aims at\nproviding a reasonable estimator when the data-generating distribution lies\nwithin an appropriately defined ball around an uncontaminated distribution.\nAlthough minimax rates of estimation have been established in recent years,\nmany existing robust estimators with provably optimal convergence rates are\nalso computationally intractable. In this paper, we study several estimation\nproblems under a Wasserstein contamination model and present computationally\ntractable estimators motivated by generative adversarial networks (GANs).\nSpecifically, we analyze properties of Wasserstein GAN-based estimators for\nlocation estimation, covariance matrix estimation, and linear regression and\nshow that our proposed estimators are minimax optimal in many scenarios.\nFinally, we present numerical results which demonstrate the effectiveness of\nour estimators.\n", "versions": [{"version": "v1", "created": "Wed, 20 Jan 2021 05:15:16 GMT"}], "update_date": "2021-01-21", "authors_parsed": [["Liu", "Zheng", ""], ["Loh", "Po-Ling", ""]]}, {"id": "2101.07976", "submitter": "Dan Yang", "authors": "Dan Yang, Xin Peng, Yusheng Lu, Haojie Huang, Weimin Zhong", "title": "Representation Evaluation Block-based Teacher-Student Network for the\n  Industrial Quality-relevant Performance Modeling and Monitoring", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Quality-relevant fault detection plays an important role in industrial\nprocesses, while the current quality-related fault detection methods based on\nneural networks main concentrate on process-relevant variables and ignore\nquality-relevant variables, which restrict the application of process\nmonitoring. Therefore, in this paper, a fault detection scheme based on the\nimproved teacher-student network is proposed for quality-relevant fault\ndetection. In the traditional teacher-student network, as the features\ndifferences between the teacher network and the student network will cause\nperformance degradation on the student network, representation evaluation block\n(REB) is proposed to quantify the features differences between the teacher and\nthe student networks, and uncertainty modeling is used to add this difference\nin modeling process, which are beneficial to reduce the features differences\nand improve the performance of the student network. Accordingly, REB and\nuncertainty modeling is applied in the teacher-student network named as\nuncertainty modeling teacher-student uncertainty autoencoder (TSUAE). Then, the\nproposed TSUAE is applied to process monitoring, which can effectively detect\nfaults in the process-relevant subspace and quality-relevant subspace\nsimultaneously. The proposed TSUAE-based fault detection method is verified in\ntwo simulation experiments illustrating that it has satisfactory fault\ndetection performance compared to other fault detection methods.\n", "versions": [{"version": "v1", "created": "Wed, 20 Jan 2021 05:40:44 GMT"}], "update_date": "2021-01-21", "authors_parsed": [["Yang", "Dan", ""], ["Peng", "Xin", ""], ["Lu", "Yusheng", ""], ["Huang", "Haojie", ""], ["Zhong", "Weimin", ""]]}, {"id": "2101.08001", "submitter": "Siyi Hu", "authors": "Siyi Hu, Fengda Zhu, Xiaojun Chang, Xiaodan Liang", "title": "UPDeT: Universal Multi-agent Reinforcement Learning via Policy\n  Decoupling with Transformers", "comments": "15 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent advances in multi-agent reinforcement learning have been largely\nlimited in training one model from scratch for every new task. The limitation\nis due to the restricted model architecture related to fixed input and output\ndimensions. This hinders the experience accumulation and transfer of the\nlearned agent over tasks with diverse levels of difficulty (e.g. 3 vs 3 or 5 vs\n6 multi-agent games). In this paper, we make the first attempt to explore a\nuniversal multi-agent reinforcement learning pipeline, designing one single\narchitecture to fit tasks with the requirement of different observation and\naction configurations. Unlike previous RNN-based models, we utilize a\ntransformer-based model to generate a flexible policy by decoupling the policy\ndistribution from the intertwined input observation with an importance weight\nmeasured by the merits of the self-attention mechanism. Compared to a standard\ntransformer block, the proposed model, named as Universal Policy Decoupling\nTransformer (UPDeT), further relaxes the action restriction and makes the\nmulti-agent task's decision process more explainable. UPDeT is general enough\nto be plugged into any multi-agent reinforcement learning pipeline and equip\nthem with strong generalization abilities that enables the handling of multiple\ntasks at a time. Extensive experiments on large-scale SMAC multi-agent\ncompetitive games demonstrate that the proposed UPDeT-based multi-agent\nreinforcement learning achieves significant results relative to\nstate-of-the-art approaches, demonstrating advantageous transfer capability in\nterms of both performance and training speed (10 times faster).\n", "versions": [{"version": "v1", "created": "Wed, 20 Jan 2021 07:24:24 GMT"}, {"version": "v2", "created": "Fri, 22 Jan 2021 05:12:07 GMT"}, {"version": "v3", "created": "Sun, 7 Feb 2021 10:28:41 GMT"}], "update_date": "2021-02-09", "authors_parsed": [["Hu", "Siyi", ""], ["Zhu", "Fengda", ""], ["Chang", "Xiaojun", ""], ["Liang", "Xiaodan", ""]]}, {"id": "2101.08007", "submitter": "Jose M. Pe\\~na", "authors": "Jose M. Pe\\~na", "title": "On the Non-Monotonicity of a Non-Differentially Mismeasured Binary\n  Confounder", "comments": "arXiv admin note: text overlap with arXiv:2005.13245", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Suppose that we are interested in the average causal effect of a binary\ntreatment on an outcome when this relationship is confounded by a binary\nconfounder. Suppose that the confounder is unobserved but a non-differential\nbinary proxy of it is observed. We identify conditions under which adjusting\nfor the proxy comes closer to the incomputable true average causal effect than\nnot adjusting at all. Unlike other works, we do not assume that the average\ncausal effect of the confounder on the outcome is in the same direction among\ntreated and untreated.\n", "versions": [{"version": "v1", "created": "Wed, 20 Jan 2021 07:42:54 GMT"}, {"version": "v2", "created": "Sat, 23 Jan 2021 16:04:50 GMT"}, {"version": "v3", "created": "Thu, 28 Jan 2021 09:02:05 GMT"}], "update_date": "2021-01-29", "authors_parsed": [["Pe\u00f1a", "Jose M.", ""]]}, {"id": "2101.08013", "submitter": "Quoc-Viet Pham", "authors": "Prabadevi B, Quoc-Viet Pham, Madhusanka Liyanage, N Deepa, Mounik\n  VVSS, Shivani Reddy, Praveen Kumar Reddy Maddikunta, Neelu Khare, Thippa\n  Reddy Gadekallu, Won-Joo Hwang", "title": "Deep Learning for Intelligent Demand Response and Smart Grids: A\n  Comprehensive Survey", "comments": "This work has been submitted for possible publication. Any comments\n  and suggestions are appreciated", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.SY eess.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Electricity is one of the mandatory commodities for mankind today. To address\nchallenges and issues in the transmission of electricity through the\ntraditional grid, the concepts of smart grids and demand response have been\ndeveloped. In such systems, a large amount of data is generated daily from\nvarious sources such as power generation (e.g., wind turbines), transmission\nand distribution (microgrids and fault detectors), load management (smart\nmeters and smart electric appliances). Thanks to recent advancements in big\ndata and computing technologies, Deep Learning (DL) can be leveraged to learn\nthe patterns from the generated data and predict the demand for electricity and\npeak hours. Motivated by the advantages of deep learning in smart grids, this\npaper sets to provide a comprehensive survey on the application of DL for\nintelligent smart grids and demand response. Firstly, we present the\nfundamental of DL, smart grids, demand response, and the motivation behind the\nuse of DL. Secondly, we review the state-of-the-art applications of DL in smart\ngrids and demand response, including electric load forecasting, state\nestimation, energy theft detection, energy sharing and trading. Furthermore, we\nillustrate the practicality of DL via various use cases and projects. Finally,\nwe highlight the challenges presented in existing research works and highlight\nimportant issues and potential directions in the use of DL for smart grids and\ndemand response.\n", "versions": [{"version": "v1", "created": "Wed, 20 Jan 2021 08:07:41 GMT"}], "update_date": "2021-01-21", "authors_parsed": [["B", "Prabadevi", ""], ["Pham", "Quoc-Viet", ""], ["Liyanage", "Madhusanka", ""], ["Deepa", "N", ""], ["VVSS", "Mounik", ""], ["Reddy", "Shivani", ""], ["Maddikunta", "Praveen Kumar Reddy", ""], ["Khare", "Neelu", ""], ["Gadekallu", "Thippa Reddy", ""], ["Hwang", "Won-Joo", ""]]}, {"id": "2101.08030", "submitter": "Francesco Cartella", "authors": "Francesco Cartella, Orlando Anunciacao, Yuki Funabiki, Daisuke\n  Yamaguchi, Toru Akishita, Olivier Elshocht", "title": "Adversarial Attacks for Tabular Data: Application to Fraud Detection and\n  Imbalanced Data", "comments": "Will be published on Proceedings of the Workshop on Artificial\n  Intelligence Safety (SafeAI 2021) co-located with 35th AAAI Conference on\n  Artificial Intelligence (AAAI 2021)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Guaranteeing the security of transactional systems is a crucial priority of\nall institutions that process transactions, in order to protect their\nbusinesses against cyberattacks and fraudulent attempts. Adversarial attacks\nare novel techniques that, other than being proven to be effective to fool\nimage classification models, can also be applied to tabular data. Adversarial\nattacks aim at producing adversarial examples, in other words, slightly\nmodified inputs that induce the Artificial Intelligence (AI) system to return\nincorrect outputs that are advantageous for the attacker. In this paper we\nillustrate a novel approach to modify and adapt state-of-the-art algorithms to\nimbalanced tabular data, in the context of fraud detection. Experimental\nresults show that the proposed modifications lead to a perfect attack success\nrate, obtaining adversarial examples that are also less perceptible when\nanalyzed by humans. Moreover, when applied to a real-world production system,\nthe proposed techniques shows the possibility of posing a serious threat to the\nrobustness of advanced AI-based fraud detection procedures.\n", "versions": [{"version": "v1", "created": "Wed, 20 Jan 2021 08:58:29 GMT"}], "update_date": "2021-01-21", "authors_parsed": [["Cartella", "Francesco", ""], ["Anunciacao", "Orlando", ""], ["Funabiki", "Yuki", ""], ["Yamaguchi", "Daisuke", ""], ["Akishita", "Toru", ""], ["Elshocht", "Olivier", ""]]}, {"id": "2101.08032", "submitter": "Wanguang Yin", "authors": "Wanguang Yin, Zhengming Ma, Quanying Liu", "title": "Riemannian Manifold Optimization for Discriminant Subspace Learning", "comments": "13 pages, 4 figures, 6 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG eess.IV eess.SP", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Linear discriminant analysis (LDA) is a widely used algorithm in machine\nlearning to extract a low-dimensional representation of high-dimensional data,\nit features to find the orthogonal discriminant projection subspace by using\nthe Fisher discriminant criterion. However, the traditional Euclidean-based\nmethods for solving LDA are easily convergent to spurious local minima and\nhardly obtain an optimal solution. To address such a problem, in this paper, we\npropose a novel algorithm namely Riemannian-based discriminant analysis (RDA)\nfor subspace learning. In order to obtain an explicit solution, we transform\nthe traditional Euclidean-based methods to the Riemannian manifold space and\nuse the trust-region method to learn the discriminant projection subspace. We\ncompare the proposed algorithm to existing variants of LDA, as well as the\nunsupervised tensor decomposition methods on image classification tasks. The\nnumerical results suggest that RDA achieves state-of-the-art performance in\nclassification accuracy.\n", "versions": [{"version": "v1", "created": "Wed, 20 Jan 2021 09:13:34 GMT"}, {"version": "v2", "created": "Tue, 26 Jan 2021 07:17:29 GMT"}, {"version": "v3", "created": "Tue, 20 Jul 2021 02:37:14 GMT"}], "update_date": "2021-07-21", "authors_parsed": [["Yin", "Wanguang", ""], ["Ma", "Zhengming", ""], ["Liu", "Quanying", ""]]}, {"id": "2101.08039", "submitter": "Chang Liu", "authors": "Zhuqing Jiang, Chang Liu, Ya'nan Wang, Kai Li, Aidong Men, Haiying\n  Wang, Haiyong Luo", "title": "Bridge the Vision Gap from Field to Command: A Deep Learning Network\n  Enhancing Illumination and Details", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the goal of tuning up the brightness, low-light image enhancement enjoys\nnumerous applications, such as surveillance, remote sensing and computational\nphotography. Images captured under low-light conditions often suffer from poor\nvisibility and blur. Solely brightening the dark regions will inevitably\namplify the blur, thus may lead to detail loss. In this paper, we propose a\nsimple yet effective two-stream framework named NEID to tune up the brightness\nand enhance the details simultaneously without introducing many computational\ncosts. Precisely, the proposed method consists of three parts: Light\nEnhancement (LE), Detail Refinement (DR) and Feature Fusing (FF) module, which\ncan aggregate composite features oriented to multiple tasks based on channel\nattention mechanism. Extensive experiments conducted on several benchmark\ndatasets demonstrate the efficacy of our method and its superiority over\nstate-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Wed, 20 Jan 2021 09:39:57 GMT"}], "update_date": "2021-01-21", "authors_parsed": [["Jiang", "Zhuqing", ""], ["Liu", "Chang", ""], ["Wang", "Ya'nan", ""], ["Li", "Kai", ""], ["Men", "Aidong", ""], ["Wang", "Haiying", ""], ["Luo", "Haiyong", ""]]}, {"id": "2101.08049", "submitter": "Luka \\v{Z}nidari\\v{c}", "authors": "Luka \\v{Z}nidari\\v{c}, Gjorgji Nusev, Bertrand Morel, Julie Mougin,\n  {\\DJ}ani Juri\\v{c}i\\'c and Pavle Bo\\v{s}koski", "title": "Evaluating uncertainties in electrochemical impedance spectra of solid\n  oxide fuel cells", "comments": "28 pages, 18 figures. Submitted to: Applied Energy", "journal-ref": null, "doi": "10.1016/j.apenergy.2021.117101", "report-no": null, "categories": "stat.CO cs.LG stat.AP stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Electrochemical impedance spectroscopy (EIS) is a widely used tool for\ncharacterization of fuel cells and other electrochemical conversion systems.\nWhen applied to the on-line monitoring in the context of in-field applications,\nthe disturbances, drifts and sensor noise may cause severe distortions in the\nevaluated spectra, especially in the low-frequency part. Failure to ignore the\nrandom effects can result in misinterpreted spectra and, consequently, in\nmisleading diagnostic reasoning. This fact has not been often addressed in the\nresearch so far. In this paper, we propose an approach to the quantification of\nthe spectral uncertainty, which relies on evaluating the uncertainty of the\nequivalent circuit model (ECM). We apply the computationally efficient\nvariational Bayes (VB) method and compare the quality of the results with those\nobtained with the Markov chain Monte Carlo (MCMC) algorithm. Namely, MCMC\nalgorithm returns accurate distributions of the estimated model parameters,\nwhile VB approach provides the approximate distributions. By using simulated\nand real data we show that approximate results provided by VB approach,\nalthough slightly over-optimistic, are still close to the more realistic MCMC\nestimates. A great advantage of the VB method for online monitoring is low\ncomputational load, which is several orders of magnitude lower compared to\nMCMC. The performance of VB algorithm is demonstrated on a case of ECM\nparameters estimation in a 6 cell solid oxide fuel cell (SOFC) stack. The\ncomplete numerical implementation for recreating the results can be found at\nhttps://repo.ijs.si/lznidaric/variational-bayes-supplementary-material.\n", "versions": [{"version": "v1", "created": "Wed, 20 Jan 2021 10:07:32 GMT"}, {"version": "v2", "created": "Fri, 26 Mar 2021 12:22:32 GMT"}, {"version": "v3", "created": "Wed, 5 May 2021 11:35:23 GMT"}], "update_date": "2021-07-21", "authors_parsed": [["\u017dnidari\u010d", "Luka", ""], ["Nusev", "Gjorgji", ""], ["Morel", "Bertrand", ""], ["Mougin", "Julie", ""], ["Juri\u010di\u0107", "\u0110ani", ""], ["Bo\u0161koski", "Pavle", ""]]}, {"id": "2101.08052", "submitter": "Kimberley Timmins", "authors": "Kimberley M. Timmins, Irene C. van der Schaaf, Ynte M. Ruigrok,\n  Birgitta K. Velthuis, Hugo J. Kuijf", "title": "Variational Autoencoders with a Structural Similarity Loss in Time of\n  Flight MRAs", "comments": "SPIE Medical Imaging 2021", "journal-ref": null, "doi": "10.1117/12.2580705", "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Time-of-Flight Magnetic Resonance Angiographs (TOF-MRAs) enable visualization\nand analysis of cerebral arteries. This analysis may indicate normal variation\nof the configuration of the cerebrovascular system or vessel abnormalities,\nsuch as aneurysms. A model would be useful to represent normal cerebrovascular\nstructure and variabilities in a healthy population and to differentiate from\nabnormalities. Current anomaly detection using autoencoding convolutional\nneural networks usually use a voxelwise mean-error for optimization. We propose\noptimizing a variational-autoencoder (VAE) with structural similarity loss\n(SSIM) for TOF-MRA reconstruction. A patch-trained 2D fully-convolutional VAE\nwas optimized for TOF-MRA reconstruction by comparing vessel segmentations of\noriginal and reconstructed MRAs. The method was trained and tested on two\ndatasets: the IXI dataset, and a subset from the ADAM challenge. Both trained\nnetworks were tested on a dataset including subjects with aneurysms. We\ncompared VAE optimization with L2-loss and SSIM-loss. Performance was evaluated\nbetween original and reconstructed MRAs using mean square error, mean-SSIM,\npeak-signal-to-noise-ratio and dice similarity index (DSI) of segmented\nvessels. The L2-optimized VAE outperforms SSIM, with improved reconstruction\nmetrics and DSIs for both datasets. Optimization using SSIM performed best for\nvisual image quality, but with discrepancy in quantitative reconstruction and\nvascular segmentation. The larger, more diverse IXI dataset had overall better\nperformance. Reconstruction metrics, including SSIM, were lower for MRAs\nincluding aneurysms. A SSIM-optimized VAE improved the visual perceptive image\nquality of TOF-MRA reconstructions. A L2-optimized VAE performed best for\nTOF-MRA reconstruction, where the vascular segmentation is important. SSIM is a\npotential metric for anomaly detection of MRAs.\n", "versions": [{"version": "v1", "created": "Wed, 20 Jan 2021 10:13:57 GMT"}], "update_date": "2021-02-24", "authors_parsed": [["Timmins", "Kimberley M.", ""], ["van der Schaaf", "Irene C.", ""], ["Ruigrok", "Ynte M.", ""], ["Velthuis", "Birgitta K.", ""], ["Kuijf", "Hugo J.", ""]]}, {"id": "2101.08061", "submitter": "Eduardo C\\'esar Garrido-Merch\\'an", "authors": "Lucia Asencio-Mart\\'in, Eduardo C. Garrido-Merch\\'an", "title": "A Similarity Measure of Gaussian Process Predictive Distributions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Some scenarios require the computation of a predictive distribution of a new\nvalue evaluated on an objective function conditioned on previous observations.\nWe are interested on using a model that makes valid assumptions on the\nobjective function whose values we are trying to predict. Some of these\nassumptions may be smoothness or stationarity. Gaussian process (GPs) are\nprobabilistic models that can be interpreted as flexible distributions over\nfunctions. They encode the assumptions through covariance functions, making\nhypotheses about new data through a predictive distribution by being fitted to\nold observations. We can face the case where several GPs are used to model\ndifferent objective functions. GPs are non-parametric models whose complexity\nis cubic on the number of observations. A measure that represents how similar\nis one GP predictive distribution with respect to another would be useful to\nstop using one GP when they are modelling functions of the same input space. We\nare really inferring that two objective functions are correlated, so one GP is\nenough to model both of them by performing a transformation of the prediction\nof the other function in case of inverse correlation. We show empirical\nevidence in a set of synthetic and benchmark experiments that GPs predictive\ndistributions can be compared and that one of them is enough to predict two\ncorrelated functions in the same input space. This similarity metric could be\nextremely useful used to discard objectives in Bayesian many-objective\noptimization.\n", "versions": [{"version": "v1", "created": "Wed, 20 Jan 2021 10:52:48 GMT"}], "update_date": "2021-01-21", "authors_parsed": [["Asencio-Mart\u00edn", "Lucia", ""], ["Garrido-Merch\u00e1n", "Eduardo C.", ""]]}, {"id": "2101.08063", "submitter": "Benjamin Perret", "authors": "Benjamin Perret (LIGM), Jean Cousty (LIGM)", "title": "Component Tree Loss Function: Definition and Optimization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this article, we propose a method to design loss functions based on\ncomponent trees which can be optimized by gradient descent algorithms and which\nare therefore usable in conjunction with recent machine learning approaches\nsuch as neural networks. We show how the altitudes associated to the nodes of\nsuch hierarchical image representations can be differentiated with respect to\nthe image pixel values. This feature is used to design a generic loss function\nthat can select or discard image maxima based on various attributes such as\nextinction values. The possibilities of the proposed method are demonstrated on\nsimulated and real image filtering.\n", "versions": [{"version": "v1", "created": "Wed, 20 Jan 2021 10:55:37 GMT"}], "update_date": "2021-01-21", "authors_parsed": [["Perret", "Benjamin", "", "LIGM"], ["Cousty", "Jean", "", "LIGM"]]}, {"id": "2101.08074", "submitter": "Chao Yan", "authors": "Chao Yan, Xiaojia Xiang, Chang Wang, Zhen Lan", "title": "Flocking and Collision Avoidance for a Dynamic Squad of Fixed-Wing UAVs\n  Using Deep Reinforcement Learning", "comments": "Accepted for publication in the proceedings of the 2021 IEEE/RSJ\n  International Conference on Intelligent Robots and Systems (IROS 2021)", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SY cs.AI cs.LG cs.MA cs.SY", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Developing the flocking behavior for a dynamic squad of fixed-wing UAVs is\nstill a challenge due to kinematic complexity and environmental uncertainty. In\nthis paper, we deal with the decentralized flocking and collision avoidance\nproblem through deep reinforcement learning (DRL). Specifically, we formulate a\ndecentralized DRL-based decision making framework from the perspective of every\nfollower, where a collision avoidance mechanism is integrated into the flocking\ncontroller. Then, we propose a novel reinforcement learning algorithm PS-CACER\nfor training a shared control policy for all the followers. Besides, we design\na plug-n-play embedding module based on convolutional neural networks and the\nattention mechanism. As a result, the variable-length system state can be\nencoded into a fixed-length embedding vector, which makes the learned DRL\npolicy independent with the number and the order of followers. Finally,\nnumerical simulation results demonstrate the effectiveness of the proposed\nmethod, and the learned policies can be directly transferred to semi-physical\nsimulation without any parameter finetuning.\n", "versions": [{"version": "v1", "created": "Wed, 20 Jan 2021 11:23:35 GMT"}, {"version": "v2", "created": "Thu, 22 Jul 2021 11:37:13 GMT"}], "update_date": "2021-07-26", "authors_parsed": [["Yan", "Chao", ""], ["Xiang", "Xiaojia", ""], ["Wang", "Chang", ""], ["Lan", "Zhen", ""]]}, {"id": "2101.08095", "submitter": "Jesse Sigal", "authors": "Jesse Sigal", "title": "Automatic Differentiation via Effects and Handlers: An Implementation in\n  Frank", "comments": "Appeared as short paper in PEPM'21, see\n  https://www.youtube.com/watch?v=BmBSJFkfL2M for associated talk", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Automatic differentiation (AD) is an important family of algorithms which\nenables derivative based optimization. We show that AD can be simply\nimplemented with effects and handlers by doing so in the Frank language. By\nconsidering how our implementation behaves in Frank's operational semantics, we\nshow how our code performs the dynamic creation of programs during evaluation.\n", "versions": [{"version": "v1", "created": "Wed, 20 Jan 2021 12:34:25 GMT"}], "update_date": "2021-01-21", "authors_parsed": [["Sigal", "Jesse", ""]]}, {"id": "2101.08104", "submitter": "Till Schulz", "authors": "Till Hendrik Schulz, Tam\\'as Horv\\'ath, Pascal Welke, Stefan Wrobel", "title": "A Generalized Weisfeiler-Lehman Graph Kernel", "comments": "n/a", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Weisfeiler-Lehman graph kernels are among the most prevalent graph\nkernels due to their remarkable time complexity and predictive performance.\nTheir key concept is based on an implicit comparison of neighborhood\nrepresenting trees with respect to equality (i.e., isomorphism). This binary\nvalued comparison is, however, arguably too rigid for defining suitable\nsimilarity measures over graphs. To overcome this limitation, we propose a\ngeneralization of Weisfeiler-Lehman graph kernels which takes into account the\nsimilarity between trees rather than equality. We achieve this using a\nspecifically fitted variation of the well-known tree edit distance which can\nefficiently be calculated. We empirically show that our approach significantly\noutperforms state-of-the-art methods in terms of predictive performance on\ndatasets containing structurally more complex graphs beyond the typically\nconsidered molecular graphs.\n", "versions": [{"version": "v1", "created": "Wed, 20 Jan 2021 13:03:51 GMT"}], "update_date": "2021-01-21", "authors_parsed": [["Schulz", "Till Hendrik", ""], ["Horv\u00e1th", "Tam\u00e1s", ""], ["Welke", "Pascal", ""], ["Wrobel", "Stefan", ""]]}, {"id": "2101.08116", "submitter": "Francisco Ortin", "authors": "Javier Escalada (1), Ted Scully (2), Francisco Ortin (1 and 2) ((1)\n  University of Oviedo, (2) Cork Institute of Technology)", "title": "Improving type information inferred by decompilers with supervised\n  machine learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SE cs.LG cs.PL", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  In software reverse engineering, decompilation is the process of recovering\nsource code from binary files. Decompilers are used when it is necessary to\nunderstand or analyze software for which the source code is not available.\nAlthough existing decompilers commonly obtain source code with the same\nbehavior as the binaries, that source code is usually hard to interpret and\ncertainly differs from the original code written by the programmer. Massive\ncodebases could be used to build supervised machine learning models aimed at\nimproving existing decompilers. In this article, we build different\nclassification models capable of inferring the high-level type returned by\nfunctions, with significantly higher accuracy than existing decompilers. We\nautomatically instrument C source code to allow the association of binary\npatterns with their corresponding high-level constructs. A dataset is created\nwith a collection of real open-source applications plus a huge number of\nsynthetic programs. Our system is able to predict function return types with a\n79.1% F1-measure, whereas the best decompiler obtains a 30% F1-measure.\nMoreover, we document the binary patterns used by our classifier to allow their\naddition in the implementation of existing decompilers.\n", "versions": [{"version": "v1", "created": "Tue, 19 Jan 2021 11:45:46 GMT"}, {"version": "v2", "created": "Wed, 24 Feb 2021 11:01:27 GMT"}], "update_date": "2021-02-25", "authors_parsed": [["Escalada", "Javier", "", "1 and 2"], ["Scully", "Ted", "", "1 and 2"], ["Ortin", "Francisco", "", "1 and 2"]]}, {"id": "2101.08130", "submitter": "Alexander Stroh", "authors": "Matthias Schniewind, Alexander Stroh, Bradley P. Ladewig, Pascal\n  Friederich", "title": "Machine learning for rapid discovery of laminar flow channel wall\n  modifications that enhance heat transfer", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.flu-dyn cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The calculation of heat transfer in fluid flow in simple flat channels is a\nrelatively easy task for various simulations methods. However, once the channel\ngeometry becomes more complex, numerical simulations become a bottleneck in\noptimizing wall geometries. We present a combination of accurate numerical\nsimulations of arbitrary, non-flat channels and machine learning models\npredicting drag coefficient and Stanton number. We show that convolutional\nneural networks can accurately predict the target properties at a fraction of\nthe time of numerical simulations. We use the CNN models in a virtual\nhigh-throughput screening approach to explore a large number of possible,\nrandomly generated wall architectures. We find that S-shaped channel geometries\nare Pareto-optimal, a result which seems intuitive, but was not obvious before\nanalysing the data. The general approach is not only applicable to simple flow\nsetups as presented here, but can be extended to more complex tasks, such as\nmultiphase or even reactive unit operations in chemical engineering.\n", "versions": [{"version": "v1", "created": "Tue, 19 Jan 2021 16:14:02 GMT"}], "update_date": "2021-01-21", "authors_parsed": [["Schniewind", "Matthias", ""], ["Stroh", "Alexander", ""], ["Ladewig", "Bradley P.", ""], ["Friederich", "Pascal", ""]]}, {"id": "2101.08134", "submitter": "Mohamed Abdelfattah", "authors": "Mohamed S. Abdelfattah, Abhinav Mehrotra, {\\L}ukasz Dudziak, Nicholas\n  D. Lane", "title": "Zero-Cost Proxies for Lightweight NAS", "comments": "ICLR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.NE", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Neural Architecture Search (NAS) is quickly becoming the standard methodology\nto design neural network models. However, NAS is typically compute-intensive\nbecause multiple models need to be evaluated before choosing the best one. To\nreduce the computational power and time needed, a proxy task is often used for\nevaluating each model instead of full training. In this paper, we evaluate\nconventional reduced-training proxies and quantify how well they preserve\nranking between multiple models during search when compared with the rankings\nproduced by final trained accuracy. We propose a series of zero-cost proxies,\nbased on recent pruning literature, that use just a single minibatch of\ntraining data to compute a model's score. Our zero-cost proxies use 3 orders of\nmagnitude less computation but can match and even outperform conventional\nproxies. For example, Spearman's rank correlation coefficient between final\nvalidation accuracy and our best zero-cost proxy on NAS-Bench-201 is 0.82,\ncompared to 0.61 for EcoNAS (a recently proposed reduced-training proxy).\nFinally, we use these zero-cost proxies to enhance existing NAS search\nalgorithms such as random search, reinforcement learning, evolutionary search\nand predictor-based search. For all search methodologies and across three\ndifferent NAS datasets, we are able to significantly improve sample efficiency,\nand thereby decrease computation, by using our zero-cost proxies. For example\non NAS-Bench-101, we achieved the same accuracy 4$\\times$ quicker than the best\nprevious result. Our code is made public at:\nhttps://github.com/mohsaied/zero-cost-nas.\n", "versions": [{"version": "v1", "created": "Wed, 20 Jan 2021 13:59:52 GMT"}, {"version": "v2", "created": "Fri, 19 Mar 2021 10:43:12 GMT"}], "update_date": "2021-03-22", "authors_parsed": [["Abdelfattah", "Mohamed S.", ""], ["Mehrotra", "Abhinav", ""], ["Dudziak", "\u0141ukasz", ""], ["Lane", "Nicholas D.", ""]]}, {"id": "2101.08152", "submitter": "Daochen Zha", "authors": "Daochen Zha, Wenye Ma, Lei Yuan, Xia Hu, Ji Liu", "title": "Rank the Episodes: A Simple Approach for Exploration in\n  Procedurally-Generated Environments", "comments": "Accepted by ICLR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Exploration under sparse reward is a long-standing challenge of model-free\nreinforcement learning. The state-of-the-art methods address this challenge by\nintroducing intrinsic rewards to encourage exploration in novel states or\nuncertain environment dynamics. Unfortunately, methods based on intrinsic\nrewards often fall short in procedurally-generated environments, where a\ndifferent environment is generated in each episode so that the agent is not\nlikely to visit the same state more than once. Motivated by how humans\ndistinguish good exploration behaviors by looking into the entire episode, we\nintroduce RAPID, a simple yet effective episode-level exploration method for\nprocedurally-generated environments. RAPID regards each episode as a whole and\ngives an episodic exploration score from both per-episode and long-term views.\nThose highly scored episodes are treated as good exploration behaviors and are\nstored in a small ranking buffer. The agent then imitates the episodes in the\nbuffer to reproduce the past good exploration behaviors. We demonstrate our\nmethod on several procedurally-generated MiniGrid environments, a\nfirst-person-view 3D Maze navigation task from MiniWorld, and several sparse\nMuJoCo tasks. The results show that RAPID significantly outperforms the\nstate-of-the-art intrinsic reward strategies in terms of sample efficiency and\nfinal performance. The code is available at https://github.com/daochenzha/rapid\n", "versions": [{"version": "v1", "created": "Wed, 20 Jan 2021 14:22:01 GMT"}, {"version": "v2", "created": "Thu, 4 Feb 2021 15:48:12 GMT"}], "update_date": "2021-02-05", "authors_parsed": [["Zha", "Daochen", ""], ["Ma", "Wenye", ""], ["Yuan", "Lei", ""], ["Hu", "Xia", ""], ["Liu", "Ji", ""]]}, {"id": "2101.08156", "submitter": "Yuki Fujimoto", "authors": "Yuki Fujimoto, Kenji Fukushima, Koichi Murase", "title": "Extensive Studies of the Neutron Star Equation of State from the Deep\n  Learning Inference with the Observational Data Augmentation", "comments": "45 pages, 25 figures", "journal-ref": "JHEP 03 (2021) 273", "doi": "10.1007/JHEP03(2021)273", "report-no": null, "categories": "nucl-th astro-ph.HE astro-ph.IM cs.LG hep-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We discuss deep learning inference for the neutron star equation of state\n(EoS) using the real observational data of the mass and the radius. We make a\nquantitative comparison between the conventional polynomial regression and the\nneural network approach for the EoS parametrization. For our deep learning\nmethod to incorporate uncertainties in observation, we augment the training\ndata with noise fluctuations corresponding to observational uncertainties.\nDeduced EoSs can accommodate a weak first-order phase transition, and we make a\nhistogram for likely first-order regions. We also find that our observational\ndata augmentation has a byproduct to tame the overfitting behavior. To check\nthe performance improved by the data augmentation, we set up a toy model as the\nsimplest inference problem to recover a double-peaked function and monitor the\nvalidation loss. We conclude that the data augmentation could be a useful\ntechnique to evade the overfitting without tuning the neural network\narchitecture such as inserting the dropout.\n", "versions": [{"version": "v1", "created": "Wed, 20 Jan 2021 14:27:12 GMT"}], "update_date": "2021-06-14", "authors_parsed": [["Fujimoto", "Yuki", ""], ["Fukushima", "Kenji", ""], ["Murase", "Koichi", ""]]}, {"id": "2101.08167", "submitter": "Khaled Zaouk", "authors": "Khaled Zaouk, Fei Song, Chenghao Lyu and Yanlei Diao", "title": "Neural-based Modeling for Performance Tuning of Spark Data Analytics", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DB cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cloud data analytics has become an integral part of enterprise business\noperations for data-driven insight discovery. Performance modeling of cloud\ndata analytics is crucial for performance tuning and other critical operations\nin the cloud. Traditional modeling techniques fail to adapt to the high degree\nof diversity in workloads and system behaviors in this domain. In this paper,\nwe bring recent Deep Learning techniques to bear on the process of automated\nperformance modeling of cloud data analytics, with a focus on Spark data\nanalytics as representative workloads. At the core of our work is the notion of\nlearning workload embeddings (with a set of desired properties) to represent\nfundamental computational characteristics of different jobs, which enable\nperformance prediction when used together with job configurations that control\nresource allocation and other system knobs. Our work provides an in-depth study\nof different modeling choices that suit our requirements. Results of extensive\nexperiments reveal the strengths and limitations of different modeling methods,\nas well as superior performance of our best performing method over a\nstate-of-the-art modeling tool for cloud analytics.\n", "versions": [{"version": "v1", "created": "Wed, 20 Jan 2021 14:58:55 GMT"}], "update_date": "2021-01-21", "authors_parsed": [["Zaouk", "Khaled", ""], ["Song", "Fei", ""], ["Lyu", "Chenghao", ""], ["Diao", "Yanlei", ""]]}, {"id": "2101.08170", "submitter": "Qingyun Sun", "authors": "Qingyun Sun, Jianxin Li, Hao Peng, Jia Wu, Yuanxing Ning, Phillip S.\n  Yu, Lifang He", "title": "SUGAR: Subgraph Neural Network with Reinforcement Pooling and\n  Self-Supervised Mutual Information Mechanism", "comments": "Accepted by The Web Conference (WWW) 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Graph representation learning has attracted increasing research attention.\nHowever, most existing studies fuse all structural features and node attributes\nto provide an overarching view of graphs, neglecting finer substructures'\nsemantics, and suffering from interpretation enigmas. This paper presents a\nnovel hierarchical subgraph-level selection and embedding based graph neural\nnetwork for graph classification, namely SUGAR, to learn more discriminative\nsubgraph representations and respond in an explanatory way. SUGAR reconstructs\na sketched graph by extracting striking subgraphs as the representative part of\nthe original graph to reveal subgraph-level patterns. To adaptively select\nstriking subgraphs without prior knowledge, we develop a reinforcement pooling\nmechanism, which improves the generalization ability of the model. To\ndifferentiate subgraph representations among graphs, we present a\nself-supervised mutual information mechanism to encourage subgraph embedding to\nbe mindful of the global graph structural properties by maximizing their mutual\ninformation. Extensive experiments on six typical bioinformatics datasets\ndemonstrate a significant and consistent improvement in model quality with\ncompetitive performance and interpretability.\n", "versions": [{"version": "v1", "created": "Wed, 20 Jan 2021 15:06:16 GMT"}, {"version": "v2", "created": "Mon, 22 Mar 2021 08:57:10 GMT"}, {"version": "v3", "created": "Mon, 24 May 2021 06:08:47 GMT"}], "update_date": "2021-05-25", "authors_parsed": [["Sun", "Qingyun", ""], ["Li", "Jianxin", ""], ["Peng", "Hao", ""], ["Wu", "Jia", ""], ["Ning", "Yuanxing", ""], ["Yu", "Phillip S.", ""], ["He", "Lifang", ""]]}, {"id": "2101.08176", "submitter": "Gurtej Kanwar", "authors": "Michael S. Albergo, Denis Boyda, Daniel C. Hackett, Gurtej Kanwar,\n  Kyle Cranmer, S\\'ebastien Racani\\`ere, Danilo Jimenez Rezende, Phiala E.\n  Shanahan", "title": "Introduction to Normalizing Flows for Lattice Field Theory", "comments": "38 pages, 5 numbered figures, Jupyter notebook included as ancillary\n  file", "journal-ref": null, "doi": null, "report-no": "MIT-CTP/5272", "categories": "hep-lat cond-mat.stat-mech cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This notebook tutorial demonstrates a method for sampling Boltzmann\ndistributions of lattice field theories using a class of machine learning\nmodels known as normalizing flows. The ideas and approaches proposed in\narXiv:1904.12072, arXiv:2002.02428, and arXiv:2003.06413 are reviewed and a\nconcrete implementation of the framework is presented. We apply this framework\nto a lattice scalar field theory and to U(1) gauge theory, explicitly encoding\ngauge symmetries in the flow-based approach to the latter. This presentation is\nintended to be interactive and working with the attached Jupyter notebook is\nrecommended.\n", "versions": [{"version": "v1", "created": "Wed, 20 Jan 2021 15:16:28 GMT"}], "update_date": "2021-01-21", "authors_parsed": [["Albergo", "Michael S.", ""], ["Boyda", "Denis", ""], ["Hackett", "Daniel C.", ""], ["Kanwar", "Gurtej", ""], ["Cranmer", "Kyle", ""], ["Racani\u00e8re", "S\u00e9bastien", ""], ["Rezende", "Danilo Jimenez", ""], ["Shanahan", "Phiala E.", ""]]}, {"id": "2101.08177", "submitter": "Ximing Qiao", "authors": "Ximing Qiao, Yuhua Bai, Siping Hu, Ang Li, Yiran Chen, Hai Li", "title": "On Provable Backdoor Defense in Collaborative Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  As collaborative learning allows joint training of a model using multiple\nsources of data, the security problem has been a central concern. Malicious\nusers can upload poisoned data to prevent the model's convergence or inject\nhidden backdoors. The so-called backdoor attacks are especially difficult to\ndetect since the model behaves normally on standard test data but gives wrong\noutputs when triggered by certain backdoor keys. Although Byzantine-tolerant\ntraining algorithms provide convergence guarantee, provable defense against\nbackdoor attacks remains largely unsolved. Methods based on randomized\nsmoothing can only correct a small number of corrupted pixels or labels;\nmethods based on subset aggregation cause a severe drop in classification\naccuracy due to low data utilization. We propose a novel framework that\ngeneralizes existing subset aggregation methods. The framework shows that the\nsubset selection process, a deciding factor for subset aggregation methods, can\nbe viewed as a code design problem. We derive the theoretical bound of data\nutilization ratio and provide optimal code construction. Experiments on non-IID\nversions of MNIST and CIFAR-10 show that our method with optimal codes\nsignificantly outperforms baselines using non-overlapping partition and random\nselection. Additionally, integration with existing coding theory results shows\nthat special codes can track the location of the attackers. Such capability\nprovides new countermeasures to backdoor attacks.\n", "versions": [{"version": "v1", "created": "Tue, 19 Jan 2021 14:39:32 GMT"}], "update_date": "2021-01-21", "authors_parsed": [["Qiao", "Ximing", ""], ["Bai", "Yuhua", ""], ["Hu", "Siping", ""], ["Li", "Ang", ""], ["Chen", "Yiran", ""], ["Li", "Hai", ""]]}, {"id": "2101.08197", "submitter": "Rafael Ferreira", "authors": "Rafael Ferreira, Mariana Leite, David Semedo and Joao Magalhaes", "title": "Open-Domain Conversational Search Assistant with Transformers", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Open-domain conversational search assistants aim at answering user questions\nabout open topics in a conversational manner. In this paper we show how the\nTransformer architecture achieves state-of-the-art results in key IR tasks,\nleveraging the creation of conversational assistants that engage in open-domain\nconversational search with single, yet informative, answers. In particular, we\npropose an open-domain abstractive conversational search agent pipeline to\naddress two major challenges: first, conversation context-aware search and\nsecond, abstractive search-answers generation. To address the first challenge,\nthe conversation context is modeled with a query rewriting method that unfolds\nthe context of the conversation up to a specific moment to search for the\ncorrect answers. These answers are then passed to a Transformer-based re-ranker\nto further improve retrieval performance. The second challenge, is tackled with\nrecent Abstractive Transformer architectures to generate a digest of the top\nmost relevant passages. Experiments show that Transformers deliver a solid\nperformance across all tasks in conversational search, outperforming the best\nTREC CAsT 2019 baseline.\n", "versions": [{"version": "v1", "created": "Wed, 20 Jan 2021 16:02:15 GMT"}], "update_date": "2021-01-21", "authors_parsed": [["Ferreira", "Rafael", ""], ["Leite", "Mariana", ""], ["Semedo", "David", ""], ["Magalhaes", "Joao", ""]]}, {"id": "2101.08200", "submitter": "Gail Weiss", "authors": "Daniel M. Yellin, Gail Weiss", "title": "Synthesizing Context-free Grammars from Recurrent Neural Networks\n  (Extended Version)", "comments": "Extended version of paper to appear in TACAS 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.FL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an algorithm for extracting a subclass of the context free\ngrammars (CFGs) from a trained recurrent neural network (RNN). We develop a new\nframework, pattern rule sets (PRSs), which describe sequences of deterministic\nfinite automata (DFAs) that approximate a non-regular language. We present an\nalgorithm for recovering the PRS behind a sequence of such automata, and apply\nit to the sequences of automata extracted from trained RNNs using the L*\nalgorithm. We then show how the PRS may converted into a CFG, enabling a\nfamiliar and useful presentation of the learned language.\n  Extracting the learned language of an RNN is important to facilitate\nunderstanding of the RNN and to verify its correctness. Furthermore, the\nextracted CFG can augment the RNN in classifying correct sentences, as the\nRNN's predictive accuracy decreases when the recursion depth and distance\nbetween matching delimiters of its input sequences increases.\n", "versions": [{"version": "v1", "created": "Wed, 20 Jan 2021 16:22:25 GMT"}, {"version": "v2", "created": "Tue, 9 Feb 2021 21:28:28 GMT"}, {"version": "v3", "created": "Sun, 28 Mar 2021 21:11:32 GMT"}], "update_date": "2021-03-30", "authors_parsed": [["Yellin", "Daniel M.", ""], ["Weiss", "Gail", ""]]}, {"id": "2101.08204", "submitter": "Do Le Quoc", "authors": "Do Le Quoc, Franz Gregor, Sergei Arnautov, Roland Kunkel, Pramod\n  Bhatotia, Christof Fetzer", "title": "secureTF: A Secure TensorFlow Framework", "comments": "arXiv admin note: text overlap with arXiv:1902.04413", "journal-ref": "Pages 44-59, 2020", "doi": "10.1145/3423211.3425687", "report-no": null, "categories": "cs.CR cs.DC cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Data-driven intelligent applications in modern online services have become\nubiquitous. These applications are usually hosted in the untrusted cloud\ncomputing infrastructure. This poses significant security risks since these\napplications rely on applying machine learning algorithms on large datasets\nwhich may contain private and sensitive information.\n  To tackle this challenge, we designed secureTF, a distributed secure machine\nlearning framework based on Tensorflow for the untrusted cloud infrastructure.\nsecureTF is a generic platform to support unmodified TensorFlow applications,\nwhile providing end-to-end security for the input data, ML model, and\napplication code. secureTF is built from ground-up based on the security\nproperties provided by Trusted Execution Environments (TEEs). However, it\nextends the trust of a volatile memory region (or secure enclave) provided by\nthe single node TEE to secure a distributed infrastructure required for\nsupporting unmodified stateful machine learning applications running in the\ncloud.\n  The paper reports on our experiences about the system design choices and the\nsystem deployment in production use-cases. We conclude with the lessons learned\nbased on the limitations of our commercially available platform, and discuss\nopen research problems for the future work.\n", "versions": [{"version": "v1", "created": "Wed, 20 Jan 2021 16:36:53 GMT"}], "update_date": "2021-01-21", "authors_parsed": [["Quoc", "Do Le", ""], ["Gregor", "Franz", ""], ["Arnautov", "Sergei", ""], ["Kunkel", "Roland", ""], ["Bhatotia", "Pramod", ""], ["Fetzer", "Christof", ""]]}, {"id": "2101.08213", "submitter": "Fay\\c{c}al Ait Aoudia", "authors": "Fay\\c{c}al Ait Aoudia and Jakob Hoydis", "title": "Trimming the Fat from OFDM: Pilot- and CP-less Communication with\n  End-to-end Learning", "comments": "arXiv admin note: substantial text overlap with arXiv:2009.05261", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.LG eess.SP math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Orthogonal frequency division multiplexing (OFDM) is one of the dominant\nwaveforms in wireless communication systems due to its efficient\nimplementation. However, it suffers from a loss of spectral efficiency as it\nrequires a cyclic prefix (CP) to mitigate inter-symbol interference (ISI) and\npilots to estimate the channel. We propose in this work to address these\ndrawbacks by learning a neural network (NN)-based receiver jointly with a\nconstellation geometry and bit labeling at the transmitter, that allows CP-less\nand pilotless communication on top of OFDM without a significant loss in bit\nerror rate (BER). Our approach enables at least 18% throughput gains compared\nto a pilot and CP-based baseline, and at least 4% gains compared to a system\nthat uses a neural receiver with pilots but no CP.\n", "versions": [{"version": "v1", "created": "Wed, 20 Jan 2021 16:53:37 GMT"}, {"version": "v2", "created": "Fri, 22 Jan 2021 09:11:48 GMT"}, {"version": "v3", "created": "Wed, 14 Apr 2021 14:39:24 GMT"}], "update_date": "2021-04-15", "authors_parsed": [["Aoudia", "Fay\u00e7al Ait", ""], ["Hoydis", "Jakob", ""]]}, {"id": "2101.08236", "submitter": "Vinayak Sharma", "authors": "Vinayak Sharma, Jorge Angel Gonzalez Ordiano, Ralf Mikut, Umit Cali", "title": "Probabilistic Solar Power Forecasting: Long Short-Term Memory Network vs\n  Simpler Approaches", "comments": "Submitted to the International Symposium of forecasting 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG eess.SP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The high penetration of volatile renewable energy sources such as solar make\nmethods for coping with the uncertainty associated with them of paramount\nimportance. Probabilistic forecasts are an example of these methods, as they\nassist energy planners in their decision-making process by providing them with\ninformation about the uncertainty of future power generation. Currently, there\nis a trend towards the use of deep learning probabilistic forecasting methods.\nHowever, the point at which the more complex deep learning methods should be\npreferred over more simple approaches is not yet clear. Therefore, the current\narticle presents a simple comparison between a long short-term memory neural\nnetwork and other more simple approaches. The comparison consists of training\nand comparing models able to provide one-day-ahead probabilistic forecasts for\na solar power system. Moreover, the current paper makes use of an open-source\ndataset provided during the Global Energy Forecasting Competition of 2014\n(GEFCom14).\n", "versions": [{"version": "v1", "created": "Wed, 20 Jan 2021 18:13:07 GMT"}], "update_date": "2021-01-21", "authors_parsed": [["Sharma", "Vinayak", ""], ["Ordiano", "Jorge Angel Gonzalez", ""], ["Mikut", "Ralf", ""], ["Cali", "Umit", ""]]}, {"id": "2101.08238", "submitter": "Ammarah Farooq", "authors": "Ammarah Farooq, Muhammad Awais, Josef Kittler, Syed Safwan Khalid", "title": "AXM-Net: Cross-Modal Context Sharing Attention Network for Person Re-ID", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cross-modal person re-identification (Re-ID) is critical for modern video\nsurveillance systems. The key challenge is to align inter-modality\nrepresentations according to semantic information present for a person and\nignore background information. In this work, we present AXM-Net, a novel CNN\nbased architecture designed for learning semantically aligned visual and\ntextual representations. The underlying building block consists of multiple\nstreams of feature maps coming from visual and textual modalities and a novel\nlearnable context sharing semantic alignment network. We also propose\ncomplementary intra modal attention learning mechanisms to focus on more\nfine-grained local details in the features along with a cross-modal affinity\nloss for robust feature matching. Our design is unique in its ability to\nimplicitly learn feature alignments from data. The entire AXM-Net can be\ntrained in an end-to-end manner. We report results on both person search and\ncross-modal Re-ID tasks. Extensive experimentation validates the proposed\nframework and demonstrates its superiority by outperforming the current\nstate-of-the-art methods by a significant margin.\n", "versions": [{"version": "v1", "created": "Tue, 19 Jan 2021 16:06:39 GMT"}, {"version": "v2", "created": "Fri, 19 Mar 2021 15:28:49 GMT"}], "update_date": "2021-03-22", "authors_parsed": [["Farooq", "Ammarah", ""], ["Awais", "Muhammad", ""], ["Kittler", "Josef", ""], ["Khalid", "Syed Safwan", ""]]}, {"id": "2101.08248", "submitter": "Sam Wiseman", "authors": "Sam Wiseman, Arturs Backurs, Karl Stratos", "title": "Generating (Formulaic) Text by Splicing Together Nearest Neighbors", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose to tackle conditional text generation tasks, especially those\nwhich require generating formulaic text, by splicing together segments of text\nfrom retrieved \"neighbor\" source-target pairs. Unlike recent work that\nconditions on retrieved neighbors in an encoder-decoder setting but generates\ntext token-by-token, left-to-right, we learn a policy that directly manipulates\nsegments of neighbor text (i.e., by inserting or replacing them) to form an\noutput. Standard techniques for training such a policy require an oracle\nderivation for each generation, and we prove that finding the shortest such\nderivation can be reduced to parsing under a particular weighted context-free\ngrammar. We find that policies learned in this way allow for interpretable\ntable-to-text or headline generation that is competitive with neighbor-based\ntoken-level policies on automatic metrics, though on all but one dataset\nneighbor-based policies underperform a strong neighborless baseline. In all\ncases, however, generating by splicing is faster.\n", "versions": [{"version": "v1", "created": "Wed, 20 Jan 2021 18:43:11 GMT"}, {"version": "v2", "created": "Fri, 29 Jan 2021 18:44:33 GMT"}], "update_date": "2021-02-01", "authors_parsed": [["Wiseman", "Sam", ""], ["Backurs", "Arturs", ""], ["Stratos", "Karl", ""]]}, {"id": "2101.08254", "submitter": "Jingtao Li", "authors": "Jingtao Li, Adnan Siraj Rakin, Zhezhi He, Deliang Fan, Chaitali\n  Chakrabarti", "title": "RADAR: Run-time Adversarial Weight Attack Detection and Accuracy\n  Recovery", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Adversarial attacks on Neural Network weights, such as the progressive\nbit-flip attack (PBFA), can cause a catastrophic degradation in accuracy by\nflipping a very small number of bits. Furthermore, PBFA can be conducted at run\ntime on the weights stored in DRAM main memory. In this work, we propose RADAR,\na Run-time adversarial weight Attack Detection and Accuracy Recovery scheme to\nprotect DNN weights against PBFA. We organize weights that are interspersed in\na layer into groups and employ a checksum-based algorithm on weights to derive\na 2-bit signature for each group. At run time, the 2-bit signature is computed\nand compared with the securely stored golden signature to detect the bit-flip\nattacks in a group. After successful detection, we zero out all the weights in\na group to mitigate the accuracy drop caused by malicious bit-flips. The\nproposed scheme is embedded in the inference computation stage. For the\nResNet-18 ImageNet model, our method can detect 9.6 bit-flips out of 10 on\naverage. For this model, the proposed accuracy recovery scheme can restore the\naccuracy from below 1% caused by 10 bit flips to above 69%. The proposed method\nhas extremely low time and storage overhead. System-level simulation on gem5\nshows that RADAR only adds <1% to the inference time, making this scheme highly\nsuitable for run-time attack detection and mitigation.\n", "versions": [{"version": "v1", "created": "Wed, 20 Jan 2021 18:55:51 GMT"}], "update_date": "2021-01-21", "authors_parsed": [["Li", "Jingtao", ""], ["Rakin", "Adnan Siraj", ""], ["He", "Zhezhi", ""], ["Fan", "Deliang", ""], ["Chakrabarti", "Chaitali", ""]]}, {"id": "2101.08258", "submitter": "Ethan Ancell", "authors": "Ethan Ancell, Brennan Bean", "title": "Autocart -- spatially-aware regression trees for ecological and spatial\n  modeling", "comments": "20 pages, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.QM cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Many ecological and spatial processes are complex in nature and are not\naccurately modeled by linear models. Regression trees promise to handle the\nhigh-order interactions that are present in ecological and spatial datasets,\nbut fail to produce physically realistic characterizations of the underlying\nlandscape. The \"autocart\" (autocorrelated regression trees) R package extends\nthe functionality of previously proposed spatial regression tree methods\nthrough a spatially aware splitting function and novel adaptive inverse\ndistance weighting method in each terminal node. The efficacy of these autocart\nmodels, including an autocart extension of random forest, is demonstrated on\nmultiple datasets. This highlights the ability of autocart to model complex\ninteractions between spatial variables while still providing physically\nrealistic representations of the landscape.\n", "versions": [{"version": "v1", "created": "Wed, 20 Jan 2021 00:36:21 GMT"}], "update_date": "2021-01-22", "authors_parsed": [["Ancell", "Ethan", ""], ["Bean", "Brennan", ""]]}, {"id": "2101.08286", "submitter": "Matthew Colbrook", "authors": "Matthew J. Colbrook, Vegard Antun, Anders C. Hansen", "title": "Can stable and accurate neural networks be computed? -- On the barriers\n  of deep learning and Smale's 18th problem", "comments": "14 pages + SI Appendix", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.NA cs.NE math.NA", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Deep learning (DL) has had unprecedented success and is now entering\nscientific computing with full force. However, current DL methods typically\nsuffer from instability, even when universal approximation properties guarantee\nthe existence of stable neural networks (NNs). We address this paradox by\ndemonstrating basic well-conditioned problems in scientific computing where one\ncan prove the existence of NNs with great approximation qualities, however,\nthere does not exist any algorithm, even randomised, that can train (or\ncompute) such a NN. For any positive integers $K > 2$ and $L$, there are cases\nwhere simultaneously: (a) no randomised training algorithm can compute a NN\ncorrect to $K$ digits with probability greater than $1/2$, (b) there exists a\ndeterministic training algorithm that computes a NN with $K-1$ correct digits,\nbut any such (even randomised) algorithm needs arbitrarily many training data,\n(c) there exists a deterministic training algorithm that computes a NN with\n$K-2$ correct digits using no more than $L$ training samples. These results\nimply a classification theory describing conditions under which (stable) NNs\nwith a given accuracy can be computed by an algorithm. We begin this theory by\nestablishing sufficient conditions for the existence of algorithms that compute\nstable NNs in inverse problems. We introduce Fast Iterative REstarted NETworks\n(FIRENETs), which we both prove and numerically verify are stable. Moreover, we\nprove that only $\\mathcal{O}(|\\log(\\epsilon)|)$ layers are needed for an\n$\\epsilon$-accurate solution to the inverse problem.\n", "versions": [{"version": "v1", "created": "Wed, 20 Jan 2021 19:04:17 GMT"}, {"version": "v2", "created": "Thu, 15 Apr 2021 17:09:49 GMT"}], "update_date": "2021-04-16", "authors_parsed": [["Colbrook", "Matthew J.", ""], ["Antun", "Vegard", ""], ["Hansen", "Anders C.", ""]]}, {"id": "2101.08288", "submitter": "Yakup Kutlu", "authors": "G\\\"okhan Altan, Yakup Kutlu, Adnan \\\"Ozhan Pekmezci, Serkan Nural", "title": "The Diagnosis of Asthma using Hilbert-Huang Transform and Deep Learning\n  on Lung Sounds", "comments": "6 pages, in Turkish language, journal of intelligent systems with\n  applications", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.LG eess.AS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Lung auscultation is the most effective and indispensable method for\ndiagnosing various respiratory disorders by using the sounds from the airways\nduring inspirium and exhalation using a stethoscope. In this study, the\nstatistical features are calculated from intrinsic mode functions that are\nextracted by applying the HilbertHuang Transform to the lung sounds from 12\ndifferent auscultation regions on the chest and back. The classification of the\nlung sounds from asthma and healthy subjects is performed using Deep Belief\nNetworks (DBN). The DBN classifier model with two hidden layers has been tested\nusing 5-fold cross validation method. The proposed DBN separated lung sounds\nfrom asthmatic and healthy subjects with high classification performance rates\nof 84.61%, 85.83%, and 77.11% for overall accuracy, sensitivity, and\nselectivity, respectively using frequencytime analysis.\n", "versions": [{"version": "v1", "created": "Wed, 20 Jan 2021 19:04:33 GMT"}], "update_date": "2021-01-22", "authors_parsed": [["Altan", "G\u00f6khan", ""], ["Kutlu", "Yakup", ""], ["Pekmezci", "Adnan \u00d6zhan", ""], ["Nural", "Serkan", ""]]}, {"id": "2101.08299", "submitter": "Berat Kurar Barakat", "authors": "Berat Barakat, Ahmad Droby, Majeed Kassis and Jihad El-Sana", "title": "Text Line Segmentation for Challenging Handwritten Document Images Using\n  Fully Convolutional Network", "comments": "ICFHR 2018 16th International Conference on Frontiers in Handwriting\n  Recognition", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper presents a method for text line segmentation of challenging\nhistorical manuscript images. These manuscript images contain narrow interline\nspaces with touching components, interpenetrating vowel signs and inconsistent\nfont types and sizes. In addition, they contain curved, multi-skewed and\nmulti-directed side note lines within a complex page layout. Therefore,\nbounding polygon labeling would be very difficult and time consuming. Instead\nwe rely on line masks that connect the components on the same text line. Then\nthese line masks are predicted using a Fully Convolutional Network (FCN). In\nthe literature, FCN has been successfully used for text line segmentation of\nregular handwritten document images. The present paper shows that FCN is useful\nwith challenging manuscript images as well. Using a new evaluation metric that\nis sensitive to over segmentation as well as under segmentation, testing\nresults on a publicly available challenging handwritten dataset are comparable\nwith the results of a previous work on the same dataset.\n", "versions": [{"version": "v1", "created": "Wed, 20 Jan 2021 19:51:26 GMT"}], "update_date": "2021-01-22", "authors_parsed": [["Barakat", "Berat", ""], ["Droby", "Ahmad", ""], ["Kassis", "Majeed", ""], ["El-Sana", "Jihad", ""]]}, {"id": "2101.08301", "submitter": "Laila Khalid", "authors": "Wei Gong, Laila Khalid", "title": "Aesthetics, Personalization and Recommendation: A survey on Deep\n  Learning in Fashion", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Machine learning is completely changing the trends in the fashion industry.\nFrom big to small every brand is using machine learning techniques in order to\nimprove their revenue, increase customers and stay ahead of the trend. People\nare into fashion and they want to know what looks best and how they can improve\ntheir style and elevate their personality. Using Deep learning technology and\ninfusing it with Computer Vision techniques one can do so by utilizing\nBrain-inspired Deep Networks, and engaging into Neuroaesthetics, working with\nGANs and Training them, playing around with Unstructured Data,and infusing the\ntransformer architecture are just some highlights which can be touched with the\nFashion domain. Its all about designing a system that can tell us information\nregarding the fashion aspect that can come in handy with the ever growing\ndemand. Personalization is a big factor that impacts the spending choices of\ncustomers.The survey also shows remarkable approaches that encroach the subject\nof achieving that by divulging deep into how visual data can be interpreted and\nleveraged into different models and approaches. Aesthetics play a vital role in\nclothing recommendation as users' decision depends largely on whether the\nclothing is in line with their aesthetics, however the conventional image\nfeatures cannot portray this directly. For that the survey also highlights\nremarkable models like tensor factorization model, conditional random field\nmodel among others to cater the need to acknowledge aesthetics as an important\nfactor in Apparel recommendation.These AI inspired deep models can pinpoint\nexactly which certain style resonates best with their customers and they can\nhave an understanding of how the new designs will set in with the community.\nWith AI and machine learning your businesses can stay ahead of the fashion\ntrends.\n", "versions": [{"version": "v1", "created": "Wed, 20 Jan 2021 19:57:13 GMT"}], "update_date": "2021-01-22", "authors_parsed": [["Gong", "Wei", ""], ["Khalid", "Laila", ""]]}, {"id": "2101.08303", "submitter": "Gal Vardi", "authors": "Amit Daniely and Gal Vardi", "title": "From Local Pseudorandom Generators to Hardness of Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We prove hardness-of-learning results under a well-studied assumption on the\nexistence of local pseudorandom generators. As we show, this assumption allows\nus to surpass the current state of the art, and prove hardness of various basic\nproblems, with no hardness results to date.\n  Our results include: hardness of learning shallow ReLU neural networks under\nthe Gaussian distribution and other distributions; hardness of learning\nintersections of $\\omega(1)$ halfspaces, DNF formulas with $\\omega(1)$ terms,\nand ReLU networks with $\\omega(1)$ hidden neurons; hardness of weakly learning\ndeterministic finite automata under the uniform distribution; hardness of\nweakly learning depth-$3$ Boolean circuits under the uniform distribution, as\nwell as distribution-specific hardness results for learning DNF formulas and\nintersections of halfspaces. We also establish lower bounds on the complexity\nof learning intersections of a constant number of halfspaces, and ReLU networks\nwith a constant number of hidden neurons. Moreover, our results imply the\nhardness of virtually all improper PAC-learning problems (both\ndistribution-free and distribution-specific) that were previously shown hard\nunder other assumptions.\n", "versions": [{"version": "v1", "created": "Wed, 20 Jan 2021 20:07:47 GMT"}, {"version": "v2", "created": "Tue, 8 Jun 2021 11:49:29 GMT"}], "update_date": "2021-06-09", "authors_parsed": [["Daniely", "Amit", ""], ["Vardi", "Gal", ""]]}, {"id": "2101.08310", "submitter": "Gerrit Welper", "authors": "G. Welper", "title": "Non-Convex Compressed Sensing with Training Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NA math.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Efficient algorithms for the sparse solution of under-determined linear\nsystems $Ax = b$ are known for matrices $A$ satisfying suitable assumptions\nlike the restricted isometry property (RIP). Without such assumptions little is\nknown and without any assumptions on $A$ the problem is $NP$-hard. A common\napproach is to replace $\\ell_1$ by $\\ell_p$ minimization for $0 < p < 1$, which\nis no longer convex and typically requires some form of local initial values\nfor provably convergent algorithms.\n  In this paper, we consider an alternative, where instead of suitable initial\nvalues we are provided with extra training problems $Ax = B_l$, $l=1, \\dots, p$\nthat are related to our compressed sensing problem. They allow us to find the\nsolution of the original problem $Ax = b$ with high probability in the range of\na one layer linear neural network with comparatively few assumptions on the\nmatrix $A$.\n", "versions": [{"version": "v1", "created": "Wed, 20 Jan 2021 20:30:59 GMT"}], "update_date": "2021-01-22", "authors_parsed": [["Welper", "G.", ""]]}, {"id": "2101.08316", "submitter": "Gang Qu", "authors": "Gang Qu, Li Xiao, Wenxing Hu, Kun Zhang, Vince D. Calhoun, Yu-Ping\n  Wang", "title": "Ensemble manifold based regularized multi-modal graph convolutional\n  network for cognitive ability prediction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Objective: Multi-modal functional magnetic resonance imaging (fMRI) can be\nused to make predictions about individual behavioral and cognitive traits based\non brain connectivity networks. Methods: To take advantage of complementary\ninformation from multi-modal fMRI, we propose an interpretable multi-modal\ngraph convolutional network (MGCN) model, incorporating the fMRI time series\nand the functional connectivity (FC) between each pair of brain regions.\nSpecifically, our model learns a graph embedding from individual brain networks\nderived from multi-modal data. A manifold-based regularization term is then\nenforced to consider the relationships of subjects both within and between\nmodalities. Furthermore, we propose the gradient-weighted regression activation\nmapping (Grad-RAM) and the edge mask learning to interpret the model, which is\nused to identify significant cognition-related biomarkers. Results: We validate\nour MGCN model on the Philadelphia Neurodevelopmental Cohort to predict\nindividual wide range achievement test (WRAT) score. Our model obtains superior\npredictive performance over GCN with a single modality and other competing\napproaches. The identified biomarkers are cross-validated from different\napproaches. Conclusion and Significance: This paper develops a new\ninterpretable graph deep learning framework for cognitive ability prediction,\nwith the potential to overcome the limitations of several current data-fusion\nmodels. The results demonstrate the power of MGCN in analyzing multi-modal fMRI\nand discovering significant biomarkers for human brain studies.\n", "versions": [{"version": "v1", "created": "Wed, 20 Jan 2021 20:53:07 GMT"}], "update_date": "2021-01-22", "authors_parsed": [["Qu", "Gang", ""], ["Xiao", "Li", ""], ["Hu", "Wenxing", ""], ["Zhang", "Kun", ""], ["Calhoun", "Vince D.", ""], ["Wang", "Yu-Ping", ""]]}, {"id": "2101.08349", "submitter": "Varun Mandalapu", "authors": "Varun Mandalapu, Jiaqi Gong and Lujie Chen", "title": "Do we need to go Deep? Knowledge Tracing with Big Data", "comments": "9 Pages, 4 figures, AAAI Workshop on AI in Education (Imagining\n  Post-COVID Education with AI)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Interactive Educational Systems (IES) enabled researchers to trace student\nknowledge in different skills and provide recommendations for a better learning\npath. To estimate the student knowledge and further predict their future\nperformance, the interest in utilizing the student interaction data captured by\nIES to develop learner performance models is increasing rapidly. Moreover, with\nthe advances in computing systems, the amount of data captured by these IES\nsystems is also increasing that enables deep learning models to compete with\ntraditional logistic models and Markov processes. However, it is still not\nempirically evident if these deep models outperform traditional models on the\ncurrent scale of datasets with millions of student interactions. In this work,\nwe adopt EdNet, the largest student interaction dataset publicly available in\nthe education domain, to understand how accurately both deep and traditional\nmodels predict future student performances. Our work observes that logistic\nregression models with carefully engineered features outperformed deep models\nfrom extensive experimentation. We follow this analysis with interpretation\nstudies based on Locally Interpretable Model-agnostic Explanation (LIME) to\nunderstand the impact of various features on best performing model\npre-dictions.\n", "versions": [{"version": "v1", "created": "Wed, 20 Jan 2021 22:40:38 GMT"}], "update_date": "2021-01-22", "authors_parsed": [["Mandalapu", "Varun", ""], ["Gong", "Jiaqi", ""], ["Chen", "Lujie", ""]]}, {"id": "2101.08354", "submitter": "Xun Gao", "authors": "Xun Gao, Eric R. Anschuetz, Sheng-Tao Wang, J. Ignacio Cirac and\n  Mikhail D. Lukin", "title": "Enhancing Generative Models via Quantum Correlations", "comments": "25 pages, 13 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "quant-ph cond-mat.stat-mech cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generative modeling using samples drawn from the probability distribution\nconstitutes a powerful approach for unsupervised machine learning. Quantum\nmechanical systems can produce probability distributions that exhibit quantum\ncorrelations which are difficult to capture using classical models. We show\ntheoretically that such quantum correlations provide a powerful resource for\ngenerative modeling. In particular, we provide an unconditional proof of\nseparation in expressive power between a class of widely-used generative\nmodels, known as Bayesian networks, and its minimal quantum extension. We show\nthat this expressivity advantage is associated with quantum nonlocality and\nquantum contextuality. Furthermore, we numerically test this separation on\nstandard machine learning data sets and show that it holds for practical\nproblems. The possibility of quantum advantage demonstrated in this work not\nonly sheds light on the design of useful quantum machine learning protocols but\nalso provides inspiration to draw on ideas from quantum foundations to improve\npurely classical algorithms.\n", "versions": [{"version": "v1", "created": "Wed, 20 Jan 2021 22:57:22 GMT"}], "update_date": "2021-01-22", "authors_parsed": [["Gao", "Xun", ""], ["Anschuetz", "Eric R.", ""], ["Wang", "Sheng-Tao", ""], ["Cirac", "J. Ignacio", ""], ["Lukin", "Mikhail D.", ""]]}, {"id": "2101.08358", "submitter": "Jason Mohoney", "authors": "Jason Mohoney, Roger Waleffe, Yiheng Xu, Theodoros Rekatsinas,\n  Shivaram Venkataraman", "title": "Marius: Learning Massive Graph Embeddings on a Single Machine", "comments": "Accepted into OSDI '21", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DB cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new framework for computing the embeddings of large-scale graphs\non a single machine. A graph embedding is a fixed length vector representation\nfor each node (and/or edge-type) in a graph and has emerged as the de-facto\napproach to apply modern machine learning on graphs. We identify that current\nsystems for learning the embeddings of large-scale graphs are bottlenecked by\ndata movement, which results in poor resource utilization and inefficient\ntraining. These limitations require state-of-the-art systems to distribute\ntraining across multiple machines. We propose Marius, a system for efficient\ntraining of graph embeddings that leverages partition caching and buffer-aware\ndata orderings to minimize disk access and interleaves data movement with\ncomputation to maximize utilization. We compare Marius against two\nstate-of-the-art industrial systems on a diverse array of benchmarks. We\ndemonstrate that Marius achieves the same level of accuracy but is up to one\norder of magnitude faster. We also show that Marius can scale training to\ndatasets an order of magnitude beyond a single machine's GPU and CPU memory\ncapacity, enabling training of configurations with more than a billion edges\nand 550 GB of total parameters on a single machine with 16 GB of GPU memory and\n64 GB of CPU memory. Marius is open-sourced at www.marius-project.org.\n", "versions": [{"version": "v1", "created": "Wed, 20 Jan 2021 23:17:31 GMT"}, {"version": "v2", "created": "Wed, 26 May 2021 00:22:46 GMT"}], "update_date": "2021-05-27", "authors_parsed": [["Mohoney", "Jason", ""], ["Waleffe", "Roger", ""], ["Xu", "Yiheng", ""], ["Rekatsinas", "Theodoros", ""], ["Venkataraman", "Shivaram", ""]]}, {"id": "2101.08366", "submitter": "Jie Bu", "authors": "Jie Bu, Anuj Karpatne", "title": "Quadratic Residual Networks: A New Class of Neural Networks for Solving\n  Forward and Inverse Problems in Physics Involving PDEs", "comments": "Accepted by SIAM International Conference on Data Mining (SDM21)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CE", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  We propose quadratic residual networks (QRes) as a new type of\nparameter-efficient neural network architecture, by adding a quadratic residual\nterm to the weighted sum of inputs before applying activation functions. With\nsufficiently high functional capacity (or expressive power), we show that it is\nespecially powerful for solving forward and inverse physics problems involving\npartial differential equations (PDEs). Using tools from algebraic geometry, we\ntheoretically demonstrate that, in contrast to plain neural networks, QRes\nshows better parameter efficiency in terms of network width and depth thanks to\nhigher non-linearity in every neuron. Finally, we empirically show that QRes\nshows faster convergence speed in terms of number of training epochs especially\nin learning complex patterns.\n", "versions": [{"version": "v1", "created": "Wed, 20 Jan 2021 23:54:55 GMT"}, {"version": "v2", "created": "Thu, 28 Jan 2021 01:51:50 GMT"}], "update_date": "2021-01-29", "authors_parsed": [["Bu", "Jie", ""], ["Karpatne", "Anuj", ""]]}, {"id": "2101.08367", "submitter": "Naoyuki Terashita", "authors": "Naoyuki Terashita, Hiroki Ohashi, Yuichi Nonaka, Takashi Kanemaru", "title": "Influence Estimation for Generative Adversarial Networks", "comments": "Accepted as a conference paper at ICLR 2021 (Spotlight). This is the\n  peer-reviewed version and not ready for publication", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Identifying harmful instances, whose absence in a training dataset improves\nmodel performance, is important for building better machine learning models.\nAlthough previous studies have succeeded in estimating harmful instances under\nsupervised settings, they cannot be trivially extended to generative\nadversarial networks (GANs). This is because previous approaches require that\n(1) the absence of a training instance directly affects the loss value and that\n(2) the change in the loss directly measures the harmfulness of the instance\nfor the performance of a model. In GAN training, however, neither of the\nrequirements is satisfied. This is because, (1) the generator's loss is not\ndirectly affected by the training instances as they are not part of the\ngenerator's training steps, and (2) the values of GAN's losses normally do not\ncapture the generative performance of a model. To this end, (1) we propose an\ninfluence estimation method that uses the Jacobian of the gradient of the\ngenerator's loss with respect to the discriminator's parameters (and vice\nversa) to trace how the absence of an instance in the discriminator's training\naffects the generator's parameters, and (2) we propose a novel evaluation\nscheme, in which we assess harmfulness of each training instance on the basis\nof how GAN evaluation metric (e.g., inception score) is expect to change due to\nthe removal of the instance. We experimentally verified that our influence\nestimation method correctly inferred the changes in GAN evaluation metrics.\nFurther, we demonstrated that the removal of the identified harmful instances\neffectively improved the model's generative performance with respect to various\nGAN evaluation metrics.\n", "versions": [{"version": "v1", "created": "Wed, 20 Jan 2021 23:55:54 GMT"}], "update_date": "2021-01-22", "authors_parsed": [["Terashita", "Naoyuki", ""], ["Ohashi", "Hiroki", ""], ["Nonaka", "Yuichi", ""], ["Kanemaru", "Takashi", ""]]}, {"id": "2101.08380", "submitter": "Mario Boley", "authors": "Mario Boley and Simon Teshuva and Pierre Le Bodic and Geoffrey I Webb", "title": "Better Short than Greedy: Interpretable Models through Optimal Rule\n  Boosting", "comments": "SDM 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Rule ensembles are designed to provide a useful trade-off between predictive\naccuracy and model interpretability. However, the myopic and random search\ncomponents of current rule ensemble methods can compromise this goal: they\noften need more rules than necessary to reach a certain accuracy level or can\neven outright fail to accurately model a distribution that can actually be\ndescribed well with a few rules. Here, we present a novel approach aiming to\nfit rule ensembles of maximal predictive power for a given ensemble size (and\nthus model comprehensibility). In particular, we present an efficient\nbranch-and-bound algorithm that optimally solves the per-rule objective\nfunction of the popular second-order gradient boosting framework. Our main\ninsight is that the boosting objective can be tightly bounded in linear time of\nthe number of covered data points. Along with an additional novel pruning\ntechnique related to rule redundancy, this leads to a computationally feasible\napproach for boosting optimal rules that, as we demonstrate on a wide range of\ncommon benchmark problems, consistently outperforms the predictive performance\nof boosting greedy rules.\n", "versions": [{"version": "v1", "created": "Thu, 21 Jan 2021 01:03:48 GMT"}], "update_date": "2021-01-22", "authors_parsed": [["Boley", "Mario", ""], ["Teshuva", "Simon", ""], ["Bodic", "Pierre Le", ""], ["Webb", "Geoffrey I", ""]]}, {"id": "2101.08385", "submitter": "Ethan Moyer", "authors": "Ethan Jacob Moyer and Anup Das", "title": "Motif Identification using CNN-based Pairwise Subsequence Alignment\n  Score Prediction", "comments": "7 pages, 4 figures, submitted to the 2021 International Joint\n  Conference on Neural Networks", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.GN cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  A common problem in bioinformatics is related to identifying gene regulatory\nregions marked by relatively high frequencies of motifs, or deoxyribonucleic\nacid sequences that often code for transcription and enhancer proteins.\nPredicting alignment scores between subsequence k-mers and a given motif\nenables the identification of candidate regulatory regions in a gene, which\ncorrespond to the transcription of these proteins. We propose a one-dimensional\n(1-D) Convolution Neural Network trained on k-mer formatted sequences\ninterspaced with the given motif pattern to predict pairwise alignment scores\nbetween the consensus motif and subsequence k-mers. Our model consists of\nfifteen layers with three rounds of a one-dimensional convolution layer, a\nbatch normalization layer, a dense layer, and a 1-D maximum pooling layer. We\ntrain the model using mean squared error loss on four different data sets each\nwith a different motif pattern randomly inserted in DNA sequences: the first\nthree data sets have zero, one, and two mutations applied on each inserted\nmotif, and the fourth data set represents the inserted motif as a\nposition-specific probability matrix. We use a novel proposed metric in order\nto evaluate the model's performance, $S_{\\alpha}$, which is based on the\nJaccard Index. We use 10-fold cross validation to evaluate out model. Using\n$S_{\\alpha}$, we measure the accuracy of the model by identifying the 15\nhighest-scoring 15-mer indices of the predicted scores that agree with that of\nthe actual scores within a selected $\\alpha$ region. For the best performing\ndata set, our results indicate on average 99.3% of the top 15 motifs were\nidentified correctly within a one base pair stride ($\\alpha = 1$) in the out of\nsample data. To the best of our knowledge, this is a novel approach that\nillustrates how data formatted in an intelligent way can be extrapolated using\nmachine learning.\n", "versions": [{"version": "v1", "created": "Thu, 21 Jan 2021 01:27:42 GMT"}], "update_date": "2021-01-22", "authors_parsed": [["Moyer", "Ethan Jacob", ""], ["Das", "Anup", ""]]}, {"id": "2101.08386", "submitter": "Simone Brugiapaglia", "authors": "S. Brugiapaglia, M. Liu, P. Tupper", "title": "Invariance, encodings, and generalization: learning identity effects\n  with neural networks", "comments": "arXiv admin note: text overlap with arXiv:2005.04330", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Often in language and other areas of cognition, whether two components of an\nobject are identical or not determines if it is well formed. We call such\nconstraints identity effects. When developing a system to learn well-formedness\nfrom examples, it is easy enough to build in an identify effect. But can\nidentity effects be learned from the data without explicit guidance? We provide\na framework in which we can rigorously prove that algorithms satisfying simple\ncriteria cannot make the correct inference. We then show that a broad class of\nlearning algorithms including deep feedforward neural networks trained via\ngradient-based algorithms (such as stochastic gradient descent or the Adam\nmethod) satisfy our criteria, dependent on the encoding of inputs. In some\nbroader circumstances we are able to provide adversarial examples that the\nnetwork necessarily classifies incorrectly. Finally, we demonstrate our theory\nwith computational experiments in which we explore the effect of different\ninput encodings on the ability of algorithms to generalize to novel inputs.\n", "versions": [{"version": "v1", "created": "Thu, 21 Jan 2021 01:28:15 GMT"}, {"version": "v2", "created": "Fri, 26 Feb 2021 19:55:16 GMT"}, {"version": "v3", "created": "Mon, 26 Apr 2021 23:06:58 GMT"}], "update_date": "2021-04-28", "authors_parsed": [["Brugiapaglia", "S.", ""], ["Liu", "M.", ""], ["Tupper", "P.", ""]]}, {"id": "2101.08387", "submitter": "Yongquan Yang", "authors": "Yongquan Yang, Haijun Lv, Ning Chen", "title": "A Survey on Ensemble Learning under the Era of Deep Learning", "comments": "39 pages, 9 figures, 15 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Due to the dominant position of deep learning (mostly deep neural networks)\nin various artificial intelligence applications, recently, ensemble learning\nbased on deep neural networks (ensemble deep learning) has shown significant\nperformances in improving the generalization of learning system. However, since\nmodern deep neural networks usually have millions to billions of parameters,\nthe time and space overheads for training multiple base deep learners and\ntesting with the ensemble deep learner are far greater than that of traditional\nensemble learning. Though several algorithms of fast ensemble deep learning\nhave been proposed to promote the deployment of ensemble deep learning in some\napplications, further advances still need to be made for many applications in\nspecific fields, where the developing time and computing resources are usually\nrestricted or the data to be processed is of large dimensionality. An urgent\nproblem needs to be solved is how to take the significant advantages of\nensemble deep learning while reduce the required time and space overheads so\nthat many more applications in specific fields can benefit from it. For the\nalleviation of this problem, it is essential to know about how ensemble\nlearning has developed under the era of deep learning. Thus, in this article,\nwe present discussion focusing on data analyses of published works, the\nmethodology, recent works and unattainability of traditional ensemble learning,\nand recent developments of ensemble deep learning. We hope this article will be\nhelpful to realize the technical challenges faced by future developments of\nensemble learning under the era of deep learning.\n", "versions": [{"version": "v1", "created": "Thu, 21 Jan 2021 01:33:23 GMT"}, {"version": "v2", "created": "Fri, 16 Apr 2021 03:28:11 GMT"}, {"version": "v3", "created": "Tue, 18 May 2021 03:47:12 GMT"}], "update_date": "2021-05-19", "authors_parsed": [["Yang", "Yongquan", ""], ["Lv", "Haijun", ""], ["Chen", "Ning", ""]]}, {"id": "2101.08390", "submitter": "Sharu Theresa Jose", "authors": "Sharu Theresa Jose and Osvaldo Simeone", "title": "An Information-Theoretic Analysis of the Impact of Task Similarity on\n  Meta-Learning", "comments": "Accepted to ISIT 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.IT eess.SP math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Meta-learning aims at optimizing the hyperparameters of a model class or\ntraining algorithm from the observation of data from a number of related tasks.\nFollowing the setting of Baxter [1], the tasks are assumed to belong to the\nsame task environment, which is defined by a distribution over the space of\ntasks and by per-task data distributions. The statistical properties of the\ntask environment thus dictate the similarity of the tasks. The goal of the\nmeta-learner is to ensure that the hyperparameters obtain a small loss when\napplied for training of a new task sampled from the task environment. The\ndifference between the resulting average loss, known as meta-population loss,\nand the corresponding empirical loss measured on the available data from\nrelated tasks, known as meta-generalization gap, is a measure of the\ngeneralization capability of the meta-learner. In this paper, we present novel\ninformation-theoretic bounds on the average absolute value of the\nmeta-generalization gap. Unlike prior work [2], our bounds explicitly capture\nthe impact of task relatedness, the number of tasks, and the number of data\nsamples per task on the meta-generalization gap. Task similarity is gauged via\nthe Kullback-Leibler (KL) and Jensen-Shannon (JS) divergences. We illustrate\nthe proposed bounds on the example of ridge regression with meta-learned bias.\n", "versions": [{"version": "v1", "created": "Thu, 21 Jan 2021 01:38:16 GMT"}, {"version": "v2", "created": "Mon, 25 Jan 2021 04:55:27 GMT"}, {"version": "v3", "created": "Sat, 8 May 2021 09:23:06 GMT"}], "update_date": "2021-05-11", "authors_parsed": [["Jose", "Sharu Theresa", ""], ["Simeone", "Osvaldo", ""]]}, {"id": "2101.08393", "submitter": "Walker Ravina", "authors": "Walker Ravina, Ethan Sterling, Olexiy Oryeshko, Nathan Bell, Honglei\n  Zhuang, Xuanhui Wang, Yonghui Wu, Alexander Grushetsky", "title": "Distilling Interpretable Models into Human-Readable Code", "comments": "13 pages, Latex; Updated the introduction and preliminaries sections;\n  Updated some figures for greater clarity and brevity; Added a new dataset to\n  the experiments; Added a more detailed table of experiment results; Added a\n  discussion of distillation failures to the experiments relating to the new\n  dataset", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The goal of model distillation is to faithfully transfer teacher model\nknowledge to a model which is faster, more generalizable, more interpretable,\nor possesses other desirable characteristics. Human-readability is an important\nand desirable standard for machine-learned model interpretability. Readable\nmodels are transparent and can be reviewed, manipulated, and deployed like\ntraditional source code. As a result, such models can be improved outside the\ncontext of machine learning and manually edited if desired. Given that directly\ntraining such models is difficult, we propose to train interpretable models\nusing conventional methods, and then distill them into concise, human-readable\ncode.\n  The proposed distillation methodology approximates a model's univariate\nnumerical functions with piecewise-linear curves in a localized manner. The\nresulting curve model representations are accurate, concise, human-readable,\nand well-regularized by construction. We describe a piecewise-linear\ncurve-fitting algorithm that produces high-quality results efficiently and\nreliably across a broad range of use cases. We demonstrate the effectiveness of\nthe overall distillation technique and our curve-fitting algorithm using four\ndatasets across the tasks of classification, regression, and ranking.\n", "versions": [{"version": "v1", "created": "Thu, 21 Jan 2021 01:46:36 GMT"}, {"version": "v2", "created": "Tue, 9 Feb 2021 02:13:24 GMT"}], "update_date": "2021-02-10", "authors_parsed": [["Ravina", "Walker", ""], ["Sterling", "Ethan", ""], ["Oryeshko", "Olexiy", ""], ["Bell", "Nathan", ""], ["Zhuang", "Honglei", ""], ["Wang", "Xuanhui", ""], ["Wu", "Yonghui", ""], ["Grushetsky", "Alexander", ""]]}, {"id": "2101.08398", "submitter": "Mustafa Hajij", "authors": "Mustafa Hajij, Ghada Zamzmi, Fawwaz Batayneh", "title": "TDA-Net: Fusion of Persistent Homology and Deep Learning Features for\n  COVID-19 Detection in Chest X-Ray Images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Topological Data Analysis (TDA) has emerged recently as a robust tool to\nextract and compare the structure of datasets. TDA identifies features in data\nsuch as connected components and holes and assigns a quantitative measure to\nthese features. Several studies reported that topological features extracted by\nTDA tools provide unique information about the data, discover new insights, and\ndetermine which feature is more related to the outcome. On the other hand, the\noverwhelming success of deep neural networks in learning patterns and\nrelationships has been proven on a vast array of data applications, images in\nparticular. To capture the characteristics of both powerful tools, we propose\n\\textit{TDA-Net}, a novel ensemble network that fuses topological and deep\nfeatures for the purpose of enhancing model generalizability and accuracy. We\napply the proposed \\textit{TDA-Net} to a critical application, which is the\nautomated detection of COVID-19 from CXR images. The experimental results\nshowed that the proposed network achieved excellent performance and suggests\nthe applicability of our method in practice.\n", "versions": [{"version": "v1", "created": "Thu, 21 Jan 2021 01:51:12 GMT"}, {"version": "v2", "created": "Sat, 19 Jun 2021 02:58:34 GMT"}], "update_date": "2021-06-22", "authors_parsed": [["Hajij", "Mustafa", ""], ["Zamzmi", "Ghada", ""], ["Batayneh", "Fawwaz", ""]]}, {"id": "2101.08408", "submitter": "Ziwen Liu", "authors": "Ziwen Liu, Mingqiang Li, Congying Han", "title": "Blocked and Hierarchical Disentangled Representation From Information\n  Theory Perspective", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.LG math.IT", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We propose a novel and theoretical model, blocked and hierarchical\nvariational autoencoder (BHiVAE), to get better-disentangled representation. It\nis well known that information theory has an excellent explanatory meaning for\nthe network, so we start to solve the disentanglement problem from the\nperspective of information theory. BHiVAE mainly comes from the information\nbottleneck theory and information maximization principle. Our main idea is that\n(1) Neurons block not only one neuron node is used to represent attribute,\nwhich can contain enough information; (2) Create a hierarchical structure with\ndifferent attributes on different layers, so that we can segment the\ninformation within each layer to ensure that the final representation is\ndisentangled. Furthermore, we present supervised and unsupervised BHiVAE,\nrespectively, where the difference is mainly reflected in the separation of\ninformation between different blocks. In supervised BHiVAE, we utilize the\nlabel information as the standard to separate blocks. In unsupervised BHiVAE,\nwithout extra information, we use the Total Correlation (TC) measure to achieve\nindependence, and we design a new prior distribution of the latent space to\nguide the representation learning. It also exhibits excellent disentanglement\nresults in experiments and superior classification accuracy in representation\nlearning.\n", "versions": [{"version": "v1", "created": "Thu, 21 Jan 2021 02:33:55 GMT"}], "update_date": "2021-01-22", "authors_parsed": [["Liu", "Ziwen", ""], ["Li", "Mingqiang", ""], ["Han", "Congying", ""]]}, {"id": "2101.08413", "submitter": "Hongjiang Wei", "authors": "Ruimin Feng, Jiayi Zhao, He Wang, Baofeng Yang, Jie Feng, Yuting Shi,\n  Ming Zhang, Chunlei Liu, Yuyao Zhang, Jie Zhuang, Hongjiang Wei", "title": "MoDL-QSM: Model-based Deep Learning for Quantitative Susceptibility\n  Mapping", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Quantitative susceptibility mapping (QSM) has demonstrated great potential in\nquantifying tissue susceptibility in various brain diseases. However, the\nintrinsic ill-posed inverse problem relating the tissue phase to the underlying\nsusceptibility distribution affects the accuracy for quantifying tissue\nsusceptibility. Recently, deep learning has shown promising results to improve\naccuracy by reducing the streaking artifacts. However, there exists a mismatch\nbetween the observed phase and the theoretical forward phase estimated by the\nsusceptibility label. In this study, we proposed a model-based deep learning\narchitecture that followed the STI (susceptibility tensor imaging) physical\nmodel, referred to as MoDL-QSM. Specifically, MoDL-QSM accounts for the\nrelationship between STI-derived phase contrast induced by the susceptibility\ntensor terms (ki13,ki23,ki33) and the acquired single-orientation phase. The\nconvolution neural networks are embedded into the physical model to learn a\nregularization term containing prior information. ki33 and phase induced by\nki13 and ki23 terms were used as the labels for network training. Quantitative\nevaluation metrics (RSME, SSIM, and HFEN) were compared with recently developed\ndeep learning QSM methods. The results showed that MoDL-QSM achieved superior\nperformance, demonstrating its potential for future applications.\n", "versions": [{"version": "v1", "created": "Thu, 21 Jan 2021 02:52:05 GMT"}, {"version": "v2", "created": "Thu, 20 May 2021 07:00:52 GMT"}], "update_date": "2021-05-21", "authors_parsed": [["Feng", "Ruimin", ""], ["Zhao", "Jiayi", ""], ["Wang", "He", ""], ["Yang", "Baofeng", ""], ["Feng", "Jie", ""], ["Shi", "Yuting", ""], ["Zhang", "Ming", ""], ["Liu", "Chunlei", ""], ["Zhang", "Yuyao", ""], ["Zhuang", "Jie", ""], ["Wei", "Hongjiang", ""]]}, {"id": "2101.08427", "submitter": "Suemin Lee", "authors": "Suemin Lee and Ivan V. Baji\\'c", "title": "Analysis of Information Flow Through U-Nets", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep Neural Networks (DNNs) have become ubiquitous in medical image\nprocessing and analysis. Among them, U-Nets are very popular in various image\nsegmentation tasks. Yet, little is known about how information flows through\nthese networks and whether they are indeed properly designed for the tasks they\nare being proposed for. In this paper, we employ information-theoretic tools in\norder to gain insight into information flow through U-Nets. In particular, we\nshow how mutual information between input/output and an intermediate layer can\nbe a useful tool to understand information flow through various portions of a\nU-Net, assess its architectural efficiency, and even propose more efficient\ndesigns.\n", "versions": [{"version": "v1", "created": "Thu, 21 Jan 2021 03:53:42 GMT"}, {"version": "v2", "created": "Fri, 2 Apr 2021 05:00:24 GMT"}], "update_date": "2021-04-05", "authors_parsed": [["Lee", "Suemin", ""], ["Baji\u0107", "Ivan V.", ""]]}, {"id": "2101.08435", "submitter": "Ke He", "authors": "Ke He, Le He, Lisheng Fan, Yansha Deng, George K. Karagiannidis, and\n  Arumugam Nallanathan", "title": "Learning based signal detection for MIMO systems with unknown noise\n  statistics", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.IT math.IT", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper aims to devise a generalized maximum likelihood (ML) estimator to\nrobustly detect signals with unknown noise statistics in multiple-input\nmultiple-output (MIMO) systems. In practice, there is little or even no\nstatistical knowledge on the system noise, which in many cases is non-Gaussian,\nimpulsive and not analyzable. Existing detection methods have mainly focused on\nspecific noise models, which are not robust enough with unknown noise\nstatistics. To tackle this issue, we propose a novel ML detection framework to\neffectively recover the desired signal. Our framework is a fully probabilistic\none that can efficiently approximate the unknown noise distribution through a\nnormalizing flow. Importantly, this framework is driven by an unsupervised\nlearning approach, where only the noise samples are required. To reduce the\ncomputational complexity, we further present a low-complexity version of the\nframework, by utilizing an initial estimation to reduce the search space.\nSimulation results show that our framework outperforms other existing\nalgorithms in terms of bit error rate (BER) in non-analytical noise\nenvironments, while it can reach the ML performance bound in analytical noise\nenvironments. The code of this paper is available at\nhttps://github.com/skypitcher/manfe.\n", "versions": [{"version": "v1", "created": "Thu, 21 Jan 2021 04:48:15 GMT"}], "update_date": "2021-01-22", "authors_parsed": [["He", "Ke", ""], ["He", "Le", ""], ["Fan", "Lisheng", ""], ["Deng", "Yansha", ""], ["Karagiannidis", "George K.", ""], ["Nallanathan", "Arumugam", ""]]}, {"id": "2101.08448", "submitter": "Kishor Bharti Mr.", "authors": "Kishor Bharti, Alba Cervera-Lierta, Thi Ha Kyaw, Tobias Haug, Sumner\n  Alperin-Lea, Abhinav Anand, Matthias Degroote, Hermanni Heimonen, Jakob S.\n  Kottmann, Tim Menke, Wai-Keong Mok, Sukin Sim, Leong-Chuan Kwek, Al\\'an\n  Aspuru-Guzik", "title": "Noisy intermediate-scale quantum (NISQ) algorithms", "comments": "Review article, 82 pages, 7 figures, comments welcome", "journal-ref": null, "doi": null, "report-no": null, "categories": "quant-ph cond-mat.stat-mech cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A universal fault-tolerant quantum computer that can solve efficiently\nproblems such as integer factorization and unstructured database search\nrequires millions of qubits with low error rates and long coherence times.\nWhile the experimental advancement towards realizing such devices will\npotentially take decades of research, noisy intermediate-scale quantum (NISQ)\ncomputers already exist. These computers are composed of hundreds of noisy\nqubits, i.e. qubits that are not error-corrected, and therefore perform\nimperfect operations in a limited coherence time. In the search for quantum\nadvantage with these devices, algorithms have been proposed for applications in\nvarious disciplines spanning physics, machine learning, quantum chemistry and\ncombinatorial optimization. The goal of such algorithms is to leverage the\nlimited available resources to perform classically challenging tasks. In this\nreview, we provide a thorough summary of NISQ computational paradigms and\nalgorithms. We discuss the key structure of these algorithms, their\nlimitations, and advantages. We additionally provide a comprehensive overview\nof various benchmarking and software tools useful for programming and testing\nNISQ devices.\n", "versions": [{"version": "v1", "created": "Thu, 21 Jan 2021 05:27:34 GMT"}], "update_date": "2021-01-22", "authors_parsed": [["Bharti", "Kishor", ""], ["Cervera-Lierta", "Alba", ""], ["Kyaw", "Thi Ha", ""], ["Haug", "Tobias", ""], ["Alperin-Lea", "Sumner", ""], ["Anand", "Abhinav", ""], ["Degroote", "Matthias", ""], ["Heimonen", "Hermanni", ""], ["Kottmann", "Jakob S.", ""], ["Menke", "Tim", ""], ["Mok", "Wai-Keong", ""], ["Sim", "Sukin", ""], ["Kwek", "Leong-Chuan", ""], ["Aspuru-Guzik", "Al\u00e1n", ""]]}, {"id": "2101.08449", "submitter": "Maxim Ziatdinov", "authors": "Ayana Ghosh, Bobby G. Sumpter, Ondrej Dyck, Sergei V. Kalinin, and\n  Maxim Ziatdinov", "title": "Ensemble learning and iterative training (ELIT) machine learning:\n  applications towards uncertainty quantification and automated experiment in\n  atom-resolved microscopy", "comments": "Add supplemental material", "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.data-an cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Deep learning has emerged as a technique of choice for rapid feature\nextraction across imaging disciplines, allowing rapid conversion of the data\nstreams to spatial or spatiotemporal arrays of features of interest. However,\napplications of deep learning in experimental domains are often limited by the\nout-of-distribution drift between the experiments, where the network trained\nfor one set of imaging conditions becomes sub-optimal for different ones. This\nlimitation is particularly stringent in the quest to have an automated\nexperiment setting, where retraining or transfer learning becomes impractical\ndue to the need for human intervention and associated latencies. Here we\nexplore the reproducibility of deep learning for feature extraction in\natom-resolved electron microscopy and introduce workflows based on ensemble\nlearning and iterative training to greatly improve feature detection. This\napproach both allows incorporating uncertainty quantification into the deep\nlearning analysis and also enables rapid automated experimental workflows where\nretraining of the network to compensate for out-of-distribution drift due to\nsubtle change in imaging conditions is substituted for a human operator or\nprogrammatic selection of networks from the ensemble. This methodology can be\nfurther applied to machine learning workflows in other imaging areas including\noptical and chemical imaging.\n", "versions": [{"version": "v1", "created": "Thu, 21 Jan 2021 05:29:26 GMT"}, {"version": "v2", "created": "Fri, 22 Jan 2021 01:58:29 GMT"}], "update_date": "2021-01-25", "authors_parsed": [["Ghosh", "Ayana", ""], ["Sumpter", "Bobby G.", ""], ["Dyck", "Ondrej", ""], ["Kalinin", "Sergei V.", ""], ["Ziatdinov", "Maxim", ""]]}, {"id": "2101.08452", "submitter": "Huan Zhang", "authors": "Huan Zhang, Hongge Chen, Duane Boning, Cho-Jui Hsieh", "title": "Robust Reinforcement Learning on State Observations with Learned Optimal\n  Adversary", "comments": "Accepted by ICLR 2021. Huan Zhang and Hongge Chen contributed equally", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the robustness of reinforcement learning (RL) with adversarially\nperturbed state observations, which aligns with the setting of many adversarial\nattacks to deep reinforcement learning (DRL) and is also important for rolling\nout real-world RL agent under unpredictable sensing noise. With a fixed agent\npolicy, we demonstrate that an optimal adversary to perturb state observations\ncan be found, which is guaranteed to obtain the worst case agent reward. For\nDRL settings, this leads to a novel empirical adversarial attack to RL agents\nvia a learned adversary that is much stronger than previous ones. To enhance\nthe robustness of an agent, we propose a framework of alternating training with\nlearned adversaries (ATLA), which trains an adversary online together with the\nagent using policy gradient following the optimal adversarial attack framework.\nAdditionally, inspired by the analysis of state-adversarial Markov decision\nprocess (SA-MDP), we show that past states and actions (history) can be useful\nfor learning a robust agent, and we empirically find a LSTM based policy can be\nmore robust under adversaries. Empirical evaluations on a few continuous\ncontrol environments show that ATLA achieves state-of-the-art performance under\nstrong adversaries. Our code is available at\nhttps://github.com/huanzhang12/ATLA_robust_RL.\n", "versions": [{"version": "v1", "created": "Thu, 21 Jan 2021 05:38:52 GMT"}], "update_date": "2021-01-22", "authors_parsed": [["Zhang", "Huan", ""], ["Chen", "Hongge", ""], ["Boning", "Duane", ""], ["Hsieh", "Cho-Jui", ""]]}, {"id": "2101.08454", "submitter": "Amir Hussein", "authors": "Amir Hussein, Shinji Watanabe, Ahmed Ali", "title": "Arabic Speech Recognition by End-to-End, Modular Systems and Human", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.AS cs.CL cs.LG cs.SD", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent advances in automatic speech recognition (ASR) have achieved accuracy\nlevels comparable to human transcribers, which led researchers to debate if the\nmachine has reached human performance. Previous work focused on the English\nlanguage and modular hidden Markov model-deep neural network (HMM-DNN) systems.\nIn this paper, we perform a comprehensive benchmarking for end-to-end\ntransformer ASR, modular HMM-DNN ASR, and human speech recognition (HSR) on the\nArabic language and its dialects. For the HSR, we evaluate linguist performance\nand lay-native speaker performance on a new dataset collected as a part of this\nstudy. For ASR the end-to-end work led to 12.5%, 27.5%, 33.8% WER; a new\nperformance milestone for the MGB2, MGB3, and MGB5 challenges respectively. Our\nresults suggest that human performance in the Arabic language is still\nconsiderably better than the machine with an absolute WER gap of 3.5% on\naverage.\n", "versions": [{"version": "v1", "created": "Thu, 21 Jan 2021 05:55:29 GMT"}, {"version": "v2", "created": "Tue, 29 Jun 2021 16:10:57 GMT"}], "update_date": "2021-06-30", "authors_parsed": [["Hussein", "Amir", ""], ["Watanabe", "Shinji", ""], ["Ali", "Ahmed", ""]]}, {"id": "2101.08458", "submitter": "Jian Weng", "authors": "Jian Weng, Animesh Jain, Jie Wang, Leyuan Wang, Yida Wang, and Tony\n  Nowatzki", "title": "UNIT: Unifying Tensorized Instruction Compilation", "comments": "13 pages, 13 figures, and 1 table", "journal-ref": "2021 IEEE/ACM International Symposium on Code Generation and\n  Optimization (CGO), Seoul, Korea (South), 2021, pp. 77-89", "doi": "10.1109/CGO51591.2021.9370330", "report-no": null, "categories": "cs.PL cs.AR cs.LG cs.PF", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Because of the increasing demand for computation in DNN, researchers develope\nboth hardware and software mechanisms to reduce the compute and memory burden.\nA widely adopted approach is to use mixed precision data types. However, it is\nhard to leverage mixed precision without hardware support because of the\noverhead of data casting. Hardware vendors offer tensorized instructions for\nmixed-precision tensor operations, like Intel VNNI, Tensor Core, and ARM-DOT.\nThese instructions involve a computing idiom that reduces multiple low\nprecision elements into one high precision element. The lack of compilation\ntechniques for this makes it hard to utilize these instructions: Using\nvendor-provided libraries for computationally-intensive kernels is inflexible\nand prevents further optimizations, and manually writing hardware intrinsics is\nerror-prone and difficult for programmers. Some prior works address this\nproblem by creating compilers for each instruction. This requires excessive\neffort when it comes to many tensorized instructions. In this work, we develop\na compiler framework to unify the compilation for these instructions -- a\nunified semantics abstraction eases the integration of new instructions, and\nreuses the analysis and transformations. Tensorized instructions from different\nplatforms can be compiled via UNIT with moderate effort for favorable\nperformance. Given a tensorized instruction and a tensor operation, UNIT\nautomatically detects the applicability, transforms the loop organization of\nthe operation,and rewrites the loop body to leverage the tensorized\ninstruction. According to our evaluation, UNIT can target various mainstream\nhardware platforms. The generated end-to-end inference model achieves 1.3x\nspeedup over Intel oneDNN on an x86 CPU, 1.75x speedup over Nvidia cuDNN on an\nNvidiaGPU, and 1.13x speedup over a carefully tuned TVM solution for ARM DOT on\nan ARM CPU.\n", "versions": [{"version": "v1", "created": "Thu, 21 Jan 2021 06:22:58 GMT"}, {"version": "v2", "created": "Tue, 23 Mar 2021 03:36:45 GMT"}, {"version": "v3", "created": "Sun, 28 Mar 2021 04:11:38 GMT"}], "update_date": "2021-03-30", "authors_parsed": [["Weng", "Jian", ""], ["Jain", "Animesh", ""], ["Wang", "Jie", ""], ["Wang", "Leyuan", ""], ["Wang", "Yida", ""], ["Nowatzki", "Tony", ""]]}, {"id": "2101.08471", "submitter": "Jianping Gou", "authors": "Liyuan Sun, Jianping Gou, Baosheng Yu, Lan Du, Dacheng Tao", "title": "Collaborative Teacher-Student Learning via Multiple Knowledge Transfer", "comments": "16 pages,5 figures,6 Tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Knowledge distillation (KD), as an efficient and effective model compression\ntechnique, has been receiving considerable attention in deep learning. The key\nto its success is to transfer knowledge from a large teacher network to a small\nstudent one. However, most of the existing knowledge distillation methods\nconsider only one type of knowledge learned from either instance features or\ninstance relations via a specific distillation strategy in teacher-student\nlearning. There are few works that explore the idea of transferring different\ntypes of knowledge with different distillation strategies in a unified\nframework. Moreover, the frequently used offline distillation suffers from a\nlimited learning capacity due to the fixed teacher-student architecture. In\nthis paper we propose a collaborative teacher-student learning via multiple\nknowledge transfer (CTSL-MKT) that prompts both self-learning and collaborative\nlearning. It allows multiple students learn knowledge from both individual\ninstances and instance relations in a collaborative way. While learning from\nthemselves with self-distillation, they can also guide each other via online\ndistillation. The experiments and ablation studies on four image datasets\ndemonstrate that the proposed CTSL-MKT significantly outperforms the\nstate-of-the-art KD methods.\n", "versions": [{"version": "v1", "created": "Thu, 21 Jan 2021 07:17:04 GMT"}, {"version": "v2", "created": "Wed, 27 Jan 2021 08:20:45 GMT"}], "update_date": "2021-01-28", "authors_parsed": [["Sun", "Liyuan", ""], ["Gou", "Jianping", ""], ["Yu", "Baosheng", ""], ["Du", "Lan", ""], ["Tao", "Dacheng", ""]]}, {"id": "2101.08477", "submitter": "Thesath Nanayakkara", "authors": "Thesath Nanayakkara, Gilles Clermont, Christopher James Langmead, and\n  David Swigon", "title": "Unifying Cardiovascular Modelling with Deep Reinforcement Learning for\n  Uncertainty Aware Control of Sepsis Treatment", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI q-bio.QM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sepsis is a potentially life threatening inflammatory response to infection\nor severe tissue damage. It has a highly variable clinical course, requiring\nconstant monitoring of the patient's state to guide the management of\nintravenous fluids and vasopressors, among other interventions. Despite decades\nof research, there's still debate among experts on optimal treatment. Here, we\ncombine for the first time, distributional deep reinforcement learning with\nmechanistic physiological models to find personalized sepsis treatment\nstrategies. Our method handles partial observability by leveraging known\ncardiovascular physiology, introducing a novel physiology-driven recurrent\nautoencoder, and quantifies the uncertainty of its own results. Moreover, we\nintroduce a framework for uncertainty aware decision support with humans in the\nloop. We show that our method learns physiologically explainable, robust\npolicies that are consistent with clinical knowledge. Further our method\nconsistently identifies high risk states that lead to death, which could\npotentially benefit from more frequent vasopressor administration, providing\nvaluable guidance for future research\n", "versions": [{"version": "v1", "created": "Thu, 21 Jan 2021 07:32:02 GMT"}, {"version": "v2", "created": "Tue, 2 Feb 2021 05:55:47 GMT"}, {"version": "v3", "created": "Sat, 12 Jun 2021 18:48:03 GMT"}], "update_date": "2021-06-15", "authors_parsed": [["Nanayakkara", "Thesath", ""], ["Clermont", "Gilles", ""], ["Langmead", "Christopher James", ""], ["Swigon", "David", ""]]}, {"id": "2101.08482", "submitter": "Zhaowei Cai", "authors": "Zhaowei Cai, Avinash Ravichandran, Subhransu Maji, Charless Fowlkes,\n  Zhuowen Tu, Stefano Soatto", "title": "Exponential Moving Average Normalization for Self-supervised and\n  Semi-supervised Learning", "comments": "accepted by CVPR21 as Oral presentation", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a plug-in replacement for batch normalization (BN) called\nexponential moving average normalization (EMAN), which improves the performance\nof existing student-teacher based self- and semi-supervised learning\ntechniques. Unlike the standard BN, where the statistics are computed within\neach batch, EMAN, used in the teacher, updates its statistics by exponential\nmoving average from the BN statistics of the student. This design reduces the\nintrinsic cross-sample dependency of BN and enhances the generalization of the\nteacher. EMAN improves strong baselines for self-supervised learning by 4-6/1-2\npoints and semi-supervised learning by about 7/2 points, when 1%/10% supervised\nlabels are available on ImageNet. These improvements are consistent across\nmethods, network architectures, training duration, and datasets, demonstrating\nthe general effectiveness of this technique. The code is available at\nhttps://github.com/amazon-research/exponential-moving-average-normalization.\n", "versions": [{"version": "v1", "created": "Thu, 21 Jan 2021 07:45:37 GMT"}, {"version": "v2", "created": "Fri, 18 Jun 2021 08:16:27 GMT"}], "update_date": "2021-06-21", "authors_parsed": [["Cai", "Zhaowei", ""], ["Ravichandran", "Avinash", ""], ["Maji", "Subhransu", ""], ["Fowlkes", "Charless", ""], ["Tu", "Zhuowen", ""], ["Soatto", "Stefano", ""]]}, {"id": "2101.08486", "submitter": "Pratyush Kumar", "authors": "Pratyush Kumar, Aishwarya Das, Debayan Gupta", "title": "Differential Euler: Designing a Neural Network approximator to solve the\n  Chaotic Three Body Problem", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The three body problem is a special case of the n body problem where one\ntakes the initial positions and velocities of three point masses and attempts\nto predict their motion over time according to Newtonian laws of motion and\nuniversal gravitation. Though analytical solutions have been found for special\ncases, the general problem remains unsolved; the solutions that do exist are\nimpractical. Fortunately, for many applications, we may not need to solve the\nproblem completely, i.e., predicting with reasonable accuracy for some time\nsteps, may be sufficient. Recently, Breen et al attempted to approximately\nsolve the three body problem using a simple neural network. Although their\nmethods appear to achieve some success in reducing the computational overhead,\ntheir model is extremely restricted, applying to a specialized 2D case. The\nauthors do not provide explanations for critical decisions taken in their\nexperimental design, no details on their model or architecture, and nor do they\npublish their code. Moreover, the model does not generalize well to unseen\ncases. In this paper, we propose a detailed experimental setup to determine the\nfeasibility of using neural networks to solve the three body problem up to a\ncertain number of time steps. We establish a benchmark on the dataset size and\nset an accuracy threshold to measure the viability of our results for practical\napplications. Then, we build our models according to the listed class of NNs\nusing a dataset generated from standard numerical integrators. We gradually\nincrease the complexity of our data set to determine whether NNs can learn a\nrepresentation of the chaotic three body problem well enough to replace\nnumerical integrators in real life scenarios.\n", "versions": [{"version": "v1", "created": "Thu, 21 Jan 2021 07:52:46 GMT"}], "update_date": "2021-01-22", "authors_parsed": [["Kumar", "Pratyush", ""], ["Das", "Aishwarya", ""], ["Gupta", "Debayan", ""]]}, {"id": "2101.08490", "submitter": "Tobias Hatt", "authors": "Tobias Hatt, Stefan Feuerriegel", "title": "Estimating Average Treatment Effects via Orthogonal Regularization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Decision-making often requires accurate estimation of treatment effects from\nobservational data. This is challenging as outcomes of alternative decisions\nare not observed and have to be estimated. Previous methods estimate outcomes\nbased on unconfoundedness but neglect any constraints that unconfoundedness\nimposes on the outcomes. In this paper, we propose a novel regularization\nframework for estimating average treatment effects that exploits\nunconfoundedness. To this end, we formalize unconfoundedness as an\northogonality constraint, which ensures that the outcomes are orthogonal to the\ntreatment assignment. This orthogonality constraint is then included in the\nloss function via a regularization. Based on our regularization framework, we\ndevelop deep orthogonal networks for unconfounded treatments (DONUT), which\nlearn outcomes that are orthogonal to the treatment assignment. Using a variety\nof benchmark datasets for estimating average treatment effects, we demonstrate\nthat DONUT outperforms the state-of-the-art substantially.\n", "versions": [{"version": "v1", "created": "Thu, 21 Jan 2021 08:05:35 GMT"}], "update_date": "2021-01-22", "authors_parsed": [["Hatt", "Tobias", ""], ["Feuerriegel", "Stefan", ""]]}, {"id": "2101.08505", "submitter": "YunPeng Li", "authors": "YunPeng Li, ZhaoHui Ye", "title": "Boosting in Univariate Nonparametric Maximum Likelihood Estimation", "comments": null, "journal-ref": null, "doi": "10.1109/LSP.2021.3065881", "report-no": null, "categories": "stat.ML cs.LG eess.SP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Nonparametric maximum likelihood estimation is intended to infer the unknown\ndensity distribution while making as few assumptions as possible. To alleviate\nthe over parameterization in nonparametric data fitting, smoothing assumptions\nare usually merged into the estimation. In this paper a novel boosting-based\nmethod is introduced to the nonparametric estimation in univariate cases. We\ndeduce the boosting algorithm by the second-order approximation of\nnonparametric log-likelihood. Gaussian kernel and smooth spline are chosen as\nweak learners in boosting to satisfy the smoothing assumptions. Simulations and\nreal data experiments demonstrate the efficacy of the proposed approach.\n", "versions": [{"version": "v1", "created": "Thu, 21 Jan 2021 08:46:33 GMT"}], "update_date": "2021-04-21", "authors_parsed": [["Li", "YunPeng", ""], ["Ye", "ZhaoHui", ""]]}, {"id": "2101.08508", "submitter": "Francesca Del Bonifro", "authors": "Francesca Del Bonifro, Maurizio Gabbrielli, Stefano Zacchiroli", "title": "Content-Based Textual File Type Detection at Scale", "comments": null, "journal-ref": "ICMLC 2021 - Proceedings of International Conference on Machine\n  Learning and Computing 2021", "doi": null, "report-no": null, "categories": "cs.SE cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Programming language detection is a common need in the analysis of large\nsource code bases. It is supported by a number of existing tools that rely on\nseveral features, and most notably file extensions, to determine file types. We\nconsider the problem of accurately detecting the type of files commonly found\nin software code bases, based solely on textual file content. Doing so is\nhelpful to classify source code that lack file extensions (e.g., code snippets\nposted on the Web or executable scripts), to avoid misclassifying source code\nthat has been recorded with wrong or uncommon file extensions, and also shed\nsome light on the intrinsic recognizability of source code files. We propose a\nsimple model that (a) use a language-agnostic word tokenizer for textual files,\n(b) group tokens in 1-/2-grams, (c) build feature vectors based on N-gram\nfrequencies, and (d) use a simple fully connected neural network as classifier.\nAs training set we use textual files extracted from GitHub repositories with at\nleast 1000 stars, using existing file extensions as ground truth. Despite its\nsimplicity the proposed model reaches 85% in our experiments for a relatively\nhigh number of recognized classes (more than 130 file types).\n", "versions": [{"version": "v1", "created": "Thu, 21 Jan 2021 09:08:42 GMT"}], "update_date": "2021-03-02", "authors_parsed": [["Del Bonifro", "Francesca", ""], ["Gabbrielli", "Maurizio", ""], ["Zacchiroli", "Stefano", ""]]}, {"id": "2101.08515", "submitter": "Hirokatsu Kataoka", "authors": "Hirokatsu Kataoka and Kazushige Okayasu and Asato Matsumoto and Eisuke\n  Yamagata and Ryosuke Yamada and Nakamasa Inoue and Akio Nakamura and Yutaka\n  Satoh", "title": "Pre-training without Natural Images", "comments": "ACCV 2020 Best Paper Honorable Mention Award, Codes are publicly\n  available:\n  https://github.com/hirokatsukataoka16/FractalDB-Pretrained-ResNet-PyTorch", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Is it possible to use convolutional neural networks pre-trained without any\nnatural images to assist natural image understanding? The paper proposes a\nnovel concept, Formula-driven Supervised Learning. We automatically generate\nimage patterns and their category labels by assigning fractals, which are based\non a natural law existing in the background knowledge of the real world.\nTheoretically, the use of automatically generated images instead of natural\nimages in the pre-training phase allows us to generate an infinite scale\ndataset of labeled images. Although the models pre-trained with the proposed\nFractal DataBase (FractalDB), a database without natural images, does not\nnecessarily outperform models pre-trained with human annotated datasets at all\nsettings, we are able to partially surpass the accuracy of ImageNet/Places\npre-trained models. The image representation with the proposed FractalDB\ncaptures a unique feature in the visualization of convolutional layers and\nattentions.\n", "versions": [{"version": "v1", "created": "Thu, 21 Jan 2021 09:47:32 GMT"}], "update_date": "2021-01-22", "authors_parsed": [["Kataoka", "Hirokatsu", ""], ["Okayasu", "Kazushige", ""], ["Matsumoto", "Asato", ""], ["Yamagata", "Eisuke", ""], ["Yamada", "Ryosuke", ""], ["Inoue", "Nakamasa", ""], ["Nakamura", "Akio", ""], ["Satoh", "Yutaka", ""]]}, {"id": "2101.08521", "submitter": "Chuan-Long Xie", "authors": "Haotian Ye, Chuanlong Xie, Yue Liu, Zhenguo Li", "title": "Out-of-Distribution Generalization Analysis via Influence Function", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The mismatch between training and target data is one major challenge for\ncurrent machine learning systems. When training data is collected from multiple\ndomains and the target domains include all training domains and other new\ndomains, we are facing an Out-of-Distribution (OOD) generalization problem that\naims to find a model with the best OOD accuracy. One of the definitions of OOD\naccuracy is worst-domain accuracy. In general, the set of target domains is\nunknown, and the worst over target domains may be unseen when the number of\nobserved domains is limited. In this paper, we show that the worst accuracy\nover the observed domains may dramatically fail to identify the OOD accuracy.\nTo this end, we introduce Influence Function, a classical tool from robust\nstatistics, into the OOD generalization problem and suggest the variance of\ninfluence function to monitor the stability of a model on training domains. We\nshow that the accuracy on test domains and the proposed index together can help\nus discern whether OOD algorithms are needed and whether a model achieves good\nOOD generalization.\n", "versions": [{"version": "v1", "created": "Thu, 21 Jan 2021 09:59:55 GMT"}], "update_date": "2021-01-22", "authors_parsed": [["Ye", "Haotian", ""], ["Xie", "Chuanlong", ""], ["Liu", "Yue", ""], ["Li", "Zhenguo", ""]]}, {"id": "2101.08523", "submitter": "Ashutosh Modi", "authors": "Vijit Malik and Ashwani Bhat and Ashutosh Modi", "title": "Adv-OLM: Generating Textual Adversaries via OLM", "comments": "5 Pages + 1 Page references + 3 Pages Appendix, Accepted at EACL 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Deep learning models are susceptible to adversarial examples that have\nimperceptible perturbations in the original input, resulting in adversarial\nattacks against these models. Analysis of these attacks on the state of the art\ntransformers in NLP can help improve the robustness of these models against\nsuch adversarial inputs. In this paper, we present Adv-OLM, a black-box attack\nmethod that adapts the idea of Occlusion and Language Models (OLM) to the\ncurrent state of the art attack methods. OLM is used to rank words of a\nsentence, which are later substituted using word replacement strategies. We\nexperimentally show that our approach outperforms other attack methods for\nseveral text classification tasks.\n", "versions": [{"version": "v1", "created": "Thu, 21 Jan 2021 10:04:56 GMT"}], "update_date": "2021-01-22", "authors_parsed": [["Malik", "Vijit", ""], ["Bhat", "Ashwani", ""], ["Modi", "Ashutosh", ""]]}, {"id": "2101.08534", "submitter": "Marc Jourdan", "authors": "Marc Jourdan, Mojm\\'ir Mutn\\'y, Johannes Kirschner, Andreas Krause", "title": "Efficient Pure Exploration for Combinatorial Bandits with Semi-Bandit\n  Feedback", "comments": "45 pages. 3 tables. Appendices: from A to I. Figures: 1(a), 1(b),\n  2(a), 2(b), 3(a), 3(b), 3(c), 4(a), 4(b), 5(a), 5(b), 5(c), 5(d), 6(a), 6(b).\n  To be published in the 32nd International Conference on Algorithmic Learning\n  Theory and the Proceedings of Machine Learning Research vol 132:1-45, 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Combinatorial bandits with semi-bandit feedback generalize multi-armed\nbandits, where the agent chooses sets of arms and observes a noisy reward for\neach arm contained in the chosen set. The action set satisfies a given\nstructure such as forming a base of a matroid or a path in a graph. We focus on\nthe pure-exploration problem of identifying the best arm with fixed confidence,\nas well as a more general setting, where the structure of the answer set\ndiffers from the one of the action set. Using the recently popularized game\nframework, we interpret this problem as a sequential zero-sum game and develop\na CombGame meta-algorithm whose instances are asymptotically optimal algorithms\nwith finite time guarantees. In addition to comparing two families of learners\nto instantiate our meta-algorithm, the main contribution of our work is a\nspecific oracle efficient instance for best-arm identification with\ncombinatorial actions. Based on a projection-free online learning algorithm for\nconvex polytopes, it is the first computationally efficient algorithm which is\nasymptotically optimal and has competitive empirical performance.\n", "versions": [{"version": "v1", "created": "Thu, 21 Jan 2021 10:35:09 GMT"}], "update_date": "2021-01-22", "authors_parsed": [["Jourdan", "Marc", ""], ["Mutn\u00fd", "Mojm\u00edr", ""], ["Kirschner", "Johannes", ""], ["Krause", "Andreas", ""]]}, {"id": "2101.08539", "submitter": "Sikai Zhang", "authors": "Sikai Zhang, Zi-Qiang Lang", "title": "Orthogonal Least Squares Based Fast Feature Selection for Linear\n  Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  An Orthogonal Least Squares (OLS) based feature selection method is proposed\nfor both binomial and multinomial classification. The novel Squared Orthogonal\nCorrelation Coefficient (SOCC) is defined based on Error Reduction Ratio (ERR)\nin OLS and used as the feature ranking criterion. The equivalence between the\ncanonical correlation coefficient, Fisher's criterion, and the sum of the SOCCs\nis revealed, which unveils the statistical implication of ERR in OLS for the\nfirst time. It is also shown that the OLS based feature selection method has\nspeed advantages when applied for greedy search. The proposed method is\ncomprehensively compared with the mutual information based feature selection\nmethods in 2 synthetic and 7 real world datasets. The results show that the\nproposed method is always in the top 5 among the 10 candidate methods. Besides,\nthe proposed method can be directly applied to continuous features without\ndiscretisation, which is another significant advantage over mutual information\nbased methods.\n", "versions": [{"version": "v1", "created": "Thu, 21 Jan 2021 10:42:06 GMT"}, {"version": "v2", "created": "Wed, 9 Jun 2021 10:49:53 GMT"}], "update_date": "2021-06-10", "authors_parsed": [["Zhang", "Sikai", ""], ["Lang", "Zi-Qiang", ""]]}, {"id": "2101.08543", "submitter": "Sergei Ivanov", "authors": "Sergei Ivanov, Liudmila Prokhorenkova", "title": "Boost then Convolve: Gradient Boosting Meets Graph Neural Networks", "comments": "ICLR 2021: camera-ready", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Graph neural networks (GNNs) are powerful models that have been successful in\nvarious graph representation learning tasks. Whereas gradient boosted decision\ntrees (GBDT) often outperform other machine learning methods when faced with\nheterogeneous tabular data. But what approach should be used for graphs with\ntabular node features? Previous GNN models have mostly focused on networks with\nhomogeneous sparse features and, as we show, are suboptimal in the\nheterogeneous setting. In this work, we propose a novel architecture that\ntrains GBDT and GNN jointly to get the best of both worlds: the GBDT model\ndeals with heterogeneous features, while GNN accounts for the graph structure.\nOur model benefits from end-to-end optimization by allowing new trees to fit\nthe gradient updates of GNN. With an extensive experimental comparison to the\nleading GBDT and GNN models, we demonstrate a significant increase in\nperformance on a variety of graphs with tabular features. The code is\navailable: https://github.com/nd7141/bgnn.\n", "versions": [{"version": "v1", "created": "Thu, 21 Jan 2021 10:46:41 GMT"}, {"version": "v2", "created": "Wed, 31 Mar 2021 12:13:30 GMT"}], "update_date": "2021-04-01", "authors_parsed": [["Ivanov", "Sergei", ""], ["Prokhorenkova", "Liudmila", ""]]}, {"id": "2101.08551", "submitter": "Robert Verschuren", "authors": "Robert Matthijs Verschuren", "title": "Customer Price Sensitivities in Competitive Automobile Insurance Markets", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Insurers are increasingly adopting more demand-based strategies to\nincorporate the indirect effect of premium changes on their policyholders'\nwillingness to stay. However, since in practice both insurers' renewal premia\nand customers' responses to these premia typically depend on the customer's\nlevel of risk, it remains challenging in these strategies to determine how to\nproperly control for this confounding. We therefore consider a causal inference\napproach in this paper to account for customer price sensitivities and to\ndeduce optimal, multi-period profit maximizing premium renewal offers. More\nspecifically, we extend the discrete treatment framework of Guelman and\nGuill\\'en (2014) by Extreme Gradient Boosting, or XGBoost, and by multiple\nimputation to better account for the uncertainty in the counterfactual\nresponses. We additionally introduce the continuous treatment framework with\nXGBoost to the insurance literature to allow identification of the exact\noptimal renewal offers and account for any competition in the market by\nincluding competitor offers. The application of the two treatment frameworks to\na Dutch automobile insurance portfolio suggests that a policy's competitiveness\nin the market is crucial for a customer's price sensitivity and that XGBoost is\nmore appropriate to describe this than the traditional logistic regression.\nMoreover, an efficient frontier of both frameworks indicates that substantially\nmore profit can be gained on the portfolio than realized, also already with\nless churn and in particular if we allow for continuous rate changes. A\nmulti-period renewal optimization confirms these findings and demonstrates that\nthe competitiveness enables temporal feedback of previous rate changes on\nfuture demand.\n", "versions": [{"version": "v1", "created": "Thu, 21 Jan 2021 11:07:20 GMT"}], "update_date": "2021-01-22", "authors_parsed": [["Verschuren", "Robert Matthijs", ""]]}, {"id": "2101.08576", "submitter": "Quynh Nguyen", "authors": "Quynh Nguyen", "title": "A Note on Connectivity of Sublevel Sets in Deep Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is shown that for deep neural networks, a single wide layer of width $N+1$\n($N$ being the number of training samples) suffices to prove the connectivity\nof sublevel sets of the training loss function. In the two-layer setting, the\nsame property may not hold even if one has just one neuron less (i.e. width $N$\ncan lead to disconnected sublevel sets).\n", "versions": [{"version": "v1", "created": "Thu, 21 Jan 2021 12:43:26 GMT"}], "update_date": "2021-01-22", "authors_parsed": [["Nguyen", "Quynh", ""]]}, {"id": "2101.08578", "submitter": "Joosep Pata", "authors": "Joosep Pata, Javier Duarte, Jean-Roch Vlimant, Maurizio Pierini, Maria\n  Spiropulu", "title": "MLPF: Efficient machine-learned particle-flow reconstruction using graph\n  neural networks", "comments": "15 pages, 10 figures", "journal-ref": "Eur. Phys. J. C (2021) 81: 381", "doi": "10.1140/epjc/s10052-021-09158-w", "report-no": null, "categories": "physics.data-an cs.LG hep-ex physics.ins-det stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In general-purpose particle detectors, the particle-flow algorithm may be\nused to reconstruct a comprehensive particle-level view of the event by\ncombining information from the calorimeters and the trackers, significantly\nimproving the detector resolution for jets and the missing transverse momentum.\nIn view of the planned high-luminosity upgrade of the CERN Large Hadron\nCollider (LHC), it is necessary to revisit existing reconstruction algorithms\nand ensure that both the physics and computational performance are sufficient\nin an environment with many simultaneous proton-proton interactions (pileup).\nMachine learning may offer a prospect for computationally efficient event\nreconstruction that is well-suited to heterogeneous computing platforms, while\nsignificantly improving the reconstruction quality over rule-based algorithms\nfor granular detectors. We introduce MLPF, a novel, end-to-end trainable,\nmachine-learned particle-flow algorithm based on parallelizable,\ncomputationally efficient, and scalable graph neural networks optimized using a\nmulti-task objective on simulated events. We report the physics and\ncomputational performance of the MLPF algorithm on a Monte Carlo dataset of top\nquark-antiquark pairs produced in proton-proton collisions in conditions\nsimilar to those expected for the high-luminosity LHC. The MLPF algorithm\nimproves the physics response with respect to a rule-based benchmark algorithm\nand demonstrates computationally scalable particle-flow reconstruction in a\nhigh-pileup environment.\n", "versions": [{"version": "v1", "created": "Thu, 21 Jan 2021 12:47:54 GMT"}, {"version": "v2", "created": "Wed, 10 Mar 2021 15:18:59 GMT"}, {"version": "v3", "created": "Wed, 9 Jun 2021 06:52:41 GMT"}], "update_date": "2021-06-10", "authors_parsed": [["Pata", "Joosep", ""], ["Duarte", "Javier", ""], ["Vlimant", "Jean-Roch", ""], ["Pierini", "Maurizio", ""], ["Spiropulu", "Maria", ""]]}, {"id": "2101.08579", "submitter": "Jingxin Zhang", "authors": "Jingxin Zhang and Donghua Zhou and Maoyin Chen", "title": "Monitoring nonstationary processes based on recursive cointegration\n  analysis and elastic weight consolidation", "comments": "This paper has been submitted to IEEE Transaction on Cybernetics for\n  potential publication", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SY cs.LG cs.SY", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper considers the problem of nonstationary process monitoring under\nfrequently varying operating conditions. Traditional approaches generally\nmisidentify the normal dynamic deviations as faults and thus lead to high false\nalarms. Besides, they generally consider single relatively steady operating\ncondition and suffer from the catastrophic forgetting issue when learning\nsuccessive operating conditions. In this paper, recursive cointegration\nanalysis (RCA) is first proposed to distinguish the real faults from normal\nsystems changes, where the model is updated once a new normal sample arrives\nand can adapt to slow change of cointegration relationship. Based on the\nlong-term equilibrium information extracted by RCA, the remaining short-term\ndynamic information is monitored by recursive principal component analysis\n(RPCA). Thus a comprehensive monitoring framework is built. When the system\nenters a new operating condition, the RCA-RPCA model is rebuilt to deal with\nthe new condition. Meanwhile, elastic weight consolidation (EWC) is employed to\nsettle the `catastrophic forgetting' issue inherent in RPCA, where significant\ninformation of influential parameters is enhanced to avoid the abrupt\nperformance degradation for similar modes. The effectiveness of the proposed\nmethod is illustrated by a practical industrial system.\n", "versions": [{"version": "v1", "created": "Thu, 21 Jan 2021 12:49:18 GMT"}], "update_date": "2021-01-22", "authors_parsed": [["Zhang", "Jingxin", ""], ["Zhou", "Donghua", ""], ["Chen", "Maoyin", ""]]}, {"id": "2101.08585", "submitter": "Hadi Moradi", "authors": "Abolfazl Nadi, Hadi Moradi, Khalil Taheri", "title": "Crossbreeding in Random Forest", "comments": "21 pages, 5301 words, 9 Figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Ensemble learning methods are designed to benefit from multiple learning\nalgorithms for better predictive performance. The tradeoff of this improved\nperformance is slower speed and larger size of ensemble learning systems\ncompared to single learning systems. In this paper, we present a novel approach\nto deal with this problem in Random Forest (RF) as one of the most powerful\nensemble methods. The method is based on crossbreeding of the best tree\nbranches to increase the performance of RF in space and speed while keeping the\nperformance in the classification measures. The proposed approach has been\ntested on a group of synthetic and real datasets and compared to the standard\nRF approach. Several evaluations have been conducted to determine the effects\nof the Crossbred RF (CRF) on the accuracy and the number of trees in a forest.\nThe results show better performance of CRF compared to RF.\n", "versions": [{"version": "v1", "created": "Thu, 21 Jan 2021 12:58:54 GMT"}], "update_date": "2021-01-22", "authors_parsed": [["Nadi", "Abolfazl", ""], ["Moradi", "Hadi", ""], ["Taheri", "Khalil", ""]]}, {"id": "2101.08587", "submitter": "Aroof Aimen", "authors": "Aroof Aimen, Sahil Sidheekh, Vineet Madan, Narayanan C. Krishnan", "title": "Stress Testing of Meta-learning Approaches for Few-shot Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Meta-learning (ML) has emerged as a promising learning method under resource\nconstraints such as few-shot learning. ML approaches typically propose a\nmethodology to learn generalizable models. In this work-in-progress paper, we\nput the recent ML approaches to a stress test to discover their limitations.\nPrecisely, we measure the performance of ML approaches for few-shot learning\nagainst increasing task complexity. Our results show a quick degradation in the\nperformance of initialization strategies for ML (MAML, TAML, and MetaSGD),\nwhile surprisingly, approaches that use an optimization strategy (MetaLSTM)\nperform significantly better. We further demonstrate the effectiveness of an\noptimization strategy for ML (MetaLSTM++) trained in a MAML manner over a pure\noptimization strategy. Our experiments also show that the optimization\nstrategies for ML achieve higher transferability from simple to complex tasks.\n", "versions": [{"version": "v1", "created": "Thu, 21 Jan 2021 13:00:10 GMT"}], "update_date": "2021-01-22", "authors_parsed": [["Aimen", "Aroof", ""], ["Sidheekh", "Sahil", ""], ["Madan", "Vineet", ""], ["Krishnan", "Narayanan C.", ""]]}, {"id": "2101.08596", "submitter": "Neil Zeghidour", "authors": "Neil Zeghidour, Olivier Teboul, F\\'elix de Chaumont Quitry, Marco\n  Tagliasacchi", "title": "LEAF: A Learnable Frontend for Audio Classification", "comments": "Accepted at ICLR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.LG eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mel-filterbanks are fixed, engineered audio features which emulate human\nperception and have been used through the history of audio understanding up to\ntoday. However, their undeniable qualities are counterbalanced by the\nfundamental limitations of handmade representations. In this work we show that\nwe can train a single learnable frontend that outperforms mel-filterbanks on a\nwide range of audio signals, including speech, music, audio events and animal\nsounds, providing a general-purpose learned frontend for audio classification.\nTo do so, we introduce a new principled, lightweight, fully learnable\narchitecture that can be used as a drop-in replacement of mel-filterbanks. Our\nsystem learns all operations of audio features extraction, from filtering to\npooling, compression and normalization, and can be integrated into any neural\nnetwork at a negligible parameter cost. We perform multi-task training on eight\ndiverse audio classification tasks, and show consistent improvements of our\nmodel over mel-filterbanks and previous learnable alternatives. Moreover, our\nsystem outperforms the current state-of-the-art learnable frontend on Audioset,\nwith orders of magnitude fewer parameters.\n", "versions": [{"version": "v1", "created": "Thu, 21 Jan 2021 13:25:58 GMT"}], "update_date": "2021-01-22", "authors_parsed": [["Zeghidour", "Neil", ""], ["Teboul", "Olivier", ""], ["Quitry", "F\u00e9lix de Chaumont", ""], ["Tagliasacchi", "Marco", ""]]}, {"id": "2101.08609", "submitter": "Jinhai Yang", "authors": "Jinhai Yang, Hua Yang", "title": "MPASNET: Motion Prior-Aware Siamese Network for Unsupervised Deep Crowd\n  Segmentation in Video Scenes", "comments": "ICIP 2021 Camera Ready", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Crowd segmentation is a fundamental task serving as the basis of crowded\nscene analysis, and it is highly desirable to obtain refined pixel-level\nsegmentation maps. However, it remains a challenging problem, as existing\napproaches either require dense pixel-level annotations to train deep learning\nmodels or merely produce rough segmentation maps from optical or particle flows\nwith physical models. In this paper, we propose the Motion Prior-Aware Siamese\nNetwork (MPASNET) for unsupervised crowd semantic segmentation. This model not\nonly eliminates the need for annotation but also yields high-quality\nsegmentation maps. Specially, we first analyze the coherent motion patterns\nacross the frames and then apply a circular region merging strategy on the\ncollective particles to generate pseudo-labels. Moreover, we equip MPASNET with\nsiamese branches for augmentation-invariant regularization and siamese feature\naggregation. Experiments over benchmark datasets indicate that our model\noutperforms the state-of-the-arts by more than 12% in terms of mIoU.\n", "versions": [{"version": "v1", "created": "Thu, 21 Jan 2021 13:55:29 GMT"}, {"version": "v2", "created": "Wed, 2 Jun 2021 05:02:45 GMT"}], "update_date": "2021-06-03", "authors_parsed": [["Yang", "Jinhai", ""], ["Yang", "Hua", ""]]}, {"id": "2101.08623", "submitter": "Christian Toth", "authors": "Christian Toth, Denis Helic, Bernhard C. Geiger", "title": "Synwalk -- Community Detection via Random Walk Modelling", "comments": "31 pages, 13 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.LG physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Complex systems, abstractly represented as networks, are ubiquitous in\neveryday life. Analyzing and understanding these systems requires, among\nothers, tools for community detection. As no single best community detection\nalgorithm can exist, robustness across a wide variety of problem settings is\ndesirable. In this work, we present Synwalk, a random walk-based community\ndetection method. Synwalk builds upon a solid theoretical basis and detects\ncommunities by synthesizing the random walk induced by the given network from a\nclass of candidate random walks. We thoroughly validate the effectiveness of\nour approach on synthetic and empirical networks, respectively, and compare\nSynwalk's performance with the performance of Infomap and Walktrap. Our results\nindicate that Synwalk performs robustly on networks with varying mixing\nparameters and degree distributions. We outperform Infomap on networks with\nhigh mixing parameter, and Infomap and Walktrap on networks with many small\ncommunities and low average degree. Our work has a potential to inspire further\ndevelopment of community detection via synthesis of random walks and we provide\nconcrete ideas for future research.\n", "versions": [{"version": "v1", "created": "Thu, 21 Jan 2021 14:13:27 GMT"}], "update_date": "2021-01-22", "authors_parsed": [["Toth", "Christian", ""], ["Helic", "Denis", ""], ["Geiger", "Bernhard C.", ""]]}, {"id": "2101.08655", "submitter": "Leonardo Milhomem Franco Christino", "authors": "Leonardo Christino, Martha D. Ferreira, Asal Jalilvand and Fernando V.\n  Paulovich", "title": "Explainable Patterns: Going from Findings to Insights to Support Data\n  Analytics Democratization", "comments": "8 Figures, 10 pages, submitted to VIS 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the past decades, massive efforts involving companies, non-profit\norganizations, governments, and others have been put into supporting the\nconcept of data democratization, promoting initiatives to educate people to\nconfront information with data. Although this represents one of the most\ncritical advances in our free world, access to data without concrete facts to\ncheck or the lack of an expert to help on understanding the existing patterns\nhampers its intrinsic value and lessens its democratization. So the benefits of\ngiving full access to data will only be impactful if we go a step further and\nsupport the Data Analytics Democratization, assisting users in transforming\nfindings into insights without the need of domain experts to promote\nunconstrained access to data interpretation and verification. In this paper, we\npresent Explainable Patterns (ExPatt), a new framework to support lay users in\nexploring and creating data storytellings, automatically generating plausible\nexplanations for observed or selected findings using an external (textual)\nsource of information, avoiding or reducing the need for domain experts. ExPatt\napplicability is confirmed via different use-cases involving world demographics\nindicators and Wikipedia as an external source of explanations, showing how it\ncan be used in practice towards the data analytics democratization.\n", "versions": [{"version": "v1", "created": "Tue, 19 Jan 2021 16:13:44 GMT"}], "update_date": "2021-01-22", "authors_parsed": [["Christino", "Leonardo", ""], ["Ferreira", "Martha D.", ""], ["Jalilvand", "Asal", ""], ["Paulovich", "Fernando V.", ""]]}, {"id": "2101.08656", "submitter": "Jinxiong Zhang", "authors": "Jinxiong Zhang", "title": "Dive into Decision Trees and Forests: A Theoretical Demonstration", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Based on decision trees, many fields have arguably made tremendous progress\nin recent years. In simple words, decision trees use the strategy of\n\"divide-and-conquer\" to divide the complex problem on the dependency between\ninput features and labels into smaller ones. While decision trees have a long\nhistory, recent advances have greatly improved their performance in\ncomputational advertising, recommender system, information retrieval, etc. We\nintroduce common tree-based models (e.g., Bayesian CART, Bayesian regression\nsplines) and training techniques (e.g., mixed integer programming, alternating\noptimization, gradient descent). Along the way, we highlight probabilistic\ncharacteristics of tree-based models and explain their practical and\ntheoretical benefits. Except machine learning and data mining, we try to show\ntheoretical advances on tree-based models from other fields such as statistics\nand operation research. We list the reproducible resource at the end of each\nmethod.\n", "versions": [{"version": "v1", "created": "Wed, 20 Jan 2021 16:47:59 GMT"}], "update_date": "2021-01-22", "authors_parsed": [["Zhang", "Jinxiong", ""]]}, {"id": "2101.08658", "submitter": "Ofer Mendelevitch", "authors": "Ofer Mendelevitch, Michael D. Lesh", "title": "Fidelity and Privacy of Synthetic Medical Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The digitization of medical records ushered in a new era of big data to\nclinical science, and with it the possibility that data could be shared, to\nmultiply insights beyond what investigators could abstract from paper records.\nThe need to share individual-level medical data to accelerate innovation in\nprecision medicine continues to grow, and has never been more urgent, as\nscientists grapple with the COVID-19 pandemic. However, enthusiasm for the use\nof big data has been tempered by a fully appropriate concern for patient\nautonomy and privacy. That is, the ability to extract private or confidential\ninformation about an individual, in practice, renders it difficult to share\ndata, since significant infrastructure and data governance must be established\nbefore data can be shared. Although HIPAA provided de-identification as an\napproved mechanism for data sharing, linkage attacks were identified as a major\nvulnerability. A variety of mechanisms have been established to avoid leaking\nprivate information, such as field suppression or abstraction, strictly\nlimiting the amount of information that can be shared, or employing\nmathematical techniques such as differential privacy. Another approach, which\nwe focus on here, is creating synthetic data that mimics the underlying data.\nFor synthetic data to be a useful mechanism in support of medical innovation\nand a proxy for real-world evidence, one must demonstrate two properties of the\nsynthetic dataset: (1) any analysis on the real data must be matched by\nanalysis of the synthetic data (statistical fidelity) and (2) the synthetic\ndata must preserve privacy, with minimal risk of re-identification (privacy\nguarantee). In this paper we propose a framework for quantifying the\nstatistical fidelity and privacy preservation properties of synthetic datasets\nand demonstrate these metrics for synthetic data generated by Syntegra\ntechnology.\n", "versions": [{"version": "v1", "created": "Mon, 18 Jan 2021 23:01:27 GMT"}, {"version": "v2", "created": "Wed, 2 Jun 2021 04:41:17 GMT"}], "update_date": "2021-06-03", "authors_parsed": [["Mendelevitch", "Ofer", ""], ["Lesh", "Michael D.", ""]]}, {"id": "2101.08659", "submitter": "Lucas Jacaruso", "authors": "Lucas Cassiel Jacaruso", "title": "Free congruence: an exploration of expanded similarity measures for time\n  series data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Time series similarity measures are highly relevant in a wide range of\nemerging applications including training machine learning models,\nclassification, and predictive modeling. Standard similarity measures for time\nseries most often involve point-to-point distance measures including Euclidean\ndistance and Dynamic Time Warping. Such similarity measures fundamentally\nrequire the fluctuation of values in the time series being compared to follow a\ncorresponding order or cadence for similarity to be established. This paper is\nspurred by the exploration of a broader definition of similarity, namely one\nthat takes into account the sheer numerical resemblance between sets of\nstatistical properties for time series segments irrespectively of value\nlabeling. Further, the presence of common pattern components between time\nseries segments was examined even if they occur in a permuted order, which\nwould not necessarily satisfy the criteria of more conventional point-to-point\ndistance measures. Results were compared with those of Dynamic Time Warping on\nthe same data for context. Surprisingly, the test for the numerical resemblance\nbetween sets of statistical properties established a stronger resemblance for\npairings of decline years with greater statistical significance than Dynamic\nTime Warping on the particular data and sample size used.\n", "versions": [{"version": "v1", "created": "Sun, 17 Jan 2021 23:34:55 GMT"}], "update_date": "2021-01-22", "authors_parsed": [["Jacaruso", "Lucas Cassiel", ""]]}, {"id": "2101.08661", "submitter": "Thomas Oberlin", "authors": "Thomas Oberlin and Mathieu Verm", "title": "Regularization via deep generative models: an analysis point of view", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  This paper proposes a new way of regularizing an inverse problem in imaging\n(e.g., deblurring or inpainting) by means of a deep generative neural network.\nCompared to end-to-end models, such approaches seem particularly interesting\nsince the same network can be used for many different problems and experimental\nconditions, as soon as the generative model is suited to the data. Previous\nworks proposed to use a synthesis framework, where the estimation is performed\non the latent vector, the solution being obtained afterwards via the decoder.\nInstead, we propose an analysis formulation where we directly optimize the\nimage itself and penalize the latent vector. We illustrate the interest of such\na formulation by running experiments of inpainting, deblurring and\nsuper-resolution. In many cases our technique achieves a clear improvement of\nthe performance and seems to be more robust, in particular with respect to\ninitialization.\n", "versions": [{"version": "v1", "created": "Thu, 21 Jan 2021 15:04:57 GMT"}], "update_date": "2021-01-22", "authors_parsed": [["Oberlin", "Thomas", ""], ["Verm", "Mathieu", ""]]}, {"id": "2101.08675", "submitter": "Izzat Alsmadi", "authors": "Izzat Alsmadi", "title": "Adversarial Machine Learning in Text Analysis and Generation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The research field of adversarial machine learning witnessed a significant\ninterest in the last few years. A machine learner or model is secure if it can\ndeliver main objectives with acceptable accuracy, efficiency, etc. while at the\nsame time, it can resist different types and/or attempts of adversarial\nattacks. This paper focuses on studying aspects and research trends in\nadversarial machine learning specifically in text analysis and generation. The\npaper summarizes main research trends in the field such as GAN algorithms,\nmodels, types of attacks, and defense against those attacks.\n", "versions": [{"version": "v1", "created": "Thu, 14 Jan 2021 04:37:52 GMT"}], "update_date": "2021-01-22", "authors_parsed": [["Alsmadi", "Izzat", ""]]}, {"id": "2101.08685", "submitter": "Thomas Pfeil", "authors": "Thomas Pfeil", "title": "ItNet: iterative neural networks with small graphs for accurate and\n  efficient anytime prediction", "comments": "10 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AR cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural networks have usually to be compressed and accelerated for their\nusage in low-power, e.g. mobile, devices. Recently, massively-parallel hardware\naccelerators were developed that offer high throughput and low latency at low\npower by utilizing in-memory computation. However, to exploit these benefits\nthe computational graph of a neural network has to fit into the in-computation\nmemory of these hardware systems that is usually rather limited in size. In\nthis study, we introduce a class of network models that have a small memory\nfootprint in terms of their computational graphs. To this end, the graph is\ndesigned to contain loops by iteratively executing a single network building\nblock. Furthermore, the trade-off between accuracy and latency of these\nso-called iterative neural networks is improved by adding multiple intermediate\noutputs both during training and inference. We show state-of-the-art results\nfor semantic segmentation on the CamVid and Cityscapes datasets that are\nespecially demanding in terms of computational resources. In ablation studies,\nthe improvement of network training by intermediate network outputs as well as\nthe trade-off between weight sharing over iterations and the network size are\ninvestigated.\n", "versions": [{"version": "v1", "created": "Thu, 21 Jan 2021 15:56:29 GMT"}, {"version": "v2", "created": "Fri, 12 Mar 2021 14:25:35 GMT"}], "update_date": "2021-03-15", "authors_parsed": [["Pfeil", "Thomas", ""]]}, {"id": "2101.08687", "submitter": "Ties van Rozendaal", "authors": "Ties van Rozendaal, Iris A.M. Huijben, Taco S. Cohen", "title": "Overfitting for Fun and Profit: Instance-Adaptive Data Compression", "comments": "Accepted at International Conference on Learning Representations 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Neural data compression has been shown to outperform classical methods in\nterms of $RD$ performance, with results still improving rapidly. At a high\nlevel, neural compression is based on an autoencoder that tries to reconstruct\nthe input instance from a (quantized) latent representation, coupled with a\nprior that is used to losslessly compress these latents. Due to limitations on\nmodel capacity and imperfect optimization and generalization, such models will\nsuboptimally compress test data in general. However, one of the great strengths\nof learned compression is that if the test-time data distribution is known and\nrelatively low-entropy (e.g. a camera watching a static scene, a dash cam in an\nautonomous car, etc.), the model can easily be finetuned or adapted to this\ndistribution, leading to improved $RD$ performance. In this paper we take this\nconcept to the extreme, adapting the full model to a single video, and sending\nmodel updates (quantized and compressed using a parameter-space prior) along\nwith the latent representation. Unlike previous work, we finetune not only the\nencoder/latents but the entire model, and - during finetuning - take into\naccount both the effect of model quantization and the additional costs incurred\nby sending the model updates. We evaluate an image compression model on\nI-frames (sampled at 2 fps) from videos of the Xiph dataset, and demonstrate\nthat full-model adaptation improves $RD$ performance by ~1 dB, with respect to\nencoder-only finetuning.\n", "versions": [{"version": "v1", "created": "Thu, 21 Jan 2021 15:58:58 GMT"}, {"version": "v2", "created": "Tue, 1 Jun 2021 14:26:04 GMT"}], "update_date": "2021-06-02", "authors_parsed": [["van Rozendaal", "Ties", ""], ["Huijben", "Iris A. M.", ""], ["Cohen", "Taco S.", ""]]}, {"id": "2101.08688", "submitter": "Yingdong Lu", "authors": "Soumyadip Ghosh, Yingdong Lu, Tomasz Nowicki", "title": "HMC, an example of Functional Analysis applied to Algorithms in Data\n  Mining. The convergence in $L^p$", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.CA cs.DS cs.LG math.FA stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a proof of convergence of the Hamiltonian Monte Carlo algorithm in\nterms of Functional Analysis. We represent the algorithm as an operator on the\ndensity functions, and prove the convergence of iterations of this operator in\n$L^p$, for $1<p<\\infty$, and strong convergence for $2\\le p<\\infty$.\n", "versions": [{"version": "v1", "created": "Thu, 21 Jan 2021 15:59:32 GMT"}], "update_date": "2021-01-22", "authors_parsed": [["Ghosh", "Soumyadip", ""], ["Lu", "Yingdong", ""], ["Nowicki", "Tomasz", ""]]}, {"id": "2101.08692", "submitter": "Andrew Brock", "authors": "Andrew Brock, Soham De, Samuel L. Smith", "title": "Characterizing signal propagation to close the performance gap in\n  unnormalized ResNets", "comments": "Published as a conference paper at ICLR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Batch Normalization is a key component in almost all state-of-the-art image\nclassifiers, but it also introduces practical challenges: it breaks the\nindependence between training examples within a batch, can incur compute and\nmemory overhead, and often results in unexpected bugs. Building on recent\ntheoretical analyses of deep ResNets at initialization, we propose a simple set\nof analysis tools to characterize signal propagation on the forward pass, and\nleverage these tools to design highly performant ResNets without activation\nnormalization layers. Crucial to our success is an adapted version of the\nrecently proposed Weight Standardization. Our analysis tools show how this\ntechnique preserves the signal in networks with ReLU or Swish activation\nfunctions by ensuring that the per-channel activation means do not grow with\ndepth. Across a range of FLOP budgets, our networks attain performance\ncompetitive with the state-of-the-art EfficientNets on ImageNet.\n", "versions": [{"version": "v1", "created": "Thu, 21 Jan 2021 16:07:06 GMT"}, {"version": "v2", "created": "Wed, 27 Jan 2021 11:28:41 GMT"}], "update_date": "2021-01-28", "authors_parsed": [["Brock", "Andrew", ""], ["De", "Soham", ""], ["Smith", "Samuel L.", ""]]}, {"id": "2101.08699", "submitter": "Dimitrije Markovic", "authors": "Dimitrije Markovic, Hrvoje Stojic, Sarah Schwoebel, and Stefan J.\n  Kiebel", "title": "An empirical evaluation of active inference in multi-armed bandits", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  A key feature of sequential decision making under uncertainty is a need to\nbalance between exploiting--choosing the best action according to the current\nknowledge, and exploring--obtaining information about values of other actions.\nThe multi-armed bandit problem, a classical task that captures this trade-off,\nserved as a vehicle in machine learning for developing bandit algorithms that\nproved to be useful in numerous industrial applications. The active inference\nframework, an approach to sequential decision making recently developed in\nneuroscience for understanding human and animal behaviour, is distinguished by\nits sophisticated strategy for resolving the exploration-exploitation\ntrade-off. This makes active inference an exciting alternative to already\nestablished bandit algorithms. Here we derive an efficient and scalable\napproximate active inference algorithm and compare it to two state-of-the-art\nbandit algorithms: Bayesian upper confidence bound and optimistic Thompson\nsampling. This comparison is done on two types of bandit problems: a stationary\nand a dynamic switching bandit. Our empirical evaluation shows that the active\ninference algorithm does not produce efficient long-term behaviour in\nstationary bandits. However, in the more challenging switching bandit problem\nactive inference performs substantially better than the two state-of-the-art\nbandit algorithms. The results open exciting venues for further research in\ntheoretical and applied machine learning, as well as lend additional\ncredibility to active inference as a general framework for studying human and\nanimal behaviour.\n", "versions": [{"version": "v1", "created": "Thu, 21 Jan 2021 16:20:06 GMT"}, {"version": "v2", "created": "Sat, 23 Jan 2021 13:20:34 GMT"}, {"version": "v3", "created": "Wed, 7 Jul 2021 09:27:56 GMT"}], "update_date": "2021-07-08", "authors_parsed": [["Markovic", "Dimitrije", ""], ["Stojic", "Hrvoje", ""], ["Schwoebel", "Sarah", ""], ["Kiebel", "Stefan J.", ""]]}, {"id": "2101.08717", "submitter": "Jacson Rodrigues Correia-Silva", "authors": "Jacson Rodrigues Correia-Silva, Rodrigo F. Berriel, Claudine Badue,\n  Alberto F. De Souza, Thiago Oliveira-Santos", "title": "Copycat CNN: Are Random Non-Labeled Data Enough to Steal Knowledge from\n  Black-box Models?", "comments": "The code is available at https://github.com/jeiks/Stealing_DL_Models", "journal-ref": "Pattern Recognition 113 (2021) 107830", "doi": "10.1016/j.patcog.2021.107830", "report-no": null, "categories": "cs.CR cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Convolutional neural networks have been successful lately enabling companies\nto develop neural-based products, which demand an expensive process, involving\ndata acquisition and annotation; and model generation, usually requiring\nexperts. With all these costs, companies are concerned about the security of\ntheir models against copies and deliver them as black-boxes accessed by APIs.\nNonetheless, we argue that even black-box models still have some\nvulnerabilities. In a preliminary work, we presented a simple, yet powerful,\nmethod to copy black-box models by querying them with natural random images. In\nthis work, we consolidate and extend the copycat method: (i) some constraints\nare waived; (ii) an extensive evaluation with several problems is performed;\n(iii) models are copied between different architectures; and, (iv) a deeper\nanalysis is performed by looking at the copycat behavior. Results show that\nnatural random images are effective to generate copycats for several problems.\n", "versions": [{"version": "v1", "created": "Thu, 21 Jan 2021 16:55:14 GMT"}], "update_date": "2021-01-22", "authors_parsed": [["Correia-Silva", "Jacson Rodrigues", ""], ["Berriel", "Rodrigo F.", ""], ["Badue", "Claudine", ""], ["De Souza", "Alberto F.", ""], ["Oliveira-Santos", "Thiago", ""]]}, {"id": "2101.08732", "submitter": "Lang Huang", "authors": "Lang Huang, Chao Zhang and Hongyang Zhang", "title": "Self-Adaptive Training: Bridging the Supervised and Self-Supervised\n  Learning", "comments": "Journal version of arXiv:2002.10319 [cs.LG] (NeurIPS2020). 19 pages,\n  15 figures, 11 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We propose self-adaptive training -- a unified training algorithm that\ndynamically calibrates and enhances training process by model predictions\nwithout incurring extra computational cost -- to advance both supervised and\nself-supervised learning of deep neural networks. We analyze the training\ndynamics of deep networks on training data that are corrupted by, e.g., random\nnoise and adversarial examples. Our analysis shows that model predictions are\nable to magnify useful underlying information in data and this phenomenon\noccurs broadly even in the absence of \\emph{any} label information,\nhighlighting that model predictions could substantially benefit the training\nprocess: self-adaptive training improves the generalization of deep networks\nunder noise and enhances the self-supervised representation learning. The\nanalysis also sheds light on understanding deep learning, e.g., a potential\nexplanation of the recently-discovered double-descent phenomenon in empirical\nrisk minimization and the collapsing issue of the state-of-the-art\nself-supervised learning algorithms. Experiments on the CIFAR, STL and ImageNet\ndatasets verify the effectiveness of our approach in three applications:\nclassification with label noise, selective classification and linear\nevaluation. To facilitate future research, the code has been made public\navailable at https://github.com/LayneH/self-adaptive-training.\n", "versions": [{"version": "v1", "created": "Thu, 21 Jan 2021 17:17:30 GMT"}], "update_date": "2021-01-22", "authors_parsed": [["Huang", "Lang", ""], ["Zhang", "Chao", ""], ["Zhang", "Hongyang", ""]]}, {"id": "2101.08734", "submitter": "Nikoli Dryden", "authors": "Nikoli Dryden, Roman B\\\"ohringer, Tal Ben-Nun, Torsten Hoefler", "title": "Clairvoyant Prefetching for Distributed Machine Learning I/O", "comments": "13 pages, 16 figures; major revisions", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  I/O is emerging as a major bottleneck for machine learning training,\nespecially in distributed environments. Indeed, at large scale, I/O takes as\nmuch as 85% of training time. Addressing this I/O bottleneck necessitates\ncareful optimization, as optimal data ingestion pipelines differ between\nsystems, and require a delicate balance between access to local storage,\nexternal filesystems, and remote nodes. We introduce NoPFS, a machine learning\nI/O middleware, which provides a scalable, flexible, and easy-to-use solution\nto the I/O bottleneck. NoPFS uses clairvoyance: Given the seed generating the\nrandom access pattern for training with SGD, it can exactly predict when and\nwhere a sample will be accessed. We combine this with an analysis of access\npatterns and a performance model to provide distributed caching policies that\nadapt to different datasets and storage hierarchies. NoPFS reduces I/O times\nand improves end-to-end training by up to 5.4x on the ImageNet-1k,\nImageNet-22k, and CosmoFlow datasets.\n", "versions": [{"version": "v1", "created": "Thu, 21 Jan 2021 17:21:42 GMT"}, {"version": "v2", "created": "Thu, 10 Jun 2021 12:28:20 GMT"}], "update_date": "2021-06-11", "authors_parsed": [["Dryden", "Nikoli", ""], ["B\u00f6hringer", "Roman", ""], ["Ben-Nun", "Tal", ""], ["Hoefler", "Torsten", ""]]}, {"id": "2101.08740", "submitter": "Fabio Amadio", "authors": "Fabio Amadio, Alberto Dalla Libera, Ruggero Carli, Daniel Nikovski,\n  Diego Romeres", "title": "Model-based Policy Search for Partially Measurable Systems", "comments": "Accepted to 3rd Robot Learning Workshop: Grounding Machine Learning\n  Development in the Real World (NeurIPS 2020)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.LG cs.SY eess.SY", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper, we propose a Model-Based Reinforcement Learning (MBRL)\nalgorithm for Partially Measurable Systems (PMS), i.e., systems where the state\ncan not be directly measured, but must be estimated through proper state\nobservers. The proposed algorithm, named Monte Carlo Probabilistic Inference\nfor Learning COntrol for Partially Measurable Systems (MC-PILCO4PMS), relies on\nGaussian Processes (GPs) to model the system dynamics, and on a Monte Carlo\napproach to update the policy parameters. W.r.t. previous GP-based MBRL\nalgorithms, MC-PILCO4PMS models explicitly the presence of state observers\nduring policy optimization, allowing to deal PMS. The effectiveness of the\nproposed algorithm has been tested both in simulation and in two real systems.\n", "versions": [{"version": "v1", "created": "Thu, 21 Jan 2021 17:39:22 GMT"}], "update_date": "2021-01-22", "authors_parsed": [["Amadio", "Fabio", ""], ["Libera", "Alberto Dalla", ""], ["Carli", "Ruggero", ""], ["Nikovski", "Daniel", ""], ["Romeres", "Diego", ""]]}, {"id": "2101.08742", "submitter": "Ivan Gridin", "authors": "Ivan Gridin", "title": "Soft Genetic Programming Binary Classifiers", "comments": "21 pages, 12 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The study of the classifier's design and it's usage is one of the most\nimportant machine learning areas. With the development of automatic machine\nlearning methods, various approaches are used to build a robust classifier\nmodel. Due to some difficult implementation and customization complexity,\ngenetic programming (GP) methods are not often used to construct classifiers.\nGP classifiers have several limitations and disadvantages. However, the concept\nof \"soft\" genetic programming (SGP) has been developed, which allows the\nlogical operator tree to be more flexible and find dependencies in datasets,\nwhich gives promising results in most cases. This article discusses a method\nfor constructing binary classifiers using the SGP technique. The test results\nare presented. Source code - https://github.com/survexman/sgp_classifier.\n", "versions": [{"version": "v1", "created": "Thu, 21 Jan 2021 17:43:11 GMT"}], "update_date": "2021-01-22", "authors_parsed": [["Gridin", "Ivan", ""]]}, {"id": "2101.08743", "submitter": "Wenjie Chen", "authors": "Wenjie Chen, Shengcai Liu, and Ke Tang", "title": "A New Knowledge Gradient-based Method for Constrained Bayesian\n  Optimization", "comments": "14 pages, 0 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Black-box problems are common in real life like structural design, drug\nexperiments, and machine learning. When optimizing black-box systems,\ndecision-makers always consider multiple performances and give the final\ndecision by comprehensive evaluations. Motivated by such practical needs, we\nfocus on constrained black-box problems where the objective and constraints\nlack known special structure, and evaluations are expensive and even with\nnoise. We develop a novel constrained Bayesian optimization approach based on\nthe knowledge gradient method ($c-\\rm{KG}$). A new acquisition function is\nproposed to determine the next batch of samples considering optimality and\nfeasibility. An unbiased estimator of the gradient of the new acquisition\nfunction is derived to implement the $c-\\rm{KG}$ approach.\n", "versions": [{"version": "v1", "created": "Wed, 20 Jan 2021 05:00:38 GMT"}], "update_date": "2021-01-22", "authors_parsed": [["Chen", "Wenjie", ""], ["Liu", "Shengcai", ""], ["Tang", "Ke", ""]]}, {"id": "2101.08747", "submitter": "Yuwei Cao", "authors": "Yuwei Cao, Hao Peng, Jia Wu, Yingtong Dou, Jianxin Li, Philip S. Yu", "title": "Knowledge-Preserving Incremental Social Event Detection via\n  Heterogeneous GNNs", "comments": "This work has been accepted to The Web Conference 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Social events provide valuable insights into group social behaviors and\npublic concerns and therefore have many applications in fields such as product\nrecommendation and crisis management. The complexity and streaming nature of\nsocial messages make it appealing to address social event detection in an\nincremental learning setting, where acquiring, preserving, and extending\nknowledge are major concerns. Most existing methods, including those based on\nincremental clustering and community detection, learn limited amounts of\nknowledge as they ignore the rich semantics and structural information\ncontained in social data. Moreover, they cannot memorize previously acquired\nknowledge. In this paper, we propose a novel Knowledge-Preserving Incremental\nHeterogeneous Graph Neural Network (KPGNN) for incremental social event\ndetection. To acquire more knowledge, KPGNN models complex social messages into\nunified social graphs to facilitate data utilization and explores the\nexpressive power of GNNs for knowledge extraction. To continuously adapt to the\nincoming data, KPGNN adopts contrastive loss terms that cope with a changing\nnumber of event classes. It also leverages the inductive learning ability of\nGNNs to efficiently detect events and extends its knowledge from previously\nunseen data. To deal with large social streams, KPGNN adopts a mini-batch\nsubgraph sampling strategy for scalable training, and periodically removes\nobsolete data to maintain a dynamic embedding space. KPGNN requires no feature\nengineering and has few hyperparameters to tune. Extensive experiment results\ndemonstrate the superiority of KPGNN over various baselines.\n", "versions": [{"version": "v1", "created": "Thu, 21 Jan 2021 17:56:57 GMT"}, {"version": "v2", "created": "Sat, 13 Feb 2021 17:49:54 GMT"}], "update_date": "2021-03-22", "authors_parsed": [["Cao", "Yuwei", ""], ["Peng", "Hao", ""], ["Wu", "Jia", ""], ["Dou", "Yingtong", ""], ["Li", "Jianxin", ""], ["Yu", "Philip S.", ""]]}, {"id": "2101.08769", "submitter": "Steffen Rendle", "authors": "Steffen Rendle", "title": "Item Recommendation from Implicit Feedback", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The task of item recommendation is to select the best items for a user from a\nlarge catalogue of items. Item recommenders are commonly trained from implicit\nfeedback which consists of past actions that are positive only. Core challenges\nof item recommendation are (1) how to formulate a training objective from\nimplicit feedback and (2) how to efficiently train models over a large item\ncatalogue. This article provides an overview of item recommendation, its unique\ncharacteristics and some common approaches. It starts with an introduction to\nthe problem and discusses different training objectives. The main body deals\nwith learning algorithms and presents sampling based algorithms for general\nrecommenders and more efficient algorithms for dot product models. Finally, the\napplication of item recommenders for retrieval tasks is discussed.\n", "versions": [{"version": "v1", "created": "Thu, 21 Jan 2021 18:50:21 GMT"}], "update_date": "2021-01-22", "authors_parsed": [["Rendle", "Steffen", ""]]}, {"id": "2101.08809", "submitter": "Daiyi Peng", "authors": "Daiyi Peng, Xuanyi Dong, Esteban Real, Mingxing Tan, Yifeng Lu,\n  Hanxiao Liu, Gabriel Bender, Adam Kraft, Chen Liang, Quoc V. Le", "title": "PyGlove: Symbolic Programming for Automated Machine Learning", "comments": "NeurIPS 2020 Oral", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural networks are sensitive to hyper-parameter and architecture choices.\nAutomated Machine Learning (AutoML) is a promising paradigm for automating\nthese choices. Current ML software libraries, however, are quite limited in\nhandling the dynamic interactions among the components of AutoML. For example,\nefficientNAS algorithms, such as ENAS and DARTS, typically require an\nimplementation coupling between the search space and search algorithm, the two\nkey components in AutoML. Furthermore, implementing a complex search flow, such\nas searching architectures within a loop of searching hardware configurations,\nis difficult. To summarize, changing the search space, search algorithm, or\nsearch flow in current ML libraries usually requires a significant change in\nthe program logic. In this paper, we introduce a new way of programming AutoML\nbased on symbolic programming. Under this paradigm, ML programs are mutable,\nthus can be manipulated easily by another program. As a result, AutoML can be\nreformulated as an automated process of symbolic manipulation. With this\nformulation, we decouple the triangle of the search algorithm, the search space\nand the child program. This decoupling makes it easy to change the search space\nand search algorithm (without and with weight sharing), as well as to add\nsearch capabilities to existing code and implement complex search flows. We\nthen introduce PyGlove, a new Python library that implements this paradigm.\nThrough case studies on ImageNet and NAS-Bench-101, we show that with PyGlove\nusers can easily convert a static program into a search space, quickly iterate\non the search spaces and search algorithms, and craft complex search flows to\nachieve better results.\n", "versions": [{"version": "v1", "created": "Thu, 21 Jan 2021 19:05:44 GMT"}], "update_date": "2021-01-25", "authors_parsed": [["Peng", "Daiyi", ""], ["Dong", "Xuanyi", ""], ["Real", "Esteban", ""], ["Tan", "Mingxing", ""], ["Lu", "Yifeng", ""], ["Liu", "Hanxiao", ""], ["Bender", "Gabriel", ""], ["Kraft", "Adam", ""], ["Liang", "Chen", ""], ["Le", "Quoc V.", ""]]}, {"id": "2101.08837", "submitter": "Kerem \\\"Ozfatura", "authors": "Emre Ozfatura and Kerem Ozfatura and Deniz Gunduz", "title": "Time-Correlated Sparsification for Communication-Efficient Federated\n  Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DC cs.IT math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Federated learning (FL) enables multiple clients to collaboratively train a\nshared model without disclosing their local datasets. This is achieved by\nexchanging local model updates with the help of a parameter server (PS).\nHowever, due to the increasing size of the trained models, the communication\nload due to the iterative exchanges between the clients and the PS often\nbecomes a bottleneck in the performance. Sparse communication is often employed\nto reduce the communication load, where only a small subset of the model\nupdates are communicated from the clients to the PS. In this paper, we\nintroduce a novel time-correlated sparsification (TCS) scheme, which builds\nupon the notion that sparse communication framework can be considered as\nidentifying the most significant elements of the underlying model. Hence, TCS\nseeks a certain correlation between the sparse representations used at\nconsecutive iterations in FL, so that the overhead due to encoding and\ntransmission of the sparse representation can be significantly reduced without\ncompromising the test accuracy. Through extensive simulations on the CIFAR-10\ndataset, we show that TCS can achieve centralized training accuracy with 100\ntimes sparsification, and up to 2000 times reduction in the communication load\nwhen employed together with quantization.\n", "versions": [{"version": "v1", "created": "Thu, 21 Jan 2021 20:15:55 GMT"}], "update_date": "2021-01-25", "authors_parsed": [["Ozfatura", "Emre", ""], ["Ozfatura", "Kerem", ""], ["Gunduz", "Deniz", ""]]}, {"id": "2101.08854", "submitter": "Evgeny Krivosheev", "authors": "Evgeny Krivosheev, Fabio Casati, Alessandro Bozzon", "title": "Active Hybrid Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Hybrid crowd-machine classifiers can achieve superior performance by\ncombining the cost-effectiveness of automatic classification with the accuracy\nof human judgment. This paper shows how crowd and machines can support each\nother in tackling classification problems. Specifically, we propose an\narchitecture that orchestrates active learning and crowd classification and\ncombines them in a virtuous cycle. We show that when the pool of items to\nclassify is finite we face learning vs. exploitation trade-off in hybrid\nclassification, as we need to balance crowd tasks optimized for creating a\ntraining dataset with tasks optimized for classifying items in the pool. We\ndefine the problem, propose a set of heuristics and evaluate the approach on\nthree real-world datasets with different characteristics in terms of machine\nand crowd classification performance, showing that our active hybrid approach\nsignificantly outperforms baselines.\n", "versions": [{"version": "v1", "created": "Thu, 21 Jan 2021 21:09:07 GMT"}], "update_date": "2021-01-25", "authors_parsed": [["Krivosheev", "Evgeny", ""], ["Casati", "Fabio", ""], ["Bozzon", "Alessandro", ""]]}, {"id": "2101.08857", "submitter": "Florian Wolf", "authors": "Florian Wolf", "title": "Knowledge Generation -- Variational Bayes on Knowledge Graphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This thesis is a proof of concept for the potential of Variational\nAuto-Encoder (VAE) on representation learning of real-world Knowledge Graphs\n(KG). Inspired by successful approaches to the generation of molecular graphs,\nwe evaluate the capabilities of our model, the Relational Graph Variational\nAuto-Encoder (RGVAE). The impact of the modular hyperparameter choices,\nencoding through graph convolutions, graph matching and latent space prior, is\ncompared. The RGVAE is first evaluated on link prediction. The mean reciprocal\nrank (MRR) scores on the two datasets FB15K-237 and WN18RR are compared to the\nembedding-based model DistMult. A variational DistMult and a RGVAE without\nlatent space prior constraint are implemented as control models. The results\nshow that between different settings, the RGVAE with relaxed latent space,\nscores highest on both datasets, yet does not outperform the DistMult. Further,\nwe investigate the latent space in a twofold experiment: first, linear\ninterpolation between the latent representation of two triples, then the\nexploration of each latent dimension in a $95\\%$ confidence interval. Both\ninterpolations show that the RGVAE learns to reconstruct the adjacency matrix\nbut fails to disentangle. For the last experiment we introduce a new validation\nmethod for the FB15K-237 data set. The relation type-constrains of generated\ntriples are filtered and matched with entity types. The observed rate of valid\ngenerated triples is insignificantly higher than the random threshold. All\ngenerated and valid triples are unseen. A comparison between different latent\nspace priors, using the $\\delta$-VAE method, reveals a decoder collapse.\nFinally we analyze the limiting factors of our approach compared to molecule\ngeneration and propose solutions for the decoder collapse and successful\nrepresentation learning of multi-relational KGs.\n", "versions": [{"version": "v1", "created": "Thu, 21 Jan 2021 21:23:17 GMT"}], "update_date": "2021-01-25", "authors_parsed": [["Wolf", "Florian", ""]]}, {"id": "2101.08862", "submitter": "Shangtong Zhang", "authors": "Shangtong Zhang, Hengshuai Yao, Shimon Whiteson", "title": "Breaking the Deadly Triad with a Target Network", "comments": "ICML 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The deadly triad refers to the instability of a reinforcement learning\nalgorithm when it employs off-policy learning, function approximation, and\nbootstrapping simultaneously. In this paper, we investigate the target network\nas a tool for breaking the deadly triad, providing theoretical support for the\nconventional wisdom that a target network stabilizes training. We first propose\nand analyze a novel target network update rule which augments the commonly used\nPolyak-averaging style update with two projections. We then apply the target\nnetwork and ridge regularization in several divergent algorithms and show their\nconvergence to regularized TD fixed points. Those algorithms are off-policy\nwith linear function approximation and bootstrapping, spanning both policy\nevaluation and control, as well as both discounted and average-reward settings.\nIn particular, we provide the first convergent linear $Q$-learning algorithms\nunder nonrestrictive and changing behavior policies without bi-level\noptimization.\n", "versions": [{"version": "v1", "created": "Thu, 21 Jan 2021 21:50:10 GMT"}, {"version": "v2", "created": "Tue, 2 Feb 2021 16:04:06 GMT"}, {"version": "v3", "created": "Tue, 9 Feb 2021 16:20:36 GMT"}, {"version": "v4", "created": "Thu, 27 May 2021 22:34:29 GMT"}], "update_date": "2021-05-31", "authors_parsed": [["Zhang", "Shangtong", ""], ["Yao", "Hengshuai", ""], ["Whiteson", "Shimon", ""]]}, {"id": "2101.08878", "submitter": "Aamir Shafi", "authors": "Aamir Shafi, Jahanzeb Maqbool Hashmi, Hari Subramoni and Dhabaleswar\n  K. Panda", "title": "Efficient MPI-based Communication for GPU-Accelerated Dask Applications", "comments": "10 pages, 9 figures, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.LG cs.PF", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Dask is a popular parallel and distributed computing framework, which rivals\nApache Spark to enable task-based scalable processing of big data. The Dask\nDistributed library forms the basis of this computing engine and provides\nsupport for adding new communication devices. It currently has two\ncommunication devices: one for TCP and the other for high-speed networks using\nUCX-Py -- a Cython wrapper to UCX. This paper presents the design and\nimplementation of a new communication backend for Dask -- called MPI4Dask --\nthat is targeted for modern HPC clusters built with GPUs. MPI4Dask exploits\nmpi4py over MVAPICH2-GDR, which is a GPU-aware implementation of the Message\nPassing Interface (MPI) standard. MPI4Dask provides point-to-point asynchronous\nI/O communication coroutines, which are non-blocking concurrent operations\ndefined using the async/await keywords from the Python's asyncio framework. Our\nlatency and throughput comparisons suggest that MPI4Dask outperforms UCX by 6x\nfor 1 Byte message and 4x for large messages (2 MBytes and beyond)\nrespectively. We also conduct comparative performance evaluation of MPI4Dask\nwith UCX using two benchmark applications: 1) sum of cuPy array with its\ntranspose, and 2) cuDF merge. MPI4Dask speeds up the overall execution time of\nthe two applications by an average of 3.47x and 3.11x respectively on an\nin-house cluster built with NVIDIA Tesla V100 GPUs for 1-6 Dask workers. We\nalso perform scalability analysis of MPI4Dask against UCX for these\napplications on TACC's Frontera (GPU) system with upto 32 Dask workers on 32\nNVIDIA Quadro RTX 5000 GPUs and 256 CPU cores. MPI4Dask speeds up the execution\ntime for cuPy and cuDF applications by an average of 1.71x and 2.91x\nrespectively for 1-32 Dask workers on the Frontera (GPU) system.\n", "versions": [{"version": "v1", "created": "Thu, 21 Jan 2021 22:59:08 GMT"}], "update_date": "2021-01-25", "authors_parsed": [["Shafi", "Aamir", ""], ["Hashmi", "Jahanzeb Maqbool", ""], ["Subramoni", "Hari", ""], ["Panda", "Dhabaleswar K.", ""]]}, {"id": "2101.08884", "submitter": "Matthew Denton", "authors": "Matthew Denton and Herman Schmit", "title": "Direct Spatial Implementation of Sparse Matrix Multipliers for Reservoir\n  Computing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Reservoir computing systems rely on the recurrent multiplication of a very\nlarge, sparse, fixed matrix. We argue that direct spatial implementation of\nthese fixed matrices minimizes the work performed in the computation, and\nallows for significant reduction in latency and power through constant\npropagation and logic minimization. Bit-serial arithmetic enables massive\nstatic matrices to be implemented. We present the structure of our bit-serial\nmatrix multiplier, and evaluate using canonical signed digit representation to\nfurther reduce logic utilization. We have implemented these matrices on a large\nFPGA and provide a cost model that is simple and extensible. These FPGA\nimplementations, on average, reduce latency by 50x up to 86x versus GPU\nlibraries. Comparing against a recent sparse DNN accelerator, we measure a 4.1x\nto 47x reduction in latency depending on matrix dimension and sparsity.\nThroughput of the FPGA solution is also competitive for a wide range of matrix\ndimensions and batch sizes. Finally, we discuss ways these techniques could be\ndeployed in ASICs, making them applicable for dynamic sparse matrix\ncomputations.\n", "versions": [{"version": "v1", "created": "Thu, 21 Jan 2021 23:16:22 GMT"}], "update_date": "2021-01-25", "authors_parsed": [["Denton", "Matthew", ""], ["Schmit", "Herman", ""]]}, {"id": "2101.08888", "submitter": "Tinu Theckel Joy", "authors": "Tinu Theckel Joy, Suman Sedai, Rahil Garnavi", "title": "Analyzing Epistemic and Aleatoric Uncertainty for Drusen Segmentation in\n  Optical Coherence Tomography Images", "comments": "Accepted in AAAI 2021 Workshop on Trustworthy AI for Healthcare", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Age-related macular degeneration (AMD) is one of the leading causes of\npermanent vision loss in people aged over 60 years. Accurate segmentation of\nbiomarkers such as drusen that points to the early stages of AMD is crucial in\npreventing further vision impairment. However, segmenting drusen is extremely\nchallenging due to their varied sizes and appearances, low contrast and noise\nresemblance. Most existing literature, therefore, have focused on size\nestimation of drusen using classification, leaving the challenge of accurate\nsegmentation less tackled. Additionally, obtaining the pixel-wise annotations\nis extremely costly and such labels can often be noisy, suffering from\ninter-observer and intra-observer variability. Quantification of uncertainty\nassociated with segmentation tasks offers principled measures to inspect the\nsegmentation output. Realizing its utility in identifying erroneous\nsegmentation and the potential applications in clinical decision making, here\nwe develop a U-Net based drusen segmentation model and quantify the\nsegmentation uncertainty. We investigate epistemic and aleatoric uncertainty\ncapturing model confidence and data uncertainty respectively. We present\nsegmentation results and show how uncertainty can help formulate robust\nevaluation strategies. We visually inspect the pixel-wise uncertainty and\nsegmentation results on test images. We finally analyze the correlation between\nsegmentation uncertainty and accuracy. Our results demonstrate the utility of\nleveraging uncertainties in developing and explaining segmentation models for\nmedical image analysis.\n", "versions": [{"version": "v1", "created": "Thu, 21 Jan 2021 23:34:29 GMT"}, {"version": "v2", "created": "Mon, 8 Feb 2021 03:45:31 GMT"}], "update_date": "2021-02-09", "authors_parsed": [["Joy", "Tinu Theckel", ""], ["Sedai", "Suman", ""], ["Garnavi", "Rahil", ""]]}, {"id": "2101.08894", "submitter": "Chandan Gautam", "authors": "Chandan Gautam, Sethupathy Parameswaran, Ashish Mishra, Suresh\n  Sundaram", "title": "Generative Replay-based Continual Zero-Shot Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Zero-shot learning is a new paradigm to classify objects from classes that\nare not available at training time. Zero-shot learning (ZSL) methods have\nattracted considerable attention in recent years because of their ability to\nclassify unseen/novel class examples. Most of the existing approaches on ZSL\nworks when all the samples from seen classes are available to train the model,\nwhich does not suit real life. In this paper, we tackle this hindrance by\ndeveloping a generative replay-based continual ZSL (GRCZSL). The proposed\nmethod endows traditional ZSL to learn from streaming data and acquire new\nknowledge without forgetting the previous tasks' gained experience. We handle\ncatastrophic forgetting in GRCZSL by replaying the synthetic samples of seen\nclasses, which have appeared in the earlier tasks. These synthetic samples are\nsynthesized using the trained conditional variational autoencoder (VAE) over\nthe immediate past task. Moreover, we only require the current and immediate\nprevious VAE at any time for training and testing. The proposed GRZSL method is\ndeveloped for a single-head setting of continual learning, simulating a\nreal-world problem setting. In this setting, task identity is given during\ntraining but unavailable during testing. GRCZSL performance is evaluated on\nfive benchmark datasets for the generalized setup of ZSL with fixed and dynamic\n(incremental class) settings of continual learning. The existing class setting\npresented recently in the literature is not suitable for a class-incremental\nsetting. Therefore, this paper proposes a new setting to address this issue.\nExperimental results show that the proposed method significantly outperforms\nthe baseline and the state-of-the-art method and makes it more suitable for\nreal-world applications.\n", "versions": [{"version": "v1", "created": "Fri, 22 Jan 2021 00:03:34 GMT"}, {"version": "v2", "created": "Mon, 7 Jun 2021 00:44:37 GMT"}], "update_date": "2021-06-08", "authors_parsed": [["Gautam", "Chandan", ""], ["Parameswaran", "Sethupathy", ""], ["Mishra", "Ashish", ""], ["Sundaram", "Suresh", ""]]}, {"id": "2101.08904", "submitter": "Xiong Liu", "authors": "Zhaoyi Chen, Xiong Liu, William Hogan, Elizabeth Shenkman, Jiang Bian", "title": "Applications of artificial intelligence in drug development using\n  real-world data", "comments": null, "journal-ref": "Drug Discovery Today 2020", "doi": "10.1016/j.drudis.2020.12.013", "report-no": "PMID: 33358699", "categories": "cs.CY cs.CL cs.LG q-bio.QM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The US Food and Drug Administration (FDA) has been actively promoting the use\nof real-world data (RWD) in drug development. RWD can generate important\nreal-world evidence reflecting the real-world clinical environment where the\ntreatments are used. Meanwhile, artificial intelligence (AI), especially\nmachine- and deep-learning (ML/DL) methods, have been increasingly used across\nmany stages of the drug development process. Advancements in AI have also\nprovided new strategies to analyze large, multidimensional RWD. Thus, we\nconducted a rapid review of articles from the past 20 years, to provide an\noverview of the drug development studies that use both AI and RWD. We found\nthat the most popular applications were adverse event detection, trial\nrecruitment, and drug repurposing. Here, we also discuss current research gaps\nand future opportunities.\n", "versions": [{"version": "v1", "created": "Fri, 22 Jan 2021 01:13:54 GMT"}, {"version": "v2", "created": "Tue, 2 Feb 2021 17:59:01 GMT"}], "update_date": "2021-02-03", "authors_parsed": [["Chen", "Zhaoyi", ""], ["Liu", "Xiong", ""], ["Hogan", "William", ""], ["Shenkman", "Elizabeth", ""], ["Bian", "Jiang", ""]]}, {"id": "2101.08912", "submitter": "Larry Denneau", "authors": "Amandin Chyba Rabeendran and Larry Denneau", "title": "A Two-Stage Deep Learning Detection Classifier for the ATLAS Asteroid\n  Survey", "comments": "15 pages, 10 figures", "journal-ref": null, "doi": "10.1088/1538-3873/abc900", "report-no": null, "categories": "astro-ph.EP astro-ph.IM cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we present a two-step neural network model to separate\ndetections of solar system objects from optical and electronic artifacts in\ndata obtained with the \"Asteroid Terrestrial-impact Last Alert System\" (ATLAS),\na near-Earth asteroid sky survey system [arXiv:1802.00879]. A convolutional\nneural network [arXiv:1807.10912] is used to classify small \"postage-stamp\"\nimages of candidate detections of astronomical sources into eight classes,\nfollowed by a multi-layered perceptron that provides a probability that a\ntemporal sequence of four candidate detections represents a real astronomical\nsource. The goal of this work is to reduce the time delay between Near-Earth\nObject (NEO) detections and submission to the Minor Planet Center. Due to the\nrare and hazardous nature of NEOs [Harris and D'Abramo, 2015], a low false\nnegative rate is a priority for the model. We show that the model reaches\n99.6\\% accuracy on real asteroids in ATLAS data with a 0.4\\% false negative\nrate. Deployment of this model on ATLAS has reduced the amount of NEO\ncandidates that astronomers must screen by 90%, thereby bringing ATLAS one step\ncloser to full autonomy.\n", "versions": [{"version": "v1", "created": "Fri, 22 Jan 2021 01:35:08 GMT"}], "update_date": "2021-02-24", "authors_parsed": [["Rabeendran", "Amandin Chyba", ""], ["Denneau", "Larry", ""]]}, {"id": "2101.08917", "submitter": "Vincent Tan", "authors": "Anshoo Tandon, Aldric H. J. Yuan, Vincent Y. F. Tan", "title": "SGA: A Robust Algorithm for Partial Recovery of Tree-Structured\n  Graphical Models with Noisy Samples", "comments": "23 pages, 14 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.IT cs.LG math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider learning Ising tree models when the observations from the nodes\nare corrupted by independent but non-identically distributed noise with unknown\nstatistics. Katiyar et al. (2020) showed that although the exact tree structure\ncannot be recovered, one can recover a partial tree structure; that is, a\nstructure belonging to the equivalence class containing the true tree. This\npaper presents a systematic improvement of Katiyar et al. (2020). First, we\npresent a novel impossibility result by deriving a bound on the necessary\nnumber of samples for partial recovery. Second, we derive a significantly\nimproved sample complexity result in which the dependence on the minimum\ncorrelation $\\rho_{\\min}$ is $\\rho_{\\min}^{-8}$ instead of $\\rho_{\\min}^{-24}$.\nFinally, we propose Symmetrized Geometric Averaging (SGA), a more statistically\nrobust algorithm for partial tree recovery. We provide error exponent analyses\nand extensive numerical results on a variety of trees to show that the sample\ncomplexity of SGA is significantly better than the algorithm of Katiyar et al.\n(2020). SGA can be readily extended to Gaussian models and is shown via\nnumerical experiments to be similarly superior.\n", "versions": [{"version": "v1", "created": "Fri, 22 Jan 2021 01:57:35 GMT"}], "update_date": "2021-01-25", "authors_parsed": [["Tandon", "Anshoo", ""], ["Yuan", "Aldric H. J.", ""], ["Tan", "Vincent Y. F.", ""]]}, {"id": "2101.08925", "submitter": "Puyu Wang", "authors": "Puyu Wang, Yunwen Lei, Yiming Ying, Hai Zhang", "title": "Differentially Private SGD with Non-Smooth Losses", "comments": "29 pages, 1 table, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper, we are concerned with differentially private {stochastic\ngradient descent (SGD)} algorithms in the setting of stochastic convex\noptimization (SCO). Most of the existing work requires the loss to be Lipschitz\ncontinuous and strongly smooth, and the model parameter to be uniformly\nbounded. However, these assumptions are restrictive as many popular losses\nviolate these conditions including the hinge loss for SVM, the absolute loss in\nrobust regression, and even the least square loss in an unbounded domain. We\nsignificantly relax these restrictive assumptions and establish privacy and\ngeneralization (utility) guarantees for private SGD algorithms using output and\ngradient perturbations associated with non-smooth convex losses. Specifically,\nthe loss function is relaxed to have an $\\alpha$-H\\\"{o}lder continuous gradient\n(referred to as $\\alpha$-H\\\"{o}lder smoothness) which instantiates the\nLipschitz continuity ($\\alpha=0$) and the strong smoothness ($\\alpha=1$). We\nprove that noisy SGD with $\\alpha$-H\\\"older smooth losses using gradient\nperturbation can guarantee $(\\epsilon,\\delta)$-differential privacy (DP) and\nattain optimal excess population risk\n$\\mathcal{O}\\Big(\\frac{\\sqrt{d\\log(1/\\delta)}}{n\\epsilon}+\\frac{1}{\\sqrt{n}}\\Big)$,\nup to logarithmic terms, with the gradient complexity $ \\mathcal{O}(\nn^{2-\\alpha\\over 1+\\alpha}+ n).$ This shows an important trade-off between\n$\\alpha$-H\\\"older smoothness of the loss and the computational complexity for\nprivate SGD with statistically optimal performance. In particular, our results\nindicate that $\\alpha$-H\\\"older smoothness with $\\alpha\\ge {1/2}$ is sufficient\nto guarantee $(\\epsilon,\\delta)$-DP of noisy SGD algorithms while achieving\noptimal excess risk with the linear gradient complexity $\\mathcal{O}(n).$\n", "versions": [{"version": "v1", "created": "Fri, 22 Jan 2021 03:19:06 GMT"}, {"version": "v2", "created": "Sat, 26 Jun 2021 10:34:48 GMT"}], "update_date": "2021-06-29", "authors_parsed": [["Wang", "Puyu", ""], ["Lei", "Yunwen", ""], ["Ying", "Yiming", ""], ["Zhang", "Hai", ""]]}, {"id": "2101.08926", "submitter": "Shuai Li", "authors": "Chuankun Li, Shuai Li, Yanbo Gao, Xiang Zhang, Wanqing Li", "title": "A Two-stream Neural Network for Pose-based Hand Gesture Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.MM", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Pose based hand gesture recognition has been widely studied in the recent\nyears. Compared with full body action recognition, hand gesture involves joints\nthat are more spatially closely distributed with stronger collaboration. This\nnature requires a different approach from action recognition to capturing the\ncomplex spatial features. Many gesture categories, such as \"Grab\" and \"Pinch\",\nhave very similar motion or temporal patterns posing a challenge on temporal\nprocessing. To address these challenges, this paper proposes a two-stream\nneural network with one stream being a self-attention based graph convolutional\nnetwork (SAGCN) extracting the short-term temporal information and hierarchical\nspatial information, and the other being a residual-connection enhanced\nbidirectional Independently Recurrent Neural Network (RBi-IndRNN) for\nextracting long-term temporal information. The self-attention based graph\nconvolutional network has a dynamic self-attention mechanism to adaptively\nexploit the relationships of all hand joints in addition to the fixed topology\nand local feature extraction in the GCN. On the other hand, the\nresidual-connection enhanced Bi-IndRNN extends an IndRNN with the capability of\nbidirectional processing for temporal modelling. The two streams are fused\ntogether for recognition. The Dynamic Hand Gesture dataset and First-Person\nHand Action dataset are used to validate its effectiveness, and our method\nachieves state-of-the-art performance.\n", "versions": [{"version": "v1", "created": "Fri, 22 Jan 2021 03:22:26 GMT"}], "update_date": "2021-01-25", "authors_parsed": [["Li", "Chuankun", ""], ["Li", "Shuai", ""], ["Gao", "Yanbo", ""], ["Zhang", "Xiang", ""], ["Li", "Wanqing", ""]]}, {"id": "2101.08937", "submitter": "Jin Young Shin", "authors": "Jinyoung Shin, Cheolhyeong Kim, Hyung Ju Hwang", "title": "Prior Preference Learning from Experts:Designing a Reward with Active\n  Inference", "comments": "This paper is under consideration at Pattern Recognition Letters", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Active inference may be defined as Bayesian modeling of a brain with a\nbiologically plausible model of the agent. Its primary idea relies on the free\nenergy principle and the prior preference of the agent. An agent will choose an\naction that leads to its prior preference for a future observation. In this\npaper, we claim that active inference can be interpreted using reinforcement\nlearning (RL) algorithms and find a theoretical connection between them. We\nextend the concept of expected free energy (EFE), which is a core quantity in\nactive inference, and claim that EFE can be treated as a negative value\nfunction. Motivated by the concept of prior preference and a theoretical\nconnection, we propose a simple but novel method for learning a prior\npreference from experts. This illustrates that the problem with inverse RL can\nbe approached with a new perspective of active inference. Experimental results\nof prior preference learning show the possibility of active inference with\nEFE-based rewards and its application to an inverse RL problem.\n", "versions": [{"version": "v1", "created": "Fri, 22 Jan 2021 04:03:45 GMT"}, {"version": "v2", "created": "Tue, 9 Feb 2021 05:02:02 GMT"}], "update_date": "2021-02-10", "authors_parsed": [["Shin", "Jinyoung", ""], ["Kim", "Cheolhyeong", ""], ["Hwang", "Hyung Ju", ""]]}, {"id": "2101.08954", "submitter": "Yuling Yao", "authors": "Yuling Yao, Gregor Pir\\v{s}, Aki Vehtari, Andrew Gelman", "title": "Bayesian hierarchical stacking: Some models are (somewhere) useful", "comments": "minor revision", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Stacking is a widely used model averaging technique that asymptotically\nyields optimal predictions among linear averages. We show that stacking is most\neffective when model predictive performance is heterogeneous in inputs, and we\ncan further improve the stacked mixture with a hierarchical model. We\ngeneralize stacking to Bayesian hierarchical stacking. The model weights are\nvarying as a function of data, partially-pooled, and inferred using Bayesian\ninference. We further incorporate discrete and continuous inputs, other\nstructured priors, and time series and longitudinal data. To verify the\nperformance gain of the proposed method, we derive theory bounds, and\ndemonstrate on several applied problems.\n", "versions": [{"version": "v1", "created": "Fri, 22 Jan 2021 05:19:49 GMT"}, {"version": "v2", "created": "Thu, 20 May 2021 22:14:30 GMT"}], "update_date": "2021-05-24", "authors_parsed": [["Yao", "Yuling", ""], ["Pir\u0161", "Gregor", ""], ["Vehtari", "Aki", ""], ["Gelman", "Andrew", ""]]}, {"id": "2101.08969", "submitter": "Yuzhou Lin", "authors": "Yuzhou Lin", "title": "A novel DL approach to PE malware detection: exploring Glove\n  vectorization, MCC_RCNN and feature fusion", "comments": "Some errors made in paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, malware becomes more threatening. Concerning the increasing\nmalware variants, there comes Machine Learning (ML)-based and Deep Learning\n(DL)-based approaches for heuristic detection. Nevertheless, the prediction\naccuracy of both needs to be improved. In response to the above issues in the\nPE malware domain, we propose the DL-based approaches for detection and use\nstatic-based features fed up into models. The contributions are as follows: we\nrecapitulate existing malware detection methods. That is, we propose a\nvec-torized representation model of the malware instruction layer and semantic\nlayer based on Glove. We implement a neural network model called MCC_RCNN\n(Malware Detection and Recurrent Convolutional Neural Network), comprising of\nthe combination with CNN and RNN. Moreover, we provide a description of feature\nfusion in static behavior levels. With the numerical results generated from\nseveral comparative experiments towards evaluating the Glove-based\nvectoriza-tion, MCC_RCNN-based classification methodology and feature fusion\nstages, our proposed classification methods can obtain a higher prediction\naccuracy than the other baseline methods.\n", "versions": [{"version": "v1", "created": "Fri, 22 Jan 2021 07:08:10 GMT"}, {"version": "v2", "created": "Sun, 31 Jan 2021 06:25:38 GMT"}, {"version": "v3", "created": "Thu, 4 Feb 2021 09:06:02 GMT"}], "update_date": "2021-02-05", "authors_parsed": [["Lin", "Yuzhou", ""]]}, {"id": "2101.08980", "submitter": "Lai Wei", "authors": "Lai Wei and Vaibhav Srivastava", "title": "Nonstationary Stochastic Multiarmed Bandits: UCB Policies and Minimax\n  Regret", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  We study the nonstationary stochastic Multi-Armed Bandit (MAB) problem in\nwhich the distribution of rewards associated with each arm are assumed to be\ntime-varying and the total variation in the expected rewards is subject to a\nvariation budget. The regret of a policy is defined by the difference in the\nexpected cumulative rewards obtained using the policy and using an oracle that\nselects the arm with the maximum mean reward at each time. We characterize the\nperformance of the proposed policies in terms of the worst-case regret, which\nis the supremum of the regret over the set of reward distribution sequences\nsatisfying the variation budget. We extend Upper-Confidence Bound (UCB)-based\npolicies with three different approaches, namely, periodic resetting, sliding\nobservation window and discount factor and show that they are order-optimal\nwith respect to the minimax regret, i.e., the minimum worst-case regret\nachieved by any policy. We also relax the sub-Gaussian assumption on reward\ndistributions and develop robust versions the proposed polices that can handle\nheavy-tailed reward distributions and maintain their performance guarantees.\n", "versions": [{"version": "v1", "created": "Fri, 22 Jan 2021 07:34:09 GMT"}], "update_date": "2021-01-25", "authors_parsed": [["Wei", "Lai", ""], ["Srivastava", "Vaibhav", ""]]}, {"id": "2101.09001", "submitter": "Martin Hellkvist", "authors": "Martin Hellkvist and Ay\\c{c}a \\\"Oz\\c{c}elikkale and Anders Ahl\\'en", "title": "Linear Regression with Distributed Learning: A Generalization Error\n  Perspective", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG eess.SP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Distributed learning provides an attractive framework for scaling the\nlearning task by sharing the computational load over multiple nodes in a\nnetwork. Here, we investigate the performance of distributed learning for\nlarge-scale linear regression where the model parameters, i.e., the unknowns,\nare distributed over the network. We adopt a statistical learning approach. In\ncontrast to works that focus on the performance on the training data, we focus\non the generalization error, i.e., the performance on unseen data. We provide\nhigh-probability bounds on the generalization error for both isotropic and\ncorrelated Gaussian data as well as sub-gaussian data. These results reveal the\ndependence of the generalization performance on the partitioning of the model\nover the network. In particular, our results show that the generalization error\nof the distributed solution can be substantially higher than that of the\ncentralized solution even when the error on the training data is at the same\nlevel for both the centralized and distributed approaches. Our numerical\nresults illustrate the performance with both real-world image data as well as\nsynthetic data.\n", "versions": [{"version": "v1", "created": "Fri, 22 Jan 2021 08:43:28 GMT"}, {"version": "v2", "created": "Tue, 25 May 2021 14:16:24 GMT"}], "update_date": "2021-05-26", "authors_parsed": [["Hellkvist", "Martin", ""], ["\u00d6z\u00e7elikkale", "Ay\u00e7a", ""], ["Ahl\u00e9n", "Anders", ""]]}, {"id": "2101.09004", "submitter": "Suman Dowlagar", "authors": "Suman Dowlagar, Radhika Mamidi", "title": "CMSAOne@Dravidian-CodeMix-FIRE2020: A Meta Embedding and Transformer\n  model for Code-Mixed Sentiment Analysis on Social Media Text", "comments": "FIRE 2020: Forum for Information Retrieval Evaluation, December\n  16-20, 2020, Hyderabad, India", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Code-mixing(CM) is a frequently observed phenomenon that uses multiple\nlanguages in an utterance or sentence. CM is mostly practiced on various social\nmedia platforms and in informal conversations. Sentiment analysis (SA) is a\nfundamental step in NLP and is well studied in the monolingual text.\nCode-mixing adds a challenge to sentiment analysis due to its non-standard\nrepresentations. This paper proposes a meta embedding with a transformer method\nfor sentiment analysis on the Dravidian code-mixed dataset. In our method, we\nused meta embeddings to capture rich text representations. We used the proposed\nmethod for the Task: \"Sentiment Analysis for Dravidian Languages in Code-Mixed\nText\", and it achieved an F1 score of $0.58$ and $0.66$ for the given Dravidian\ncode mixed data sets. The code is provided in the Github\nhttps://github.com/suman101112/fire-2020-Dravidian-CodeMix.\n", "versions": [{"version": "v1", "created": "Fri, 22 Jan 2021 08:48:27 GMT"}], "update_date": "2021-01-25", "authors_parsed": [["Dowlagar", "Suman", ""], ["Mamidi", "Radhika", ""]]}, {"id": "2101.09023", "submitter": "Terry Ruas Ph.D.", "authors": "Terry Ruas, Charles Henrique Porto Ferreira, William Grosky,\n  Fabr\\'icio Olivetti de Fran\\c{c}a, D\\'ebora Maria Rossi Medeiros", "title": "Enhanced word embeddings using multi-semantic representation through\n  lexical chains", "comments": null, "journal-ref": "Information Sciences. Volume 532, September 2020, Pages 16-32", "doi": "10.1016/j.ins.2020.04.048", "report-no": null, "categories": "cs.CL cs.LG", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  The relationship between words in a sentence often tells us more about the\nunderlying semantic content of a document than its actual words, individually.\nIn this work, we propose two novel algorithms, called Flexible Lexical Chain II\nand Fixed Lexical Chain II. These algorithms combine the semantic relations\nderived from lexical chains, prior knowledge from lexical databases, and the\nrobustness of the distributional hypothesis in word embeddings as building\nblocks forming a single system. In short, our approach has three main\ncontributions: (i) a set of techniques that fully integrate word embeddings and\nlexical chains; (ii) a more robust semantic representation that considers the\nlatent relation between words in a document; and (iii) lightweight word\nembeddings models that can be extended to any natural language task. We intend\nto assess the knowledge of pre-trained models to evaluate their robustness in\nthe document classification task. The proposed techniques are tested against\nseven word embeddings algorithms using five different machine learning\nclassifiers over six scenarios in the document classification task. Our results\nshow the integration between lexical chains and word embeddings representations\nsustain state-of-the-art results, even against more complex systems.\n", "versions": [{"version": "v1", "created": "Fri, 22 Jan 2021 09:43:33 GMT"}], "update_date": "2021-01-25", "authors_parsed": [["Ruas", "Terry", ""], ["Ferreira", "Charles Henrique Porto", ""], ["Grosky", "William", ""], ["de Fran\u00e7a", "Fabr\u00edcio Olivetti", ""], ["Medeiros", "D\u00e9bora Maria Rossi", ""]]}, {"id": "2101.09048", "submitter": "Shiwei Liu", "authors": "Shiwei Liu, Decebal Constantin Mocanu, Yulong Pei, Mykola Pechenizkiy", "title": "Selfish Sparse RNN Training", "comments": "Published in Proceedings of the 38th International Conference on\n  Machine Learning. Code can be found in\n  https://github.com/Shiweiliuiiiiiii/Selfish-RNN", "journal-ref": "Proceedings of the 38th International Conference on Machine\n  Learning (2021)", "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sparse neural networks have been widely applied to reduce the computational\ndemands of training and deploying over-parameterized deep neural networks. For\ninference acceleration, methods that discover a sparse network from a\npre-trained dense network (dense-to-sparse training) work effectively.\nRecently, dynamic sparse training (DST) has been proposed to train sparse\nneural networks without pre-training a dense model (sparse-to-sparse training),\nso that the training process can also be accelerated. However, previous\nsparse-to-sparse methods mainly focus on Multilayer Perceptron Networks (MLPs)\nand Convolutional Neural Networks (CNNs), failing to match the performance of\ndense-to-sparse methods in the Recurrent Neural Networks (RNNs) setting. In\nthis paper, we propose an approach to train intrinsically sparse RNNs with a\nfixed parameter count in one single run, without compromising performance.\nDuring training, we allow RNN layers to have a non-uniform redistribution\nacross cell gates for better regularization. Further, we propose SNT-ASGD, a\nnovel variant of the averaged stochastic gradient optimizer, which\nsignificantly improves the performance of all sparse training methods for RNNs.\nUsing these strategies, we achieve state-of-the-art sparse training results,\nbetter than the dense-to-sparse methods, with various types of RNNs on Penn\nTreeBank and Wikitext-2 datasets. Our codes are available at\nhttps://github.com/Shiweiliuiiiiiii/Selfish-RNN.\n", "versions": [{"version": "v1", "created": "Fri, 22 Jan 2021 10:45:40 GMT"}, {"version": "v2", "created": "Thu, 28 Jan 2021 16:38:09 GMT"}, {"version": "v3", "created": "Tue, 15 Jun 2021 05:46:23 GMT"}], "update_date": "2021-06-16", "authors_parsed": [["Liu", "Shiwei", ""], ["Mocanu", "Decebal Constantin", ""], ["Pei", "Yulong", ""], ["Pechenizkiy", "Mykola", ""]]}, {"id": "2101.09054", "submitter": "Yuval Dagan", "authors": "Noga Alon, Omri Ben-Eliezer, Yuval Dagan, Shay Moran, Moni Naor, Eylon\n  Yogev", "title": "Adversarial Laws of Large Numbers and Optimal Regret in Online\n  Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR cs.DS math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Laws of large numbers guarantee that given a large enough sample from some\npopulation, the measure of any fixed sub-population is well-estimated by its\nfrequency in the sample. We study laws of large numbers in sampling processes\nthat can affect the environment they are acting upon and interact with it.\nSpecifically, we consider the sequential sampling model proposed by Ben-Eliezer\nand Yogev (2020), and characterize the classes which admit a uniform law of\nlarge numbers in this model: these are exactly the classes that are\n\\emph{online learnable}. Our characterization may be interpreted as an online\nanalogue to the equivalence between learnability and uniform convergence in\nstatistical (PAC) learning.\n  The sample-complexity bounds we obtain are tight for many parameter regimes,\nand as an application, we determine the optimal regret bounds in online\nlearning, stated in terms of \\emph{Littlestone's dimension}, thus resolving the\nmain open question from Ben-David, P\\'al, and Shalev-Shwartz (2009), which was\nalso posed by Rakhlin, Sridharan, and Tewari (2015).\n", "versions": [{"version": "v1", "created": "Fri, 22 Jan 2021 11:15:19 GMT"}], "update_date": "2021-01-25", "authors_parsed": [["Alon", "Noga", ""], ["Ben-Eliezer", "Omri", ""], ["Dagan", "Yuval", ""], ["Moran", "Shay", ""], ["Naor", "Moni", ""], ["Yogev", "Eylon", ""]]}, {"id": "2101.09056", "submitter": "Mark Keane", "authors": "Barry Smyth and Mark T Keane", "title": "A Few Good Counterfactuals: Generating Interpretable, Plausible and\n  Diverse Counterfactual Explanations", "comments": "8 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Counterfactual explanations provide a potentially significant solution to the\nExplainable AI (XAI) problem, but good, native counterfactuals have been shown\nto rarely occur in most datasets. Hence, the most popular methods generate\nsynthetic counterfactuals using blind perturbation. However, such methods have\nseveral shortcomings: the resulting counterfactuals (i) may not be valid\ndata-points (they often use features that do not naturally occur), (ii) may\nlack the sparsity of good counterfactuals (if they modify too many features),\nand (iii) may lack diversity (if the generated counterfactuals are minimal\nvariants of one another). We describe a method designed to overcome these\nproblems, one that adapts native counterfactuals in the original dataset, to\ngenerate sparse, diverse synthetic counterfactuals from naturally occurring\nfeatures. A series of experiments are reported that systematically explore\nparametric variations of this novel method on common datasets to establish the\nconditions for optimal performance.\n", "versions": [{"version": "v1", "created": "Fri, 22 Jan 2021 11:30:26 GMT"}], "update_date": "2021-01-25", "authors_parsed": [["Smyth", "Barry", ""], ["Keane", "Mark T", ""]]}, {"id": "2101.09060", "submitter": "Francesco Cappio Borlino", "authors": "Francesco Cappio Borlino, Antonio D'Innocente, Tatiana Tommasi", "title": "Rethinking Domain Generalization Baselines", "comments": "Accepted at ICPR 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite being very powerful in standard learning settings, deep learning\nmodels can be extremely brittle when deployed in scenarios different from those\non which they were trained. Domain generalization methods investigate this\nproblem and data augmentation strategies have shown to be helpful tools to\nincrease data variability, supporting model robustness across domains. In our\nwork we focus on style transfer data augmentation and we present how it can be\nimplemented with a simple and inexpensive strategy to improve generalization.\nMoreover, we analyze the behavior of current state of the art domain\ngeneralization methods when integrated with this augmentation solution: our\nthorough experimental evaluation shows that their original effect almost always\ndisappears with respect to the augmented baseline. This issue open new\nscenarios for domain generalization research, highlighting the need of novel\nmethods properly able to take advantage of the introduced data variability.\n", "versions": [{"version": "v1", "created": "Fri, 22 Jan 2021 11:35:58 GMT"}, {"version": "v2", "created": "Wed, 27 Jan 2021 10:41:53 GMT"}], "update_date": "2021-01-28", "authors_parsed": [["Borlino", "Francesco Cappio", ""], ["D'Innocente", "Antonio", ""], ["Tommasi", "Tatiana", ""]]}, {"id": "2101.09066", "submitter": "Luis Leiva", "authors": "Lukas Br\\\"uckner and Ioannis Arapakis and Luis A. Leiva", "title": "Query Abandonment Prediction with Recurrent Neural Models of Mouse\n  Cursor Movements", "comments": null, "journal-ref": "Proceedings of the 29th ACM Intl. Conf. on Information And\n  Knowledge Management (CIKM), 2020", "doi": "10.1145/3340531.3412126", "report-no": null, "categories": "cs.IR cs.HC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most successful search queries do not result in a click if the user can\nsatisfy their information needs directly on the SERP. Modeling query\nabandonment in the absence of click-through data is challenging because search\nengines must rely on other behavioral signals to understand the underlying\nsearch intent. We show that mouse cursor movements make a valuable, low-cost\nbehavioral signal that can discriminate good and bad abandonment. We model\nmouse movements on SERPs using recurrent neural nets and explore several data\nrepresentations that do not rely on expensive hand-crafted features and do not\ndepend on a particular SERP structure. We also experiment with data resampling\nand augmentation techniques that we adopt for sequential data. Our results can\nhelp search providers to gauge user satisfaction for queries without clicks and\nultimately contribute to a better understanding of search engine performance.\n", "versions": [{"version": "v1", "created": "Fri, 22 Jan 2021 11:57:04 GMT"}], "update_date": "2021-01-25", "authors_parsed": [["Br\u00fcckner", "Lukas", ""], ["Arapakis", "Ioannis", ""], ["Leiva", "Luis A.", ""]]}, {"id": "2101.09069", "submitter": "Simon Hengchen", "authors": "Valerio Perrone and Simon Hengchen and Marco Palma and Alessandro\n  Vatri and Jim Q. Smith and Barbara McGillivray", "title": "Lexical semantic change for Ancient Greek and Latin", "comments": "To appear in: Nina Tahmasebi, Lars Borin, Adam Jatowt, Yang Xu, Simon\n  Hengchen (eds). Computational Approaches to Semantic Change. Berlin: Language\n  Science Press. [preliminary page numbering]", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Change and its precondition, variation, are inherent in languages. Over time,\nnew words enter the lexicon, others become obsolete, and existing words acquire\nnew senses. Associating a word's correct meaning in its historical context is a\ncentral challenge in diachronic research. Historical corpora of classical\nlanguages, such as Ancient Greek and Latin, typically come with rich metadata,\nand existing models are limited by their inability to exploit contextual\ninformation beyond the document timestamp. While embedding-based methods\nfeature among the current state of the art systems, they are lacking in the\ninterpretative power. In contrast, Bayesian models provide explicit and\ninterpretable representations of semantic change phenomena. In this chapter we\nbuild on GASC, a recent computational approach to semantic change based on a\ndynamic Bayesian mixture model. In this model, the evolution of word senses\nover time is based not only on distributional information of lexical nature,\nbut also on text genres. We provide a systematic comparison of dynamic Bayesian\nmixture models for semantic change with state-of-the-art embedding-based\nmodels. On top of providing a full description of meaning change over time, we\nshow that Bayesian mixture models are highly competitive approaches to detect\nbinary semantic change in both Ancient Greek and Latin.\n", "versions": [{"version": "v1", "created": "Fri, 22 Jan 2021 12:04:08 GMT"}], "update_date": "2021-01-25", "authors_parsed": [["Perrone", "Valerio", ""], ["Hengchen", "Simon", ""], ["Palma", "Marco", ""], ["Vatri", "Alessandro", ""], ["Smith", "Jim Q.", ""], ["McGillivray", "Barbara", ""]]}, {"id": "2101.09090", "submitter": "Caglar Demir", "authors": "Caglar Demir and Diego Moussallem and Axel-Cyrille Ngonga Ngomo", "title": "A shallow neural model for relation prediction", "comments": "15th IEEE International Conference on Semantic Computing, ICSC-2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Knowledge graph completion refers to predicting missing triples. Most\napproaches achieve this goal by predicting entities, given an entity and a\nrelation. We predict missing triples via the relation prediction. To this end,\nwe frame the relation prediction problem as a multi-label classification\nproblem and propose a shallow neural model (SHALLOM) that accurately infers\nmissing relations from entities. SHALLOM is analogous to C-BOW as both\napproaches predict a central token (p) given surrounding tokens ((s,o)). Our\nexperiments indicate that SHALLOM outperforms state-of-the-art approaches on\nthe FB15K-237 and WN18RR with margins of up to $3\\%$ and $8\\%$ (absolute),\nrespectively, while requiring a maximum training time of 8 minutes on these\ndatasets. We ensure the reproducibility of our results by providing an\nopen-source implementation including training and evaluation scripts at\n{\\url{https://github.com/dice-group/Shallom}.}\n", "versions": [{"version": "v1", "created": "Fri, 22 Jan 2021 13:10:11 GMT"}], "update_date": "2021-01-25", "authors_parsed": [["Demir", "Caglar", ""], ["Moussallem", "Diego", ""], ["Ngomo", "Axel-Cyrille Ngonga", ""]]}, {"id": "2101.09101", "submitter": "Ming Liang", "authors": "Ming Liang and Kui Xue and Tong Ruan", "title": "A multi-perspective combined recall and rank framework for Chinese\n  procedure terminology normalization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Medical terminology normalization aims to map the clinical mention to\nterminologies come from a knowledge base, which plays an important role in\nanalyzing Electronic Health Record(EHR) and many downstream tasks. In this\npaper, we focus on Chinese procedure terminology normalization. The expression\nof terminologies are various and one medical mention may be linked to multiple\nterminologies. Previous study explores some methods such as multi-class\nclassification or learning to rank(LTR) to sort the terminologies by literature\nand semantic information. However, these information is inadequate to find the\nright terminologies, particularly in multi-implication cases. In this work, we\npropose a combined recall and rank framework to solve the above problems. This\nframework is composed of a multi-task candidate generator(MTCG), a keywords\nattentive ranker(KAR) and a fusion block(FB). MTCG is utilized to predict the\nmention implication number and recall candidates with semantic similarity. KAR\nis based on Bert with a keywords attentive mechanism which focuses on keywords\nsuch as procedure sites and procedure types. FB merges the similarity come from\nMTCG and KAR to sort the terminologies from different perspectives. Detailed\nexperimental analysis shows our proposed framework has a remarkable improvement\non both performance and efficiency.\n", "versions": [{"version": "v1", "created": "Fri, 22 Jan 2021 13:37:10 GMT"}], "update_date": "2021-01-25", "authors_parsed": [["Liang", "Ming", ""], ["Xue", "Kui", ""], ["Ruan", "Tong", ""]]}, {"id": "2101.09108", "submitter": "Jay Morgan", "authors": "Jay Morgan, Adeline Paiement, Arno Pauly, Monika Seisenberger", "title": "Adaptive Neighbourhoods for the Discovery of Adversarial Examples", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Deep Neural Networks (DNNs) have often supplied state-of-the-art results in\npattern recognition tasks. Despite their advances, however, the existence of\nadversarial examples have caught the attention of the community. Many existing\nworks have proposed methods for searching for adversarial examples within\nfixed-sized regions around training points. Our work complements and improves\nthese existing approaches by adapting the size of these regions based on the\nproblem complexity and data sampling density. This makes such approaches more\nappropriate for other types of data and may further improve adversarial\ntraining methods by increasing the region sizes without creating incorrect\nlabels.\n", "versions": [{"version": "v1", "created": "Fri, 22 Jan 2021 13:53:51 GMT"}], "update_date": "2021-01-25", "authors_parsed": [["Morgan", "Jay", ""], ["Paiement", "Adeline", ""], ["Pauly", "Arno", ""], ["Seisenberger", "Monika", ""]]}, {"id": "2101.09113", "submitter": "Todd Huster", "authors": "Todd Huster, Jeremy E.J. Cohen, Zinan Lin, Kevin Chan, Charles\n  Kamhoua, Nandi Leslie, Cho-Yu Jason Chiang, Vyas Sekar", "title": "Pareto GAN: Extending the Representational Power of GANs to Heavy-Tailed\n  Distributions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generative adversarial networks (GANs) are often billed as \"universal\ndistribution learners\", but precisely what distributions they can represent and\nlearn is still an open question. Heavy-tailed distributions are prevalent in\nmany different domains such as financial risk-assessment, physics, and\nepidemiology. We observe that existing GAN architectures do a poor job of\nmatching the asymptotic behavior of heavy-tailed distributions, a problem that\nwe show stems from their construction. Additionally, when faced with the\ninfinite moments and large distances between outlier points that are\ncharacteristic of heavy-tailed distributions, common loss functions produce\nunstable or near-zero gradients. We address these problems with the Pareto GAN.\nA Pareto GAN leverages extreme value theory and the functional properties of\nneural networks to learn a distribution that matches the asymptotic behavior of\nthe marginal distributions of the features. We identify issues with standard\nloss functions and propose the use of alternative metric spaces that enable\nstable and efficient learning. Finally, we evaluate our proposed approach on a\nvariety of heavy-tailed datasets.\n", "versions": [{"version": "v1", "created": "Fri, 22 Jan 2021 14:06:02 GMT"}], "update_date": "2021-01-25", "authors_parsed": [["Huster", "Todd", ""], ["Cohen", "Jeremy E. J.", ""], ["Lin", "Zinan", ""], ["Chan", "Kevin", ""], ["Kamhoua", "Charles", ""], ["Leslie", "Nandi", ""], ["Chiang", "Cho-Yu Jason", ""], ["Sekar", "Vyas", ""]]}, {"id": "2101.09126", "submitter": "Christopher Irrgang", "authors": "Christopher Irrgang (1), Niklas Boers (2 and 3 and 4), Maike Sonnewald\n  (5 and 6 and 7), Elizabeth A. Barnes (8), Christopher Kadow (9), Joanna\n  Staneva (10), Jan Saynisch-Wagner (1) ((1) Helmholtz Centre Potsdam, German\n  Research Centre for Geosciences GFZ, Potsdam, Germany, (2) Department of\n  Mathematics and Computer Science, Free University of Berlin, Germany, (3)\n  Potsdam Institute for Climate Impact Research, Potsdam, Germany (4)\n  Department of Mathematics and Global Systems Institute, University of Exeter,\n  Exeter, UK (5) Program in Atmospheric and Oceanic Sciences, Princeton\n  University, Princeton, USA (6) NOAA/OAR Geophysical Fluid Dynamics\n  Laboratory, Ocean and Cryosphere Division, Princeton, USA (7) University of\n  Washington, School of Oceanography, Seattle, USA (8) Colorado State\n  University, Fort Collins, USA (9) German Climate Computing Center DKRZ,\n  Hamburg, Germany (10) Helmholtz-Zentrum Geesthacht, Center for Material and\n  Coastal Research HZG, Geesthacht, Germany)", "title": "Will Artificial Intelligence supersede Earth System and Climate Models?", "comments": "Perspective paper submitted to Nature Machine Intelligence, 23 pages,\n  3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG physics.ao-ph", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We outline a perspective of an entirely new research branch in Earth and\nclimate sciences, where deep neural networks and Earth system models are\ndismantled as individual methodological approaches and reassembled as learning,\nself-validating, and interpretable Earth system model-network hybrids.\nFollowing this path, we coin the term \"Neural Earth System Modelling\" (NESYM)\nand highlight the necessity of a transdisciplinary discussion platform,\nbringing together Earth and climate scientists, big data analysts, and AI\nexperts. We examine the concurrent potential and pitfalls of Neural Earth\nSystem Modelling and discuss the open question whether artificial intelligence\nwill not only infuse Earth system modelling, but ultimately render them\nobsolete.\n", "versions": [{"version": "v1", "created": "Fri, 22 Jan 2021 14:33:24 GMT"}], "update_date": "2021-01-25", "authors_parsed": [["Irrgang", "Christopher", "", "2 and 3 and 4"], ["Boers", "Niklas", "", "2 and 3 and 4"], ["Sonnewald", "Maike", "", "5 and 6 and 7"], ["Barnes", "Elizabeth A.", ""], ["Kadow", "Christopher", ""], ["Staneva", "Joanna", ""], ["Saynisch-Wagner", "Jan", ""]]}, {"id": "2101.09137", "submitter": "Chongwen Huang", "authors": "Chongwen Huang, Zhaohui Yang, George C. Alexandropoulos, Kai Xiong, Li\n  Wei, Chau Yuen, Zhaoyang Zhang, and Merouane Debbah", "title": "Multi-hop RIS-Empowered Terahertz Communications: A DRL-based Hybrid\n  Beamforming Design", "comments": "IEEE Journal on Selected Areas in Communications, Special issue:\n  TeraHertz Communications and Networking. arXiv admin note: substantial text\n  overlap with arXiv:2009.09380", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SP cs.IT cs.LG math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Wireless communication in the TeraHertz band (0.1--10 THz) is envisioned as\none of the key enabling technologies for the future sixth generation (6G)\nwireless communication systems scaled up beyond massive multiple input multiple\noutput (Massive-MIMO) technology. However, very high propagation attenuations\nand molecular absorptions of THz frequencies often limit the signal\ntransmission distance and coverage range. Benefited from the recent\nbreakthrough on the reconfigurable intelligent surfaces (RIS) for realizing\nsmart radio propagation environment, we propose a novel hybrid beamforming\nscheme for the multi-hop RIS-assisted communication networks to improve the\ncoverage range at THz-band frequencies. Particularly, multiple passive and\ncontrollable RISs are deployed to assist the transmissions between the base\nstation (BS) and multiple single-antenna users. We investigate the joint design\nof digital beamforming matrix at the BS and analog beamforming matrices at the\nRISs, by leveraging the recent advances in deep reinforcement learning (DRL) to\ncombat the propagation loss. To improve the convergence of the proposed\nDRL-based algorithm, two algorithms are then designed to initialize the digital\nbeamforming and the analog beamforming matrices utilizing the alternating\noptimization technique. Simulation results show that our proposed scheme is\nable to improve 50\\% more coverage range of THz communications compared with\nthe benchmarks. Furthermore, it is also shown that our proposed DRL-based\nmethod is a state-of-the-art method to solve the NP-hard beamforming problem,\nespecially when the signals at RIS-assisted THz communication networks\nexperience multiple hops.\n", "versions": [{"version": "v1", "created": "Fri, 22 Jan 2021 14:56:28 GMT"}], "update_date": "2021-01-25", "authors_parsed": [["Huang", "Chongwen", ""], ["Yang", "Zhaohui", ""], ["Alexandropoulos", "George C.", ""], ["Xiong", "Kai", ""], ["Wei", "Li", ""], ["Yuen", "Chau", ""], ["Zhang", "Zhaoyang", ""], ["Debbah", "Merouane", ""]]}, {"id": "2101.09142", "submitter": "Stanis{\\l}aw Purga{\\l}", "authors": "Stanis{\\l}aw Purga{\\l}, Julian Parsert, Cezary Kaliszyk", "title": "A Study of Continuous Vector Representationsfor Theorem Proving", "comments": null, "journal-ref": null, "doi": "10.1093/logcom/exab006", "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Applying machine learning to mathematical terms and formulas requires a\nsuitable representation of formulas that is adequate for AI methods. In this\npaper, we develop an encoding that allows for logical properties to be\npreserved and is additionally reversible. This means that the tree shape of a\nformula including all symbols can be reconstructed from the dense vector\nrepresentation. We do that by training two decoders: one that extracts the top\nsymbol of the tree and one that extracts embedding vectors of subtrees. The\nsyntactic and semantic logical properties that we aim to reserve include both\nstructural formula properties, applicability of natural deduction steps, and\neven more complex operations like unifiability. We propose datasets that can be\nused to train these syntactic and semantic properties. We evaluate the\nviability of the developed encoding across the proposed datasets as well as for\nthe practical theorem proving problem of premise selection in the Mizar corpus.\n", "versions": [{"version": "v1", "created": "Fri, 22 Jan 2021 15:04:54 GMT"}], "update_date": "2021-01-25", "authors_parsed": [["Purga\u0142", "Stanis\u0142aw", ""], ["Parsert", "Julian", ""], ["Kaliszyk", "Cezary", ""]]}, {"id": "2101.09143", "submitter": "Armin Catovic", "authors": "Forough Yaghoubi (1), Armin Catovic (2), Arthur Gusmao (1), Jan\n  Pieczkowski (1), Peter Boros (1) ((1) Ericsson AB, (2) Schibsted Media Group)", "title": "Traffic Flow Estimation using LTE Radio Frequency Counters and Machine\n  Learning", "comments": "9 pages, 5 figures; submitted to ACM SIGCOMM 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SP cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  As the demand for vehicles continues to outpace construction of new roads, it\nbecomes imperative we implement strategies that improve utilization of existing\ntransport infrastructure. Traffic sensors form a crucial part of many such\nstrategies, giving us valuable insights into road utilization. However, due to\ncost and lead time associated with installation and maintenance of traffic\nsensors, municipalities and traffic authorities look toward cheaper and more\nscalable alternatives. Due to their ubiquitous nature and wide global\ndeployment, cellular networks offer one such alternative. In this paper we\npresent a novel method for traffic flow estimation using standardized LTE/4G\nradio frequency performance measurement counters. The problem is cast as a\nsupervised regression task using both classical and deep learning methods. We\nfurther apply transfer learning to compensate that many locations lack traffic\nsensor data that could be used for training. We show that our approach benefits\nfrom applying transfer learning to generalize the solution not only in time but\nalso in space (i.e., various parts of the city). The results are very promising\nand, unlike competing solutions, our approach utilizes aggregate LTE radio\nfrequency counter data that is inherently privacy-preserving, readily\navailable, and scales globally without any additional network impact.\n", "versions": [{"version": "v1", "created": "Fri, 22 Jan 2021 15:05:10 GMT"}], "update_date": "2021-01-25", "authors_parsed": [["Yaghoubi", "Forough", "", "Ericsson AB"], ["Catovic", "Armin", "", "Schibsted Media Group"], ["Gusmao", "Arthur", "", "Ericsson AB"], ["Pieczkowski", "Jan", "", "Ericsson AB"], ["Boros", "Peter", "", "Ericsson AB"]]}, {"id": "2101.09149", "submitter": "Orion Weller", "authors": "Orion Weller and Matthias Sperber and Christian Gollan and Joris\n  Kluivers", "title": "Streaming Models for Joint Speech Recognition and Translation", "comments": "Camera Ready for EACL 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Using end-to-end models for speech translation (ST) has increasingly been the\nfocus of the ST community. These models condense the previously cascaded\nsystems by directly converting sound waves into translated text. However,\ncascaded models have the advantage of including automatic speech recognition\noutput, useful for a variety of practical ST systems that often display\ntranscripts to the user alongside the translations. To bridge this gap, recent\nwork has shown initial progress into the feasibility for end-to-end models to\nproduce both of these outputs. However, all previous work has only looked at\nthis problem from the consecutive perspective, leaving uncertainty on whether\nthese approaches are effective in the more challenging streaming setting. We\ndevelop an end-to-end streaming ST model based on a re-translation approach and\ncompare against standard cascading approaches. We also introduce a novel\ninference method for the joint case, interleaving both transcript and\ntranslation in generation and removing the need to use separate decoders. Our\nevaluation across a range of metrics capturing accuracy, latency, and\nconsistency shows that our end-to-end models are statistically similar to\ncascading models, while having half the number of parameters. We also find that\nboth systems provide strong translation quality at low latency, keeping 99% of\nconsecutive quality at a lag of just under a second.\n", "versions": [{"version": "v1", "created": "Fri, 22 Jan 2021 15:16:54 GMT"}], "update_date": "2021-01-25", "authors_parsed": [["Weller", "Orion", ""], ["Sperber", "Matthias", ""], ["Gollan", "Christian", ""], ["Kluivers", "Joris", ""]]}, {"id": "2101.09162", "submitter": "Elias Iosif", "authors": "Elias Iosif and Klitos Christodoulou and Andreas Vlachos", "title": "A Robust Blockchain Readiness Index Model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As the blockchain ecosystem gets more mature many businesses, investors, and\nentrepreneurs are seeking opportunities on working with blockchain systems and\ncryptocurrencies. A critical challenge for these actors is to identify the most\nsuitable environment to start or evolve their businesses. In general, the\nquestion is to identify which countries are offering the most suitable\nconditions to host their blockchain-based activities and implement their\ninnovative projects. The Blockchain Readiness Index (BRI) provides a numerical\nmetric (referred to as the blockchain readiness score) in measuring the\nmaturity/readiness levels of a country in adopting blockchain and\ncryptocurrencies. In doing so, BRI leverages on techniques from information\nretrieval to algorithmically derive an index ranking for a set of countries.\nThe index considers a range of indicators organized under five pillars:\nGovernment Regulation, Research, Technology, Industry, and User Engagement. In\nthis paper, we further extent BRI with the capability of deriving the index -\nat the country level - even in the presence of missing information for the\nindicators. In doing so, we are proposing two weighting schemes namely, linear\nand sigmoid weighting for refining the initial estimates for the indicator\nvalues. A classification framework was employed to evaluate the effectiveness\nof the developed techniques which yielded to a significant classification\naccuracy.\n", "versions": [{"version": "v1", "created": "Wed, 20 Jan 2021 16:14:33 GMT"}], "update_date": "2021-01-25", "authors_parsed": [["Iosif", "Elias", ""], ["Christodoulou", "Klitos", ""], ["Vlachos", "Andreas", ""]]}, {"id": "2101.09167", "submitter": "Sajib Saha Dr.", "authors": "Sajib Saha, Fan Gu, Xue Luo, and Robert L. Lytton", "title": "Improved Sensitivity of Base Layer on the Performance of Rigid Pavement", "comments": "45 pages, 11 figures, 6 tables. journal", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  The performance of rigid pavement is greatly affected by the properties of\nbase/subbase as well as subgrade layer. However, the performance predicted by\nthe AASHTOWare Pavement ME design shows low sensitivity to the properties of\nbase and subgrade layers. To improve the sensitivity and better reflect the\ninfluence of unbound layers a new set of improved models i.e., resilient\nmodulus (MR) and modulus of subgrade reaction (k-value) are adopted in this\nstudy. An Artificial Neural Network (ANN) model is developed to predict the\nmodified k-value based on finite element (FE) analysis. The training and\nvalidation datasets in the ANN model consist of 27000 simulation cases with\ndifferent combinations of pavement layer thickness, layer modulus and slab-base\ninterface bond ratio. To examine the sensitivity of modified MR and k-values on\npavement response, eight pavement sections data are collected from the\nLong-Term Pavement performance (LTPP) database and modeled by using the FE\nsoftware ISLAB2000. The computational results indicate that the modified MR\nvalues have higher sensitivity to water content in base layer on critical\nstress and deflection response of rigid pavements compared to the results using\nthe Pavement ME design model. It is also observed that the k-values using ANN\nmodel has the capability of predicting critical pavement response at any\npartially bonded conditions whereas the Pavement ME design model can only\ncalculate at two extreme bonding conditions (i.e., fully bonding and no\nbonding).\n", "versions": [{"version": "v1", "created": "Wed, 20 Jan 2021 23:43:41 GMT"}], "update_date": "2021-01-25", "authors_parsed": [["Saha", "Sajib", ""], ["Gu", "Fan", ""], ["Luo", "Xue", ""], ["Lytton", "Robert L.", ""]]}, {"id": "2101.09174", "submitter": "Anindya Sundar Chakrabarti", "authors": "Arnab Chakrabarti and Anindya S. Chakrabarti", "title": "Sparsistent filtering of comovement networks from high-dimensional data", "comments": "31 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Network filtering is an important form of dimension reduction to isolate the\ncore constituents of large and interconnected complex systems. We introduce a\nnew technique to filter large dimensional networks arising out of dynamical\nbehavior of the constituent nodes, exploiting their spectral properties. As\nopposed to the well known network filters that rely on preserving key\ntopological properties of the realized network, our method treats the spectrum\nas the fundamental object and preserves spectral properties. Applying\nasymptotic theory for high dimensional data for the filter, we show that it can\nbe tuned to interpolate between zero filtering to maximal filtering that\ninduces sparsity and consistency while having the least spectral distance from\na linear shrinkage estimator. We apply our proposed filter to covariance\nnetworks constructed from financial data, to extract the key subnetwork\nembedded in the full sample network.\n", "versions": [{"version": "v1", "created": "Fri, 22 Jan 2021 15:44:41 GMT"}], "update_date": "2021-01-25", "authors_parsed": [["Chakrabarti", "Arnab", ""], ["Chakrabarti", "Anindya S.", ""]]}, {"id": "2101.09178", "submitter": "Tabish Rashid", "authors": "Tabish Rashid, Cheng Zhang, Kamil Ciosek", "title": "Estimating $\\alpha$-Rank by Maximizing Information Gain", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MA cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Game theory has been increasingly applied in settings where the game is not\nknown outright, but has to be estimated by sampling. For example, meta-games\nthat arise in multi-agent evaluation can only be accessed by running a\nsuccession of expensive experiments that may involve simultaneous deployment of\nseveral agents. In this paper, we focus on $\\alpha$-rank, a popular\ngame-theoretic solution concept designed to perform well in such scenarios. We\naim to estimate the $\\alpha$-rank of the game using as few samples as possible.\nOur algorithm maximizes information gain between an epistemic belief over the\n$\\alpha$-ranks and the observed payoff. This approach has two main benefits.\nFirst, it allows us to focus our sampling on the entries that matter the most\nfor identifying the $\\alpha$-rank. Second, the Bayesian formulation provides a\nfacility to build in modeling assumptions by using a prior over game payoffs.\nWe show the benefits of using information gain as compared to the confidence\ninterval criterion of ResponseGraphUCB (Rowland et al. 2019), and provide\ntheoretical results justifying our method.\n", "versions": [{"version": "v1", "created": "Fri, 22 Jan 2021 15:46:35 GMT"}], "update_date": "2021-01-25", "authors_parsed": [["Rashid", "Tabish", ""], ["Zhang", "Cheng", ""], ["Ciosek", "Kamil", ""]]}, {"id": "2101.09184", "submitter": "Michele Nazareth da Costa", "authors": "M. Nazareth da Costa, R. Attux, A. Cichocki, J. M. T. Romano", "title": "Tensor-Train Networks for Learning Predictive Modeling of\n  Multidimensional Data", "comments": "34 pages, 16 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  In this work, we firstly apply the Train-Tensor (TT) networks to construct a\ncompact representation of the classical Multilayer Perceptron, representing a\nreduction of up to 95% of the coefficients. A comparative analysis between\ntensor model and standard multilayer neural networks is also carried out in the\ncontext of prediction of the Mackey-Glass noisy chaotic time series and NASDAQ\nindex. We show that the weights of a multidimensional regression model can be\nlearned by means of TT network and the optimization of TT weights is a more\nrobust to the impact of coefficient initialization and hyper-parameter setting.\nFurthermore, an efficient algorithm based on alternating least squares has been\nproposed for approximating the weights in TT-format with a reduction of\ncomputational calculus, providing a much faster convergence than the well-known\nadaptive learning-method algorithms, widely applied for optimizing neural\nnetworks.\n", "versions": [{"version": "v1", "created": "Fri, 22 Jan 2021 16:14:38 GMT"}, {"version": "v2", "created": "Mon, 29 Mar 2021 08:56:33 GMT"}, {"version": "v3", "created": "Tue, 30 Mar 2021 11:50:04 GMT"}], "update_date": "2021-03-31", "authors_parsed": [["da Costa", "M. Nazareth", ""], ["Attux", "R.", ""], ["Cichocki", "A.", ""], ["Romano", "J. M. T.", ""]]}, {"id": "2101.09192", "submitter": "Sadegh Pouriyan Zadeh", "authors": "Dariush Bahrami, Sadegh Pouriyan Zadeh", "title": "Gravity Optimizer: a Kinematic Approach on Optimization in Deep Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI math.OC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We introduce Gravity, another algorithm for gradient-based optimization. In\nthis paper, we explain how our novel idea change parameters to reduce the deep\nlearning model's loss. It has three intuitive hyper-parameters that the best\nvalues for them are proposed. Also, we propose an alternative to moving\naverage. To compare the performance of the Gravity optimizer with two common\noptimizers, Adam and RMSProp, five standard datasets were trained on two VGGNet\nmodels with a batch size of 128 for 100 epochs. Gravity hyper-parameters did\nnot need to be tuned for different models. As will be explained more in the\npaper, to investigate the direct impact of the optimizer itself on loss\nreduction no overfitting prevention technique was used. The obtained results\nshow that the Gravity optimizer has more stable performance than Adam and\nRMSProp and gives greater values of validation accuracy for datasets with more\noutput classes like CIFAR-100 (Fine).\n", "versions": [{"version": "v1", "created": "Fri, 22 Jan 2021 16:27:34 GMT"}], "update_date": "2021-01-25", "authors_parsed": [["Bahrami", "Dariush", ""], ["Zadeh", "Sadegh Pouriyan", ""]]}, {"id": "2101.09207", "submitter": "Fabian Otto", "authors": "Fabian Otto, Philipp Becker, Ngo Anh Vien, Hanna Carolin Ziesche, and\n  Gerhard Neumann", "title": "Differentiable Trust Region Layers for Deep Reinforcement Learning", "comments": "Accepted at ICLR 2021, camera ready version", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.IT math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Trust region methods are a popular tool in reinforcement learning as they\nyield robust policy updates in continuous and discrete action spaces. However,\nenforcing such trust regions in deep reinforcement learning is difficult.\nHence, many approaches, such as Trust Region Policy Optimization (TRPO) and\nProximal Policy Optimization (PPO), are based on approximations. Due to those\napproximations, they violate the constraints or fail to find the optimal\nsolution within the trust region. Moreover, they are difficult to implement,\noften lack sufficient exploration, and have been shown to depend on seemingly\nunrelated implementation choices. In this work, we propose differentiable\nneural network layers to enforce trust regions for deep Gaussian policies via\nclosed-form projections. Unlike existing methods, those layers formalize trust\nregions for each state individually and can complement existing reinforcement\nlearning algorithms. We derive trust region projections based on the\nKullback-Leibler divergence, the Wasserstein L2 distance, and the Frobenius\nnorm for Gaussian distributions. We empirically demonstrate that those\nprojection layers achieve similar or better results than existing methods while\nbeing almost agnostic to specific implementation choices. The code is available\nat https://git.io/Jthb0.\n", "versions": [{"version": "v1", "created": "Fri, 22 Jan 2021 16:52:06 GMT"}, {"version": "v2", "created": "Tue, 9 Mar 2021 08:44:43 GMT"}], "update_date": "2021-03-10", "authors_parsed": [["Otto", "Fabian", ""], ["Becker", "Philipp", ""], ["Vien", "Ngo Anh", ""], ["Ziesche", "Hanna Carolin", ""], ["Neumann", "Gerhard", ""]]}, {"id": "2101.09214", "submitter": "Ni Zhan", "authors": "Ni Zhan, Yijia Sun, Aman Jakhar, He Liu", "title": "Graphical Models for Financial Time Series and Portfolio Selection", "comments": "Published at ACM International Conference on AI in Finance (ICAIF\n  '20)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG q-fin.CP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We examine a variety of graphical models to construct optimal portfolios.\nGraphical models such as PCA-KMeans, autoencoders, dynamic clustering, and\nstructural learning can capture the time varying patterns in the covariance\nmatrix and allow the creation of an optimal and robust portfolio. We compared\nthe resulting portfolios from the different models with baseline methods. In\nmany cases our graphical strategies generated steadily increasing returns with\nlow risk and outgrew the S&P 500 index. This work suggests that graphical\nmodels can effectively learn the temporal dependencies in time series data and\nare proved useful in asset management.\n", "versions": [{"version": "v1", "created": "Fri, 22 Jan 2021 16:56:54 GMT"}], "update_date": "2021-01-25", "authors_parsed": [["Zhan", "Ni", ""], ["Sun", "Yijia", ""], ["Jakhar", "Aman", ""], ["Liu", "He", ""]]}, {"id": "2101.09225", "submitter": "Mehmet Dedeoglu", "authors": "Mehmet Dedeoglu, Sen Lin, Zhaofeng Zhang, Junshan Zhang", "title": "Continual Learning of Generative Models with Limited Data: From\n  Wasserstein-1 Barycenter to Adaptive Coalescence", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG eess.IV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Learning generative models is challenging for a network edge node with\nlimited data and computing power. Since tasks in similar environments share\nmodel similarity, it is plausible to leverage pre-trained generative models\nfrom the cloud or other edge nodes. Appealing to optimal transport theory\ntailored towards Wasserstein-1 generative adversarial networks (WGAN), this\nstudy aims to develop a framework which systematically optimizes continual\nlearning of generative models using local data at the edge node while\nexploiting adaptive coalescence of pre-trained generative models. Specifically,\nby treating the knowledge transfer from other nodes as Wasserstein balls\ncentered around their pre-trained models, continual learning of generative\nmodels is cast as a constrained optimization problem, which is further reduced\nto a Wasserstein-1 barycenter problem. A two-stage approach is devised\naccordingly: 1) The barycenters among the pre-trained models are computed\noffline, where displacement interpolation is used as the theoretic foundation\nfor finding adaptive barycenters via a \"recursive\" WGAN configuration; 2) the\nbarycenter computed offline is used as meta-model initialization for continual\nlearning and then fast adaptation is carried out to find the generative model\nusing the local samples at the target edge node. Finally, a weight\nternarization method, based on joint optimization of weights and threshold for\nquantization, is developed to compress the generative model further.\n", "versions": [{"version": "v1", "created": "Fri, 22 Jan 2021 17:15:39 GMT"}], "update_date": "2021-01-25", "authors_parsed": [["Dedeoglu", "Mehmet", ""], ["Lin", "Sen", ""], ["Zhang", "Zhaofeng", ""], ["Zhang", "Junshan", ""]]}, {"id": "2101.09227", "submitter": "Sriram Rao", "authors": "Sriram Rao, Ashish Mahabal, Niyanth Rao, and Cauligi Raghavendra", "title": "Nigraha: Machine-learning based pipeline to identify and evaluate planet\n  candidates from TESS", "comments": "15 pages, 18 figures, and 6 tables. Accepted for publication as a\n  full paper in Monthly Notices of the Royal Astronomical Society", "journal-ref": "journal = {Monthly Notices of the Royal Astronomical Society},\n  volume = {502}, number = {2}, pages = {2845-2858}, year = {2021}, month =\n  {01}, issn = {0035-8711},", "doi": "10.1093/mnras/stab203", "report-no": null, "categories": "astro-ph.EP astro-ph.IM cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Transiting Exoplanet Survey Satellite (TESS) has now been operational for\na little over two years, covering the Northern and the Southern hemispheres\nonce. The TESS team processes the downlinked data using the Science Processing\nOperations Center pipeline and Quick Look pipeline to generate alerts for\nfollow-up. Combined with other efforts from the community, over two thousand\nplanet candidates have been found of which tens have been confirmed as planets.\nWe present our pipeline, Nigraha, that is complementary to these approaches.\nNigraha uses a combination of transit finding, supervised machine learning, and\ndetailed vetting to identify with high confidence a few planet candidates that\nwere missed by prior searches. In particular, we identify high signal to noise\nratio (SNR) shallow transits that may represent more Earth-like planets. In the\nspirit of open data exploration we provide details of our pipeline, release our\nsupervised machine learning model and code as open source, and make public the\n38 candidates we have found in seven sectors. The model can easily be run on\nother sectors as is. As part of future work we outline ways to increase the\nyield by strengthening some of the steps where we have been conservative and\ndiscarded objects for lack of a datum or two.\n", "versions": [{"version": "v1", "created": "Fri, 22 Jan 2021 17:17:54 GMT"}, {"version": "v2", "created": "Mon, 22 Feb 2021 14:04:53 GMT"}], "update_date": "2021-02-23", "authors_parsed": [["Rao", "Sriram", ""], ["Mahabal", "Ashish", ""], ["Rao", "Niyanth", ""], ["Raghavendra", "Cauligi", ""]]}, {"id": "2101.09230", "submitter": "Ni Zhan", "authors": "Ni Zhan", "title": "Where does the Stimulus go? Deep Generative Model for Commercial Banking\n  Deposits", "comments": null, "journal-ref": "NeurIPS 2020 workshop on ML for Economic Policy", "doi": null, "report-no": null, "categories": "cs.LG econ.GN q-fin.EC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper examines deposits of individuals (\"retail\") and large companies\n(\"wholesale\") in the U.S. banking industry, and how these deposit types are\nimpacted by macroeconomic factors, such as quantitative easing (QE). Actual\ndata for deposits by holder are unavailable. We use a dataset on banks'\nfinancial information and probabilistic generative model to predict industry\nretail-wholesale deposit split from 2000 to 2020. Our model assumes account\nbalances arise from separate retail and wholesale lognormal distributions and\nfit parameters of distributions by minimizing error between actual bank metrics\nand simulated metrics using the model's generative process. We use time-series\nregression to forward predict retail-wholesale deposits as function of loans,\nretail loans, and reserve balances at Fed banks. We find increase in reserves\n(representing QE) increases wholesale but not retail deposits, and increase in\nloans increase both wholesale and retail deposits evenly. The result shows that\nQE following the 2008 financial crisis benefited large companies more than\naverage individuals, a relevant finding for economic decision making. In\naddition, this work benefits bank management strategy by providing forecasting\ncapability for retail-wholesale deposits.\n", "versions": [{"version": "v1", "created": "Fri, 22 Jan 2021 17:22:47 GMT"}], "update_date": "2021-01-25", "authors_parsed": [["Zhan", "Ni", ""]]}, {"id": "2101.09231", "submitter": "Fabio Valerio Massoli", "authors": "Donato Cafarelli, Fabio Valerio Massoli, Fabrizio Falchi, Claudio\n  Gennaro, Giuseppe Amato", "title": "Expression Recognition Analysis in the Wild", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.HC cs.LG", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Facial Expression Recognition(FER) is one of the most important topic in\nHuman-Computer interactions(HCI). In this work we report details and\nexperimental results about a facial expression recognition method based on\nstate-of-the-art methods. We fine-tuned a SeNet deep learning architecture\npre-trained on the well-known VGGFace2 dataset, on the AffWild2 facial\nexpression recognition dataset. The main goal of this work is to define a\nbaseline for a novel method we are going to propose in the near future. This\npaper is also required by the Affective Behavior Analysis in-the-wild (ABAW)\ncompetition in order to evaluate on the test set this approach. The results\nreported here are on the validation set and are related on the Expression\nChallenge part (seven basic emotion recognition) of the competition. We will\nupdate them as soon as the actual results on the test set will be published on\nthe leaderboard.\n", "versions": [{"version": "v1", "created": "Fri, 22 Jan 2021 17:28:31 GMT"}], "update_date": "2021-01-25", "authors_parsed": [["Cafarelli", "Donato", ""], ["Massoli", "Fabio Valerio", ""], ["Falchi", "Fabrizio", ""], ["Gennaro", "Claudio", ""], ["Amato", "Giuseppe", ""]]}, {"id": "2101.09253", "submitter": "Vera De Vos", "authors": "V. de Vos, K.M. Timmins, I.C. van der Schaaf, Y. Ruigrok, B.K.\n  Velthuis, H.J. Kuijf", "title": "Automatic Cerebral Vessel Extraction in TOF-MRA Using Deep Learning", "comments": "Preprint for the SPIE Medical Imaging, Image Processing Conference\n  2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Deep learning approaches may help radiologists in the early diagnosis and\ntimely treatment of cerebrovascular diseases. Accurate cerebral vessel\nsegmentation of Time-of-Flight Magnetic Resonance Angiographs (TOF-MRAs) is an\nessential step in this process. This study investigates deep learning\napproaches for automatic, fast and accurate cerebrovascular segmentation for\nTOF-MRAs. The performance of several data augmentation and selection methods\nfor training a 2D and 3D U-Net for vessel segmentation was investigated in five\nexperiments: a) without augmentation, b) Gaussian blur, c) rotation and\nflipping, d) Gaussian blur, rotation and flipping and e) different input patch\nsizes. All experiments were performed by patch-training both a 2D and 3D U-Net\nand predicted on a test set of MRAs. Ground truth was manually defined using an\ninteractive threshold and region growing method. The performance was evaluated\nusing the Dice Similarity Coefficient (DSC), Modified Hausdorff Distance and\nVolumetric Similarity, between the predicted images and the interactively\ndefined ground truth. The segmentation performance of all trained networks on\nthe test set was found to be good, with DSC scores ranging from 0.72 to 0.83.\nBoth the 2D and 3D U-Net had the best segmentation performance with Gaussian\nblur, rotation and flipping compared to other experiments without augmentation\nor only one of those augmentation techniques. Additionally, training on larger\npatches or slices gave optimal segmentation results. In conclusion, vessel\nsegmentation can be optimally performed on TOF-MRAs using a trained 3D U-Net on\nlarger patches, where data augmentation including Gaussian blur, rotation and\nflipping was performed on the training data.\n", "versions": [{"version": "v1", "created": "Fri, 22 Jan 2021 18:11:34 GMT"}], "update_date": "2021-01-25", "authors_parsed": [["de Vos", "V.", ""], ["Timmins", "K. M.", ""], ["van der Schaaf", "I. C.", ""], ["Ruigrok", "Y.", ""], ["Velthuis", "B. K.", ""], ["Kuijf", "H. J.", ""]]}, {"id": "2101.09258", "submitter": "Yang Song", "authors": "Yang Song and Conor Durkan and Iain Murray and Stefano Ermon", "title": "Maximum Likelihood Training of Score-Based Diffusion Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Score-based diffusion models synthesize samples by reversing a stochastic\nprocess that diffuses data to noise, and are trained by minimizing a weighted\ncombination of score matching losses. The log-likelihood of score-based models\ncan be tractably computed through a connection to continuous normalizing flows,\nbut log-likelihood is not directly optimized by the weighted combination of\nscore matching losses. We show that for a specific weighting scheme, the\nobjective upper bounds the negative log-likelihood, thus enabling approximate\nmaximum likelihood training of score-based models. We empirically observe that\nmaximum likelihood training consistently improves the likelihood of score-based\nmodels across multiple datasets, stochastic processes, and model architectures.\nOur best models achieve negative log-likelihoods of 2.74 and 3.76 bits/dim on\nCIFAR-10 and ImageNet 32x32, outperforming autoregressive models on these\ntasks.\n", "versions": [{"version": "v1", "created": "Fri, 22 Jan 2021 18:22:29 GMT"}, {"version": "v2", "created": "Fri, 4 Jun 2021 04:42:19 GMT"}, {"version": "v3", "created": "Wed, 23 Jun 2021 04:48:04 GMT"}], "update_date": "2021-06-24", "authors_parsed": [["Song", "Yang", ""], ["Durkan", "Conor", ""], ["Murray", "Iain", ""], ["Ermon", "Stefano", ""]]}, {"id": "2101.09279", "submitter": "Koushik Chowdhury", "authors": "Koushik Chowdhury, Mir Ahmad Iraj", "title": "Predicting Autism Spectrum Disorder Using Machine Learning Classifiers", "comments": null, "journal-ref": "2020 International Conference on Recent Trends on Electronics,\n  Information, Communication & Technology (RTEICT), Bangalore, India, 2020, pp.\n  324-327", "doi": "10.1109/RTEICT49044.2020.9315717", "report-no": "CFP20F77-ART", "categories": "cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Autism Spectrum Disorder (ASD) is on the rise and constantly growing. Earlier\nidentify of ASD with the best outcome will allow someone to be safe and healthy\nby proper nursing. Humans can hardly estimate the present condition and stage\nof ASD by measuring primary symptoms. Therefore, it is being necessary to\ndevelop a method that will provide the best outcome and measurement of ASD.\nThis paper aims to show several measurements that implemented in several\nclassifiers. Among them, Support Vector Machine (SVM) provides the best result\nand under SVM, there are also some kernels to perform. Among them, the Gaussian\nRadial Kernel gives the best result. The proposed classifier achieves 95%\naccuracy using the publicly available standard ASD dataset.\n", "versions": [{"version": "v1", "created": "Fri, 22 Jan 2021 18:58:58 GMT"}, {"version": "v2", "created": "Tue, 26 Jan 2021 21:24:38 GMT"}], "update_date": "2021-01-28", "authors_parsed": [["Chowdhury", "Koushik", ""], ["Iraj", "Mir Ahmad", ""]]}, {"id": "2101.09294", "submitter": "Eddie Yang", "authors": "Eddie Yang, Margaret E. Roberts", "title": "Censorship of Online Encyclopedias: Implications for NLP Models", "comments": "Accepted for publication at ACM FAccT 2021", "journal-ref": null, "doi": "10.1145/3442188.3445916", "report-no": null, "categories": "cs.CL cs.AI cs.CY cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  While artificial intelligence provides the backbone for many tools people use\naround the world, recent work has brought to attention that the algorithms\npowering AI are not free of politics, stereotypes, and bias. While most work in\nthis area has focused on the ways in which AI can exacerbate existing\ninequalities and discrimination, very little work has studied how governments\nactively shape training data. We describe how censorship has affected the\ndevelopment of Wikipedia corpuses, text data which are regularly used for\npre-trained inputs into NLP algorithms. We show that word embeddings trained on\nBaidu Baike, an online Chinese encyclopedia, have very different associations\nbetween adjectives and a range of concepts about democracy, freedom, collective\naction, equality, and people and historical events in China than its regularly\nblocked but uncensored counterpart - Chinese language Wikipedia. We examine the\nimplications of these discrepancies by studying their use in downstream AI\napplications. Our paper shows how government repression, censorship, and\nself-censorship may impact training data and the applications that draw from\nthem.\n", "versions": [{"version": "v1", "created": "Fri, 22 Jan 2021 19:09:53 GMT"}], "update_date": "2021-01-26", "authors_parsed": [["Yang", "Eddie", ""], ["Roberts", "Margaret E.", ""]]}, {"id": "2101.09298", "submitter": "Kaiwen Liu", "authors": "Kaiwen Liu, Nan Li, Ilya Kolmanovsky, Denise Rizzo, and Anouck Girard", "title": "Safe Learning Reference Governor for Constrained Systems with\n  Application to Fuel Truck Rollover Avoidance", "comments": "16 pages, 18 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SY cs.LG cs.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a learning reference governor (LRG) approach to enforce\nstate and control constraints in systems for which an accurate model is\nunavailable; and this approach enables the reference governor to gradually\nimprove command tracking performance through learning while enforcing the\nconstraints during learning and after learning is completed. The learning can\nbe performed either on a black-box type model of the system or directly on the\nhardware. After introducing the LRG algorithm and outlining its theoretical\nproperties, this paper investigates LRG application to fuel truck rollover\navoidance. Through simulations based on a fuel truck model that accounts for\nliquid fuel sloshing effects, we show that the proposed LRG can effectively\nprotect fuel trucks from rollover accidents under various operating conditions.\n", "versions": [{"version": "v1", "created": "Fri, 22 Jan 2021 19:13:11 GMT"}], "update_date": "2021-01-26", "authors_parsed": [["Liu", "Kaiwen", ""], ["Li", "Nan", ""], ["Kolmanovsky", "Ilya", ""], ["Rizzo", "Denise", ""], ["Girard", "Anouck", ""]]}, {"id": "2101.09300", "submitter": "Yingfang Yuan", "authors": "Yingfang Yuan and Wenjun Wang and George M. Coghill and Wei Pang", "title": "A Novel Genetic Algorithm with Hierarchical Evaluation Strategy for\n  Hyperparameter Optimisation of Graph Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Graph representation of structured data can facilitate the extraction of\nstereoscopic features, and it has demonstrated excellent ability when working\nwith deep learning systems, the so-called Graph Neural Networks (GNNs).\nChoosing a promising architecture for constructing GNNs can be transferred to a\nhyperparameter optimisation problem, a very challenging task due to the size of\nthe underlying search space and high computational cost for evaluating\ncandidate GNNs. To address this issue, this research presents a novel genetic\nalgorithm with a hierarchical evaluation strategy (HESGA), which combines the\nfull evaluation of GNNs with a fast evaluation approach. By using full\nevaluation, a GNN is represented by a set of hyperparameter values and trained\non a specified dataset, and root mean square error (RMSE) will be used to\nmeasure the quality of the GNN represented by the set of hyperparameter values\n(for regression problems). While in the proposed fast evaluation process, the\ntraining will be interrupted at an early stage, the difference of RMSE values\nbetween the starting and interrupted epochs will be used as a fast score, which\nimplies the potential of the GNN being considered. To coordinate both types of\nevaluations, the proposed hierarchical strategy uses the fast evaluation in a\nlower level for recommending candidates to a higher level, where the full\nevaluation will act as a final assessor to maintain a group of elite\nindividuals. To validate the effectiveness of HESGA, we apply it to optimise\ntwo types of deep graph neural networks. The experimental results on three\nbenchmark datasets demonstrate its advantages compared to Bayesian\nhyperparameter optimization.\n", "versions": [{"version": "v1", "created": "Fri, 22 Jan 2021 19:19:59 GMT"}, {"version": "v2", "created": "Tue, 26 Jan 2021 11:38:54 GMT"}], "update_date": "2021-01-27", "authors_parsed": [["Yuan", "Yingfang", ""], ["Wang", "Wenjun", ""], ["Coghill", "George M.", ""], ["Pang", "Wei", ""]]}, {"id": "2101.09301", "submitter": "Ting Wang", "authors": "Xinyang Zhang, Ren Pang, Shouling Ji, Fenglong Ma, Ting Wang", "title": "i-Algebra: Towards Interactive Interpretability of Deep Neural Networks", "comments": "Accepted by the 35th AAAI Conference on Artificial Intelligence (AAAI\n  '21)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Providing explanations for deep neural networks (DNNs) is essential for their\nuse in domains wherein the interpretability of decisions is a critical\nprerequisite. Despite the plethora of work on interpreting DNNs, most existing\nsolutions offer interpretability in an ad hoc, one-shot, and static manner,\nwithout accounting for the perception, understanding, or response of end-users,\nresulting in their poor usability in practice. In this paper, we argue that DNN\ninterpretability should be implemented as the interactions between users and\nmodels. We present i-Algebra, a first-of-its-kind interactive framework for\ninterpreting DNNs. At its core is a library of atomic, composable operators,\nwhich explain model behaviors at varying input granularity, during different\ninference stages, and from distinct interpretation perspectives. Leveraging a\ndeclarative query language, users are enabled to build various analysis tools\n(e.g., \"drill-down\", \"comparative\", \"what-if\" analysis) via flexibly composing\nsuch operators. We prototype i-Algebra and conduct user studies in a set of\nrepresentative analysis tasks, including inspecting adversarial inputs,\nresolving model inconsistency, and cleansing contaminated data, all\ndemonstrating its promising usability.\n", "versions": [{"version": "v1", "created": "Fri, 22 Jan 2021 19:22:57 GMT"}], "update_date": "2021-01-26", "authors_parsed": [["Zhang", "Xinyang", ""], ["Pang", "Ren", ""], ["Ji", "Shouling", ""], ["Ma", "Fenglong", ""], ["Wang", "Ting", ""]]}, {"id": "2101.09306", "submitter": "Brendon G. Anderson", "authors": "Brendon G. Anderson, Ziye Ma, Jingqi Li, Somayeh Sojoudi", "title": "Partition-Based Convex Relaxations for Certifying the Robustness of ReLU\n  Neural Networks", "comments": "This is an extension of our IEEE CDC 2020 conference paper\n  arXiv:2004.00570", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we study certifying the robustness of ReLU neural networks\nagainst adversarial input perturbations. To diminish the relaxation error\nsuffered by the popular linear programming (LP) and semidefinite programming\n(SDP) certification methods, we propose partitioning the input uncertainty set\nand solving the relaxations on each part separately. We show that this approach\nreduces relaxation error, and that the error is eliminated entirely upon\nperforming an LP relaxation with an intelligently designed partition. To scale\nthis approach to large networks, we consider courser partitions that take the\nsame form as this motivating partition. We prove that computing such a\npartition that directly minimizes the LP relaxation error is NP-hard. By\ninstead minimizing the worst-case LP relaxation error, we develop a\ncomputationally tractable scheme with a closed-form optimal two-part partition.\nWe extend the analysis to the SDP, where the feasible set geometry is exploited\nto design a two-part partition that minimizes the worst-case SDP relaxation\nerror. Experiments on IRIS classifiers demonstrate significant reduction in\nrelaxation error, offering certificates that are otherwise void without\npartitioning. By independently increasing the input size and the number of\nlayers, we empirically illustrate under which regimes the partitioned LP and\nSDP are best applied.\n", "versions": [{"version": "v1", "created": "Fri, 22 Jan 2021 19:36:40 GMT"}], "update_date": "2021-01-26", "authors_parsed": [["Anderson", "Brendon G.", ""], ["Ma", "Ziye", ""], ["Li", "Jingqi", ""], ["Sojoudi", "Somayeh", ""]]}, {"id": "2101.09313", "submitter": "James O' Neill", "authors": "James O' Neill and Danushka Bollegala", "title": "$k$-Neighbor Based Curriculum Sampling for Sequence Prediction", "comments": "arXiv admin note: substantial text overlap with arXiv:1809.05916", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Multi-step ahead prediction in language models is challenging due to the\ndiscrepancy between training and test time processes. At test time, a sequence\npredictor is required to make predictions given past predictions as the input,\ninstead of the past targets that are provided during training. This difference,\nknown as exposure bias, can lead to the compounding of errors along a generated\nsequence at test time. To improve generalization in neural language models and\naddress compounding errors, we propose \\textit{Nearest-Neighbor Replacement\nSampling} -- a curriculum learning-based method that gradually changes an\ninitially deterministic teacher policy to a stochastic policy. A token at a\ngiven time-step is replaced with a sampled nearest neighbor of the past target\nwith a truncated probability proportional to the cosine similarity between the\noriginal word and its top $k$ most similar words. This allows the learner to\nexplore alternatives when the current policy provided by the teacher is\nsub-optimal or difficult to learn from. The proposed method is straightforward,\nonline and requires little additional memory requirements. We report our\nfindings on two language modelling benchmarks and find that the proposed method\nfurther improves performance when used in conjunction with scheduled sampling.\n", "versions": [{"version": "v1", "created": "Fri, 22 Jan 2021 20:07:29 GMT"}], "update_date": "2021-01-26", "authors_parsed": [["Neill", "James O'", ""], ["Bollegala", "Danushka", ""]]}, {"id": "2101.09315", "submitter": "Borja Rodr\\'iguez G\\'alvez", "authors": "Borja Rodr\\'iguez-G\\'alvez, Germ\\'an Bassi, Ragnar Thobaben, and\n  Mikael Skoglund", "title": "Tighter expected generalization error bounds via Wasserstein distance", "comments": "22 pages: 12 of the main text, 2 of references, and 8 of appendices", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.IT cs.LG math.IT", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this work, we introduce several expected generalization error bounds based\non the Wasserstein distance. More precisely, we present full-dataset,\nsingle-letter, and random-subset bounds on both the standard setting and the\nrandomized-subsample setting from Steinke and Zakynthinou [2020]. Moreover, we\nshow that, when the loss function is bounded, these bounds recover from below\n(and thus are tighter than) current bounds based on the relative entropy and,\nfor the standard setting, generate new, non-vacuous bounds also based on the\nrelative entropy. Then, we show how similar bounds featuring the backward\nchannel can be derived with the proposed proof techniques. Finally, we show how\nvarious new bounds based on different information measures (e.g., the lautum\ninformation or several $f$-divergences) can be derived from the presented\nbounds.\n", "versions": [{"version": "v1", "created": "Fri, 22 Jan 2021 20:13:59 GMT"}], "update_date": "2021-01-26", "authors_parsed": [["Rodr\u00edguez-G\u00e1lvez", "Borja", ""], ["Bassi", "Germ\u00e1n", ""], ["Thobaben", "Ragnar", ""], ["Skoglund", "Mikael", ""]]}, {"id": "2101.09318", "submitter": "F. Patricia Medina", "authors": "F. Patricia Medina, Randy Paffenroth", "title": "Machine Learning in LiDAR 3D point clouds", "comments": "21 pages, 12 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  LiDAR point clouds contain measurements of complicated natural scenes and can\nbe used to update digital elevation models, glacial monitoring, detecting\nfaults and measuring uplift detecting, forest inventory, detect shoreline and\nbeach volume changes, landslide risk analysis, habitat mapping, and urban\ndevelopment, among others. A very important application is the classification\nof the 3D cloud into elementary classes. For example, it can be used to\ndifferentiate between vegetation, man-made structures, and water. Our goal is\nto present a preliminary comparison study for the classification of 3D point\ncloud LiDAR data that includes several types of feature engineering. In\nparticular, we demonstrate that providing context by augmenting each point in\nthe LiDAR point cloud with information about its neighboring points can improve\nthe performance of downstream learning algorithms. We also experiment with\nseveral dimension reduction strategies, ranging from Principal Component\nAnalysis (PCA) to neural network-based auto-encoders, and demonstrate how they\naffect classification performance in LiDAR point clouds. For instance, we\nobserve that combining feature engineering with a dimension reduction a method\nsuch as PCA, there is an improvement in the accuracy of the classification with\nrespect to doing a straightforward classification with the raw data.\n", "versions": [{"version": "v1", "created": "Fri, 22 Jan 2021 20:23:23 GMT"}], "update_date": "2021-01-26", "authors_parsed": [["Medina", "F. Patricia", ""], ["Paffenroth", "Randy", ""]]}, {"id": "2101.09320", "submitter": "Dominic Neu", "authors": "Dominic A. Neu, Johannes Lahann and Peter Fettke", "title": "A systematic literature review on state-of-the-art deep learning methods\n  for process prediction", "comments": "Accepted for publication with Artificial Intelligence Review (initial\n  Submission 13.07.20). Document is identical to that of the previous version", "journal-ref": null, "doi": "10.1007/s10462-021-09960-8", "report-no": null, "categories": "cs.LG", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Process mining enables the reconstruction and evaluation of business\nprocesses based on digital traces in IT systems. An increasingly important\ntechnique in this context is process prediction. Given a sequence of events of\nan ongoing trace, process prediction allows forecasting upcoming events or\nperformance measurements. In recent years, multiple process prediction\napproaches have been proposed, applying different data processing schemes and\nprediction algorithms. This study focuses on deep learning algorithms since\nthey seem to outperform their machine learning alternatives consistently.\nWhilst having a common learning algorithm, they use different data\npreprocessing techniques, implement a variety of network topologies and focus\non various goals such as outcome prediction, time prediction or control-flow\nprediction. Additionally, the set of log-data, evaluation metrics and baselines\nused by the authors diverge, making the results hard to compare. This paper\nattempts to synthesise the advantages and disadvantages of the procedural\ndecisions in these approaches by conducting a systematic literature review.\n", "versions": [{"version": "v1", "created": "Fri, 22 Jan 2021 20:26:40 GMT"}, {"version": "v2", "created": "Tue, 26 Jan 2021 11:23:08 GMT"}], "update_date": "2021-06-09", "authors_parsed": [["Neu", "Dominic A.", ""], ["Lahann", "Johannes", ""], ["Fettke", "Peter", ""]]}, {"id": "2101.09324", "submitter": "Hadi Zanddizari", "authors": "Hadi Zanddizari and J. Morris Chang", "title": "Generating Black-Box Adversarial Examples in Sparse Domain", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Applications of machine learning (ML) models and convolutional neural\nnetworks (CNNs) have been rapidly increased. Although ML models provide high\naccuracy in many applications, recent investigations show that such networks\nare highly vulnerable to adversarial attacks. The black-box adversarial attack\nis one type of attack that the attacker does not have any knowledge about the\nmodel or the training dataset. In this paper, we propose a novel approach to\ngenerate a black-box attack in sparse domain whereas the most important\ninformation of an image can be observed. Our investigation shows that large\nsparse components play a critical role in the performance of the image\nclassifiers. Under this presumption, to generate adversarial example, we\ntransfer an image into a sparse domain and put a threshold to choose only k\nlargest components. In contrast to the very recent works that randomly perturb\nk low frequency (LoF) components, we perturb k largest sparse (LaS)components\neither randomly (query-based) or in the direction of the most correlated sparse\nsignal from a different class. We show that LaS components contain some middle\nor higher frequency components information which can help us fool the\nclassifiers with a fewer number of queries. We also demonstrate the\neffectiveness of this approach by fooling the TensorFlow Lite (TFLite) model of\nGoogle Cloud Vision platform. Mean squared error (MSE) and peak signal to noise\nratio (PSNR) are used as quality metrics. We present a theoretical proof to\nconnect these metrics to the level of perturbation in the sparse domain. We\ntested our adversarial examples to the state-of-the-art CNNs and support vector\nmachine (SVM) classifiers on color and grayscale image datasets. The results\nshow the proposed method can highly increase the misclassification rate of the\nclassifiers.\n", "versions": [{"version": "v1", "created": "Fri, 22 Jan 2021 20:45:33 GMT"}], "update_date": "2021-01-26", "authors_parsed": [["Zanddizari", "Hadi", ""], ["Chang", "J. Morris", ""]]}, {"id": "2101.09336", "submitter": "Hadjer Benmeziane", "authors": "Hadjer Benmeziane, Kaoutar El Maghraoui, Hamza Ouarnoughi, Smail Niar,\n  Martin Wistuba, Naigang Wang", "title": "A Comprehensive Survey on Hardware-Aware Neural Architecture Search", "comments": "Submitted to Proceedings of IEEE", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural Architecture Search (NAS) methods have been growing in popularity.\nThese techniques have been fundamental to automate and speed up the time\nconsuming and error-prone process of synthesizing novel Deep Learning (DL)\narchitectures. NAS has been extensively studied in the past few years. Arguably\ntheir most significant impact has been in image classification and object\ndetection tasks where the state of the art results have been obtained. Despite\nthe significant success achieved to date, applying NAS to real-world problems\nstill poses significant challenges and is not widely practical. In general, the\nsynthesized Convolution Neural Network (CNN) architectures are too complex to\nbe deployed in resource-limited platforms, such as IoT, mobile, and embedded\nsystems. One solution growing in popularity is to use multi-objective\noptimization algorithms in the NAS search strategy by taking into account\nexecution latency, energy consumption, memory footprint, etc. This kind of NAS,\ncalled hardware-aware NAS (HW-NAS), makes searching the most efficient\narchitecture more complicated and opens several questions.\n  In this survey, we provide a detailed review of existing HW-NAS research and\ncategorize them according to four key dimensions: the search space, the search\nstrategy, the acceleration technique, and the hardware cost estimation\nstrategies. We further discuss the challenges and limitations of existing\napproaches and potential future directions. This is the first survey paper\nfocusing on hardware-aware NAS. We hope it serves as a valuable reference for\nthe various techniques and algorithms discussed and paves the road for future\nresearch towards hardware-aware NAS.\n", "versions": [{"version": "v1", "created": "Fri, 22 Jan 2021 21:13:46 GMT"}], "update_date": "2021-01-26", "authors_parsed": [["Benmeziane", "Hadjer", ""], ["Maghraoui", "Kaoutar El", ""], ["Ouarnoughi", "Hamza", ""], ["Niar", "Smail", ""], ["Wistuba", "Martin", ""], ["Wang", "Naigang", ""]]}, {"id": "2101.09346", "submitter": "Shixiang Chen", "authors": "Shixiang Chen, Alfredo Garcia, Mingyi Hong, Shahin Shahrampour", "title": "On the Local Linear Rate of Consensus on the Stiefel Manifold", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.LG cs.SY eess.SY", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We study the convergence properties of Riemannian gradient method for solving\nthe consensus problem (for an undirected connected graph) over the Stiefel\nmanifold. The Stiefel manifold is a non-convex set and the standard notion of\naveraging in the Euclidean space does not work for this problem. We propose\nDistributed Riemannian Consensus on Stiefel Manifold (DRCS) and prove that it\nenjoys a local linear convergence rate to global consensus. More importantly,\nthis local rate asymptotically scales with the second largest singular value of\nthe communication matrix, which is on par with the well-known rate in the\nEuclidean space. To the best of our knowledge, this is the first work showing\nthe equality of the two rates. The main technical challenges include (i)\ndeveloping a Riemannian restricted secant inequality for convergence analysis,\nand (ii) to identify the conditions (e.g., suitable step-size and\ninitialization) under which the algorithm always stays in the local region.\n", "versions": [{"version": "v1", "created": "Fri, 22 Jan 2021 21:52:38 GMT"}], "update_date": "2021-01-26", "authors_parsed": [["Chen", "Shixiang", ""], ["Garcia", "Alfredo", ""], ["Hong", "Mingyi", ""], ["Shahrampour", "Shahin", ""]]}, {"id": "2101.09354", "submitter": "M. Ali Vosoughi", "authors": "Axel Wismuller and M. Ali Vosoughi", "title": "Large-scale Augmented Granger Causality (lsAGC) for Connectivity\n  Analysis in Complex Systems: From Computer Simulations to Functional MRI\n  (fMRI)", "comments": "15 pages, conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC cs.LG eess.IV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We introduce large-scale Augmented Granger Causality (lsAGC) as a method for\nconnectivity analysis in complex systems. The lsAGC algorithm combines\ndimension reduction with source time-series augmentation and uses predictive\ntime-series modeling for estimating directed causal relationships among\ntime-series. This method is a multivariate approach, since it is capable of\nidentifying the influence of each time-series on any other time-series in the\npresence of all other time-series of the underlying dynamic system. We\nquantitatively evaluate the performance of lsAGC on synthetic directional\ntime-series networks with known ground truth. As a reference method, we compare\nour results with cross-correlation, which is typically used as a standard\nmeasure of connectivity in the functional MRI (fMRI) literature. Using\nextensive simulations for a wide range of time-series lengths and two different\nsignal-to-noise ratios of 5 and 15 dB, lsAGC consistently outperforms\ncross-correlation at accurately detecting network connections, using Receiver\nOperator Characteristic Curve (ROC) analysis, across all tested time-series\nlengths and noise levels. In addition, as an outlook to possible clinical\napplication, we perform a preliminary qualitative analysis of connectivity\nmatrices for fMRI data of Autism Spectrum Disorder (ASD) patients and typical\ncontrols, using a subset of 59 subjects of the Autism Brain Imaging Data\nExchange II (ABIDE II) data repository. Our results suggest that lsAGC, by\nextracting sparse connectivity matrices, may be useful for network analysis in\ncomplex systems, and may be applicable to clinical fMRI analysis in future\nresearch, such as targeting disease-related classification or regression tasks\non clinical data.\n", "versions": [{"version": "v1", "created": "Sun, 10 Jan 2021 01:44:48 GMT"}], "update_date": "2021-01-26", "authors_parsed": [["Wismuller", "Axel", ""], ["Vosoughi", "M. Ali", ""]]}, {"id": "2101.09356", "submitter": "Kyungsik Kim", "authors": "Ki Hong Shin, Jae Won Jung, Sung Kyu Seo, Cheol Hwan You, Dong In Lee,\n  Jisun Lee, Ki Ho Chang, Woon Seon Jung, Kyungsik Kim", "title": "Dynamical prediction of two meteorological factors using the deep neural\n  network and the long short term memory $(1)$", "comments": "33 Pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.ao-ph cond-mat.dis-nn cs.LG physics.comp-ph", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  It is important to calculate and analyze temperature and humidity prediction\naccuracies among quantitative meteorological forecasting. This study\nmanipulates the extant neural network methods to foster the predictive\naccuracy. To achieve such tasks, we analyze and explore the predictive accuracy\nand performance in the neural networks using two combined meteorological\nfactors (temperature and humidity). Simulated studies are performed by applying\nthe artificial neural network (ANN), deep neural network (DNN), extreme\nlearning machine (ELM), long short-term memory (LSTM), and long short-term\nmemory with peephole connections (LSTM-PC) machine learning methods, and the\naccurate prediction value are compared to that obtained from each other\nmethods. Data are extracted from low frequency time-series of ten metropolitan\ncities of South Korea from March 2014 to February 2020 to validate our\nobservations. To test the robustness of methods, the error of LSTM is found to\noutperform that of the other four methods in predictive accuracy. Particularly,\nas testing results, the temperature prediction of LSTM in summer in Tongyeong\nhas a root mean squared error (RMSE) value of 0.866 lower than that of other\nneural network methods, while the mean absolute percentage error (MAPE) value\nof LSTM for humidity prediction is 5.525 in summer in Mokpo, significantly\nbetter than other metropolitan cities.\n", "versions": [{"version": "v1", "created": "Sat, 16 Jan 2021 16:24:24 GMT"}], "update_date": "2021-01-26", "authors_parsed": [["Shin", "Ki Hong", ""], ["Jung", "Jae Won", ""], ["Seo", "Sung Kyu", ""], ["You", "Cheol Hwan", ""], ["Lee", "Dong In", ""], ["Lee", "Jisun", ""], ["Chang", "Ki Ho", ""], ["Jung", "Woon Seon", ""], ["Kim", "Kyungsik", ""]]}, {"id": "2101.09361", "submitter": "Shaoxing Mo", "authors": "Shaoxing Mo, Yulong Zhong, Xiaoqing Shi, Wei Feng, Xin Yin, Jichun Wu", "title": "Improving prediction of the terrestrial water storage anomalies during\n  the GRACE and GRACE-FO gap with Bayesian convolutional neural networks", "comments": "27 pages, 13 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.ao-ph cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Gravity Recovery and Climate Experiment (GRACE) satellite and its\nsuccessor GRACE Follow-On (GRACE-FO) provide valuable and accurate observations\nof terrestrial water storage anomalies (TWSAs) at a global scale. However,\nthere is an approximately one-year observation gap of TWSAs between GRACE and\nGRACE-FO. This poses a challenge for practical applications, as discontinuity\nin the TWSA observations may introduce significant biases and uncertainties in\nthe hydrological model predictions and consequently mislead decision making. To\ntackle this challenge, a Bayesian convolutional neural network (BCNN) driven by\nclimatic data is proposed in this study to bridge this gap at a global scale.\nEnhanced by integrating recent advances in deep learning, including the\nattention mechanisms and the residual and dense connections, BCNN can\nautomatically and efficiently extract important features for TWSA predictions\nfrom multi-source input data. The predicted TWSAs are compared to the\nhydrological model outputs and three recent TWSA prediction products. The\ncomparison suggests the superior performance of BCNN in providing improved\npredictions of TWSAs during the gap in particular in the relatively arid\nregions. The BCNN's ability to identify the extreme dry and wet events during\nthe gap period is further discussed and comprehensively demonstrated by\ncomparing with the precipitation anomalies, drought index, ground/surface water\nlevels. Results indicate that BCNN is capable of offering a reliable solution\nto maintain the TWSA data continuity and quantify the impacts of climate\nextremes during the gap.\n", "versions": [{"version": "v1", "created": "Thu, 21 Jan 2021 05:53:19 GMT"}, {"version": "v2", "created": "Sun, 7 Mar 2021 13:58:04 GMT"}], "update_date": "2021-03-09", "authors_parsed": [["Mo", "Shaoxing", ""], ["Zhong", "Yulong", ""], ["Shi", "Xiaoqing", ""], ["Feng", "Wei", ""], ["Yin", "Xin", ""], ["Wu", "Jichun", ""]]}, {"id": "2101.09379", "submitter": "Ulugbek Kamilov", "authors": "Jiaming Liu, Yu Sun, Weijie Gan, Xiaojian Xu, Brendt Wohlberg, and\n  Ulugbek S. Kamilov", "title": "SGD-Net: Efficient Model-Based Deep Learning with Theoretical Guarantees", "comments": null, "journal-ref": null, "doi": "10.1109/TCI.2021.3085534", "report-no": null, "categories": "eess.IV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep unfolding networks have recently gained popularity in the context of\nsolving imaging inverse problems. However, the computational and memory\ncomplexity of data-consistency layers within traditional deep unfolding\nnetworks scales with the number of measurements, limiting their applicability\nto large-scale imaging inverse problems. We propose SGD-Net as a new\nmethodology for improving the efficiency of deep unfolding through stochastic\napproximations of the data-consistency layers. Our theoretical analysis shows\nthat SGD-Net can be trained to approximate batch deep unfolding networks to an\narbitrary precision. Our numerical results on intensity diffraction tomography\nand sparse-view computed tomography show that SGD-Net can match the performance\nof the batch network at a fraction of training and testing complexity.\n", "versions": [{"version": "v1", "created": "Fri, 22 Jan 2021 23:33:11 GMT"}], "update_date": "2021-06-04", "authors_parsed": [["Liu", "Jiaming", ""], ["Sun", "Yu", ""], ["Gan", "Weijie", ""], ["Xu", "Xiaojian", ""], ["Wohlberg", "Brendt", ""], ["Kamilov", "Ulugbek S.", ""]]}, {"id": "2101.09387", "submitter": "Changhao Shi", "authors": "Changhao Shi, Chester Holtz and Gal Mishne", "title": "Online Adversarial Purification based on Self-Supervision", "comments": "Accepted to ICLR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Deep neural networks are known to be vulnerable to adversarial examples,\nwhere a perturbation in the input space leads to an amplified shift in the\nlatent network representation. In this paper, we combine canonical supervised\nlearning with self-supervised representation learning, and present\nSelf-supervised Online Adversarial Purification (SOAP), a novel defense\nstrategy that uses a self-supervised loss to purify adversarial examples at\ntest-time. Our approach leverages the label-independent nature of\nself-supervised signals and counters the adversarial perturbation with respect\nto the self-supervised tasks. SOAP yields competitive robust accuracy against\nstate-of-the-art adversarial training and purification methods, with\nconsiderably less training complexity. In addition, our approach is robust even\nwhen adversaries are given knowledge of the purification defense strategy. To\nthe best of our knowledge, our paper is the first that generalizes the idea of\nusing self-supervised signals to perform online test-time purification.\n", "versions": [{"version": "v1", "created": "Sat, 23 Jan 2021 00:19:52 GMT"}], "update_date": "2021-01-26", "authors_parsed": [["Shi", "Changhao", ""], ["Holtz", "Chester", ""], ["Mishne", "Gal", ""]]}, {"id": "2101.09391", "submitter": "Brendan Tidd", "authors": "Brendan Tidd, Nicolas Hudson, Akansel Cosgun, Jurgen Leitner", "title": "Learning Setup Policies: Reliable Transition Between Locomotion\n  Behaviours", "comments": "Submitted to Humanoids 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Dynamic platforms that operate over manyunique terrain conditions typically\nrequire multiple controllers.To transition safely between controllers, there\nmust be anoverlap of states between adjacent controllers. We developa novel\nmethod for training Setup Policies that bridge thetrajectories between\npre-trained Deep Reinforcement Learning(DRL) policies. We demonstrate our\nmethod with a simulatedbiped traversing a difficult jump terrain, where a\nsingle policyfails to learn the task, and switching between pre-trainedpolicies\nwithout Setup Policies also fails. We perform anablation of key components of\nour system, and show thatour method outperforms others that learn transition\npolicies.We demonstrate our method with several difficult and diverseterrain\ntypes, and show that we can use Setup Policies as partof a modular control\nsuite to successfully traverse a sequence ofcomplex terrains. We show that\nusing Setup Policies improvesthe success rate for traversing a single difficult\njump terrain(from 1.5%success rate without Setup Policies to 82%), and\nasequence of various terrains (from 6.5%without Setup Policiesto 29.1%).\n", "versions": [{"version": "v1", "created": "Sat, 23 Jan 2021 01:17:07 GMT"}], "update_date": "2021-01-26", "authors_parsed": [["Tidd", "Brendan", ""], ["Hudson", "Nicolas", ""], ["Cosgun", "Akansel", ""], ["Leitner", "Jurgen", ""]]}, {"id": "2101.09417", "submitter": "James Flamino", "authors": "James Flamino, Ross DeVito, Boleslaw K. Szymanski, Omar Lizardo", "title": "A Machine Learning Approach to Predicting Continuous Tie Strengths", "comments": "14 Pages, 1 Table, 2 Figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.LG physics.soc-ph", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Relationships between people constantly evolve, altering interpersonal\nbehavior and defining social groups. Relationships between nodes in social\nnetworks can be represented by a tie strength, often empirically assessed using\nsurveys. While this is effective for taking static snapshots of relationships,\nsuch methods are difficult to scale to dynamic networks. In this paper, we\npropose a system that allows for the continuous approximation of relationships\nas they evolve over time. We evaluate this system using the NetSense study,\nwhich provides comprehensive communication records of students at the\nUniversity of Notre Dame over the course of four years. These records are\ncomplemented by semesterly ego network surveys, which provide discrete samples\nover time of each participant's true social tie strength with others. We\ndevelop a pair of powerful machine learning models (complemented by a suite of\nbaselines extracted from past works) that learn from these surveys to interpret\nthe communications records as signals. These signals represent dynamic tie\nstrengths, accurately recording the evolution of relationships between the\nindividuals in our social networks. With these evolving tie values, we are able\nto make several empirically derived observations which we compare to past\nworks.\n", "versions": [{"version": "v1", "created": "Sat, 23 Jan 2021 05:01:05 GMT"}], "update_date": "2021-01-26", "authors_parsed": [["Flamino", "James", ""], ["DeVito", "Ross", ""], ["Szymanski", "Boleslaw K.", ""], ["Lizardo", "Omar", ""]]}, {"id": "2101.09421", "submitter": "Gita Sukthankar", "authors": "Ayesha Enayet and Gita Sukthankar", "title": "Analyzing Team Performance with Embeddings from Multiparty Dialogues", "comments": "To be published in the 15th IEEE International Conference on Semantic\n  Computing", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Good communication is indubitably the foundation of effective teamwork. Over\ntime teams develop their own communication styles and often exhibit\nentrainment, a conversational phenomena in which humans synchronize their\nlinguistic choices. This paper examines the problem of predicting team\nperformance from embeddings learned from multiparty dialogues such that teams\nwith similar conflict scores lie close to one another in vector space.\nEmbeddings were extracted from three types of features: 1) dialogue acts 2)\nsentiment polarity 3) syntactic entrainment. Although all of these features can\nbe used to effectively predict team performance, their utility varies by the\nteamwork phase. We separate the dialogues of players playing a cooperative game\ninto stages: 1) early (knowledge building) 2) middle (problem-solving) and 3)\nlate (culmination). Unlike syntactic entrainment, both dialogue act and\nsentiment embeddings are effective for classifying team performance, even\nduring the initial phase. This finding has potential ramifications for the\ndevelopment of conversational agents that facilitate teaming.\n", "versions": [{"version": "v1", "created": "Sat, 23 Jan 2021 05:18:12 GMT"}], "update_date": "2021-01-26", "authors_parsed": [["Enayet", "Ayesha", ""], ["Sukthankar", "Gita", ""]]}, {"id": "2101.09428", "submitter": "WenJie Song", "authors": "Song WenJie, Shen Xuan", "title": "Vertical federated learning based on DFP and BFGS", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  As data privacy is gradually valued by people, federated learning(FL) has\nemerged because of its potential to protect data. FL uses homomorphic\nencryption and differential privacy encryption on the promise of ensuring data\nsecurity to realize distributed machine learning by exchanging encrypted\ninformation between different data providers. However, there are still many\nproblems in FL, such as the communication efficiency between the client and the\nserver and the data is non-iid. In order to solve the two problems mentioned\nabove, we propose a novel vertical federated learning framework based on the\nDFP and the BFGS(denoted as BDFL), then apply it to logistic regression.\nFinally, we perform experiments using real datasets to test efficiency of BDFL\nframework.\n", "versions": [{"version": "v1", "created": "Sat, 23 Jan 2021 06:15:04 GMT"}], "update_date": "2021-01-26", "authors_parsed": [["WenJie", "Song", ""], ["Xuan", "Shen", ""]]}, {"id": "2101.09429", "submitter": "Sheikh Rabiul Islam", "authors": "Sheikh Rabiul Islam, William Eberle, Sheikh Khaled Ghafoor, Mohiuddin\n  Ahmed", "title": "Explainable Artificial Intelligence Approaches: A Survey", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The lack of explainability of a decision from an Artificial Intelligence (AI)\nbased \"black box\" system/model, despite its superiority in many real-world\napplications, is a key stumbling block for adopting AI in many high stakes\napplications of different domain or industry. While many popular Explainable\nArtificial Intelligence (XAI) methods or approaches are available to facilitate\na human-friendly explanation of the decision, each has its own merits and\ndemerits, with a plethora of open challenges. We demonstrate popular XAI\nmethods with a mutual case study/task (i.e., credit default prediction),\nanalyze for competitive advantages from multiple perspectives (e.g., local,\nglobal), provide meaningful insight on quantifying explainability, and\nrecommend paths towards responsible or human-centered AI using XAI as a medium.\nPractitioners can use this work as a catalog to understand, compare, and\ncorrelate competitive advantages of popular XAI methods. In addition, this\nsurvey elicits future research directions towards responsible or human-centric\nAI systems, which is crucial to adopt AI in high stakes applications.\n", "versions": [{"version": "v1", "created": "Sat, 23 Jan 2021 06:15:34 GMT"}], "update_date": "2021-01-26", "authors_parsed": [["Islam", "Sheikh Rabiul", ""], ["Eberle", "William", ""], ["Ghafoor", "Sheikh Khaled", ""], ["Ahmed", "Mohiuddin", ""]]}, {"id": "2101.09436", "submitter": "Xudong Sun", "authors": "Xudong Sun, Florian Buettner", "title": "Hierarchical Variational Auto-Encoding for Unsupervised Domain\n  Generalization", "comments": "Presented at ICLR 2021 RobustML Workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We address the task of domain generalization, where the goal is to train a\npredictive model such that it is able to generalize to a new, previously unseen\ndomain. We choose a hierarchical generative approach within the framework of\nvariational autoencoders and propose a domain-unsupervised algorithm that is\nable to generalize to new domains without domain supervision. We show that our\nmethod is able to learn representations that disentangle domain-specific\ninformation from class-label specific information even in complex settings\nwhere domain structure is not observed during training. Our interpretable\nmethod outperforms previously proposed generative algorithms for domain\ngeneralization as well as other non-generative state-of-the-art approaches in\nseveral hierarchical domain settings including sequential overlapped near\ncontinuous domain shift. It also achieves competitive performance on the\nstandard domain generalization benchmark dataset PACS compared to\nstate-of-the-art approaches which rely on observing domain-specific information\nduring training, as well as another domain unsupervised method. Additionally,\nwe proposed model selection purely based on Evidence Lower Bound (ELBO) and\nalso proposed weak domain supervision where implicit domain information can be\nadded into the algorithm.\n", "versions": [{"version": "v1", "created": "Sat, 23 Jan 2021 07:09:59 GMT"}, {"version": "v2", "created": "Mon, 22 Feb 2021 19:00:48 GMT"}, {"version": "v3", "created": "Sat, 27 Feb 2021 13:35:03 GMT"}, {"version": "v4", "created": "Thu, 6 May 2021 17:36:04 GMT"}, {"version": "v5", "created": "Fri, 14 May 2021 20:51:15 GMT"}], "update_date": "2021-05-18", "authors_parsed": [["Sun", "Xudong", ""], ["Buettner", "Florian", ""]]}, {"id": "2101.09438", "submitter": "Dheeraj Baby", "authors": "Dheeraj Baby and Xuandong Zhao and Yu-Xiang Wang", "title": "An Optimal Reduction of TV-Denoising to Adaptive Online Learning", "comments": "To appear at AISTATS 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.OC stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  We consider the problem of estimating a function from $n$ noisy samples whose\ndiscrete Total Variation (TV) is bounded by $C_n$. We reveal a deep connection\nto the seemingly disparate problem of Strongly Adaptive online learning\n(Daniely et al, 2015) and provide an $O(n \\log n)$ time algorithm that attains\nthe near minimax optimal rate of $\\tilde O (n^{1/3}C_n^{2/3})$ under squared\nerror loss. The resulting algorithm runs online and optimally adapts to the\nunknown smoothness parameter $C_n$. This leads to a new and more versatile\nalternative to wavelets-based methods for (1) adaptively estimating TV bounded\nfunctions; (2) online forecasting of TV bounded trends in time series.\n", "versions": [{"version": "v1", "created": "Sat, 23 Jan 2021 07:13:53 GMT"}, {"version": "v2", "created": "Tue, 26 Jan 2021 06:55:03 GMT"}], "update_date": "2021-01-27", "authors_parsed": [["Baby", "Dheeraj", ""], ["Zhao", "Xuandong", ""], ["Wang", "Yu-Xiang", ""]]}, {"id": "2101.09446", "submitter": "Yunzhen Yao", "authors": "Yunzhen Yao, Liangzu Peng and Manolis C. Tsakiris", "title": "Unlabeled Principal Component Analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of principal component analysis from a data matrix\nwhere the entries of each column have undergone some unknown permutation,\ntermed Unlabeled Principal Component Analysis (UPCA). Using algebraic geometry,\nwe establish that for generic enough data, and up to a permutation of the\ncoordinates of the ambient space, there is a unique subspace of minimal\ndimension that explains the data. We show that a permutation-invariant system\nof polynomial equations has finitely many solutions, with each solution\ncorresponding to a row permutation of the ground-truth data matrix. Allowing\nfor missing entries on top of permutations leads to the problem of unlabeled\nmatrix completion, for which we give theoretical results of similar flavor. We\nalso propose a two-stage algorithmic pipeline for UPCA suitable for the\npractically relevant case where only a fraction of the data has been permuted.\nStage-I of this pipeline employs robust-PCA methods to estimate the\nground-truth column-space. Equipped with the column-space, stage-II applies\nmethods for linear regression without correspondences to restore the permuted\ndata. A computational study reveals encouraging findings, including the ability\nof UPCA to handle face images from the Extended Yale-B database with\narbitrarily permuted patches of arbitrary size in $0.3$ seconds on a standard\ndesktop computer.\n", "versions": [{"version": "v1", "created": "Sat, 23 Jan 2021 07:34:48 GMT"}], "update_date": "2021-01-26", "authors_parsed": [["Yao", "Yunzhen", ""], ["Peng", "Liangzu", ""], ["Tsakiris", "Manolis C.", ""]]}, {"id": "2101.09451", "submitter": "Shao-Yuan Lo", "authors": "Shao-Yuan Lo and Vishal M. Patel", "title": "Error Diffusion Halftoning Against Adversarial Examples", "comments": "Accepted at IEEE International Conference on Image Processing (ICIP)\n  2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CR cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Adversarial examples contain carefully crafted perturbations that can fool\ndeep neural networks (DNNs) into making wrong predictions. Enhancing the\nadversarial robustness of DNNs has gained considerable interest in recent\nyears. Although image transformation-based defenses were widely considered at\nan earlier time, most of them have been defeated by adaptive attacks. In this\npaper, we propose a new image transformation defense based on error diffusion\nhalftoning, and combine it with adversarial training to defend against\nadversarial examples. Error diffusion halftoning projects an image into a 1-bit\nspace and diffuses quantization error to neighboring pixels. This process can\nremove adversarial perturbations from a given image while maintaining\nacceptable image quality in the meantime in favor of recognition. Experimental\nresults demonstrate that the proposed method is able to improve adversarial\nrobustness even under advanced adaptive attacks, while most of the other image\ntransformation-based defenses do not. We show that a proper image\ntransformation can still be an effective defense approach. Code:\nhttps://github.com/shaoyuanlo/Halftoning-Defense\n", "versions": [{"version": "v1", "created": "Sat, 23 Jan 2021 07:55:02 GMT"}, {"version": "v2", "created": "Mon, 14 Jun 2021 23:03:02 GMT"}, {"version": "v3", "created": "Sat, 24 Jul 2021 06:59:58 GMT"}], "update_date": "2021-07-27", "authors_parsed": [["Lo", "Shao-Yuan", ""], ["Patel", "Vishal M.", ""]]}, {"id": "2101.09453", "submitter": "Linxing Jiang", "authors": "Linxing Preston Jiang, Luciano de la Iglesia", "title": "Improved Training of Sparse Coding Variational Autoencoder via Weight\n  Normalization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning a generative model of visual information with sparse and\ncompositional features has been a challenge for both theoretical neuroscience\nand machine learning communities. Sparse coding models have achieved great\nsuccess in explaining the receptive fields of mammalian primary visual cortex\nwith sparsely activated latent representation. In this paper, we focus on a\nrecently proposed model, sparse coding variational autoencoder (SVAE) (Barello\net al., 2018), and show that the end-to-end training scheme of SVAE leads to a\nlarge group of decoding filters not fully optimized with noise-like receptive\nfields. We propose a few heuristics to improve the training of SVAE and show\nthat a unit $L_2$ norm constraint on the decoder is critical to produce sparse\ncoding filters. Such normalization can be considered as local lateral\ninhibition in the cortex. We verify this claim empirically on both natural\nimage patches and MNIST dataset and show that projection of the filters onto\nunit norm drastically increases the number of active filters. Our results\nhighlight the importance of weight normalization for learning sparse\nrepresentation from data and suggest a new way of reducing the number of\ninactive latent components in VAE learning.\n", "versions": [{"version": "v1", "created": "Sat, 23 Jan 2021 08:07:20 GMT"}], "update_date": "2021-01-26", "authors_parsed": [["Jiang", "Linxing Preston", ""], ["de la Iglesia", "Luciano", ""]]}, {"id": "2101.09458", "submitter": "William Whitney", "authors": "William F. Whitney, Michael Bloesch, Jost Tobias Springenberg, Abbas\n  Abdolmaleki, Kyunghyun Cho, Martin Riedmiller", "title": "Decoupled Exploration and Exploitation Policies for Sample-Efficient\n  Reinforcement Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite the close connection between exploration and sample efficiency, most\nstate of the art reinforcement learning algorithms include no considerations\nfor exploration beyond maximizing the entropy of the policy. In this work we\naddress this seeming missed opportunity. We observe that the most common\nformulation of directed exploration in deep RL, known as bonus-based\nexploration (BBE), suffers from bias and slow coverage in the few-sample\nregime. This causes BBE to be actively detrimental to policy learning in many\ncontrol tasks. We show that by decoupling the task policy from the exploration\npolicy, directed exploration can be highly effective for sample-efficient\ncontinuous control. Our method, Decoupled Exploration and Exploitation Policies\n(DEEP), can be combined with any off-policy RL algorithm without modification.\nWhen used in conjunction with soft actor-critic, DEEP incurs no performance\npenalty in densely-rewarding environments. On sparse environments, DEEP gives a\nseveral-fold improvement in data efficiency due to better exploration.\n", "versions": [{"version": "v1", "created": "Sat, 23 Jan 2021 08:51:04 GMT"}, {"version": "v2", "created": "Thu, 1 Jul 2021 16:03:55 GMT"}], "update_date": "2021-07-02", "authors_parsed": [["Whitney", "William F.", ""], ["Bloesch", "Michael", ""], ["Springenberg", "Jost Tobias", ""], ["Abdolmaleki", "Abbas", ""], ["Cho", "Kyunghyun", ""], ["Riedmiller", "Martin", ""]]}, {"id": "2101.09460", "submitter": "Sodiq Adewole", "authors": "Sali Rasoul, Sodiq Adewole, Alphonse Akakpo", "title": "Feature Selection Using Reinforcement Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the decreasing cost of data collection, the space of variables or\nfeatures that can be used to characterize a particular predictor of interest\ncontinues to grow exponentially. Therefore, identifying the most characterizing\nfeatures that minimizes the variance without jeopardizing the bias of our\nmodels is critical to successfully training a machine learning model. In\naddition, identifying such features is critical for interpretability,\nprediction accuracy and optimal computation cost. While statistical methods\nsuch as subset selection, shrinkage, dimensionality reduction have been applied\nin selecting the best set of features, some other approaches in literature have\napproached feature selection task as a search problem where each state in the\nsearch space is a possible feature subset. In this paper, we solved the feature\nselection problem using Reinforcement Learning. Formulating the state space as\na Markov Decision Process (MDP), we used Temporal Difference (TD) algorithm to\nselect the best subset of features. Each state was evaluated using a robust and\nlow cost classifier algorithm which could handle any non-linearities in the\ndataset.\n", "versions": [{"version": "v1", "created": "Sat, 23 Jan 2021 09:24:37 GMT"}], "update_date": "2021-01-26", "authors_parsed": [["Rasoul", "Sali", ""], ["Adewole", "Sodiq", ""], ["Akakpo", "Alphonse", ""]]}, {"id": "2101.09486", "submitter": "Siyuan Chen", "authors": "Siyuan Chen and Jiahai Wang and Guoqing Li", "title": "Neural Relational Inference with Efficient Message Passing Mechanisms", "comments": "Accepted by AAAI 2021, 13 pages, 9 figures, 4 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Many complex processes can be viewed as dynamical systems of interacting\nagents. In many cases, only the state sequences of individual agents are\nobserved, while the interacting relations and the dynamical rules are unknown.\nThe neural relational inference (NRI) model adopts graph neural networks that\npass messages over a latent graph to jointly learn the relations and the\ndynamics based on the observed data. However, NRI infers the relations\nindependently and suffers from error accumulation in multi-step prediction at\ndynamics learning procedure. Besides, relation reconstruction without prior\nknowledge becomes more difficult in more complex systems. This paper introduces\nefficient message passing mechanisms to the graph neural networks with\nstructural prior knowledge to address these problems. A relation interaction\nmechanism is proposed to capture the coexistence of all relations, and a\nspatio-temporal message passing mechanism is proposed to use historical\ninformation to alleviate error accumulation. Additionally, the structural prior\nknowledge, symmetry as a special case, is introduced for better relation\nprediction in more complex systems. The experimental results on simulated\nphysics systems show that the proposed method outperforms existing\nstate-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Sat, 23 Jan 2021 11:27:31 GMT"}], "update_date": "2021-01-26", "authors_parsed": [["Chen", "Siyuan", ""], ["Wang", "Jiahai", ""], ["Li", "Guoqing", ""]]}, {"id": "2101.09492", "submitter": "Xuecan Yang", "authors": "Xuecan Yang, Sumanta Chaudhuri, Laurence Likforman, Lirida Naviner", "title": "MinConvNets: A new class of multiplication-less Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolutional Neural Networks have achieved unprecedented success in image\nclassification, recognition, or detection applications. However, their\nlarge-scale deployment in embedded devices is still limited by the huge\ncomputational requirements, i.e., millions of MAC operations per layer. In this\narticle, MinConvNets where the multiplications in the forward propagation are\napproximated by minimum comparator operations are introduced. Hardware\nimplementation of minimum operation is much simpler than multipliers. Firstly,\na methodology to find approximate operations based on statistical correlation\nis presented. We show that it is possible to replace multipliers by minimum\noperations in the forward propagation under certain constraints, i.e. given\nsimilar mean and variances of the feature and the weight vectors. A modified\ntraining method which guarantees the above constraints is proposed. And it is\nshown that equivalent precision can be achieved during inference with\nMinConvNets by using transfer learning from well trained exact CNNs.\n", "versions": [{"version": "v1", "created": "Sat, 23 Jan 2021 12:18:52 GMT"}], "update_date": "2021-01-26", "authors_parsed": [["Yang", "Xuecan", ""], ["Chaudhuri", "Sumanta", ""], ["Likforman", "Laurence", ""], ["Naviner", "Lirida", ""]]}, {"id": "2101.09498", "submitter": "Danding Wang", "authors": "Danding Wang, Wencan Zhang and Brian Y. Lim", "title": "Show or Suppress? Managing Input Uncertainty in Machine Learning Model\n  Explanations", "comments": "to be published in Artificial Intelligence Special Issue on\n  Explainable Artificial Intelligence", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Feature attribution is widely used in interpretable machine learning to\nexplain how influential each measured input feature value is for an output\ninference. However, measurements can be uncertain, and it is unclear how the\nawareness of input uncertainty can affect the trust in explanations. We propose\nand study two approaches to help users to manage their perception of\nuncertainty in a model explanation: 1) transparently show uncertainty in\nfeature attributions to allow users to reflect on, and 2) suppress attribution\nto features with uncertain measurements and shift attribution to other features\nby regularizing with an uncertainty penalty. Through simulation experiments,\nqualitative interviews, and quantitative user evaluations, we identified the\nbenefits of moderately suppressing attribution uncertainty, and concerns\nregarding showing attribution uncertainty. This work adds to the understanding\nof handling and communicating uncertainty for model interpretability.\n", "versions": [{"version": "v1", "created": "Sat, 23 Jan 2021 13:10:48 GMT"}], "update_date": "2021-01-26", "authors_parsed": [["Wang", "Danding", ""], ["Zhang", "Wencan", ""], ["Lim", "Brian Y.", ""]]}, {"id": "2101.09500", "submitter": "Mark Zolotas", "authors": "Mark Zolotas, Yiannis Demiris", "title": "Disentangled Sequence Clustering for Human Intention Inference", "comments": "21 pages, 10 figures, submitted to Robotics and Autonomous Systems", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.LG", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Equipping robots with the ability to infer human intent is a vital\nprecondition for effective collaboration. Most computational approaches towards\nthis objective employ probabilistic reasoning to recover a distribution of\n\"intent\" conditioned on the robot's perceived sensory state. However, these\napproaches typically assume task-specific notions of human intent (e.g.\nlabelled goals) are known a priori. To overcome this constraint, we propose the\nDisentangled Sequence Clustering Variational Autoencoder (DiSCVAE), a\nclustering framework that can be used to learn such a distribution of intent in\nan unsupervised manner. The DiSCVAE leverages recent advances in unsupervised\nlearning to derive a disentangled latent representation of sequential data,\nseparating time-varying local features from time-invariant global aspects.\nThough unlike previous frameworks for disentanglement, the proposed variant\nalso infers a discrete variable to form a latent mixture model and enable\nclustering of global sequence concepts, e.g. intentions from observed human\nbehaviour. To evaluate the DiSCVAE, we first validate its capacity to discover\nclasses from unlabelled sequences using video datasets of bouncing digits and\n2D animations. We then report results from a real-world human-robot interaction\nexperiment conducted on a robotic wheelchair. Our findings glean insights into\nhow the inferred discrete variable coincides with human intent and thus serves\nto improve assistance in collaborative settings, such as shared control.\n", "versions": [{"version": "v1", "created": "Sat, 23 Jan 2021 13:39:34 GMT"}], "update_date": "2021-01-26", "authors_parsed": [["Zolotas", "Mark", ""], ["Demiris", "Yiannis", ""]]}, {"id": "2101.09505", "submitter": "Youngmin Kim", "authors": "Youngmin Kim, Richard Allmendinger and Manuel L\\'opez-Ib\\'a\\~nez", "title": "Safe Learning and Optimization Techniques: Towards a Survey of the State\n  of the Art", "comments": "The final authenticated publication was made In: Heintz F., Milano\n  M., O'Sullivan B. (eds) Trustworthy AI - Integrating Learning, Optimization\n  and Reasoning. TAILOR 2020. Lecture Notes in Computer Science, vol 12641.\n  Springer, Cham. The final authenticated publication is available online at\n  \\<https://doi.org/10.1007/978-3-030-73959-1_12>", "journal-ref": null, "doi": "10.1007/978-3-030-73959-1_12", "report-no": null, "categories": "cs.LG cs.NE math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Safe learning and optimization deals with learning and optimization problems\nthat avoid, as much as possible, the evaluation of non-safe input points, which\nare solutions, policies, or strategies that cause an irrecoverable loss (e.g.,\nbreakage of a machine or equipment, or life threat). Although a comprehensive\nsurvey of safe reinforcement learning algorithms was published in 2015, a\nnumber of new algorithms have been proposed thereafter, and related works in\nactive learning and in optimization were not considered. This paper reviews\nthose algorithms from a number of domains including reinforcement learning,\nGaussian process regression and classification, evolutionary algorithms, and\nactive learning. We provide the fundamental concepts on which the reviewed\nalgorithms are based and a characterization of the individual algorithms. We\nconclude by explaining how the algorithms are connected and suggestions for\nfuture research.\n", "versions": [{"version": "v1", "created": "Sat, 23 Jan 2021 13:58:09 GMT"}, {"version": "v2", "created": "Thu, 18 Feb 2021 13:38:59 GMT"}, {"version": "v3", "created": "Wed, 23 Jun 2021 18:19:08 GMT"}], "update_date": "2021-06-25", "authors_parsed": [["Kim", "Youngmin", ""], ["Allmendinger", "Richard", ""], ["L\u00f3pez-Ib\u00e1\u00f1ez", "Manuel", ""]]}, {"id": "2101.09509", "submitter": "Donlapark Ponnoprat", "authors": "Donlapark Ponnoprat", "title": "Short-term daily precipitation forecasting with seasonally-integrated\n  autoencoder", "comments": "35 pages, 13 figures", "journal-ref": "Applied Soft Computing, 102, 107083 (2021)", "doi": "10.1016/j.asoc.2021.107083", "report-no": null, "categories": "cs.LG", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Short-term precipitation forecasting is essential for planning of human\nactivities in multiple scales, ranging from individuals' planning, urban\nmanagement to flood prevention. Yet the short-term atmospheric dynamics are\nhighly nonlinear that it cannot be easily captured with classical time series\nmodels. On the other hand, deep learning models are good at learning nonlinear\ninteractions, but they are not designed to deal with the seasonality in time\nseries. In this study, we aim to develop a forecasting model that can both\nhandle the nonlinearities and detect the seasonality hidden within the daily\nprecipitation data. To this end, we propose a seasonally-integrated autoencoder\n(SSAE) consisting of two long short-term memory (LSTM) autoencoders: one for\nlearning short-term dynamics, and the other for learning the seasonality in the\ntime series. Our experimental results show that not only does the SSAE\noutperform various time series models regardless of the climate type, but it\nalso has low output variance compared to other deep learning models. The\nresults also show that the seasonal component of the SSAE helped improve the\ncorrelation between the forecast and the actual values from 4% at horizon 1 to\n37% at horizon 3.\n", "versions": [{"version": "v1", "created": "Sat, 23 Jan 2021 14:19:56 GMT"}], "update_date": "2021-01-26", "authors_parsed": [["Ponnoprat", "Donlapark", ""]]}, {"id": "2101.09512", "submitter": "Karthigan Sinnathamby", "authors": "Karthigan Sinnathamby, Chang-Yu Hou, Lalitha Venkataramanan,\n  Vasileios-Marios Gkortsas, Fran\\c{c}ois Fleuret", "title": "Unsupervised clustering of series using dynamic programming", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We are interested in clustering parts of a given single multi-variate series\nin an unsupervised manner. We would like to segment and cluster the series such\nthat the resulting blocks present in each cluster are coherent with respect to\na known model (e.g. physics model). Data points are said to be coherent if they\ncan be described using this model with the same parameters. We have designed an\nalgorithm based on dynamic programming with constraints on the number of\nclusters, the number of transitions as well as the minimal size of a block such\nthat the clusters are coherent with this process. We present an use-case:\nclustering of petrophysical series using the Waxman-Smits equation.\n", "versions": [{"version": "v1", "created": "Sat, 23 Jan 2021 14:35:35 GMT"}], "update_date": "2021-01-26", "authors_parsed": [["Sinnathamby", "Karthigan", ""], ["Hou", "Chang-Yu", ""], ["Venkataramanan", "Lalitha", ""], ["Gkortsas", "Vasileios-Marios", ""], ["Fleuret", "Fran\u00e7ois", ""]]}, {"id": "2101.09536", "submitter": "James Smith", "authors": "James Smith, Jonathan Balloch, Yen-Chang Hsu, Zsolt Kira", "title": "Memory-Efficient Semi-Supervised Continual Learning: The World is its\n  Own Replay Buffer", "comments": "Accepted by the 2021 International Joint Conference on Neural\n  Networks (IJCNN 2021)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Rehearsal is a critical component for class-incremental continual learning,\nyet it requires a substantial memory budget. Our work investigates whether we\ncan significantly reduce this memory budget by leveraging unlabeled data from\nan agent's environment in a realistic and challenging continual learning\nparadigm. Specifically, we explore and formalize a novel semi-supervised\ncontinual learning (SSCL) setting, where labeled data is scarce yet non-i.i.d.\nunlabeled data from the agent's environment is plentiful. Importantly, data\ndistributions in the SSCL setting are realistic and therefore reflect object\nclass correlations between, and among, the labeled and unlabeled data\ndistributions. We show that a strategy built on pseudo-labeling, consistency\nregularization, Out-of-Distribution (OoD) detection, and knowledge distillation\nreduces forgetting in this setting. Our approach, DistillMatch, increases\nperformance over the state-of-the-art by no less than 8.7% average task\naccuracy and up to 54.5% average task accuracy in SSCL CIFAR-100 experiments.\nMoreover, we demonstrate that DistillMatch can save up to 0.23 stored images\nper processed unlabeled image compared to the next best method which only saves\n0.08. Our results suggest that focusing on realistic correlated distributions\nis a significantly new perspective, which accentuates the importance of\nleveraging the world's structure as a continual learning strategy.\n", "versions": [{"version": "v1", "created": "Sat, 23 Jan 2021 17:23:08 GMT"}, {"version": "v2", "created": "Thu, 6 May 2021 17:55:20 GMT"}], "update_date": "2021-05-07", "authors_parsed": [["Smith", "James", ""], ["Balloch", "Jonathan", ""], ["Hsu", "Yen-Chang", ""], ["Kira", "Zsolt", ""]]}, {"id": "2101.09545", "submitter": "Damien Scieur", "authors": "Alexandre d'Aspremont, Damien Scieur and Adrien Taylor", "title": "Acceleration Methods", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.LG cs.NA math.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This monograph covers some recent advances on a range of acceleration\ntechniques frequently used in convex optimization. We first use quadratic\noptimization problems to introduce two key families of methods, momentum and\nnested optimization schemes, which coincide in the quadratic case to form the\nChebyshev method whose complexity is analyzed using Chebyshev polynomials.\n  We discuss momentum methods in detail, starting with the seminal work of\nNesterov (1983) and structure convergence proofs using a few master templates,\nsuch as that of \\emph{optimized gradient methods} which have the key benefit of\nshowing how momentum methods maximize convergence rates.\n  We further cover proximal acceleration techniques, at the heart of the\n\\emph{Catalyst} and \\emph{Accelerated Hybrid Proximal Extragradient}\nframeworks, using similar algorithmic patterns.\n  Common acceleration techniques directly rely on the knowledge of some\nregularity parameters of the problem at hand, and we conclude by discussing\n\\emph{restart} schemes, a set of simple techniques to reach nearly optimal\nconvergence rates while adapting to unobserved regularity parameters.\n", "versions": [{"version": "v1", "created": "Sat, 23 Jan 2021 17:58:25 GMT"}, {"version": "v2", "created": "Thu, 11 Mar 2021 18:30:36 GMT"}], "update_date": "2021-03-12", "authors_parsed": [["d'Aspremont", "Alexandre", ""], ["Scieur", "Damien", ""], ["Taylor", "Adrien", ""]]}, {"id": "2101.09553", "submitter": "Kevin Black", "authors": "Kevin Black, Shrivu Shankar, Daniel Fonseka, Jacob Deutsch, Abhimanyu\n  Dhir, and Maruthi R. Akella", "title": "Real-Time, Flight-Ready, Non-Cooperative Spacecraft Pose Estimation\n  Using Monocular Imagery", "comments": "Presented at the 31st AAS/AIAA Space Flight Mechanics Meeting,\n  February 2021. 16 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": "AAS 21-283", "categories": "cs.CV cs.LG cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A key requirement for autonomous on-orbit proximity operations is the\nestimation of a target spacecraft's relative pose (position and orientation).\nIt is desirable to employ monocular cameras for this problem due to their low\ncost, weight, and power requirements. This work presents a novel convolutional\nneural network (CNN)-based monocular pose estimation system that achieves\nstate-of-the-art accuracy with low computational demand. In combination with a\nBlender-based synthetic data generation scheme, the system demonstrates the\nability to generalize from purely synthetic training data to real in-space\nimagery of the Northrop Grumman Enhanced Cygnus spacecraft. Additionally, the\nsystem achieves real-time performance on low-power flight-like hardware.\n", "versions": [{"version": "v1", "created": "Sat, 23 Jan 2021 18:40:08 GMT"}], "update_date": "2021-01-26", "authors_parsed": [["Black", "Kevin", ""], ["Shankar", "Shrivu", ""], ["Fonseka", "Daniel", ""], ["Deutsch", "Jacob", ""], ["Dhir", "Abhimanyu", ""], ["Akella", "Maruthi R.", ""]]}, {"id": "2101.09569", "submitter": "Mokshith Voodarla", "authors": "Mokshith Voodarla, Shubham Shrivastava, Sagar Manglani, Ankit Vora,\n  Siddharth Agarwal, Punarjay Chakravarty", "title": "S-BEV: Semantic Birds-Eye View Representation for Weather and Lighting\n  Invariant 3-DoF Localization", "comments": "7 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe a light-weight, weather and lighting invariant, Semantic Bird's\nEye View (S-BEV) signature for vision-based vehicle re-localization. A\ntopological map of S-BEV signatures is created during the first traversal of\nthe route, which are used for coarse localization in subsequent route\ntraversal. A fine-grained localizer is then trained to output the global 3-DoF\npose of the vehicle using its S-BEV and its coarse localization. We conduct\nexperiments on vKITTI2 virtual dataset and show the potential of the S-BEV to\nbe robust to weather and lighting. We also demonstrate results with 2 vehicles\non a 22 km long highway route in the Ford AV dataset.\n", "versions": [{"version": "v1", "created": "Sat, 23 Jan 2021 19:37:09 GMT"}], "update_date": "2021-01-26", "authors_parsed": [["Voodarla", "Mokshith", ""], ["Shrivastava", "Shubham", ""], ["Manglani", "Sagar", ""], ["Vora", "Ankit", ""], ["Agarwal", "Siddharth", ""], ["Chakravarty", "Punarjay", ""]]}, {"id": "2101.09571", "submitter": "Vadim Liventsev", "authors": "Vadim Liventsev, Aki H\\\"arm\\\"a and Milan Petkovi\\'c", "title": "BF++: a language for general-purpose program synthesis", "comments": "8+2 pages (paper+references)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG cs.NE", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Most state of the art decision systems based on Reinforcement Learning (RL)\nare data-driven black-box neural models, where it is often difficult to\nincorporate expert knowledge into the models or let experts review and validate\nthe learned decision mechanisms. Knowledge-insertion and model review are\nimportant requirements in many applications involving human health and safety.\nOne way to bridge the gap between data and knowledge driven systems is program\nsynthesis: replacing a neural network that outputs decisions with a symbolic\nprogram generated by a neural network or by means of genetic programming. We\npropose a new programming language, BF++, designed specifically for automatic\nprogramming of agents in a Partially Observable Markov Decision Process (POMDP)\nsetting and apply neural program synthesis to solve standard OpenAI Gym\nbenchmarks.\n", "versions": [{"version": "v1", "created": "Sat, 23 Jan 2021 19:44:44 GMT"}, {"version": "v2", "created": "Wed, 27 Jan 2021 12:25:25 GMT"}, {"version": "v3", "created": "Thu, 18 Feb 2021 20:24:02 GMT"}, {"version": "v4", "created": "Thu, 17 Jun 2021 13:01:09 GMT"}], "update_date": "2021-06-18", "authors_parsed": [["Liventsev", "Vadim", ""], ["H\u00e4rm\u00e4", "Aki", ""], ["Petkovi\u0107", "Milan", ""]]}, {"id": "2101.09577", "submitter": "Bla\\v{z} \\v{S}krlj", "authors": "Bla\\v{z} \\v{S}krlj, Sa\\v{s}o D\\v{z}eroski, Nada Lavra\\v{c} and Matej\n  Petkovi\\'c", "title": "ReliefE: Feature Ranking in High-dimensional Spaces via Manifold\n  Embeddings", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Feature ranking has been widely adopted in machine learning applications such\nas high-throughput biology and social sciences. The approaches of the popular\nRelief family of algorithms assign importances to features by iteratively\naccounting for nearest relevant and irrelevant instances. Despite their high\nutility, these algorithms can be computationally expensive and not-well suited\nfor high-dimensional sparse input spaces. In contrast, recent embedding-based\nmethods learn compact, low-dimensional representations, potentially\nfacilitating down-stream learning capabilities of conventional learners. This\npaper explores how the Relief branch of algorithms can be adapted to benefit\nfrom (Riemannian) manifold-based embeddings of instance and target spaces,\nwhere a given embedding's dimensionality is intrinsic to the dimensionality of\nthe considered data set. The developed ReliefE algorithm is faster and can\nresult in better feature rankings, as shown by our evaluation on 20 real-life\ndata sets for multi-class and multi-label classification tasks. The utility of\nReliefE for high-dimensional data sets is ensured by its implementation that\nutilizes sparse matrix algebraic operations. Finally, the relation of ReliefE\nto other ranking algorithms is studied via the Fuzzy Jaccard Index.\n", "versions": [{"version": "v1", "created": "Sat, 23 Jan 2021 20:23:31 GMT"}], "update_date": "2021-01-26", "authors_parsed": [["\u0160krlj", "Bla\u017e", ""], ["D\u017eeroski", "Sa\u0161o", ""], ["Lavra\u010d", "Nada", ""], ["Petkovi\u0107", "Matej", ""]]}, {"id": "2101.09593", "submitter": "Jie Chen", "authors": "Yuliang Ji, Ru Huang, Jie Chen, Yuanzhe Xi", "title": "Generating a Doppelganger Graph: Resembling but Distinct", "comments": "Code is available at\n  https://github.com/yizhidamiaomiao/DoppelgangerGraph", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep generative models, since their inception, have become increasingly more\ncapable of generating novel and perceptually realistic signals (e.g., images\nand sound waves). With the emergence of deep models for graph structured data,\nnatural interests seek extensions of these generative models for graphs.\nSuccessful extensions were seen recently in the case of learning from a\ncollection of graphs (e.g., protein data banks), but the learning from a single\ngraph has been largely under explored. The latter case, however, is important\nin practice. For example, graphs in financial and healthcare systems contain so\nmuch confidential information that their public accessibility is nearly\nimpossible, but open science in these fields can only advance when similar data\nare available for benchmarking.\n  In this work, we propose an approach to generating a doppelganger graph that\nresembles a given one in many graph properties but nonetheless can hardly be\nused to reverse engineer the original one, in the sense of a near zero edge\noverlap. The approach is an orchestration of graph representation learning,\ngenerative adversarial networks, and graph realization algorithms. Through\ncomparison with several graph generative models (either parameterized by neural\nnetworks or not), we demonstrate that our result barely reproduces the given\ngraph but closely matches its properties. We further show that downstream\ntasks, such as node classification, on the generated graphs reach similar\nperformance to the use of the original ones.\n", "versions": [{"version": "v1", "created": "Sat, 23 Jan 2021 22:08:27 GMT"}], "update_date": "2021-01-26", "authors_parsed": [["Ji", "Yuliang", ""], ["Huang", "Ru", ""], ["Chen", "Jie", ""], ["Xi", "Yuanzhe", ""]]}, {"id": "2101.09603", "submitter": "Ryan D'Orazio", "authors": "Ryan D'Orazio and Ruitong Huang", "title": "Optimistic and Adaptive Lagrangian Hedging", "comments": "To be presented at the workshop on reinforcement learning and games\n  at the AAAI 2021 conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In online learning an algorithm plays against an environment with losses\npossibly picked by an adversary at each round. The generality of this framework\nincludes problems that are not adversarial, for example offline optimization,\nor saddle point problems (i.e. min max optimization). However, online\nalgorithms are typically not designed to leverage additional structure present\nin non-adversarial problems. Recently, slight modifications to well-known\nonline algorithms such as optimism and adaptive step sizes have been used in\nseveral domains to accelerate online learning -- recovering optimal rates in\noffline smooth optimization, and accelerating convergence to saddle points or\nsocial welfare in smooth games. In this work we introduce optimism and adaptive\nstepsizes to Lagrangian hedging, a class of online algorithms that includes\nregret-matching, and hedge (i.e. multiplicative weights). Our results include:\na general general regret bound; a path length regret bound for a fixed smooth\nloss, applicable to an optimistic variant of regret-matching and\nregret-matching+; optimistic regret bounds for $\\Phi$ regret, a framework that\nincludes external, internal, and swap regret; and optimistic bounds for a\nfamily of algorithms that includes regret-matching+ as a special case.\n", "versions": [{"version": "v1", "created": "Sat, 23 Jan 2021 23:32:40 GMT"}, {"version": "v2", "created": "Wed, 3 Feb 2021 02:59:24 GMT"}], "update_date": "2021-02-04", "authors_parsed": [["D'Orazio", "Ryan", ""], ["Huang", "Ruitong", ""]]}, {"id": "2101.09612", "submitter": "Quynh Nguyen", "authors": "Quynh Nguyen", "title": "On the Proof of Global Convergence of Gradient Descent for Deep ReLU\n  Networks with Linear Widths", "comments": "ICML 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We give a simple proof for the global convergence of gradient descent in\ntraining deep ReLU networks with the standard square loss, and show some of its\nimprovements over the state-of-the-art. In particular, while prior works\nrequire all the hidden layers to be wide with width at least $\\Omega(N^8)$ ($N$\nbeing the number of training samples), we require a single wide layer of\nlinear, quadratic or cubic width depending on the type of initialization.\nUnlike many recent proofs based on the Neural Tangent Kernel (NTK), our proof\nneed not track the evolution of the entire NTK matrix, or more generally, any\nquantities related to the changes of activation patterns during training.\nInstead, we only need to track the evolution of the output at the last hidden\nlayer, which can be done much more easily thanks to the Lipschitz property of\nReLU. Some highlights of our setting: (i) all the layers are trained with\nstandard gradient descent, (ii) the network has standard parameterization as\nopposed to the NTK one, and (iii) the network has a single wide layer as\nopposed to having all wide hidden layers as in most of NTK-related results.\n", "versions": [{"version": "v1", "created": "Sun, 24 Jan 2021 00:29:19 GMT"}, {"version": "v2", "created": "Sun, 6 Jun 2021 21:18:44 GMT"}, {"version": "v3", "created": "Fri, 11 Jun 2021 07:39:59 GMT"}], "update_date": "2021-06-14", "authors_parsed": [["Nguyen", "Quynh", ""]]}, {"id": "2101.09614", "submitter": "Pedro Santos", "authors": "Guilherme S. Varela, Pedro P. Santos, Alberto Sardinha and Francisco\n  S. Melo", "title": "A Methodology for the Development of RL-Based Adaptive Traffic Signal\n  Controllers", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SY cs.LG cs.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article proposes a methodology for the development of adaptive traffic\nsignal controllers using reinforcement learning. Our methodology addresses the\nlack of standardization in the literature that renders the comparison of\napproaches in different works meaningless, due to differences in metrics,\nenvironments, and even experimental design and methodology. The proposed\nmethodology thus comprises all the steps necessary to develop, deploy and\nevaluate an adaptive traffic signal controller -- from simulation setup to\nproblem formulation and experimental design. We illustrate the proposed\nmethodology in two simple scenarios, highlighting how its different steps\naddress limitations found in the current literature.\n", "versions": [{"version": "v1", "created": "Sun, 24 Jan 2021 00:47:00 GMT"}], "update_date": "2021-01-26", "authors_parsed": [["Varela", "Guilherme S.", ""], ["Santos", "Pedro P.", ""], ["Sardinha", "Alberto", ""], ["Melo", "Francisco S.", ""]]}, {"id": "2101.09637", "submitter": "Vasudevan Lakshminarayanan", "authors": "Yuliana Jim\\'enez Gaona, Mar\\'ia Jos\\'e Rodriguez-Alvarez, Hector\n  Espin\\'o Morat\\'o, Darwin Castillo Malla, and Vasudevan Lakshminarayanan", "title": "DenseNet for Breast Tumor Classification in Mammographic Images", "comments": "to be submitted to The 2nd International Conference on Medical\n  Imaging and Computer-Aided Diagnosis (MICAD2021)", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Breast cancer is the most common invasive cancer in women, and the second\nmain cause of death. Breast cancer screening is an efficient method to detect\nindeterminate breast lesions early. The common approaches of screening for\nwomen are tomosynthesis and mammography images. However, the traditional manual\ndiagnosis requires an intense workload by pathologists, who are prone to\ndiagnostic errors. Thus, the aim of this study is to build a deep convolutional\nneural network method for automatic detection, segmentation, and classification\nof breast lesions in mammography images. Based on deep learning the Mask-CNN\n(RoIAlign) method was developed to features selection and extraction; and the\nclassification was carried out by DenseNet architecture. Finally, the precision\nand accuracy of the model is evaluated by cross validation matrix and AUC\ncurve. To summarize, the findings of this study may provide a helpful to\nimprove the diagnosis and efficiency in the automatic tumor localization\nthrough the medical image classification.\n", "versions": [{"version": "v1", "created": "Sun, 24 Jan 2021 03:30:59 GMT"}], "update_date": "2021-01-26", "authors_parsed": [["Gaona", "Yuliana Jim\u00e9nez", ""], ["Rodriguez-Alvarez", "Mar\u00eda Jos\u00e9", ""], ["Morat\u00f3", "Hector Espin\u00f3", ""], ["Malla", "Darwin Castillo", ""], ["Lakshminarayanan", "Vasudevan", ""]]}, {"id": "2101.09640", "submitter": "Hu Wang", "authors": "Hu Wang, Hao Chen, Qi Wu, Congbo Ma, Yidong Li, Chunhua Shen", "title": "Multi-intersection Traffic Optimisation: A Benchmark Dataset and a\n  Strong Baseline", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SY cs.LG cs.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The control of traffic signals is fundamental and critical to alleviate\ntraffic congestion in urban areas. However, it is challenging since traffic\ndynamics are complicated in real-world scenarios. Because of the high\ncomplexity of the optimisation problem for modelling the traffic, experimental\nsettings of existing works are often inconsistent. Moreover, it is not trivial\nto control multiple intersections properly in real complex traffic scenarios\ndue to its vast state and action space. Failing to take intersection topology\nrelations into account also results in inferior solutions. To address these\nissues, in this work we carefully design our settings and propose a new dataset\nincluding both synthetic and real traffic data in more complex scenarios.\nAdditionally, we propose a novel baseline model with strong performance. It is\nbased on deep reinforcement learning with an encoder-decoder structure: an\nedge-weighted graph convolutional encoder to excavate multi-intersection\nrelations; and an unified structure decoder to jointly model multiple junctions\nin a comprehensive manner, which significantly reduces the number of the model\nparameters. By doing so, the proposed model is able to effectively deal with\nthe multi-intersection traffic optimisation problem. Models are trained/tested\non both synthetic and real maps and traffic data with the Simulation of Urban\nMobility (SUMO) simulator. Experimental results show that the proposed model\nsurpasses multiple competitive methods.\n", "versions": [{"version": "v1", "created": "Sun, 24 Jan 2021 03:55:39 GMT"}, {"version": "v2", "created": "Fri, 7 May 2021 06:06:14 GMT"}], "update_date": "2021-05-10", "authors_parsed": [["Wang", "Hu", ""], ["Chen", "Hao", ""], ["Wu", "Qi", ""], ["Ma", "Congbo", ""], ["Li", "Yidong", ""], ["Shen", "Chunhua", ""]]}, {"id": "2101.09645", "submitter": "Zekai Chen", "authors": "Zekai Chen, Jiaze E, Xiao Zhang, Hao Sheng, Xiuzheng Cheng", "title": "Multi-Task Time Series Forecasting With Shared Attention", "comments": "Accepted by ICDMW 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Time series forecasting is a key component in many industrial and business\ndecision processes and recurrent neural network (RNN) based models have\nachieved impressive progress on various time series forecasting tasks. However,\nmost of the existing methods focus on single-task forecasting problems by\nlearning separately based on limited supervised objectives, which often suffer\nfrom insufficient training instances. As the Transformer architecture and other\nattention-based models have demonstrated its great capability of capturing long\nterm dependency, we propose two self-attention based sharing schemes for\nmulti-task time series forecasting which can train jointly across multiple\ntasks. We augment a sequence of paralleled Transformer encoders with an\nexternal public multi-head attention function, which is updated by all data of\nall tasks. Experiments on a number of real-world multi-task time series\nforecasting tasks show that our proposed architectures can not only outperform\nthe state-of-the-art single-task forecasting baselines but also outperform the\nRNN-based multi-task forecasting method.\n", "versions": [{"version": "v1", "created": "Sun, 24 Jan 2021 04:25:08 GMT"}], "update_date": "2021-01-26", "authors_parsed": [["Chen", "Zekai", ""], ["E", "Jiaze", ""], ["Zhang", "Xiao", ""], ["Sheng", "Hao", ""], ["Cheng", "Xiuzheng", ""]]}, {"id": "2101.09648", "submitter": "Maria De-Arteaga", "authors": "Maria De-Arteaga, Artur Dubrawski, Alexandra Chouldechova", "title": "Leveraging Expert Consistency to Improve Algorithmic Decision Support", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Due to their promise of superior predictive power relative to human\nassessment, machine learning models are increasingly being used to support\nhigh-stakes decisions. However, the nature of the labels available for training\nthese models often hampers the usefulness of predictive models for decision\nsupport. In this paper, we explore the use of historical expert decisions as a\nrich--yet imperfect--source of information, and we show that it can be\nleveraged to mitigate some of the limitations of learning from observed labels\nalone. We consider the problem of estimating expert consistency indirectly when\neach case in the data is assessed by a single expert, and propose influence\nfunctions based methodology as a solution to this problem. We then incorporate\nthe estimated expert consistency into the predictive model meant for decision\nsupport through an approach we term label amalgamation. This allows the machine\nlearning models to learn from experts in instances where there is expert\nconsistency, and learn from the observed labels elsewhere. We show how the\nproposed approach can help mitigate common challenges of learning from observed\nlabels alone, reducing the gap between the construct that the algorithm\noptimizes for and the construct of interest to experts. After providing\nintuition and theoretical results, we present empirical results in the context\nof child maltreatment hotline screenings. Here, we find that (1) there are\nhigh-risk cases whose risk is considered by the experts but not wholly captured\nin the target labels used to train a deployed model, and (2) the proposed\napproach improves recall for these cases.\n", "versions": [{"version": "v1", "created": "Sun, 24 Jan 2021 05:40:29 GMT"}], "update_date": "2021-01-26", "authors_parsed": [["De-Arteaga", "Maria", ""], ["Dubrawski", "Artur", ""], ["Chouldechova", "Alexandra", ""]]}, {"id": "2101.09650", "submitter": "Juhyoung Lee", "authors": "Juhyoung Lee, Sangyeob Kim, Sangjin Kim, Wooyoung Jo, Hoi-Jun Yoo", "title": "GST: Group-Sparse Training for Accelerating Deep Reinforcement Learning", "comments": "10 pages, 10 figures, CVPR 2021 Submitted", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep reinforcement learning (DRL) has shown remarkable success in sequential\ndecision-making problems but suffers from a long training time to obtain such\ngood performance. Many parallel and distributed DRL training approaches have\nbeen proposed to solve this problem, but it is difficult to utilize them on\nresource-limited devices. In order to accelerate DRL in real-world edge\ndevices, memory bandwidth bottlenecks due to large weight transactions have to\nbe resolved. However, previous iterative pruning not only shows a low\ncompression ratio at the beginning of training but also makes DRL training\nunstable. To overcome these shortcomings, we propose a novel weight compression\nmethod for DRL training acceleration, named group-sparse training (GST). GST\nselectively utilizes block-circulant compression to maintain a high weight\ncompression ratio during all iterations of DRL training and dynamically adapt\ntarget sparsity through reward-aware pruning for stable training. Thanks to the\nfeatures, GST achieves a 25 \\%p $\\sim$ 41.5 \\%p higher average compression\nratio than the iterative pruning method without reward drop in Mujoco\nHalfcheetah-v2 and Mujoco humanoid-v2 environment with TD3 training.\n", "versions": [{"version": "v1", "created": "Sun, 24 Jan 2021 05:52:31 GMT"}], "update_date": "2021-01-26", "authors_parsed": [["Lee", "Juhyoung", ""], ["Kim", "Sangyeob", ""], ["Kim", "Sangjin", ""], ["Jo", "Wooyoung", ""], ["Yoo", "Hoi-Jun", ""]]}, {"id": "2101.09667", "submitter": "Md Abul Bashar", "authors": "Fahim Shahriar, Md Abul Bashar", "title": "Automatic Monitoring Social Dynamics During Big Incidences: A Case Study\n  of COVID-19 in Bangladesh", "comments": "Very minor change", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.CL cs.LG cs.SI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Newspapers are trustworthy media where people get the most reliable and\ncredible information compared with other sources. On the other hand, social\nmedia often spread rumors and misleading news to get more traffic and\nattention. Careful characterization, evaluation, and interpretation of\nnewspaper data can provide insight into intrigue and passionate social issues\nto monitor any big social incidence. This study analyzed a large set of\nspatio-temporal Bangladeshi newspaper data related to the COVID-19 pandemic.\nThe methodology included volume analysis, topic analysis, automated\nclassification, and sentiment analysis of news articles to get insight into the\nCOVID-19 pandemic in different sectors and regions in Bangladesh over a period\nof time. This analysis will help the government and other organizations to\nfigure out the challenges that have arisen in society due to this pandemic,\nwhat steps should be taken immediately and in the post-pandemic period, how the\ngovernment and its allies can come together to address the crisis in the\nfuture, keeping these problems in mind.\n", "versions": [{"version": "v1", "created": "Sun, 24 Jan 2021 07:46:17 GMT"}, {"version": "v2", "created": "Sun, 31 Jan 2021 16:47:37 GMT"}], "update_date": "2021-02-02", "authors_parsed": [["Shahriar", "Fahim", ""], ["Bashar", "Md Abul", ""]]}, {"id": "2101.09688", "submitter": "Pasquale Minervini", "authors": "Daniel de Vassimon Manela, David Errington, Thomas Fisher, Boris van\n  Breugel, Pasquale Minervini", "title": "Stereotype and Skew: Quantifying Gender Bias in Pre-trained and\n  Fine-tuned Language Models", "comments": "Proceedings of the 16th Conference of the European Chapter of the\n  Association for Computational Linguistics (EACL 2021)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes two intuitive metrics, skew and stereotype, that quantify\nand analyse the gender bias present in contextual language models when tackling\nthe WinoBias pronoun resolution task. We find evidence that gender stereotype\ncorrelates approximately negatively with gender skew in out-of-the-box models,\nsuggesting that there is a trade-off between these two forms of bias. We\ninvestigate two methods to mitigate bias. The first approach is an online\nmethod which is effective at removing skew at the expense of stereotype. The\nsecond, inspired by previous work on ELMo, involves the fine-tuning of BERT\nusing an augmented gender-balanced dataset. We show that this reduces both skew\nand stereotype relative to its unaugmented fine-tuned counterpart. However, we\nfind that existing gender bias benchmarks do not fully probe professional bias\nas pronoun resolution may be obfuscated by cross-correlations from other\nmanifestations of gender prejudice. Our code is available online, at\nhttps://github.com/12kleingordon34/NLP_masters_project.\n", "versions": [{"version": "v1", "created": "Sun, 24 Jan 2021 10:57:59 GMT"}, {"version": "v2", "created": "Tue, 16 Feb 2021 14:17:41 GMT"}], "update_date": "2021-02-17", "authors_parsed": [["Manela", "Daniel de Vassimon", ""], ["Errington", "David", ""], ["Fisher", "Thomas", ""], ["van Breugel", "Boris", ""], ["Minervini", "Pasquale", ""]]}, {"id": "2101.09693", "submitter": "Mohsen Ahmadzadeh", "authors": "Mohsen Ahmadzadeh, Mehdi Kamal, Ali Afzali-Kusha, Massoud Pedram", "title": "A2P-MANN: Adaptive Attention Inference Hops Pruned Memory-Augmented\n  Neural Networks", "comments": "10 pages, 9 figures, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.CC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, to limit the number of required attention inference hops in\nmemory-augmented neural networks, we propose an online adaptive approach called\nA2P-MANN. By exploiting a small neural network classifier, an adequate number\nof attention inference hops for the input query is determined. The technique\nresults in elimination of a large number of unnecessary computations in\nextracting the correct answer. In addition, to further lower computations in\nA2P-MANN, we suggest pruning weights of the final FC (fully-connected) layers.\nTo this end, two pruning approaches, one with negligible accuracy loss and the\nother with controllable loss on the final accuracy, are developed. The efficacy\nof the technique is assessed by using the twenty question-answering (QA) tasks\nof bAbI dataset. The analytical assessment reveals, on average, more than 42%\nfewer computations compared to the baseline MANN at the cost of less than 1%\naccuracy loss. In addition, when used along with the previously published\nzero-skipping technique, a computation count reduction of up to 68% is\nachieved. Finally, when the proposed approach (without zero-skipping) is\nimplemented on the CPU and GPU platforms, up to 43% runtime reduction is\nachieved.\n", "versions": [{"version": "v1", "created": "Sun, 24 Jan 2021 12:02:12 GMT"}], "update_date": "2021-01-26", "authors_parsed": [["Ahmadzadeh", "Mohsen", ""], ["Kamal", "Mehdi", ""], ["Afzali-Kusha", "Ali", ""], ["Pedram", "Massoud", ""]]}, {"id": "2101.09704", "submitter": "Lin Xiao", "authors": "Lin Xiao, Xiangliang Zhang, Liping Jing, Chi Huang, Mingyang Song", "title": "Does Head Label Help for Long-Tailed Multi-Label Text Classification", "comments": "Accepted by AAAI2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multi-label text classification (MLTC) aims to annotate documents with the\nmost relevant labels from a number of candidate labels. In real applications,\nthe distribution of label frequency often exhibits a long tail, i.e., a few\nlabels are associated with a large number of documents (a.k.a. head labels),\nwhile a large fraction of labels are associated with a small number of\ndocuments (a.k.a. tail labels). To address the challenge of insufficient\ntraining data on tail label classification, we propose a Head-to-Tail Network\n(HTTN) to transfer the meta-knowledge from the data-rich head labels to\ndata-poor tail labels. The meta-knowledge is the mapping from few-shot network\nparameters to many-shot network parameters, which aims to promote the\ngeneralizability of tail classifiers. Extensive experimental results on three\nbenchmark datasets demonstrate that HTTN consistently outperforms the\nstate-of-the-art methods. The code and hyper-parameter settings are released\nfor reproducibility\n", "versions": [{"version": "v1", "created": "Sun, 24 Jan 2021 12:31:39 GMT"}], "update_date": "2021-01-26", "authors_parsed": [["Xiao", "Lin", ""], ["Zhang", "Xiangliang", ""], ["Jing", "Liping", ""], ["Huang", "Chi", ""], ["Song", "Mingyang", ""]]}, {"id": "2101.09705", "submitter": "Brenda Vilas Boas", "authors": "Brenda Vilas Boas, Wolfgang Zirwas and Martin Haardt", "title": "Two-step Machine Learning Approach for Channel Estimation with Mixed\n  Resolution RF Chains", "comments": "to be published", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SP cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Massive MIMO is one of the main features of 5G mobile radio systems. However,\nit often leads to high cost, size and power consumption. To overcome these\nissues, the use of constrained radio frequency (RF) frontends has been\nproposed, as well as novel precoders, e.g., a multi-antenna, greedy, iterative\nand quantized precoding algorithm (MAGIQ). Nevertheless, the best performance\nof MAGIQ assumes accurate channel knowledge per antenna element, for example,\nfrom uplink sounding reference signals. In this context, we propose an\nefficient uplink channel estimator by applying machine learning (ML)\nalgorithms. In a first step a conditional generative adversarial network (cGAN)\npredicts the radio channels from a limited set of full resolution RF chains to\nthe rest of the low resolution RF chain antenna elements. A long-short term\nmemory (LSTM) neural network extracts further phase information from the low\nresolution RF chain antenna elements. Our results indicate that our proposed\napproach is competitive with traditional Unitary tensor-ESPRIT in scenarios\nwith various closely spaced multipath components (MPCs).\n", "versions": [{"version": "v1", "created": "Sun, 24 Jan 2021 12:33:54 GMT"}], "update_date": "2021-01-26", "authors_parsed": [["Boas", "Brenda Vilas", ""], ["Zirwas", "Wolfgang", ""], ["Haardt", "Martin", ""]]}, {"id": "2101.09721", "submitter": "Fabio Ferreira", "authors": "Fabio Ferreira, Thomas Nierhoff, Frank Hutter", "title": "Learning Synthetic Environments for Reinforcement Learning with\n  Evolution Strategies", "comments": null, "journal-ref": "AAAI 2021 Meta-Learning Workshop", "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.NE", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This work explores learning agent-agnostic synthetic environments (SEs) for\nReinforcement Learning. SEs act as a proxy for target environments and allow\nagents to be trained more efficiently than when directly trained on the target\nenvironment. We formulate this as a bi-level optimization problem and represent\nan SE as a neural network. By using Natural Evolution Strategies and a\npopulation of SE parameter vectors, we train agents in the inner loop on\nevolving SEs while in the outer loop we use the performance on the target task\nas a score for meta-updating the SE population. We show empirically that our\nmethod is capable of learning SEs for two discrete-action-space tasks\n(CartPole-v0 and Acrobot-v1) that allow us to train agents more robustly and\nwith up to 60% fewer steps. Not only do we show in experiments with 4000\nevaluations that the SEs are robust against hyperparameter changes such as the\nlearning rate, batch sizes and network sizes, we also show that SEs trained\nwith DDQN agents transfer in limited ways to a discrete-action-space version of\nTD3 and very well to Dueling DDQN.\n", "versions": [{"version": "v1", "created": "Sun, 24 Jan 2021 14:16:13 GMT"}, {"version": "v2", "created": "Tue, 26 Jan 2021 18:53:35 GMT"}, {"version": "v3", "created": "Mon, 8 Feb 2021 15:03:39 GMT"}], "update_date": "2021-02-09", "authors_parsed": [["Ferreira", "Fabio", ""], ["Nierhoff", "Thomas", ""], ["Hutter", "Frank", ""]]}, {"id": "2101.09747", "submitter": "Emmanuel Vazquez", "authors": "Subhasish Basak, S\\'ebastien Petit, Julien Bect, Emmanuel Vazquez", "title": "Numerical issues in maximum likelihood parameter estimation for Gaussian\n  process interpolation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.CO", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  This article investigates the origin of numerical issues in maximum\nlikelihood parameter estimation for Gaussian process (GP) interpolation and\ninvestigates simple but effective strategies for improving commonly used\nopen-source software implementations. This work targets a basic problem but a\nhost of studies, particularly in the literature of Bayesian optimization, rely\non off-the-shelf GP implementations. For the conclusions of these studies to be\nreliable and reproducible, robust GP implementations are critical.\n", "versions": [{"version": "v1", "created": "Sun, 24 Jan 2021 16:30:55 GMT"}, {"version": "v2", "created": "Tue, 27 Jul 2021 19:31:13 GMT"}], "update_date": "2021-07-29", "authors_parsed": [["Basak", "Subhasish", ""], ["Petit", "S\u00e9bastien", ""], ["Bect", "Julien", ""], ["Vazquez", "Emmanuel", ""]]}, {"id": "2101.09752", "submitter": "Sibendu Paul", "authors": "Sibendu Paul, Utsav Drolia, Y. Charlie Hu, Srimat T. Chakradhar", "title": "AQuA: Analytical Quality Assessment for Optimizing Video Analytics\n  Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Millions of cameras at edge are being deployed to power a variety of\ndifferent deep learning applications. However, the frames captured by these\ncameras are not always pristine - they can be distorted due to lighting issues,\nsensor noise, compression etc. Such distortions not only deteriorate visual\nquality, they impact the accuracy of deep learning applications that process\nsuch video streams. In this work, we introduce AQuA, to protect application\naccuracy against such distorted frames by scoring the level of distortion in\nthe frames. It takes into account the analytical quality of frames, not the\nvisual quality, by learning a novel metric, classifier opinion score, and uses\na lightweight, CNN-based, object-independent feature extractor. AQuA accurately\nscores distortion levels of frames and generalizes to multiple different deep\nlearning applications. When used for filtering poor quality frames at edge, it\nreduces high-confidence errors for analytics applications by 17%. Through\nfiltering, and due to its low overhead (14ms), AQuA can also reduce computation\ntime and average bandwidth usage by 25%.\n", "versions": [{"version": "v1", "created": "Sun, 24 Jan 2021 16:56:59 GMT"}], "update_date": "2021-01-26", "authors_parsed": [["Paul", "Sibendu", ""], ["Drolia", "Utsav", ""], ["Hu", "Y. Charlie", ""], ["Chakradhar", "Srimat T.", ""]]}, {"id": "2101.09756", "submitter": "Tam Le", "authors": "Tam Le, Truyen Nguyen", "title": "Entropy Partial Transport with Tree Metrics: Theory and Practice", "comments": "To appear in AISTATS 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Optimal transport (OT) theory provides powerful tools to compare probability\nmeasures. However, OT is limited to nonnegative measures having the same mass,\nand suffers serious drawbacks about its computation and statistics. This leads\nto several proposals of regularized variants of OT in the recent literature. In\nthis work, we consider an \\textit{entropy partial transport} (EPT) problem for\nnonnegative measures on a tree having different masses. The EPT is shown to be\nequivalent to a standard complete OT problem on a one-node extended tree. We\nderive its dual formulation, then leverage this to propose a novel\nregularization for EPT which admits fast computation and negative definiteness.\nTo our knowledge, the proposed regularized EPT is the first approach that\nyields a \\textit{closed-form} solution among available variants of unbalanced\nOT. For practical applications without priori knowledge about the tree\nstructure for measures, we propose tree-sliced variants of the regularized EPT,\ncomputed by averaging the regularized EPT between these measures using random\ntree metrics, built adaptively from support data points. Exploiting the\nnegative definiteness of our regularized EPT, we introduce a positive definite\nkernel, and evaluate it against other baselines on benchmark tasks such as\ndocument classification with word embedding and topological data analysis. In\naddition, we empirically demonstrate that our regularization also provides\neffective approximations.\n", "versions": [{"version": "v1", "created": "Sun, 24 Jan 2021 17:04:24 GMT"}], "update_date": "2021-01-26", "authors_parsed": [["Le", "Tam", ""], ["Nguyen", "Truyen", ""]]}, {"id": "2101.09763", "submitter": "Michael A. Hedderich", "authors": "Michael A. Hedderich, Dawei Zhu, Dietrich Klakow", "title": "Analysing the Noise Model Error for Realistic Noisy Label Data", "comments": "Accepted at AAAI 2021, additional material at\n  https://github.com/uds-lsv/noise-estimation", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Distant and weak supervision allow to obtain large amounts of labeled\ntraining data quickly and cheaply, but these automatic annotations tend to\ncontain a high amount of errors. A popular technique to overcome the negative\neffects of these noisy labels is noise modelling where the underlying noise\nprocess is modelled. In this work, we study the quality of these estimated\nnoise models from the theoretical side by deriving the expected error of the\nnoise model. Apart from evaluating the theoretical results on commonly used\nsynthetic noise, we also publish NoisyNER, a new noisy label dataset from the\nNLP domain that was obtained through a realistic distant supervision technique.\nIt provides seven sets of labels with differing noise patterns to evaluate\ndifferent noise levels on the same instances. Parallel, clean labels are\navailable making it possible to study scenarios where a small amount of\ngold-standard data can be leveraged. Our theoretical results and the\ncorresponding experiments give insights into the factors that influence the\nnoise model estimation like the noise distribution and the sampling technique.\n", "versions": [{"version": "v1", "created": "Sun, 24 Jan 2021 17:45:15 GMT"}, {"version": "v2", "created": "Mon, 1 Mar 2021 11:14:54 GMT"}], "update_date": "2021-03-02", "authors_parsed": [["Hedderich", "Michael A.", ""], ["Zhu", "Dawei", ""], ["Klakow", "Dietrich", ""]]}, {"id": "2101.09808", "submitter": "Rui Li", "authors": "Rui Li, Yufan Xu, Aravind Sukumaran-Rajam, Atanas Rountev, and P.\n  Sadayappan", "title": "Analytical Characterization and Design Space Exploration for\n  Optimization of CNNs", "comments": "In proceedings of the 26th ACM International Conference on\n  Architectural Support for Programming Languages and Operating Systems (ASPLOS\n  '21), April 19-23, 2021, Virtual, USA", "journal-ref": "Proceedings of the 26th ACM International Conference on\n  Architectural Support for Programming Languages and Operating Systems, 2021", "doi": "10.1145/3445814.3446759", "report-no": null, "categories": "cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Moving data through the memory hierarchy is a fundamental bottleneck that can\nlimit the performance of core algorithms of machine learning, such as\nconvolutional neural networks (CNNs). Loop-level optimization, including loop\ntiling and loop permutation, are fundamental transformations to reduce data\nmovement. However, the search space for finding the best loop-level\noptimization configuration is explosively large. This paper develops an\nanalytical modeling approach for finding the best loop-level optimization\nconfiguration for CNNs on multi-core CPUs. Experimental evaluation shows that\nthis approach achieves comparable or better performance than state-of-the-art\nlibraries and auto-tuning based optimizers for CNNs.\n", "versions": [{"version": "v1", "created": "Sun, 24 Jan 2021 21:36:52 GMT"}, {"version": "v2", "created": "Sat, 6 Mar 2021 00:40:24 GMT"}], "update_date": "2021-04-13", "authors_parsed": [["Li", "Rui", ""], ["Xu", "Yufan", ""], ["Sukumaran-Rajam", "Aravind", ""], ["Rountev", "Atanas", ""], ["Sadayappan", "P.", ""]]}, {"id": "2101.09809", "submitter": "Lin Qiu", "authors": "Lin Qiu, Nils Murrugarra-Llerena, V\\'itor Silva, Lin Lin, Vernon M.\n  Chinchilli", "title": "NeurT-FDR: Controlling FDR by Incorporating Feature Hierarchy", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Controlling false discovery rate (FDR) while leveraging the side information\nof multiple hypothesis testing is an emerging research topic in modern data\nscience. Existing methods rely on the test-level covariates while ignoring\npossible hierarchy among the covariates. This strategy may not be optimal for\ncomplex large-scale problems, where hierarchical information often exists among\nthose test-level covariates. We propose NeurT-FDR which boosts statistical\npower and controls FDR for multiple hypothesis testing while leveraging the\nhierarchy among test-level covariates. Our method parametrizes the test-level\ncovariates as a neural network and adjusts the feature hierarchy through a\nregression framework, which enables flexible handling of high-dimensional\nfeatures as well as efficient end-to-end optimization. We show that NeurT-FDR\nhas strong FDR guarantees and makes substantially more discoveries in synthetic\nand real datasets compared to competitive baselines.\n", "versions": [{"version": "v1", "created": "Sun, 24 Jan 2021 21:55:10 GMT"}], "update_date": "2021-01-26", "authors_parsed": [["Qiu", "Lin", ""], ["Murrugarra-Llerena", "Nils", ""], ["Silva", "V\u00edtor", ""], ["Lin", "Lin", ""], ["Chinchilli", "Vernon M.", ""]]}, {"id": "2101.09815", "submitter": "Francesco D'Angelo", "authors": "Francesco D'Angelo, Vincent Fortuin", "title": "Annealed Stein Variational Gradient Descent", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Particle based optimization algorithms have recently been developed as\nsampling methods that iteratively update a set of particles to approximate a\ntarget distribution. In particular Stein variational gradient descent has\ngained attention in the approximate inference literature for its flexibility\nand accuracy. We empirically explore the ability of this method to sample from\nmulti-modal distributions and focus on two important issues: (i) the inability\nof the particles to escape from local modes and (ii) the inefficacy in\nreproducing the density of the different regions. We propose an annealing\nschedule to solve these issues and show, through various experiments, how this\nsimple solution leads to significant improvements in mode coverage, without\ninvalidating any theoretical properties of the original algorithm.\n", "versions": [{"version": "v1", "created": "Sun, 24 Jan 2021 22:18:30 GMT"}, {"version": "v2", "created": "Mon, 8 Feb 2021 10:19:25 GMT"}, {"version": "v3", "created": "Thu, 18 Mar 2021 15:02:52 GMT"}], "update_date": "2021-03-19", "authors_parsed": [["D'Angelo", "Francesco", ""], ["Fortuin", "Vincent", ""]]}, {"id": "2101.09818", "submitter": "Ali Rasteh", "authors": "Ali Rasteh, Florian Delpech, Carlos Aguilar-Melchor, Romain Zimmer,\n  Saeed Bagheri Shouraki and Timoth\\'ee Masquelier", "title": "Encrypted Internet traffic classification using a supervised Spiking\n  Neural Network", "comments": "22 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Internet traffic recognition is an essential tool for access providers since\nrecognizing traffic categories related to different data packets transmitted on\na network help them define adapted priorities. That means, for instance, high\npriority requirements for an audio conference and low ones for a file transfer,\nto enhance user experience. As internet traffic becomes increasingly encrypted,\nthe mainstream classic traffic recognition technique, payload inspection, is\nrendered ineffective. This paper uses machine learning techniques for encrypted\ntraffic classification, looking only at packet size and time of arrival.\nSpiking neural networks (SNN), largely inspired by how biological neurons\noperate, were used for two reasons. Firstly, they are able to recognize\ntime-related data packet features. Secondly, they can be implemented\nefficiently on neuromorphic hardware with a low energy footprint. Here we used\na very simple feedforward SNN, with only one fully-connected hidden layer, and\ntrained in a supervised manner using the newly introduced method known as\nSurrogate Gradient Learning. Surprisingly, such a simple SNN reached an\naccuracy of 95.9% on ISCX datasets, outperforming previous approaches. Besides\nbetter accuracy, there is also a very significant improvement on simplicity:\ninput size, number of neurons, trainable parameters are all reduced by one to\nfour orders of magnitude. Next, we analyzed the reasons for this good accuracy.\nIt turns out that, beyond spatial (i.e. packet size) features, the SNN also\nexploits temporal ones, mostly the nearly synchronous (within a 200ms range)\narrival times of packets with certain sizes. Taken together, these results show\nthat SNNs are an excellent fit for encrypted internet traffic classification:\nthey can be more accurate than conventional artificial neural networks (ANN),\nand they could be implemented efficiently on low power embedded systems.\n", "versions": [{"version": "v1", "created": "Sun, 24 Jan 2021 22:46:08 GMT"}], "update_date": "2021-01-26", "authors_parsed": [["Rasteh", "Ali", ""], ["Delpech", "Florian", ""], ["Aguilar-Melchor", "Carlos", ""], ["Zimmer", "Romain", ""], ["Shouraki", "Saeed Bagheri", ""], ["Masquelier", "Timoth\u00e9e", ""]]}, {"id": "2101.09819", "submitter": "Shubham Shrivastava", "authors": "Edwin Pan and Pankaj Rajak and Shubham Shrivastava", "title": "Meta-Regularization by Enforcing Mutual-Exclusiveness", "comments": "12 pages, 8 figures, 4 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Meta-learning models have two objectives. First, they need to be able to make\npredictions over a range of task distributions while utilizing only a small\namount of training data. Second, they also need to adapt to new novel unseen\ntasks at meta-test time again by using only a small amount of training data\nfrom that task. It is the second objective where meta-learning models fail for\nnon-mutually exclusive tasks due to task overfitting. Given that guaranteeing\nmutually exclusive tasks is often difficult, there is a significant need for\nregularization methods that can help reduce the impact of task-memorization in\nmeta-learning. For example, in the case of N-way, K-shot classification\nproblems, tasks becomes non-mutually exclusive when the labels associated with\neach task is fixed. Under this design, the model will simply memorize the class\nlabels of all the training tasks, and thus will fail to recognize a new task\n(class) at meta-test time. A direct observable consequence of this memorization\nis that the meta-learning model simply ignores the task-specific training data\nin favor of directly classifying based on the test-data input. In our work, we\npropose a regularization technique for meta-learning models that gives the\nmodel designer more control over the information flow during meta-training. Our\nmethod consists of a regularization function that is constructed by maximizing\nthe distance between task-summary statistics, in the case of black-box models\nand task specific network parameters in the case of optimization based models\nduring meta-training. Our proposed regularization function shows an accuracy\nboost of $\\sim$ $36\\%$ on the Omniglot dataset for 5-way, 1-shot classification\nusing black-box method and for 20-way, 1-shot classification problem using\noptimization-based method.\n", "versions": [{"version": "v1", "created": "Sun, 24 Jan 2021 22:57:19 GMT"}], "update_date": "2021-01-26", "authors_parsed": [["Pan", "Edwin", ""], ["Rajak", "Pankaj", ""], ["Shrivastava", "Shubham", ""]]}, {"id": "2101.09824", "submitter": "Harini Suresh", "authors": "Harini Suresh, Steven R. Gomez, Kevin K. Nam, Arvind Satyanarayan", "title": "Beyond Expertise and Roles: A Framework to Characterize the Stakeholders\n  of Interpretable Machine Learning and their Needs", "comments": "In CHI Conference on Human Factors in Computing Systems (CHI '21)", "journal-ref": null, "doi": "10.1145/3411764.3445088", "report-no": null, "categories": "cs.HC cs.CY cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To ensure accountability and mitigate harm, it is critical that diverse\nstakeholders can interrogate black-box automated systems and find information\nthat is understandable, relevant, and useful to them. In this paper, we eschew\nprior expertise- and role-based categorizations of interpretability\nstakeholders in favor of a more granular framework that decouples stakeholders'\nknowledge from their interpretability needs. We characterize stakeholders by\ntheir formal, instrumental, and personal knowledge and how it manifests in the\ncontexts of machine learning, the data domain, and the general milieu. We\nadditionally distill a hierarchical typology of stakeholder needs that\ndistinguishes higher-level domain goals from lower-level interpretability\ntasks. In assessing the descriptive, evaluative, and generative powers of our\nframework, we find our more nuanced treatment of stakeholders reveals gaps and\nopportunities in the interpretability literature, adds precision to the design\nand comparison of user studies, and facilitates a more reflexive approach to\nconducting this research.\n", "versions": [{"version": "v1", "created": "Sun, 24 Jan 2021 23:21:21 GMT"}], "update_date": "2021-01-26", "authors_parsed": [["Suresh", "Harini", ""], ["Gomez", "Steven R.", ""], ["Nam", "Kevin K.", ""], ["Satyanarayan", "Arvind", ""]]}, {"id": "2101.09825", "submitter": "Guillaume Lagrange", "authors": "Nathaniel Simard and Guillaume Lagrange", "title": "Improving Few-Shot Learning with Auxiliary Self-Supervised Pretext Tasks", "comments": "Research project report for graduate class IFT 6268-A2020 on\n  Self-supervised Representation Learning", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Recent work on few-shot learning \\cite{tian2020rethinking} showed that\nquality of learned representations plays an important role in few-shot\nclassification performance. On the other hand, the goal of self-supervised\nlearning is to recover useful semantic information of the data without the use\nof class labels. In this work, we exploit the complementarity of both paradigms\nvia a multi-task framework where we leverage recent self-supervised methods as\nauxiliary tasks. We found that combining multiple tasks is often beneficial,\nand that solving them simultaneously can be done efficiently. Our results\nsuggest that self-supervised auxiliary tasks are effective data-dependent\nregularizers for representation learning. Our code is available at:\n\\url{https://github.com/nathanielsimard/improving-fs-ssl}.\n", "versions": [{"version": "v1", "created": "Sun, 24 Jan 2021 23:21:43 GMT"}], "update_date": "2021-01-26", "authors_parsed": [["Simard", "Nathaniel", ""], ["Lagrange", "Guillaume", ""]]}, {"id": "2101.09833", "submitter": "Aaron Buhendwa", "authors": "Aaron B. Buhendwa, Stefan Adami, Nikolaus A. Adams (Technical\n  University of Munich)", "title": "Inferring incompressible two-phase flow fields from the interface motion\n  using physics-informed neural networks", "comments": "43 pages, 29 Figures, 15 Tables, Preprint submitted to \"Machine\n  Learning with Applications\"", "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.flu-dyn cs.LG physics.comp-ph", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  In this work, physics-informed neural networks are applied to incompressible\ntwo-phase flow problems. We investigate the forward problem, where the\ngoverning equations are solved from initial and boundary conditions, as well as\nthe inverse problem, where continuous velocity and pressure fields are inferred\nfrom scattered-time data on the interface position. We employ a volume of fluid\napproach, i.e. the auxiliary variable here is the volume fraction of the fluids\nwithin each phase. For the forward problem, we solve the two-phase Couette and\nPoiseuille flow. For the inverse problem, three classical test cases for\ntwo-phase modeling are investigated: (i) drop in a shear flow, (ii) oscillating\ndrop and (iii) rising bubble. Data of the interface position over time is\ngenerated by numerical simulation. An effective way to distribute spatial\ntraining points to fit the interface, i.e. the volume fraction field, and the\nresidual points is proposed. Furthermore, we show that appropriate weighting of\nlosses associated with the residual of the partial differential equations is\ncrucial for successful training. The benefit of using adaptive activation\nfunctions is evaluated for both the forward and inverse problem.\n", "versions": [{"version": "v1", "created": "Mon, 25 Jan 2021 00:26:33 GMT"}], "update_date": "2021-01-26", "authors_parsed": [["Buhendwa", "Aaron B.", "", "Technical\n  University of Munich"], ["Adami", "Stefan", "", "Technical\n  University of Munich"], ["Adams", "Nikolaus A.", "", "Technical\n  University of Munich"]]}, {"id": "2101.09841", "submitter": "Leslie Tiong", "authors": "Leslie Ching Ow Tiong and HeeJeong Jasmine Lee", "title": "E-cheating Prevention Measures: Detection of Cheating at Online\n  Examinations Using Deep Learning Approach -- A Case Study", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.CR cs.CY cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This study addresses the current issues in online assessments, which are\nparticularly relevant during the Covid-19 pandemic. Our focus is on academic\ndishonesty associated with online assessments. We investigated the prevalence\nof potential e-cheating using a case study and propose preventive measures that\ncould be implemented. We have utilised an e-cheating intelligence agent as a\nmechanism for detecting the practices of online cheating, which is composed of\ntwo major modules: the internet protocol (IP) detector and the behaviour\ndetector. The intelligence agent monitors the behaviour of the students and has\nthe ability to prevent and detect any malicious practices. It can be used to\nassign randomised multiple-choice questions in a course examination and be\nintegrated with online learning programs to monitor the behaviour of the\nstudents. The proposed method was tested on various data sets confirming its\neffectiveness. The results revealed accuracies of 68% for the deep neural\nnetwork (DNN); 92% for the long-short term memory (LSTM); 95% for the\nDenseLSTM; and, 86% for the recurrent neural network (RNN).\n", "versions": [{"version": "v1", "created": "Mon, 25 Jan 2021 01:09:54 GMT"}], "update_date": "2021-01-26", "authors_parsed": [["Tiong", "Leslie Ching Ow", ""], ["Lee", "HeeJeong Jasmine", ""]]}, {"id": "2101.09844", "submitter": "Stanislav Sobolevsky", "authors": "Shivam Pathak, Mingyi He, Sergey Malinchik, Stanislav Sobolevsky", "title": "Pattern Ensembling for Spatial Trajectory Reconstruction", "comments": "11 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.data-an cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Digital sensing provides an unprecedented opportunity to assess and\nunderstand mobility. However, incompleteness, missing information, possible\ninaccuracies, and temporal heterogeneity in the geolocation data can undermine\nits applicability. As mobility patterns are often repeated, we propose a method\nto use similar trajectory patterns from the local vicinity and\nprobabilistically ensemble them to robustly reconstruct missing or unreliable\nobservations. We evaluate the proposed approach in comparison with traditional\nfunctional trajectory interpolation using a case of sea vessel trajectory data\nprovided by The Automatic Identification System (AIS). By effectively\nleveraging the similarities in real-world trajectories, our pattern ensembling\nmethod helps to reconstruct missing trajectory segments of extended length and\ncomplex geometry. It can be used for locating mobile objects when temporary\nunobserved as well as for creating an evenly sampled trajectory interpolation\nuseful for further trajectory mining.\n", "versions": [{"version": "v1", "created": "Mon, 25 Jan 2021 01:44:00 GMT"}], "update_date": "2021-01-26", "authors_parsed": [["Pathak", "Shivam", ""], ["He", "Mingyi", ""], ["Malinchik", "Sergey", ""], ["Sobolevsky", "Stanislav", ""]]}, {"id": "2101.09847", "submitter": "Yash Chandak", "authors": "Yash Chandak, Shiv Shankar, Philip S. Thomas", "title": "High-Confidence Off-Policy (or Counterfactual) Variance Estimation", "comments": "Thirty-fifth AAAI Conference on Artificial Intelligence (AAAI 2021)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many sequential decision-making systems leverage data collected using prior\npolicies to propose a new policy. For critical applications, it is important\nthat high-confidence guarantees on the new policy's behavior are provided\nbefore deployment, to ensure that the policy will behave as desired. Prior\nworks have studied high-confidence off-policy estimation of the expected\nreturn, however, high-confidence off-policy estimation of the variance of\nreturns can be equally critical for high-risk applications. In this paper, we\ntackle the previously open problem of estimating and bounding, with high\nconfidence, the variance of returns from off-policy data\n", "versions": [{"version": "v1", "created": "Mon, 25 Jan 2021 01:47:00 GMT"}], "update_date": "2021-01-26", "authors_parsed": [["Chandak", "Yash", ""], ["Shankar", "Shiv", ""], ["Thomas", "Philip S.", ""]]}, {"id": "2101.09848", "submitter": "Chen Zhao", "authors": "Chen Zhao, Haipeng Tang, Daniel McGonigle, Zhuo He, Chaoyang Zhang,\n  Yu-Ping Wang, Hong-Wen Deng, Robert Bober, Weihua Zhou", "title": "A new approach to extracting coronary arteries and detecting stenosis in\n  invasive coronary angiograms", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In stable coronary artery disease (CAD), reduction in mortality and/or\nmyocardial infarction with revascularization over medical therapy has not been\nreliably achieved. Coronary arteries are usually extracted to perform stenosis\ndetection. We aim to develop an automatic algorithm by deep learning to extract\ncoronary arteries from ICAs.In this study, a multi-input and multi-scale (MIMS)\nU-Net with a two-stage recurrent training strategy was proposed for the\nautomatic vessel segmentation. Incorporating features such as the Inception\nresidual module with depth-wise separable convolutional layers, the proposed\nmodel generated a refined prediction map with the following two training\nstages: (i) Stage I coarsely segmented the major coronary arteries from\npre-processed single-channel ICAs and generated the probability map of vessels;\n(ii) during the Stage II, a three-channel image consisting of the original\npreprocessed image, a generated probability map, and an edge-enhanced image\ngenerated from the preprocessed image was fed to the proposed MIMS U-Net to\nproduce the final segmentation probability map. During the training stage, the\nprobability maps were iteratively and recurrently updated by feeding into the\nneural network. After segmentation, an arterial stenosis detection algorithm\nwas developed to extract vascular centerlines and calculate arterial diameters\nto evaluate stenotic level. Experimental results demonstrated that the proposed\nmethod achieved an average Dice score of 0.8329, an average sensitivity of\n0.8281, and an average specificity of 0.9979 in our dataset with 294 ICAs\nobtained from 73 patient. Moreover, our stenosis detection algorithm achieved a\ntrue positive rate of 0.6668 and a positive predictive value of 0.7043.\n", "versions": [{"version": "v1", "created": "Mon, 25 Jan 2021 01:48:27 GMT"}], "update_date": "2021-01-26", "authors_parsed": [["Zhao", "Chen", ""], ["Tang", "Haipeng", ""], ["McGonigle", "Daniel", ""], ["He", "Zhuo", ""], ["Zhang", "Chaoyang", ""], ["Wang", "Yu-Ping", ""], ["Deng", "Hong-Wen", ""], ["Bober", "Robert", ""], ["Zhou", "Weihua", ""]]}, {"id": "2101.09849", "submitter": "Roozbeh Yousefzadeh", "authors": "Roozbeh Yousefzadeh", "title": "Deep Learning Generalization and the Convex Hull of Training Sets", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.DG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the generalization of deep learning models in relation to the convex\nhull of their training sets. A trained image classifier basically partitions\nits domain via decision boundaries and assigns a class to each of those\npartitions. The location of decision boundaries inside the convex hull of\ntraining set can be investigated in relation to the training samples. However,\nour analysis shows that in standard image classification datasets, all testing\nimages are considerably outside that convex hull, in the pixel space, in the\nwavelet space, and in the internal representations learned by deep networks.\nTherefore, the performance of a trained model partially depends on how its\ndecision boundaries are extended outside the convex hull of its training data.\nFrom this perspective which is not studied before, over-parameterization of\ndeep learning models may be considered a necessity for shaping the extension of\ndecision boundaries. At the same time, over-parameterization should be\naccompanied by a specific training regime, in order to yield a model that not\nonly fits the training set, but also its decision boundaries extend desirably\noutside the convex hull. To illustrate this, we investigate the decision\nboundaries of a neural network, with various degrees of parameters, inside and\noutside the convex hull of its training set. Moreover, we use a polynomial\ndecision boundary to study the necessity of over-parameterization and the\ninfluence of training regime in shaping its extensions outside the convex hull\nof training set.\n", "versions": [{"version": "v1", "created": "Mon, 25 Jan 2021 01:54:02 GMT"}], "update_date": "2021-01-26", "authors_parsed": [["Yousefzadeh", "Roozbeh", ""]]}, {"id": "2101.09855", "submitter": "Kuang Xu", "authors": "Stefan Wager and Kuang Xu", "title": "Diffusion Asymptotics for Sequential Experiments", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.LG stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We propose a new diffusion-asymptotic analysis for sequentially randomized\nexperiments, including those that arise in solving multi-armed bandit problems.\nIn an experiment with $ n $ time steps, we let the mean reward gaps between\nactions scale to the order $1/\\sqrt{n}$ so as to preserve the difficulty of the\nlearning task as $n$ grows. In this regime, we show that the behavior of a\nclass of sequentially randomized Markov experiments converges to a diffusion\nlimit, given as the solution of a stochastic differential equation. The\ndiffusion limit thus enables us to derive refined, instance-specific\ncharacterization of the stochastic dynamics of adaptive experiments. As an\napplication of this framework, we use the diffusion limit to obtain several new\ninsights on the regret and belief evolution of Thompson sampling. We show that\na version of Thompson sampling with an asymptotically uninformative prior\nvariance achieves nearly-optimal instance-specific regret scaling when the\nreward gaps are relatively large. We also demonstrate that, in this regime, the\nposterior beliefs underlying Thompson sampling are highly unstable over time.\n", "versions": [{"version": "v1", "created": "Mon, 25 Jan 2021 02:20:20 GMT"}, {"version": "v2", "created": "Wed, 10 Feb 2021 03:50:40 GMT"}, {"version": "v3", "created": "Thu, 10 Jun 2021 22:02:29 GMT"}], "update_date": "2021-06-14", "authors_parsed": [["Wager", "Stefan", ""], ["Xu", "Kuang", ""]]}, {"id": "2101.09864", "submitter": "Wang Bo", "authors": "Tao Li and Wang Bo and Chunyu Hu and Hong Kang and Hanruo Liu and Kai\n  Wang and Huazhu Fu", "title": "Applications of Deep Learning in Fundus Images: A Review", "comments": null, "journal-ref": "Medical Image Analysis 2021", "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  The use of fundus images for the early screening of eye diseases is of great\nclinical importance. Due to its powerful performance, deep learning is becoming\nmore and more popular in related applications, such as lesion segmentation,\nbiomarkers segmentation, disease diagnosis and image synthesis. Therefore, it\nis very necessary to summarize the recent developments in deep learning for\nfundus images with a review paper. In this review, we introduce 143 application\npapers with a carefully designed hierarchy. Moreover, 33 publicly available\ndatasets are presented. Summaries and analyses are provided for each task.\nFinally, limitations common to all tasks are revealed and possible solutions\nare given. We will also release and regularly update the state-of-the-art\nresults and newly-released datasets at https://github.com/nkicsl/Fundus Review\nto adapt to the rapid development of this field.\n", "versions": [{"version": "v1", "created": "Mon, 25 Jan 2021 02:39:40 GMT"}], "update_date": "2021-01-26", "authors_parsed": [["Li", "Tao", ""], ["Bo", "Wang", ""], ["Hu", "Chunyu", ""], ["Kang", "Hong", ""], ["Liu", "Hanruo", ""], ["Wang", "Kai", ""], ["Fu", "Huazhu", ""]]}, {"id": "2101.09868", "submitter": "Yonggan Fu", "authors": "Yonggan Fu, Han Guo, Meng Li, Xin Yang, Yining Ding, Vikas Chandra,\n  Yingyan Lin", "title": "CPT: Efficient Deep Neural Network Training via Cyclic Precision", "comments": "Accepted at ICLR 2021 (Spotlight)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Low-precision deep neural network (DNN) training has gained tremendous\nattention as reducing precision is one of the most effective knobs for boosting\nDNNs' training time/energy efficiency. In this paper, we attempt to explore\nlow-precision training from a new perspective as inspired by recent findings in\nunderstanding DNN training: we conjecture that DNNs' precision might have a\nsimilar effect as the learning rate during DNN training, and advocate dynamic\nprecision along the training trajectory for further boosting the time/energy\nefficiency of DNN training. Specifically, we propose Cyclic Precision Training\n(CPT) to cyclically vary the precision between two boundary values which can be\nidentified using a simple precision range test within the first few training\nepochs. Extensive simulations and ablation studies on five datasets and eleven\nmodels demonstrate that CPT's effectiveness is consistent across various\nmodels/tasks (including classification and language modeling). Furthermore,\nthrough experiments and visualization we show that CPT helps to (1) converge to\na wider minima with a lower generalization error and (2) reduce training\nvariance which we believe opens up a new design knob for simultaneously\nimproving the optimization and efficiency of DNN training. Our codes are\navailable at: https://github.com/RICE-EIC/CPT.\n", "versions": [{"version": "v1", "created": "Mon, 25 Jan 2021 02:56:18 GMT"}, {"version": "v2", "created": "Wed, 24 Mar 2021 23:37:03 GMT"}, {"version": "v3", "created": "Fri, 7 May 2021 01:59:49 GMT"}], "update_date": "2021-05-10", "authors_parsed": [["Fu", "Yonggan", ""], ["Guo", "Han", ""], ["Li", "Meng", ""], ["Yang", "Xin", ""], ["Ding", "Yining", ""], ["Chandra", "Vikas", ""], ["Lin", "Yingyan", ""]]}, {"id": "2101.09869", "submitter": "Lelia Marie Hampton", "authors": "Lelia Marie Hampton", "title": "Black Feminist Musings on Algorithmic Oppression", "comments": "12 pages, accepted to ACM Conference on Fairness, Accountability, and\n  Transparency 2021", "journal-ref": null, "doi": "10.1145/3442188.3445929", "report-no": null, "categories": "cs.CY cs.AI cs.HC cs.LG", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  This paper unapologetically reflects on the critical role that Black feminism\ncan and should play in abolishing algorithmic oppression. Positioning\nalgorithmic oppression in the broader field of feminist science and technology\nstudies, I draw upon feminist philosophical critiques of science and technology\nand discuss histories and continuities of scientific oppression against\nhistorically marginalized people. Moreover, I examine the concepts of\ninvisibility and hypervisibility in oppressive technologies a l\\'a the\ncanonical double bind. Furthermore, I discuss what it means to call for\ndiversity as a solution to algorithmic violence, and I critique dialectics of\nthe fairness, accountability, and transparency community. I end by inviting you\nto envision and imagine the struggle to abolish algorithmic oppression by\nabolishing oppressive systems and shifting algorithmic development practices,\nincluding engaging our communities in scientific processes, centering\nmarginalized communities in design, and consensual data and algorithmic\npractices.\n", "versions": [{"version": "v1", "created": "Mon, 25 Jan 2021 03:04:05 GMT"}, {"version": "v2", "created": "Wed, 3 Feb 2021 01:54:26 GMT"}], "update_date": "2021-02-04", "authors_parsed": [["Hampton", "Lelia Marie", ""]]}, {"id": "2101.09875", "submitter": "Xiuyuan Cheng", "authors": "Xiuyuan Cheng, Nan Wu", "title": "Eigen-convergence of Gaussian kernelized graph Laplacian by manifold\n  heat interpolation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.LG stat.ML stat.TH", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  This work studies the spectral convergence of graph Laplacian to the\nLaplace-Beltrami operator when the graph affinity matrix is constructed from\n$N$ random samples on a $d$-dimensional manifold embedded in a possibly high\ndimensional space. By analyzing Dirichlet form convergence and constructing\ncandidate approximate eigenfunctions via convolution with manifold heat kernel,\nwe prove that, with Gaussian kernel, one can set the kernel bandwidth parameter\n$\\epsilon \\sim (\\log N/ N)^{1/(d/2+2)}$ such that the eigenvalue convergence\nrate is $N^{-1/(d/2+2)}$ and the eigenvector convergence in 2-norm has rate\n$N^{-1/(d+4)}$; When $\\epsilon \\sim N^{-1/(d/2+3)}$, both eigenvalue and\neigenvector rates are $N^{-1/(d/2+3)}$. These rates are up to a $\\log N$ factor\nand proved for finitely many low-lying eigenvalues. The result holds for\nun-normalized and random-walk graph Laplacians when data are uniformly sampled\non the manifold, as well as the density-corrected graph Laplacian (where the\naffinity matrix is normalized by the degree matrix from both sides) with\nnon-uniformly sampled data. As an intermediate result, we prove new point-wise\nand Dirichlet form convergence rates for the density-corrected graph Laplacian.\nNumerical results are provided to verify the theory.\n", "versions": [{"version": "v1", "created": "Mon, 25 Jan 2021 03:22:18 GMT"}], "update_date": "2021-01-26", "authors_parsed": [["Cheng", "Xiuyuan", ""], ["Wu", "Nan", ""]]}, {"id": "2101.09878", "submitter": "Ajesh Koyatan Chathoth", "authors": "Ajesh Koyatan Chathoth (1), Abhyuday Jagannatha (2), Stephen Lee (1)\n  ((1) University of Pittsburgh, (2) University of Massachusetts Amherst)", "title": "Federated Intrusion Detection for IoT with Heterogeneous Cohort Privacy", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DC cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Internet of Things (IoT) devices are becoming increasingly popular and are\ninfluencing many application domains such as healthcare and transportation.\nThese devices are used for real-world applications such as sensor monitoring,\nreal-time control. In this work, we look at differentially private (DP) neural\nnetwork (NN) based network intrusion detection systems (NIDS) to detect\nintrusion attacks on networks of such IoT devices. Existing NN training\nsolutions in this domain either ignore privacy considerations or assume that\nthe privacy requirements are homogeneous across all users. We show that the\nperformance of existing differentially private stochastic methods degrade for\nclients with non-identical data distributions when clients' privacy\nrequirements are heterogeneous. We define a cohort-based $(\\epsilon,\\delta)$-DP\nframework that models the more practical setting of IoT device cohorts with\nnon-identical clients and heterogeneous privacy requirements. We propose two\nnovel continual-learning based DP training methods that are designed to improve\nmodel performance in the aforementioned setting. To the best of our knowledge,\nours is the first system that employs a continual learning-based approach to\nhandle heterogeneity in client privacy requirements. We evaluate our approach\non real datasets and show that our techniques outperform the baselines. We also\nshow that our methods are robust to hyperparameter changes. Lastly, we show\nthat one of our proposed methods can easily adapt to post-hoc relaxations of\nclient privacy requirements.\n", "versions": [{"version": "v1", "created": "Mon, 25 Jan 2021 03:33:27 GMT"}], "update_date": "2021-01-26", "authors_parsed": [["Chathoth", "Ajesh Koyatan", "", "University of Pittsburgh"], ["Jagannatha", "Abhyuday", "", "University of Massachusetts Amherst"], ["Lee", "Stephen", "", "University of Pittsburgh"]]}, {"id": "2101.09884", "submitter": "Md Sahidullah", "authors": "A Kishore Kumar, Shefali Waldekar, Goutam Saha, Md Sahidullah", "title": "Domain-Dependent Speaker Diarization for the Third DIHARD Challenge", "comments": "This work was presented in The Third DIHARD Speech Diarization\n  Challenge Workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.LG eess.AS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This report presents the system developed by the ABSP Laboratory team for the\nthird DIHARD speech diarization challenge. Our main contribution in this work\nis to develop a simple and efficient solution for acoustic domain dependent\nspeech diarization. We explore speaker embeddings for \\emph{acoustic domain\nidentification} (ADI) task. Our study reveals that i-vector based method\nachieves considerably better performance than x-vector based approach in the\nthird DIHARD challenge dataset. Next, we integrate the ADI module with the\ndiarization framework. The performance substantially improved over that of the\nbaseline when we optimized the thresholds for agglomerative hierarchical\nclustering and the parameters for dimensionality reduction during scoring for\nindividual acoustic domains. We achieved a relative improvement of $9.63\\%$ and\n$10.64\\%$ in DER for core and full conditions, respectively, for Track 1 of the\nDIHARD III evaluation set.\n", "versions": [{"version": "v1", "created": "Mon, 25 Jan 2021 04:01:13 GMT"}], "update_date": "2021-01-26", "authors_parsed": [["Kumar", "A Kishore", ""], ["Waldekar", "Shefali", ""], ["Saha", "Goutam", ""], ["Sahidullah", "Md", ""]]}, {"id": "2101.09903", "submitter": "Weixin Jiang", "authors": "Weixin Jiang, Eric Schwenker, Trevor Spreadbury, Nicola Ferrier, Maria\n  K.Y. Chan, Oliver Cossairt", "title": "A Two-stage Framework for Compound Figure Separation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Scientific literature contains large volumes of complex, unstructured figures\nthat are compound in nature (i.e. composed of multiple images, graphs, and\ndrawings). Separation of these compound figures is critical for information\nretrieval from these figures. In this paper, we propose a new strategy for\ncompound figure separation, which decomposes the compound figures into\nconstituent subfigures while preserving the association between the subfigures\nand their respective caption components. We propose a two-stage framework to\naddress the proposed compound figure separation problem. In particular, the\nsubfigure label detection module detects all subfigure labels in the first\nstage. Then, in the subfigure detection module, the detected subfigure labels\nhelp to detect the subfigures by optimizing the feature selection process and\nproviding the global layout information as extra features. Extensive\nexperiments are conducted to validate the effectiveness and superiority of the\nproposed framework, which improves the detection precision by 9%.\n", "versions": [{"version": "v1", "created": "Mon, 25 Jan 2021 05:43:36 GMT"}], "update_date": "2021-01-26", "authors_parsed": [["Jiang", "Weixin", ""], ["Schwenker", "Eric", ""], ["Spreadbury", "Trevor", ""], ["Ferrier", "Nicola", ""], ["Chan", "Maria K. Y.", ""], ["Cossairt", "Oliver", ""]]}, {"id": "2101.09915", "submitter": "Hong-Gyu Jung", "authors": "Hyun-Woo Kim, Hong-Gyu Jung, Seong-Whan Lee", "title": "Weakly Supervised Thoracic Disease Localization via Disease Masks", "comments": "The first two authors contributed equally to this work", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To enable a deep learning-based system to be used in the medical domain as a\ncomputer-aided diagnosis system, it is essential to not only classify diseases\nbut also present the locations of the diseases. However, collecting\ninstance-level annotations for various thoracic diseases is expensive.\nTherefore, weakly supervised localization methods have been proposed that use\nonly image-level annotation. While the previous methods presented the disease\nlocation as the most discriminative part for classification, this causes a deep\nnetwork to localize wrong areas for indistinguishable X-ray images. To solve\nthis issue, we propose a spatial attention method using disease masks that\ndescribe the areas where diseases mainly occur. We then apply the spatial\nattention to find the precise disease area by highlighting the highest\nprobability of disease occurrence. Meanwhile, the various sizes, rotations and\nnoise in chest X-ray images make generating the disease masks challenging. To\nreduce the variation among images, we employ an alignment module to transform\nan input X-ray image into a generalized image. Through extensive experiments on\nthe NIH-Chest X-ray dataset with eight kinds of diseases, we show that the\nproposed method results in superior localization performances compared to\nstate-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Mon, 25 Jan 2021 06:52:57 GMT"}], "update_date": "2021-01-26", "authors_parsed": [["Kim", "Hyun-Woo", ""], ["Jung", "Hong-Gyu", ""], ["Lee", "Seong-Whan", ""]]}, {"id": "2101.09930", "submitter": "Yixiang Wang", "authors": "Yixiang Wang, Jiqiang Liu, Xiaolin Chang", "title": "Generalizing Adversarial Examples by AdaBelief Optimizer", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent research has proved that deep neural networks (DNNs) are vulnerable to\nadversarial examples, the legitimate input added with imperceptible and\nwell-designed perturbations can fool DNNs easily in the testing stage. However,\nmost of the existing adversarial attacks are difficult to fool adversarially\ntrained models. To solve this issue, we propose an AdaBelief iterative Fast\nGradient Sign Method (AB-FGSM) to generalize adversarial examples. By\nintegrating AdaBelief optimization algorithm to I-FGSM, we believe that the\ngeneralization of adversarial examples will be improved, relying on the strong\ngeneralization of AdaBelief optimizer. To validate the effectiveness and\ntransferability of adversarial examples generated by our proposed AB-FGSM, we\nconduct the white-box and black-box attacks on various single models and\nensemble models. Compared with state-of-the-art attack methods, our proposed\nmethod can generate adversarial examples effectively in the white-box setting,\nand the transfer rate is 7%-21% higher than latest attack methods.\n", "versions": [{"version": "v1", "created": "Mon, 25 Jan 2021 07:39:16 GMT"}], "update_date": "2021-01-26", "authors_parsed": [["Wang", "Yixiang", ""], ["Liu", "Jiqiang", ""], ["Chang", "Xiaolin", ""]]}, {"id": "2101.09933", "submitter": "Peng Liu", "authors": "Peng Liu, Lizhe Wang, Guojin He, Lei Zhao", "title": "A Survey on Active Deep Learning: From Model-driven to Data-driven", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Which samples should be labelled in a large data set is one of the most\nimportant problems for trainingof deep learning. So far, a variety of active\nsample selection strategies related to deep learning havebeen proposed in many\nliteratures. We defined them as Active Deep Learning (ADL) only if\ntheirpredictor is deep model, where the basic learner is called as predictor\nand the labeling schemes iscalled selector. In this survey, three fundamental\nfactors in selector designation were summarized. Wecategory ADL into\nmodel-driven ADL and data-driven ADL, by whether its selector is model-drivenor\ndata-driven. The different characteristics of the two major type of ADL were\naddressed in indetail respectively. Furthermore, different sub-classes of\ndata-driven and model-driven ADL are alsosummarized and discussed emphatically.\nThe advantages and disadvantages between data-driven ADLand model-driven ADL\nare thoroughly analyzed. We pointed out that, with the development of\ndeeplearning, the selector in ADL also is experiencing the stage from\nmodel-driven to data-driven. Finally,we make discussion on ADL about its\nuncertainty, explanatory, foundations of cognitive science etc.and survey on\nthe trend of ADL from model-driven to data-driven.\n", "versions": [{"version": "v1", "created": "Mon, 25 Jan 2021 07:49:41 GMT"}, {"version": "v2", "created": "Wed, 26 May 2021 15:14:56 GMT"}], "update_date": "2021-05-27", "authors_parsed": [["Liu", "Peng", ""], ["Wang", "Lizhe", ""], ["He", "Guojin", ""], ["Zhao", "Lei", ""]]}, {"id": "2101.09948", "submitter": "Abdourrahmane Mahamane Atto", "authors": "Abdourrahmane Mahamane Atto (LISTIC), Sylvie Galichet (LISTIC),\n  Dominique Pastor, Nicolas M\\'eger (LISTIC)", "title": "Parametric Rectified Power Sigmoid Units: Learning Nonlinear Neural\n  Transfer Analytical Forms", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG eess.SP math.FA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The paper proposes representation functionals in a dual paradigm where\nlearning jointly concerns both linear convolutional weights and parametric\nforms of nonlinear activation functions. The nonlinear forms proposed for\nperforming the functional representation are associated with a new class of\nparametric neural transfer functions called rectified power sigmoid units. This\nclass is constructed to integrate both advantages of sigmoid and rectified\nlinear unit functions, in addition with rejecting the drawbacks of these\nfunctions. Moreover, the analytic form of this new neural class involves scale,\nshift and shape parameters so as to obtain a wide range of activation shapes,\nincluding the standard rectified linear unit as a limit case. Parameters of\nthis neural transfer class are considered as learnable for the sake of\ndiscovering the complex shapes that can contribute in solving machine learning\nissues. Performance achieved by the joint learning of convolutional and\nrectified power sigmoid learnable parameters are shown outstanding in both\nshallow and deep learning frameworks. This class opens new prospects with\nrespect to machine learning in the sense that learnable parameters are not only\nattached to linear transformations, but also to suitable nonlinear operators.\n", "versions": [{"version": "v1", "created": "Mon, 25 Jan 2021 08:25:22 GMT"}, {"version": "v2", "created": "Fri, 5 Feb 2021 11:42:08 GMT"}], "update_date": "2021-02-08", "authors_parsed": [["Atto", "Abdourrahmane Mahamane", "", "LISTIC"], ["Galichet", "Sylvie", "", "LISTIC"], ["Pastor", "Dominique", "", "LISTIC"], ["M\u00e9ger", "Nicolas", "", "LISTIC"]]}, {"id": "2101.09957", "submitter": "Johannes Lederer", "authors": "Johannes Lederer", "title": "Activation Functions in Artificial Neural Networks: A Systematic\n  Overview", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Activation functions shape the outputs of artificial neurons and, therefore,\nare integral parts of neural networks in general and deep learning in\nparticular. Some activation functions, such as logistic and relu, have been\nused for many decades. But with deep learning becoming a mainstream research\ntopic, new activation functions have mushroomed, leading to confusion in both\ntheory and practice. This paper provides an analytic yet up-to-date overview of\npopular activation functions and their properties, which makes it a timely\nresource for anyone who studies or applies neural networks.\n", "versions": [{"version": "v1", "created": "Mon, 25 Jan 2021 08:55:26 GMT"}], "update_date": "2021-01-26", "authors_parsed": [["Lederer", "Johannes", ""]]}, {"id": "2101.09961", "submitter": "Keyan Zhai", "authors": "Keyan Zhai, Chu'an Li, Andre Rosendo", "title": "Scaffolded Learning of In-place Trotting Gait for a Quadruped Robot with\n  Bayesian Optimization", "comments": "9 pages, 6 figures, 16-th International Conference on Intelligent\n  Autonomous System (IAS-16)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  During learning trials, systems are exposed to different failure conditions\nwhich may break robotic parts before a safe behavior is discovered. Humans\ncontour this problem by grounding their learning to a safer structure/control\nfirst and gradually increasing its difficulty. This paper presents the impact\nof a similar supports in the learning of a stable gait on a quadruped robot.\nBased on the psychological theory of instructional scaffolding, we provide\ndifferent support settings to our robot, evaluated with strain gauges, and use\nBayesian Optimization to conduct a parametric search towards a stable Raibert\ncontroller. We perform several experiments to measure the relation between\nconstant supports and gradually reduced supports during gait learning, and our\nresults show that a gradually reduced support is capable of creating a more\nstable gait than a support at a fixed height. Although gaps between simulation\nand reality can lead robots to catastrophic failures, our proposed method\ncombines speed and safety when learning a new behavior.\n", "versions": [{"version": "v1", "created": "Mon, 25 Jan 2021 08:58:30 GMT"}, {"version": "v2", "created": "Sat, 3 Apr 2021 12:25:52 GMT"}], "update_date": "2021-04-06", "authors_parsed": [["Zhai", "Keyan", ""], ["Li", "Chu'an", ""], ["Rosendo", "Andre", ""]]}, {"id": "2101.09973", "submitter": "Manuj Mukherjee", "authors": "Manuj Mukherjee and Aslan Tchamkerten and Mansoor Yousefi", "title": "Approximating Probability Distributions by ReLU Networks", "comments": "Longer version of a paper accepted for presentation at the ITW 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.IT math.IT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  How many neurons are needed to approximate a target probability distribution\nusing a neural network with a given input distribution and approximation error?\nThis paper examines this question for the case when the input distribution is\nuniform, and the target distribution belongs to the class of histogram\ndistributions. We obtain a new upper bound on the number of required neurons,\nwhich is strictly better than previously existing upper bounds. The key\ningredient in this improvement is an efficient construction of the neural nets\nrepresenting piecewise linear functions. We also obtain a lower bound on the\nminimum number of neurons needed to approximate the histogram distributions.\n", "versions": [{"version": "v1", "created": "Mon, 25 Jan 2021 09:31:20 GMT"}], "update_date": "2021-01-26", "authors_parsed": [["Mukherjee", "Manuj", ""], ["Tchamkerten", "Aslan", ""], ["Yousefi", "Mansoor", ""]]}, {"id": "2101.09976", "submitter": "Keno Bressem", "authors": "Keno K. Bressem, Stefan M. Niehues, Bernd Hamm, Marcus R. Makowski,\n  Janis L. Vahldiek, Lisa C. Adams", "title": "3D U-Net for segmentation of COVID-19 associated pulmonary infiltrates\n  using transfer learning: State-of-the-art results on affordable hardware", "comments": "8 Pages, 2 figures, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Segmentation of pulmonary infiltrates can help assess severity of COVID-19,\nbut manual segmentation is labor and time-intensive. Using neural networks to\nsegment pulmonary infiltrates would enable automation of this task. However,\ntraining a 3D U-Net from computed tomography (CT) data is time- and\nresource-intensive. In this work, we therefore developed and tested a solution\non how transfer learning can be used to train state-of-the-art segmentation\nmodels on limited hardware and in shorter time. We use the recently published\nRSNA International COVID-19 Open Radiology Database (RICORD) to train a fully\nthree-dimensional U-Net architecture using an 18-layer 3D ResNet, pretrained on\nthe Kinetics-400 dataset as encoder. The generalization of the model was then\ntested on two openly available datasets of patients with COVID-19, who received\nchest CTs (Corona Cases and MosMed datasets). Our model performed comparable to\npreviously published 3D U-Net architectures, achieving a mean Dice score of\n0.679 on the tuning dataset, 0.648 on the Coronacases dataset and 0.405 on the\nMosMed dataset. Notably, these results were achieved with shorter training time\non a single GPU with less memory available than the GPUs used in previous\nstudies.\n", "versions": [{"version": "v1", "created": "Mon, 25 Jan 2021 09:37:32 GMT"}], "update_date": "2021-01-26", "authors_parsed": [["Bressem", "Keno K.", ""], ["Niehues", "Stefan M.", ""], ["Hamm", "Bernd", ""], ["Makowski", "Marcus R.", ""], ["Vahldiek", "Janis L.", ""], ["Adams", "Lisa C.", ""]]}, {"id": "2101.09978", "submitter": "Tianming Zhao", "authors": "Tianming Zhao (1), Chunyang Chen (2), Yuanning Liu (1), Xiaodong Zhu\n  (1) ((1) Jilin University, (2) Monash University)", "title": "GUIGAN: Learning to Generate GUI Designs Using Generative Adversarial\n  Networks", "comments": "13 pages, 10 figures, accepted for publication at ICSE2021 Technical\n  Track", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.CV cs.LG cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Graphical User Interface (GUI) is ubiquitous in almost all modern desktop\nsoftware, mobile applications, and online websites. A good GUI design is\ncrucial to the success of the software in the market, but designing a good GUI\nwhich requires much innovation and creativity is difficult even to well-trained\ndesigners. Besides, the requirement of the rapid development of GUI design also\naggravates designers' working load. So, the availability of various automated\ngenerated GUIs can help enhance the design personalization and specialization\nas they can cater to the taste of different designers. To assist designers, we\ndevelop a model GUIGAN to automatically generate GUI designs. Different from\nconventional image generation models based on image pixels, our GUIGAN is to\nreuse GUI components collected from existing mobile app GUIs for composing a\nnew design that is similar to natural-language generation. Our GUIGAN is based\non SeqGAN by modeling the GUI component style compatibility and GUI structure.\nThe evaluation demonstrates that our model significantly outperforms the best\nof the baseline methods by 30.77% in Frechet Inception distance (FID) and\n12.35% in 1-Nearest Neighbor Accuracy (1-NNA). Through a pilot user study, we\nprovide initial evidence of the usefulness of our approach for generating\nacceptable brand new GUI designs.\n", "versions": [{"version": "v1", "created": "Mon, 25 Jan 2021 09:42:58 GMT"}, {"version": "v2", "created": "Wed, 27 Jan 2021 04:42:42 GMT"}], "update_date": "2021-02-02", "authors_parsed": [["Zhao", "Tianming", "", "Jilin University"], ["Chen", "Chunyang", "", "Monash University"], ["Liu", "Yuanning", "", "Jilin University"], ["Zhu", "Xiaodong", "", "Jilin University"]]}, {"id": "2101.09979", "submitter": "Wei Wang", "authors": "Wei Wang, Baopu Li, Shuhui Yang, Jing Sun, Zhengming Ding, Junyang\n  Chen, Xiao Dong, Zhihui Wang, Haojie Li", "title": "A Unified Joint Maximum Mean Discrepancy for Domain Adaptation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Domain adaptation has received a lot of attention in recent years, and many\nalgorithms have been proposed with impressive progress. However, it is still\nnot fully explored concerning the joint probability distribution (P(X, Y))\ndistance for this problem, since its empirical estimation derived from the\nmaximum mean discrepancy (joint maximum mean discrepancy, JMMD) will involve\ncomplex tensor-product operator that is hard to manipulate. To solve this\nissue, this paper theoretically derives a unified form of JMMD that is easy to\noptimize, and proves that the marginal, class conditional and weighted class\nconditional probability distribution distances are our special cases with\ndifferent label kernels, among which the weighted class conditional one not\nonly can realize feature alignment across domains in the category level, but\nalso deal with imbalance dataset using the class prior probabilities. From the\nrevealed unified JMMD, we illustrate that JMMD degrades the feature-label\ndependence (discriminability) that benefits to classification, and it is\nsensitive to the label distribution shift when the label kernel is the weighted\nclass conditional one. Therefore, we leverage Hilbert Schmidt independence\ncriterion and propose a novel MMD matrix to promote the dependence, and devise\na novel label kernel that is robust to label distribution shift. Finally, we\nconduct extensive experiments on several cross-domain datasets to demonstrate\nthe validity and effectiveness of the revealed theoretical results.\n", "versions": [{"version": "v1", "created": "Mon, 25 Jan 2021 09:46:14 GMT"}], "update_date": "2021-01-26", "authors_parsed": [["Wang", "Wei", ""], ["Li", "Baopu", ""], ["Yang", "Shuhui", ""], ["Sun", "Jing", ""], ["Ding", "Zhengming", ""], ["Chen", "Junyang", ""], ["Dong", "Xiao", ""], ["Wang", "Zhihui", ""], ["Li", "Haojie", ""]]}, {"id": "2101.09986", "submitter": "Yurim Lee", "authors": "Yurim Lee, Eunji Jun, Heung-Il Suk", "title": "Multi-view Integration Learning for Irregularly-sampled Clinical Time\n  Series", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Electronic health record (EHR) data is sparse and irregular as it is recorded\nat irregular time intervals, and different clinical variables are measured at\neach observation point. In this work, we propose a multi-view features\nintegration learning from irregular multivariate time series data by\nself-attention mechanism in an imputation-free manner. Specifically, we devise\na novel multi-integration attention module (MIAM) to extract complex\ninformation inherent in irregular time series data. In particular, we\nexplicitly learn the relationships among the observed values, missing\nindicators, and time interval between the consecutive observations,\nsimultaneously. The rationale behind our approach is the use of human knowledge\nsuch as what to measure and when to measure in different situations, which are\nindirectly represented in the data. In addition, we build an attention-based\ndecoder as a missing value imputer that helps empower the representation\nlearning of the inter-relations among multi-view observations for the\nprediction task, which operates at the training phase only. We validated the\neffectiveness of our method over the public MIMIC-III and PhysioNet challenge\n2012 datasets by comparing with and outperforming the state-of-the-art methods\nfor in-hospital mortality prediction.\n", "versions": [{"version": "v1", "created": "Mon, 25 Jan 2021 10:02:50 GMT"}, {"version": "v2", "created": "Tue, 26 Jan 2021 03:25:12 GMT"}], "update_date": "2021-01-27", "authors_parsed": [["Lee", "Yurim", ""], ["Jun", "Eunji", ""], ["Suk", "Heung-Il", ""]]}, {"id": "2101.09987", "submitter": "Erkan Bostanci", "authors": "K. E. Sengun, Y. T. Cetin, M.S Guzel, S. Can and E. Bostanci", "title": "Automatic Liver Segmentation from CT Images Using Deep Learning\n  Algorithms: A Comparative Study", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Medical imaging has been employed to support medical diagnosis and treatment.\nIt may also provide crucial information to surgeons to facilitate optimal\nsurgical preplanning and perioperative management. Essentially, semi-automatic\norgan and tumor segmentation has been studied by many researchers. Recently,\nwith the development of Deep Learning (DL) algorithms, automatic organ\nsegmentation has been gathered lots of attention from the researchers. This\npaper addresses to propose the most efficient DL architectures for Liver\nsegmentation by adapting and comparing state-of-the-art DL frameworks, studied\nin different disciplines. These frameworks are implemented and adapted into a\nCommercial software, 'LiverVision'. It is aimed to reveal the most effective\nand accurate DL architecture for fully automatic liver segmentation. Equal\nconditions were provided to all architectures in the experiments so as to\nmeasure the effectiveness of algorithms accuracy, and Dice coefficient metrics\nwere also employed to support comparative analysis. Experimental results prove\nthat 'U-Net' and 'SegNet' have been superior in line with the experiments\nconducted considering the concepts of time, cost, and effectiveness.\nConsidering both architectures, 'SegNet' was observed to be more successful in\neliminating false-positive values. Besides, it was seen that the accuracy\nmetric used to measure effectiveness in image segmentation alone was not\nenough. Results reveal that DL algorithms are able to automate organ\nsegmentation from DICOM images with high accuracy. This contribution is\ncritical for surgical preplanning and motivates author to apply this approach\nto the different organs and field of medicine.\n", "versions": [{"version": "v1", "created": "Mon, 25 Jan 2021 10:05:46 GMT"}], "update_date": "2021-01-26", "authors_parsed": [["Sengun", "K. E.", ""], ["Cetin", "Y. T.", ""], ["Guzel", "M. S", ""], ["Can", "S.", ""], ["Bostanci", "E.", ""]]}, {"id": "2101.09991", "submitter": "Carlo Alberto Barbano", "authors": "Carlo Alberto Barbano, Daniele Perlo, Enzo Tartaglione, Attilio\n  Fiandrotti, Luca Bertero, Paola Cassoni, Marco Grangetto", "title": "UniToPatho, a labeled histopathological dataset for colorectal polyps\n  classification and adenoma dysplasia grading", "comments": "5 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Histopathological characterization of colorectal polyps allows to tailor\npatients' management and follow up with the ultimate aim of avoiding or\npromptly detecting an invasive carcinoma. Colorectal polyps characterization\nrelies on the histological analysis of tissue samples to determine the polyps\nmalignancy and dysplasia grade. Deep neural networks achieve outstanding\naccuracy in medical patterns recognition, however they require large sets of\nannotated training images. We introduce UniToPatho, an annotated dataset of\n9536 hematoxylin and eosin (H&E) stained patches extracted from 292 whole-slide\nimages, meant for training deep neural networks for colorectal polyps\nclassification and adenomas grading. We present our dataset and provide\ninsights on how to tackle the problem of automatic colorectal polyps\ncharacterization.\n", "versions": [{"version": "v1", "created": "Mon, 25 Jan 2021 10:18:44 GMT"}, {"version": "v2", "created": "Wed, 10 Feb 2021 09:23:19 GMT"}], "update_date": "2021-02-11", "authors_parsed": [["Barbano", "Carlo Alberto", ""], ["Perlo", "Daniele", ""], ["Tartaglione", "Enzo", ""], ["Fiandrotti", "Attilio", ""], ["Bertero", "Luca", ""], ["Cassoni", "Paola", ""], ["Grangetto", "Marco", ""]]}, {"id": "2101.09994", "submitter": "Luca Corinzia", "authors": "Luca Corinzia, Paolo Penna, Wojciech Szpankowski, Joachim M. Buhmann", "title": "On maximum-likelihood estimation in the all-or-nothing regime", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.AI cs.LG math.IT", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We study the problem of estimating a rank-1 additive deformation of a\nGaussian tensor according to the \\emph{maximum-likelihood estimator} (MLE). The\nanalysis is carried out in the sparse setting, where the underlying signal has\na support that scales sublinearly with the total number of dimensions. We show\nthat for Bernoulli distributed signals, the MLE undergoes an\n\\emph{all-or-nothing} (AoN) phase transition, already established for the\nminimum mean-square-error estimator (MMSE) in the same problem. The result\nfollows from two main technical points: (i) the connection established between\nthe MLE and the MMSE, using the first and second-moment methods in the\nconstrained signal space, (ii) a recovery regime for the MMSE stricter than the\nsimple error vanishing characterization given in the standard AoN, that is here\nproved as a general result.\n", "versions": [{"version": "v1", "created": "Mon, 25 Jan 2021 10:20:36 GMT"}], "update_date": "2021-01-26", "authors_parsed": [["Corinzia", "Luca", ""], ["Penna", "Paolo", ""], ["Szpankowski", "Wojciech", ""], ["Buhmann", "Joachim M.", ""]]}, {"id": "2101.09995", "submitter": "Vinodkumar Prabhakaran", "authors": "Nithya Sambasivan, Erin Arnesen, Ben Hutchinson, Tulsee Doshi,\n  Vinodkumar Prabhakaran", "title": "Re-imagining Algorithmic Fairness in India and Beyond", "comments": null, "journal-ref": "Proceedings of the 2021 conference on Fairness, Accountability,\n  and Transparency", "doi": null, "report-no": null, "categories": "cs.CY cs.AI cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Conventional algorithmic fairness is West-centric, as seen in its sub-groups,\nvalues, and methods. In this paper, we de-center algorithmic fairness and\nanalyse AI power in India. Based on 36 qualitative interviews and a discourse\nanalysis of algorithmic deployments in India, we find that several assumptions\nof algorithmic fairness are challenged. We find that in India, data is not\nalways reliable due to socio-economic factors, ML makers appear to follow\ndouble standards, and AI evokes unquestioning aspiration. We contend that\nlocalising model fairness alone can be window dressing in India, where the\ndistance between models and oppressed communities is large. Instead, we\nre-imagine algorithmic fairness in India and provide a roadmap to\nre-contextualise data and models, empower oppressed communities, and enable\nFair-ML ecosystems.\n", "versions": [{"version": "v1", "created": "Mon, 25 Jan 2021 10:20:57 GMT"}, {"version": "v2", "created": "Wed, 27 Jan 2021 02:30:20 GMT"}], "update_date": "2021-01-28", "authors_parsed": [["Sambasivan", "Nithya", ""], ["Arnesen", "Erin", ""], ["Hutchinson", "Ben", ""], ["Doshi", "Tulsee", ""], ["Prabhakaran", "Vinodkumar", ""]]}, {"id": "2101.10001", "submitter": "Xudong Han", "authors": "Xudong Han, Timothy Baldwin, Trevor Cohn", "title": "Diverse Adversaries for Mitigating Bias in Training", "comments": "EACL 2021 (5 pages + 1 references)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Adversarial learning can learn fairer and less biased models of language than\nstandard methods. However, current adversarial techniques only partially\nmitigate model bias, added to which their training procedures are often\nunstable. In this paper, we propose a novel approach to adversarial learning\nbased on the use of multiple diverse discriminators, whereby discriminators are\nencouraged to learn orthogonal hidden representations from one another.\nExperimental results show that our method substantially improves over standard\nadversarial removal methods, in terms of reducing bias and the stability of\ntraining.\n", "versions": [{"version": "v1", "created": "Mon, 25 Jan 2021 10:35:13 GMT"}], "update_date": "2021-01-26", "authors_parsed": [["Han", "Xudong", ""], ["Baldwin", "Timothy", ""], ["Cohn", "Trevor", ""]]}, {"id": "2101.10007", "submitter": "Konstantinos Gatsis", "authors": "Konstantinos Gatsis", "title": "Adaptive Scheduling for Machine Learning Tasks over Networks", "comments": "Accepted at 2021 American Control Conference (ACC)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.SY eess.SY math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A key functionality of emerging connected autonomous systems such as smart\ntransportation systems, smart cities, and the industrial Internet-of-Things, is\nthe ability to process and learn from data collected at different physical\nlocations. This is increasingly attracting attention under the terms of\ndistributed learning and federated learning. However, in this setup data\ntransfer takes place over communication resources that are shared among many\nusers and tasks or subject to capacity constraints. This paper examines\nalgorithms for efficiently allocating resources to linear regression tasks by\nexploiting the informativeness of the data. The algorithms developed enable\nadaptive scheduling of learning tasks with reliable performance guarantees.\n", "versions": [{"version": "v1", "created": "Mon, 25 Jan 2021 10:59:00 GMT"}], "update_date": "2021-01-26", "authors_parsed": [["Gatsis", "Konstantinos", ""]]}, {"id": "2101.10015", "submitter": "Kai Han", "authors": "Yunhe Wang, Mingqiang Huang, Kai Han, Hanting Chen, Wei Zhang,\n  Chunjing Xu, Dacheng Tao", "title": "AdderNet and its Minimalist Hardware Design for Energy-Efficient\n  Artificial Intelligence", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AR cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolutional neural networks (CNN) have been widely used for boosting the\nperformance of many machine intelligence tasks. However, the CNN models are\nusually computationally intensive and energy consuming, since they are often\ndesigned with numerous multiply-operations and considerable parameters for the\naccuracy reason. Thus, it is difficult to directly apply them in the\nresource-constrained environments such as 'Internet of Things' (IoT) devices\nand smart phones. To reduce the computational complexity and energy burden,\nhere we present a novel minimalist hardware architecture using adder\nconvolutional neural network (AdderNet), in which the original convolution is\nreplaced by adder kernel using only additions. To maximally excavate the\npotential energy consumption, we explore the low-bit quantization algorithm for\nAdderNet with shared-scaling-factor method, and we design both specific and\ngeneral-purpose hardware accelerators for AdderNet. Experimental results show\nthat the adder kernel with int8/int16 quantization also exhibits high\nperformance, meanwhile consuming much less resources (theoretically ~81% off).\nIn addition, we deploy the quantized AdderNet on FPGA (Field Programmable Gate\nArray) platform. The whole AdderNet can practically achieve 16% enhancement in\nspeed, 67.6%-71.4% decrease in logic resource utilization and 47.85%-77.9%\ndecrease in power consumption compared to CNN under the same circuit\narchitecture. With a comprehensive comparison on the performance, power\nconsumption, hardware resource consumption and network generalization\ncapability, we conclude the AdderNet is able to surpass all the other\ncompetitors including the classical CNN, novel memristor-network, XNOR-Net and\nthe shift-kernel based network, indicating its great potential in future high\nperformance and energy-efficient artificial intelligence applications.\n", "versions": [{"version": "v1", "created": "Mon, 25 Jan 2021 11:31:52 GMT"}, {"version": "v2", "created": "Wed, 3 Feb 2021 06:48:54 GMT"}], "update_date": "2021-02-04", "authors_parsed": [["Wang", "Yunhe", ""], ["Huang", "Mingqiang", ""], ["Han", "Kai", ""], ["Chen", "Hanting", ""], ["Zhang", "Wei", ""], ["Xu", "Chunjing", ""], ["Tao", "Dacheng", ""]]}, {"id": "2101.10025", "submitter": "Wenlong Liao", "authors": "Wenlong Liao, Birgitte Bak-Jensen, Jayakrishnan Radhakrishna Pillai,\n  Yuelong Wang, and Yusen Wang", "title": "A Review of Graph Neural Networks and Their Applications in Power\n  Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.SY eess.SY", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Deep neural networks have revolutionized many machine learning tasks in power\nsystems, ranging from pattern recognition to signal processing. The data in\nthese tasks is typically represented in Euclidean domains. Nevertheless, there\nis an increasing number of applications in power systems, where data are\ncollected from non-Euclidean domains and represented as graph-structured data\nwith high dimensional features and interdependency among nodes. The complexity\nof graph-structured data has brought significant challenges to the existing\ndeep neural networks defined in Euclidean domains. Recently, many publications\ngeneralizing deep neural networks for graph-structured data in power systems\nhave emerged. In this paper, a comprehensive overview of graph neural networks\n(GNNs) in power systems is proposed. Specifically, several classical paradigms\nof GNNs structures (e.g., graph convolutional networks) are summarized, and key\napplications in power systems, such as fault scenario application, time series\nprediction, power flow calculation, and data generation are reviewed in detail.\nFurthermore, main issues and some research trends about the applications of\nGNNs in power systems are discussed.\n", "versions": [{"version": "v1", "created": "Mon, 25 Jan 2021 11:50:45 GMT"}, {"version": "v2", "created": "Sat, 12 Jun 2021 11:38:48 GMT"}], "update_date": "2021-06-15", "authors_parsed": [["Liao", "Wenlong", ""], ["Bak-Jensen", "Birgitte", ""], ["Pillai", "Jayakrishnan Radhakrishna", ""], ["Wang", "Yuelong", ""], ["Wang", "Yusen", ""]]}, {"id": "2101.10027", "submitter": "Tuan Anh Bui", "authors": "Anh Bui, Trung Le, He Zhao, Paul Montague, Seyit Camtepe, Dinh Phung", "title": "Understanding and Achieving Efficient Robustness with Adversarial\n  Supervised Contrastive Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Contrastive learning (CL) has recently emerged as an effective approach to\nlearning representation in a range of downstream tasks. Central to this\napproach is the selection of positive (similar) and negative (dissimilar) sets\nto provide the model the opportunity to `contrast' between data and class\nrepresentation in the latent space. In this paper, we investigate CL for\nimproving model robustness using adversarial samples. We first designed and\nperformed a comprehensive study to understand how adversarial vulnerability\nbehaves in the latent space. Based on these empirical evidences, we propose an\neffective and efficient supervised contrastive learning to achieve model\nrobustness against adversarial attacks. Moreover, we propose a new sample\nselection strategy that optimizes the positive/negative sets by removing\nredundancy and improving correlation with the anchor. Experiments conducted on\nbenchmark datasets show that our Adversarial Supervised Contrastive Learning\n(ASCL) approach outperforms the state-of-the-art defenses by $2.6\\%$ in terms\nof the robust accuracy, whilst our ASCL with the proposed selection strategy\ncan further gain $1.4\\%$ improvement with only $42.8\\%$ positives and $6.3\\%$\nnegatives compared with ASCL without a selection strategy.\n", "versions": [{"version": "v1", "created": "Mon, 25 Jan 2021 11:57:52 GMT"}, {"version": "v2", "created": "Wed, 31 Mar 2021 03:46:14 GMT"}], "update_date": "2021-04-01", "authors_parsed": [["Bui", "Anh", ""], ["Le", "Trung", ""], ["Zhao", "He", ""], ["Montague", "Paul", ""], ["Camtepe", "Seyit", ""], ["Phung", "Dinh", ""]]}, {"id": "2101.10033", "submitter": "Florian Jug", "authors": "Manan Lalit, Pavel Tomancak, Florian Jug", "title": "Embedding-based Instance Segmentation in Microscopy", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Automatic detection and segmentation of objects in 2D and 3D microscopy data\nis important for countless biomedical applications. In the natural image\ndomain, spatial embedding-based instance segmentation methods are known to\nyield high-quality results, but their utility for segmenting microscopy data is\ncurrently little researched. Here we introduce EmbedSeg, an embedding-based\ninstance segmentation method which outperforms existing state-of-the-art\nbaselines on 2D as well as 3D microscopy datasets. Additionally, we show that\nEmbedSeg has a GPU memory footprint small enough to train even on laptop GPUs,\nmaking it accessible to virtually everyone. Finally, we introduce four new 3D\nmicroscopy datasets, which we make publicly available alongside ground truth\ntraining labels. Our open-source implementation is available at\nhttps://github.com/juglab/EmbedSeg.\n", "versions": [{"version": "v1", "created": "Mon, 25 Jan 2021 12:06:44 GMT"}, {"version": "v2", "created": "Thu, 29 Apr 2021 11:13:47 GMT"}], "update_date": "2021-04-30", "authors_parsed": [["Lalit", "Manan", ""], ["Tomancak", "Pavel", ""], ["Jug", "Florian", ""]]}, {"id": "2101.10037", "submitter": "Kevin Styp-Rekowski", "authors": "Kevin Styp-Rekowski, Florian Schmidt, Odej Kao", "title": "Optimizing Convergence for Iterative Learning of ARIMA for Stationary\n  Time Series", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Forecasting of time series in continuous systems becomes an increasingly\nrelevant task due to recent developments in IoT and 5G. The popular forecasting\nmodel ARIMA is applied to a large variety of applications for decades. An\nonline variant of ARIMA applies the Online Newton Step in order to learn the\nunderlying process of the time series. This optimization method has pitfalls\nconcerning the computational complexity and convergence. Thus, this work\nfocuses on the computational less expensive Online Gradient Descent\noptimization method, which became popular for learning of neural networks in\nrecent years. For the iterative training of such models, we propose a new\napproach combining different Online Gradient Descent learners (such as Adam,\nAMSGrad, Adagrad, Nesterov) to achieve fast convergence. The evaluation on\nsynthetic data and experimental datasets show that the proposed approach\noutperforms the existing methods resulting in an overall lower prediction\nerror.\n", "versions": [{"version": "v1", "created": "Mon, 25 Jan 2021 12:07:46 GMT"}], "update_date": "2021-01-26", "authors_parsed": [["Styp-Rekowski", "Kevin", ""], ["Schmidt", "Florian", ""], ["Kao", "Odej", ""]]}, {"id": "2101.10040", "submitter": "Cyrille W. Combettes", "authors": "Cyrille W. Combettes and Sebastian Pokutta", "title": "Complexity of Linear Minimization and Projection on Some Sets", "comments": "14 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.DS cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Frank-Wolfe algorithm is a method for constrained optimization that\nrelies on linear minimizations, as opposed to projections. Therefore, a\nmotivation put forward in a large body of work on the Frank-Wolfe algorithm is\nthe computational advantage of solving linear minimizations instead of\nprojections. However, the discussions supporting this advantage are often too\nsuccinct or incomplete. In this paper, we review the complexity bounds for both\ntasks on several sets commonly used in optimization. Projection methods onto\nthe $\\ell_p$-ball, $p\\in\\left]1,2\\right[\\cup\\left]2,+\\infty\\right[$, and the\nBirkhoff polytope are also proposed.\n", "versions": [{"version": "v1", "created": "Mon, 25 Jan 2021 12:14:34 GMT"}, {"version": "v2", "created": "Sat, 12 Jun 2021 17:08:10 GMT"}], "update_date": "2021-06-15", "authors_parsed": [["Combettes", "Cyrille W.", ""], ["Pokutta", "Sebastian", ""]]}, {"id": "2101.10041", "submitter": "Siamak Mehrkanoon", "authors": "Tomasz Sta\\'nczyk and Siamak Mehrkanoon", "title": "Deep Graph Convolutional Networks for Wind Speed Prediction", "comments": "10 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Wind speed prediction and forecasting is important for various business and\nmanagement sectors. In this paper, we introduce new models for wind speed\nprediction based on graph convolutional networks (GCNs). Given hourly data of\nseveral weather variables acquired from multiple weather stations, wind speed\nvalues are predicted for multiple time steps ahead. In particular, the weather\nstations are treated as nodes of a graph whose associated adjacency matrix is\nlearnable. In this way, the network learns the graph spatial structure and\ndetermines the strength of relations between the weather stations based on the\nhistorical weather data. We add a self-loop connection to the learnt adjacency\nmatrix and normalize the adjacency matrix. We examine two scenarios with the\nself-loop connection setting (two separate models). In the first scenario, the\nself-loop connection is imposed as a constant additive. In the second scenario\na learnable parameter is included to enable the network to decide about the\nself-loop connection strength. Furthermore, we incorporate data from multiple\ntime steps with temporal convolution, which together with spatial graph\nconvolution constitutes spatio-temporal graph convolution. We perform\nexperiments on real datasets collected from weather stations located in cities\nin Denmark and the Netherlands. The numerical experiments show that our\nproposed models outperform previously developed baseline models on the\nreferenced datasets. We provide additional insights by visualizing learnt\nadjacency matrices from each layer of our models.\n", "versions": [{"version": "v1", "created": "Mon, 25 Jan 2021 12:22:09 GMT"}], "update_date": "2021-01-26", "authors_parsed": [["Sta\u0144czyk", "Tomasz", ""], ["Mehrkanoon", "Siamak", ""]]}, {"id": "2101.10050", "submitter": "George Dasoulas", "authors": "George Dasoulas, Johannes Lutzeyer, Michalis Vazirgiannis", "title": "Learning Parametrised Graph Shift Operators", "comments": "17 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In many domains data is currently represented as graphs and therefore, the\ngraph representation of this data becomes increasingly important in machine\nlearning. Network data is, implicitly or explicitly, always represented using a\ngraph shift operator (GSO) with the most common choices being the adjacency,\nLaplacian matrices and their normalisations. In this paper, a novel\nparametrised GSO (PGSO) is proposed, where specific parameter values result in\nthe most commonly used GSOs and message-passing operators in graph neural\nnetwork (GNN) frameworks. The PGSO is suggested as a replacement of the\nstandard GSOs that are used in state-of-the-art GNN architectures and the\noptimisation of the PGSO parameters is seamlessly included in the model\ntraining. It is proved that the PGSO has real eigenvalues and a set of real\neigenvectors independent of the parameter values and spectral bounds on the\nPGSO are derived. PGSO parameters are shown to adapt to the sparsity of the\ngraph structure in a study on stochastic blockmodel networks, where they are\nfound to automatically replicate the GSO regularisation found in the\nliterature. On several real-world datasets the accuracy of state-of-the-art GNN\narchitectures is improved by the inclusion of the PGSO in both node- and\ngraph-classification tasks.\n", "versions": [{"version": "v1", "created": "Mon, 25 Jan 2021 13:01:26 GMT"}, {"version": "v2", "created": "Tue, 27 Apr 2021 12:14:19 GMT"}], "update_date": "2021-04-28", "authors_parsed": [["Dasoulas", "George", ""], ["Lutzeyer", "Johannes", ""], ["Vazirgiannis", "Michalis", ""]]}, {"id": "2101.10074", "submitter": "Setareh Maghsudi", "authors": "Setareh Maghsudi, Andrew Lan, Jie Xu, and Mihaela van der Schaar", "title": "Personalized Education in the AI Era: What to Expect Next?", "comments": null, "journal-ref": null, "doi": "10.1109/MSP.2021.3055032", "report-no": null, "categories": "cs.CY cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The objective of personalized learning is to design an effective knowledge\nacquisition track that matches the learner's strengths and bypasses her\nweaknesses to ultimately meet her desired goal. This concept emerged several\nyears ago and is being adopted by a rapidly-growing number of educational\ninstitutions around the globe. In recent years, the boost of artificial\nintelligence (AI) and machine learning (ML), together with the advances in big\ndata analysis, has unfolded novel perspectives to enhance personalized\neducation in numerous dimensions. By taking advantage of AI/ML methods, the\neducational platform precisely acquires the student's characteristics. This is\ndone, in part, by observing the past experiences as well as analyzing the\navailable big data through exploring the learners' features and similarities.\nIt can, for example, recommend the most appropriate content among numerous\naccessible ones, advise a well-designed long-term curriculum, connect\nappropriate learners by suggestion, accurate performance evaluation, and the\nlike. Still, several aspects of AI-based personalized education remain\nunexplored. These include, among others, compensating for the adverse effects\nof the absence of peers, creating and maintaining motivations for learning,\nincreasing diversity, removing the biases induced by the data and algorithms,\nand the like. In this paper, while providing a brief review of state-of-the-art\nresearch, we investigate the challenges of AI/ML-based personalized education\nand discuss potential solutions.\n", "versions": [{"version": "v1", "created": "Tue, 19 Jan 2021 12:23:32 GMT"}], "update_date": "2021-02-16", "authors_parsed": [["Maghsudi", "Setareh", ""], ["Lan", "Andrew", ""], ["Xu", "Jie", ""], ["van der Schaar", "Mihaela", ""]]}, {"id": "2101.10087", "submitter": "Will Crichton", "authors": "Will Crichton, Georgia Gabriela Sampaio, Pat Hanrahan", "title": "Automating Program Structure Classification", "comments": "To appear at SIGCSE 2021", "journal-ref": null, "doi": "10.1145/3408877.3432358", "report-no": null, "categories": "cs.CY cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When students write programs, their program structure provides insight into\ntheir learning process. However, analyzing program structure by hand is\ntime-consuming, and teachers need better tools for computer-assisted\nexploration of student solutions. As a first step towards an education-oriented\nprogram analysis toolkit, we show how supervised machine learning methods can\nautomatically classify student programs into a predetermined set of high-level\nstructures. We evaluate two models on classifying student solutions to the\nRainfall problem: a nearest-neighbors classifier using syntax tree edit\ndistance and a recurrent neural network. We demonstrate that these models can\nachieve 91% classification accuracy when trained on 108 programs. We further\nexplore the generality, trade-offs, and failure cases of each model.\n", "versions": [{"version": "v1", "created": "Fri, 15 Jan 2021 21:24:37 GMT"}], "update_date": "2021-01-26", "authors_parsed": [["Crichton", "Will", ""], ["Sampaio", "Georgia Gabriela", ""], ["Hanrahan", "Pat", ""]]}, {"id": "2101.10102", "submitter": "Renjue Li", "authors": "Renjue Li and Pengfei Yang and Cheng-Chao Huang and Bai Xue and Lijun\n  Zhang", "title": "Probabilistic Robustness Analysis for DNNs based on PAC Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a black box based approach for analysing deep neural\nnetworks (DNNs). We view a DNN as a function $\\boldsymbol{f}$ from inputs to\noutputs, and consider the local robustness property for a given input. Based on\nscenario optimization technique in robust control design, we learn the score\ndifference function $f_i-f_\\ell$ with respect to the target label $\\ell$ and\nattacking label $i$. We use a linear template over the input pixels, and learn\nthe corresponding coefficients of the score difference function, based on a\nreduction to a linear programming (LP) problems. To make it scalable, we\npropose optimizations including components based learning and focused learning.\nThe learned function offers a probably approximately correct (PAC) guarantee\nfor the robustness property. Since the score difference function is an\napproximation of the local behaviour of the DNN, it can be used to generate\npotential adversarial examples, and the original network can be used to check\nwhether they are spurious or not. Finally, we focus on the input pixels with\nlarge absolute coefficients, and use them to explain the attacking scenario. We\nhave implemented our approach in a prototypical tool DeepPAC. Our experimental\nresults show that our framework can handle very large neural networks like\nResNet152 with $6.5$M neurons, and often generates adversarial examples which\nare very close to the decision boundary.\n", "versions": [{"version": "v1", "created": "Mon, 25 Jan 2021 14:10:52 GMT"}], "update_date": "2021-01-26", "authors_parsed": [["Li", "Renjue", ""], ["Yang", "Pengfei", ""], ["Huang", "Cheng-Chao", ""], ["Xue", "Bai", ""], ["Zhang", "Lijun", ""]]}, {"id": "2101.10115", "submitter": "Iosu Rodr\\'iguez-Mart\\'inez", "authors": "Martin Pap\\v{c}o, Iosu Rodr\\'iguez-Mart\\'inez, Javier Fumanal-Idocin,\n  Abdulrahman H. Altalhi and Humberto Bustince", "title": "A fusion method for multi-valued data", "comments": null, "journal-ref": "Information Fusion, Volume 71, 2021, Pages 1-10", "doi": "10.1016/j.inffus.2021.01.001", "report-no": null, "categories": "cs.LG cs.AI cs.NE", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  In this paper we propose an extension of the notion of deviation-based\naggregation function tailored to aggregate multidimensional data. Our objective\nis both to improve the results obtained by other methods that try to select the\nbest aggregation function for a particular set of data, such as penalty\nfunctions, and to reduce the temporal complexity required by such approaches.\nWe discuss how this notion can be defined and present three illustrative\nexamples of the applicability of our new proposal in areas where temporal\nconstraints can be strict, such as image processing, deep learning and decision\nmaking, obtaining favourable results in the process.\n", "versions": [{"version": "v1", "created": "Mon, 25 Jan 2021 14:27:21 GMT"}], "update_date": "2021-01-26", "authors_parsed": [["Pap\u010do", "Martin", ""], ["Rodr\u00edguez-Mart\u00ednez", "Iosu", ""], ["Fumanal-Idocin", "Javier", ""], ["Altalhi", "Abdulrahman H.", ""], ["Bustince", "Humberto", ""]]}, {"id": "2101.10121", "submitter": "Mu Zhu", "authors": "Mu Zhu, Ahmed H. Anwar, Zelin Wan, Jin-Hee Cho, Charles Kamhoua, and\n  Munindar P. Singh", "title": "Game-Theoretic and Machine Learning-based Approaches for Defensive\n  Deception: A Survey", "comments": "37 pages, 184 citations", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.GT cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Defensive deception is a promising approach for cyber defense. Via defensive\ndeception, the defender can anticipate attacker actions; it can mislead or lure\nattacker, or hide real resources. Although defensive deception is increasingly\npopular in the research community, there has not been a systematic\ninvestigation of its key components, the underlying principles, and its\ntradeoffs in various problem settings. This survey paper focuses on defensive\ndeception research centered on game theory and machine learning, since these\nare prominent families of artificial intelligence approaches that are widely\nemployed in defensive deception. This paper brings forth insights, lessons, and\nlimitations from prior work. It closes with an outline of some research\ndirections to tackle major gaps in current defensive deception research.\n", "versions": [{"version": "v1", "created": "Thu, 21 Jan 2021 21:55:43 GMT"}, {"version": "v2", "created": "Sat, 8 May 2021 18:57:26 GMT"}], "update_date": "2021-05-11", "authors_parsed": [["Zhu", "Mu", ""], ["Anwar", "Ahmed H.", ""], ["Wan", "Zelin", ""], ["Cho", "Jin-Hee", ""], ["Kamhoua", "Charles", ""], ["Singh", "Munindar P.", ""]]}, {"id": "2101.10123", "submitter": "Janis Klaise", "authors": "Arnaud Van Looveren, Janis Klaise, Giovanni Vacanti, Oliver Cobb", "title": "Conditional Generative Models for Counterfactual Explanations", "comments": "12 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Counterfactual instances offer human-interpretable insight into the local\nbehaviour of machine learning models. We propose a general framework to\ngenerate sparse, in-distribution counterfactual model explanations which match\na desired target prediction with a conditional generative model, allowing\nbatches of counterfactual instances to be generated with a single forward pass.\nThe method is flexible with respect to the type of generative model used as\nwell as the task of the underlying predictive model. This allows\nstraightforward application of the framework to different modalities such as\nimages, time series or tabular data as well as generative model paradigms such\nas GANs or autoencoders and predictive tasks like classification or regression.\nWe illustrate the effectiveness of our method on image (CelebA), time series\n(ECG) and mixed-type tabular (Adult Census) data.\n", "versions": [{"version": "v1", "created": "Mon, 25 Jan 2021 14:31:13 GMT"}], "update_date": "2021-01-26", "authors_parsed": [["Van Looveren", "Arnaud", ""], ["Klaise", "Janis", ""], ["Vacanti", "Giovanni", ""], ["Cobb", "Oliver", ""]]}, {"id": "2101.10141", "submitter": "George Papakostas Prof.", "authors": "K.D. Apostolidis, P.S. Amanatidis, G.A. Papakostas", "title": "Performance Evaluation of Convolutional Neural Networks for Gait\n  Recognition", "comments": "6 pages, 15 figures, to be published in proceedings of the 24th\n  Pan-Hellenic Conference on Informatics (PCI)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  In this paper, a performance evaluation of well-known deep learning models in\ngait recognition is presented. For this purpose, the transfer learning scheme\nis adopted to pre-trained models in order to fit the models to the CASIA-B\ndataset for solving a gait recognition task. In this context, 18 popular\nConvolutional Neural Networks (CNNs), were re-trained using Gait Energy Images\n(GEIs) of CASIA-B containing almost 14000 images of 124 classes under various\nconditions, and their performance was studied in terms of accuracy. Moreover,\nthe performance of the studied models is managed to be explained by examining\nthe parts of the images being considered by the models towards providing their\ndecisions. The experimental results are very promising since almost all the\nmodels achieved a high accuracy of over 90%, which is robust to the increasing\nnumber of classes. Furthermore, an important outcome of this study is the fact\nthat a recognition problem can be effectively solved by using CNNs pre-trained\nto different problems, thus eliminating the need for customized model design.\n", "versions": [{"version": "v1", "created": "Mon, 25 Jan 2021 14:44:05 GMT"}], "update_date": "2021-01-26", "authors_parsed": [["Apostolidis", "K. D.", ""], ["Amanatidis", "P. S.", ""], ["Papakostas", "G. A.", ""]]}, {"id": "2101.10143", "submitter": "Nergis Tomen", "authors": "Nergis Tomen, Jan van Gemert", "title": "Spectral Leakage and Rethinking the Kernel Size in CNNs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolutional layers in CNNs implement linear filters which decompose the\ninput into different frequency bands. However, most modern architectures\nneglect standard principles of filter design when optimizing their model\nchoices regarding the size and shape of the convolutional kernel. In this work,\nwe consider the well-known problem of spectral leakage caused by windowing\nartifacts in filtering operations in the context of CNNs. We show that the\nsmall size of CNN kernels make them susceptible to spectral leakage, which may\ninduce performance-degrading artifacts. To address this issue, we propose the\nuse of larger kernel sizes along with the Hamming window function to alleviate\nleakage in CNN architectures. We demonstrate improved classification accuracy\non multiple benchmark datasets including Fashion-MNIST, CIFAR-10, CIFAR-100 and\nImageNet with the simple use of a standard window function in convolutional\nlayers. Finally, we show that CNNs employing the Hamming window display\nincreased robustness against various adversarial attacks.\n", "versions": [{"version": "v1", "created": "Mon, 25 Jan 2021 14:49:29 GMT"}, {"version": "v2", "created": "Thu, 29 Jul 2021 10:30:21 GMT"}], "update_date": "2021-07-30", "authors_parsed": [["Tomen", "Nergis", ""], ["van Gemert", "Jan", ""]]}, {"id": "2101.10150", "submitter": "Runcong Zhao", "authors": "Runcong Zhao and Lin Gui and Gabriele Pergola and Yulan He", "title": "Adversarial Learning of Poisson Factorisation Model for Gauging Brand\n  Sentiment in User Reviews", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  In this paper, we propose the Brand-Topic Model (BTM) which aims to detect\nbrand-associated polarity-bearing topics from product reviews. Different from\nexisting models for sentiment-topic extraction which assume topics are grouped\nunder discrete sentiment categories such as `positive', `negative' and\n`neural', BTM is able to automatically infer real-valued brand-associated\nsentiment scores and generate fine-grained sentiment-topics in which we can\nobserve continuous changes of words under a certain topic (e.g., `shaver' or\n`cream') while its associated sentiment gradually varies from negative to\npositive. BTM is built on the Poisson factorisation model with the\nincorporation of adversarial learning. It has been evaluated on a dataset\nconstructed from Amazon reviews. Experimental results show that BTM outperforms\na number of competitive baselines in brand ranking, achieving a better balance\nof topic coherence and uniqueness, and extracting better-separated\npolarity-bearing topics.\n", "versions": [{"version": "v1", "created": "Mon, 25 Jan 2021 14:58:17 GMT"}], "update_date": "2021-01-26", "authors_parsed": [["Zhao", "Runcong", ""], ["Gui", "Lin", ""], ["Pergola", "Gabriele", ""], ["He", "Yulan", ""]]}, {"id": "2101.10154", "submitter": "Mohamed Hibat-Allah", "authors": "Mohamed Hibat-Allah, Estelle M. Inack, Roeland Wiersema, Roger G.\n  Melko, Juan Carrasquilla", "title": "Variational Neural Annealing", "comments": "19 pages, 9 figures, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "cond-mat.dis-nn cond-mat.stat-mech cs.LG quant-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many important challenges in science and technology can be cast as\noptimization problems. When viewed in a statistical physics framework, these\ncan be tackled by simulated annealing, where a gradual cooling procedure helps\nsearch for groundstate solutions of a target Hamiltonian. While powerful,\nsimulated annealing is known to have prohibitively slow sampling dynamics when\nthe optimization landscape is rough or glassy. Here we show that by\ngeneralizing the target distribution with a parameterized model, an analogous\nannealing framework based on the variational principle can be used to search\nfor groundstate solutions. Modern autoregressive models such as recurrent\nneural networks provide ideal parameterizations since they can be exactly\nsampled without slow dynamics even when the model encodes a rough landscape. We\nimplement this procedure in the classical and quantum settings on several\nprototypical spin glass Hamiltonians, and find that it significantly\noutperforms traditional simulated annealing in the asymptotic limit,\nillustrating the potential power of this yet unexplored route to optimization.\n", "versions": [{"version": "v1", "created": "Mon, 25 Jan 2021 15:06:42 GMT"}], "update_date": "2021-01-26", "authors_parsed": [["Hibat-Allah", "Mohamed", ""], ["Inack", "Estelle M.", ""], ["Wiersema", "Roeland", ""], ["Melko", "Roger G.", ""], ["Carrasquilla", "Juan", ""]]}, {"id": "2101.10160", "submitter": "Shujian Yu", "authors": "Shujian Yu, Francesco Alesiani, Xi Yu, Robert Jenssen, Jose C.\n  Principe", "title": "Measuring Dependence with Matrix-based Entropy Functional", "comments": "Accepted at AAAI-21. An interpretable and differentiable dependence\n  (or independence) measure that can be used to 1) train deep network under\n  covariate shift and non-Gaussian noise; 2) implement a deep deterministic\n  information bottleneck; and 3) understand the dynamics of learning of CNN.\n  Code available at https://bit.ly/AAAI-dependence", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.IT math.IT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Measuring the dependence of data plays a central role in statistics and\nmachine learning. In this work, we summarize and generalize the main idea of\nexisting information-theoretic dependence measures into a higher-level\nperspective by the Shearer's inequality. Based on our generalization, we then\npropose two measures, namely the matrix-based normalized total correlation\n($T_\\alpha^*$) and the matrix-based normalized dual total correlation\n($D_\\alpha^*$), to quantify the dependence of multiple variables in arbitrary\ndimensional space, without explicit estimation of the underlying data\ndistributions. We show that our measures are differentiable and statistically\nmore powerful than prevalent ones. We also show the impact of our measures in\nfour different machine learning problems, namely the gene regulatory network\ninference, the robust machine learning under covariate shift and non-Gaussian\nnoises, the subspace outlier detection, and the understanding of the learning\ndynamics of convolutional neural networks (CNNs), to demonstrate their\nutilities, advantages, as well as implications to those problems. Code of our\ndependence measure is available at: https://bit.ly/AAAI-dependence\n", "versions": [{"version": "v1", "created": "Mon, 25 Jan 2021 15:18:16 GMT"}], "update_date": "2021-01-26", "authors_parsed": [["Yu", "Shujian", ""], ["Alesiani", "Francesco", ""], ["Yu", "Xi", ""], ["Jenssen", "Robert", ""], ["Principe", "Jose C.", ""]]}, {"id": "2101.10181", "submitter": "Yongxin Liu", "authors": "Yongxin Liu, Jian Wang, Jianqiang Li, Shuteng Niu, Houbing Song", "title": "Machine Learning for the Detection and Identification of Internet of\n  Things (IoT) Devices: A Survey", "comments": "This paper is currently under revision", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The Internet of Things (IoT) is becoming an indispensable part of everyday\nlife, enabling a variety of emerging services and applications. However, the\npresence of rogue IoT devices has exposed the IoT to untold risks with severe\nconsequences. The first step in securing the IoT is detecting rogue IoT devices\nand identifying legitimate ones. Conventional approaches use cryptographic\nmechanisms to authenticate and verify legitimate devices' identities. However,\ncryptographic protocols are not available in many systems. Meanwhile, these\nmethods are less effective when legitimate devices can be exploited or\nencryption keys are disclosed. Therefore, non-cryptographic IoT device\nidentification and rogue device detection become efficient solutions to secure\nexisting systems and will provide additional protection to systems with\ncryptographic protocols. Non-cryptographic approaches require more effort and\nare not yet adequately investigated. In this paper, we provide a comprehensive\nsurvey on machine learning technologies for the identification of IoT devices\nalong with the detection of compromised or falsified ones from the viewpoint of\npassive surveillance agents or network operators. We classify the IoT device\nidentification and detection into four categories: device-specific pattern\nrecognition, Deep Learning enabled device identification, unsupervised device\nidentification, and abnormal device detection. Meanwhile, we discuss various\nML-related enabling technologies for this purpose. These enabling technologies\ninclude learning algorithms, feature engineering on network traffic traces and\nwireless signals, continual learning, and abnormality detection.\n", "versions": [{"version": "v1", "created": "Mon, 25 Jan 2021 15:51:04 GMT"}], "update_date": "2021-01-26", "authors_parsed": [["Liu", "Yongxin", ""], ["Wang", "Jian", ""], ["Li", "Jianqiang", ""], ["Niu", "Shuteng", ""], ["Song", "Houbing", ""]]}, {"id": "2101.10229", "submitter": "Masato Kimura Dr.", "authors": "Yuto Aizawa and Masato Kimura", "title": "Universal Approximation Properties for ODENet and ResNet", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.NA math.CA math.NA stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We prove a universal approximation property (UAP) for a class of ODENet and a\nclass of ResNet, which are used in many deep learning algorithms. The UAP can\nbe stated as follows. Let $n$ and $m$ be the dimension of input and output\ndata, and assume $m\\leq n$. Then we show that ODENet width $n+m$ with any\nnon-polynomial continuous activation function can approximate any continuous\nfunction on a compact subset on $\\mathbb{R}^n$. We also show that ResNet has\nthe same property as the depth tends to infinity. Furthermore, we derive\nexplicitly the gradient of a loss function with respect to a certain tuning\nvariable. We use this to construct a learning algorithm for ODENet. To\ndemonstrate the usefulness of this algorithm, we apply it to a regression\nproblem, a binary classification, and a multinomial classification in MNIST.\n", "versions": [{"version": "v1", "created": "Tue, 22 Dec 2020 06:04:09 GMT"}, {"version": "v2", "created": "Sat, 30 Jan 2021 23:21:38 GMT"}], "update_date": "2021-02-02", "authors_parsed": [["Aizawa", "Yuto", ""], ["Kimura", "Masato", ""]]}, {"id": "2101.10247", "submitter": "Alexander Rodr\\'iguez", "authors": "Alexander Rodr\\'iguez, Bijaya Adhikari, Naren Ramakrishnan, B. Aditya\n  Prakash", "title": "Incorporating Expert Guidance in Epidemic Forecasting", "comments": "Appears in SIGKDD 2020 epiDAMIK", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Forecasting influenza like illnesses (ILI) has rapidly progressed in recent\nyears from an art to a science with a plethora of data-driven methods. While\nthese methods have achieved qualified success, their applicability is limited\ndue to their inability to incorporate expert feedback and guidance\nsystematically into the forecasting framework. We propose a new approach\nleveraging the Seldonian optimization framework from AI safety and demonstrate\nhow it can be adapted to epidemic forecasting. We study two types of guidance:\nsmoothness and regional consistency of errors, where we show that by its\nsuccessful incorporation, we are able to not only bound the probability of\nundesirable behavior to happen, but also to reduce RMSE on test data by up to\n17%.\n", "versions": [{"version": "v1", "created": "Thu, 24 Dec 2020 06:21:53 GMT"}], "update_date": "2021-01-26", "authors_parsed": [["Rodr\u00edguez", "Alexander", ""], ["Adhikari", "Bijaya", ""], ["Ramakrishnan", "Naren", ""], ["Prakash", "B. Aditya", ""]]}, {"id": "2101.10249", "submitter": "Greta Laage", "authors": "Greta Laage, Emma Frejinger, Andrea Lodi and Guillaume Rabusseau", "title": "Assessing the Impact: Does an Improvement to a Revenue Management System\n  Lead to an Improved Revenue?", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI econ.EM", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Airlines and other industries have been making use of sophisticated Revenue\nManagement Systems to maximize revenue for decades. While improving the\ndifferent components of these systems has been the focus of numerous studies,\nestimating the impact of such improvements on the revenue has been overlooked\nin the literature despite its practical importance. Indeed, quantifying the\nbenefit of a change in a system serves as support for investment decisions.\nThis is a challenging problem as it corresponds to the difference between the\ngenerated value and the value that would have been generated keeping the system\nas before. The latter is not observable. Moreover, the expected impact can be\nsmall in relative value. In this paper, we cast the problem as counterfactual\nprediction of unobserved revenue. The impact on revenue is then the difference\nbetween the observed and the estimated revenue. The originality of this work\nlies in the innovative application of econometric methods proposed for\nmacroeconomic applications to a new problem setting. Broadly applicable, the\napproach benefits from only requiring revenue data observed for\norigin-destination pairs in the network of the airline at each day, before and\nafter a change in the system is applied. We report results using real\nlarge-scale data from Air Canada. We compare a deep neural network\ncounterfactual predictions model with econometric models. They achieve\nrespectively 1% and 1.1% of error on the counterfactual revenue predictions,\nand allow to accurately estimate small impacts (in the order of 2%).\n", "versions": [{"version": "v1", "created": "Wed, 13 Jan 2021 15:55:29 GMT"}, {"version": "v2", "created": "Wed, 16 Jun 2021 14:00:47 GMT"}], "update_date": "2021-06-17", "authors_parsed": [["Laage", "Greta", ""], ["Frejinger", "Emma", ""], ["Lodi", "Andrea", ""], ["Rabusseau", "Guillaume", ""]]}, {"id": "2101.10253", "submitter": "Daniela Mihai", "authors": "Daniela Mihai and Jonathon Hare", "title": "The emergence of visual semantics through communication games", "comments": "arXiv admin note: text overlap with arXiv:1911.05546", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The emergence of communication systems between agents which learn to play\nreferential signalling games with realistic images has attracted a lot of\nattention recently. The majority of work has focused on using fixed, pretrained\nimage feature extraction networks which potentially bias the information the\nagents learn to communicate. In this work, we consider a signalling game\nsetting in which a `sender' agent must communicate the information about an\nimage to a `receiver' who must select the correct image from many distractors.\nWe investigate the effect of the feature extractor's weights and of the task\nbeing solved on the visual semantics learned by the models. We first\ndemonstrate to what extent the use of pretrained feature extraction networks\ninductively bias the visual semantics conveyed by emergent communication\nchannel and quantify the visual semantics that are induced.\n  We then go on to explore ways in which inductive biases can be introduced to\nencourage the emergence of semantically meaningful communication without the\nneed for any form of supervised pretraining of the visual feature extractor. We\nimpose various augmentations to the input images and additional tasks in the\ngame with the aim to induce visual representations which capture conceptual\nproperties of images. Through our experiments, we demonstrate that\ncommunication systems which capture visual semantics can be learned in a\ncompletely self-supervised manner by playing the right types of game. Our work\nbridges a gap between emergent communication research and self-supervised\nfeature learning.\n", "versions": [{"version": "v1", "created": "Mon, 25 Jan 2021 17:43:37 GMT"}], "update_date": "2021-01-26", "authors_parsed": [["Mihai", "Daniela", ""], ["Hare", "Jonathon", ""]]}, {"id": "2101.10254", "submitter": "Jithin Jagannath", "authors": "Anu Jagannath, Jithin Jagannath", "title": "Multi-task Learning Approach for Automatic Modulation and Wireless\n  Signal Classification", "comments": "To appear in Proc. of IEEE International Conference on Communications\n  (ICC) 2021. Open data set also included in the reference", "journal-ref": "IEEE International Conference on Communications (ICC) 2021", "doi": null, "report-no": null, "categories": "cs.LG cs.NI", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Wireless signal recognition is becoming increasingly more significant for\nspectrum monitoring, spectrum management, and secure communications.\nConsequently, it will become a key enabler with the emerging fifth-generation\n(5G) and beyond 5G communications, Internet of Things networks, among others.\nState-of-the-art studies in wireless signal recognition have only focused on a\nsingle task which in many cases is insufficient information for a system to act\non. In this work, for the first time in the wireless communication domain, we\nexploit the potential of deep neural networks in conjunction with multi-task\nlearning (MTL) framework to simultaneously learn modulation and signal\nclassification tasks. The proposed MTL architecture benefits from the mutual\nrelation between the two tasks in improving the classification accuracy as well\nas the learning efficiency with a lightweight neural network model.\nAdditionally, we consider the problem of heterogeneous wireless signals such as\nradar and communication signals in the electromagnetic spectrum. Accordingly,\nwe have shown how the proposed MTL model outperforms several state-of-the-art\nsingle-task learning classifiers while maintaining a lighter architecture and\nperforming two signal characterization tasks simultaneously. Finally, we also\nrelease the only known open heterogeneous wireless signals dataset that\ncomprises of radar and communication signals with multiple labels.\n", "versions": [{"version": "v1", "created": "Mon, 25 Jan 2021 17:43:42 GMT"}, {"version": "v2", "created": "Sat, 20 Feb 2021 21:14:59 GMT"}], "update_date": "2021-02-23", "authors_parsed": [["Jagannath", "Anu", ""], ["Jagannath", "Jithin", ""]]}, {"id": "2101.10261", "submitter": "Youssef Aboutaleb", "authors": "Youssef M. Aboutaleb, Mazen Danaf, Yifei Xie, Moshe Ben-Akiva", "title": "Discrete Choice Analysis with Machine Learning Capabilities", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG econ.EM stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper discusses capabilities that are essential to models applied in\npolicy analysis settings and the limitations of direct applications of\noff-the-shelf machine learning methodologies to such settings. Traditional\neconometric methodologies for building discrete choice models for policy\nanalysis involve combining data with modeling assumptions guided by\nsubject-matter considerations. Such considerations are typically most useful in\nspecifying the systematic component of random utility discrete choice models\nbut are typically of limited aid in determining the form of the random\ncomponent. We identify an area where machine learning paradigms can be\nleveraged, namely in specifying and systematically selecting the best\nspecification of the random component of the utility equations. We review two\nrecent novel applications where mixed-integer optimization and cross-validation\nare used to algorithmically select optimal specifications for the random\nutility components of nested logit and logit mixture models subject to\ninterpretability constraints.\n", "versions": [{"version": "v1", "created": "Thu, 21 Jan 2021 21:34:43 GMT"}], "update_date": "2021-01-26", "authors_parsed": [["Aboutaleb", "Youssef M.", ""], ["Danaf", "Mazen", ""], ["Xie", "Yifei", ""], ["Ben-Akiva", "Moshe", ""]]}, {"id": "2101.10263", "submitter": "Yakup Kutlu", "authors": "Gokhan Altan, Yakup Kutlu", "title": "Generative Autoencoder Kernels on Deep Learning for Brain Activity\n  Analysis", "comments": "12 pages, 2 figures, Natural and Engineering Sciences", "journal-ref": "Natural and Engineering Sciences, 2018", "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Deep Learning (DL) is a two-step classification model that consists feature\nlearning, generating feature representations using unsupervised ways and the\nsupervised learning stage at the last step of model using at least two hidden\nlayers on the proposed structures by fully connected layers depending on of the\nartificial neural networks. The optimization of the predefined classification\nparameters for the supervised models eases reaching the global optimality with\nexact zero training error. The autoencoder (AE) models are the highly\ngeneralized ways of the unsupervised stages for the DL to define the output\nweights of the hidden neurons with various representations. As alternatively to\nthe conventional Extreme Learning Machines (ELM) AE, Hessenberg\ndecomposition-based ELM autoencoder (HessELM-AE) is a novel kernel to generate\ndifferent presentations of the input data within the intended sizes of the\nmodels. The aim of the study is analyzing the performance of the novel Deep AE\nkernel for clinical availability on electroencephalogram (EEG) with stroke\npatients. The slow cortical potentials (SCP) training in stroke patients during\neight neurofeedback sessions were analyzed using Hilbert-Huang Transform. The\nstatistical features of different frequency modulations were fed into the Deep\nELM model for generative AE kernels. The novel Deep ELM-AE kernels have\ndiscriminated the brain activity with high classification performances for\npositivity and negativity tasks in stroke patients.\n", "versions": [{"version": "v1", "created": "Thu, 21 Jan 2021 08:19:47 GMT"}], "update_date": "2021-01-26", "authors_parsed": [["Altan", "Gokhan", ""], ["Kutlu", "Yakup", ""]]}, {"id": "2101.10265", "submitter": "Yakup Kutlu", "authors": "Gokhan Altan, Yakup Kutlu", "title": "Superiorities of Deep Extreme Learning Machines against Convolutional\n  Neural Networks", "comments": "7 pages, 2 figures, Natural and Engineering Sciences", "journal-ref": "Natural and Engineering Sciences, 2018", "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Deep Learning (DL) is a machine learning procedure for artificial\nintelligence that analyzes the input data in detail by increasing neuron sizes\nand number of the hidden layers. DL has a popularity with the common\nimprovements on the graphical processing unit capabilities. Increasing number\nof the neuron sizes at each layer and hidden layers is directly related to the\ncomputation time and training speed of the classifier models. The\nclassification parameters including neuron weights, output weights, and biases\nneed to be optimized for obtaining an optimum model. Most of the popular DL\nalgorithms require long training times for optimization of the parameters with\nfeature learning progresses and back-propagated training procedures. Reducing\nthe training time and providing a real-time decision system are the basic focus\npoints of the novel approaches. Deep Extreme Learning machines (Deep ELM)\nclassifier model is one of the fastest and effective way to meet fast\nclassification problems. In this study, Deep ELM model, its superiorities and\nweaknesses are discussed, the problems that are more suitable for the\nclassifiers against Convolutional neural network based DL algorithms.\n", "versions": [{"version": "v1", "created": "Thu, 21 Jan 2021 08:22:18 GMT"}], "update_date": "2021-01-26", "authors_parsed": [["Altan", "Gokhan", ""], ["Kutlu", "Yakup", ""]]}, {"id": "2101.10266", "submitter": "Rohan Sukumaran", "authors": "Rohan Sukumaran, Parth Patwa, T V Sethuraman, Sheshank Shankar,\n  Rishank Kanaparti, Joseph Bae, Yash Mathur, Abhishek Singh, Ayush Chopra,\n  Myungsun Kang, Priya Ramaswamy and Ramesh Raskar", "title": "COVID-19 Outbreak Prediction and Analysis using Self Reported Symptoms", "comments": "15 pages, 16 Figures - Latest version on the Journal of Behavioural\n  Data Science - https://isdsa.org/_media/jbds/v1n1/v1n1p8.pdf", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  It is crucial for policymakers to understand the community prevalence of\nCOVID-19 so combative resources can be effectively allocated and prioritized\nduring the COVID-19 pandemic. Traditionally, community prevalence has been\nassessed through diagnostic and antibody testing data. However, despite the\nincreasing availability of COVID-19 testing, the required level has not been\nmet in most parts of the globe, introducing a need for an alternative method\nfor communities to determine disease prevalence. This is further complicated by\nthe observation that COVID-19 prevalence and spread varies across different\nspatial, temporal, and demographics. In this study, we understand trends in the\nspread of COVID-19 by utilizing the results of self-reported COVID-19 symptoms\nsurveys as an alternative to COVID-19 testing reports. This allows us to assess\ncommunity disease prevalence, even in areas with low COVID-19 testing ability.\nUsing individually reported symptom data from various populations, our method\npredicts the likely percentage of the population that tested positive for\nCOVID-19. We do so with a Mean Absolute Error (MAE) of 1.14 and Mean Relative\nError (MRE) of 60.40\\% with 95\\% confidence interval as (60.12, 60.67). This\nimplies that our model predicts +/- 1140 cases than the original in a\npopulation of 1 million. In addition, we forecast the location-wise percentage\nof the population testing positive for the next 30 days using self-reported\nsymptoms data from previous days. The MAE for this method is as low as 0.15\n(MRE of 23.61\\% with 95\\% confidence interval as (23.6, 13.7)) for New York. We\npresent an analysis of these results, exposing various clinical attributes of\ninterest across different demographics. Lastly, we qualitatively analyze how\nvarious policy enactments (testing, curfew) affect the prevalence of COVID-19\nin a community.\n", "versions": [{"version": "v1", "created": "Mon, 21 Dec 2020 00:37:24 GMT"}, {"version": "v2", "created": "Sat, 19 Jun 2021 17:07:03 GMT"}], "update_date": "2021-06-22", "authors_parsed": [["Sukumaran", "Rohan", ""], ["Patwa", "Parth", ""], ["Sethuraman", "T V", ""], ["Shankar", "Sheshank", ""], ["Kanaparti", "Rishank", ""], ["Bae", "Joseph", ""], ["Mathur", "Yash", ""], ["Singh", "Abhishek", ""], ["Chopra", "Ayush", ""], ["Kang", "Myungsun", ""], ["Ramaswamy", "Priya", ""], ["Raskar", "Ramesh", ""]]}, {"id": "2101.10267", "submitter": "Uwe Aickelin", "authors": "Mansoureh Maadia, Uwe Aickelin, Hadi Akbarzadeh Khorshidi", "title": "A new interval-based aggregation approach based on bagging and Interval\n  Agreement Approach (IAA) in ensemble learning", "comments": "The Australasian Data Mining Conference 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The main aim in ensemble learning is using multiple individual classifiers\noutputs rather than one classifier output to aggregate them for more accurate\nclassification. Generating an ensemble classifier generally is composed of\nthree steps: selecting the base classifier, applying a sampling strategy to\ngenerate different individual classifiers and aggregation the classifiers\noutputs. This paper focuses on the classifiers outputs aggregation step and\npresents a new interval-based aggregation modeling using bagging resampling\napproach and Interval Agreement Approach (IAA) in ensemble learning. IAA is an\ninteresting and practical aggregation approach in decision making which was\nintroduced to combine decision makers opinions when they present their opinions\nby intervals. In this paper, in addition to implementing a new aggregation\napproach in ensemble learning, we designed some experiments to encourage\nresearchers to use interval modeling in ensemble learning because it preserves\nmore uncertainty and this leads to more accurate classification. For this\npurpose, we compared the results of implementing the proposed method to the\nmajority vote as the most common and successful aggregation function in the\nliterature on 10 medical data sets to show the better performance of the\ninterval modeling and the proposed interval-based aggregation function in\nbinary classification when it comes to ensemble learning. The results confirm\nthe good performance of our proposed approach.\n", "versions": [{"version": "v1", "created": "Tue, 15 Dec 2020 09:33:12 GMT"}], "update_date": "2021-01-26", "authors_parsed": [["Maadia", "Mansoureh", ""], ["Aickelin", "Uwe", ""], ["Khorshidi", "Hadi Akbarzadeh", ""]]}, {"id": "2101.10268", "submitter": "Hong Sun", "authors": "Hong Sun, Kristof Depraetere, Laurent Meesseman, Jos De Roo, Martijn\n  Vanbiervliet, Jos De Baerdemaeker, Herman Muys, Vera von Dossow, Nikolai\n  Hulde, Ralph Szymanowsky", "title": "A scalable approach for developing clinical risk prediction applications\n  in different hospitals", "comments": "Preprint of Journal of Biomedical Informatics", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Objective: Machine learning algorithms are now widely used in predicting\nacute events for clinical applications. While most of such prediction\napplications are developed to predict the risk of a particular acute event at\none hospital, few efforts have been made in extending the developed solutions\nto other events or to different hospitals. We provide a scalable solution to\nextend the process of clinical risk prediction model development of multiple\ndiseases and their deployment in different Electronic Health Records (EHR)\nsystems.\n  Materials and Methods: We defined a generic process for clinical risk\nprediction model development. A calibration tool has been created to automate\nthe model generation process. We applied the model calibration process at four\nhospitals, and generated risk prediction models for delirium, sepsis and acute\nkidney injury (AKI) respectively at each of these hospitals.\n  Results: The delirium risk prediction models achieved area under the\nreceiver-operating characteristic curve (AUROC) ranging from 0.82 to 0.95 over\ndifferent stages of a hospital stay on the test datasets of the four hospitals.\nThe sepsis models achieved AUROC ranging from 0.88 to 0.95, and the AKI models\nachieved AUROC ranging from 0.85 to 0.92.\n  Discussion: The scalability discussed in this paper is based on building\ncommon data representations (syntactic interoperability) between EHRs stored in\ndifferent hospitals. Semantic interoperability, a more challenging requirement\nthat different EHRs share the same meaning of data, e.g. a same lab coding\nsystem, is not mandated with our approach.\n  Conclusions: Our study describes a method to develop and deploy clinical risk\nprediction models in a scalable way. We demonstrate its feasibility by\ndeveloping risk prediction models for three diseases across four hospitals.\n", "versions": [{"version": "v1", "created": "Thu, 21 Jan 2021 21:22:32 GMT"}, {"version": "v2", "created": "Wed, 14 Apr 2021 11:32:49 GMT"}], "update_date": "2021-04-15", "authors_parsed": [["Sun", "Hong", ""], ["Depraetere", "Kristof", ""], ["Meesseman", "Laurent", ""], ["De Roo", "Jos", ""], ["Vanbiervliet", "Martijn", ""], ["De Baerdemaeker", "Jos", ""], ["Muys", "Herman", ""], ["von Dossow", "Vera", ""], ["Hulde", "Nikolai", ""], ["Szymanowsky", "Ralph", ""]]}, {"id": "2101.10276", "submitter": "Michael Noukhovitch", "authors": "Michael Noukhovitch, Travis LaCroix, Angeliki Lazaridou, Aaron\n  Courville", "title": "Emergent Communication under Competition", "comments": "To be presented at AAMAS 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.MA", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  The literature in modern machine learning has only negative results for\nlearning to communicate between competitive agents using standard RL. We\nintroduce a modified sender-receiver game to study the spectrum of\npartially-competitive scenarios and show communication can indeed emerge in a\ncompetitive setting. We empirically demonstrate three key takeaways for future\nresearch. First, we show that communication is proportional to cooperation, and\nit can occur for partially competitive scenarios using standard learning\nalgorithms. Second, we highlight the difference between communication and\nmanipulation and extend previous metrics of communication to the competitive\ncase. Third, we investigate the negotiation game where previous work failed to\nlearn communication between independent agents (Cao et al., 2018). We show\nthat, in this setting, both agents must benefit from communication for it to\nemerge; and, with a slight modification to the game, we demonstrate successful\ncommunication between competitive agents. We hope this work overturns\nmisconceptions and inspires more research in competitive emergent\ncommunication.\n", "versions": [{"version": "v1", "created": "Mon, 25 Jan 2021 17:58:22 GMT"}], "update_date": "2021-01-26", "authors_parsed": [["Noukhovitch", "Michael", ""], ["LaCroix", "Travis", ""], ["Lazaridou", "Angeliki", ""], ["Courville", "Aaron", ""]]}, {"id": "2101.10277", "submitter": "Madhusudan Verma", "authors": "Madhusudan Verma", "title": "Revisiting Linformer with a modified self-attention with linear\n  complexity", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Although Transformer models such as Google's BERT and OpenAI's GPT-3 are\nsuccessful in many natural language processing tasks, training and deploying\nthese models are costly and inefficient.Even if pre-trained models are used,\ndeploying these models still remained a challenge due to their large size.\nApart from deployment, these models take higher time during inference\nrestricting user-friendliness. The main bottleneck is self-attention which uses\nquadratic time and space with respect to the sequence length. In order to\nreduce the quadratic time complexity of the self-attention mechanism, Linformer\nby Facebook's AI research team was introduced where they showed that the\nself-attention mechanism can be approximated by a low-rank matrix and\nexploiting this finding, a new method for self-attention with linear time and\nspace complexity was proposed by them. In the Linformer, the time complexity\ndepends on the projection mapping dimension which acts as a hyperparameter and\naffects the performance of the model, tuning this hyperparameter can be\ntime-consuming. In this paper, I proposed an alternative method for\nself-attention with linear complexity in time and space and is independent of\nthe projection mapping dimension. Since this method works for long sequences\nthis can be used for images as well as audios.\n", "versions": [{"version": "v1", "created": "Wed, 16 Dec 2020 13:23:29 GMT"}], "update_date": "2021-01-26", "authors_parsed": [["Verma", "Madhusudan", ""]]}, {"id": "2101.10279", "submitter": "Pablo Antonio Moreno Casares", "authors": "P A M Casares, Roberto Campos, M A Martin-Delgado", "title": "QFold: Quantum Walks and Deep Learning to Solve Protein Folding", "comments": "RevTex 4.1, 9 color figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "quant-ph cond-mat.stat-mech cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We develop quantum computational tools to predict how proteins fold in 3D,\none of the most important problems in current biochemical research. We explain\nhow to combine recent deep learning advances with the well known technique of\nquantum walks applied to a Metropolis algorithm. The result, QFold, is a fully\nscalable hybrid quantum algorithm that in contrast to previous quantum\napproaches does not require a lattice model simplification and instead relies\non the much more realistic assumption of parameterization in terms of torsion\nangles of the amino acids. We compare it with its classical analog for\ndifferent annealing schedules and find a polynomial quantum advantage, and\nvalidate a proof-of-concept realization of the quantum Metropolis in IBMQ\nCasablanca quantum processor.\n", "versions": [{"version": "v1", "created": "Mon, 25 Jan 2021 18:00:03 GMT"}], "update_date": "2021-01-28", "authors_parsed": [["Casares", "P A M", ""], ["Campos", "Roberto", ""], ["Martin-Delgado", "M A", ""]]}, {"id": "2101.10280", "submitter": "Dongdong Wang", "authors": "Dongdong Wang, Shunpu Zhang, and Liqiang Wang", "title": "Deep Epidemiological Modeling by Black-box Knowledge Distillation: An\n  Accurate Deep Learning Model for COVID-19", "comments": "Accepted by AAAI-21/IAAI-21, 7 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An accurate and efficient forecasting system is imperative to the prevention\nof emerging infectious diseases such as COVID-19 in public health. This system\nrequires accurate transient modeling, lower computation cost, and fewer\nobservation data. To tackle these three challenges, we propose a novel deep\nlearning approach using black-box knowledge distillation for both accurate and\nefficient transmission dynamics prediction in a practical manner. First, we\nleverage mixture models to develop an accurate, comprehensive, yet impractical\nsimulation system. Next, we use simulated observation sequences to query the\nsimulation system to retrieve simulated projection sequences as knowledge.\nThen, with the obtained query data, sequence mixup is proposed to improve query\nefficiency, increase knowledge diversity, and boost distillation model\naccuracy. Finally, we train a student deep neural network with the retrieved\nand mixed observation-projection sequences for practical use. The case study on\nCOVID-19 justifies that our approach accurately projects infections with much\nlower computation cost when observation data are limited.\n", "versions": [{"version": "v1", "created": "Wed, 20 Jan 2021 19:49:00 GMT"}], "update_date": "2021-01-26", "authors_parsed": [["Wang", "Dongdong", ""], ["Zhang", "Shunpu", ""], ["Wang", "Liqiang", ""]]}, {"id": "2101.10292", "submitter": "Xiaoqian Wu", "authors": "Yong-Lu Li, Xinpeng Liu, Xiaoqian Wu, Xijie Huang, Liang Xu, Cewu Lu", "title": "Transferable Interactiveness Knowledge for Human-Object Interaction\n  Detection", "comments": "TPAMI version of our CVPR2019 paper with a new benchmark\n  PaStaNet-HOI. Code:\n  https://github.com/DirtyHarryLYL/Transferable-Interactiveness-Network. arXiv\n  admin note: substantial text overlap with arXiv:1811.08264", "journal-ref": "IEEE Transactions on Pattern Analysis and Machine Intelligence,\n  2021", "doi": "10.1109/TPAMI.2021.3054048", "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Human-Object Interaction (HOI) detection is an important problem to\nunderstand how humans interact with objects. In this paper, we explore\ninteractiveness knowledge which indicates whether a human and an object\ninteract with each other or not. We found that interactiveness knowledge can be\nlearned across HOI datasets and bridge the gap between diverse HOI category\nsettings. Our core idea is to exploit an interactiveness network to learn the\ngeneral interactiveness knowledge from multiple HOI datasets and perform\nNon-Interaction Suppression (NIS) before HOI classification in inference. On\naccount of the generalization ability of interactiveness, interactiveness\nnetwork is a transferable knowledge learner and can be cooperated with any HOI\ndetection models to achieve desirable results. We utilize the human instance\nand body part features together to learn the interactiveness in hierarchical\nparadigm, i.e., instance-level and body part-level interactivenesses.\nThereafter, a consistency task is proposed to guide the learning and extract\ndeeper interactive visual clues. We extensively evaluate the proposed method on\nHICO-DET, V-COCO, and a newly constructed PaStaNet-HOI dataset. With the\nlearned interactiveness, our method outperforms state-of-the-art HOI detection\nmethods, verifying its efficacy and flexibility. Code is available at\nhttps://github.com/DirtyHarryLYL/Transferable-Interactiveness-Network.\n", "versions": [{"version": "v1", "created": "Mon, 25 Jan 2021 18:21:07 GMT"}, {"version": "v2", "created": "Sat, 27 Feb 2021 04:21:24 GMT"}, {"version": "v3", "created": "Wed, 3 Mar 2021 10:04:29 GMT"}], "update_date": "2021-04-13", "authors_parsed": [["Li", "Yong-Lu", ""], ["Liu", "Xinpeng", ""], ["Wu", "Xiaoqian", ""], ["Huang", "Xijie", ""], ["Xu", "Liang", ""], ["Lu", "Cewu", ""]]}, {"id": "2101.10300", "submitter": "Myeung Suk Oh", "authors": "Myeung Suk Oh, Seyyedali Hosseinalipour, Taejoon Kim, Christopher G.\n  Brinton, David J. Love", "title": "Channel Estimation via Successive Denoising in MIMO OFDM Systems: A\n  Reinforcement Learning Approach", "comments": "This paper is accepted for publication in the proceedings of 2021\n  IEEE International Conference on Communications (ICC)", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SP cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In general, reliable communication via multiple-input multiple-output (MIMO)\northogonal frequency division multiplexing (OFDM) requires accurate channel\nestimation at the receiver. The existing literature largely focuses on\ndenoising methods for channel estimation that depend on either (i)~channel\nanalysis in the time-domain with prior channel knowledge or (ii)~supervised\nlearning techniques which require large pre-labeled datasets for training. To\naddress these limitations, we present a frequency-domain denoising method based\non a reinforcement learning framework that does not need a priori channel\nknowledge and pre-labeled data. Our methodology includes a new successive\nchannel denoising process based on channel curvature computation, for which we\nobtain a channel curvature magnitude threshold to identify unreliable channel\nestimates. Based on this process, we formulate the denoising mechanism as a\nMarkov decision process, where we define the actions through a geometry-based\nchannel estimation update, and the reward function based on a policy that\nreduces mean squared error (MSE). We then resort to Q-learning to update the\nchannel estimates. Numerical results verify that our denoising algorithm can\nsuccessfully mitigate noise in channel estimates. In particular, our algorithm\nprovides a significant improvement over the practical least squares (LS)\nestimation method and provides performance that approaches that of the ideal\nlinear minimum mean square error (LMMSE) estimation with perfect knowledge of\nchannel statistics.\n", "versions": [{"version": "v1", "created": "Mon, 25 Jan 2021 18:33:54 GMT"}, {"version": "v2", "created": "Wed, 27 Jan 2021 00:47:57 GMT"}, {"version": "v3", "created": "Mon, 15 Feb 2021 04:11:40 GMT"}, {"version": "v4", "created": "Tue, 23 Mar 2021 03:06:45 GMT"}], "update_date": "2021-03-24", "authors_parsed": [["Oh", "Myeung Suk", ""], ["Hosseinalipour", "Seyyedali", ""], ["Kim", "Taejoon", ""], ["Brinton", "Christopher G.", ""], ["Love", "David J.", ""]]}, {"id": "2101.10318", "submitter": "Satya Narayan Shukla", "authors": "Satya Narayan Shukla, Benjamin M. Marlin", "title": "Multi-Time Attention Networks for Irregularly Sampled Time Series", "comments": "Accepted at International Conference on Learning Representations\n  (ICLR) 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Irregular sampling occurs in many time series modeling applications where it\npresents a significant challenge to standard deep learning models. This work is\nmotivated by the analysis of physiological time series data in electronic\nhealth records, which are sparse, irregularly sampled, and multivariate. In\nthis paper, we propose a new deep learning framework for this setting that we\ncall Multi-Time Attention Networks. Multi-Time Attention Networks learn an\nembedding of continuous-time values and use an attention mechanism to produce a\nfixed-length representation of a time series containing a variable number of\nobservations. We investigate the performance of this framework on interpolation\nand classification tasks using multiple datasets. Our results show that the\nproposed approach performs as well or better than a range of baseline and\nrecently proposed models while offering significantly faster training times\nthan current state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Mon, 25 Jan 2021 18:57:42 GMT"}, {"version": "v2", "created": "Mon, 7 Jun 2021 17:52:42 GMT"}], "update_date": "2021-06-08", "authors_parsed": [["Shukla", "Satya Narayan", ""], ["Marlin", "Benjamin M.", ""]]}, {"id": "2101.10320", "submitter": "Jiaxuan You", "authors": "Jiaxuan You, Jonathan Gomes-Selman, Rex Ying, Jure Leskovec", "title": "Identity-aware Graph Neural Networks", "comments": "AAAI 2021. Version with appendix", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Message passing Graph Neural Networks (GNNs) provide a powerful modeling\nframework for relational data. However, the expressive power of existing GNNs\nis upper-bounded by the 1-Weisfeiler-Lehman (1-WL) graph isomorphism test,\nwhich means GNNs that are not able to predict node clustering coefficients and\nshortest path distances, and cannot differentiate between different d-regular\ngraphs. Here we develop a class of message passing GNNs, named Identity-aware\nGraph Neural Networks (ID-GNNs), with greater expressive power than the 1-WL\ntest. ID-GNN offers a minimal but powerful solution to limitations of existing\nGNNs. ID-GNN extends existing GNN architectures by inductively considering\nnodes' identities during message passing. To embed a given node, ID-GNN first\nextracts the ego network centered at the node, then conducts rounds of\nheterogeneous message passing, where different sets of parameters are applied\nto the center node than to other surrounding nodes in the ego network. We\nfurther propose a simplified but faster version of ID-GNN that injects node\nidentity information as augmented node features. Altogether, both versions of\nID-GNN represent general extensions of message passing GNNs, where experiments\nshow that transforming existing GNNs to ID-GNNs yields on average 40% accuracy\nimprovement on challenging node, edge, and graph property prediction tasks; 3%\naccuracy improvement on node and graph classification benchmarks; and 15% ROC\nAUC improvement on real-world link prediction tasks. Additionally, ID-GNNs\ndemonstrate improved or comparable performance over other task-specific graph\nnetworks.\n", "versions": [{"version": "v1", "created": "Mon, 25 Jan 2021 18:59:01 GMT"}, {"version": "v2", "created": "Fri, 5 Feb 2021 08:14:23 GMT"}], "update_date": "2021-02-08", "authors_parsed": [["You", "Jiaxuan", ""], ["Gomes-Selman", "Jonathan", ""], ["Ying", "Rex", ""], ["Leskovec", "Jure", ""]]}, {"id": "2101.10356", "submitter": "Muyuan Chen", "authors": "Muyuan Chen and Steven Ludtke", "title": "Deep learning based mixed-dimensional GMM for characterizing variability\n  in CryoEM", "comments": "31 pages, 5 main figures and 8 supplementary figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.BM cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Structural flexibility and/or dynamic interactions with other molecules is a\ncritical aspect of protein function. CryoEM provides direct visualization of\nindividual macromolecules sampling different conformational and compositional\nstates. While numerous methods are available for computational classification\nof discrete states, characterization of continuous conformational changes or\nlarge numbers of discrete state without human supervision remains challenging.\nHere we present e2gmm, a machine learning algorithm to determine a\nconformational landscape for proteins or complexes using a 3-D Gaussian mixture\nmodel mapped onto 2-D particle images in known orientations. Using a deep\nneural network architecture, e2gmm can automatically resolve the structural\nheterogeneity within the protein complex and map particles onto a small latent\nspace describing conformational and compositional changes. This system presents\na more intuitive and flexible representation than other manifold methods\ncurrently in use. We demonstrate this method on both simulated data as well as\nthree biological systems, to explore compositional and conformational changes\nat a range of scales. The software is distributed as part of EMAN2.\n", "versions": [{"version": "v1", "created": "Mon, 25 Jan 2021 19:05:23 GMT"}, {"version": "v2", "created": "Sun, 23 May 2021 14:06:07 GMT"}], "update_date": "2021-05-25", "authors_parsed": [["Chen", "Muyuan", ""], ["Ludtke", "Steven", ""]]}, {"id": "2101.10357", "submitter": "Oron Sabag Dr.", "authors": "Oron Sabag, Babak Hassibi", "title": "Regret-Optimal Filtering", "comments": "Accepted to AISTATS 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.LG cs.SY eess.SY", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We consider the problem of filtering in linear state-space models (e.g., the\nKalman filter setting) through the lens of regret optimization. Different\nassumptions on the driving disturbance and the observation noise sequences give\nrise to different estimators: in the stochastic setting to the celebrated\nKalman filter, and in the deterministic setting of bounded energy disturbances\nto $H_\\infty$ estimators. In this work, we formulate a novel criterion for\nfilter design based on the concept of regret between the estimation error\nenergy of a clairvoyant estimator that has access to all future observations (a\nso-called smoother) and a causal one that only has access to current and past\nobservations. The regret-optimal estimator is chosen to minimize this\nworst-case difference across all bounded-energy noise sequences. The resulting\nestimator is adaptive in the sense that it aims to mimic the behavior of the\nclairvoyant estimator, irrespective of what the realization of the noise will\nbe and thus interpolates between the stochastic and deterministic approaches.\nWe provide a solution for the regret estimation problem at two different\nlevels. First, we provide a solution at the operator level by reducing it to\nthe Nehari problem. Second, for state-space models, we explicitly find the\nestimator that achieves the optimal regret. From a computational perspective,\nthe regret-optimal estimator can be easily implemented by solving three Riccati\nequations and a single Lyapunov equation. For a state-space model of dimension\n$n$, the regret-optimal estimator has a state-space structure of dimension\n$3n$. We demonstrate the applicability and efficacy of the estimator in a\nvariety of problems and observe that the estimator has average and worst-case\nperformances that are simultaneously close to their optimal values. We\ntherefore argue that regret-optimality is a viable approach to estimator\ndesign.\n", "versions": [{"version": "v1", "created": "Mon, 25 Jan 2021 19:06:52 GMT"}], "update_date": "2021-01-27", "authors_parsed": [["Sabag", "Oron", ""], ["Hassibi", "Babak", ""]]}, {"id": "2101.10369", "submitter": "Tze-Yang Tung", "authors": "Tze-Yang Tung, Szymon Kobus, Joan Roig Pujol, Deniz Gunduz", "title": "Effective Communications: A Joint Learning and Communication Framework\n  for Multi-Agent Reinforcement Learning over Noisy Channels", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SP cs.AI cs.IT cs.LG math.IT stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We propose a novel formulation of the \"effectiveness problem\" in\ncommunications, put forth by Shannon and Weaver in their seminal work [2], by\nconsidering multiple agents communicating over a noisy channel in order to\nachieve better coordination and cooperation in a multi-agent reinforcement\nlearning (MARL) framework. Specifically, we consider a multi-agent partially\nobservable Markov decision process (MA-POMDP), in which the agents, in addition\nto interacting with the environment can also communicate with each other over a\nnoisy communication channel. The noisy communication channel is considered\nexplicitly as part of the dynamics of the environment and the message each\nagent sends is part of the action that the agent can take. As a result, the\nagents learn not only to collaborate with each other but also to communicate\n\"effectively\" over a noisy channel. This framework generalizes both the\ntraditional communication problem, where the main goal is to convey a message\nreliably over a noisy channel, and the \"learning to communicate\" framework that\nhas received recent attention in the MARL literature, where the underlying\ncommunication channels are assumed to be error-free. We show via examples that\nthe joint policy learned using the proposed framework is superior to that where\nthe communication is considered separately from the underlying MA-POMDP. This\nis a very powerful framework, which has many real world applications, from\nautonomous vehicle planning to drone swarm control, and opens up the rich\ntoolbox of deep reinforcement learning for the design of multi-user\ncommunication systems.\n", "versions": [{"version": "v1", "created": "Sat, 2 Jan 2021 10:43:41 GMT"}, {"version": "v2", "created": "Thu, 1 Apr 2021 17:30:45 GMT"}], "update_date": "2021-04-02", "authors_parsed": [["Tung", "Tze-Yang", ""], ["Kobus", "Szymon", ""], ["Pujol", "Joan Roig", ""], ["Gunduz", "Deniz", ""]]}, {"id": "2101.10382", "submitter": "Radu Tudor Ionescu", "authors": "Petru Soviany, Radu Tudor Ionescu, Paolo Rota, Nicu Sebe", "title": "Curriculum Learning: A Survey", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Training machine learning models in a meaningful order, from the easy samples\nto the hard ones, using curriculum learning can provide performance\nimprovements over the standard training approach based on random data\nshuffling, without any additional computational costs. Curriculum learning\nstrategies have been successfully employed in all areas of machine learning, in\na wide range of tasks. However, the necessity of finding a way to rank the\nsamples from easy to hard, as well as the right pacing function for introducing\nmore difficult data can limit the usage of the curriculum approaches. In this\nsurvey, we show how these limits have been tackled in the literature, and we\npresent different curriculum learning instantiations for various tasks in\nmachine learning. We construct a multi-perspective taxonomy of curriculum\nlearning approaches by hand, considering various classification criteria. We\nfurther build a hierarchical tree of curriculum learning methods using an\nagglomerative clustering algorithm, linking the discovered clusters with our\ntaxonomy. At the end, we provide some interesting directions for future work.\n", "versions": [{"version": "v1", "created": "Mon, 25 Jan 2021 20:08:32 GMT"}], "update_date": "2021-01-27", "authors_parsed": [["Soviany", "Petru", ""], ["Ionescu", "Radu Tudor", ""], ["Rota", "Paolo", ""], ["Sebe", "Nicu", ""]]}, {"id": "2101.10385", "submitter": "Michael Tashman", "authors": "Jiayi Xie, Michael Tashman, John Hoffman, Lee Winikor, Rouzbeh Gerami", "title": "Online and Scalable Model Selection with Multi-Armed Bandits", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many online applications running on live traffic are powered by machine\nlearning models, for which training, validation, and hyper-parameter tuning are\nconducted on historical data. However, it is common for models demonstrating\nstrong performance in offline analysis to yield poorer performance when\ndeployed online. This problem is a consequence of the difficulty of training on\nhistorical data in non-stationary environments. Moreover, the machine learning\nmetrics used for model selection may not sufficiently correlate with real-world\nbusiness metrics used to determine the success of the applications being\ntested. These problems are particularly prominent in the Real-Time Bidding\n(RTB) domain, in which ML models power bidding strategies, and a change in\nmodels will likely affect performance of the advertising campaigns. In this\nwork, we present Automatic Model Selector (AMS), a system for scalable online\nselection of RTB bidding strategies based on real-world performance metrics.\nAMS employs Multi-Armed Bandits (MAB) to near-simultaneously run and evaluate\nmultiple models against live traffic, allocating the most traffic to the\nbest-performing models while decreasing traffic to those with poorer online\nperformance, thereby minimizing the impact of inferior models on overall\ncampaign performance. The reliance on offline data is avoided, instead making\nmodel selections on a case-by-case basis according to actionable business\ngoals. AMS allows new models to be safely introduced into live campaigns as\nsoon as they are developed, minimizing the risk to overall performance. In\nlive-traffic tests on multiple ad campaigns, the AMS system proved highly\neffective at improving ad campaign performance.\n", "versions": [{"version": "v1", "created": "Mon, 25 Jan 2021 20:12:52 GMT"}], "update_date": "2021-01-27", "authors_parsed": [["Xie", "Jiayi", ""], ["Tashman", "Michael", ""], ["Hoffman", "John", ""], ["Winikor", "Lee", ""], ["Gerami", "Rouzbeh", ""]]}, {"id": "2101.10390", "submitter": "Joeri Zwerts", "authors": "Joeri A. Zwerts, Jelle Treep, Casper S. Kaandorp, Floor Meewis, Amparo\n  C. Koot, Heysem Kaya", "title": "Introducing a Central African Primate Vocalisation Dataset for Automated\n  Species Classification", "comments": "5 pages, 3 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG q-bio.PE", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Automated classification of animal vocalisations is a potentially powerful\nwildlife monitoring tool. Training robust classifiers requires sizable\nannotated datasets, which are not easily recorded in the wild. To circumvent\nthis problem, we recorded four primate species under semi-natural conditions in\na wildlife sanctuary in Cameroon with the objective to train a classifier\ncapable of detecting species in the wild. Here, we introduce the collected\ndataset, describe our approach and initial results of classifier development.\nTo increase the efficiency of the annotation process, we condensed the\nrecordings with an energy/change based automatic vocalisation detection.\nSegmenting the annotated chunks into training, validation and test sets,\ninitial results reveal up to 82% unweighted average recall (UAR) test set\nperformance in four-class primate species classification.\n", "versions": [{"version": "v1", "created": "Mon, 25 Jan 2021 20:21:54 GMT"}], "update_date": "2021-01-27", "authors_parsed": [["Zwerts", "Joeri A.", ""], ["Treep", "Jelle", ""], ["Kaandorp", "Casper S.", ""], ["Meewis", "Floor", ""], ["Koot", "Amparo C.", ""], ["Kaya", "Heysem", ""]]}, {"id": "2101.10404", "submitter": "Alena Rodionova", "authors": "Al\\\"ena Rodionova (1), Yash Vardhan Pant (2), Connor Kurtz (3), Kuk\n  Jang (1), Houssam Abbas (3), Rahul Mangharam (1) ((1) University of\n  Pennsylvania, (2) University of California Berkeley, (3) Oregon State\n  University)", "title": "Learning-'N-Flying: A Learning-based, Decentralized Mission Aware UAS\n  Collision Avoidance Scheme", "comments": "to be published in ACM Transactions on Cyber-Physical Systems. arXiv\n  admin note: text overlap with arXiv:2006.13267", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SY cs.LG cs.RO cs.SY", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Urban Air Mobility, the scenario where hundreds of manned and Unmanned\nAircraft System (UAS) carry out a wide variety of missions (e.g. moving humans\nand goods within the city), is gaining acceptance as a transportation solution\nof the future. One of the key requirements for this to happen is safely\nmanaging the air traffic in these urban airspaces. Due to the expected density\nof the airspace, this requires fast autonomous solutions that can be deployed\nonline. We propose Learning-'N-Flying (LNF) a multi-UAS Collision Avoidance\n(CA) framework. It is decentralized, works on-the-fly and allows autonomous UAS\nmanaged by different operators to safely carry out complex missions,\nrepresented using Signal Temporal Logic, in a shared airspace. We initially\nformulate the problem of predictive collision avoidance for two UAS as a\nmixed-integer linear program, and show that it is intractable to solve online.\nInstead, we first develop Learning-to-Fly (L2F) by combining: a) learning-based\ndecision-making, and b) decentralized convex optimization-based control. LNF\nextends L2F to cases where there are more than two UAS on a collision path.\nThrough extensive simulations, we show that our method can run online\n(computation time in the order of milliseconds), and under certain assumptions\nhas failure rates of less than 1% in the worst-case, improving to near 0% in\nmore relaxed operations. We show the applicability of our scheme to a wide\nvariety of settings through multiple case studies.\n", "versions": [{"version": "v1", "created": "Mon, 25 Jan 2021 20:38:17 GMT"}], "update_date": "2021-01-27", "authors_parsed": [["Rodionova", "Al\u00ebna", ""], ["Pant", "Yash Vardhan", ""], ["Kurtz", "Connor", ""], ["Jang", "Kuk", ""], ["Abbas", "Houssam", ""], ["Mangharam", "Rahul", ""]]}, {"id": "2101.10420", "submitter": "Shibo Zhou", "authors": "Shibo Zhou, Yu Pan", "title": "Spectrum Attention Mechanism for Time Series Classification", "comments": "6 pages, 9 figures, China Control Conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Time series classification(TSC) has always been an important and challenging\nresearch task. With the wide application of deep learning, more and more\nresearchers use deep learning models to solve TSC problems. Since time series\nalways contains a lot of noise, which has a negative impact on network\ntraining, people usually filter the original data before training the network.\nThe existing schemes are to treat the filtering and training as two stages, and\nthe design of the filter requires expert experience, which increases the design\ndifficulty of the algorithm and is not universal. We note that the essence of\nfiltering is to filter out the insignificant frequency components and highlight\nthe important ones, which is similar to the attention mechanism. In this paper,\nwe propose an attention mechanism that acts on spectrum (SAM). The network can\nassign appropriate weights to each frequency component to achieve adaptive\nfiltering. We use L1 regularization to further enhance the frequency screening\ncapability of SAM. We also propose a segmented-SAM (SSAM) to avoid the loss of\ntime domain information caused by using the spectrum of the whole sequence. In\nwhich, a tumbling window is introduced to segment the original data. Then SAM\nis applied to each segment to generate new features. We propose a heuristic\nstrategy to search for the appropriate number of segments. Experimental results\nshow that SSAM can produce better feature representations, make the network\nconverge faster, and improve the robustness and classification accuracy.\n", "versions": [{"version": "v1", "created": "Mon, 25 Jan 2021 21:14:05 GMT"}], "update_date": "2021-01-27", "authors_parsed": [["Zhou", "Shibo", ""], ["Pan", "Yu", ""]]}, {"id": "2101.10423", "submitter": "Zheda Mai", "authors": "Zheda Mai, Ruiwen Li, Jihwan Jeong, David Quispe, Hyunwoo Kim, Scott\n  Sanner", "title": "Online Continual Learning in Image Classification: An Empirical Survey", "comments": "Submitted to Neurocomputing. Codes available at\n  https://github.com/RaptorMai/online-continual-learning", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Online continual learning for image classification studies the problem of\nlearning to classify images from an online stream of data and tasks, where\ntasks may include new classes (class incremental) or data nonstationarity\n(domain incremental). One of the key challenges of continual learning is to\navoid catastrophic forgetting (CF), i.e., forgetting old tasks in the presence\nof more recent tasks. Over the past few years, many methods and tricks have\nbeen introduced to address this problem, but many have not been fairly and\nsystematically compared under a variety of realistic and practical settings. To\nbetter understand the relative advantages of various approaches and the\nsettings where they work best, this survey aims to (1) compare state-of-the-art\nmethods such as MIR, iCARL, and GDumb and determine which works best at\ndifferent experimental settings; (2) determine if the best class incremental\nmethods are also competitive in domain incremental setting; (3) evaluate the\nperformance of 7 simple but effective trick such as \"review\" trick and nearest\nclass mean (NCM) classifier to assess their relative impact. Regarding (1), we\nobserve iCaRL remains competitive when the memory buffer is small; GDumb\noutperforms many recently proposed methods in medium-size datasets and MIR\nperforms the best in larger-scale datasets. For (2), we note that GDumb\nperforms quite poorly while MIR -- already competitive for (1) -- is also\nstrongly competitive in this very different but important setting. Overall,\nthis allows us to conclude that MIR is overall a strong and versatile method\nacross a wide variety of settings. For (3), we find that all 7 tricks are\nbeneficial, and when augmented with the \"review\" trick and NCM classifier, MIR\nproduces performance levels that bring online continual learning much closer to\nits ultimate goal of matching offline training.\n", "versions": [{"version": "v1", "created": "Mon, 25 Jan 2021 21:20:02 GMT"}, {"version": "v2", "created": "Tue, 1 Jun 2021 21:07:30 GMT"}], "update_date": "2021-06-03", "authors_parsed": [["Mai", "Zheda", ""], ["Li", "Ruiwen", ""], ["Jeong", "Jihwan", ""], ["Quispe", "David", ""], ["Kim", "Hyunwoo", ""], ["Sanner", "Scott", ""]]}, {"id": "2101.10427", "submitter": "Nihal Acharya Adde", "authors": "Thilo Moshagen, Nihal Acharya Adde, Ajay Navilarekal Rajgopal", "title": "Finding hidden-feature depending laws inside a data set and classifying\n  it using Neural Network", "comments": "arXiv admin note: substantial text overlap with arXiv:2011.07332", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE stat.ML", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  The logcosh loss function for neural networks has been developed to combine\nthe advantage of the absolute error loss function of not overweighting outliers\nwith the advantage of the mean square error of continuous derivative near the\nmean, which makes the last phase of learning easier. It is clear, and one\nexperiences it soon, that in the case of clustered data, an artificial neural\nnetwork with logcosh loss learns the bigger cluster rather than the mean of the\ntwo. Even more so, the ANN, when used for regression of a set-valued function,\nwill learn a value close to one of the choices, in other words, one branch of\nthe set-valued function, while a mean-square-error NN will learn the value in\nbetween. This work suggests a method that uses artificial neural networks with\nlogcosh loss to find the branches of set-valued mappings in parameter-outcome\nsample sets and classifies the samples according to those branches.\n", "versions": [{"version": "v1", "created": "Mon, 25 Jan 2021 21:37:37 GMT"}], "update_date": "2021-01-27", "authors_parsed": [["Moshagen", "Thilo", ""], ["Adde", "Nihal Acharya", ""], ["Rajgopal", "Ajay Navilarekal", ""]]}, {"id": "2101.10435", "submitter": "Maria Leonor Pacheco", "authors": "Manuel Widmoser, Maria Leonor Pacheco, Jean Honorio, Dan Goldwasser", "title": "Randomized Deep Structured Prediction for Discourse-Level Processing", "comments": "Accepted to EACL 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Expressive text encoders such as RNNs and Transformer Networks have been at\nthe center of NLP models in recent work. Most of the effort has focused on\nsentence-level tasks, capturing the dependencies between words in a single\nsentence, or pairs of sentences. However, certain tasks, such as argumentation\nmining, require accounting for longer texts and complicated structural\ndependencies between them. Deep structured prediction is a general framework to\ncombine the complementary strengths of expressive neural encoders and\nstructured inference for highly structured domains. Nevertheless, when the need\narises to go beyond sentences, most work relies on combining the output scores\nof independently trained classifiers. One of the main reasons for this is that\nconstrained inference comes at a high computational cost. In this paper, we\nexplore the use of randomized inference to alleviate this concern and show that\nwe can efficiently leverage deep structured prediction and expressive neural\nencoders for a set of tasks involving complicated argumentative structures.\n", "versions": [{"version": "v1", "created": "Mon, 25 Jan 2021 21:49:32 GMT"}], "update_date": "2021-01-27", "authors_parsed": [["Widmoser", "Manuel", ""], ["Pacheco", "Maria Leonor", ""], ["Honorio", "Jean", ""], ["Goldwasser", "Dan", ""]]}, {"id": "2101.10437", "submitter": "Jun Zhu", "authors": "Jun Zhu, Ye Chen, Frank Brinker, Winfried Decking, Sergey Tomin,\n  Holger Schlarb", "title": "Deep Learning-Based Autoencoder for Data-Driven Modeling of an RF\n  Photoinjector", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG physics.acc-ph", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Modeling of large-scale research facilities is extremely challenging due to\ncomplex physical processes and engineering problems. Here, we adopt a\ndata-driven approach to model the photoinector of European XFEL with a deep\nlearning-based autoencoder. A deep convolutional neural network (decoder) is\nused to build images measured on the screen from a small feature map generated\nby another neural network (encoder). We demonstrate that the autoencoder\ntrained only with experimental data can make high-fidelity predictions of\nmegapixel images for the longitudinal phase-space measurement. The prediction\nsignificantly outperforms existing methods. We also show the scalability and\nexplicability of the autoencoder by sharing the same decoder with more than one\nencoder used for different setups of the photoinjector, and propose a pragmatic\nway to model a photoinjector with various diagnostics and working points. This\nopens the door to a new way of accurately modeling a photoinjector using neural\nnetworks. The approach can possibly be extended to the whole accelerator and\neven other types of scientific facilities.\n", "versions": [{"version": "v1", "created": "Mon, 25 Jan 2021 21:59:09 GMT"}, {"version": "v2", "created": "Thu, 18 Feb 2021 00:21:03 GMT"}, {"version": "v3", "created": "Thu, 11 Mar 2021 19:43:48 GMT"}], "update_date": "2021-03-15", "authors_parsed": [["Zhu", "Jun", ""], ["Chen", "Ye", ""], ["Brinker", "Frank", ""], ["Decking", "Winfried", ""], ["Tomin", "Sergey", ""], ["Schlarb", "Holger", ""]]}, {"id": "2101.10443", "submitter": "Piduguralla Manaswini", "authors": "Piduguralla Manaswini, Jignesh S. Bhatt", "title": "Towards glass-box CNNs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolution neural networks (CNNs) are brain-inspired architectures popular\nfor their ability to train and relearn visually complex tasks. It is\nincremental and scalable; however, CNN is mostly treated as black-box and\ninvolves multiple trial & error runs. We observe that CNN constructs powerful\ninternal representations that help achieve state-of-the-art performance. Here\nwe propose three layer glass-box (analytical) CNN for two-class image\nclassifcation problems. First is a representation layer that encompasses both\nthe class information (group invariant) and symmetric transformations (group\nequivariant) of input images. It is then passed through dimension reduction\nlayer (PCA). Finally the compact yet complete representation is provided to a\nclassifer. Analytical machine learning classifers and multilayer perceptrons\nare used to assess sensitivity. Proposed glass-box CNN is compared with\nequivariance of AlexNet (CNN) internal representation for better understanding\nand dissemination of results. In future, we would like to construct glass-box\nCNN for multiclass visually complex tasks.\n", "versions": [{"version": "v1", "created": "Mon, 11 Jan 2021 15:00:35 GMT"}], "update_date": "2021-01-27", "authors_parsed": [["Manaswini", "Piduguralla", ""], ["Bhatt", "Jignesh S.", ""]]}, {"id": "2101.10450", "submitter": "Enkhtogtokh Togootogtokh", "authors": "Enkhtogtokh Togootogtokh, Christian Klasen", "title": "LAIF: AI, Deep Learning for Germany Suetterlin Letter Recognition and\n  Generation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  One of the successful early implementation of deep learning AI technology was\non letter recognition. With the recent breakthrough of artificial intelligence\n(AI) brings more solid technology for complex problems like handwritten letter\nrecognition and even automatic generation of them. In this research, we\nproposed deep learning framework called Ludwig AI Framework(LAIF) for Germany\nSuetterlin letter recognition and generation. To recognize Suetterlin letter,\nwe proposed deep convolutional neural network. Since lack of big amount of data\nto train for the deep models and huge cost to label existing hard copy of\nhandwritten letters, we also introduce the methodology with deep generative\nadversarial network to generate handwritten letters as synthetic data. Main\nsource code is in https://github.com/enkhtogtokh/LAIF repository.\n", "versions": [{"version": "v1", "created": "Wed, 30 Dec 2020 08:28:11 GMT"}], "update_date": "2021-01-27", "authors_parsed": [["Togootogtokh", "Enkhtogtokh", ""], ["Klasen", "Christian", ""]]}, {"id": "2101.10451", "submitter": "Vanlin Sathya", "authors": "Vanlin Sathya", "title": "Evolution of Small Cell from 4G to 6G: Past, Present, and Future", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  To boost the capacity of the cellular system, the operators have started to\nreuse the same licensed spectrum by deploying 4G LTE small cells (Femto Cells)\nin the past. But in time, these small cell licensed spectrum is not sufficient\nto satisfy future applications like augmented reality (AR)and virtual reality\n(VR). Hence, cellular operators look for alternate unlicensed spectrum in Wi-Fi\n5 GHz band, later 3GPP named as LTE Licensed Assisted Access (LAA). The recent\nand current rollout of LAA deployments (in developed nations like the US)\nprovides an opportunity to understand coexistence profound ground truth. This\npaper discusses a high-level overview of my past, present, and future research\nworks in the direction of small cell benefits. In the future, we shift the\nfocus onto the latest unlicensed band: 6 GHz, where the latest Wi-Fi version,\n802.11ax, will coexist with the latest cellular technology, 5G New Radio(NR) in\nunlicensed\n", "versions": [{"version": "v1", "created": "Tue, 29 Dec 2020 17:28:08 GMT"}], "update_date": "2021-01-27", "authors_parsed": [["Sathya", "Vanlin", ""]]}, {"id": "2101.10460", "submitter": "Brian Quanz", "authors": "Nam Nguyen, Brian Quanz", "title": "Temporal Latent Auto-Encoder: A Method for Probabilistic Multivariate\n  Time Series Forecasting", "comments": "Accepted at AAAI 2021 (main conference)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Probabilistic forecasting of high dimensional multivariate time series is a\nnotoriously challenging task, both in terms of computational burden and\ndistribution modeling. Most previous work either makes simple distribution\nassumptions or abandons modeling cross-series correlations. A promising line of\nwork exploits scalable matrix factorization for latent-space forecasting, but\nis limited to linear embeddings, unable to model distributions, and not\ntrainable end-to-end when using deep learning forecasting. We introduce a novel\ntemporal latent auto-encoder method which enables nonlinear factorization of\nmultivariate time series, learned end-to-end with a temporal deep learning\nlatent space forecast model. By imposing a probabilistic latent space model,\ncomplex distributions of the input series are modeled via the decoder.\nExtensive experiments demonstrate that our model achieves state-of-the-art\nperformance on many popular multivariate datasets, with gains sometimes as high\nas $50\\%$ for several standard metrics.\n", "versions": [{"version": "v1", "created": "Mon, 25 Jan 2021 22:29:40 GMT"}], "update_date": "2021-01-27", "authors_parsed": [["Nguyen", "Nam", ""], ["Quanz", "Brian", ""]]}, {"id": "2101.10461", "submitter": "Anthony Constantinou", "authors": "Anthony C. Constantinou, Norman Fenton, Martin Neil", "title": "How do some Bayesian Network machine learned graphs compare to causal\n  knowledge?", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The graph of a Bayesian Network (BN) can be machine learned, determined by\ncausal knowledge, or a combination of both. In disciplines like bioinformatics,\napplying BN structure learning algorithms can reveal new insights that would\notherwise remain unknown. However, these algorithms are less effective when the\ninput data are limited in terms of sample size, which is often the case when\nworking with real data. This paper focuses on purely machine learned and purely\nknowledge-based BNs and investigates their differences in terms of graphical\nstructure and how well the implied statistical models explain the data. The\ntests are based on four previous case studies whose BN structure was determined\nby domain knowledge. Using various metrics, we compare the knowledge-based\ngraphs to the machine learned graphs generated from various algorithms\nimplemented in TETRAD spanning all three classes of learning. The results show\nthat, while the algorithms produce graphs with much higher model selection\nscore, the knowledge-based graphs are more accurate predictors of variables of\ninterest. Maximising score fitting is ineffective in the presence of limited\nsample size because the fitting becomes increasingly distorted with limited\ndata, guiding algorithms towards graphical patterns that share higher fitting\nscores and yet deviate considerably from the true graph. This highlights the\nvalue of causal knowledge in these cases, as well as the need for more\nappropriate fitting scores suitable for limited data. Lastly, the experiments\nalso provide new evidence that support the notion that results from simulated\ndata tell us little about actual real-world performance.\n", "versions": [{"version": "v1", "created": "Mon, 25 Jan 2021 22:29:54 GMT"}, {"version": "v2", "created": "Tue, 2 Feb 2021 15:10:57 GMT"}], "update_date": "2021-02-03", "authors_parsed": [["Constantinou", "Anthony C.", ""], ["Fenton", "Norman", ""], ["Neil", "Martin", ""]]}, {"id": "2101.10471", "submitter": "M. Ali Vosoughi", "authors": "Axel Wism\\\"uller and M. Ali Vosoughi", "title": "Classification of Schizophrenia from Functional MRI Using Large-scale\n  Extended Granger Causality", "comments": "The paper is the preprint of the paper accepted at the SPIE 2021\n  conference. The manuscript includes 14 pages with two figures. arXiv admin\n  note: substantial text overlap with arXiv:2101.01832. text overlap with\n  arXiv:2101.09354", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC cs.LG eess.IV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The literature manifests that schizophrenia is associated with alterations in\nbrain network connectivity. We investigate whether large-scale Extended Granger\nCausality (lsXGC) can capture such alterations using resting-state fMRI data.\nOur method utilizes dimension reduction combined with the augmentation of\nsource time-series in a predictive time-series model for estimating directed\ncausal relationships among fMRI time-series. The lsXGC is a multivariate\napproach since it identifies the relationship of the underlying dynamic system\nin the presence of all other time-series. Here lsXGC serves as a biomarker for\nclassifying schizophrenia patients from typical controls using a subset of 62\nsubjects from the Centers of Biomedical Research Excellence (COBRE) data\nrepository. We use brain connections estimated by lsXGC as features for\nclassification. After feature extraction, we perform feature selection by\nKendall's tau rank correlation coefficient followed by classification using a\nsupport vector machine. As a reference method, we compare our results with\ncross-correlation, typically used in the literature as a standard measure of\nfunctional connectivity. We cross-validate 100 different training/test\n(90%/10%) data split to obtain mean accuracy and a mean Area Under the receiver\noperating characteristic Curve (AUC) across all tested numbers of features for\nlsXGC. Our results demonstrate a mean accuracy range of [0.767, 0.940] and a\nmean AUC range of [0.861, 0.983] for lsXGC. The result of lsXGC is\nsignificantly higher than the results obtained with the cross-correlation,\nnamely mean accuracy of [0.721, 0.751] and mean AUC of [0.744, 0.860]. Our\nresults suggest the applicability of lsXGC as a potential biomarker for\nschizophrenia.\n", "versions": [{"version": "v1", "created": "Tue, 12 Jan 2021 20:36:26 GMT"}], "update_date": "2021-01-27", "authors_parsed": [["Wism\u00fcller", "Axel", ""], ["Vosoughi", "M. Ali", ""]]}, {"id": "2101.10472", "submitter": "Abdelkareem Jaradat", "authors": "Abdelkareem Jaradat, Hanan Lutfiyya, Anwar Haque", "title": "Appliance Operation Modes Identification Using Cycles Clustering", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The increasing cost, energy demand, and environmental issues has led many\nresearchers to find approaches for energy monitoring, and hence energy\nconservation. The emerging technologies of Internet of Things (IoT) and Machine\nLearning (ML) deliver techniques that have the potential to efficiently\nconserve energy and improve the utilization of energy consumption. Smart Home\nEnergy Management Systems (SHEMSs) have the potential to contribute in energy\nconservation through the application of Demand Response (DR) in the residential\nsector. In this paper, we propose appliances Operation Modes Identification\nusing Cycles Clustering (OMICC) which is SHEMS fundamental approach that\nutilizes the sensed residential disaggregated power consumption in supporting\nDR by providing consumers the opportunity to select lighter appliance operation\nmodes. The cycles of the Single Usage Profile (SUP) of an appliance are\nextracted and reformed into features in terms of clusters of cycles. These\nfeatures are then used to identify the operation mode used in every occurrence\nusing K-Nearest Neighbors (KNN). Operation modes identification is considered a\nbasis for many potential smart DR applications within SHEMS towards the\nconsumers or the suppliers\n", "versions": [{"version": "v1", "created": "Mon, 25 Jan 2021 23:25:45 GMT"}], "update_date": "2021-01-27", "authors_parsed": [["Jaradat", "Abdelkareem", ""], ["Lutfiyya", "Hanan", ""], ["Haque", "Anwar", ""]]}, {"id": "2101.10488", "submitter": "EPTCS", "authors": "Paul Wilson (University of Southampton), Fabio Zanasi (University\n  College London)", "title": "Reverse Derivative Ascent: A Categorical Approach to Learning Boolean\n  Circuits", "comments": "In Proceedings ACT 2020, arXiv:2101.07888", "journal-ref": "EPTCS 333, 2021, pp. 247-260", "doi": "10.4204/EPTCS.333.17", "report-no": null, "categories": "cs.LO cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce Reverse Derivative Ascent: a categorical analogue of gradient\nbased methods for machine learning. Our algorithm is defined at the level of\nso-called reverse differential categories. It can be used to learn the\nparameters of models which are expressed as morphisms of such categories. Our\nmotivating example is boolean circuits: we show how our algorithm can be\napplied to such circuits by using the theory of reverse differential\ncategories. Note our methodology allows us to learn the parameters of boolean\ncircuits directly, in contrast to existing binarised neural network approaches.\nMoreover, we demonstrate its empirical value by giving experimental results on\nbenchmark machine learning datasets.\n", "versions": [{"version": "v1", "created": "Tue, 26 Jan 2021 00:07:20 GMT"}], "update_date": "2021-01-27", "authors_parsed": [["Wilson", "Paul", "", "University of Southampton"], ["Zanasi", "Fabio", "", "University\n  College London"]]}, {"id": "2101.10502", "submitter": "Wilson Marc\\'ilio-Jr", "authors": "Wilson E. Marc\\'ilio-Jr, Danilo M. Eler, Fabr\\'icio Breve", "title": "Model-agnostic interpretation by visualization of feature perturbations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Interpretation of machine learning models has become one of the most\nimportant topics of research due to the necessity of maintaining control and\navoid bias in these algorithms. Since many machine learning algorithms are\npublished every day, there is a need for novel model-agnostic interpretation\napproaches that could be used to interpret a great variety of algorithms. One\nparticularly useful way to interpret machine learning models is to feed\ndifferent input data to understand the changes in the prediction. Using such an\napproach, practitioners can define relations among patterns of data and a\nmodel's decision. In this work, we propose a model-agnostic interpretation\napproach that uses visualization of feature perturbations induced by the\nparticle swarm optimization algorithm. We validate our approach both\nqualitatively and quantitatively on publicly available datasets, showing the\ncapability to enhance the interpretation of different classifiers while\nyielding very stable results if compared with the state of the art algorithms.\n", "versions": [{"version": "v1", "created": "Tue, 26 Jan 2021 00:53:29 GMT"}], "update_date": "2021-01-27", "authors_parsed": [["Marc\u00edlio-Jr", "Wilson E.", ""], ["Eler", "Danilo M.", ""], ["Breve", "Fabr\u00edcio", ""]]}, {"id": "2101.10506", "submitter": "Sajad Khodadadian", "authors": "Sajad Khodadadian, Thinh T. Doan, Siva Theja Maguluri, Justin Romberg", "title": "Finite Sample Analysis of Two-Time-Scale Natural Actor-Critic Algorithm", "comments": "34 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Actor-critic style two-time-scale algorithms are very popular in\nreinforcement learning, and have seen great empirical success. However, their\nperformance is not completely understood theoretically. In this paper, we\ncharacterize the global convergence of an online natural actor-critic algorithm\nin the tabular setting using a single trajectory. Our analysis applies to very\ngeneral settings, as we only assume that the underlying Markov chain is ergodic\nunder all policies (the so-called Recurrence assumption). We employ\n$\\epsilon$-greedy sampling in order to ensure enough exploration.\n  For a fixed exploration parameter $\\epsilon$, we show that the natural actor\ncritic algorithm is $\\mathcal{O}(\\frac{1}{\\epsilon T^{1/4}}+\\epsilon)$ close to\nthe global optimum after $T$ iterations of the algorithm.\n  By carefully diminishing the exploration parameter $\\epsilon$ as the\niterations proceed, we also show convergence to the global optimum at a rate of\n$\\mathcal{O}(1/T^{1/6})$.\n", "versions": [{"version": "v1", "created": "Tue, 26 Jan 2021 01:12:07 GMT"}], "update_date": "2021-01-27", "authors_parsed": [["Khodadadian", "Sajad", ""], ["Doan", "Thinh T.", ""], ["Maguluri", "Siva Theja", ""], ["Romberg", "Justin", ""]]}, {"id": "2101.10509", "submitter": "Ali Ayub", "authors": "Ali Ayub, Alan R. Wagner", "title": "Continual Learning of Visual Concepts for Robots through Limited\n  Supervision", "comments": "Accepted at ACM/IEEE HRI 2021, Pioneers Workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  For many real-world robotics applications, robots need to continually adapt\nand learn new concepts. Further, robots need to learn through limited data\nbecause of scarcity of labeled data in the real-world environments. To this\nend, my research focuses on developing robots that continually learn in dynamic\nunseen environments/scenarios, learn from limited human supervision, remember\npreviously learned knowledge and use that knowledge to learn new concepts. I\ndevelop machine learning models that not only produce State-of-the-results on\nbenchmark datasets but also allow robots to learn new objects and scenes in\nunconstrained environments which lead to a variety of novel robotics\napplications.\n", "versions": [{"version": "v1", "created": "Tue, 26 Jan 2021 01:26:07 GMT"}], "update_date": "2021-01-27", "authors_parsed": [["Ayub", "Ali", ""], ["Wagner", "Alan R.", ""]]}, {"id": "2101.10524", "submitter": "Abhinav Arora", "authors": "Arash Einolghozati, Abhinav Arora, Lorena Sainz-Maza Lecanda, Anuj\n  Kumar, Sonal Gupta", "title": "El Volumen Louder Por Favor: Code-switching in Task-oriented Semantic\n  Parsing", "comments": null, "journal-ref": "EACL 2021", "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Being able to parse code-switched (CS) utterances, such as Spanish+English or\nHindi+English, is essential to democratize task-oriented semantic parsing\nsystems for certain locales. In this work, we focus on Spanglish\n(Spanish+English) and release a dataset, CSTOP, containing 5800 CS utterances\nalongside their semantic parses. We examine the CS generalizability of various\nCross-lingual (XL) models and exhibit the advantage of pre-trained XL language\nmodels when data for only one language is present. As such, we focus on\nimproving the pre-trained models for the case when only English corpus\nalongside either zero or a few CS training instances are available. We propose\ntwo data augmentation methods for the zero-shot and the few-shot settings:\nfine-tune using translate-and-align and augment using a generation model\nfollowed by match-and-filter. Combining the few-shot setting with the above\nimprovements decreases the initial 30-point accuracy gap between the zero-shot\nand the full-data settings by two thirds.\n", "versions": [{"version": "v1", "created": "Tue, 26 Jan 2021 02:40:44 GMT"}, {"version": "v2", "created": "Wed, 27 Jan 2021 04:28:49 GMT"}, {"version": "v3", "created": "Thu, 28 Jan 2021 08:09:08 GMT"}], "update_date": "2021-01-29", "authors_parsed": [["Einolghozati", "Arash", ""], ["Arora", "Abhinav", ""], ["Lecanda", "Lorena Sainz-Maza", ""], ["Kumar", "Anuj", ""], ["Gupta", "Sonal", ""]]}, {"id": "2101.10532", "submitter": "Muhammad Ahmad", "authors": "Muhammad Ahmad, Sidrah Shabbir, Rana Aamir Raza, Manuel Mazzara,\n  Salvatore Distefano, Adil Mehmood Khan", "title": "Hyperspectral Image Classification: Artifacts of Dimension Reduction on\n  Hybrid CNN", "comments": "9 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Convolutional Neural Networks (CNN) has been extensively studied for\nHyperspectral Image Classification (HSIC) more specifically, 2D and 3D CNN\nmodels have proved highly efficient in exploiting the spatial and spectral\ninformation of Hyperspectral Images. However, 2D CNN only considers the spatial\ninformation and ignores the spectral information whereas 3D CNN jointly\nexploits spatial-spectral information at a high computational cost. Therefore,\nthis work proposed a lightweight CNN (3D followed by 2D-CNN) model which\nsignificantly reduces the computational cost by distributing spatial-spectral\nfeature extraction across a lighter model alongside a preprocessing that has\nbeen carried out to improve the classification results. Five benchmark\nHyperspectral datasets (i.e., SalinasA, Salinas, Indian Pines, Pavia\nUniversity, Pavia Center, and Botswana) are used for experimental evaluation.\nThe experimental results show that the proposed pipeline outperformed in terms\nof generalization performance, statistical significance, and computational\ncomplexity, as compared to the state-of-the-art 2D/3D CNN models except\ncommonly used computationally expensive design choices.\n", "versions": [{"version": "v1", "created": "Mon, 25 Jan 2021 18:43:57 GMT"}], "update_date": "2021-03-02", "authors_parsed": [["Ahmad", "Muhammad", ""], ["Shabbir", "Sidrah", ""], ["Raza", "Rana Aamir", ""], ["Mazzara", "Manuel", ""], ["Distefano", "Salvatore", ""], ["Khan", "Adil Mehmood", ""]]}, {"id": "2101.10534", "submitter": "Chirag Gupta", "authors": "Chirag Gupta", "title": "Modern Machine and Deep Learning Systems as a way to achieve\n  Man-Computer Symbiosis", "comments": "8 pages, 1 figure. Collaboration ongoing with coauthors for final\n  manuscript. To be submitted to the IEEE access for peer review", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Man-Computer Symbiosis (MCS) was originally envisioned by the famous computer\npioneer J.C.R. Licklider in 1960, as a logical evolution of the then inchoate\nrelationship between computer and humans. In his paper, Licklider provided a\nset of criteria by which to judge if a Man-Computer System is a symbiotic one,\nand also provided some predictions about such systems in the near and far\nfuture. Since then, innovations in computer networks and the invention of the\nInternet were major developments towards that end. However, with most systems\nbased on conventional logical algorithms, many aspects of Licklider's MCS\nremained unfulfilled. This paper explores the extent to which modern machine\nlearning systems in general, and deep learning ones in particular best\nexemplify MCS systems, and why they are the prime contenders to achieve a true\nMan-Computer Symbiosis as described by Licklider in his original paper in the\nfuture. The case for deep learning is built by illustrating each point of the\noriginal criteria as well as the criteria laid by subsequent research into MCS\nsystems, with specific examples and applications provided to strengthen the\narguments. The efficacy of deep neural networks in achieving Artificial General\nIntelligence, which would be the perfect version of an MCS system is also\nexplored.\n", "versions": [{"version": "v1", "created": "Sun, 24 Jan 2021 17:55:21 GMT"}], "update_date": "2021-01-27", "authors_parsed": [["Gupta", "Chirag", ""]]}, {"id": "2101.10537", "submitter": "Joseph Marvin Imperial", "authors": "Joseph Marvin Imperial, Ethel Ong", "title": "Application of Lexical Features Towards Improvement of Filipino\n  Readability Identification of Children's Literature", "comments": "8 tables, 1 figure. Presented at the Philippine Computing Science\n  Congress 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Proper identification of grade levels of children's reading materials is an\nimportant step towards effective learning. Recent studies in readability\nassessment for the English domain applied modern approaches in natural language\nprocessing (NLP) such as machine learning (ML) techniques to automate the\nprocess. There is also a need to extract the correct linguistic features when\nmodeling readability formulas. In the context of the Filipino language, limited\nwork has been done [1, 2], especially in considering the language's lexical\ncomplexity as main features. In this paper, we explore the use of lexical\nfeatures towards improving the development of readability identification of\nchildren's books written in Filipino. Results show that combining lexical\nfeatures (LEX) consisting of type-token ratio, lexical density, lexical\nvariation, foreign word count with traditional features (TRAD) used by previous\nworks such as sentence length, average syllable length, polysyllabic words,\nword, sentence, and phrase counts increased the performance of readability\nmodels by almost a 5% margin (from 42% to 47.2%). Further analysis and ranking\nof the most important features were shown to identify which features contribute\nthe most in terms of reading complexity.\n", "versions": [{"version": "v1", "created": "Fri, 22 Jan 2021 19:54:37 GMT"}], "update_date": "2021-01-27", "authors_parsed": [["Imperial", "Joseph Marvin", ""], ["Ong", "Ethel", ""]]}, {"id": "2101.10540", "submitter": "Nikolaos Athanasios Anagnostopoulos", "authors": "Nikolaos Athanasios Anagnostopoulos", "title": "Ear Recognition", "comments": "Submission to Biometrics in University of Twente under the auspices\n  of the EIT ICT Labs Master School in the academic year 2013-14", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CR cs.LG eess.IV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Ear recognition can be described as a revived scientific field. Ear\nbiometrics were long believed to not be accurate enough and held a secondary\nplace in scientific research, being seen as only complementary to other types\nof biometrics, due to difficulties in measuring correctly the ear\ncharacteristics and the potential occlusion of the ear by hair, clothes and ear\njewellery. However, recent research has reinstated them as a vivid research\nfield, after having addressed these problems and proven that ear biometrics can\nprovide really accurate identification and verification results. Several 2D and\n3D imaging techniques, as well as acoustical techniques using sound emission\nand reflection, have been developed and studied for ear recognition, while\nthere have also been significant advances towards a fully automated recognition\nof the ear. Furthermore, ear biometrics have been proven to be mostly\nnon-invasive, adequately permanent and accurate, and hard to spoof and\ncounterfeit. Moreover, different ear recognition techniques have proven to be\nas effective as face recognition ones, thus providing the opportunity for ear\nrecognition to be used in identification and verification applications.\nFinally, even though some issues still remain open and require further\nresearch, the scientific field of ear biometrics has proven to be not only\nviable, but really thriving.\n", "versions": [{"version": "v1", "created": "Tue, 26 Jan 2021 03:26:00 GMT"}], "update_date": "2021-01-27", "authors_parsed": [["Anagnostopoulos", "Nikolaos Athanasios", ""]]}, {"id": "2101.10542", "submitter": "Jonathan Libgober", "authors": "In-Koo Cho and Jonathan Libgober", "title": "Iterative Weak Learnability and Multi-Class AdaBoost", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We construct an efficient recursive ensemble algorithm for the multi-class\nclassification problem, inspired by SAMME (Zhu, Zou, Rosset, and Hastie\n(2009)). We strengthen the weak learnability condition in Zhu, Zou, Rosset, and\nHastie (2009) by requiring that the weak learnability condition holds for any\nsubset of labels with at least two elements. This condition is simpler to check\nthan many proposed alternatives (e.g., Mukherjee and Schapire (2013)). As\nSAMME, our algorithm is reduced to the Adaptive Boosting algorithm (Schapire\nand Freund (2012)) if the number of labels is two, and can be motivated as a\nfunctional version of the steepest descending method to find an optimal\nsolution. In contrast to SAMME, our algorithm's final hypothesis converges to\nthe correct label with probability 1. For any number of labels, the probability\nof misclassification vanishes exponentially as the training period increases.\nThe sum of the training error and an additional term, that depends only on the\nsample size, bounds the generalization error of our algorithm as the Adaptive\nBoosting algorithm.\n", "versions": [{"version": "v1", "created": "Tue, 26 Jan 2021 03:30:30 GMT"}], "update_date": "2021-01-27", "authors_parsed": [["Cho", "In-Koo", ""], ["Libgober", "Jonathan", ""]]}, {"id": "2101.10552", "submitter": "Udit Saxena", "authors": "Thomas Gebhart, Udit Saxena, Paul Schrater", "title": "A Unified Paths Perspective for Pruning at Initialization", "comments": "13 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A number of recent approaches have been proposed for pruning neural network\nparameters at initialization with the goal of reducing the size and\ncomputational burden of models while minimally affecting their training\ndynamics and generalization performance. While each of these approaches have\nsome amount of well-founded motivation, a rigorous analysis of the effect of\nthese pruning methods on network training dynamics and their formal\nrelationship to each other has thus far received little attention. Leveraging\nrecent theoretical approximations provided by the Neural Tangent Kernel, we\nunify a number of popular approaches for pruning at initialization under a\nsingle path-centric framework. We introduce the Path Kernel as the\ndata-independent factor in a decomposition of the Neural Tangent Kernel and\nshow the global structure of the Path Kernel can be computed efficiently. This\nPath Kernel decomposition separates the architectural effects from the\ndata-dependent effects within the Neural Tangent Kernel, providing a means to\npredict the convergence dynamics of a network from its architecture alone. We\nanalyze the use of this structure in approximating training and generalization\nperformance of networks in the absence of data across a number of\ninitialization pruning approaches. Observing the relationship between input\ndata and paths and the relationship between the Path Kernel and its natural\nnorm, we additionally propose two augmentations of the SynFlow algorithm for\npruning at initialization.\n", "versions": [{"version": "v1", "created": "Tue, 26 Jan 2021 04:29:50 GMT"}], "update_date": "2021-01-27", "authors_parsed": [["Gebhart", "Thomas", ""], ["Saxena", "Udit", ""], ["Schrater", "Paul", ""]]}, {"id": "2101.10553", "submitter": "Zijiang Yang", "authors": "Zijiang Yang, Dipendra Jha, Arindam Paul, Wei-keng Liao, Alok\n  Choudhary, Ankit Agrawal", "title": "A General Framework Combining Generative Adversarial Networks and\n  Mixture Density Networks for Inverse Modeling in Microstructural Materials\n  Design", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Microstructural materials design is one of the most important applications of\ninverse modeling in materials science. Generally speaking, there are two broad\nmodeling paradigms in scientific applications: forward and inverse. While the\nforward modeling estimates the observations based on known parameters, the\ninverse modeling attempts to infer the parameters given the observations.\nInverse problems are usually more critical as well as difficult in scientific\napplications as they seek to explore the parameters that cannot be directly\nobserved. Inverse problems are used extensively in various scientific fields,\nsuch as geophysics, healthcare and materials science. However, it is\nchallenging to solve inverse problems, because they usually need to learn a\none-to-many non-linear mapping, and also require significant computing time,\nespecially for high-dimensional parameter space. Further, inverse problems\nbecome even more difficult to solve when the dimension of input (i.e.\nobservation) is much lower than that of output (i.e. parameters). In this work,\nwe propose a framework consisting of generative adversarial networks and\nmixture density networks for inverse modeling, and it is evaluated on a\nmaterials science dataset for microstructural materials design. Compared with\nbaseline methods, the results demonstrate that the proposed framework can\novercome the above-mentioned challenges and produce multiple promising\nsolutions in an efficient manner.\n", "versions": [{"version": "v1", "created": "Tue, 26 Jan 2021 04:30:31 GMT"}], "update_date": "2021-01-27", "authors_parsed": [["Yang", "Zijiang", ""], ["Jha", "Dipendra", ""], ["Paul", "Arindam", ""], ["Liao", "Wei-keng", ""], ["Choudhary", "Alok", ""], ["Agrawal", "Ankit", ""]]}, {"id": "2101.10562", "submitter": "Utku Ozbulak", "authors": "Utku Ozbulak, Baptist Vandersmissen, Azarakhsh Jalalvand, Ivo\n  Couckuyt, Arnout Van Messem, Wesley De Neve", "title": "Investigating the significance of adversarial attacks and their relation\n  to interpretability for radar-based human activity recognition systems", "comments": "Accepted for publication on Computer Vision and Image Understanding,\n  Special issue on Adversarial Deep Learning in Biometrics & Forensics", "journal-ref": null, "doi": "10.1016/j.cviu.2020.103111", "report-no": null, "categories": "cs.CV cs.LG cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given their substantial success in addressing a wide range of computer vision\nchallenges, Convolutional Neural Networks (CNNs) are increasingly being used in\nsmart home applications, with many of these applications relying on the\nautomatic recognition of human activities. In this context, low-power radar\ndevices have recently gained in popularity as recording sensors, given that the\nusage of these devices allows mitigating a number of privacy concerns, a key\nissue when making use of conventional video cameras. Another concern that is\noften cited when designing smart home applications is the resilience of these\napplications against cyberattacks. It is, for instance, well-known that the\ncombination of images and CNNs is vulnerable against adversarial examples,\nmischievous data points that force machine learning models to generate wrong\nclassifications during testing time. In this paper, we investigate the\nvulnerability of radar-based CNNs to adversarial attacks, and where these\nradar-based CNNs have been designed to recognize human gestures. Through\nexperiments with four unique threat models, we show that radar-based CNNs are\nsusceptible to both white- and black-box adversarial attacks. We also expose\nthe existence of an extreme adversarial attack case, where it is possible to\nchange the prediction made by the radar-based CNNs by only perturbing the\npadding of the inputs, without touching the frames where the action itself\noccurs. Moreover, we observe that gradient-based attacks exercise perturbation\nnot randomly, but on important features of the input data. We highlight these\nimportant features by making use of Grad-CAM, a popular neural network\ninterpretability method, hereby showing the connection between adversarial\nperturbation and prediction interpretability.\n", "versions": [{"version": "v1", "created": "Tue, 26 Jan 2021 05:16:16 GMT"}], "update_date": "2021-01-27", "authors_parsed": [["Ozbulak", "Utku", ""], ["Vandersmissen", "Baptist", ""], ["Jalalvand", "Azarakhsh", ""], ["Couckuyt", "Ivo", ""], ["Van Messem", "Arnout", ""], ["De Neve", "Wesley", ""]]}, {"id": "2101.10572", "submitter": "Shuaicheng Ma", "authors": "Shuaicheng Ma, Yang Cao, Li Xiong", "title": "Transparent Contribution Evaluation for Secure Federated Learning on\n  Blockchain", "comments": null, "journal-ref": "2021 IEEE 37th International Conference on Data Engineering\n  Workshops (ICDEW)", "doi": "10.1109/ICDEW53142.2021.00023", "report-no": null, "categories": "cs.LG cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Federated Learning is a promising machine learning paradigm when multiple\nparties collaborate to build a high-quality machine learning model.\nNonetheless, these parties are only willing to participate when given enough\nincentives, such as a fair reward based on their contributions. Many studies\nexplored Shapley value based methods to evaluate each party's contribution to\nthe learned model. However, they commonly assume a semi-trusted server to train\nthe model and evaluate the data owners' model contributions, which lacks\ntransparency and may hinder the success of federated learning in practice. In\nthis work, we propose a blockchain-based federated learning framework and a\nprotocol to transparently evaluate each participant's contribution. Our\nframework protects all parties' privacy in the model building phase and\ntransparently evaluates contributions based on the model updates. The\nexperiment with the handwritten digits dataset demonstrates that the proposed\nmethod can effectively evaluate the contributions.\n", "versions": [{"version": "v1", "created": "Tue, 26 Jan 2021 05:49:59 GMT"}, {"version": "v2", "created": "Sat, 24 Apr 2021 04:23:37 GMT"}], "update_date": "2021-06-10", "authors_parsed": [["Ma", "Shuaicheng", ""], ["Cao", "Yang", ""], ["Xiong", "Li", ""]]}, {"id": "2101.10578", "submitter": "Tajuddin Manhar Mohammed", "authors": "Tajuddin Manhar Mohammed, Lakshmanan Nataraj, Satish Chikkagoudar,\n  Shivkumar Chandrasekaran, B.S. Manjunath", "title": "Malware Detection Using Frequency Domain-Based Image Visualization and\n  Deep Learning", "comments": "Submitted version - Proceedings of the 54th Hawaii International\n  Conference on System Sciences (HICSS) 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  We propose a novel method to detect and visualize malware through image\nclassification. The executable binaries are represented as grayscale images\nobtained from the count of N-grams (N=2) of bytes in the Discrete Cosine\nTransform (DCT) domain and a neural network is trained for malware detection. A\nshallow neural network is trained for classification, and its accuracy is\ncompared with deep-network architectures such as ResNet that are trained using\ntransfer learning. Neither dis-assembly nor behavioral analysis of malware is\nrequired for these methods. Motivated by the visual similarity of these images\nfor different malware families, we compare our deep neural network models with\nstandard image features like GIST descriptors to evaluate the performance. A\njoint feature measure is proposed to combine different features using error\nanalysis to get an accurate ensemble model for improved classification\nperformance. A new dataset called MaleX which contains around 1 million malware\nand benign Windows executable samples is created for large-scale malware\ndetection and classification experiments. Experimental results are quite\npromising with 96% binary classification accuracy on MaleX. The proposed model\nis also able to generalize well on larger unseen malware samples and the\nresults compare favorably with state-of-the-art static analysis-based malware\ndetection algorithms.\n", "versions": [{"version": "v1", "created": "Tue, 26 Jan 2021 06:07:46 GMT"}], "update_date": "2021-01-27", "authors_parsed": [["Mohammed", "Tajuddin Manhar", ""], ["Nataraj", "Lakshmanan", ""], ["Chikkagoudar", "Satish", ""], ["Chandrasekaran", "Shivkumar", ""], ["Manjunath", "B. S.", ""]]}, {"id": "2101.10599", "submitter": "Mehul S. Raval", "authors": "Rupal Agravat, Mehul S Raval", "title": "A Survey and Analysis on Automated Glioma Brain Tumor Segmentation and\n  Overall Patient Survival Prediction", "comments": "40 pages, 19 figures, 11 Tables", "journal-ref": "Archives of Computational Methods in Engineering, Springer, 2021", "doi": "10.1007/s11831-021-09559-w", "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Glioma is the most deadly brain tumor with high mortality. Treatment planning\nby human experts depends on the proper diagnosis of physical symptoms along\nwith Magnetic Resonance(MR) image analysis. Highly variability of a brain tumor\nin terms of size, shape, location, and a high volume of MR images makes the\nanalysis time-consuming. Automatic segmentation methods achieve a reduction in\ntime with excellent reproducible results. The article aims to survey the\nadvancement of automated methods for Glioma brain tumor segmentation. It is\nalso essential to make an objective evaluation of various models based on the\nbenchmark. Therefore, the 2012 - 2019 BraTS challenges database evaluates\nstate-of-the-art methods. The complexity of tasks under the challenge has grown\nfrom segmentation (Task1) to overall survival prediction (Task 2) to\nuncertainty prediction for classification (Task 3). The paper covers the\ncomplete gamut of brain tumor segmentation using handcrafted features to deep\nneural network models for Task 1. The aim is to showcase a complete change of\ntrends in automated brain tumor models. The paper also covers end to end joint\nmodels involving brain tumor segmentation and overall survival prediction. All\nthe methods are probed, and parameters that affect performance are tabulated\nand analyzed.\n", "versions": [{"version": "v1", "created": "Tue, 26 Jan 2021 07:22:52 GMT"}, {"version": "v2", "created": "Mon, 8 Mar 2021 15:34:56 GMT"}], "update_date": "2021-03-09", "authors_parsed": [["Agravat", "Rupal", ""], ["Raval", "Mehul S", ""]]}, {"id": "2101.10625", "submitter": "Jakub Klus", "authors": "Jakub Klus, Pavel Grunt, Martin Dobrovoln\\'y", "title": "Hyper-optimization with Gaussian Process and Differential Evolution\n  Algorithm", "comments": "BlackBox2020 challenge track", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Optimization of problems with high computational power demands is a\nchallenging task. A probabilistic approach to such optimization called Bayesian\noptimization lowers performance demands by solving mathematically simpler model\nof the problem. Selected approach, Gaussian Process, models problem using a\nmixture of Gaussian functions. This paper presents specific modifications of\nGaussian Process optimization components from available scientific libraries.\nPresented modifications were submitted to BlackBox 2020 challenge, where it\noutperformed some conventionally available optimization libraries.\n", "versions": [{"version": "v1", "created": "Tue, 26 Jan 2021 08:33:00 GMT"}], "update_date": "2021-01-27", "authors_parsed": [["Klus", "Jakub", ""], ["Grunt", "Pavel", ""], ["Dobrovoln\u00fd", "Martin", ""]]}, {"id": "2101.10636", "submitter": "Michael Bianco", "authors": "Michael J. Bianco, Sharon Gannot, Efren Fernandez-Grande, and Peter\n  Gerstoft", "title": "Semi-supervised source localization in reverberant environments with\n  deep generative modeling", "comments": "Revision, submitted to IEEE Access", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SP cs.LG cs.SD eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a semi-supervised approach to acoustic source localization in\nreverberant environments based on deep generative modeling. Localization in\nreverberant environments remains an open challenge. Even with large data\nvolumes, the number of labels available for supervised learning in reverberant\nenvironments is usually small. We address this issue by performing\nsemi-supervised learning (SSL) with convolutional variational autoencoders\n(VAEs) on reverberant speech signals recorded with microphone arrays. The VAE\nis trained to generate the phase of relative transfer functions (RTFs) between\nmicrophones, in parallel with a direction of arrival (DOA) classifier based on\nRTF-phase. These models are trained using both labeled and unlabeled RTF-phase\nsequences. In learning to perform these tasks, the VAE-SSL explicitly learns to\nseparate the physical causes of the RTF-phase (i.e., source location) from\ndistracting signal characteristics such as noise and speech activity. Relative\nto existing semi-supervised localization methods in acoustics, VAE-SSL is\neffectively an end-to-end processing approach which relies on minimal\npreprocessing of RTF-phase features. As far as we are aware, our paper presents\nthe first approach to modeling the physics of acoustic propagation using deep\ngenerative modeling. The VAE-SSL approach is compared with two signal\nprocessing-based approaches, steered response power with phase transform\n(SRP-PHAT) and MUltiple SIgnal Classification (MUSIC), as well as fully\nsupervised CNNs. We find that VAE-SSL can outperform the conventional\napproaches and the CNN in label-limited scenarios. Further, the trained VAE-SSL\nsystem can generate new RTF-phase samples, which shows the VAE-SSL approach\nlearns the physics of the acoustic environment. The generative modeling in\nVAE-SSL thus provides a means of interpreting the learned representations.\n", "versions": [{"version": "v1", "created": "Tue, 26 Jan 2021 08:54:38 GMT"}, {"version": "v2", "created": "Thu, 1 Apr 2021 20:12:16 GMT"}], "update_date": "2021-04-05", "authors_parsed": [["Bianco", "Michael J.", ""], ["Gannot", "Sharon", ""], ["Fernandez-Grande", "Efren", ""], ["Gerstoft", "Peter", ""]]}, {"id": "2101.10643", "submitter": "Jie Zhu", "authors": "Jie Zhu, Blanca Gallego", "title": "Casual Inference using Deep Bayesian Dynamic Survival Model (CDS)", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.LG q-bio.QM", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Causal inference in longitudinal observational health data often requires the\naccurate estimation of treatment effects on time-to-event outcomes in the\npresence of time-varying covariates. To tackle this sequential treatment effect\nestimation problem, we have developed a causal dynamic survival (CDS) model\nthat uses the potential outcomes framework with the recurrent sub-networks with\nrandom seed ensembles to estimate the difference in survival curves of its\nconfidence interval. Using simulated survival datasets, the CDS model has shown\ngood causal effect estimation performance across scenarios of sample dimension,\nevent rate, confounding and overlapping. However, increasing the sample size is\nnot effective to alleviate the adverse impact from high level of confounding.\nIn two large clinical cohort studies, our model identified the expected\nconditional average treatment effect and detected individual effect\nheterogeneity over time and patient subgroups. CDS provides individualised\nabsolute treatment effect estimations to improve clinical decisions.\n", "versions": [{"version": "v1", "created": "Tue, 26 Jan 2021 09:15:49 GMT"}, {"version": "v2", "created": "Mon, 1 Feb 2021 13:41:07 GMT"}, {"version": "v3", "created": "Wed, 24 Feb 2021 23:06:53 GMT"}, {"version": "v4", "created": "Sun, 28 Feb 2021 12:55:21 GMT"}, {"version": "v5", "created": "Tue, 2 Mar 2021 12:03:38 GMT"}, {"version": "v6", "created": "Wed, 3 Mar 2021 08:23:41 GMT"}, {"version": "v7", "created": "Tue, 13 Jul 2021 09:03:46 GMT"}], "update_date": "2021-07-14", "authors_parsed": [["Zhu", "Jie", ""], ["Gallego", "Blanca", ""]]}, {"id": "2101.10655", "submitter": "Weizhu Qian", "authors": "Weizhu Qian and Franck Gechter", "title": "Variational Information Bottleneck Model for Accurate Indoor Position\n  Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Recognizing user location with WiFi fingerprints is a popular approach for\naccurate indoor positioning problems. In this work, our goal is to interpret\nWiFi fingerprints into actual user locations. However, WiFi fingerprint data\ncan be very high dimensional in some cases, we need to find a good\nrepresentation of the input data for the learning task first. Otherwise, using\nneural networks will suffer from severe overfitting. In this work, we solve\nthis issue by combining the Information Bottleneck method and Variational\nInference. Based on these two approaches, we propose a Variational Information\nBottleneck model for accurate indoor positioning. The proposed model consists\nof an encoder structure and a predictor structure. The encoder is to find a\ngood representation in the input data for the learning task. The predictor is\nto use the latent representation to predict the final output. To enhance the\ngeneralization of our model, we also adopt the Dropout technique for each\nhidden layer of the decoder. We conduct the validation experiments on a\nreal-world dataset. We also compare the proposed model to other existing\nmethods so as to quantify the performances of our method.\n", "versions": [{"version": "v1", "created": "Tue, 26 Jan 2021 09:29:53 GMT"}], "update_date": "2021-01-27", "authors_parsed": [["Qian", "Weizhu", ""], ["Gechter", "Franck", ""]]}, {"id": "2101.10657", "submitter": "Alessandro Sebastianelli", "authors": "Daniela A. Zaidenberg, Alessandro Sebastianelli, Dario Spiller,\n  Bertrand Le Saux and Silvia Liberata Ullo", "title": "Advantages and Bottlenecks of Quantum Machine Learning for Remote\n  Sensing", "comments": "Submitted and accepted for IEEE IGARSS2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "quant-ph cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This concept paper aims to provide a brief outline of quantum computers,\nexplore existing methods of quantum image classification techniques, so\nfocusing on remote sensing applications, and discuss the bottlenecks of\nperforming these algorithms on currently available open source platforms.\nInitial results demonstrate feasibility. Next steps include expanding the size\nof the quantum hidden layer and increasing the variety of output image options.\n", "versions": [{"version": "v1", "created": "Tue, 26 Jan 2021 09:31:46 GMT"}, {"version": "v2", "created": "Thu, 28 Jan 2021 09:31:29 GMT"}, {"version": "v3", "created": "Wed, 30 Jun 2021 07:15:05 GMT"}], "update_date": "2021-07-01", "authors_parsed": [["Zaidenberg", "Daniela A.", ""], ["Sebastianelli", "Alessandro", ""], ["Spiller", "Dario", ""], ["Saux", "Bertrand Le", ""], ["Ullo", "Silvia Liberata", ""]]}, {"id": "2101.10674", "submitter": "Benjamin Lambert", "authors": "Benjamin Lambert, Maxime Louis, Senan Doyle, Florence Forbes, Michel\n  Dojat, Alan Tucholka", "title": "Leveraging 3D Information in Unsupervised Brain MRI Segmentation", "comments": "Accepted for presentation at IEEE International Symposium on\n  Biomedical Imaging 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Automatic segmentation of brain abnormalities is challenging, as they vary\nconsiderably from one pathology to another. Current methods are supervised and\nrequire numerous annotated images for each pathology, a strenuous task. To\ntackle anatomical variability, Unsupervised Anomaly Detection (UAD) methods are\nproposed, detecting anomalies as outliers of a healthy model learned using a\nVariational Autoencoder (VAE). Previous work on UAD adopted a 2D approach,\nmeaning that MRIs are processed as a collection of independent slices. Yet, it\ndoes not fully exploit the spatial information contained in MRI. Here, we\npropose to perform UAD in a 3D fashion and compare 2D and 3D VAEs. As a side\ncontribution, we present a new loss function guarantying a robust training.\nLearning is performed using a multicentric dataset of healthy brain MRIs, and\nsegmentation performances are estimated on White-Matter Hyperintensities and\ntumors lesions. Experiments demonstrate the interest of 3D methods which\noutperform their 2D counterparts.\n", "versions": [{"version": "v1", "created": "Tue, 26 Jan 2021 10:04:57 GMT"}], "update_date": "2021-01-27", "authors_parsed": [["Lambert", "Benjamin", ""], ["Louis", "Maxime", ""], ["Doyle", "Senan", ""], ["Forbes", "Florence", ""], ["Dojat", "Michel", ""], ["Tucholka", "Alan", ""]]}, {"id": "2101.10698", "submitter": "Paolo Avogadro", "authors": "Paolo Avogadro, Matteo Alessandro Dominoni", "title": "A fast algorithm for complex discord searches in time series: HOT SAX\n  Time", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Time series analysis is quickly proceeding towards long and complex tasks. In\nrecent years, fast approximate algorithms for discord search have been proposed\nin order to compensate for the increasing size of the time series. It is more\ninteresting, however, to find quick exact solutions. In this research, we\nimproved HOT SAX by exploiting two main ideas: the warm-up process, and the\nsimilarity between sequences close in time. The resulting algorithm, called HOT\nSAX Time (HST), has been validated with real and synthetic time series, and\nsuccessfully compared with HOT SAX, RRA, SCAMP, and DADD. The complexity of a\ndiscord search has been evaluated with a new indicator, the cost per sequence\n(cps), which allows one to compare searches on time series of different\nlengths. Numerical evidence suggests that two conditions are involved in\ndetermining the complexity of a discord search in a non-trivial way: the length\nof the discords, and the noise/signal ratio. In the case of complex searches,\nHST can be more than 100 times faster than HOT SAX, thus being at the forefront\nof the exact discord search.\n", "versions": [{"version": "v1", "created": "Tue, 26 Jan 2021 10:42:57 GMT"}], "update_date": "2021-01-27", "authors_parsed": [["Avogadro", "Paolo", ""], ["Dominoni", "Matteo Alessandro", ""]]}, {"id": "2101.10708", "submitter": "Zhuang Li", "authors": "Zhuang Li, Lizhen Qu, Shuo Huang, Gholamreza Haffari", "title": "Few-Shot Semantic Parsing for New Predicates", "comments": "Accepted to EACL 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this work, we investigate the problems of semantic parsing in a few-shot\nlearning setting. In this setting, we are provided with utterance-logical form\npairs per new predicate. The state-of-the-art neural semantic parsers achieve\nless than 25% accuracy on benchmark datasets when k= 1. To tackle this problem,\nwe proposed to i) apply a designated meta-learning method to train the model;\nii) regularize attention scores with alignment statistics; iii) apply a\nsmoothing technique in pre-training. As a result, our method consistently\noutperforms all the baselines in both one and two-shot settings.\n", "versions": [{"version": "v1", "created": "Tue, 26 Jan 2021 11:08:08 GMT"}], "update_date": "2021-01-27", "authors_parsed": [["Li", "Zhuang", ""], ["Qu", "Lizhen", ""], ["Huang", "Shuo", ""], ["Haffari", "Gholamreza", ""]]}, {"id": "2101.10710", "submitter": "Mohammad Naser Sabet Jahromi", "authors": "Satya M. Muddamsetty, Mohammad N. S. Jahromi, Andreea E. Ciontos,\n  Laura M. Fenoy, Thomas B. Moeslund", "title": "Introducing and assessing the explainable AI (XAI)method: SIDU", "comments": "Preprint-submitted to Journal of Pattern Recognition (Elsevier)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.HC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Explainable Artificial Intelligence (XAI) has in recent years become a\nwell-suited framework to generate human understandable explanations of black\nbox models. In this paper, we present a novel XAI visual explanation algorithm\ndenoted SIDU that can effectively localize entire object regions responsible\nfor prediction in a full extend. We analyze its robustness and effectiveness\nthrough various computational and human subject experiments. In particular, we\nassess the SIDU algorithm using three different types of evaluations\n(Application, Human and Functionally-Grounded) to demonstrate its superior\nperformance. The robustness of SIDU is further studied in presence of\nadversarial attack on black box models to better understand its performance.\n", "versions": [{"version": "v1", "created": "Tue, 26 Jan 2021 11:13:50 GMT"}], "update_date": "2021-01-27", "authors_parsed": [["Muddamsetty", "Satya M.", ""], ["Jahromi", "Mohammad N. S.", ""], ["Ciontos", "Andreea E.", ""], ["Fenoy", "Laura M.", ""], ["Moeslund", "Thomas B.", ""]]}, {"id": "2101.10719", "submitter": "Pedro Cadahia Delgado", "authors": "Pedro Cadah\\'ia and Jose Manuel Bravo Caro", "title": "Short-term prediction of Time Series based on bounding techniques", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper it is reconsidered the prediction problem in time series\nframework by using a new non-parametric approach. Through this reconsideration,\nthe prediction is obtained by a weighted sum of past observed data. These\nweights are obtained by solving a constrained linear optimization problem that\nminimizes an outer bound of the prediction error. The innovation is to consider\nboth deterministic and stochastic assumptions in order to obtain the upper\nbound of the prediction error, a tuning parameter is used to balance these\ndeterministic-stochastic assumptions in order to improve the predictor\nperformance. A benchmark is included to illustrate that the proposed predictor\ncan obtain suitable results in a prediction scheme, and can be an interesting\nalternative method to the classical non-parametric methods. Besides, it is\nshown how this model can outperform the preexisting ones in a short term\nforecast.\n", "versions": [{"version": "v1", "created": "Tue, 26 Jan 2021 11:27:36 GMT"}], "update_date": "2021-01-27", "authors_parsed": [["Cadah\u00eda", "Pedro", ""], ["Caro", "Jose Manuel Bravo", ""]]}, {"id": "2101.10721", "submitter": "Victor Gallego", "authors": "V\\'ictor Gallego, Roi Naveiro, David R\\'ios Insua, Wolfram Rozas", "title": "Data sharing games", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GT cs.LG econ.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data sharing issues pervade online social and economic environments. To\nfoster social progress, it is important to develop models of the interaction\nbetween data producers and consumers that can promote the rise of cooperation\nbetween the involved parties. We formalize this interaction as a game, the data\nsharing game, based on the Iterated Prisoner's Dilemma and deal with it through\nmulti-agent reinforcement learning techniques. We consider several strategies\nfor how the citizens may behave, depending on the degree of centralization\nsought. Simulations suggest mechanisms for cooperation to take place and, thus,\nachieve maximum social utility: data consumers should perform some kind of\nopponent modeling, or a regulator should transfer utility between both players\nand incentivise them.\n", "versions": [{"version": "v1", "created": "Tue, 26 Jan 2021 11:29:01 GMT"}], "update_date": "2021-01-27", "authors_parsed": [["Gallego", "V\u00edctor", ""], ["Naveiro", "Roi", ""], ["Insua", "David R\u00edos", ""], ["Rozas", "Wolfram", ""]]}, {"id": "2101.10739", "submitter": "Jie Zhu", "authors": "Jie Zhu, Blanca Gallego", "title": "Dynamic prediction of time to event with survival curves", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  With the ever-growing complexity of primary health care system, proactive\npatient failure management is an effective way to enhancing the availability of\nhealth care resource. One key enabler is the dynamic prediction of\ntime-to-event outcomes. Conventional explanatory statistical approach lacks the\ncapability of making precise individual level prediction, while the data\nadaptive binary predictors does not provide nominal survival curves for\nbiologically plausible survival analysis. The purpose of this article is to\nelucidate that the knowledge of explanatory survival analysis can significantly\nenhance the current black-box data adaptive prediction models. We apply our\nrecently developed counterfactual dynamic survival model (CDSM) to static and\nlongitudinal observational data and testify that the inflection point of its\nestimated individual survival curves provides reliable prediction of the\npatient failure time.\n", "versions": [{"version": "v1", "created": "Tue, 26 Jan 2021 12:17:27 GMT"}, {"version": "v2", "created": "Thu, 11 Mar 2021 04:32:31 GMT"}], "update_date": "2021-03-12", "authors_parsed": [["Zhu", "Jie", ""], ["Gallego", "Blanca", ""]]}, {"id": "2101.10759", "submitter": "Xutan Peng", "authors": "Xutan Peng, Yi Zheng, Chenghua Lin, Advaith Siddharthan", "title": "Summarising Historical Text in Modern Languages", "comments": "To appear at EACL 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.CY cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce the task of historical text summarisation, where documents in\nhistorical forms of a language are summarised in the corresponding modern\nlanguage. This is a fundamentally important routine to historians and digital\nhumanities researchers but has never been automated. We compile a high-quality\ngold-standard text summarisation dataset, which consists of historical German\nand Chinese news from hundreds of years ago summarised in modern German or\nChinese. Based on cross-lingual transfer learning techniques, we propose a\nsummarisation model that can be trained even with no cross-lingual (historical\nto modern) parallel data, and further benchmark it against state-of-the-art\nalgorithms. We report automatic and human evaluations that distinguish the\nhistoric to modern language summarisation task from standard cross-lingual\nsummarisation (i.e., modern to modern language), highlight the distinctness and\nvalue of our dataset, and demonstrate that our transfer learning approach\noutperforms standard cross-lingual benchmarks on this task.\n", "versions": [{"version": "v1", "created": "Tue, 26 Jan 2021 13:00:07 GMT"}, {"version": "v2", "created": "Wed, 27 Jan 2021 04:17:02 GMT"}], "update_date": "2021-01-28", "authors_parsed": [["Peng", "Xutan", ""], ["Zheng", "Yi", ""], ["Lin", "Chenghua", ""], ["Siddharthan", "Advaith", ""]]}, {"id": "2101.10761", "submitter": "Ahmed Elzanaty Dr.", "authors": "Ahmed M. Abdelmoniem and Ahmed Elzanaty and Mohamed-Slim Alouini and\n  Marco Canini", "title": "An Efficient Statistical-based Gradient Compression Technique for\n  Distributed Training Systems", "comments": "Accepted at the 2021 Machine Learning and Systems (MLSys) Conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The recent many-fold increase in the size of deep neural networks makes\nefficient distributed training challenging. Many proposals exploit the\ncompressibility of the gradients and propose lossy compression techniques to\nspeed up the communication stage of distributed training. Nevertheless,\ncompression comes at the cost of reduced model quality and extra computation\noverhead. In this work, we design an efficient compressor with minimal\noverhead. Noting the sparsity of the gradients, we propose to model the\ngradients as random variables distributed according to some sparsity-inducing\ndistributions (SIDs). We empirically validate our assumption by studying the\nstatistical characteristics of the evolution of gradient vectors over the\ntraining process. We then propose Sparsity-Inducing Distribution-based\nCompression (SIDCo), a threshold-based sparsification scheme that enjoys\nsimilar threshold estimation quality to deep gradient compression (DGC) while\nbeing faster by imposing lower compression overhead. Our extensive evaluation\nof popular machine learning benchmarks involving both recurrent neural network\n(RNN) and convolution neural network (CNN) models shows that SIDCo speeds up\ntraining by up to 41:7%, 7:6%, and 1:9% compared to the no-compression\nbaseline, Topk, and DGC compressors, respectively.\n", "versions": [{"version": "v1", "created": "Tue, 26 Jan 2021 13:06:00 GMT"}, {"version": "v2", "created": "Wed, 17 Mar 2021 18:18:47 GMT"}], "update_date": "2021-03-19", "authors_parsed": [["Abdelmoniem", "Ahmed M.", ""], ["Elzanaty", "Ahmed", ""], ["Alouini", "Mohamed-Slim", ""], ["Canini", "Marco", ""]]}, {"id": "2101.10763", "submitter": "Jakob Kruse", "authors": "Jakob Kruse, Lynton Ardizzone, Carsten Rother, Ullrich K\\\"othe", "title": "Benchmarking Invertible Architectures on Inverse Problems", "comments": null, "journal-ref": "Workshop on Invertible Neural Networks and Normalizing Flows (ICML\n  2019)", "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Recent work demonstrated that flow-based invertible neural networks are\npromising tools for solving ambiguous inverse problems. Following up on this,\nwe investigate how ten invertible architectures and related models fare on two\nintuitive, low-dimensional benchmark problems, obtaining the best results with\ncoupling layers and simple autoencoders. We hope that our initial efforts\ninspire other researchers to evaluate their invertible architectures in the\nsame setting and put forth additional benchmarks, so our evaluation may\neventually grow into an official community challenge.\n", "versions": [{"version": "v1", "created": "Tue, 26 Jan 2021 13:10:37 GMT"}, {"version": "v2", "created": "Wed, 5 May 2021 15:04:06 GMT"}, {"version": "v3", "created": "Tue, 22 Jun 2021 10:53:42 GMT"}], "update_date": "2021-06-23", "authors_parsed": [["Kruse", "Jakob", ""], ["Ardizzone", "Lynton", ""], ["Rother", "Carsten", ""], ["K\u00f6the", "Ullrich", ""]]}, {"id": "2101.10785", "submitter": "Michael Gresser", "authors": "Marc Franzen, Michael Stephan Gresser, Tobias M\\\"uller, Prof. Dr.\n  Sebastian Mauser", "title": "Developing emotion recognition for video conference software to support\n  people with autism", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop an emotion recognition software for the use with a video\nconference software for autistic individuals which are unable to recognize\nemotions properly. It can get an image out of the video stream, detect the\nemotion in it with the help of a neural network and display the prediction to\nthe user. The network is trained on facial landmark features. The software is\nfully modular to support adaption to different video conference software,\nprogramming languages and implementations.\n", "versions": [{"version": "v1", "created": "Tue, 26 Jan 2021 13:54:36 GMT"}], "update_date": "2021-01-27", "authors_parsed": [["Franzen", "Marc", ""], ["Gresser", "Michael Stephan", ""], ["M\u00fcller", "Tobias", ""], ["Mauser", "Prof. Dr. Sebastian", ""]]}, {"id": "2101.10790", "submitter": "Simon Meyer Lauritsen", "authors": "Simon Meyer Lauritsen, Bo Thiesson, Marianne Johansson J{\\o}rgensen,\n  Anders Hammerich Riis, Ulrick Skipper Espelund, Jesper Bo Weile and Jeppe\n  Lange", "title": "The Consequences of the Framing of Machine Learning Risk Prediction\n  Models: Evaluation of Sepsis in General Wards", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Objectives: To evaluate the consequences of the framing of machine learning\nrisk prediction models. We evaluate how framing affects model performance and\nmodel learning in four different approaches previously applied in published\nartificial-intelligence (AI) models.\n  Setting and participants: We analysed structured secondary healthcare data\nfrom 221,283 citizens from four Danish municipalities who were 18 years of age\nor older.\n  Results: The four models had similar population level performance (a mean\narea under the receiver operating characteristic curve of 0.73 to 0.82), in\ncontrast to the mean average precision, which varied greatly from 0.007 to\n0.385. Correspondingly, the percentage of missing values also varied between\nframing approaches. The on-clinical-demand framing, which involved samples for\neach time the clinicians made an early warning score assessment, showed the\nlowest percentage of missing values among the vital sign parameters, and this\nmodel was also able to learn more temporal dependencies than the others. The\nShapley additive explanations demonstrated opposing interpretations of SpO2 in\nthe prediction of sepsis as a consequence of differentially framed models.\n  Conclusions: The profound consequences of framing mandate attention from\nclinicians and AI developers, as the understanding and reporting of framing are\npivotal to the successful development and clinical implementation of future AI\ntechnology. Model framing must reflect the expected clinical environment. The\nimportance of proper problem framing is by no means exclusive to sepsis\nprediction and applies to most clinical risk prediction models.\n", "versions": [{"version": "v1", "created": "Tue, 26 Jan 2021 14:00:05 GMT"}], "update_date": "2021-01-27", "authors_parsed": [["Lauritsen", "Simon Meyer", ""], ["Thiesson", "Bo", ""], ["J\u00f8rgensen", "Marianne Johansson", ""], ["Riis", "Anders Hammerich", ""], ["Espelund", "Ulrick Skipper", ""], ["Weile", "Jesper Bo", ""], ["Lange", "Jeppe", ""]]}, {"id": "2101.10792", "submitter": "Nicolas Michael M\\\"uller", "authors": "Nicolas M. M\\\"uller, Konstantin B\\\"ottinger", "title": "Adversarial Vulnerability of Active Transfer Learning", "comments": "Accepted for publication at IDA 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Two widely used techniques for training supervised machine learning models on\nsmall datasets are Active Learning and Transfer Learning. The former helps to\noptimally use a limited budget to label new data. The latter uses large\npre-trained models as feature extractors and enables the design of complex,\nnon-linear models even on tiny datasets. Combining these two approaches is an\neffective, state-of-the-art method when dealing with small datasets.\n  In this paper, we share an intriguing observation: Namely, that the\ncombination of these techniques is particularly susceptible to a new kind of\ndata poisoning attack: By adding small adversarial noise on the input, it is\npossible to create a collision in the output space of the transfer learner. As\na result, Active Learning algorithms no longer select the optimal instances,\nbut almost exclusively the ones injected by the attacker. This allows an\nattacker to manipulate the active learner to select and include arbitrary\nimages into the data set, even against an overwhelming majority of unpoisoned\nsamples. We show that a model trained on such a poisoned dataset has a\nsignificantly deteriorated performance, dropping from 86\\% to 34\\% test\naccuracy. We evaluate this attack on both audio and image datasets and support\nour findings empirically. To the best of our knowledge, this weakness has not\nbeen described before in literature.\n", "versions": [{"version": "v1", "created": "Tue, 26 Jan 2021 14:07:09 GMT"}], "update_date": "2021-01-28", "authors_parsed": [["M\u00fcller", "Nicolas M.", ""], ["B\u00f6ttinger", "Konstantin", ""]]}, {"id": "2101.10795", "submitter": "Dasara Shullani", "authors": "Pengpeng Yang, Daniele Baracchi, Massimo Iuliani, Dasara Shullani,\n  Rongrong Ni, Yao Zhao, Alessandro Piva", "title": "Efficient video integrity analysis through container characterization", "comments": "Accepted by IEEE Journal of Selected Topics in Signal Processing", "journal-ref": "IEEE Journal of Selected Topics in Signal Processing, vol. 14, no.\n  5, pp. 947-954, Aug. 2020", "doi": "10.1109/JSTSP.2020.3008088", "report-no": null, "categories": "cs.MM cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most video forensic techniques look for traces within the data stream that\nare, however, mostly ineffective when dealing with strongly compressed or low\nresolution videos. Recent research highlighted that useful forensic traces are\nalso left in the video container structure, thus offering the opportunity to\nunderstand the life-cycle of a video file without looking at the media stream\nitself.\n  In this paper we introduce a container-based method to identify the software\nused to perform a video manipulation and, in most cases, the operating system\nof the source device. As opposed to the state of the art, the proposed method\nis both efficient and effective and can also provide a simple explanation for\nits decisions. This is achieved by using a decision-tree-based classifier\napplied to a vectorial representation of the video container structure. We\nconducted an extensive validation on a dataset of 7000 video files including\nboth software manipulated contents (ffmpeg, Exiftool, Adobe Premiere, Avidemux,\nand Kdenlive), and videos exchanged through social media platforms (Facebook,\nTikTok, Weibo and YouTube). This dataset has been made available to the\nresearch community. The proposed method achieves an accuracy of 97.6% in\ndistinguishing pristine from tampered videos and classifying the editing\nsoftware, even when the video is cut without re-encoding or when it is\ndownscaled to the size of a thumbnail. Furthermore, it is capable of correctly\nidentifying the operating system of the source device for most of the tampered\nvideos.\n", "versions": [{"version": "v1", "created": "Tue, 26 Jan 2021 14:13:39 GMT"}], "update_date": "2021-01-27", "authors_parsed": [["Yang", "Pengpeng", ""], ["Baracchi", "Daniele", ""], ["Iuliani", "Massimo", ""], ["Shullani", "Dasara", ""], ["Ni", "Rongrong", ""], ["Zhao", "Yao", ""], ["Piva", "Alessandro", ""]]}, {"id": "2101.10799", "submitter": "Xiaowei Xu", "authors": "Xiaowei Xu, Tianchen Wang, Jian Zhuang, Haiyun Yuan, Meiping Huang,\n  Jianzheng Cen, Qianjun Jia, Yuhao Dong, Yiyu Shi", "title": "ImageCHD: A 3D Computed Tomography Image Dataset for Classification of\n  Congenital Heart Disease", "comments": "11 pages, 6 figures, 2 tables, published at MICCAI 2020. The\n  diagnosis info of the dataset is updated (thanks to the help of Kadirbarut\n  from Bilgiuzayi)", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Congenital heart disease (CHD) is the most common type of birth defect, which\noccurs 1 in every 110 births in the United States. CHD usually comes with\nsevere variations in heart structure and great artery connections that can be\nclassified into many types. Thus highly specialized domain knowledge and the\ntime-consuming human process is needed to analyze the associated medical\nimages. On the other hand, due to the complexity of CHD and the lack of\ndataset, little has been explored on the automatic diagnosis (classification)\nof CHDs. In this paper, we present ImageCHD, the first medical image dataset\nfor CHD classification. ImageCHD contains 110 3D Computed Tomography (CT)\nimages covering most types of CHD, which is of decent size Classification of\nCHDs requires the identification of large structural changes without any local\ntissue changes, with limited data. It is an example of a larger class of\nproblems that are quite difficult for current machine-learning-based vision\nmethods to solve. To demonstrate this, we further present a baseline framework\nfor the automatic classification of CHD, based on a state-of-the-art CHD\nsegmentation method. Experimental results show that the baseline framework can\nonly achieve a classification accuracy of 82.0\\% under a selective prediction\nscheme with 88.4\\% coverage, leaving big room for further improvement. We hope\nthat ImageCHD can stimulate further research and lead to innovative and generic\nsolutions that would have an impact in multiple domains. Our dataset is\nreleased to the public compared with existing medical imaging datasets.\n", "versions": [{"version": "v1", "created": "Tue, 26 Jan 2021 14:15:31 GMT"}, {"version": "v2", "created": "Wed, 12 May 2021 02:07:17 GMT"}], "update_date": "2021-05-13", "authors_parsed": [["Xu", "Xiaowei", ""], ["Wang", "Tianchen", ""], ["Zhuang", "Jian", ""], ["Yuan", "Haiyun", ""], ["Huang", "Meiping", ""], ["Cen", "Jianzheng", ""], ["Jia", "Qianjun", ""], ["Dong", "Yuhao", ""], ["Shi", "Yiyu", ""]]}, {"id": "2101.10809", "submitter": "Nuredin Ali", "authors": "Nuredin Ali", "title": "Exploring Transfer Learning on Face Recognition of Dark Skinned, Low\n  Quality and Low Resource Face Data", "comments": "3 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  There is a big difference in the tone of color of skin between dark and light\nskinned people. Despite this fact, most face recognition tasks almost all\nclassical state-of-the-art models are trained on datasets containing an\noverwhelming majority of light skinned face images. It is tedious to collect a\nhuge amount of data for dark skinned faces and train a model from scratch. In\nthis paper, we apply transfer learning on VGGFace to check how it works on\nrecognising dark skinned mainly Ethiopian faces. The dataset is of low quality\nand low resource. Our experimental results show above 95\\% accuracy which\nindicates that transfer learning in such settings works.\n", "versions": [{"version": "v1", "created": "Sun, 3 Jan 2021 19:50:15 GMT"}], "update_date": "2021-01-27", "authors_parsed": [["Ali", "Nuredin", ""]]}, {"id": "2101.10831", "submitter": "Subhasish Goswami", "authors": "Mriganka Nath and Subhasish Goswami", "title": "Toxicity Detection in Drug Candidates using Simplified Molecular-Input\n  Line-Entry System", "comments": "4 Pages, 4 Figures, Published with International Journal of Computer\n  Applications (IJCA)", "journal-ref": "International Journal of Computer Applications 175(21):1-4,\n  September 2020", "doi": "10.5120/ijca2020920695", "report-no": null, "categories": "q-bio.QM cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The need for analysis of toxicity in new drug candidates and the requirement\nof doing it fast have asked the consideration of scientists towards the use of\nartificial intelligence tools to examine toxicity levels and to develop models\nto a degree where they can be used commercially to measure toxicity levels\nefficiently in upcoming drugs. Artificial Intelligence based models can be used\nto predict the toxic nature of a chemical using Quantitative Structure Activity\nRelationship techniques. Convolutional Neural Network models have demonstrated\ngreat outcomes in predicting the qualitative analysis of chemicals in order to\ndetermine the toxicity. This paper goes for the study of Simplified Molecular\nInput Line-Entry System (SMILES) as a parameter to develop Long short term\nmemory (LSTM) based models in order to examine the toxicity of a molecule and\nthe degree to which the need can be fulfilled for practical use alongside its\nfuture outlooks for the purpose of real world applications.\n", "versions": [{"version": "v1", "created": "Thu, 21 Jan 2021 07:02:21 GMT"}], "update_date": "2021-01-27", "authors_parsed": [["Nath", "Mriganka", ""], ["Goswami", "Subhasish", ""]]}, {"id": "2101.10832", "submitter": "Yulin Wang", "authors": "Yulin Wang, Zanlin Ni, Shiji Song, Le Yang, Gao Huang", "title": "Revisiting Locally Supervised Learning: an Alternative to End-to-end\n  Training", "comments": "Accepted by ICLR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Due to the need to store the intermediate activations for back-propagation,\nend-to-end (E2E) training of deep networks usually suffers from high GPUs\nmemory footprint. This paper aims to address this problem by revisiting the\nlocally supervised learning, where a network is split into gradient-isolated\nmodules and trained with local supervision. We experimentally show that simply\ntraining local modules with E2E loss tends to collapse task-relevant\ninformation at early layers, and hence hurts the performance of the full model.\nTo avoid this issue, we propose an information propagation (InfoPro) loss,\nwhich encourages local modules to preserve as much useful information as\npossible, while progressively discard task-irrelevant information. As InfoPro\nloss is difficult to compute in its original form, we derive a feasible upper\nbound as a surrogate optimization objective, yielding a simple but effective\nalgorithm. In fact, we show that the proposed method boils down to minimizing\nthe combination of a reconstruction loss and a normal cross-entropy/contrastive\nterm. Extensive empirical results on five datasets (i.e., CIFAR, SVHN, STL-10,\nImageNet and Cityscapes) validate that InfoPro is capable of achieving\ncompetitive performance with less than 40% memory footprint compared to E2E\ntraining, while allowing using training data with higher-resolution or larger\nbatch sizes under the same GPU memory constraint. Our method also enables\ntraining local modules asynchronously for potential training acceleration. Code\nis available at: https://github.com/blackfeather-wang/InfoPro-Pytorch.\n", "versions": [{"version": "v1", "created": "Tue, 26 Jan 2021 15:02:18 GMT"}], "update_date": "2021-01-27", "authors_parsed": [["Wang", "Yulin", ""], ["Ni", "Zanlin", ""], ["Song", "Shiji", ""], ["Yang", "Le", ""], ["Huang", "Gao", ""]]}, {"id": "2101.10833", "submitter": "Amr Hilal", "authors": "Amr E Hilal, Ismail Arai, Samy El-Tawab", "title": "DataLoc+: A Data Augmentation Technique for Machine Learning in\n  Room-Level Indoor Localization", "comments": "7 pages, 7 figures, 1 table, 1 algorithm. Accepted at IEEE WCNC 2021,\n  and final version is submitted", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Indoor localization has been a hot area of research over the past two\ndecades. Since its advent, it has been steadily utilizing the emerging\ntechnologies to improve accuracy, and machine learning has been at the heart of\nthat. Machine learning has been increasingly used in fingerprint-based indoor\nlocalization to replace or emulate the radio map that is used to predict\nlocations given a location signature. The prediction quality of a machine\nlearning model primarily depends on how well the model was trained, which\nrelies on the amount and quality of data used to train it. Data augmentation\nhas been used to improve quality of the trained models by synthetically\nproducing more training data, and several approaches were used in the\nliterature that tackles the problem of lack of training data from different\nangles. In this paper, we propose DataLoc+, a data augmentation technique for\nroom-level indoor localization that combines different approaches in a simple\nalgorithm. We evaluate the technique by comparing it to the typical direct\nsnapshot approach using data collected from a field experiment conducted in a\nhospital. Our evaluation shows that the model trained using the proposed\ntechnique achieves higher accuracy. We also show that the technique adapts to\nlarger problems using a limited dataset while maintaining high accuracy.\n", "versions": [{"version": "v1", "created": "Thu, 21 Jan 2021 17:41:41 GMT"}], "update_date": "2021-01-27", "authors_parsed": [["Hilal", "Amr E", ""], ["Arai", "Ismail", ""], ["El-Tawab", "Samy", ""]]}, {"id": "2101.10838", "submitter": "Mehmet Ilter", "authors": "Mehmet C. Ilter, Alexis A. Dowhuszko, Jyri H\\\"am\\\"al\\\"ainen and Risto\n  Wichman", "title": "Visible light communication-based monitoring for indoor environments\n  using unsupervised learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SP cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Visible Light Communication~(VLC) systems provide not only illumination and\ndata communication, but also indoor monitoring services if the effect that\ndifferent events create on the received optical signal is properly tracked. For\nthis purpose, the Channel State Information that a VLC receiver computes to\nequalize the subcarriers of the OFDM signal can be also reused to train an\nUnsupervised Learning classifier. This way, different clusters can be created\non the collected CSI data, which could be then mapped into relevant events\nto-be-monitored in the indoor environments, such as the presence of a new\nobject in a given position or the change of the position of a given object.\nWhen compared to supervised learning algorithms, the proposed approach does not\nneed to add tags in the training data, simplifying notably the implementation\nof the machine learning classifier. The practical validation the monitoring\napproach was done with the aid of a software-defined VLC link based on OFDM, in\nwhich a copy of the intensity modulated signal coming from a Phosphor-converted\nLED was captured by a pair of Photodetectors~(PDs). The performance evaluation\nof the experimental VLC-based monitoring demo achieved a positioning accuracy\nin the few-centimeter-range, without the necessity of deploying a large number\nof sensors and/or adding a VLC-enabled sensor on the object to-be-tracked.\n", "versions": [{"version": "v1", "created": "Wed, 20 Jan 2021 12:30:24 GMT"}], "update_date": "2021-01-27", "authors_parsed": [["Ilter", "Mehmet C.", ""], ["Dowhuszko", "Alexis A.", ""], ["H\u00e4m\u00e4l\u00e4inen", "Jyri", ""], ["Wichman", "Risto", ""]]}, {"id": "2101.10841", "submitter": "Yong-Goo Shin", "authors": "Seung Park, Yoon-Jae Yeo, and Yong-Goo Shin", "title": "Generative Adversarial Network using Perturbed-Convolutions", "comments": "Submitted to IEEE transactions on Neural networks and learning\n  systems. arXiv admin note: text overlap with arXiv:1911.10979", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite growing insights into the GAN training, it still suffers from\ninstability during the training procedure. To alleviate this problem, this\npaper presents a novel convolutional layer, called perturbed-convolution\n(PConv), which focuses on achieving two goals simultaneously: penalize the\ndiscriminator for training GAN stably and prevent the overfitting problem in\nthe discriminator. PConv generates perturbed features by randomly disturbing an\ninput tensor before performing the convolution operation. This approach is\nsimple but surprisingly effective. First, to reliably classify real and\ngenerated samples using the disturbed input tensor, the intermediate layers in\nthe discriminator should learn features having a small local Lipschitz value.\nSecond, due to the perturbed features in PConv, the discriminator is difficult\nto memorize the real images; this makes the discriminator avoid the overfitting\nproblem. To show the generalization ability of the proposed method, we\nconducted extensive experiments with various loss functions and datasets\nincluding CIFAR-10, CelebA-HQ, LSUN, and tiny-ImageNet. Quantitative\nevaluations demonstrate that WCL significantly improves the performance of GAN\nand conditional GAN in terms of Frechet inception distance (FID). For instance,\nthe proposed method improves FID scores on the tiny-ImageNet dataset from 58.59\nto 50.42.\n", "versions": [{"version": "v1", "created": "Tue, 19 Jan 2021 22:05:13 GMT"}, {"version": "v2", "created": "Tue, 2 Feb 2021 11:32:20 GMT"}], "update_date": "2021-02-03", "authors_parsed": [["Park", "Seung", ""], ["Yeo", "Yoon-Jae", ""], ["Shin", "Yong-Goo", ""]]}, {"id": "2101.10842", "submitter": "Masato Ishii", "authors": "Masato Ishii and Masashi Sugiyama", "title": "Source-free Domain Adaptation via Distributional Alignment by Matching\n  Batch Normalization Statistics", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a novel domain adaptation method for the\nsource-free setting. In this setting, we cannot access source data during\nadaptation, while unlabeled target data and a model pretrained with source data\nare given. Due to lack of source data, we cannot directly match the data\ndistributions between domains unlike typical domain adaptation algorithms. To\ncope with this problem, we propose utilizing batch normalization statistics\nstored in the pretrained model to approximate the distribution of unobserved\nsource data. Specifically, we fix the classifier part of the model during\nadaptation and only fine-tune the remaining feature encoder part so that batch\nnormalization statistics of the features extracted by the encoder match those\nstored in the fixed classifier. Additionally, we also maximize the mutual\ninformation between the features and the classifier's outputs to further boost\nthe classification performance. Experimental results with several benchmark\ndatasets show that our method achieves competitive performance with\nstate-of-the-art domain adaptation methods even though it does not require\naccess to source data.\n", "versions": [{"version": "v1", "created": "Tue, 19 Jan 2021 14:22:33 GMT"}], "update_date": "2021-01-27", "authors_parsed": [["Ishii", "Masato", ""], ["Sugiyama", "Masashi", ""]]}, {"id": "2101.10857", "submitter": "Vinayak Elangovan", "authors": "Vinayak Elangovan", "title": "Indoor Group Activity Recognition using Multi-Layered HMMs", "comments": "8 pages, 7 figures, 3 tables", "journal-ref": "Proceedings of Academics World International Conference,\n  Philadelphia, USA, 28th - 29th December, 2019", "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Discovery and recognition of Group Activities (GA) based on imagery data\nprocessing have significant applications in persistent surveillance systems,\nwhich play an important role in some Internet services. The process is involved\nwith analysis of sequential imagery data with spatiotemporal associations.\nDiscretion of video imagery requires a proper inference system capable of\ndiscriminating and differentiating cohesive observations and interlinking them\nto known ontologies. We propose an Ontology based GAR with a proper inference\nmodel that is capable of identifying and classifying a sequence of events in\ngroup activities. A multi-layered Hidden Markov Model (HMM) is proposed to\nrecognize different levels of abstract GA. The multi-layered HMM consists of N\nlayers of HMMs where each layer comprises of M number of HMMs running in\nparallel. The number of layers depends on the order of information to be\nextracted. At each layer, by matching and correlating attributes of detected\ngroup events, the model attempts to associate sensory observations to known\nontology perceptions. This paper demonstrates and compares performance of three\ndifferent implementation of HMM, namely, concatenated N-HMM, cascaded C-HMM and\nhybrid H-HMM for building effective multi-layered HMM.\n", "versions": [{"version": "v1", "created": "Sat, 23 Jan 2021 22:02:12 GMT"}], "update_date": "2021-01-27", "authors_parsed": [["Elangovan", "Vinayak", ""]]}, {"id": "2101.10865", "submitter": "Jonathan Spring", "authors": "Jonathan M. Spring and April Galyardt and Allen D. Householder and\n  Nathan VanHoudnos", "title": "On managing vulnerabilities in AI/ML systems", "comments": "16 pages. New Security Paradigms Workshop", "journal-ref": null, "doi": "10.1145/3442167.3442177", "report-no": null, "categories": "cs.CR cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper explores how the current paradigm of vulnerability management\nmight adapt to include machine learning systems through a thought experiment:\nwhat if flaws in machine learning (ML) were assigned Common Vulnerabilities and\nExposures (CVE) identifiers (CVE-IDs)? We consider both ML algorithms and model\nobjects. The hypothetical scenario is structured around exploring the changes\nto the six areas of vulnerability management: discovery, report intake,\nanalysis, coordination, disclosure, and response. While algorithm flaws are\nwell-known in the academic research community, there is no apparent clear line\nof communication between this research community and the operational\ncommunities that deploy and manage systems that use ML. The thought experiments\nidentify some ways in which CVE-IDs may establish some useful lines of\ncommunication between these two communities. In particular, it would start to\nintroduce the research community to operational security concepts, which\nappears to be a gap left by existing efforts.\n", "versions": [{"version": "v1", "created": "Fri, 22 Jan 2021 21:59:44 GMT"}], "update_date": "2021-01-27", "authors_parsed": [["Spring", "Jonathan M.", ""], ["Galyardt", "April", ""], ["Householder", "Allen D.", ""], ["VanHoudnos", "Nathan", ""]]}, {"id": "2101.10869", "submitter": "Navjodh Singh Dhillon", "authors": "Navjodh Singh Dhillon, Agustinus Sutandi, Manoj Vishwanath, Miranda M.\n  Lim, Hung Cao, Dong Si", "title": "A Raspberry Pi-based Traumatic Brain Injury Detection System for\n  Single-Channel Electroencephalogram", "comments": "12 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SP cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Traumatic Brain Injury (TBI) is a common cause of death and disability.\nHowever, existing tools for TBI diagnosis are either subjective or require\nextensive clinical setup and expertise. The increasing affordability and\nreduction in size of relatively high-performance computing systems combined\nwith promising results from TBI related machine learning research make it\npossible to create compact and portable systems for early detection of TBI.\nThis work describes a Raspberry Pi based portable, real-time data acquisition,\nand automated processing system that uses machine learning to efficiently\nidentify TBI and automatically score sleep stages from a single-channel\nElectroen-cephalogram (EEG) signal. We discuss the design, implementation, and\nverification of the system that can digitize EEG signal using an Analog to\nDigital Converter (ADC) and perform real-time signal classification to detect\nthe presence of mild TBI (mTBI). We utilize Convolutional Neural Networks (CNN)\nand XGBoost based predictive models to evaluate the performance and demonstrate\nthe versatility of the system to operate with multiple types of predictive\nmodels. We achieve a peak classification accuracy of more than 90% with a\nclassification time of less than 1 s across 16 s - 64 s epochs for TBI vs\ncontrol conditions. This work can enable development of systems suitable for\nfield use without requiring specialized medical equipment for early TBI\ndetection applications and TBI research. Further, this work opens avenues to\nimplement connected, real-time TBI related health and wellness monitoring\nsystems.\n", "versions": [{"version": "v1", "created": "Sat, 23 Jan 2021 09:49:33 GMT"}, {"version": "v2", "created": "Fri, 29 Jan 2021 06:26:14 GMT"}], "update_date": "2021-02-01", "authors_parsed": [["Dhillon", "Navjodh Singh", ""], ["Sutandi", "Agustinus", ""], ["Vishwanath", "Manoj", ""], ["Lim", "Miranda M.", ""], ["Cao", "Hung", ""], ["Si", "Dong", ""]]}, {"id": "2101.10870", "submitter": "Florenc Demrozi Dr.", "authors": "Florenc Demrozi, Cristian Turetta, Graziano Pravadelli", "title": "B-HAR: an open-source baseline framework for in depth study of human\n  activity recognition datasets and workflows", "comments": "9 Pages, 3 Figures, 3 Tables, Link to B-HAR Library:\n  https://github.com/B-HAR-HumanActivityRecognition/B-HAR", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SP cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Human Activity Recognition (HAR), based on machine and deep learning\nalgorithms is considered one of the most promising technologies to monitor\nprofessional and daily life activities for different categories of people\n(e.g., athletes, elderly, kids, employers) in order to provide a variety of\nservices related, for example to well-being, empowering of technical\nperformances, prevention of risky situation, and educational purposes. However,\nthe analysis of the effectiveness and the efficiency of HAR methodologies\nsuffers from the lack of a standard workflow, which might represent the\nbaseline for the estimation of the quality of the developed pattern recognition\nmodels. This makes the comparison among different approaches a challenging\ntask. In addition, researchers can make mistakes that, when not detected,\ndefinitely affect the achieved results. To mitigate such issues, this paper\nproposes an open-source automatic and highly configurable framework, named\nB-HAR, for the definition, standardization, and development of a baseline\nframework in order to evaluate and compare HAR methodologies. It implements the\nmost popular data processing methods for data preparation and the most commonly\nused machine and deep learning pattern recognition models.\n", "versions": [{"version": "v1", "created": "Sat, 23 Jan 2021 12:42:41 GMT"}], "update_date": "2021-01-27", "authors_parsed": [["Demrozi", "Florenc", ""], ["Turetta", "Cristian", ""], ["Pravadelli", "Graziano", ""]]}, {"id": "2101.10876", "submitter": "Rasika Karkare", "authors": "Rasika Karkare, Randy Paffenroth and Gunjan Mahindre", "title": "Blind Image Denoising and Inpainting Using Robust Hadamard Autoencoders", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we demonstrate how deep autoencoders can be generalized to the\ncase of inpainting and denoising, even when no clean training data is\navailable. In particular, we show how neural networks can be trained to perform\nall of these tasks simultaneously. While, deep autoencoders implemented by way\nof neural networks have demonstrated potential for denoising and anomaly\ndetection, standard autoencoders have the drawback that they require access to\nclean data for training. However, recent work in Robust Deep Autoencoders\n(RDAEs) shows how autoencoders can be trained to eliminate outliers and noise\nin a dataset without access to any clean training data. Inspired by this work,\nwe extend RDAEs to the case where data are not only noisy and have outliers,\nbut also only partially observed. Moreover, the dataset we train the neural\nnetwork on has the properties that all entries have noise, some entries are\ncorrupted by large mistakes, and many entries are not even known. Given such an\nalgorithm, many standard tasks, such as denoising, image inpainting, and\nunobserved entry imputation can all be accomplished simultaneously within the\nsame framework. Herein we demonstrate these techniques on standard machine\nlearning tasks, such as image inpainting and denoising for the MNIST and\nCIFAR10 datasets. However, these approaches are not only applicable to image\nprocessing problems, but also have wide ranging impacts on datasets arising\nfrom real-world problems, such as manufacturing and network processing, where\nnoisy, partially observed data naturally arise.\n", "versions": [{"version": "v1", "created": "Tue, 26 Jan 2021 15:33:22 GMT"}], "update_date": "2021-01-27", "authors_parsed": [["Karkare", "Rasika", ""], ["Paffenroth", "Randy", ""], ["Mahindre", "Gunjan", ""]]}, {"id": "2101.10892", "submitter": "Pedro Vicente", "authors": "Gon\\c{c}alo Cunha, Pedro Vicente, Alexandre Bernardino, Ricardo\n  Ribeiro, Pl\\'inio Moreno", "title": "Online Body Schema Adaptation through Cost-Sensitive Active Learning", "comments": "6 pages, 7 figures. Submitted to Humanoids 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AI cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Humanoid robots have complex bodies and kinematic chains with several\nDegrees-of-Freedom (DoF) which are difficult to model. Learning the parameters\nof a kinematic model can be achieved by observing the position of the robot\nlinks during prospective motions and minimising the prediction errors. This\nwork proposes a movement efficient approach for estimating online the\nbody-schema of a humanoid robot arm in the form of Denavit-Hartenberg (DH)\nparameters. A cost-sensitive active learning approach based on the A-Optimality\ncriterion is used to select optimal joint configurations. The chosen joint\nconfigurations simultaneously minimise the error in the estimation of the body\nschema and minimise the movement between samples. This reduces energy\nconsumption, along with mechanical fatigue and wear, while not compromising the\nlearning accuracy. The work was implemented in a simulation environment, using\nthe 7DoF arm of the iCub robot simulator. The hand pose is measured with a\nsingle camera via markers placed in the palm and back of the robot's hand. A\nnon-parametric occlusion model is proposed to avoid choosing joint\nconfigurations where the markers are not visible, thus preventing worthless\nattempts. The results show cost-sensitive active learning has similar accuracy\nto the standard active learning approach, while reducing in about half the\nexecuted movement.\n", "versions": [{"version": "v1", "created": "Tue, 26 Jan 2021 16:01:02 GMT"}], "update_date": "2021-01-27", "authors_parsed": [["Cunha", "Gon\u00e7alo", ""], ["Vicente", "Pedro", ""], ["Bernardino", "Alexandre", ""], ["Ribeiro", "Ricardo", ""], ["Moreno", "Pl\u00ednio", ""]]}, {"id": "2101.10893", "submitter": "Raisa Dzhamtyrova PhD", "authors": "Raisa Dzhamtyrova and Carsten Maple", "title": "Dynamic cyber risk estimation with Competitive Quantile Autoregression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The increasing value of data held in enterprises makes it an attractive\ntarget to attackers. The increasing likelihood and impact of a cyber attack\nhave highlighted the importance of effective cyber risk estimation. We propose\ntwo methods for modelling Value-at-Risk (VaR) which can be used for any\ntime-series data. The first approach is based on Quantile Autoregression (QAR),\nwhich can estimate VaR for different quantiles, i.e. confidence levels. The\nsecond method, we term Competitive Quantile Autoregression (CQAR), dynamically\nre-estimates cyber risk as soon as new data becomes available. This method\nprovides a theoretical guarantee that it asymptotically performs as well as any\nQAR at any time point in the future. We show that these methods can predict the\nsize and inter-arrival time of cyber hacking breaches by running coverage\ntests. The proposed approaches allow to model a separate stochastic process for\neach significance level and therefore provide more flexibility compared to\npreviously proposed techniques. We provide a fully reproducible code used for\nconducting the experiments.\n", "versions": [{"version": "v1", "created": "Mon, 25 Jan 2021 16:52:27 GMT"}, {"version": "v2", "created": "Thu, 22 Apr 2021 10:06:10 GMT"}], "update_date": "2021-04-23", "authors_parsed": [["Dzhamtyrova", "Raisa", ""], ["Maple", "Carsten", ""]]}, {"id": "2101.10902", "submitter": "Brandon Carter", "authors": "Ge Liu, Alexander Dimitrakakis, Brandon Carter, David Gifford", "title": "Maximum n-times Coverage for Vaccine Design", "comments": "10 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.QM cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce the maximum $n$-times coverage problem that selects $k$ overlays\nto maximize the summed coverage of weighted elements, where each element must\nbe covered at least $n$ times. We also define the min-cost $n$-times coverage\nproblem where the objective is to select the minimum set of overlays such that\nthe sum of the weights of elements that are covered at least $n$ times is at\nleast $\\tau$. Maximum $n$-times coverage is a generalization of the multi-set\nmulti-cover problem, is NP-complete, and is not submodular. We introduce two\nnew practical solutions for $n$-times coverage based on integer linear\nprogramming and sequential greedy optimization. We show that maximum $n$-times\ncoverage is a natural way to frame peptide vaccine design, and find that it\nproduces a pan-strain COVID-19 vaccine design that is superior to 29 other\npublished designs in predicted population coverage and the expected number of\npeptides displayed by each individual's HLA molecules.\n", "versions": [{"version": "v1", "created": "Sun, 24 Jan 2021 22:20:24 GMT"}, {"version": "v2", "created": "Sat, 12 Jun 2021 00:46:04 GMT"}, {"version": "v3", "created": "Tue, 15 Jun 2021 15:07:02 GMT"}], "update_date": "2021-06-16", "authors_parsed": [["Liu", "Ge", ""], ["Dimitrakakis", "Alexander", ""], ["Carter", "Brandon", ""], ["Gifford", "David", ""]]}, {"id": "2101.10904", "submitter": "Ranwa Al Mallah", "authors": "Ranwa Al Mallah, David Lopez, Bilal Farooq", "title": "Untargeted Poisoning Attack Detection in Federated Learning via Behavior\n  Attestation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Federated Learning (FL) is a paradigm in Machine Learning (ML) that addresses\ndata privacy, security, access rights and access to heterogeneous information\nissues by training a global model using distributed nodes. Despite its\nadvantages, there is an increased potential for cyberattacks on FL-based ML\ntechniques that can undermine the benefits. Model-poisoning attacks on FL\ntarget the availability of the model. The adversarial objective is to disrupt\nthe training. We propose attestedFL, a defense mechanism that monitors the\ntraining of individual nodes through state persistence in order to detect a\nmalicious worker. A fine-grained assessment of the history of the worker\npermits the evaluation of its behavior in time and results in innovative\ndetection strategies. We present three lines of defense that aim at assessing\nif the worker is reliable by observing if the node is really training,\nadvancing towards a goal. Our defense exposes an attacker's malicious behavior\nand removes unreliable nodes from the aggregation process so that the FL\nprocess converge faster. Through extensive evaluations and against various\nadversarial settings, attestedFL increased the accuracy of the model between\n12% to 58% under different scenarios such as attacks performed at different\nstages of convergence, attackers colluding and continuous attacks.\n", "versions": [{"version": "v1", "created": "Sun, 24 Jan 2021 20:52:55 GMT"}, {"version": "v2", "created": "Thu, 28 Jan 2021 14:50:24 GMT"}], "update_date": "2021-01-29", "authors_parsed": [["Mallah", "Ranwa Al", ""], ["Lopez", "David", ""], ["Farooq", "Bilal", ""]]}, {"id": "2101.10905", "submitter": "Francesco Silvestri", "authors": "Martin Aum\\\"uller, Sariel Har-Peled, Sepideh Mahabadi, Rasmus Pagh,\n  Francesco Silvestri", "title": "Sampling a Near Neighbor in High Dimensions -- Who is the Fairest of\n  Them All?", "comments": "arXiv admin note: text overlap with arXiv:1906.02640", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DB cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Similarity search is a fundamental algorithmic primitive, widely used in many\ncomputer science disciplines. Given a set of points $S$ and a radius parameter\n$r>0$, the $r$-near neighbor ($r$-NN) problem asks for a data structure that,\ngiven any query point $q$, returns a point $p$ within distance at most $r$ from\n$q$. In this paper, we study the $r$-NN problem in the light of individual\nfairness and providing equal opportunities: all points that are within distance\n$r$ from the query should have the same probability to be returned. In the\nlow-dimensional case, this problem was first studied by Hu, Qiao, and Tao (PODS\n2014). Locality sensitive hashing (LSH), the theoretically strongest approach\nto similarity search in high dimensions, does not provide such a fairness\nguarantee. In this work, we show that LSH based algorithms can be made fair,\nwithout a significant loss in efficiency. We propose several efficient data\nstructures for the exact and approximate variants of the fair NN problem. Our\napproach works more generally for sampling uniformly from a sub-collection of\nsets of a given collection and can be used in a few other applications. We also\ndevelop a data structure for fair similarity search under inner product that\nrequires nearly-linear space and exploits locality sensitive filters. The paper\nconcludes with an experimental evaluation that highlights the inherent\nunfairness of NN data structures and shows the performance of our algorithms on\nreal-world datasets.\n", "versions": [{"version": "v1", "created": "Tue, 26 Jan 2021 16:13:07 GMT"}], "update_date": "2021-01-27", "authors_parsed": [["Aum\u00fcller", "Martin", ""], ["Har-Peled", "Sariel", ""], ["Mahabadi", "Sepideh", ""], ["Pagh", "Rasmus", ""], ["Silvestri", "Francesco", ""]]}, {"id": "2101.10919", "submitter": "Greg Olmschenk", "authors": "Greg Olmschenk, Stela Ishitani Silva, Gioia Rau, Richard K. Barry,\n  Ethan Kruse, Luca Cacciapuoti, Veselin Kostov, Brian P. Powell, Edward\n  Wyrwas, Jeremy D. Schnittman, Thomas Barclay", "title": "Identifying Planetary Transit Candidates in TESS Full-Frame Image Light\n  Curves via Convolutional Neural Networks", "comments": "Updated to match AJ revision", "journal-ref": "The Astronomical Journal, 2021, Volume 161, Article 273", "doi": "10.3847/1538-3881/abf4c6", "report-no": null, "categories": "astro-ph.EP astro-ph.IM cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Transiting Exoplanet Survey Satellite (TESS) mission measured light from\nstars in ~75% of the sky throughout its two year primary mission, resulting in\nmillions of TESS 30-minute cadence light curves to analyze in the search for\ntransiting exoplanets. To search this vast data trove for transit signals, we\naim to provide an approach that is both computationally efficient and produces\nhighly performant predictions. This approach minimizes the required human\nsearch effort. We present a convolutional neural network, which we train to\nidentify planetary transit signals and dismiss false positives. To make a\nprediction for a given light curve, our network requires no prior transit\nparameters identified using other methods. Our network performs inference on a\nTESS 30-minute cadence light curve in ~5ms on a single GPU, enabling large\nscale archival searches. We present 181 new planet candidates identified by our\nnetwork, which pass subsequent human vetting designed to rule out false\npositives. Our neural network model is additionally provided as open-source\ncode for public use and extension.\n", "versions": [{"version": "v1", "created": "Tue, 26 Jan 2021 16:40:51 GMT"}, {"version": "v2", "created": "Mon, 24 May 2021 20:33:19 GMT"}], "update_date": "2021-05-26", "authors_parsed": [["Olmschenk", "Greg", ""], ["Silva", "Stela Ishitani", ""], ["Rau", "Gioia", ""], ["Barry", "Richard K.", ""], ["Kruse", "Ethan", ""], ["Cacciapuoti", "Luca", ""], ["Kostov", "Veselin", ""], ["Powell", "Brian P.", ""], ["Wyrwas", "Edward", ""], ["Schnittman", "Jeremy D.", ""], ["Barclay", "Thomas", ""]]}, {"id": "2101.10932", "submitter": "Ce Zhang Mr.", "authors": "Ce Zhang, Young-Keun Kim, Azim Eskandarian", "title": "EEG-Inception: An Accurate and Robust End-to-End Neural Network for\n  EEG-based Motor Imagery Classification", "comments": "Provisionally Accepted by Journal of Neural Engineering", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SP cs.HC cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Classification of EEG-based motor imagery (MI) is a crucial non-invasive\napplication in brain-computer interface (BCI) research. This paper proposes a\nnovel convolutional neural network (CNN) architecture for accurate and robust\nEEG-based MI classification that outperforms the state-of-the-art methods. The\nproposed CNN model, namely EEG-Inception, is built on the backbone of the\nInception-Time network, which showed to be highly efficient and accurate for\ntime-series classification. Also, the proposed network is an end-to-end\nclassification, as it takes the raw EEG signals as the input and does not\nrequire complex EEG signal-preprocessing. Furthermore, this paper proposes a\nnovel data augmentation method for EEG signals to enhance the accuracy, at\nleast by 3%, and reduce overfitting with limited BCI datasets. The proposed\nmodel outperforms all the state-of-the-art methods by achieving the average\naccuracy of 88.4% and 88.6% on the 2008 BCI Competition IV 2a (four-classes)\nand 2b datasets (binary-classes), respectively. Furthermore, it takes less than\n0.025 seconds to test a sample suitable for real-time processing. Moreover, the\nclassification standard deviation for nine different subjects achieves the\nlowest value of 5.5 for the 2b dataset and 7.1 for the 2a dataset, which\nvalidates that the proposed method is highly robust. From the experiment\nresults, it can be inferred that the EEG-Inception network exhibits a strong\npotential as a subject-independent classifier for EEG-based MI tasks.\n", "versions": [{"version": "v1", "created": "Sun, 24 Jan 2021 19:03:10 GMT"}, {"version": "v2", "created": "Mon, 1 Feb 2021 22:19:11 GMT"}, {"version": "v3", "created": "Mon, 8 Mar 2021 15:51:01 GMT"}], "update_date": "2021-03-09", "authors_parsed": [["Zhang", "Ce", ""], ["Kim", "Young-Keun", ""], ["Eskandarian", "Azim", ""]]}, {"id": "2101.10942", "submitter": "Yi Wei", "authors": "Yi Wei", "title": "Absolute Value Constraint: The Reason for Invalid Performance Evaluation\n  Results of Neural Network Models for Stock Price Prediction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.ST cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Neural networks for stock price prediction(NNSPP) have been popular for\ndecades. However, most of its study results remain in the research paper and\ncannot truly play a role in the securities market. One of the main reasons\nleading to this situation is that the prediction error(PE) based evaluation\nresults have statistical flaws. Its prediction results cannot represent the\nmost critical financial direction attributes. So it cannot provide investors\nwith convincing, interpretable, and consistent model performance evaluation\nresults for practical applications in the securities market. To illustrate, we\nhave used data selected from 20 stock datasets over six years from the Shanghai\nand Shenzhen stock market in China, and 20 stock datasets from NASDAQ and NYSE\nin the USA. We implement six shallow and deep neural networks to predict stock\nprices and use four prediction error measures for evaluation. The results show\nthat the prediction error value only partially reflects the model accuracy of\nthe stock price prediction, and cannot reflect the change in the direction of\nthe model predicted stock price. This characteristic determines that PE is not\nsuitable as an evaluation indicator of NNSPP. Otherwise, it will bring huge\npotential risks to investors. Therefore, this paper establishes an experiment\nplatform to confirm that the PE method is not suitable for the NNSPP\nevaluation, and provides a theoretical basis for the necessity of creating a\nnew NNSPP evaluation method in the future.\n", "versions": [{"version": "v1", "created": "Sun, 10 Jan 2021 06:51:23 GMT"}, {"version": "v2", "created": "Fri, 19 Mar 2021 03:56:09 GMT"}], "update_date": "2021-03-22", "authors_parsed": [["Wei", "Yi", ""]]}, {"id": "2101.10943", "submitter": "Alicia Curth", "authors": "Alicia Curth and Mihaela van der Schaar", "title": "Nonparametric Estimation of Heterogeneous Treatment Effects: From Theory\n  to Learning Algorithms", "comments": "To appear in the Proceedings of the 24th International Conference on\n  Artificial Intelligence and Statistics (AISTATS) 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The need to evaluate treatment effectiveness is ubiquitous in most of\nempirical science, and interest in flexibly investigating effect heterogeneity\nis growing rapidly. To do so, a multitude of model-agnostic, nonparametric\nmeta-learners have been proposed in recent years. Such learners decompose the\ntreatment effect estimation problem into separate sub-problems, each solvable\nusing standard supervised learning methods. Choosing between different\nmeta-learners in a data-driven manner is difficult, as it requires access to\ncounterfactual information. Therefore, with the ultimate goal of building\nbetter understanding of the conditions under which some learners can be\nexpected to perform better than others a priori, we theoretically analyze four\nbroad meta-learning strategies which rely on plug-in estimation and\npseudo-outcome regression. We highlight how this theoretical reasoning can be\nused to guide principled algorithm design and translate our analyses into\npractice by considering a variety of neural network architectures as\nbase-learners for the discussed meta-learning strategies. In a simulation\nstudy, we showcase the relative strengths of the learners under different\ndata-generating processes.\n", "versions": [{"version": "v1", "created": "Tue, 26 Jan 2021 17:11:40 GMT"}, {"version": "v2", "created": "Thu, 25 Feb 2021 13:36:14 GMT"}], "update_date": "2021-02-26", "authors_parsed": [["Curth", "Alicia", ""], ["van der Schaar", "Mihaela", ""]]}, {"id": "2101.10950", "submitter": "Ali Amiryousefi", "authors": "Ali Amiryousefi", "title": "Asymptotic Supervised Predictive Classifiers under Partition\n  Exchangeability", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.ST stat.TH", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  The convergence of simultaneous and marginal predictive classifiers under\npartition exchangeability in supervised classification is obtained. The result\nshows the asymptotic convergence of these classifiers under infinite amount of\ntraining or test data, such that after observing umpteen amount of data, the\ndifferences between these classifiers would be negligible. This is an important\nresult from the practical perspective as under the presence of sufficiently\nlarge amount of data, one can replace the simpler marginal classifier with\ncomputationally more expensive simultaneous one.\n", "versions": [{"version": "v1", "created": "Tue, 26 Jan 2021 17:17:40 GMT"}], "update_date": "2021-01-27", "authors_parsed": [["Amiryousefi", "Ali", ""]]}, {"id": "2101.10951", "submitter": "Marc Z\\\"oller", "authors": "Marc-Andr\\'e Z\\\"oller, Tien-Dung Nguyen, Marco F. Huber", "title": "Incremental Search Space Construction for Machine Learning Pipeline\n  Synthesis", "comments": "Conference paper accepted at Symposium on Intelligent Data Analysis\n  (IDA) 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automated machine learning (AutoML) aims for constructing machine learning\n(ML) pipelines automatically. Many studies have investigated efficient methods\nfor algorithm selection and hyperparameter optimization. However, methods for\nML pipeline synthesis and optimization considering the impact of complex\npipeline structures containing multiple preprocessing and classification\nalgorithms have not been studied thoroughly. In this paper, we propose a\ndata-centric approach based on meta-features for pipeline construction and\nhyperparameter optimization inspired by human behavior. By expanding the\npipeline search space incrementally in combination with meta-features of\nintermediate data sets, we are able to prune the pipeline structure search\nspace efficiently. Consequently, flexible and data set specific ML pipelines\ncan be constructed. We prove the effectiveness and competitiveness of our\napproach on 28 data sets used in well-established AutoML benchmarks in\ncomparison with state-of-the-art AutoML frameworks.\n", "versions": [{"version": "v1", "created": "Tue, 26 Jan 2021 17:17:49 GMT"}], "update_date": "2021-01-27", "authors_parsed": [["Z\u00f6ller", "Marc-Andr\u00e9", ""], ["Nguyen", "Tien-Dung", ""], ["Huber", "Marco F.", ""]]}, {"id": "2101.10967", "submitter": "Kushal Chakrabarti", "authors": "Kushal Chakrabarti, Nirupam Gupta and Nikhil Chopra", "title": "Robustness of Iteratively Pre-Conditioned Gradient-Descent Method: The\n  Case of Distributed Linear Regression Problem", "comments": "in IEEE Control Systems Letters. Related articles: arXiv:2003.07180v2\n  [math.OC], arXiv:2008.02856v1 [math.OC], and arXiv:2011.07595v2 [math.OC]", "journal-ref": null, "doi": "10.1109/LCSYS.2020.3045533", "report-no": null, "categories": "math.OC cs.LG cs.SY eess.SY stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper considers the problem of multi-agent distributed linear regression\nin the presence of system noises. In this problem, the system comprises\nmultiple agents wherein each agent locally observes a set of data points, and\nthe agents' goal is to compute a linear model that best fits the collective\ndata points observed by all the agents. We consider a server-based distributed\narchitecture where the agents interact with a common server to solve the\nproblem; however, the server cannot access the agents' data points. We consider\na practical scenario wherein the system either has observation noise, i.e., the\ndata points observed by the agents are corrupted, or has process noise, i.e.,\nthe computations performed by the server and the agents are corrupted. In\nnoise-free systems, the recently proposed distributed linear regression\nalgorithm, named the Iteratively Pre-conditioned Gradient-descent (IPG) method,\nhas been claimed to converge faster than related methods. In this paper, we\nstudy the robustness of the IPG method, against both the observation noise and\nthe process noise. We empirically show that the robustness of the IPG method\ncompares favorably to the state-of-the-art algorithms.\n", "versions": [{"version": "v1", "created": "Tue, 26 Jan 2021 17:51:49 GMT"}], "update_date": "2021-01-27", "authors_parsed": [["Chakrabarti", "Kushal", ""], ["Gupta", "Nirupam", ""], ["Chopra", "Nikhil", ""]]}, {"id": "2101.10973", "submitter": "Benjamin Horne", "authors": "Maur\\'icio Gruppi, Benjamin D. Horne, Sibel Adal{\\i}", "title": "Tell Me Who Your Friends Are: Using Content Sharing Behavior for News\n  Source Veracity Detection", "comments": "Preprint Version", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.CY cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Stopping the malicious spread and production of false and misleading news has\nbecome a top priority for researchers. Due to this prevalence, many automated\nmethods for detecting low quality information have been introduced. The\nmajority of these methods have used article-level features, such as their\nwriting style, to detect veracity. While writing style models have been shown\nto work well in lab-settings, there are concerns of generalizability and\nrobustness. In this paper, we begin to address these concerns by proposing a\nnovel and robust news veracity detection model that uses the content sharing\nbehavior of news sources formulated as a network. We represent these content\nsharing networks (CSN) using a deep walk based method for embedding graphs that\naccounts for similarity in both the network space and the article text space.\nWe show that state of the art writing style and CSN features make diverse\nmistakes when predicting, meaning that they both play different roles in the\nclassification task. Moreover, we show that the addition of CSN features\nincreases the accuracy of writing style models, boosting accuracy as much as\n14\\% when using Random Forests. Similarly, we show that the combination of\nhand-crafted article-level features and CSN features is robust to concept\ndrift, performing consistently well over a 10-month time frame.\n", "versions": [{"version": "v1", "created": "Fri, 15 Jan 2021 21:39:51 GMT"}], "update_date": "2021-01-27", "authors_parsed": [["Gruppi", "Maur\u00edcio", ""], ["Horne", "Benjamin D.", ""], ["Adal\u0131", "Sibel", ""]]}, {"id": "2101.10977", "submitter": "Lukas Brunke", "authors": "Lukas Brunke, Prateek Agrawal, Nikhil George", "title": "Evaluating Input Perturbation Methods for Interpreting CNNs and Saliency\n  Map Comparison", "comments": null, "journal-ref": "ECCV 2020: Computer Vision - ECCV 2020 Workshops pp 120-134", "doi": "10.1007/978-3-030-66415-2_8", "report-no": null, "categories": "cs.LG cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Input perturbation methods occlude parts of an input to a function and\nmeasure the change in the function's output. Recently, input perturbation\nmethods have been applied to generate and evaluate saliency maps from\nconvolutional neural networks. In practice, neutral baseline images are used\nfor the occlusion, such that the baseline image's impact on the classification\nprobability is minimal. However, in this paper we show that arguably neutral\nbaseline images still impact the generated saliency maps and their evaluation\nwith input perturbations. We also demonstrate that many choices of\nhyperparameters lead to the divergence of saliency maps generated by input\nperturbations. We experimentally reveal inconsistencies among a selection of\ninput perturbation methods and find that they lack robustness for generating\nsaliency maps and for evaluating saliency maps as saliency metrics.\n", "versions": [{"version": "v1", "created": "Tue, 26 Jan 2021 18:11:06 GMT"}], "update_date": "2021-01-27", "authors_parsed": [["Brunke", "Lukas", ""], ["Agrawal", "Prateek", ""], ["George", "Nikhil", ""]]}, {"id": "2101.10983", "submitter": "Karthigan Sinnathamby", "authors": "Karthigan Sinnathamby, Chang-Yu Hou, Lalitha Venkataramanan,\n  Vasileios-Marios Gkortsas, Fran\\c{c}ois Fleuret", "title": "Unsupervised clustering of series using dynamic programming and neural\n  processes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Following the work of arXiv:2101.09512, we are interested in clustering a\ngiven multi-variate series in an unsupervised manner. We would like to segment\nand cluster the series such that the resulting blocks present in each cluster\nare coherent with respect to a predefined model structure (e.g. a physics model\nwith a functional form defined by a number of parameters). However, such\napproach might have its limitation, partly because there may exist multiple\nmodels that describe the same data, and partly because the exact model behind\nthe data may not immediately known. Hence, it is useful to establish a general\nframework that enables the integration of plausible models and also\naccommodates data-driven approach into one approximated model to assist the\nclustering task. Hence, in this work, we investigate the use of neural\nprocesses to build the approximated model while yielding the same assumptions\nrequired by the algorithm presented in arXiv:2101.09512.\n", "versions": [{"version": "v1", "created": "Tue, 26 Jan 2021 18:17:10 GMT"}], "update_date": "2021-01-27", "authors_parsed": [["Sinnathamby", "Karthigan", ""], ["Hou", "Chang-Yu", ""], ["Venkataramanan", "Lalitha", ""], ["Gkortsas", "Vasileios-Marios", ""], ["Fleuret", "Fran\u00e7ois", ""]]}, {"id": "2101.10998", "submitter": "Cong Shen", "authors": "Hyun-Suk Lee, Cong Shen, William Zame, Jang-Won Lee, Mihaela van der\n  Schaar", "title": "SDF-Bayes: Cautious Optimism in Safe Dose-Finding Clinical Trials with\n  Drug Combinations and Heterogeneous Patient Groups", "comments": "Accepted to AISTATS 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Phase I clinical trials are designed to test the safety (non-toxicity) of\ndrugs and find the maximum tolerated dose (MTD). This task becomes\nsignificantly more challenging when multiple-drug dose-combinations (DC) are\ninvolved, due to the inherent conflict between the exponentially increasing DC\ncandidates and the limited patient budget. This paper proposes a novel Bayesian\ndesign, SDF-Bayes, for finding the MTD for drug combinations in the presence of\nsafety constraints. Rather than the conventional principle of escalating or\nde-escalating the current dose of one drug (perhaps alternating between drugs),\nSDF-Bayes proceeds by cautious optimism: it chooses the next DC that, on the\nbasis of current information, is most likely to be the MTD (optimism), subject\nto the constraint that it only chooses DCs that have a high probability of\nbeing safe (caution). We also propose an extension, SDF-Bayes-AR, that accounts\nfor patient heterogeneity and enables heterogeneous patient recruitment.\nExtensive experiments based on both synthetic and real-world datasets\ndemonstrate the advantages of SDF-Bayes over state of the art DC trial designs\nin terms of accuracy and safety.\n", "versions": [{"version": "v1", "created": "Tue, 26 Jan 2021 18:59:26 GMT"}], "update_date": "2021-01-27", "authors_parsed": [["Lee", "Hyun-Suk", ""], ["Shen", "Cong", ""], ["Zame", "William", ""], ["Lee", "Jang-Won", ""], ["van der Schaar", "Mihaela", ""]]}, {"id": "2101.11003", "submitter": "Steven Golovkine", "authors": "Steven Golovkine", "title": "FDApy: a Python package for functional data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MS cs.LG stat.CO stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We introduce the Python package, FDApy, as an implementation of functional\ndata. This package provide modules for the analysis of such data. It includes\nclasses for different dimensional data as well as irregularly sampled\nfunctional data. A simulation toolbox is also provided. It might be used to\nsimulate different clusters of functional data. Some methodologies to handle\nthese data are implemented, such as dimension reduction and clustering. New\nmethods can be easily added. The package is publicly available on the Python\nPackage Index and Github.\n", "versions": [{"version": "v1", "created": "Tue, 26 Jan 2021 10:07:33 GMT"}], "update_date": "2021-01-28", "authors_parsed": [["Golovkine", "Steven", ""]]}, {"id": "2101.11037", "submitter": "Oliver Urs Lenz", "authors": "Oliver Urs Lenz, Daniel Peralta, Chris Cornelis", "title": "Average Localised Proximity: A new data descriptor with good default\n  one-class classification performance", "comments": "Accepted manuscript", "journal-ref": "Pattern Recognition 118 (2021) 107991", "doi": "10.1016/j.patcog.2021.107991", "report-no": null, "categories": "cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  One-class classification is a challenging subfield of machine learning in\nwhich so-called data descriptors are used to predict membership of a class\nbased solely on positive examples of that class, and no counter-examples. A\nnumber of data descriptors that have been shown to perform well in previous\nstudies of one-class classification, like the Support Vector Machine (SVM),\nrequire setting one or more hyperparameters. There has been no systematic\nattempt to date to determine optimal default values for these hyperparameters,\nwhich limits their ease of use, especially in comparison with\nhyperparameter-free proposals like the Isolation Forest (IF). We address this\nissue by determining optimal default hyperparameter values across a collection\nof 246 one-class classification problems derived from 50 different real-world\ndatasets. In addition, we propose a new data descriptor, Average Localised\nProximity (ALP) to address certain issues with existing approaches based on\nnearest neighbour distances. Finally, we evaluate classification performance\nusing a leave-one-dataset-out procedure, and find strong evidence that ALP\noutperforms IF and a number of other data descriptors, as well as weak evidence\nthat it outperforms SVM, making ALP a good default choice.\n", "versions": [{"version": "v1", "created": "Tue, 26 Jan 2021 19:14:14 GMT"}, {"version": "v2", "created": "Mon, 31 May 2021 12:17:09 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Lenz", "Oliver Urs", ""], ["Peralta", "Daniel", ""], ["Cornelis", "Chris", ""]]}, {"id": "2101.11038", "submitter": "Armen Aghajanyan", "authors": "Armen Aghajanyan, Anchit Gupta, Akshat Shrivastava, Xilun Chen, Luke\n  Zettlemoyer, Sonal Gupta", "title": "Muppet: Massive Multi-task Representations with Pre-Finetuning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We propose pre-finetuning, an additional large-scale learning stage between\nlanguage model pre-training and fine-tuning. Pre-finetuning is massively\nmulti-task learning (around 50 datasets, over 4.8 million total labeled\nexamples), and is designed to encourage learning of representations that\ngeneralize better to many different tasks. We show that pre-finetuning\nconsistently improves performance for pretrained discriminators (e.g.~RoBERTa)\nand generation models (e.g.~BART) on a wide range of tasks (sentence\nprediction, commonsense reasoning, MRC, etc.), while also significantly\nimproving sample efficiency during fine-tuning. We also show that large-scale\nmulti-tasking is crucial; pre-finetuning can hurt performance when few tasks\nare used up until a critical point (usually above 15) after which performance\nimproves linearly in the number of tasks.\n", "versions": [{"version": "v1", "created": "Tue, 26 Jan 2021 19:18:27 GMT"}], "update_date": "2021-01-28", "authors_parsed": [["Aghajanyan", "Armen", ""], ["Gupta", "Anchit", ""], ["Shrivastava", "Akshat", ""], ["Chen", "Xilun", ""], ["Zettlemoyer", "Luke", ""], ["Gupta", "Sonal", ""]]}, {"id": "2101.11041", "submitter": "Jelena Diakonikolas", "authors": "Jelena Diakonikolas and Crist\\'obal Guzm\\'an", "title": "Complementary Composite Minimization, Small Gradients in General Norms,\n  and Applications to Regression Problems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.DS cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Composite minimization is a powerful framework in large-scale convex\noptimization, based on decoupling of the objective function into terms with\nstructurally different properties and allowing for more flexible algorithmic\ndesign. In this work, we introduce a new algorithmic framework for\ncomplementary composite minimization, where the objective function decouples\ninto a (weakly) smooth and a uniformly convex term. This particular form of\ndecoupling is pervasive in statistics and machine learning, due to its link to\nregularization.\n  The main contributions of our work are summarized as follows. First, we\nintroduce the problem of complementary composite minimization in general normed\nspaces; second, we provide a unified accelerated algorithmic framework to\naddress broad classes of complementary composite minimization problems; and\nthird, we prove that the algorithms resulting from our framework are\nnear-optimal in most of the standard optimization settings. Additionally, we\nshow that our algorithmic framework can be used to address the problem of\nmaking the gradients small in general normed spaces. As a concrete example, we\nobtain a nearly-optimal method for the standard $\\ell_1$ setup (small gradients\nin the $\\ell_\\infty$ norm), essentially matching the bound of Nesterov (2012)\nthat was previously known only for the Euclidean setup. Finally, we show that\nour composite methods are broadly applicable to a number of regression\nproblems, leading to complexity bounds that are either new or match the best\nexisting ones.\n", "versions": [{"version": "v1", "created": "Tue, 26 Jan 2021 19:21:28 GMT"}], "update_date": "2021-01-28", "authors_parsed": [["Diakonikolas", "Jelena", ""], ["Guzm\u00e1n", "Crist\u00f3bal", ""]]}, {"id": "2101.11046", "submitter": "Matthias Bauer", "authors": "Matthias Bauer and Andriy Mnih", "title": "Generalized Doubly Reparameterized Gradient Estimators", "comments": null, "journal-ref": "38th International Conference on Machine Learning (ICML 2021)", "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Efficient low-variance gradient estimation enabled by the reparameterization\ntrick (RT) has been essential to the success of variational autoencoders.\nDoubly-reparameterized gradients (DReGs) improve on the RT for multi-sample\nvariational bounds by applying reparameterization a second time for an\nadditional reduction in variance. Here, we develop two generalizations of the\nDReGs estimator and show that they can be used to train conditional and\nhierarchical VAEs on image modelling tasks more effectively. First, we extend\nthe estimator to hierarchical models with several stochastic layers by showing\nhow to treat additional score function terms due to the hierarchical\nvariational posterior. We then generalize DReGs to score functions of arbitrary\ndistributions instead of just those of the sampling distribution, which makes\nthe estimator applicable to the parameters of the prior in addition to those of\nthe posterior.\n", "versions": [{"version": "v1", "created": "Tue, 26 Jan 2021 19:30:00 GMT"}, {"version": "v2", "created": "Tue, 13 Jul 2021 15:52:44 GMT"}], "update_date": "2021-07-14", "authors_parsed": [["Bauer", "Matthias", ""], ["Mnih", "Andriy", ""]]}, {"id": "2101.11055", "submitter": "Dhruv Kohli", "authors": "Dhruv Kohli, Alexander Cloninger, Gal Mishne", "title": "LDLE: Low Distortion Local Eigenmaps", "comments": "37 pages, 23 figures, preprint", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.SP cs.LG math.AP stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We present Low Distortion Local Eigenmaps (LDLE), a manifold learning\ntechnique which constructs a set of low distortion local views of a dataset in\nlower dimension and registers them to obtain a global embedding. The local\nviews are constructed using the global eigenvectors of the graph Laplacian and\nare registered using Procrustes analysis. The choice of these eigenvectors may\nvary across the regions. In contrast to existing techniques, LDLE is more\ngeometric and can embed manifolds without boundary as well as non-orientable\nmanifolds into their intrinsic dimension.\n", "versions": [{"version": "v1", "created": "Tue, 26 Jan 2021 19:55:05 GMT"}], "update_date": "2021-01-28", "authors_parsed": [["Kohli", "Dhruv", ""], ["Cloninger", "Alexander", ""], ["Mishne", "Gal", ""]]}, {"id": "2101.11058", "submitter": "Orchid Majumder", "authors": "Orchid Majumder, Avinash Ravichandran, Subhransu Maji, Alessandro\n  Achille, Marzia Polito, Stefano Soatto", "title": "Supervised Momentum Contrastive Learning for Few-Shot Classification", "comments": "V2 version; updated with new experiments and figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Few-shot learning aims to transfer information from one task to enable\ngeneralization on novel tasks given a few examples. This information is present\nboth in the domain and the class labels. In this work we investigate the\ncomplementary roles of these two sources of information by combining\ninstance-discriminative contrastive learning and supervised learning in a\nsingle framework called Supervised Momentum Contrastive learning (SUPMOCO). Our\napproach avoids a problem observed in supervised learning where information in\nimages not relevant to the task is discarded, which hampers their\ngeneralization to novel tasks. We show that (self-supervised) contrastive\nlearning and supervised learning are mutually beneficial, leading to a new\nstate-of-the-art on the META-DATASET - a recently introduced benchmark for\nfew-shot learning. Our method is based on a simple modification of MOCO and\nscales better than prior work on combining supervised and self-supervised\nlearning. This allows us to easily combine data from multiple domains leading\nto further improvements.\n", "versions": [{"version": "v1", "created": "Tue, 26 Jan 2021 19:58:08 GMT"}, {"version": "v2", "created": "Mon, 21 Jun 2021 19:34:56 GMT"}], "update_date": "2021-06-23", "authors_parsed": [["Majumder", "Orchid", ""], ["Ravichandran", "Avinash", ""], ["Maji", "Subhransu", ""], ["Achille", "Alessandro", ""], ["Polito", "Marzia", ""], ["Soatto", "Stefano", ""]]}, {"id": "2101.11071", "submitter": "William Guss", "authors": "William H. Guss, Mario Ynocente Castro, Sam Devlin, Brandon Houghton,\n  Noboru Sean Kuno, Crissman Loomis, Stephanie Milani, Sharada Mohanty, Keisuke\n  Nakata, Ruslan Salakhutdinov, John Schulman, Shinya Shiroshita, Nicholay\n  Topin, Avinash Ummadisingu, Oriol Vinyals", "title": "The MineRL 2020 Competition on Sample Efficient Reinforcement Learning\n  using Human Priors", "comments": "37 pages, initial submission, accepted at NeurIPS. arXiv admin note:\n  substantial text overlap with arXiv:1904.10079", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Although deep reinforcement learning has led to breakthroughs in many\ndifficult domains, these successes have required an ever-increasing number of\nsamples, affording only a shrinking segment of the AI community access to their\ndevelopment. Resolution of these limitations requires new, sample-efficient\nmethods. To facilitate research in this direction, we propose this second\niteration of the MineRL Competition. The primary goal of the competition is to\nfoster the development of algorithms which can efficiently leverage human\ndemonstrations to drastically reduce the number of samples needed to solve\ncomplex, hierarchical, and sparse environments. To that end, participants\ncompete under a limited environment sample-complexity budget to develop systems\nwhich solve the MineRL ObtainDiamond task in Minecraft, a sequential decision\nmaking environment requiring long-term planning, hierarchical control, and\nefficient exploration methods. The competition is structured into two rounds in\nwhich competitors are provided several paired versions of the dataset and\nenvironment with different game textures and shaders. At the end of each round,\ncompetitors submit containerized versions of their learning algorithms to the\nAIcrowd platform where they are trained from scratch on a hold-out\ndataset-environment pair for a total of 4-days on a pre-specified hardware\nplatform. In this follow-up iteration to the NeurIPS 2019 MineRL Competition,\nwe implement new features to expand the scale and reach of the competition. In\nresponse to the feedback of the previous participants, we introduce a second\nminor track focusing on solutions without access to environment interactions of\nany kind except during test-time. Further we aim to prompt domain agnostic\nsubmissions by implementing several novel competition mechanics including\naction-space randomization and desemantization of observations and actions.\n", "versions": [{"version": "v1", "created": "Tue, 26 Jan 2021 20:32:30 GMT"}], "update_date": "2021-01-28", "authors_parsed": [["Guss", "William H.", ""], ["Castro", "Mario Ynocente", ""], ["Devlin", "Sam", ""], ["Houghton", "Brandon", ""], ["Kuno", "Noboru Sean", ""], ["Loomis", "Crissman", ""], ["Milani", "Stephanie", ""], ["Mohanty", "Sharada", ""], ["Nakata", "Keisuke", ""], ["Salakhutdinov", "Ruslan", ""], ["Schulman", "John", ""], ["Shiroshita", "Shinya", ""], ["Topin", "Nicholay", ""], ["Ummadisingu", "Avinash", ""], ["Vinyals", "Oriol", ""]]}, {"id": "2101.11073", "submitter": "Saeed Mahloujifar", "authors": "Melissa Chase, Esha Ghosh, Saeed Mahloujifar", "title": "Property Inference From Poisoning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Property inference attacks consider an adversary who has access to the\ntrained model and tries to extract some global statistics of the training data.\nIn this work, we study property inference in scenarios where the adversary can\nmaliciously control part of the training data (poisoning data) with the goal of\nincreasing the leakage.\n  Previous work on poisoning attacks focused on trying to decrease the accuracy\nof models either on the whole population or on specific sub-populations or\ninstances. Here, for the first time, we study poisoning attacks where the goal\nof the adversary is to increase the information leakage of the model. Our\nfindings suggest that poisoning attacks can boost the information leakage\nsignificantly and should be considered as a stronger threat model in sensitive\napplications where some of the data sources may be malicious.\n  We describe our \\emph{property inference poisoning attack} that allows the\nadversary to learn the prevalence in the training data of any property it\nchooses. We theoretically prove that our attack can always succeed as long as\nthe learning algorithm used has good generalization properties.\n  We then verify the effectiveness of our attack by experimentally evaluating\nit on two datasets: a Census dataset and the Enron email dataset. We were able\nto achieve above $90\\%$ attack accuracy with $9-10\\%$ poisoning in all of our\nexperiments.\n", "versions": [{"version": "v1", "created": "Tue, 26 Jan 2021 20:35:28 GMT"}], "update_date": "2021-01-28", "authors_parsed": [["Chase", "Melissa", ""], ["Ghosh", "Esha", ""], ["Mahloujifar", "Saeed", ""]]}, {"id": "2101.11075", "submitter": "Aaron Defazio", "authors": "Aaron Defazio and Samy Jelassi", "title": "Adaptivity without Compromise: A Momentumized, Adaptive, Dual Averaged\n  Gradient Method for Stochastic Optimization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI math.OC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We introduce MADGRAD, a novel optimization method in the family of AdaGrad\nadaptive gradient methods. MADGRAD shows excellent performance on deep learning\noptimization problems from multiple fields, including classification and\nimage-to-image tasks in vision, and recurrent and bidirectionally-masked models\nin natural language processing. For each of these tasks, MADGRAD matches or\noutperforms both SGD and ADAM in test set performance, even on problems for\nwhich adaptive methods normally perform poorly.\n", "versions": [{"version": "v1", "created": "Tue, 26 Jan 2021 20:38:26 GMT"}, {"version": "v2", "created": "Thu, 1 Apr 2021 18:20:52 GMT"}], "update_date": "2021-04-05", "authors_parsed": [["Defazio", "Aaron", ""], ["Jelassi", "Samy", ""]]}, {"id": "2101.11081", "submitter": "Xinwei Zhao", "authors": "Xinwei Zhao and Matthew C. Stamm", "title": "The Effect of Class Definitions on the Transferability of Adversarial\n  Attacks Against Forensic CNNs", "comments": null, "journal-ref": "Published at Electronic Imaging, Media Watermarking, Security, and\n  Forensics 2020, pp. 119-1-119-7(7)", "doi": null, "report-no": null, "categories": "cs.CV cs.CR cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In recent years, convolutional neural networks (CNNs) have been widely used\nby researchers to perform forensic tasks such as image tampering detection. At\nthe same time, adversarial attacks have been developed that are capable of\nfooling CNN-based classifiers. Understanding the transferability of adversarial\nattacks, i.e. an attacks ability to attack a different CNN than the one it was\ntrained against, has important implications for designing CNNs that are\nresistant to attacks. While attacks on object recognition CNNs are believed to\nbe transferrable, recent work by Barni et al. has shown that attacks on\nforensic CNNs have difficulty transferring to other CNN architectures or CNNs\ntrained using different datasets. In this paper, we demonstrate that\nadversarial attacks on forensic CNNs are even less transferrable than\npreviously thought even between virtually identical CNN architectures! We show\nthat several common adversarial attacks against CNNs trained to identify image\nmanipulation fail to transfer to CNNs whose only difference is in the class\ndefinitions (i.e. the same CNN architectures trained using the same data). We\nnote that all formulations of class definitions contain the unaltered class.\nThis has important implications for the future design of forensic CNNs that are\nrobust to adversarial and anti-forensic attacks.\n", "versions": [{"version": "v1", "created": "Tue, 26 Jan 2021 20:59:37 GMT"}], "update_date": "2021-01-28", "authors_parsed": [["Zhao", "Xinwei", ""], ["Stamm", "Matthew C.", ""]]}, {"id": "2101.11095", "submitter": "Pablo Jos\\'e Del Moral Pastor", "authors": "Pablo del Moral, Slawomir Nowaczyk, Anita Sant'Anna, Sepideh Pashami", "title": "Pitfalls of Assessing Extracted Hierarchies for Multi-Class\n  Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Using hierarchies of classes is one of the standard methods to solve\nmulti-class classification problems. In the literature, selecting the right\nhierarchy is considered to play a key role in improving classification\nperformance. Although different methods have been proposed, there is still a\nlack of understanding of what makes one method to extract hierarchies perform\nbetter or worse. To this effect, we analyze and compare some of the most\npopular approaches to extracting hierarchies. We identify some common pitfalls\nthat may lead practitioners to make misleading conclusions about their methods.\nIn addition, to address some of these problems, we demonstrate that using\nrandom hierarchies is an appropriate benchmark to assess how the hierarchy's\nquality affects the classification performance. In particular, we show how the\nhierarchy's quality can become irrelevant depending on the experimental setup:\nwhen using powerful enough classifiers, the final performance is not affected\nby the quality of the hierarchy. We also show how comparing the effect of the\nhierarchies against non-hierarchical approaches might incorrectly indicate\ntheir superiority. Our results confirm that datasets with a high number of\nclasses generally present complex structures in how these classes relate to\neach other. In these datasets, the right hierarchy can dramatically improve\nclassification performance.\n", "versions": [{"version": "v1", "created": "Tue, 26 Jan 2021 21:50:57 GMT"}], "update_date": "2021-01-28", "authors_parsed": [["del Moral", "Pablo", ""], ["Nowaczyk", "Slawomir", ""], ["Sant'Anna", "Anita", ""], ["Pashami", "Sepideh", ""]]}, {"id": "2101.11108", "submitter": "Shuyu Dong", "authors": "Shuyu Dong, Bin Gao, Yu Guan, Fran\\c{c}ois Glineur", "title": "New Riemannian preconditioned algorithms for tensor completion via\n  polyadic decomposition", "comments": "24 Pages, 7 figures, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.LG cs.NA math.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose new Riemannian preconditioned algorithms for low-rank tensor\ncompletion via the polyadic decomposition of a tensor. These algorithms exploit\na non-Euclidean metric on the product space of the factor matrices of the\nlow-rank tensor in the polyadic decomposition form. This new metric is designed\nusing an approximation of the diagonal blocks of the Hessian of the tensor\ncompletion cost function, thus has a preconditioning effect on these\nalgorithms. We prove that the proposed Riemannian gradient descent algorithm\nglobally converges to a stationary point of the tensor completion problem, with\nconvergence rate estimates using the $\\L{}$ojasiewicz property. Numerical\nresults on synthetic and real-world data suggest that the proposed algorithms\nare more efficient in memory and time compared to state-of-the-art algorithms.\nMoreover, the proposed algorithms display a greater tolerance for overestimated\nrank parameters in terms of the tensor recovery performance, thus enable a\nflexible choice of the rank parameter.\n", "versions": [{"version": "v1", "created": "Tue, 26 Jan 2021 22:11:06 GMT"}], "update_date": "2021-01-28", "authors_parsed": [["Dong", "Shuyu", ""], ["Gao", "Bin", ""], ["Guan", "Yu", ""], ["Glineur", "Fran\u00e7ois", ""]]}, {"id": "2101.11118", "submitter": "Donghwan Shin", "authors": "Fitash Ul Haq, Donghwan Shin, Shiva Nejati, Lionel Briand", "title": "Can Offline Testing of Deep Neural Networks Replace Their Online\n  Testing?", "comments": "Journal extension of arXiv:1912.00805; To appear in Empirical\n  Software Engineering", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SE cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We distinguish two general modes of testing for Deep Neural Networks (DNNs):\nOffline testing where DNNs are tested as individual units based on test\ndatasets obtained without involving the DNNs under test, and online testing\nwhere DNNs are embedded into a specific application environment and tested in a\nclosed-loop mode in interaction with the application environment. Typically,\nDNNs are subjected to both types of testing during their development life cycle\nwhere offline testing is applied immediately after DNN training and online\ntesting follows after offline testing and once a DNN is deployed within a\nspecific application environment. In this paper, we study the relationship\nbetween offline and online testing. Our goal is to determine how offline\ntesting and online testing differ or complement one another and if offline\ntesting results can be used to help reduce the cost of online testing? Though\nthese questions are generally relevant to all autonomous systems, we study them\nin the context of automated driving systems where, as study subjects, we use\nDNNs automating end-to-end controls of steering functions of self-driving\nvehicles. Our results show that offline testing is less effective than online\ntesting as many safety violations identified by online testing could not be\nidentified by offline testing, while large prediction errors generated by\noffline testing always led to severe safety violations detectable by online\ntesting. Further, we cannot exploit offline testing results to reduce the cost\nof online testing in practice since we are not able to identify specific\nsituations where offline testing could be as accurate as online testing in\nidentifying safety requirement violations.\n", "versions": [{"version": "v1", "created": "Tue, 26 Jan 2021 22:28:54 GMT"}, {"version": "v2", "created": "Fri, 30 Apr 2021 12:37:38 GMT"}], "update_date": "2021-05-03", "authors_parsed": [["Haq", "Fitash Ul", ""], ["Shin", "Donghwan", ""], ["Nejati", "Shiva", ""], ["Briand", "Lionel", ""]]}, {"id": "2101.11132", "submitter": "Su Yeon Chang", "authors": "Su Yeon Chang, Sofia Vallecorsa, El\\'ias F. Combarro, and Federico\n  Carminati", "title": "Quantum Generative Adversarial Networks in a Continuous-Variable\n  Architecture to Simulate High Energy Physics Detectors", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "quant-ph cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Deep Neural Networks (DNNs) come into the limelight in High Energy Physics\n(HEP) in order to manipulate the increasing amount of data encountered in the\nnext generation of accelerators. Recently, the HEP community has suggested\nGenerative Adversarial Networks (GANs) to replace traditional time-consuming\nGeant4 simulations based on the Monte Carlo method. In parallel with advances\nin deep learning, intriguing studies have been conducted in the last decade on\nquantum computing, including the Quantum GAN model suggested by IBM. However,\nthis model is limited in learning a probability distribution over discrete\nvariables, while we initially aim to reproduce a distribution over continuous\nvariables in HEP. We introduce and analyze a new prototype of quantum GAN\n(qGAN) employed in continuous-variable (CV) quantum computing, which encodes\nquantum information in a continuous physical observable. Two CV qGAN models\nwith a quantum and a classical discriminator have been tested to reproduce\ncalorimeter outputs in a reduced size, and their advantages and limitations are\ndiscussed.\n", "versions": [{"version": "v1", "created": "Tue, 26 Jan 2021 23:33:14 GMT"}], "update_date": "2021-01-28", "authors_parsed": [["Chang", "Su Yeon", ""], ["Vallecorsa", "Sofia", ""], ["Combarro", "El\u00edas F.", ""], ["Carminati", "Federico", ""]]}, {"id": "2101.11134", "submitter": "Alexandry Augustin", "authors": "A. Augustin, A. Papangelis, M. Kotti, P. Vougiouklis, J. Hare, N.\n  Braunschweiler", "title": "Open-domain Topic Identification of Out-of-domain Utterances using\n  Wikipedia", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Users of spoken dialogue systems (SDS) expect high quality interactions\nacross a wide range of diverse topics. However, the implementation of SDS\ncapable of responding to every conceivable user utterance in an informative way\nis a challenging problem. Multi-domain SDS must necessarily identify and deal\nwith out-of-domain (OOD) utterances to generate appropriate responses as users\ndo not always know in advance what domains the SDS can handle. To address this\nproblem, we extend the current state-of-the-art in multi-domain SDS by\nestimating the topic of OOD utterances using external knowledge representation\nfrom Wikipedia. Experimental results on real human-to-human dialogues showed\nthat our approach does not degrade domain prediction performance when compared\nto the base model. But more significantly, our joint training achieves more\naccurate predictions of the nearest Wikipedia article by up to about 30% when\ncompared to the benchmarks.\n", "versions": [{"version": "v1", "created": "Tue, 26 Jan 2021 23:46:52 GMT"}], "update_date": "2021-01-28", "authors_parsed": [["Augustin", "A.", ""], ["Papangelis", "A.", ""], ["Kotti", "M.", ""], ["Vougiouklis", "P.", ""], ["Hare", "J.", ""], ["Braunschweiler", "N.", ""]]}, {"id": "2101.11144", "submitter": "Akira Imakura", "authors": "Akira Imakura, Anna Bogdanova, Takaya Yamazoe, Kazumasa Omote, Tetsuya\n  Sakurai", "title": "Accuracy and Privacy Evaluations of Collaborative Data Analysis", "comments": "16 pages; 2 figures; 1 table", "journal-ref": "To be presented at The Second AAAI Workshop on Privacy-Preserving\n  Artificial Intelligence (PPAI-21) (2021)", "doi": null, "report-no": null, "categories": "cs.LG cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Distributed data analysis without revealing the individual data has recently\nattracted significant attention in several applications. A collaborative data\nanalysis through sharing dimensionality reduced representations of data has\nbeen proposed as a non-model sharing-type federated learning. This paper\nanalyzes the accuracy and privacy evaluations of this novel framework. In the\naccuracy analysis, we provided sufficient conditions for the equivalence of the\ncollaborative data analysis and the centralized analysis with dimensionality\nreduction. In the privacy analysis, we proved that collaborative users' private\ndatasets are protected with a double privacy layer against insider and external\nattacking scenarios.\n", "versions": [{"version": "v1", "created": "Wed, 27 Jan 2021 00:38:47 GMT"}], "update_date": "2021-01-28", "authors_parsed": [["Imakura", "Akira", ""], ["Bogdanova", "Anna", ""], ["Yamazoe", "Takaya", ""], ["Omote", "Kazumasa", ""], ["Sakurai", "Tetsuya", ""]]}, {"id": "2101.11155", "submitter": "Shubhanshu Mishra", "authors": "Sudhanshu Mishra, Shivangi Prasad, Shubhanshu Mishra", "title": "Exploring multi-task multi-lingual learning of transformer models for\n  hate speech and offensive speech identification in social media", "comments": "\"To be published in SN Computer Science at\n  https://doi.org/10.1007/s42979-021-00455-5\" \"30 pages, 6 figures\" \"Code\n  available at https://github.com/socialmediaie/MTML_HateSpeech\"", "journal-ref": null, "doi": "10.1007/s42979-021-00455-5", "report-no": null, "categories": "cs.CL cs.AI cs.LG cs.SI", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Hate Speech has become a major content moderation issue for online social\nmedia platforms. Given the volume and velocity of online content production, it\nis impossible to manually moderate hate speech related content on any platform.\nIn this paper we utilize a multi-task and multi-lingual approach based on\nrecently proposed Transformer Neural Networks to solve three sub-tasks for hate\nspeech. These sub-tasks were part of the 2019 shared task on hate speech and\noffensive content (HASOC) identification in Indo-European languages. We expand\non our submission to that competition by utilizing multi-task models which are\ntrained using three approaches, a) multi-task learning with separate task\nheads, b) back-translation, and c) multi-lingual training. Finally, we\ninvestigate the performance of various models and identify instances where the\nTransformer based models perform differently and better. We show that it is\npossible to to utilize different combined approaches to obtain models that can\ngeneralize easily on different languages and tasks, while trading off slight\naccuracy (in some cases) for a much reduced inference time compute cost. We\nopen source an updated version of our HASOC 2019 code with the new improvements\nat https://github.com/socialmediaie/MTML_HateSpeech.\n", "versions": [{"version": "v1", "created": "Wed, 27 Jan 2021 01:25:22 GMT"}], "update_date": "2021-01-28", "authors_parsed": [["Mishra", "Sudhanshu", ""], ["Prasad", "Shivangi", ""], ["Mishra", "Shubhanshu", ""]]}, {"id": "2101.11156", "submitter": "Lan Truong", "authors": "Lan V. Truong", "title": "Fundamental limits and algorithms for sparse linear regression with\n  sublinear sparsity", "comments": "45 pages, 2 figures. Under review for publication. Add some auxiliary\n  proofs", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.LG math.IT stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We establish exact asymptotic expressions for the normalized mutual\ninformation and minimum mean-square-error (MMSE) of sparse linear regression in\nthe sub-linear sparsity regime. Our result is achieved by a generalization of\nthe adaptive interpolation method in Bayesian inference for linear regimes to\nsub-linear ones. A modification of the well-known approximate message passing\nalgorithm to approach the MMSE fundamental limit is also proposed, and its\nstate evolution is rigorously analyzed. Our results show that the traditional\nlinear assumption between the signal dimension and number of observations in\nthe replica and adaptive interpolation methods is not necessary for sparse\nsignals. They also show how to modify the existing well-known AMP algorithms\nfor linear regimes to sub-linear ones.\n", "versions": [{"version": "v1", "created": "Wed, 27 Jan 2021 01:27:03 GMT"}, {"version": "v2", "created": "Wed, 3 Feb 2021 03:53:21 GMT"}, {"version": "v3", "created": "Wed, 19 May 2021 20:03:55 GMT"}, {"version": "v4", "created": "Wed, 14 Jul 2021 15:22:05 GMT"}], "update_date": "2021-07-15", "authors_parsed": [["Truong", "Lan V.", ""]]}, {"id": "2101.11162", "submitter": "Samuel Otto", "authors": "Samuel E. Otto and Clarence W. Rowley", "title": "Inadequacy of Linear Methods for Minimal Sensor Placement and Feature\n  Selection in Nonlinear Systems; a New Approach Using Secants", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.LG cs.SY eess.SY stat.ML", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Sensor placement and feature selection are critical steps in engineering,\nmodeling, and data science that share a common mathematical theme: the selected\nmeasurements should enable solution of an inverse problem. Most real-world\nsystems of interest are nonlinear, yet the majority of available techniques for\nfeature selection and sensor placement rely on assumptions of linearity or\nsimple statistical models. We show that when these assumptions are violated,\nstandard techniques can lead to costly over-sensing without guaranteeing that\nthe desired information can be recovered from the measurements. In order to\nremedy these problems, we introduce a novel data-driven approach for sensor\nplacement and feature selection for a general type of nonlinear inverse problem\nbased on the information contained in secant vectors between data points. Using\nthe secant-based approach, we develop three efficient greedy algorithms that\neach provide different types of robust, near-minimal reconstruction guarantees.\nWe demonstrate them on two problems where linear techniques consistently fail:\nsensor placement to reconstruct a fluid flow formed by a complicated\nshock-mixing layer interaction and selecting fundamental manifold learning\ncoordinates on a torus.\n", "versions": [{"version": "v1", "created": "Wed, 27 Jan 2021 01:57:34 GMT"}], "update_date": "2021-01-28", "authors_parsed": [["Otto", "Samuel E.", ""], ["Rowley", "Clarence W.", ""]]}, {"id": "2101.11164", "submitter": "Adam Byerly", "authors": "Adam Byerly, Tatiana Kalganova, Anthony J. Grichnik", "title": "On the Importance of Capturing a Sufficient Diversity of Perspective for\n  the Classification of micro-PCBs", "comments": "12 pages, 6 figures, 8 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a dataset consisting of high-resolution images of 13 micro-PCBs\ncaptured in various rotations and perspectives relative to the camera, with\neach sample labeled for PCB type, rotation category, and perspective\ncategories. We then present the design and results of experimentation on\ncombinations of rotations and perspectives used during training and the\nresulting impact on test accuracy. We then show when and how well data\naugmentation techniques are capable of simulating rotations vs. perspectives\nnot present in the training data. We perform all experiments using CNNs with\nand without homogeneous vector capsules (HVCs) and investigate and show the\ncapsules' ability to better encode the equivariance of the sub-components of\nthe micro-PCBs. The results of our experiments lead us to conclude that\ntraining a neural network equipped with HVCs, capable of modeling equivariance\namong sub-components, coupled with training on a diversity of perspectives,\nachieves the greatest classification accuracy on micro-PCB data.\n", "versions": [{"version": "v1", "created": "Wed, 27 Jan 2021 02:06:09 GMT"}], "update_date": "2021-01-28", "authors_parsed": [["Byerly", "Adam", ""], ["Kalganova", "Tatiana", ""], ["Grichnik", "Anthony J.", ""]]}, {"id": "2101.11174", "submitter": "Weiwei Jiang", "authors": "Weiwei Jiang, Jiayun Luo", "title": "Graph Neural Network for Traffic Forecasting: A Survey", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Traffic forecasting is important for the success of intelligent\ntransportation systems. Deep learning models, including convolution neural\nnetworks and recurrent neural networks, have been extensively applied in\ntraffic forecasting problems to model spatial and temporal dependencies. In\nrecent years, to model the graph structures in transportation systems as well\nas contextual information, graph neural networks have been introduced and have\nachieved state-of-the-art performance in a series of traffic forecasting\nproblems. In this survey, we review the rapidly growing body of research using\ndifferent graph neural networks, e.g. graph convolutional and graph attention\nnetworks, in various traffic forecasting problems, e.g. road traffic flow and\nspeed forecasting, passenger flow forecasting in urban rail transit systems,\nand demand forecasting in ride-hailing platforms. We also present a\ncomprehensive list of open data and source resources for each problem and\nidentify future research directions. To the best of our knowledge, this paper\nis the first comprehensive survey that explores the application of graph neural\nnetworks for traffic forecasting problems. We have also created a public GitHub\nrepository where the latest papers, open data, and source resources will be\nupdated.\n", "versions": [{"version": "v1", "created": "Wed, 27 Jan 2021 02:35:41 GMT"}, {"version": "v2", "created": "Mon, 15 Feb 2021 14:19:27 GMT"}], "update_date": "2021-02-16", "authors_parsed": [["Jiang", "Weiwei", ""], ["Luo", "Jiayun", ""]]}, {"id": "2101.11186", "submitter": "Junjie Li", "authors": "Junjie Li, Junwei Zhang, Xiaoyu Gong, Shuai L\\\"u", "title": "Evolutionary Generative Adversarial Networks with Crossover Based\n  Knowledge Distillation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generative Adversarial Networks (GAN) is an adversarial model, and it has\nbeen demonstrated to be effective for various generative tasks. However, GAN\nand its variants also suffer from many training problems, such as mode collapse\nand gradient vanish. In this paper, we firstly propose a general crossover\noperator, which can be widely applied to GANs using evolutionary strategies.\nThen we design an evolutionary GAN framework C-GAN based on it. And we combine\nthe crossover operator with evolutionary generative adversarial networks (EGAN)\nto implement the evolutionary generative adversarial networks with crossover\n(CE-GAN). Under the premise that a variety of loss functions are used as\nmutation operators to generate mutation individuals, we evaluate the generated\nsamples and allow the mutation individuals to learn experiences from the output\nin a knowledge distillation manner, imitating the best output outcome,\nresulting in better offspring. Then, we greedily selected the best offspring as\nparents for subsequent training using discriminator as evaluator. Experiments\non real datasets demonstrate the effectiveness of CE-GAN and show that our\nmethod is competitive in terms of generated images quality and time efficiency.\n", "versions": [{"version": "v1", "created": "Wed, 27 Jan 2021 03:24:30 GMT"}, {"version": "v2", "created": "Mon, 19 Jul 2021 13:26:35 GMT"}], "update_date": "2021-07-20", "authors_parsed": [["Li", "Junjie", ""], ["Zhang", "Junwei", ""], ["Gong", "Xiaoyu", ""], ["L\u00fc", "Shuai", ""]]}, {"id": "2101.11188", "submitter": "David Cotton Mr", "authors": "David Cotton, Zenon Chaczko", "title": "GymD2D: A Device-to-Device Underlay Cellular Offload Evaluation Platform", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SY cs.LG cs.SY", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Cellular offloading in device-to-device communication is a challenging\noptimisation problem in which the improved allocation of radio resources can\nincrease spectral efficiency, energy efficiency, throughout and reduce latency.\nThe academic community have explored different optimisation methods on these\nproblems and initial results are encouraging. However, there exists significant\nfriction in the lack of a simple, configurable, open-source framework for\ncellular offload research. Prior research utilises a variety of network\nsimulators and system models, making it difficult to compare results. In this\npaper we present GymD2D, a framework for experimentation with physical layer\nresource allocation problems in device-to-device communication. GymD2D allows\nusers to simulate a variety of cellular offload scenarios and to extend its\nbehaviour to meet their research needs. GymD2D provides researchers an\nevaluation platform to compare, share and build upon previous research. We\nevaluated GymD2D with state-of-the-art deep reinforcement learning and\ndemonstrate these algorithms provide significant efficiency improvements.\n", "versions": [{"version": "v1", "created": "Wed, 27 Jan 2021 03:50:22 GMT"}], "update_date": "2021-01-28", "authors_parsed": [["Cotton", "David", ""], ["Chaczko", "Zenon", ""]]}, {"id": "2101.11196", "submitter": "Ingy ElSayed-Aly", "authors": "Ingy Elsayed-Aly, Suda Bharadwaj, Christopher Amato, R\\\"udiger Ehlers,\n  Ufuk Topcu, Lu Feng", "title": "Safe Multi-Agent Reinforcement Learning via Shielding", "comments": "8 pages, 11 figures and 2 tables, to be published in AAMAS 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.FL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multi-agent reinforcement learning (MARL) has been increasingly used in a\nwide range of safety-critical applications, which require guaranteed safety\n(e.g., no unsafe states are ever visited) during the learning\nprocess.Unfortunately, current MARL methods do not have safety guarantees.\nTherefore, we present two shielding approaches for safe MARL. In centralized\nshielding, we synthesize a single shield to monitor all agents' joint actions\nand correct any unsafe action if necessary. In factored shielding, we\nsynthesize multiple shields based on a factorization of the joint state space\nobserved by all agents; the set of shields monitors agents concurrently and\neach shield is only responsible for a subset of agents at each\nstep.Experimental results show that both approaches can guarantee the safety of\nagents during learning without compromising the quality of learned policies;\nmoreover, factored shielding is more scalable in the number of agents than\ncentralized shielding.\n", "versions": [{"version": "v1", "created": "Wed, 27 Jan 2021 04:27:06 GMT"}, {"version": "v2", "created": "Tue, 2 Feb 2021 18:30:53 GMT"}], "update_date": "2021-02-03", "authors_parsed": [["Elsayed-Aly", "Ingy", ""], ["Bharadwaj", "Suda", ""], ["Amato", "Christopher", ""], ["Ehlers", "R\u00fcdiger", ""], ["Topcu", "Ufuk", ""], ["Feng", "Lu", ""]]}, {"id": "2101.11201", "submitter": "Cuong Nguyen", "authors": "Cuong Nguyen, Thanh-Toan Do, Gustavo Carneiro", "title": "Similarity of Classification Tasks", "comments": "Accepted at Neurips Meta-learning Workshop 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Recent advances in meta-learning has led to remarkable performances on\nseveral few-shot learning benchmarks. However, such success often ignores the\nsimilarity between training and testing tasks, resulting in a potential bias\nevaluation. We, therefore, propose a generative approach based on a variant of\nLatent Dirichlet Allocation to analyse task similarity to optimise and better\nunderstand the performance of meta-learning. We demonstrate that the proposed\nmethod can provide an insightful evaluation for meta-learning algorithms on two\nfew-shot classification benchmarks that matches common intuition: the more\nsimilar the higher performance. Based on this similarity measure, we propose a\ntask-selection strategy for meta-learning and show that it can produce more\naccurate classification results than methods that randomly select training\ntasks.\n", "versions": [{"version": "v1", "created": "Wed, 27 Jan 2021 04:37:34 GMT"}], "update_date": "2021-01-28", "authors_parsed": [["Nguyen", "Cuong", ""], ["Do", "Thanh-Toan", ""], ["Carneiro", "Gustavo", ""]]}, {"id": "2101.11203", "submitter": "Haibo Yang Mr", "authors": "Haibo Yang, Minghong Fang, Jia Liu", "title": "Achieving Linear Speedup with Partial Worker Participation in Non-IID\n  Federated Learning", "comments": "Published as a conference paper at ICLR 2021, fixed errors in theorem\n  2", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Federated learning (FL) is a distributed machine learning architecture that\nleverages a large number of workers to jointly learn a model with decentralized\ndata. FL has received increasing attention in recent years thanks to its data\nprivacy protection, communication efficiency and a linear speedup for\nconvergence in training (i.e., convergence performance increases linearly with\nrespect to the number of workers). However, existing studies on linear speedup\nfor convergence are only limited to the assumptions of i.i.d. datasets across\nworkers and/or full worker participation, both of which rarely hold in\npractice. So far, it remains an open question whether or not the linear speedup\nfor convergence is achievable under non-i.i.d. datasets with partial worker\nparticipation in FL. In this paper, we show that the answer is affirmative.\nSpecifically, we show that the federated averaging (FedAvg) algorithm (with\ntwo-sided learning rates) on non-i.i.d. datasets in non-convex settings\nachieves a convergence rate $\\mathcal{O}(\\frac{1}{\\sqrt{mKT}} + \\frac{1}{T})$\nfor full worker participation and a convergence rate\n$\\mathcal{O}(\\frac{\\sqrt{K}}{\\sqrt{nT}} + \\frac{1}{T})$ for partial worker\nparticipation, where $K$ is the number of local steps, $T$ is the number of\ntotal communication rounds, $m$ is the total worker number and $n$ is the\nworker number in one communication round if for partial worker participation.\nOur results also reveal that the local steps in FL could help the convergence\nand show that the maximum number of local steps can be improved to $T/m$ in\nfull worker participation. We conduct extensive experiments on MNIST and\nCIFAR-10 to verify our theoretical results.\n", "versions": [{"version": "v1", "created": "Wed, 27 Jan 2021 04:38:27 GMT"}, {"version": "v2", "created": "Thu, 25 Feb 2021 23:15:23 GMT"}, {"version": "v3", "created": "Tue, 4 May 2021 03:30:18 GMT"}], "update_date": "2021-05-05", "authors_parsed": [["Yang", "Haibo", ""], ["Fang", "Minghong", ""], ["Liu", "Jia", ""]]}, {"id": "2101.11206", "submitter": "Yuxiang Ren", "authors": "Yuxiang Ren, Bo Wang, Jiawei Zhang and Yi Chang", "title": "Adversarial Active Learning based Heterogeneous Graph Neural Network for\n  Fake News Detection", "comments": "Accepted by ICDM 2020. arXiv admin note: text overlap with\n  arXiv:2002.04397", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.CY cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The explosive growth of fake news along with destructive effects on politics,\neconomy, and public safety has increased the demand for fake news detection.\nFake news on social media does not exist independently in the form of an\narticle. Many other entities, such as news creators, news subjects, and so on,\nexist on social media and have relationships with news articles. Different\nentities and relationships can be modeled as a heterogeneous information\nnetwork (HIN). In this paper, we attempt to solve the fake news detection\nproblem with the support of a news-oriented HIN. We propose a novel fake news\ndetection framework, namely Adversarial Active Learning-based Heterogeneous\nGraph Neural Network (AA-HGNN) which employs a novel hierarchical attention\nmechanism to perform node representation learning in the HIN. AA-HGNN utilizes\nan active learning framework to enhance learning performance, especially when\nfacing the paucity of labeled data. An adversarial selector will be trained to\nquery high-value candidates for the active learning framework. When the\nadversarial active learning is completed, AA-HGNN detects fake news by\nclassifying news article nodes. Experiments with two real-world fake news\ndatasets show that our model can outperform text-based models and other\ngraph-based models when using less labeled data benefiting from the adversarial\nactive learning. As a model with generalizability, AA-HGNN also has the ability\nto be widely used in other node classification-related applications on\nheterogeneous graphs.\n", "versions": [{"version": "v1", "created": "Wed, 27 Jan 2021 05:05:25 GMT"}], "update_date": "2021-01-28", "authors_parsed": [["Ren", "Yuxiang", ""], ["Wang", "Bo", ""], ["Zhang", "Jiawei", ""], ["Chang", "Yi", ""]]}, {"id": "2101.11214", "submitter": "Siddhant Garg", "authors": "Siddhant Garg, Goutham Ramakrishnan, Varun Thumbe", "title": "Towards Robustness to Label Noise in Text Classification via Noise\n  Modeling", "comments": "Accepted at ICLR 2021 RobustML and S2D-OLAD Workshops", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Large datasets in NLP suffer from noisy labels, due to erroneous automatic\nand human annotation procedures. We study the problem of text classification\nwith label noise, and aim to capture this noise through an auxiliary noise\nmodel over the classifier. We first assign a probability score to each training\nsample of having a noisy label, through a beta mixture model fitted on the\nlosses at an early epoch of training. Then, we use this score to selectively\nguide the learning of the noise model and classifier. Our empirical evaluation\non two text classification tasks shows that our approach can improve over the\nbaseline accuracy, and prevent over-fitting to the noise.\n", "versions": [{"version": "v1", "created": "Wed, 27 Jan 2021 05:41:57 GMT"}, {"version": "v2", "created": "Thu, 22 Apr 2021 02:48:02 GMT"}], "update_date": "2021-04-23", "authors_parsed": [["Garg", "Siddhant", ""], ["Ramakrishnan", "Goutham", ""], ["Thumbe", "Varun", ""]]}, {"id": "2101.11224", "submitter": "Jianzhe Lin", "authors": "Jianzhe Lin, Ghazal Sahebzamani, Christina Luong, Fatemeh Taheri\n  Dezaki, Mohammad Jafari, Purang Abolmaesumi, Teresa Tsang", "title": "Reciprocal Landmark Detection and Tracking with Extremely Few\n  Annotations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Localization of anatomical landmarks to perform two-dimensional measurements\nin echocardiography is part of routine clinical workflow in cardiac disease\ndiagnosis. Automatic localization of those landmarks is highly desirable to\nimprove workflow and reduce interobserver variability. Training a machine\nlearning framework to perform such localization is hindered given the sparse\nnature of gold standard labels; only few percent of cardiac cine series frames\nare normally manually labeled for clinical use. In this paper, we propose a new\nend-to-end reciprocal detection and tracking model that is specifically\ndesigned to handle the sparse nature of echocardiography labels. The model is\ntrained using few annotated frames across the entire cardiac cine sequence to\ngenerate consistent detection and tracking of landmarks, and an adversarial\ntraining for the model is proposed to take advantage of these annotated frames.\nThe superiority of the proposed reciprocal model is demonstrated using a series\nof experiments.\n", "versions": [{"version": "v1", "created": "Wed, 27 Jan 2021 06:59:41 GMT"}], "update_date": "2021-01-28", "authors_parsed": [["Lin", "Jianzhe", ""], ["Sahebzamani", "Ghazal", ""], ["Luong", "Christina", ""], ["Dezaki", "Fatemeh Taheri", ""], ["Jafari", "Mohammad", ""], ["Abolmaesumi", "Purang", ""], ["Tsang", "Teresa", ""]]}, {"id": "2101.11256", "submitter": "Mamikon Gulian", "authors": "Kookjin Lee, Nathaniel A. Trask, Ravi G. Patel, Mamikon A. Gulian,\n  Eric C. Cyr", "title": "Partition of unity networks: deep hp-approximation", "comments": "8 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NA math.NA stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Approximation theorists have established best-in-class optimal approximation\nrates of deep neural networks by utilizing their ability to simultaneously\nemulate partitions of unity and monomials. Motivated by this, we propose\npartition of unity networks (POUnets) which incorporate these elements directly\ninto the architecture. Classification architectures of the type used to learn\nprobability measures are used to build a meshfree partition of space, while\npolynomial spaces with learnable coefficients are associated to each partition.\nThe resulting hp-element-like approximation allows use of a fast least-squares\noptimizer, and the resulting architecture size need not scale exponentially\nwith spatial dimension, breaking the curse of dimensionality. An abstract\napproximation result establishes desirable properties to guide network design.\nNumerical results for two choices of architecture demonstrate that POUnets\nyield hp-convergence for smooth functions and consistently outperform MLPs for\npiecewise polynomial functions with large numbers of discontinuities.\n", "versions": [{"version": "v1", "created": "Wed, 27 Jan 2021 08:26:11 GMT"}], "update_date": "2021-01-28", "authors_parsed": [["Lee", "Kookjin", ""], ["Trask", "Nathaniel A.", ""], ["Patel", "Ravi G.", ""], ["Gulian", "Mamikon A.", ""], ["Cyr", "Eric C.", ""]]}, {"id": "2101.11286", "submitter": "Zhou Lu", "authors": "Zhou Lu", "title": "A Note on the Representation Power of GHHs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.IT math.IT stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this note we prove a sharp lower bound on the necessary number of nestings\nof nested absolute-value functions of generalized hinging hyperplanes (GHH) to\nrepresent arbitrary CPWL functions. Previous upper bound states that $n+1$\nnestings is sufficient for GHH to achieve universal representation power, but\nthe corresponding lower bound was unknown. We prove that $n$ nestings is\nnecessary for universal representation power, which provides an almost tight\nlower bound. We also show that one-hidden-layer neural networks don't have\nuniversal approximation power over the whole domain. The analysis is based on a\nkey lemma showing that any finite sum of periodic functions is either\nnon-integrable or the zero function, which might be of independent interest.\n", "versions": [{"version": "v1", "created": "Wed, 27 Jan 2021 09:46:37 GMT"}], "update_date": "2021-01-28", "authors_parsed": [["Lu", "Zhou", ""]]}, {"id": "2101.11287", "submitter": "Lucas Weber", "authors": "Lucas Weber, Jaap Jumelet, Elia Bruni and Dieuwke Hupkes", "title": "Language Modelling as a Multi-Task Problem", "comments": "Accepted for publication at EACL 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose to study language modelling as a multi-task\nproblem, bringing together three strands of research: multi-task learning,\nlinguistics, and interpretability. Based on hypotheses derived from linguistic\ntheory, we investigate whether language models adhere to learning principles of\nmulti-task learning during training. To showcase the idea, we analyse the\ngeneralisation behaviour of language models as they learn the linguistic\nconcept of Negative Polarity Items (NPIs). Our experiments demonstrate that a\nmulti-task setting naturally emerges within the objective of the more general\ntask of language modelling.We argue that this insight is valuable for\nmulti-task learning, linguistics and interpretability research and can lead to\nexciting new findings in all three domains.\n", "versions": [{"version": "v1", "created": "Wed, 27 Jan 2021 09:47:42 GMT"}], "update_date": "2021-01-28", "authors_parsed": [["Weber", "Lucas", ""], ["Jumelet", "Jaap", ""], ["Bruni", "Elia", ""], ["Hupkes", "Dieuwke", ""]]}, {"id": "2101.11296", "submitter": "Wei Zhou", "authors": "Yiying Li, Wei Zhou, Huaimin Wang, Haibo Mi, Timothy M. Hospedales", "title": "FedH2L: Federated Learning with Model and Statistical Heterogeneity", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Federated learning (FL) enables distributed participants to collectively\nlearn a strong global model without sacrificing their individual data privacy.\nMainstream FL approaches require each participant to share a common network\narchitecture and further assume that data are are sampled IID across\nparticipants. However, in real-world deployments participants may require\nheterogeneous network architectures; and the data distribution is almost\ncertainly non-uniform across participants. To address these issues we introduce\nFedH2L, which is agnostic to both the model architecture and robust to\ndifferent data distributions across participants. In contrast to approaches\nsharing parameters or gradients, FedH2L relies on mutual distillation,\nexchanging only posteriors on a shared seed set between participants in a\ndecentralized manner. This makes it extremely bandwidth efficient, model\nagnostic, and crucially produces models capable of performing well on the whole\ndata distribution when learning from heterogeneous silos.\n", "versions": [{"version": "v1", "created": "Wed, 27 Jan 2021 10:10:18 GMT"}, {"version": "v2", "created": "Mon, 19 Jul 2021 07:17:05 GMT"}, {"version": "v3", "created": "Tue, 27 Jul 2021 10:43:44 GMT"}], "update_date": "2021-07-28", "authors_parsed": [["Li", "Yiying", ""], ["Zhou", "Wei", ""], ["Wang", "Huaimin", ""], ["Mi", "Haibo", ""], ["Hospedales", "Timothy M.", ""]]}, {"id": "2101.11306", "submitter": "Shuohui Li", "authors": "Shuo-Hui Li", "title": "Learning Non-linear Wavelet Transformation via Normalizing Flow", "comments": "Main text: 7 pages, 5 figures. Supplement: 5 pages. Github link:\n  https://github.com/li012589/NeuralWavelet", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Wavelet transformation stands as a cornerstone in modern data analysis and\nsignal processing. Its mathematical essence is an invertible transformation\nthat discerns slow patterns from fast patterns in the frequency domain, which\nrepeats at each level. Such an invertible transformation can be learned by a\ndesigned normalizing flow model. With a factor-out scheme resembling the\nwavelet downsampling mechanism, a mutually independent prior, and parameter\nsharing along the depth of the network, one can train normalizing flow models\nto factor-out variables corresponding to fast patterns at different levels,\nthus extending linear wavelet transformations to non-linear learnable models.\nIn this paper, a concrete way of building such flows is given. Then, a\ndemonstration of the model's ability in lossless compression task, progressive\nloading, and super-resolution (upsampling) task. Lastly, an analysis of the\nlearned model in terms of low-pass/high-pass filters is given.\n", "versions": [{"version": "v1", "created": "Wed, 27 Jan 2021 10:28:51 GMT"}], "update_date": "2021-01-28", "authors_parsed": [["Li", "Shuo-Hui", ""]]}, {"id": "2101.11325", "submitter": "Florian Heyder", "authors": "Florian Heyder and J\\\"org Schumacher", "title": "Echo State Network for two-dimensional turbulent moist Rayleigh-B\\'enard\n  convection", "comments": null, "journal-ref": "Phys. Rev. E 103, 053107 (2021)", "doi": "10.1103/PhysRevE.103.053107", "report-no": null, "categories": "physics.flu-dyn cs.LG nlin.CD", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Recurrent neural networks are machine learning algorithms which are suited\nwell to predict time series. Echo state networks are one specific\nimplementation of such neural networks that can describe the evolution of\ndynamical systems by supervised machine learning without solving the underlying\nnonlinear mathematical equations. In this work, we apply an echo state network\nto approximate the evolution of two-dimensional moist Rayleigh-B\\'enard\nconvection and the resulting low-order turbulence statistics. We conduct\nlong-term direct numerical simulations in order to obtain training and test\ndata for the algorithm. Both sets are pre-processed by a Proper Orthogonal\nDecomposition (POD) using the snapshot method to reduce the amount of data. The\ntraining data comprise long time series of the first 150 most energetic POD\ncoefficients. The reservoir is subsequently fed by the data and results in\npredictions of future flow states. The predictions are thoroughly validated by\nthe data of the original simulation. Our results show good agreement of the\nlow-order statistics. This incorporates also derived statistical moments such\nas the cloud cover close to the top of the convection layer and the flux of\nliquid water across the domain. We conclude that our model is capable of\nlearning complex dynamics which is introduced here by the tight interaction of\nturbulence with the nonlinear thermodynamics of phase changes between vapor and\nliquid water. Our work opens new ways for the dynamic parametrization of\nsubgrid-scale transport in larger-scale circulation models.\n", "versions": [{"version": "v1", "created": "Wed, 27 Jan 2021 11:27:16 GMT"}, {"version": "v2", "created": "Mon, 3 May 2021 06:20:14 GMT"}], "update_date": "2021-05-26", "authors_parsed": [["Heyder", "Florian", ""], ["Schumacher", "J\u00f6rg", ""]]}, {"id": "2101.11331", "submitter": "Philip Ball", "authors": "Philip J. Ball and Stephen J. Roberts", "title": "OffCon$^3$: What is state of the art anyway?", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.OC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Two popular approaches to model-free continuous control tasks are SAC and\nTD3. At first glance these approaches seem rather different; SAC aims to solve\nthe entropy-augmented MDP by minimising the KL-divergence between a stochastic\nproposal policy and a hypotheical energy-basd soft Q-function policy, whereas\nTD3 is derived from DPG, which uses a deterministic policy to perform policy\ngradient ascent along the value function. In reality, both approaches are\nremarkably similar, and belong to a family of approaches we call `Off-Policy\nContinuous Generalized Policy Iteration'. This illuminates their similar\nperformance in most continuous control benchmarks, and indeed when\nhyperparameters are matched, their performance can be statistically\nindistinguishable. To further remove any difference due to implementation, we\nprovide OffCon$^3$ (Off-Policy Continuous Control: Consolidated), a code base\nfeaturing state-of-the-art versions of both algorithms.\n", "versions": [{"version": "v1", "created": "Wed, 27 Jan 2021 11:45:08 GMT"}, {"version": "v2", "created": "Sun, 14 Mar 2021 23:29:24 GMT"}], "update_date": "2021-03-16", "authors_parsed": [["Ball", "Philip J.", ""], ["Roberts", "Stephen J.", ""]]}, {"id": "2101.11335", "submitter": "Xinyi Ding", "authors": "Xinyi Ding and Eric C. Larson", "title": "On the Interpretability of Deep Learning Based Models for Knowledge\n  Tracing", "comments": null, "journal-ref": "AAAI 2021 workshop", "doi": null, "report-no": null, "categories": "cs.LG cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Knowledge tracing allows Intelligent Tutoring Systems to infer which topics\nor skills a student has mastered, thus adjusting curriculum accordingly. Deep\nLearning based models like Deep Knowledge Tracing (DKT) and Dynamic Key-Value\nMemory Network (DKVMN) have achieved significant improvements compared with\nmodels like Bayesian Knowledge Tracing (BKT) and Performance Factors Analysis\n(PFA). However, these deep learning based models are not as interpretable as\nother models because the decision-making process learned by deep neural\nnetworks is not wholly understood by the research community. In previous work,\nwe critically examined the DKT model, visualizing and analyzing the behaviors\nof DKT in high dimensional space. In this work, we extend our original analyses\nwith a much larger dataset and add discussions about the memory states of the\nDKVMN model. We discover that Deep Knowledge Tracing has some critical\npitfalls: 1) instead of tracking each skill through time, DKT is more likely to\nlearn an `ability' model; 2) the recurrent nature of DKT reinforces irrelevant\ninformation that it uses during the tracking task; 3) an untrained recurrent\nnetwork can achieve similar results to a trained DKT model, supporting a\nconclusion that recurrence relations are not properly learned and, instead,\nimprovements are simply a benefit of projection into a high dimensional, sparse\nvector space. Based on these observations, we propose improvements and future\ndirections for conducting knowledge tracing research using deep neural network\nmodels.\n", "versions": [{"version": "v1", "created": "Wed, 27 Jan 2021 11:55:03 GMT"}], "update_date": "2021-01-28", "authors_parsed": [["Ding", "Xinyi", ""], ["Larson", "Eric C.", ""]]}, {"id": "2101.11347", "submitter": "Jinxiong Zhang", "authors": "Jinxiong Zhang", "title": "Decision Machines: Interpreting Decision Tree as a Model Combination\n  Method", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.OC stat.ML", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Based on decision trees, it is efficient to handle tabular data. Conventional\ndecision tree growth methods often result in suboptimal trees because of their\ngreedy nature. Their inherent structure limits the options of hardware to\nimplement decision trees in parallel. Here is a compact representation of\nbinary decision trees to overcome these deficiencies. We explicitly formulate\nthe dependence of prediction on binary tests for binary decision trees and\nconstruct a function to guide the input sample from the root to the appropriate\nleaf node. And based on this formulation we introduce a new interpretation of\nbinary decision trees. Then we approximate this formulation via continuous\nfunctions. Finally, we interpret the decision tree as a model combination\nmethod. And we propose the selection-prediction scheme to unify a few learning\nmethods.\n", "versions": [{"version": "v1", "created": "Wed, 27 Jan 2021 12:23:24 GMT"}], "update_date": "2021-01-28", "authors_parsed": [["Zhang", "Jinxiong", ""]]}, {"id": "2101.11353", "submitter": "Yufei Cui", "authors": "Yufei Cui, Ziquan Liu, Qiao Li, Yu Mao, Antoni B. Chan, Chun Jason Xue", "title": "Bayesian Nested Neural Networks for Uncertainty Calibration and Adaptive\n  Compression", "comments": "16 pages, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Nested networks or slimmable networks are neural networks whose architectures\ncan be adjusted instantly during testing time, e.g., based on computational\nconstraints. Recent studies have focused on a \"nested dropout\" layer, which is\nable to order the nodes of a layer by importance during training, thus\ngenerating a nested set of sub-networks that are optimal for different\nconfigurations of resources. However, the dropout rate is fixed as a\nhyper-parameter over different layers during the whole training process.\nTherefore, when nodes are removed, the performance decays in a human-specified\ntrajectory rather than in a trajectory learned from data. Another drawback is\nthe generated sub-networks are deterministic networks without well-calibrated\nuncertainty. To address these two problems, we develop a Bayesian approach to\nnested neural networks. We propose a variational ordering unit that draws\nsamples for nested dropout at a low cost, from a proposed Downhill\ndistribution, which provides useful gradients to the parameters of nested\ndropout. Based on this approach, we design a Bayesian nested neural network\nthat learns the order knowledge of the node distributions. In experiments, we\nshow that the proposed approach outperforms the nested network in terms of\naccuracy, calibration, and out-of-domain detection in classification tasks. It\nalso outperforms the related approach on uncertainty-critical tasks in computer\nvision.\n", "versions": [{"version": "v1", "created": "Wed, 27 Jan 2021 12:34:58 GMT"}], "update_date": "2021-01-28", "authors_parsed": [["Cui", "Yufei", ""], ["Liu", "Ziquan", ""], ["Li", "Qiao", ""], ["Mao", "Yu", ""], ["Chan", "Antoni B.", ""], ["Xue", "Chun Jason", ""]]}, {"id": "2101.11354", "submitter": "Yongchun Zhu", "authors": "Yongchun Zhu, Fuzhen Zhuang, Xiangliang Zhang, Zhiyuan Qi, Zhiping Shi\n  and Qing He", "title": "Combat Data Shift in Few-shot Learning with Knowledge Graph", "comments": "10 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many few-shot learning approaches have been designed under the meta-learning\nframework, which learns from a variety of learning tasks and generalizes to new\ntasks. These meta-learning approaches achieve the expected performance in the\nscenario where all samples are drawn from the same distributions (i.i.d.\nobservations). However, in real-world applications, few-shot learning paradigm\noften suffers from data shift, i.e., samples in different tasks, even in the\nsame task, could be drawn from various data distributions. Most existing\nfew-shot learning approaches are not designed with the consideration of data\nshift, and thus show downgraded performance when data distribution shifts.\nHowever, it is non-trivial to address the data shift problem in few-shot\nlearning, due to the limited number of labeled samples in each task. Targeting\nat addressing this problem, we propose a novel metric-based meta-learning\nframework to extract task-specific representations and task-shared\nrepresentations with the help of knowledge graph. The data shift within/between\ntasks can thus be combated by the combination of task-shared and task-specific\nrepresentations. The proposed model is evaluated on popular benchmarks and two\nconstructed new challenging datasets. The evaluation results demonstrate its\nremarkable performance.\n", "versions": [{"version": "v1", "created": "Wed, 27 Jan 2021 12:35:18 GMT"}, {"version": "v2", "created": "Fri, 26 Feb 2021 06:22:21 GMT"}], "update_date": "2021-03-01", "authors_parsed": [["Zhu", "Yongchun", ""], ["Zhuang", "Fuzhen", ""], ["Zhang", "Xiangliang", ""], ["Qi", "Zhiyuan", ""], ["Shi", "Zhiping", ""], ["He", "Qing", ""]]}, {"id": "2101.11358", "submitter": "Elena Beretta", "authors": "Elena Beretta, Antonio Vetr\\`o, Bruno Lepri, Juan Carlos De Martin", "title": "Detecting discriminatory risk through data annotation based on Bayesian\n  inferences", "comments": "11 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Thanks to the increasing growth of computational power and data availability,\nthe research in machine learning has advanced with tremendous rapidity.\nNowadays, the majority of automatic decision making systems are based on data.\nHowever, it is well known that machine learning systems can present problematic\nresults if they are built on partial or incomplete data. In fact, in recent\nyears several studies have found a convergence of issues related to the ethics\nand transparency of these systems in the process of data collection and how\nthey are recorded. Although the process of rigorous data collection and\nanalysis is fundamental in the model design, this step is still largely\noverlooked by the machine learning community. For this reason, we propose a\nmethod of data annotation based on Bayesian statistical inference that aims to\nwarn about the risk of discriminatory results of a given data set. In\nparticular, our method aims to deepen knowledge and promote awareness about the\nsampling practices employed to create the training set, highlighting that the\nprobability of success or failure conditioned to a minority membership is given\nby the structure of the data available. We empirically test our system on three\ndatasets commonly accessed by the machine learning community and we investigate\nthe risk of racial discrimination.\n", "versions": [{"version": "v1", "created": "Wed, 27 Jan 2021 12:43:42 GMT"}], "update_date": "2021-01-28", "authors_parsed": [["Beretta", "Elena", ""], ["Vetr\u00f2", "Antonio", ""], ["Lepri", "Bruno", ""], ["De Martin", "Juan Carlos", ""]]}, {"id": "2101.11359", "submitter": "Shishir Rao", "authors": "Shishir Rao, Yikuan Li, Rema Ramakrishnan, Abdelaali Hassaine, Dexter\n  Canoy, John Cleland, Thomas Lukasiewicz, Gholamreza Salimi-Khorshidi, Kazem\n  Rahimi", "title": "An explainable Transformer-based deep learning model for the prediction\n  of incident heart failure", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Predicting the incidence of complex chronic conditions such as heart failure\nis challenging. Deep learning models applied to rich electronic health records\nmay improve prediction but remain unexplainable hampering their wider use in\nmedical practice. We developed a novel Transformer deep-learning model for more\naccurate and yet explainable prediction of incident heart failure involving\n100,071 patients from longitudinal linked electronic health records across the\nUK. On internal 5-fold cross validation and held-out external validation, our\nmodel achieved 0.93 and 0.93 area under the receiver operator curve and 0.69\nand 0.70 area under the precision-recall curve, respectively and outperformed\nexisting deep learning models. Predictor groups included all community and\nhospital diagnoses and medications contextualised within the age and calendar\nyear for each patient's clinical encounter. The importance of contextualised\nmedical information was revealed in a number of sensitivity analyses, and our\nperturbation method provided a way of identifying factors contributing to risk.\nMany of the identified risk factors were consistent with existing knowledge\nfrom clinical and epidemiological research but several new associations were\nrevealed which had not been considered in expert-driven risk prediction models.\n", "versions": [{"version": "v1", "created": "Wed, 27 Jan 2021 12:45:15 GMT"}], "update_date": "2021-01-28", "authors_parsed": [["Rao", "Shishir", ""], ["Li", "Yikuan", ""], ["Ramakrishnan", "Rema", ""], ["Hassaine", "Abdelaali", ""], ["Canoy", "Dexter", ""], ["Cleland", "John", ""], ["Lukasiewicz", "Thomas", ""], ["Salimi-Khorshidi", "Gholamreza", ""], ["Rahimi", "Kazem", ""]]}, {"id": "2101.11361", "submitter": "Francesco Picetti", "authors": "Francesco Picetti, Vincenzo Lipari, Paolo Bestagini, Stefano Tubaro", "title": "Anti-Aliasing Add-On for Deep Prior Seismic Data Interpolation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SP cs.LG physics.geo-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data interpolation is a fundamental step in any seismic processing workflow.\nAmong machine learning techniques recently proposed to solve data interpolation\nas an inverse problem, Deep Prior paradigm aims at employing a convolutional\nneural network to capture priors on the data in order to regularize the\ninversion. However, this technique lacks of reconstruction precision when\ninterpolating highly decimated data due to the presence of aliasing. In this\nwork, we propose to improve Deep Prior inversion by adding a directional\nLaplacian as regularization term to the problem. This regularizer drives the\noptimization towards solutions that honor the slopes estimated from the\ninterpolated data low frequencies. We provide some numerical examples to\nshowcase the methodology devised in this manuscript, showing that our results\nare less prone to aliasing also in presence of noisy and corrupted data.\n", "versions": [{"version": "v1", "created": "Wed, 27 Jan 2021 12:46:58 GMT"}], "update_date": "2021-01-28", "authors_parsed": [["Picetti", "Francesco", ""], ["Lipari", "Vincenzo", ""], ["Bestagini", "Paolo", ""], ["Tubaro", "Stefano", ""]]}, {"id": "2101.11363", "submitter": "Hyunjae Lee", "authors": "Hyunjae Lee, Jaewoong Yoon, Bonggyu Hwang, Seongho Joe, Seungjai Min,\n  Youngjune Gwon", "title": "KoreALBERT: Pretraining a Lite BERT Model for Korean Language\n  Understanding", "comments": "7 pages, 1 figure, to be published in 25th International Conference\n  on Pattern Recognition, ICPR 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  A Lite BERT (ALBERT) has been introduced to scale up deep bidirectional\nrepresentation learning for natural languages. Due to the lack of pretrained\nALBERT models for Korean language, the best available practice is the\nmultilingual model or resorting back to the any other BERT-based model. In this\npaper, we develop and pretrain KoreALBERT, a monolingual ALBERT model\nspecifically for Korean language understanding. We introduce a new training\nobjective, namely Word Order Prediction (WOP), and use alongside the existing\nMLM and SOP criteria to the same architecture and model parameters. Despite\nhaving significantly fewer model parameters (thus, quicker to train), our\npretrained KoreALBERT outperforms its BERT counterpart on 6 different NLU\ntasks. Consistent with the empirical results in English by Lan et al.,\nKoreALBERT seems to improve downstream task performance involving\nmulti-sentence encoding for Korean language. The pretrained KoreALBERT is\npublicly available to encourage research and application development for Korean\nNLP.\n", "versions": [{"version": "v1", "created": "Wed, 27 Jan 2021 12:48:53 GMT"}], "update_date": "2021-01-28", "authors_parsed": [["Lee", "Hyunjae", ""], ["Yoon", "Jaewoong", ""], ["Hwang", "Bonggyu", ""], ["Joe", "Seongho", ""], ["Min", "Seungjai", ""], ["Gwon", "Youngjune", ""]]}, {"id": "2101.11376", "submitter": "Charles Wilmot", "authors": "Charles Wilmot, Jochen Triesch", "title": "Learning Abstract Representations through Lossy Compression of\n  Multi-Modal Signals", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.MM", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  A key competence for open-ended learning is the formation of increasingly\nabstract representations useful for driving complex behavior. Abstract\nrepresentations ignore specific details and facilitate generalization. Here we\nconsider the learning of abstract representations in a multi-modal setting with\ntwo or more input modalities. We treat the problem as a lossy compression\nproblem and show that generic lossy compression of multimodal sensory input\nnaturally extracts abstract representations that tend to strip away modalitiy\nspecific details and preferentially retain information that is shared across\nthe different modalities. Furthermore, we propose an architecture to learn\nabstract representations by identifying and retaining only the information that\nis shared across multiple modalities while discarding any modality specific\ninformation.\n", "versions": [{"version": "v1", "created": "Wed, 27 Jan 2021 13:19:00 GMT"}, {"version": "v2", "created": "Fri, 11 Jun 2021 08:12:11 GMT"}], "update_date": "2021-06-14", "authors_parsed": [["Wilmot", "Charles", ""], ["Triesch", "Jochen", ""]]}, {"id": "2101.11410", "submitter": "Yuka Hashimoto", "authors": "Yuka Hashimoto, Isao Ishikawa, Masahiro Ikeda, Fuyuta Komura, Takeshi\n  Katsura, and Yoshinobu Kawahara", "title": "Reproducing kernel Hilbert C*-module and kernel mean embeddings", "comments": "merged two unpablished papers arXiv:2003.00738 and 2007.14698 into\n  this paper. arXiv admin note: text overlap with arXiv:2007.14698", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.OA", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Kernel methods have been among the most popular techniques in machine\nlearning, where learning tasks are solved using the property of reproducing\nkernel Hilbert space (RKHS). In this paper, we propose a novel data analysis\nframework with reproducing kernel Hilbert $C^*$-module (RKHM) and kernel mean\nembedding (KME) in RKHM. Since RKHM contains richer information than RKHS or\nvector-valued RKHS (vv RKHS), analysis with RKHM enables us to capture and\nextract structural properties in multivariate data, functional data and other\nstructured data. We show a branch of theories for RKHM to apply to data\nanalysis, including the representer theorem, and the injectivity and\nuniversality of the proposed KME. We also show RKHM generalizes RKHS and vv\nRKHS. Then, we provide concrete procedures for employing RKHM and the proposed\nKME to data analysis.\n", "versions": [{"version": "v1", "created": "Wed, 27 Jan 2021 14:02:18 GMT"}], "update_date": "2021-01-28", "authors_parsed": [["Hashimoto", "Yuka", ""], ["Ishikawa", "Isao", ""], ["Ikeda", "Masahiro", ""], ["Komura", "Fuyuta", ""], ["Katsura", "Takeshi", ""], ["Kawahara", "Yoshinobu", ""]]}, {"id": "2101.11422", "submitter": "Harsh Jaykumar Jalan", "authors": "Sreyan Ghosh, Sonal Kumar, Harsh Jalan, Hemant Yadav, Rajiv Ratn Shah", "title": "Cisco at AAAI-CAD21 shared task: Predicting Emphasis in Presentation\n  Slides using Contextualized Embeddings", "comments": "7 pages, 5 figures, 10 tables Submitted as a part of CAD-21 workshop\n  at AAAI-2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  This paper describes our proposed system for the AAAI-CAD21 shared task:\nPredicting Emphasis in Presentation Slides. In this specific task, given the\ncontents of a slide we are asked to predict the degree of emphasis to be laid\non each word in the slide. We propose 2 approaches to this problem including a\nBiLSTM-ELMo approach and a transformers based approach based on RoBERTa and\nXLNet architectures. We achieve a score of 0.518 on the evaluation leaderboard\nwhich ranks us 3rd and 0.543 on the post-evaluation leaderboard which ranks us\n1st at the time of writing the paper.\n", "versions": [{"version": "v1", "created": "Sun, 10 Jan 2021 10:43:12 GMT"}, {"version": "v2", "created": "Tue, 9 Feb 2021 07:34:10 GMT"}], "update_date": "2021-02-10", "authors_parsed": [["Ghosh", "Sreyan", ""], ["Kumar", "Sonal", ""], ["Jalan", "Harsh", ""], ["Yadav", "Hemant", ""], ["Shah", "Rajiv Ratn", ""]]}, {"id": "2101.11427", "submitter": "Xiang-Rong Sheng", "authors": "Xiang-Rong Sheng, Liqin Zhao, Guorui Zhou, Xinyao Ding, Binding Dai,\n  Qiang Luo, Siran Yang, Jingshan Lv, Chi Zhang, Hongbo Deng, Xiaoqiang Zhu", "title": "One Model to Serve All: Star Topology Adaptive Recommender for\n  Multi-Domain CTR Prediction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Traditional industrial recommenders are usually trained on a single business\ndomain and then serve for this domain. However, in large commercial platforms,\nit is often the case that the recommenders need to make click-through rate\n(CTR) predictions for multiple business domains. Different domains have\noverlapping user groups and items. Thus, there exist commonalities. Since the\nspecific user groups have disparity and the user behaviors may change in\nvarious business domains, there also have distinctions. The distinctions result\nin domain-specific data distributions, making it hard for a single shared model\nto work well on all domains. To learn an effective and efficient CTR model to\nhandle multiple domains simultaneously, we present Star Topology Adaptive\nRecommender (STAR). Concretely, STAR has the star topology, which consists of\nthe shared centered parameters and domain-specific parameters. The shared\nparameters are applied to learn commonalities of all domains, and the\ndomain-specific parameters capture domain distinction for more refined\nprediction. Given requests from different business domains, STAR can adapt its\nparameters conditioned on the domain characteristics. The experimental result\nfrom production data validates the superiority of the proposed STAR model.\nSince 2020, STAR has been deployed in the display advertising system of\nAlibaba, obtaining averaging 8.0% improvement on CTR and 6.0% on RPM (Revenue\nPer Mille).\n", "versions": [{"version": "v1", "created": "Wed, 27 Jan 2021 14:17:55 GMT"}, {"version": "v2", "created": "Wed, 26 May 2021 06:34:18 GMT"}], "update_date": "2021-05-27", "authors_parsed": [["Sheng", "Xiang-Rong", ""], ["Zhao", "Liqin", ""], ["Zhou", "Guorui", ""], ["Ding", "Xinyao", ""], ["Dai", "Binding", ""], ["Luo", "Qiang", ""], ["Yang", "Siran", ""], ["Lv", "Jingshan", ""], ["Zhang", "Chi", ""], ["Deng", "Hongbo", ""], ["Zhu", "Xiaoqiang", ""]]}, {"id": "2101.11430", "submitter": "Fei Teng", "authors": "Shu Yuan Hu and Fei Teng", "title": "An Explainable CNN Approach for Medical Codes Prediction from Clinical\n  Text", "comments": "10 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Method: We develop CNN-based methods for automatic ICD coding based on\nclinical text from intensive care unit (ICU) stays. We come up with the Shallow\nand Wide Attention convolutional Mechanism (SWAM), which allows our model to\nlearn local and low-level features for each label. The key idea behind our\nmodel design is to look for the presence of informative snippets in the\nclinical text that correlated with each code, and we infer that there exists a\ncorrespondence between \"informative snippet\" and convolution filter. Results:\nWe evaluate our approach on MIMIC-III, an open-access dataset of ICU medical\nrecords. Our approach substantially outperforms previous results on top-50\nmedical code prediction on MIMIC-III dataset. We attribute this improvement to\nSWAM, by which the wide architecture gives the model ability to more\nextensively learn the unique features of different codes, and we prove it by\nablation experiment. Besides, we perform manual analysis of the performance\nimbalance between different codes, and preliminary conclude the characteristics\nthat determine the difficulty of learning specific codes. Conclusions: We\npresent SWAM, an explainable CNN approach for multi-label document\nclassification, which employs a wide convolution layer to learn local and\nlow-level features for each label, yields strong improvements over previous\nmetrics on the ICD-9 code prediction task, while providing satisfactory\nexplanations for its internal mechanics.\n", "versions": [{"version": "v1", "created": "Thu, 14 Jan 2021 02:05:34 GMT"}], "update_date": "2021-01-28", "authors_parsed": [["Hu", "Shu Yuan", ""], ["Teng", "Fei", ""]]}, {"id": "2101.11432", "submitter": "Mahboobeh Parsapoor", "authors": "Hillary Ngai, Yoona Park, John Chen and Mahboobeh Parsapoor (Mah\n  Parsa)", "title": "Transformer-Based Models for Question Answering on COVID19", "comments": "7 pages, 3 figures, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  In response to the Kaggle's COVID-19 Open Research Dataset (CORD-19)\nchallenge, we have proposed three transformer-based question-answering systems\nusing BERT, ALBERT, and T5 models. Since the CORD-19 dataset is unlabeled, we\nhave evaluated the question-answering models' performance on two labeled\nquestions answers datasets \\textemdash CovidQA and CovidGQA. The BERT-based QA\nsystem achieved the highest F1 score (26.32), while the ALBERT-based QA system\nachieved the highest Exact Match (13.04). However, numerous challenges are\nassociated with developing high-performance question-answering systems for the\nongoing COVID-19 pandemic and future pandemics. At the end of this paper, we\ndiscuss these challenges and suggest potential solutions to address them.\n", "versions": [{"version": "v1", "created": "Sat, 16 Jan 2021 23:06:30 GMT"}], "update_date": "2021-01-28", "authors_parsed": [["Ngai", "Hillary", "", "Mah\n  Parsa"], ["Park", "Yoona", "", "Mah\n  Parsa"], ["Chen", "John", "", "Mah\n  Parsa"], ["Parsapoor", "Mahboobeh", "", "Mah\n  Parsa"]]}, {"id": "2101.11433", "submitter": "Artem Artemov", "authors": "A. Artemov, A. Veselovskiy, I. Khasenevich, I. Bolokhov", "title": "Analysis of Basic Emotions in Texts Based on BERT Vector Representation", "comments": "8 pages, 2 figures, 5 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In the following paper the authors present a GAN-type model and the most\nimportant stages of its development for the task of emotion recognition in\ntext. In particular, we propose an approach for generating a synthetic dataset\nof all possible emotions combinations based on manually labelled incomplete\ndata.\n", "versions": [{"version": "v1", "created": "Thu, 21 Jan 2021 07:11:21 GMT"}, {"version": "v2", "created": "Sun, 31 Jan 2021 12:47:33 GMT"}], "update_date": "2021-02-02", "authors_parsed": [["Artemov", "A.", ""], ["Veselovskiy", "A.", ""], ["Khasenevich", "I.", ""], ["Bolokhov", "I.", ""]]}, {"id": "2101.11442", "submitter": "Xiaobo Qu", "authors": "Wanqi Hu, Dicheng Chen, Tianyu Qiu, Hao Chen, Xi Chen, Lin Yang, Gen\n  Yan, Di Guo, Xiaobo Qu", "title": "Denoising Single Voxel Magnetic Resonance Spectroscopy with Deep\n  Learning on Repeatedly Sampled In Vivo Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.med-ph cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Objective: Magnetic Resonance Spectroscopy (MRS) is a noninvasive tool to\nreveal metabolic information. One challenge of MRS is the relatively low\nSignal-Noise Ratio (SNR) due to low concentrations of metabolites. To improve\nthe SNR, the most common approach is to average signals that are acquired in\nmultiple times. The data acquisition time, however, is increased by multiple\ntimes accordingly, resulting in the scanned objects uncomfortable or even\nunbearable. Methods: By exploring the multiple sampled data, a deep learning\ndenoising approach is proposed to learn a mapping from the low SNR signal to\nthe high SNR one. Results: Results on simulated and in vivo data show that the\nproposed method significantly reduces the data acquisition time with slightly\ncompromised metabolic accuracy. Conclusion: A deep learning denoising method\nwas proposed to significantly shorten the time of data acquisition, while\nmaintaining signal accuracy and reliability. Significance: Provide a solution\nof the fundamental low SNR problem in MRS with artificial intelligence.\n", "versions": [{"version": "v1", "created": "Tue, 26 Jan 2021 05:36:44 GMT"}], "update_date": "2021-01-28", "authors_parsed": [["Hu", "Wanqi", ""], ["Chen", "Dicheng", ""], ["Qiu", "Tianyu", ""], ["Chen", "Hao", ""], ["Chen", "Xi", ""], ["Yang", "Lin", ""], ["Yan", "Gen", ""], ["Guo", "Di", ""], ["Qu", "Xiaobo", ""]]}, {"id": "2101.11443", "submitter": "Sebastian Pokutta", "authors": "Sebastian Pokutta and Huan Xu", "title": "Adversaries in Online Learning Revisited: with applications in Robust\n  Optimization and Adversarial training", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We revisit the concept of \"adversary\" in online learning, motivated by\nsolving robust optimization and adversarial training using online learning\nmethods. While one of the classical setups in online learning deals with the\n\"adversarial\" setup, it appears that this concept is used less rigorously,\ncausing confusion in applying results and insights from online learning.\nSpecifically, there are two fundamentally different types of adversaries,\ndepending on whether the \"adversary\" is able to anticipate the exogenous\nrandomness of the online learning algorithms. This is particularly relevant to\nrobust optimization and adversarial training because the adversarial sequences\nare often anticipative, and many online learning algorithms do not achieve\ndiminishing regret in such a case.\n  We then apply this to solving robust optimization problems or (equivalently)\nadversarial training problems via online learning and establish a general\napproach for a large variety of problem classes using imaginary play. Here two\nplayers play against each other, the primal player playing the decisions and\nthe dual player playing realizations of uncertain data. When the game\nterminates, the primal player has obtained an approximately robust solution.\nThis meta-game allows for solving a large variety of robust optimization and\nmulti-objective optimization problems and generalizes the approach of\narXiv:1402.6361.\n", "versions": [{"version": "v1", "created": "Wed, 27 Jan 2021 14:23:06 GMT"}], "update_date": "2021-01-28", "authors_parsed": [["Pokutta", "Sebastian", ""], ["Xu", "Huan", ""]]}, {"id": "2101.11453", "submitter": "Jan Metzen", "authors": "Jan Hendrik Metzen, Nicole Finnie, Robin Hutmacher", "title": "Meta Adversarial Training against Universal Patches", "comments": "Accepted by the ICML 2021 workshop on \"A Blessing in Disguise: The\n  Prospects and Perils of Adversarial Machine Learning\"", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently demonstrated physical-world adversarial attacks have exposed\nvulnerabilities in perception systems that pose severe risks for\nsafety-critical applications such as autonomous driving. These attacks place\nadversarial artifacts in the physical world that indirectly cause the addition\nof a universal patch to inputs of a model that can fool it in a variety of\ncontexts. Adversarial training is the most effective defense against\nimage-dependent adversarial attacks. However, tailoring adversarial training to\nuniversal patches is computationally expensive since the optimal universal\npatch depends on the model weights which change during training. We propose\nmeta adversarial training (MAT), a novel combination of adversarial training\nwith meta-learning, which overcomes this challenge by meta-learning universal\npatches along with model training. MAT requires little extra computation while\ncontinuously adapting a large set of patches to the current model. MAT\nconsiderably increases robustness against universal patch attacks on image\nclassification and traffic-light detection.\n", "versions": [{"version": "v1", "created": "Wed, 27 Jan 2021 14:36:23 GMT"}, {"version": "v2", "created": "Tue, 22 Jun 2021 14:07:54 GMT"}], "update_date": "2021-06-23", "authors_parsed": [["Metzen", "Jan Hendrik", ""], ["Finnie", "Nicole", ""], ["Hutmacher", "Robin", ""]]}, {"id": "2101.11476", "submitter": "Alvaro Gomariz", "authors": "Alvaro Gomariz, Raphael Egli, Tiziano Portenier, C\\'esar\n  Nombela-Arrieta, Orcun Goksel", "title": "Utilizing Uncertainty Estimation in Deep Learning Segmentation of\n  Fluorescence Microscopy Images with Missing Markers", "comments": "Accepted at the IEEE International Symposium on Biomedical Imaging\n  (ISBI) 2021. 4 pages and 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Fluorescence microscopy images contain several channels, each indicating a\nmarker staining the sample. Since many different marker combinations are\nutilized in practice, it has been challenging to apply deep learning based\nsegmentation models, which expect a predefined channel combination for all\ntraining samples as well as at inference for future application. Recent work\ncircumvents this problem using a modality attention approach to be effective\nacross any possible marker combination. However, for combinations that do not\nexist in a labeled training dataset, one cannot have any estimation of\npotential segmentation quality if that combination is encountered during\ninference. Without this, not only one lacks quality assurance but one also does\nnot know where to put any additional imaging and labeling effort. We herein\npropose a method to estimate segmentation quality on unlabeled images by (i)\nestimating both aleatoric and epistemic uncertainties of convolutional neural\nnetworks for image segmentation, and (ii) training a Random Forest model for\nthe interpretation of uncertainty features via regression to their\ncorresponding segmentation metrics. Additionally, we demonstrate that including\nthese uncertainty measures during training can provide an improvement on\nsegmentation performance.\n", "versions": [{"version": "v1", "created": "Wed, 27 Jan 2021 15:06:04 GMT"}], "update_date": "2021-01-28", "authors_parsed": [["Gomariz", "Alvaro", ""], ["Egli", "Raphael", ""], ["Portenier", "Tiziano", ""], ["Nombela-Arrieta", "C\u00e9sar", ""], ["Goksel", "Orcun", ""]]}, {"id": "2101.11477", "submitter": "Maha Alkhairy", "authors": "Maha Alkhairy", "title": "Medical Segment Coloring of Clinical Notes", "comments": "11 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a deep learning-based method to identify the segments of\na clinical note corresponding to ICD-9 broad categories which are further\ncolor-coded with respect to 17 ICD-9 categories. The proposed Medical Segment\nColorer (MSC) architecture is a pipeline framework that works in three stages:\n(1) word categorization, (2) phrase allocation, and (3) document\nclassification. MSC uses gated recurrent unit neural networks (GRUs) to map\nfrom an input document to word multi-labels to phrase allocations, and uses\nstatistical median to map phrase allocation to document multi-label. We compute\nvariable length segment coloring from overlapping phrase allocation\nprobabilities. These cross-level bidirectional contextual links identify\nadaptive context and then produce segment coloring. We train and evaluate MSC\nusing the document labeled MIMIC-III clinical notes. Training is conducted\nsolely using document multi-labels without any information on phrases,\nsegments, or words. In addition to coloring a clinical note, MSC generates as\nbyproducts document multi-labeling and word tagging -- creation of ICD9\ncategory keyword lists based on segment coloring. Performance comparison of MSC\nbyproduct document multi-labels versus methods whose purpose is to produce\njustifiable document multi-labels is 64% vs 52.4% micro-average F1-score\nagainst the CAML (CNN attention multi label) method. For evaluation of MSC\nsegment coloring results, medical practitioners independently assigned the\ncolors to broad ICD9 categories given a sample of 40 colored notes and a sample\nof 50 words related to each category based on the word tags. Binary scoring of\nthis evaluation has a median value of 83.3% and mean of 63.7%.\n", "versions": [{"version": "v1", "created": "Tue, 26 Jan 2021 09:49:37 GMT"}], "update_date": "2021-01-28", "authors_parsed": [["Alkhairy", "Maha", ""]]}, {"id": "2101.11487", "submitter": "George Jeffreys", "authors": "George Jeffreys and Siu-Cheong Lau", "title": "K\\\"ahler Geometry of Quiver Varieties and Machine Learning", "comments": "v2: expanded on related works. 47 pages, 13 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.AG cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We develop an algebro-geometric formulation for neural networks in machine\nlearning using the moduli space of framed quiver representations. We find\nnatural Hermitian metrics on the universal bundles over the moduli which are\ncompatible with the GIT quotient construction by the general linear group, and\nshow that their Ricci curvatures give a K\\\"ahler metric on the moduli.\nMoreover, we use toric moment maps to construct activation functions, and prove\nthe universal approximation theorem for the multi-variable activation function\nconstructed from the complex projective space.\n", "versions": [{"version": "v1", "created": "Wed, 27 Jan 2021 15:32:24 GMT"}, {"version": "v2", "created": "Wed, 10 Feb 2021 16:09:49 GMT"}], "update_date": "2021-02-11", "authors_parsed": [["Jeffreys", "George", ""], ["Lau", "Siu-Cheong", ""]]}, {"id": "2101.11505", "submitter": "Di Tong", "authors": "Di Tong (Massachusetts Institute of Technology), Lingfei Wu\n  (University of Pittsburgh), James Allen Evans (University of Chicago)", "title": "Low-skilled Occupations Face the Highest Re-skilling Pressure", "comments": "11 pages; 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.LG", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  While substantial scholarship has focused on estimating the susceptibility of\njobs to automation, little has examined how job contents evolve in the\ninformation age despite the fact that new technologies typically substitute for\nspecific job tasks, shifting job skills rather than eliminating whole jobs.\nHere we explore the process and consequences of changes in occupational skill\ncontents and characterize occupations subject to the most re-skilling pressure.\nRecent research suggests that high-skilled STEM and technology-intensive\nbusiness occupations have experienced the highest rates of skill content\nchange. Using a dataset covering the near universe of U.S. online job postings\nbetween 2010 and 2018, we find that when the number and similarity of skills\nwithin a job are taken into account, the re-skilling pressure is much higher\nfor workers in low complexity, low education and low compensation occupations.\nWe use high-dimensional embeddings of skills estimated across all jobs to\nprecisely assess skill similarity, and characterize occupational skill\ntransformations, demonstrating that skills requiring machine-operation and\ninterface rise sharply in importance in the past decade, much more than human\ninterface skills in low and mid-education occupations. We establish that large\norganizations buffer jobs from skill instability and obsolescence, especially\nlow-skilled jobs with unstable skill requirements. Finally, the gap in\nre-skilling pressure between low/mid-education and high-education occupations\nis smaller in large organizations, suggesting that by controlling the\nsurrounding skill environment, such organizations reduce the rate of required\nre-skilling and sustain short-term productivity for those occupations.\n", "versions": [{"version": "v1", "created": "Wed, 27 Jan 2021 16:02:57 GMT"}], "update_date": "2021-01-28", "authors_parsed": [["Tong", "Di", "", "Massachusetts Institute of Technology"], ["Wu", "Lingfei", "", "University of Pittsburgh"], ["Evans", "James Allen", "", "University of Chicago"]]}, {"id": "2101.11508", "submitter": "Olivier Rukundo", "authors": "Olivier Rukundo", "title": "Effects of Image Size on Deep Learning", "comments": "8 pages, 16 figures, 4 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper presents the evaluation of effects of image size on deep learning\nperformance via semantic segmentation of magnetic resonance heart images with\nU-net for fully automated quantification of myocardial infarction. Both\nnon-extra pixel and extra pixel interpolation algorithms are used to change the\nsize of images in datasets of interest. Extra class labels, in interpolated\nground truth segmentation images, are removed using thresholding, median\nfiltering, and subtraction strategies. Common class metrics are used to\nevaluate the quality of semantic segmentation with U-net against the ground\ntruth segmentation while arbitrary threshold, comparison of the sums, and sums\nof differences between medical experts and fully automated results are options\nused to estimate the relationship between medical experts-based quantification\nand fully automated quantification results.\n", "versions": [{"version": "v1", "created": "Wed, 27 Jan 2021 16:07:48 GMT"}, {"version": "v2", "created": "Mon, 26 Jul 2021 20:25:11 GMT"}], "update_date": "2021-07-28", "authors_parsed": [["Rukundo", "Olivier", ""]]}, {"id": "2101.11513", "submitter": "Yash Kumar", "authors": "Yash Kumar, Pranav Bahl, Souvik Chakraborty", "title": "State estimation with limited sensors -- A deep learning based approach", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.flu-dyn cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The importance of state estimation in fluid mechanics is well-established; it\nis required for accomplishing several tasks including design/optimization,\nactive control, and future state prediction. A common tactic in this regards is\nto rely on reduced order models. Such approaches, in general, use measurement\ndata of one-time instance. However, oftentimes data available from sensors is\nsequential and ignoring it results in information loss. In this paper, we\npropose a novel deep learning based state estimation framework that learns from\nsequential data. The proposed model structure consists of the recurrent cell to\npass information from different time steps enabling utilization of this\ninformation to recover the full state. We illustrate that utilizing sequential\ndata allows for state recovery from only one or two sensors. For efficient\nrecovery of the state, the proposed approached is coupled with an auto-encoder\nbased reduced order model. We illustrate the performance of the proposed\napproach using two examples and it is found to outperform other alternatives\nexisting in the literature.\n", "versions": [{"version": "v1", "created": "Wed, 27 Jan 2021 16:14:59 GMT"}], "update_date": "2021-01-28", "authors_parsed": [["Kumar", "Yash", ""], ["Bahl", "Pranav", ""], ["Chakraborty", "Souvik", ""]]}, {"id": "2101.11517", "submitter": "Risheng Liu", "authors": "Risheng Liu, Jiaxin Gao, Jin Zhang, Deyu Meng and Zhouchen Lin", "title": "Investigating Bi-Level Optimization for Learning and Vision from a\n  Unified Perspective: A Survey and Beyond", "comments": "21 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV math.DS math.OC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Bi-Level Optimization (BLO) is originated from the area of economic game\ntheory and then introduced into the optimization community. BLO is able to\nhandle problems with a hierarchical structure, involving two levels of\noptimization tasks, where one task is nested inside the other. In machine\nlearning and computer vision fields, despite the different motivations and\nmechanisms, a lot of complex problems, such as hyper-parameter optimization,\nmulti-task and meta-learning, neural architecture search, adversarial learning\nand deep reinforcement learning, actually all contain a series of closely\nrelated subproblms. In this paper, we first uniformly express these complex\nlearning and vision problems from the perspective of BLO. Then we construct a\nbest-response-based single-level reformulation and establish a unified\nalgorithmic framework to understand and formulate mainstream gradient-based BLO\nmethodologies, covering aspects ranging from fundamental automatic\ndifferentiation schemes to various accelerations, simplifications, extensions\nand their convergence and complexity properties. Last but not least, we discuss\nthe potentials of our unified BLO framework for designing new algorithms and\npoint out some promising directions for future research.\n", "versions": [{"version": "v1", "created": "Wed, 27 Jan 2021 16:20:23 GMT"}, {"version": "v2", "created": "Sun, 25 Jul 2021 14:16:15 GMT"}], "update_date": "2021-07-27", "authors_parsed": [["Liu", "Risheng", ""], ["Gao", "Jiaxin", ""], ["Zhang", "Jin", ""], ["Meng", "Deyu", ""], ["Lin", "Zhouchen", ""]]}, {"id": "2101.11520", "submitter": "Yuki Takezawa", "authors": "Yuki Takezawa, Ryoma Sato, Makoto Yamada", "title": "Supervised Tree-Wasserstein Distance", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To measure the similarity of documents, the Wasserstein distance is a\npowerful tool, but it requires a high computational cost. Recently, for fast\ncomputation of the Wasserstein distance, methods for approximating the\nWasserstein distance using a tree metric have been proposed. These tree-based\nmethods allow fast comparisons of a large number of documents; however, they\nare unsupervised and do not learn task-specific distances. In this work, we\npropose the Supervised Tree-Wasserstein (STW) distance, a fast, supervised\nmetric learning method based on the tree metric. Specifically, we rewrite the\nWasserstein distance on the tree metric by the parent-child relationships of a\ntree and formulate it as a continuous optimization problem using a contrastive\nloss. Experimentally, we show that the STW distance can be computed fast, and\nimproves the accuracy of document classification tasks. Furthermore, the STW\ndistance is formulated by matrix multiplications, runs on a GPU, and is\nsuitable for batch processing. Therefore, we show that the STW distance is\nextremely efficient when comparing a large number of documents.\n", "versions": [{"version": "v1", "created": "Wed, 27 Jan 2021 16:24:51 GMT"}, {"version": "v2", "created": "Fri, 23 Jul 2021 07:55:07 GMT"}], "update_date": "2021-07-26", "authors_parsed": [["Takezawa", "Yuki", ""], ["Sato", "Ryoma", ""], ["Yamada", "Makoto", ""]]}, {"id": "2101.11525", "submitter": "Kaili Ma", "authors": "Kaili Ma, Haochen Yang, Han Yang, Tatiana Jin, Pengfei Chen, Yongqiang\n  Chen, Barakeel Fanseu Kamhoua, James Cheng", "title": "Improving Graph Representation Learning by Contrastive Regularization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Graph representation learning is an important task with applications in\nvarious areas such as online social networks, e-commerce networks, WWW, and\nsemantic webs. For unsupervised graph representation learning, many algorithms\nsuch as Node2Vec and Graph-SAGE make use of \"negative sampling\" and/or noise\ncontrastive estimation loss. This bears similar ideas to contrastive learning,\nwhich \"contrasts\" the node representation similarities of semantically similar\n(positive) pairs against those of negative pairs. However, despite the success\nof contrastive learning, we found that directly applying this technique to\ngraph representation learning models (e.g., graph convolutional networks) does\nnot always work. We theoretically analyze the generalization performance and\npropose a light-weight regularization term that avoids the high scales of node\nrepresentations' norms and the high variance among them to improve the\ngeneralization performance. Our experimental results further validate that this\nregularization term significantly improves the representation quality across\ndifferent node similarity definitions and outperforms the state-of-the-art\nmethods.\n", "versions": [{"version": "v1", "created": "Wed, 27 Jan 2021 16:31:33 GMT"}], "update_date": "2021-01-28", "authors_parsed": [["Ma", "Kaili", ""], ["Yang", "Haochen", ""], ["Yang", "Han", ""], ["Jin", "Tatiana", ""], ["Chen", "Pengfei", ""], ["Chen", "Yongqiang", ""], ["Kamhoua", "Barakeel Fanseu", ""], ["Cheng", "James", ""]]}, {"id": "2101.11531", "submitter": "Ruriko Yoshida", "authors": "Ruriko Yoshida and Misaki Takamori and Hideyuki Matsumoto and Keiji\n  Miura", "title": "Tropical Support Vector Machines: Evaluations and Extension to Function\n  Spaces", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.CO math.ST stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Support Vector Machines (SVMs) are one of the most popular supervised\nlearning models to classify using a hyperplane in an Euclidean space. Similar\nto SVMs, tropical SVMs classify data points using a tropical hyperplane under\nthe tropical metric with the max-plus algebra. In this paper, first we show\ngeneralization error bounds of tropical SVMs over the tropical projective\nspace. While the generalization error bounds attained via VC dimensions in a\ndistribution-free manner still depend on the dimension, we also show\ntheoretically by extreme value statistics that the tropical SVMs for\nclassifying data points from two Gaussian distributions as well as empirical\ndata sets of different neuron types are fairly robust against the curse of\ndimensionality. Extreme value statistics also underlie the anomalous scaling\nbehaviors of the tropical distance between random vectors with additional noise\ndimensions. Finally, we define tropical SVMs over a function space with the\ntropical metric and discuss the Gaussian function space as an example.\n", "versions": [{"version": "v1", "created": "Wed, 27 Jan 2021 16:35:34 GMT"}], "update_date": "2021-01-28", "authors_parsed": [["Yoshida", "Ruriko", ""], ["Takamori", "Misaki", ""], ["Matsumoto", "Hideyuki", ""], ["Miura", "Keiji", ""]]}, {"id": "2101.11539", "submitter": "Kevin Styp-Rekowski", "authors": "Sabtain Ahmad, Kevin Styp-Rekowski, Sasho Nedelkoski, Odej Kao", "title": "Autoencoder-based Condition Monitoring and Anomaly Detection Method for\n  Rotating Machines", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Rotating machines like engines, pumps, or turbines are ubiquitous in modern\nday societies. Their mechanical parts such as electrical engines, rotors, or\nbearings are the major components and any failure in them may result in their\ntotal shutdown. Anomaly detection in such critical systems is very important to\nmonitor the system's health. As the requirement to obtain a dataset from\nrotating machines where all possible faults are explicitly labeled is difficult\nto satisfy, we propose a method that focuses on the normal behavior of the\nmachine instead. We propose an autoencoder model-based method for condition\nmonitoring of rotating machines by using an anomaly detection approach. The\nmethod learns the characteristics of a rotating machine using the normal\nvibration signals to model the healthy state of the machine. A threshold-based\napproach is then applied to the reconstruction error of unseen data, thus\nenabling the detection of unseen anomalies. The proposed method can directly\nextract the salient features from raw vibration signals and eliminate the need\nfor manually engineered features. We demonstrate the effectiveness of the\nproposed method by employing two rotating machine datasets and the quality of\nthe automatically learned features is compared with a set of handcrafted\nfeatures by training an Isolation Forest model on either of these two sets.\nExperimental results on two real-world datasets indicate that our proposed\nsolution gives promising results, achieving an average F1-score of 99.6%.\n", "versions": [{"version": "v1", "created": "Wed, 27 Jan 2021 16:49:49 GMT"}], "update_date": "2021-01-28", "authors_parsed": [["Ahmad", "Sabtain", ""], ["Styp-Rekowski", "Kevin", ""], ["Nedelkoski", "Sasho", ""], ["Kao", "Odej", ""]]}, {"id": "2101.11550", "submitter": "Shin-Nosuke Ishikawa", "authors": "Shin-nosuke Ishikawa, Hideaki Matsumura, Yasunobu Uchiyama and Lindsay\n  Glesener", "title": "Automatic Detection of Occulted Hard X-ray Flares Using Deep-Learning\n  Methods", "comments": "11 pages, 3 figures, accepted for publication in Solar Physics", "journal-ref": null, "doi": "10.1007/s11207-021-01780-x", "report-no": null, "categories": "astro-ph.SR astro-ph.IM cs.AI cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a concept for a machine-learning classification of hard X-ray\n(HXR) emissions from solar flares observed by the Reuven Ramaty High Energy\nSolar Spectroscopic Imager (RHESSI), identifying flares that are either\nocculted by the solar limb or located on the solar disk. Although HXR\nobservations of occulted flares are important for particle-acceleration\nstudies, HXR data analyses for past observations were time consuming and\nrequired specialized expertise. Machine-learning techniques are promising for\nthis situation, and we constructed a sample model to demonstrate the concept\nusing a deep-learning technique. Input data to the model are HXR spectrograms\nthat are easily produced from RHESSI data. The model can detect occulted flares\nwithout the need for image reconstruction nor for visual inspection by experts.\nA technique of convolutional neural networks was used in this model by\nregarding the input data as images. Our model achieved a classification\naccuracy better than 90 %, and the ability for the application of the method to\neither event screening or for an event alert for occulted flares was\nsuccessfully demonstrated.\n", "versions": [{"version": "v1", "created": "Wed, 27 Jan 2021 17:11:35 GMT"}], "update_date": "2021-02-24", "authors_parsed": [["Ishikawa", "Shin-nosuke", ""], ["Matsumura", "Hideaki", ""], ["Uchiyama", "Yasunobu", ""], ["Glesener", "Lindsay", ""]]}, {"id": "2101.11552", "submitter": "Jun Hu", "authors": "Jun Hu, Shengsheng Qian, Quan Fang, Youze Wang, Quan Zhao, Huaiwen\n  Zhang, Changsheng Xu", "title": "Efficient Graph Deep Learning in TensorFlow with tf_geometric", "comments": "7 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce tf_geometric, an efficient and friendly library for graph deep\nlearning, which is compatible with both TensorFlow 1.x and 2.x. tf_geometric\nprovides kernel libraries for building Graph Neural Networks (GNNs) as well as\nimplementations of popular GNNs. The kernel libraries consist of\ninfrastructures for building efficient GNNs, including graph data structures,\ngraph map-reduce framework, graph mini-batch strategy, etc. These\ninfrastructures enable tf_geometric to support single-graph computation,\nmulti-graph computation, graph mini-batch, distributed training, etc.;\ntherefore, tf_geometric can be used for a variety of graph deep learning tasks,\nsuch as transductive node classification, inductive node classification, link\nprediction, and graph classification. Based on the kernel libraries,\ntf_geometric implements a variety of popular GNN models for different tasks. To\nfacilitate the implementation of GNNs, tf_geometric also provides some other\nlibraries for dataset management, graph sampling, etc. Different from existing\npopular GNN libraries, tf_geometric provides not only Object-Oriented\nProgramming (OOP) APIs, but also Functional APIs, which enable tf_geometric to\nhandle advanced graph deep learning tasks such as graph meta-learning. The APIs\nof tf_geometric are friendly, and they are suitable for both beginners and\nexperts. In this paper, we first present an overview of tf_geometric's\nframework. Then, we conduct experiments on some benchmark datasets and report\nthe performance of several popular GNN models implemented by tf_geometric.\n", "versions": [{"version": "v1", "created": "Wed, 27 Jan 2021 17:16:36 GMT"}], "update_date": "2021-01-28", "authors_parsed": [["Hu", "Jun", ""], ["Qian", "Shengsheng", ""], ["Fang", "Quan", ""], ["Wang", "Youze", ""], ["Zhao", "Quan", ""], ["Zhang", "Huaiwen", ""], ["Xu", "Changsheng", ""]]}, {"id": "2101.11560", "submitter": "Ece Calikus", "authors": "Ece Calikus, Slawomir Nowaczyk, Mohamed-Rafik Bouguelia, and Onur\n  Dikmen", "title": "Wisdom of the Contexts: Active Ensemble Learning for Contextual Anomaly\n  Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In contextual anomaly detection, an object is only considered anomalous\nwithin a specific context. Most existing methods for CAD use a single context\nbased on a set of user-specified contextual features. However, identifying the\nright context can be very challenging in practice, especially in datasets, with\na large number of attributes. Furthermore, in real-world systems, there might\nbe multiple anomalies that occur in different contexts and, therefore, require\na combination of several \"useful\" contexts to unveil them. In this work, we\nleverage active learning and ensembles to effectively detect complex contextual\nanomalies in situations where the true contextual and behavioral attributes are\nunknown. We propose a novel approach, called WisCon (Wisdom of the Contexts),\nthat automatically creates contexts from the feature set. Our method constructs\nan ensemble of multiple contexts, with varying importance scores, based on the\nassumption that not all useful contexts are equally so. Experiments show that\nWisCon significantly outperforms existing baselines in different categories\n(i.e., active classifiers, unsupervised contextual and non-contextual anomaly\ndetectors, and supervised classifiers) on seven datasets. Furthermore, the\nresults support our initial hypothesis that there is no single perfect context\nthat successfully uncovers all kinds of contextual anomalies, and leveraging\nthe \"wisdom\" of multiple contexts is necessary.\n", "versions": [{"version": "v1", "created": "Wed, 27 Jan 2021 17:34:13 GMT"}, {"version": "v2", "created": "Thu, 15 Apr 2021 23:16:56 GMT"}], "update_date": "2021-04-19", "authors_parsed": [["Calikus", "Ece", ""], ["Nowaczyk", "Slawomir", ""], ["Bouguelia", "Mohamed-Rafik", ""], ["Dikmen", "Onur", ""]]}, {"id": "2101.11588", "submitter": "Daniel Schwalbe-Koda", "authors": "Daniel Schwalbe-Koda, Aik Rui Tan, Rafael G\\'omez-Bombarelli", "title": "Differentiable sampling of molecular geometries with uncertainty-based\n  adversarial attacks", "comments": "12 pages, 4 figures, supporting information", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cond-mat.stat-mech physics.chem-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural network (NN) interatomic potentials provide fast prediction of\npotential energy surfaces, closely matching the accuracy of the electronic\nstructure methods used to produce the training data. However, NN predictions\nare only reliable within well-learned training domains, and show volatile\nbehavior when extrapolating. Uncertainty quantification approaches can flag\natomic configurations for which prediction confidence is low, but arriving at\nsuch uncertain regions requires expensive sampling of the NN phase space, often\nusing atomistic simulations. Here, we exploit automatic differentiation to\ndrive atomistic systems towards high-likelihood, high-uncertainty\nconfigurations without the need for molecular dynamics simulations. By\nperforming adversarial attacks on an uncertainty metric, informative geometries\nthat expand the training domain of NNs are sampled. When combined to an active\nlearning loop, this approach bootstraps and improves NN potentials while\ndecreasing the number of calls to the ground truth method. This efficiency is\ndemonstrated on sampling of kinetic barriers and collective variables in\nmolecules, and can be extended to any NN potential architecture and materials\nsystem.\n", "versions": [{"version": "v1", "created": "Wed, 27 Jan 2021 18:33:59 GMT"}, {"version": "v2", "created": "Mon, 1 Feb 2021 01:03:38 GMT"}, {"version": "v3", "created": "Mon, 29 Mar 2021 00:22:32 GMT"}], "update_date": "2021-03-30", "authors_parsed": [["Schwalbe-Koda", "Daniel", ""], ["Tan", "Aik Rui", ""], ["G\u00f3mez-Bombarelli", "Rafael", ""]]}, {"id": "2101.11589", "submitter": "Mirco H\\\"unnefeld", "authors": "R. Abbasi, M. Ackermann, J. Adams, J. A. Aguilar, M. Ahlers, M.\n  Ahrens, C. Alispach, A. A. Alves Jr., N. M. Amin, R. An, K. Andeen, T.\n  Anderson, I. Ansseau, G. Anton, C. Arg\\\"uelles, S. Axani, X. Bai, A.\n  Balagopal V., A. Barbano, S. W. Barwick, B. Bastian, V. Basu, V. Baum, S.\n  Baur, R. Bay, J. J. Beatty, K.-H. Becker, J. Becker Tjus, C. Bellenghi, S.\n  BenZvi, D. Berley, E. Bernardini, D. Z. Besson, G. Binder, D. Bindig, E.\n  Blaufuss, S. Blot, S. B\\\"oser, O. Botner, J. B\\\"ottcher, E. Bourbeau, J.\n  Bourbeau, F. Bradascio, J. Braun, S. Bron, J. Brostean-Kaiser, A. Burgman, R.\n  S. Busse, M. A. Campana, C. Chen, D. Chirkin, S. Choi, B. A. Clark, K. Clark,\n  L. Classen, A. Coleman, G. H. Collin, J. M. Conrad, P. Coppin, P. Correa, D.\n  F. Cowen, R. Cross, P. Dave, C. De Clercq, J. J. DeLaunay, H. Dembinski, K.\n  Deoskar, S. De Ridder, A. Desai, P. Desiati, K. D. de Vries, G. de Wasseige,\n  M. de With, T. DeYoung, S. Dharani, A. Diaz, J. C. D\\'iaz-V\\'elez, H.\n  Dujmovic, M. Dunkman, M. A. DuVernois, E. Dvorak, T. Ehrhardt, P. Eller, R.\n  Engel, J. Evans, P. A. Evenson, S. Fahey, A. R. Fazely, S. Fiedlschuster,\n  A.T. Fienberg, K. Filimonov, C. Finley, L. Fischer, D. Fox, A. Franckowiak,\n  E. Friedman, A. Fritz, P. F\\\"urst, T. K. Gaisser, J. Gallagher, E. Ganster,\n  S. Garrappa, L. Gerhardt, A. Ghadimi, C. Glaser, T. Glauch, T. Gl\\\"usenkamp,\n  A. Goldschmidt, J. G. Gonzalez, S. Goswami, D. Grant, T. Gr\\'egoire, Z.\n  Griffith, S. Griswold, M. G\\\"und\\\"uz, C. Haack, A. Hallgren, R. Halliday, L.\n  Halve, F. Halzen, M. Ha Minh, K. Hanson, J. Hardin, A. A. Harnisch, A.\n  Haungs, S. Hauser, D. Hebecker, K. Helbing, F. Henningsen, E. C. Hettinger,\n  S. Hickford, J. Hignight, C. Hill, G. C. Hill, K. D. Hoffman, R. Hoffmann, T.\n  Hoinka, B. Hokanson-Fasig, K. Hoshina, F. Huang, M. Huber, T. Huber, K.\n  Hultqvist, M. H\\\"unnefeld, R. Hussain, S. In, N. Iovine, A. Ishihara, M.\n  Jansson, G. S. Japaridze, M. Jeong, B. J. P. Jones, R. Joppe, D. Kang, W.\n  Kang, X. Kang, A. Kappes, D. Kappesser, T. Karg, M. Karl, A. Karle, U. Katz,\n  M. Kauer, M. Kellermann, J. L. Kelley, A. Kheirandish, J. Kim, K. Kin, T.\n  Kintscher, J. Kiryluk, S. R. Klein, R. Koirala, H. Kolanoski, L. K\\\"opke, C.\n  Kopper, S. Kopper, D. J. Koskinen, P. Koundal, M. Kovacevich, M. Kowalski, K.\n  Krings, G. Kr\\\"uckl, N. Kurahashi, A. Kyriacou, C. Lagunas Gualda, J. L.\n  Lanfranchi, M. J. Larson, F. Lauber, J. P. Lazar, K. Leonard, A.\n  Leszczy\\'nska, Y. Li, Q. R. Liu, E. Lohfink, C. J. Lozano Mariscal, L. Lu, F.\n  Lucarelli, A. Ludwig, W. Luszczak, Y. Lyu, W. Y. Ma, J. Madsen, K. B. M.\n  Mahn, Y. Makino, P. Mallik, S. Mancina, I. C. Mari{\\c{s}}, R. Maruyama, K.\n  Mase, F. McNally, K. Meagher, A. Medina, M. Meier, S. Meighen-Berger, J.\n  Merz, J. Micallef, D. Mockler, G. Moment\\'e, T. Montaruli, R. W. Moore, K.\n  Morik, R. Morse, M. Moulai, R. Naab, R. Nagai, U. Naumann, J. Necker, L. V.\n  Nguy{\\~{\\^{{e}}}}n, H. Niederhausen, M. U. Nisa, S. C. Nowicki, D. R. Nygren,\n  A. Obertacke Pollmann, M. Oehler, A. Olivas, E. O'Sullivan, H. Pandya, D. V.\n  Pankova, N. Park, G. K. Parker, E. N. Paudel, P. Peiffer, C. P\\'erez de los\n  Heros, S. Philippen, D. Pieloth, S. Pieper, A. Pizzuto, M. Plum, Y. Popovych,\n  A. Porcelli, M. Prado Rodriguez, P. B. Price, B. Pries, G. T. Przybylski, C.\n  Raab, A. Raissi, M. Rameez, K. Rawlins, I. C. Rea, A. Rehman, R. Reimann, M.\n  Renschler, G. Renzi, E. Resconi, S. Reusch, W. Rhode, M. Richman, B. Riedel,\n  S. Robertson, G. Roellinghoff, M. Rongen, C. Rott, T. Ruhe, D. Ryckbosch, D.\n  Rysewyk Cantu, I. Safa, S. E. Sanchez Herrera, A. Sandrock, J. Sandroos, M.\n  Santander, S. Sarkar, S. Sarkar, K. Satalecka, M. Scharf, M. Schaufel, H.\n  Schieler, P. Schlunder, T. Schmidt, A. Schneider, J. Schneider, F. G.\n  Schr\\\"oder, L. Schumacher, S. Sclafani, D. Seckel, S. Seunarine, A. Sharma,\n  S. Shefali, M. Silva, B. Skrzypek, B. Smithers, R. Snihur, J. Soedingrekso,\n  D. Soldin, G. M. Spiczak, C. Spiering, J. Stachurska, M. Stamatikos, T.\n  Stanev, R. Stein, J. Stettner, A. Steuer, T. Stezelberger, R. G. Stokstad, T.\n  St\\\"urwald, T. Stuttard, G. W. Sullivan, I. Taboada, F. Tenholt, S.\n  Ter-Antonyan, S. Tilav, F. Tischbein, K. Tollefson, L. Tomankova, C.\n  T\\\"onnis, S. Toscano, D. Tosi, A. Trettin, M. Tselengidou, C. F. Tung, A.\n  Turcati, R. Turcotte, C. F. Turley, J. P. Twagirayezu, B. Ty, M. A. Unland\n  Elorrieta, N. Valtonen-Mattila, J. Vandenbroucke, D. van Eijk, N. van\n  Eijndhoven, D. Vannerom, J. van Santen, S. Verpoest, M. Vraeghe, C. Walck, A.\n  Wallace, T. B. Watson, C. Weaver, A. Weindl, M. J. Weiss, J. Weldert, C.\n  Wendt, J. Werthebach, M. Weyrauch, B. J. Whelan, N. Whitehorn, K. Wiebe, C.\n  H. Wiebusch, D. R. Williams, M. Wolf, K. Woschnagg, G. Wrede, J. Wulff, X. W.\n  Xu, Y. Xu, J. P. Yanez, S. Yoshida, T. Yuan, Z. Zhang", "title": "A Convolutional Neural Network based Cascade Reconstruction for the\n  IceCube Neutrino Observatory", "comments": "39 pages, 15 figures, submitted to Journal of Instrumentation; added\n  references", "journal-ref": "JINST 16 P07041 (2021)", "doi": "10.1088/1748-0221/16/07/p07041", "report-no": null, "categories": "hep-ex cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Continued improvements on existing reconstruction methods are vital to the\nsuccess of high-energy physics experiments, such as the IceCube Neutrino\nObservatory. In IceCube, further challenges arise as the detector is situated\nat the geographic South Pole where computational resources are limited.\nHowever, to perform real-time analyses and to issue alerts to telescopes around\nthe world, powerful and fast reconstruction methods are desired. Deep neural\nnetworks can be extremely powerful, and their usage is computationally\ninexpensive once the networks are trained. These characteristics make a deep\nlearning-based approach an excellent candidate for the application in IceCube.\nA reconstruction method based on convolutional architectures and hexagonally\nshaped kernels is presented. The presented method is robust towards systematic\nuncertainties in the simulation and has been tested on experimental data. In\ncomparison to standard reconstruction methods in IceCube, it can improve upon\nthe reconstruction accuracy, while reducing the time necessary to run the\nreconstruction by two to three orders of magnitude.\n", "versions": [{"version": "v1", "created": "Wed, 27 Jan 2021 18:34:58 GMT"}, {"version": "v2", "created": "Mon, 26 Jul 2021 09:03:22 GMT"}], "update_date": "2021-07-27", "authors_parsed": [["Abbasi", "R.", ""], ["Ackermann", "M.", ""], ["Adams", "J.", ""], ["Aguilar", "J. A.", ""], ["Ahlers", "M.", ""], ["Ahrens", "M.", ""], ["Alispach", "C.", ""], ["Alves", "A. A.", "Jr."], ["Amin", "N. M.", ""], ["An", "R.", ""], ["Andeen", "K.", ""], ["Anderson", "T.", ""], ["Ansseau", "I.", ""], ["Anton", "G.", ""], ["Arg\u00fcelles", "C.", ""], ["Axani", "S.", ""], ["Bai", "X.", ""], ["V.", "A. Balagopal", ""], ["Barbano", "A.", ""], ["Barwick", "S. W.", ""], ["Bastian", "B.", ""], ["Basu", "V.", ""], ["Baum", "V.", ""], ["Baur", "S.", ""], ["Bay", "R.", ""], ["Beatty", "J. J.", ""], ["Becker", "K. -H.", ""], ["Tjus", "J. Becker", ""], ["Bellenghi", "C.", ""], ["BenZvi", "S.", ""], ["Berley", "D.", ""], ["Bernardini", "E.", ""], ["Besson", "D. Z.", ""], ["Binder", "G.", ""], ["Bindig", "D.", ""], ["Blaufuss", "E.", ""], ["Blot", "S.", ""], ["B\u00f6ser", "S.", ""], ["Botner", "O.", ""], ["B\u00f6ttcher", "J.", ""], ["Bourbeau", "E.", ""], ["Bourbeau", "J.", ""], ["Bradascio", "F.", ""], ["Braun", "J.", ""], ["Bron", "S.", ""], ["Brostean-Kaiser", "J.", ""], ["Burgman", "A.", ""], ["Busse", "R. S.", ""], ["Campana", "M. A.", ""], ["Chen", "C.", ""], ["Chirkin", "D.", ""], ["Choi", "S.", ""], ["Clark", "B. A.", ""], ["Clark", "K.", ""], ["Classen", "L.", ""], ["Coleman", "A.", ""], ["Collin", "G. H.", ""], ["Conrad", "J. M.", ""], ["Coppin", "P.", ""], ["Correa", "P.", ""], ["Cowen", "D. F.", ""], ["Cross", "R.", ""], ["Dave", "P.", ""], ["De Clercq", "C.", ""], ["DeLaunay", "J. J.", ""], ["Dembinski", "H.", ""], ["Deoskar", "K.", ""], ["De Ridder", "S.", ""], ["Desai", "A.", ""], ["Desiati", "P.", ""], ["de Vries", "K. D.", ""], ["de Wasseige", "G.", ""], ["de With", "M.", ""], ["DeYoung", "T.", ""], ["Dharani", "S.", ""], ["Diaz", "A.", ""], ["D\u00edaz-V\u00e9lez", "J. C.", ""], ["Dujmovic", "H.", ""], ["Dunkman", "M.", ""], ["DuVernois", "M. A.", ""], ["Dvorak", "E.", ""], ["Ehrhardt", "T.", ""], ["Eller", "P.", ""], ["Engel", "R.", ""], ["Evans", "J.", ""], ["Evenson", "P. A.", ""], ["Fahey", "S.", ""], ["Fazely", "A. R.", ""], ["Fiedlschuster", "S.", ""], ["Fienberg", "A. T.", ""], ["Filimonov", "K.", ""], ["Finley", "C.", ""], ["Fischer", "L.", ""], ["Fox", "D.", ""], ["Franckowiak", "A.", ""], ["Friedman", "E.", ""], ["Fritz", "A.", ""], ["F\u00fcrst", "P.", ""], ["Gaisser", "T. K.", ""], ["Gallagher", "J.", ""], ["Ganster", "E.", ""], ["Garrappa", "S.", ""], ["Gerhardt", "L.", ""], ["Ghadimi", "A.", ""], ["Glaser", "C.", ""], ["Glauch", "T.", ""], ["Gl\u00fcsenkamp", "T.", ""], ["Goldschmidt", "A.", ""], ["Gonzalez", "J. G.", ""], ["Goswami", "S.", ""], ["Grant", "D.", ""], ["Gr\u00e9goire", "T.", ""], ["Griffith", "Z.", ""], ["Griswold", "S.", ""], ["G\u00fcnd\u00fcz", "M.", ""], ["Haack", "C.", ""], ["Hallgren", "A.", ""], ["Halliday", "R.", ""], ["Halve", "L.", ""], ["Halzen", "F.", ""], ["Minh", "M. Ha", ""], ["Hanson", "K.", ""], ["Hardin", "J.", ""], ["Harnisch", "A. A.", ""], ["Haungs", "A.", ""], ["Hauser", "S.", ""], ["Hebecker", "D.", ""], ["Helbing", "K.", ""], ["Henningsen", "F.", ""], ["Hettinger", "E. C.", ""], ["Hickford", "S.", ""], ["Hignight", "J.", ""], ["Hill", "C.", ""], ["Hill", "G. C.", ""], ["Hoffman", "K. D.", ""], ["Hoffmann", "R.", ""], ["Hoinka", "T.", ""], ["Hokanson-Fasig", "B.", ""], ["Hoshina", "K.", ""], ["Huang", "F.", ""], ["Huber", "M.", ""], ["Huber", "T.", ""], ["Hultqvist", "K.", ""], ["H\u00fcnnefeld", "M.", ""], ["Hussain", "R.", ""], ["In", "S.", ""], ["Iovine", "N.", ""], ["Ishihara", "A.", ""], ["Jansson", "M.", ""], ["Japaridze", "G. S.", ""], ["Jeong", "M.", ""], ["Jones", "B. J. P.", ""], ["Joppe", "R.", ""], ["Kang", "D.", ""], ["Kang", "W.", ""], ["Kang", "X.", ""], ["Kappes", "A.", ""], ["Kappesser", "D.", ""], ["Karg", "T.", ""], ["Karl", "M.", ""], ["Karle", "A.", ""], ["Katz", "U.", ""], ["Kauer", "M.", ""], ["Kellermann", "M.", ""], ["Kelley", "J. L.", ""], ["Kheirandish", "A.", ""], ["Kim", "J.", ""], ["Kin", "K.", ""], ["Kintscher", "T.", ""], ["Kiryluk", "J.", ""], ["Klein", "S. R.", ""], ["Koirala", "R.", ""], ["Kolanoski", "H.", ""], ["K\u00f6pke", "L.", ""], ["Kopper", "C.", ""], ["Kopper", "S.", ""], ["Koskinen", "D. J.", ""], ["Koundal", "P.", ""], ["Kovacevich", "M.", ""], ["Kowalski", "M.", ""], ["Krings", "K.", ""], ["Kr\u00fcckl", "G.", ""], ["Kurahashi", "N.", ""], ["Kyriacou", "A.", ""], ["Gualda", "C. Lagunas", ""], ["Lanfranchi", "J. L.", ""], ["Larson", "M. J.", ""], ["Lauber", "F.", ""], ["Lazar", "J. P.", ""], ["Leonard", "K.", ""], ["Leszczy\u0144ska", "A.", ""], ["Li", "Y.", ""], ["Liu", "Q. R.", ""], ["Lohfink", "E.", ""], ["Mariscal", "C. J. Lozano", ""], ["Lu", "L.", ""], ["Lucarelli", "F.", ""], ["Ludwig", "A.", ""], ["Luszczak", "W.", ""], ["Lyu", "Y.", ""], ["Ma", "W. Y.", ""], ["Madsen", "J.", ""], ["Mahn", "K. B. M.", ""], ["Makino", "Y.", ""], ["Mallik", "P.", ""], ["Mancina", "S.", ""], ["Mari{\u015f}", "I. C.", ""], ["Maruyama", "R.", ""], ["Mase", "K.", ""], ["McNally", "F.", ""], ["Meagher", "K.", ""], ["Medina", "A.", ""], ["Meier", "M.", ""], ["Meighen-Berger", "S.", ""], ["Merz", "J.", ""], ["Micallef", "J.", ""], ["Mockler", "D.", ""], ["Moment\u00e9", "G.", ""], ["Montaruli", "T.", ""], ["Moore", "R. W.", ""], ["Morik", "K.", ""], ["Morse", "R.", ""], ["Moulai", "M.", ""], ["Naab", "R.", ""], ["Nagai", "R.", ""], ["Naumann", "U.", ""], ["Necker", "J.", ""], ["Nguy{\\~{\u00ea}}n", "L. V.", ""], ["Niederhausen", "H.", ""], ["Nisa", "M. U.", ""], ["Nowicki", "S. C.", ""], ["Nygren", "D. R.", ""], ["Pollmann", "A. Obertacke", ""], ["Oehler", "M.", ""], ["Olivas", "A.", ""], ["O'Sullivan", "E.", ""], ["Pandya", "H.", ""], ["Pankova", "D. V.", ""], ["Park", "N.", ""], ["Parker", "G. K.", ""], ["Paudel", "E. N.", ""], ["Peiffer", "P.", ""], ["Heros", "C. P\u00e9rez de los", ""], ["Philippen", "S.", ""], ["Pieloth", "D.", ""], ["Pieper", "S.", ""], ["Pizzuto", "A.", ""], ["Plum", "M.", ""], ["Popovych", "Y.", ""], ["Porcelli", "A.", ""], ["Rodriguez", "M. Prado", ""], ["Price", "P. B.", ""], ["Pries", "B.", ""], ["Przybylski", "G. T.", ""], ["Raab", "C.", ""], ["Raissi", "A.", ""], ["Rameez", "M.", ""], ["Rawlins", "K.", ""], ["Rea", "I. C.", ""], ["Rehman", "A.", ""], ["Reimann", "R.", ""], ["Renschler", "M.", ""], ["Renzi", "G.", ""], ["Resconi", "E.", ""], ["Reusch", "S.", ""], ["Rhode", "W.", ""], ["Richman", "M.", ""], ["Riedel", "B.", ""], ["Robertson", "S.", ""], ["Roellinghoff", "G.", ""], ["Rongen", "M.", ""], ["Rott", "C.", ""], ["Ruhe", "T.", ""], ["Ryckbosch", "D.", ""], ["Cantu", "D. Rysewyk", ""], ["Safa", "I.", ""], ["Herrera", "S. E. Sanchez", ""], ["Sandrock", "A.", ""], ["Sandroos", "J.", ""], ["Santander", "M.", ""], ["Sarkar", "S.", ""], ["Sarkar", "S.", ""], ["Satalecka", "K.", ""], ["Scharf", "M.", ""], ["Schaufel", "M.", ""], ["Schieler", "H.", ""], ["Schlunder", "P.", ""], ["Schmidt", "T.", ""], ["Schneider", "A.", ""], ["Schneider", "J.", ""], ["Schr\u00f6der", "F. G.", ""], ["Schumacher", "L.", ""], ["Sclafani", "S.", ""], ["Seckel", "D.", ""], ["Seunarine", "S.", ""], ["Sharma", "A.", ""], ["Shefali", "S.", ""], ["Silva", "M.", ""], ["Skrzypek", "B.", ""], ["Smithers", "B.", ""], ["Snihur", "R.", ""], ["Soedingrekso", "J.", ""], ["Soldin", "D.", ""], ["Spiczak", "G. M.", ""], ["Spiering", "C.", ""], ["Stachurska", "J.", ""], ["Stamatikos", "M.", ""], ["Stanev", "T.", ""], ["Stein", "R.", ""], ["Stettner", "J.", ""], ["Steuer", "A.", ""], ["Stezelberger", "T.", ""], ["Stokstad", "R. G.", ""], ["St\u00fcrwald", "T.", ""], ["Stuttard", "T.", ""], ["Sullivan", "G. W.", ""], ["Taboada", "I.", ""], ["Tenholt", "F.", ""], ["Ter-Antonyan", "S.", ""], ["Tilav", "S.", ""], ["Tischbein", "F.", ""], ["Tollefson", "K.", ""], ["Tomankova", "L.", ""], ["T\u00f6nnis", "C.", ""], ["Toscano", "S.", ""], ["Tosi", "D.", ""], ["Trettin", "A.", ""], ["Tselengidou", "M.", ""], ["Tung", "C. F.", ""], ["Turcati", "A.", ""], ["Turcotte", "R.", ""], ["Turley", "C. F.", ""], ["Twagirayezu", "J. P.", ""], ["Ty", "B.", ""], ["Elorrieta", "M. A. Unland", ""], ["Valtonen-Mattila", "N.", ""], ["Vandenbroucke", "J.", ""], ["van Eijk", "D.", ""], ["van Eijndhoven", "N.", ""], ["Vannerom", "D.", ""], ["van Santen", "J.", ""], ["Verpoest", "S.", ""], ["Vraeghe", "M.", ""], ["Walck", "C.", ""], ["Wallace", "A.", ""], ["Watson", "T. B.", ""], ["Weaver", "C.", ""], ["Weindl", "A.", ""], ["Weiss", "M. J.", ""], ["Weldert", "J.", ""], ["Wendt", "C.", ""], ["Werthebach", "J.", ""], ["Weyrauch", "M.", ""], ["Whelan", "B. J.", ""], ["Whitehorn", "N.", ""], ["Wiebe", "K.", ""], ["Wiebusch", "C. H.", ""], ["Williams", "D. R.", ""], ["Wolf", "M.", ""], ["Woschnagg", "K.", ""], ["Wrede", "G.", ""], ["Wulff", "J.", ""], ["Xu", "X. W.", ""], ["Xu", "Y.", ""], ["Yanez", "J. P.", ""], ["Yoshida", "S.", ""], ["Yuan", "T.", ""], ["Zhang", "Z.", ""]]}, {"id": "2101.11605", "submitter": "Aravind Srinivas Lakshminarayanan", "authors": "Aravind Srinivas, Tsung-Yi Lin, Niki Parmar, Jonathon Shlens, Pieter\n  Abbeel, Ashish Vaswani", "title": "Bottleneck Transformers for Visual Recognition", "comments": "Technical Report, 20 pages, 13 figures, 19 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present BoTNet, a conceptually simple yet powerful backbone architecture\nthat incorporates self-attention for multiple computer vision tasks including\nimage classification, object detection and instance segmentation. By just\nreplacing the spatial convolutions with global self-attention in the final\nthree bottleneck blocks of a ResNet and no other changes, our approach improves\nupon the baselines significantly on instance segmentation and object detection\nwhile also reducing the parameters, with minimal overhead in latency. Through\nthe design of BoTNet, we also point out how ResNet bottleneck blocks with\nself-attention can be viewed as Transformer blocks. Without any bells and\nwhistles, BoTNet achieves 44.4% Mask AP and 49.7% Box AP on the COCO Instance\nSegmentation benchmark using the Mask R-CNN framework; surpassing the previous\nbest published single model and single scale results of ResNeSt evaluated on\nthe COCO validation set. Finally, we present a simple adaptation of the BoTNet\ndesign for image classification, resulting in models that achieve a strong\nperformance of 84.7% top-1 accuracy on the ImageNet benchmark while being up to\n2.33x faster in compute time than the popular EfficientNet models on TPU-v3\nhardware. We hope our simple and effective approach will serve as a strong\nbaseline for future research in self-attention models for vision.\n", "versions": [{"version": "v1", "created": "Wed, 27 Jan 2021 18:55:27 GMT"}], "update_date": "2021-01-28", "authors_parsed": [["Srinivas", "Aravind", ""], ["Lin", "Tsung-Yi", ""], ["Parmar", "Niki", ""], ["Shlens", "Jonathon", ""], ["Abbeel", "Pieter", ""], ["Vaswani", "Ashish", ""]]}, {"id": "2101.11614", "submitter": "Donghyun Kim", "authors": "Donghyun Kim", "title": "Predicting Participation in Cancer Screening Programs with Machine\n  Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.OT cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present machine learning models based on random forest\nclassifiers, support vector machines, gradient boosted decision trees, and\nartificial neural networks to predict participation in cancer screening\nprograms in South Korea. The top performing model was based on gradient boosted\ndecision trees and achieved an area under the receiver operating characteristic\ncurve (AUC-ROC) of 0.8706 and average precision of 0.8776. The results of this\nstudy are encouraging and suggest that with further research, these models can\nbe directly applied to Korea's healthcare system, thus increasing participation\nin Korea's National Cancer Screening Program.\n", "versions": [{"version": "v1", "created": "Wed, 27 Jan 2021 11:05:46 GMT"}], "update_date": "2021-01-29", "authors_parsed": [["Kim", "Donghyun", ""]]}, {"id": "2101.11653", "submitter": "Mahdi Soleymani", "authors": "Mahdi Soleymani, Ramy E. Ali, Hessam Mahdavifar, A. Salman Avestimehr", "title": "List-Decodable Coded Computing: Breaking the Adversarial Toleration\n  Barrier", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.DC cs.LG math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of coded computing where a computational task is\nperformed in a distributed fashion in the presence of adversarial workers. We\npropose techniques to break the adversarial toleration threshold barrier\npreviously known in coded computing. More specifically, we leverage\nlist-decoding techniques for folded Reed-Solomon (FRS) codes and propose novel\nalgorithms to recover the correct codeword using side information. In the coded\ncomputing setting, we show how the master node can perform certain carefully\ndesigned extra computations in order to obtain the side information. This side\ninformation will be then utilized to prune the output of list decoder in order\nto uniquely recover the true outcome. We further propose folded Lagrange coded\ncomputing, referred to as folded LCC or FLCC, to incorporate the developed\ntechniques into a specific coded computing setting. Our results show that FLCC\noutperforms LCC by breaking the barrier on the number of adversaries that can\nbe tolerated. In particular, the corresponding threshold in FLCC is improved by\na factor of two compared to that of LCC.\n", "versions": [{"version": "v1", "created": "Wed, 27 Jan 2021 19:17:33 GMT"}], "update_date": "2021-01-29", "authors_parsed": [["Soleymani", "Mahdi", ""], ["Ali", "Ramy E.", ""], ["Mahdavifar", "Hessam", ""], ["Avestimehr", "A. Salman", ""]]}, {"id": "2101.11656", "submitter": "Sayan Ghosal", "authors": "Sayan Ghosal, Qiang Chen, Giulio Pergola, Aaron L. Goldman, William\n  Ulrich, Karen F. Berman, Giuseppe Blasi, Leonardo Fazio, Antonio Rampino,\n  Alessandro Bertolino, Daniel R. Weinberger, Venkata S. Mattay, and Archana\n  Venkataraman", "title": "G-MIND: An End-to-End Multimodal Imaging-Genetics Framework for\n  Biomarker Identification and Disease Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.QM cs.LG eess.IV", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  We propose a novel deep neural network architecture to integrate imaging and\ngenetics data, as guided by diagnosis, that provides interpretable biomarkers.\nOur model consists of an encoder, a decoder and a classifier. The encoder\nlearns a non-linear subspace shared between the input data modalities. The\nclassifier and the decoder act as regularizers to ensure that the\nlow-dimensional encoding captures predictive differences between patients and\ncontrols. We use a learnable dropout layer to extract interpretable biomarkers\nfrom the data, and our unique training strategy can easily accommodate missing\ndata modalities across subjects. We have evaluated our model on a population\nstudy of schizophrenia that includes two functional MRI (fMRI) paradigms and\nSingle Nucleotide Polymorphism (SNP) data. Using 10-fold cross validation, we\ndemonstrate that our model achieves better classification accuracy than\nbaseline methods, and that this performance generalizes to a second dataset\ncollected at a different site. In an exploratory analysis we further show that\nthe biomarkers identified by our model are closely associated with the\nwell-documented deficits in schizophrenia.\n", "versions": [{"version": "v1", "created": "Wed, 27 Jan 2021 19:28:04 GMT"}], "update_date": "2021-01-29", "authors_parsed": [["Ghosal", "Sayan", ""], ["Chen", "Qiang", ""], ["Pergola", "Giulio", ""], ["Goldman", "Aaron L.", ""], ["Ulrich", "William", ""], ["Berman", "Karen F.", ""], ["Blasi", "Giuseppe", ""], ["Fazio", "Leonardo", ""], ["Rampino", "Antonio", ""], ["Bertolino", "Alessandro", ""], ["Weinberger", "Daniel R.", ""], ["Mattay", "Venkata S.", ""], ["Venkataraman", "Archana", ""]]}, {"id": "2101.11665", "submitter": "Sebastian Farquhar", "authors": "Sebastian Farquhar, Yarin Gal, Tom Rainforth", "title": "On Statistical Bias In Active Learning: How and When To Fix It", "comments": "Published at ICLR 2021 (Spotlight)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Active learning is a powerful tool when labelling data is expensive, but it\nintroduces a bias because the training data no longer follows the population\ndistribution. We formalize this bias and investigate the situations in which it\ncan be harmful and sometimes even helpful. We further introduce novel\ncorrective weights to remove bias when doing so is beneficial. Through this,\nour work not only provides a useful mechanism that can improve the active\nlearning approach, but also an explanation of the empirical successes of\nvarious existing approaches which ignore this bias. In particular, we show that\nthis bias can be actively helpful when training overparameterized models --\nlike neural networks -- with relatively little data.\n", "versions": [{"version": "v1", "created": "Wed, 27 Jan 2021 19:52:24 GMT"}, {"version": "v2", "created": "Mon, 31 May 2021 11:46:48 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Farquhar", "Sebastian", ""], ["Gal", "Yarin", ""], ["Rainforth", "Tom", ""]]}, {"id": "2101.11684", "submitter": "Gurpreet Singh", "authors": "Gurpreet Singh, Soumyajit Gupta, Matthew Lease, Clint Dawson", "title": "A Hybrid 2-stage Neural Optimization for Pareto Front Extraction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.OC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Classification, recommendation, and ranking problems often involve competing\ngoals with additional constraints (e.g., to satisfy fairness or diversity\ncriteria). Such optimization problems are quite challenging, often involving\nnon-convex functions along with considerations of user preferences in balancing\ntrade-offs. Pareto solutions represent optimal frontiers for jointly optimizing\nmultiple competing objectives. A major obstacle for frequently used\nlinear-scalarization strategies is that the resulting optimization problem\nmight not always converge to a global optimum. Furthermore, such methods only\nreturn one solution point per run. A Pareto solution set is a subset of all\nsuch global optima over multiple runs for different trade-off choices.\nTherefore, a Pareto front can only be guaranteed with multiple runs of the\nlinear-scalarization problem, where all runs converge to their respective\nglobal optima. Consequently, extracting a Pareto front for practical problems\nis computationally intractable with substantial computational overheads,\nlimited scalability, and reduced accuracy. We propose a robust, low cost,\ntwo-stage, hybrid neural Pareto optimization approach that is accurate and\nscales (compute space and time) with data dimensions, as well as number of\nfunctions and constraints. The first stage (neural network) efficiently\nextracts a weak Pareto front, using Fritz-John conditions as the discriminator,\nwith no assumptions of convexity on the objectives or constraints. The second\nstage (efficient Pareto filter) extracts the strong Pareto optimal subset given\nthe weak front from stage 1. Fritz-John conditions provide us with theoretical\nbounds on approximation error between the true and network extracted weak\nPareto front. Numerical experiments demonstrates the accuracy and efficiency on\na canonical set of benchmark problems and a fairness optimization task from\nprior works.\n", "versions": [{"version": "v1", "created": "Wed, 27 Jan 2021 20:56:19 GMT"}, {"version": "v2", "created": "Sat, 13 Feb 2021 17:03:10 GMT"}], "update_date": "2021-02-16", "authors_parsed": [["Singh", "Gurpreet", ""], ["Gupta", "Soumyajit", ""], ["Lease", "Matthew", ""], ["Dawson", "Clint", ""]]}, {"id": "2101.11685", "submitter": "Rasul Karimov", "authors": "Rasul Karimov, Yury Malkov, Karim Iskakov, Victor Lempitsky", "title": "CNN with large memory layers", "comments": "Master's dissertation paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This work is centred around the recently proposed product key memory\nstructure \\cite{large_memory}, implemented for a number of computer vision\napplications. The memory structure can be regarded as a simple computation\nprimitive suitable to be augmented to nearly all neural network architectures.\nThe memory block allows implementing sparse access to memory with square root\ncomplexity scaling with respect to the memory capacity. The latter scaling is\npossible due to the incorporation of Cartesian product space decomposition of\nthe key space for the nearest neighbour search. We have tested the memory layer\non the classification, image reconstruction and relocalization problems and\nfound that for some of those, the memory layers can provide significant\nspeed/accuracy improvement with the high utilization of the key-value elements,\nwhile others require more careful fine-tuning and suffer from dying keys. To\ntackle the later problem we have introduced a simple technique of memory\nre-initialization which helps us to eliminate unused key-value pairs from the\nmemory and engage them in training again. We have conducted various experiments\nand got improvements in speed and accuracy for classification and PoseNet\nrelocalization models.\n  We showed that the re-initialization has a huge impact on a toy example of\nrandomly labeled data and observed some gains in performance on the image\nclassification task. We have also demonstrated the generalization property\nperseverance of the large memory layers on the relocalization problem, while\nobserving the spatial correlations between the images and the selected memory\ncells.\n", "versions": [{"version": "v1", "created": "Wed, 27 Jan 2021 20:58:20 GMT"}, {"version": "v2", "created": "Mon, 26 Apr 2021 09:42:58 GMT"}], "update_date": "2021-04-27", "authors_parsed": [["Karimov", "Rasul", ""], ["Malkov", "Yury", ""], ["Iskakov", "Karim", ""], ["Lempitsky", "Victor", ""]]}, {"id": "2101.11688", "submitter": "Leonard Schulman", "authors": "Spencer L. Gordon, Leonard J. Schulman", "title": "Hadamard Extensions and the Identification of Mixtures of Product\n  Distributions", "comments": "V2: re-titled and slight edits", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG eess.SP stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The Hadamard Extension of a matrix is the matrix consisting of all Hadamard\nproducts of subsets of its rows. This construction arises in the context of\nidentifying a mixture of product distributions on binary random variables: full\ncolumn rank of such extensions is a necessary ingredient of identification\nalgorithms. We provide several results concerning when a Hadamard Extension has\nfull column rank.\n", "versions": [{"version": "v1", "created": "Wed, 27 Jan 2021 21:07:54 GMT"}, {"version": "v2", "created": "Fri, 12 Feb 2021 20:34:41 GMT"}], "update_date": "2021-02-16", "authors_parsed": [["Gordon", "Spencer L.", ""], ["Schulman", "Leonard J.", ""]]}, {"id": "2101.11693", "submitter": "Mohammad Malekzadeh", "authors": "Mohammad Malekzadeh, Burak Hasircioglu, Nitish Mital, Kunal Katarya,\n  Mehmet Emre Ozfatura, Deniz G\\\"und\\\"uz", "title": "Dopamine: Differentially Private Federated Learning on Medical Data", "comments": "The Second AAAI Workshop on Privacy-Preserving Artificial\n  Intelligence (PPAI-21)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While rich medical datasets are hosted in hospitals distributed across the\nworld, concerns on patients' privacy is a barrier against using such data to\ntrain deep neural networks (DNNs) for medical diagnostics. We propose Dopamine,\na system to train DNNs on distributed datasets, which employs federated\nlearning (FL) with differentially-private stochastic gradient descent (DPSGD),\nand, in combination with secure aggregation, can establish a better trade-off\nbetween differential privacy (DP) guarantee and DNN's accuracy than other\napproaches. Results on a diabetic retinopathy~(DR) task show that Dopamine\nprovides a DP guarantee close to the centralized training counterpart, while\nachieving a better classification accuracy than FL with parallel DP where DPSGD\nis applied without coordination. Code is available at\nhttps://github.com/ipc-lab/private-ml-for-health.\n", "versions": [{"version": "v1", "created": "Wed, 27 Jan 2021 21:27:23 GMT"}, {"version": "v2", "created": "Fri, 29 Jan 2021 16:40:17 GMT"}], "update_date": "2021-02-01", "authors_parsed": [["Malekzadeh", "Mohammad", ""], ["Hasircioglu", "Burak", ""], ["Mital", "Nitish", ""], ["Katarya", "Kunal", ""], ["Ozfatura", "Mehmet Emre", ""], ["G\u00fcnd\u00fcz", "Deniz", ""]]}, {"id": "2101.11702", "submitter": "Domen Vre\\v{s}", "authors": "Domen Vre\\v{s} and Marko Robnik \\v{S}ikonja", "title": "Better sampling in explanation methods can prevent dieselgate-like\n  deception", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Machine learning models are used in many sensitive areas where besides\npredictive accuracy their comprehensibility is also important. Interpretability\nof prediction models is necessary to determine their biases and causes of\nerrors, and is a necessary prerequisite for users' confidence. For complex\nstate-of-the-art black-box models post-hoc model-independent explanation\ntechniques are an established solution. Popular and effective techniques, such\nas IME, LIME, and SHAP, use perturbation of instance features to explain\nindividual predictions. Recently, Slack et al. (2020) put their robustness into\nquestion by showing that their outcomes can be manipulated due to poor\nperturbation sampling employed. This weakness would allow dieselgate type\ncheating of owners of sensitive models who could deceive inspection and hide\npotentially unethical or illegal biases existing in their predictive models.\nThis could undermine public trust in machine learning models and give rise to\nlegal restrictions on their use.\n  We show that better sampling in these explanation methods prevents malicious\nmanipulations. The proposed sampling uses data generators that learn the\ntraining set distribution and generate new perturbation instances much more\nsimilar to the training set. We show that the improved sampling increases the\nrobustness of the LIME and SHAP, while previously untested method IME is\nalready the most robust of all.\n", "versions": [{"version": "v1", "created": "Tue, 26 Jan 2021 13:41:37 GMT"}], "update_date": "2021-01-29", "authors_parsed": [["Vre\u0161", "Domen", ""], ["\u0160ikonja", "Marko Robnik", ""]]}, {"id": "2101.11703", "submitter": "Hongjie Zhang", "authors": "Hongjie Zhang", "title": "A Unified Framework for Feature Extraction based on Contrastive Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Feature extraction is an efficient approach for alleviating the curse of\ndimensionality in high-dimensional data. With the development of contrastive\nlearning in the field of self-supervised learning, we propose a unified\nframework for feature extraction based on contrastive learning from a new\nperspective, which is suitable for both unsupervised and supervised feature\nextraction. In this framework, we first construct a contrastive learning graph\nbased on graph embedding (GE), which proposes a new way to define positive and\nnegative pairs. Then, we solve the projection matrix by minimizing the\ncontrastive loss function. In this framework, we can consider not only similar\nsamples but also dissimilar samples on the basis of unsupervised GE, so as to\nnarrow the gap with supervised feature extraction. In order to verify the\neffectiveness of our proposed framework for unsupervised and supervised feature\nextraction, we improved the unsupervised GE method LPP with local preserving,\nthe supervised GE method LDA without local preserving, and the supervised GE\nmethod LFDA with local preserving, and proposed CL-LPP, CL-LDA, and CL-LFDA,\nrespectively. Finally, we performed numerical experiments on five real\ndatasets.\n", "versions": [{"version": "v1", "created": "Mon, 25 Jan 2021 16:43:03 GMT"}], "update_date": "2021-01-29", "authors_parsed": [["Zhang", "Hongjie", ""]]}, {"id": "2101.11704", "submitter": "Mahsa Shafaei", "authors": "Mahsa Shafaei, Christos Smailis, Ioannis A. Kakadiaris, Thamar Solorio", "title": "A Case Study of Deep Learning Based Multi-Modal Methods for Predicting\n  the Age-Suitability Rating of Movie Trailers", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.MM cs.SD eess.AS eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we explore different approaches to combine modalities for the\nproblem of automated age-suitability rating of movie trailers. First, we\nintroduce a new dataset containing videos of movie trailers in English\ndownloaded from IMDB and YouTube, along with their corresponding\nage-suitability rating labels. Secondly, we propose a multi-modal deep learning\npipeline addressing the movie trailer age suitability rating problem. This is\nthe first attempt to combine video, audio, and speech information for this\nproblem, and our experimental results show that multi-modal approaches\nsignificantly outperform the best mono and bimodal models in this task.\n", "versions": [{"version": "v1", "created": "Tue, 26 Jan 2021 17:15:35 GMT"}], "update_date": "2021-01-29", "authors_parsed": [["Shafaei", "Mahsa", ""], ["Smailis", "Christos", ""], ["Kakadiaris", "Ioannis A.", ""], ["Solorio", "Thamar", ""]]}, {"id": "2101.11710", "submitter": "James Powell", "authors": "James Powell and Kari Sentz", "title": "Tracking Short-Term Temporal Linguistic Dynamics to Characterize\n  Candidate Therapeutics for COVID-19 in the CORD-19 Corpus", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.OT cs.CL cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Scientific literature tends to grow as a function of funding and interest in\na given field. Mining such literature can reveal trends that may not be\nimmediately apparent. The CORD-19 corpus represents a growing corpus of\nscientific literature associated with COVID-19. We examined the intersection of\na set of candidate therapeutics identified in a drug-repurposing study with\ntemporal instances of the CORD-19 corpus to determine if it was possible to\nfind and measure changes associated with them over time. We propose that the\ntechniques we used could form the basis of a tool to pre-screen new candidate\ntherapeutics early in the research process.\n", "versions": [{"version": "v1", "created": "Sat, 9 Jan 2021 23:24:05 GMT"}], "update_date": "2021-01-29", "authors_parsed": [["Powell", "James", ""], ["Sentz", "Kari", ""]]}, {"id": "2101.11711", "submitter": "Nikolaos Dervilis Dr", "authors": "Kartik Chandrasekhar, Nevena Stevanovic, Elizabeth J. Cross, Nikolaos\n  Dervilis, Keith Worden", "title": "Damage detection in operational wind turbine blades using a new approach\n  based on machine learning", "comments": null, "journal-ref": "This is an author produced version of a paper subsequently\n  published in Renewable Energy, Elsevier, 2021. Uploaded in accordance with\n  the publisher's self-archiving policy", "doi": "10.1016/j.renene.2020.12.119", "report-no": null, "categories": "cs.LG physics.data-an", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The application of reliable structural health monitoring (SHM) technologies\nto operational wind turbine blades is a challenging task, due to the uncertain\nnature of the environments they operate in. In this paper, a novel SHM\nmethodology, which uses Gaussian Processes (GPs) is proposed. The methodology\ntakes advantage of the fact that the blades on a turbine are nominally\nidentical in structural properties and encounter the same environmental and\noperational variables (EOVs). The properties of interest are the first edgewise\nfrequencies of the blades. The GPs are used to predict the edge frequencies of\none blade given that of another, after these relationships between the pairs of\nblades have been learned when the blades are in a healthy state. In using this\napproach, the proposed SHM methodology is able to identify when the blades\nstart behaving differently from one another over time. To validate the concept,\nthe proposed SHM system is applied to real onshore wind turbine blade data,\nwhere some form of damage was known to have taken place. X-bar control chart\nanalysis of the residual errors between the GP predictions and actual\nfrequencies show that the system successfully identified early onset of damage\nas early as six months before it was identified and remedied.\n", "versions": [{"version": "v1", "created": "Mon, 25 Jan 2021 21:56:33 GMT"}], "update_date": "2021-01-29", "authors_parsed": [["Chandrasekhar", "Kartik", ""], ["Stevanovic", "Nevena", ""], ["Cross", "Elizabeth J.", ""], ["Dervilis", "Nikolaos", ""], ["Worden", "Keith", ""]]}, {"id": "2101.11712", "submitter": "Adrian Buganza Tepole", "authors": "Yue Leng, Sarah Calve, Adrian Buganza Tepole", "title": "Predicting the Mechanical Properties of Fibrin Using Neural Networks\n  Trained on Discrete Fiber Network Data", "comments": "20 pages, 9 figures, for associated files please see\n  https://bitbucket.org/buganzalab/nn_rve/src/master/", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.QM cs.CE cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fibrin is a structural protein key for processes such as wound healing and\nthrombus formation. At the macroscale, fibrin forms a gel and has a mechanical\nresponse that is dictated by the mechanics of a microscale fiber network.\nHence, accurate description of fibrin gels can be achieved using representative\nvolume elements (RVE) that explicitly model the discrete fiber networks of the\nmicroscale. These RVE models, however, cannot be efficiently used to model the\nmacroscale due to the challenges and computational demands of multiscale\ncoupling. Here, we propose the use of an artificial, fully connected neural\nnetwork (FCNN) to efficiently capture the behavior of the RVE models. The FCNN\nwas trained on 1100 fiber networks subjected to 121 biaxial deformations. The\nstress data from the RVE, together with the total energy on the fibers and the\ncondition of incompressibility of the surrounding matrix, were used to\ndetermine the derivatives of an unknown strain energy function with respect to\nthe deformation invariants. During training, the loss function was modified to\nensure convexity of the strain energy function and symmetry of its Hessian. A\ngeneral FCNN model was coded into a user material subroutine (UMAT) in the\nsoftware Abaqus. The UMAT implementation takes in the structure and parameters\nof an arbitrary FCNN as material parameters from the input file. The inputs to\nthe FCNN include the first two isochoric invariants of the deformation. The\nFCNN outputs the derivatives of the strain energy with respect to the isochoric\ninvariants. In this work, the FCNN trained on the discrete fiber network data\nwas used in finite element simulations of fibrin gels using our UMAT. We\nanticipate that this work will enable further integration of machine learning\ntools with computational mechanics. It will also improve computational modeling\nof biological materials characterized by a multiscale structure.\n", "versions": [{"version": "v1", "created": "Sat, 23 Jan 2021 23:52:33 GMT"}], "update_date": "2021-01-29", "authors_parsed": [["Leng", "Yue", ""], ["Calve", "Sarah", ""], ["Tepole", "Adrian Buganza", ""]]}, {"id": "2101.11713", "submitter": "Loris Nanni", "authors": "Loris Nanni, Alessandra Lumini and Sheryl Brahnam", "title": "Neural networks for Anatomical Therapeutic Chemical (ATC)", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.QM cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Motivation: Automatic Anatomical Therapeutic Chemical (ATC) classification is\na critical and highly competitive area of research in bioinformatics because of\nits potential for expediting drug develop-ment and research. Predicting an\nunknown compound's therapeutic and chemical characteristics ac-cording to how\nthese characteristics affect multiple organs/systems makes automatic ATC\nclassifica-tion a challenging multi-label problem. Results: In this work, we\npropose combining multiple multi-label classifiers trained on distinct sets of\nfeatures, including sets extracted from a Bidirectional Long Short-Term Memory\nNetwork (BiLSTM). Experiments demonstrate the power of this approach, which is\nshown to outperform the best methods reported in the literature, including the\nstate-of-the-art developed by the fast.ai research group. Availability: All\nsource code developed for this study is available at\nhttps://github.com/LorisNanni. Contact: loris.nanni@unipd.it\n", "versions": [{"version": "v1", "created": "Fri, 22 Jan 2021 19:49:47 GMT"}, {"version": "v2", "created": "Thu, 1 Jul 2021 23:06:44 GMT"}], "update_date": "2021-07-05", "authors_parsed": [["Nanni", "Loris", ""], ["Lumini", "Alessandra", ""], ["Brahnam", "Sheryl", ""]]}, {"id": "2101.11714", "submitter": "Bilge Acun", "authors": "Chunxing Yin and Bilge Acun and Xing Liu and Carole-Jean Wu", "title": "TT-Rec: Tensor Train Compression for Deep Learning Recommendation Models", "comments": "To appear in Conference on Machine Learning and Systems (MlSys 2021,\n  https://mlsys.org/)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The memory capacity of embedding tables in deep learning recommendation\nmodels (DLRMs) is increasing dramatically from tens of GBs to TBs across the\nindustry. Given the fast growth in DLRMs, novel solutions are urgently needed,\nin order to enable fast and efficient DLRM innovations. At the same time, this\nmust be done without having to exponentially increase infrastructure capacity\ndemands. In this paper, we demonstrate the promising potential of Tensor Train\ndecomposition for DLRMs (TT-Rec), an important yet under-investigated context.\nWe design and implement optimized kernels (TT-EmbeddingBag) to evaluate the\nproposed TT-Rec design. TT-EmbeddingBag is 3 times faster than the SOTA TT\nimplementation. The performance of TT-Rec is further optimized with the batched\nmatrix multiplication and caching strategies for embedding vector lookup\noperations. In addition, we present mathematically and empirically the effect\nof weight initialization distribution on DLRM accuracy and propose to\ninitialize the tensor cores of TT-Rec following the sampled Gaussian\ndistribution. We evaluate TT-Rec across three important design space dimensions\n-- memory capacity, accuracy, and timing performance -- by training MLPerf-DLRM\nwith Criteo's Kaggle and Terabyte data sets. TT-Rec achieves 117 times and 112\ntimes model size compression, for Kaggle and Terabyte, respectively. This\nimpressive model size reduction can come with no accuracy nor training time\noverhead as compared to the uncompressed baseline.\n", "versions": [{"version": "v1", "created": "Mon, 25 Jan 2021 23:19:03 GMT"}], "update_date": "2021-01-29", "authors_parsed": [["Yin", "Chunxing", ""], ["Acun", "Bilge", ""], ["Liu", "Xing", ""], ["Wu", "Carole-Jean", ""]]}, {"id": "2101.11715", "submitter": "Ning Ge", "authors": "Ning Ge, Guanghao Li, Li Zhang, Yi Liu Yi Liu", "title": "Failure Prediction in Production Line Based on Federated Learning: An\n  Empirical Study", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Data protection across organizations is limiting the application of\ncentralized learning (CL) techniques. Federated learning (FL) enables multiple\nparticipants to build a learning model without sharing data. Nevertheless,\nthere are very few research works on FL in intelligent manufacturing. This\npaper presents the results of an empirical study on failure prediction in the\nproduction line based on FL. This paper (1) designs Federated Support Vector\nMachine (FedSVM) and Federated Random Forest (FedRF) algorithms for the\nhorizontal FL and vertical FL scenarios, respectively; (2) proposes an\nexperiment process for evaluating the effectiveness between the FL and CL\nalgorithms; (3) finds that the performance of FL and CL are not significantly\ndifferent on the global testing data, on the random partial testing data, and\non the estimated unknown Bosch data, respectively. The fact that the testing\ndata is heterogeneous enhances our findings. Our study reveals that FL can\nreplace CL for failure prediction.\n", "versions": [{"version": "v1", "created": "Mon, 25 Jan 2021 10:27:19 GMT"}], "update_date": "2021-01-29", "authors_parsed": [["Ge", "Ning", ""], ["Li", "Guanghao", ""], ["Zhang", "Li", ""], ["Liu", "Yi Liu Yi", ""]]}, {"id": "2101.11716", "submitter": "Cezary Kaliszyk", "authors": "Dennis M\\\"uller and Cezary Kaliszyk", "title": "Disambiguating Symbolic Expressions in Informal Documents", "comments": "ICLR 2021 conference paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose the task of disambiguating symbolic expressions in informal STEM\ndocuments in the form of LaTeX files - that is, determining their precise\nsemantics and abstract syntax tree - as a neural machine translation task. We\ndiscuss the distinct challenges involved and present a dataset with roughly\n33,000 entries. We evaluated several baseline models on this dataset, which\nfailed to yield even syntactically valid LaTeX before overfitting.\nConsequently, we describe a methodology using a transformer language model\npre-trained on sources obtained from arxiv.org, which yields promising results\ndespite the small size of the dataset. We evaluate our model using a plurality\nof dedicated techniques, taking the syntax and semantics of symbolic\nexpressions into account.\n", "versions": [{"version": "v1", "created": "Mon, 25 Jan 2021 10:14:37 GMT"}], "update_date": "2021-01-29", "authors_parsed": [["M\u00fcller", "Dennis", ""], ["Kaliszyk", "Cezary", ""]]}, {"id": "2101.11717", "submitter": "Francois Malgouyres", "authors": "Adrien Gauffriau, Fran\\c{c}ois Malgouyres (IMT), M\\'elanie Ducoffe", "title": "Overestimation learning with guarantees", "comments": null, "journal-ref": "AAAI-21, workshop on safeAI, Feb 2021, Valence (Virtual), Spain", "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe a complete method that learns a neural network which is\nguaranteed to overestimate a reference function on a given domain. The neural\nnetwork can then be used as a surrogate for the reference function. The method\ninvolves two steps. In the first step, we construct an adaptive set of Majoring\nPoints. In the second step, we optimize a well-chosen neural network to\noverestimate the Majoring Points. In order to extend the guarantee on the\nMajoring Points to the whole domain, we necessarily have to make an assumption\non the reference function. In this study, we assume that the reference function\nis monotonic. We provide experiments on synthetic and real problems. The\nexperiments show that the density of the Majoring Points concentrate where the\nreference function varies. The learned over-estimations are both guaranteed to\noverestimate the reference function and are proven empirically to provide good\napproximations of it. Experiments on real data show that the method makes it\npossible to use the surrogate function in embedded systems for which an\nunderestimation is critical; when computing the reference function requires too\nmany resources.\n", "versions": [{"version": "v1", "created": "Tue, 26 Jan 2021 09:06:03 GMT"}], "update_date": "2021-01-29", "authors_parsed": [["Gauffriau", "Adrien", "", "IMT"], ["Malgouyres", "Fran\u00e7ois", "", "IMT"], ["Ducoffe", "M\u00e9lanie", ""]]}, {"id": "2101.11718", "submitter": "Jwala Dhamala", "authors": "Jwala Dhamala, Tony Sun, Varun Kumar, Satyapriya Krishna, Yada\n  Pruksachatkun, Kai-Wei Chang, Rahul Gupta", "title": "BOLD: Dataset and Metrics for Measuring Biases in Open-Ended Language\n  Generation", "comments": null, "journal-ref": null, "doi": "10.1145/3442188.3445924", "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent advances in deep learning techniques have enabled machines to generate\ncohesive open-ended text when prompted with a sequence of words as context.\nWhile these models now empower many downstream applications from conversation\nbots to automatic storytelling, they have been shown to generate texts that\nexhibit social biases. To systematically study and benchmark social biases in\nopen-ended language generation, we introduce the Bias in Open-Ended Language\nGeneration Dataset (BOLD), a large-scale dataset that consists of 23,679\nEnglish text generation prompts for bias benchmarking across five domains:\nprofession, gender, race, religion, and political ideology. We also propose new\nautomated metrics for toxicity, psycholinguistic norms, and text gender\npolarity to measure social biases in open-ended text generation from multiple\nangles. An examination of text generated from three popular language models\nreveals that the majority of these models exhibit a larger social bias than\nhuman-written Wikipedia text across all domains. With these results we\nhighlight the need to benchmark biases in open-ended language generation and\ncaution users of language generation models on downstream tasks to be cognizant\nof these embedded prejudices.\n", "versions": [{"version": "v1", "created": "Wed, 27 Jan 2021 22:07:03 GMT"}], "update_date": "2021-01-29", "authors_parsed": [["Dhamala", "Jwala", ""], ["Sun", "Tony", ""], ["Kumar", "Varun", ""], ["Krishna", "Satyapriya", ""], ["Pruksachatkun", "Yada", ""], ["Chang", "Kai-Wei", ""], ["Gupta", "Rahul", ""]]}, {"id": "2101.11731", "submitter": "Eric Cosatto", "authors": "Eric Cosatto, Kyle Gerard, Hans-Peter Graf, Maki Ogura, Tomoharu\n  Kiyuna, Kanako C. Hatanaka, Yoshihiro Matsuno, Yutaka Hatanaka", "title": "A Multi-Scale Conditional Deep Model for Tumor Cell Ratio Counting", "comments": "To be published in SPIE Medical Imaging 2021 online proceedings", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  We propose a method to accurately obtain the ratio of tumor cells over an\nentire histological slide. We use deep fully convolutional neural network\nmodels trained to detect and classify cells on images of H&E-stained tissue\nsections. Pathologists' labels consisting of exhaustive nuclei locations and\ntumor regions were used to trained the model in a supervised fashion. We show\nthat combining two models, each working at a different magnification allows the\nsystem to capture both cell-level details and surrounding context to enable\nsuccessful detection and classification of cells as either tumor-cell or\nnormal-cell. Indeed, by conditioning the classification of a single cell on a\nmulti-scale context information, our models mimic the process used by\npathologists who assess cell neoplasticity and tumor extent at different\nmicroscope magnifications. The ratio of tumor cells can then be readily\nobtained by counting the number of cells in each class. To analyze an entire\nslide, we split it into multiple tiles that can be processed in parallel. The\noverall tumor cell ratio can then be aggregated. We perform experiments on a\ndataset of 100 slides with lung tumor specimens from both resection and tissue\nmicro-array (TMA). We train fully-convolutional models using heavy data\naugmentation and batch normalization. On an unseen test set, we obtain an\naverage mean absolute error on predicting the tumor cell ratio of less than 6%,\nwhich is significantly better than the human average of 20% and is key in\nproperly selecting tissue samples for recent genetic panel tests geared at\nprescribing targeted cancer drugs. We perform ablation studies to show the\nimportance of training two models at different magnifications and to justify\nthe choice of some parameters, such as the size of the receptive field.\n", "versions": [{"version": "v1", "created": "Wed, 27 Jan 2021 22:40:33 GMT"}], "update_date": "2021-01-29", "authors_parsed": [["Cosatto", "Eric", ""], ["Gerard", "Kyle", ""], ["Graf", "Hans-Peter", ""], ["Ogura", "Maki", ""], ["Kiyuna", "Tomoharu", ""], ["Hatanaka", "Kanako C.", ""], ["Matsuno", "Yoshihiro", ""], ["Hatanaka", "Yutaka", ""]]}, {"id": "2101.11744", "submitter": "Matthew Smart", "authors": "Matthew Smart, Anton Zilman", "title": "On the mapping between Hopfield networks and Restricted Boltzmann\n  Machines", "comments": "ICLR 2021 oral paper", "journal-ref": "The 9th International Conference on Learning Representations (ICLR\n  2021)", "doi": null, "report-no": null, "categories": "cs.LG cond-mat.dis-nn cond-mat.stat-mech", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hopfield networks (HNs) and Restricted Boltzmann Machines (RBMs) are two\nimportant models at the interface of statistical physics, machine learning, and\nneuroscience. Recently, there has been interest in the relationship between HNs\nand RBMs, due to their similarity under the statistical mechanics formalism. An\nexact mapping between HNs and RBMs has been previously noted for the special\ncase of orthogonal (uncorrelated) encoded patterns. We present here an exact\nmapping in the case of correlated pattern HNs, which are more broadly\napplicable to existing datasets. Specifically, we show that any HN with $N$\nbinary variables and $p<N$ arbitrary binary patterns can be transformed into an\nRBM with $N$ binary visible variables and $p$ gaussian hidden variables. We\noutline the conditions under which the reverse mapping exists, and conduct\nexperiments on the MNIST dataset which suggest the mapping provides a useful\ninitialization to the RBM weights. We discuss extensions, the potential\nimportance of this correspondence for the training of RBMs, and for\nunderstanding the performance of deep architectures which utilize RBMs.\n", "versions": [{"version": "v1", "created": "Wed, 27 Jan 2021 23:49:48 GMT"}, {"version": "v2", "created": "Sat, 6 Mar 2021 02:08:12 GMT"}], "update_date": "2021-03-09", "authors_parsed": [["Smart", "Matthew", ""], ["Zilman", "Anton", ""]]}, {"id": "2101.11745", "submitter": "Jorge Cipri\\'an S\\'anchez", "authors": "J. F. Cipri\\'an-S\\'anchez and G. Ochoa-Ruiz and M. Gonzalez-Mendoza\n  and L. Rossi", "title": "FIRe-GAN: A novel Deep Learning-based infrared-visible fusion method for\n  wildfire imagery", "comments": "16 pages, 10 figures. Submitted to the Special Issue (SI) in the\n  Neural Computing and Applications Journal", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Early wildfire detection is of paramount importance to avoid as much damage\nas possible to the environment, properties, and lives. Deep Learning (DL)\nmodels that can leverage both visible and infrared information have the\npotential to display state-of-the-art performance, with lower false-positive\nrates than existing techniques. However, most DL-based image fusion methods\nhave not been evaluated in the domain of fire imagery. Additionally, to the\nbest of our knowledge, no publicly available dataset contains visible-infrared\nfused fire images. There is a growing interest in DL-based image fusion\ntechniques due to their reduced complexity. Due to the latter, we select three\nstate-of-the-art, DL-based image fusion techniques and evaluate them for the\nspecific task of fire image fusion. We compare the performance of these methods\non selected metrics. Finally, we also present an extension to one of the said\nmethods, that we called FIRe-GAN, that improves the generation of artificial\ninfrared images and fused ones on selected metrics.\n", "versions": [{"version": "v1", "created": "Wed, 27 Jan 2021 23:53:36 GMT"}, {"version": "v2", "created": "Mon, 22 Feb 2021 17:07:41 GMT"}], "update_date": "2021-02-23", "authors_parsed": [["Cipri\u00e1n-S\u00e1nchez", "J. F.", ""], ["Ochoa-Ruiz", "G.", ""], ["Gonzalez-Mendoza", "M.", ""], ["Rossi", "L.", ""]]}, {"id": "2101.11748", "submitter": "Hamzah Abdel-Aziz", "authors": "Hamzah Abdel-Aziz, Ali Shafiee, Jong Hoon Shin, Ardavan Pedram and\n  Joseph H. Hassoun", "title": "Rethinking Floating Point Overheads for Mixed Precision DNN Accelerators", "comments": "Accepted to appear in 4th Conference on Machine Learning and Systems\n  2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a mixed-precision convolution unit architecture\nwhich supports different integer and floating point (FP) precisions. The\nproposed architecture is based on low-bit inner product units and realizes\nhigher precision based on temporal decomposition. We illustrate how to\nintegrate FP computations on integer-based architecture and evaluate overheads\nincurred by FP arithmetic support. We argue that alignment and addition\noverhead for FP inner product can be significant since the maximum exponent\ndifference could be up to 58 bits, which results into a large alignment logic.\nTo address this issue, we illustrate empirically that no more than\n26-bitproduct bits are required and up to 8-bit of alignment is sufficient in\nmost inference cases. We present novel optimizations based on the above\nobservations to reduce the FP arithmetic hardware overheads. Our empirical\nresults, based on simulation and hardware implementation, show significant\nreduction in FP16 overhead. Over typical mixed precision implementation, the\nproposed architecture achieves area improvements of up to 25% in TFLOPS/mm2and\nup to 46% in TOPS/mm2with power efficiency improvements of up to 40% in\nTFLOPS/Wand up to 63% in TOPS/W.\n", "versions": [{"version": "v1", "created": "Wed, 27 Jan 2021 23:57:43 GMT"}], "update_date": "2021-01-29", "authors_parsed": [["Abdel-Aziz", "Hamzah", ""], ["Shafiee", "Ali", ""], ["Shin", "Jong Hoon", ""], ["Pedram", "Ardavan", ""], ["Hassoun", "Joseph H.", ""]]}, {"id": "2101.11750", "submitter": "Chuteng Zhou", "authors": "Chuteng Zhou, Quntao Zhuang, Matthew Mattina, Paul N. Whatmough", "title": "Information contraction in noisy binary neural networks and its\n  implications", "comments": "14 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.AI cs.CC cs.LG math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural networks have gained importance as the machine learning models that\nachieve state-of-the-art performance on large-scale image classification,\nobject detection and natural language processing tasks. In this paper, we\nconsider noisy binary neural networks, where each neuron has a non-zero\nprobability of producing an incorrect output. These noisy models may arise from\nbiological, physical and electronic contexts and constitute an important class\nof models that are relevant to the physical world. Intuitively, the number of\nneurons in such systems has to grow to compensate for the noise while\nmaintaining the same level of expressive power and computation reliability. Our\nkey finding is a lower bound for the required number of neurons in noisy neural\nnetworks, which is first of its kind. To prove this lower bound, we take an\ninformation theoretic approach and obtain a novel strong data processing\ninequality (SDPI), which not only generalizes the Evans-Schulman results for\nbinary symmetric channels to general channels, but also improves the tightness\ndrastically when applied to estimate end-to-end information contraction in\nnetworks. Our SDPI can be applied to various information processing systems,\nincluding neural networks and cellular automata. Applying the SDPI in noisy\nbinary neural networks, we obtain our key lower bound and investigate its\nimplications on network depth-width trade-offs, our results suggest a\ndepth-width trade-off for noisy neural networks that is very different from the\nestablished understanding regarding noiseless neural networks. Furthermore, we\napply the SDPI to study fault-tolerant cellular automata and obtain bounds on\nthe error correction overheads and the relaxation time. This paper offers new\nunderstanding of noisy information processing systems through the lens of\ninformation theory.\n", "versions": [{"version": "v1", "created": "Thu, 28 Jan 2021 00:01:45 GMT"}, {"version": "v2", "created": "Mon, 1 Feb 2021 17:19:25 GMT"}], "update_date": "2021-02-02", "authors_parsed": [["Zhou", "Chuteng", ""], ["Zhuang", "Quntao", ""], ["Mattina", "Matthew", ""], ["Whatmough", "Paul N.", ""]]}, {"id": "2101.11751", "submitter": "Mohit Yadav", "authors": "Mohit Yadav, Daniel Sheldon, Cameron Musco", "title": "Faster Kernel Interpolation for Gaussian Processes", "comments": "To appear, Artificial Intelligence and Statistics (AISTATS) 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  A key challenge in scaling Gaussian Process (GP) regression to massive\ndatasets is that exact inference requires computation with a dense n x n kernel\nmatrix, where n is the number of data points. Significant work focuses on\napproximating the kernel matrix via interpolation using a smaller set of m\ninducing points. Structured kernel interpolation (SKI) is among the most\nscalable methods: by placing inducing points on a dense grid and using\nstructured matrix algebra, SKI achieves per-iteration time of O(n + m log m)\nfor approximate inference. This linear scaling in n enables inference for very\nlarge data sets; however the cost is per-iteration, which remains a limitation\nfor extremely large n. We show that the SKI per-iteration time can be reduced\nto O(m log m) after a single O(n) time precomputation step by reframing SKI as\nsolving a natural Bayesian linear regression problem with a fixed set of m\ncompact basis functions. With per-iteration complexity independent of the\ndataset size n for a fixed grid, our method scales to truly massive data sets.\nWe demonstrate speedups in practice for a wide range of m and n and apply the\nmethod to GP inference on a three-dimensional weather radar dataset with over\n100 million points.\n", "versions": [{"version": "v1", "created": "Thu, 28 Jan 2021 00:09:22 GMT"}], "update_date": "2021-01-29", "authors_parsed": [["Yadav", "Mohit", ""], ["Sheldon", "Daniel", ""], ["Musco", "Cameron", ""]]}, {"id": "2101.11753", "submitter": "Manoj Kumar", "authors": "Manoj Kumar, Varun Kumar, Hadrien Glaude, Cyprien delichy, Aman Alok\n  and Rahul Gupta", "title": "ProtoDA: Efficient Transfer Learning for Few-Shot Intent Classification", "comments": "Accepted at IEEE Spoken Language Technology Workshop 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Practical sequence classification tasks in natural language processing often\nsuffer from low training data availability for target classes. Recent works\ntowards mitigating this problem have focused on transfer learning using\nembeddings pre-trained on often unrelated tasks, for instance, language\nmodeling. We adopt an alternative approach by transfer learning on an ensemble\nof related tasks using prototypical networks under the meta-learning paradigm.\nUsing intent classification as a case study, we demonstrate that increasing\nvariability in training tasks can significantly improve classification\nperformance. Further, we apply data augmentation in conjunction with\nmeta-learning to reduce sampling bias. We make use of a conditional generator\nfor data augmentation that is trained directly using the meta-learning\nobjective and simultaneously with prototypical networks, hence ensuring that\ndata augmentation is customized to the task. We explore augmentation in the\nsentence embedding space as well as prototypical embedding space. Combining\nmeta-learning with augmentation provides upto 6.49% and 8.53% relative F1-score\nimprovements over the best performing systems in the 5-shot and 10-shot\nlearning, respectively.\n", "versions": [{"version": "v1", "created": "Thu, 28 Jan 2021 00:19:13 GMT"}], "update_date": "2021-01-29", "authors_parsed": [["Kumar", "Manoj", ""], ["Kumar", "Varun", ""], ["Glaude", "Hadrien", ""], ["delichy", "Cyprien", ""], ["Alok", "Aman", ""], ["Gupta", "Rahul", ""]]}, {"id": "2101.11760", "submitter": "Aristeidis Seretis", "authors": "Aristeidis Seretis, Costas D. Sarris", "title": "An Overview of Machine Learning Techniques for Radiowave Propagation\n  Modeling", "comments": "15 pages, 9 figures, 2 small tables and 1 1-page table", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SP cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We give an overview of recent developments in the modeling of radiowave\npropagation, based on machine learning algorithms. We identify the input and\noutput specification and the architecture of the model as the main challenges\nassociated with machine learning-driven propagation models. Relevant papers are\ndiscussed and categorized based on their approach to each of these challenges.\nEmphasis is given on presenting the prospects and open problems in this\npromising and rapidly evolving area.\n", "versions": [{"version": "v1", "created": "Thu, 28 Jan 2021 00:55:11 GMT"}], "update_date": "2021-01-29", "authors_parsed": [["Seretis", "Aristeidis", ""], ["Sarris", "Costas D.", ""]]}, {"id": "2101.11766", "submitter": "Bingyuan Liu", "authors": "Bingyuan Liu, Christopher Malon, Lingzhou Xue and Erik Kruus", "title": "Improving Neural Network Robustness through Neighborhood Preserving\n  Layers", "comments": "An earlier short version of this paper without proof is presented in\n  25th International Conference on Pattern Recognition(ICPR), Manifold Learning\n  from Euclid to Riemann workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Robustness against adversarial attack in neural networks is an important\nresearch topic in the machine learning community. We observe one major source\nof vulnerability of neural nets is from overparameterized fully-connected\nlayers. In this paper, we propose a new neighborhood preserving layer which can\nreplace these fully connected layers to improve the network robustness. We\ndemonstrate a novel neural network architecture which can incorporate such\nlayers and also can be trained efficiently. We theoretically prove that our\nmodels are more robust against distortion because they effectively control the\nmagnitude of gradients. Finally, we empirically show that our designed network\narchitecture is more robust against state-of-art gradient descent based\nattacks, such as a PGD attack on the benchmark datasets MNIST and CIFAR10.\n", "versions": [{"version": "v1", "created": "Thu, 28 Jan 2021 01:26:35 GMT"}, {"version": "v2", "created": "Fri, 29 Jan 2021 16:06:58 GMT"}], "update_date": "2021-02-01", "authors_parsed": [["Liu", "Bingyuan", ""], ["Malon", "Christopher", ""], ["Xue", "Lingzhou", ""], ["Kruus", "Erik", ""]]}, {"id": "2101.11769", "submitter": "Can Xu", "authors": "Can Xu, Ahmed M. Alaa, Ioana Bica, Brent D. Ershoff, Maxime Cannesson,\n  Mihaela van der Schaar", "title": "Learning Matching Representations for Individualized Organ\n  Transplantation Allocation", "comments": "Accepted to AISTATS 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Organ transplantation is often the last resort for treating end-stage\nillness, but the probability of a successful transplantation depends greatly on\ncompatibility between donors and recipients. Current medical practice relies on\ncoarse rules for donor-recipient matching, but is short of domain knowledge\nregarding the complex factors underlying organ compatibility. In this paper, we\nformulate the problem of learning data-driven rules for organ matching using\nobservational data for organ allocations and transplant outcomes. This problem\ndeparts from the standard supervised learning setup in that it involves\nmatching the two feature spaces (i.e., donors and recipients), and requires\nestimating transplant outcomes under counterfactual matches not observed in the\ndata. To address these problems, we propose a model based on representation\nlearning to predict donor-recipient compatibility; our model learns\nrepresentations that cluster donor features, and applies donor-invariant\ntransformations to recipient features to predict outcomes for a given\ndonor-recipient feature instance. Experiments on semi-synthetic and real-world\ndatasets show that our model outperforms state-of-art allocation methods and\npolicies executed by human experts.\n", "versions": [{"version": "v1", "created": "Thu, 28 Jan 2021 01:33:21 GMT"}, {"version": "v2", "created": "Tue, 2 Feb 2021 02:46:43 GMT"}], "update_date": "2021-02-03", "authors_parsed": [["Xu", "Can", ""], ["Alaa", "Ahmed M.", ""], ["Bica", "Ioana", ""], ["Ershoff", "Brent D.", ""], ["Cannesson", "Maxime", ""], ["van der Schaar", "Mihaela", ""]]}, {"id": "2101.11799", "submitter": "Kang Wei", "authors": "Kang Wei, Jun Li, Ming Ding, Chuan Ma, Yo-Seb Jeon and H. Vincent Poor", "title": "Covert Model Poisoning Against Federated Learning: Algorithm Design and\n  Optimization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Federated learning (FL), as a type of distributed machine learning\nframeworks, is vulnerable to external attacks on FL models during parameters\ntransmissions. An attacker in FL may control a number of participant clients,\nand purposely craft the uploaded model parameters to manipulate system outputs,\nnamely, model poisoning (MP). In this paper, we aim to propose effective MP\nalgorithms to combat state-of-the-art defensive aggregation mechanisms (e.g.,\nKrum and Trimmed mean) implemented at the server without being noticed, i.e.,\ncovert MP (CMP). Specifically, we first formulate the MP as an optimization\nproblem by minimizing the Euclidean distance between the manipulated model and\ndesignated one, constrained by a defensive aggregation rule. Then, we develop\nCMP algorithms against different defensive mechanisms based on the solutions of\ntheir corresponding optimization problems. Furthermore, to reduce the\noptimization complexity, we propose low complexity CMP algorithms with a slight\nperformance degradation. In the case that the attacker does not know the\ndefensive aggregation mechanism, we design a blind CMP algorithm, in which the\nmanipulated model will be adjusted properly according to the aggregated model\ngenerated by the unknown defensive aggregation. Our experimental results\ndemonstrate that the proposed CMP algorithms are effective and substantially\noutperform existing attack mechanisms.\n", "versions": [{"version": "v1", "created": "Thu, 28 Jan 2021 03:28:18 GMT"}], "update_date": "2021-01-29", "authors_parsed": [["Wei", "Kang", ""], ["Li", "Jun", ""], ["Ding", "Ming", ""], ["Ma", "Chuan", ""], ["Jeon", "Yo-Seb", ""], ["Poor", "H. Vincent", ""]]}, {"id": "2101.11800", "submitter": "Sicong Liu", "authors": "Sicong Liu, Bin Guo, Ke Ma, Zhiwen Yu, Junzhao Du", "title": "AdaSpring: Context-adaptive and Runtime-evolutionary Deep Model\n  Compression for Mobile Applications", "comments": "Ubicomp 2021", "journal-ref": null, "doi": "10.1145/3448125", "report-no": null, "categories": "cs.LG cs.NI", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  There are many deep learning (e.g., DNN) powered mobile and wearable\napplications today continuously and unobtrusively sensing the ambient\nsurroundings to enhance all aspects of human lives. To enable robust and\nprivate mobile sensing, DNN tends to be deployed locally on the\nresource-constrained mobile devices via model compression. The current practice\neither hand-crafted DNN compression techniques, i.e., for optimizing\nDNN-relative performance (e.g., parameter size), or on-demand DNN compression\nmethods, i.e., for optimizing hardware-dependent metrics (e.g., latency),\ncannot be locally online because they require offline retraining to ensure\naccuracy. Also, none of them have correlated their efforts with runtime\nadaptive compression to consider the dynamic nature of the deployment context\nof mobile applications. To address those challenges, we present AdaSpring, a\ncontext-adaptive and self-evolutionary DNN compression framework. It enables\nthe runtime adaptive DNN compression locally online. Specifically, it presents\nthe ensemble training of a retraining-free and self-evolutionary network to\nintegrate multiple alternative DNN compression configurations (i.e., compressed\narchitectures and weights). It then introduces the runtime search strategy to\nquickly search for the most suitable compression configurations and evolve the\ncorresponding weights. With evaluation on five tasks across three platforms and\na real-world case study, experiment outcomes show that AdaSpring obtains up to\n3.1x latency reduction, 4.2 x energy efficiency improvement in DNNs, compared\nto hand-crafted compression techniques, while only incurring <= 6.2ms\nruntime-evolution latency.\n", "versions": [{"version": "v1", "created": "Thu, 28 Jan 2021 03:30:04 GMT"}], "update_date": "2021-01-29", "authors_parsed": [["Liu", "Sicong", ""], ["Guo", "Bin", ""], ["Ma", "Ke", ""], ["Yu", "Zhiwen", ""], ["Du", "Junzhao", ""]]}, {"id": "2101.11805", "submitter": "Ningtao Liu", "authors": "Ningtao Liu", "title": "Chronological age estimation of lateral cephalometric radiographs with\n  deep learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The traditional manual age estimation method is crucial labor based on many\nkinds of the X-Ray image. Some current studies have shown that lateral\ncephalometric(LC) images can be used to estimate age. However, these methods\nare based on manually measuring some image features and making age estimates\nbased on experience or scoring. Therefore, these methods are time-consuming and\nlabor-intensive, and the effect will be affected by subjective opinions. In\nthis work, we propose a saliency map-enhanced age estimation method, which can\nautomatically perform age estimation based on LC images. Meanwhile, it can also\nshow the importance of each region in the image for age estimation, which\nundoubtedly increases the method's Interpretability. Our method was tested on\n3014 LC images from 4 to 40 years old. The MEA of the experimental result is\n1.250, which is less than the result of the state-of-the-art benchmark because\nit performs significantly better in the age group with fewer data. Besides, our\nmodel is trained in each area with a high contribution to age estimation in LC\nimages, so the effect of these different areas on the age estimation task was\nverified. Consequently, we conclude that the proposed saliency map enhancements\nchronological age estimation method of lateral cephalometric radiographs can\nwork well in chronological age estimation task, especially when the amount of\ndata is small. Besides, compared with traditional deep learning, our method is\nalso interpretable.\n", "versions": [{"version": "v1", "created": "Thu, 28 Jan 2021 03:43:24 GMT"}], "update_date": "2021-01-29", "authors_parsed": [["Liu", "Ningtao", ""]]}, {"id": "2101.11810", "submitter": "Teeratorn Kadeethum", "authors": "T. Kadeethum, F. Ballarin, N. Bouklas", "title": "Non-intrusive reduced order modeling of poroelasticity of heterogeneous\n  media based on a discontinuous Galerkin approximation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.NA cs.CE cs.LG cs.NA", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We present a non-intrusive model reduction framework for linear\nporoelasticity problems in heterogeneous porous media using proper orthogonal\ndecomposition (POD) and neural networks, based on the usual offline-online\nparadigm. As the conductivity of porous media can be highly heterogeneous and\nspan several orders of magnitude, we utilize the interior penalty discontinuous\nGalerkin (DG) method as a full order solver to handle discontinuity and ensure\nlocal mass conservation during the offline stage. We then use POD as a data\ncompression tool and compare the nested POD technique, in which time and\nuncertain parameter domains are compressed consecutively, to the classical POD\nmethod in which all domains are compressed simultaneously. The neural networks\nare finally trained to map the set of uncertain parameters, which could\ncorrespond to material properties, boundary conditions, or geometric\ncharacteristics, to the collection of coefficients calculated from an $L^2$\nprojection over the reduced basis. We then perform a non-intrusive evaluation\nof the neural networks to obtain coefficients corresponding to new values of\nthe uncertain parameters during the online stage. We show that our framework\nprovides reasonable approximations of the DG solution, but it is significantly\nfaster. Moreover, the reduced order framework can capture sharp discontinuities\nof both displacement and pressure fields resulting from the heterogeneity in\nthe media conductivity, which is generally challenging for intrusive reduced\norder methods. The sources of error are presented, showing that the nested POD\ntechnique is computationally advantageous and still provides comparable\naccuracy to the classical POD method. We also explore the effect of different\nchoices of the hyperparameters of the neural network on the framework\nperformance.\n", "versions": [{"version": "v1", "created": "Thu, 28 Jan 2021 04:21:06 GMT"}], "update_date": "2021-01-29", "authors_parsed": [["Kadeethum", "T.", ""], ["Ballarin", "F.", ""], ["Bouklas", "N.", ""]]}, {"id": "2101.11812", "submitter": "Shaoxiong Wang", "authors": "Chen Wang, Shaoxiong Wang, Branden Romero, Filipe Veiga, Edward\n  Adelson", "title": "SwingBot: Learning Physical Features from In-hand Tactile Exploration\n  for Dynamic Swing-up Manipulation", "comments": "IROS 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AI cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Several robot manipulation tasks are extremely sensitive to variations of the\nphysical properties of the manipulated objects. One such task is manipulating\nobjects by using gravity or arm accelerations, increasing the importance of\nmass, center of mass, and friction information. We present SwingBot, a robot\nthat is able to learn the physical features of a held object through tactile\nexploration. Two exploration actions (tilting and shaking) provide the tactile\ninformation used to create a physical feature embedding space. With this\nembedding, SwingBot is able to predict the swing angle achieved by a robot\nperforming dynamic swing-up manipulations on a previously unseen object. Using\nthese predictions, it is able to search for the optimal control parameters for\na desired swing-up angle. We show that with the learned physical features our\nend-to-end self-supervised learning pipeline is able to substantially improve\nthe accuracy of swinging up unseen objects. We also show that objects with\nsimilar dynamics are closer to each other on the embedding space and that the\nembedding can be disentangled into values of specific physical properties.\n", "versions": [{"version": "v1", "created": "Thu, 28 Jan 2021 04:35:15 GMT"}], "update_date": "2021-01-29", "authors_parsed": [["Wang", "Chen", ""], ["Wang", "Shaoxiong", ""], ["Romero", "Branden", ""], ["Veiga", "Filipe", ""], ["Adelson", "Edward", ""]]}, {"id": "2101.11815", "submitter": "Tengyuan Liang", "authors": "Tengyuan Liang, Benjamin Recht", "title": "Interpolating Classifiers Make Few Mistakes", "comments": "23 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG cs.NA math.NA math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper provides elementary analyses of the regret and generalization of\nminimum-norm interpolating classifiers (MNIC). The MNIC is the function of\nsmallest Reproducing Kernel Hilbert Space norm that perfectly interpolates a\nlabel pattern on a finite data set. We derive a mistake bound for MNIC and a\nregularized variant that holds for all data sets. This bound follows from\nelementary properties of matrix inverses. Under the assumption that the data is\nindependently and identically distributed, the mistake bound implies that MNIC\ngeneralizes at a rate proportional to the norm of the interpolating solution\nand inversely proportional to the number of data points. This rate matches\nsimilar rates derived for margin classifiers and perceptrons. We derive several\nplausible generative models where the norm of the interpolating classifier is\nbounded or grows at a rate sublinear in $n$. We also show that as long as the\npopulation class conditional distributions are sufficiently separable in total\nvariation, then MNIC generalizes with a fast rate.\n", "versions": [{"version": "v1", "created": "Thu, 28 Jan 2021 04:51:24 GMT"}, {"version": "v2", "created": "Thu, 29 Jul 2021 01:40:30 GMT"}], "update_date": "2021-07-30", "authors_parsed": [["Liang", "Tengyuan", ""], ["Recht", "Benjamin", ""]]}, {"id": "2101.11828", "submitter": "Md Geaur Rahman", "authors": "Md Geaur Rahman and Md Zahidul Islam", "title": "Adaptive Decision Forest: An Incremental Machine Learning Framework", "comments": "35 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this study, we present an incremental machine learning framework called\nAdaptive Decision Forest (ADF), which produces a decision forest to classify\nnew records. Based on our two novel theorems, we introduce a new splitting\nstrategy called iSAT, which allows ADF to classify new records even if they are\nassociated with previously unseen classes. ADF is capable of identifying and\nhandling concept drift; it, however, does not forget previously gained\nknowledge. Moreover, ADF is capable of handling big data if the data can be\ndivided into batches. We evaluate ADF on five publicly available natural data\nsets and one synthetic data set, and compare the performance of ADF against the\nperformance of eight state-of-the-art techniques. Our experimental results,\nincluding statistical sign test and Nemenyi test analyses, indicate a clear\nsuperiority of the proposed framework over the state-of-the-art techniques.\n", "versions": [{"version": "v1", "created": "Thu, 28 Jan 2021 06:24:08 GMT"}], "update_date": "2021-01-29", "authors_parsed": [["Rahman", "Md Geaur", ""], ["Islam", "Md Zahidul", ""]]}, {"id": "2101.11836", "submitter": "Hrituraj Singh", "authors": "Hrituraj Singh, Gaurav Verma, Aparna Garimella, Balaji Vasan\n  Srinivasan", "title": "DRAG: Director-Generator Language Modelling Framework for Non-Parallel\n  Author Stylized Rewriting", "comments": "Accepted as Long Paper to EACL 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Author stylized rewriting is the task of rewriting an input text in a\nparticular author's style. Recent works in this area have leveraged\nTransformer-based language models in a denoising autoencoder setup to generate\nauthor stylized text without relying on a parallel corpus of data. However,\nthese approaches are limited by the lack of explicit control of target\nattributes and being entirely data-driven. In this paper, we propose a\nDirector-Generator framework to rewrite content in the target author's style,\nspecifically focusing on certain target attributes. We show that our proposed\nframework works well even with a limited-sized target author corpus. Our\nexperiments on corpora consisting of relatively small-sized text authored by\nthree distinct authors show significant improvements upon existing works to\nrewrite input texts in target author's style. Our quantitative and qualitative\nanalyses further show that our model has better meaning retention and results\nin more fluent generations.\n", "versions": [{"version": "v1", "created": "Thu, 28 Jan 2021 06:52:40 GMT"}], "update_date": "2021-01-29", "authors_parsed": [["Singh", "Hrituraj", ""], ["Verma", "Gaurav", ""], ["Garimella", "Aparna", ""], ["Srinivasan", "Balaji Vasan", ""]]}, {"id": "2101.11845", "submitter": "Andrea Manzoni", "authors": "Stefania Fresca, Andrea Manzoni", "title": "POD-DL-ROM: enhancing deep learning-based reduced order models for\n  nonlinear parametrized PDEs by proper orthogonal decomposition", "comments": "26 pages. arXiv admin note: text overlap with arXiv:2001.04001", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.NA cs.LG cs.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning-based reduced order models (DL-ROMs) have been recently\nproposed to overcome common limitations shared by conventional reduced order\nmodels (ROMs) - built, e.g., through proper orthogonal decomposition (POD) -\nwhen applied to nonlinear time-dependent parametrized partial differential\nequations (PDEs). These might be related to (i) the need to deal with\nprojections onto high dimensional linear approximating trial manifolds, (ii)\nexpensive hyper-reduction strategies, or (iii) the intrinsic difficulty to\nhandle physical complexity with a linear superimposition of modes. All these\naspects are avoided when employing DL-ROMs, which learn in a non-intrusive way\nboth the nonlinear trial manifold and the reduced dynamics, by relying on deep\n(e.g., feedforward, convolutional, autoencoder) neural networks. Although\nextremely efficient at testing time, when evaluating the PDE solution for any\nnew testing-parameter instance, DL-ROMs require an expensive training stage,\nbecause of the extremely large number of network parameters to be estimated. In\nthis paper we propose a possible way to avoid an expensive training stage of\nDL-ROMs, by (i) performing a prior dimensionality reduction through POD, and\n(ii) relying on a multi-fidelity pretraining stage, where different physical\nmodels can be efficiently combined. The proposed POD-DL-ROM is tested on\nseveral (both scalar and vector, linear and nonlinear) time-dependent\nparametrized PDEs (such as, e.g., linear advection-diffusion-reaction,\nnonlinear diffusion-reaction, nonlinear elastodynamics, and Navier-Stokes\nequations) to show the generality of this approach and its remarkable\ncomputational savings.\n", "versions": [{"version": "v1", "created": "Thu, 28 Jan 2021 07:34:15 GMT"}], "update_date": "2021-01-29", "authors_parsed": [["Fresca", "Stefania", ""], ["Manzoni", "Andrea", ""]]}, {"id": "2101.11859", "submitter": "Meiqi Zhu", "authors": "Meiqi Zhu, Xiao Wang, Chuan Shi, Houye Ji, Peng Cui", "title": "Interpreting and Unifying Graph Neural Networks with An Optimization\n  Framework", "comments": "WWW2021, 12 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Graph Neural Networks (GNNs) have received considerable attention on\ngraph-structured data learning for a wide variety of tasks. The well-designed\npropagation mechanism which has been demonstrated effective is the most\nfundamental part of GNNs. Although most of GNNs basically follow a message\npassing manner, litter effort has been made to discover and analyze their\nessential relations. In this paper, we establish a surprising connection\nbetween different propagation mechanisms with a unified optimization problem,\nshowing that despite the proliferation of various GNNs, in fact, their proposed\npropagation mechanisms are the optimal solution optimizing a feature fitting\nfunction over a wide class of graph kernels with a graph regularization term.\nOur proposed unified optimization framework, summarizing the commonalities\nbetween several of the most representative GNNs, not only provides a\nmacroscopic view on surveying the relations between different GNNs, but also\nfurther opens up new opportunities for flexibly designing new GNNs. With the\nproposed framework, we discover that existing works usually utilize naive graph\nconvolutional kernels for feature fitting function, and we further develop two\nnovel objective functions considering adjustable graph kernels showing low-pass\nor high-pass filtering capabilities respectively. Moreover, we provide the\nconvergence proofs and expressive power comparisons for the proposed models.\nExtensive experiments on benchmark datasets clearly show that the proposed GNNs\nnot only outperform the state-of-the-art methods but also have good ability to\nalleviate over-smoothing, and further verify the feasibility for designing GNNs\nwith our unified optimization framework.\n", "versions": [{"version": "v1", "created": "Thu, 28 Jan 2021 08:06:02 GMT"}], "update_date": "2021-01-29", "authors_parsed": [["Zhu", "Meiqi", ""], ["Wang", "Xiao", ""], ["Shi", "Chuan", ""], ["Ji", "Houye", ""], ["Cui", "Peng", ""]]}, {"id": "2101.11863", "submitter": "Romann Weber", "authors": "Romann M. Weber", "title": "Exploiting the Hidden Tasks of GANs: Making Implicit Subproblems\n  Explicit", "comments": "12 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an alternative perspective on the training of generative\nadversarial networks (GANs), showing that the training step for a GAN generator\ndecomposes into two implicit subproblems. In the first, the discriminator\nprovides new target data to the generator in the form of \"inverse examples\"\nproduced by approximately inverting classifier labels. In the second, these\nexamples are used as targets to update the generator via least-squares\nregression, regardless of the main loss specified to train the network. We\nexperimentally validate our main theoretical result and demonstrate significant\nimprovements over standard GAN training made possible by making these\nsubproblems explicit.\n", "versions": [{"version": "v1", "created": "Thu, 28 Jan 2021 08:17:29 GMT"}, {"version": "v2", "created": "Fri, 29 Jan 2021 12:47:36 GMT"}, {"version": "v3", "created": "Mon, 12 Apr 2021 13:35:48 GMT"}, {"version": "v4", "created": "Wed, 12 May 2021 08:16:23 GMT"}], "update_date": "2021-05-13", "authors_parsed": [["Weber", "Romann M.", ""]]}, {"id": "2101.11871", "submitter": "Pengwei Zhan", "authors": "Pengwei Zhan, Liming Wang, Yi Tang", "title": "Website Fingerprinting on Early QUIC Traffic", "comments": "30 pages, 7 figures, submitted to Elsevier Computer Networks", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.LG cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cryptographic protocols have been widely used to protect the user's privacy\nand avoid exposing private information. QUIC (Quick UDP Internet Connections),\nas an alternative to traditional HTTP, demonstrates its unique transmission\ncharacteristics: based on UDP for encrypted resource transmission, accelerating\nweb page rendering. However, existing encrypted transmission schemes based on\nTCP are vulnerable to website fingerprinting (WFP) attacks, allowing\nadversaries to infer the users' visited websites by eavesdropping on the\ntransmission channel. Whether QUIC protocol can effectively resisting to such\nattacks is worth investigating. In this work, we demonstrated the extreme\nvulnerability of QUIC under WFP attacks by comparing attack results under\nwell-designed conditions. We also study the transferability of features, which\nenable the adversary to use proven effective features on a special protocol\nattacking a new protocol. This study shows that QUIC is more vulnerable to WFP\nattacks than HTTPS in the early traffic scenario but is similar in the normal\nscenario. The maximum attack accuracy on QUIC is 56.8 % and 73 % higher than on\nHTTPS utilizing Simple features and Transfer features. The insecurity\ncharacteristic of QUIC explains the dramatic gap. We also find that features\nare transferable between protocols, and the feature importance is partially\ninherited on normal traffic due to the relatively fixed browser rendering\nsequence and the similar request-response model of protocols. However, the\ntransferability is inefficient when on early traffic, as QUIC and HTTPS show\nsignificantly different vulnerability when considering early traffic. We also\nshow that attack accuracy on QUIC could reach 95.4 % with only 40 packets and\njust using simple features, whereas only 60.7 % when on HTTPS.\n", "versions": [{"version": "v1", "created": "Thu, 28 Jan 2021 08:53:51 GMT"}], "update_date": "2021-01-29", "authors_parsed": [["Zhan", "Pengwei", ""], ["Wang", "Liming", ""], ["Tang", "Yi", ""]]}, {"id": "2101.11881", "submitter": "Rohitash Chandra", "authors": "Rohitash Chandra, Ayush Jain, Divyanshu Singh Chauhan", "title": "Deep learning via LSTM models for COVID-19 infection forecasting in\n  India", "comments": "Under review", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.AP stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We have entered an era of a pandemic that has shaken the world with major\nimpact to medical systems, economics and agriculture. Prominent computational\nand mathematical models have been unreliable due to the complexity of the\nspread of infections. Moreover, lack of data collection and reporting makes any\nsuch modelling attempts unreliable. Hence we need to re-look at the situation\nwith the latest data sources and most comprehensive forecasting models. Deep\nlearning models such as recurrent neural networks are well suited for modelling\ntemporal sequences. In this paper, prominent recurrent neural networks, in\nparticular \\textit{long short term memory} (LSTMs) networks, bidirectional\nLSTM, and encoder-decoder LSTM models for multi-step (short-term) forecasting\nthe spread of COVID-infections among selected states in India. We select states\nwith COVID-19 hotpots in terms of the rate of infections and compare with\nstates where infections have been contained or reached their peak and provide\ntwo months ahead forecast that shows that cases will slowly decline. Our\nresults show that long-term forecasts are promising which motivates the\napplication of the method in other countries or areas. We note that although we\nmade some progress in forecasting, the challenges in modelling remain due to\ndata and difficulty in capturing factors such as population density, travel\nlogistics, and social aspects such culture and lifestyle.\n", "versions": [{"version": "v1", "created": "Thu, 28 Jan 2021 09:19:10 GMT"}], "update_date": "2021-01-29", "authors_parsed": [["Chandra", "Rohitash", ""], ["Jain", "Ayush", ""], ["Chauhan", "Divyanshu Singh", ""]]}, {"id": "2101.11889", "submitter": "David Harbecke", "authors": "David Harbecke", "title": "Explaining Natural Language Processing Classifiers with Occlusion and\n  Language Modeling", "comments": "Master's Thesis at University of Potsdam without Acknowledgements", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Deep neural networks are powerful statistical learners. However, their\npredictions do not come with an explanation of their process. To analyze these\nmodels, explanation methods are being developed. We present a novel explanation\nmethod, called OLM, for natural language processing classifiers. This method\ncombines occlusion and language modeling, which are techniques central to\nexplainability and NLP, respectively. OLM gives explanations that are\ntheoretically sound and easy to understand.\n  We make several contributions to the theory of explanation methods. Axioms\nfor explanation methods are an interesting theoretical concept to explore their\nbasics and deduce methods. We introduce a new axiom, give its intuition and\nshow it contradicts another existing axiom. Additionally, we point out\ntheoretical difficulties of existing gradient-based and some occlusion-based\nexplanation methods in natural language processing. We provide an extensive\nargument why evaluation of explanation methods is difficult. We compare OLM to\nother explanation methods and underline its uniqueness experimentally. Finally,\nwe investigate corner cases of OLM and discuss its validity and possible\nimprovements.\n", "versions": [{"version": "v1", "created": "Thu, 28 Jan 2021 09:44:04 GMT"}], "update_date": "2021-01-29", "authors_parsed": [["Harbecke", "David", ""]]}, {"id": "2101.11890", "submitter": "Timothy Atkinson", "authors": "Timothy Atkinson, Saeed Saremi, Faustino Gomez, Jonathan Masci", "title": "Automatic design of novel potential 3CL$^{\\text{pro}}$ and\n  PL$^{\\text{pro}}$ inhibitors", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI q-bio.QM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the goal of designing novel inhibitors for SARS-CoV-1 and SARS-CoV-2, we\npropose the general molecule optimization framework, Molecular Neural Assay\nSearch (MONAS), consisting of three components: a property predictor which\nidentifies molecules with specific desirable properties, an energy model which\napproximates the statistical similarity of a given molecule to known training\nmolecules, and a molecule search method. In this work, these components are\ninstantiated with graph neural networks (GNNs), Deep Energy Estimator Networks\n(DEEN) and Monte Carlo tree search (MCTS), respectively. This implementation is\nused to identify 120K molecules (out of 40-million explored) which the GNN\ndetermined to be likely SARS-CoV-1 inhibitors, and, at the same time, are\nstatistically close to the dataset used to train the GNN.\n", "versions": [{"version": "v1", "created": "Thu, 28 Jan 2021 09:47:23 GMT"}, {"version": "v2", "created": "Fri, 29 Jan 2021 07:32:36 GMT"}], "update_date": "2021-02-01", "authors_parsed": [["Atkinson", "Timothy", ""], ["Saremi", "Saeed", ""], ["Gomez", "Faustino", ""], ["Masci", "Jonathan", ""]]}, {"id": "2101.11896", "submitter": "Jiahuan Luo", "authors": "Xinle Liang, Yang Liu, Jiahuan Luo, Yuanqin He, Tianjian Chen, Qiang\n  Yang", "title": "Self-supervised Cross-silo Federated Neural Architecture Search", "comments": "This work has been submitted to the IEEE for possible publication.\n  Copyright may be transferred without notice, after which this version may no\n  longer be accessible", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Federated Learning (FL) provides both model performance and data privacy for\nmachine learning tasks where samples or features are distributed among\ndifferent parties. In the training process of FL, no party has a global view of\ndata distributions or model architectures of other parties. Thus the\nmanually-designed architectures may not be optimal. In the past, Neural\nArchitecture Search (NAS) has been applied to FL to address this critical\nissue. However, existing Federated NAS approaches require prohibitive\ncommunication and computation effort, as well as the availability of\nhigh-quality labels. In this work, we present Self-supervised Vertical\nFederated Neural Architecture Search (SS-VFNAS) for automating FL where\nparticipants hold feature-partitioned data, a common cross-silo scenario called\nVertical Federated Learning (VFL). In the proposed framework, each party first\nconducts NAS using self-supervised approach to find a local optimal\narchitecture with its own data. Then, parties collaboratively improve the local\noptimal architecture in a VFL framework with supervision. We demonstrate\nexperimentally that our approach has superior performance, communication\nefficiency and privacy compared to Federated NAS and is capable of generating\nhigh-performance and highly-transferable heterogeneous architectures even with\ninsufficient overlapping samples, providing automation for those parties\nwithout deep learning expertise.\n", "versions": [{"version": "v1", "created": "Thu, 28 Jan 2021 09:57:30 GMT"}, {"version": "v2", "created": "Thu, 18 Feb 2021 02:23:50 GMT"}], "update_date": "2021-02-19", "authors_parsed": [["Liang", "Xinle", ""], ["Liu", "Yang", ""], ["Luo", "Jiahuan", ""], ["He", "Yuanqin", ""], ["Chen", "Tianjian", ""], ["Yang", "Qiang", ""]]}, {"id": "2101.11906", "submitter": "Taikan Suehara", "authors": "Kiichi Goto, Taikan Suehara, Tamaki Yoshioka, Masakazu Kurata, Hajime\n  Nagahara, Yuta Nakashima, Noriko Takemura, Masako Iwasaki", "title": "Development of a Vertex Finding Algorithm using Recurrent Neural Network", "comments": "8 pages, 8 figures, preliminary version currently under review by ILD", "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.data-an cs.LG hep-ex physics.ins-det", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning is a rapidly-evolving technology with possibility to\nsignificantly improve physics reach of collider experiments. In this study we\ndeveloped a novel algorithm of vertex finding for future lepton colliders such\nas the International Linear Collider. We deploy two networks; one is simple\nfully-connected layers to look for vertex seeds from track pairs, and the other\nis a customized Recurrent Neural Network with an attention mechanism and an\nencoder-decoder structure to associate tracks to the vertex seeds. The\nperformance of the vertex finder is compared with the standard ILC\nreconstruction algorithm.\n", "versions": [{"version": "v1", "created": "Thu, 28 Jan 2021 10:13:15 GMT"}, {"version": "v2", "created": "Mon, 15 Mar 2021 04:48:56 GMT"}], "update_date": "2021-03-16", "authors_parsed": [["Goto", "Kiichi", ""], ["Suehara", "Taikan", ""], ["Yoshioka", "Tamaki", ""], ["Kurata", "Masakazu", ""], ["Nagahara", "Hajime", ""], ["Nakashima", "Yuta", ""], ["Takemura", "Noriko", ""], ["Iwasaki", "Masako", ""]]}, {"id": "2101.11915", "submitter": "Rachit Agarwal", "authors": "Rachit Agarwal, Tanmay Thapliyal, Sandeep K. Shukla", "title": "Detecting Malicious Accounts showing Adversarial Behavior in\n  Permissionless Blockchains", "comments": "Submitted", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Different types of malicious activities have been flagged in multiple\npermissionless blockchains such as bitcoin, Ethereum etc. While some malicious\nactivities exploit vulnerabilities in the infrastructure of the blockchain,\nsome target its users through social engineering techniques. To address these\nproblems, we aim at automatically flagging blockchain accounts that originate\nsuch malicious exploitation of accounts of other participants. To that end, we\nidentify a robust supervised machine learning (ML) algorithm that is resistant\nto any bias induced by an over representation of certain malicious activity in\nthe available dataset, as well as is robust against adversarial attacks. We\nfind that most of the malicious activities reported thus far, for example, in\nEthereum blockchain ecosystem, behaves statistically similar. Further, the\npreviously used ML algorithms for identifying malicious accounts show bias\ntowards a particular malicious activity which is over-represented. In the\nsequel, we identify that Neural Networks (NN) holds up the best in the face of\nsuch bias inducing dataset at the same time being robust against certain\nadversarial attacks.\n", "versions": [{"version": "v1", "created": "Thu, 28 Jan 2021 10:33:50 GMT"}], "update_date": "2021-01-29", "authors_parsed": [["Agarwal", "Rachit", ""], ["Thapliyal", "Tanmay", ""], ["Shukla", "Sandeep K.", ""]]}, {"id": "2101.11924", "submitter": "Alberto Guill\\'en", "authors": "B.S. Gonz\\'alez, R. Concei\\c{c}\\~ao, M. Pimenta, B. Tom\\'e, A.\n  Guill\\'en", "title": "Tackling the muon identification in water Cherenkov detectors problem\n  for the future Southern Wide-field Gamma-ray Observatory by means of Machine\n  Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.ins-det cs.LG hep-ex", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  This paper presents several approaches to deal with the problem of\nidentifying muons in a water Cherenkov detector with a reduced water volume and\n4 PMTs. Different perspectives of information representation are used and new\nfeatures are engineered using the specific domain knowledge. As results show,\nthese new features, in combination with the convolutional layers, are able to\nachieve a good performance avoiding overfitting and being able to generalise\nproperly for the test set. The results also prove that the combination of\nstate-of-the-art Machine Learning analysis techniques and water Cherenkov\ndetectors with low water depth can be used to efficiently identify muons, which\nmay lead to huge investment savings due to the reduction of the amount of water\nneeded at high altitudes. This achievement can be used in further research to\nbe able to discriminate between gamma and hadron induced showers using muons as\ndiscriminant.\n", "versions": [{"version": "v1", "created": "Thu, 28 Jan 2021 10:54:25 GMT"}], "update_date": "2021-01-29", "authors_parsed": [["Gonz\u00e1lez", "B. S.", ""], ["Concei\u00e7\u00e3o", "R.", ""], ["Pimenta", "M.", ""], ["Tom\u00e9", "B.", ""], ["Guill\u00e9n", "A.", ""]]}, {"id": "2101.11932", "submitter": "Mazen Ali", "authors": "Mazen Ali and Anthony Nouy", "title": "Approximation with Tensor Networks. Part III: Multivariate Approximation", "comments": "For part I see arXiv:2007.00118, for part II see arXiv:2007.00128", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.FA cs.LG cs.NA math.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the approximation of multivariate functions with tensor networks\n(TNs). The main conclusion of this work is an answer to the following two\nquestions: \"What are the approximation capabilities of TNs?\" and \"What is an\nappropriate model class of functions that can be approximated with TNs?\" To\nanswer the former: we show that TNs can (near to) optimally replicate\n$h$-uniform and $h$-adaptive approximation, for any smoothness order of the\ntarget function. Tensor networks thus exhibit universal expressivity w.r.t.\nisotropic, anisotropic and mixed smoothness spaces that is comparable with more\ngeneral neural networks families such as deep rectified linear unit (ReLU)\nnetworks. Put differently, TNs have the capacity to (near to) optimally\napproximate many function classes -- without being adapted to the particular\nclass in question. To answer the latter: as a candidate model class we consider\napproximation classes of TNs and show that these are (quasi-)Banach spaces,\nthat many types of classical smoothness spaces are continuously embedded into\nsaid approximation classes and that TN approximation classes are themselves not\nembedded in any classical smoothness space.\n", "versions": [{"version": "v1", "created": "Thu, 28 Jan 2021 11:09:40 GMT"}], "update_date": "2021-01-29", "authors_parsed": [["Ali", "Mazen", ""], ["Nouy", "Anthony", ""]]}, {"id": "2101.11935", "submitter": "Michal Kazmierski", "authors": "Michal Kazmierski, Mattea Welch, Sejin Kim, Chris McIntosh, Princess\n  Margaret Head and Neck Cancer Group, Katrina Rey-McIntyre, Shao Hui Huang,\n  Tirth Patel, Tony Tadic, Michael Milosevic, Fei-Fei Liu, Andrew Hope, Scott\n  Bratman and Benjamin Haibe-Kains", "title": "A Machine Learning Challenge for Prognostic Modelling in Head and Neck\n  Cancer Using Multi-modal Data", "comments": "27 pages, 7 figures, under review", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Accurate prognosis for an individual patient is a key component of precision\noncology. Recent advances in machine learning have enabled the development of\nmodels using a wider range of data, including imaging. Radiomics aims to\nextract quantitative predictive and prognostic biomarkers from routine medical\nimaging, but evidence for computed tomography radiomics for prognosis remains\ninconclusive. We have conducted an institutional machine learning challenge to\ndevelop an accurate model for overall survival prediction in head and neck\ncancer using clinical data etxracted from electronic medical records and\npre-treatment radiological images, as well as to evaluate the true added\nbenefit of radiomics for head and neck cancer prognosis. Using a large,\nretrospective dataset of 2,552 patients and a rigorous evaluation framework, we\ncompared 12 different submissions using imaging and clinical data, separately\nor in combination. The winning approach used non-linear, multitask learning on\nclinical data and tumour volume, achieving high prognostic accuracy for 2-year\nand lifetime survival prediction and outperforming models relying on clinical\ndata only, engineered radiomics and deep learning. Combining all submissions in\nan ensemble model resulted in improved accuracy, with the highest gain from a\nimage-based deep learning model. Our results show the potential of machine\nlearning and simple, informative prognostic factors in combination with large\ndatasets as a tool to guide personalized cancer care.\n", "versions": [{"version": "v1", "created": "Thu, 28 Jan 2021 11:20:34 GMT"}], "update_date": "2021-01-29", "authors_parsed": [["Kazmierski", "Michal", ""], ["Welch", "Mattea", ""], ["Kim", "Sejin", ""], ["McIntosh", "Chris", ""], ["Head", "Princess Margaret", ""], ["Group", "Neck Cancer", ""], ["Rey-McIntyre", "Katrina", ""], ["Huang", "Shao Hui", ""], ["Patel", "Tirth", ""], ["Tadic", "Tony", ""], ["Milosevic", "Michael", ""], ["Liu", "Fei-Fei", ""], ["Hope", "Andrew", ""], ["Bratman", "Scott", ""], ["Haibe-Kains", "Benjamin", ""]]}, {"id": "2101.11948", "submitter": "Sander Van Cranenburgh", "authors": "S. Van Cranenburgh, S. Wang, A. Vij, F. Pereira, J. Walker", "title": "Choice modelling in the age of machine learning", "comments": "31 pages, 2 tables, 0 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "econ.EM cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Since its inception, the choice modelling field has been dominated by\ntheory-driven models. The recent emergence and growing popularity of machine\nlearning models offer an alternative data-driven approach. Machine learning\nmodels, techniques and practices could help overcome problems and limitations\nof the current theory-driven modelling paradigm, e.g. relating to the\nad-hocness in search for the optimal model specification, and theory-driven\nchoice model's inability to work with text and image data. However, despite the\npotential value of machine learning to improve choice modelling practices, the\nchoice modelling field has been somewhat hesitant to embrace machine learning.\nThe aim of this paper is to facilitate (further) integration of machine\nlearning in the choice modelling field. To achieve this objective, we make the\ncase that (further) integration of machine learning in the choice modelling\nfield is beneficial for the choice modelling field, and, we shed light on where\nthe benefits of further integration can be found. Specifically, we take the\nfollowing approach. First, we clarify the similarities and differences between\nthe two modelling paradigms. Second, we provide a literature overview on the\nuse of machine learning for choice modelling. Third, we reinforce the strengths\nof the current theory-driven modelling paradigm and compare this with the\nmachine learning modelling paradigm, Fourth, we identify opportunities for\nembracing machine learning for choice modelling, while recognising the\nstrengths of the current theory-driven paradigm. Finally, we put forward a\nvision on the future relationship between the theory-driven choice models and\nmachine learning.\n", "versions": [{"version": "v1", "created": "Thu, 28 Jan 2021 11:57:08 GMT"}], "update_date": "2021-01-29", "authors_parsed": [["Van Cranenburgh", "S.", ""], ["Wang", "S.", ""], ["Vij", "A.", ""], ["Pereira", "F.", ""], ["Walker", "J.", ""]]}, {"id": "2101.11950", "submitter": "Nikolai Stulov", "authors": "Nikolay Stulov and Michael Chertkov", "title": "Neural Particle Image Velocimetry", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the past decades, great progress has been made in the field of optical and\nparticle-based measurement techniques for experimental analysis of fluid flows.\nParticle Image Velocimetry (PIV) technique is widely used to identify flow\nparameters from time-consecutive snapshots of particles injected into the\nfluid. The computation is performed as post-processing of the experimental data\nvia proximity measure between particles in frames of reference. However, the\npost-processing step becomes problematic as the motility and density of the\nparticles increases, since the data emerges in extreme rates and volumes.\nMoreover, existing algorithms for PIV either provide sparse estimations of the\nflow or require large computational time frame preventing from on-line use. The\ngoal of this manuscript is therefore to develop an accurate on-line algorithm\nfor estimation of the fine-grained velocity field from PIV data. As the data\nconstitutes a pair of images, we employ computer vision methods to solve the\nproblem. In this work, we introduce a convolutional neural network adapted to\nthe problem, namely Volumetric Correspondence Network (VCN) which was recently\nproposed for the end-to-end optical flow estimation in computer vision. The\nnetwork is thoroughly trained and tested on a dataset containing both synthetic\nand real flow data. Experimental results are analyzed and compared to that of\nconventional methods as well as other recently introduced methods based on\nneural networks. Our analysis indicates that the proposed approach provides\nimproved efficiency also keeping accuracy on par with other state-of-the-art\nmethods in the field. We also verify through a-posteriori tests that our newly\nconstructed VCN schemes are reproducing well physically relevant statistics of\nvelocity and velocity gradients.\n", "versions": [{"version": "v1", "created": "Thu, 28 Jan 2021 12:03:39 GMT"}], "update_date": "2021-01-29", "authors_parsed": [["Stulov", "Nikolay", ""], ["Chertkov", "Michael", ""]]}, {"id": "2101.11970", "submitter": "Diego Rojo", "authors": "Diego Rojo, Nyi Nyi Htun, Denis Parra, Robin De Croon and Katrien\n  Verbert", "title": "AHMoSe: A Knowledge-Based Visual Support System for Selecting Regression\n  Machine Learning Models", "comments": "36 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Decision support systems have become increasingly popular in the domain of\nagriculture. With the development of Automated Machine Learning, agricultural\nexperts are now able to train, evaluate and make predictions using cutting edge\nmachine learning (ML) models without the need for much ML knowledge. Although\nthis automated approach has led to successful results in many scenarios, in\ncertain cases (e.g., when few labeled datasets are available) choosing among\ndifferent models with similar performance metrics is a difficult task.\nFurthermore, these systems do not commonly allow users to incorporate their\ndomain knowledge that could facilitate the task of model selection, and to gain\ninsight into the prediction system for eventual decision making. To address\nthese issues, in this paper we present AHMoSe, a visual support system that\nallows domain experts to better understand, diagnose and compare different\nregression models, primarily by enriching model-agnostic explanations with\ndomain knowledge. To validate AHMoSE, we describe a use case scenario in the\nviticulture domain, grape quality prediction, where the system enables users to\ndiagnose and select prediction models that perform better. We also discuss\nfeedback concerning the design of the tool from both ML and viticulture\nexperts.\n", "versions": [{"version": "v1", "created": "Thu, 28 Jan 2021 12:55:06 GMT"}], "update_date": "2021-01-29", "authors_parsed": [["Rojo", "Diego", ""], ["Htun", "Nyi Nyi", ""], ["Parra", "Denis", ""], ["De Croon", "Robin", ""], ["Verbert", "Katrien", ""]]}, {"id": "2101.11972", "submitter": "Shuhan Zhang", "authors": "Ruqian Lu and Shuhan Zhang", "title": "PSpan:Mining Frequent Subnets of Petri Nets", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes for the first time an algorithm PSpan for mining frequent\ncomplete subnets from a set of Petri nets. We introduced the concept of\ncomplete subnets and the net graph representation. PSpan transforms Petri nets\nin net graphs and performs sub-net graph mining on them, then transforms the\nresults back to frequent subnets. PSpan follows the pattern growth approach and\nhas similar complexity like gSpan in graph mining. Experiments have been done\nto confirm PSpan's reliability and complexity. Besides C/E nets, it applies\nalso to a set of other Petri net subclasses.\n", "versions": [{"version": "v1", "created": "Thu, 28 Jan 2021 12:56:48 GMT"}], "update_date": "2021-01-29", "authors_parsed": [["Lu", "Ruqian", ""], ["Zhang", "Shuhan", ""]]}, {"id": "2101.11981", "submitter": "Yaqi Xie", "authors": "Yaqi Xie, Fan Zhou, Harold Soh", "title": "Embedding Symbolic Temporal Knowledge into Deep Sequential Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sequences and time-series often arise in robot tasks, e.g., in activity\nrecognition and imitation learning. In recent years, deep neural networks\n(DNNs) have emerged as an effective data-driven methodology for processing\nsequences given sufficient training data and compute resources. However, when\ndata is limited, simpler models such as logic/rule-based methods work\nsurprisingly well, especially when relevant prior knowledge is applied in their\nconstruction. However, unlike DNNs, these \"structured\" models can be difficult\nto extend, and do not work well with raw unstructured data. In this work, we\nseek to learn flexible DNNs, yet leverage prior temporal knowledge when\navailable. Our approach is to embed symbolic knowledge expressed as linear\ntemporal logic (LTL) and use these embeddings to guide the training of deep\nmodels. Specifically, we construct semantic-based embeddings of automata\ngenerated from LTL formula via a Graph Neural Network. Experiments show that\nthese learnt embeddings can lead to improvements in downstream robot tasks such\nas sequential action recognition and imitation learning.\n", "versions": [{"version": "v1", "created": "Thu, 28 Jan 2021 13:17:46 GMT"}], "update_date": "2021-01-29", "authors_parsed": [["Xie", "Yaqi", ""], ["Zhou", "Fan", ""], ["Soh", "Harold", ""]]}, {"id": "2101.11984", "submitter": "George Papakostas Prof.", "authors": "V.N. Tsakalidou, P. Mitsou, G.A. Papakostas", "title": "Machine learning for cloud resources management -- An overview", "comments": "13 pages, 3 figures, to be published in proceedings of International\n  Conference on Expert Clouds and Applications (ICOECA 2021)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NI", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Nowadays, an important topic that is considered a lot is how to integrate\nMachine Learning(ML) to cloud resources management. In this study, our goal is\nto explore the most important cloud resources management issues that have been\ncombined with ML and which present many promising results. To accomplish this,\nwe used chronological charts based on some keywords that we considered\nimportant and tried to answer the question: is ML suitable for resources\nmanagement problems in the cloud? Furthermore, a short discussion takes place\non the data that are available and the open challenges on it. A big collection\nof researches is used to make sensible comparisons between the ML techniques\nthat are used in the different kinds of cloud resources management fields and\nwe propose the most suitable ML model for each field. 1\n", "versions": [{"version": "v1", "created": "Thu, 28 Jan 2021 13:23:00 GMT"}], "update_date": "2021-01-29", "authors_parsed": [["Tsakalidou", "V. N.", ""], ["Mitsou", "P.", ""], ["Papakostas", "G. A.", ""]]}, {"id": "2101.11987", "submitter": "Shankar Gangisetty", "authors": "Sindhu Hegde and Shankar Gangisetty", "title": "PIG-Net: Inception based Deep Learning Architecture for 3D Point Cloud\n  Segmentation", "comments": "11 pages, 5 Figures, 6 Tables, Accepted in Computers & Graphics\n  Journal 2021", "journal-ref": null, "doi": "10.1016/j.cag.2021.01.004", "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Point clouds, being the simple and compact representation of surface geometry\nof 3D objects, have gained increasing popularity with the evolution of deep\nlearning networks for classification and segmentation tasks. Unlike human,\nteaching the machine to analyze the segments of an object is a challenging task\nand quite essential in various machine vision applications. In this paper, we\naddress the problem of segmentation and labelling of the 3D point clouds by\nproposing a inception based deep network architecture called PIG-Net, that\neffectively characterizes the local and global geometric details of the point\nclouds. In PIG-Net, the local features are extracted from the transformed input\npoints using the proposed inception layers and then aligned by feature\ntransform. These local features are aggregated using the global average pooling\nlayer to obtain the global features. Finally, feed the concatenated local and\nglobal features to the convolution layers for segmenting the 3D point clouds.\nWe perform an exhaustive experimental analysis of the PIG-Net architecture on\ntwo state-of-the-art datasets, namely, ShapeNet [1] and PartNet [2]. We\nevaluate the effectiveness of our network by performing ablation study.\n", "versions": [{"version": "v1", "created": "Thu, 28 Jan 2021 13:27:55 GMT"}], "update_date": "2021-01-29", "authors_parsed": [["Hegde", "Sindhu", ""], ["Gangisetty", "Shankar", ""]]}, {"id": "2101.11992", "submitter": "Esther Derman", "authors": "Esther Derman, Gal Dalal, Shie Mannor", "title": "Acting in Delayed Environments with Non-Stationary Markov Policies", "comments": "Accepted to ICLR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The standard Markov Decision Process (MDP) formulation hinges on the\nassumption that an action is executed immediately after it was chosen. However,\nassuming it is often unrealistic and can lead to catastrophic failures in\napplications such as robotic manipulation, cloud computing, and finance. We\nintroduce a framework for learning and planning in MDPs where the\ndecision-maker commits actions that are executed with a delay of $m$ steps. The\nbrute-force state augmentation baseline where the state is concatenated to the\nlast $m$ committed actions suffers from an exponential complexity in $m$, as we\nshow for policy iteration. We then prove that with execution delay,\ndeterministic Markov policies in the original state-space are sufficient for\nattaining maximal reward, but need to be non-stationary. As for stationary\nMarkov policies, we show they are sub-optimal in general. Consequently, we\ndevise a non-stationary Q-learning style model-based algorithm that solves\ndelayed execution tasks without resorting to state-augmentation. Experiments on\ntabular, physical, and Atari domains reveal that it converges quickly to high\nperformance even for substantial delays, while standard approaches that either\nignore the delay or rely on state-augmentation struggle or fail due to\ndivergence. The code is available at\nhttps://github.com/galdl/rl_delay_basic.git.\n", "versions": [{"version": "v1", "created": "Thu, 28 Jan 2021 13:35:37 GMT"}, {"version": "v2", "created": "Thu, 18 Mar 2021 08:40:13 GMT"}], "update_date": "2021-03-19", "authors_parsed": [["Derman", "Esther", ""], ["Dalal", "Gal", ""], ["Mannor", "Shie", ""]]}, {"id": "2101.12002", "submitter": "Sebastien Destercke", "authors": "Soundouss Messoudi, S\\'ebastien Destercke, Sylvain Rousseau", "title": "Copula-based conformal prediction for Multi-Target Regression", "comments": "17 pages, 8 figures, under review", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  There are relatively few works dealing with conformal prediction for\nmulti-task learning issues, and this is particularly true for multi-target\nregression. This paper focuses on the problem of providing valid (i.e.,\nfrequency calibrated) multi-variate predictions. To do so, we propose to use\ncopula functions applied to deep neural networks for inductive conformal\nprediction. We show that the proposed method ensures efficiency and validity\nfor multi-target regression problems on various data sets.\n", "versions": [{"version": "v1", "created": "Thu, 28 Jan 2021 14:06:25 GMT"}], "update_date": "2021-01-29", "authors_parsed": [["Messoudi", "Soundouss", ""], ["Destercke", "S\u00e9bastien", ""], ["Rousseau", "Sylvain", ""]]}, {"id": "2101.12010", "submitter": "Chengqiao Lin", "authors": "Wei Zeng, Chengqiao Lin, Kang Liu, Juncong Lin, Anthony K. H. Tung", "title": "Modeling Spatial Nonstationarity via Deformable Convolutions for Deep\n  Traffic Flow Prediction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.soc-ph cs.CV cs.LG cs.SY eess.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural networks are being increasingly used for short-term traffic flow\nprediction. Existing convolution-based approaches typically partition an\nunderlying territory into grid-like spatial units, and employ standard\nconvolutions to learn spatial dependence among the units. However, standard\nconvolutions with fixed geometric structures cannot fully model the\nnonstationary characteristics of local traffic flows. To overcome the\ndeficiency, we introduce deformable convolution that augments the spatial\nsampling locations with additional offsets, to enhance the modeling capability\nof spatial nonstationarity. On this basis, we design a deep deformable\nconvolutional residual network, namely DeFlow-Net, that can effectively model\nglobal spatial dependence, local spatial nonstationarity, and temporal\nperiodicity of traffic flows. Furthermore, to fit better with convolutions, we\nsuggest to first aggregate traffic flows according to pre-conceived regions of\ninterest, then dispose to sequentially organized raster images for network\ninput. Extensive experiments on real-world traffic flows demonstrate that\nDeFlow-Net outperforms existing solutions using standard convolutions, and\nspatial partition by pre-conceived regions further enhances the performance.\nFinally, we demonstrate the advantage of DeFlow-Net in maintaining spatial\nautocorrelation, and reveal the impacts of partition shapes and scales on deep\ntraffic flow prediction.\n", "versions": [{"version": "v1", "created": "Fri, 8 Jan 2021 10:16:03 GMT"}], "update_date": "2021-01-29", "authors_parsed": [["Zeng", "Wei", ""], ["Lin", "Chengqiao", ""], ["Liu", "Kang", ""], ["Lin", "Juncong", ""], ["Tung", "Anthony K. H.", ""]]}, {"id": "2101.12015", "submitter": "Vinicius Carid\\'a", "authors": "Paulo Finardi, Jos\\'e Di\\'e Viegas, Gustavo T. Ferreira, Alex F.\n  Mansano, Vinicius F. Carid\\'a", "title": "BERTa\\'u: Ita\\'u BERT for digital customer service", "comments": "10 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the last few years, three major topics received increased interest: deep\nlearning, NLP and conversational agents. Bringing these three topics together\nto create an amazing digital customer experience and indeed deploy in\nproduction and solve real-world problems is something innovative and\ndisruptive. We introduce a new Portuguese financial domain language\nrepresentation model called BERTa\\'u. BERTa\\'u is an uncased BERT-base trained\nfrom scratch with data from the Ita\\'u virtual assistant chatbot solution. Our\nnovel contribution is that BERTa\\'u pretrained language model requires less\ndata, reached state-of-the-art performance in three NLP tasks, and generates a\nsmaller and lighter model that makes the deployment feasible. We developed\nthree tasks to validate our model: information retrieval with Frequently Asked\nQuestions (FAQ) from Ita\\'u bank, sentiment analysis from our virtual assistant\ndata, and a NER solution. All proposed tasks are real-world solutions in\nproduction on our environment and the usage of a specialist model proved to be\neffective when compared to Google BERT multilingual and the DPRQuestionEncoder\nfrom Facebook, available at Hugging Face. The BERTa\\'u improves the performance\nin 22% of FAQ Retrieval MRR metric, 2.1% in Sentiment Analysis F1 score, 4.4%\nin NER F1 score and can also represent the same sequence in up to 66% fewer\ntokens when compared to \"shelf models\".\n", "versions": [{"version": "v1", "created": "Thu, 28 Jan 2021 14:29:03 GMT"}, {"version": "v2", "created": "Thu, 22 Jul 2021 01:46:41 GMT"}, {"version": "v3", "created": "Sun, 25 Jul 2021 23:26:02 GMT"}], "update_date": "2021-07-27", "authors_parsed": [["Finardi", "Paulo", ""], ["Viegas", "Jos\u00e9 Di\u00e9", ""], ["Ferreira", "Gustavo T.", ""], ["Mansano", "Alex F.", ""], ["Carid\u00e1", "Vinicius F.", ""]]}, {"id": "2101.12016", "submitter": "Peter Bajcsy", "authors": "Peter Bajcsy and Michael Majurski", "title": "Baseline Pruning-Based Approach to Trojan Detection in Neural Networks", "comments": "The funding for all authors was provided by IARPA:\n  IARPA-20001-D2020-2007180011", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.AI cs.LG", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  This paper addresses the problem of detecting trojans in neural networks\n(NNs) by analyzing systematically pruned NN models. Our pruning-based approach\nconsists of three main steps. First, detect any deviations from the reference\nlook-up tables of model file sizes and model graphs. Next, measure the accuracy\nof a set of systematically pruned NN models following multiple pruning schemas.\nFinally, classify a NN model as clean or poisoned by applying a mapping between\naccuracy measurements and NN model labels. This work outlines a theoretical and\nexperimental framework for finding the optimal mapping over a large search\nspace of pruning parameters. Based on our experiments using Round 1 and Round 2\nTrojAI Challenge datasets, the approach achieves average classification\naccuracy of 69.73 % and 82.41% respectively with an average processing time of\nless than 60 s per model. For both datasets random guessing would produce 50%\nclassification accuracy. Reference model graphs and source code are available\nfrom GitHub.\n", "versions": [{"version": "v1", "created": "Fri, 22 Jan 2021 23:10:31 GMT"}, {"version": "v2", "created": "Tue, 9 Feb 2021 14:58:21 GMT"}], "update_date": "2021-02-10", "authors_parsed": [["Bajcsy", "Peter", ""], ["Majurski", "Michael", ""]]}, {"id": "2101.12017", "submitter": "Quynh Nguyen", "authors": "Quynh Nguyen", "title": "A Fully Rigorous Proof of the Derivation of Xavier and He's\n  Initialization for Deep ReLU Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A fully rigorous proof of the derivation of Xavier/He's initialization for\nReLU nets is given.\n", "versions": [{"version": "v1", "created": "Thu, 21 Jan 2021 21:43:39 GMT"}], "update_date": "2021-01-29", "authors_parsed": [["Nguyen", "Quynh", ""]]}, {"id": "2101.12031", "submitter": "Mohit Sewak", "authors": "Hemant Rathore and Sanjay K. Sahay and Piyush Nikam and Mohit Sewak", "title": "Robust Android Malware Detection System against Adversarial Attacks\n  using Q-Learning", "comments": "Inf Syst Front (2020)", "journal-ref": null, "doi": "10.1007/s10796-020-10083-8", "report-no": null, "categories": "cs.CR cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The current state-of-the-art Android malware detection systems are based on\nmachine learning and deep learning models. Despite having superior performance,\nthese models are susceptible to adversarial attacks. Therefore in this paper,\nwe developed eight Android malware detection models based on machine learning\nand deep neural network and investigated their robustness against adversarial\nattacks. For this purpose, we created new variants of malware using\nReinforcement Learning, which will be misclassified as benign by the existing\nAndroid malware detection models. We propose two novel attack strategies,\nnamely single policy attack and multiple policy attack using reinforcement\nlearning for white-box and grey-box scenario respectively. Putting ourselves in\nthe adversary's shoes, we designed adversarial attacks on the detection models\nwith the goal of maximizing fooling rate, while making minimum modifications to\nthe Android application and ensuring that the app's functionality and behavior\ndo not change. We achieved an average fooling rate of 44.21% and 53.20% across\nall the eight detection models with a maximum of five modifications using a\nsingle policy attack and multiple policy attack, respectively. The highest\nfooling rate of 86.09% with five changes was attained against the decision\ntree-based model using the multiple policy approach. Finally, we propose an\nadversarial defense strategy that reduces the average fooling rate by threefold\nto 15.22% against a single policy attack, thereby increasing the robustness of\nthe detection models i.e. the proposed model can effectively detect variants\n(metamorphic) of malware. The experimental analysis shows that our proposed\nAndroid malware detection system using reinforcement learning is more robust\nagainst adversarial attacks.\n", "versions": [{"version": "v1", "created": "Wed, 27 Jan 2021 16:45:57 GMT"}], "update_date": "2021-01-29", "authors_parsed": [["Rathore", "Hemant", ""], ["Sahay", "Sanjay K.", ""], ["Nikam", "Piyush", ""], ["Sewak", "Mohit", ""]]}, {"id": "2101.12037", "submitter": "Demetres Kostas", "authors": "Demetres Kostas, Stephane Aroca-Ouellette, Frank Rudzicz", "title": "BENDR: using transformers and a contrastive self-supervised learning\n  task to learn from massive amounts of EEG data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE q-bio.QM", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Deep neural networks (DNNs) used for brain-computer-interface (BCI)\nclassification are commonly expected to learn general features when trained\nacross a variety of contexts, such that these features could be fine-tuned to\nspecific contexts. While some success is found in such an approach, we suggest\nthat this interpretation is limited and an alternative would better leverage\nthe newly (publicly) available massive EEG datasets. We consider how to adapt\ntechniques and architectures used for language modelling (LM), that appear\ncapable of ingesting awesome amounts of data, towards the development of\nencephalography modelling (EM) with DNNs in the same vein. We specifically\nadapt an approach effectively used for automatic speech recognition, which\nsimilarly (to LMs) uses a self-supervised training objective to learn\ncompressed representations of raw data signals. After adaptation to EEG, we\nfind that a single pre-trained model is capable of modelling completely novel\nraw EEG sequences recorded with differing hardware, and different subjects\nperforming different tasks. Furthermore, both the internal representations of\nthis model and the entire architecture can be fine-tuned to a variety of\ndownstream BCI and EEG classification tasks, outperforming prior work in more\ntask-specific (sleep stage classification) self-supervision.\n", "versions": [{"version": "v1", "created": "Thu, 28 Jan 2021 14:54:01 GMT"}], "update_date": "2021-01-29", "authors_parsed": [["Kostas", "Demetres", ""], ["Aroca-Ouellette", "Stephane", ""], ["Rudzicz", "Frank", ""]]}, {"id": "2101.12041", "submitter": "Amitojdeep Singh", "authors": "Amitojdeep Singh, Sourya Sengupta, Mohammed Abdul Rasheed,\n  Varadharajan Jayakumar, and Vasudevan Lakshminarayanan", "title": "Uncertainty aware and explainable diagnosis of retinal disease", "comments": "Submitted to SPIE Medical Imaging 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Deep learning methods for ophthalmic diagnosis have shown considerable\nsuccess in tasks like segmentation and classification. However, their\nwidespread application is limited due to the models being opaque and vulnerable\nto making a wrong decision in complicated cases. Explainability methods show\nthe features that a system used to make prediction while uncertainty awareness\nis the ability of a system to highlight when it is not sure about the decision.\nThis is one of the first studies using uncertainty and explanations for\ninformed clinical decision making. We perform uncertainty analysis of a deep\nlearning model for diagnosis of four retinal diseases - age-related macular\ndegeneration (AMD), central serous retinopathy (CSR), diabetic retinopathy\n(DR), and macular hole (MH) using images from a publicly available (OCTID)\ndataset. Monte Carlo (MC) dropout is used at the test time to generate a\ndistribution of parameters and the predictions approximate the predictive\nposterior of a Bayesian model. A threshold is computed using the distribution\nand uncertain cases can be referred to the ophthalmologist thus avoiding an\nerroneous diagnosis. The features learned by the model are visualized using a\nproven attribution method from a previous study. The effects of uncertainty on\nmodel performance and the relationship between uncertainty and explainability\nare discussed in terms of clinical significance. The uncertainty information\nalong with the heatmaps make the system more trustworthy for use in clinical\nsettings.\n", "versions": [{"version": "v1", "created": "Tue, 26 Jan 2021 23:37:30 GMT"}], "update_date": "2021-01-29", "authors_parsed": [["Singh", "Amitojdeep", ""], ["Sengupta", "Sourya", ""], ["Rasheed", "Mohammed Abdul", ""], ["Jayakumar", "Varadharajan", ""], ["Lakshminarayanan", "Vasudevan", ""]]}, {"id": "2101.12044", "submitter": "Wilson Marc\\'ilio-Jr", "authors": "Wilson E. Marc\\'ilio-Jr, Danilo M. Eler, Rog\\'erio E. Garcia", "title": "Contrastive analysis for scatter plot-based representations of\n  dimensionality reduction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Exploring multidimensional datasets is a ubiquitous part of the ones working\nwith data, where interpreting clusters is one of the main tasks. These\nmultidimensional datasets are usually encoded using scatter-plots\nrepresentations, where spatial proximity encodes similarity among data samples.\nIn the literature, techniques try to understand the scatter plot organization\nby visualizing the importance of the features for clusters definition with\ninteraction and layout enrichment strategies. However, the approaches used to\ninterpret dimensionality reduction usually do not differentiate clusters well,\nwhich hampers analysis where the focus is to understand the differences among\nclusters. This paper introduces a methodology to visually explore\nmultidimensional datasets and interpret clusters' formation based on the\ncontrastive analysis. We also introduce a bipartite graph to visually interpret\nand explore the relationship between the statistical variables used to\nunderstand how the attributes influenced cluster formation. Our methodology is\nvalidated through case studies. We explore a multivariate dataset of patients\nwith vertebral problems and two document collections, one related to news\narticles and other related to tweets about COVID-19 symptoms. Finally, we also\nvalidate our approach through quantitative results to demonstrate how it can be\nrobust enough to support multidimensional analysis.\n", "versions": [{"version": "v1", "created": "Tue, 26 Jan 2021 01:16:31 GMT"}], "update_date": "2021-01-29", "authors_parsed": [["Marc\u00edlio-Jr", "Wilson E.", ""], ["Eler", "Danilo M.", ""], ["Garcia", "Rog\u00e9rio E.", ""]]}, {"id": "2101.12072", "submitter": "Kashif Rasul", "authors": "Kashif Rasul, Calvin Seward, Ingmar Schuster, Roland Vollgraf", "title": "Autoregressive Denoising Diffusion Models for Multivariate Probabilistic\n  Time Series Forecasting", "comments": null, "journal-ref": "Proceedings of the 38th International Conference on Machine\n  Learning, PMLR 139:8857-8868, 2021", "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this work, we propose \\texttt{TimeGrad}, an autoregressive model for\nmultivariate probabilistic time series forecasting which samples from the data\ndistribution at each time step by estimating its gradient. To this end, we use\ndiffusion probabilistic models, a class of latent variable models closely\nconnected to score matching and energy-based methods. Our model learns\ngradients by optimizing a variational bound on the data likelihood and at\ninference time converts white noise into a sample of the distribution of\ninterest through a Markov chain using Langevin sampling. We demonstrate\nexperimentally that the proposed autoregressive denoising diffusion model is\nthe new state-of-the-art multivariate probabilistic forecasting method on\nreal-world data sets with thousands of correlated dimensions. We hope that this\nmethod is a useful tool for practitioners and lays the foundation for future\nresearch in this area.\n", "versions": [{"version": "v1", "created": "Thu, 28 Jan 2021 15:46:10 GMT"}, {"version": "v2", "created": "Tue, 2 Feb 2021 12:32:30 GMT"}], "update_date": "2021-07-09", "authors_parsed": [["Rasul", "Kashif", ""], ["Seward", "Calvin", ""], ["Schuster", "Ingmar", ""], ["Vollgraf", "Roland", ""]]}, {"id": "2101.12081", "submitter": "Alessia Bertugli", "authors": "Alessia Bertugli, Stefano Vincenzi, Simone Calderara, Andrea Passerini", "title": "Generalising via Meta-Examples for Continual Learning in the Wild", "comments": "16 pages, 11 figures, 13 tables. arXiv admin note: substantial text\n  overlap with arXiv:2009.08107", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning quickly and continually is still an ambitious task for neural\nnetworks. Indeed, many real-world applications do not reflect the learning\nsetting where neural networks shine, as data are usually few, mostly unlabelled\nand come as a stream. To narrow this gap, we introduce FUSION - Few-shot\nUnSupervIsed cONtinual learning - a novel strategy which aims to deal with\nneural networks that \"learn in the wild\", simulating a real distribution and\nflow of unbalanced tasks. We equip FUSION with MEML - Meta-Example\nMeta-Learning - a new module that simultaneously alleviates catastrophic\nforgetting and favours the generalisation and future learning of new tasks. To\nencourage features reuse during the meta-optimisation, our model exploits a\nsingle inner loop per task, taking advantage of an aggregated representation\nachieved through the use of a self-attention mechanism. To further enhance the\ngeneralisation capability of MEML, we extend it by adopting a technique that\ncreates various augmented tasks and optimises over the hardest. Experimental\nresults on few-shot learning benchmarks show that our model exceeds the other\nbaselines in both FUSION and fully supervised case. We also explore how it\nbehaves in standard continual learning consistently outperforming\nstate-of-the-art approaches.\n", "versions": [{"version": "v1", "created": "Thu, 28 Jan 2021 15:51:54 GMT"}], "update_date": "2021-01-29", "authors_parsed": [["Bertugli", "Alessia", ""], ["Vincenzi", "Stefano", ""], ["Calderara", "Simone", ""], ["Passerini", "Andrea", ""]]}, {"id": "2101.12087", "submitter": "Ziyu Yao", "authors": "Ziyu Yao, Frank F. Xu, Pengcheng Yin, Huan Sun, Graham Neubig", "title": "Learning Structural Edits via Incremental Tree Transformations", "comments": "ICLR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While most neural generative models generate outputs in a single pass, the\nhuman creative process is usually one of iterative building and refinement.\nRecent work has proposed models of editing processes, but these mostly focus on\nediting sequential data and/or only model a single editing pass. In this paper,\nwe present a generic model for incremental editing of structured data (i.e.,\n\"structural edits\"). Particularly, we focus on tree-structured data, taking\nabstract syntax trees of computer programs as our canonical example. Our editor\nlearns to iteratively generate tree edits (e.g., deleting or adding a subtree)\nand applies them to the partially edited data, thereby the entire editing\nprocess can be formulated as consecutive, incremental tree transformations. To\nshow the unique benefits of modeling tree edits directly, we further propose a\nnovel edit encoder for learning to represent edits, as well as an imitation\nlearning method that allows the editor to be more robust. We evaluate our\nproposed editor on two source code edit datasets, where results show that, with\nthe proposed edit encoder, our editor significantly improves accuracy over\nprevious approaches that generate the edited program directly in one pass.\nFinally, we demonstrate that training our editor to imitate experts and correct\nits mistakes dynamically can further improve its performance.\n", "versions": [{"version": "v1", "created": "Thu, 28 Jan 2021 16:11:32 GMT"}, {"version": "v2", "created": "Fri, 5 Mar 2021 00:46:18 GMT"}], "update_date": "2021-03-08", "authors_parsed": [["Yao", "Ziyu", ""], ["Xu", "Frank F.", ""], ["Yin", "Pengcheng", ""], ["Sun", "Huan", ""], ["Neubig", "Graham", ""]]}, {"id": "2101.12090", "submitter": "Manoj B.R", "authors": "B. R. Manoj, Meysam Sadeghi, Erik G. Larsson", "title": "Adversarial Attacks on Deep Learning Based Power Allocation in a Massive\n  MIMO Network", "comments": "to be published in proceedings of IEEE International Conference on\n  Communications (ICC) 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.LG eess.SP math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning (DL) is becoming popular as a new tool for many applications in\nwireless communication systems. However, for many classification tasks (e.g.,\nmodulation classification) it has been shown that DL-based wireless systems are\nsusceptible to adversarial examples; adversarial examples are well-crafted\nmalicious inputs to the neural network (NN) with the objective to cause\nerroneous outputs. In this paper, we extend this to regression problems and\nshow that adversarial attacks can break DL-based power allocation in the\ndownlink of a massive multiple-input-multiple-output (maMIMO) network.\nSpecifically, we extend the fast gradient sign method (FGSM), momentum\niterative FGSM, and projected gradient descent adversarial attacks in the\ncontext of power allocation in a maMIMO system. We benchmark the performance of\nthese attacks and show that with a small perturbation in the input of the NN,\nthe white-box attacks can result in infeasible solutions up to 86%.\nFurthermore, we investigate the performance of black-box attacks. All the\nevaluations conducted in this work are based on an open dataset and NN models,\nwhich are publicly available.\n", "versions": [{"version": "v1", "created": "Thu, 28 Jan 2021 16:18:19 GMT"}], "update_date": "2021-01-29", "authors_parsed": [["Manoj", "B. R.", ""], ["Sadeghi", "Meysam", ""], ["Larsson", "Erik G.", ""]]}, {"id": "2101.12097", "submitter": "Hamidreza Habibollahi Najaf Abadi", "authors": "Hamidreza Habibollahi Najaf Abadi", "title": "Adversarial Machine Learning Attacks on Condition-Based Maintenance\n  Capabilities", "comments": "9 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Condition-based maintenance (CBM) strategies exploit machine learning models\nto assess the health status of systems based on the collected data from the\nphysical environment, while machine learning models are vulnerable to\nadversarial attacks. A malicious adversary can manipulate the collected data to\ndeceive the machine learning model and affect the CBM system's performance.\nAdversarial machine learning techniques introduced in the computer vision\ndomain can be used to make stealthy attacks on CBM systems by adding\nperturbation to data to confuse trained models. The stealthy nature causes\ndifficulty and delay in detection of the attacks. In this paper, adversarial\nmachine learning in the domain of CBM is introduced. A case study shows how\nadversarial machine learning can be used to attack CBM capabilities.\nAdversarial samples are crafted using the Fast Gradient Sign method, and the\nperformance of a CBM system under attack is investigated. The obtained results\nreveal that CBM systems are vulnerable to adversarial machine learning attacks\nand defense strategies need to be considered.\n", "versions": [{"version": "v1", "created": "Thu, 28 Jan 2021 16:34:04 GMT"}], "update_date": "2021-01-29", "authors_parsed": [["Abadi", "Hamidreza Habibollahi Najaf", ""]]}, {"id": "2101.12099", "submitter": "Salman Seyedi", "authors": "Salman Seyedi, Li Xiong, Shamim Nemati, Gari D. Clifford", "title": "An Analysis Of Protected Health Information Leakage In Deep-Learning\n  Based De-Identification Algorithms", "comments": "8 pages, 5 figures, PPAI-2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NA math.NA", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  The increasing complexity of algorithms for analyzing medical data, including\nde-identification tasks, raises the possibility that complex algorithms are\nlearning not just the general representation of the problem, but specifics of\ngiven individuals within the data. Modern legal frameworks specifically\nprohibit the intentional or accidental distribution of patient data, but have\nnot addressed this potential avenue for leakage of such protected health\ninformation. Modern deep learning algorithms have the highest potential of such\nleakage due to complexity of the models. Recent research in the field has\nhighlighted such issues in non-medical data, but all analysis is likely to be\ndata and algorithm specific. We, therefore, chose to analyze a state-of-the-art\nfree-text de-identification algorithm based on LSTM (Long Short-Term Memory)\nand its potential in encoding any individual in the training set. Using the\ni2b2 Challenge Data, we trained, then analyzed the model to assess whether the\noutput of the LSTM, before the compression layer of the classifier, could be\nused to estimate the membership of the training data. Furthermore, we used\ndifferent attacks including membership inference attack method to attack the\nmodel. Results indicate that the attacks could not identify whether members of\nthe training data were distinguishable from non-members based on the model\noutput. This indicates that the model does not provide any strong evidence into\nthe identification of the individuals in the training data set and there is not\nyet empirical evidence it is unsafe to distribute the model for general use.\n", "versions": [{"version": "v1", "created": "Thu, 28 Jan 2021 16:38:11 GMT"}, {"version": "v2", "created": "Fri, 21 May 2021 13:23:11 GMT"}], "update_date": "2021-05-24", "authors_parsed": [["Seyedi", "Salman", ""], ["Xiong", "Li", ""], ["Nemati", "Shamim", ""], ["Clifford", "Gari D.", ""]]}, {"id": "2101.12100", "submitter": "Giulio Rossolini", "authors": "Giulio Rossolini, Alessandro Biondi, Giorgio Carlo Buttazzo", "title": "Increasing the Confidence of Deep Neural Networks by Coverage Analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  The great performance of machine learning algorithms and deep neural networks\nin several perception and control tasks is pushing the industry to adopt such\ntechnologies in safety-critical applications, as autonomous robots and\nself-driving vehicles. At present, however, several issues need to be solved to\nmake deep learning methods more trustworthy, predictable, safe, and secure\nagainst adversarial attacks. Although several methods have been proposed to\nimprove the trustworthiness of deep neural networks, most of them are tailored\nfor specific classes of adversarial examples, hence failing to detect other\ncorner cases or unsafe inputs that heavily deviate from the training samples.\n  This paper presents a lightweight monitoring architecture based on coverage\nparadigms to enhance the model robustness against different unsafe inputs. In\nparticular, four coverage analysis methods are proposed and tested in the\narchitecture for evaluating multiple detection logics. Experimental results\nshow that the proposed approach is effective in detecting both powerful\nadversarial examples and out-of-distribution inputs, introducing limited\nextra-execution time and memory requirements.\n", "versions": [{"version": "v1", "created": "Thu, 28 Jan 2021 16:38:26 GMT"}], "update_date": "2021-01-29", "authors_parsed": [["Rossolini", "Giulio", ""], ["Biondi", "Alessandro", ""], ["Buttazzo", "Giorgio Carlo", ""]]}, {"id": "2101.12101", "submitter": "Jelena Diakonikolas", "authors": "Jelena Diakonikolas and Puqian Wang", "title": "Potential Function-based Framework for Making the Gradients Small in\n  Convex and Min-Max Optimization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.DS cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Making the gradients small is a fundamental optimization problem that has\neluded unifying and simple convergence arguments in first-order optimization,\nso far primarily reserved for other convergence criteria, such as reducing the\noptimality gap. We introduce a novel potential function-based framework to\nstudy the convergence of standard methods for making the gradients small in\nsmooth convex optimization and convex-concave min-max optimization. Our\nframework is intuitive and it provides a lens for viewing algorithms that make\nthe gradients small as being driven by a trade-off between reducing either the\ngradient norm or a certain notion of an optimality gap. On the lower bounds\nside, we discuss tightness of the obtained convergence results for the convex\nsetup and provide a new lower bound for minimizing norm of cocoercive operators\nthat allows us to argue about optimality of methods in the min-max setup.\n", "versions": [{"version": "v1", "created": "Thu, 28 Jan 2021 16:41:00 GMT"}], "update_date": "2021-01-29", "authors_parsed": [["Diakonikolas", "Jelena", ""], ["Wang", "Puqian", ""]]}, {"id": "2101.12113", "submitter": "Gil Shamir", "authors": "Gil I. Shamir and Wojciech Szpankowski", "title": "Low Complexity Approximate Bayesian Logistic Regression for Sparse\n  Online Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Theoretical results show that Bayesian methods can achieve lower bounds on\nregret for online logistic regression. In practice, however, such techniques\nmay not be feasible especially for very large feature sets. Various\napproximations that, for huge sparse feature sets, diminish the theoretical\nadvantages, must be used. Often, they apply stochastic gradient methods with\nhyper-parameters that must be tuned on some surrogate loss, defeating\ntheoretical advantages of Bayesian methods. The surrogate loss, defined to\napproximate the mixture, requires techniques as Monte Carlo sampling,\nincreasing computations per example. We propose low complexity analytical\napproximations for sparse online logistic and probit regressions. Unlike\nvariational inference and other methods, our methods use analytical closed\nforms, substantially lowering computations. Unlike dense solutions, as Gaussian\nMixtures, our methods allow for sparse problems with huge feature sets without\nincreasing complexity. With the analytical closed forms, there is also no need\nfor applying stochastic gradient methods on surrogate losses, and for tuning\nand balancing learning and regularization hyper-parameters. Empirical results\ntop the performance of the more computationally involved methods. Like such\nmethods, our methods still reveal per feature and per example uncertainty\nmeasures.\n", "versions": [{"version": "v1", "created": "Thu, 28 Jan 2021 16:59:31 GMT"}], "update_date": "2021-01-29", "authors_parsed": [["Shamir", "Gil I.", ""], ["Szpankowski", "Wojciech", ""]]}, {"id": "2101.12115", "submitter": "Fabio Amadio", "authors": "Fabio Amadio, Alberto Dalla Libera, Riccardo Antonello, Daniel\n  Nikovski, Ruggero Carli, Diego Romeres", "title": "Model-Based Policy Search Using Monte Carlo Gradient Estimation with\n  Real Systems Application", "comments": "Submitted to IEEE Transactions on Robotics", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.RO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper, we present a Model-Based Reinforcement Learning algorithm\nnamed Monte Carlo Probabilistic Inference for Learning COntrol (MC-PILCO). The\nalgorithm relies on Gaussian Processes (GPs) to model the system dynamics and\non a Monte Carlo approach to estimate the policy gradient. This defines a\nframework in which we ablate the choice of the following components: (i) the\nselection of the cost function, (ii) the optimization of policies using\ndropout, (iii) an improved data efficiency through the use of structured\nkernels in the GP models. The combination of the aforementioned aspects affects\ndramatically the performance of MC-PILCO. Numerical comparisons in a simulated\ncart-pole environment show that MC-PILCO exhibits better data-efficiency and\ncontrol performance w.r.t. state-of-the-art GP-based MBRL algorithms. Finally,\nwe apply MC-PILCO to real systems, considering in particular systems with\npartially measurable states. We discuss the importance of modeling both the\nmeasurement system and the state estimators during policy optimization. The\neffectiveness of the proposed solutions has been tested in simulation and in\ntwo real systems, a Furuta pendulum and a ball-and-plate.\n", "versions": [{"version": "v1", "created": "Thu, 28 Jan 2021 17:01:15 GMT"}, {"version": "v2", "created": "Mon, 3 May 2021 16:01:23 GMT"}], "update_date": "2021-05-04", "authors_parsed": [["Amadio", "Fabio", ""], ["Libera", "Alberto Dalla", ""], ["Antonello", "Riccardo", ""], ["Nikovski", "Daniel", ""], ["Carli", "Ruggero", ""], ["Romeres", "Diego", ""]]}, {"id": "2101.12127", "submitter": "Jiri Simsa", "authors": "Derek G. Murray, Jiri Simsa, Ana Klimovic, Ihor Indyk", "title": "tf.data: A Machine Learning Data Processing Framework", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.MS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Training machine learning models requires feeding input data for models to\ningest. Input pipelines for machine learning jobs are often challenging to\nimplement efficiently as they require reading large volumes of data, applying\ncomplex transformations, and transferring data to hardware accelerators while\noverlapping computation and communication to achieve optimal performance. We\npresent tf.data, a framework for building and executing efficient input\npipelines for machine learning jobs. The tf.data API provides operators which\ncan be parameterized with user-defined computation, composed, and reused across\ndifferent machine learning domains. These abstractions allow users to focus on\nthe application logic of data processing, while tf.data's runtime ensures that\npipelines run efficiently.\n  We demonstrate that input pipeline performance is critical to the end-to-end\ntraining time of state-of-the-art machine learning models. tf.data delivers the\nhigh performance required, while avoiding the need for manual tuning of\nperformance knobs. We show that tf.data features, such as parallelism, caching,\nstatic optimizations, and non-deterministic execution are essential for high\nperformance. Finally, we characterize machine learning input pipelines for\nmillions of jobs that ran in Google's fleet, showing that input data processing\nis highly diverse and consumes a significant fraction of job resources. Our\nanalysis motivates future research directions, such as sharing computation\nacross jobs and pushing data projection to the storage layer.\n", "versions": [{"version": "v1", "created": "Thu, 28 Jan 2021 17:16:46 GMT"}, {"version": "v2", "created": "Tue, 23 Feb 2021 22:56:12 GMT"}], "update_date": "2021-02-25", "authors_parsed": [["Murray", "Derek G.", ""], ["Simsa", "Jiri", ""], ["Klimovic", "Ana", ""], ["Indyk", "Ihor", ""]]}, {"id": "2101.12136", "submitter": "Ghada Sokar", "authors": "Ghada Sokar, Decebal Constantin Mocanu, Mykola Pechenizkiy", "title": "Self-Attention Meta-Learner for Continual Learning", "comments": null, "journal-ref": "20th International Conference on Autonomous Agents and Multiagent\n  Systems (AAMAS 2021)", "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Continual learning aims to provide intelligent agents capable of learning\nmultiple tasks sequentially with neural networks. One of its main challenging,\ncatastrophic forgetting, is caused by the neural networks non-optimal ability\nto learn in non-stationary distributions. In most settings of the current\napproaches, the agent starts from randomly initialized parameters and is\noptimized to master the current task regardless of the usefulness of the\nlearned representation for future tasks. Moreover, each of the future tasks\nuses all the previously learned knowledge although parts of this knowledge\nmight not be helpful for its learning. These cause interference among tasks,\nespecially when the data of previous tasks is not accessible. In this paper, we\npropose a new method, named Self-Attention Meta-Learner (SAM), which learns a\nprior knowledge for continual learning that permits learning a sequence of\ntasks, while avoiding catastrophic forgetting. SAM incorporates an attention\nmechanism that learns to select the particular relevant representation for each\nfuture task. Each task builds a specific representation branch on top of the\nselected knowledge, avoiding the interference between tasks. We evaluate the\nproposed method on the Split CIFAR-10/100 and Split MNIST benchmarks in the\ntask agnostic inference. We empirically show that we can achieve a better\nperformance than several state-of-the-art methods for continual learning by\nbuilding on the top of selected representation learned by SAM. We also show the\nrole of the meta-attention mechanism in boosting informative features\ncorresponding to the input data and identifying the correct target in the task\nagnostic inference. Finally, we demonstrate that popular existing continual\nlearning methods gain a performance boost when they adopt SAM as a starting\npoint.\n", "versions": [{"version": "v1", "created": "Thu, 28 Jan 2021 17:35:04 GMT"}], "update_date": "2021-01-29", "authors_parsed": [["Sokar", "Ghada", ""], ["Mocanu", "Decebal Constantin", ""], ["Pechenizkiy", "Mykola", ""]]}, {"id": "2101.12154", "submitter": "Navneet Garg", "authors": "Navneet Garg, Mathini Sellathurai and Tharmalingam Ratnarajah", "title": "Reinforcement Learning based Per-antenna Discrete Power Control for\n  Massive MIMO Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.LG math.IT", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Power consumption is one of the major issues in massive MIMO (multiple input\nmultiple output) systems, causing increased long-term operational cost and\noverheating issues. In this paper, we consider per-antenna power allocation\nwith a given finite set of power levels towards maximizing the long-term energy\nefficiency of the multi-user systems, while satisfying the QoS (quality of\nservice) constraints at the end users in terms of required SINRs\n(signal-to-interference-plus-noise ratio), which depends on channel\ninformation. Assuming channel states to vary as a Markov process, the\nconstraint problem is modeled as an unconstraint problem, followed by the power\nallocation based on Q-learning algorithm. Simulation results are presented to\ndemonstrate the successful minimization of power consumption while achieving\nthe SINR threshold at users.\n", "versions": [{"version": "v1", "created": "Thu, 28 Jan 2021 18:03:51 GMT"}], "update_date": "2021-01-29", "authors_parsed": [["Garg", "Navneet", ""], ["Sellathurai", "Mathini", ""], ["Ratnarajah", "Tharmalingam", ""]]}, {"id": "2101.12160", "submitter": "Daan Rutten", "authors": "Daan Rutten, Debankur Mukherjee", "title": "A New Approach to Capacity Scaling Augmented With Unreliable Machine\n  Learning Predictions", "comments": "47 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.LG cs.NI cs.PF math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modern data centers suffer from immense power consumption. The erratic\nbehavior of internet traffic forces data centers to maintain excess capacity in\nthe form of idle servers in case the workload suddenly increases. As an idle\nserver still consumes a significant fraction of the peak energy, data center\noperators have heavily invested in capacity scaling solutions. In simple terms,\nthese aim to deactivate servers if the demand is low and to activate them again\nwhen the workload increases. To do so, an algorithm needs to strike a delicate\nbalance between power consumption, flow-time, and switching costs. Over the\nlast decade, the research community has developed competitive online algorithms\nwith worst-case guarantees. In the presence of historic data patterns,\nprescription from Machine Learning (ML) predictions typically outperform such\ncompetitive algorithms. This, however, comes at the cost of sacrificing the\nrobustness of performance, since unpredictable surges in the workload are not\nuncommon. The current work builds on the emerging paradigm of augmenting\nunreliable ML predictions with online algorithms to develop novel robust\nalgorithms that enjoy the benefits of both worlds.\n  We analyze a continuous-time model for capacity scaling, where the goal is to\nminimize the weighted sum of flow-time, switching cost, and power consumption\nin an online fashion. We propose a novel algorithm, called Adaptive Balanced\nCapacity Scaling (ABCS), that has access to black-box ML predictions, but is\ncompletely oblivious to the accuracy of these predictions. In particular, if\nthe predictions turn out to be accurate in hindsight, we prove that ABCS is\n$(1+\\varepsilon)$-competitive. Moreover, even when the predictions are\ninaccurate, ABCS guarantees a bounded competitive ratio. The performance of the\nABCS algorithm on a real-world dataset positively support the theoretical\nresults.\n", "versions": [{"version": "v1", "created": "Thu, 28 Jan 2021 18:14:18 GMT"}], "update_date": "2021-01-29", "authors_parsed": [["Rutten", "Daan", ""], ["Mukherjee", "Debankur", ""]]}, {"id": "2101.12176", "submitter": "Samuel L. Smith", "authors": "Samuel L. Smith, Benoit Dherin, David G. T. Barrett and Soham De", "title": "On the Origin of Implicit Regularization in Stochastic Gradient Descent", "comments": "Accepted as a conference paper at ICLR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For infinitesimal learning rates, stochastic gradient descent (SGD) follows\nthe path of gradient flow on the full batch loss function. However moderately\nlarge learning rates can achieve higher test accuracies, and this\ngeneralization benefit is not explained by convergence bounds, since the\nlearning rate which maximizes test accuracy is often larger than the learning\nrate which minimizes training loss. To interpret this phenomenon we prove that\nfor SGD with random shuffling, the mean SGD iterate also stays close to the\npath of gradient flow if the learning rate is small and finite, but on a\nmodified loss. This modified loss is composed of the original loss function and\nan implicit regularizer, which penalizes the norms of the minibatch gradients.\nUnder mild assumptions, when the batch size is small the scale of the implicit\nregularization term is proportional to the ratio of the learning rate to the\nbatch size. We verify empirically that explicitly including the implicit\nregularizer in the loss can enhance the test accuracy when the learning rate is\nsmall.\n", "versions": [{"version": "v1", "created": "Thu, 28 Jan 2021 18:32:14 GMT"}], "update_date": "2021-01-29", "authors_parsed": [["Smith", "Samuel L.", ""], ["Dherin", "Benoit", ""], ["Barrett", "David G. T.", ""], ["De", "Soham", ""]]}, {"id": "2101.12190", "submitter": "Xin Wang", "authors": "Xuanqiang Zhao, Benchi Zhao, Zihe Wang, Zhixin Song, Xin Wang", "title": "LOCCNet: a machine learning framework for distributed quantum\n  information processing", "comments": "18 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "quant-ph cond-mat.dis-nn cs.IT cs.LG hep-th math.IT", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Distributed quantum information processing is essential for building quantum\nnetworks and enabling more extensive quantum computations. In this regime,\nseveral spatially separated parties share a multipartite quantum system, and\nthe most natural set of operations are Local Operations and Classical\nCommunication (LOCC). As a pivotal part in quantum information theory and\npractice, LOCC has led to many vital protocols such as quantum teleportation.\nHowever, designing practical LOCC protocols is challenging due to LOCC's\nintractable structure and limitations set by near-term quantum devices. Here we\nintroduce LOCCNet, a machine learning framework facilitating protocol design\nand optimization for distributed quantum information processing tasks. As\napplications, we explore various quantum information tasks such as entanglement\ndistillation, quantum state discrimination, and quantum channel simulation. We\ndiscover novel protocols with evident improvements, in particular, for\nentanglement distillation with quantum states of interest in quantum\ninformation. Our approach opens up new opportunities for exploring entanglement\nand its applications with machine learning, which will potentially sharpen our\nunderstanding of the power and limitations of LOCC.\n", "versions": [{"version": "v1", "created": "Thu, 28 Jan 2021 18:53:12 GMT"}], "update_date": "2021-01-29", "authors_parsed": [["Zhao", "Xuanqiang", ""], ["Zhao", "Benchi", ""], ["Wang", "Zihe", ""], ["Song", "Zhixin", ""], ["Wang", "Xin", ""]]}, {"id": "2101.12204", "submitter": "Cong Shen", "authors": "Chengshuai Shi and Cong Shen", "title": "Federated Multi-Armed Bandits", "comments": "AAAI 2021, Camera Ready. Code is available at:\n  https://github.com/ShenGroup/FMAB", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.IT cs.MA math.IT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Federated multi-armed bandits (FMAB) is a new bandit paradigm that parallels\nthe federated learning (FL) framework in supervised learning. It is inspired by\npractical applications in cognitive radio and recommender systems, and enjoys\nfeatures that are analogous to FL. This paper proposes a general framework of\nFMAB and then studies two specific federated bandit models. We first study the\napproximate model where the heterogeneous local models are random realizations\nof the global model from an unknown distribution. This model introduces a new\nuncertainty of client sampling, as the global model may not be reliably learned\neven if the finite local models are perfectly known. Furthermore, this\nuncertainty cannot be quantified a priori without knowledge of the\nsuboptimality gap. We solve the approximate model by proposing Federated Double\nUCB (Fed2-UCB), which constructs a novel \"double UCB\" principle accounting for\nuncertainties from both arm and client sampling. We show that gradually\nadmitting new clients is critical in achieving an O(log(T)) regret while\nexplicitly considering the communication cost. The exact model, where the\nglobal bandit model is the exact average of heterogeneous local models, is then\nstudied as a special case. We show that, somewhat surprisingly, the\norder-optimal regret can be achieved independent of the number of clients with\na careful choice of the update periodicity. Experiments using both synthetic\nand real-world datasets corroborate the theoretical analysis and demonstrate\nthe effectiveness and efficiency of the proposed algorithms.\n", "versions": [{"version": "v1", "created": "Thu, 28 Jan 2021 18:59:19 GMT"}, {"version": "v2", "created": "Wed, 3 Mar 2021 14:36:34 GMT"}], "update_date": "2021-03-04", "authors_parsed": [["Shi", "Chengshuai", ""], ["Shen", "Cong", ""]]}, {"id": "2101.12208", "submitter": "arXiv Admin", "authors": "Maritza Tynes, Mahboobeh Parsapoor", "title": "Meta-learning on Spectral Images of Electroencephalogram of\n  Schizophenics", "comments": "Withdrawn by arXiv administrators as co-author did not consent to\n  submit", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Schizophrenia is a complex psychiatric disorder involving changes in thought\npatterns, perception, mood, and behavior. The diagnosis of schizophrenia is\nchallenging and requires that patients show two or more positive symptoms for\nat least one month. Delays in identifying this debilitating disorder can impede\na patient ability to receive much needed treatment. Advances in neuroimaging\nand machine learning algorithms can facilitate the diagnosis of schizophrenia\nand help clinicians to provide an accurate diagnosis of the disease. This paper\npresents a methodology for analyzing spectral images of Electroencephalography\ncollected from patients with schizophrenia using convolutional neural networks.\nIt also explains how we have developed accurate classifiers employing\nModel-Agnostic Meta-Learning and prototypical networks. Such classifiers have\nthe capacity to distinguish people with schizophrenia from healthy controls\nbased on their brain activity.\n", "versions": [{"version": "v1", "created": "Wed, 27 Jan 2021 20:51:25 GMT"}, {"version": "v2", "created": "Tue, 16 Feb 2021 18:34:47 GMT"}], "update_date": "2021-02-17", "authors_parsed": [["Tynes", "Maritza", ""], ["Parsapoor", "Mahboobeh", ""]]}, {"id": "2101.12210", "submitter": "David Medina-Ortiz Mr", "authors": "Cristofer Quiroz, Yasna Barrera Saavedra, Benjam\\'in Armijo-Galdames,\n  Juan Amado-Hinojosa, \\'Alvaro Olivera-Nappa, Anamaria Sanchez-Daza, and David\n  Medina-Ortiz", "title": "Peptipedia: a comprehensive database for peptide research supported by\n  Assembled predictive models and Data Mining approaches", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.GN cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Motivation: Peptides have attracted the attention in this century due to\ntheir remarkable therapeutic properties. Computational tools are being\ndeveloped to take advantage of existing information, encapsulating knowledge\nand making it available in a simple way for general public use. However, these\nare property-specific redundant data systems, and usually do not display the\ndata in a clear way. In some cases, information download is not even possible.\nThis data needs to be available in a simple form for drug design and other\nbiotechnological applications.\n  Results: We developed Peptipedia, a user-friendly database and web\napplication to search, characterise and analyse peptide sequences. Our tool\nintegrates the information from thirty previously reported databases, making it\nthe largest repository of peptides with recorded activities so far. Besides, we\nimplemented a variety of services to increase our tool's usability. The\nsignificant differences of our tools with other existing alternatives becomes a\nsubstantial contribution to develop biotechnological and bioengineering\napplications for peptides.\n  Availability: Peptipedia is available for non-commercial use as an\nopen-access software, licensed under the GNU General Public License, version\nGPL 3.0. The web platform is publicly available at pesb2.cl/peptipedia. Both\nthe source code and sample datasets are available in the GitHub repository\nhttps://github.com/CristoferQ/PeptideDatabase.\n  Contact: david.medina@cebib.cl, ana.sanchez@ing.uchile.cl\n", "versions": [{"version": "v1", "created": "Thu, 28 Jan 2021 10:59:51 GMT"}], "update_date": "2021-02-01", "authors_parsed": [["Quiroz", "Cristofer", ""], ["Saavedra", "Yasna Barrera", ""], ["Armijo-Galdames", "Benjam\u00edn", ""], ["Amado-Hinojosa", "Juan", ""], ["Olivera-Nappa", "\u00c1lvaro", ""], ["Sanchez-Daza", "Anamaria", ""], ["Medina-Ortiz", "David", ""]]}, {"id": "2101.12240", "submitter": "Nima Mohammadi", "authors": "Nima Mohammadi, Jianan Bai, Qiang Fan, Yifei Song, Yang Yi, Lingjia\n  Liu", "title": "Differential Privacy Meets Federated Learning under Communication\n  Constraints", "comments": "11 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The performance of federated learning systems is bottlenecked by\ncommunication costs and training variance. The communication overhead problem\nis usually addressed by three communication-reduction techniques, namely, model\ncompression, partial device participation, and periodic aggregation, at the\ncost of increased training variance. Different from traditional distributed\nlearning systems, federated learning suffers from data heterogeneity (since the\ndevices sample their data from possibly different distributions), which induces\nadditional variance among devices during training. Various variance-reduced\ntraining algorithms have been introduced to combat the effects of data\nheterogeneity, while they usually cost additional communication resources to\ndeliver necessary control information. Additionally, data privacy remains a\ncritical issue in FL, and thus there have been attempts at bringing\nDifferential Privacy to this framework as a mediator between utility and\nprivacy requirements. This paper investigates the trade-offs between\ncommunication costs and training variance under a resource-constrained\nfederated system theoretically and experimentally, and how communication\nreduction techniques interplay in a differentially private setting. The results\nprovide important insights into designing practical privacy-aware federated\nlearning systems.\n", "versions": [{"version": "v1", "created": "Thu, 28 Jan 2021 19:20:56 GMT"}], "update_date": "2021-02-01", "authors_parsed": [["Mohammadi", "Nima", ""], ["Bai", "Jianan", ""], ["Fan", "Qiang", ""], ["Song", "Yifei", ""], ["Yi", "Yang", ""], ["Liu", "Lingjia", ""]]}, {"id": "2101.12246", "submitter": "Moritz Kulessa", "authors": "Moritz Kulessa, Eneldo Loza Menc\\'ia, Johannes F\\\"urnkranz", "title": "Revisiting Non-Specific Syndromic Surveillance", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Infectious disease surveillance is of great importance for the prevention of\nmajor outbreaks. Syndromic surveillance aims at developing algorithms which can\ndetect outbreaks as early as possible by monitoring data sources which allow to\ncapture the occurrences of a certain disease. Recent research mainly focuses on\nthe surveillance of specific, known diseases, putting the focus on the\ndefinition of the disease pattern under surveillance. Until now, only little\neffort has been devoted to what we call non-specific syndromic surveillance,\ni.e., the use of all available data for detecting any kind of outbreaks,\nincluding infectious diseases which are unknown beforehand. In this work, we\nrevisit published approaches for non-specific syndromic surveillance and\npresent a set of simple statistical modeling techniques which can serve as\nbenchmarks for more elaborate machine learning approaches. Our experimental\ncomparison on established synthetic data and real data in which we injected\nsynthetic outbreaks shows that these benchmarks already achieve very\ncompetitive results and often outperform more elaborate algorithms.\n", "versions": [{"version": "v1", "created": "Thu, 28 Jan 2021 19:33:30 GMT"}], "update_date": "2021-02-01", "authors_parsed": [["Kulessa", "Moritz", ""], ["Menc\u00eda", "Eneldo Loza", ""], ["F\u00fcrnkranz", "Johannes", ""]]}, {"id": "2101.12249", "submitter": "Joshua Bassey", "authors": "Joshua Bassey, Lijun Qian, Xianfang Li", "title": "A Survey of Complex-Valued Neural Networks", "comments": "15 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Artificial neural networks (ANNs) based machine learning models and\nespecially deep learning models have been widely applied in computer vision,\nsignal processing, wireless communications, and many other domains, where\ncomplex numbers occur either naturally or by design. However, most of the\ncurrent implementations of ANNs and machine learning frameworks are using real\nnumbers rather than complex numbers. There are growing interests in building\nANNs using complex numbers, and exploring the potential advantages of the\nso-called complex-valued neural networks (CVNNs) over their real-valued\ncounterparts. In this paper, we discuss the recent development of CVNNs by\nperforming a survey of the works on CVNNs in the literature. Specifically, a\ndetailed review of various CVNNs in terms of activation function, learning and\noptimization, input and output representations, and their applications in tasks\nsuch as signal processing and computer vision are provided, followed by a\ndiscussion on some pertinent challenges and future research directions.\n", "versions": [{"version": "v1", "created": "Thu, 28 Jan 2021 19:40:50 GMT"}], "update_date": "2021-02-01", "authors_parsed": [["Bassey", "Joshua", ""], ["Qian", "Lijun", ""], ["Li", "Xianfang", ""]]}, {"id": "2101.12252", "submitter": "Georges Sfeir", "authors": "Georges Sfeir, Filipe Rodrigues, Maya Abou-Zeid", "title": "Gaussian Process Latent Class Choice Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "econ.EM cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We present a Gaussian Process - Latent Class Choice Model (GP-LCCM) to\nintegrate a non-parametric class of probabilistic machine learning within\ndiscrete choice models (DCMs). Gaussian Processes (GPs) are kernel-based\nalgorithms that incorporate expert knowledge by assuming priors over latent\nfunctions rather than priors over parameters, which makes them more flexible in\naddressing nonlinear problems. By integrating a Gaussian Process within a LCCM\nstructure, we aim at improving discrete representations of unobserved\nheterogeneity. The proposed model would assign individuals probabilistically to\nbehaviorally homogeneous clusters (latent classes) using GPs and simultaneously\nestimate class-specific choice models by relying on random utility models.\nFurthermore, we derive and implement an Expectation-Maximization (EM) algorithm\nto jointly estimate/infer the hyperparameters of the GP kernel function and the\nclass-specific choice parameters by relying on a Laplace approximation and\ngradient-based numerical optimization methods, respectively. The model is\ntested on two different mode choice applications and compared against different\nLCCM benchmarks. Results show that GP-LCCM allows for a more complex and\nflexible representation of heterogeneity and improves both in-sample fit and\nout-of-sample predictive power. Moreover, behavioral and economic\ninterpretability is maintained at the class-specific choice model level while\nlocal interpretation of the latent classes can still be achieved, although the\nnon-parametric characteristic of GPs lessens the transparency of the model.\n", "versions": [{"version": "v1", "created": "Thu, 28 Jan 2021 19:56:42 GMT"}], "update_date": "2021-02-01", "authors_parsed": [["Sfeir", "Georges", ""], ["Rodrigues", "Filipe", ""], ["Abou-Zeid", "Maya", ""]]}, {"id": "2101.12254", "submitter": "Aysen Degerli", "authors": "Aysen Degerli, Mete Ahishali, Serkan Kiranyaz, Muhammad E. H.\n  Chowdhury, Moncef Gabbouj", "title": "Reliable COVID-19 Detection Using Chest X-ray Images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Coronavirus disease 2019 (COVID-19) has emerged the need for computer-aided\ndiagnosis with automatic, accurate, and fast algorithms. Recent studies have\napplied Machine Learning algorithms for COVID-19 diagnosis over chest X-ray\n(CXR) images. However, the data scarcity in these studies prevents a reliable\nevaluation with the potential of overfitting and limits the performance of deep\nnetworks. Moreover, these networks can discriminate COVID-19 pneumonia usually\nfrom healthy subjects only or occasionally, from limited pneumonia types. Thus,\nthere is a need for a robust and accurate COVID-19 detector evaluated over a\nlarge CXR dataset. To address this need, in this study, we propose a reliable\nCOVID-19 detection network: ReCovNet, which can discriminate COVID-19 pneumonia\nfrom 14 different thoracic diseases and healthy subjects. To accomplish this,\nwe have compiled the largest COVID-19 CXR dataset: QaTa-COV19 with 124,616\nimages including 4603 COVID-19 samples. The proposed ReCovNet achieved a\ndetection performance with 98.57% sensitivity and 99.77% specificity.\n", "versions": [{"version": "v1", "created": "Thu, 28 Jan 2021 19:57:21 GMT"}], "update_date": "2021-02-01", "authors_parsed": [["Degerli", "Aysen", ""], ["Ahishali", "Mete", ""], ["Kiranyaz", "Serkan", ""], ["Chowdhury", "Muhammad E. H.", ""], ["Gabbouj", "Moncef", ""]]}, {"id": "2101.12270", "submitter": "William Buchanan Prof", "authors": "Andrew Churcher, Rehmat Ullah, Jawad Ahmad, Sadaqat ur Rehman, Fawad\n  Masood, Mandar Gogate, Fehaid Alqahtani, Boubakr Nour and William J. Buchanan", "title": "An Experimental Analysis of Attack Classification Using Machine Learning\n  in IoT Networks", "comments": null, "journal-ref": "Sensors. 2021; 21(2):446", "doi": "10.3390/s21020446", "report-no": null, "categories": "cs.CR cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In recent years, there has been a massive increase in the amount of Internet\nof Things (IoT) devices as well as the data generated by such devices. The\nparticipating devices in IoT networks can be problematic due to their\nresource-constrained nature, and integrating security on these devices is often\noverlooked. This has resulted in attackers having an increased incentive to\ntarget IoT devices. As the number of attacks possible on a network increases,\nit becomes more difficult for traditional intrusion detection systems (IDS) to\ncope with these attacks efficiently. In this paper, we highlight several\nmachine learning (ML) methods such as k-nearest neighbour (KNN), support vector\nmachine (SVM), decision tree (DT), naive Bayes (NB), random forest (RF),\nartificial neural network (ANN), and logistic regression (LR) that can be used\nin IDS. In this work, ML algorithms are compared for both binary and\nmulti-class classification on Bot-IoT dataset. Based on several parameters such\nas accuracy, precision, recall, F1 score, and log loss, we experimentally\ncompared the aforementioned ML algorithms. In the case of HTTP distributed\ndenial-of-service (DDoS) attack, the accuracy of RF is 99%. Furthermore, other\nsimulation results-based precision, recall, F1 score, and log loss metric\nreveal that RF outperforms on all types of attacks in binary classification.\nHowever, in multi-class classification, KNN outperforms other ML algorithms\nwith an accuracy of 99%, which is 4% higher than RF.\n", "versions": [{"version": "v1", "created": "Sun, 10 Jan 2021 11:48:37 GMT"}], "update_date": "2021-02-01", "authors_parsed": [["Churcher", "Andrew", ""], ["Ullah", "Rehmat", ""], ["Ahmad", "Jawad", ""], ["Rehman", "Sadaqat ur", ""], ["Masood", "Fawad", ""], ["Gogate", "Mandar", ""], ["Alqahtani", "Fehaid", ""], ["Nour", "Boubakr", ""], ["Buchanan", "William J.", ""]]}, {"id": "2101.12288", "submitter": "Elchanan Solomon", "authors": "Elchanan Solomon, Alexander Wagner, Paul Bendich", "title": "From Geometry to Topology: Inverse Theorems for Distributed Persistence", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.AT cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  What is the \"right\" topological invariant of a large point cloud X? Prior\nresearch has focused on estimating the full persistence diagram of X, a\nquantity that is very expensive to compute, unstable to outliers, and far from\na sufficient statistic. We therefore propose that the correct invariant is not\nthe persistence diagram of X, but rather the collection of persistence diagrams\nof many small subsets. This invariant, which we call \"distributed persistence,\"\nis trivially parallelizable, more stable to outliers, and has a rich inverse\ntheory. The map from the space of point clouds (with the quasi-isometry metric)\nto the space of distributed persistence invariants (with the\nHausdorff-Bottleneck distance) is a global quasi-isometry. This is a much\nstronger property than simply being injective, as it implies that the inverse\nof a small neighborhood is a small neighborhood, and is to our knowledge the\nonly result of its kind in the TDA literature. Moreover, the quasi-isometry\nbounds depend on the size of the subsets taken, so that as the size of these\nsubsets goes from small to large, the invariant interpolates between a purely\ngeometric one and a topological one. Lastly, we note that our inverse results\ndo not actually require considering all subsets of a fixed size (an enormous\ncollection), but a relatively small collection satisfying certain covering\nproperties that arise with high probability when randomly sampling subsets.\nThese theoretical results are complemented by two synthetic experiments\ndemonstrating the use of distributed persistence in practice.\n", "versions": [{"version": "v1", "created": "Thu, 28 Jan 2021 21:36:45 GMT"}, {"version": "v2", "created": "Wed, 3 Feb 2021 22:35:52 GMT"}], "update_date": "2021-02-05", "authors_parsed": [["Solomon", "Elchanan", ""], ["Wagner", "Alexander", ""], ["Bendich", "Paul", ""]]}, {"id": "2101.12336", "submitter": "Breno Serrano", "authors": "Breno Serrano and Thibaut Vidal", "title": "Community Detection in the Stochastic Block Model by Mixed Integer\n  Programming", "comments": "added funding source in the Acknowledgements", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Degree-Corrected Stochastic Block Model (DCSBM) is a popular model to\ngenerate random graphs with community structure given an expected degree\nsequence. The standard approach of community detection based on the DCSBM is to\nsearch for the model parameters that are the most likely to have produced the\nobserved network data through maximum likelihood estimation (MLE). Current\ntechniques for the MLE problem are heuristics, and therefore do not guarantee\nconvergence to the optimum. We present mathematical programming formulations\nand exact solution methods that can provably find the model parameters and\ncommunity assignments of maximum likelihood given an observed graph. We compare\nthese exact methods with classical heuristic algorithms based on\nexpectation-maximization (EM). The solutions given by exact methods give us a\nprincipled way of measuring the experimental performance of classical\nheuristics and comparing different variations thereof.\n", "versions": [{"version": "v1", "created": "Tue, 26 Jan 2021 22:04:40 GMT"}, {"version": "v2", "created": "Tue, 4 May 2021 07:53:32 GMT"}], "update_date": "2021-05-05", "authors_parsed": [["Serrano", "Breno", ""], ["Vidal", "Thibaut", ""]]}, {"id": "2101.12339", "submitter": "Yutong Wang", "authors": "Ziqi Tang, Yutong Wang, Jiebo Luo", "title": "Are Top School Students More Critical of Their Professors? Mining\n  Comments on RateMyProfessor.com", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Student reviews and comments on RateMyProfessor.com reflect realistic\nlearning experiences of students. Such information provides a large-scale data\nsource to examine the teaching quality of the lecturers. In this paper, we\npropose an in-depth analysis of these comments. First, we partition our data\ninto different comparison groups. Next, we perform exploratory data analysis to\ndelve into the data. Furthermore, we employ Latent Dirichlet Allocation and\nsentiment analysis to extract topics and understand the sentiments associated\nwith the comments. We uncover interesting insights about the characteristics of\nboth college students and professors. Our study proves that student reviews and\ncomments contain crucial information and can serve as essential references for\nenrollment in courses and universities.\n", "versions": [{"version": "v1", "created": "Sat, 23 Jan 2021 20:01:36 GMT"}], "update_date": "2021-02-01", "authors_parsed": [["Tang", "Ziqi", ""], ["Wang", "Yutong", ""], ["Luo", "Jiebo", ""]]}, {"id": "2101.12353", "submitter": "Yunfei Yang", "authors": "Yunfei Yang, Zhen Li, Yang Wang", "title": "On the capacity of deep generative networks for approximating\n  distributions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.PR math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the efficacy and efficiency of deep generative networks for\napproximating probability distributions. We prove that neural networks can\ntransform a low-dimensional source distribution to a distribution that is\narbitrarily close to a high-dimensional target distribution, when the closeness\nare measured by Wasserstein distances and maximum mean discrepancy. Upper\nbounds of the approximation error are obtained in terms of the width and depth\nof neural network. Furthermore, it is shown that the approximation error in\nWasserstein distance grows at most linearly on the ambient dimension and that\nthe approximation order only depends on the intrinsic dimension of the target\ndistribution. On the contrary, when $f$-divergences are used as metrics of\ndistributions, the approximation property is different. We show that in order\nto approximate the target distribution in $f$-divergences, the dimension of the\nsource distribution cannot be smaller than the intrinsic dimension of the\ntarget distribution.\n", "versions": [{"version": "v1", "created": "Fri, 29 Jan 2021 01:45:02 GMT"}, {"version": "v2", "created": "Thu, 13 May 2021 07:42:34 GMT"}], "update_date": "2021-05-14", "authors_parsed": [["Yang", "Yunfei", ""], ["Li", "Zhen", ""], ["Wang", "Yang", ""]]}, {"id": "2101.12365", "submitter": "Jonathan Siegel", "authors": "Jonathan W. Siegel, Jinchao Xu", "title": "Improved Approximation Properties of Dictionaries and Applications to\n  Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.IT cs.LG math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article addresses the problem of approximating a function in a Hilbert\nspace by an expansion over a dictionary $\\mathbb{D}$. We introduce the notion\nof a smoothly parameterized dictionary and give upper bounds on the\napproximation rates, metric entropy and $n$-widths of the absolute convex hull,\nwhich we denote $B_1(\\mathbb{D})$, of such dictionaries. The upper bounds\ndepend upon the order of smoothness of the parameterization, and improve upon\nexisting results in many cases. The main applications of these results is to\nthe dictionaries $\\mathbb{D} = \\{\\sigma(\\omega\\cdot x + b)\\}\\subset L^2$\ncorresponding to shallow neural networks with activation function $\\sigma$, and\nto the dictionary of decaying Fourier modes corresponding to the spectral\nBarron space. This improves upon existing approximation rates for shallow\nneural networks when $\\sigma = \\text{ReLU}^k$ for $k\\geq 2$, sharpens bounds on\nthe metric entropy, and provides the first bounds on the Gelfand $n$-widths of\nthe Barron space and spectral Barron space.\n", "versions": [{"version": "v1", "created": "Fri, 29 Jan 2021 02:29:48 GMT"}, {"version": "v2", "created": "Tue, 9 Feb 2021 18:14:32 GMT"}, {"version": "v3", "created": "Mon, 5 Apr 2021 16:34:46 GMT"}, {"version": "v4", "created": "Thu, 22 Apr 2021 13:17:28 GMT"}, {"version": "v5", "created": "Thu, 20 May 2021 01:38:30 GMT"}, {"version": "v6", "created": "Mon, 28 Jun 2021 21:54:22 GMT"}], "update_date": "2021-06-30", "authors_parsed": [["Siegel", "Jonathan W.", ""], ["Xu", "Jinchao", ""]]}, {"id": "2101.12369", "submitter": "Jiajun Liang", "authors": "Jiajun Liang, Chuyang Ke and Jean Honorio", "title": "Information Theoretic Limits of Exact Recovery in Sub-hypergraph Models\n  for Community Detection", "comments": null, "journal-ref": "IEEE International Symposium on Information Theory (ISIT), 2021", "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we study the information theoretic bounds for exact recovery\nin sub-hypergraph models for community detection. We define a general model\ncalled the $m-$uniform sub-hypergraph stochastic block model ($m-$ShSBM). Under\nthe $m-$ShSBM, we use Fano's inequality to identify the region of model\nparameters where any algorithm fails to exactly recover the planted communities\nwith a large probability. We also identify the region where a Maximum\nLikelihood Estimation (MLE) algorithm succeeds to exactly recover the\ncommunities with high probability. Our bounds are tight and pertain to the\ncommunity detection problems in various models such as the planted hypergraph\nstochastic block model, the planted densest sub-hypergraph model, and the\nplanted multipartite hypergraph model.\n", "versions": [{"version": "v1", "created": "Fri, 29 Jan 2021 02:50:34 GMT"}], "update_date": "2021-07-07", "authors_parsed": [["Liang", "Jiajun", ""], ["Ke", "Chuyang", ""], ["Honorio", "Jean", ""]]}, {"id": "2101.12372", "submitter": "Ran Wang", "authors": "Haojing Shen, Sihong Chen, Ran Wang, Xizhao Wang", "title": "Adversarial Learning with Cost-Sensitive Classes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  It is necessary to improve the performance of some special classes or to\nparticularly protect them from attacks in adversarial learning. This paper\nproposes a framework combining cost-sensitive classification and adversarial\nlearning together to train a model that can distinguish between protected and\nunprotected classes, such that the protected classes are less vulnerable to\nadversarial examples. We find in this framework an interesting phenomenon\nduring the training of deep neural networks, called Min-Max property, that is,\nthe absolute values of most parameters in the convolutional layer approach zero\nwhile the absolute values of a few parameters are significantly larger becoming\nbigger. Based on this Min-Max property which is formulated and analyzed in a\nview of random distribution, we further build a new defense model against\nadversarial examples for adversarial robustness improvement. An advantage of\nthe built model is that it does no longer need adversarial training, and thus,\nhas a higher computational efficiency than most existing models of needing\nadversarial training. It is experimentally confirmed that, regarding the\naverage accuracy of all classes, our model is almost as same as the existing\nmodels when an attack does not occur and is better than the existing models\nwhen an attack occurs. Specifically, regarding the accuracy of protected\nclasses, the proposed model is much better than the existing models when an\nattack occurs.\n", "versions": [{"version": "v1", "created": "Fri, 29 Jan 2021 03:15:40 GMT"}], "update_date": "2021-02-01", "authors_parsed": [["Shen", "Haojing", ""], ["Chen", "Sihong", ""], ["Wang", "Ran", ""], ["Wang", "Xizhao", ""]]}, {"id": "2101.12403", "submitter": "Jittat Fakcharoenphol", "authors": "Vacharapat Mettanant, Jittat Fakcharoenphol", "title": "Fair Resource Allocation for Demands with Sharp Lower Tail Inequalities", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CY cs.GT", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We consider a fairness problem in resource allocation where multiple groups\ndemand resources from a common source with the total fixed amount. The general\nmodel was introduced by Elzayn et al. [FAT*'19]. We follow Donahue and\nKleinberg [FAT*'20] who considered the case when the demand distribution is\nknown. We show that for many common demand distributions that satisfy sharp\nlower tail inequalities, a natural allocation that provides resources\nproportional to each group's average demand performs very well. More\nspecifically, this natural allocation is approximately fair and efficient\n(i.e., it provides near maximum utilization). We also show that, when small\namount of unfairness is allowed, the Price of Fairness (PoF), in this case, is\nclose to 1.\n", "versions": [{"version": "v1", "created": "Fri, 29 Jan 2021 04:53:34 GMT"}], "update_date": "2021-02-01", "authors_parsed": [["Mettanant", "Vacharapat", ""], ["Fakcharoenphol", "Jittat", ""]]}, {"id": "2101.12410", "submitter": "Chao Qian Mr", "authors": "Chao Qian, Renkai Tan, Wenjing Ye", "title": "An adaptive artificial neural network-based generative design method for\n  layout designs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Layout designs are encountered in a variety of fields. For problems with many\ndesign degrees of freedom, efficiency of design methods becomes a major\nconcern. In recent years, machine learning methods such as artificial neural\nnetworks have been used increasingly to speed up the design process. A main\nissue of many such approaches is the need for a large corpus of training data\nthat are generated using high-dimensional simulations. The high computational\ncost associated with training data generation largely diminishes the efficiency\ngained by using machine learning methods. In this work, an adaptive artificial\nneural network-based generative design approach is proposed and developed. This\nmethod uses a generative adversarial network to generate design candidates and\nthus the number of design variables is greatly reduced. To speed up the\nevaluation of the objective function, a convolutional neural network is\nconstructed as the surrogate model for function evaluation. The inverse design\nis carried out using the genetic algorithm in conjunction with two neural\nnetworks. A novel adaptive learning and optimization strategy is proposed,\nwhich allows the design space to be effectively explored for the search for\noptimal solutions. As such the number of training data needed is greatly\nreduced. The performance of the proposed design method is demonstrated on two\nheat source layout design problems. In both problems, optimal designs have been\nobtained. Compared with several existing approaches, the proposed approach has\nthe best performance in terms of accuracy and efficiency.\n", "versions": [{"version": "v1", "created": "Fri, 29 Jan 2021 05:32:17 GMT"}], "update_date": "2021-02-01", "authors_parsed": [["Qian", "Chao", ""], ["Tan", "Renkai", ""], ["Ye", "Wenjing", ""]]}, {"id": "2101.12414", "submitter": "Shane Barratt", "authors": "Shane Barratt, Yining Dong, Stephen Boyd", "title": "Low Rank Forecasting", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.LG math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of forecasting multiple values of the future of a\nvector time series, using some past values. This problem, and related ones such\nas one-step-ahead prediction, have a very long history, and there are a number\nof well-known methods for it, including vector auto-regressive models,\nstate-space methods, multi-task regression, and others. Our focus is on low\nrank forecasters, which break forecasting up into two steps: estimating a\nvector that can be interpreted as a latent state, given the past, and then\nestimating the future values of the time series, given the latent state\nestimate. We introduce the concept of forecast consistency, which means that\nthe estimates of the same value made at different times are consistent. We\nformulate the forecasting problem in general form, and focus on linear\nforecasters, for which we propose a formulation that can be solved via convex\noptimization. We describe a number of extensions and variations, including\nnonlinear forecasters, data weighting, the inclusion of auxiliary data, and\nadditional objective terms. We illustrate our methods with several examples.\n", "versions": [{"version": "v1", "created": "Fri, 29 Jan 2021 05:59:19 GMT"}], "update_date": "2021-02-01", "authors_parsed": [["Barratt", "Shane", ""], ["Dong", "Yining", ""], ["Boyd", "Stephen", ""]]}, {"id": "2101.12416", "submitter": "Shane Barratt", "authors": "Shane Barratt and Stephen Boyd", "title": "Covariance Prediction via Convex Optimization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.LG math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of predicting the covariance of a zero mean Gaussian\nvector, based on another feature vector. We describe a covariance predictor\nthat has the form of a generalized linear model, i.e., an affine function of\nthe features followed by an inverse link function that maps vectors to\nsymmetric positive definite matrices. The log-likelihood is a concave function\nof the predictor parameters, so fitting the predictor involves convex\noptimization. Such predictors can be combined with others, or recursively\napplied to improve performance.\n", "versions": [{"version": "v1", "created": "Fri, 29 Jan 2021 06:06:58 GMT"}], "update_date": "2021-02-01", "authors_parsed": [["Barratt", "Shane", ""], ["Boyd", "Stephen", ""]]}, {"id": "2101.12430", "submitter": "Vince Lyzinski", "authors": "Al-Fahad M. Al-Qadhi, Carey E. Priebe, Hayden S. Helm, Vince Lyzinski", "title": "Subgraph nomination: Query by Example Subgraph Retrieval in Networks", "comments": "31 pages, 11 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.IR cs.SI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces the subgraph nomination inference task, in which\nexample subgraphs of interest are used to query a network for similarly\ninteresting subgraphs. This type of problem appears time and again in real\nworld problems connected to, for example, user recommendation systems and\nstructural retrieval tasks in social and biological/connectomic networks. We\nformally define the subgraph nomination framework with an emphasis on the\nnotion of a user-in-the-loop in the subgraph nomination pipeline. In this\nsetting, a user can provide additional post-nomination light supervision that\ncan be incorporated into the retrieval task. After introducing and formalizing\nthe retrieval task, we examine the nuanced effect that user-supervision can\nhave on performance, both analytically and across real and simulated data\nexamples.\n", "versions": [{"version": "v1", "created": "Fri, 29 Jan 2021 06:50:27 GMT"}], "update_date": "2021-02-01", "authors_parsed": [["Al-Qadhi", "Al-Fahad M.", ""], ["Priebe", "Carey E.", ""], ["Helm", "Hayden S.", ""], ["Lyzinski", "Vince", ""]]}, {"id": "2101.12431", "submitter": "Feng Quan", "authors": "Quan Feng and Songcan Chen", "title": "Learning Twofold Heterogeneous Multi-Task by Sharing Similar Convolution\n  Kernel Pairs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Heterogeneous multi-task learning (HMTL) is an important topic in multi-task\nlearning (MTL). Most existing HMTL methods usually solve either scenario where\nall tasks reside in the same input (feature) space yet unnecessarily the\nconsistent output (label) space or scenario where their input (feature) spaces\nare heterogeneous while the output (label) space is consistent. However, to the\nbest of our knowledge, there is limited study on twofold heterogeneous MTL\n(THMTL) scenario where the input and the output spaces are both inconsistent or\nheterogeneous. In order to handle this complicated scenario, in this paper, we\ndesign a simple and effective multi-task adaptive learning (MTAL) network to\nlearn multiple tasks in such THMTL setting. Specifically, we explore and\nutilize the inherent relationship between tasks for knowledge sharing from\nsimilar convolution kernels in individual layers of the MTAL network. Then in\norder to realize the sharing, we weightedly aggregate any pair of convolutional\nkernels with their similarity greater than some threshold $\\rho$, consequently,\nour model effectively performs cross-task learning while suppresses the\nintra-redundancy of the entire network. Finally, we conduct end-to-end\ntraining. Our experimental results demonstrate the effectiveness of our method\nin comparison with the state-of-the-art counterparts.\n", "versions": [{"version": "v1", "created": "Fri, 29 Jan 2021 06:52:19 GMT"}], "update_date": "2021-02-01", "authors_parsed": [["Feng", "Quan", ""], ["Chen", "Songcan", ""]]}, {"id": "2101.12439", "submitter": "Yu-Jen Ma", "authors": "Yu-Jen Ma, Hong-Han Shuai, and Wen-Huang Cheng", "title": "Spatiotemporal Dilated Convolution with Uncertain Matching for\n  Video-based Crowd Estimation", "comments": "Accepted by IEEE Transactions on Multimedia, 2021\n  (https://ieeexplore.ieee.org/document/9316927)", "journal-ref": null, "doi": "10.1109/TMM.2021.3050059", "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a novel SpatioTemporal convolutional Dense Network\n(STDNet) to address the video-based crowd counting problem, which contains the\ndecomposition of 3D convolution and the 3D spatiotemporal dilated dense\nconvolution to alleviate the rapid growth of the model size caused by the\nConv3D layer. Moreover, since the dilated convolution extracts the multiscale\nfeatures, we combine the dilated convolution with the channel attention block\nto enhance the feature representations. Due to the error that occurs from the\ndifficulty of labeling crowds, especially for videos, imprecise or\nstandard-inconsistent labels may lead to poor convergence for the model. To\naddress this issue, we further propose a new patch-wise regression loss (PRL)\nto improve the original pixel-wise loss. Experimental results on three\nvideo-based benchmarks, i.e., the UCSD, Mall and WorldExpo'10 datasets, show\nthat STDNet outperforms both image- and video-based state-of-the-art methods.\nThe source codes are released at \\url{https://github.com/STDNet/STDNet}.\n", "versions": [{"version": "v1", "created": "Fri, 29 Jan 2021 07:21:33 GMT"}], "update_date": "2021-02-01", "authors_parsed": [["Ma", "Yu-Jen", ""], ["Shuai", "Hong-Han", ""], ["Cheng", "Wen-Huang", ""]]}, {"id": "2101.12446", "submitter": "Matthew Olson", "authors": "Matthew L. Olson, Roli Khanna, Lawrence Neal, Fuxin Li, Weng-Keen Wong", "title": "Counterfactual State Explanations for Reinforcement Learning Agents via\n  Generative Deep Learning", "comments": "Full source code available at\n  https://github.com/mattolson93/counterfactual-state-explanations", "journal-ref": "Artificial Intelligence, 2021, 103455, ISSN 0004-3702", "doi": "10.1016/j.artint.2021.103455", "report-no": null, "categories": "cs.AI cs.HC cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Counterfactual explanations, which deal with \"why not?\" scenarios, can\nprovide insightful explanations to an AI agent's behavior. In this work, we\nfocus on generating counterfactual explanations for deep reinforcement learning\n(RL) agents which operate in visual input environments like Atari. We introduce\ncounterfactual state explanations, a novel example-based approach to\ncounterfactual explanations based on generative deep learning. Specifically, a\ncounterfactual state illustrates what minimal change is needed to an Atari game\nimage such that the agent chooses a different action. We also evaluate the\neffectiveness of counterfactual states on human participants who are not\nmachine learning experts. Our first user study investigates if humans can\ndiscern if the counterfactual state explanations are produced by the actual\ngame or produced by a generative deep learning approach. Our second user study\ninvestigates if counterfactual state explanations can help non-expert\nparticipants identify a flawed agent; we compare against a baseline approach\nbased on a nearest neighbor explanation which uses images from the actual game.\nOur results indicate that counterfactual state explanations have sufficient\nfidelity to the actual game images to enable non-experts to more effectively\nidentify a flawed RL agent compared to the nearest neighbor baseline and to\nhaving no explanation at all.\n", "versions": [{"version": "v1", "created": "Fri, 29 Jan 2021 07:43:41 GMT"}], "update_date": "2021-02-01", "authors_parsed": [["Olson", "Matthew L.", ""], ["Khanna", "Roli", ""], ["Neal", "Lawrence", ""], ["Li", "Fuxin", ""], ["Wong", "Weng-Keen", ""]]}, {"id": "2101.12457", "submitter": "Cheng-Te Li", "authors": "Cheng Hsu, Cheng-Te Li", "title": "RetaGNN: Relational Temporal Attentive Graph Neural Networks for\n  Holistic Sequential Recommendation", "comments": "Accepted to The Web Conference (WWW) 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sequential recommendation (SR) is to accurately recommend a list of items for\na user based on her current accessed ones. While new-coming users continuously\narrive in the real world, one crucial task is to have inductive SR that can\nproduce embeddings of users and items without re-training. Given user-item\ninteractions can be extremely sparse, another critical task is to have\ntransferable SR that can transfer the knowledge derived from one domain with\nrich data to another domain. In this work, we aim to present the holistic SR\nthat simultaneously accommodates conventional, inductive, and transferable\nsettings. We propose a novel deep learning-based model, Relational Temporal\nAttentive Graph Neural Networks (RetaGNN), for holistic SR. The main idea of\nRetaGNN is three-fold. First, to have inductive and transferable capabilities,\nwe train a relational attentive GNN on the local subgraph extracted from a\nuser-item pair, in which the learnable weight matrices are on various relations\namong users, items, and attributes, rather than nodes or edges. Second,\nlong-term and short-term temporal patterns of user preferences are encoded by a\nproposed sequential self-attention mechanism. Third, a relation-aware\nregularization term is devised for better training of RetaGNN. Experiments\nconducted on MovieLens, Instagram, and Book-Crossing datasets exhibit that\nRetaGNN can outperform state-of-the-art methods under conventional, inductive,\nand transferable settings. The derived attention weights also bring model\nexplainability.\n", "versions": [{"version": "v1", "created": "Fri, 29 Jan 2021 08:08:34 GMT"}], "update_date": "2021-02-01", "authors_parsed": [["Hsu", "Cheng", ""], ["Li", "Cheng-Te", ""]]}, {"id": "2101.12465", "submitter": "Cheng-Te Li", "authors": "Yi-Ju Lu, Cheng-Te Li", "title": "AGSTN: Learning Attention-adjusted Graph Spatio-Temporal Networks for\n  Short-term Urban Sensor Value Forecasting", "comments": "Published in IEEE ICDM 2020. Code is available at\n  https://github.com/l852888/AGSTN", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.IR cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Forecasting spatio-temporal correlated time series of sensor values is\ncrucial in urban applications, such as air pollution alert, biking resource\nmanagement, and intelligent transportation systems. While recent advances\nexploit graph neural networks (GNN) to better learn spatial and temporal\ndependencies between sensors, they cannot model time-evolving spatio-temporal\ncorrelation (STC) between sensors, and require pre-defined graphs, which are\nneither always available nor totally reliable, and target at only a specific\ntype of sensor data at one time. Moreover, since the form of time-series\nfluctuation is varied across sensors, a model needs to learn fluctuation\nmodulation. To tackle these issues, in this work, we propose a novel GNN-based\nmodel, Attention-adjusted Graph Spatio-Temporal Network (AGSTN). In AGSTN,\nmulti-graph convolution with sequential learning is developed to learn\ntime-evolving STC. Fluctuation modulation is realized by a proposed attention\nadjustment mechanism. Experiments on three sensor data, air quality, bike\ndemand, and traffic flow, exhibit that AGSTN outperforms the state-of-the-art\nmethods.\n", "versions": [{"version": "v1", "created": "Fri, 29 Jan 2021 08:31:38 GMT"}], "update_date": "2021-02-01", "authors_parsed": [["Lu", "Yi-Ju", ""], ["Li", "Cheng-Te", ""]]}, {"id": "2101.12476", "submitter": "Niki Kilbertus", "authors": "Niki Kilbertus", "title": "Beyond traditional assumptions in fair machine learning", "comments": "PhD Thesis submitted at the University of Cambridge, October 2020.\n  The thesis is based on a number of previous works also available on arxiv\n  (see Chapter 1)", "journal-ref": null, "doi": "10.17863/CAM.59055", "report-no": null, "categories": "cs.LG cs.AI cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This thesis scrutinizes common assumptions underlying traditional machine\nlearning approaches to fairness in consequential decision making. After\nchallenging the validity of these assumptions in real-world applications, we\npropose ways to move forward when they are violated. First, we show that group\nfairness criteria purely based on statistical properties of observed data are\nfundamentally limited. Revisiting this limitation from a causal viewpoint we\ndevelop a more versatile conceptual framework, causal fairness criteria, and\nfirst algorithms to achieve them. We also provide tools to analyze how\nsensitive a believed-to-be causally fair algorithm is to misspecifications of\nthe causal graph. Second, we overcome the assumption that sensitive data is\nreadily available in practice. To this end we devise protocols based on secure\nmulti-party computation to train, validate, and contest fair decision\nalgorithms without requiring users to disclose their sensitive data or decision\nmakers to disclose their models. Finally, we also accommodate the fact that\noutcome labels are often only observed when a certain decision has been made.\nWe suggest a paradigm shift away from training predictive models towards\ndirectly learning decisions to relax the traditional assumption that labels can\nalways be recorded. The main contribution of this thesis is the development of\ntheoretically substantiated and practically feasible methods to move research\non fair machine learning closer to real-world applications.\n", "versions": [{"version": "v1", "created": "Fri, 29 Jan 2021 09:02:15 GMT"}], "update_date": "2021-02-01", "authors_parsed": [["Kilbertus", "Niki", ""]]}, {"id": "2101.12501", "submitter": "Thomas Chaffre", "authors": "Thomas Chaffre, Julien Moras, Adrien Chan-Hon-Tong, Julien Marzat,\n  Karl Sammut, Gilles Le Chenadec, Benoit Clement", "title": "Learning-based vs Model-free Adaptive Control of a MAV under Wind Gust", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AI cs.LG cs.SY eess.SY math.OC", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Navigation problems under unknown varying conditions are among the most\nimportant and well-studied problems in the control field. Classic model-based\nadaptive control methods can be applied only when a convenient model of the\nplant or environment is provided. Recent model-free adaptive control methods\naim at removing this dependency by learning the physical characteristics of the\nplant and/or process directly from sensor feedback. Although there have been\nprior attempts at improving these techniques, it remains an open question as to\nwhether it is possible to cope with real-world uncertainties in a control\nsystem that is fully based on either paradigm. We propose a conceptually simple\nlearning-based approach composed of a full state feedback controller, tuned\nrobustly by a deep reinforcement learning framework based on the Soft\nActor-Critic algorithm. We compare it, in realistic simulations, to a\nmodel-free controller that uses the same deep reinforcement learning framework\nfor the control of a micro aerial vehicle under wind gust. The results indicate\nthe great potential of learning-based adaptive control methods in modern\ndynamical systems.\n", "versions": [{"version": "v1", "created": "Fri, 29 Jan 2021 10:13:56 GMT"}, {"version": "v2", "created": "Tue, 6 Jul 2021 01:05:45 GMT"}], "update_date": "2021-07-07", "authors_parsed": [["Chaffre", "Thomas", ""], ["Moras", "Julien", ""], ["Chan-Hon-Tong", "Adrien", ""], ["Marzat", "Julien", ""], ["Sammut", "Karl", ""], ["Chenadec", "Gilles Le", ""], ["Clement", "Benoit", ""]]}, {"id": "2101.12506", "submitter": "Wasim Huleihel", "authors": "Wasim Huleihel and Soumyabrata Pal and Ofer Shayevitz", "title": "Learning User Preferences in Non-Stationary Environments", "comments": "31 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.IR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Recommendation systems often use online collaborative filtering (CF)\nalgorithms to identify items a given user likes over time, based on ratings\nthat this user and a large number of other users have provided in the past.\nThis problem has been studied extensively when users' preferences do not change\nover time (static case); an assumption that is often violated in practical\nsettings. In this paper, we introduce a novel model for online non-stationary\nrecommendation systems which allows for temporal uncertainties in the users'\npreferences. For this model, we propose a user-based CF algorithm, and provide\na theoretical analysis of its achievable reward. Compared to related\nnon-stationary multi-armed bandit literature, the main fundamental difficulty\nin our model lies in the fact that variations in the preferences of a certain\nuser may affect the recommendations for other users severely. We also test our\nalgorithm over real-world datasets, showing its effectiveness in real-world\napplications. One of the main surprising observations in our experiments is the\nfact our algorithm outperforms other static algorithms even when preferences do\nnot change over time. This hints toward the general conclusion that in\npractice, dynamic algorithms, such as the one we propose, might be beneficial\neven in stationary environments.\n", "versions": [{"version": "v1", "created": "Fri, 29 Jan 2021 10:26:16 GMT"}], "update_date": "2021-02-01", "authors_parsed": [["Huleihel", "Wasim", ""], ["Pal", "Soumyabrata", ""], ["Shayevitz", "Ofer", ""]]}, {"id": "2101.12509", "submitter": "David Lindner", "authors": "David Lindner and Kyle Matoba and Alexander Meulemans", "title": "Challenges for Using Impact Regularizers to Avoid Negative Side Effects", "comments": "Presented at the SafeAI workshop at AAAI 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Designing reward functions for reinforcement learning is difficult: besides\nspecifying which behavior is rewarded for a task, the reward also has to\ndiscourage undesired outcomes. Misspecified reward functions can lead to\nunintended negative side effects, and overall unsafe behavior. To overcome this\nproblem, recent work proposed to augment the specified reward function with an\nimpact regularizer that discourages behavior that has a big impact on the\nenvironment. Although initial results with impact regularizers seem promising\nin mitigating some types of side effects, important challenges remain. In this\npaper, we examine the main current challenges of impact regularizers and relate\nthem to fundamental design decisions. We discuss in detail which challenges\nrecent approaches address and which remain unsolved. Finally, we explore\npromising directions to overcome the unsolved challenges in preventing negative\nside effects with impact regularizers.\n", "versions": [{"version": "v1", "created": "Fri, 29 Jan 2021 10:32:51 GMT"}, {"version": "v2", "created": "Tue, 23 Feb 2021 13:49:47 GMT"}], "update_date": "2021-02-24", "authors_parsed": [["Lindner", "David", ""], ["Matoba", "Kyle", ""], ["Meulemans", "Alexander", ""]]}, {"id": "2101.12523", "submitter": "Vojtech Franc", "authors": "V. Franc, D. Prusa, V. Voracek", "title": "Optimal strategies for reject option classifiers", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In classification with a reject option, the classifier is allowed in\nuncertain cases to abstain from prediction. The classical cost-based model of a\nreject option classifier requires the cost of rejection to be defined\nexplicitly. An alternative bounded-improvement model, avoiding the notion of\nthe reject cost, seeks for a classifier with a guaranteed selective risk and\nmaximal cover. We coin a symmetric definition, the bounded-coverage model,\nwhich seeks for a classifier with minimal selective risk and guaranteed\ncoverage. We prove that despite their different formulations the three\nrejection models lead to the same prediction strategy: a Bayes classifier\nendowed with a randomized Bayes selection function. We define a notion of a\nproper uncertainty score as a scalar summary of prediction uncertainty\nsufficient to construct the randomized Bayes selection function. We propose two\nalgorithms to learn the proper uncertainty score from examples for an arbitrary\nblack-box classifier. We prove that both algorithms provide Fisher consistent\nestimates of the proper uncertainty score and we demonstrate their efficiency\non different prediction problems including classification, ordinal regression\nand structured output classification.\n", "versions": [{"version": "v1", "created": "Fri, 29 Jan 2021 11:09:32 GMT"}], "update_date": "2021-02-01", "authors_parsed": [["Franc", "V.", ""], ["Prusa", "D.", ""], ["Voracek", "V.", ""]]}, {"id": "2101.12547", "submitter": "Yifan Wu", "authors": "Yifan Wu, Min Gao, Min Zeng, Feiyang Chen, Min Li and Jie Zhang", "title": "BridgeDPI: A Novel Graph Neural Network for Predicting Drug-Protein\n  Interactions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.SI q-bio.QM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motivation: Exploring drug-protein interactions (DPIs) work as a pivotal step\nin drug discovery. The fast expansion of available biological data enables\ncomputational methods effectively assist in experimental methods. Among them,\ndeep learning methods extract features only from basic characteristics, such as\nprotein sequences, molecule structures. Others achieve significant improvement\nby learning from not only sequences/molecules but the protein-protein and\ndrug-drug associations (PPAs and DDAs). The PPAs and DDAs are generally\nobtained by using computational methods. However, existing computational\nmethods have some limitations, resulting in low-quality PPAs and DDAs that\nhamper the prediction performance. Therefore, we hope to develop a novel\nsupervised learning method to learn the PPAs and DDAs effectively and thereby\nimprove the prediction performance of the specific task of DPI. Results: In\nthis research, we propose a novel deep learning framework, namely BridgeDPI.\nBridgeDPI introduces a class of nodes named hyper-nodes, which bridge different\nproteins/drugs to work as PPAs and DDAs. The hyper-nodes can be supervised\nlearned for the specific task of DPI since the whole process is an end-to-end\nlearning. Consequently, such a model would improve prediction performance of\nDPI. In three real-world datasets, we further demonstrate that BridgeDPI\noutperforms state-of-the-art methods. Moreover, ablation studies verify the\neffectiveness of the hyper-nodes. Last, in an independent verification,\nBridgeDPI explores the candidate bindings among COVID-19's proteins and various\nantiviral drugs. And the predictive results accord with the statement of the\nWorld Health Organization and Food and Drug Administration, showing the\nvalidity and reliability of BridgeDPI.\n", "versions": [{"version": "v1", "created": "Fri, 29 Jan 2021 12:53:39 GMT"}], "update_date": "2021-02-01", "authors_parsed": [["Wu", "Yifan", ""], ["Gao", "Min", ""], ["Zeng", "Min", ""], ["Chen", "Feiyang", ""], ["Li", "Min", ""], ["Zhang", "Jie", ""]]}, {"id": "2101.12550", "submitter": "Vlad Landa", "authors": "Vlad Landa and Yuval Reuveni", "title": "Low Dimensional Convolutional Neural Network For Solar Flares GOES Time\n  Series Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "astro-ph.SR astro-ph.HE astro-ph.IM cs.LG", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Space weather phenomena such as solar flares, have massive destructive power\nwhen reaches certain amount of magnitude. Such high magnitude solar flare event\ncan interfere space-earth radio communications and neutralize space-earth\nelectronics equipment. In the current study, we explorer the deep learning\napproach to build a solar flare forecasting model and examine its limitations\nalong with the ability of features extraction, based on the available\ntime-series data. For that purpose, we present a multi-layer 1D Convolutional\nNeural Network (CNN) to forecast solar flare events probability occurrence of M\nand X classes at 1,3,6,12,24,48,72,96 hours time frame. In order to train and\nevaluate the performance of the model, we utilised the available Geostationary\nOperational Environmental Satellite (GOES) X-ray time series data, ranged\nbetween July 1998 and January 2019, covering almost entirely the solar cycles\n23 and 24. The forecasting model were trained and evaluated in two different\nscenarios (1) random selection and (2) chronological selection, which were\ncompare afterward. Moreover we compare our results to those considered as\nstate-of-the-art flare forecasting models, both with similar approaches and\ndifferent ones.The majority of the results indicates that (1) chronological\nselection obtain a degradation factor of 3\\% versus the random selection for\nthe M class model and elevation factor of 2\\% for the X class model. (2) When\nconsider utilizing only X-ray time-series data, the suggested model achieve\nhigh score results compare to other studies. (3) The suggested model combined\nwith solely X-ray time-series fails to distinguish between M class magnitude\nand X class magnitude solar flare events. All source code are available at\nhttps://github.com/vladlanda/Low-Dimensional-Convolutional-Neural-Network-For-Solar-Flares-GOES-Time-Series-Classification\n", "versions": [{"version": "v1", "created": "Fri, 29 Jan 2021 12:55:57 GMT"}], "update_date": "2021-02-01", "authors_parsed": [["Landa", "Vlad", ""], ["Reuveni", "Yuval", ""]]}, {"id": "2101.12555", "submitter": "Xinjiang Lu", "authors": "Haoran Xin, Xinjiang Lu, Tong Xu, Hao Liu, Jingjing Gu, Dejing Dou,\n  Hui Xiong", "title": "Out-of-Town Recommendation with Travel Intention Modeling", "comments": "Accepted by AAAI-2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Out-of-town recommendation is designed for those users who leave their\nhome-town areas and visit the areas they have never been to before. It is\nchallenging to recommend Point-of-Interests (POIs) for out-of-town users since\nthe out-of-town check-in behavior is determined by not only the user's\nhome-town preference but also the user's travel intention. Besides, the user's\ntravel intentions are complex and dynamic, which leads to big difficulties in\nunderstanding such intentions precisely. In this paper, we propose a\nTRAvel-INtention-aware Out-of-town Recommendation framework, named TRAINOR. The\nproposed TRAINOR framework distinguishes itself from existing out-of-town\nrecommenders in three aspects. First, graph neural networks are explored to\nrepresent users' home-town check-in preference and geographical constraints in\nout-of-town check-in behaviors. Second, a user-specific travel intention is\nformulated as an aggregation combining home-town preference and generic travel\nintention together, where the generic travel intention is regarded as a mixture\nof inherent intentions that can be learned by Neural Topic Model (NTM). Third,\na non-linear mapping function, as well as a matrix factorization method, are\nemployed to transfer users' home-town preference and estimate out-of-town POI's\nrepresentation, respectively. Extensive experiments on real-world data sets\nvalidate the effectiveness of the TRAINOR framework. Moreover, the learned\ntravel intention can deliver meaningful explanations for understanding a user's\ntravel purposes.\n", "versions": [{"version": "v1", "created": "Fri, 29 Jan 2021 13:14:29 GMT"}, {"version": "v2", "created": "Sat, 6 Feb 2021 05:12:13 GMT"}], "update_date": "2021-02-09", "authors_parsed": [["Xin", "Haoran", ""], ["Lu", "Xinjiang", ""], ["Xu", "Tong", ""], ["Liu", "Hao", ""], ["Gu", "Jingjing", ""], ["Dou", "Dejing", ""], ["Xiong", "Hui", ""]]}, {"id": "2101.12578", "submitter": "Fan-Keng Sun", "authors": "Fan-Keng Sun and Christopher I. Lang and Duane S. Boning", "title": "Adjusting for Autocorrelated Errors in Neural Networks for Time Series\n  Regression and Forecasting", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In many cases, it is difficult to generate highly accurate models for time\nseries data using a known parametric model structure. In response, an\nincreasing body of research focuses on using neural networks to model time\nseries approximately. A common assumption in training neural networks on time\nseries is that the errors at different time steps are uncorrelated. However,\ndue to the temporality of the data, errors are actually autocorrelated in many\ncases, which makes such maximum likelihood estimation inaccurate. In this\npaper, we propose to learn the autocorrelation coefficient jointly with the\nmodel parameters in order to adjust for autocorrelated errors. For time series\nregression, large-scale experiments indicate that our method outperforms the\nPrais-Winsten method, especially when the autocorrelation is strong.\nFurthermore, we broaden our method to time series forecasting and apply it with\nvarious state-of-the-art models. Results across a wide range of real-world\ndatasets show that our method enhances performance in almost all cases.\n", "versions": [{"version": "v1", "created": "Thu, 28 Jan 2021 04:25:51 GMT"}, {"version": "v2", "created": "Mon, 1 Feb 2021 04:12:58 GMT"}, {"version": "v3", "created": "Wed, 3 Mar 2021 01:01:16 GMT"}], "update_date": "2021-03-04", "authors_parsed": [["Sun", "Fan-Keng", ""], ["Lang", "Christopher I.", ""], ["Boning", "Duane S.", ""]]}, {"id": "2101.12583", "submitter": "Sachin Kasture Dr.", "authors": "Sachin Kasture", "title": "Discovering dependencies in complex physical systems using Neural\n  Networks", "comments": "6 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.data-an cs.LG physics.comp-ph", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In todays age of data, discovering relationships between different variables\nis an interesting and a challenging problem. This problem becomes even more\ncritical with regards to complex dynamical systems like weather forecasting and\neconometric models, which can show highly non-linear behavior. A method based\non mutual information and deep neural networks is proposed as a versatile\nframework for discovering non-linear relationships ranging from functional\ndependencies to causality. We demonstrate the application of this method to\nactual multivariable non-linear dynamical systems. We also show that this\nmethod can find relationships even for datasets with small number of\ndatapoints, as is often the case with empirical data.\n", "versions": [{"version": "v1", "created": "Wed, 27 Jan 2021 18:59:19 GMT"}], "update_date": "2021-02-01", "authors_parsed": [["Kasture", "Sachin", ""]]}, {"id": "2101.12588", "submitter": "Tareq Si Salem", "authors": "Tareq Si Salem, Giovanni Neglia and Stratis Ioannidis", "title": "No-Regret Caching via Online Mirror Descent", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NI cs.PF", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We study an online caching problem in which requests can be served by a local\ncache to avoid retrieval costs from a remote server. The cache can update its\nstate after a batch of requests and store an arbitrarily small fraction of each\ncontent. We study no-regret algorithms based on Online Mirror Descent (OMD)\nstrategies. We show that the optimal OMD strategy depends on the request\ndiversity present in a batch. We also prove that, when the cache must store the\nentire content, rather than a fraction, OMD strategies can be coupled with a\nrandomized rounding scheme that preserves regret guarantees.\n", "versions": [{"version": "v1", "created": "Fri, 29 Jan 2021 13:56:51 GMT"}, {"version": "v2", "created": "Mon, 1 Feb 2021 08:14:49 GMT"}, {"version": "v3", "created": "Mon, 8 Feb 2021 10:52:42 GMT"}], "update_date": "2021-02-09", "authors_parsed": [["Salem", "Tareq Si", ""], ["Neglia", "Giovanni", ""], ["Ioannidis", "Stratis", ""]]}, {"id": "2101.12609", "submitter": "Massimiliano Mancini", "authors": "Massimiliano Mancini, Muhammad Ferjad Naeem, Yongqin Xian, Zeynep\n  Akata", "title": "Open World Compositional Zero-Shot Learning", "comments": "Accepted in IEEE CVPR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Compositional Zero-Shot learning (CZSL) requires to recognize state-object\ncompositions unseen during training. In this work, instead of assuming prior\nknowledge about the unseen compositions, we operate in the open world setting,\nwhere the search space includes a large number of unseen compositions some of\nwhich might be unfeasible. In this setting, we start from the cosine similarity\nbetween visual features and compositional embeddings. After estimating the\nfeasibility score of each composition, we use these scores to either directly\nmask the output space or as a margin for the cosine similarity between visual\nfeatures and compositional embeddings during training. Our experiments on two\nstandard CZSL benchmarks show that all the methods suffer severe performance\ndegradation when applied in the open world setting. While our simple CZSL model\nachieves state-of-the-art performances in the closed world scenario, our\nfeasibility scores boost the performance of our approach in the open world\nsetting, clearly outperforming the previous state of the art.\n", "versions": [{"version": "v1", "created": "Fri, 29 Jan 2021 14:45:52 GMT"}, {"version": "v2", "created": "Wed, 3 Feb 2021 10:07:28 GMT"}, {"version": "v3", "created": "Tue, 30 Mar 2021 15:48:21 GMT"}], "update_date": "2021-03-31", "authors_parsed": [["Mancini", "Massimiliano", ""], ["Naeem", "Muhammad Ferjad", ""], ["Xian", "Yongqin", ""], ["Akata", "Zeynep", ""]]}, {"id": "2101.12615", "submitter": "Thomas Johnson", "authors": "Thomas Johnson, Eiman Kanjo, Kieran Woodward", "title": "DigitalExposome: Quantifying the Urban Environment Influence on\n  Wellbeing based on Real-Time Multi-Sensor Fusion and Deep Belief Network", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  In this paper, we define the term 'DigitalExposome' as a conceptual framework\nthat takes us closer towards understanding the relationship between\nenvironment, personal characteristics, behaviour and wellbeing using multimodel\nmobile sensing technology. Specifically, we simultaneously collected (for the\nfirst time) multi-sensor data including urban environmental factors (e.g. air\npollution including: PM1, PM2.5, PM10, Oxidised, Reduced, NH3 and Noise, People\nCount in the vicinity), body reaction (physiological reactions including: EDA,\nHR, HRV, Body Temperature, BVP and movement) and individuals' perceived\nresponses (e.g. self-reported valence) in urban settings. Our users followed a\npre-specified urban path and collected the data using a comprehensive sensing\nedge devices. The data is instantly fused, time-stamped and geo-tagged at the\npoint of collection. A range of multivariate statistical analysis techniques\nhave been applied including Principle Component Analysis, Regression and\nspatial visualisations to unravel the relationship between the variables.\nResults showed that EDA and Heart Rate Variability HRV are noticeably impacted\nby the level of Particulate Matters (PM) in the environment well with the\nenvironmental variables. Furthermore, we adopted Deep Belief Network to extract\nfeatures from the multimodel data feed which outperformed Convolutional Neural\nNetwork and achieved up to (a=80.8%, {\\sigma}=0.001) accuracy.\n", "versions": [{"version": "v1", "created": "Fri, 29 Jan 2021 14:55:19 GMT"}], "update_date": "2021-02-01", "authors_parsed": [["Johnson", "Thomas", ""], ["Kanjo", "Eiman", ""], ["Woodward", "Kieran", ""]]}, {"id": "2101.12616", "submitter": "Ido Freeman", "authors": "Ido Freeman, Kun Zhao, Anton Kummert", "title": "Polynomial Trajectory Predictions for Improved Learning Performance", "comments": "To appear in IEEE ICIP 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The rising demand for Active Safety systems in automotive applications\nstresses the need for a reliable short to mid-term trajectory prediction.\nAnticipating the unfolding path of road users, one can act to increase the\noverall safety. In this work, we propose to train artificial neural networks\nfor movement understanding by predicting trajectories in their natural form, as\na function of time. Predicting polynomial coefficients allows us to increased\naccuracy and improve generalisation.\n", "versions": [{"version": "v1", "created": "Fri, 29 Jan 2021 14:58:27 GMT"}, {"version": "v2", "created": "Wed, 16 Jun 2021 11:56:26 GMT"}], "update_date": "2021-06-17", "authors_parsed": [["Freeman", "Ido", ""], ["Zhao", "Kun", ""], ["Kummert", "Anton", ""]]}, {"id": "2101.12632", "submitter": "Mehran Hossein Zadeh Bazargani", "authors": "Mehran H. Z. Bazargani, Arjun Pakrashi, Brian Mac Namee", "title": "The Deep Radial Basis Function Data Descriptor (D-RBFDD) Network: A\n  One-Class Neural Network for Anomaly Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Anomaly detection is a challenging problem in machine learning, and is even\nmore so when dealing with instances that are captured in low-level, raw data\nrepresentations without a well-behaved set of engineered features. The Radial\nBasis Function Data Descriptor (RBFDD) network is an effective solution for\nanomaly detection, however, it is a shallow model that does not deal\neffectively with raw data representations. This paper investigates approaches\nto modifying the RBFDD network to transform it into a deep one-class classifier\nsuitable for anomaly detection problems with low-level raw data\nrepresentations. We show that approaches based on transfer learning are not\neffective and our results suggest that this is because the latent\nrepresentations learned by generic classification models are not suitable for\nanomaly detection. Instead we show that an approach that adds multiple\nconvolutional layers before the RBF layer, to form a Deep Radial Basis Function\nData Descriptor (D-RBFDD) network, is very effective. This is shown in a set of\nevaluation experiments using multiple anomaly detection scenarios created from\npublicly available image classification datasets, and a real-world anomaly\ndetection dataset in which different types of arrhythmia are detected in\nelectrocardiogram (ECG) data. Our experiments show that the D-RBFDD network\nout-performs state-of-the-art anomaly detection methods including the Deep\nSupport Vector Data Descriptor (Deep SVDD), One-Class SVM, and Isolation Forest\non the image datasets, and produces competitive results for the ECG dataset.\n", "versions": [{"version": "v1", "created": "Fri, 29 Jan 2021 15:15:17 GMT"}], "update_date": "2021-02-01", "authors_parsed": [["Bazargani", "Mehran H. Z.", ""], ["Pakrashi", "Arjun", ""], ["Mac Namee", "Brian", ""]]}, {"id": "2101.12640", "submitter": "Leshem Choshen", "authors": "Leshem Choshen, Omri Abend", "title": "Transition based Graph Decoder for Neural Machine Translation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  While a number of works showed gains from incorporating source-side symbolic\nsyntactic and semantic structure into neural machine translation (NMT), much\nfewer works addressed the decoding of such structure.\n  We propose a general Transformer-based approach for tree and graph decoding\nbased on generating a sequence of transitions, inspired by a similar approach\nthat uses RNNs by Dyer (2016).\n  Experiments with using the proposed decoder with Universal Dependencies\nsyntax on English-German, German-English and English-Russian show improved\nperformance over the standard Transformer decoder, as well as over ablated\nversions of the model.\\tacltxt{\\footnote{All code implementing the presented\nmodels will be released upon acceptance.\n", "versions": [{"version": "v1", "created": "Fri, 29 Jan 2021 15:20:45 GMT"}], "update_date": "2021-02-01", "authors_parsed": [["Choshen", "Leshem", ""], ["Abend", "Omri", ""]]}, {"id": "2101.12677", "submitter": "Benjamin Kiefer", "authors": "Benjamin Kiefer, Martin Messmer, Andreas Zell", "title": "Leveraging domain labels for object detection from UAVs", "comments": "Under review for ICIP 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Object detection from Unmanned Aerial Vehicles (UAVs) is of great importance\nin many aerial vision-based applications. Despite the great success of generic\nobject detection methods, a large performance drop is observed when applied to\nimages captured by UAVs. This is due to large variations in imaging conditions,\nsuch as varying altitudes, dynamically changing viewing angles, and different\ncapture times. We demonstrate that domain knowledge is a valuable source of\ninformation and thus propose domain-aware object detectors by using freely\naccessible sensor data. By splitting the model into cross-domain and\ndomain-specific parts, substantial performance improvements are achieved on\nmultiple datasets across multiple models and metrics. In particular, we achieve\na new state-of-the-art performance on UAVDT for real-time detectors.\nFurthermore, we create a new airborne image dataset by annotating 13 713\nobjects in 2 900 images featuring precise altitude and viewing angle\nannotations.\n", "versions": [{"version": "v1", "created": "Fri, 29 Jan 2021 16:42:52 GMT"}], "update_date": "2021-02-01", "authors_parsed": [["Kiefer", "Benjamin", ""], ["Messmer", "Martin", ""], ["Zell", "Andreas", ""]]}, {"id": "2101.12678", "submitter": "Hannes K\\\"ohler", "authors": "Hannes K\\\"ohler, Andreas Christmann", "title": "Total Stability of SVMs and Localized SVMs", "comments": "30 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Regularized kernel-based methods such as support vector machines (SVMs)\ntypically depend on the underlying probability measure $\\mathrm{P}$\n(respectively an empirical measure $\\mathrm{D}_n$ in applications) as well as\non the regularization parameter $\\lambda$ and the kernel $k$. Whereas classical\nstatistical robustness only considers the effect of small perturbations in\n$\\mathrm{P}$, the present paper investigates the influence of simultaneous\nslight variations in the whole triple $(\\mathrm{P},\\lambda,k)$, respectively\n$(\\mathrm{D}_n,\\lambda_n,k)$, on the resulting predictor. Existing results from\nthe literature are considerably generalized and improved. In order to also make\nthem applicable to big data, where regular SVMs suffer from their super-linear\ncomputational requirements, we show how our results can be transferred to the\ncontext of localized learning. Here, the effect of slight variations in the\napplied regionalization, which might for example stem from changes in\n$\\mathrm{P}$ respectively $\\mathrm{D}_n$, is considered as well.\n", "versions": [{"version": "v1", "created": "Fri, 29 Jan 2021 16:44:14 GMT"}], "update_date": "2021-02-01", "authors_parsed": [["K\u00f6hler", "Hannes", ""], ["Christmann", "Andreas", ""]]}, {"id": "2101.12684", "submitter": "Michel van der Wel", "authors": "Bart H.L. Overes and Michel van der Wel", "title": "Modelling Sovereign Credit Ratings: Evaluating the Accuracy and Driving\n  Factors using Machine Learning Techniques", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.ST cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sovereign credit ratings summarize the creditworthiness of countries. These\nratings have a large influence on the economy and the yields at which\ngovernments can issue new debt. This paper investigates the use of a Multilayer\nPerceptron (MLP), Classification and Regression Trees (CART), Support Vector\nMachines (SVM), Na\\\"ive Bayes (NB), and an Ordered Logit (OL) model for the\nprediction of sovereign credit ratings. We show that MLP is best suited for\npredicting sovereign credit ratings, with a random cross-validated accuracy of\n68%, followed by CART (59%), SVM (41%), NB (38%), and OL (33%). Investigation\nof the determining factors shows that there is some heterogeneity in the\nimportant variables across the models. However, the two models with the highest\nout-of-sample predictive accuracy, MLP and CART, show a lot of similarities in\nthe influential variables, with regulatory quality, and GDP per capita as\ncommon important variables. Consistent with economic theory, a higher\nregulatory quality and/or GDP per capita are associated with a higher credit\nrating.\n", "versions": [{"version": "v1", "created": "Fri, 29 Jan 2021 17:06:18 GMT"}, {"version": "v2", "created": "Thu, 15 Jul 2021 09:11:25 GMT"}], "update_date": "2021-07-16", "authors_parsed": [["Overes", "Bart H. L.", ""], ["van der Wel", "Michel", ""]]}, {"id": "2101.12699", "submitter": "Weijie J. Su", "authors": "Cong Fang, Hangfeng He, Qi Long, Weijie J. Su", "title": "Layer-Peeled Model: Toward Understanding Well-Trained Deep Neural\n  Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV math.OC stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper, we introduce the Layer-Peeled Model, a nonconvex yet\nanalytically tractable optimization program, in a quest to better understand\ndeep neural networks that are trained for a sufficiently long time. As the name\nsuggests, this new model is derived by isolating the topmost layer from the\nremainder of the neural network, followed by imposing certain constraints\nseparately on the two parts. We demonstrate that the Layer-Peeled Model, albeit\nsimple, inherits many characteristics of well-trained neural networks, thereby\noffering an effective tool for explaining and predicting common empirical\npatterns of deep learning training. First, when working on class-balanced\ndatasets, we prove that any solution to this model forms a simplex equiangular\ntight frame, which in part explains the recently discovered phenomenon of\nneural collapse in deep learning training [PHD20]. Moreover, when moving to the\nimbalanced case, our analysis of the Layer-Peeled Model reveals a hitherto\nunknown phenomenon that we term Minority Collapse, which fundamentally limits\nthe performance of deep learning models on the minority classes. In addition,\nwe use the Layer-Peeled Model to gain insights into how to mitigate Minority\nCollapse. Interestingly, this phenomenon is first predicted by the Layer-Peeled\nModel before its confirmation by our computational experiments.\n", "versions": [{"version": "v1", "created": "Fri, 29 Jan 2021 17:37:17 GMT"}, {"version": "v2", "created": "Mon, 15 Feb 2021 20:31:42 GMT"}], "update_date": "2021-02-17", "authors_parsed": [["Fang", "Cong", ""], ["He", "Hangfeng", ""], ["Long", "Qi", ""], ["Su", "Weijie J.", ""]]}, {"id": "2101.12700", "submitter": "Matthew Dale", "authors": "Matthew Dale, Richard F. L. Evans, Sarah Jenkins, Simon O'Keefe,\n  Angelika Sebald, Susan Stepney, Fernando Torre, Martin Trefzer", "title": "Reservoir Computing with Thin-film Ferromagnetic Devices", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.ET cond-mat.mtrl-sci cs.AR cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Advances in artificial intelligence are driven by technologies inspired by\nthe brain, but these technologies are orders of magnitude less powerful and\nenergy efficient than biological systems. Inspired by the nonlinear dynamics of\nneural networks, new unconventional computing hardware has emerged with the\npotential for extreme parallelism and ultra-low power consumption. Physical\nreservoir computing demonstrates this with a variety of unconventional systems\nfrom optical-based to spintronic. Reservoir computers provide a nonlinear\nprojection of the task input into a high-dimensional feature space by\nexploiting the system's internal dynamics. A trained readout layer then\ncombines features to perform tasks, such as pattern recognition and time-series\nanalysis. Despite progress, achieving state-of-the-art performance without\nexternal signal processing to the reservoir remains challenging. Here we show,\nthrough simulation, that magnetic materials in thin-film geometries can realise\nreservoir computers with greater than or similar accuracy to digital recurrent\nneural networks. Our results reveal that basic spin properties of magnetic\nfilms generate the required nonlinear dynamics and memory to solve machine\nlearning tasks. Furthermore, we show that neuromorphic hardware can be reduced\nin size by removing the need for discrete neural components and external\nprocessing. The natural dynamics and nanoscale size of magnetic thin-films\npresent a new path towards fast energy-efficient computing with the potential\nto innovate portable smart devices, self driving vehicles, and robotics.\n", "versions": [{"version": "v1", "created": "Fri, 29 Jan 2021 17:37:17 GMT"}], "update_date": "2021-02-01", "authors_parsed": [["Dale", "Matthew", ""], ["Evans", "Richard F. L.", ""], ["Jenkins", "Sarah", ""], ["O'Keefe", "Simon", ""], ["Sebald", "Angelika", ""], ["Stepney", "Susan", ""], ["Torre", "Fernando", ""], ["Trefzer", "Martin", ""]]}, {"id": "2101.12704", "submitter": "Hong Xing", "authors": "Hong Xing and Osvaldo Simeone and Suzhi Bi", "title": "Federated Learning over Wireless Device-to-Device Networks: Algorithms\n  and Convergence Analysis", "comments": "41 pages, 10 figures, submitted for possible journal publication", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.LG cs.NI eess.SP math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The proliferation of Internet-of-Things (IoT) devices and cloud-computing\napplications over siloed data centers is motivating renewed interest in the\ncollaborative training of a shared model by multiple individual clients via\nfederated learning (FL). To improve the communication efficiency of FL\nimplementations in wireless systems, recent works have proposed compression and\ndimension reduction mechanisms, along with digital and analog transmission\nschemes that account for channel noise, fading, and interference. This prior\nart has mainly focused on star topologies consisting of distributed clients and\na central server. In contrast, this paper studies FL over wireless\ndevice-to-device (D2D) networks by providing theoretical insights into the\nperformance of digital and analog implementations of decentralized stochastic\ngradient descent (DSGD). First, we introduce generic digital and analog\nwireless implementations of communication-efficient DSGD algorithms, leveraging\nrandom linear coding (RLC) for compression and over-the-air computation\n(AirComp) for simultaneous analog transmissions. Next, under the assumptions of\nconvexity and connectivity, we provide convergence bounds for both\nimplementations. The results demonstrate the dependence of the optimality gap\non the connectivity and on the signal-to-noise ratio (SNR) levels in the\nnetwork. The analysis is corroborated by experiments on an image-classification\ntask.\n", "versions": [{"version": "v1", "created": "Fri, 29 Jan 2021 17:42:26 GMT"}], "update_date": "2021-02-01", "authors_parsed": [["Xing", "Hong", ""], ["Simeone", "Osvaldo", ""], ["Bi", "Suzhi", ""]]}, {"id": "2101.12708", "submitter": "Calvin Tsay", "authors": "Jan Kronqvist and Ruth Misener and Calvin Tsay", "title": "Between steps: Intermediate relaxations between big-M and convex hull\n  formulations", "comments": "16 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work develops a class of relaxations in between the big-M and convex\nhull formulations of disjunctions, drawing advantages from both. The proposed\n\"P-split\" formulations split convex additively separable constraints into P\npartitions and form the convex hull of the partitioned disjuncts. Parameter P\nrepresents the trade-off of model size vs. relaxation strength. We examine the\nnovel formulations and prove that, under certain assumptions, the relaxations\nform a hierarchy starting from a big-M equivalent and converging to the convex\nhull. We computationally compare the proposed formulations to big-M and convex\nhull formulations on a test set including: K-means clustering, P_ball problems,\nand ReLU neural networks. The computational results show that the intermediate\nP-split formulations can form strong outer approximations of the convex hull\nwith fewer variables and constraints than the extended convex hull\nformulations, giving significant computational advantages over both the big-M\nand convex hull.\n", "versions": [{"version": "v1", "created": "Fri, 29 Jan 2021 18:03:15 GMT"}], "update_date": "2021-02-01", "authors_parsed": [["Kronqvist", "Jan", ""], ["Misener", "Ruth", ""], ["Tsay", "Calvin", ""]]}, {"id": "2101.12719", "submitter": "Emma Benjaminson", "authors": "Emma Benjaminson (1), Rebecca E. Taylor (1,2,3), Matthew Travers (4)\n  ((1) Mechanical Engineering, Carnegie Mellon University, Pittsburgh, PA, (2)\n  Biomedical Engineering, Carnegie Mellon University, Pittsburgh, PA, (3)\n  Electrical and Computer Engineering, Carnegie Mellon University, Pittsburgh\n  PA, (4) Robotics Institute, Carnegie Mellon University, Pittsburgh, PA)", "title": "Predicting Nanorobot Shapes via Generative Models", "comments": "8 pages, 2 figures, accepted to Machine Learning for Engineering\n  Modeling, Simulation, and Design Workshop at Neural Information Processing\n  Systems 2020, December 12, 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.RO", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  The field of DNA nanotechnology has made it possible to assemble, with high\nyields, different structures that have actionable properties. For example,\nresearchers have created components that can be actuated. An exciting next step\nis to combine these components into multifunctional nanorobots that could,\npotentially, perform complex tasks like swimming to a target location in the\nhuman body, detect an adverse reaction and then release a drug load to stop it.\nHowever, as we start to assemble more complex nanorobots, the yield of the\ndesired nanorobot begins to decrease as the number of possible component\ncombinations increases. Therefore, the ultimate goal of this work is to develop\na predictive model to maximize yield. However, training predictive models\ntypically requires a large dataset. For the nanorobots we are interested in\nassembling, this will be difficult to collect. This is because high-fidelity\ndata, which allows us to characterize the shape and size of individual\nstructures, is very time-consuming to collect, whereas low-fidelity data is\nreadily available but only captures bulk statistics for different processes.\nTherefore, this work combines low- and high-fidelity data to train a generative\nmodel using a two-step process. We first use a relatively small, high-fidelity\ndataset to train a generative model. At run time, the model takes low-fidelity\ndata and uses it to approximate the high-fidelity content. We do this by\nbiasing the model towards samples with specific properties as measured by\nlow-fidelity data. In this work we bias our distribution towards a desired node\ndegree of a graphical model that we take as a surrogate representation of the\nnanorobots that this work will ultimately focus on. We have not yet accumulated\na high-fidelity dataset of nanorobots, so we leverage the MolGAN architecture\n[1] and the QM9 small molecule dataset [2-3] to demonstrate our approach.\n", "versions": [{"version": "v1", "created": "Fri, 29 Jan 2021 18:29:51 GMT"}], "update_date": "2021-02-01", "authors_parsed": [["Benjaminson", "Emma", ""], ["Taylor", "Rebecca E.", ""], ["Travers", "Matthew", ""]]}, {"id": "2101.12723", "submitter": "Luis Felipe Casta\\~no Ledesma", "authors": "F. Casta\\~no, E. Fidalgo, E. Alegre, D. Chaves, M. Sanchez-Paniagua", "title": "State of the Art: Content-based and Hybrid Phishing Detection", "comments": "6 pages, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Phishing attacks have evolved and increased over time and, for this reason,\nthe task of distinguishing between a legitimate site and a phishing site is\nmore and more difficult, fooling even the most expert users. The main proposals\nfocused on addressing this problem can be divided into four approaches:\nList-based, URL based, content-based, and hybrid. In this state of the art, the\nmost recent techniques using web content-based and hybrid approaches for\nPhishing Detection are reviewed and compared.\n", "versions": [{"version": "v1", "created": "Fri, 29 Jan 2021 18:34:59 GMT"}], "update_date": "2021-02-01", "authors_parsed": [["Casta\u00f1o", "F.", ""], ["Fidalgo", "E.", ""], ["Alegre", "E.", ""], ["Chaves", "D.", ""], ["Sanchez-Paniagua", "M.", ""]]}, {"id": "2101.12727", "submitter": "Samarth Mishra", "authors": "Samarth Mishra, Kate Saenko, Venkatesh Saligrama", "title": "Surprisingly Simple Semi-Supervised Domain Adaptation with Pretraining\n  and Consistency", "comments": "13 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Visual domain adaptation involves learning to classify images from a target\nvisual domain using labels available in a different source domain. A range of\nprior work uses adversarial domain alignment to try and learn a domain\ninvariant feature space, where a good source classifier can perform well on\ntarget data. This however, can lead to errors where class A features in the\ntarget domain get aligned to class B features in source. We show that in the\npresence of a few target labels, simple techniques like self-supervision (via\nrotation prediction) and consistency regularization can be effective without\nany adversarial alignment to learn a good target classifier. Our Pretraining\nand Consistency (PAC) approach, can achieve state of the art accuracy on this\nsemi-supervised domain adaptation task, surpassing multiple adversarial domain\nalignment methods, across multiple datasets. Notably, it outperforms all recent\napproaches by 3-5% on the large and challenging DomainNet benchmark, showing\nthe strength of these simple techniques in fixing errors made by adversarial\nalignment.\n", "versions": [{"version": "v1", "created": "Fri, 29 Jan 2021 18:40:17 GMT"}], "update_date": "2021-02-01", "authors_parsed": [["Mishra", "Samarth", ""], ["Saenko", "Kate", ""], ["Saligrama", "Venkatesh", ""]]}, {"id": "2101.12741", "submitter": "Renshen Wang", "authors": "Renshen Wang, Yasuhisa Fujii and Ashok C. Popat", "title": "Post-OCR Paragraph Recognition by Graph Convolutional Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Paragraphs are an important class of document entities. We propose a new\napproach for paragraph identification by spatial graph convolutional neural\nnetworks (GCN) applied on OCR text boxes. Two steps, namely line splitting and\nline clustering, are performed to extract paragraphs from the lines in OCR\nresults. Each step uses a beta-skeleton graph constructed from bounding boxes,\nwhere the graph edges provide efficient support for graph convolution\noperations. With only pure layout input features, the GCN model size is 3~4\norders of magnitude smaller compared to R-CNN based models, while achieving\ncomparable or better accuracies on PubLayNet and other datasets. Furthermore,\nthe GCN models show good generalization from synthetic training data to\nreal-world images, and good adaptivity for variable document styles.\n", "versions": [{"version": "v1", "created": "Fri, 29 Jan 2021 18:54:53 GMT"}, {"version": "v2", "created": "Mon, 1 Feb 2021 19:17:29 GMT"}, {"version": "v3", "created": "Wed, 26 May 2021 22:05:02 GMT"}, {"version": "v4", "created": "Tue, 20 Jul 2021 18:53:39 GMT"}], "update_date": "2021-07-22", "authors_parsed": [["Wang", "Renshen", ""], ["Fujii", "Yasuhisa", ""], ["Popat", "Ashok C.", ""]]}, {"id": "2101.12745", "submitter": "Jiaqi Yang", "authors": "Zihan Zhang, Jiaqi Yang, Xiangyang Ji, Simon S. Du", "title": "Variance-Aware Confidence Set: Variance-Dependent Bound for Linear\n  Bandits and Horizon-Free Bound for Linear Mixture MDP", "comments": "31 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We show how to construct variance-aware confidence sets for linear bandits\nand linear mixture Markov Decision Process (MDP). Our method yields the\nfollowing new regret bounds:\n  * For linear bandits, we obtain an $\\widetilde{O}(\\mathrm{poly}(d)\\sqrt{1 +\n\\sum_{i=1}^{K}\\sigma_i^2})$ regret bound, where $d$ is the feature dimension,\n$K$ is the number of rounds, and $\\sigma_i^2$ is the (unknown) variance of the\nreward at the $i$-th round. This is the first regret bound that only scales\nwith the variance and the dimension, with no explicit polynomial dependency on\n$K$.\n  * For linear mixture MDP, we obtain an $\\widetilde{O}(\\mathrm{poly}(d, \\log\nH)\\sqrt{K})$ regret bound, where $d$ is the number of base models, $K$ is the\nnumber of episodes, and $H$ is the planning horizon. This is the first regret\nbound that only scales logarithmically with $H$ in the reinforcement learning\nwith linear function approximation setting, thus exponentially improving\nexisting results.\n  Our methods utilize three novel ideas that may be of independent interest: 1)\napplications of the peeling techniques to the norm of input and the magnitude\nof variance, 2) a recursion-based approach to estimate the variance, and 3) a\nconvex potential lemma that somewhat generalizes the seminal elliptical\npotential lemma.\n", "versions": [{"version": "v1", "created": "Fri, 29 Jan 2021 18:57:52 GMT"}, {"version": "v2", "created": "Fri, 19 Feb 2021 15:46:46 GMT"}], "update_date": "2021-02-22", "authors_parsed": [["Zhang", "Zihan", ""], ["Yang", "Jiaqi", ""], ["Ji", "Xiangyang", ""], ["Du", "Simon S.", ""]]}]