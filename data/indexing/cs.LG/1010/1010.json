[{"id": "1010.0056", "submitter": "Cem Tekin", "authors": "Cem Tekin, Mingyan Liu", "title": "Online Learning in Opportunistic Spectrum Access: A Restless Bandit\n  Approach", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider an opportunistic spectrum access (OSA) problem where the\ntime-varying condition of each channel (e.g., as a result of random fading or\ncertain primary users' activities) is modeled as an arbitrary finite-state\nMarkov chain. At each instance of time, a (secondary) user probes a channel and\ncollects a certain reward as a function of the state of the channel (e.g., good\nchannel condition results in higher data rate for the user). Each channel has\npotentially different state space and statistics, both unknown to the user, who\ntries to learn which one is the best as it goes and maximizes its usage of the\nbest channel. The objective is to construct a good online learning algorithm so\nas to minimize the difference between the user's performance in total rewards\nand that of using the best channel (on average) had it known which one is the\nbest from a priori knowledge of the channel statistics (also known as the\nregret). This is a classic exploration and exploitation problem and results\nabound when the reward processes are assumed to be iid. Compared to prior work,\nthe biggest difference is that in our case the reward process is assumed to be\nMarkovian, of which iid is a special case. In addition, the reward processes\nare restless in that the channel conditions will continue to evolve independent\nof the user's actions. This leads to a restless bandit problem, for which there\nexists little result on either algorithms or performance bounds in this\nlearning context to the best of our knowledge. In this paper we introduce an\nalgorithm that utilizes regenerative cycles of a Markov chain and computes a\nsample-mean based index policy, and show that under mild conditions on the\nstate transition probabilities of the Markov chains this algorithm achieves\nlogarithmic regret uniformly over time, and that this regret bound is also\noptimal.\n", "versions": [{"version": "v1", "created": "Fri, 1 Oct 2010 03:23:17 GMT"}], "update_date": "2010-10-04", "authors_parsed": [["Tekin", "Cem", ""], ["Liu", "Mingyan", ""]]}, {"id": "1010.0287", "submitter": "Ying Cui", "authors": "Rui Wang, Vincent K. N. Lau and Ying Cui", "title": "Queue-Aware Distributive Resource Control for Delay-Sensitive Two-Hop\n  MIMO Cooperative Systems", "comments": "30 pages, 7 figures", "journal-ref": null, "doi": "10.1109/TSP.2010.2086449", "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we consider a queue-aware distributive resource control\nalgorithm for two-hop MIMO cooperative systems. We shall illustrate that relay\nbuffering is an effective way to reduce the intrinsic half-duplex penalty in\ncooperative systems. The complex interactions of the queues at the source node\nand the relays are modeled as an average-cost infinite horizon Markov Decision\nProcess (MDP). The traditional approach solving this MDP problem involves\ncentralized control with huge complexity. To obtain a distributive and low\ncomplexity solution, we introduce a linear structure which approximates the\nvalue function of the associated Bellman equation by the sum of per-node value\nfunctions. We derive a distributive two-stage two-winner auction-based control\npolicy which is a function of the local CSI and local QSI only. Furthermore, to\nestimate the best fit approximation parameter, we propose a distributive online\nstochastic learning algorithm using stochastic approximation theory. Finally,\nwe establish technical conditions for almost-sure convergence and show that\nunder heavy traffic, the proposed low complexity distributive control is global\noptimal.\n", "versions": [{"version": "v1", "created": "Sat, 2 Oct 2010 03:57:46 GMT"}], "update_date": "2015-05-20", "authors_parsed": [["Wang", "Rui", ""], ["Lau", "Vincent K. N.", ""], ["Cui", "Ying", ""]]}, {"id": "1010.1042", "submitter": "James Y. Zhao", "authors": "James Y. Zhao", "title": "Hidden Markov Models with Multiple Observation Processes", "comments": "Masters Thesis, 79 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR cs.IT cs.LG math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider a hidden Markov model with multiple observation processes, one of\nwhich is chosen at each point in time by a policy---a deterministic function of\nthe information state---and attempt to determine which policy minimises the\nlimiting expected entropy of the information state. Focusing on a special case,\nwe prove analytically that the information state always converges in\ndistribution, and derive a formula for the limiting entropy which can be used\nfor calculations with high precision. Using this fomula, we find\ncomputationally that the optimal policy is always a threshold policy, allowing\nit to be easily found. We also find that the greedy policy is almost optimal.\n", "versions": [{"version": "v1", "created": "Wed, 6 Oct 2010 00:36:04 GMT"}, {"version": "v2", "created": "Wed, 26 Jan 2011 00:58:25 GMT"}, {"version": "v3", "created": "Thu, 5 May 2011 08:34:07 GMT"}], "update_date": "2015-03-17", "authors_parsed": [["Zhao", "James Y.", ""]]}, {"id": "1010.1526", "submitter": "Zoltan Prekopcsak", "authors": "Zolt\\'an Prekopcs\\'ak and Daniel Lemire", "title": "Time Series Classification by Class-Specific Mahalanobis Distance\n  Measures", "comments": null, "journal-ref": "Advances in Data Analysis and Classification 6 (3), 2012", "doi": "10.1007/s11634-012-0110-6", "report-no": null, "categories": "cs.LG", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  To classify time series by nearest neighbors, we need to specify or learn one\nor several distance measures. We consider variations of the Mahalanobis\ndistance measures which rely on the inverse covariance matrix of the data.\nUnfortunately --- for time series data --- the covariance matrix has often low\nrank. To alleviate this problem we can either use a pseudoinverse, covariance\nshrinking or limit the matrix to its diagonal. We review these alternatives and\nbenchmark them against competitive methods such as the related Large Margin\nNearest Neighbor Classification (LMNN) and the Dynamic Time Warping (DTW)\ndistance. As we expected, we find that the DTW is superior, but the Mahalanobis\ndistance measures are one to two orders of magnitude faster. To get best\nresults with Mahalanobis distance measures, we recommend learning one distance\nmeasure per class using either covariance shrinking or the diagonal approach.\n", "versions": [{"version": "v1", "created": "Thu, 7 Oct 2010 19:48:23 GMT"}, {"version": "v2", "created": "Sat, 16 Apr 2011 01:25:57 GMT"}, {"version": "v3", "created": "Mon, 27 Jun 2011 09:20:28 GMT"}, {"version": "v4", "created": "Fri, 30 Dec 2011 22:21:45 GMT"}, {"version": "v5", "created": "Tue, 22 May 2012 10:02:44 GMT"}, {"version": "v6", "created": "Mon, 2 Jul 2012 20:57:01 GMT"}], "update_date": "2015-03-17", "authors_parsed": [["Prekopcs\u00e1k", "Zolt\u00e1n", ""], ["Lemire", "Daniel", ""]]}, {"id": "1010.1763", "submitter": "Cedric Fevotte", "authors": "C\\'edric F\\'evotte (LTCI), J\\'er\\^ome Idier (IRCCyN)", "title": "Algorithms for nonnegative matrix factorization with the beta-divergence", "comments": "\\`a para\\^itre dans Neural Computation", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper describes algorithms for nonnegative matrix factorization (NMF)\nwith the beta-divergence (beta-NMF). The beta-divergence is a family of cost\nfunctions parametrized by a single shape parameter beta that takes the\nEuclidean distance, the Kullback-Leibler divergence and the Itakura-Saito\ndivergence as special cases (beta = 2,1,0, respectively). The proposed\nalgorithms are based on a surrogate auxiliary function (a local majorization of\nthe criterion function). We first describe a majorization-minimization (MM)\nalgorithm that leads to multiplicative updates, which differ from standard\nheuristic multiplicative updates by a beta-dependent power exponent. The\nmonotonicity of the heuristic algorithm can however be proven for beta in (0,1)\nusing the proposed auxiliary function. Then we introduce the concept of\nmajorization-equalization (ME) algorithm which produces updates that move along\nconstant level sets of the auxiliary function and lead to larger steps than MM.\nSimulations on synthetic and real data illustrate the faster convergence of the\nME approach. The paper also describes how the proposed algorithms can be\nadapted to two common variants of NMF : penalized NMF (i.e., when a penalty\nfunction of the factors is added to the criterion function) and convex-NMF\n(when the dictionary is assumed to belong to a known subspace).\n", "versions": [{"version": "v1", "created": "Fri, 8 Oct 2010 18:53:27 GMT"}, {"version": "v2", "created": "Wed, 13 Oct 2010 17:10:38 GMT"}, {"version": "v3", "created": "Tue, 8 Mar 2011 12:56:39 GMT"}], "update_date": "2011-03-09", "authors_parsed": [["F\u00e9votte", "C\u00e9dric", "", "LTCI"], ["Idier", "J\u00e9r\u00f4me", "", "IRCCyN"]]}, {"id": "1010.1888", "submitter": "Ilknur Icke", "authors": "Ilknur Icke and Andrew Rosenberg", "title": "Multi-Objective Genetic Programming Projection Pursuit for Exploratory\n  Data Modeling", "comments": "Submitted to the New York Academy of Sciences, 5th Annual Machine\n  Learning Symposium", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For classification problems, feature extraction is a crucial process which\naims to find a suitable data representation that increases the performance of\nthe machine learning algorithm. According to the curse of dimensionality\ntheorem, the number of samples needed for a classification task increases\nexponentially as the number of dimensions (variables, features) increases. On\nthe other hand, it is costly to collect, store and process data. Moreover,\nirrelevant and redundant features might hinder classifier performance. In\nexploratory analysis settings, high dimensionality prevents the users from\nexploring the data visually. Feature extraction is a two-step process: feature\nconstruction and feature selection. Feature construction creates new features\nbased on the original features and feature selection is the process of\nselecting the best features as in filter, wrapper and embedded methods.\n  In this work, we focus on feature construction methods that aim to decrease\ndata dimensionality for visualization tasks. Various linear (such as principal\ncomponents analysis (PCA), multiple discriminants analysis (MDA), exploratory\nprojection pursuit) and non-linear (such as multidimensional scaling (MDS),\nmanifold learning, kernel PCA/LDA, evolutionary constructive induction)\ntechniques have been proposed for dimensionality reduction. Our algorithm is an\nadaptive feature extraction method which consists of evolutionary constructive\ninduction for feature construction and a hybrid filter/wrapper method for\nfeature selection.\n", "versions": [{"version": "v1", "created": "Sun, 10 Oct 2010 02:34:22 GMT"}], "update_date": "2010-10-12", "authors_parsed": [["Icke", "Ilknur", ""], ["Rosenberg", "Andrew", ""]]}, {"id": "1010.2955", "submitter": "Guangcan Liu", "authors": "Guangcan Liu, Zhouchen Lin, Shuicheng Yan, Ju Sun, Yong Yu, Yi Ma", "title": "Robust Recovery of Subspace Structures by Low-Rank Representation", "comments": "IEEE Trans. Pattern Analysis and Machine Intelligence", "journal-ref": "IEEE Trans. Pattern Analysis and Machine Intelligence, 35(2013)\n  171-184", "doi": "10.1109/TPAMI.2012.88", "report-no": null, "categories": "cs.IT cs.CV cs.LG math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work we address the subspace recovery problem. Given a set of data\nsamples (vectors) approximately drawn from a union of multiple subspaces, our\ngoal is to segment the samples into their respective subspaces and correct the\npossible errors as well. To this end, we propose a novel method termed Low-Rank\nRepresentation (LRR), which seeks the lowest-rank representation among all the\ncandidates that can represent the data samples as linear combinations of the\nbases in a given dictionary. It is shown that LRR well solves the subspace\nrecovery problem: when the data is clean, we prove that LRR exactly captures\nthe true subspace structures; for the data contaminated by outliers, we prove\nthat under certain conditions LRR can exactly recover the row space of the\noriginal data and detect the outlier as well; for the data corrupted by\narbitrary errors, LRR can also approximately recover the row space with\ntheoretical guarantees. Since the subspace membership is provably determined by\nthe row space, these further imply that LRR can perform robust subspace\nsegmentation and error correction, in an efficient way.\n", "versions": [{"version": "v1", "created": "Thu, 14 Oct 2010 15:38:48 GMT"}, {"version": "v2", "created": "Tue, 9 Nov 2010 14:07:04 GMT"}, {"version": "v3", "created": "Mon, 22 Nov 2010 09:27:15 GMT"}, {"version": "v4", "created": "Thu, 8 Sep 2011 09:02:03 GMT"}, {"version": "v5", "created": "Wed, 28 Mar 2012 05:09:27 GMT"}, {"version": "v6", "created": "Sun, 6 May 2012 08:23:16 GMT"}], "update_date": "2013-01-29", "authors_parsed": [["Liu", "Guangcan", ""], ["Lin", "Zhouchen", ""], ["Yan", "Shuicheng", ""], ["Sun", "Ju", ""], ["Yu", "Yong", ""], ["Ma", "Yi", ""]]}, {"id": "1010.3091", "submitter": "Daniel Golovin", "authors": "Daniel Golovin and Andreas Krause and Debajyoti Ray", "title": "Near-Optimal Bayesian Active Learning with Noisy Observations", "comments": "15 pages. Version 2 contains only one major change, namely an amended\n  proof of Lemma 6", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We tackle the fundamental problem of Bayesian active learning with noise,\nwhere we need to adaptively select from a number of expensive tests in order to\nidentify an unknown hypothesis sampled from a known prior distribution. In the\ncase of noise-free observations, a greedy algorithm called generalized binary\nsearch (GBS) is known to perform near-optimally. We show that if the\nobservations are noisy, perhaps surprisingly, GBS can perform very poorly. We\ndevelop EC2, a novel, greedy active learning algorithm and prove that it is\ncompetitive with the optimal policy, thus obtaining the first competitiveness\nguarantees for Bayesian active learning with noisy observations. Our bounds\nrely on a recently discovered diminishing returns property called adaptive\nsubmodularity, generalizing the classical notion of submodular set functions to\nadaptive policies. Our results hold even if the tests have non-uniform cost and\ntheir noise is correlated. We also propose EffECXtive, a particularly fast\napproximation of EC2, and evaluate it on a Bayesian experimental design problem\ninvolving human subjects, intended to tease apart competing economic theories\nof how people make decisions under uncertainty.\n", "versions": [{"version": "v1", "created": "Fri, 15 Oct 2010 08:20:46 GMT"}, {"version": "v2", "created": "Mon, 16 Dec 2013 06:42:05 GMT"}], "update_date": "2013-12-17", "authors_parsed": [["Golovin", "Daniel", ""], ["Krause", "Andreas", ""], ["Ray", "Debajyoti", ""]]}, {"id": "1010.3467", "submitter": "Koray Kavukcuoglu", "authors": "Koray Kavukcuoglu, Marc'Aurelio Ranzato and Yann LeCun", "title": "Fast Inference in Sparse Coding Algorithms with Applications to Object\n  Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": "CBLL-TR-2008-12-01", "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Adaptive sparse coding methods learn a possibly overcomplete set of basis\nfunctions, such that natural image patches can be reconstructed by linearly\ncombining a small subset of these bases. The applicability of these methods to\nvisual object recognition tasks has been limited because of the prohibitive\ncost of the optimization algorithms required to compute the sparse\nrepresentation. In this work we propose a simple and efficient algorithm to\nlearn basis functions. After training, this model also provides a fast and\nsmooth approximator to the optimal representation, achieving even better\naccuracy than exact sparse coding algorithms on visual object recognition\ntasks.\n", "versions": [{"version": "v1", "created": "Mon, 18 Oct 2010 02:31:21 GMT"}], "update_date": "2010-10-19", "authors_parsed": [["Kavukcuoglu", "Koray", ""], ["Ranzato", "Marc'Aurelio", ""], ["LeCun", "Yann", ""]]}, {"id": "1010.3484", "submitter": "Ilias Diakonikolas", "authors": "Ilias Diakonikolas and Ryan O'Donnell and Rocco A. Servedio and Yi Wu", "title": "Hardness Results for Agnostically Learning Low-Degree Polynomial\n  Threshold Functions", "comments": "full version of SODA'11 paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hardness results for maximum agreement problems have close connections to\nhardness results for proper learning in computational learning theory. In this\npaper we prove two hardness results for the problem of finding a low degree\npolynomial threshold function (PTF) which has the maximum possible agreement\nwith a given set of labeled examples in $\\R^n \\times \\{-1,1\\}.$ We prove that\nfor any constants $d\\geq 1, \\eps > 0$,\n  {itemize}\n  Assuming the Unique Games Conjecture, no polynomial-time algorithm can find a\ndegree-$d$ PTF that is consistent with a $(\\half + \\eps)$ fraction of a given\nset of labeled examples in $\\R^n \\times \\{-1,1\\}$, even if there exists a\ndegree-$d$ PTF that is consistent with a $1-\\eps$ fraction of the examples.\n  It is $\\NP$-hard to find a degree-2 PTF that is consistent with a $(\\half +\n\\eps)$ fraction of a given set of labeled examples in $\\R^n \\times \\{-1,1\\}$,\neven if there exists a halfspace (degree-1 PTF) that is consistent with a $1 -\n\\eps$ fraction of the examples.\n  {itemize}\n  These results immediately imply the following hardness of learning results:\n(i) Assuming the Unique Games Conjecture, there is no better-than-trivial\nproper learning algorithm that agnostically learns degree-$d$ PTFs under\narbitrary distributions; (ii) There is no better-than-trivial learning\nalgorithm that outputs degree-2 PTFs and agnostically learns halfspaces (i.e.\ndegree-1 PTFs) under arbitrary distributions.\n", "versions": [{"version": "v1", "created": "Mon, 18 Oct 2010 05:46:46 GMT"}], "update_date": "2010-10-19", "authors_parsed": [["Diakonikolas", "Ilias", ""], ["O'Donnell", "Ryan", ""], ["Servedio", "Rocco A.", ""], ["Wu", "Yi", ""]]}, {"id": "1010.4050", "submitter": "Guoshen Yu", "authors": "Flavien L\\'eger, Guoshen Yu, Guillermo Sapiro", "title": "Efficient Matrix Completion with Gaussian Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A general framework based on Gaussian models and a MAP-EM algorithm is\nintroduced in this paper for solving matrix/table completion problems. The\nnumerical experiments with the standard and challenging movie ratings data show\nthat the proposed approach, based on probably one of the simplest probabilistic\nmodels, leads to the results in the same ballpark as the state-of-the-art, at a\nlower computational cost.\n", "versions": [{"version": "v1", "created": "Tue, 19 Oct 2010 21:01:45 GMT"}], "update_date": "2010-10-21", "authors_parsed": [["L\u00e9ger", "Flavien", ""], ["Yu", "Guoshen", ""], ["Sapiro", "Guillermo", ""]]}, {"id": "1010.4207", "submitter": "Francis Bach", "authors": "Francis Bach (INRIA Rocquencourt, LIENS)", "title": "Convex Analysis and Optimization with Submodular Functions: a Tutorial", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Set-functions appear in many areas of computer science and applied\nmathematics, such as machine learning, computer vision, operations research or\nelectrical networks. Among these set-functions, submodular functions play an\nimportant role, similar to convex functions on vector spaces. In this tutorial,\nthe theory of submodular functions is presented, in a self-contained way, with\nall results shown from first principles. A good knowledge of convex analysis is\nassumed.\n", "versions": [{"version": "v1", "created": "Wed, 20 Oct 2010 14:02:21 GMT"}, {"version": "v2", "created": "Sun, 14 Nov 2010 17:19:42 GMT"}], "update_date": "2010-11-17", "authors_parsed": [["Bach", "Francis", "", "INRIA Rocquencourt, LIENS"]]}, {"id": "1010.4237", "submitter": "Constantine Caramanis", "authors": "Huan Xu, Constantine Caramanis and Sujay Sanghavi", "title": "Robust PCA via Outlier Pursuit", "comments": "26 pages, appeared in NIPS 2010. v2 has typos corrected, some\n  re-writing. results essentially unchanged", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.IT math.IT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Singular Value Decomposition (and Principal Component Analysis) is one of the\nmost widely used techniques for dimensionality reduction: successful and\nefficiently computable, it is nevertheless plagued by a well-known,\nwell-documented sensitivity to outliers. Recent work has considered the setting\nwhere each point has a few arbitrarily corrupted components. Yet, in\napplications of SVD or PCA such as robust collaborative filtering or\nbioinformatics, malicious agents, defective genes, or simply corrupted or\ncontaminated experiments may effectively yield entire points that are\ncompletely corrupted.\n  We present an efficient convex optimization-based algorithm we call Outlier\nPursuit, that under some mild assumptions on the uncorrupted points (satisfied,\ne.g., by the standard generative assumption in PCA problems) recovers the exact\noptimal low-dimensional subspace, and identifies the corrupted points. Such\nidentification of corrupted points that do not conform to the low-dimensional\napproximation, is of paramount interest in bioinformatics and financial\napplications, and beyond. Our techniques involve matrix decomposition using\nnuclear norm minimization, however, our results, setup, and approach,\nnecessarily differ considerably from the existing line of work in matrix\ncompletion and matrix decomposition, since we develop an approach to recover\nthe correct column space of the uncorrupted matrix, rather than the exact\nmatrix itself. In any problem where one seeks to recover a structure rather\nthan the exact initial matrices, techniques developed thus far relying on\ncertificates of optimality, will fail. We present an important extension of\nthese methods, that allows the treatment of such problems.\n", "versions": [{"version": "v1", "created": "Wed, 20 Oct 2010 16:05:28 GMT"}, {"version": "v2", "created": "Fri, 31 Dec 2010 18:36:49 GMT"}], "update_date": "2011-01-04", "authors_parsed": [["Xu", "Huan", ""], ["Caramanis", "Constantine", ""], ["Sanghavi", "Sujay", ""]]}, {"id": "1010.4253", "submitter": "Xudong Ma", "authors": "Xudong Ma", "title": "Large-Scale Clustering Based on Data Compression", "comments": null, "journal-ref": "Proceeding of the 8th International Conference on Information\n  Technology : New Generations, April 11-13, 2011, Las Vegas, Nevada, USA", "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper considers the clustering problem for large data sets. We propose\nan approach based on distributed optimization. The clustering problem is\nformulated as an optimization problem of maximizing the classification gain. We\nshow that the optimization problem can be reformulated and decomposed into\nsmall-scale sub optimization problems by using the Dantzig-Wolfe decomposition\nmethod. Generally speaking, the Dantzig-Wolfe method can only be used for\nconvex optimization problems, where the duality gaps are zero. Even though, the\nconsidered optimization problem in this paper is non-convex, we prove that the\nduality gap goes to zero, as the problem size goes to infinity. Therefore, the\nDantzig-Wolfe method can be applied here. In the proposed approach, the\nclustering problem is iteratively solved by a group of computers coordinated by\none center processor, where each computer solves one independent small-scale\nsub optimization problem during each iteration, and only a small amount of data\ncommunication is needed between the computers and center processor. Numerical\nresults show that the proposed approach is effective and efficient.\n", "versions": [{"version": "v1", "created": "Wed, 20 Oct 2010 17:21:38 GMT"}], "update_date": "2010-12-10", "authors_parsed": [["Ma", "Xudong", ""]]}, {"id": "1010.4408", "submitter": "Elad Hazan", "authors": "Kenneth L. Clarkson and Elad Hazan and David P. Woodruff", "title": "Sublinear Optimization for Machine Learning", "comments": "extended abstract appeared in FOCS 2010", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We give sublinear-time approximation algorithms for some optimization\nproblems arising in machine learning, such as training linear classifiers and\nfinding minimum enclosing balls. Our algorithms can be extended to some\nkernelized versions of these problems, such as SVDD, hard margin SVM, and\nL2-SVM, for which sublinear-time algorithms were not known before. These new\nalgorithms use a combination of a novel sampling techniques and a new\nmultiplicative update algorithm. We give lower bounds which show the running\ntimes of many of our algorithms to be nearly best possible in the unit-cost RAM\nmodel. We also give implementations of our algorithms in the semi-streaming\nsetting, obtaining the first low pass polylogarithmic space and sublinear time\nalgorithms achieving arbitrary approximation factor.\n", "versions": [{"version": "v1", "created": "Thu, 21 Oct 2010 09:57:12 GMT"}], "update_date": "2010-10-22", "authors_parsed": [["Clarkson", "Kenneth L.", ""], ["Hazan", "Elad", ""], ["Woodruff", "David P.", ""]]}, {"id": "1010.4466", "submitter": "Mordechai Nisenson", "authors": "Ran El-Yaniv and Mordechai Nisenson", "title": "On the Foundations of Adversarial Single-Class Classification", "comments": "52 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motivated by authentication, intrusion and spam detection applications we\nconsider single-class classification (SCC) as a two-person game between the\nlearner and an adversary. In this game the learner has a sample from a target\ndistribution and the goal is to construct a classifier capable of\ndistinguishing observations from the target distribution from observations\nemitted from an unknown other distribution. The ideal SCC classifier must\nguarantee a given tolerance for the false-positive error (false alarm rate)\nwhile minimizing the false negative error (intruder pass rate). Viewing SCC as\na two-person zero-sum game we identify both deterministic and randomized\noptimal classification strategies for different game variants. We demonstrate\nthat randomized classification can provide a significant advantage. In the\ndeterministic setting we show how to reduce SCC to two-class classification\nwhere in the two-class problem the other class is a synthetically generated\ndistribution. We provide an efficient and practical algorithm for constructing\nand solving the two class problem. The algorithm distinguishes low density\nregions of the target distribution and is shown to be consistent.\n", "versions": [{"version": "v1", "created": "Thu, 21 Oct 2010 13:28:09 GMT"}], "update_date": "2010-10-22", "authors_parsed": [["El-Yaniv", "Ran", ""], ["Nisenson", "Mordechai", ""]]}, {"id": "1010.4951", "submitter": "Mahmoud Khademi", "authors": "Mahmoud Khademi, Mohammad T. Manzuri-Shalmani, and Meharn safayani", "title": "Local Component Analysis for Nonparametric Bayes Classifier", "comments": "This paper has been withdrawn by the author due to an error in\n  experimental results", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The decision boundaries of Bayes classifier are optimal because they lead to\nmaximum probability of correct decision. It means if we knew the prior\nprobabilities and the class-conditional densities, we could design a classifier\nwhich gives the lowest probability of error. However, in classification based\non nonparametric density estimation methods such as Parzen windows, the\ndecision regions depend on the choice of parameters such as window width.\nMoreover, these methods suffer from curse of dimensionality of the feature\nspace and small sample size problem which severely restricts their practical\napplications. In this paper, we address these problems by introducing a novel\ndimension reduction and classification method based on local component\nanalysis. In this method, by adopting an iterative cross-validation algorithm,\nwe simultaneously estimate the optimal transformation matrices (for dimension\nreduction) and classifier parameters based on local information. The proposed\nmethod can classify the data with complicated boundary and also alleviate the\ncourse of dimensionality dilemma. Experiments on real data show the superiority\nof the proposed algorithm in term of classification accuracies for pattern\nclassification applications like age, facial expression and character\nrecognition. Keywords: Bayes classifier, curse of dimensionality dilemma,\nParzen window, pattern classification, subspace learning.\n", "versions": [{"version": "v1", "created": "Sun, 24 Oct 2010 11:28:11 GMT"}, {"version": "v2", "created": "Fri, 20 Jul 2012 01:17:25 GMT"}], "update_date": "2012-07-23", "authors_parsed": [["Khademi", "Mahmoud", ""], ["Manzuri-Shalmani", "Mohammad T.", ""], ["safayani", "Meharn", ""]]}, {"id": "1010.5290", "submitter": "Andri Mirzal", "authors": "Andri Mirzal", "title": "Converged Algorithms for Orthogonal Nonnegative Matrix Factorizations", "comments": "55 pages, 11 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NA", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  This paper proposes uni-orthogonal and bi-orthogonal nonnegative matrix\nfactorization algorithms with robust convergence proofs. We design the\nalgorithms based on the work of Lee and Seung [1], and derive the converged\nversions by utilizing ideas from the work of Lin [2]. The experimental results\nconfirm the theoretical guarantees of the convergences.\n", "versions": [{"version": "v1", "created": "Tue, 26 Oct 2010 00:28:36 GMT"}, {"version": "v2", "created": "Wed, 16 Mar 2011 05:53:38 GMT"}], "update_date": "2011-03-17", "authors_parsed": [["Mirzal", "Andri", ""]]}, {"id": "1010.5470", "submitter": "Maria Lopez-Valdes", "authors": "Ricard Gavalda, Maria Lopez-Valdes, Elvira Mayordomo, N. V.\n  Vinodchandran", "title": "Resource-bounded Dimension in Computational Learning Theory", "comments": "23 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper focuses on the relation between computational learning theory and\nresource-bounded dimension. We intend to establish close connections between\nthe learnability/nonlearnability of a concept class and its corresponding size\nin terms of effective dimension, which will allow the use of powerful dimension\ntechniques in computational learning and viceversa, the import of learning\nresults into complexity via dimension. Firstly, we obtain a tight result on the\ndimension of online mistake-bound learnable classes. Secondly, in relation with\nPAC learning, we show that the polynomial-space dimension of PAC learnable\nclasses of concepts is zero. This provides a hypothesis on effective dimension\nthat implies the inherent unpredictability of concept classes (the classes that\nverify this property are classes not efficiently PAC learnable using any\nhypothesis). Thirdly, in relation to space dimension of classes that are\nlearnable by membership query algorithms, the main result proves that\npolynomial-space dimension of concept classes learnable by a membership-query\nalgorithm is zero.\n", "versions": [{"version": "v1", "created": "Tue, 26 Oct 2010 17:48:25 GMT"}, {"version": "v2", "created": "Fri, 14 Jan 2011 11:21:46 GMT"}], "update_date": "2015-03-17", "authors_parsed": [["Gavalda", "Ricard", ""], ["Lopez-Valdes", "Maria", ""], ["Mayordomo", "Elvira", ""], ["Vinodchandran", "N. V.", ""]]}, {"id": "1010.5511", "submitter": "Peter Stobbe", "authors": "Peter Stobbe, Andreas Krause", "title": "Efficient Minimization of Decomposable Submodular Functions", "comments": "Expanded version of paper for Neural Information Processing Systems\n  2010", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many combinatorial problems arising in machine learning can be reduced to the\nproblem of minimizing a submodular function. Submodular functions are a natural\ndiscrete analog of convex functions, and can be minimized in strongly\npolynomial time. Unfortunately, state-of-the-art algorithms for general\nsubmodular minimization are intractable for larger problems. In this paper, we\nintroduce a novel subclass of submodular minimization problems that we call\ndecomposable. Decomposable submodular functions are those that can be\nrepresented as sums of concave functions applied to modular functions. We\ndevelop an algorithm, SLG, that can efficiently minimize decomposable\nsubmodular functions with tens of thousands of variables. Our algorithm\nexploits recent results in smoothed convex minimization. We apply SLG to\nsynthetic benchmarks and a joint classification-and-segmentation task, and show\nthat it outperforms the state-of-the-art general purpose submodular\nminimization algorithms by several orders of magnitude.\n", "versions": [{"version": "v1", "created": "Tue, 26 Oct 2010 20:23:39 GMT"}], "update_date": "2015-03-17", "authors_parsed": [["Stobbe", "Peter", ""], ["Krause", "Andreas", ""]]}, {"id": "1010.6234", "submitter": "Grazia Bombini", "authors": "Grazia Bombini, Raquel Ros, Stefano Ferilli, Ramon Lopez de Mantaras", "title": "Analysing the behaviour of robot teams through relational sequential\n  pattern mining", "comments": "25 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This report outlines the use of a relational representation in a Multi-Agent\ndomain to model the behaviour of the whole system. A desired property in this\nsystems is the ability of the team members to work together to achieve a common\ngoal in a cooperative manner. The aim is to define a systematic method to\nverify the effective collaboration among the members of a team and comparing\nthe different multi-agent behaviours. Using external observations of a\nMulti-Agent System to analyse, model, recognize agent behaviour could be very\nuseful to direct team actions. In particular, this report focuses on the\nchallenge of autonomous unsupervised sequential learning of the team's\nbehaviour from observations. Our approach allows to learn a symbolic sequence\n(a relational representation) to translate raw multi-agent, multi-variate\nobservations of a dynamic, complex environment, into a set of sequential\nbehaviours that are characteristic of the team in question, represented by a\nset of sequences expressed in first-order logic atoms. We propose to use a\nrelational learning algorithm to mine meaningful frequent patterns among the\nrelational sequences to characterise team behaviours. We compared the\nperformance of two teams in the RoboCup four-legged league environment, that\nhave a very different approach to the game. One uses a Case Based Reasoning\napproach, the other uses a pure reactive behaviour.\n", "versions": [{"version": "v1", "created": "Fri, 29 Oct 2010 14:50:49 GMT"}], "update_date": "2010-11-01", "authors_parsed": [["Bombini", "Grazia", ""], ["Ros", "Raquel", ""], ["Ferilli", "Stefano", ""], ["de Mantaras", "Ramon Lopez", ""]]}]