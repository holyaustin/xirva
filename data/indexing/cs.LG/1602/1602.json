[{"id": "1602.00061", "submitter": "Weihao Kong", "authors": "Weihao Kong and Gregory Valiant", "title": "Spectrum Estimation from Samples", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of approximating the set of eigenvalues of the\ncovariance matrix of a multivariate distribution (equivalently, the problem of\napproximating the \"population spectrum\"), given access to samples drawn from\nthe distribution. The eigenvalues of the covariance of a distribution contain\nbasic information about the distribution, including the presence or lack of\nstructure in the distribution, the effective dimensionality of the\ndistribution, and the applicability of higher-level machine learning and\nmultivariate statistical tools. We consider this fundamental recovery problem\nin the regime where the number of samples is comparable, or even sublinear in\nthe dimensionality of the distribution in question. First, we propose a\ntheoretically optimal and computationally efficient algorithm for recovering\nthe moments of the eigenvalues of the population covariance matrix. We then\nleverage this accurate moment recovery, via a Wasserstein distance argument, to\nshow that the vector of eigenvalues can be accurately recovered. We provide\nfinite--sample bounds on the expected error of the recovered eigenvalues, which\nimply that our estimator is asymptotically consistent as the dimensionality of\nthe distribution and sample size tend towards infinity, even in the sublinear\nsample regime where the ratio of the sample size to the dimensionality tends to\nzero. In addition to our theoretical results, we show that our approach\nperforms well in practice for a broad range of distributions and sample sizes.\n", "versions": [{"version": "v1", "created": "Sat, 30 Jan 2016 02:28:39 GMT"}, {"version": "v2", "created": "Fri, 27 May 2016 18:43:05 GMT"}, {"version": "v3", "created": "Tue, 15 Nov 2016 19:10:15 GMT"}, {"version": "v4", "created": "Wed, 21 Dec 2016 17:46:25 GMT"}, {"version": "v5", "created": "Sun, 16 Jul 2017 04:50:25 GMT"}], "update_date": "2017-07-18", "authors_parsed": [["Kong", "Weihao", ""], ["Valiant", "Gregory", ""]]}, {"id": "1602.00110", "submitter": "Stefano Cresci", "authors": "Stefano Cresci, Roberto Di Pietro, Marinella Petrocchi, Angelo\n  Spognardi, and Maurizio Tesconi", "title": "DNA-inspired online behavioral modeling and its application to spambot\n  detection", "comments": null, "journal-ref": "IEEE Intelligent Systems 31(5):58-64, 2016", "doi": "10.1109/MIS.2016.29", "report-no": null, "categories": "cs.SI cs.CR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a strikingly novel, simple, and effective approach to model online\nuser behavior: we extract and analyze digital DNA sequences from user online\nactions and we use Twitter as a benchmark to test our proposal. We obtain an\nincisive and compact DNA-inspired characterization of user actions. Then, we\napply standard DNA analysis techniques to discriminate between genuine and\nspambot accounts on Twitter. An experimental campaign supports our proposal,\nshowing its effectiveness and viability. To the best of our knowledge, we are\nthe first ones to identify and adapt DNA-inspired techniques to online user\nbehavioral modeling. While Twitter spambot detection is a specific use case on\na specific social media, our proposed methodology is platform and technology\nagnostic, hence paving the way for diverse behavioral characterization tasks.\n", "versions": [{"version": "v1", "created": "Sat, 30 Jan 2016 12:03:45 GMT"}], "update_date": "2020-06-29", "authors_parsed": [["Cresci", "Stefano", ""], ["Di Pietro", "Roberto", ""], ["Petrocchi", "Marinella", ""], ["Spognardi", "Angelo", ""], ["Tesconi", "Maurizio", ""]]}, {"id": "1602.00133", "submitter": "Zhao Shen-Yi", "authors": "Shen-Yi Zhao, Ru Xiang, Ying-Hao Shi, Peng Gao, Wu-Jun Li", "title": "SCOPE: Scalable Composite Optimization for Learning on Spark", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many machine learning models, such as logistic regression~(LR) and support\nvector machine~(SVM), can be formulated as composite optimization problems.\nRecently, many distributed stochastic optimization~(DSO) methods have been\nproposed to solve the large-scale composite optimization problems, which have\nshown better performance than traditional batch methods. However, most of these\nDSO methods are not scalable enough. In this paper, we propose a novel DSO\nmethod, called \\underline{s}calable \\underline{c}omposite\n\\underline{op}timization for l\\underline{e}arning~({SCOPE}), and implement it\non the fault-tolerant distributed platform \\mbox{Spark}. SCOPE is both\ncomputation-efficient and communication-efficient. Theoretical analysis shows\nthat SCOPE is convergent with linear convergence rate when the objective\nfunction is convex. Furthermore, empirical results on real datasets show that\nSCOPE can outperform other state-of-the-art distributed learning methods on\nSpark, including both batch learning methods and DSO methods.\n", "versions": [{"version": "v1", "created": "Sat, 30 Jan 2016 16:11:53 GMT"}, {"version": "v2", "created": "Sun, 7 Feb 2016 07:07:56 GMT"}, {"version": "v3", "created": "Wed, 1 Jun 2016 07:50:39 GMT"}, {"version": "v4", "created": "Thu, 2 Jun 2016 07:01:25 GMT"}, {"version": "v5", "created": "Sun, 11 Dec 2016 16:10:37 GMT"}], "update_date": "2016-12-13", "authors_parsed": [["Zhao", "Shen-Yi", ""], ["Xiang", "Ru", ""], ["Shi", "Ying-Hao", ""], ["Gao", "Peng", ""], ["Li", "Wu-Jun", ""]]}, {"id": "1602.00163", "submitter": "Sabeur Aridhi", "authors": "Manel Zoghlami, Sabeur Aridhi, Mondher Maddouri, Engelbert Mephu\n  Nguifo", "title": "Multiple instance learning for sequence data with across bag\n  dependencies", "comments": null, "journal-ref": null, "doi": "10.1007/s13042-019-01021-5", "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In Multiple Instance Learning (MIL) problem for sequence data, the instances\ninside the bags are sequences. In some real world applications such as\nbioinformatics, comparing a random couple of sequences makes no sense. In fact,\neach instance may have structural and/or functional relations with instances of\nother bags. Thus, the classification task should take into account this across\nbag relation. In this work, we present two novel MIL approaches for sequence\ndata classification named ABClass and ABSim. ABClass extracts motifs from\nrelated instances and use them to encode sequences. A discriminative classifier\nis then applied to compute a partial classification result for each set of\nrelated sequences. ABSim uses a similarity measure to discriminate the related\ninstances and to compute a scores matrix. For both approaches, an aggregation\nmethod is applied in order to generate the final classification result. We\napplied both approaches to solve the problem of bacterial Ionizing Radiation\nResistance prediction. The experimental results of the presented approaches are\nsatisfactory.\n", "versions": [{"version": "v1", "created": "Sat, 30 Jan 2016 21:15:10 GMT"}, {"version": "v2", "created": "Thu, 11 Jun 2020 19:32:08 GMT"}], "update_date": "2020-06-15", "authors_parsed": [["Zoghlami", "Manel", ""], ["Aridhi", "Sabeur", ""], ["Maddouri", "Mondher", ""], ["Nguifo", "Engelbert Mephu", ""]]}, {"id": "1602.00172", "submitter": "Patrick O. Glauner", "authors": "Patrick O. Glauner", "title": "Deep Learning For Smile Recognition", "comments": "Proceedings of the 12th Conference on Uncertainty Modelling in\n  Knowledge Engineering and Decision Making (FLINS 2016)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Inspired by recent successes of deep learning in computer vision, we propose\na novel application of deep convolutional neural networks to facial expression\nrecognition, in particular smile recognition. A smile recognition test accuracy\nof 99.45% is achieved for the Denver Intensity of Spontaneous Facial Action\n(DISFA) database, significantly outperforming existing approaches based on\nhand-crafted features with accuracies ranging from 65.55% to 79.67%. The\nnovelty of this approach includes a comprehensive model selection of the\narchitecture parameters, allowing to find an appropriate architecture for each\nexpression such as smile. This is feasible because all experiments were run on\na Tesla K40c GPU, allowing a speedup of factor 10 over traditional computations\non a CPU.\n", "versions": [{"version": "v1", "created": "Sat, 30 Jan 2016 23:59:04 GMT"}, {"version": "v2", "created": "Tue, 25 Jul 2017 04:46:01 GMT"}], "update_date": "2017-07-26", "authors_parsed": [["Glauner", "Patrick O.", ""]]}, {"id": "1602.00203", "submitter": "Angshul Majumdar Dr.", "authors": "Snigdha Tariyal, Angshul Majumdar, Richa Singh and Mayank Vatsa", "title": "Greedy Deep Dictionary Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work we propose a new deep learning tool called deep dictionary\nlearning. Multi-level dictionaries are learnt in a greedy fashion, one layer at\na time. This requires solving a simple (shallow) dictionary learning problem,\nthe solution to this is well known. We apply the proposed technique on some\nbenchmark deep learning datasets. We compare our results with other deep\nlearning tools like stacked autoencoder and deep belief network; and state of\nthe art supervised dictionary learning tools like discriminative KSVD and label\nconsistent KSVD. Our method yields better results than all.\n", "versions": [{"version": "v1", "created": "Sun, 31 Jan 2016 06:12:58 GMT"}], "update_date": "2016-02-02", "authors_parsed": [["Tariyal", "Snigdha", ""], ["Majumdar", "Angshul", ""], ["Singh", "Richa", ""], ["Vatsa", "Mayank", ""]]}, {"id": "1602.00206", "submitter": "Zhaoqiang Xia", "authors": "Zhaoqiang Xia, Xiaoyi Feng, Jinye Peng, Abdenour Hadid", "title": "Unsupervised Deep Hashing for Large-scale Visual Search", "comments": null, "journal-ref": "2016 6th International Conference on Image Processing Theory Tools\n  and Applications (IPTA)", "doi": "10.1109/IPTA.2016.7821007", "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning based hashing plays a pivotal role in large-scale visual search.\nHowever, most existing hashing algorithms tend to learn shallow models that do\nnot seek representative binary codes. In this paper, we propose a novel hashing\napproach based on unsupervised deep learning to hierarchically transform\nfeatures into hash codes. Within the heterogeneous deep hashing framework, the\nautoencoder layers with specific constraints are considered to model the\nnonlinear mapping between features and binary codes. Then, a Restricted\nBoltzmann Machine (RBM) layer with constraints is utilized to reduce the\ndimension in the hamming space. Extensive experiments on the problem of visual\nsearch demonstrate the competitiveness of our proposed approach compared to\nstate-of-the-art.\n", "versions": [{"version": "v1", "created": "Sun, 31 Jan 2016 06:36:47 GMT"}], "update_date": "2018-04-26", "authors_parsed": [["Xia", "Zhaoqiang", ""], ["Feng", "Xiaoyi", ""], ["Peng", "Jinye", ""], ["Hadid", "Abdenour", ""]]}, {"id": "1602.00216", "submitter": "Jean Golay", "authors": "Jean Golay, Michael Leuenberger, Mikhail Kanevski", "title": "Feature Selection for Regression Problems Based on the Morisita\n  Estimator of Intrinsic Dimension", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data acquisition, storage and management have been improved, while the key\nfactors of many phenomena are not well known. Consequently, irrelevant and\nredundant features artificially increase the size of datasets, which\ncomplicates learning tasks, such as regression. To address this problem,\nfeature selection methods have been proposed. This paper introduces a new\nsupervised filter based on the Morisita estimator of intrinsic dimension. It\ncan identify relevant features and distinguish between redundant and irrelevant\ninformation. Besides, it offers a clear graphical representation of the\nresults, and it can be easily implemented in different programming languages.\nComprehensive numerical experiments are conducted using simulated datasets\ncharacterized by different levels of complexity, sample size and noise. The\nsuggested algorithm is also successfully tested on a selection of real world\napplications and compared with RReliefF using extreme learning machine. In\naddition, a new measure of feature relevance is presented and discussed.\n", "versions": [{"version": "v1", "created": "Sun, 31 Jan 2016 09:59:27 GMT"}, {"version": "v2", "created": "Wed, 3 Feb 2016 17:03:26 GMT"}, {"version": "v3", "created": "Mon, 7 Mar 2016 20:40:06 GMT"}, {"version": "v4", "created": "Fri, 11 Mar 2016 14:39:24 GMT"}, {"version": "v5", "created": "Fri, 8 Apr 2016 18:37:17 GMT"}, {"version": "v6", "created": "Tue, 4 Apr 2017 13:28:48 GMT"}], "update_date": "2017-04-05", "authors_parsed": [["Golay", "Jean", ""], ["Leuenberger", "Michael", ""], ["Kanevski", "Mikhail", ""]]}, {"id": "1602.00223", "submitter": "Luo Luo", "authors": "Luo Luo, Zihao Chen, Zhihua Zhang, Wu-Jun Li", "title": "A Proximal Stochastic Quasi-Newton Algorithm", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we discuss the problem of minimizing the sum of two convex\nfunctions: a smooth function plus a non-smooth function. Further, the smooth\npart can be expressed by the average of a large number of smooth component\nfunctions, and the non-smooth part is equipped with a simple proximal mapping.\nWe propose a proximal stochastic second-order method, which is efficient and\nscalable. It incorporates the Hessian in the smooth part of the function and\nexploits multistage scheme to reduce the variance of the stochastic gradient.\nWe prove that our method can achieve linear rate of convergence.\n", "versions": [{"version": "v1", "created": "Sun, 31 Jan 2016 10:56:29 GMT"}, {"version": "v2", "created": "Wed, 16 Nov 2016 05:03:14 GMT"}], "update_date": "2016-11-17", "authors_parsed": [["Luo", "Luo", ""], ["Chen", "Zihao", ""], ["Zhang", "Zhihua", ""], ["Li", "Wu-Jun", ""]]}, {"id": "1602.00287", "submitter": "Kirthevasan Kandasamy", "authors": "Kirthevasan Kandasamy, Yaoliang Yu", "title": "Additive Approximations in High Dimensional Nonparametric Regression via\n  the SALSA", "comments": "International Conference on Machine Learning (ICML) 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  High dimensional nonparametric regression is an inherently difficult problem\nwith known lower bounds depending exponentially in dimension. A popular\nstrategy to alleviate this curse of dimensionality has been to use additive\nmodels of \\emph{first order}, which model the regression function as a sum of\nindependent functions on each dimension. Though useful in controlling the\nvariance of the estimate, such models are often too restrictive in practical\nsettings. Between non-additive models which often have large variance and first\norder additive models which have large bias, there has been little work to\nexploit the trade-off in the middle via additive models of intermediate order.\nIn this work, we propose SALSA, which bridges this gap by allowing interactions\nbetween variables, but controls model capacity by limiting the order of\ninteractions. SALSA minimises the residual sum of squares with squared RKHS\nnorm penalties. Algorithmically, it can be viewed as Kernel Ridge Regression\nwith an additive kernel. When the regression function is additive, the excess\nrisk is only polynomial in dimension. Using the Girard-Newton formulae, we\nefficiently sum over a combinatorial number of terms in the additive expansion.\nVia a comparison on $15$ real datasets, we show that our method is competitive\nagainst $21$ other alternatives.\n", "versions": [{"version": "v1", "created": "Sun, 31 Jan 2016 17:32:51 GMT"}, {"version": "v2", "created": "Sun, 20 Mar 2016 23:11:13 GMT"}, {"version": "v3", "created": "Tue, 24 May 2016 23:15:24 GMT"}], "update_date": "2016-05-26", "authors_parsed": [["Kandasamy", "Kirthevasan", ""], ["Yu", "Yaoliang", ""]]}, {"id": "1602.00309", "submitter": "Yonatan Glassner", "authors": "Yonatan Glassner, Koby Crammer", "title": "Bandits meet Computer Architecture: Designing a Smartly-allocated Cache", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In many embedded systems, such as imaging sys- tems, the system has a single\ndesignated purpose, and same threads are executed repeatedly. Profiling thread\nbehavior, allows the system to allocate each thread its resources in a way that\nimproves overall system performance. We study an online resource al-\nlocationproblem,wherearesourcemanagersimulta- neously allocates resources\n(exploration), learns the impact on the different consumers (learning) and im-\nproves allocation towards optimal performance (ex- ploitation). We build on the\nrich framework of multi- armed bandits and present online and offline algo-\nrithms. Through extensive experiments with both synthetic data and real-world\ncache allocation to threads we show the merits and properties of our al-\ngorithms\n", "versions": [{"version": "v1", "created": "Sun, 31 Jan 2016 19:53:49 GMT"}], "update_date": "2016-02-02", "authors_parsed": [["Glassner", "Yonatan", ""], ["Crammer", "Koby", ""]]}, {"id": "1602.00351", "submitter": "Yi Ding", "authors": "Yi Ding, Peilin Zhao, Steven C.H. Hoi, Yew-Soon Ong", "title": "Adaptive Subgradient Methods for Online AUC Maximization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning for maximizing AUC performance is an important research problem in\nMachine Learning and Artificial Intelligence. Unlike traditional batch learning\nmethods for maximizing AUC which often suffer from poor scalability, recent\nyears have witnessed some emerging studies that attempt to maximize AUC by\nsingle-pass online learning approaches. Despite their encouraging results\nreported, the existing online AUC maximization algorithms often adopt simple\nonline gradient descent approaches that fail to exploit the geometrical\nknowledge of the data observed during the online learning process, and thus\ncould suffer from relatively larger regret. To address the above limitation, in\nthis work, we explore a novel algorithm of Adaptive Online AUC Maximization\n(AdaOAM) which employs an adaptive gradient method that exploits the knowledge\nof historical gradients to perform more informative online learning. The new\nadaptive updating strategy of the AdaOAM is less sensitive to the parameter\nsettings and maintains the same time complexity as previous non-adaptive\ncounterparts. Additionally, we extend the algorithm to handle high-dimensional\nsparse data (SAdaOAM) and address sparsity in the solution by performing lazy\ngradient updating. We analyze the theoretical bounds and evaluate their\nempirical performance on various types of data sets. The encouraging empirical\nresults obtained clearly highlighted the effectiveness and efficiency of the\nproposed algorithms.\n", "versions": [{"version": "v1", "created": "Mon, 1 Feb 2016 00:25:18 GMT"}], "update_date": "2016-02-02", "authors_parsed": [["Ding", "Yi", ""], ["Zhao", "Peilin", ""], ["Hoi", "Steven C. H.", ""], ["Ong", "Yew-Soon", ""]]}, {"id": "1602.00354", "submitter": "Gautam Dasarathy", "authors": "Gautam Dasarathy, Aarti Singh, Maria-Florina Balcan, Jong Hyuk Park", "title": "Active Learning Algorithms for Graphical Model Selection", "comments": "26 pages, 3 figures. Preliminary version to appear in AI & Statistics\n  2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.IT cs.LG math.IT math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The problem of learning the structure of a high dimensional graphical model\nfrom data has received considerable attention in recent years. In many\napplications such as sensor networks and proteomics it is often expensive to\nobtain samples from all the variables involved simultaneously. For instance,\nthis might involve the synchronization of a large number of sensors or the\ntagging of a large number of proteins. To address this important issue, we\ninitiate the study of a novel graphical model selection problem, where the goal\nis to optimize the total number of scalar samples obtained by allowing the\ncollection of samples from only subsets of the variables. We propose a general\nparadigm for graphical model selection where feedback is used to guide the\nsampling to high degree vertices, while obtaining only few samples from the\nones with the low degrees. We instantiate this framework with two specific\nactive learning algorithms, one of which makes mild assumptions but is\ncomputationally expensive, while the other is more computationally efficient\nbut requires stronger (nevertheless standard) assumptions. Whereas the sample\ncomplexity of passive algorithms is typically a function of the maximum degree\nof the graph, we show that the sample complexity of our algorithms is provable\nsmaller and that it depends on a novel local complexity measure that is akin to\nthe average degree of the graph. We finally demonstrate the efficacy of our\nframework via simulations.\n", "versions": [{"version": "v1", "created": "Mon, 1 Feb 2016 01:04:34 GMT"}, {"version": "v2", "created": "Thu, 7 Apr 2016 05:49:56 GMT"}], "update_date": "2016-04-08", "authors_parsed": [["Dasarathy", "Gautam", ""], ["Singh", "Aarti", ""], ["Balcan", "Maria-Florina", ""], ["Park", "Jong Hyuk", ""]]}, {"id": "1602.00357", "submitter": "Trang Pham", "authors": "Trang Pham, Truyen Tran, Dinh Phung and Svetha Venkatesh", "title": "DeepCare: A Deep Dynamic Memory Model for Predictive Medicine", "comments": "Accepted at JBI under the new name: \"Predicting healthcare\n  trajectories from medical records: A deep learning approach\"", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Personalized predictive medicine necessitates the modeling of patient illness\nand care processes, which inherently have long-term temporal dependencies.\nHealthcare observations, recorded in electronic medical records, are episodic\nand irregular in time. We introduce DeepCare, an end-to-end deep dynamic neural\nnetwork that reads medical records, stores previous illness history, infers\ncurrent illness states and predicts future medical outcomes. At the data level,\nDeepCare represents care episodes as vectors in space, models patient health\nstate trajectories through explicit memory of historical records. Built on Long\nShort-Term Memory (LSTM), DeepCare introduces time parameterizations to handle\nirregular timed events by moderating the forgetting and consolidation of memory\ncells. DeepCare also incorporates medical interventions that change the course\nof illness and shape future medical risk. Moving up to the health state level,\nhistorical and present health states are then aggregated through multiscale\ntemporal pooling, before passing through a neural network that estimates future\noutcomes. We demonstrate the efficacy of DeepCare for disease progression\nmodeling, intervention recommendation, and future risk prediction. On two\nimportant cohorts with heavy social and economic burden -- diabetes and mental\nhealth -- the results show improved modeling and risk prediction accuracy.\n", "versions": [{"version": "v1", "created": "Mon, 1 Feb 2016 01:47:00 GMT"}, {"version": "v2", "created": "Mon, 10 Apr 2017 22:54:01 GMT"}], "update_date": "2017-04-12", "authors_parsed": [["Pham", "Trang", ""], ["Tran", "Truyen", ""], ["Phung", "Dinh", ""], ["Venkatesh", "Svetha", ""]]}, {"id": "1602.00370", "submitter": "Jian Tang", "authors": "Jian Tang, Jingzhou Liu, Ming Zhang and Qiaozhu Mei", "title": "Visualizing Large-scale and High-dimensional Data", "comments": "WWW 2016", "journal-ref": null, "doi": "10.1145/2872427.2883041", "report-no": null, "categories": "cs.LG cs.HC", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  We study the problem of visualizing large-scale and high-dimensional data in\na low-dimensional (typically 2D or 3D) space. Much success has been reported\nrecently by techniques that first compute a similarity structure of the data\npoints and then project them into a low-dimensional space with the structure\npreserved. These two steps suffer from considerable computational costs,\npreventing the state-of-the-art methods such as the t-SNE from scaling to\nlarge-scale and high-dimensional data (e.g., millions of data points and\nhundreds of dimensions). We propose the LargeVis, a technique that first\nconstructs an accurately approximated K-nearest neighbor graph from the data\nand then layouts the graph in the low-dimensional space. Comparing to t-SNE,\nLargeVis significantly reduces the computational cost of the graph construction\nstep and employs a principled probabilistic model for the visualization step,\nthe objective of which can be effectively optimized through asynchronous\nstochastic gradient descent with a linear time complexity. The whole procedure\nthus easily scales to millions of high-dimensional data points. Experimental\nresults on real-world data sets demonstrate that the LargeVis outperforms the\nstate-of-the-art methods in both efficiency and effectiveness. The\nhyper-parameters of LargeVis are also much more stable over different data\nsets.\n", "versions": [{"version": "v1", "created": "Mon, 1 Feb 2016 03:01:33 GMT"}, {"version": "v2", "created": "Tue, 5 Apr 2016 03:59:57 GMT"}], "update_date": "2016-04-06", "authors_parsed": [["Tang", "Jian", ""], ["Liu", "Jingzhou", ""], ["Zhang", "Ming", ""], ["Mei", "Qiaozhu", ""]]}, {"id": "1602.00374", "submitter": "Ahmed Alaa", "authors": "Ahmed M. Alaa, Kyeong H. Moon, William Hsu, and Mihaela van der Schaar", "title": "ConfidentCare: A Clinical Decision Support System for Personalized\n  Breast Cancer Screening", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Breast cancer screening policies attempt to achieve timely diagnosis by the\nregular screening of apparently healthy women. Various clinical decisions are\nneeded to manage the screening process; those include: selecting the screening\ntests for a woman to take, interpreting the test outcomes, and deciding whether\nor not a woman should be referred to a diagnostic test. Such decisions are\ncurrently guided by clinical practice guidelines (CPGs), which represent a\none-size-fits-all approach that are designed to work well on average for a\npopulation, without guaranteeing that it will work well uniformly over that\npopulation. Since the risks and benefits of screening are functions of each\npatients features, personalized screening policies that are tailored to the\nfeatures of individuals are needed in order to ensure that the right tests are\nrecommended to the right woman. In order to address this issue, we present\nConfidentCare: a computer-aided clinical decision support system that learns a\npersonalized screening policy from the electronic health record (EHR) data.\nConfidentCare operates by recognizing clusters of similar patients, and\nlearning the best screening policy to adopt for each cluster. A cluster of\npatients is a set of patients with similar features (e.g. age, breast density,\nfamily history, etc.), and the screening policy is a set of guidelines on what\nactions to recommend for a woman given her features and screening test scores.\nConfidentCare algorithm ensures that the policy adopted for every cluster of\npatients satisfies a predefined accuracy requirement with a high level of\nconfidence. We show that our algorithm outperforms the current CPGs in terms of\ncost-efficiency and false positive rates.\n", "versions": [{"version": "v1", "created": "Mon, 1 Feb 2016 03:21:46 GMT"}], "update_date": "2016-02-02", "authors_parsed": [["Alaa", "Ahmed M.", ""], ["Moon", "Kyeong H.", ""], ["Hsu", "William", ""], ["van der Schaar", "Mihaela", ""]]}, {"id": "1602.00426", "submitter": "Cheng-Tao Chung", "authors": "Cheng-Tao Chung, Cheng-Yu Tsai, Hsiang-Hung Lu, Chia-Hsiang Liu,\n  Hung-yi Lee and Lin-shan Lee", "title": "An Iterative Deep Learning Framework for Unsupervised Discovery of\n  Speech Features and Linguistic Units with Applications on Spoken Term\n  Detection", "comments": "arXiv admin note: text overlap with arXiv:1506.02327", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work we aim to discover high quality speech features and linguistic\nunits directly from unlabeled speech data in a zero resource scenario. The\nresults are evaluated using the metrics and corpora proposed in the Zero\nResource Speech Challenge organized at Interspeech 2015. A Multi-layered\nAcoustic Tokenizer (MAT) was proposed for automatic discovery of multiple sets\nof acoustic tokens from the given corpus. Each acoustic token set is specified\nby a set of hyperparameters that describe the model configuration. These sets\nof acoustic tokens carry different characteristics fof the given corpus and the\nlanguage behind, thus can be mutually reinforced. The multiple sets of token\nlabels are then used as the targets of a Multi-target Deep Neural Network\n(MDNN) trained on low-level acoustic features. Bottleneck features extracted\nfrom the MDNN are then used as the feedback input to the MAT and the MDNN\nitself in the next iteration. We call this iterative deep learning framework\nthe Multi-layered Acoustic Tokenizing Deep Neural Network (MAT-DNN), which\ngenerates both high quality speech features for the Track 1 of the Challenge\nand acoustic tokens for the Track 2 of the Challenge. In addition, we performed\nextra experiments on the same corpora on the application of query-by-example\nspoken term detection. The experimental results showed the iterative deep\nlearning framework of MAT-DNN improved the detection performance due to better\nunderlying speech features and acoustic tokens.\n", "versions": [{"version": "v1", "created": "Mon, 1 Feb 2016 08:37:56 GMT"}], "update_date": "2016-02-02", "authors_parsed": [["Chung", "Cheng-Tao", ""], ["Tsai", "Cheng-Yu", ""], ["Lu", "Hsiang-Hung", ""], ["Liu", "Chia-Hsiang", ""], ["Lee", "Hung-yi", ""], ["Lee", "Lin-shan", ""]]}, {"id": "1602.00489", "submitter": "Amit Dvir Dr.", "authors": "Ran Dubin, Amit Dvir, Ofir Pele, Ofer Hadar, Itay Richman and Ofir\n  Trabelsi", "title": "Real Time Video Quality Representation Classification of Encrypted HTTP\n  Adaptive Video Streaming - the Case of Safari", "comments": "9 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM cs.CR cs.LG cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The increasing popularity of HTTP adaptive video streaming services has\ndramatically increased bandwidth requirements on operator networks, which\nattempt to shape their traffic through Deep Packet Inspection (DPI). However,\nGoogle and certain content providers have started to encrypt their video\nservices. As a result, operators often encounter difficulties in shaping their\nencrypted video traffic via DPI. This highlights the need for new traffic\nclassification methods for encrypted HTTP adaptive video streaming to enable\nsmart traffic shaping. These new methods will have to effectively estimate the\nquality representation layer and playout buffer. We present a new method and\nshow for the first time that video quality representation classification for\n(YouTube) encrypted HTTP adaptive streaming is possible. We analyze the\nperformance of this classification method with Safari over HTTPS. Based on a\nlarge number of offline and online traffic classification experiments, we\ndemonstrate that it can independently classify, in real time, every video\nsegment into one of the quality representation layers with 97.18% average\naccuracy.\n", "versions": [{"version": "v1", "created": "Mon, 1 Feb 2016 12:12:17 GMT"}, {"version": "v2", "created": "Fri, 19 Feb 2016 15:23:02 GMT"}], "update_date": "2016-02-22", "authors_parsed": [["Dubin", "Ran", ""], ["Dvir", "Amit", ""], ["Pele", "Ofir", ""], ["Hadar", "Ofer", ""], ["Richman", "Itay", ""], ["Trabelsi", "Ofir", ""]]}, {"id": "1602.00490", "submitter": "Amit Dvir Dr.", "authors": "Ran Dubin, Amit Dvir, Ofir Pele, Ofer Hadar", "title": "I Know What You Saw Last Minute - Encrypted HTTP Adaptive Video\n  Streaming Title Classification", "comments": "9 pages. arXiv admin note: text overlap with arXiv:1602.00489", "journal-ref": "IEEE TRANSACTIONS ON INFORMATION FORENSICS AND SECURITY, VOL. 12,\n  NO. 12, DECEMBER 2017", "doi": "10.1109/TIFS.2017.2730819", "report-no": null, "categories": "cs.MM cs.LG cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Desktops and laptops can be maliciously exploited to violate privacy. There\nare two main types of attack scenarios: active and passive. In this paper, we\nconsider the passive scenario where the adversary does not interact actively\nwith the device, but he is able to eavesdrop on the network traffic of the\ndevice from the network side. Most of the Internet traffic is encrypted and\nthus passive attacks are challenging. Previous research has shown that\ninformation can be extracted from encrypted multimedia streams. This includes\nvideo title classification of non HTTP adaptive streams (non-HAS). This paper\npresents an algorithm for encrypted HTTP adaptive video streaming title\nclassification. We show that an external attacker can identify the video title\nfrom video HTTP adaptive streams (HAS) sites such as YouTube. To the best of\nour knowledge, this is the first work that shows this. We provide a large data\nset of 10000 YouTube video streams of 100 popular video titles (each title\ndownloaded 100 times) as examples for this task. The dataset was collected\nunder real-world network conditions. We present several machine algorithms for\nthe task and run a through set of experiments, which shows that our\nclassification accuracy is more than 95%. We also show that our algorithms are\nable to classify video titles that are not in the training set as unknown and\nsome of the algorithms are also able to eliminate false prediction of video\ntitles and instead report unknown. Finally, we evaluate our algorithms\nrobustness to delays and packet losses at test time and show that a solution\nthat uses SVM is the most robust against these changes given enough training\ndata. We provide the dataset and the crawler for future research.\n", "versions": [{"version": "v1", "created": "Mon, 1 Feb 2016 12:15:27 GMT"}, {"version": "v2", "created": "Thu, 21 Jul 2016 06:42:01 GMT"}], "update_date": "2017-12-04", "authors_parsed": [["Dubin", "Ran", ""], ["Dvir", "Amit", ""], ["Pele", "Ofir", ""], ["Hadar", "Ofer", ""]]}, {"id": "1602.00554", "submitter": "Bj\\\"orn Weghenkel", "authors": "Bj\\\"orn Weghenkel and Asja Fischer and Laurenz Wiskott", "title": "Graph-based Predictable Feature Analysis", "comments": null, "journal-ref": null, "doi": "10.1007/s10994-017-5632-x", "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose graph-based predictable feature analysis (GPFA), a new method for\nunsupervised learning of predictable features from high-dimensional time\nseries, where high predictability is understood very generically as low\nvariance in the distribution of the next data point given the previous ones. We\nshow how this measure of predictability can be understood in terms of graph\nembedding as well as how it relates to the information-theoretic measure of\npredictive information in special cases. We confirm the effectiveness of GPFA\non different datasets, comparing it to three existing algorithms with similar\nobjectives---namely slow feature analysis, forecastable component analysis, and\npredictable feature analysis---to which GPFA shows very competitive results.\n", "versions": [{"version": "v1", "created": "Mon, 1 Feb 2016 15:11:48 GMT"}, {"version": "v2", "created": "Thu, 11 May 2017 12:41:25 GMT"}], "update_date": "2017-05-12", "authors_parsed": [["Weghenkel", "Bj\u00f6rn", ""], ["Fischer", "Asja", ""], ["Wiskott", "Laurenz", ""]]}, {"id": "1602.00575", "submitter": "Qunwei Li", "authors": "Qunwei Li, Aditya Vempaty, Lav R. Varshney, and Pramod K. Varshney", "title": "Multi-object Classification via Crowdsourcing with a Reject Option", "comments": "two column, 15 pages, 8 figures, submitted to IEEE Trans. Signal\n  Process", "journal-ref": null, "doi": "10.1109/TSP.2016.2630038", "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Consider designing an effective crowdsourcing system for an $M$-ary\nclassification task. Crowd workers complete simple binary microtasks whose\nresults are aggregated to give the final result. We consider the novel scenario\nwhere workers have a reject option so they may skip microtasks when they are\nunable or choose not to respond. For example, in mismatched speech\ntranscription, workers who do not know the language may not be able to respond\nto microtasks focused on phonological dimensions outside their categorical\nperception. We present an aggregation approach using a weighted majority voting\nrule, where each worker's response is assigned an optimized weight to maximize\nthe crowd's classification performance. We evaluate system performance in both\nexact and asymptotic forms. Further, we consider the setting where there may be\na set of greedy workers that complete microtasks even when they are unable to\nperform it reliably. We consider an oblivious and an expurgation strategy to\ndeal with greedy workers, developing an algorithm to adaptively switch between\nthe two based on the estimated fraction of greedy workers in the anonymous\ncrowd. Simulation results show improved performance compared with conventional\nmajority voting.\n", "versions": [{"version": "v1", "created": "Mon, 1 Feb 2016 16:00:47 GMT"}, {"version": "v2", "created": "Fri, 10 Jun 2016 21:02:10 GMT"}], "update_date": "2016-12-14", "authors_parsed": [["Li", "Qunwei", ""], ["Vempaty", "Aditya", ""], ["Varshney", "Lav R.", ""], ["Varshney", "Pramod K.", ""]]}, {"id": "1602.00734", "submitter": "Yen-Huan  Li", "authors": "Yen-Huan Li and Volkan Cevher", "title": "Learning Data Triage: Linear Decoding Works for Compressive MRI", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.LG math.IT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The standard approach to compressive sampling considers recovering an unknown\ndeterministic signal with certain known structure, and designing the\nsub-sampling pattern and recovery algorithm based on the known structure. This\napproach requires looking for a good representation that reveals the signal\nstructure, and solving a non-smooth convex minimization problem (e.g., basis\npursuit). In this paper, another approach is considered: We learn a good\nsub-sampling pattern based on available training signals, without knowing the\nsignal structure in advance, and reconstruct an accordingly sub-sampled signal\nby computationally much cheaper linear reconstruction. We provide a theoretical\nguarantee on the recovery error, and show via experiments on real-world MRI\ndata the effectiveness of the proposed compressive MRI scheme.\n", "versions": [{"version": "v1", "created": "Mon, 1 Feb 2016 22:43:55 GMT"}], "update_date": "2016-02-03", "authors_parsed": [["Li", "Yen-Huan", ""], ["Cevher", "Volkan", ""]]}, {"id": "1602.00991", "submitter": "Peter Ondruska", "authors": "Peter Ondruska and Ingmar Posner", "title": "Deep Tracking: Seeing Beyond Seeing Using Recurrent Neural Networks", "comments": "Published in The Thirtieth AAAI Conference on Artificial Intelligence\n  (AAAI-16), Video: https://youtu.be/cdeWCpfUGWc, Code:\n  http://mrg.robots.ox.ac.uk/mrg_people/peter-ondruska/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV cs.NE cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents to the best of our knowledge the first end-to-end object\ntracking approach which directly maps from raw sensor input to object tracks in\nsensor space without requiring any feature engineering or system identification\nin the form of plant or sensor models. Specifically, our system accepts a\nstream of raw sensor data at one end and, in real-time, produces an estimate of\nthe entire environment state at the output including even occluded objects. We\nachieve this by framing the problem as a deep learning task and exploit\nsequence models in the form of recurrent neural networks to learn a mapping\nfrom sensor measurements to object tracks. In particular, we propose a learning\nmethod based on a form of input dropout which allows learning in an\nunsupervised manner, only based on raw, occluded sensor data without access to\nground-truth annotations. We demonstrate our approach using a synthetic dataset\ndesigned to mimic the task of tracking objects in 2D laser data -- as commonly\nencountered in robotics applications -- and show that it learns to track many\ndynamic objects despite occlusions and the presence of sensor noise.\n", "versions": [{"version": "v1", "created": "Tue, 2 Feb 2016 16:10:16 GMT"}, {"version": "v2", "created": "Tue, 8 Mar 2016 22:09:05 GMT"}], "update_date": "2016-03-10", "authors_parsed": [["Ondruska", "Peter", ""], ["Posner", "Ingmar", ""]]}, {"id": "1602.01024", "submitter": "Weiran Wang", "authors": "Weiran Wang, Raman Arora, Karen Livescu, Jeff Bilmes", "title": "On Deep Multi-View Representation Learning: Objectives and Optimization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider learning representations (features) in the setting in which we\nhave access to multiple unlabeled views of the data for learning while only one\nview is available for downstream tasks. Previous work on this problem has\nproposed several techniques based on deep neural networks, typically involving\neither autoencoder-like networks with a reconstruction objective or paired\nfeedforward networks with a batch-style correlation-based objective. We analyze\nseveral techniques based on prior work, as well as new variants, and compare\nthem empirically on image, speech, and text tasks. We find an advantage for\ncorrelation-based representation learning, while the best results on most tasks\nare obtained with our new variant, deep canonically correlated autoencoders\n(DCCAE). We also explore a stochastic optimization procedure for minibatch\ncorrelation-based objectives and discuss the time/performance trade-offs for\nkernel-based and neural network-based implementations.\n", "versions": [{"version": "v1", "created": "Tue, 2 Feb 2016 17:51:43 GMT"}], "update_date": "2016-02-03", "authors_parsed": [["Wang", "Weiran", ""], ["Arora", "Raman", ""], ["Livescu", "Karen", ""], ["Bilmes", "Jeff", ""]]}, {"id": "1602.01042", "submitter": "Daniel Cullina", "authors": "Daniel Cullina, Negar Kiyavash", "title": "Improved Achievability and Converse Bounds for Erd\\H{o}s-R\\'enyi Graph\n  Matching", "comments": "10 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.LG math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of perfectly recovering the vertex correspondence\nbetween two correlated Erd\\H{o}s-R\\'enyi (ER) graphs. For a pair of correlated\ngraphs on the same vertex set, the correspondence between the vertices can be\nobscured by randomly permuting the vertex labels of one of the graphs. In some\ncases, the structural information in the graphs allow this correspondence to be\nrecovered. We investigate the information-theoretic threshold for exact\nrecovery, i.e. the conditions under which the entire vertex correspondence can\nbe correctly recovered given unbounded computational resources.\n  Pedarsani and Grossglauser provided an achievability result of this type.\nTheir result establishes the scaling dependence of the threshold on the number\nof vertices. We improve on their achievability bound. We also provide a\nconverse bound, establishing conditions under which exact recovery is\nimpossible. Together, these establish the scaling dependence of the threshold\non the level of correlation between the two graphs. The converse and\nachievability bounds differ by a factor of two for sparse, significantly\ncorrelated graphs.\n", "versions": [{"version": "v1", "created": "Tue, 2 Feb 2016 18:54:04 GMT"}], "update_date": "2016-02-03", "authors_parsed": [["Cullina", "Daniel", ""], ["Kiyavash", "Negar", ""]]}, {"id": "1602.01052", "submitter": "Eric Schulz", "authors": "Eric Schulz, Quentin J. M. Huys, Dominik R. Bach, Maarten\n  Speekenbrink, Andreas Krause", "title": "Better safe than sorry: Risky function exploitation through safe\n  optimization", "comments": "6 pages, submitted to Cognitive Science Conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Exploration-exploitation of functions, that is learning and optimizing a\nmapping between inputs and expected outputs, is ubiquitous to many real world\nsituations. These situations sometimes require us to avoid certain outcomes at\nall cost, for example because they are poisonous, harmful, or otherwise\ndangerous. We test participants' behavior in scenarios in which they have to\nfind the optimum of a function while at the same time avoid outputs below a\ncertain threshold. In two experiments, we find that Safe-Optimization, a\nGaussian Process-based exploration-exploitation algorithm, describes\nparticipants' behavior well and that participants seem to care firstly whether\na point is safe and then try to pick the optimal point from all such safe\npoints. This means that their trade-off between exploration and exploitation\ncan be seen as an intelligent, approximate, and homeostasis-driven strategy.\n", "versions": [{"version": "v1", "created": "Tue, 2 Feb 2016 19:27:54 GMT"}, {"version": "v2", "created": "Sat, 14 May 2016 15:42:05 GMT"}], "update_date": "2016-05-17", "authors_parsed": [["Schulz", "Eric", ""], ["Huys", "Quentin J. M.", ""], ["Bach", "Dominik R.", ""], ["Speekenbrink", "Maarten", ""], ["Krause", "Andreas", ""]]}, {"id": "1602.01064", "submitter": "Jan Hendrik Metzen", "authors": "Jan Hendrik Metzen", "title": "Minimum Regret Search for Single- and Multi-Task Optimization", "comments": "Final version for ICML 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.IT cs.LG cs.RO math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose minimum regret search (MRS), a novel acquisition function for\nBayesian optimization. MRS bears similarities with information-theoretic\napproaches such as entropy search (ES). However, while ES aims in each query at\nmaximizing the information gain with respect to the global maximum, MRS aims at\nminimizing the expected simple regret of its ultimate recommendation for the\noptimum. While empirically ES and MRS perform similar in most of the cases, MRS\nproduces fewer outliers with high simple regret than ES. We provide empirical\nresults both for a synthetic single-task optimization problem as well as for a\nsimulated multi-task robotic control problem.\n", "versions": [{"version": "v1", "created": "Tue, 2 Feb 2016 19:58:11 GMT"}, {"version": "v2", "created": "Tue, 9 Feb 2016 18:58:45 GMT"}, {"version": "v3", "created": "Tue, 24 May 2016 06:57:30 GMT"}], "update_date": "2016-05-25", "authors_parsed": [["Metzen", "Jan Hendrik", ""]]}, {"id": "1602.01132", "submitter": "Sivan Sabato", "authors": "Sivan Sabato and Tom Hess", "title": "Interactive algorithms: from pool to stream", "comments": "Appearing in COLT 2016", "journal-ref": "S. Sabato and T. Hess, \"Interactive Algorithms: from Pool to\n  Stream\", Proceedings of the 29th Annual Conference on Learning Theory (COLT),\n  JMLR Workshop and Conference Proceedings 49:1419-1439, 2016", "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider interactive algorithms in the pool-based setting, and in the\nstream-based setting. Interactive algorithms observe suggested elements\n(representing actions or queries), and interactively select some of them and\nreceive responses. Pool-based algorithms can select elements at any order,\nwhile stream-based algorithms observe elements in sequence, and can only select\nelements immediately after observing them. We assume that the suggested\nelements are generated independently from some source distribution, and ask\nwhat is the stream size required for emulating a pool algorithm with a given\npool size. We provide algorithms and matching lower bounds for general pool\nalgorithms, and for utility-based pool algorithms. We further show that a\nmaximal gap between the two settings exists also in the special case of active\nlearning for binary classification.\n", "versions": [{"version": "v1", "created": "Tue, 2 Feb 2016 22:06:02 GMT"}, {"version": "v2", "created": "Sun, 14 Feb 2016 09:27:44 GMT"}, {"version": "v3", "created": "Thu, 16 Jun 2016 13:40:06 GMT"}], "update_date": "2018-11-21", "authors_parsed": [["Sabato", "Sivan", ""], ["Hess", "Tom", ""]]}, {"id": "1602.01164", "submitter": "Conrado Miranda", "authors": "Conrado S. Miranda and Fernando J. Von Zuben", "title": "Single-Solution Hypervolume Maximization and its use for Improving\n  Generalization of Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces the hypervolume maximization with a single solution as\nan alternative to the mean loss minimization. The relationship between the two\nproblems is proved through bounds on the cost function when an optimal solution\nto one of the problems is evaluated on the other, with a hyperparameter to\ncontrol the similarity between the two problems. This same hyperparameter\nallows higher weight to be placed on samples with higher loss when computing\nthe hypervolume's gradient, whose normalized version can range from the mean\nloss to the max loss. An experiment on MNIST with a neural network is used to\nvalidate the theory developed, showing that the hypervolume maximization can\nbehave similarly to the mean loss minimization and can also provide better\nperformance, resulting on a 20% reduction of the classification error on the\ntest set.\n", "versions": [{"version": "v1", "created": "Wed, 3 Feb 2016 01:32:01 GMT"}], "update_date": "2016-02-04", "authors_parsed": [["Miranda", "Conrado S.", ""], ["Von Zuben", "Fernando J.", ""]]}, {"id": "1602.01168", "submitter": "Zhuolin Jiang", "authors": "Zhuolin Jiang, Yaming Wang, Larry Davis, Walt Andrews, Viktor Rozgic", "title": "Learning Discriminative Features via Label Consistent Neural Network", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.MM cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep Convolutional Neural Networks (CNN) enforces supervised information only\nat the output layer, and hidden layers are trained by back propagating the\nprediction error from the output layer without explicit supervision. We propose\na supervised feature learning approach, Label Consistent Neural Network, which\nenforces direct supervision in late hidden layers. We associate each neuron in\na hidden layer with a particular class label and encourage it to be activated\nfor input signals from the same class. More specifically, we introduce a label\nconsistency regularization called \"discriminative representation error\" loss\nfor late hidden layers and combine it with classification error loss to build\nour overall objective function. This label consistency constraint alleviates\nthe common problem of gradient vanishing and tends to faster convergence; it\nalso makes the features derived from late hidden layers discriminative enough\nfor classification even using a simple $k$-NN classifier, since input signals\nfrom the same class will have very similar representations. Experimental\nresults demonstrate that our approach achieves state-of-the-art performances on\nseveral public benchmarks for action and object category recognition.\n", "versions": [{"version": "v1", "created": "Wed, 3 Feb 2016 02:41:33 GMT"}, {"version": "v2", "created": "Sun, 5 Jun 2016 02:45:35 GMT"}], "update_date": "2016-06-07", "authors_parsed": [["Jiang", "Zhuolin", ""], ["Wang", "Yaming", ""], ["Davis", "Larry", ""], ["Andrews", "Walt", ""], ["Rozgic", "Viktor", ""]]}, {"id": "1602.01198", "submitter": "Richard Nock", "authors": "Richard Nock, Rapha\\\"el Canyasse, Roksana Boreli and Frank Nielsen", "title": "k-variates++: more pluses in the k-means++", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  k-means++ seeding has become a de facto standard for hard clustering\nalgorithms. In this paper, our first contribution is a two-way generalisation\nof this seeding, k-variates++, that includes the sampling of general densities\nrather than just a discrete set of Dirac densities anchored at the point\nlocations, and a generalisation of the well known Arthur-Vassilvitskii (AV)\napproximation guarantee, in the form of a bias+variance approximation bound of\nthe global optimum. This approximation exhibits a reduced dependency on the\n\"noise\" component with respect to the optimal potential --- actually\napproaching the statistical lower bound. We show that k-variates++ reduces to\nefficient (biased seeding) clustering algorithms tailored to specific\nframeworks; these include distributed, streaming and on-line clustering, with\ndirect approximation results for these algorithms. Finally, we present a novel\napplication of k-variates++ to differential privacy. For either the specific\nframeworks considered here, or for the differential privacy setting, there is\nlittle to no prior results on the direct application of k-means++ and its\napproximation bounds --- state of the art contenders appear to be significantly\nmore complex and / or display less favorable (approximation) properties. We\nstress that our algorithms can still be run in cases where there is \\textit{no}\nclosed form solution for the population minimizer. We demonstrate the\napplicability of our analysis via experimental evaluation on several domains\nand settings, displaying competitive performances vs state of the art.\n", "versions": [{"version": "v1", "created": "Wed, 3 Feb 2016 06:31:09 GMT"}, {"version": "v2", "created": "Sat, 13 Feb 2016 00:36:41 GMT"}], "update_date": "2016-02-16", "authors_parsed": [["Nock", "Richard", ""], ["Canyasse", "Rapha\u00ebl", ""], ["Boreli", "Roksana", ""], ["Nielsen", "Frank", ""]]}, {"id": "1602.01323", "submitter": "Joey McCollum", "authors": "Joey McCollum and Stephen Brown", "title": "Biclustering Readings and Manuscripts via Non-negative Matrix\n  Factorization, with Application to the Text of Jude", "comments": "31 pages, 2 figures, 42 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The text-critical practice of grouping witnesses into families or texttypes\noften faces two obstacles: Contamination in the manuscript tradition, and\nco-dependence in identifying characteristic readings and manuscripts. We\nintroduce non-negative matrix factorization (NMF) as a simple, unsupervised,\nand efficient way to cluster large numbers of manuscripts and readings\nsimultaneously while summarizing contamination using an easy-to-interpret\nmixture model. We apply this method to an extensive collation of the New\nTestament epistle of Jude and show that the resulting clusters correspond to\nhuman-identified textual families from existing research.\n", "versions": [{"version": "v1", "created": "Wed, 3 Feb 2016 14:54:05 GMT"}], "update_date": "2016-02-04", "authors_parsed": [["McCollum", "Joey", ""], ["Brown", "Stephen", ""]]}, {"id": "1602.01407", "submitter": "Roger Grosse", "authors": "Roger Grosse and James Martens", "title": "A Kronecker-factored approximate Fisher matrix for convolution layers", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Second-order optimization methods such as natural gradient descent have the\npotential to speed up training of neural networks by correcting for the\ncurvature of the loss function. Unfortunately, the exact natural gradient is\nimpractical to compute for large models, and most approximations either require\nan expensive iterative procedure or make crude approximations to the curvature.\nWe present Kronecker Factors for Convolution (KFC), a tractable approximation\nto the Fisher matrix for convolutional networks based on a structured\nprobabilistic model for the distribution over backpropagated derivatives.\nSimilarly to the recently proposed Kronecker-Factored Approximate Curvature\n(K-FAC), each block of the approximate Fisher matrix decomposes as the\nKronecker product of small matrices, allowing for efficient inversion. KFC\ncaptures important curvature information while still yielding comparably\nefficient updates to stochastic gradient descent (SGD). We show that the\nupdates are invariant to commonly used reparameterizations, such as centering\nof the activations. In our experiments, approximate natural gradient descent\nwith KFC was able to train convolutional networks several times faster than\ncarefully tuned SGD. Furthermore, it was able to train the networks in 10-20\ntimes fewer iterations than SGD, suggesting its potential applicability in a\ndistributed setting.\n", "versions": [{"version": "v1", "created": "Wed, 3 Feb 2016 18:45:07 GMT"}, {"version": "v2", "created": "Mon, 23 May 2016 22:44:56 GMT"}], "update_date": "2016-05-25", "authors_parsed": [["Grosse", "Roger", ""], ["Martens", "James", ""]]}, {"id": "1602.01557", "submitter": "Miguel \\'A. Carreira-Perpi\\~n\\'an", "authors": "Miguel \\'A. Carreira-Perpi\\~n\\'an and Ramin Raziperchikolaei", "title": "An ensemble diversity approach to supervised binary hashing", "comments": "17 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Binary hashing is a well-known approach for fast approximate nearest-neighbor\nsearch in information retrieval. Much work has focused on affinity-based\nobjective functions involving the hash functions or binary codes. These\nobjective functions encode neighborhood information between data points and are\noften inspired by manifold learning algorithms. They ensure that the hash\nfunctions differ from each other through constraints or penalty terms that\nencourage codes to be orthogonal or dissimilar across bits, but this couples\nthe binary variables and complicates the already difficult optimization. We\npropose a much simpler approach: we train each hash function (or bit)\nindependently from each other, but introduce diversity among them using\ntechniques from classifier ensembles. Surprisingly, we find that not only is\nthis faster and trivially parallelizable, but it also improves over the more\ncomplex, coupled objective function, and achieves state-of-the-art precision\nand recall in experiments with image retrieval.\n", "versions": [{"version": "v1", "created": "Thu, 4 Feb 2016 04:59:54 GMT"}], "update_date": "2016-02-05", "authors_parsed": [["Carreira-Perpi\u00f1\u00e1n", "Miguel \u00c1.", ""], ["Raziperchikolaei", "Ramin", ""]]}, {"id": "1602.01580", "submitter": "Shai Shalev-Shwartz", "authors": "Shai Shalev-Shwartz and Nir Ben-Zrihem and Aviad Cohen and Amnon\n  Shashua", "title": "Long-term Planning by Short-term Prediction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider planning problems, that often arise in autonomous driving\napplications, in which an agent should decide on immediate actions so as to\noptimize a long term objective. For example, when a car tries to merge in a\nroundabout it should decide on an immediate acceleration/braking command, while\nthe long term effect of the command is the success/failure of the merge. Such\nproblems are characterized by continuous state and action spaces, and by\ninteraction with multiple agents, whose behavior can be adversarial. We argue\nthat dual versions of the MDP framework (that depend on the value function and\nthe $Q$ function) are problematic for autonomous driving applications due to\nthe non Markovian of the natural state space representation, and due to the\ncontinuous state and action spaces. We propose to tackle the planning task by\ndecomposing the problem into two phases: First, we apply supervised learning\nfor predicting the near future based on the present. We require that the\npredictor will be differentiable with respect to the representation of the\npresent. Second, we model a full trajectory of the agent using a recurrent\nneural network, where unexplained factors are modeled as (additive) input\nnodes. This allows us to solve the long-term planning problem using supervised\nlearning techniques and direct optimization over the recurrent neural network.\nOur approach enables us to learn robust policies by incorporating adversarial\nelements to the environment.\n", "versions": [{"version": "v1", "created": "Thu, 4 Feb 2016 08:06:59 GMT"}], "update_date": "2016-02-05", "authors_parsed": [["Shalev-Shwartz", "Shai", ""], ["Ben-Zrihem", "Nir", ""], ["Cohen", "Aviad", ""], ["Shashua", "Amnon", ""]]}, {"id": "1602.01582", "submitter": "Shai Shalev-Shwartz", "authors": "Shai Shalev-Shwartz", "title": "SDCA without Duality, Regularization, and Individual Convexity", "comments": "ICML 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Stochastic Dual Coordinate Ascent is a popular method for solving regularized\nloss minimization for the case of convex losses. We describe variants of SDCA\nthat do not require explicit regularization and do not rely on duality. We\nprove linear convergence rates even if individual loss functions are\nnon-convex, as long as the expected loss is strongly convex.\n", "versions": [{"version": "v1", "created": "Thu, 4 Feb 2016 08:14:06 GMT"}, {"version": "v2", "created": "Sat, 21 May 2016 12:33:05 GMT"}], "update_date": "2016-05-24", "authors_parsed": [["Shalev-Shwartz", "Shai", ""]]}, {"id": "1602.01690", "submitter": "Shai Shalev-Shwartz", "authors": "Shai Shalev-Shwartz and Yonatan Wexler", "title": "Minimizing the Maximal Loss: How and Why?", "comments": "ICML 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A commonly used learning rule is to approximately minimize the \\emph{average}\nloss over the training set. Other learning algorithms, such as AdaBoost and\nhard-SVM, aim at minimizing the \\emph{maximal} loss over the training set. The\naverage loss is more popular, particularly in deep learning, due to three main\nreasons. First, it can be conveniently minimized using online algorithms, that\nprocess few examples at each iteration. Second, it is often argued that there\nis no sense to minimize the loss on the training set too much, as it will not\nbe reflected in the generalization loss. Last, the maximal loss is not robust\nto outliers. In this paper we describe and analyze an algorithm that can\nconvert any online algorithm to a minimizer of the maximal loss. We prove that\nin some situations better accuracy on the training set is crucial to obtain\ngood performance on unseen examples. Last, we propose robust versions of the\napproach that can handle outliers.\n", "versions": [{"version": "v1", "created": "Thu, 4 Feb 2016 14:32:23 GMT"}, {"version": "v2", "created": "Sun, 22 May 2016 15:13:56 GMT"}], "update_date": "2016-05-24", "authors_parsed": [["Shalev-Shwartz", "Shai", ""], ["Wexler", "Yonatan", ""]]}, {"id": "1602.01711", "submitter": "Anthony Bagnall Dr", "authors": "Anthony Bagnall, Aaron Bostrom, James Large and Jason Lines", "title": "The Great Time Series Classification Bake Off: An Experimental\n  Evaluation of Recently Proposed Algorithms. Extended Version", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the last five years there have been a large number of new time series\nclassification algorithms proposed in the literature. These algorithms have\nbeen evaluated on subsets of the 47 data sets in the University of California,\nRiverside time series classification archive. The archive has recently been\nexpanded to 85 data sets, over half of which have been donated by researchers\nat the University of East Anglia. Aspects of previous evaluations have made\ncomparisons between algorithms difficult. For example, several different\nprogramming languages have been used, experiments involved a single train/test\nsplit and some used normalised data whilst others did not. The relaunch of the\narchive provides a timely opportunity to thoroughly evaluate algorithms on a\nlarger number of datasets. We have implemented 18 recently proposed algorithms\nin a common Java framework and compared them against two standard benchmark\nclassifiers (and each other) by performing 100 resampling experiments on each\nof the 85 datasets. We use these results to test several hypotheses relating to\nwhether the algorithms are significantly more accurate than the benchmarks and\neach other. Our results indicate that only 9 of these algorithms are\nsignificantly more accurate than both benchmarks and that one classifier, the\nCollective of Transformation Ensembles, is significantly more accurate than all\nof the others. All of our experiments and results are reproducible: we release\nall of our code, results and experimental details and we hope these experiments\nform the basis for more rigorous testing of new algorithms in the future.\n", "versions": [{"version": "v1", "created": "Thu, 4 Feb 2016 15:24:22 GMT"}], "update_date": "2016-02-05", "authors_parsed": [["Bagnall", "Anthony", ""], ["Bostrom", "Aaron", ""], ["Large", "James", ""], ["Lines", "Jason", ""]]}, {"id": "1602.01783", "submitter": "Volodymyr Mnih", "authors": "Volodymyr Mnih, Adri\\`a Puigdom\\`enech Badia, Mehdi Mirza, Alex\n  Graves, Timothy P. Lillicrap, Tim Harley, David Silver, Koray Kavukcuoglu", "title": "Asynchronous Methods for Deep Reinforcement Learning", "comments": null, "journal-ref": "ICML 2016", "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a conceptually simple and lightweight framework for deep\nreinforcement learning that uses asynchronous gradient descent for optimization\nof deep neural network controllers. We present asynchronous variants of four\nstandard reinforcement learning algorithms and show that parallel\nactor-learners have a stabilizing effect on training allowing all four methods\nto successfully train neural network controllers. The best performing method,\nan asynchronous variant of actor-critic, surpasses the current state-of-the-art\non the Atari domain while training for half the time on a single multi-core CPU\ninstead of a GPU. Furthermore, we show that asynchronous actor-critic succeeds\non a wide variety of continuous motor control problems as well as on a new task\nof navigating random 3D mazes using a visual input.\n", "versions": [{"version": "v1", "created": "Thu, 4 Feb 2016 18:38:41 GMT"}, {"version": "v2", "created": "Thu, 16 Jun 2016 16:38:45 GMT"}], "update_date": "2016-06-17", "authors_parsed": [["Mnih", "Volodymyr", ""], ["Badia", "Adri\u00e0 Puigdom\u00e8nech", ""], ["Mirza", "Mehdi", ""], ["Graves", "Alex", ""], ["Lillicrap", "Timothy P.", ""], ["Harley", "Tim", ""], ["Silver", "David", ""], ["Kavukcuoglu", "Koray", ""]]}, {"id": "1602.01818", "submitter": "Alexander Wong", "authors": "A. G. Chung, M. J. Shafiee, and A. Wong", "title": "Random Feature Maps via a Layered Random Projection (LaRP) Framework for\n  Object Classification", "comments": "5 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The approximation of nonlinear kernels via linear feature maps has recently\ngained interest due to their applications in reducing the training and testing\ntime of kernel-based learning algorithms. Current random projection methods\navoid the curse of dimensionality by embedding the nonlinear feature space into\na low dimensional Euclidean space to create nonlinear kernels. We introduce a\nLayered Random Projection (LaRP) framework, where we model the linear kernels\nand nonlinearity separately for increased training efficiency. The proposed\nLaRP framework was assessed using the MNIST hand-written digits database and\nthe COIL-100 object database, and showed notable improvement in object\nclassification performance relative to other state-of-the-art random projection\nmethods.\n", "versions": [{"version": "v1", "created": "Thu, 4 Feb 2016 20:31:44 GMT"}], "update_date": "2016-02-05", "authors_parsed": [["Chung", "A. G.", ""], ["Shafiee", "M. J.", ""], ["Wong", "A.", ""]]}, {"id": "1602.01895", "submitter": "Shijian Tang", "authors": "Shijian Tang, Song Han", "title": "Generate Image Descriptions based on Deep RNN and Memory Cells for\n  Images Features", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generating natural language descriptions for images is a challenging task.\nThe traditional way is to use the convolutional neural network (CNN) to extract\nimage features, followed by recurrent neural network (RNN) to generate\nsentences. In this paper, we present a new model that added memory cells to\ngate the feeding of image features to the deep neural network. The intuition is\nenabling our model to memorize how much information from images should be fed\nat each stage of the RNN. Experiments on Flickr8K and Flickr30K datasets showed\nthat our model outperforms other state-of-the-art models with higher BLEU\nscores.\n", "versions": [{"version": "v1", "created": "Fri, 5 Feb 2016 00:17:18 GMT"}], "update_date": "2016-02-08", "authors_parsed": [["Tang", "Shijian", ""], ["Han", "Song", ""]]}, {"id": "1602.01910", "submitter": "Yangyang Hou", "authors": "Yangyang Hou, Joyce Jiyoung Whang, David F. Gleich, Inderjit S.\n  Dhillon", "title": "Fast Multiplier Methods to Optimize Non-exhaustive, Overlapping\n  Clustering", "comments": "9 pages. 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Clustering is one of the most fundamental and important tasks in data mining.\nTraditional clustering algorithms, such as K-means, assign every data point to\nexactly one cluster. However, in real-world datasets, the clusters may overlap\nwith each other. Furthermore, often, there are outliers that should not belong\nto any cluster. We recently proposed the NEO-K-Means (Non-Exhaustive,\nOverlapping K-Means) objective as a way to address both issues in an integrated\nfashion. Optimizing this discrete objective is NP-hard, and even though there\nis a convex relaxation of the objective, straightforward convex optimization\napproaches are too expensive for large datasets. A practical alternative is to\nuse a low-rank factorization of the solution matrix in the convex formulation.\nThe resulting optimization problem is non-convex, and we can locally optimize\nthe objective function using an augmented Lagrangian method. In this paper, we\nconsider two fast multiplier methods to accelerate the convergence of an\naugmented Lagrangian scheme: a proximal method of multipliers and an\nalternating direction method of multipliers (ADMM). For the proximal augmented\nLagrangian or proximal method of multipliers, we show a convergence result for\nthe non-convex case with bound-constrained subproblems. These methods are up to\n13 times faster---with no change in quality---compared with a standard\naugmented Lagrangian method on problems with over 10,000 variables and bring\nruntimes down from over an hour to around 5 minutes.\n", "versions": [{"version": "v1", "created": "Fri, 5 Feb 2016 02:08:57 GMT"}], "update_date": "2016-02-08", "authors_parsed": [["Hou", "Yangyang", ""], ["Whang", "Joyce Jiyoung", ""], ["Gleich", "David F.", ""], ["Dhillon", "Inderjit S.", ""]]}, {"id": "1602.01921", "submitter": "Haanvid Lee", "authors": "Haanvid Lee, Minju Jung, and Jun Tani", "title": "Recognition of Visually Perceived Compositional Human Actions by\n  Multiple Spatio-Temporal Scales Recurrent Neural Networks", "comments": "10 pages, 9 figures, 5 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The current paper proposes a novel neural network model for recognizing\nvisually perceived human actions. The proposed multiple spatio-temporal scales\nrecurrent neural network (MSTRNN) model is derived by introducing multiple\ntimescale recurrent dynamics to the conventional convolutional neural network\nmodel. One of the essential characteristics of the MSTRNN is that its\narchitecture imposes both spatial and temporal constraints simultaneously on\nthe neural activity which vary in multiple scales among different layers. As\nsuggested by the principle of the upward and downward causation, it is assumed\nthat the network can develop meaningful structures such as functional hierarchy\nby taking advantage of such constraints during the course of learning. To\nevaluate the characteristics of the model, the current study uses three types\nof human action video dataset consisting of different types of primitive\nactions and different levels of compositionality on them. The performance of\nthe MSTRNN in testing with these dataset is compared with the ones by other\nrepresentative deep learning models used in the field. The analysis of the\ninternal representation obtained through the learning with the dataset\nclarifies what sorts of functional hierarchy can be developed by extracting the\nessential compositionality underlying the dataset.\n", "versions": [{"version": "v1", "created": "Fri, 5 Feb 2016 04:00:16 GMT"}, {"version": "v2", "created": "Wed, 5 Oct 2016 07:59:03 GMT"}, {"version": "v3", "created": "Wed, 22 Feb 2017 16:33:49 GMT"}], "update_date": "2017-02-23", "authors_parsed": [["Lee", "Haanvid", ""], ["Jung", "Minju", ""], ["Tani", "Jun", ""]]}, {"id": "1602.02018", "submitter": "Nicolas Tremblay", "authors": "Nicolas Tremblay, Gilles Puy, Remi Gribonval, Pierre Vandergheynst", "title": "Compressive Spectral Clustering", "comments": "12 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Spectral clustering has become a popular technique due to its high\nperformance in many contexts. It comprises three main steps: create a\nsimilarity graph between N objects to cluster, compute the first k eigenvectors\nof its Laplacian matrix to define a feature vector for each object, and run\nk-means on these features to separate objects into k classes. Each of these\nthree steps becomes computationally intensive for large N and/or k. We propose\nto speed up the last two steps based on recent results in the emerging field of\ngraph signal processing: graph filtering of random signals, and random sampling\nof bandlimited graph signals. We prove that our method, with a gain in\ncomputation time that can reach several orders of magnitude, is in fact an\napproximation of spectral clustering, for which we are able to control the\nerror. We test the performance of our method on artificial and real-world\nnetwork data.\n", "versions": [{"version": "v1", "created": "Fri, 5 Feb 2016 13:42:27 GMT"}, {"version": "v2", "created": "Mon, 23 May 2016 13:21:56 GMT"}], "update_date": "2016-05-24", "authors_parsed": [["Tremblay", "Nicolas", ""], ["Puy", "Gilles", ""], ["Gribonval", "Remi", ""], ["Vandergheynst", "Pierre", ""]]}, {"id": "1602.02068", "submitter": "Ram\\'on Fernandez Astudillo", "authors": "Andr\\'e F. T. Martins and Ram\\'on Fernandez Astudillo", "title": "From Softmax to Sparsemax: A Sparse Model of Attention and Multi-Label\n  Classification", "comments": "Minor corrections", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose sparsemax, a new activation function similar to the traditional\nsoftmax, but able to output sparse probabilities. After deriving its\nproperties, we show how its Jacobian can be efficiently computed, enabling its\nuse in a network trained with backpropagation. Then, we propose a new smooth\nand convex loss function which is the sparsemax analogue of the logistic loss.\nWe reveal an unexpected connection between this new loss and the Huber\nclassification loss. We obtain promising empirical results in multi-label\nclassification problems and in attention-based neural networks for natural\nlanguage inference. For the latter, we achieve a similar performance as the\ntraditional softmax, but with a selective, more compact, attention focus.\n", "versions": [{"version": "v1", "created": "Fri, 5 Feb 2016 15:49:02 GMT"}, {"version": "v2", "created": "Mon, 8 Feb 2016 09:41:36 GMT"}], "update_date": "2016-02-09", "authors_parsed": [["Martins", "Andr\u00e9 F. T.", ""], ["Astudillo", "Ram\u00f3n Fernandez", ""]]}, {"id": "1602.02070", "submitter": "Nauman Shahid", "authors": "Nauman Shahid, Nathanael Perraudin, Gilles Puy, Pierre Vandergheynst", "title": "Compressive PCA for Low-Rank Matrices on Graphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a novel framework for an approxi- mate recovery of data matrices\nwhich are low-rank on graphs, from sampled measurements. The rows and columns\nof such matrices belong to the span of the first few eigenvectors of the graphs\nconstructed between their rows and columns. We leverage this property to\nrecover the non-linear low-rank structures efficiently from sampled data\nmeasurements, with a low cost (linear in n). First, a Resrtricted Isometry\nProperty (RIP) condition is introduced for efficient uniform sampling of the\nrows and columns of such matrices based on the cumulative coherence of graph\neigenvectors. Secondly, a state-of-the-art fast low-rank recovery method is\nsuggested for the sampled data. Finally, several efficient, parallel and\nparameter-free decoders are presented along with their theoretical analysis for\ndecoding the low-rank and cluster indicators for the full data matrix. Thus, we\novercome the computational limitations of the standard linear low-rank recovery\nmethods for big datasets. Our method can also be seen as a major step towards\nefficient recovery of non- linear low-rank structures. For a matrix of size n X\np, on a single core machine, our method gains a speed up of $p^2/k$ over Robust\nPrincipal Component Analysis (RPCA), where k << p is the subspace dimension.\nNumerically, we can recover a low-rank matrix of size 10304 X 1000, 100 times\nfaster than Robust PCA.\n", "versions": [{"version": "v1", "created": "Fri, 5 Feb 2016 15:51:34 GMT"}, {"version": "v2", "created": "Mon, 11 Apr 2016 10:51:25 GMT"}, {"version": "v3", "created": "Mon, 2 May 2016 13:49:40 GMT"}, {"version": "v4", "created": "Tue, 4 Oct 2016 08:35:35 GMT"}], "update_date": "2016-10-05", "authors_parsed": [["Shahid", "Nauman", ""], ["Perraudin", "Nathanael", ""], ["Puy", "Gilles", ""], ["Vandergheynst", "Pierre", ""]]}, {"id": "1602.02101", "submitter": "Haipeng Luo", "authors": "Elad Hazan and Haipeng Luo", "title": "Variance-Reduced and Projection-Free Stochastic Optimization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Frank-Wolfe optimization algorithm has recently regained popularity for\nmachine learning applications due to its projection-free property and its\nability to handle structured constraints. However, in the stochastic learning\nsetting, it is still relatively understudied compared to the gradient descent\ncounterpart. In this work, leveraging a recent variance reduction technique, we\npropose two stochastic Frank-Wolfe variants which substantially improve\nprevious results in terms of the number of stochastic gradient evaluations\nneeded to achieve $1-\\epsilon$ accuracy. For example, we improve from\n$O(\\frac{1}{\\epsilon})$ to $O(\\ln\\frac{1}{\\epsilon})$ if the objective function\nis smooth and strongly convex, and from $O(\\frac{1}{\\epsilon^2})$ to\n$O(\\frac{1}{\\epsilon^{1.5}})$ if the objective function is smooth and\nLipschitz. The theoretical improvement is also observed in experiments on\nreal-world datasets for a multiclass classification application.\n", "versions": [{"version": "v1", "created": "Fri, 5 Feb 2016 17:14:59 GMT"}, {"version": "v2", "created": "Thu, 14 Sep 2017 00:03:37 GMT"}], "update_date": "2017-09-15", "authors_parsed": [["Hazan", "Elad", ""], ["Luo", "Haipeng", ""]]}, {"id": "1602.02123", "submitter": "Myriam Abramson", "authors": "Myriam Abramson", "title": "Sequence Classification with Neural Conditional Random Fields", "comments": "14th International Conference on Machine Learning and Applications\n  (ICMLA) 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The proliferation of sensor devices monitoring human activity generates\nvoluminous amount of temporal sequences needing to be interpreted and\ncategorized. Moreover, complex behavior detection requires the personalization\nof multi-sensor fusion algorithms. Conditional random fields (CRFs) are\ncommonly used in structured prediction tasks such as part-of-speech tagging in\nnatural language processing. Conditional probabilities guide the choice of each\ntag/label in the sequence conflating the structured prediction task with the\nsequence classification task where different models provide different\ncategorization of the same sequence. The claim of this paper is that CRF models\nalso provide discriminative models to distinguish between types of sequence\nregardless of the accuracy of the labels obtained if we calibrate the class\nmembership estimate of the sequence. We introduce and compare different neural\nnetwork based linear-chain CRFs and we present experiments on two complex\nsequence classification and structured prediction tasks to support this claim.\n", "versions": [{"version": "v1", "created": "Fri, 5 Feb 2016 19:19:46 GMT"}], "update_date": "2016-02-08", "authors_parsed": [["Abramson", "Myriam", ""]]}, {"id": "1602.02136", "submitter": "Jialei Wang", "authors": "Jialei Wang, Hai Wang, Nathan Srebro", "title": "Reducing Runtime by Recycling Samples", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Contrary to the situation with stochastic gradient descent, we argue that\nwhen using stochastic methods with variance reduction, such as SDCA, SAG or\nSVRG, as well as their variants, it could be beneficial to reuse previously\nused samples instead of fresh samples, even when fresh samples are available.\nWe demonstrate this empirically for SDCA, SAG and SVRG, studying the optimal\nsample size one should use, and also uncover be-havior that suggests running\nSDCA for an integer number of epochs could be wasteful.\n", "versions": [{"version": "v1", "created": "Fri, 5 Feb 2016 19:46:23 GMT"}], "update_date": "2016-02-08", "authors_parsed": [["Wang", "Jialei", ""], ["Wang", "Hai", ""], ["Srebro", "Nathan", ""]]}, {"id": "1602.02151", "submitter": "Zeyuan Allen-Zhu", "authors": "Zeyuan Allen-Zhu, Yang Yuan, Karthik Sridharan", "title": "Exploiting the Structure: Stochastic Gradient Methods Using Raw Clusters", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The amount of data available in the world is growing faster than our ability\nto deal with it. However, if we take advantage of the internal\n\\emph{structure}, data may become much smaller for machine learning purposes.\nIn this paper we focus on one of the fundamental machine learning tasks,\nempirical risk minimization (ERM), and provide faster algorithms with the help\nfrom the clustering structure of the data.\n  We introduce a simple notion of raw clustering that can be efficiently\ncomputed from the data, and propose two algorithms based on clustering\ninformation. Our accelerated algorithm ClusterACDM is built on a novel Haar\ntransformation applied to the dual space of the ERM problem, and our\nvariance-reduction based algorithm ClusterSVRG introduces a new gradient\nestimator using clustering. Our algorithms outperform their classical\ncounterparts ACDM and SVRG respectively.\n", "versions": [{"version": "v1", "created": "Fri, 5 Feb 2016 20:58:18 GMT"}, {"version": "v2", "created": "Fri, 4 Nov 2016 17:10:04 GMT"}], "update_date": "2016-11-07", "authors_parsed": [["Allen-Zhu", "Zeyuan", ""], ["Yuan", "Yang", ""], ["Sridharan", "Karthik", ""]]}, {"id": "1602.02159", "submitter": "Yehia Elkhatib PhD", "authors": "Faiza Samreen, Yehia Elkhatib, Matthew Rowe, Gordon S. Blair", "title": "Daleel: Simplifying Cloud Instance Selection Using Machine Learning", "comments": "In the IEEE/IFIP Network Operations and Management Symposium (NOMS),\n  April 2016", "journal-ref": null, "doi": "10.1109/NOMS.2016.7502858", "report-no": null, "categories": "cs.DC cs.LG cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Decision making in cloud environments is quite challenging due to the\ndiversity in service offerings and pricing models, especially considering that\nthe cloud market is an incredibly fast moving one. In addition, there are no\nhard and fast rules, each customer has a specific set of constraints (e.g.\nbudget) and application requirements (e.g. minimum computational resources).\nMachine learning can help address some of the complicated decisions by carrying\nout customer-specific analytics to determine the most suitable instance type(s)\nand the most opportune time for starting or migrating instances. We employ\nmachine learning techniques to develop an adaptive deployment policy, providing\nan optimal match between the customer demands and the available cloud service\nofferings. We provide an experimental study based on extensive set of job\nexecutions over a major public cloud infrastructure.\n", "versions": [{"version": "v1", "created": "Fri, 5 Feb 2016 21:00:02 GMT"}], "update_date": "2016-11-17", "authors_parsed": [["Samreen", "Faiza", ""], ["Elkhatib", "Yehia", ""], ["Rowe", "Matthew", ""], ["Blair", "Gordon S.", ""]]}, {"id": "1602.02164", "submitter": "David Gamarnik", "authors": "David Gamarnik and Sidhant Misra", "title": "A Note on Alternating Minimization Algorithm for the Matrix Completion\n  Problem", "comments": "8 pages, 2 figures", "journal-ref": null, "doi": "10.1109/LSP.2016.2576979", "report-no": null, "categories": "stat.ML cs.LG cs.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of reconstructing a low rank matrix from a subset of\nits entries and analyze two variants of the so-called Alternating Minimization\nalgorithm, which has been proposed in the past. We establish that when the\nunderlying matrix has rank $r=1$, has positive bounded entries, and the graph\n$\\mathcal{G}$ underlying the revealed entries has bounded degree and diameter\nwhich is at most logarithmic in the size of the matrix, both algorithms succeed\nin reconstructing the matrix approximately in polynomial time starting from an\narbitrary initialization. We further provide simulation results which suggest\nthat the second algorithm which is based on the message passing type updates,\nperforms significantly better.\n", "versions": [{"version": "v1", "created": "Fri, 5 Feb 2016 21:07:16 GMT"}], "update_date": "2016-09-21", "authors_parsed": [["Gamarnik", "David", ""], ["Misra", "Sidhant", ""]]}, {"id": "1602.02172", "submitter": "Weiran Wang", "authors": "Weiran Wang", "title": "On Column Selection in Approximate Kernel Canonical Correlation Analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of column selection in large-scale kernel canonical\ncorrelation analysis (KCCA) using the Nystr\\\"om approximation, where one\napproximates two positive semi-definite kernel matrices using \"landmark\" points\nfrom the training set. When building low-rank kernel approximations in KCCA,\nprevious work mostly samples the landmarks uniformly at random from the\ntraining set. We propose novel strategies for sampling the landmarks\nnon-uniformly based on a version of statistical leverage scores recently\ndeveloped for kernel ridge regression. We study the approximation accuracy of\nthe proposed non-uniform sampling strategy, develop an incremental algorithm\nthat explores the path of approximation ranks and facilitates efficient model\nselection, and derive the kernel stability of out-of-sample mapping for our\nmethod. Experimental results on both synthetic and real-world datasets\ndemonstrate the promise of our method.\n", "versions": [{"version": "v1", "created": "Fri, 5 Feb 2016 21:51:41 GMT"}], "update_date": "2016-02-09", "authors_parsed": [["Wang", "Weiran", ""]]}, {"id": "1602.02181", "submitter": "Paul Mineiro", "authors": "He He, Paul Mineiro, Nikos Karampatziakis", "title": "Active Information Acquisition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a general framework for sequential and dynamic acquisition of\nuseful information in order to solve a particular task. While our goal could in\nprinciple be tackled by general reinforcement learning, our particular setting\nis constrained enough to allow more efficient algorithms. In this paper, we\nwork under the Learning to Search framework and show how to formulate the goal\nof finding a dynamic information acquisition policy in that framework. We apply\nour formulation on two tasks, sentiment analysis and image recognition, and\nshow that the learned policies exhibit good statistical performance. As an\nemergent byproduct, the learned policies show a tendency to focus on the most\nprominent parts of each instance and give harder instances more attention\nwithout explicitly being trained to do so.\n", "versions": [{"version": "v1", "created": "Fri, 5 Feb 2016 22:32:50 GMT"}], "update_date": "2016-02-09", "authors_parsed": [["He", "He", ""], ["Mineiro", "Paul", ""], ["Karampatziakis", "Nikos", ""]]}, {"id": "1602.02191", "submitter": "Mohammad Gheshlaghi Azar", "authors": "Mohammad Gheshlaghi Azar, Eva Dyer, Konrad Kording", "title": "Convex Relaxation Regression: Black-Box Optimization of Smooth Functions\n  by Learning Their Convex Envelopes", "comments": null, "journal-ref": "Proc. of the Conference on Uncertainty in Artificial Intelligence,\n  pg. 22-31, 2016", "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Finding efficient and provable methods to solve non-convex optimization\nproblems is an outstanding challenge in machine learning and optimization\ntheory. A popular approach used to tackle non-convex problems is to use convex\nrelaxation techniques to find a convex surrogate for the problem.\nUnfortunately, convex relaxations typically must be found on a\nproblem-by-problem basis. Thus, providing a general-purpose strategy to\nestimate a convex relaxation would have a wide reaching impact. Here, we\nintroduce Convex Relaxation Regression (CoRR), an approach for learning convex\nrelaxations for a class of smooth functions. The main idea behind our approach\nis to estimate the convex envelope of a function $f$ by evaluating $f$ at a set\nof $T$ random points and then fitting a convex function to these function\nevaluations. We prove that with probability greater than $1-\\delta$, the\nsolution of our algorithm converges to the global optimizer of $f$ with error\n$\\mathcal{O} \\Big( \\big(\\frac{\\log(1/\\delta) }{T} \\big)^{\\alpha} \\Big)$ for\nsome $\\alpha> 0$. Our approach enables the use of convex optimization tools to\nsolve a class of non-convex optimization problems.\n", "versions": [{"version": "v1", "created": "Fri, 5 Feb 2016 23:23:32 GMT"}, {"version": "v2", "created": "Tue, 16 Feb 2016 01:36:29 GMT"}, {"version": "v3", "created": "Thu, 3 Mar 2016 20:09:26 GMT"}], "update_date": "2016-10-31", "authors_parsed": [["Azar", "Mohammad Gheshlaghi", ""], ["Dyer", "Eva", ""], ["Kording", "Konrad", ""]]}, {"id": "1602.02196", "submitter": "Alexander Rakhlin", "authors": "Alexander Rakhlin and Karthik Sridharan", "title": "BISTRO: An Efficient Relaxation-Based Method for Contextual Bandits", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present efficient algorithms for the problem of contextual bandits with\ni.i.d. covariates, an arbitrary sequence of rewards, and an arbitrary class of\npolicies. Our algorithm BISTRO requires d calls to the empirical risk\nminimization (ERM) oracle per round, where d is the number of actions. The\nmethod uses unlabeled data to make the problem computationally simple. When the\nERM problem itself is computationally hard, we extend the approach by employing\nmultiplicative approximation algorithms for the ERM. The integrality gap of the\nrelaxation only enters in the regret bound rather than the benchmark. Finally,\nwe show that the adversarial version of the contextual bandit problem is\nlearnable (and efficient) whenever the full-information supervised online\nlearning problem has a non-trivial regret guarantee (and efficient).\n", "versions": [{"version": "v1", "created": "Sat, 6 Feb 2016 00:34:59 GMT"}], "update_date": "2016-02-09", "authors_parsed": [["Rakhlin", "Alexander", ""], ["Sridharan", "Karthik", ""]]}, {"id": "1602.02202", "submitter": "Haipeng Luo", "authors": "Haipeng Luo, Alekh Agarwal, Nicolo Cesa-Bianchi, John Langford", "title": "Efficient Second Order Online Learning by Sketching", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose Sketched Online Newton (SON), an online second order learning\nalgorithm that enjoys substantially improved regret guarantees for\nill-conditioned data. SON is an enhanced version of the Online Newton Step,\nwhich, via sketching techniques enjoys a running time linear in the dimension\nand sketch size. We further develop sparse forms of the sketching methods (such\nas Oja's rule), making the computation linear in the sparsity of features.\nTogether, the algorithm eliminates all computational obstacles in previous\nsecond order online learning approaches.\n", "versions": [{"version": "v1", "created": "Sat, 6 Feb 2016 02:33:53 GMT"}, {"version": "v2", "created": "Sat, 21 May 2016 15:10:31 GMT"}, {"version": "v3", "created": "Tue, 4 Oct 2016 15:22:07 GMT"}, {"version": "v4", "created": "Tue, 17 Oct 2017 05:01:59 GMT"}], "update_date": "2017-10-18", "authors_parsed": [["Luo", "Haipeng", ""], ["Agarwal", "Alekh", ""], ["Cesa-Bianchi", "Nicolo", ""], ["Langford", "John", ""]]}, {"id": "1602.02210", "submitter": "Aaditya Ramdas", "authors": "Ilmun Kim, Aaditya Ramdas, Aarti Singh, Larry Wasserman", "title": "Classification accuracy as a proxy for two sample testing", "comments": "71 pages, 4 figures. Accepted for publication at the Annals of\n  Statistics (2020)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When data analysts train a classifier and check if its accuracy is\nsignificantly different from chance, they are implicitly performing a\ntwo-sample test. We investigate the statistical properties of this flexible\napproach in the high-dimensional setting. We prove two results that hold for\nall classifiers in any dimensions: if its true error remains $\\epsilon$-better\nthan chance for some $\\epsilon>0$ as $d,n \\to \\infty$, then (a) the\npermutation-based test is consistent (has power approaching to one), (b) a\ncomputationally efficient test based on a Gaussian approximation of the null\ndistribution is also consistent. To get a finer understanding of the rates of\nconsistency, we study a specialized setting of distinguishing Gaussians with\nmean-difference $\\delta$ and common (known or unknown) covariance $\\Sigma$,\nwhen $d/n \\to c \\in (0,\\infty)$. We study variants of Fisher's linear\ndiscriminant analysis (LDA) such as \"naive Bayes\" in a nontrivial regime when\n$\\epsilon \\to 0$ (the Bayes classifier has true accuracy approaching 1/2), and\ncontrast their power with corresponding variants of Hotelling's test.\nSurprisingly, the expressions for their power match exactly in terms of\n$n,d,\\delta,\\Sigma$, and the LDA approach is only worse by a constant factor,\nachieving an asymptotic relative efficiency (ARE) of $1/\\sqrt{\\pi}$ for\nbalanced samples. We also extend our results to high-dimensional elliptical\ndistributions with finite kurtosis. Other results of independent interest\ninclude minimax lower bounds, and the optimality of Hotelling's test when\n$d=o(n)$. Simulation results validate our theory, and we present practical\ntakeaway messages along with natural open problems.\n", "versions": [{"version": "v1", "created": "Sat, 6 Feb 2016 03:48:04 GMT"}, {"version": "v2", "created": "Fri, 24 May 2019 22:36:47 GMT"}, {"version": "v3", "created": "Fri, 17 Jan 2020 21:29:08 GMT"}, {"version": "v4", "created": "Mon, 17 Feb 2020 17:56:24 GMT"}], "update_date": "2020-02-18", "authors_parsed": [["Kim", "Ilmun", ""], ["Ramdas", "Aaditya", ""], ["Singh", "Aarti", ""], ["Wasserman", "Larry", ""]]}, {"id": "1602.02218", "submitter": "David Balduzzi", "authors": "David Balduzzi, Muhammad Ghifary", "title": "Strongly-Typed Recurrent Neural Networks", "comments": "10 pages, final version, ICML 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recurrent neural networks are increasing popular models for sequential\nlearning. Unfortunately, although the most effective RNN architectures are\nperhaps excessively complicated, extensive searches have not found simpler\nalternatives. This paper imports ideas from physics and functional programming\ninto RNN design to provide guiding principles. From physics, we introduce type\nconstraints, analogous to the constraints that forbids adding meters to\nseconds. From functional programming, we require that strongly-typed\narchitectures factorize into stateless learnware and state-dependent firmware,\nreducing the impact of side-effects. The features learned by strongly-typed\nnets have a simple semantic interpretation via dynamic average-pooling on\none-dimensional convolutions. We also show that strongly-typed gradients are\nbetter behaved than in classical architectures, and characterize the\nrepresentational power of strongly-typed nets. Finally, experiments show that,\ndespite being more constrained, strongly-typed architectures achieve lower\ntraining and comparable generalization error to classical architectures.\n", "versions": [{"version": "v1", "created": "Sat, 6 Feb 2016 05:34:03 GMT"}, {"version": "v2", "created": "Tue, 24 May 2016 21:35:23 GMT"}], "update_date": "2016-05-26", "authors_parsed": [["Balduzzi", "David", ""], ["Ghifary", "Muhammad", ""]]}, {"id": "1602.02220", "submitter": "Tianbao Yang", "authors": "Zhe Li, Boqing Gong, Tianbao Yang", "title": "Improved Dropout for Shallow and Deep Learning", "comments": "In NIPS 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Dropout has been witnessed with great success in training deep neural\nnetworks by independently zeroing out the outputs of neurons at random. It has\nalso received a surge of interest for shallow learning, e.g., logistic\nregression. However, the independent sampling for dropout could be suboptimal\nfor the sake of convergence. In this paper, we propose to use multinomial\nsampling for dropout, i.e., sampling features or neurons according to a\nmultinomial distribution with different probabilities for different\nfeatures/neurons. To exhibit the optimal dropout probabilities, we analyze the\nshallow learning with multinomial dropout and establish the risk bound for\nstochastic optimization. By minimizing a sampling dependent factor in the risk\nbound, we obtain a distribution-dependent dropout with sampling probabilities\ndependent on the second order statistics of the data distribution. To tackle\nthe issue of evolving distribution of neurons in deep learning, we propose an\nefficient adaptive dropout (named \\textbf{evolutional dropout}) that computes\nthe sampling probabilities on-the-fly from a mini-batch of examples. Empirical\nstudies on several benchmark datasets demonstrate that the proposed dropouts\nachieve not only much faster convergence and but also a smaller testing error\nthan the standard dropout. For example, on the CIFAR-100 data, the evolutional\ndropout achieves relative improvements over 10\\% on the prediction performance\nand over 50\\% on the convergence speed compared to the standard dropout.\n", "versions": [{"version": "v1", "created": "Sat, 6 Feb 2016 05:41:57 GMT"}, {"version": "v2", "created": "Sun, 4 Dec 2016 05:31:19 GMT"}], "update_date": "2016-12-06", "authors_parsed": [["Li", "Zhe", ""], ["Gong", "Boqing", ""], ["Yang", "Tianbao", ""]]}, {"id": "1602.02256", "submitter": "Kohei Hayashi", "authors": "Kohei Hayashi, Takuya Konishi, Tatsuro Kawamoto", "title": "A Tractable Fully Bayesian Method for the Stochastic Block Model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The stochastic block model (SBM) is a generative model revealing macroscopic\nstructures in graphs. Bayesian methods are used for (i) cluster assignment\ninference and (ii) model selection for the number of clusters. In this paper,\nwe study the behavior of Bayesian inference in the SBM in the large sample\nlimit. Combining variational approximation and Laplace's method, a consistent\ncriterion of the fully marginalized log-likelihood is established. Based on\nthat, we derive a tractable algorithm that solves tasks (i) and (ii)\nconcurrently, obviating the need for an outer loop to check all model\ncandidates. Our empirical and theoretical results demonstrate that our method\nis scalable in computation, accurate in approximation, and concise in model\nselection.\n", "versions": [{"version": "v1", "created": "Sat, 6 Feb 2016 13:47:34 GMT"}], "update_date": "2016-02-09", "authors_parsed": [["Hayashi", "Kohei", ""], ["Konishi", "Takuya", ""], ["Kawamoto", "Tatsuro", ""]]}, {"id": "1602.02262", "submitter": "Yingyu Liang", "authors": "Yuanzhi Li, Yingyu Liang, Andrej Risteski", "title": "Recovery guarantee of weighted low-rank approximation via alternating\n  minimization", "comments": "40 pages. Updated with the ICML 2016 camera ready version, together\n  with an additional algorithm which needs less assumptions in Appendix C", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many applications require recovering a ground truth low-rank matrix from\nnoisy observations of the entries, which in practice is typically formulated as\na weighted low-rank approximation problem and solved by non-convex optimization\nheuristics such as alternating minimization. In this paper, we provide provable\nrecovery guarantee of weighted low-rank via a simple alternating minimization\nalgorithm. In particular, for a natural class of matrices and weights and\nwithout any assumption on the noise, we bound the spectral norm of the\ndifference between the recovered matrix and the ground truth, by the spectral\nnorm of the weighted noise plus an additive error that decreases exponentially\nwith the number of rounds of alternating minimization, from either\ninitialization by SVD or, more importantly, random initialization. These\nprovide the first theoretical results for weighted low-rank via alternating\nminimization with non-binary deterministic weights, significantly generalizing\nthose for matrix completion, the special case with binary weights, since our\nassumptions are similar or weaker than those made in existing works.\nFurthermore, this is achieved by a very simple algorithm that improves the\nvanilla alternating minimization with a simple clipping step.\n  The key technical challenge is that under non-binary deterministic weights,\nna\\\"ive alternating steps will destroy the incoherence and spectral properties\nof the intermediate solutions, which are needed for making progress towards the\nground truth. We show that the properties only need to hold in an average sense\nand can be achieved by the clipping step.\n  We further provide an alternating algorithm that uses a whitening step that\nkeeps the properties via SDP and Rademacher rounding and thus requires weaker\nassumptions. This technique can potentially be applied in some other\napplications and is of independent interest.\n", "versions": [{"version": "v1", "created": "Sat, 6 Feb 2016 14:55:12 GMT"}, {"version": "v2", "created": "Thu, 8 Dec 2016 17:05:41 GMT"}], "update_date": "2016-12-09", "authors_parsed": [["Li", "Yuanzhi", ""], ["Liang", "Yingyu", ""], ["Risteski", "Andrej", ""]]}, {"id": "1602.02263", "submitter": "Andreas Tillmann", "authors": "Andreas M. Tillmann, Yonina C. Eldar, Julien Mairal", "title": "DOLPHIn - Dictionary Learning for Phase Retrieval", "comments": null, "journal-ref": null, "doi": "10.1109/TSP.2016.2607180", "report-no": null, "categories": "math.OC cs.IT cs.LG math.IT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new algorithm to learn a dictionary for reconstructing and\nsparsely encoding signals from measurements without phase. Specifically, we\nconsider the task of estimating a two-dimensional image from squared-magnitude\nmeasurements of a complex-valued linear transformation of the original image.\nSeveral recent phase retrieval algorithms exploit underlying sparsity of the\nunknown signal in order to improve recovery performance. In this work, we\nconsider such a sparse signal prior in the context of phase retrieval, when the\nsparsifying dictionary is not known in advance. Our algorithm jointly\nreconstructs the unknown signal - possibly corrupted by noise - and learns a\ndictionary such that each patch of the estimated image can be sparsely\nrepresented. Numerical experiments demonstrate that our approach can obtain\nsignificantly better reconstructions for phase retrieval problems with noise\nthan methods that cannot exploit such \"hidden\" sparsity. Moreover, on the\ntheoretical side, we provide a convergence result for our method.\n", "versions": [{"version": "v1", "created": "Sat, 6 Feb 2016 15:03:26 GMT"}, {"version": "v2", "created": "Wed, 3 Aug 2016 09:53:47 GMT"}], "update_date": "2016-11-23", "authors_parsed": [["Tillmann", "Andreas M.", ""], ["Eldar", "Yonina C.", ""], ["Mairal", "Julien", ""]]}, {"id": "1602.02282", "submitter": "Casper Kaae S{\\o}nderby", "authors": "Casper Kaae S{\\o}nderby, Tapani Raiko, Lars Maal{\\o}e, S{\\o}ren Kaae\n  S{\\o}nderby, Ole Winther", "title": "Ladder Variational Autoencoders", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Variational Autoencoders are powerful models for unsupervised learning.\nHowever deep models with several layers of dependent stochastic variables are\ndifficult to train which limits the improvements obtained using these highly\nexpressive models. We propose a new inference model, the Ladder Variational\nAutoencoder, that recursively corrects the generative distribution by a data\ndependent approximate likelihood in a process resembling the recently proposed\nLadder Network. We show that this model provides state of the art predictive\nlog-likelihood and tighter log-likelihood lower bound compared to the purely\nbottom-up inference in layered Variational Autoencoders and other generative\nmodels. We provide a detailed analysis of the learned hierarchical latent\nrepresentation and show that our new inference model is qualitatively different\nand utilizes a deeper more distributed hierarchy of latent variables. Finally,\nwe observe that batch normalization and deterministic warm-up (gradually\nturning on the KL-term) are crucial for training variational models with many\nstochastic layers.\n", "versions": [{"version": "v1", "created": "Sat, 6 Feb 2016 17:32:48 GMT"}, {"version": "v2", "created": "Tue, 24 May 2016 10:41:45 GMT"}, {"version": "v3", "created": "Fri, 27 May 2016 09:05:10 GMT"}], "update_date": "2016-05-30", "authors_parsed": [["S\u00f8nderby", "Casper Kaae", ""], ["Raiko", "Tapani", ""], ["Maal\u00f8e", "Lars", ""], ["S\u00f8nderby", "S\u00f8ren Kaae", ""], ["Winther", "Ole", ""]]}, {"id": "1602.02283", "submitter": "Dominik Csiba", "authors": "Dominik Csiba and Peter Richt\\'arik", "title": "Importance Sampling for Minibatches", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Minibatching is a very well studied and highly popular technique in\nsupervised learning, used by practitioners due to its ability to accelerate\ntraining through better utilization of parallel processing power and reduction\nof stochastic variance. Another popular technique is importance sampling -- a\nstrategy for preferential sampling of more important examples also capable of\naccelerating the training process. However, despite considerable effort by the\ncommunity in these areas, and due to the inherent technical difficulty of the\nproblem, there is no existing work combining the power of importance sampling\nwith the strength of minibatching. In this paper we propose the first {\\em\nimportance sampling for minibatches} and give simple and rigorous complexity\nanalysis of its performance. We illustrate on synthetic problems that for\ntraining data of certain properties, our sampling can lead to several orders of\nmagnitude improvement in training time. We then test the new sampling on\nseveral popular datasets, and show that the improvement can reach an order of\nmagnitude.\n", "versions": [{"version": "v1", "created": "Sat, 6 Feb 2016 17:35:53 GMT"}], "update_date": "2016-02-09", "authors_parsed": [["Csiba", "Dominik", ""], ["Richt\u00e1rik", "Peter", ""]]}, {"id": "1602.02285", "submitter": "Uri Shaham", "authors": "Uri Shaham, Xiuyuan Cheng, Omer Dror, Ariel Jaffe, Boaz Nadler, Joseph\n  Chang, Yuval Kluger", "title": "A Deep Learning Approach to Unsupervised Ensemble Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": "PMLR 48:30-39", "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We show how deep learning methods can be applied in the context of\ncrowdsourcing and unsupervised ensemble learning. First, we prove that the\npopular model of Dawid and Skene, which assumes that all classifiers are\nconditionally independent, is {\\em equivalent} to a Restricted Boltzmann\nMachine (RBM) with a single hidden node. Hence, under this model, the posterior\nprobabilities of the true labels can be instead estimated via a trained RBM.\nNext, to address the more general case, where classifiers may strongly violate\nthe conditional independence assumption, we propose to apply RBM-based Deep\nNeural Net (DNN). Experimental results on various simulated and real-world\ndatasets demonstrate that our proposed DNN approach outperforms other\nstate-of-the-art methods, in particular when the data violates the conditional\nindependence assumption.\n", "versions": [{"version": "v1", "created": "Sat, 6 Feb 2016 17:56:59 GMT"}], "update_date": "2017-04-24", "authors_parsed": [["Shaham", "Uri", ""], ["Cheng", "Xiuyuan", ""], ["Dror", "Omer", ""], ["Jaffe", "Ariel", ""], ["Nadler", "Boaz", ""], ["Chang", "Joseph", ""], ["Kluger", "Yuval", ""]]}, {"id": "1602.02311", "submitter": "Yingzhen Li", "authors": "Yingzhen Li, Richard E. Turner", "title": "R\\'enyi Divergence Variational Inference", "comments": "NIPS 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces the variational R\\'enyi bound (VR) that extends\ntraditional variational inference to R\\'enyi's alpha-divergences. This new\nfamily of variational methods unifies a number of existing approaches, and\nenables a smooth interpolation from the evidence lower-bound to the log\n(marginal) likelihood that is controlled by the value of alpha that\nparametrises the divergence. The reparameterization trick, Monte Carlo\napproximation and stochastic optimisation methods are deployed to obtain a\ntractable and unified framework for optimisation. We further consider negative\nalpha values and propose a novel variational inference method as a new special\ncase in the proposed framework. Experiments on Bayesian neural networks and\nvariational auto-encoders demonstrate the wide applicability of the VR bound.\n", "versions": [{"version": "v1", "created": "Sat, 6 Feb 2016 21:35:23 GMT"}, {"version": "v2", "created": "Fri, 20 May 2016 08:35:54 GMT"}, {"version": "v3", "created": "Fri, 28 Oct 2016 18:16:29 GMT"}], "update_date": "2016-10-31", "authors_parsed": [["Li", "Yingzhen", ""], ["Turner", "Richard E.", ""]]}, {"id": "1602.02334", "submitter": "Leopoldo Bertossi", "authors": "Zeinab Bahmani, Leopoldo Bertossi and Nikolaos Vasiloglou", "title": "ERBlox: Combining Matching Dependencies with Machine Learning for Entity\n  Resolution", "comments": "Final journal version, with some minor technical corrections.\n  Extended version of arXiv:1508.06013", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Entity resolution (ER), an important and common data cleaning problem, is\nabout detecting data duplicate representations for the same external entities,\nand merging them into single representations. Relatively recently, declarative\nrules called \"matching dependencies\" (MDs) have been proposed for specifying\nsimilarity conditions under which attribute values in database records are\nmerged. In this work we show the process and the benefits of integrating four\ncomponents of ER: (a) Building a classifier for duplicate/non-duplicate record\npairs built using machine learning (ML) techniques; (b) Use of MDs for\nsupporting the blocking phase of ML; (c) Record merging on the basis of the\nclassifier results; and (d) The use of the declarative language \"LogiQL\" -an\nextended form of Datalog supported by the \"LogicBlox\" platform- for all\nactivities related to data processing, and the specification and enforcement of\nMDs.\n", "versions": [{"version": "v1", "created": "Sun, 7 Feb 2016 03:06:40 GMT"}, {"version": "v2", "created": "Sun, 27 Nov 2016 21:09:37 GMT"}, {"version": "v3", "created": "Wed, 18 Jan 2017 17:43:43 GMT"}], "update_date": "2017-01-19", "authors_parsed": [["Bahmani", "Zeinab", ""], ["Bertossi", "Leopoldo", ""], ["Vasiloglou", "Nikolaos", ""]]}, {"id": "1602.02338", "submitter": "Saul Toscano-Palmerin", "authors": "Saul Toscano-Palmerin and Peter I. Frazier", "title": "Stratified Bayesian Optimization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider derivative-free black-box global optimization of expensive noisy\nfunctions, when most of the randomness in the objective is produced by a few\ninfluential scalar random inputs. We present a new Bayesian global optimization\nalgorithm, called Stratified Bayesian Optimization (SBO), which uses this\nstrong dependence to improve performance. Our algorithm is similar in spirit to\nstratification, a technique from simulation, which uses strong dependence on a\ncategorical representation of the random input to reduce variance. We\ndemonstrate in numerical experiments that SBO outperforms state-of-the-art\nBayesian optimization benchmarks that do not leverage this dependence.\n", "versions": [{"version": "v1", "created": "Sun, 7 Feb 2016 03:53:16 GMT"}, {"version": "v2", "created": "Sat, 20 Feb 2016 10:08:18 GMT"}], "update_date": "2016-02-23", "authors_parsed": [["Toscano-Palmerin", "Saul", ""], ["Frazier", "Peter I.", ""]]}, {"id": "1602.02350", "submitter": "Alon Gonen", "authors": "Alon Gonen, Francesco Orabona, Shai Shalev-Shwartz", "title": "Solving Ridge Regression using Sketched Preconditioned SVRG", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop a novel preconditioning method for ridge regression, based on\nrecent linear sketching methods. By equipping Stochastic Variance Reduced\nGradient (SVRG) with this preconditioning process, we obtain a significant\nspeed-up relative to fast stochastic methods such as SVRG, SDCA and SAG.\n", "versions": [{"version": "v1", "created": "Sun, 7 Feb 2016 08:37:18 GMT"}, {"version": "v2", "created": "Thu, 26 May 2016 07:42:58 GMT"}], "update_date": "2016-05-27", "authors_parsed": [["Gonen", "Alon", ""], ["Orabona", "Francesco", ""], ["Shalev-Shwartz", "Shai", ""]]}, {"id": "1602.02355", "submitter": "Fabian Pedregosa", "authors": "Fabian Pedregosa", "title": "Hyperparameter optimization with approximate gradient", "comments": "Proceedings of the International conference on Machine Learning\n  (ICML)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most models in machine learning contain at least one hyperparameter to\ncontrol for model complexity. Choosing an appropriate set of hyperparameters is\nboth crucial in terms of model accuracy and computationally challenging. In\nthis work we propose an algorithm for the optimization of continuous\nhyperparameters using inexact gradient information. An advantage of this method\nis that hyperparameters can be updated before model parameters have fully\nconverged. We also give sufficient conditions for the global convergence of\nthis method, based on regularity conditions of the involved functions and\nsummability of errors. Finally, we validate the empirical performance of this\nmethod on the estimation of regularization constants of L2-regularized logistic\nregression and kernel Ridge regression. Empirical benchmarks indicate that our\napproach is highly competitive with respect to state of the art methods.\n", "versions": [{"version": "v1", "created": "Sun, 7 Feb 2016 10:37:13 GMT"}, {"version": "v2", "created": "Tue, 9 Feb 2016 11:24:08 GMT"}, {"version": "v3", "created": "Fri, 27 May 2016 09:50:52 GMT"}, {"version": "v4", "created": "Thu, 9 Jun 2016 15:42:04 GMT"}, {"version": "v5", "created": "Sun, 26 Jun 2016 01:02:54 GMT"}], "update_date": "2016-09-06", "authors_parsed": [["Pedregosa", "Fabian", ""]]}, {"id": "1602.02358", "submitter": "Haohan Zhu", "authors": "Haohan Zhu, Xianrui Meng and George Kollios", "title": "NED: An Inter-Graph Node Metric Based On Edit Distance", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.LG cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Node similarity is a fundamental problem in graph analytics. However, node\nsimilarity between nodes in different graphs (inter-graph nodes) has not\nreceived a lot of attention yet. The inter-graph node similarity is important\nin learning a new graph based on the knowledge of an existing graph (transfer\nlearning on graphs) and has applications in biological, communication, and\nsocial networks. In this paper, we propose a novel distance function for\nmeasuring inter-graph node similarity with edit distance, called NED. In NED,\ntwo nodes are compared according to their local neighborhood structures which\nare represented as unordered k-adjacent trees, without relying on labels or\nother assumptions. Since the computation problem of tree edit distance on\nunordered trees is NP-Complete, we propose a modified tree edit distance,\ncalled TED*, for comparing neighborhood trees. TED* is a metric distance, as\nthe original tree edit distance, but more importantly, TED* is polynomially\ncomputable. As a metric distance, NED admits efficient indexing, provides\ninterpretable results, and shows to perform better than existing approaches on\na number of data analysis tasks, including graph de-anonymization. Finally, the\nefficiency and effectiveness of NED are empirically demonstrated using\nreal-world graphs.\n", "versions": [{"version": "v1", "created": "Sun, 7 Feb 2016 11:00:03 GMT"}, {"version": "v2", "created": "Sat, 13 Feb 2016 21:14:57 GMT"}, {"version": "v3", "created": "Tue, 16 Feb 2016 04:24:07 GMT"}], "update_date": "2016-02-17", "authors_parsed": [["Zhu", "Haohan", ""], ["Meng", "Xianrui", ""], ["Kollios", "George", ""]]}, {"id": "1602.02373", "submitter": "Rie Johnson", "authors": "Rie Johnson, Tong Zhang", "title": "Supervised and Semi-Supervised Text Categorization using LSTM for Region\n  Embeddings", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One-hot CNN (convolutional neural network) has been shown to be effective for\ntext categorization (Johnson & Zhang, 2015). We view it as a special case of a\ngeneral framework which jointly trains a linear model with a non-linear feature\ngenerator consisting of `text region embedding + pooling'. Under this\nframework, we explore a more sophisticated region embedding method using Long\nShort-Term Memory (LSTM). LSTM can embed text regions of variable (and possibly\nlarge) sizes, whereas the region size needs to be fixed in a CNN. We seek\neffective and efficient use of LSTM for this purpose in the supervised and\nsemi-supervised settings. The best results were obtained by combining region\nembeddings in the form of LSTM and convolution layers trained on unlabeled\ndata. The results indicate that on this task, embeddings of text regions, which\ncan convey complex concepts, are more useful than embeddings of single words in\nisolation. We report performances exceeding the previous best results on four\nbenchmark datasets.\n", "versions": [{"version": "v1", "created": "Sun, 7 Feb 2016 14:05:58 GMT"}, {"version": "v2", "created": "Thu, 26 May 2016 15:26:34 GMT"}], "update_date": "2016-05-27", "authors_parsed": [["Johnson", "Rie", ""], ["Zhang", "Tong", ""]]}, {"id": "1602.02383", "submitter": "William Whitney", "authors": "William Whitney", "title": "Disentangled Representations in Neural Models", "comments": "MIT Master's of Engineering thesis", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Representation learning is the foundation for the recent success of neural\nnetwork models. However, the distributed representations generated by neural\nnetworks are far from ideal. Due to their highly entangled nature, they are di\ncult to reuse and interpret, and they do a poor job of capturing the sparsity\nwhich is present in real- world transformations. In this paper, I describe\nmethods for learning disentangled representations in the two domains of\ngraphics and computation. These methods allow neural methods to learn\nrepresentations which are easy to interpret and reuse, yet they incur little or\nno penalty to performance. In the Graphics section, I demonstrate the ability\nof these methods to infer the generating parameters of images and rerender\nthose images under novel conditions. In the Computation section, I describe a\nmodel which is able to factorize a multitask learning problem into subtasks and\nwhich experiences no catastrophic forgetting. Together these techniques provide\nthe tools to design a wide range of models that learn disentangled\nrepresentations and better model the factors of variation in the real world.\n", "versions": [{"version": "v1", "created": "Sun, 7 Feb 2016 15:32:30 GMT"}], "update_date": "2016-02-09", "authors_parsed": [["Whitney", "William", ""]]}, {"id": "1602.02386", "submitter": "Qingming Tang", "authors": "Qingming Tang, Lifu Tu, Weiran Wang and Jinbo Xu", "title": "Network Inference by Learned Node-Specific Degree Prior", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel method for network inference from partially observed edges\nusing a node-specific degree prior. The degree prior is derived from observed\nedges in the network to be inferred, and its hyper-parameters are determined by\ncross validation. Then we formulate network inference as a matrix completion\nproblem regularized by our degree prior. Our theoretical analysis indicates\nthat this prior favors a network following the learned degree distribution, and\nmay lead to improved network recovery error bound than previous work.\nExperimental results on both simulated and real biological networks demonstrate\nthe superior performance of our method in various settings.\n", "versions": [{"version": "v1", "created": "Sun, 7 Feb 2016 16:11:18 GMT"}], "update_date": "2016-02-09", "authors_parsed": [["Tang", "Qingming", ""], ["Tu", "Lifu", ""], ["Wang", "Weiran", ""], ["Xu", "Jinbo", ""]]}, {"id": "1602.02389", "submitter": "Tom Zahavy", "authors": "Tom Zahavy, Bingyi Kang, Alex Sivak, Jiashi Feng, Huan Xu, Shie Mannor", "title": "Ensemble Robustness and Generalization of Stochastic Deep Learning\n  Algorithms", "comments": "16 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The question why deep learning algorithms generalize so well has attracted\nincreasing research interest. However, most of the well-established approaches,\nsuch as hypothesis capacity, stability or sparseness, have not provided\ncomplete explanations (Zhang et al., 2016; Kawaguchi et al., 2017). In this\nwork, we focus on the robustness approach (Xu & Mannor, 2012), i.e., if the\nerror of a hypothesis will not change much due to perturbations of its training\nexamples, then it will also generalize well. As most deep learning algorithms\nare stochastic (e.g., Stochastic Gradient Descent, Dropout, and\nBayes-by-backprop), we revisit the robustness arguments of Xu & Mannor, and\nintroduce a new approach, ensemble robustness, that concerns the robustness of\na population of hypotheses. Through the lens of ensemble robustness, we reveal\nthat a stochastic learning algorithm can generalize well as long as its\nsensitiveness to adversarial perturbations is bounded in average over training\nexamples. Moreover, an algorithm may be sensitive to some adversarial examples\n(Goodfellow et al., 2015) but still generalize well. To support our claims, we\nprovide extensive simulations for different deep learning algorithms and\ndifferent network architectures exhibiting a strong correlation between\nensemble robustness and the ability to generalize.\n", "versions": [{"version": "v1", "created": "Sun, 7 Feb 2016 16:50:14 GMT"}, {"version": "v2", "created": "Wed, 10 Feb 2016 01:59:39 GMT"}, {"version": "v3", "created": "Wed, 29 Jun 2016 21:02:34 GMT"}, {"version": "v4", "created": "Sun, 5 Nov 2017 12:18:24 GMT"}], "update_date": "2017-11-07", "authors_parsed": [["Zahavy", "Tom", ""], ["Kang", "Bingyi", ""], ["Sivak", "Alex", ""], ["Feng", "Jiashi", ""], ["Xu", "Huan", ""], ["Mannor", "Shie", ""]]}, {"id": "1602.02442", "submitter": "Aaron Defazio Dr", "authors": "Aaron Defazio", "title": "A Simple Practical Accelerated Method for Finite Sums", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  We describe a novel optimization method for finite sums (such as empirical\nrisk minimization problems) building on the recently introduced SAGA method.\nOur method achieves an accelerated convergence rate on strongly convex smooth\nproblems. Our method has only one parameter (a step size), and is radically\nsimpler than other accelerated methods for finite sums. Additionally it can be\napplied when the terms are non-smooth, yielding a method applicable in many\nareas where operator splitting methods would traditionally be applied.\n", "versions": [{"version": "v1", "created": "Mon, 8 Feb 2016 00:24:01 GMT"}, {"version": "v2", "created": "Thu, 27 Oct 2016 23:18:05 GMT"}], "update_date": "2016-10-31", "authors_parsed": [["Defazio", "Aaron", ""]]}, {"id": "1602.02450", "submitter": "Giorgio Patrini", "authors": "Giorgio Patrini, Frank Nielsen, Richard Nock, Marcello Carioni", "title": "Loss factorization, weakly supervised learning and label noise\n  robustness", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We prove that the empirical risk of most well-known loss functions factors\ninto a linear term aggregating all labels with a term that is label free, and\ncan further be expressed by sums of the loss. This holds true even for\nnon-smooth, non-convex losses and in any RKHS. The first term is a (kernel)\nmean operator --the focal quantity of this work-- which we characterize as the\nsufficient statistic for the labels. The result tightens known generalization\nbounds and sheds new light on their interpretation.\n  Factorization has a direct application on weakly supervised learning. In\nparticular, we demonstrate that algorithms like SGD and proximal methods can be\nadapted with minimal effort to handle weak supervision, once the mean operator\nhas been estimated. We apply this idea to learning with asymmetric noisy\nlabels, connecting and extending prior work. Furthermore, we show that most\nlosses enjoy a data-dependent (by the mean operator) form of noise robustness,\nin contrast with known negative results.\n", "versions": [{"version": "v1", "created": "Mon, 8 Feb 2016 01:50:43 GMT"}, {"version": "v2", "created": "Tue, 9 Feb 2016 23:10:23 GMT"}], "update_date": "2016-02-11", "authors_parsed": [["Patrini", "Giorgio", ""], ["Nielsen", "Frank", ""], ["Nock", "Richard", ""], ["Carioni", "Marcello", ""]]}, {"id": "1602.02454", "submitter": "Vasilis Syrgkanis", "authors": "Vasilis Syrgkanis, Akshay Krishnamurthy, Robert E. Schapire", "title": "Efficient Algorithms for Adversarial Contextual Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We provide the first oracle efficient sublinear regret algorithms for\nadversarial versions of the contextual bandit problem. In this problem, the\nlearner repeatedly makes an action on the basis of a context and receives\nreward for the chosen action, with the goal of achieving reward competitive\nwith a large class of policies. We analyze two settings: i) in the transductive\nsetting the learner knows the set of contexts a priori, ii) in the small\nseparator setting, there exists a small set of contexts such that any two\npolicies behave differently in one of the contexts in the set. Our algorithms\nfall into the follow the perturbed leader family \\cite{Kalai2005} and achieve\nregret $O(T^{3/4}\\sqrt{K\\log(N)})$ in the transductive setting and $O(T^{2/3}\nd^{3/4} K\\sqrt{\\log(N)})$ in the separator setting, where $K$ is the number of\nactions, $N$ is the number of baseline policies, and $d$ is the size of the\nseparator. We actually solve the more general adversarial contextual\nsemi-bandit linear optimization problem, whilst in the full information setting\nwe address the even more general contextual combinatorial optimization. We\nprovide several extensions and implications of our algorithms, such as\nswitching regret and efficient learning with predictable sequences.\n", "versions": [{"version": "v1", "created": "Mon, 8 Feb 2016 03:11:39 GMT"}], "update_date": "2016-02-09", "authors_parsed": [["Syrgkanis", "Vasilis", ""], ["Krishnamurthy", "Akshay", ""], ["Schapire", "Robert E.", ""]]}, {"id": "1602.02505", "submitter": "Itay Hubara", "authors": "Itay Hubara, Daniel Soudry, Ran El Yaniv", "title": "Binarized Neural Networks", "comments": "This is an obsolete version, up to date version is available here:\n  arXiv:1602.02830", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a method to train Binarized Neural Networks (BNNs) - neural\nnetworks with binary weights and activations at run-time and when computing the\nparameters' gradient at train-time. We conduct two sets of experiments, each\nbased on a different framework, namely Torch7 and Theano, where we train BNNs\non MNIST, CIFAR-10 and SVHN, and achieve nearly state-of-the-art results.\nDuring the forward pass, BNNs drastically reduce memory size and accesses, and\nreplace most arithmetic operations with bit-wise operations, which might lead\nto a great increase in power-efficiency. Last but not least, we wrote a binary\nmatrix multiplication GPU kernel with which it is possible to run our MNIST BNN\n7 times faster than with an unoptimized GPU kernel, without suffering any loss\nin classification accuracy. The code for training and running our BNNs is\navailable.\n", "versions": [{"version": "v1", "created": "Mon, 8 Feb 2016 09:37:46 GMT"}, {"version": "v2", "created": "Thu, 11 Feb 2016 07:40:57 GMT"}, {"version": "v3", "created": "Thu, 10 Mar 2016 12:37:44 GMT"}], "update_date": "2016-03-11", "authors_parsed": [["Hubara", "Itay", ""], ["Soudry", "Daniel", ""], ["Yaniv", "Ran El", ""]]}, {"id": "1602.02514", "submitter": "James Newling", "authors": "James Newling and Fran\\c{c}ois Fleuret", "title": "Fast K-Means with Accurate Bounds", "comments": "8 pages + supplementary material v2: mlpack installed with\n  optimisation (previously installed in DEBUG) v3: Annulus -> Annular v4:\n  Author affiliation update v5: Synced with version at ICML, now including\n  Suppl. Mat", "journal-ref": "Proceedings of the International Conference on Machine Learning\n  (ICML) pp. 936-944, 2016", "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel accelerated exact k-means algorithm, which performs better\nthan the current state-of-the-art low-dimensional algorithm in 18 of 22\nexperiments, running up to 3 times faster. We also propose a general\nimprovement of existing state-of-the-art accelerated exact k-means algorithms\nthrough better estimates of the distance bounds used to reduce the number of\ndistance calculations, and get a speedup in 36 of 44 experiments, up to 1.8\ntimes faster.\n  We have conducted experiments with our own implementations of existing\nmethods to ensure homogeneous evaluation of performance, and we show that our\nimplementations perform as well or better than existing available\nimplementations. Finally, we propose simplified variants of standard approaches\nand show that they are faster than their fully-fledged counterparts in 59 of 62\nexperiments.\n", "versions": [{"version": "v1", "created": "Mon, 8 Feb 2016 10:19:09 GMT"}, {"version": "v2", "created": "Wed, 10 Feb 2016 18:51:11 GMT"}, {"version": "v3", "created": "Thu, 25 Feb 2016 15:18:03 GMT"}, {"version": "v4", "created": "Mon, 4 Apr 2016 09:12:21 GMT"}, {"version": "v5", "created": "Thu, 28 Apr 2016 12:11:49 GMT"}, {"version": "v6", "created": "Sun, 11 Sep 2016 14:57:29 GMT"}], "update_date": "2016-09-13", "authors_parsed": [["Newling", "James", ""], ["Fleuret", "Fran\u00e7ois", ""]]}, {"id": "1602.02518", "submitter": "Sahely Bhadra", "authors": "Sahely Bhadra, Samuel Kaski and Juho Rousu", "title": "Multi-view Kernel Completion", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we introduce the first method that (1) can complete kernel\nmatrices with completely missing rows and columns as opposed to individual\nmissing kernel values, (2) does not require any of the kernels to be complete a\npriori, and (3) can tackle non-linear kernels. These aspects are necessary in\npractical applications such as integrating legacy data sets, learning under\nsensor failures and learning when measurements are costly for some of the\nviews. The proposed approach predicts missing rows by modelling both\nwithin-view and between-view relationships among kernel values. We show, both\non simulated data and real world data, that the proposed method outperforms\nexisting techniques in the restricted settings where they are available, and\nextends applicability to new settings.\n", "versions": [{"version": "v1", "created": "Mon, 8 Feb 2016 10:29:13 GMT"}], "update_date": "2016-02-09", "authors_parsed": [["Bhadra", "Sahely", ""], ["Kaski", "Samuel", ""], ["Rousu", "Juho", ""]]}, {"id": "1602.02523", "submitter": "Rowan McAllister", "authors": "Rowan McAllister, Carl Edward Rasmussen", "title": "Data-Efficient Reinforcement Learning in Continuous-State POMDPs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG cs.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a data-efficient reinforcement learning algorithm resistant to\nobservation noise. Our method extends the highly data-efficient PILCO algorithm\n(Deisenroth & Rasmussen, 2011) into partially observed Markov decision\nprocesses (POMDPs) by considering the filtering process during policy\nevaluation. PILCO conducts policy search, evaluating each policy by first\npredicting an analytic distribution of possible system trajectories. We\nadditionally predict trajectories w.r.t. a filtering process, achieving\nsignificantly higher performance than combining a filter with a policy\noptimised by the original (unfiltered) framework. Our test setup is the\ncartpole swing-up task with sensor noise, which involves nonlinear dynamics and\nrequires nonlinear control.\n", "versions": [{"version": "v1", "created": "Mon, 8 Feb 2016 11:08:49 GMT"}], "update_date": "2016-02-09", "authors_parsed": [["McAllister", "Rowan", ""], ["Rasmussen", "Carl Edward", ""]]}, {"id": "1602.02543", "submitter": "Brijnesh Jain", "authors": "Brijnesh J. Jain", "title": "Homogeneity of Cluster Ensembles", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The expectation and the mean of partitions generated by a cluster ensemble\nare not unique in general. This issue poses challenges in statistical inference\nand cluster stability. In this contribution, we state sufficient conditions for\nuniqueness of expectation and mean. The proposed conditions show that a unique\nmean is neither exceptional nor generic. To cope with this issue, we introduce\nhomogeneity as a measure of how likely is a unique mean for a sample of\npartitions. We show that homogeneity is related to cluster stability. This\nresult points to a possible conflict between cluster stability and diversity in\nconsensus clustering. To assess homogeneity in a practical setting, we propose\nan efficient way to compute a lower bound of homogeneity. Empirical results\nusing the k-means algorithm suggest that uniqueness of the mean partition is\nnot exceptional for real-world data. Moreover, for samples of high homogeneity,\nuniqueness can be enforced by increasing the number of data points or by\nremoving outlier partitions. In a broader context, this contribution can be\nplaced as a further step towards a statistical theory of partitions.\n", "versions": [{"version": "v1", "created": "Mon, 8 Feb 2016 12:28:57 GMT"}], "update_date": "2016-02-09", "authors_parsed": [["Jain", "Brijnesh J.", ""]]}, {"id": "1602.02644", "submitter": "Alexey Dosovitskiy", "authors": "Alexey Dosovitskiy and Thomas Brox", "title": "Generating Images with Perceptual Similarity Metrics based on Deep\n  Networks", "comments": "minor corrections", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image-generating machine learning models are typically trained with loss\nfunctions based on distance in the image space. This often leads to\nover-smoothed results. We propose a class of loss functions, which we call deep\nperceptual similarity metrics (DeePSiM), that mitigate this problem. Instead of\ncomputing distances in the image space, we compute distances between image\nfeatures extracted by deep neural networks. This metric better reflects\nperceptually similarity of images and thus leads to better results. We show\nthree applications: autoencoder training, a modification of a variational\nautoencoder, and inversion of deep convolutional networks. In all cases, the\ngenerated images look sharp and resemble natural images.\n", "versions": [{"version": "v1", "created": "Mon, 8 Feb 2016 16:50:28 GMT"}, {"version": "v2", "created": "Tue, 9 Feb 2016 09:36:36 GMT"}], "update_date": "2016-02-10", "authors_parsed": [["Dosovitskiy", "Alexey", ""], ["Brox", "Thomas", ""]]}, {"id": "1602.02658", "submitter": "Tom Zahavy", "authors": "Tom Zahavy, Nir Ben Zrihem, Shie Mannor", "title": "Graying the black box: Understanding DQNs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years there is a growing interest in using deep representations for\nreinforcement learning. In this paper, we present a methodology and tools to\nanalyze Deep Q-networks (DQNs) in a non-blind matter. Moreover, we propose a\nnew model, the Semi Aggregated Markov Decision Process (SAMDP), and an\nalgorithm that learns it automatically. The SAMDP model allows us to identify\nspatio-temporal abstractions directly from features and may be used as a\nsub-goal detector in future work. Using our tools we reveal that the features\nlearned by DQNs aggregate the state space in a hierarchical fashion, explaining\nits success. Moreover, we are able to understand and describe the policies\nlearned by DQNs for three different Atari2600 games and suggest ways to\ninterpret, debug and optimize deep neural networks in reinforcement learning.\n", "versions": [{"version": "v1", "created": "Mon, 8 Feb 2016 17:27:31 GMT"}, {"version": "v2", "created": "Tue, 9 Feb 2016 16:13:00 GMT"}, {"version": "v3", "created": "Wed, 17 Feb 2016 19:15:55 GMT"}, {"version": "v4", "created": "Mon, 24 Apr 2017 09:57:21 GMT"}], "update_date": "2017-04-25", "authors_parsed": [["Zahavy", "Tom", ""], ["Zrihem", "Nir Ben", ""], ["Mannor", "Shie", ""]]}, {"id": "1602.02660", "submitter": "Sander Dieleman", "authors": "Sander Dieleman, Jeffrey De Fauw, Koray Kavukcuoglu", "title": "Exploiting Cyclic Symmetry in Convolutional Neural Networks", "comments": "10 pages, 6 figures, accepted for publication at ICML 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many classes of images exhibit rotational symmetry. Convolutional neural\nnetworks are sometimes trained using data augmentation to exploit this, but\nthey are still required to learn the rotation equivariance properties from the\ndata. Encoding these properties into the network architecture, as we are\nalready used to doing for translation equivariance by using convolutional\nlayers, could result in a more efficient use of the parameter budget by\nrelieving the model from learning them. We introduce four operations which can\nbe inserted into neural network models as layers, and which can be combined to\nmake these models partially equivariant to rotations. They also enable\nparameter sharing across different orientations. We evaluate the effect of\nthese architectural modifications on three datasets which exhibit rotational\nsymmetry and demonstrate improved performance with smaller models.\n", "versions": [{"version": "v1", "created": "Mon, 8 Feb 2016 17:37:16 GMT"}, {"version": "v2", "created": "Thu, 26 May 2016 11:47:18 GMT"}], "update_date": "2016-05-27", "authors_parsed": [["Dieleman", "Sander", ""], ["De Fauw", "Jeffrey", ""], ["Kavukcuoglu", "Koray", ""]]}, {"id": "1602.02666", "submitter": "Stephan Mandt", "authors": "Stephan Mandt, Matthew D. Hoffman, and David M. Blei", "title": "A Variational Analysis of Stochastic Gradient Algorithms", "comments": "8 pages, 3 figures", "journal-ref": "International Conference on Machine Learning (ICML 2016), p.\n  354--363", "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Stochastic Gradient Descent (SGD) is an important algorithm in machine\nlearning. With constant learning rates, it is a stochastic process that, after\nan initial phase of convergence, generates samples from a stationary\ndistribution. We show that SGD with constant rates can be effectively used as\nan approximate posterior inference algorithm for probabilistic modeling.\nSpecifically, we show how to adjust the tuning parameters of SGD such as to\nmatch the resulting stationary distribution to the posterior. This analysis\nrests on interpreting SGD as a continuous-time stochastic process and then\nminimizing the Kullback-Leibler divergence between its stationary distribution\nand the target posterior. (This is in the spirit of variational inference.) In\nmore detail, we model SGD as a multivariate Ornstein-Uhlenbeck process and then\nuse properties of this process to derive the optimal parameters. This\ntheoretical framework also connects SGD to modern scalable inference\nalgorithms; we analyze the recently proposed stochastic gradient Fisher scoring\nunder this perspective. We demonstrate that SGD with properly chosen constant\nrates gives a new way to optimize hyperparameters in probabilistic models.\n", "versions": [{"version": "v1", "created": "Mon, 8 Feb 2016 17:46:18 GMT"}], "update_date": "2017-09-12", "authors_parsed": [["Mandt", "Stephan", ""], ["Hoffman", "Matthew D.", ""], ["Blei", "David M.", ""]]}, {"id": "1602.02672", "submitter": "Jakob Foerster", "authors": "Jakob N. Foerster, Yannis M. Assael, Nando de Freitas, Shimon Whiteson", "title": "Learning to Communicate to Solve Riddles with Deep Distributed Recurrent\n  Q-Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose deep distributed recurrent Q-networks (DDRQN), which enable teams\nof agents to learn to solve communication-based coordination tasks. In these\ntasks, the agents are not given any pre-designed communication protocol.\nTherefore, in order to successfully communicate, they must first automatically\ndevelop and agree upon their own communication protocol. We present empirical\nresults on two multi-agent learning problems based on well-known riddles,\ndemonstrating that DDRQN can successfully solve such tasks and discover elegant\ncommunication protocols to do so. To our knowledge, this is the first time deep\nreinforcement learning has succeeded in learning communication protocols. In\naddition, we present ablation experiments that confirm that each of the main\ncomponents of the DDRQN architecture are critical to its success.\n", "versions": [{"version": "v1", "created": "Mon, 8 Feb 2016 18:01:35 GMT"}], "update_date": "2016-02-09", "authors_parsed": [["Foerster", "Jakob N.", ""], ["Assael", "Yannis M.", ""], ["de Freitas", "Nando", ""], ["Whiteson", "Shimon", ""]]}, {"id": "1602.02685", "submitter": "Cristobal Esteban", "authors": "Crist\\'obal Esteban, Oliver Staeck, Yinchong Yang and Volker Tresp", "title": "Predicting Clinical Events by Combining Static and Dynamic Information\n  Using Recurrent Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In clinical data sets we often find static information (e.g. patient gender,\nblood type, etc.) combined with sequences of data that are recorded during\nmultiple hospital visits (e.g. medications prescribed, tests performed, etc.).\nRecurrent Neural Networks (RNNs) have proven to be very successful for\nmodelling sequences of data in many areas of Machine Learning. In this work we\npresent an approach based on RNNs, specifically designed for the clinical\ndomain, that combines static and dynamic information in order to predict future\nevents. We work with a database collected in the Charit\\'{e} Hospital in Berlin\nthat contains complete information concerning patients that underwent a kidney\ntransplantation. After the transplantation three main endpoints can occur:\nrejection of the kidney, loss of the kidney and death of the patient. Our goal\nis to predict, based on information recorded in the Electronic Health Record of\neach patient, whether any of those endpoints will occur within the next six or\ntwelve months after each visit to the clinic. We compared different types of\nRNNs that we developed for this work, with a model based on a Feedforward\nNeural Network and a Logistic Regression model. We found that the RNN that we\ndeveloped based on Gated Recurrent Units provides the best performance for this\ntask. We also used the same models for a second task, i.e., next event\nprediction, and found that here the model based on a Feedforward Neural Network\noutperformed the other models. Our hypothesis is that long-term dependencies\nare not as relevant in this task.\n", "versions": [{"version": "v1", "created": "Mon, 8 Feb 2016 18:30:58 GMT"}, {"version": "v2", "created": "Thu, 17 Nov 2016 11:52:19 GMT"}], "update_date": "2016-11-18", "authors_parsed": [["Esteban", "Crist\u00f3bal", ""], ["Staeck", "Oliver", ""], ["Yang", "Yinchong", ""], ["Tresp", "Volker", ""]]}, {"id": "1602.02697", "submitter": "Nicolas Papernot", "authors": "Nicolas Papernot and Patrick McDaniel and Ian Goodfellow and Somesh\n  Jha and Z. Berkay Celik and Ananthram Swami", "title": "Practical Black-Box Attacks against Machine Learning", "comments": "Proceedings of the 2017 ACM Asia Conference on Computer and\n  Communications Security, Abu Dhabi, UAE", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Machine learning (ML) models, e.g., deep neural networks (DNNs), are\nvulnerable to adversarial examples: malicious inputs modified to yield\nerroneous model outputs, while appearing unmodified to human observers.\nPotential attacks include having malicious content like malware identified as\nlegitimate or controlling vehicle behavior. Yet, all existing adversarial\nexample attacks require knowledge of either the model internals or its training\ndata. We introduce the first practical demonstration of an attacker controlling\na remotely hosted DNN with no such knowledge. Indeed, the only capability of\nour black-box adversary is to observe labels given by the DNN to chosen inputs.\nOur attack strategy consists in training a local model to substitute for the\ntarget DNN, using inputs synthetically generated by an adversary and labeled by\nthe target DNN. We use the local substitute to craft adversarial examples, and\nfind that they are misclassified by the targeted DNN. To perform a real-world\nand properly-blinded evaluation, we attack a DNN hosted by MetaMind, an online\ndeep learning API. We find that their DNN misclassifies 84.24% of the\nadversarial examples crafted with our substitute. We demonstrate the general\napplicability of our strategy to many ML techniques by conducting the same\nattack against models hosted by Amazon and Google, using logistic regression\nsubstitutes. They yield adversarial examples misclassified by Amazon and Google\nat rates of 96.19% and 88.94%. We also find that this black-box attack strategy\nis capable of evading defense strategies previously found to make adversarial\nexample crafting harder.\n", "versions": [{"version": "v1", "created": "Mon, 8 Feb 2016 19:12:25 GMT"}, {"version": "v2", "created": "Fri, 19 Feb 2016 01:49:44 GMT"}, {"version": "v3", "created": "Mon, 7 Nov 2016 00:01:18 GMT"}, {"version": "v4", "created": "Sun, 19 Mar 2017 14:50:18 GMT"}], "update_date": "2017-03-21", "authors_parsed": [["Papernot", "Nicolas", ""], ["McDaniel", "Patrick", ""], ["Goodfellow", "Ian", ""], ["Jha", "Somesh", ""], ["Celik", "Z. Berkay", ""], ["Swami", "Ananthram", ""]]}, {"id": "1602.02701", "submitter": "Arthur Mensch", "authors": "Arthur Mensch (PARIETAL), Ga\\\"el Varoquaux (PARIETAL), Bertrand\n  Thirion (PARIETAL)", "title": "Compressed Online Dictionary Learning for Fast fMRI Decomposition", "comments": null, "journal-ref": "IEEE International Symposium on Biomedical Imaging, 2016", "doi": "10.1109/ISBI.2016.7493501", "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a method for fast resting-state fMRI spatial decomposi-tions of\nvery large datasets, based on the reduction of the temporal dimension before\napplying dictionary learning on concatenated individual records from groups of\nsubjects. Introducing a measure of correspondence between spatial\ndecompositions of rest fMRI, we demonstrates that time-reduced dictionary\nlearning produces result as reliable as non-reduced decompositions. We also\nshow that this reduction significantly improves computational scalability.\n", "versions": [{"version": "v1", "created": "Mon, 8 Feb 2016 19:19:08 GMT"}], "update_date": "2019-05-16", "authors_parsed": [["Mensch", "Arthur", "", "PARIETAL"], ["Varoquaux", "Ga\u00ebl", "", "PARIETAL"], ["Thirion", "Bertrand", "", "PARIETAL"]]}, {"id": "1602.02706", "submitter": "Julien Audiffren", "authors": "Julien Audiffren (CMLA), Ralaivola Liva (LIF)", "title": "Decoy Bandits Dueling on a Poset", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We adress the problem of dueling bandits defined on partially ordered sets,\nor posets. In this setting, arms may not be comparable, and there may be\nseveral (incomparable) optimal arms. We propose an algorithm, UnchainedBandits,\nthat efficiently finds the set of optimal arms of any poset even when pairs of\ncomparable arms cannot be distinguished from pairs of incomparable arms, with a\nset of minimal assumptions. This algorithm relies on the concept of decoys,\nwhich stems from social psychology. For the easier case where the\nincomparability information may be accessible, we propose a second algorithm,\nSlicingBandits, which takes advantage of this information and achieves a very\nsignificant gain of performance compared to UnchainedBandits. We provide\ntheoretical guarantees and experimental evaluation for both algorithms.\n", "versions": [{"version": "v1", "created": "Mon, 8 Feb 2016 19:32:18 GMT"}, {"version": "v2", "created": "Thu, 9 Jun 2016 06:57:28 GMT"}], "update_date": "2016-06-10", "authors_parsed": [["Audiffren", "Julien", "", "CMLA"], ["Liva", "Ralaivola", "", "LIF"]]}, {"id": "1602.02722", "submitter": "Akshay Krishnamurthy", "authors": "Akshay Krishnamurthy, Alekh Agarwal, John Langford", "title": "PAC Reinforcement Learning with Rich Observations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose and study a new model for reinforcement learning with rich\nobservations, generalizing contextual bandits to sequential decision making.\nThese models require an agent to take actions based on observations (features)\nwith the goal of achieving long-term performance competitive with a large set\nof policies. To avoid barriers to sample-efficient learning associated with\nlarge observation spaces and general POMDPs, we focus on problems that can be\nsummarized by a small number of hidden states and have long-term rewards that\nare predictable by a reactive function class. In this setting, we design and\nanalyze a new reinforcement learning algorithm, Least Squares Value Elimination\nby Exploration. We prove that the algorithm learns near optimal behavior after\na number of episodes that is polynomial in all relevant parameters, logarithmic\nin the number of policies, and independent of the size of the observation\nspace. Our result provides theoretical justification for reinforcement learning\nwith function approximation.\n", "versions": [{"version": "v1", "created": "Mon, 8 Feb 2016 20:12:50 GMT"}, {"version": "v2", "created": "Tue, 1 Mar 2016 15:16:12 GMT"}, {"version": "v3", "created": "Tue, 24 May 2016 13:20:29 GMT"}, {"version": "v4", "created": "Fri, 28 Oct 2016 15:37:17 GMT"}], "update_date": "2016-10-31", "authors_parsed": [["Krishnamurthy", "Akshay", ""], ["Agarwal", "Alekh", ""], ["Langford", "John", ""]]}, {"id": "1602.02726", "submitter": "Patrick Johnstone", "authors": "Patrick R. Johnstone and Pierre Moulin", "title": "Local and Global Convergence of a General Inertial Proximal Splitting\n  Scheme", "comments": "33 pages 1 figure", "journal-ref": "Comput Optim Appl 67, 259-292 (2017)", "doi": "10.1007/s10589-017-9896-7", "report-no": null, "categories": "math.OC cs.LG math.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper is concerned with convex composite minimization problems in a\nHilbert space. In these problems, the objective is the sum of two closed,\nproper, and convex functions where one is smooth and the other admits a\ncomputationally inexpensive proximal operator. We analyze a general family of\ninertial proximal splitting algorithms (GIPSA) for solving such problems. We\nestablish finiteness of the sum of squared increments of the iterates and\noptimality of the accumulation points. Weak convergence of the entire sequence\nthen follows if the minimum is attained. Our analysis unifies and extends\nseveral previous results.\n  We then focus on $\\ell_1$-regularized optimization, which is the ubiquitous\nspecial case where the nonsmooth term is the $\\ell_1$-norm. For certain\nparameter choices, GIPSA is amenable to a local analysis for this problem. For\nthese choices we show that GIPSA achieves finite \"active manifold\nidentification\", i.e. convergence in a finite number of iterations to the\noptimal support and sign, after which GIPSA reduces to minimizing a local\nsmooth function. Local linear convergence then holds under certain conditions.\nWe determine the rate in terms of the inertia, stepsize, and local curvature.\nOur local analysis is applicable to certain recent variants of the Fast\nIterative Shrinkage-Thresholding Algorithm (FISTA), for which we establish\nactive manifold identification and local linear convergence. Our analysis\nmotivates the use of a momentum restart scheme in these FISTA variants to\nobtain the optimal local linear convergence rate.\n", "versions": [{"version": "v1", "created": "Mon, 8 Feb 2016 20:27:19 GMT"}], "update_date": "2020-02-19", "authors_parsed": [["Johnstone", "Patrick R.", ""], ["Moulin", "Pierre", ""]]}, {"id": "1602.02823", "submitter": "Mark Tygert", "authors": "Mark Tygert", "title": "Poor starting points in machine learning", "comments": "11 pages, 3 figures, 1 table; this initial version is literally\n  identical to that circulated among a restricted audience over a month ago", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Poor (even random) starting points for learning/training/optimization are\ncommon in machine learning. In many settings, the method of Robbins and Monro\n(online stochastic gradient descent) is known to be optimal for good starting\npoints, but may not be optimal for poor starting points -- indeed, for poor\nstarting points Nesterov acceleration can help during the initial iterations,\neven though Nesterov methods not designed for stochastic approximation could\nhurt during later iterations. The common practice of training with nontrivial\nminibatches enhances the advantage of Nesterov acceleration.\n", "versions": [{"version": "v1", "created": "Tue, 9 Feb 2016 00:14:03 GMT"}], "update_date": "2016-02-10", "authors_parsed": [["Tygert", "Mark", ""]]}, {"id": "1602.02830", "submitter": "Itay Hubara", "authors": "Matthieu Courbariaux, Itay Hubara, Daniel Soudry, Ran El-Yaniv and\n  Yoshua Bengio", "title": "Binarized Neural Networks: Training Deep Neural Networks with Weights\n  and Activations Constrained to +1 or -1", "comments": "11 pages and 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a method to train Binarized Neural Networks (BNNs) - neural\nnetworks with binary weights and activations at run-time. At training-time the\nbinary weights and activations are used for computing the parameters gradients.\nDuring the forward pass, BNNs drastically reduce memory size and accesses, and\nreplace most arithmetic operations with bit-wise operations, which is expected\nto substantially improve power-efficiency. To validate the effectiveness of\nBNNs we conduct two sets of experiments on the Torch7 and Theano frameworks. On\nboth, BNNs achieved nearly state-of-the-art results over the MNIST, CIFAR-10\nand SVHN datasets. Last but not least, we wrote a binary matrix multiplication\nGPU kernel with which it is possible to run our MNIST BNN 7 times faster than\nwith an unoptimized GPU kernel, without suffering any loss in classification\naccuracy. The code for training and running our BNNs is available on-line.\n", "versions": [{"version": "v1", "created": "Tue, 9 Feb 2016 01:01:59 GMT"}, {"version": "v2", "created": "Mon, 29 Feb 2016 21:26:53 GMT"}, {"version": "v3", "created": "Thu, 17 Mar 2016 14:54:25 GMT"}], "update_date": "2016-03-18", "authors_parsed": [["Courbariaux", "Matthieu", ""], ["Hubara", "Itay", ""], ["Soudry", "Daniel", ""], ["El-Yaniv", "Ran", ""], ["Bengio", "Yoshua", ""]]}, {"id": "1602.02842", "submitter": "Truyen Tran", "authors": "Truyen Tran, Dinh Phung and Svetha Venkatesh", "title": "Collaborative filtering via sparse Markov random fields", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recommender systems play a central role in providing individualized access to\ninformation and services. This paper focuses on collaborative filtering, an\napproach that exploits the shared structure among mind-liked users and similar\nitems. In particular, we focus on a formal probabilistic framework known as\nMarkov random fields (MRF). We address the open problem of structure learning\nand introduce a sparsity-inducing algorithm to automatically estimate the\ninteraction structures between users and between items. Item-item and user-user\ncorrelation networks are obtained as a by-product. Large-scale experiments on\nmovie recommendation and date matching datasets demonstrate the power of the\nproposed method.\n", "versions": [{"version": "v1", "created": "Tue, 9 Feb 2016 02:30:27 GMT"}], "update_date": "2016-02-10", "authors_parsed": [["Tran", "Truyen", ""], ["Phung", "Dinh", ""], ["Venkatesh", "Svetha", ""]]}, {"id": "1602.02845", "submitter": "Carlos Riquelme Ruiz", "authors": "Carlos Riquelme, Ramesh Johari, Baosen Zhang", "title": "Online Active Linear Regression via Thresholding", "comments": "Published in AAAI 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of online active learning to collect data for\nregression modeling. Specifically, we consider a decision maker with a limited\nexperimentation budget who must efficiently learn an underlying linear\npopulation model. Our main contribution is a novel threshold-based algorithm\nfor selection of most informative observations; we characterize its performance\nand fundamental lower bounds. We extend the algorithm and its guarantees to\nsparse linear regression in high-dimensional settings. Simulations suggest the\nalgorithm is remarkably robust: it provides significant benefits over passive\nrandom sampling in real-world datasets that exhibit high nonlinearity and high\ndimensionality --- significantly reducing both the mean and variance of the\nsquared error.\n", "versions": [{"version": "v1", "created": "Tue, 9 Feb 2016 02:51:12 GMT"}, {"version": "v2", "created": "Wed, 10 Feb 2016 17:53:33 GMT"}, {"version": "v3", "created": "Thu, 23 Jun 2016 18:36:58 GMT"}, {"version": "v4", "created": "Wed, 21 Dec 2016 13:36:50 GMT"}], "update_date": "2016-12-22", "authors_parsed": [["Riquelme", "Carlos", ""], ["Johari", "Ramesh", ""], ["Zhang", "Baosen", ""]]}, {"id": "1602.02850", "submitter": "Bo Tang", "authors": "Bo Tang, Steven Kay, and Haibo He", "title": "Toward Optimal Feature Selection in Naive Bayes for Text Categorization", "comments": "This paper has been submitted to the IEEE Trans. Knowledge and Data\n  Engineering. 14 pages, 5 figures", "journal-ref": null, "doi": "10.1109/TKDE.2016.2563436", "report-no": null, "categories": "stat.ML cs.CL cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automated feature selection is important for text categorization to reduce\nthe feature size and to speed up the learning process of classifiers. In this\npaper, we present a novel and efficient feature selection framework based on\nthe Information Theory, which aims to rank the features with their\ndiscriminative capacity for classification. We first revisit two information\nmeasures: Kullback-Leibler divergence and Jeffreys divergence for binary\nhypothesis testing, and analyze their asymptotic properties relating to type I\nand type II errors of a Bayesian classifier. We then introduce a new divergence\nmeasure, called Jeffreys-Multi-Hypothesis (JMH) divergence, to measure\nmulti-distribution divergence for multi-class classification. Based on the\nJMH-divergence, we develop two efficient feature selection methods, termed\nmaximum discrimination ($MD$) and $MD-\\chi^2$ methods, for text categorization.\nThe promising results of extensive experiments demonstrate the effectiveness of\nthe proposed approaches.\n", "versions": [{"version": "v1", "created": "Tue, 9 Feb 2016 03:43:21 GMT"}], "update_date": "2016-11-15", "authors_parsed": [["Tang", "Bo", ""], ["Kay", "Steven", ""], ["He", "Haibo", ""]]}, {"id": "1602.02852", "submitter": "Nicol\\'as Della Penna", "authors": "Nicol\\'as Della Penna, Mark D. Reid, David Balduzzi", "title": "Compliance-Aware Bandits", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motivated by clinical trials, we study bandits with observable\nnon-compliance. At each step, the learner chooses an arm, after, instead of\nobserving only the reward, it also observes the action that took place. We show\nthat such noncompliance can be helpful or hurtful to the learner in general.\nUnfortunately, naively incorporating compliance information into bandit\nalgorithms loses guarantees on sublinear regret. We present hybrid algorithms\nthat maintain regret bounds up to a multiplicative factor and can incorporate\ncompliance information. Simulations based on real data from the International\nStoke Trial show the practical potential of these algorithms.\n", "versions": [{"version": "v1", "created": "Tue, 9 Feb 2016 04:00:32 GMT"}], "update_date": "2016-02-10", "authors_parsed": [["Della Penna", "Nicol\u00e1s", ""], ["Reid", "Mark D.", ""], ["Balduzzi", "David", ""]]}, {"id": "1602.02865", "submitter": "Babak Saleh", "authors": "Babak Saleh and Ahmed Elgammal and Jacob Feldman", "title": "The Role of Typicality in Object Classification: Improving The\n  Generalization Capacity of Convolutional Neural Networks", "comments": "In Submission", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep artificial neural networks have made remarkable progress in different\ntasks in the field of computer vision. However, the empirical analysis of these\nmodels and investigation of their failure cases has received attention\nrecently. In this work, we show that deep learning models cannot generalize to\natypical images that are substantially different from training images. This is\nin contrast to the superior generalization ability of the visual system in the\nhuman brain. We focus on Convolutional Neural Networks (CNN) as the\nstate-of-the-art models in object recognition and classification; investigate\nthis problem in more detail, and hypothesize that training CNN models suffer\nfrom unstructured loss minimization. We propose computational models to improve\nthe generalization capacity of CNNs by considering how typical a training image\nlooks like. By conducting an extensive set of experiments we show that\ninvolving a typicality measure can improve the classification results on a new\nset of images by a large margin. More importantly, this significant improvement\nis achieved without fine-tuning the CNN model on the target image set.\n", "versions": [{"version": "v1", "created": "Tue, 9 Feb 2016 05:30:33 GMT"}], "update_date": "2016-02-10", "authors_parsed": [["Saleh", "Babak", ""], ["Elgammal", "Ahmed", ""], ["Feldman", "Jacob", ""]]}, {"id": "1602.02867", "submitter": "Aviv Tamar", "authors": "Aviv Tamar, Yi Wu, Garrett Thomas, Sergey Levine, Pieter Abbeel", "title": "Value Iteration Networks", "comments": "Fixed missing table values", "journal-ref": "Advances in Neural Information Processing Systems 29 pages\n  2154--2162, 2016", "doi": null, "report-no": null, "categories": "cs.AI cs.LG cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce the value iteration network (VIN): a fully differentiable neural\nnetwork with a `planning module' embedded within. VINs can learn to plan, and\nare suitable for predicting outcomes that involve planning-based reasoning,\nsuch as policies for reinforcement learning. Key to our approach is a novel\ndifferentiable approximation of the value-iteration algorithm, which can be\nrepresented as a convolutional neural network, and trained end-to-end using\nstandard backpropagation. We evaluate VIN based policies on discrete and\ncontinuous path-planning domains, and on a natural-language based search task.\nWe show that by learning an explicit planning computation, VIN policies\ngeneralize better to new, unseen domains.\n", "versions": [{"version": "v1", "created": "Tue, 9 Feb 2016 05:44:36 GMT"}, {"version": "v2", "created": "Sun, 29 May 2016 18:33:04 GMT"}, {"version": "v3", "created": "Sun, 5 Feb 2017 20:06:14 GMT"}, {"version": "v4", "created": "Mon, 20 Mar 2017 21:41:51 GMT"}], "update_date": "2017-03-22", "authors_parsed": [["Tamar", "Aviv", ""], ["Wu", "Yi", ""], ["Thomas", "Garrett", ""], ["Levine", "Sergey", ""], ["Abbeel", "Pieter", ""]]}, {"id": "1602.02887", "submitter": "F. Ozgur Catak", "authors": "Ferhat \\\"Ozg\\\"ur \\c{C}atak", "title": "Classification with Boosting of Extreme Learning Machine Over\n  Arbitrarily Partitioned Data", "comments": "Springer Soft Computing", "journal-ref": null, "doi": "10.1007/s00500-015-1938-4", "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Machine learning based computational intelligence methods are widely used to\nanalyze large scale data sets in this age of big data. Extracting useful\npredictive modeling from these types of data sets is a challenging problem due\nto their high complexity. Analyzing large amount of streaming data that can be\nleveraged to derive business value is another complex problem to solve. With\nhigh levels of data availability (\\textit{i.e. Big Data}) automatic\nclassification of them has become an important and complex task. Hence, we\nexplore the power of applying MapReduce based Distributed AdaBoosting of\nExtreme Learning Machine (ELM) to build a predictive bag of classification\nmodels. Accordingly, (i) data set ensembles are created; (ii) ELM algorithm is\nused to build weak learners (classifier functions); and (iii) builds a strong\nlearner from a set of weak learners. We applied this training model to the\nbenchmark knowledge discovery and data mining data sets.\n", "versions": [{"version": "v1", "created": "Tue, 9 Feb 2016 08:09:26 GMT"}], "update_date": "2016-02-10", "authors_parsed": [["\u00c7atak", "Ferhat \u00d6zg\u00fcr", ""]]}, {"id": "1602.02888", "submitter": "F. Ozgur Catak", "authors": "Ferhat \\\"Ozg\\\"ur \\c{C}atak", "title": "Robust Ensemble Classifier Combination Based on Noise Removal with\n  One-Class SVM", "comments": "22nd International Conference, ICONIP 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In machine learning area, as the number of labeled input samples becomes very\nlarge, it is very difficult to build a classification model because of input\ndata set is not fit in a memory in training phase of the algorithm, therefore,\nit is necessary to utilize data partitioning to handle overall data set.\nBagging and boosting based data partitioning methods have been broadly used in\ndata mining and pattern recognition area. Both of these methods have shown a\ngreat possibility for improving classification model performance. This study is\nconcerned with the analysis of data set partitioning with noise removal and its\nimpact on the performance of multiple classifier models. In this study, we\npropose noise filtering preprocessing at each data set partition to increment\nclassifier model performance. We applied Gini impurity approach to find the\nbest split percentage of noise filter ratio. The filtered sub data set is then\nused to train individual ensemble models.\n", "versions": [{"version": "v1", "created": "Tue, 9 Feb 2016 08:14:29 GMT"}], "update_date": "2016-02-10", "authors_parsed": [["\u00c7atak", "Ferhat \u00d6zg\u00fcr", ""]]}, {"id": "1602.02899", "submitter": "F. Ozgur Catak", "authors": "Ferhat \\\"Ozg\\\"ur \\c{C}atak", "title": "Secure Multi-Party Computation Based Privacy Preserving Extreme Learning\n  Machine Algorithm Over Vertically Distributed Data", "comments": "22nd International Conference, ICONIP 2015", "journal-ref": null, "doi": "10.1007/978-3-319-26535-3_39", "report-no": null, "categories": "cs.CR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Especially in the Big Data era, the usage of different classification methods\nis increasing day by day. The success of these classification methods depends\non the effectiveness of learning methods. Extreme learning machine (ELM)\nclassification algorithm is a relatively new learning method built on\nfeed-forward neural-network. ELM classification algorithm is a simple and fast\nmethod that can create a model from high-dimensional data sets. Traditional ELM\nlearning algorithm implicitly assumes complete access to whole data set. This\nis a major privacy concern in most of cases. Sharing of private data (i.e.\nmedical records) is prevented because of security concerns. In this research,\nwe propose an efficient and secure privacy-preserving learning algorithm for\nELM classification over data that is vertically partitioned among several\nparties. The new learning method preserves the privacy on numerical attributes,\nbuilds a classification model without sharing private data without disclosing\nthe data of each party to others.\n", "versions": [{"version": "v1", "created": "Tue, 9 Feb 2016 08:37:26 GMT"}], "update_date": "2016-02-10", "authors_parsed": [["\u00c7atak", "Ferhat \u00d6zg\u00fcr", ""]]}, {"id": "1602.02934", "submitter": "James Newling", "authors": "James Newling and Fran\\c{c}ois Fleuret", "title": "Nested Mini-Batch K-Means", "comments": "8 pages + Supplementary Material. Version 2 : new experiments added.\n  Version 3 : Add acknowledgments, upper case in title. Version 4 : Correct\n  spelling of Acknowledgements, change title. Version 5: camera ready NIPS", "journal-ref": "Nested Mini-Batch K-Means, Proceedings of the International\n  Conference on Neural Information Processing Systems (NIPS), 2016", "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A new algorithm is proposed which accelerates the mini-batch k-means\nalgorithm of Sculley (2010) by using the distance bounding approach of Elkan\n(2003). We argue that, when incorporating distance bounds into a mini-batch\nalgorithm, already used data should preferentially be reused. To this end we\npropose using nested mini-batches, whereby data in a mini-batch at iteration t\nis automatically reused at iteration t+1.\n  Using nested mini-batches presents two difficulties. The first is that\nunbalanced use of data can bias estimates, which we resolve by ensuring that\neach data sample contributes exactly once to centroids. The second is in\nchoosing mini-batch sizes, which we address by balancing premature fine-tuning\nof centroids with redundancy induced slow-down. Experiments show that the\nresulting nmbatch algorithm is very effective, often arriving within 1% of the\nempirical minimum 100 times earlier than the standard mini-batch algorithm.\n", "versions": [{"version": "v1", "created": "Tue, 9 Feb 2016 11:05:42 GMT"}, {"version": "v2", "created": "Sun, 22 May 2016 12:04:17 GMT"}, {"version": "v3", "created": "Mon, 30 May 2016 07:40:13 GMT"}, {"version": "v4", "created": "Wed, 29 Jun 2016 06:17:32 GMT"}, {"version": "v5", "created": "Mon, 12 Sep 2016 22:25:20 GMT"}], "update_date": "2016-09-14", "authors_parsed": [["Newling", "James", ""], ["Fleuret", "Fran\u00e7ois", ""]]}, {"id": "1602.02950", "submitter": "Zhizheng Wu", "authors": "Xiaohai Tian, Zhizheng Wu, Xiong Xiao, Eng Siong Chng, Haizhou Li", "title": "Spoofing detection under noisy conditions: a preliminary investigation\n  and an initial database", "comments": "Submitted to Odyssey: The Speaker and Language Recognition Workshop\n  2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.SD", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Spoofing detection for automatic speaker verification (ASV), which is to\ndiscriminate between live speech and attacks, has received increasing\nattentions recently. However, all the previous studies have been done on the\nclean data without significant additive noise. To simulate the real-life\nscenarios, we perform a preliminary investigation of spoofing detection under\nadditive noisy conditions, and also describe an initial database for this task.\nThe noisy database is based on the ASVspoof challenge 2015 database and\ngenerated by artificially adding background noises at different signal-to-noise\nratios (SNRs). Five different additive noises are included. Our preliminary\nresults show that using the model trained from clean data, the system\nperformance degrades significantly in noisy conditions. Phase-based feature is\nmore noise robust than magnitude-based features. And the systems perform\nsignificantly differ under different noise scenarios.\n", "versions": [{"version": "v1", "created": "Tue, 9 Feb 2016 12:00:56 GMT"}], "update_date": "2016-02-10", "authors_parsed": [["Tian", "Xiaohai", ""], ["Wu", "Zhizheng", ""], ["Xiao", "Xiong", ""], ["Chng", "Eng Siong", ""], ["Li", "Haizhou", ""]]}, {"id": "1602.02990", "submitter": "Georg Martius", "authors": "Ralf Der and Georg Martius", "title": "Self-organized control for musculoskeletal robots", "comments": "11 pages, 4 figures, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.LG cs.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the accelerated development of robot technologies, optimal control\nbecomes one of the central themes of research. In traditional approaches, the\ncontroller, by its internal functionality, finds appropriate actions on the\nbasis of the history of sensor values, guided by the goals, intentions,\nobjectives, learning schemes, and so on planted into it. The idea is that the\ncontroller controls the world---the body plus its environment---as reliably as\npossible. However, in elastically actuated robots this approach faces severe\ndifficulties. This paper advocates for a new paradigm of self-organized\ncontrol. The paper presents a solution with a controller that is devoid of any\nfunctionalities of its own, given by a fixed, explicit and context-free\nfunction of the recent history of the sensor values. When applying this\ncontroller to a muscle-tendon driven arm-shoulder system from the Myorobotics\ntoolkit, we observe a vast variety of self-organized behavior patterns: when\nleft alone, the arm realizes pseudo-random sequences of different poses but one\ncan also manipulate the system into definite motion patterns. But most\ninterestingly, after attaching an object, the controller gets in a functional\nresonance with the object's internal dynamics: when given a half-filled bottle,\nthe system spontaneously starts shaking the bottle so that maximum response\nfrom the dynamics of the water is being generated. After attaching a pendulum\nto the arm, the controller drives the pendulum into a circular mode. In this\nway, the robot discovers dynamical affordances of objects its body is\ninteracting with. We also discuss perspectives for using this controller\nparadigm for intention driven behavior generation.\n", "versions": [{"version": "v1", "created": "Tue, 9 Feb 2016 14:16:26 GMT"}, {"version": "v2", "created": "Wed, 15 Jun 2016 12:37:19 GMT"}], "update_date": "2016-06-16", "authors_parsed": [["Der", "Ralf", ""], ["Martius", "Georg", ""]]}, {"id": "1602.03001", "submitter": "Miltiadis Allamanis", "authors": "Miltiadis Allamanis, Hao Peng, Charles Sutton", "title": "A Convolutional Attention Network for Extreme Summarization of Source\n  Code", "comments": "Code, data and visualization at\n  http://groups.inf.ed.ac.uk/cup/codeattention/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Attention mechanisms in neural networks have proved useful for problems in\nwhich the input and output do not have fixed dimension. Often there exist\nfeatures that are locally translation invariant and would be valuable for\ndirecting the model's attention, but previous attentional architectures are not\nconstructed to learn such features specifically. We introduce an attentional\nneural network that employs convolution on the input tokens to detect local\ntime-invariant and long-range topical attention features in a context-dependent\nway. We apply this architecture to the problem of extreme summarization of\nsource code snippets into short, descriptive function name-like summaries.\nUsing those features, the model sequentially generates a summary by\nmarginalizing over two attention mechanisms: one that predicts the next summary\ntoken based on the attention weights of the input tokens and another that is\nable to copy a code token as-is directly into the summary. We demonstrate our\nconvolutional attention neural network's performance on 10 popular Java\nprojects showing that it achieves better performance compared to previous\nattentional mechanisms.\n", "versions": [{"version": "v1", "created": "Tue, 9 Feb 2016 14:36:49 GMT"}, {"version": "v2", "created": "Wed, 25 May 2016 12:18:28 GMT"}], "update_date": "2016-05-26", "authors_parsed": [["Allamanis", "Miltiadis", ""], ["Peng", "Hao", ""], ["Sutton", "Charles", ""]]}, {"id": "1602.03014", "submitter": "Yutian Chen", "authors": "Yutian Chen and Max Welling", "title": "Herding as a Learning System with Edge-of-Chaos Dynamics", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Herding defines a deterministic dynamical system at the edge of chaos. It\ngenerates a sequence of model states and parameters by alternating parameter\nperturbations with state maximizations, where the sequence of states can be\ninterpreted as \"samples\" from an associated MRF model. Herding differs from\nmaximum likelihood estimation in that the sequence of parameters does not\nconverge to a fixed point and differs from an MCMC posterior sampling approach\nin that the sequence of states is generated deterministically. Herding may be\ninterpreted as a\"perturb and map\" method where the parameter perturbations are\ngenerated using a deterministic nonlinear dynamical system rather than randomly\nfrom a Gumbel distribution. This chapter studies the distinct statistical\ncharacteristics of the herding algorithm and shows that the fast convergence\nrate of the controlled moments may be attributed to edge of chaos dynamics. The\nherding algorithm can also be generalized to models with latent variables and\nto a discriminative learning setting. The perceptron cycling theorem ensures\nthat the fast moment matching property is preserved in the more general\nframework.\n", "versions": [{"version": "v1", "created": "Tue, 9 Feb 2016 14:59:45 GMT"}, {"version": "v2", "created": "Tue, 1 Mar 2016 09:37:28 GMT"}], "update_date": "2016-03-02", "authors_parsed": [["Chen", "Yutian", ""], ["Welling", "Max", ""]]}, {"id": "1602.03027", "submitter": "Ilya Tolstikhin", "authors": "Ilya Tolstikhin and David Lopez-Paz", "title": "Minimax Lower Bounds for Realizable Transductive Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Transductive learning considers a training set of $m$ labeled samples and a\ntest set of $u$ unlabeled samples, with the goal of best labeling that\nparticular test set. Conversely, inductive learning considers a training set of\n$m$ labeled samples drawn iid from $P(X,Y)$, with the goal of best labeling any\nfuture samples drawn iid from $P(X)$. This comparison suggests that\ntransduction is a much easier type of inference than induction, but is this\nreally the case? This paper provides a negative answer to this question, by\nproving the first known minimax lower bounds for transductive, realizable,\nbinary classification. Our lower bounds show that $m$ should be at least\n$\\Omega(d/\\epsilon + \\log(1/\\delta)/\\epsilon)$ when $\\epsilon$-learning a\nconcept class $\\mathcal{H}$ of finite VC-dimension $d<\\infty$ with confidence\n$1-\\delta$, for all $m \\leq u$. This result draws three important conclusions.\nFirst, general transduction is as hard as general induction, since both\nproblems have $\\Omega(d/m)$ minimax values. Second, the use of unlabeled data\ndoes not help general transduction, since supervised learning algorithms such\nas ERM and (Hanneke, 2015) match our transductive lower bounds while ignoring\nthe unlabeled test set. Third, our transductive lower bounds imply lower bounds\nfor semi-supervised learning, which add to the important discussion about the\nrole of unlabeled data in machine learning.\n", "versions": [{"version": "v1", "created": "Tue, 9 Feb 2016 15:17:24 GMT"}], "update_date": "2016-02-10", "authors_parsed": [["Tolstikhin", "Ilya", ""], ["Lopez-Paz", "David", ""]]}, {"id": "1602.03040", "submitter": "Rotem Dror", "authors": "Rotem Dror, Roi Reichart", "title": "The Structured Weighted Violations Perceptron Algorithm", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present the Structured Weighted Violations Perceptron (SWVP) algorithm, a\nnew structured prediction algorithm that generalizes the Collins Structured\nPerceptron (CSP). Unlike CSP, the update rule of SWVP explicitly exploits the\ninternal structure of the predicted labels. We prove the convergence of SWVP\nfor linearly separable training sets, provide mistake and generalization\nbounds, and show that in the general case these bounds are tighter than those\nof the CSP special case. In synthetic data experiments with data drawn from an\nHMM, various variants of SWVP substantially outperform its CSP special case.\nSWVP also provides encouraging initial dependency parsing results.\n", "versions": [{"version": "v1", "created": "Tue, 9 Feb 2016 15:51:19 GMT"}, {"version": "v2", "created": "Mon, 6 Jun 2016 08:00:15 GMT"}, {"version": "v3", "created": "Wed, 14 Sep 2016 07:24:10 GMT"}], "update_date": "2016-09-15", "authors_parsed": [["Dror", "Rotem", ""], ["Reichart", "Roi", ""]]}, {"id": "1602.03061", "submitter": "Matthew Reyes", "authors": "Matthew G. Reyes and David L. Neuhoff", "title": "Minimum Conditional Description Length Estimation for Markov Random\n  Fields", "comments": "Information Theory and Applications (ITA) workshop, February 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.LG math.IT math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we discuss a method, which we call Minimum Conditional\nDescription Length (MCDL), for estimating the parameters of a subset of sites\nwithin a Markov random field. We assume that the edges are known for the entire\ngraph $G=(V,E)$. Then, for a subset $U\\subset V$, we estimate the parameters\nfor nodes and edges in $U$ as well as for edges incident to a node in $U$, by\nfinding the exponential parameter for that subset that yields the best\ncompression conditioned on the values on the boundary $\\partial U$. Our\nestimate is derived from a temporally stationary sequence of observations on\nthe set $U$. We discuss how this method can also be applied to estimate a\nspatially invariant parameter from a single configuration, and in so doing,\nderive the Maximum Pseudo-Likelihood (MPL) estimate.\n", "versions": [{"version": "v1", "created": "Tue, 9 Feb 2016 16:36:51 GMT"}, {"version": "v2", "created": "Tue, 23 Feb 2016 07:24:23 GMT"}], "update_date": "2016-02-25", "authors_parsed": [["Reyes", "Matthew G.", ""], ["Neuhoff", "David L.", ""]]}, {"id": "1602.03105", "submitter": "Branislav Kveton", "authors": "Branislav Kveton, Hung Bui, Mohammad Ghavamzadeh, Georgios\n  Theocharous, S. Muthukrishnan, and Siqi Sun", "title": "Graphical Model Sketch", "comments": "Proceedings of the European Conference on Machine Learning and\n  Knowledge Discovery in Databases", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Structured high-cardinality data arises in many domains, and poses a major\nchallenge for both modeling and inference. Graphical models are a popular\napproach to modeling structured data but they are unsuitable for\nhigh-cardinality variables. The count-min (CM) sketch is a popular approach to\nestimating probabilities in high-cardinality data but it does not scale well\nbeyond a few variables. In this work, we bring together the ideas of graphical\nmodels and count sketches; and propose and analyze several approaches to\nestimating probabilities in structured high-cardinality streams of data. The\nkey idea of our approximations is to use the structure of a graphical model and\napproximately estimate its factors by \"sketches\", which hash high-cardinality\nvariables using random projections. Our approximations are computationally\nefficient and their space complexity is independent of the cardinality of\nvariables. Our error bounds are multiplicative and significantly improve upon\nthose of the CM sketch, a state-of-the-art approach to estimating probabilities\nin streams. We evaluate our approximations on synthetic and real-world\nproblems, and report an order of magnitude improvements over the CM sketch.\n", "versions": [{"version": "v1", "created": "Tue, 9 Feb 2016 18:07:51 GMT"}, {"version": "v2", "created": "Mon, 18 Jul 2016 05:48:03 GMT"}], "update_date": "2016-07-19", "authors_parsed": [["Kveton", "Branislav", ""], ["Bui", "Hung", ""], ["Ghavamzadeh", "Mohammad", ""], ["Theocharous", "Georgios", ""], ["Muthukrishnan", "S.", ""], ["Sun", "Siqi", ""]]}, {"id": "1602.03146", "submitter": "Sumeet Katariya", "authors": "Sumeet Katariya, Branislav Kveton, Csaba Szepesv\\'ari, Zheng Wen", "title": "DCM Bandits: Learning to Rank with Multiple Clicks", "comments": "Proceedings of the 33rd International Conference on Machine Learning", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A search engine recommends to the user a list of web pages. The user examines\nthis list, from the first page to the last, and clicks on all attractive pages\nuntil the user is satisfied. This behavior of the user can be described by the\ndependent click model (DCM). We propose DCM bandits, an online learning variant\nof the DCM where the goal is to maximize the probability of recommending\nsatisfactory items, such as web pages. The main challenge of our learning\nproblem is that we do not observe which attractive item is satisfactory. We\npropose a computationally-efficient learning algorithm for solving our problem,\ndcmKL-UCB; derive gap-dependent upper bounds on its regret under reasonable\nassumptions; and also prove a matching lower bound up to logarithmic factors.\nWe evaluate our algorithm on synthetic and real-world problems, and show that\nit performs well even when our model is misspecified. This work presents the\nfirst practical and regret-optimal online algorithm for learning to rank with\nmultiple clicks in a cascade-like click model.\n", "versions": [{"version": "v1", "created": "Tue, 9 Feb 2016 20:03:30 GMT"}, {"version": "v2", "created": "Tue, 31 May 2016 20:52:17 GMT"}], "update_date": "2016-06-02", "authors_parsed": [["Katariya", "Sumeet", ""], ["Kveton", "Branislav", ""], ["Szepesv\u00e1ri", "Csaba", ""], ["Wen", "Zheng", ""]]}, {"id": "1602.03218", "submitter": "Karol Kurach", "authors": "Marcin Andrychowicz and Karol Kurach", "title": "Learning Efficient Algorithms with Hierarchical Attentive Memory", "comments": "Added soft attention appendix", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose and investigate a novel memory architecture for\nneural networks called Hierarchical Attentive Memory (HAM). It is based on a\nbinary tree with leaves corresponding to memory cells. This allows HAM to\nperform memory access in O(log n) complexity, which is a significant\nimprovement over the standard attention mechanism that requires O(n)\noperations, where n is the size of the memory.\n  We show that an LSTM network augmented with HAM can learn algorithms for\nproblems like merging, sorting or binary searching from pure input-output\nexamples. In particular, it learns to sort n numbers in time O(n log n) and\ngeneralizes well to input sequences much longer than the ones seen during the\ntraining. We also show that HAM can be trained to act like classic data\nstructures: a stack, a FIFO queue and a priority queue.\n", "versions": [{"version": "v1", "created": "Tue, 9 Feb 2016 23:24:33 GMT"}, {"version": "v2", "created": "Tue, 23 Feb 2016 10:22:25 GMT"}], "update_date": "2016-02-24", "authors_parsed": [["Andrychowicz", "Marcin", ""], ["Kurach", "Karol", ""]]}, {"id": "1602.03220", "submitter": "Alex Lamb", "authors": "Alex Lamb, Vincent Dumoulin and Aaron Courville", "title": "Discriminative Regularization for Generative Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We explore the question of whether the representations learned by classifiers\ncan be used to enhance the quality of generative models. Our conjecture is that\nlabels correspond to characteristics of natural data which are most salient to\nhumans: identity in faces, objects in images, and utterances in speech. We\npropose to take advantage of this by using the representations from\ndiscriminative classifiers to augment the objective function corresponding to a\ngenerative model. In particular we enhance the objective function of the\nvariational autoencoder, a popular generative model, with a discriminative\nregularization term. We show that enhancing the objective function in this way\nleads to samples that are clearer and have higher visual quality than the\nsamples from the standard variational autoencoders.\n", "versions": [{"version": "v1", "created": "Tue, 9 Feb 2016 23:35:18 GMT"}, {"version": "v2", "created": "Thu, 11 Feb 2016 01:24:48 GMT"}, {"version": "v3", "created": "Fri, 12 Feb 2016 06:19:34 GMT"}, {"version": "v4", "created": "Mon, 15 Feb 2016 17:38:37 GMT"}], "update_date": "2016-02-16", "authors_parsed": [["Lamb", "Alex", ""], ["Dumoulin", "Vincent", ""], ["Courville", "Aaron", ""]]}, {"id": "1602.03258", "submitter": "Sharad Vikram", "authors": "Sharad Vikram, Sanjoy Dasgupta", "title": "Interactive Bayesian Hierarchical Clustering", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Clustering is a powerful tool in data analysis, but it is often difficult to\nfind a grouping that aligns with a user's needs. To address this, several\nmethods incorporate constraints obtained from users into clustering algorithms,\nbut unfortunately do not apply to hierarchical clustering. We design an\ninteractive Bayesian algorithm that incorporates user interaction into\nhierarchical clustering while still utilizing the geometry of the data by\nsampling a constrained posterior distribution over hierarchies. We also suggest\nseveral ways to intelligently query a user. The algorithm, along with the\nquerying schemes, shows promising results on real data.\n", "versions": [{"version": "v1", "created": "Wed, 10 Feb 2016 03:59:57 GMT"}, {"version": "v2", "created": "Fri, 12 Feb 2016 23:39:15 GMT"}, {"version": "v3", "created": "Wed, 27 Apr 2016 01:36:46 GMT"}], "update_date": "2016-04-28", "authors_parsed": [["Vikram", "Sharad", ""], ["Dasgupta", "Sanjoy", ""]]}, {"id": "1602.03264", "submitter": "Yang Lu", "authors": "Jianwen Xie, Yang Lu, Song-Chun Zhu, Ying Nian Wu", "title": "A Theory of Generative ConvNet", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We show that a generative random field model, which we call generative\nConvNet, can be derived from the commonly used discriminative ConvNet, by\nassuming a ConvNet for multi-category classification and assuming one of the\ncategories is a base category generated by a reference distribution. If we\nfurther assume that the non-linearity in the ConvNet is Rectified Linear Unit\n(ReLU) and the reference distribution is Gaussian white noise, then we obtain a\ngenerative ConvNet model that is unique among energy-based models: The model is\npiecewise Gaussian, and the means of the Gaussian pieces are defined by an\nauto-encoder, where the filters in the bottom-up encoding become the basis\nfunctions in the top-down decoding, and the binary activation variables\ndetected by the filters in the bottom-up convolution process become the\ncoefficients of the basis functions in the top-down deconvolution process. The\nLangevin dynamics for sampling the generative ConvNet is driven by the\nreconstruction error of this auto-encoder. The contrastive divergence learning\nof the generative ConvNet reconstructs the training images by the auto-encoder.\nThe maximum likelihood learning algorithm can synthesize realistic natural\nimage patterns.\n", "versions": [{"version": "v1", "created": "Wed, 10 Feb 2016 04:46:45 GMT"}, {"version": "v2", "created": "Sun, 29 May 2016 05:52:10 GMT"}, {"version": "v3", "created": "Tue, 31 May 2016 06:02:19 GMT"}], "update_date": "2016-06-01", "authors_parsed": [["Xie", "Jianwen", ""], ["Lu", "Yang", ""], ["Zhu", "Song-Chun", ""], ["Wu", "Ying Nian", ""]]}, {"id": "1602.03348", "submitter": "Daniel J Mankowitz", "authors": "Daniel J. Mankowitz, Timothy A. Mann, Shie Mannor", "title": "Iterative Hierarchical Optimization for Misspecified Problems (IHOMP)", "comments": "arXiv admin note: text overlap with arXiv:1506.03624", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For complex, high-dimensional Markov Decision Processes (MDPs), it may be\nnecessary to represent the policy with function approximation. A problem is\nmisspecified whenever, the representation cannot express any policy with\nacceptable performance. We introduce IHOMP : an approach for solving\nmisspecified problems. IHOMP iteratively learns a set of context specialized\noptions and combines these options to solve an otherwise misspecified problem.\nOur main contribution is proving that IHOMP enjoys theoretical convergence\nguarantees. In addition, we extend IHOMP to exploit Option Interruption (OI)\nenabling it to decide where the learned options can be reused. Our experiments\ndemonstrate that IHOMP can find near-optimal solutions to otherwise\nmisspecified problems and that OI can further improve the solutions.\n", "versions": [{"version": "v1", "created": "Wed, 10 Feb 2016 12:27:04 GMT"}, {"version": "v2", "created": "Tue, 7 Jun 2016 20:05:14 GMT"}], "update_date": "2016-06-09", "authors_parsed": [["Mankowitz", "Daniel J.", ""], ["Mann", "Timothy A.", ""], ["Mannor", "Shie", ""]]}, {"id": "1602.03351", "submitter": "Daniel J Mankowitz", "authors": "Daniel J. Mankowitz, Timothy A. Mann, Shie Mannor", "title": "Adaptive Skills, Adaptive Partitions (ASAP)", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce the Adaptive Skills, Adaptive Partitions (ASAP) framework that\n(1) learns skills (i.e., temporally extended actions or options) as well as (2)\nwhere to apply them. We believe that both (1) and (2) are necessary for a truly\ngeneral skill learning framework, which is a key building block needed to scale\nup to lifelong learning agents. The ASAP framework can also solve related new\ntasks simply by adapting where it applies its existing learned skills. We prove\nthat ASAP converges to a local optimum under natural conditions. Finally, our\nexperimental results, which include a RoboCup domain, demonstrate the ability\nof ASAP to learn where to reuse skills as well as solve multiple tasks with\nconsiderably less experience than solving each task from scratch.\n", "versions": [{"version": "v1", "created": "Wed, 10 Feb 2016 12:35:37 GMT"}, {"version": "v2", "created": "Tue, 7 Jun 2016 19:50:33 GMT"}], "update_date": "2016-06-08", "authors_parsed": [["Mankowitz", "Daniel J.", ""], ["Mann", "Timothy A.", ""], ["Mannor", "Shie", ""]]}, {"id": "1602.03368", "submitter": "Aydin Demircioglu", "authors": "Aydin Demircioglu, Daniel Horn, Tobias Glasmachers, Bernd Bischl,\n  Claus Weihs", "title": "Fast model selection by limiting SVM training times", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Kernelized Support Vector Machines (SVMs) are among the best performing\nsupervised learning methods. But for optimal predictive performance,\ntime-consuming parameter tuning is crucial, which impedes application. To\ntackle this problem, the classic model selection procedure based on grid-search\nand cross-validation was refined, e.g. by data subsampling and direct search\nheuristics. Here we focus on a different aspect, the stopping criterion for SVM\ntraining. We show that by limiting the training time given to the SVM solver\nduring parameter tuning we can reduce model selection times by an order of\nmagnitude.\n", "versions": [{"version": "v1", "created": "Wed, 10 Feb 2016 13:34:30 GMT"}], "update_date": "2016-02-11", "authors_parsed": [["Demircioglu", "Aydin", ""], ["Horn", "Daniel", ""], ["Glasmachers", "Tobias", ""], ["Bischl", "Bernd", ""], ["Weihs", "Claus", ""]]}, {"id": "1602.03476", "submitter": "Sewoong Oh", "authors": "Weihao Gao, Sreeram Kannan, Sewoong Oh, Pramod Viswanath", "title": "Conditional Dependence via Shannon Capacity: Axioms, Estimators and\n  Applications", "comments": "43 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.LG math.IT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We conduct an axiomatic study of the problem of estimating the strength of a\nknown causal relationship between a pair of variables. We propose that an\nestimate of causal strength should be based on the conditional distribution of\nthe effect given the cause (and not on the driving distribution of the cause),\nand study dependence measures on conditional distributions. Shannon capacity,\nappropriately regularized, emerges as a natural measure under these axioms. We\nexamine the problem of calculating Shannon capacity from the observed samples\nand propose a novel fixed-$k$ nearest neighbor estimator, and demonstrate its\nconsistency. Finally, we demonstrate an application to single-cell\nflow-cytometry, where the proposed estimators significantly reduce sample\ncomplexity.\n", "versions": [{"version": "v1", "created": "Wed, 10 Feb 2016 18:27:04 GMT"}, {"version": "v2", "created": "Fri, 12 Feb 2016 19:32:09 GMT"}, {"version": "v3", "created": "Thu, 2 Jun 2016 15:55:46 GMT"}], "update_date": "2016-06-03", "authors_parsed": [["Gao", "Weihao", ""], ["Kannan", "Sreeram", ""], ["Oh", "Sewoong", ""], ["Viswanath", "Pramod", ""]]}, {"id": "1602.03481", "submitter": "Sewoong Oh", "authors": "Ashish Khetan and Sewoong Oh", "title": "Achieving Budget-optimality with Adaptive Schemes in Crowdsourcing", "comments": "32 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.HC cs.SI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Crowdsourcing platforms provide marketplaces where task requesters can pay to\nget labels on their data. Such markets have emerged recently as popular venues\nfor collecting annotations that are crucial in training machine learning models\nin various applications. However, as jobs are tedious and payments are low,\nerrors are common in such crowdsourced labels. A common strategy to overcome\nsuch noise in the answers is to add redundancy by getting multiple answers for\neach task and aggregating them using some methods such as majority voting. For\nsuch a system, there is a fundamental question of interest: how can we maximize\nthe accuracy given a fixed budget on how many responses we can collect on the\ncrowdsourcing system. We characterize this fundamental trade-off between the\nbudget (how many answers the requester can collect in total) and the accuracy\nin the estimated labels. In particular, we ask whether adaptive task assignment\nschemes lead to a more efficient trade-off between the accuracy and the budget.\n  Adaptive schemes, where tasks are assigned adaptively based on the data\ncollected thus far, are widely used in practical crowdsourcing systems to\nefficiently use a given fixed budget. However, existing theoretical analyses of\ncrowdsourcing systems suggest that the gain of adaptive task assignments is\nminimal. To bridge this gap, we investigate this question under a strictly more\ngeneral probabilistic model, which has been recently introduced to model\npractical crowdsourced annotations. Under this generalized Dawid-Skene model,\nwe characterize the fundamental trade-off between budget and accuracy. We\nintroduce a novel adaptive scheme that matches this fundamental limit. We\nfurther quantify the fundamental gap between adaptive and non-adaptive schemes,\nby comparing the trade-off with the one for non-adaptive schemes. Our analyses\nconfirm that the gap is significant.\n", "versions": [{"version": "v1", "created": "Wed, 10 Feb 2016 18:46:30 GMT"}, {"version": "v2", "created": "Tue, 29 Nov 2016 04:31:25 GMT"}, {"version": "v3", "created": "Fri, 25 Aug 2017 16:35:55 GMT"}], "update_date": "2017-08-28", "authors_parsed": [["Khetan", "Ashish", ""], ["Oh", "Sewoong", ""]]}, {"id": "1602.03483", "submitter": "Felix Hill Mr", "authors": "Felix Hill, Kyunghyun Cho, Anna Korhonen", "title": "Learning Distributed Representations of Sentences from Unlabelled Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Unsupervised methods for learning distributed representations of words are\nubiquitous in today's NLP research, but far less is known about the best ways\nto learn distributed phrase or sentence representations from unlabelled data.\nThis paper is a systematic comparison of models that learn such\nrepresentations. We find that the optimal approach depends critically on the\nintended application. Deeper, more complex models are preferable for\nrepresentations to be used in supervised systems, but shallow log-linear models\nwork best for building representation spaces that can be decoded with simple\nspatial distance metrics. We also propose two new unsupervised\nrepresentation-learning objectives designed to optimise the trade-off between\ntraining time, domain portability and performance.\n", "versions": [{"version": "v1", "created": "Wed, 10 Feb 2016 18:49:58 GMT"}], "update_date": "2016-02-11", "authors_parsed": [["Hill", "Felix", ""], ["Cho", "Kyunghyun", ""], ["Korhonen", "Anna", ""]]}, {"id": "1602.03534", "submitter": "Ozan Sener", "authors": "Ozan Sener, Hyun Oh Song, Ashutosh Saxena, Silvio Savarese", "title": "Unsupervised Transductive Domain Adaptation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Supervised learning with large scale labeled datasets and deep layered models\nhas made a paradigm shift in diverse areas in learning and recognition.\nHowever, this approach still suffers generalization issues under the presence\nof a domain shift between the training and the test data distribution. In this\nregard, unsupervised domain adaptation algorithms have been proposed to\ndirectly address the domain shift problem. In this paper, we approach the\nproblem from a transductive perspective. We incorporate the domain shift and\nthe transductive target inference into our framework by jointly solving for an\nasymmetric similarity metric and the optimal transductive target label\nassignment. We also show that our model can easily be extended for deep feature\nlearning in order to learn features which are discriminative in the target\ndomain. Our experiments show that the proposed method significantly outperforms\nstate-of-the-art algorithms in both object recognition and digit classification\nexperiments by a large margin.\n", "versions": [{"version": "v1", "created": "Wed, 10 Feb 2016 21:07:23 GMT"}, {"version": "v2", "created": "Fri, 12 Feb 2016 22:37:36 GMT"}, {"version": "v3", "created": "Fri, 25 Mar 2016 16:47:54 GMT"}], "update_date": "2016-03-28", "authors_parsed": [["Sener", "Ozan", ""], ["Song", "Hyun Oh", ""], ["Saxena", "Ashutosh", ""], ["Savarese", "Silvio", ""]]}, {"id": "1602.03552", "submitter": "Jihun Hamm", "authors": "Jihun Hamm, Paul Cao, Mikhail Belkin", "title": "Learning Privately from Multiparty Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning a classifier from private data collected by multiple parties is an\nimportant problem that has many potential applications. How can we build an\naccurate and differentially private global classifier by combining\nlocally-trained classifiers from different parties, without access to any\nparty's private data? We propose to transfer the `knowledge' of the local\nclassifier ensemble by first creating labeled data from auxiliary unlabeled\ndata, and then train a global $\\epsilon$-differentially private classifier. We\nshow that majority voting is too sensitive and therefore propose a new risk\nweighted by class probabilities estimated from the ensemble. Relative to a\nnon-private solution, our private solution has a generalization error bounded\nby $O(\\epsilon^{-2}M^{-2})$ where $M$ is the number of parties. This allows\nstrong privacy without performance loss when $M$ is large, such as in\ncrowdsensing applications. We demonstrate the performance of our method with\nrealistic tasks of activity recognition, network intrusion detection, and\nmalicious URL detection.\n", "versions": [{"version": "v1", "created": "Wed, 10 Feb 2016 22:02:43 GMT"}], "update_date": "2016-02-12", "authors_parsed": [["Hamm", "Jihun", ""], ["Cao", "Paul", ""], ["Belkin", "Mikhail", ""]]}, {"id": "1602.03571", "submitter": "Anand Sarwate", "authors": "Tamir Hazan, Francesco Orabona, Anand D. Sarwate, Subhransu Maji,\n  Tommi Jaakkola", "title": "High Dimensional Inference with Random Maximum A-Posteriori\n  Perturbations", "comments": "47 pages, 10 figures, under review", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.IT math.IT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a new approach, called perturb-max, for high-dimensional\nstatistical inference that is based on applying random perturbations followed\nby optimization. This framework injects randomness to maximum a-posteriori\n(MAP) predictors by randomly perturbing the potential function for the input. A\nclassic result from extreme value statistics asserts that perturb-max\noperations generate unbiased samples from the Gibbs distribution using\nhigh-dimensional perturbations. Unfortunately, the computational cost of\ngenerating so many high-dimensional random variables can be prohibitive.\nHowever, when the perturbations are of low dimension, sampling the perturb-max\nprediction is as efficient as MAP optimization. This paper shows that the\nexpected value of perturb-max inference with low dimensional perturbations can\nbe used sequentially to generate unbiased samples from the Gibbs distribution.\nFurthermore the expected value of the maximal perturbations is a natural bound\non the entropy of such perturb-max models. A measure concentration result for\nperturb-max values shows that the deviation of their sampled average from its\nexpectation decays exponentially in the number of samples, allowing effective\napproximation of the expectation.\n", "versions": [{"version": "v1", "created": "Wed, 10 Feb 2016 23:15:39 GMT"}, {"version": "v2", "created": "Tue, 15 Nov 2016 22:56:18 GMT"}, {"version": "v3", "created": "Tue, 30 May 2017 18:20:26 GMT"}], "update_date": "2017-06-01", "authors_parsed": [["Hazan", "Tamir", ""], ["Orabona", "Francesco", ""], ["Sarwate", "Anand D.", ""], ["Maji", "Subhransu", ""], ["Jaakkola", "Tommi", ""]]}, {"id": "1602.03600", "submitter": "Onur Atan", "authors": "Onur Atan, Mihaela van der Schaar", "title": "Data-Driven Online Decision Making with Costly Information Acquisition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In most real-world settings such as recommender systems, finance, and\nhealthcare, collecting useful information is costly and requires an active\nchoice on the part of the decision maker. The decision-maker needs to learn\nsimultaneously what observations to make and what actions to take. This paper\nincorporates the information acquisition decision into an online learning\nframework. We propose two different algorithms for this dual learning problem:\nSim-OOS and Seq-OOS where observations are made simultaneously and\nsequentially, respectively. We prove that both algorithms achieve a regret that\nis sublinear in time. The developed framework and algorithms can be used in\nmany applications including medical informatics, recommender systems and\nactionable intelligence in transportation, finance, cyber-security etc., in\nwhich collecting information prior to making decisions is costly. We validate\nour algorithms in a breast cancer example setting in which we show substantial\nperformance gains for our proposed algorithms.\n", "versions": [{"version": "v1", "created": "Thu, 11 Feb 2016 01:43:22 GMT"}, {"version": "v2", "created": "Sat, 21 Oct 2017 01:15:31 GMT"}], "update_date": "2017-10-24", "authors_parsed": [["Atan", "Onur", ""], ["van der Schaar", "Mihaela", ""]]}, {"id": "1602.03609", "submitter": "Cicero dos Santos", "authors": "Cicero dos Santos, Ming Tan, Bing Xiang, Bowen Zhou", "title": "Attentive Pooling Networks", "comments": "10 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we propose Attentive Pooling (AP), a two-way attention\nmechanism for discriminative model training. In the context of pair-wise\nranking or classification with neural networks, AP enables the pooling layer to\nbe aware of the current input pair, in a way that information from the two\ninput items can directly influence the computation of each other's\nrepresentations. Along with such representations of the paired inputs, AP\njointly learns a similarity measure over projected segments (e.g. trigrams) of\nthe pair, and subsequently, derives the corresponding attention vector for each\ninput to guide the pooling. Our two-way attention mechanism is a general\nframework independent of the underlying representation learning, and it has\nbeen applied to both convolutional neural networks (CNNs) and recurrent neural\nnetworks (RNNs) in our studies. The empirical results, from three very\ndifferent benchmark tasks of question answering/answer selection, demonstrate\nthat our proposed models outperform a variety of strong baselines and achieve\nstate-of-the-art performance in all the benchmarks.\n", "versions": [{"version": "v1", "created": "Thu, 11 Feb 2016 03:06:33 GMT"}], "update_date": "2016-02-12", "authors_parsed": [["Santos", "Cicero dos", ""], ["Tan", "Ming", ""], ["Xiang", "Bing", ""], ["Zhou", "Bowen", ""]]}, {"id": "1602.03619", "submitter": "Jungseul Ok", "authors": "Jungseul Ok, Sewoong Oh, Jinwoo Shin and Yung Yi", "title": "Optimal Inference in Crowdsourced Classification via Belief Propagation", "comments": "This article is partially based on preliminary results published in\n  the proceeding of the 33rd International Conference on Machine Learning (ICML\n  2016)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Crowdsourcing systems are popular for solving large-scale labelling tasks\nwith low-paid workers. We study the problem of recovering the true labels from\nthe possibly erroneous crowdsourced labels under the popular Dawid-Skene model.\nTo address this inference problem, several algorithms have recently been\nproposed, but the best known guarantee is still significantly larger than the\nfundamental limit. We close this gap by introducing a tighter lower bound on\nthe fundamental limit and proving that Belief Propagation (BP) exactly matches\nthis lower bound. The guaranteed optimality of BP is the strongest in the sense\nthat it is information-theoretically impossible for any other algorithm to\ncorrectly label a larger fraction of the tasks. Experimental results suggest\nthat BP is close to optimal for all regimes considered and improves upon\ncompeting state-of-the-art algorithms.\n", "versions": [{"version": "v1", "created": "Thu, 11 Feb 2016 05:35:19 GMT"}, {"version": "v2", "created": "Thu, 10 Mar 2016 10:04:23 GMT"}, {"version": "v3", "created": "Wed, 19 Oct 2016 15:28:25 GMT"}, {"version": "v4", "created": "Thu, 12 Jan 2017 01:16:00 GMT"}], "update_date": "2017-01-13", "authors_parsed": [["Ok", "Jungseul", ""], ["Oh", "Sewoong", ""], ["Shin", "Jinwoo", ""], ["Yi", "Yung", ""]]}, {"id": "1602.03647", "submitter": "Jonathan Scarlett", "authors": "Jonathan Scarlett and Volkan Cevher", "title": "On the Difficulty of Selecting Ising Models with Approximate Recovery", "comments": "Accepted to IEEE Transactions on Signal and Information Processing\n  over Networks (TSIPN)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.LG cs.SI math.IT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we consider the problem of estimating the underlying graph\nassociated with an Ising model given a number of independent and identically\ndistributed samples. We adopt an \\emph{approximate recovery} criterion that\nallows for a number of missed edges or incorrectly-included edges, in contrast\nwith the widely-studied exact recovery problem. Our main results provide\ninformation-theoretic lower bounds on the sample complexity for graph classes\nimposing constraints on the number of edges, maximal degree, and other\nproperties. We identify a broad range of scenarios where, either up to constant\nfactors or logarithmic factors, our lower bounds match the best known lower\nbounds for the exact recovery criterion, several of which are known to be tight\nor near-tight. Hence, in these cases, approximate recovery has a similar\ndifficulty to exact recovery in the minimax sense.\n  Our bounds are obtained via a modification of Fano's inequality for handling\nthe approximate recovery criterion, along with suitably-designed ensembles of\ngraphs that can broadly be classed into two categories: (i) Those containing\ngraphs that contain several isolated edges or cliques and are thus difficult to\ndistinguish from the empty graph; (ii) Those containing graphs for which\ncertain groups of nodes are highly correlated, thus making it difficult to\ndetermine precisely which edges connect them. We support our theoretical\nresults on these ensembles with numerical experiments.\n", "versions": [{"version": "v1", "created": "Thu, 11 Feb 2016 09:25:24 GMT"}, {"version": "v2", "created": "Fri, 8 Jul 2016 15:30:47 GMT"}], "update_date": "2016-07-11", "authors_parsed": [["Scarlett", "Jonathan", ""], ["Cevher", "Volkan", ""]]}, {"id": "1602.03686", "submitter": "Edward Choi", "authors": "Edward Choi, Andy Schuetz, Walter F. Stewart, Jimeng Sun", "title": "Medical Concept Representation Learning from Electronic Health Records\n  and its Application on Heart Failure Prediction", "comments": "45 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Objective: To transform heterogeneous clinical data from electronic health\nrecords into clinically meaningful constructed features using data driven\nmethod that rely, in part, on temporal relations among data. Materials and\nMethods: The clinically meaningful representations of medical concepts and\npatients are the key for health analytic applications. Most of existing\napproaches directly construct features mapped to raw data (e.g., ICD or CPT\ncodes), or utilize some ontology mapping such as SNOMED codes. However, none of\nthe existing approaches leverage EHR data directly for learning such concept\nrepresentation. We propose a new way to represent heterogeneous medical\nconcepts (e.g., diagnoses, medications and procedures) based on co-occurrence\npatterns in longitudinal electronic health records. The intuition behind the\nmethod is to map medical concepts that are co-occuring closely in time to\nsimilar concept vectors so that their distance will be small. We also derive a\nsimple method to construct patient vectors from the related medical concept\nvectors. Results: For qualitative evaluation, we study similar medical concepts\nacross diagnosis, medication and procedure. In quantitative evaluation, our\nproposed representation significantly improves the predictive modeling\nperformance for onset of heart failure (HF), where classification methods (e.g.\nlogistic regression, neural network, support vector machine and K-nearest\nneighbors) achieve up to 23% improvement in area under the ROC curve (AUC)\nusing this proposed representation. Conclusion: We proposed an effective method\nfor patient and medical concept representation learning. The resulting\nrepresentation can map relevant concepts together and also improves predictive\nmodeling performance.\n", "versions": [{"version": "v1", "created": "Thu, 11 Feb 2016 12:03:42 GMT"}, {"version": "v2", "created": "Tue, 20 Jun 2017 22:28:50 GMT"}], "update_date": "2017-06-22", "authors_parsed": [["Choi", "Edward", ""], ["Schuetz", "Andy", ""], ["Stewart", "Walter F.", ""], ["Sun", "Jimeng", ""]]}, {"id": "1602.03779", "submitter": "Raphael F\\'eraud", "authors": "Rapha\\\"el F\\'eraud", "title": "Network of Bandits insure Privacy of end-users", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.DC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In order to distribute the best arm identification task as close as possible\nto the user's devices, on the edge of the Radio Access Network, we propose a\nnew problem setting, where distributed players collaborate to find the best\narm. This architecture guarantees privacy to end-users since no events are\nstored. The only thing that can be observed by an adversary through the core\nnetwork is aggregated information across users. We provide a first algorithm,\nDistributed Median Elimination, which is optimal in term of number of\ntransmitted bits and near optimal in term of speed-up factor with respect to an\noptimal algorithm run independently on each player. In practice, this first\nalgorithm cannot handle the trade-off between the communication cost and the\nspeed-up factor, and requires some knowledge about the distribution of players.\nExtended Distributed Median Elimination overcomes these limitations, by playing\nin parallel different instances of Distributed Median Elimination and selecting\nthe best one. Experiments illustrate and complete the analysis. According to\nthe analysis, in comparison to Median Elimination performed on each player, the\nproposed algorithm shows significant practical improvements.\n", "versions": [{"version": "v1", "created": "Thu, 11 Feb 2016 15:55:59 GMT"}, {"version": "v10", "created": "Mon, 5 Dec 2016 15:10:40 GMT"}, {"version": "v11", "created": "Sat, 17 Dec 2016 17:24:05 GMT"}, {"version": "v12", "created": "Mon, 6 Feb 2017 13:09:27 GMT"}, {"version": "v13", "created": "Mon, 20 Mar 2017 14:04:42 GMT"}, {"version": "v14", "created": "Wed, 29 Mar 2017 09:42:40 GMT"}, {"version": "v2", "created": "Sun, 14 Feb 2016 16:28:45 GMT"}, {"version": "v3", "created": "Mon, 22 Feb 2016 12:26:15 GMT"}, {"version": "v4", "created": "Tue, 1 Mar 2016 17:03:16 GMT"}, {"version": "v5", "created": "Wed, 30 Mar 2016 15:32:59 GMT"}, {"version": "v6", "created": "Tue, 26 Apr 2016 07:37:45 GMT"}, {"version": "v7", "created": "Mon, 6 Jun 2016 12:56:21 GMT"}, {"version": "v8", "created": "Mon, 19 Sep 2016 14:10:21 GMT"}, {"version": "v9", "created": "Tue, 11 Oct 2016 07:28:28 GMT"}], "update_date": "2017-03-30", "authors_parsed": [["F\u00e9raud", "Rapha\u00ebl", ""]]}, {"id": "1602.03808", "submitter": "Kwang In Kim", "authors": "Kwang In Kim and James Tompkin and Hanspeter Pfister and Christian\n  Theobalt", "title": "Semi-supervised Learning with Explicit Relationship Regularization", "comments": "Accepted version of paper published at CVPR 2015,\n  http://dx.doi.org/10.1109/CVPR.2015.7298831", "journal-ref": null, "doi": "10.1109/CVPR.2015.7298831", "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In many learning tasks, the structure of the target space of a function holds\nrich information about the relationships between evaluations of functions on\ndifferent data points. Existing approaches attempt to exploit this relationship\ninformation implicitly by enforcing smoothness on function evaluations only.\nHowever, what happens if we explicitly regularize the relationships between\nfunction evaluations? Inspired by homophily, we regularize based on a smooth\nrelationship function, either defined from the data or with labels. In\nexperiments, we demonstrate that this significantly improves the performance of\nstate-of-the-art algorithms in semi-supervised classification and in spectral\ndata embedding for constrained clustering and dimensionality reduction.\n", "versions": [{"version": "v1", "created": "Thu, 11 Feb 2016 18:08:58 GMT"}], "update_date": "2016-02-12", "authors_parsed": [["Kim", "Kwang In", ""], ["Tompkin", "James", ""], ["Pfister", "Hanspeter", ""], ["Theobalt", "Christian", ""]]}, {"id": "1602.03822", "submitter": "Robert Murphy", "authors": "Robert A. Murphy", "title": "A Critical Connectivity Radius for Randomly-Generated, High Dimensional\n  Data Points", "comments": "This paper is a combined replacement for the papers \"A Neural Network\n  Anomaly Detector using the Random Cluster Model\" (arXiv:1501.07227), \"On the\n  Sharp Threshold Interval Length of Partially Connected Random Geometric\n  Graphs During K-Means Classification\" (arXiv:1412.4178) and \"Estimating the\n  Mean Number of K-Means Clusters to Form\" (arXiv:1503.03488), which have all\n  been withdrawn", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We use random geometric graphs to describe clusters of higher dimensional\ndata points which are bijectively mapped to a (possibly) lower dimensional\nspace where an equivalent random cluster model is used to calculate the\nexpected number of modes to be found when separating the data of a multi-modal\ndata set into distinct clusters. Furthermore, as a function of the expected\nnumber of modes and the number of data points in the sample, an upper bound on\na given distance measure is found such that data points have the greatest\ncorrelation if their mutual distances from a common center is less than or\nequal to the calculated bound. Anomalies are exposed, which lie outside of the\nunion of all regularized clusters of data points. Finally, similarly to finding\na hyperplane which can be shifted along its normal to expose the maximal\ndistance between binary classes, it is shown that the union of regularized\nclusters can be used to define a hyperplane which can be shifted by a certain\namount to separate the data into binary classes.\n", "versions": [{"version": "v1", "created": "Thu, 11 Feb 2016 18:37:53 GMT"}, {"version": "v2", "created": "Fri, 12 Feb 2016 19:40:52 GMT"}, {"version": "v3", "created": "Wed, 24 Feb 2016 20:15:24 GMT"}, {"version": "v4", "created": "Sun, 10 Apr 2016 19:45:22 GMT"}, {"version": "v5", "created": "Tue, 19 Apr 2016 02:14:02 GMT"}, {"version": "v6", "created": "Mon, 5 Jul 2021 00:34:27 GMT"}], "update_date": "2021-07-06", "authors_parsed": [["Murphy", "Robert A.", ""]]}, {"id": "1602.03828", "submitter": "Yuxin Chen", "authors": "Yuxin Chen, Govinda Kamath, Changho Suh, David Tse", "title": "Community Recovery in Graphs with Locality", "comments": "accepted in part to International Conference on Machine Learning\n  (ICML), 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.LG cs.SI math.IT math.ST q-bio.GN stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motivated by applications in domains such as social networks and\ncomputational biology, we study the problem of community recovery in graphs\nwith locality. In this problem, pairwise noisy measurements of whether two\nnodes are in the same community or different communities come mainly or\nexclusively from nearby nodes rather than uniformly sampled between all nodes\npairs, as in most existing models. We present an algorithm that runs nearly\nlinearly in the number of measurements and which achieves the information\ntheoretic limit for exact recovery.\n", "versions": [{"version": "v1", "created": "Thu, 11 Feb 2016 19:13:20 GMT"}, {"version": "v2", "created": "Mon, 15 Feb 2016 19:26:32 GMT"}, {"version": "v3", "created": "Wed, 1 Jun 2016 18:18:04 GMT"}], "update_date": "2016-06-02", "authors_parsed": [["Chen", "Yuxin", ""], ["Kamath", "Govinda", ""], ["Suh", "Changho", ""], ["Tse", "David", ""]]}, {"id": "1602.03903", "submitter": "Marco Duarte", "authors": "Siwei Feng, Yuki Itoh, Mario Parente, and Marco F. Duarte", "title": "Wavelet-Based Semantic Features for Hyperspectral Signature\n  Discrimination", "comments": "21 pages, 8 figures, 4 tables, preprint, revised April 8 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hyperspectral signature classification is a quantitative analysis approach\nfor hyperspectral imagery which performs detection and classification of the\nconstituent materials at the pixel level in the scene. The classification\nprocedure can be operated directly on hyperspectral data or performed by using\nsome features extracted from the corresponding hyperspectral signatures\ncontaining information like the signature's energy or shape. In this paper, we\ndescribe a technique that applies non-homogeneous hidden Markov chain (NHMC)\nmodels to hyperspectral signature classification. The basic idea is to use\nstatistical models (such as NHMC) to characterize wavelet coefficients which\ncapture the spectrum semantics (i.e., structural information) at multiple\nlevels. Experimental results show that the approach based on NHMC models can\noutperform existing approaches relevant in classification tasks.\n", "versions": [{"version": "v1", "created": "Thu, 11 Feb 2016 21:25:36 GMT"}, {"version": "v2", "created": "Fri, 8 Apr 2016 19:50:27 GMT"}], "update_date": "2016-04-11", "authors_parsed": [["Feng", "Siwei", ""], ["Itoh", "Yuki", ""], ["Parente", "Mario", ""], ["Duarte", "Marco F.", ""]]}, {"id": "1602.03943", "submitter": "Naman Agarwal", "authors": "Naman Agarwal, Brian Bullins, Elad Hazan", "title": "Second-Order Stochastic Optimization for Machine Learning in Linear Time", "comments": null, "journal-ref": "Journal of Machine Learning Research 18(116) (2017) 1-40", "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  First-order stochastic methods are the state-of-the-art in large-scale\nmachine learning optimization owing to efficient per-iteration complexity.\nSecond-order methods, while able to provide faster convergence, have been much\nless explored due to the high cost of computing the second-order information.\nIn this paper we develop second-order stochastic methods for optimization\nproblems in machine learning that match the per-iteration cost of gradient\nbased methods, and in certain settings improve upon the overall running time\nover popular first-order methods. Furthermore, our algorithm has the desirable\nproperty of being implementable in time linear in the sparsity of the input\ndata.\n", "versions": [{"version": "v1", "created": "Fri, 12 Feb 2016 01:38:05 GMT"}, {"version": "v2", "created": "Fri, 26 Feb 2016 19:31:38 GMT"}, {"version": "v3", "created": "Tue, 15 Mar 2016 19:26:55 GMT"}, {"version": "v4", "created": "Fri, 14 Oct 2016 16:45:09 GMT"}, {"version": "v5", "created": "Thu, 30 Nov 2017 18:38:02 GMT"}], "update_date": "2017-12-01", "authors_parsed": [["Agarwal", "Naman", ""], ["Bullins", "Brian", ""], ["Hazan", "Elad", ""]]}, {"id": "1602.03950", "submitter": "Hong Zhao", "authors": "Hong Zhao", "title": "General Vector Machine", "comments": "57pages, 20 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The support vector machine (SVM) is an important class of learning machines\nfor function approach, pattern recognition, and time-serious prediction, etc.\nIt maps samples into the feature space by so-called support vectors of selected\nsamples, and then feature vectors are separated by maximum margin hyperplane.\nThe present paper presents the general vector machine (GVM) to replace the SVM.\nThe support vectors are replaced by general project vectors selected from the\nusual vector space, and a Monte Carlo (MC) algorithm is developed to find the\ngeneral vectors. The general project vectors improves the feature-extraction\nability, and the MC algorithm can control the width of the separation margin of\nthe hyperplane. By controlling the separation margin, we show that the maximum\nmargin hyperplane can usually induce the overlearning, and the best learning\nmachine is achieved with a proper separation margin. Applications in function\napproach, pattern recognition, and classification indicate that the developed\nmethod is very successful, particularly for small-set training problems.\nAdditionally, our algorithm may induce some particular applications, such as\nfor the transductive inference.\n", "versions": [{"version": "v1", "created": "Fri, 12 Feb 2016 02:55:34 GMT"}], "update_date": "2016-02-15", "authors_parsed": [["Zhao", "Hong", ""]]}, {"id": "1602.03992", "submitter": "Konstantinos Benidis", "authors": "Konstantinos Benidis, Ying Sun, Prabhu Babu, and Daniel P. Palomar", "title": "Orthogonal Sparse PCA and Covariance Estimation via Procrustes\n  Reformulation", "comments": null, "journal-ref": null, "doi": "10.1109/TSP.2016.2605073", "report-no": null, "categories": "stat.ML cs.LG math.OC stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The problem of estimating sparse eigenvectors of a symmetric matrix attracts\na lot of attention in many applications, especially those with high dimensional\ndata set. While classical eigenvectors can be obtained as the solution of a\nmaximization problem, existing approaches formulate this problem by adding a\npenalty term into the objective function that encourages a sparse solution.\nHowever, the resulting methods achieve sparsity at the expense of sacrificing\nthe orthogonality property. In this paper, we develop a new method to estimate\ndominant sparse eigenvectors without trading off their orthogonality. The\nproblem is highly non-convex and hard to handle. We apply the MM framework\nwhere we iteratively maximize a tight lower bound (surrogate function) of the\nobjective function over the Stiefel manifold. The inner maximization problem\nturns out to be a rectangular Procrustes problem, which has a closed form\nsolution. In addition, we propose a method to improve the covariance estimation\nproblem when its underlying eigenvectors are known to be sparse. We use the\neigenvalue decomposition of the covariance matrix to formulate an optimization\nproblem where we impose sparsity on the corresponding eigenvectors. Numerical\nexperiments show that the proposed eigenvector extraction algorithm matches or\noutperforms existing algorithms in terms of support recovery and explained\nvariance, while the covariance estimation algorithms improve significantly the\nsample covariance estimator.\n", "versions": [{"version": "v1", "created": "Fri, 12 Feb 2016 09:48:22 GMT"}], "update_date": "2016-11-03", "authors_parsed": [["Benidis", "Konstantinos", ""], ["Sun", "Ying", ""], ["Babu", "Prabhu", ""], ["Palomar", "Daniel P.", ""]]}, {"id": "1602.04062", "submitter": "Samantha Hansen", "authors": "Samantha Hansen", "title": "Using Deep Q-Learning to Control Optimization Hyperparameters", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a novel definition of the reinforcement learning state, actions\nand reward function that allows a deep Q-network (DQN) to learn to control an\noptimization hyperparameter. Using Q-learning with experience replay, we train\ntwo DQNs to accept a state representation of an objective function as input and\noutput the expected discounted return of rewards, or q-values, connected to the\nactions of either adjusting the learning rate or leaving it unchanged. The two\nDQNs learn a policy similar to a line search, but differ in the number of\nallowed actions. The trained DQNs in combination with a gradient-based update\nroutine form the basis of the Q-gradient descent algorithms. To demonstrate the\nviability of this framework, we show that the DQN's q-values associated with\noptimal action converge and that the Q-gradient descent algorithms outperform\ngradient descent with an Armijo or nonmonotone line search. Unlike traditional\noptimization methods, Q-gradient descent can incorporate any objective\nstatistic and by varying the actions we gain insight into the type of learning\nrate adjustment strategies that are successful for neural network optimization.\n", "versions": [{"version": "v1", "created": "Fri, 12 Feb 2016 14:16:59 GMT"}, {"version": "v2", "created": "Sun, 19 Jun 2016 20:21:08 GMT"}], "update_date": "2016-06-21", "authors_parsed": [["Hansen", "Samantha", ""]]}, {"id": "1602.04105", "submitter": "Timothy O'Shea", "authors": "Timothy J O'Shea, Johnathan Corgan, T. Charles Clancy", "title": "Convolutional Radio Modulation Recognition Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the adaptation of convolutional neural networks to the complex\ntemporal radio signal domain. We compare the efficacy of radio modulation\nclassification using naively learned features against using expert features\nwhich are widely used in the field today and we show significant performance\nimprovements. We show that blind temporal learning on large and densely encoded\ntime series using deep convolutional neural networks is viable and a strong\ncandidate approach for this task especially at low signal to noise ratio.\n", "versions": [{"version": "v1", "created": "Fri, 12 Feb 2016 16:28:59 GMT"}, {"version": "v2", "created": "Sun, 24 Apr 2016 21:38:29 GMT"}, {"version": "v3", "created": "Fri, 10 Jun 2016 21:44:09 GMT"}], "update_date": "2016-06-14", "authors_parsed": [["O'Shea", "Timothy J", ""], ["Corgan", "Johnathan", ""], ["Clancy", "T. Charles", ""]]}, {"id": "1602.04128", "submitter": "Francesco Orabona", "authors": "Francesco Orabona, D\\'avid P\\'al", "title": "Coin Betting and Parameter-Free Online Learning", "comments": "Fixed an compilation error in the latex", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the recent years, a number of parameter-free algorithms have been\ndeveloped for online linear optimization over Hilbert spaces and for learning\nwith expert advice. These algorithms achieve optimal regret bounds that depend\non the unknown competitors, without having to tune the learning rates with\noracle choices.\n  We present a new intuitive framework to design parameter-free algorithms for\n\\emph{both} online linear optimization over Hilbert spaces and for learning\nwith expert advice, based on reductions to betting on outcomes of adversarial\ncoins. We instantiate it using a betting algorithm based on the\nKrichevsky-Trofimov estimator. The resulting algorithms are simple, with no\nparameters to be tuned, and they improve or match previous results in terms of\nregret guarantee and per-round complexity.\n", "versions": [{"version": "v1", "created": "Fri, 12 Feb 2016 17:11:42 GMT"}, {"version": "v2", "created": "Mon, 27 Jun 2016 20:16:44 GMT"}, {"version": "v3", "created": "Fri, 28 Oct 2016 16:43:55 GMT"}, {"version": "v4", "created": "Fri, 4 Nov 2016 01:30:29 GMT"}], "update_date": "2016-11-07", "authors_parsed": [["Orabona", "Francesco", ""], ["P\u00e1l", "D\u00e1vid", ""]]}, {"id": "1602.04133", "submitter": "Thang Bui", "authors": "Thang D. Bui and Daniel Hern\\'andez-Lobato and Yingzhen Li and Jos\\'e\n  Miguel Hern\\'andez-Lobato and Richard E. Turner", "title": "Deep Gaussian Processes for Regression using Approximate Expectation\n  Propagation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep Gaussian processes (DGPs) are multi-layer hierarchical generalisations\nof Gaussian processes (GPs) and are formally equivalent to neural networks with\nmultiple, infinitely wide hidden layers. DGPs are nonparametric probabilistic\nmodels and as such are arguably more flexible, have a greater capacity to\ngeneralise, and provide better calibrated uncertainty estimates than\nalternative deep models. This paper develops a new approximate Bayesian\nlearning scheme that enables DGPs to be applied to a range of medium to large\nscale regression problems for the first time. The new method uses an\napproximate Expectation Propagation procedure and a novel and efficient\nextension of the probabilistic backpropagation algorithm for learning. We\nevaluate the new method for non-linear regression on eleven real-world\ndatasets, showing that it always outperforms GP regression and is almost always\nbetter than state-of-the-art deterministic and sampling-based approximate\ninference methods for Bayesian neural networks. As a by-product, this work\nprovides a comprehensive analysis of six approximate Bayesian methods for\ntraining neural networks.\n", "versions": [{"version": "v1", "created": "Fri, 12 Feb 2016 17:32:39 GMT"}], "update_date": "2016-02-15", "authors_parsed": [["Bui", "Thang D.", ""], ["Hern\u00e1ndez-Lobato", "Daniel", ""], ["Li", "Yingzhen", ""], ["Hern\u00e1ndez-Lobato", "Jos\u00e9 Miguel", ""], ["Turner", "Richard E.", ""]]}, {"id": "1602.04208", "submitter": "Martin Jaggi", "authors": "Rajiv Khanna, Michael Tschannen, Martin Jaggi", "title": "Pursuits in Structured Non-Convex Matrix Factorizations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Efficiently representing real world data in a succinct and parsimonious\nmanner is of central importance in many fields. We present a generalized greedy\npursuit framework, allowing us to efficiently solve structured matrix\nfactorization problems, where the factors are allowed to be from arbitrary sets\nof structured vectors. Such structure may include sparsity, non-negativeness,\norder, or a combination thereof. The algorithm approximates a given matrix by a\nlinear combination of few rank-1 matrices, each factorized into an outer\nproduct of two vector atoms of the desired structure. For the non-convex\nsubproblems of obtaining good rank-1 structured matrix atoms, we employ and\nanalyze a general atomic power method. In addition to the above applications,\nwe prove linear convergence for generalized pursuit variants in Hilbert spaces\n- for the task of approximation over the linear span of arbitrary dictionaries\n- which generalizes OMP and is useful beyond matrix problems. Our experiments\non real datasets confirm both the efficiency and also the broad applicability\nof our framework in practice.\n", "versions": [{"version": "v1", "created": "Fri, 12 Feb 2016 20:57:35 GMT"}], "update_date": "2016-02-15", "authors_parsed": [["Khanna", "Rajiv", ""], ["Tschannen", "Michael", ""], ["Jaggi", "Martin", ""]]}, {"id": "1602.04259", "submitter": "Viktoriya Krakovna", "authors": "Viktoriya Krakovna, Moshe Looks", "title": "A Minimalistic Approach to Sum-Product Network Learning for Real\n  Applications", "comments": "Accepted to ICLR 2016 workshop track", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sum-Product Networks (SPNs) are a class of expressive yet tractable\nhierarchical graphical models. LearnSPN is a structure learning algorithm for\nSPNs that uses hierarchical co-clustering to simultaneously identifying similar\nentities and similar features. The original LearnSPN algorithm assumes that all\nthe variables are discrete and there is no missing data. We introduce a\npractical, simplified version of LearnSPN, MiniSPN, that runs faster and can\nhandle missing data and heterogeneous features common in real applications. We\ndemonstrate the performance of MiniSPN on standard benchmark datasets and on\ntwo datasets from Google's Knowledge Graph exhibiting high missingness rates\nand a mix of discrete and continuous features.\n", "versions": [{"version": "v1", "created": "Fri, 12 Feb 2016 23:11:05 GMT"}, {"version": "v2", "created": "Thu, 24 Mar 2016 22:37:52 GMT"}, {"version": "v3", "created": "Sun, 24 Apr 2016 23:38:43 GMT"}], "update_date": "2016-04-26", "authors_parsed": [["Krakovna", "Viktoriya", ""], ["Looks", "Moshe", ""]]}, {"id": "1602.04265", "submitter": "Kam Chung Wong", "authors": "Kam Chung Wong, Zifan Li and Ambuj Tewari", "title": "Lasso Guarantees for Time Series Estimation Under Subgaussian Tails and\n  $ \\beta $-Mixing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many theoretical results on estimation of high dimensional time series\nrequire specifying an underlying data generating model (DGM). Instead, along\nthe footsteps of~\\cite{wong2017lasso}, this paper relies only on (strict)\nstationarity and $ \\beta $-mixing condition to establish consistency of lasso\nwhen data comes from a $\\beta$-mixing process with marginals having subgaussian\ntails. Because of the general assumptions, the data can come from DGMs\ndifferent than standard time series models such as VAR or ARCH. When the true\nDGM is not VAR, the lasso estimates correspond to those of the best linear\npredictors using the past observations. We establish non-asymptotic\ninequalities for estimation and prediction errors of the lasso estimates.\nTogether with~\\cite{wong2017lasso}, we provide lasso guarantees that cover full\nspectrum of the parameters in specifications of $ \\beta $-mixing subgaussian\ntime series. Applications of these results potentially extend to non-Gaussian,\nnon-Markovian and non-linear times series models as the examples we provide\ndemonstrate. In order to prove our results, we derive a novel Hanson-Wright\ntype concentration inequality for $\\beta$-mixing subgaussian random vectors\nthat may be of independent interest.\n", "versions": [{"version": "v1", "created": "Fri, 12 Feb 2016 23:44:53 GMT"}, {"version": "v2", "created": "Thu, 18 May 2017 18:20:17 GMT"}, {"version": "v3", "created": "Tue, 23 May 2017 17:52:08 GMT"}, {"version": "v4", "created": "Mon, 5 Feb 2018 17:42:51 GMT"}], "update_date": "2018-02-06", "authors_parsed": [["Wong", "Kam Chung", ""], ["Li", "Zifan", ""], ["Tewari", "Ambuj", ""]]}, {"id": "1602.04277", "submitter": "Renzhi Cao", "authors": "Renzhi Cao, Taeho Jo, Jianlin Cheng", "title": "Evaluation of Protein Structural Models Using Random Forests", "comments": "13 pages, 3 figures, 6 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG q-bio.BM q-bio.QM stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Protein structure prediction has been a grand challenge problem in the\nstructure biology over the last few decades. Protein quality assessment plays a\nvery important role in protein structure prediction. In the paper, we propose a\nnew protein quality assessment method which can predict both local and global\nquality of the protein 3D structural models. Our method uses both multi and\nsingle model quality assessment method for global quality assessment, and uses\nchemical, physical, geo-metrical features, and global quality score for local\nquality assessment. CASP9 targets are used to generate the features for local\nquality assessment. We evaluate the performance of our local quality assessment\nmethod on CASP10, which is comparable with two stage-of-art QA methods based on\nthe average absolute distance between the real and predicted distance. In\naddition, we blindly tested our method on CASP11, and the good performance\nshows that combining single and multiple model quality assessment method could\nbe a good way to improve the accuracy of model quality assessment, and the\nrandom forest technique could be used to train a good local quality assessment\nmodel.\n", "versions": [{"version": "v1", "created": "Sat, 13 Feb 2016 03:18:23 GMT"}], "update_date": "2016-02-20", "authors_parsed": [["Cao", "Renzhi", ""], ["Jo", "Taeho", ""], ["Cheng", "Jianlin", ""]]}, {"id": "1602.04282", "submitter": "Roshan Shariff", "authors": "Yifan Wu, Roshan Shariff, Tor Lattimore and Csaba Szepesv\\'ari", "title": "Conservative Bandits", "comments": "9 pages, plus 4-page appendix, with 3 figures. Submitted to ICML 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study a novel multi-armed bandit problem that models the challenge faced\nby a company wishing to explore new strategies to maximize revenue whilst\nsimultaneously maintaining their revenue above a fixed baseline, uniformly over\ntime. While previous work addressed the problem under the weaker requirement of\nmaintaining the revenue constraint only at a given fixed time in the future,\nthe algorithms previously proposed are unsuitable due to their design under the\nmore stringent constraints. We consider both the stochastic and the adversarial\nsettings, where we propose, natural, yet novel strategies and analyze the price\nfor maintaining the constraints. Amongst other things, we prove both high\nprobability and expectation bounds on the regret, while we also consider both\nthe problem of maintaining the constraints with high probability or\nexpectation. For the adversarial setting the price of maintaining the\nconstraint appears to be higher, at least for the algorithm considered. A lower\nbound is given showing that the algorithm for the stochastic setting is almost\noptimal. Empirical results obtained in synthetic environments complement our\ntheoretical findings.\n", "versions": [{"version": "v1", "created": "Sat, 13 Feb 2016 03:47:11 GMT"}], "update_date": "2016-02-16", "authors_parsed": [["Wu", "Yifan", ""], ["Shariff", "Roshan", ""], ["Lattimore", "Tor", ""], ["Szepesv\u00e1ri", "Csaba", ""]]}, {"id": "1602.04283", "submitter": "Griffin Lacey", "authors": "Griffin Lacey, Graham W. Taylor, Shawki Areibi", "title": "Deep Learning on FPGAs: Past, Present, and Future", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The rapid growth of data size and accessibility in recent years has\ninstigated a shift of philosophy in algorithm design for artificial\nintelligence. Instead of engineering algorithms by hand, the ability to learn\ncomposable systems automatically from massive amounts of data has led to\nground-breaking performance in important domains such as computer vision,\nspeech recognition, and natural language processing. The most popular class of\ntechniques used in these domains is called deep learning, and is seeing\nsignificant attention from industry. However, these models require incredible\namounts of data and compute power to train, and are limited by the need for\nbetter hardware acceleration to accommodate scaling beyond current data and\nmodel sizes. While the current solution has been to use clusters of graphics\nprocessing units (GPU) as general purpose processors (GPGPU), the use of field\nprogrammable gate arrays (FPGA) provide an interesting alternative. Current\ntrends in design tools for FPGAs have made them more compatible with the\nhigh-level software practices typically practiced in the deep learning\ncommunity, making FPGAs more accessible to those who build and deploy models.\nSince FPGA architectures are flexible, this could also allow researchers the\nability to explore model-level optimizations beyond what is possible on fixed\narchitectures such as GPUs. As well, FPGAs tend to provide high performance per\nwatt of power consumption, which is of particular importance for application\nscientists interested in large scale server-based deployment or\nresource-limited embedded applications. This review takes a look at deep\nlearning and FPGAs from a hardware acceleration perspective, identifying trends\nand innovations that make these technologies a natural fit, and motivates a\ndiscussion on how FPGAs may best serve the needs of the deep learning community\nmoving forward.\n", "versions": [{"version": "v1", "created": "Sat, 13 Feb 2016 03:50:37 GMT"}], "update_date": "2016-02-16", "authors_parsed": [["Lacey", "Griffin", ""], ["Taylor", "Graham W.", ""], ["Areibi", "Shawki", ""]]}, {"id": "1602.04287", "submitter": "Yu-Xiang Wang", "authors": "Yu-Xiang Wang, Jing Lei, Stephen E. Fienberg", "title": "A Minimax Theory for Adaptive Data Analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In adaptive data analysis, the user makes a sequence of queries on the data,\nwhere at each step the choice of query may depend on the results in previous\nsteps. The releases are often randomized in order to reduce overfitting for\nsuch adaptively chosen queries. In this paper, we propose a minimax framework\nfor adaptive data analysis. Assuming Gaussianity of queries, we establish the\nfirst sharp minimax lower bound on the squared error in the order of\n$O(\\frac{\\sqrt{k}\\sigma^2}{n})$, where $k$ is the number of queries asked, and\n$\\sigma^2/n$ is the ordinary signal-to-noise ratio for a single query. Our\nlower bound is based on the construction of an approximately least favorable\nadversary who picks a sequence of queries that are most likely to be affected\nby overfitting. This approximately least favorable adversary uses only one\nlevel of adaptivity, suggesting that the minimax risk for 1-step adaptivity\nwith k-1 initial releases and that for $k$-step adaptivity are on the same\norder. The key technical component of the lower bound proof is a reduction to\nfinding the convoluting distribution that optimally obfuscates the sign of a\nGaussian signal. Our lower bound construction also reveals a transparent and\nelementary proof of the matching upper bound as an alternative approach to\nRusso and Zou (2015), who used information-theoretic tools to provide the same\nupper bound. We believe that the proposed framework opens up opportunities to\nobtain theoretical insights for many other settings of adaptive data analysis,\nwhich would extend the idea to more practical realms.\n", "versions": [{"version": "v1", "created": "Sat, 13 Feb 2016 04:18:03 GMT"}], "update_date": "2016-02-16", "authors_parsed": [["Wang", "Yu-Xiang", ""], ["Lei", "Jing", ""], ["Fienberg", "Stephen E.", ""]]}, {"id": "1602.04302", "submitter": "Ganzhao Yuan", "authors": "Ganzhao Yuan, Yin Yang, Zhenjie Zhang, Zhifeng Hao", "title": "Convex Optimization for Linear Query Processing under Approximate\n  Differential Privacy", "comments": "to appear in ACM SIGKDD 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Differential privacy enables organizations to collect accurate aggregates\nover sensitive data with strong, rigorous guarantees on individuals' privacy.\nPrevious work has found that under differential privacy, computing multiple\ncorrelated aggregates as a batch, using an appropriate \\emph{strategy}, may\nyield higher accuracy than computing each of them independently. However,\nfinding the best strategy that maximizes result accuracy is non-trivial, as it\ninvolves solving a complex constrained optimization program that appears to be\nnon-linear and non-convex. Hence, in the past much effort has been devoted in\nsolving this non-convex optimization program. Existing approaches include\nvarious sophisticated heuristics and expensive numerical solutions. None of\nthem, however, guarantees to find the optimal solution of this optimization\nproblem.\n  This paper points out that under ($\\epsilon$, $\\delta$)-differential privacy,\nthe optimal solution of the above constrained optimization problem in search of\na suitable strategy can be found, rather surprisingly, by solving a simple and\nelegant convex optimization program. Then, we propose an efficient algorithm\nbased on Newton's method, which we prove to always converge to the optimal\nsolution with linear global convergence rate and quadratic local convergence\nrate. Empirical evaluations demonstrate the accuracy and efficiency of the\nproposed solution.\n", "versions": [{"version": "v1", "created": "Sat, 13 Feb 2016 08:31:14 GMT"}, {"version": "v2", "created": "Wed, 17 Feb 2016 06:51:59 GMT"}, {"version": "v3", "created": "Mon, 16 May 2016 23:20:13 GMT"}], "update_date": "2016-05-18", "authors_parsed": [["Yuan", "Ganzhao", ""], ["Yang", "Yin", ""], ["Zhang", "Zhenjie", ""], ["Hao", "Zhifeng", ""]]}, {"id": "1602.04364", "submitter": "Jimmy Ren", "authors": "Jimmy Ren, Yongtao Hu, Yu-Wing Tai, Chuan Wang, Li Xu, Wenxiu Sun,\n  Qiong Yan", "title": "Look, Listen and Learn - A Multimodal LSTM for Speaker Identification", "comments": "The 30th AAAI Conference on Artificial Intelligence (AAAI-16)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Speaker identification refers to the task of localizing the face of a person\nwho has the same identity as the ongoing voice in a video. This task not only\nrequires collective perception over both visual and auditory signals, the\nrobustness to handle severe quality degradations and unconstrained content\nvariations are also indispensable. In this paper, we describe a novel\nmultimodal Long Short-Term Memory (LSTM) architecture which seamlessly unifies\nboth visual and auditory modalities from the beginning of each sequence input.\nThe key idea is to extend the conventional LSTM by not only sharing weights\nacross time steps, but also sharing weights across modalities. We show that\nmodeling the temporal dependency across face and voice can significantly\nimprove the robustness to content quality degradations and variations. We also\nfound that our multimodal LSTM is robustness to distractors, namely the\nnon-speaking identities. We applied our multimodal LSTM to The Big Bang Theory\ndataset and showed that our system outperforms the state-of-the-art systems in\nspeaker identification with lower false alarm rate and higher recognition\naccuracy.\n", "versions": [{"version": "v1", "created": "Sat, 13 Feb 2016 18:49:50 GMT"}], "update_date": "2016-02-16", "authors_parsed": [["Ren", "Jimmy", ""], ["Hu", "Yongtao", ""], ["Tai", "Yu-Wing", ""], ["Wang", "Chuan", ""], ["Xu", "Li", ""], ["Sun", "Wenxiu", ""], ["Yan", "Qiong", ""]]}, {"id": "1602.04375", "submitter": "Mrinmaya Sachan", "authors": "Mrinmaya Sachan, Avinava Dubey, Eric P. Xing", "title": "Science Question Answering using Instructional Materials", "comments": "Corrected that the science QA dataset is NOT freely available", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We provide a solution for elementary science test using instructional\nmaterials. We posit that there is a hidden structure that explains the\ncorrectness of an answer given the question and instructional materials and\npresent a unified max-margin framework that learns to find these hidden\nstructures (given a corpus of question-answer pairs and instructional\nmaterials), and uses what it learns to answer novel elementary science\nquestions. Our evaluation shows that our framework outperforms several strong\nbaselines.\n", "versions": [{"version": "v1", "created": "Sat, 13 Feb 2016 20:13:48 GMT"}, {"version": "v2", "created": "Tue, 5 Apr 2016 01:17:56 GMT"}], "update_date": "2016-04-06", "authors_parsed": [["Sachan", "Mrinmaya", ""], ["Dubey", "Avinava", ""], ["Xing", "Eric P.", ""]]}, {"id": "1602.04398", "submitter": "Yanjun Li", "authors": "Yanjun Li, Yoram Bresler", "title": "Joint Dimensionality Reduction for Two Feature Vectors", "comments": "19 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.IT cs.LG math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many machine learning problems, especially multi-modal learning problems,\nhave two sets of distinct features (e.g., image and text features in news story\nclassification, or neuroimaging data and neurocognitive data in cognitive\nscience research). This paper addresses the joint dimensionality reduction of\ntwo feature vectors in supervised learning problems. In particular, we assume a\ndiscriminative model where low-dimensional linear embeddings of the two feature\nvectors are sufficient statistics for predicting a dependent variable. We show\nthat a simple algorithm involving singular value decomposition can accurately\nestimate the embeddings provided that certain sample complexities are\nsatisfied, without specifying the nonlinear link function (regressor or\nclassifier). The main results establish sample complexities under multiple\nsettings. Sample complexities for different link functions only differ by\nconstant factors.\n", "versions": [{"version": "v1", "created": "Sat, 13 Feb 2016 23:11:56 GMT"}, {"version": "v2", "created": "Tue, 16 Feb 2016 06:01:05 GMT"}, {"version": "v3", "created": "Mon, 31 Oct 2016 16:57:32 GMT"}], "update_date": "2016-11-01", "authors_parsed": [["Li", "Yanjun", ""], ["Bresler", "Yoram", ""]]}, {"id": "1602.04409", "submitter": "Julian Yarkony", "authors": "Julian Yarkony, Kamalika Chaudhuri", "title": "Convex Optimization For Non-Convex Problems via Column Generation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We apply column generation to approximating complex structured objects via a\nset of primitive structured objects under either the cross entropy or L2 loss.\nWe use L1 regularization to encourage the use of few structured primitive\nobjects. We attack approximation using convex optimization over an infinite\nnumber of variables each corresponding to a primitive structured object that\nare generated on demand by easy inference in the Lagrangian dual. We apply our\napproach to producing low rank approximations to large 3-way tensors.\n", "versions": [{"version": "v1", "created": "Sun, 14 Feb 2016 03:10:30 GMT"}], "update_date": "2016-02-16", "authors_parsed": [["Yarkony", "Julian", ""], ["Chaudhuri", "Kamalika", ""]]}, {"id": "1602.04418", "submitter": "Gunwoong Park", "authors": "Gunwoong Park and Garvesh Raskutti", "title": "Identifiability Assumptions and Algorithm for Directed Graphical Models\n  with Feedback", "comments": "28 pages, 17 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Directed graphical models provide a useful framework for modeling causal or\ndirectional relationships for multivariate data. Prior work has largely focused\non identifiability and search algorithms for directed acyclic graphical (DAG)\nmodels. In many applications, feedback naturally arises and directed graphical\nmodels that permit cycles occur. In this paper we address the issue of\nidentifiability for general directed cyclic graphical (DCG) models satisfying\nthe Markov assumption. In particular, in addition to the faithfulness\nassumption which has already been introduced for cyclic models, we introduce\ntwo new identifiability assumptions, one based on selecting the model with the\nfewest edges and the other based on selecting the DCG model that entails the\nmaximum number of d-separation rules. We provide theoretical results comparing\nthese assumptions which show that: (1) selecting models with the largest number\nof d-separation rules is strictly weaker than the faithfulness assumption; (2)\nunlike for DAG models, selecting models with the fewest edges does not\nnecessarily result in a milder assumption than the faithfulness assumption. We\nalso provide connections between our two new principles and minimality\nassumptions. We use our identifiability assumptions to develop search\nalgorithms for small-scale DCG models. Our simulation study supports our\ntheoretical results, showing that the algorithms based on our two new\nprinciples generally out-perform algorithms based on the faithfulness\nassumption in terms of selecting the true skeleton for DCG models.\n", "versions": [{"version": "v1", "created": "Sun, 14 Feb 2016 05:15:50 GMT"}, {"version": "v2", "created": "Wed, 6 Jul 2016 04:44:52 GMT"}], "update_date": "2016-07-07", "authors_parsed": [["Park", "Gunwoong", ""], ["Raskutti", "Garvesh", ""]]}, {"id": "1602.04433", "submitter": "Mingsheng Long", "authors": "Mingsheng Long, Han Zhu, Jianmin Wang, Michael I. Jordan", "title": "Unsupervised Domain Adaptation with Residual Transfer Networks", "comments": "30th Conference on Neural Information Processing Systems (NIPS 2016),\n  Barcelona, Spain", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The recent success of deep neural networks relies on massive amounts of\nlabeled data. For a target task where labeled data is unavailable, domain\nadaptation can transfer a learner from a different source domain. In this\npaper, we propose a new approach to domain adaptation in deep networks that can\njointly learn adaptive classifiers and transferable features from labeled data\nin the source domain and unlabeled data in the target domain. We relax a\nshared-classifier assumption made by previous methods and assume that the\nsource classifier and target classifier differ by a residual function. We\nenable classifier adaptation by plugging several layers into deep network to\nexplicitly learn the residual function with reference to the target classifier.\nWe fuse features of multiple layers with tensor product and embed them into\nreproducing kernel Hilbert spaces to match distributions for feature\nadaptation. The adaptation can be achieved in most feed-forward models by\nextending them with new residual layers and loss functions, which can be\ntrained efficiently via back-propagation. Empirical evidence shows that the new\napproach outperforms state of the art methods on standard domain adaptation\nbenchmarks.\n", "versions": [{"version": "v1", "created": "Sun, 14 Feb 2016 09:47:30 GMT"}, {"version": "v2", "created": "Thu, 16 Feb 2017 07:56:49 GMT"}], "update_date": "2017-02-17", "authors_parsed": [["Long", "Mingsheng", ""], ["Zhu", "Han", ""], ["Wang", "Jianmin", ""], ["Jordan", "Michael I.", ""]]}, {"id": "1602.04434", "submitter": "Andreas Loukas", "authors": "Andreas Loukas and Damien Foucard", "title": "Frequency Analysis of Temporal Graph Signals", "comments": "5 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.SY stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This letter extends the concept of graph-frequency to graph signals that\nevolve with time. Our goal is to generalize and, in fact, unify the familiar\nconcepts from time- and graph-frequency analysis. To this end, we study a joint\ntemporal and graph Fourier transform (JFT) and demonstrate its attractive\nproperties. We build on our results to create filters which act on the joint\n(temporal and graph) frequency domain, and show how these can be used to\nperform interference cancellation. The proposed algorithms are distributed,\nhave linear complexity, and can approximate any desired joint filtering\nobjective.\n", "versions": [{"version": "v1", "created": "Sun, 14 Feb 2016 09:48:56 GMT"}], "update_date": "2016-02-17", "authors_parsed": [["Loukas", "Andreas", ""], ["Foucard", "Damien", ""]]}, {"id": "1602.04435", "submitter": "Denis Sidorov", "authors": "A. Zhukov, D. Sidorov and A. Foley", "title": "Random Forest Based Approach for Concept Drift Handling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Concept drift has potential in smart grid analysis because the socio-economic\nbehaviour of consumers is not governed by the laws of physics. Likewise there\nare also applications in wind power forecasting. In this paper we present\ndecision tree ensemble classification method based on the Random Forest\nalgorithm for concept drift. The weighted majority voting ensemble aggregation\nrule is employed based on the ideas of Accuracy Weighted Ensemble (AWE) method.\nBase learner weight in our case is computed for each sample evaluation using\nbase learners accuracy and intrinsic proximity measure of Random Forest. Our\nalgorithm exploits both temporal weighting of samples and ensemble pruning as a\nforgetting strategy. We present results of empirical comparison of our method\nwith original random forest with incorporated \"replace-the-looser\" forgetting\nandother state-of-the-art concept-drfit classifiers like AWE2.\n", "versions": [{"version": "v1", "created": "Sun, 14 Feb 2016 09:58:39 GMT"}], "update_date": "2016-02-16", "authors_parsed": [["Zhukov", "A.", ""], ["Sidorov", "D.", ""], ["Foley", "A.", ""]]}, {"id": "1602.04436", "submitter": "Andreas Loukas", "authors": "Elvin Isufi and Andreas Loukas and Andrea Simonetto and Geert Leus", "title": "Autoregressive Moving Average Graph Filtering", "comments": null, "journal-ref": "IEEE Transactions on Signal Processing, vol. 67 (2), pages 274 -\n  288, 2017", "doi": "10.1109/TSP.2016.2614793", "report-no": null, "categories": "cs.LG cs.SY stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One of the cornerstones of the field of signal processing on graphs are graph\nfilters, direct analogues of classical filters, but intended for signals\ndefined on graphs. This work brings forth new insights on the distributed graph\nfiltering problem. We design a family of autoregressive moving average (ARMA)\nrecursions, which (i) are able to approximate any desired graph frequency\nresponse, and (ii) give exact solutions for tasks such as graph signal\ndenoising and interpolation. The design philosophy, which allows us to design\nthe ARMA coefficients independently from the underlying graph, renders the ARMA\ngraph filters suitable in static and, particularly, time-varying settings. The\nlatter occur when the graph signal and/or graph are changing over time. We show\nthat in case of a time-varying graph signal our approach extends naturally to a\ntwo-dimensional filter, operating concurrently in the graph and regular time\ndomains. We also derive sufficient conditions for filter stability when the\ngraph and signal are time-varying. The analytical and numerical results\npresented in this paper illustrate that ARMA graph filters are practically\nappealing for static and time-varying settings, as predicted by theoretical\nderivations.\n", "versions": [{"version": "v1", "created": "Sun, 14 Feb 2016 10:14:54 GMT"}, {"version": "v2", "created": "Wed, 21 Sep 2016 14:34:07 GMT"}], "update_date": "2017-09-18", "authors_parsed": [["Isufi", "Elvin", ""], ["Loukas", "Andreas", ""], ["Simonetto", "Andrea", ""], ["Leus", "Geert", ""]]}, {"id": "1602.04450", "submitter": "Felix Berkenkamp", "authors": "Felix Berkenkamp, Andreas Krause, Angela P. Schoellig", "title": "Bayesian Optimization with Safety Constraints: Safe and Automatic\n  Parameter Tuning in Robotics", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.LG cs.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Robotic algorithms typically depend on various parameters, the choice of\nwhich significantly affects the robot's performance. While an initial guess for\nthe parameters may be obtained from dynamic models of the robot, parameters are\nusually tuned manually on the real system to achieve the best performance.\nOptimization algorithms, such as Bayesian optimization, have been used to\nautomate this process. However, these methods may evaluate unsafe parameters\nduring the optimization process that lead to safety-critical system failures.\nRecently, a safe Bayesian optimization algorithm, called SafeOpt, has been\ndeveloped, which guarantees that the performance of the system never falls\nbelow a critical value; that is, safety is defined based on the performance\nfunction. However, coupling performance and safety is often not desirable in\nrobotics. For example, high-gain controllers might achieve low average tracking\nerror (performance), but can overshoot and violate input constraints. In this\npaper, we present a generalized algorithm that allows for multiple safety\nconstraints separate from the objective. Given an initial set of safe\nparameters, the algorithm maximizes performance but only evaluates parameters\nthat satisfy safety for all constraints with high probability. To this end, it\ncarefully explores the parameter space by exploiting regularity assumptions in\nterms of a Gaussian process prior. Moreover, we show how context variables can\nbe used to safely transfer knowledge to new situations and tasks. We provide a\ntheoretical analysis and demonstrate that the proposed algorithm enables fast,\nautomatic, and safe optimization of tuning parameters in experiments on a\nquadrotor vehicle.\n", "versions": [{"version": "v1", "created": "Sun, 14 Feb 2016 13:30:43 GMT"}, {"version": "v2", "created": "Fri, 2 Mar 2018 17:42:52 GMT"}, {"version": "v3", "created": "Tue, 7 Apr 2020 15:07:12 GMT"}], "update_date": "2020-04-08", "authors_parsed": [["Berkenkamp", "Felix", ""], ["Krause", "Andreas", ""], ["Schoellig", "Angela P.", ""]]}, {"id": "1602.04474", "submitter": "Alessandro Rudi", "authors": "Alessandro Rudi and Lorenzo Rosasco", "title": "Generalization Properties of Learning with Random Features", "comments": "NIPS 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the generalization properties of ridge regression with random\nfeatures in the statistical learning framework. We show for the first time that\n$O(1/\\sqrt{n})$ learning bounds can be achieved with only $O(\\sqrt{n}\\log n)$\nrandom features rather than $O({n})$ as suggested by previous results. Further,\nwe prove faster learning rates and show that they might require more random\nfeatures, unless they are sampled according to a possibly problem dependent\ndistribution. Our results shed light on the statistical computational\ntrade-offs in large scale kernelized learning, showing the potential\neffectiveness of random features in reducing the computational complexity while\nkeeping optimal generalization properties.\n", "versions": [{"version": "v1", "created": "Sun, 14 Feb 2016 16:26:39 GMT"}, {"version": "v2", "created": "Sat, 21 May 2016 08:43:32 GMT"}, {"version": "v3", "created": "Wed, 31 May 2017 07:04:58 GMT"}, {"version": "v4", "created": "Wed, 31 Jan 2018 17:43:18 GMT"}, {"version": "v5", "created": "Thu, 15 Apr 2021 09:03:18 GMT"}], "update_date": "2021-04-16", "authors_parsed": [["Rudi", "Alessandro", ""], ["Rosasco", "Lorenzo", ""]]}, {"id": "1602.04484", "submitter": "Phil Long", "authors": "David P. Helmbold and Philip M. Long", "title": "Surprising properties of dropout in deep networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.NE math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We analyze dropout in deep networks with rectified linear units and the\nquadratic loss. Our results expose surprising differences between the behavior\nof dropout and more traditional regularizers like weight decay. For example, on\nsome simple data sets dropout training produces negative weights even though\nthe output is the sum of the inputs. This provides a counterpoint to the\nsuggestion that dropout discourages co-adaptation of weights. We also show that\nthe dropout penalty can grow exponentially in the depth of the network while\nthe weight-decay penalty remains essentially linear, and that dropout is\ninsensitive to various re-scalings of the input features, outputs, and network\nweights. This last insensitivity implies that there are no isolated local\nminima of the dropout training criterion. Our work uncovers new properties of\ndropout, extends our understanding of why dropout succeeds, and lays the\nfoundation for further progress.\n", "versions": [{"version": "v1", "created": "Sun, 14 Feb 2016 18:20:29 GMT"}, {"version": "v2", "created": "Sat, 5 Mar 2016 23:00:10 GMT"}, {"version": "v3", "created": "Fri, 27 May 2016 23:24:17 GMT"}, {"version": "v4", "created": "Thu, 3 Nov 2016 16:39:19 GMT"}, {"version": "v5", "created": "Wed, 19 Apr 2017 21:15:15 GMT"}], "update_date": "2017-04-21", "authors_parsed": [["Helmbold", "David P.", ""], ["Long", "Philip M.", ""]]}, {"id": "1602.04485", "submitter": "Matus Telgarsky", "authors": "Matus Telgarsky", "title": "Benefits of depth in neural networks", "comments": "To appear, COLT 2016. For a simplified version, see\n  http://arxiv.org/abs/1509.08101", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For any positive integer $k$, there exist neural networks with $\\Theta(k^3)$\nlayers, $\\Theta(1)$ nodes per layer, and $\\Theta(1)$ distinct parameters which\ncan not be approximated by networks with $\\mathcal{O}(k)$ layers unless they\nare exponentially large --- they must possess $\\Omega(2^k)$ nodes. This result\nis proved here for a class of nodes termed \"semi-algebraic gates\" which\nincludes the common choices of ReLU, maximum, indicator, and piecewise\npolynomial functions, therefore establishing benefits of depth against not just\nstandard networks with ReLU gates, but also convolutional networks with ReLU\nand maximization gates, sum-product networks, and boosted decision trees (in\nthis last case with a stronger separation: $\\Omega(2^{k^3})$ total tree nodes\nare required).\n", "versions": [{"version": "v1", "created": "Sun, 14 Feb 2016 18:36:59 GMT"}, {"version": "v2", "created": "Fri, 27 May 2016 22:11:26 GMT"}], "update_date": "2016-05-31", "authors_parsed": [["Telgarsky", "Matus", ""]]}, {"id": "1602.04489", "submitter": "Aharon Bar Hillel", "authors": "Aharon Bar-Hillel and Eyal Krupka and Noam Bloom", "title": "Convolutional Tables Ensemble: classification in microseconds", "comments": "10 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study classifiers operating under severe classification time constraints,\ncorresponding to 1-1000 CPU microseconds, using Convolutional Tables Ensemble\n(CTE), an inherently fast architecture for object category recognition. The\narchitecture is based on convolutionally-applied sparse feature extraction,\nusing trees or ferns, and a linear voting layer. Several structure and\noptimization variants are considered, including novel decision functions, tree\nlearning algorithm, and distillation from CNN to CTE architecture. Accuracy\nimprovements of 24-45% over related art of similar speed are demonstrated on\nstandard object recognition benchmarks. Using Pareto speed-accuracy curves, we\nshow that CTE can provide better accuracy than Convolutional Neural Networks\n(CNN) for a certain range of classification time constraints, or alternatively\nprovide similar error rates with 5-200X speedup.\n", "versions": [{"version": "v1", "created": "Sun, 14 Feb 2016 19:21:17 GMT"}], "update_date": "2016-02-16", "authors_parsed": [["Bar-Hillel", "Aharon", ""], ["Krupka", "Eyal", ""], ["Bloom", "Noam", ""]]}, {"id": "1602.04511", "submitter": "Hongteng Xu", "authors": "Hongteng Xu and Mehrdad Farajtabar and Hongyuan Zha", "title": "Learning Granger Causality for Hawkes Processes", "comments": "International Conference on Machine Learning, 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning Granger causality for general point processes is a very challenging\ntask. In this paper, we propose an effective method, learning Granger\ncausality, for a special but significant type of point processes --- Hawkes\nprocess. We reveal the relationship between Hawkes process's impact function\nand its Granger causality graph. Specifically, our model represents impact\nfunctions using a series of basis functions and recovers the Granger causality\ngraph via group sparsity of the impact functions' coefficients. We propose an\neffective learning algorithm combining a maximum likelihood estimator (MLE)\nwith a sparse-group-lasso (SGL) regularizer. Additionally, the flexibility of\nour model allows to incorporate the clustering structure event types into\nlearning framework. We analyze our learning algorithm and propose an adaptive\nprocedure to select basis functions. Experiments on both synthetic and\nreal-world data show that our method can learn the Granger causality graph and\nthe triggering patterns of the Hawkes processes simultaneously.\n", "versions": [{"version": "v1", "created": "Sun, 14 Feb 2016 21:14:07 GMT"}, {"version": "v2", "created": "Sat, 11 Jun 2016 23:47:23 GMT"}], "update_date": "2016-06-14", "authors_parsed": [["Xu", "Hongteng", ""], ["Farajtabar", "Mehrdad", ""], ["Zha", "Hongyuan", ""]]}, {"id": "1602.04567", "submitter": "Changho Suh", "authors": "Changho Suh, Vincent Y. F. Tan, Renbo Zhao", "title": "Adversarial Top-$K$ Ranking", "comments": "32 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.IT cs.LG math.IT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the top-$K$ ranking problem where the goal is to recover the set of\ntop-$K$ ranked items out of a large collection of items based on partially\nrevealed preferences. We consider an adversarial crowdsourced setting where\nthere are two population sets, and pairwise comparison samples drawn from one\nof the populations follow the standard Bradley-Terry-Luce model (i.e., the\nchance of item $i$ beating item $j$ is proportional to the relative score of\nitem $i$ to item $j$), while in the other population, the corresponding chance\nis inversely proportional to the relative score. When the relative size of the\ntwo populations is known, we characterize the minimax limit on the sample size\nrequired (up to a constant) for reliably identifying the top-$K$ items, and\ndemonstrate how it scales with the relative size. Moreover, by leveraging a\ntensor decomposition method for disambiguating mixture distributions, we extend\nour result to the more realistic scenario in which the relative population size\nis unknown, thus establishing an upper bound on the fundamental limit of the\nsample size for recovering the top-$K$ set.\n", "versions": [{"version": "v1", "created": "Mon, 15 Feb 2016 06:22:59 GMT"}], "update_date": "2016-02-16", "authors_parsed": [["Suh", "Changho", ""], ["Tan", "Vincent Y. F.", ""], ["Zhao", "Renbo", ""]]}, {"id": "1602.04572", "submitter": "Viet Ha-Thuc", "authors": "Viet Ha-Thuc and Ganesh Venkataraman and Mario Rodriguez and Shakti\n  Sinha and Senthil Sundaram and Lin Guo", "title": "Personalized Expertise Search at LinkedIn", "comments": "2015 IEEE International Conference on Big Data", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  LinkedIn is the largest professional network with more than 350 million\nmembers. As the member base increases, searching for experts becomes more and\nmore challenging. In this paper, we propose an approach to address the problem\nof personalized expertise search on LinkedIn, particularly for exploratory\nsearch queries containing {\\it skills}. In the offline phase, we introduce a\ncollaborative filtering approach based on matrix factorization. Our approach\nestimates expertise scores for both the skills that members list on their\nprofiles as well as the skills they are likely to have but do not explicitly\nlist. In the online phase (at query time) we use expertise scores on these\nskills as a feature in combination with other features to rank the results. To\nlearn the personalized ranking function, we propose a heuristic to extract\ntraining data from search logs while handling position and sample selection\nbiases. We tested our models on two products - LinkedIn homepage and LinkedIn\nrecruiter. A/B tests showed significant improvements in click through rates -\n31% for CTR@1 for recruiter (18% for homepage) as well as downstream messages\nsent from search - 37% for recruiter (20% for homepage). As of writing this\npaper, these models serve nearly all live traffic for skills search on LinkedIn\nhomepage as well as LinkedIn recruiter.\n", "versions": [{"version": "v1", "created": "Mon, 15 Feb 2016 06:44:59 GMT"}], "update_date": "2016-02-16", "authors_parsed": [["Ha-Thuc", "Viet", ""], ["Venkataraman", "Ganesh", ""], ["Rodriguez", "Mario", ""], ["Sinha", "Shakti", ""], ["Sundaram", "Senthil", ""], ["Guo", "Lin", ""]]}, {"id": "1602.04579", "submitter": "Ichiro Takeuchi Prof.", "authors": "Toshiyuki Takada, Hiroyuki Hanada, Yoshiji Yamada, Jun Sakuma, Ichiro\n  Takeuchi", "title": "Secure Approximation Guarantee for Cryptographically Private Empirical\n  Risk Minimization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Privacy concern has been increasingly important in many machine learning (ML)\nproblems. We study empirical risk minimization (ERM) problems under secure\nmulti-party computation (MPC) frameworks. Main technical tools for MPC have\nbeen developed based on cryptography. One of limitations in current\ncryptographically private ML is that it is computationally intractable to\nevaluate non-linear functions such as logarithmic functions or exponential\nfunctions. Therefore, for a class of ERM problems such as logistic regression\nin which non-linear function evaluations are required, one can only obtain\napproximate solutions. In this paper, we introduce a novel cryptographically\nprivate tool called secure approximation guarantee (SAG) method. The key\nproperty of SAG method is that, given an arbitrary approximate solution, it can\nprovide a non-probabilistic assumption-free bound on the approximation quality\nunder cryptographically secure computation framework. We demonstrate the\nbenefit of the SAG method by applying it to several problems including a\npractical privacy-preserving data analysis task on genomic and clinical\ninformation.\n", "versions": [{"version": "v1", "created": "Mon, 15 Feb 2016 07:22:42 GMT"}], "update_date": "2016-02-16", "authors_parsed": [["Takada", "Toshiyuki", ""], ["Hanada", "Hiroyuki", ""], ["Yamada", "Yoshiji", ""], ["Sakuma", "Jun", ""], ["Takeuchi", "Ichiro", ""]]}, {"id": "1602.04589", "submitter": "Aurelien Garivier", "authors": "Aur\\'elien Garivier (IMT), Emilie Kaufmann (CRIStAL, SEQUEL)", "title": "Optimal Best Arm Identification with Fixed Confidence", "comments": "Conference on Learning Theory (COLT), Jun 2016, New York, United\n  States", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.LG stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We give a complete characterization of the complexity of best-arm\nidentification in one-parameter bandit problems. We prove a new, tight lower\nbound on the sample complexity. We propose the `Track-and-Stop' strategy, which\nwe prove to be asymptotically optimal. It consists in a new sampling rule\n(which tracks the optimal proportions of arm draws highlighted by the lower\nbound) and in a stopping rule named after Chernoff, for which we give a new\nanalysis.\n", "versions": [{"version": "v1", "created": "Mon, 15 Feb 2016 08:25:02 GMT"}, {"version": "v2", "created": "Wed, 1 Jun 2016 14:29:14 GMT"}], "update_date": "2016-06-02", "authors_parsed": [["Garivier", "Aur\u00e9lien", "", "IMT"], ["Kaufmann", "Emilie", "", "CRIStAL, SEQUEL"]]}, {"id": "1602.04605", "submitter": "Georg Pichler", "authors": "Georg Pichler, Pablo Piantanida and Gerald Matz", "title": "Distributed Information-Theoretic Clustering", "comments": "30 pages, 4 figures, 1 table; to be published in Information and\n  Inference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.LG math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study a novel multi-terminal source coding setup motivated by the\nbiclustering problem. Two separate encoders observe two i.i.d. sequences $X^n$\nand $Y^n$, respectively. The goal is to find rate-limited encodings $f(x^n)$\nand $g(z^n)$ that maximize the mutual information $I(f(X^n); g(Y^n))/n$. We\ndiscuss connections of this problem with hypothesis testing against\nindependence, pattern recognition, and the information bottleneck method.\nImproving previous cardinality bounds for the inner and outer bounds allows us\nto thoroughly study the special case of a binary symmetric source and to\nquantify the gap between the inner and the outer bound in this special case.\nFurthermore, we investigate a multiple description (MD) extension of the Chief\nOperating Officer (CEO) problem with mutual information constraint.\nSurprisingly, this MD-CEO problem permits a tight single-letter\ncharacterization of the achievable region.\n", "versions": [{"version": "v1", "created": "Mon, 15 Feb 2016 10:07:58 GMT"}, {"version": "v2", "created": "Thu, 7 Apr 2016 12:48:19 GMT"}, {"version": "v3", "created": "Sun, 19 Feb 2017 16:25:06 GMT"}, {"version": "v4", "created": "Fri, 17 Nov 2017 12:06:51 GMT"}, {"version": "v5", "created": "Sun, 2 Feb 2020 13:01:13 GMT"}, {"version": "v6", "created": "Sat, 8 May 2021 20:12:22 GMT"}], "update_date": "2021-05-11", "authors_parsed": [["Pichler", "Georg", ""], ["Piantanida", "Pablo", ""], ["Matz", "Gerald", ""]]}, {"id": "1602.04621", "submitter": "Ian Osband", "authors": "Ian Osband, Charles Blundell, Alexander Pritzel, Benjamin Van Roy", "title": "Deep Exploration via Bootstrapped DQN", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.SY stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Efficient exploration in complex environments remains a major challenge for\nreinforcement learning. We propose bootstrapped DQN, a simple algorithm that\nexplores in a computationally and statistically efficient manner through use of\nrandomized value functions. Unlike dithering strategies such as epsilon-greedy\nexploration, bootstrapped DQN carries out temporally-extended (or deep)\nexploration; this can lead to exponentially faster learning. We demonstrate\nthese benefits in complex stochastic MDPs and in the large-scale Arcade\nLearning Environment. Bootstrapped DQN substantially improves learning times\nand performance across most Atari games.\n", "versions": [{"version": "v1", "created": "Mon, 15 Feb 2016 10:54:20 GMT"}, {"version": "v2", "created": "Fri, 1 Jul 2016 16:23:55 GMT"}, {"version": "v3", "created": "Mon, 4 Jul 2016 17:11:52 GMT"}], "update_date": "2016-07-05", "authors_parsed": [["Osband", "Ian", ""], ["Blundell", "Charles", ""], ["Pritzel", "Alexander", ""], ["Van Roy", "Benjamin", ""]]}, {"id": "1602.04723", "submitter": "David Jacobs", "authors": "Ronen Basri and David Jacobs", "title": "Efficient Representation of Low-Dimensional Manifolds using Deep\n  Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the ability of deep neural networks to represent data that lies\nnear a low-dimensional manifold in a high-dimensional space. We show that deep\nnetworks can efficiently extract the intrinsic, low-dimensional coordinates of\nsuch data. We first show that the first two layers of a deep network can\nexactly embed points lying on a monotonic chain, a special type of piecewise\nlinear manifold, mapping them to a low-dimensional Euclidean space. Remarkably,\nthe network can do this using an almost optimal number of parameters. We also\nshow that this network projects nearby points onto the manifold and then embeds\nthem with little error. We then extend these results to more general manifolds.\n", "versions": [{"version": "v1", "created": "Mon, 15 Feb 2016 16:16:56 GMT"}], "update_date": "2016-02-16", "authors_parsed": [["Basri", "Ronen", ""], ["Jacobs", "David", ""]]}, {"id": "1602.04741", "submitter": "Claudio Gentile", "authors": "Nicolo' Cesa-Bianchi and Claudio Gentile and Yishay Mansour and\n  Alberto Minora", "title": "Delay and Cooperation in Nonstochastic Bandits", "comments": "30 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study networks of communicating learning agents that cooperate to solve a\ncommon nonstochastic bandit problem. Agents use an underlying communication\nnetwork to get messages about actions selected by other agents, and drop\nmessages that took more than $d$ hops to arrive, where $d$ is a delay\nparameter. We introduce \\textsc{Exp3-Coop}, a cooperative version of the {\\sc\nExp3} algorithm and prove that with $K$ actions and $N$ agents the average\nper-agent regret after $T$ rounds is at most of order $\\sqrt{\\bigl(d+1 +\n\\tfrac{K}{N}\\alpha_{\\le d}\\bigr)(T\\ln K)}$, where $\\alpha_{\\le d}$ is the\nindependence number of the $d$-th power of the connected communication graph\n$G$. We then show that for any connected graph, for $d=\\sqrt{K}$ the regret\nbound is $K^{1/4}\\sqrt{T}$, strictly better than the minimax regret $\\sqrt{KT}$\nfor noncooperating agents. More informed choices of $d$ lead to bounds which\nare arbitrarily close to the full information minimax regret $\\sqrt{T\\ln K}$\nwhen $G$ is dense. When $G$ has sparse components, we show that a variant of\n\\textsc{Exp3-Coop}, allowing agents to choose their parameters according to\ntheir centrality in $G$, strictly improves the regret. Finally, as a by-product\nof our analysis, we provide the first characterization of the minimax regret\nfor bandit learning with delay.\n", "versions": [{"version": "v1", "created": "Mon, 15 Feb 2016 17:20:28 GMT"}, {"version": "v2", "created": "Wed, 1 Jun 2016 16:20:30 GMT"}], "update_date": "2016-06-02", "authors_parsed": [["Cesa-Bianchi", "Nicolo'", ""], ["Gentile", "Claudio", ""], ["Mansour", "Yishay", ""], ["Minora", "Alberto", ""]]}, {"id": "1602.04799", "submitter": "Nathan Wiebe", "authors": "Nathan Wiebe, Ashish Kapoor, Krysta M Svore", "title": "Quantum Perceptron Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "quant-ph cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We demonstrate how quantum computation can provide non-trivial improvements\nin the computational and statistical complexity of the perceptron model. We\ndevelop two quantum algorithms for perceptron learning. The first algorithm\nexploits quantum information processing to determine a separating hyperplane\nusing a number of steps sublinear in the number of data points $N$, namely\n$O(\\sqrt{N})$. The second algorithm illustrates how the classical mistake bound\nof $O(\\frac{1}{\\gamma^2})$ can be further improved to\n$O(\\frac{1}{\\sqrt{\\gamma}})$ through quantum means, where $\\gamma$ denotes the\nmargin. Such improvements are achieved through the application of quantum\namplitude amplification to the version space interpretation of the perceptron\nmodel.\n", "versions": [{"version": "v1", "created": "Mon, 15 Feb 2016 20:45:35 GMT"}], "update_date": "2016-02-20", "authors_parsed": [["Wiebe", "Nathan", ""], ["Kapoor", "Ashish", ""], ["Svore", "Krysta M", ""]]}, {"id": "1602.04805", "submitter": "Jovana Mitrovic", "authors": "Jovana Mitrovic, Dino Sejdinovic, Yee Whye Teh", "title": "DR-ABC: Approximate Bayesian Computation with Kernel-Based Distribution\n  Regression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Performing exact posterior inference in complex generative models is often\ndifficult or impossible due to an expensive to evaluate or intractable\nlikelihood function. Approximate Bayesian computation (ABC) is an inference\nframework that constructs an approximation to the true likelihood based on the\nsimilarity between the observed and simulated data as measured by a predefined\nset of summary statistics. Although the choice of appropriate problem-specific\nsummary statistics crucially influences the quality of the likelihood\napproximation and hence also the quality of the posterior sample in ABC, there\nare only few principled general-purpose approaches to the selection or\nconstruction of such summary statistics. In this paper, we develop a novel\nframework for this task using kernel-based distribution regression. We model\nthe functional relationship between data distributions and the optimal choice\n(with respect to a loss function) of summary statistics using kernel-based\ndistribution regression. We show that our approach can be implemented in a\ncomputationally and statistically efficient way using the random Fourier\nfeatures framework for large-scale kernel learning. In addition to that, our\nframework shows superior performance when compared to related methods on toy\nand real-world problems.\n", "versions": [{"version": "v1", "created": "Mon, 15 Feb 2016 20:55:57 GMT"}], "update_date": "2016-02-16", "authors_parsed": [["Mitrovic", "Jovana", ""], ["Sejdinovic", "Dino", ""], ["Teh", "Yee Whye", ""]]}, {"id": "1602.04847", "submitter": "Sebastien Bubeck", "authors": "S\\'ebastien Bubeck and Yin-Tat Lee", "title": "Black-box optimization with a politician", "comments": "19 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.DS cs.LG cs.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new framework for black-box convex optimization which is\nwell-suited for situations where gradient computations are expensive. We derive\na new method for this framework which leverages several concepts from convex\noptimization, from standard first-order methods (e.g. gradient descent or\nquasi-Newton methods) to analytical centers (i.e. minimizers of self-concordant\nbarriers). We demonstrate empirically that our new technique compares favorably\nwith state of the art algorithms (such as BFGS).\n", "versions": [{"version": "v1", "created": "Mon, 15 Feb 2016 21:35:58 GMT"}], "update_date": "2016-02-17", "authors_parsed": [["Bubeck", "S\u00e9bastien", ""], ["Lee", "Yin-Tat", ""]]}, {"id": "1602.04874", "submitter": "Yushi Yao", "authors": "Yushi Yao, Zheng Huang", "title": "Bi-directional LSTM Recurrent Neural Network for Chinese Word\n  Segmentation", "comments": "2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recurrent neural network(RNN) has been broadly applied to natural language\nprocessing(NLP) problems. This kind of neural network is designed for modeling\nsequential data and has been testified to be quite efficient in sequential\ntagging tasks. In this paper, we propose to use bi-directional RNN with long\nshort-term memory(LSTM) units for Chinese word segmentation, which is a crucial\npreprocess task for modeling Chinese sentences and articles. Classical methods\nfocus on designing and combining hand-craft features from context, whereas\nbi-directional LSTM network(BLSTM) does not need any prior knowledge or\npre-designing, and it is expert in keeping the contextual information in both\ndirections. Experiment result shows that our approach gets state-of-the-art\nperformance in word segmentation on both traditional Chinese datasets and\nsimplified Chinese datasets.\n", "versions": [{"version": "v1", "created": "Tue, 16 Feb 2016 00:45:19 GMT"}], "update_date": "2016-02-22", "authors_parsed": [["Yao", "Yushi", ""], ["Huang", "Zheng", ""]]}, {"id": "1602.04889", "submitter": "Jordan Ash", "authors": "Jordan T. Ash, Robert E. Schapire and Barbara E. Engelhardt", "title": "Unsupervised Domain Adaptation Using Approximate Label Matching", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Domain adaptation addresses the problem created when training data is\ngenerated by a so-called source distribution, but test data is generated by a\nsignificantly different target distribution. In this work, we present\napproximate label matching (ALM), a new unsupervised domain adaptation\ntechnique that creates and leverages a rough labeling on the test samples, then\nuses these noisy labels to learn a transformation that aligns the source and\ntarget samples. We show that the transformation estimated by ALM has favorable\nproperties compared to transformations estimated by other methods, which do not\nuse any kind of target labeling. Our model is regularized by requiring that a\nclassifier trained to discriminate source from transformed target samples\ncannot distinguish between the two. We experiment with ALM on simulated and\nreal data, and show that it outperforms techniques commonly used in the field.\n", "versions": [{"version": "v1", "created": "Tue, 16 Feb 2016 02:38:25 GMT"}, {"version": "v2", "created": "Wed, 17 Feb 2016 05:25:20 GMT"}, {"version": "v3", "created": "Wed, 1 Mar 2017 19:17:35 GMT"}], "update_date": "2017-03-03", "authors_parsed": [["Ash", "Jordan T.", ""], ["Schapire", "Robert E.", ""], ["Engelhardt", "Barbara E.", ""]]}, {"id": "1602.04906", "submitter": "Junyan Wang", "authors": "Junyan Wang, Sai-kit Yeung, Jue Wang and Kun Zhou", "title": "Segmentation Rectification for Video Cutout via One-Class Structured\n  Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent works on interactive video object cutout mainly focus on designing\ndynamic foreground-background (FB) classifiers for segmentation propagation.\nHowever, the research on optimally removing errors from the FB classification\nis sparse, and the errors often accumulate rapidly, causing significant errors\nin the propagated frames. In this work, we take the initial steps to addressing\nthis problem, and we call this new task \\emph{segmentation rectification}. Our\nkey observation is that the possibly asymmetrically distributed false positive\nand false negative errors were handled equally in the conventional methods. We,\nalternatively, propose to optimally remove these two types of errors. To this\neffect, we propose a novel bilayer Markov Random Field (MRF) model for this new\ntask. We also adopt the well-established structured learning framework to learn\nthe optimal model from data. Additionally, we propose a novel one-class\nstructured SVM (OSSVM) which greatly speeds up the structured learning process.\nOur method naturally extends to RGB-D videos as well. Comprehensive experiments\non both RGB and RGB-D data demonstrate that our simple and effective method\nsignificantly outperforms the segmentation propagation methods adopted in the\nstate-of-the-art video cutout systems, and the results also suggest the\npotential usefulness of our method in image cutout system.\n", "versions": [{"version": "v1", "created": "Tue, 16 Feb 2016 04:31:20 GMT"}], "update_date": "2016-02-17", "authors_parsed": [["Wang", "Junyan", ""], ["Yeung", "Sai-kit", ""], ["Wang", "Jue", ""], ["Zhou", "Kun", ""]]}, {"id": "1602.04915", "submitter": "Jason Lee", "authors": "Jason D. Lee, Max Simchowitz, Michael I. Jordan, Benjamin Recht", "title": "Gradient Descent Converges to Minimizers", "comments": "Submitted to COLT 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We show that gradient descent converges to a local minimizer, almost surely\nwith random initialization. This is proved by applying the Stable Manifold\nTheorem from dynamical systems theory.\n", "versions": [{"version": "v1", "created": "Tue, 16 Feb 2016 05:43:31 GMT"}, {"version": "v2", "created": "Fri, 4 Mar 2016 08:04:58 GMT"}], "update_date": "2016-03-07", "authors_parsed": [["Lee", "Jason D.", ""], ["Simchowitz", "Max", ""], ["Jordan", "Michael I.", ""], ["Recht", "Benjamin", ""]]}, {"id": "1602.04924", "submitter": "Viet Ha-Thuc", "authors": "Dhruv Arya and Viet Ha-Thuc and Shakti Sinha", "title": "Personalized Federated Search at LinkedIn", "comments": "in Proceedings of the 24th ACM International on Conference on\n  Information and Knowledge Management (CIKM 2015)", "journal-ref": null, "doi": "10.1145/2806416.2806615", "report-no": null, "categories": "cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  LinkedIn has grown to become a platform hosting diverse sources of\ninformation ranging from member profiles, jobs, professional groups, slideshows\netc. Given the existence of multiple sources, when a member issues a query like\n\"software engineer\", the member could look for software engineer profiles, jobs\nor professional groups. To tackle this problem, we exploit a data-driven\napproach that extracts searcher intents from their profile data and recent\nactivities at a large scale. The intents such as job seeking, hiring, content\nconsuming are used to construct features to personalize federated search\nexperience. We tested the approach on the LinkedIn homepage and A/B tests show\nsignificant improvements in member engagement. As of writing this paper, the\napproach powers all of federated search on LinkedIn homepage.\n", "versions": [{"version": "v1", "created": "Tue, 16 Feb 2016 07:10:42 GMT"}], "update_date": "2016-02-17", "authors_parsed": [["Arya", "Dhruv", ""], ["Ha-Thuc", "Viet", ""], ["Sinha", "Shakti", ""]]}, {"id": "1602.04938", "submitter": "Marco Tulio Ribeiro", "authors": "Marco Tulio Ribeiro and Sameer Singh and Carlos Guestrin", "title": "\"Why Should I Trust You?\": Explaining the Predictions of Any Classifier", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite widespread adoption, machine learning models remain mostly black\nboxes. Understanding the reasons behind predictions is, however, quite\nimportant in assessing trust, which is fundamental if one plans to take action\nbased on a prediction, or when choosing whether to deploy a new model. Such\nunderstanding also provides insights into the model, which can be used to\ntransform an untrustworthy model or prediction into a trustworthy one. In this\nwork, we propose LIME, a novel explanation technique that explains the\npredictions of any classifier in an interpretable and faithful manner, by\nlearning an interpretable model locally around the prediction. We also propose\na method to explain models by presenting representative individual predictions\nand their explanations in a non-redundant way, framing the task as a submodular\noptimization problem. We demonstrate the flexibility of these methods by\nexplaining different models for text (e.g. random forests) and image\nclassification (e.g. neural networks). We show the utility of explanations via\nnovel experiments, both simulated and with human subjects, on various scenarios\nthat require trust: deciding if one should trust a prediction, choosing between\nmodels, improving an untrustworthy classifier, and identifying why a classifier\nshould not be trusted.\n", "versions": [{"version": "v1", "created": "Tue, 16 Feb 2016 08:20:14 GMT"}, {"version": "v2", "created": "Wed, 3 Aug 2016 22:30:58 GMT"}, {"version": "v3", "created": "Tue, 9 Aug 2016 17:54:52 GMT"}], "update_date": "2016-08-10", "authors_parsed": [["Ribeiro", "Marco Tulio", ""], ["Singh", "Sameer", ""], ["Guestrin", "Carlos", ""]]}, {"id": "1602.04951", "submitter": "Anna Harutyunyan", "authors": "Anna Harutyunyan and Marc G. Bellemare and Tom Stepleton and Remi\n  Munos", "title": "Q($\\lambda$) with Off-Policy Corrections", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose and analyze an alternate approach to off-policy multi-step\ntemporal difference learning, in which off-policy returns are corrected with\nthe current Q-function in terms of rewards, rather than with the target policy\nin terms of transition probabilities. We prove that such approximate\ncorrections are sufficient for off-policy convergence both in policy evaluation\nand control, provided certain conditions. These conditions relate the distance\nbetween the target and behavior policies, the eligibility trace parameter and\nthe discount factor, and formalize an underlying tradeoff in off-policy\nTD($\\lambda$). We illustrate this theoretical relationship empirically on a\ncontinuous-state control task.\n", "versions": [{"version": "v1", "created": "Tue, 16 Feb 2016 09:09:56 GMT"}, {"version": "v2", "created": "Thu, 11 Aug 2016 09:40:12 GMT"}], "update_date": "2016-08-12", "authors_parsed": [["Harutyunyan", "Anna", ""], ["Bellemare", "Marc G.", ""], ["Stepleton", "Tom", ""], ["Munos", "Remi", ""]]}, {"id": "1602.04976", "submitter": "Emile Contal", "authors": "Emile Contal and Nicolas Vayatis", "title": "Stochastic Process Bandits: Upper Confidence Bounds Algorithms via\n  Generic Chaining", "comments": "preprint", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The paper considers the problem of global optimization in the setup of\nstochastic process bandits. We introduce an UCB algorithm which builds a\ncascade of discretization trees based on generic chaining in order to render\npossible his operability over a continuous domain. The theoretical framework\napplies to functions under weak probabilistic smoothness assumptions and also\nextends significantly the spectrum of application of UCB strategies. Moreover\ngeneric regret bounds are derived which are then specialized to Gaussian\nprocesses indexed on infinite-dimensional spaces as well as to quadratic forms\nof Gaussian processes. Lower bounds are also proved in the case of Gaussian\nprocesses to assess the optimality of the proposed algorithm.\n", "versions": [{"version": "v1", "created": "Tue, 16 Feb 2016 10:48:28 GMT"}], "update_date": "2016-02-22", "authors_parsed": [["Contal", "Emile", ""], ["Vayatis", "Nicolas", ""]]}, {"id": "1602.05012", "submitter": "Jaroslav Fowkes", "authors": "Jaroslav Fowkes and Charles Sutton", "title": "A Subsequence Interleaving Model for Sequential Pattern Mining", "comments": "10 pages in KDD 2016: Proceedings of the 22nd ACM SIGKDD\n  International Conference on Knowledge Discovery and Data Mining", "journal-ref": null, "doi": "10.1145/2939672.2939787", "report-no": null, "categories": "stat.ML cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent sequential pattern mining methods have used the minimum description\nlength (MDL) principle to define an encoding scheme which describes an\nalgorithm for mining the most compressing patterns in a database. We present a\nnovel subsequence interleaving model based on a probabilistic model of the\nsequence database, which allows us to search for the most compressing set of\npatterns without designing a specific encoding scheme. Our proposed algorithm\nis able to efficiently mine the most relevant sequential patterns and rank them\nusing an associated measure of interestingness. The efficient inference in our\nmodel is a direct result of our use of a structural expectation-maximization\nframework, in which the expectation-step takes the form of a submodular\noptimization problem subject to a coverage constraint. We show on both\nsynthetic and real world datasets that our model mines a set of sequential\npatterns with low spuriousness and redundancy, high interpretability and\nusefulness in real-world applications. Furthermore, we demonstrate that the\nquality of the patterns from our approach is comparable to, if not better than,\nexisting state of the art sequential pattern mining algorithms.\n", "versions": [{"version": "v1", "created": "Tue, 16 Feb 2016 13:30:10 GMT"}, {"version": "v2", "created": "Fri, 11 Nov 2016 10:43:36 GMT"}], "update_date": "2016-11-14", "authors_parsed": [["Fowkes", "Jaroslav", ""], ["Sutton", "Charles", ""]]}, {"id": "1602.05110", "submitter": "Daniel Jiwoong Im", "authors": "Daniel Jiwoong Im, Chris Dongjoo Kim, Hui Jiang, Roland Memisevic", "title": "Generating images with recurrent adversarial networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Gatys et al. (2015) showed that optimizing pixels to match features in a\nconvolutional network with respect reference image features is a way to render\nimages of high visual quality. We show that unrolling this gradient-based\noptimization yields a recurrent computation that creates images by\nincrementally adding onto a visual \"canvas\". We propose a recurrent generative\nmodel inspired by this view, and show that it can be trained using adversarial\ntraining to generate very good image samples. We also propose a way to\nquantitatively compare adversarial networks by having the generators and\ndiscriminators of these networks compete against each other.\n", "versions": [{"version": "v1", "created": "Tue, 16 Feb 2016 17:51:39 GMT"}, {"version": "v2", "created": "Wed, 17 Feb 2016 14:41:52 GMT"}, {"version": "v3", "created": "Sun, 10 Apr 2016 19:17:27 GMT"}, {"version": "v4", "created": "Sun, 29 May 2016 01:17:59 GMT"}, {"version": "v5", "created": "Tue, 13 Dec 2016 03:21:03 GMT"}], "update_date": "2016-12-14", "authors_parsed": [["Im", "Daniel Jiwoong", ""], ["Kim", "Chris Dongjoo", ""], ["Jiang", "Hui", ""], ["Memisevic", "Roland", ""]]}, {"id": "1602.05112", "submitter": "Hongteng Xu", "authors": "Hongteng Xu and Weichang Wu and Shamim Nemati and Hongyuan Zha", "title": "Patient Flow Prediction via Discriminative Learning of\n  Mutually-Correcting Processes", "comments": "in IEEE Transactions on Knowledge and Data Engineering (TKDE), 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Over the past decade the rate of care unit (CU) use in the United States has\nbeen increasing. With an aging population and ever-growing demand for medical\ncare, effective management of patients' transitions among different care\nfacilities will prove indispensible for shortening the length of hospital\nstays, improving patient outcomes, allocating critical care resources, and\nreducing preventable re-admissions. In this paper, we focus on an important\nproblem of predicting the so-called \"patient flow\" from longitudinal electronic\nhealth records (EHRs), which has not been explored via existing machine\nlearning techniques. By treating a sequence of transition events as a point\nprocess, we develop a novel framework for modeling patient flow through various\nCUs and jointly predicting patients' destination CUs and duration days. Instead\nof learning a generative point process model via maximum likelihood estimation,\nwe propose a novel discriminative learning algorithm aiming at improving the\nprediction of transition events in the case of sparse data. By parameterizing\nthe proposed model as a mutually-correcting process, we formulate the\nestimation problem via generalized linear models, which lends itself to\nefficient learning based on alternating direction method of multipliers (ADMM).\nFurthermore, we achieve simultaneous feature selection and learning by adding a\ngroup-lasso regularizer to the ADMM algorithm. Additionally, for suppressing\nthe negative influence of data imbalance on the learning of model, we\nsynthesize auxiliary training data for the classes with extremely few samples,\nand improve the robustness of our learning method accordingly. Testing on\nreal-world data, we show that our method obtains superior performance in terms\nof accuracy of predicting the destination CU transition and duration of each CU\noccupancy.\n", "versions": [{"version": "v1", "created": "Sun, 14 Feb 2016 21:29:29 GMT"}, {"version": "v2", "created": "Mon, 18 Jul 2016 12:13:15 GMT"}, {"version": "v3", "created": "Thu, 10 Nov 2016 05:18:26 GMT"}], "update_date": "2016-11-11", "authors_parsed": [["Xu", "Hongteng", ""], ["Wu", "Weichang", ""], ["Nemati", "Shamim", ""], ["Zha", "Hongyuan", ""]]}, {"id": "1602.05124", "submitter": "Alexander K. Hartmann", "authors": "Alexander K. Hartmann", "title": "Practical Introduction to Clustering Data", "comments": "22 pages. All source code in anc directory included. Section 8.5.6 of\n  book: A.K. Hartmann, Big Practical Guide to Computer Simulations,\n  World-Scientifc, Singapore (2015)", "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.data-an astro-ph.IM cond-mat.stat-mech cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data clustering is an approach to seek for structure in sets of complex data,\ni.e., sets of \"objects\". The main objective is to identify groups of objects\nwhich are similar to each other, e.g., for classification. Here, an\nintroduction to clustering is given and three basic approaches are introduced:\nthe k-means algorithm, neighbour-based clustering, and an agglomerative\nclustering method. For all cases, C source code examples are given, allowing\nfor an easy implementation.\n", "versions": [{"version": "v1", "created": "Tue, 16 Feb 2016 18:26:55 GMT"}], "update_date": "2016-02-17", "authors_parsed": [["Hartmann", "Alexander K.", ""]]}, {"id": "1602.05127", "submitter": "Da Kuang", "authors": "Da Kuang, Zuoqiang Shi, Stanley Osher, Andrea Bertozzi", "title": "A Harmonic Extension Approach for Collaborative Ranking", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a new perspective on graph-based methods for collaborative ranking\nfor recommender systems. Unlike user-based or item-based methods that compute a\nweighted average of ratings given by the nearest neighbors, or low-rank\napproximation methods using convex optimization and the nuclear norm, we\nformulate matrix completion as a series of semi-supervised learning problems,\nand propagate the known ratings to the missing ones on the user-user or\nitem-item graph globally. The semi-supervised learning problems are expressed\nas Laplace-Beltrami equations on a manifold, or namely, harmonic extension, and\ncan be discretized by a point integral method. We show that our approach does\nnot impose a low-rank Euclidean subspace on the data points, but instead\nminimizes the dimension of the underlying manifold. Our method, named LDM (low\ndimensional manifold), turns out to be particularly effective in generating\nrankings of items, showing decent computational efficiency and robust ranking\nquality compared to state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Tue, 16 Feb 2016 18:35:25 GMT"}], "update_date": "2016-02-17", "authors_parsed": [["Kuang", "Da", ""], ["Shi", "Zuoqiang", ""], ["Osher", "Stanley", ""], ["Bertozzi", "Andrea", ""]]}, {"id": "1602.05161", "submitter": "Ran Raz", "authors": "Ran Raz", "title": "Fast Learning Requires Good Memory: A Time-Space Lower Bound for Parity\n  Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CC cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We prove that any algorithm for learning parities requires either a memory of\nquadratic size or an exponential number of samples. This proves a recent\nconjecture of Steinhardt, Valiant and Wager and shows that for some learning\nproblems a large storage space is crucial.\n  More formally, in the problem of parity learning, an unknown string $x \\in\n\\{0,1\\}^n$ was chosen uniformly at random. A learner tries to learn $x$ from a\nstream of samples $(a_1, b_1), (a_2, b_2) \\ldots$, where each~$a_t$ is\nuniformly distributed over $\\{0,1\\}^n$ and $b_t$ is the inner product of $a_t$\nand $x$, modulo~2. We show that any algorithm for parity learning, that uses\nless than $\\frac{n^2}{25}$ bits of memory, requires an exponential number of\nsamples.\n  Previously, there was no non-trivial lower bound on the number of samples\nneeded, for any learning problem, even if the allowed memory size is $O(n)$\n(where $n$ is the space needed to store one sample).\n  We also give an application of our result in the field of bounded-storage\ncryptography. We show an encryption scheme that requires a private key of\nlength $n$, as well as time complexity of $n$ per encryption/decription of each\nbit, and is provenly and unconditionally secure as long as the attacker uses\nless than $\\frac{n^2}{25}$ memory bits and the scheme is used at most an\nexponential number of times. Previous works on bounded-storage cryptography\nassumed that the memory size used by the attacker is at most linear in the time\nneeded for encryption/decription.\n", "versions": [{"version": "v1", "created": "Tue, 16 Feb 2016 20:13:55 GMT"}], "update_date": "2016-02-17", "authors_parsed": [["Raz", "Ran", ""]]}, {"id": "1602.05179", "submitter": "Benjamin Scellier", "authors": "Benjamin Scellier and Yoshua Bengio", "title": "Equilibrium Propagation: Bridging the Gap Between Energy-Based Models\n  and Backpropagation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce Equilibrium Propagation, a learning framework for energy-based\nmodels. It involves only one kind of neural computation, performed in both the\nfirst phase (when the prediction is made) and the second phase of training\n(after the target or prediction error is revealed). Although this algorithm\ncomputes the gradient of an objective function just like Backpropagation, it\ndoes not need a special computation or circuit for the second phase, where\nerrors are implicitly propagated. Equilibrium Propagation shares similarities\nwith Contrastive Hebbian Learning and Contrastive Divergence while solving the\ntheoretical issues of both algorithms: our algorithm computes the gradient of a\nwell defined objective function. Because the objective function is defined in\nterms of local perturbations, the second phase of Equilibrium Propagation\ncorresponds to only nudging the prediction (fixed point, or stationary\ndistribution) towards a configuration that reduces prediction error. In the\ncase of a recurrent multi-layer supervised network, the output units are\nslightly nudged towards their target in the second phase, and the perturbation\nintroduced at the output layer propagates backward in the hidden layers. We\nshow that the signal 'back-propagated' during this second phase corresponds to\nthe propagation of error derivatives and encodes the gradient of the objective\nfunction, when the synaptic update corresponds to a standard form of\nspike-timing dependent plasticity. This work makes it more plausible that a\nmechanism similar to Backpropagation could be implemented by brains, since\nleaky integrator neural computation performs both inference and error\nback-propagation in our model. The only local difference between the two phases\nis whether synaptic changes are allowed or not.\n", "versions": [{"version": "v1", "created": "Tue, 16 Feb 2016 20:46:51 GMT"}, {"version": "v2", "created": "Wed, 24 Feb 2016 11:13:08 GMT"}, {"version": "v3", "created": "Tue, 20 Sep 2016 16:15:26 GMT"}, {"version": "v4", "created": "Mon, 26 Sep 2016 09:55:15 GMT"}, {"version": "v5", "created": "Tue, 28 Mar 2017 18:31:11 GMT"}], "update_date": "2017-03-30", "authors_parsed": [["Scellier", "Benjamin", ""], ["Bengio", "Yoshua", ""]]}, {"id": "1602.05205", "submitter": "Martin Jaggi", "authors": "Celestine D\\\"unner and Simone Forte and Martin Tak\\'a\\v{c} and Martin\n  Jaggi", "title": "Primal-Dual Rates and Certificates", "comments": "appearing at ICML 2016 - Proceedings of the 33rd International\n  Conference on Machine Learning, New York, NY, USA, 2016. JMLR: W&CP volume 48", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose an algorithm-independent framework to equip existing optimization\nmethods with primal-dual certificates. Such certificates and corresponding rate\nof convergence guarantees are important for practitioners to diagnose progress,\nin particular in machine learning applications. We obtain new primal-dual\nconvergence rates, e.g., for the Lasso as well as many L1, Elastic Net, group\nLasso and TV-regularized problems. The theory applies to any norm-regularized\ngeneralized linear model. Our approach provides efficiently computable duality\ngaps which are globally defined, without modifying the original problems in the\nregion of interest.\n", "versions": [{"version": "v1", "created": "Tue, 16 Feb 2016 21:06:01 GMT"}, {"version": "v2", "created": "Thu, 2 Jun 2016 21:29:48 GMT"}], "update_date": "2016-06-06", "authors_parsed": [["D\u00fcnner", "Celestine", ""], ["Forte", "Simone", ""], ["Tak\u00e1\u010d", "Martin", ""], ["Jaggi", "Martin", ""]]}, {"id": "1602.05242", "submitter": "Alireza Rezaei", "authors": "Nima Anari, Shayan Oveis Gharan, Alireza Rezaei", "title": "Monte Carlo Markov Chain Algorithms for Sampling Strongly Rayleigh\n  Distributions and Determinantal Point Processes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS math.PR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Strongly Rayleigh distributions are natural generalizations of product and\ndeterminantal probability distributions and satisfy strongest form of negative\ndependence properties. We show that the \"natural\" Monte Carlo Markov Chain\n(MCMC) is rapidly mixing in the support of a {\\em homogeneous} strongly\nRayleigh distribution. As a byproduct, our proof implies Markov chains can be\nused to efficiently generate approximate samples of a $k$-determinantal point\nprocess. This answers an open question raised by Deshpande and Rademacher.\n", "versions": [{"version": "v1", "created": "Tue, 16 Feb 2016 23:32:53 GMT"}, {"version": "v2", "created": "Thu, 18 Feb 2016 04:08:26 GMT"}, {"version": "v3", "created": "Thu, 24 Mar 2016 19:58:03 GMT"}], "update_date": "2016-03-25", "authors_parsed": [["Anari", "Nima", ""], ["Gharan", "Shayan Oveis", ""], ["Rezaei", "Alireza", ""]]}, {"id": "1602.05257", "submitter": "Deovrat Kakde", "authors": "Deovrat Kakde, Arin Chaudhuri, Seunghyun Kong, Maria Jahja, Hansi\n  Jiang, Jorge Silva", "title": "Peak Criterion for Choosing Gaussian Kernel Bandwidth in Support Vector\n  Data Description", "comments": null, "journal-ref": null, "doi": "10.1109/ICPHM.2017.7998302", "report-no": null, "categories": "cs.LG stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Support Vector Data Description (SVDD) is a machine-learning technique used\nfor single class classification and outlier detection. SVDD formulation with\nkernel function provides a flexible boundary around data. The value of kernel\nfunction parameters affects the nature of the data boundary. For example, it is\nobserved that with a Gaussian kernel, as the value of kernel bandwidth is\nlowered, the data boundary changes from spherical to wiggly. The spherical data\nboundary leads to underfitting, and an extremely wiggly data boundary leads to\noverfitting. In this paper, we propose empirical criterion to obtain good\nvalues of the Gaussian kernel bandwidth parameter. This criterion provides a\nsmooth boundary that captures the essential geometric features of the data.\n", "versions": [{"version": "v1", "created": "Wed, 17 Feb 2016 00:51:18 GMT"}, {"version": "v2", "created": "Wed, 11 May 2016 21:39:53 GMT"}, {"version": "v3", "created": "Tue, 8 Aug 2017 18:00:45 GMT"}], "update_date": "2017-09-06", "authors_parsed": [["Kakde", "Deovrat", ""], ["Chaudhuri", "Arin", ""], ["Kong", "Seunghyun", ""], ["Jahja", "Maria", ""], ["Jiang", "Hansi", ""], ["Silva", "Jorge", ""]]}, {"id": "1602.05264", "submitter": "Puneet Chhabra", "authors": "Puneet S Chhabra, Andrew M Wallace and James R Hopgood", "title": "Anomaly Detection in Clutter using Spectrally Enhanced Ladar", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.optics cs.LG physics.ins-det stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Discrete return (DR) Laser Detection and Ranging (Ladar) systems provide a\nseries of echoes that reflect from objects in a scene. These can be first, last\nor multi-echo returns. In contrast, Full-Waveform (FW)-Ladar systems measure\nthe intensity of light reflected from objects continuously over a period of\ntime. In a camouflaged scenario, e.g., objects hidden behind dense foliage, a\nFW-Ladar penetrates such foliage and returns a sequence of echoes including\nburied faint echoes. The aim of this paper is to learn local-patterns of\nco-occurring echoes characterised by their measured spectra. A deviation from\nsuch patterns defines an abnormal event in a forest/tree depth profile. As far\nas the authors know, neither DR or FW-Ladar, along with several spectral\nmeasurements, has not been applied to anomaly detection. This work presents an\nalgorithm that allows detection of spectral and temporal anomalies in FW-Multi\nSpectral Ladar (FW-MSL) data samples. An anomaly is defined as a full waveform\ntemporal and spectral signature that does not conform to a prior expectation,\nrepresented using a learnt subspace (dictionary) and set of coefficients that\ncapture co-occurring local-patterns using an overlapping temporal window. A\nmodified optimization scheme is proposed for subspace learning based on\nstochastic approximations. The objective function is augmented with a\ndiscriminative term that represents the subspace's separability properties and\nsupports anomaly characterisation. The algorithm detects several man-made\nobjects and anomalous spectra hidden in a dense clutter of vegetation and also\nallows tree species classification.\n", "versions": [{"version": "v1", "created": "Wed, 17 Feb 2016 01:39:29 GMT"}], "update_date": "2016-02-18", "authors_parsed": [["Chhabra", "Puneet S", ""], ["Wallace", "Andrew M", ""], ["Hopgood", "James R", ""]]}, {"id": "1602.05285", "submitter": "Truyen Tran", "authors": "Truyen Tran, Dinh Phung and Svetha Venkatesh", "title": "Choice by Elimination via Deep Neural Networks", "comments": "PAKDD workshop on Biologically Inspired Techniques for Data Mining\n  (BDM'16)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce Neural Choice by Elimination, a new framework that integrates\ndeep neural networks into probabilistic sequential choice models for learning\nto rank. Given a set of items to chose from, the elimination strategy starts\nwith the whole item set and iteratively eliminates the least worthy item in the\nremaining subset. We prove that the choice by elimination is equivalent to\nmarginalizing out the random Gompertz latent utilities. Coupled with the choice\nmodel is the recently introduced Neural Highway Networks for approximating\narbitrarily complex rank functions. We evaluate the proposed framework on a\nlarge-scale public dataset with over 425K items, drawn from the Yahoo! learning\nto rank challenge. It is demonstrated that the proposed method is competitive\nagainst state-of-the-art learning to rank methods.\n", "versions": [{"version": "v1", "created": "Wed, 17 Feb 2016 03:17:10 GMT"}], "update_date": "2016-02-18", "authors_parsed": [["Tran", "Truyen", ""], ["Phung", "Dinh", ""], ["Venkatesh", "Svetha", ""]]}, {"id": "1602.05307", "submitter": "Xiang Ren", "authors": "Xiang Ren, Wenqi He, Meng Qu, Clare R. Voss, Heng Ji, Jiawei Han", "title": "Label Noise Reduction in Entity Typing by Heterogeneous Partial-Label\n  Embedding", "comments": "Submitted to KDD 2016. 11 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Current systems of fine-grained entity typing use distant supervision in\nconjunction with existing knowledge bases to assign categories (type labels) to\nentity mentions. However, the type labels so obtained from knowledge bases are\noften noisy (i.e., incorrect for the entity mention's local context). We define\na new task, Label Noise Reduction in Entity Typing (LNR), to be the automatic\nidentification of correct type labels (type-paths) for training examples, given\nthe set of candidate type labels obtained by distant supervision with a given\ntype hierarchy. The unknown type labels for individual entity mentions and the\nsemantic similarity between entity types pose unique challenges for solving the\nLNR task. We propose a general framework, called PLE, to jointly embed entity\nmentions, text features and entity types into the same low-dimensional space\nwhere, in that space, objects whose types are semantically close have similar\nrepresentations. Then we estimate the type-path for each training example in a\ntop-down manner using the learned embeddings. We formulate a global objective\nfor learning the embeddings from text corpora and knowledge bases, which adopts\na novel margin-based loss that is robust to noisy labels and faithfully models\ntype correlation derived from knowledge bases. Our experiments on three public\ntyping datasets demonstrate the effectiveness and robustness of PLE, with an\naverage of 25% improvement in accuracy compared to next best method.\n", "versions": [{"version": "v1", "created": "Wed, 17 Feb 2016 05:26:47 GMT"}], "update_date": "2016-02-18", "authors_parsed": [["Ren", "Xiang", ""], ["He", "Wenqi", ""], ["Qu", "Meng", ""], ["Voss", "Clare R.", ""], ["Ji", "Heng", ""], ["Han", "Jiawei", ""]]}, {"id": "1602.05310", "submitter": "Stephen Tu", "authors": "Stephen Tu and Rebecca Roelofs and Shivaram Venkataraman and Benjamin\n  Recht", "title": "Large Scale Kernel Learning using Block Coordinate Descent", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We demonstrate that distributed block coordinate descent can quickly solve\nkernel regression and classification problems with millions of data points.\nArmed with this capability, we conduct a thorough comparison between the full\nkernel, the Nystr\\\"om method, and random features on three large classification\ntasks from various domains. Our results suggest that the Nystr\\\"om method\ngenerally achieves better statistical accuracy than random features, but can\nrequire significantly more iterations of optimization. Lastly, we derive new\nrates for block coordinate descent which support our experimental findings when\nspecialized to kernel methods.\n", "versions": [{"version": "v1", "created": "Wed, 17 Feb 2016 05:41:07 GMT"}], "update_date": "2016-02-18", "authors_parsed": [["Tu", "Stephen", ""], ["Roelofs", "Rebecca", ""], ["Venkataraman", "Shivaram", ""], ["Recht", "Benjamin", ""]]}, {"id": "1602.05350", "submitter": "Di Chen", "authors": "Di Chen and Jeff M. Phillips", "title": "Relative Error Embeddings for the Gaussian Kernel Distance", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A reproducing kernel can define an embedding of a data point into an infinite\ndimensional reproducing kernel Hilbert space (RKHS). The norm in this space\ndescribes a distance, which we call the kernel distance. The random Fourier\nfeatures (of Rahimi and Recht) describe an oblivious approximate mapping into\nfinite dimensional Euclidean space that behaves similar to the RKHS. We show in\nthis paper that for the Gaussian kernel the Euclidean norm between these mapped\nto features has $(1+\\epsilon)$-relative error with respect to the kernel\ndistance. When there are $n$ data points, we show that $O((1/\\epsilon^2)\n\\log(n))$ dimensions of the approximate feature space are sufficient and\nnecessary.\n  Without a bound on $n$, but when the original points lie in $\\mathbb{R}^d$\nand have diameter bounded by $\\mathcal{M}$, then we show that $O((d/\\epsilon^2)\n\\log(\\mathcal{M}))$ dimensions are sufficient, and that this many are required,\nup to $\\log(1/\\epsilon)$ factors.\n", "versions": [{"version": "v1", "created": "Wed, 17 Feb 2016 09:35:08 GMT"}, {"version": "v2", "created": "Tue, 20 Sep 2016 17:13:17 GMT"}], "update_date": "2016-09-21", "authors_parsed": [["Chen", "Di", ""], ["Phillips", "Jeff M.", ""]]}, {"id": "1602.05352", "submitter": "Tobias Schnabel", "authors": "Tobias Schnabel, Adith Swaminathan, Ashudeep Singh, Navin Chandak and\n  Thorsten Joachims", "title": "Recommendations as Treatments: Debiasing Learning and Evaluation", "comments": "10 pages in ICML 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most data for evaluating and training recommender systems is subject to\nselection biases, either through self-selection by the users or through the\nactions of the recommendation system itself. In this paper, we provide a\nprincipled approach to handling selection biases, adapting models and\nestimation techniques from causal inference. The approach leads to unbiased\nperformance estimators despite biased data, and to a matrix factorization\nmethod that provides substantially improved prediction performance on\nreal-world data. We theoretically and empirically characterize the robustness\nof the approach, finding that it is highly practical and scalable.\n", "versions": [{"version": "v1", "created": "Wed, 17 Feb 2016 09:58:25 GMT"}, {"version": "v2", "created": "Fri, 27 May 2016 03:18:59 GMT"}], "update_date": "2016-05-30", "authors_parsed": [["Schnabel", "Tobias", ""], ["Swaminathan", "Adith", ""], ["Singh", "Ashudeep", ""], ["Chandak", "Navin", ""], ["Joachims", "Thorsten", ""]]}, {"id": "1602.05394", "submitter": "Rodolphe Jenatton", "authors": "Rodolphe Jenatton, Jim Huang, Dominik Csiba, Cedric Archambeau", "title": "Online optimization and regret guarantees for non-additive long-term\n  constraints", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.OC math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider online optimization in the 1-lookahead setting, where the\nobjective does not decompose additively over the rounds of the online game. The\nresulting formulation enables us to deal with non-stationary and/or long-term\nconstraints , which arise, for example, in online display advertising problems.\nWe propose an on-line primal-dual algorithm for which we obtain dynamic\ncumulative regret guarantees. They depend on the convexity and the smoothness\nof the non-additive penalty, as well as terms capturing the smoothness with\nwhich the residuals of the non-stationary and long-term constraints vary over\nthe rounds. We conduct experiments on synthetic data to illustrate the benefits\nof the non-additive penalty and show vanishing regret convergence on live\ntraffic data collected by a display advertising platform in production.\n", "versions": [{"version": "v1", "created": "Wed, 17 Feb 2016 12:57:08 GMT"}, {"version": "v2", "created": "Wed, 8 Jun 2016 09:04:18 GMT"}], "update_date": "2016-06-09", "authors_parsed": [["Jenatton", "Rodolphe", ""], ["Huang", "Jim", ""], ["Csiba", "Dominik", ""], ["Archambeau", "Cedric", ""]]}, {"id": "1602.05419", "submitter": "Nicolas Flammarion", "authors": "Aymeric Dieuleveut (SIERRA, LIENS), Nicolas Flammarion (LIENS,\n  SIERRA), Francis Bach (SIERRA, LIENS)", "title": "Harder, Better, Faster, Stronger Convergence Rates for Least-Squares\n  Regression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the optimization of a quadratic objective function whose\ngradients are only accessible through a stochastic oracle that returns the\ngradient at any given point plus a zero-mean finite variance random error. We\npresent the first algorithm that achieves jointly the optimal prediction error\nrates for least-squares regression, both in terms of forgetting of initial\nconditions in O(1/n 2), and in terms of dependence on the noise and dimension d\nof the problem, as O(d/n). Our new algorithm is based on averaged accelerated\nregularized gradient descent, and may also be analyzed through finer\nassumptions on initial conditions and the Hessian matrix, leading to\ndimension-free quantities that may still be small while the \" optimal \" terms\nabove are large. In order to characterize the tightness of these new bounds, we\nconsider an application to non-parametric regression and use the known lower\nbounds on the statistical performance (without computational limits), which\nhappen to match our bounds obtained from a single pass on the data and thus\nshow optimality of our algorithm in a wide variety of particular trade-offs\nbetween bias and variance.\n", "versions": [{"version": "v1", "created": "Wed, 17 Feb 2016 14:06:34 GMT"}, {"version": "v2", "created": "Wed, 24 Feb 2016 06:37:55 GMT"}], "update_date": "2016-02-25", "authors_parsed": [["Dieuleveut", "Aymeric", "", "SIERRA, LIENS"], ["Flammarion", "Nicolas", "", "LIENS,\n  SIERRA"], ["Bach", "Francis", "", "SIERRA, LIENS"]]}, {"id": "1602.05436", "submitter": "Mike Gartrell", "authors": "Mike Gartrell, Ulrich Paquet, Noam Koenigstein", "title": "Low-Rank Factorization of Determinantal Point Processes for\n  Recommendation", "comments": "10 pages, 4 figures. Submitted to KDD 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Determinantal point processes (DPPs) have garnered attention as an elegant\nprobabilistic model of set diversity. They are useful for a number of subset\nselection tasks, including product recommendation. DPPs are parametrized by a\npositive semi-definite kernel matrix. In this work we present a new method for\nlearning the DPP kernel from observed data using a low-rank factorization of\nthis kernel. We show that this low-rank factorization enables a learning\nalgorithm that is nearly an order of magnitude faster than previous approaches,\nwhile also providing for a method for computing product recommendation\npredictions that is far faster (up to 20x faster or more for large item\ncatalogs) than previous techniques that involve a full-rank DPP kernel.\nFurthermore, we show that our method provides equivalent or sometimes better\npredictive performance than prior full-rank DPP approaches, and better\nperformance than several other competing recommendation methods in many cases.\nWe conduct an extensive experimental evaluation using several real-world\ndatasets in the domain of product recommendation to demonstrate the utility of\nour method, along with its limitations.\n", "versions": [{"version": "v1", "created": "Wed, 17 Feb 2016 14:40:52 GMT"}], "update_date": "2016-02-18", "authors_parsed": [["Gartrell", "Mike", ""], ["Paquet", "Ulrich", ""], ["Koenigstein", "Noam", ""]]}, {"id": "1602.05439", "submitter": "Arnaud Browet", "authors": "Arnaud Browet, Christophe De Vleeschouwer, Laurent Jacques, Navrita\n  Mathiah, Bechara Saykali, Isabelle Migeotte", "title": "Cell segmentation with random ferns and graph-cuts", "comments": "submitted to ICIP", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The progress in imaging techniques have allowed the study of various aspect\nof cellular mechanisms. To isolate individual cells in live imaging data, we\nintroduce an elegant image segmentation framework that effectively extracts\ncell boundaries, even in the presence of poor edge details. Our approach works\nin two stages. First, we estimate pixel interior/border/exterior class\nprobabilities using random ferns. Then, we use an energy minimization framework\nto compute boundaries whose localization is compliant with the pixel class\nprobabilities. We validate our approach on a manually annotated dataset.\n", "versions": [{"version": "v1", "created": "Wed, 17 Feb 2016 14:47:32 GMT"}], "update_date": "2016-02-18", "authors_parsed": [["Browet", "Arnaud", ""], ["De Vleeschouwer", "Christophe", ""], ["Jacques", "Laurent", ""], ["Mathiah", "Navrita", ""], ["Saykali", "Bechara", ""], ["Migeotte", "Isabelle", ""]]}, {"id": "1602.05473", "submitter": "Lars Maal{\\o}e", "authors": "Lars Maal{\\o}e, Casper Kaae S{\\o}nderby, S{\\o}ren Kaae S{\\o}nderby,\n  Ole Winther", "title": "Auxiliary Deep Generative Models", "comments": "Proceedings of the 33rd International Conference on Machine Learning,\n  New York, NY, USA, 2016, JMLR: Workshop and Conference Proceedings volume 48,\n  Proceedings of the 33rd International Conference on Machine Learning, New\n  York, NY, USA, 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep generative models parameterized by neural networks have recently\nachieved state-of-the-art performance in unsupervised and semi-supervised\nlearning. We extend deep generative models with auxiliary variables which\nimproves the variational approximation. The auxiliary variables leave the\ngenerative model unchanged but make the variational distribution more\nexpressive. Inspired by the structure of the auxiliary variable we also propose\na model with two stochastic layers and skip connections. Our findings suggest\nthat more expressive and properly specified deep generative models converge\nfaster with better results. We show state-of-the-art performance within\nsemi-supervised learning on MNIST, SVHN and NORB datasets.\n", "versions": [{"version": "v1", "created": "Wed, 17 Feb 2016 16:24:50 GMT"}, {"version": "v2", "created": "Thu, 26 May 2016 10:21:34 GMT"}, {"version": "v3", "created": "Fri, 3 Jun 2016 09:19:21 GMT"}, {"version": "v4", "created": "Thu, 16 Jun 2016 06:39:08 GMT"}], "update_date": "2016-06-17", "authors_parsed": [["Maal\u00f8e", "Lars", ""], ["S\u00f8nderby", "Casper Kaae", ""], ["S\u00f8nderby", "S\u00f8ren Kaae", ""], ["Winther", "Ole", ""]]}, {"id": "1602.05568", "submitter": "Mohammad Taha Bahadori", "authors": "Edward Choi, Mohammad Taha Bahadori, Elizabeth Searles, Catherine\n  Coffey, Jimeng Sun", "title": "Multi-layer Representation Learning for Medical Concepts", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning efficient representations for concepts has been proven to be an\nimportant basis for many applications such as machine translation or document\nclassification. Proper representations of medical concepts such as diagnosis,\nmedication, procedure codes and visits will have broad applications in\nhealthcare analytics. However, in Electronic Health Records (EHR) the visit\nsequences of patients include multiple concepts (diagnosis, procedure, and\nmedication codes) per visit. This structure provides two types of relational\ninformation, namely sequential order of visits and co-occurrence of the codes\nwithin each visit. In this work, we propose Med2Vec, which not only learns\ndistributed representations for both medical codes and visits from a large EHR\ndataset with over 3 million visits, but also allows us to interpret the learned\nrepresentations confirmed positively by clinical experts. In the experiments,\nMed2Vec displays significant improvement in key medical applications compared\nto popular baselines such as Skip-gram, GloVe and stacked autoencoder, while\nproviding clinically meaningful interpretation.\n", "versions": [{"version": "v1", "created": "Wed, 17 Feb 2016 20:55:40 GMT"}], "update_date": "2016-02-18", "authors_parsed": [["Choi", "Edward", ""], ["Bahadori", "Mohammad Taha", ""], ["Searles", "Elizabeth", ""], ["Coffey", "Catherine", ""], ["Sun", "Jimeng", ""]]}, {"id": "1602.05629", "submitter": "Hugh Brendan McMahan", "authors": "H. Brendan McMahan, Eider Moore, Daniel Ramage, Seth Hampson, Blaise\n  Ag\\\"uera y Arcas", "title": "Communication-Efficient Learning of Deep Networks from Decentralized\n  Data", "comments": "This version updates the large-scale LSTM experiments, along with\n  other minor changes. In earlier versions, an inconsistency in our\n  implementation of FedSGD caused us to report much lower learning rates for\n  the large-scale LSTM. We reran these experiments, and also found that fewer\n  local epochs offers better performance, leading to slightly better results\n  for FedAvg than previously reported", "journal-ref": "Proceedings of the 20 th International Conference on Artificial\n  Intelligence and Statistics (AISTATS) 2017. JMLR: W&CP volume 54", "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modern mobile devices have access to a wealth of data suitable for learning\nmodels, which in turn can greatly improve the user experience on the device.\nFor example, language models can improve speech recognition and text entry, and\nimage models can automatically select good photos. However, this rich data is\noften privacy sensitive, large in quantity, or both, which may preclude logging\nto the data center and training there using conventional approaches. We\nadvocate an alternative that leaves the training data distributed on the mobile\ndevices, and learns a shared model by aggregating locally-computed updates. We\nterm this decentralized approach Federated Learning.\n  We present a practical method for the federated learning of deep networks\nbased on iterative model averaging, and conduct an extensive empirical\nevaluation, considering five different model architectures and four datasets.\nThese experiments demonstrate the approach is robust to the unbalanced and\nnon-IID data distributions that are a defining characteristic of this setting.\nCommunication costs are the principal constraint, and we show a reduction in\nrequired communication rounds by 10-100x as compared to synchronized stochastic\ngradient descent.\n", "versions": [{"version": "v1", "created": "Wed, 17 Feb 2016 23:40:56 GMT"}, {"version": "v2", "created": "Fri, 21 Oct 2016 22:39:11 GMT"}, {"version": "v3", "created": "Tue, 28 Feb 2017 21:03:49 GMT"}], "update_date": "2017-03-02", "authors_parsed": [["McMahan", "H. Brendan", ""], ["Moore", "Eider", ""], ["Ramage", "Daniel", ""], ["Hampson", "Seth", ""], ["Arcas", "Blaise Ag\u00fcera y", ""]]}, {"id": "1602.05659", "submitter": "Fuqiang Liu", "authors": "Fuqiang Liu, Fukun Bi, Yiding Yang, Liang Chen", "title": "Boost Picking: A Universal Method on Converting Supervised\n  Classification to Semi-supervised Classification", "comments": "This paper has been withdraw by the author due to format error", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper proposes a universal method, Boost Picking, to train supervised\nclassification models mainly by un-labeled data. Boost Picking only adopts two\nweak classifiers to estimate and correct the error. It is theoretically proved\nthat Boost Picking could train a supervised model mainly by un-labeled data as\neffectively as the same model trained by 100% labeled data, only if recalls of\nthe two weak classifiers are all greater than zero and the sum of precisions is\ngreater than one. Based on Boost Picking, we present \"Test along with Training\n(TawT)\" to improve the generalization of supervised models. Both Boost Picking\nand TawT are successfully tested in varied little data sets.\n", "versions": [{"version": "v1", "created": "Thu, 18 Feb 2016 02:24:54 GMT"}, {"version": "v2", "created": "Mon, 29 Feb 2016 13:16:23 GMT"}, {"version": "v3", "created": "Sat, 12 Nov 2016 09:25:54 GMT"}], "update_date": "2016-11-15", "authors_parsed": [["Liu", "Fuqiang", ""], ["Bi", "Fukun", ""], ["Yang", "Yiding", ""], ["Chen", "Liang", ""]]}, {"id": "1602.05682", "submitter": "Simeng Qi", "authors": "Simeng Qi, Zheng Huang, Yan Li, Shaopei Shi", "title": "Audio Recording Device Identification Based on Deep Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we present a research on identification of audio recording\ndevices from background noise, thus providing a method for forensics. The audio\nsignal is the sum of speech signal and noise signal. Usually, people pay more\nattention to speech signal, because it carries the information to deliver. So a\ngreat amount of researches have been dedicated to getting higher\nSignal-Noise-Ratio (SNR). There are many speech enhancement algorithms to\nimprove the quality of the speech, which can be seen as reducing the noise.\nHowever, noises can be regarded as the intrinsic fingerprint traces of an audio\nrecording device. These digital traces can be characterized and identified by\nnew machine learning techniques. Therefore, in our research, we use the noise\nas the intrinsic features. As for the identification, multiple classifiers of\ndeep learning methods are used and compared. The identification result shows\nthat the method of getting feature vector from the noise of each device and\nidentifying them with deep learning techniques is viable, and well-preformed.\n", "versions": [{"version": "v1", "created": "Thu, 18 Feb 2016 05:49:37 GMT"}, {"version": "v2", "created": "Wed, 27 Apr 2016 02:32:38 GMT"}], "update_date": "2016-04-28", "authors_parsed": [["Qi", "Simeng", ""], ["Huang", "Zheng", ""], ["Li", "Yan", ""], ["Shi", "Shaopei", ""]]}, {"id": "1602.05703", "submitter": "Paolo Di Lorenzo", "authors": "Paolo Di Lorenzo, Sergio Barbarossa, Paolo Banelli, and Stefania\n  Sardellitti", "title": "Adaptive Least Mean Squares Estimation of Graph Signals", "comments": "Submitted to IEEE Transactions on Signal and Information Processing\n  over Networks", "journal-ref": null, "doi": "10.1109/TSIPN.2016.2613687", "report-no": null, "categories": "cs.LG cs.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The aim of this paper is to propose a least mean squares (LMS) strategy for\nadaptive estimation of signals defined over graphs. Assuming the graph signal\nto be band-limited, over a known bandwidth, the method enables reconstruction,\nwith guaranteed performance in terms of mean-square error, and tracking from a\nlimited number of observations over a subset of vertices. A detailed mean\nsquare analysis provides the performance of the proposed method, and leads to\nseveral insights for designing useful sampling strategies for graph signals.\nNumerical results validate our theoretical findings, and illustrate the\nperformance of the proposed method. Furthermore, to cope with the case where\nthe bandwidth is not known beforehand, we propose a method that performs a\nsparse online estimation of the signal support in the (graph) frequency domain,\nwhich enables online adaptation of the graph sampling strategy. Finally, we\napply the proposed method to build the power spatial density cartography of a\ngiven operational region in a cognitive network environment.\n", "versions": [{"version": "v1", "created": "Thu, 18 Feb 2016 07:34:04 GMT"}, {"version": "v2", "created": "Fri, 19 Feb 2016 10:15:47 GMT"}, {"version": "v3", "created": "Mon, 11 Jul 2016 15:40:59 GMT"}], "update_date": "2016-11-17", "authors_parsed": [["Di Lorenzo", "Paolo", ""], ["Barbarossa", "Sergio", ""], ["Banelli", "Paolo", ""], ["Sardellitti", "Stefania", ""]]}, {"id": "1602.05719", "submitter": "Jelani Nelson", "authors": "Jaros{\\l}aw B{\\l}asiok, Jelani Nelson", "title": "An improved analysis of the ER-SpUD dictionary learning algorithm", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS cs.IT math.IT math.PR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In \"dictionary learning\" we observe $Y = AX + E$ for some\n$Y\\in\\mathbb{R}^{n\\times p}$, $A \\in\\mathbb{R}^{m\\times n}$, and\n$X\\in\\mathbb{R}^{m\\times p}$. The matrix $Y$ is observed, and $A, X, E$ are\nunknown. Here $E$ is \"noise\" of small norm, and $X$ is column-wise sparse. The\nmatrix $A$ is referred to as a {\\em dictionary}, and its columns as {\\em\natoms}. Then, given some small number $p$ of samples, i.e.\\ columns of $Y$, the\ngoal is to learn the dictionary $A$ up to small error, as well as $X$. The\nmotivation is that in many applications data is expected to sparse when\nrepresented by atoms in the \"right\" dictionary $A$ (e.g.\\ images in the Haar\nwavelet basis), and the goal is to learn $A$ from the data to then use it for\nother applications.\n  Recently, [SWW12] proposed the dictionary learning algorithm ER-SpUD with\nprovable guarantees when $E = 0$ and $m = n$. They showed if $X$ has\nindependent entries with an expected $s$ non-zeroes per column for $1 \\lesssim\ns \\lesssim \\sqrt{n}$, and with non-zero entries being subgaussian, then for\n$p\\gtrsim n^2\\log^2 n$ with high probability ER-SpUD outputs matrices $A', X'$\nwhich equal $A, X$ up to permuting and scaling columns (resp.\\ rows) of $A$\n(resp.\\ $X$). They conjectured $p\\gtrsim n\\log n$ suffices, which they showed\nwas information theoretically necessary for {\\em any} algorithm to succeed when\n$s \\simeq 1$. Significant progress was later obtained in [LV15].\n  We show that for a slight variant of ER-SpUD, $p\\gtrsim n\\log(n/\\delta)$\nsamples suffice for successful recovery with probability $1-\\delta$. We also\nshow that for the unmodified ER-SpUD, $p\\gtrsim n^{1.99}$ samples are required\neven to learn $A, X$ with polynomially small success probability. This resolves\nthe main conjecture of [SWW12], and contradicts the main result of [LV15],\nwhich claimed that $p\\gtrsim n\\log^4 n$ guarantees success whp.\n", "versions": [{"version": "v1", "created": "Thu, 18 Feb 2016 08:51:08 GMT"}], "update_date": "2016-02-19", "authors_parsed": [["B\u0142asiok", "Jaros\u0142aw", ""], ["Nelson", "Jelani", ""]]}, {"id": "1602.05897", "submitter": "Amit Daniely", "authors": "Amit Daniely and Roy Frostig and Yoram Singer", "title": "Toward Deeper Understanding of Neural Networks: The Power of\n  Initialization and a Dual View on Expressivity", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CC cs.DS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop a general duality between neural networks and compositional\nkernels, striving towards a better understanding of deep learning. We show that\ninitial representations generated by common random initializations are\nsufficiently rich to express all functions in the dual kernel space. Hence,\nthough the training objective is hard to optimize in the worst case, the\ninitial weights form a good starting point for optimization. Our dual view also\nreveals a pragmatic and aesthetic perspective of neural networks and\nunderscores their expressive power.\n", "versions": [{"version": "v1", "created": "Thu, 18 Feb 2016 18:14:19 GMT"}, {"version": "v2", "created": "Fri, 19 May 2017 18:39:00 GMT"}], "update_date": "2017-05-23", "authors_parsed": [["Daniely", "Amit", ""], ["Frostig", "Roy", ""], ["Singer", "Yoram", ""]]}, {"id": "1602.05908", "submitter": "Rong Ge", "authors": "Anima Anandkumar, Rong Ge", "title": "Efficient approaches for escaping higher order saddle points in\n  non-convex optimization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Local search heuristics for non-convex optimizations are popular in applied\nmachine learning. However, in general it is hard to guarantee that such\nalgorithms even converge to a local minimum, due to the existence of\ncomplicated saddle point structures in high dimensions. Many functions have\ndegenerate saddle points such that the first and second order derivatives\ncannot distinguish them with local optima. In this paper we use higher order\nderivatives to escape these saddle points: we design the first efficient\nalgorithm guaranteed to converge to a third order local optimum (while existing\ntechniques are at most second order). We also show that it is NP-hard to extend\nthis further to finding fourth order local optima.\n", "versions": [{"version": "v1", "created": "Thu, 18 Feb 2016 18:52:15 GMT"}], "update_date": "2016-02-19", "authors_parsed": [["Anandkumar", "Anima", ""], ["Ge", "Rong", ""]]}, {"id": "1602.05916", "submitter": "Niloofar Yousefi", "authors": "Niloofar Yousefi, Yunwen Lei, Marius Kloft, Mansooreh Mollaghasemi and\n  Georgios Anagnostopoulos", "title": "Local Rademacher Complexity-based Learning Guarantees for Multi-Task\n  Learning", "comments": "In this version, some arguments and results (of the previous version)\n  have been corrected, or modified", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We show a Talagrand-type concentration inequality for Multi-Task Learning\n(MTL), using which we establish sharp excess risk bounds for MTL in terms of\ndistribution- and data-dependent versions of the Local Rademacher Complexity\n(LRC). We also give a new bound on the LRC for norm regularized as well as\nstrongly convex hypothesis classes, which applies not only to MTL but also to\nthe standard i.i.d. setting. Combining both results, one can now easily derive\nfast-rate bounds on the excess risk for many prominent MTL methods,\nincluding---as we demonstrate---Schatten-norm, group-norm, and\ngraph-regularized MTL. The derived bounds reflect a relationship akeen to a\nconservation law of asymptotic convergence rates. This very relationship allows\nfor trading off slower rates w.r.t. the number of tasks for faster rates with\nrespect to the number of available samples per task, when compared to the rates\nobtained via a traditional, global Rademacher analysis.\n", "versions": [{"version": "v1", "created": "Thu, 18 Feb 2016 19:13:23 GMT"}, {"version": "v2", "created": "Thu, 9 Feb 2017 22:48:06 GMT"}], "update_date": "2017-02-13", "authors_parsed": [["Yousefi", "Niloofar", ""], ["Lei", "Yunwen", ""], ["Kloft", "Marius", ""], ["Mollaghasemi", "Mansooreh", ""], ["Anagnostopoulos", "Georgios", ""]]}, {"id": "1602.05920", "submitter": "Kamran Kowsari", "authors": "Kamran Kowsari, Manal H. Alassaf", "title": "Weighted Unsupervised Learning for 3D Object Detection", "comments": "IJACSA", "journal-ref": null, "doi": "10.14569/IJACSA.2016.070180", "report-no": null, "categories": "cs.CV cs.GR cs.LG cs.MM cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces a novel weighted unsupervised learning for object\ndetection using an RGB-D camera. This technique is feasible for detecting the\nmoving objects in the noisy environments that are captured by an RGB-D camera.\nThe main contribution of this paper is a real-time algorithm for detecting each\nobject using weighted clustering as a separate cluster. In a preprocessing\nstep, the algorithm calculates the pose 3D position X, Y, Z and RGB color of\neach data point and then it calculates each data point's normal vector using\nthe point's neighbor. After preprocessing, our algorithm calculates k-weights\nfor each data point; each weight indicates membership. Resulting in clustered\nobjects of the scene.\n", "versions": [{"version": "v1", "created": "Thu, 18 Feb 2016 19:40:26 GMT"}, {"version": "v2", "created": "Mon, 4 Jun 2018 23:51:27 GMT"}], "update_date": "2018-10-24", "authors_parsed": [["Kowsari", "Kamran", ""], ["Alassaf", "Manal H.", ""]]}, {"id": "1602.05980", "submitter": "Bing Xu", "authors": "Bing Xu, Ruitong Huang, Mu Li", "title": "Revise Saturated Activation Functions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we revise two commonly used saturated functions, the logistic\nsigmoid and the hyperbolic tangent (tanh).\n  We point out that, besides the well-known non-zero centered property, slope\nof the activation function near the origin is another possible reason making\ntraining deep networks with the logistic function difficult to train. We\ndemonstrate that, with proper rescaling, the logistic sigmoid achieves\ncomparable results with tanh.\n  Then following the same argument, we improve tahn by penalizing in the\nnegative part. We show that \"penalized tanh\" is comparable and even outperforms\nthe state-of-the-art non-saturated functions including ReLU and leaky ReLU on\ndeep convolution neural networks.\n  Our results contradict to the conclusion of previous works that the\nsaturation property causes the slow convergence. It suggests further\ninvestigation is necessary to better understand activation functions in deep\narchitectures.\n", "versions": [{"version": "v1", "created": "Thu, 18 Feb 2016 21:26:53 GMT"}, {"version": "v2", "created": "Mon, 2 May 2016 08:56:19 GMT"}], "update_date": "2016-05-03", "authors_parsed": [["Xu", "Bing", ""], ["Huang", "Ruitong", ""], ["Li", "Mu", ""]]}, {"id": "1602.06025", "submitter": "Yong Ren", "authors": "Yong Ren, Yining Wang, Jun Zhu", "title": "Spectral Learning for Supervised Topic Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL cs.IR stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Supervised topic models simultaneously model the latent topic structure of\nlarge collections of documents and a response variable associated with each\ndocument. Existing inference methods are based on variational approximation or\nMonte Carlo sampling, which often suffers from the local minimum defect.\nSpectral methods have been applied to learn unsupervised topic models, such as\nlatent Dirichlet allocation (LDA), with provable guarantees. This paper\ninvestigates the possibility of applying spectral methods to recover the\nparameters of supervised LDA (sLDA). We first present a two-stage spectral\nmethod, which recovers the parameters of LDA followed by a power update method\nto recover the regression model parameters. Then, we further present a\nsingle-phase spectral algorithm to jointly recover the topic distribution\nmatrix as well as the regression weights. Our spectral algorithms are provably\ncorrect and computationally efficient. We prove a sample complexity bound for\neach algorithm and subsequently derive a sufficient condition for the\nidentifiability of sLDA. Thorough experiments on synthetic and real-world\ndatasets verify the theory and demonstrate the practical effectiveness of the\nspectral algorithms. In fact, our results on a large-scale review rating\ndataset demonstrate that our single-phase spectral algorithm alone gets\ncomparable or even better performance than state-of-the-art methods, while\nprevious work on spectral methods has rarely reported such promising\nperformance.\n", "versions": [{"version": "v1", "created": "Fri, 19 Feb 2016 02:07:20 GMT"}], "update_date": "2016-02-22", "authors_parsed": [["Ren", "Yong", ""], ["Wang", "Yining", ""], ["Zhu", "Jun", ""]]}, {"id": "1602.06042", "submitter": "Nikhil Rao", "authors": "Prateek Jain, Nikhil Rao, Inderjit Dhillon", "title": "Structured Sparse Regression via Greedy Hard-Thresholding", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Several learning applications require solving high-dimensional regression\nproblems where the relevant features belong to a small number of (overlapping)\ngroups. For very large datasets and under standard sparsity constraints, hard\nthresholding methods have proven to be extremely efficient, but such methods\nrequire NP hard projections when dealing with overlapping groups. In this\npaper, we show that such NP-hard projections can not only be avoided by\nappealing to submodular optimization, but such methods come with strong\ntheoretical guarantees even in the presence of poorly conditioned data (i.e.\nsay when two features have correlation $\\geq 0.99$), which existing analyses\ncannot handle. These methods exhibit an interesting computation-accuracy\ntrade-off and can be extended to significantly harder problems such as sparse\noverlapping groups. Experiments on both real and synthetic data validate our\nclaims and demonstrate that the proposed methods are orders of magnitude faster\nthan other greedy and convex relaxation techniques for learning with\ngroup-structured sparsity.\n", "versions": [{"version": "v1", "created": "Fri, 19 Feb 2016 04:28:50 GMT"}, {"version": "v2", "created": "Fri, 27 May 2016 04:47:38 GMT"}], "update_date": "2016-05-30", "authors_parsed": [["Jain", "Prateek", ""], ["Rao", "Nikhil", ""], ["Dhillon", "Inderjit", ""]]}, {"id": "1602.06053", "submitter": "Hongyi Zhang", "authors": "Hongyi Zhang, Suvrit Sra", "title": "First-order Methods for Geodesically Convex Optimization", "comments": "21 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Geodesic convexity generalizes the notion of (vector space) convexity to\nnonlinear metric spaces. But unlike convex optimization, geodesically convex\n(g-convex) optimization is much less developed. In this paper we contribute to\nthe understanding of g-convex optimization by developing iteration complexity\nanalysis for several first-order algorithms on Hadamard manifolds.\nSpecifically, we prove upper bounds for the global complexity of deterministic\nand stochastic (sub)gradient methods for optimizing smooth and nonsmooth\ng-convex functions, both with and without strong g-convexity. Our analysis also\nreveals how the manifold geometry, especially \\emph{sectional curvature},\nimpacts convergence rates. To the best of our knowledge, our work is the first\nto provide global complexity analysis for first-order algorithms for general\ng-convex optimization.\n", "versions": [{"version": "v1", "created": "Fri, 19 Feb 2016 06:56:50 GMT"}], "update_date": "2016-02-22", "authors_parsed": [["Zhang", "Hongyi", ""], ["Sra", "Suvrit", ""]]}, {"id": "1602.06183", "submitter": "Malik Magdon-Ismail", "authors": "Ke Wu and Malik Magdon-Ismail", "title": "Node-By-Node Greedy Deep Learning for Interpretable Features", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multilayer networks have seen a resurgence under the umbrella of deep\nlearning. Current deep learning algorithms train the layers of the network\nsequentially, improving algorithmic performance as well as providing some\nregularization. We present a new training algorithm for deep networks which\ntrains \\emph{each node in the network} sequentially. Our algorithm is orders of\nmagnitude faster, creates more interpretable internal representations at the\nnode level, while not sacrificing on the ultimate out-of-sample performance.\n", "versions": [{"version": "v1", "created": "Fri, 19 Feb 2016 15:36:38 GMT"}], "update_date": "2016-02-22", "authors_parsed": [["Wu", "Ke", ""], ["Magdon-Ismail", "Malik", ""]]}, {"id": "1602.06225", "submitter": "Eugene Ndiaye", "authors": "Eugene Ndiaye, Olivier Fercoq, Alexandre Gramfort, Joseph Salmon", "title": "GAP Safe Screening Rules for Sparse-Group-Lasso", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.OC stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In high dimensional settings, sparse structures are crucial for efficiency,\neither in term of memory, computation or performance. In some contexts, it is\nnatural to handle more refined structures than pure sparsity, such as for\ninstance group sparsity. Sparse-Group Lasso has recently been introduced in the\ncontext of linear regression to enforce sparsity both at the feature level and\nat the group level. We adapt to the case of Sparse-Group Lasso recent safe\nscreening rules that discard early in the solver irrelevant features/groups.\nSuch rules have led to important speed-ups for a wide range of iterative\nmethods. Thanks to dual gap computations, we provide new safe screening rules\nfor Sparse-Group Lasso and show significant gains in term of computing time for\na coordinate descent implementation.\n", "versions": [{"version": "v1", "created": "Fri, 19 Feb 2016 17:08:34 GMT"}], "update_date": "2016-02-24", "authors_parsed": [["Ndiaye", "Eugene", ""], ["Fercoq", "Olivier", ""], ["Gramfort", "Alexandre", ""], ["Salmon", "Joseph", ""]]}, {"id": "1602.06294", "submitter": "Ben Hoyle Dr", "authors": "Roman Zitlau, Ben Hoyle, Kerstin Paech, Jochen Weller, Markus Michael\n  Rau, Stella Seitz", "title": "Stacking for machine learning redshifts applied to SDSS galaxies", "comments": "13 pages, 3 tables, 7 figures version accepted by MNRAS, minor text\n  updates. Results and conclusions unchanged", "journal-ref": null, "doi": "10.1093/mnras/stw1454", "report-no": null, "categories": "astro-ph.IM astro-ph.CO cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an analysis of a general machine learning technique called\n'stacking' for the estimation of photometric redshifts. Stacking techniques can\nfeed the photometric redshift estimate, as output by a base algorithm, back\ninto the same algorithm as an additional input feature in a subsequent learning\nround. We shown how all tested base algorithms benefit from at least one\nadditional stacking round (or layer). To demonstrate the benefit of stacking,\nwe apply the method to both unsupervised machine learning techniques based on\nself-organising maps (SOMs), and supervised machine learning methods based on\ndecision trees. We explore a range of stacking architectures, such as the\nnumber of layers and the number of base learners per layer. Finally we explore\nthe effectiveness of stacking even when using a successful algorithm such as\nAdaBoost. We observe a significant improvement of between 1.9% and 21% on all\ncomputed metrics when stacking is applied to weak learners (such as SOMs and\ndecision trees). When applied to strong learning algorithms (such as AdaBoost)\nthe ratio of improvement shrinks, but still remains positive and is between\n0.4% and 2.5% for the explored metrics and comes at almost no additional\ncomputational cost.\n", "versions": [{"version": "v1", "created": "Fri, 19 Feb 2016 11:29:10 GMT"}, {"version": "v2", "created": "Thu, 16 Jun 2016 17:36:34 GMT"}], "update_date": "2016-07-27", "authors_parsed": [["Zitlau", "Roman", ""], ["Hoyle", "Ben", ""], ["Paech", "Kerstin", ""], ["Weller", "Jochen", ""], ["Rau", "Markus Michael", ""], ["Seitz", "Stella", ""]]}, {"id": "1602.06346", "submitter": "Bernardo \\'Avila Pires", "authors": "Bernardo \\'Avila Pires and Csaba Szepesv\\'ari", "title": "Policy Error Bounds for Model-Based Reinforcement Learning with Factored\n  Linear Models", "comments": "30 pages. Corrected typos. Appears in JMLR Workshop and Conference\n  Proceedings 49: Proceedings of the 29th Annual Conference on Learning Theory\n  (COLT 2016)", "journal-ref": "JMLR W&CP 49: COLT 2016 Proceedings (2016) 1-31", "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we study a model-based approach to calculating approximately\noptimal policies in Markovian Decision Processes. In particular, we derive\nnovel bounds on the loss of using a policy derived from a factored linear\nmodel, a class of models which generalize numerous previous models out of those\nthat come with strong computational guarantees. For the first time in the\nliterature, we derive performance bounds for model-based techniques where the\nmodel inaccuracy is measured in weighted norms. Moreover, our bounds show a\ndecreased sensitivity to the discount factor and, unlike similar bounds derived\nfor other approaches, they are insensitive to measure mismatch. Similarly to\nprevious works, our proofs are also based on contraction arguments, but with\nthe main differences that we use carefully constructed norms building on Banach\nlattices, and the contraction property is only assumed for operators acting on\n\"compressed\" spaces, thus weakening previous assumptions, while strengthening\nprevious results.\n", "versions": [{"version": "v1", "created": "Fri, 19 Feb 2016 23:46:11 GMT"}, {"version": "v2", "created": "Tue, 20 Sep 2016 23:36:02 GMT"}], "update_date": "2016-09-22", "authors_parsed": [["Pires", "Bernardo \u00c1vila", ""], ["Szepesv\u00e1ri", "Csaba", ""]]}, {"id": "1602.06468", "submitter": "Yuyu Zhang", "authors": "Yuyu Zhang, Mohammad Taha Bahadori, Hang Su, Jimeng Sun", "title": "FLASH: Fast Bayesian Optimization for Data Analytic Pipelines", "comments": "21 pages, KDD 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modern data science relies on data analytic pipelines to organize\ninterdependent computational steps. Such analytic pipelines often involve\ndifferent algorithms across multiple steps, each with its own hyperparameters.\nTo achieve the best performance, it is often critical to select optimal\nalgorithms and to set appropriate hyperparameters, which requires large\ncomputational efforts. Bayesian optimization provides a principled way for\nsearching optimal hyperparameters for a single algorithm. However, many\nchallenges remain in solving pipeline optimization problems with\nhigh-dimensional and highly conditional search space. In this work, we propose\nFast LineAr SearcH (FLASH), an efficient method for tuning analytic pipelines.\nFLASH is a two-layer Bayesian optimization framework, which firstly uses a\nparametric model to select promising algorithms, then computes a nonparametric\nmodel to fine-tune hyperparameters of the promising algorithms. FLASH also\nincludes an effective caching algorithm which can further accelerate the search\nprocess. Extensive experiments on a number of benchmark datasets have\ndemonstrated that FLASH significantly outperforms previous state-of-the-art\nmethods in both search speed and accuracy. Using 50% of the time budget, FLASH\nachieves up to 20% improvement on test error rate compared to the baselines.\nFLASH also yields state-of-the-art performance on a real-world application for\nhealthcare predictive modeling.\n", "versions": [{"version": "v1", "created": "Sat, 20 Feb 2016 21:56:49 GMT"}, {"version": "v2", "created": "Mon, 13 Jun 2016 02:06:27 GMT"}, {"version": "v3", "created": "Fri, 24 Jun 2016 01:28:23 GMT"}], "update_date": "2016-06-27", "authors_parsed": [["Zhang", "Yuyu", ""], ["Bahadori", "Mohammad Taha", ""], ["Su", "Hang", ""], ["Sun", "Jimeng", ""]]}, {"id": "1602.06489", "submitter": "Chencheng Li", "authors": "Chencheng Li and Pan Zhou and Yingxue Zhou and Kaigui Bian and Tao\n  Jiang and Susanto Rahardja", "title": "Distributed Private Online Learning for Social Big Data Computing over\n  Data Center Networks", "comments": "ICC2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.LG cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the rapid growth of Internet technologies, cloud computing and social\nnetworks have become ubiquitous. An increasing number of people participate in\nsocial networks and massive online social data are obtained. In order to\nexploit knowledge from copious amounts of data obtained and predict social\nbehavior of users, we urge to realize data mining in social networks. Almost\nall online websites use cloud services to effectively process the large scale\nof social data, which are gathered from distributed data centers. These data\nare so large-scale, high-dimension and widely distributed that we propose a\ndistributed sparse online algorithm to handle them. Additionally,\nprivacy-protection is an important point in social networks. We should not\ncompromise the privacy of individuals in networks, while these social data are\nbeing learned for data mining. Thus we also consider the privacy problem in\nthis article. Our simulations shows that the appropriate sparsity of data would\nenhance the performance of our algorithm and the privacy-preserving method does\nnot significantly hurt the performance of the proposed algorithm.\n", "versions": [{"version": "v1", "created": "Sun, 21 Feb 2016 02:32:25 GMT"}], "update_date": "2016-02-23", "authors_parsed": [["Li", "Chencheng", ""], ["Zhou", "Pan", ""], ["Zhou", "Yingxue", ""], ["Bian", "Kaigui", ""], ["Jiang", "Tao", ""], ["Rahardja", "Susanto", ""]]}, {"id": "1602.06516", "submitter": "Debarghya Ghoshdastidar", "authors": "Debarghya Ghoshdastidar, Ambedkar Dukkipati", "title": "Uniform Hypergraph Partitioning: Provable Tensor Methods and Sampling\n  Techniques", "comments": "To appear in Journal of Machine Learning Research (vol 18)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In a series of recent works, we have generalised the consistency results in\nthe stochastic block model literature to the case of uniform and non-uniform\nhypergraphs. The present paper continues the same line of study, where we focus\non partitioning weighted uniform hypergraphs---a problem often encountered in\ncomputer vision. This work is motivated by two issues that arise when a\nhypergraph partitioning approach is used to tackle computer vision problems:\n(i) The uniform hypergraphs constructed for higher-order learning contain all\nedges, but most have negligible weights. Thus, the adjacency tensor is nearly\nsparse, and yet, not binary. (ii) A more serious concern is that standard\npartitioning algorithms need to compute all edge weights, which is\ncomputationally expensive for hypergraphs. This is usually resolved in practice\nby merging the clustering algorithm with a tensor sampling strategy---an\napproach that is yet to be analysed rigorously. We build on our earlier work on\npartitioning dense unweighted uniform hypergraphs (Ghoshdastidar and Dukkipati,\nICML, 2015), and address the aforementioned issues by proposing provable and\nefficient partitioning algorithms. Our analysis justifies the empirical success\nof practical sampling techniques. We also complement our theoretical findings\nby elaborate empirical comparison of various hypergraph partitioning schemes.\n", "versions": [{"version": "v1", "created": "Sun, 21 Feb 2016 10:52:42 GMT"}, {"version": "v2", "created": "Sun, 6 Mar 2016 09:10:27 GMT"}, {"version": "v3", "created": "Thu, 1 Dec 2016 17:49:56 GMT"}, {"version": "v4", "created": "Wed, 17 May 2017 07:26:18 GMT"}], "update_date": "2017-05-18", "authors_parsed": [["Ghoshdastidar", "Debarghya", ""], ["Dukkipati", "Ambedkar", ""]]}, {"id": "1602.06518", "submitter": "Anastasia Pentina", "authors": "Anastasia Pentina and Christoph H. Lampert", "title": "Multi-Task Learning with Labeled and Unlabeled Tasks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In multi-task learning, a learner is given a collection of prediction tasks\nand needs to solve all of them. In contrast to previous work, which required\nthat annotated training data is available for all tasks, we consider a new\nsetting, in which for some tasks, potentially most of them, only unlabeled\ntraining data is provided. Consequently, to solve all tasks, information must\nbe transferred between tasks with labels and tasks without labels. Focusing on\nan instance-based transfer method we analyze two variants of this setting: when\nthe set of labeled tasks is fixed, and when it can be actively selected by the\nlearner. We state and prove a generalization bound that covers both scenarios\nand derive from it an algorithm for making the choice of labeled tasks (in the\nactive case) and for transferring information between the tasks in a principled\nway. We also illustrate the effectiveness of the algorithm by experiments on\nsynthetic and real data.\n", "versions": [{"version": "v1", "created": "Sun, 21 Feb 2016 11:18:10 GMT"}, {"version": "v2", "created": "Wed, 30 Mar 2016 09:30:21 GMT"}, {"version": "v3", "created": "Wed, 1 Mar 2017 12:22:56 GMT"}, {"version": "v4", "created": "Thu, 8 Jun 2017 09:14:03 GMT"}], "update_date": "2017-06-09", "authors_parsed": [["Pentina", "Anastasia", ""], ["Lampert", "Christoph H.", ""]]}, {"id": "1602.06522", "submitter": "Carlo Vittorio Cannistraci", "authors": "Josephine Maria Thomas, Alessandro Muscoloni, Sara Ciucci, Ginestra\n  Bianconi and Carlo Vittorio Cannistraci", "title": "Machine learning meets network science: dimensionality reduction for\n  fast and efficient embedding of networks in the hyperbolic space", "comments": null, "journal-ref": "Nature Communications 8, 1615 (2017)", "doi": "10.1038/s41467-017-01825-5", "report-no": null, "categories": "cond-mat.dis-nn cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Complex network topologies and hyperbolic geometry seem specularly connected,\nand one of the most fascinating and challenging problems of recent complex\nnetwork theory is to map a given network to its hyperbolic space. The\nPopularity Similarity Optimization (PSO) model represents - at the moment - the\nclimax of this theory. It suggests that the trade-off between node popularity\nand similarity is a mechanism to explain how complex network topologies emerge\n- as discrete samples - from the continuous world of hyperbolic geometry. The\nhyperbolic space seems appropriate to represent real complex networks. In fact,\nit preserves many of their fundamental topological properties, and can be\nexploited for real applications such as, among others, link prediction and\ncommunity detection. Here, we observe for the first time that a\ntopological-based machine learning class of algorithms - for nonlinear\nunsupervised dimensionality reduction - can directly approximate the network's\nnode angular coordinates of the hyperbolic model into a two-dimensional space,\naccording to a similar topological organization that we named angular\ncoalescence. On the basis of this phenomenon, we propose a new class of\nalgorithms that offers fast and accurate coalescent embedding of networks in\nthe hyperbolic space even for graphs with thousands of nodes.\n", "versions": [{"version": "v1", "created": "Sun, 21 Feb 2016 12:39:58 GMT"}], "update_date": "2017-12-08", "authors_parsed": [["Thomas", "Josephine Maria", ""], ["Muscoloni", "Alessandro", ""], ["Ciucci", "Sara", ""], ["Bianconi", "Ginestra", ""], ["Cannistraci", "Carlo Vittorio", ""]]}, {"id": "1602.06531", "submitter": "Anastasia Pentina", "authors": "Anastasia Pentina and Shai Ben-David", "title": "Multi-task and Lifelong Learning of Kernels", "comments": "Appears in Proceedings of ALT 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider a problem of learning kernels for use in SVM classification in\nthe multi-task and lifelong scenarios and provide generalization bounds on the\nerror of a large margin classifier. Our results show that, under mild\nconditions on the family of kernels used for learning, solving several related\ntasks simultaneously is beneficial over single task learning. In particular, as\nthe number of observed tasks grows, assuming that in the considered family of\nkernels there exists one that yields low approximation error on all tasks, the\noverhead associated with learning such a kernel vanishes and the complexity\nconverges to that of learning when this good kernel is given to the learner.\n", "versions": [{"version": "v1", "created": "Sun, 21 Feb 2016 14:05:48 GMT"}, {"version": "v2", "created": "Thu, 18 Aug 2016 11:35:32 GMT"}], "update_date": "2016-08-19", "authors_parsed": [["Pentina", "Anastasia", ""], ["Ben-David", "Shai", ""]]}, {"id": "1602.06539", "submitter": "Liangcheng Liu", "authors": "Liangchen Liu and Arnold Wiliem and Shaokang Chen and Kun Zhao and\n  Brian C. Lovell", "title": "Determining the best attributes for surveillance video keywords\n  generation", "comments": "7 pages, ISBA 2016. arXiv admin note: text overlap with\n  arXiv:1602.01940", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automatic video keyword generation is one of the key ingredients in reducing\nthe burden of security officers in analyzing surveillance videos. Keywords or\nattributes are generally chosen manually based on expert knowledge of\nsurveillance. Most existing works primarily aim at either supervised learning\napproaches relying on extensive manual labelling or hierarchical probabilistic\nmodels that assume the features are extracted using the bag-of-words approach;\nthus limiting the utilization of the other features. To address this, we turn\nour attention to automatic attribute discovery approaches. However, it is not\nclear which automatic discovery approach can discover the most meaningful\nattributes. Furthermore, little research has been done on how to compare and\nchoose the best automatic attribute discovery methods. In this paper, we\npropose a novel approach, based on the shared structure exhibited amongst\nmeaningful attributes, that enables us to compare between different automatic\nattribute discovery approaches.We then validate our approach by comparing\nvarious attribute discovery methods such as PiCoDeS on two attribute datasets.\nThe evaluation shows that our approach is able to select the automatic\ndiscovery approach that discovers the most meaningful attributes. We then\nemploy the best discovery approach to generate keywords for videos recorded\nfrom a surveillance system. This work shows it is possible to massively reduce\nthe amount of manual work in generating video keywords without limiting\nourselves to a particular video feature descriptor.\n", "versions": [{"version": "v1", "created": "Sun, 21 Feb 2016 15:08:51 GMT"}], "update_date": "2016-02-23", "authors_parsed": [["Liu", "Liangchen", ""], ["Wiliem", "Arnold", ""], ["Chen", "Shaokang", ""], ["Zhao", "Kun", ""], ["Lovell", "Brian C.", ""]]}, {"id": "1602.06550", "submitter": "Igor Melnyk", "authors": "Igor Melnyk, Arindam Banerjee, Bryan Matthews, and Nikunj Oza", "title": "Semi-Markov Switching Vector Autoregressive Model-based Anomaly\n  Detection in Aviation Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work we consider the problem of anomaly detection in heterogeneous,\nmultivariate, variable-length time series datasets. Our focus is on the\naviation safety domain, where data objects are flights and time series are\nsensor readings and pilot switches. In this context the goal is to detect\nanomalous flight segments, due to mechanical, environmental, or human factors\nin order to identifying operationally significant events and provide insights\ninto the flight operations and highlight otherwise unavailable potential safety\nrisks and precursors to accidents. For this purpose, we propose a framework\nwhich represents each flight using a semi-Markov switching vector\nautoregressive (SMS-VAR) model. Detection of anomalies is then based on\nmeasuring dissimilarities between the model's prediction and data observation.\nThe framework is scalable, due to the inherent parallel nature of most\ncomputations, and can be used to perform online anomaly detection. Extensive\nexperimental results on simulated and real datasets illustrate that the\nframework can detect various types of anomalies along with the key parameters\ninvolved.\n", "versions": [{"version": "v1", "created": "Sun, 21 Feb 2016 16:55:36 GMT"}, {"version": "v2", "created": "Sun, 28 Feb 2016 23:12:31 GMT"}], "update_date": "2016-03-01", "authors_parsed": [["Melnyk", "Igor", ""], ["Banerjee", "Arindam", ""], ["Matthews", "Bryan", ""], ["Oza", "Nikunj", ""]]}, {"id": "1602.06561", "submitter": "Jan Hendrik Witte", "authors": "J. B. Heaton, N. G. Polson, J. H. Witte", "title": "Deep Learning in Finance", "comments": "20 Pages, 5 Figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We explore the use of deep learning hierarchical models for problems in\nfinancial prediction and classification. Financial prediction problems -- such\nas those presented in designing and pricing securities, constructing\nportfolios, and risk management -- often involve large data sets with complex\ndata interactions that currently are difficult or impossible to specify in a\nfull economic model. Applying deep learning methods to these problems can\nproduce more useful results than standard methods in finance. In particular,\ndeep learning can detect and exploit interactions in the data that are, at\nleast currently, invisible to any existing financial economic theory.\n", "versions": [{"version": "v1", "created": "Sun, 21 Feb 2016 18:19:56 GMT"}, {"version": "v2", "created": "Tue, 23 Feb 2016 19:14:51 GMT"}, {"version": "v3", "created": "Sun, 14 Jan 2018 13:19:20 GMT"}], "update_date": "2018-01-16", "authors_parsed": [["Heaton", "J. B.", ""], ["Polson", "N. G.", ""], ["Witte", "J. H.", ""]]}, {"id": "1602.06566", "submitter": "Mohammad Islam", "authors": "Dipayan Maiti and Mohammad Raihanul Islam and Scotland Leman and Naren\n  Ramakrishnan", "title": "Interactive Storytelling over Document Collections", "comments": "This paper has been submitted to a conference for review", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Storytelling algorithms aim to 'connect the dots' between disparate documents\nby linking starting and ending documents through a series of intermediate\ndocuments. Existing storytelling algorithms are based on notions of coherence\nand connectivity, and thus the primary way by which users can steer the story\nconstruction is via design of suitable similarity functions. We present an\nalternative approach to storytelling wherein the user can interactively and\niteratively provide 'must use' constraints to preferentially support the\nconstruction of some stories over others. The three innovations in our approach\nare distance measures based on (inferred) topic distributions, the use of\nconstraints to define sets of linear inequalities over paths, and the\nintroduction of slack and surplus variables to condition the topic distribution\nto preferentially emphasize desired terms over others. We describe experimental\nresults to illustrate the effectiveness of our interactive storytelling\napproach over multiple text datasets.\n", "versions": [{"version": "v1", "created": "Sun, 21 Feb 2016 18:46:35 GMT"}], "update_date": "2016-02-23", "authors_parsed": [["Maiti", "Dipayan", ""], ["Islam", "Mohammad Raihanul", ""], ["Leman", "Scotland", ""], ["Ramakrishnan", "Naren", ""]]}, {"id": "1602.06577", "submitter": "Ping Li", "authors": "Ping Li, Michael Mitzenmacher, Anshumali Shrivastava", "title": "2-Bit Random Projections, NonLinear Estimators, and Approximate Near\n  Neighbor Search", "comments": "arXiv admin note: text overlap with arXiv:1403.8144", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.DS cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The method of random projections has become a standard tool for machine\nlearning, data mining, and search with massive data at Web scale. The effective\nuse of random projections requires efficient coding schemes for quantizing\n(real-valued) projected data into integers. In this paper, we focus on a simple\n2-bit coding scheme. In particular, we develop accurate nonlinear estimators of\ndata similarity based on the 2-bit strategy. This work will have important\npractical applications. For example, in the task of near neighbor search, a\ncrucial step (often called re-ranking) is to compute or estimate data\nsimilarities once a set of candidate data points have been identified by hash\ntable techniques. This re-ranking step can take advantage of the proposed\ncoding scheme and estimator.\n  As a related task, in this paper, we also study a simple uniform quantization\nscheme for the purpose of building hash tables with projected data. Our\nanalysis shows that typically only a small number of bits are needed. For\nexample, when the target similarity level is high, 2 or 3 bits might be\nsufficient. When the target similarity level is not so high, it is preferable\nto use only 1 or 2 bits. Therefore, a 2-bit scheme appears to be overall a good\nchoice for the task of sublinear time approximate near neighbor search via hash\ntables.\n  Combining these results, we conclude that 2-bit random projections should be\nrecommended for approximate near neighbor search and similarity estimation.\nExtensive experimental results are provided.\n", "versions": [{"version": "v1", "created": "Sun, 21 Feb 2016 20:46:13 GMT"}], "update_date": "2016-02-23", "authors_parsed": [["Li", "Ping", ""], ["Mitzenmacher", "Michael", ""], ["Shrivastava", "Anshumali", ""]]}, {"id": "1602.06586", "submitter": "Weihao Kong", "authors": "Qingqing Huang, Sham M. Kakade, Weihao Kong, Gregory Valiant", "title": "Recovering Structured Probability Matrices", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of accurately recovering a matrix B of size M by M ,\nwhich represents a probability distribution over M2 outcomes, given access to\nan observed matrix of \"counts\" generated by taking independent samples from the\ndistribution B. How can structural properties of the underlying matrix B be\nleveraged to yield computationally efficient and information theoretically\noptimal reconstruction algorithms? When can accurate reconstruction be\naccomplished in the sparse data regime? This basic problem lies at the core of\na number of questions that are currently being considered by different\ncommunities, including building recommendation systems and collaborative\nfiltering in the sparse data regime, community detection in sparse random\ngraphs, learning structured models such as topic models or hidden Markov\nmodels, and the efforts from the natural language processing community to\ncompute \"word embeddings\".\n  Our results apply to the setting where B has a low rank structure. For this\nsetting, we propose an efficient algorithm that accurately recovers the\nunderlying M by M matrix using Theta(M) samples. This result easily translates\nto Theta(M) sample algorithms for learning topic models and learning hidden\nMarkov Models. These linear sample complexities are optimal, up to constant\nfactors, in an extremely strong sense: even testing basic properties of the\nunderlying matrix (such as whether it has rank 1 or 2) requires Omega(M)\nsamples. We provide an even stronger lower bound where distinguishing whether a\nsequence of observations were drawn from the uniform distribution over M\nobservations versus being generated by an HMM with two hidden states requires\nOmega(M) observations. This precludes sublinear-sample hypothesis tests for\nbasic properties, such as identity or uniformity, as well as sublinear sample\nestimators for quantities such as the entropy rate of HMMs.\n", "versions": [{"version": "v1", "created": "Sun, 21 Feb 2016 21:47:45 GMT"}, {"version": "v2", "created": "Mon, 29 Feb 2016 22:20:50 GMT"}, {"version": "v3", "created": "Sat, 9 Apr 2016 06:59:37 GMT"}, {"version": "v4", "created": "Fri, 15 Apr 2016 04:58:58 GMT"}, {"version": "v5", "created": "Thu, 9 Nov 2017 22:01:09 GMT"}, {"version": "v6", "created": "Tue, 6 Feb 2018 02:25:08 GMT"}], "update_date": "2018-02-07", "authors_parsed": [["Huang", "Qingqing", ""], ["Kakade", "Sham M.", ""], ["Kong", "Weihao", ""], ["Valiant", "Gregory", ""]]}, {"id": "1602.06612", "submitter": "Soledad Villar", "authors": "Dustin G. Mixon, Soledad Villar, Rachel Ward", "title": "Clustering subgaussian mixtures by semidefinite programming", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.DS cs.IT cs.LG math.IT math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a model-free relax-and-round algorithm for k-means clustering\nbased on a semidefinite relaxation due to Peng and Wei. The algorithm\ninterprets the SDP output as a denoised version of the original data and then\nrounds this output to a hard clustering. We provide a generic method for\nproving performance guarantees for this algorithm, and we analyze the algorithm\nin the context of subgaussian mixture models. We also study the fundamental\nlimits of estimating Gaussian centers by k-means clustering in order to compare\nour approximation guarantee to the theoretically optimal k-means clustering\nsolution.\n", "versions": [{"version": "v1", "created": "Mon, 22 Feb 2016 00:19:20 GMT"}, {"version": "v2", "created": "Tue, 10 May 2016 19:38:37 GMT"}], "update_date": "2016-05-11", "authors_parsed": [["Mixon", "Dustin G.", ""], ["Villar", "Soledad", ""], ["Ward", "Rachel", ""]]}, {"id": "1602.06654", "submitter": "Chunhua Shen", "authors": "Guosheng Lin, Fayao Liu, Chunhua Shen, Jianxin Wu, Heng Tao Shen", "title": "Structured Learning of Binary Codes with Column Generation", "comments": "20 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hashing methods aim to learn a set of hash functions which map the original\nfeatures to compact binary codes with similarity preserving in the Hamming\nspace. Hashing has proven a valuable tool for large-scale information\nretrieval. We propose a column generation based binary code learning framework\nfor data-dependent hash function learning. Given a set of triplets that encode\nthe pairwise similarity comparison information, our column generation based\nmethod learns hash functions that preserve the relative comparison relations\nwithin the large-margin learning framework. Our method iteratively learns the\nbest hash functions during the column generation procedure. Existing hashing\nmethods optimize over simple objectives such as the reconstruction error or\ngraph Laplacian related loss functions, instead of the performance evaluation\ncriteria of interest---multivariate performance measures such as the AUC and\nNDCG. Our column generation based method can be further generalized from the\ntriplet loss to a general structured learning based framework that allows one\nto directly optimize multivariate performance measures. For optimizing general\nranking measures, the resulting optimization problem can involve exponentially\nor infinitely many variables and constraints, which is more challenging than\nstandard structured output learning. We use a combination of column generation\nand cutting-plane techniques to solve the optimization problem. To speed-up the\ntraining we further explore stage-wise training and propose to use a simplified\nNDCG loss for efficient inference. We demonstrate the generality of our method\nby applying it to ranking prediction and image retrieval, and show that it\noutperforms a few state-of-the-art hashing methods.\n", "versions": [{"version": "v1", "created": "Mon, 22 Feb 2016 06:02:25 GMT"}], "update_date": "2016-02-23", "authors_parsed": [["Lin", "Guosheng", ""], ["Liu", "Fayao", ""], ["Shen", "Chunhua", ""], ["Wu", "Jianxin", ""], ["Shen", "Heng Tao", ""]]}, {"id": "1602.06662", "submitter": "Mikael Henaff", "authors": "Mikael Henaff, Arthur Szlam, Yann LeCun", "title": "Recurrent Orthogonal Networks and Long-Memory Tasks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Although RNNs have been shown to be powerful tools for processing sequential\ndata, finding architectures or optimization strategies that allow them to model\nvery long term dependencies is still an active area of research. In this work,\nwe carefully analyze two synthetic datasets originally outlined in (Hochreiter\nand Schmidhuber, 1997) which are used to evaluate the ability of RNNs to store\ninformation over many time steps. We explicitly construct RNN solutions to\nthese problems, and using these constructions, illuminate both the problems\nthemselves and the way in which RNNs store different types of information in\ntheir hidden states. These constructions furthermore explain the success of\nrecent methods that specify unitary initializations or constraints on the\ntransition matrices.\n", "versions": [{"version": "v1", "created": "Mon, 22 Feb 2016 06:51:25 GMT"}, {"version": "v2", "created": "Wed, 15 Mar 2017 17:45:08 GMT"}], "update_date": "2017-03-16", "authors_parsed": [["Henaff", "Mikael", ""], ["Szlam", "Arthur", ""], ["LeCun", "Yann", ""]]}, {"id": "1602.06687", "submitter": "Margareta Ackerman Margareta Ackerman", "authors": "Margareta Ackerman, Andreas Adolfsson, and Naomi Brownstein", "title": "An Effective and Efficient Approach for Clusterability Evaluation", "comments": "10 pages, 2 tables, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Clustering is an essential data mining tool that aims to discover inherent\ncluster structure in data. As such, the study of clusterability, which\nevaluates whether data possesses such structure, is an integral part of cluster\nanalysis. Yet, despite their central role in the theory and application of\nclustering, current notions of clusterability fall short in two crucial aspects\nthat render them impractical; most are computationally infeasible and others\nfail to classify the structure of real datasets.\n  In this paper, we propose a novel approach to clusterability evaluation that\nis both computationally efficient and successfully captures the structure of\nreal data. Our method applies multimodality tests to the (one-dimensional) set\nof pairwise distances based on the original, potentially high-dimensional data.\nWe present extensive analyses of our approach for both the Dip and Silverman\nmultimodality tests on real data as well as 17,000 simulations, demonstrating\nthe success of our approach as the first practical notion of clusterability.\n", "versions": [{"version": "v1", "created": "Mon, 22 Feb 2016 09:01:10 GMT"}], "update_date": "2016-02-24", "authors_parsed": [["Ackerman", "Margareta", ""], ["Adolfsson", "Andreas", ""], ["Brownstein", "Naomi", ""]]}, {"id": "1602.06709", "submitter": "Dheevatsa Mudigere", "authors": "Dipankar Das, Sasikanth Avancha, Dheevatsa Mudigere, Karthikeyan\n  Vaidynathan, Srinivas Sridharan, Dhiraj Kalamkar, Bharat Kaul, Pradeep Dubey", "title": "Distributed Deep Learning Using Synchronous Stochastic Gradient Descent", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We design and implement a distributed multinode synchronous SGD algorithm,\nwithout altering hyper parameters, or compressing data, or altering algorithmic\nbehavior. We perform a detailed analysis of scaling, and identify optimal\ndesign points for different networks. We demonstrate scaling of CNNs on 100s of\nnodes, and present what we believe to be record training throughputs. A 512\nminibatch VGG-A CNN training run is scaled 90X on 128 nodes. Also 256 minibatch\nVGG-A and OverFeat-FAST networks are scaled 53X and 42X respectively on a 64\nnode cluster. We also demonstrate the generality of our approach via\nbest-in-class 6.5X scaling for a 7-layer DNN on 16 nodes. Thereafter we attempt\nto democratize deep-learning by training on an Ethernet based AWS cluster and\nshow ~14X scaling on 16 nodes.\n", "versions": [{"version": "v1", "created": "Mon, 22 Feb 2016 10:31:24 GMT"}], "update_date": "2016-03-02", "authors_parsed": [["Das", "Dipankar", ""], ["Avancha", "Sasikanth", ""], ["Mudigere", "Dheevatsa", ""], ["Vaidynathan", "Karthikeyan", ""], ["Sridharan", "Srinivas", ""], ["Kalamkar", "Dhiraj", ""], ["Kaul", "Bharat", ""], ["Dubey", "Pradeep", ""]]}, {"id": "1602.06725", "submitter": "Andriy Mnih", "authors": "Andriy Mnih, Danilo J. Rezende", "title": "Variational inference for Monte Carlo objectives", "comments": "Appears in Proceedings of the 33rd International Conference on\n  Machine Learning (ICML), New York, NY, USA, 2016. JMLR: W&CP volume 48", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent progress in deep latent variable models has largely been driven by the\ndevelopment of flexible and scalable variational inference methods. Variational\ntraining of this type involves maximizing a lower bound on the log-likelihood,\nusing samples from the variational posterior to compute the required gradients.\nRecently, Burda et al. (2016) have derived a tighter lower bound using a\nmulti-sample importance sampling estimate of the likelihood and showed that\noptimizing it yields models that use more of their capacity and achieve higher\nlikelihoods. This development showed the importance of such multi-sample\nobjectives and explained the success of several related approaches.\n  We extend the multi-sample approach to discrete latent variables and analyze\nthe difficulty encountered when estimating the gradients involved. We then\ndevelop the first unbiased gradient estimator designed for importance-sampled\nobjectives and evaluate it at training generative and structured output\nprediction models. The resulting estimator, which is based on low-variance\nper-sample learning signals, is both simpler and more effective than the NVIL\nestimator proposed for the single-sample variational objective, and is\ncompetitive with the currently used biased estimators.\n", "versions": [{"version": "v1", "created": "Mon, 22 Feb 2016 11:06:06 GMT"}, {"version": "v2", "created": "Wed, 1 Jun 2016 16:36:06 GMT"}], "update_date": "2016-06-02", "authors_parsed": [["Mnih", "Andriy", ""], ["Rezende", "Danilo J.", ""]]}, {"id": "1602.06746", "submitter": "Bjoern Andres", "authors": "Iaroslav Shcherbatyi and Bjoern Andres", "title": "Convexification of Learning from Constraints", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Regularized empirical risk minimization with constrained labels (in contrast\nto fixed labels) is a remarkably general abstraction of learning. For common\nloss and regularization functions, this optimization problem assumes the form\nof a mixed integer program (MIP) whose objective function is non-convex. In\nthis form, the problem is resistant to standard optimization techniques. We\nconstruct MIPs with the same solutions whose objective functions are convex.\nSpecifically, we characterize the tightest convex extension of the objective\nfunction, given by the Legendre-Fenchel biconjugate. Computing values of this\ntightest convex extension is NP-hard. However, by applying our characterization\nto every function in an additive decomposition of the objective function, we\nobtain a class of looser convex extensions that can be computed efficiently.\nFor some decompositions, common loss and regularization functions, we derive a\nclosed form.\n", "versions": [{"version": "v1", "created": "Mon, 22 Feb 2016 12:20:50 GMT"}], "update_date": "2016-02-23", "authors_parsed": [["Shcherbatyi", "Iaroslav", ""], ["Andres", "Bjoern", ""]]}, {"id": "1602.06818", "submitter": "Yubao Sun", "authors": "Yubao Sun, Renlong Hang, Qingshan Liu, Fuping Zhu, Hucheng Pei", "title": "Graph Regularized Low Rank Representation for Aerosol Optical Depth\n  Retrieval", "comments": "16 pages, 6 figures", "journal-ref": null, "doi": "10.1080/01431161.2016.1249302", "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a novel data-driven regression model for aerosol\noptical depth (AOD) retrieval. First, we adopt a low rank representation (LRR)\nmodel to learn a powerful representation of the spectral response. Then, graph\nregularization is incorporated into the LRR model to capture the local\nstructure information and the nonlinear property of the remote-sensing data.\nSince it is easy to acquire the rich satellite-retrieval results, we use them\nas a baseline to construct the graph. Finally, the learned feature\nrepresentation is feeded into support vector machine (SVM) to retrieve AOD.\nExperiments are conducted on two widely used data sets acquired by different\nsensors, and the experimental results show that the proposed method can achieve\nsuperior performance compared to the physical models and other state-of-the-art\nempirical models.\n", "versions": [{"version": "v1", "created": "Mon, 22 Feb 2016 15:31:19 GMT"}, {"version": "v2", "created": "Mon, 7 Mar 2016 16:03:39 GMT"}], "update_date": "2016-11-23", "authors_parsed": [["Sun", "Yubao", ""], ["Hang", "Renlong", ""], ["Liu", "Qingshan", ""], ["Zhu", "Fuping", ""], ["Pei", "Hucheng", ""]]}, {"id": "1602.06822", "submitter": "William Whitney", "authors": "William F. Whitney, Michael Chang, Tejas Kulkarni, Joshua B. Tenenbaum", "title": "Understanding Visual Concepts with Continuation Learning", "comments": "Under review as a workshop paper for ICLR 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a neural network architecture and a learning algorithm to\nproduce factorized symbolic representations. We propose to learn these concepts\nby observing consecutive frames, letting all the components of the hidden\nrepresentation except a small discrete set (gating units) be predicted from the\nprevious frame, and let the factors of variation in the next frame be\nrepresented entirely by these discrete gated units (corresponding to symbolic\nrepresentations). We demonstrate the efficacy of our approach on datasets of\nfaces undergoing 3D transformations and Atari 2600 games.\n", "versions": [{"version": "v1", "created": "Mon, 22 Feb 2016 15:38:59 GMT"}], "update_date": "2016-02-23", "authors_parsed": [["Whitney", "William F.", ""], ["Chang", "Michael", ""], ["Kulkarni", "Tejas", ""], ["Tenenbaum", "Joshua B.", ""]]}, {"id": "1602.06863", "submitter": "Guillaume Rabusseau", "authors": "Guillaume Rabusseau and Hachem Kadri", "title": "Higher-Order Low-Rank Regression", "comments": "submitted to ICML 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes an efficient algorithm (HOLRR) to handle regression tasks\nwhere the outputs have a tensor structure. We formulate the regression problem\nas the minimization of a least square criterion under a multilinear rank\nconstraint, a difficult non convex problem. HOLRR computes efficiently an\napproximate solution of this problem, with solid theoretical guarantees. A\nkernel extension is also presented. Experiments on synthetic and real data show\nthat HOLRR outperforms multivariate and multilinear regression methods and is\nconsiderably faster than existing tensor methods.\n", "versions": [{"version": "v1", "created": "Mon, 22 Feb 2016 17:21:11 GMT"}], "update_date": "2016-02-23", "authors_parsed": [["Rabusseau", "Guillaume", ""], ["Kadri", "Hachem", ""]]}, {"id": "1602.06872", "submitter": "Christopher Musco", "authors": "Roy Frostig, Cameron Musco, Christopher Musco, Aaron Sidford", "title": "Principal Component Projection Without Principal Component Analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We show how to efficiently project a vector onto the top principal components\nof a matrix, without explicitly computing these components. Specifically, we\nintroduce an iterative algorithm that provably computes the projection using\nfew calls to any black-box routine for ridge regression.\n  By avoiding explicit principal component analysis (PCA), our algorithm is the\nfirst with no runtime dependence on the number of top principal components. We\nshow that it can be used to give a fast iterative method for the popular\nprincipal component regression problem, giving the first major runtime\nimprovement over the naive method of combining PCA with regression.\n  To achieve our results, we first observe that ridge regression can be used to\nobtain a \"smooth projection\" onto the top principal components. We then sharpen\nthis approximation to true projection using a low-degree polynomial\napproximation to the matrix step function. Step function approximation is a\ntopic of long-term interest in scientific computing. We extend prior theory by\nconstructing polynomials with simple iterative structure and rigorously\nanalyzing their behavior under limited precision.\n", "versions": [{"version": "v1", "created": "Mon, 22 Feb 2016 17:52:02 GMT"}, {"version": "v2", "created": "Tue, 26 Nov 2019 17:29:20 GMT"}], "update_date": "2019-11-27", "authors_parsed": [["Frostig", "Roy", ""], ["Musco", "Cameron", ""], ["Musco", "Christopher", ""], ["Sidford", "Aaron", ""]]}, {"id": "1602.06886", "submitter": "Akash Srivastava", "authors": "Akash Srivastava, James Zou and Charles Sutton", "title": "Clustering with a Reject Option: Interactive Clustering as Bayesian\n  Prior Elicitation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A good clustering can help a data analyst to explore and understand a data\nset, but what constitutes a good clustering may depend on domain-specific and\napplication-specific criteria. These criteria can be difficult to formalize,\neven when it is easy for an analyst to know a good clustering when she sees\none. We present a new approach to interactive clustering for data exploration,\ncalled \\ciif, based on a particularly simple feedback mechanism, in which an\nanalyst can choose to reject individual clusters and request new ones. The new\nclusters should be different from previously rejected clusters while still\nfitting the data well. We formalize this interaction in a novel Bayesian prior\nelicitation framework. In each iteration, the prior is adapted to account for\nall the previous feedback, and a new clustering is then produced from the\nposterior distribution. To achieve the computational efficiency necessary for\nan interactive setting, we propose an incremental optimization method over data\nminibatches using Lagrangian relaxation. Experiments demonstrate that \\ciif can\nproduce accurate and diverse clusterings.\n", "versions": [{"version": "v1", "created": "Mon, 22 Feb 2016 18:38:27 GMT"}, {"version": "v2", "created": "Wed, 8 Jun 2016 14:15:58 GMT"}], "update_date": "2016-06-09", "authors_parsed": [["Srivastava", "Akash", ""], ["Zou", "James", ""], ["Sutton", "Charles", ""]]}, {"id": "1602.06916", "submitter": "Abolfazl Hashemi", "authors": "Abolfazl Hashemi, Haris Vikalo", "title": "Sparse Linear Regression via Generalized Orthogonal Least-Squares", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.IT cs.LG math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sparse linear regression, which entails finding a sparse solution to an\nunderdetermined system of linear equations, can formally be expressed as an\n$l_0$-constrained least-squares problem. The Orthogonal Least-Squares (OLS)\nalgorithm sequentially selects the features (i.e., columns of the coefficient\nmatrix) to greedily find an approximate sparse solution. In this paper, a\ngeneralization of Orthogonal Least-Squares which relies on a recursive relation\nbetween the components of the optimal solution to select L features at each\nstep and solve the resulting overdetermined system of equations is proposed.\nSimulation results demonstrate that the generalized OLS algorithm is\ncomputationally efficient and achieves performance superior to that of existing\ngreedy algorithms broadly used in the literature.\n", "versions": [{"version": "v1", "created": "Mon, 22 Feb 2016 19:55:01 GMT"}, {"version": "v2", "created": "Thu, 28 Jul 2016 22:52:35 GMT"}], "update_date": "2016-08-01", "authors_parsed": [["Hashemi", "Abolfazl", ""], ["Vikalo", "Haris", ""]]}, {"id": "1602.06929", "submitter": "Praneeth Netrapalli", "authors": "Prateek Jain and Chi Jin and Sham M. Kakade and Praneeth Netrapalli\n  and Aaron Sidford", "title": "Streaming PCA: Matching Matrix Bernstein and Near-Optimal Finite Sample\n  Guarantees for Oja's Algorithm", "comments": "Updated title", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work provides improved guarantees for streaming principle component\nanalysis (PCA). Given $A_1, \\ldots, A_n\\in \\mathbb{R}^{d\\times d}$ sampled\nindependently from distributions satisfying $\\mathbb{E}[A_i] = \\Sigma$ for\n$\\Sigma \\succeq \\mathbf{0}$, this work provides an $O(d)$-space linear-time\nsingle-pass streaming algorithm for estimating the top eigenvector of $\\Sigma$.\nThe algorithm nearly matches (and in certain cases improves upon) the accuracy\nobtained by the standard batch method that computes top eigenvector of the\nempirical covariance $\\frac{1}{n} \\sum_{i \\in [n]} A_i$ as analyzed by the\nmatrix Bernstein inequality. Moreover, to achieve constant accuracy, our\nalgorithm improves upon the best previous known sample complexities of\nstreaming algorithms by either a multiplicative factor of $O(d)$ or\n$1/\\mathrm{gap}$ where $\\mathrm{gap}$ is the relative distance between the top\ntwo eigenvalues of $\\Sigma$.\n  These results are achieved through a novel analysis of the classic Oja's\nalgorithm, one of the oldest and most popular algorithms for streaming PCA. In\nparticular, this work shows that simply picking a random initial point $w_0$\nand applying the update rule $w_{i + 1} = w_i + \\eta_i A_i w_i$ suffices to\naccurately estimate the top eigenvector, with a suitable choice of $\\eta_i$. We\nbelieve our result sheds light on how to efficiently perform streaming PCA both\nin theory and in practice and we hope that our analysis may serve as the basis\nfor analyzing many variants and extensions of streaming PCA.\n", "versions": [{"version": "v1", "created": "Mon, 22 Feb 2016 20:30:37 GMT"}, {"version": "v2", "created": "Mon, 28 Mar 2016 17:45:51 GMT"}], "update_date": "2016-03-29", "authors_parsed": [["Jain", "Prateek", ""], ["Jin", "Chi", ""], ["Kakade", "Sham M.", ""], ["Netrapalli", "Praneeth", ""], ["Sidford", "Aaron", ""]]}, {"id": "1602.06967", "submitter": "Danila Doroshin", "authors": "Danila Doroshin, Nikolay Lubimov, Marina Nastasenko and Mikhail Kotov", "title": "Blind score normalization method for PLDA based speaker recognition", "comments": "4 pages, 1 figure, presented at the Interspeech 2015. In Sixteenth\n  Annual Conference of the International Speech Communication Association 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG cs.SD", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Probabilistic Linear Discriminant Analysis (PLDA) has become state-of-the-art\nmethod for modeling $i$-vector space in speaker recognition task. However the\nperformance degradation is observed if enrollment data size differs from one\nspeaker to another. This paper presents a solution to such problem by\nintroducing new PLDA scoring normalization technique. Normalization parameters\nare derived in a blind way, so that, unlike traditional \\textit{ZT-norm}, no\nextra development data is required. Moreover, proposed method has shown to be\noptimal in terms of detection cost function. The experiments conducted on NIST\nSRE 2014 database demonstrate an improved accuracy in a mixed enrollment number\ncondition.\n", "versions": [{"version": "v1", "created": "Mon, 22 Feb 2016 21:22:49 GMT"}], "update_date": "2016-02-24", "authors_parsed": [["Doroshin", "Danila", ""], ["Lubimov", "Nikolay", ""], ["Nastasenko", "Marina", ""], ["Kotov", "Mikhail", ""]]}, {"id": "1602.06989", "submitter": "Renato Cordeiro De Amorim", "authors": "Renato Cordeiro de Amorim and Christian Hennig", "title": "Recovering the number of clusters in data sets with noise features using\n  feature rescaling factors", "comments": null, "journal-ref": "Information Sciences 324 (2015), 126-145", "doi": "10.1016/j.ins.2015.06.039", "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we introduce three methods for re-scaling data sets aiming at\nimproving the likelihood of clustering validity indexes to return the true\nnumber of spherical Gaussian clusters with additional noise features. Our\nmethod obtains feature re-scaling factors taking into account the structure of\na given data set and the intuitive idea that different features may have\ndifferent degrees of relevance at different clusters.\n  We experiment with the Silhouette (using squared Euclidean, Manhattan, and\nthe p$^{th}$ power of the Minkowski distance), Dunn's, Calinski-Harabasz and\nHartigan indexes on data sets with spherical Gaussian clusters with and without\nnoise features. We conclude that our methods indeed increase the chances of\nestimating the true number of clusters in a data set.\n", "versions": [{"version": "v1", "created": "Mon, 22 Feb 2016 22:40:00 GMT"}], "update_date": "2016-02-24", "authors_parsed": [["de Amorim", "Renato Cordeiro", ""], ["Hennig", "Christian", ""]]}, {"id": "1602.07017", "submitter": "Zheng Zhang", "authors": "Zheng Zhang, Yong Xu, Jian Yang, Xuelong Li, David Zhang", "title": "A survey of sparse representation: algorithms and applications", "comments": "Published on IEEE Access, Vol. 3, pp. 490-530, 2015", "journal-ref": null, "doi": "10.1109/ACCESS.2015.2430359", "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sparse representation has attracted much attention from researchers in fields\nof signal processing, image processing, computer vision and pattern\nrecognition. Sparse representation also has a good reputation in both\ntheoretical research and practical applications. Many different algorithms have\nbeen proposed for sparse representation. The main purpose of this article is to\nprovide a comprehensive study and an updated review on sparse representation\nand to supply a guidance for researchers. The taxonomy of sparse representation\nmethods can be studied from various viewpoints. For example, in terms of\ndifferent norm minimizations used in sparsity constraints, the methods can be\nroughly categorized into five groups: sparse representation with $l_0$-norm\nminimization, sparse representation with $l_p$-norm (0$<$p$<$1) minimization,\nsparse representation with $l_1$-norm minimization and sparse representation\nwith $l_{2,1}$-norm minimization. In this paper, a comprehensive overview of\nsparse representation is provided. The available sparse representation\nalgorithms can also be empirically categorized into four groups: greedy\nstrategy approximation, constrained optimization, proximity algorithm-based\noptimization, and homotopy algorithm-based sparse representation. The\nrationales of different algorithms in each category are analyzed and a wide\nrange of sparse representation applications are summarized, which could\nsufficiently reveal the potential nature of the sparse representation theory.\nSpecifically, an experimentally comparative study of these sparse\nrepresentation algorithms was presented. The Matlab code used in this paper can\nbe available at: http://www.yongxu.org/lunwen.html.\n", "versions": [{"version": "v1", "created": "Tue, 23 Feb 2016 02:44:53 GMT"}], "update_date": "2016-02-24", "authors_parsed": [["Zhang", "Zheng", ""], ["Xu", "Yong", ""], ["Yang", "Jian", ""], ["Li", "Xuelong", ""], ["Zhang", "David", ""]]}, {"id": "1602.07029", "submitter": "Siddharth Reddy", "authors": "Siddharth Reddy, Igor Labutov, Thorsten Joachims", "title": "Latent Skill Embedding for Personalized Lesson Sequence Recommendation", "comments": "Under review by the ACM SIGKDD Conference on Knowledge Discovery and\n  Data Mining", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Students in online courses generate large amounts of data that can be used to\npersonalize the learning process and improve quality of education. In this\npaper, we present the Latent Skill Embedding (LSE), a probabilistic model of\nstudents and educational content that can be used to recommend personalized\nsequences of lessons with the goal of helping students prepare for specific\nassessments. Akin to collaborative filtering for recommender systems, the\nalgorithm does not require students or content to be described by features, but\nit learns a representation using access traces. We formulate this problem as a\nregularized maximum-likelihood embedding of students, lessons, and assessments\nfrom historical student-content interactions. An empirical evaluation on\nlarge-scale data from Knewton, an adaptive learning technology company, shows\nthat this approach predicts assessment results competitively with benchmark\nmodels and is able to discriminate between lesson sequences that lead to\nmastery and failure.\n", "versions": [{"version": "v1", "created": "Tue, 23 Feb 2016 04:20:40 GMT"}], "update_date": "2016-02-24", "authors_parsed": [["Reddy", "Siddharth", ""], ["Labutov", "Igor", ""], ["Joachims", "Thorsten", ""]]}, {"id": "1602.07031", "submitter": "Mohammad Abu Alsheikh", "authors": "Mohammad Abu Alsheikh, Dusit Niyato, Shaowei Lin, Hwee-Pink Tan, and\n  Zhu Han", "title": "Mobile Big Data Analytics Using Deep Learning and Apache Spark", "comments": null, "journal-ref": "IEEE Network, vol. 30, no. 3, pp. 22-29, June 2016", "doi": "10.1109/MNET.2016.7474340", "report-no": null, "categories": "cs.DC cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The proliferation of mobile devices, such as smartphones and Internet of\nThings (IoT) gadgets, results in the recent mobile big data (MBD) era.\nCollecting MBD is unprofitable unless suitable analytics and learning methods\nare utilized for extracting meaningful information and hidden patterns from\ndata. This article presents an overview and brief tutorial of deep learning in\nMBD analytics and discusses a scalable learning framework over Apache Spark.\nSpecifically, a distributed deep learning is executed as an iterative MapReduce\ncomputing on many Spark workers. Each Spark worker learns a partial deep model\non a partition of the overall MBD, and a master deep model is then built by\naveraging the parameters of all partial models. This Spark-based framework\nspeeds up the learning of deep models consisting of many hidden layers and\nmillions of parameters. We use a context-aware activity recognition application\nwith a real-world dataset containing millions of samples to validate our\nframework and assess its speedup effectiveness.\n", "versions": [{"version": "v1", "created": "Tue, 23 Feb 2016 04:32:02 GMT"}], "update_date": "2016-08-16", "authors_parsed": [["Alsheikh", "Mohammad Abu", ""], ["Niyato", "Dusit", ""], ["Lin", "Shaowei", ""], ["Tan", "Hwee-Pink", ""], ["Han", "Zhu", ""]]}, {"id": "1602.07043", "submitter": "Suresh Venkatasubramanian", "authors": "Philip Adler, Casey Falk, Sorelle A. Friedler, Gabriel Rybeck, Carlos\n  Scheidegger, Brandon Smith and Suresh Venkatasubramanian", "title": "Auditing Black-box Models for Indirect Influence", "comments": "Final version of paper that appears in the IEEE International\n  Conference on Data Mining (ICDM), 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data-trained predictive models see widespread use, but for the most part they\nare used as black boxes which output a prediction or score. It is therefore\nhard to acquire a deeper understanding of model behavior, and in particular how\ndifferent features influence the model prediction. This is important when\ninterpreting the behavior of complex models, or asserting that certain\nproblematic attributes (like race or gender) are not unduly influencing\ndecisions.\n  In this paper, we present a technique for auditing black-box models, which\nlets us study the extent to which existing models take advantage of particular\nfeatures in the dataset, without knowing how the models work. Our work focuses\non the problem of indirect influence: how some features might indirectly\ninfluence outcomes via other, related features. As a result, we can find\nattribute influences even in cases where, upon further direct examination of\nthe model, the attribute is not referred to by the model at all.\n  Our approach does not require the black-box model to be retrained. This is\nimportant if (for example) the model is only accessible via an API, and\ncontrasts our work with other methods that investigate feature influence like\nfeature selection. We present experimental evidence for the effectiveness of\nour procedure using a variety of publicly available datasets and models. We\nalso validate our procedure using techniques from interpretable learning and\nfeature selection, as well as against other black-box auditing procedures.\n", "versions": [{"version": "v1", "created": "Tue, 23 Feb 2016 04:52:28 GMT"}, {"version": "v2", "created": "Wed, 30 Nov 2016 06:55:16 GMT"}], "update_date": "2016-12-01", "authors_parsed": [["Adler", "Philip", ""], ["Falk", "Casey", ""], ["Friedler", "Sorelle A.", ""], ["Rybeck", "Gabriel", ""], ["Scheidegger", "Carlos", ""], ["Smith", "Brandon", ""], ["Venkatasubramanian", "Suresh", ""]]}, {"id": "1602.07046", "submitter": "Simon Du", "authors": "Maria Florina Balcan, Simon S. Du, Yining Wang, Adams Wei Yu", "title": "An Improved Gap-Dependency Analysis of the Noisy Power Method", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the noisy power method algorithm, which has wide applications in\nmachine learning and statistics, especially those related to principal\ncomponent analysis (PCA) under resource (communication, memory or privacy)\nconstraints. Existing analysis of the noisy power method shows an\nunsatisfactory dependency over the \"consecutive\" spectral gap\n$(\\sigma_k-\\sigma_{k+1})$ of an input data matrix, which could be very small\nand hence limits the algorithm's applicability. In this paper, we present a new\nanalysis of the noisy power method that achieves improved gap dependency for\nboth sample complexity and noise tolerance bounds. More specifically, we\nimprove the dependency over $(\\sigma_k-\\sigma_{k+1})$ to dependency over\n$(\\sigma_k-\\sigma_{q+1})$, where $q$ is an intermediate algorithm parameter and\ncould be much larger than the target rank $k$. Our proofs are built upon a\nnovel characterization of proximity between two subspaces that differ from\ncanonical angle characterizations analyzed in previous works. Finally, we apply\nour improved bounds to distributed private PCA and memory-efficient streaming\nPCA and obtain bounds that are superior to existing results in the literature.\n", "versions": [{"version": "v1", "created": "Tue, 23 Feb 2016 05:15:08 GMT"}], "update_date": "2017-02-28", "authors_parsed": [["Balcan", "Maria Florina", ""], ["Du", "Simon S.", ""], ["Wang", "Yining", ""], ["Yu", "Adams Wei", ""]]}, {"id": "1602.07107", "submitter": "Richard Combes", "authors": "Thomas Bonald and Richard Combes", "title": "A Streaming Algorithm for Crowdsourced Data Classification", "comments": "23 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a streaming algorithm for the binary classification of data based\non crowdsourcing. The algorithm learns the competence of each labeller by\ncomparing her labels to those of other labellers on the same tasks and uses\nthis information to minimize the prediction error rate on each task. We provide\nperformance guarantees of our algorithm for a fixed population of independent\nlabellers. In particular, we show that our algorithm is optimal in the sense\nthat the cumulative regret compared to the optimal decision with known labeller\nerror probabilities is finite, independently of the number of tasks to label.\nThe complexity of the algorithm is linear in the number of labellers and the\nnumber of tasks, up to some logarithmic factors. Numerical experiments\nillustrate the performance of our algorithm compared to existing algorithms,\nincluding simple majority voting and expectation-maximization algorithms, on\nboth synthetic and real datasets.\n", "versions": [{"version": "v1", "created": "Tue, 23 Feb 2016 10:21:58 GMT"}], "update_date": "2016-02-24", "authors_parsed": [["Bonald", "Thomas", ""], ["Combes", "Richard", ""]]}, {"id": "1602.07109", "submitter": "Maximilian Soelch", "authors": "Maximilian Soelch, Justin Bayer, Marvin Ludersdorfer, Patrick van der\n  Smagt", "title": "Variational Inference for On-line Anomaly Detection in High-Dimensional\n  Time Series", "comments": "Accepted as workshop paper at ICLR 2016; accepted as workshop paper\n  for anomaly detection workshop at ICML 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Approximate variational inference has shown to be a powerful tool for\nmodeling unknown complex probability distributions. Recent advances in the\nfield allow us to learn probabilistic models of sequences that actively exploit\nspatial and temporal structure. We apply a Stochastic Recurrent Network (STORN)\nto learn robot time series data. Our evaluation demonstrates that we can\nrobustly detect anomalies both off- and on-line.\n", "versions": [{"version": "v1", "created": "Tue, 23 Feb 2016 10:31:51 GMT"}, {"version": "v2", "created": "Wed, 23 Mar 2016 15:42:23 GMT"}, {"version": "v3", "created": "Tue, 5 Apr 2016 16:56:29 GMT"}, {"version": "v4", "created": "Thu, 19 May 2016 20:20:45 GMT"}, {"version": "v5", "created": "Tue, 14 Jun 2016 10:01:00 GMT"}], "update_date": "2016-06-15", "authors_parsed": [["Soelch", "Maximilian", ""], ["Bayer", "Justin", ""], ["Ludersdorfer", "Marvin", ""], ["van der Smagt", "Patrick", ""]]}, {"id": "1602.07120", "submitter": "Sivan Sabato", "authors": "Sivan Sabato", "title": "Submodular Learning and Covering with Response-Dependent Costs", "comments": null, "journal-ref": "S. Sabato, \"Submodular Learning and Covering with\n  Response-Dependent Costs \", Theoretical Computer Science, 742:98--113, 2018", "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider interactive learning and covering problems, in a setting where\nactions may incur different costs, depending on the response to the action. We\npropose a natural greedy algorithm for response-dependent costs. We bound the\napproximation factor of this greedy algorithm in active learning settings as\nwell as in the general setting. We show that a different property of the cost\nfunction controls the approximation factor in each of these scenarios. We\nfurther show that in both settings, the approximation factor of this greedy\nalgorithm is near-optimal among all greedy algorithms. Experiments demonstrate\nthe advantages of the proposed algorithm in the response-dependent cost\nsetting.\n", "versions": [{"version": "v1", "created": "Tue, 23 Feb 2016 11:20:37 GMT"}, {"version": "v2", "created": "Mon, 29 Aug 2016 12:10:15 GMT"}, {"version": "v3", "created": "Sun, 18 Nov 2018 12:25:25 GMT"}], "update_date": "2018-11-21", "authors_parsed": [["Sabato", "Sivan", ""]]}, {"id": "1602.07182", "submitter": "Gilles Stoltz", "authors": "Aur\\'elien Garivier (IMT), Pierre M\\'enard (IMT), Gilles Stoltz\n  (GREGH)", "title": "Explore First, Exploit Next: The True Shape of Regret in Bandit Problems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.LG stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We revisit lower bounds on the regret in the case of multi-armed bandit\nproblems. We obtain non-asymptotic, distribution-dependent bounds and provide\nstraightforward proofs based only on well-known properties of Kullback-Leibler\ndivergences. These bounds show in particular that in an initial phase the\nregret grows almost linearly, and that the well-known logarithmic growth of the\nregret only holds in a final phase. The proof techniques come to the essence of\nthe information-theoretic arguments used and they are deprived of all\nunnecessary complications.\n", "versions": [{"version": "v1", "created": "Tue, 23 Feb 2016 15:04:13 GMT"}, {"version": "v2", "created": "Fri, 17 Jun 2016 11:25:23 GMT"}, {"version": "v3", "created": "Sat, 13 Oct 2018 12:25:47 GMT"}], "update_date": "2018-10-16", "authors_parsed": [["Garivier", "Aur\u00e9lien", "", "IMT"], ["M\u00e9nard", "Pierre", "", "IMT"], ["Stoltz", "Gilles", "", "GREGH"]]}, {"id": "1602.07194", "submitter": "Matth\\\"aus Kleindessner", "authors": "Matth\\\"aus Kleindessner and Ulrike von Luxburg", "title": "Lens depth function and k-relative neighborhood graph: versatile tools\n  for ordinal data analysis", "comments": null, "journal-ref": "Journal of Machine Learning Research 18(58):1-52, 2017", "doi": null, "report-no": null, "categories": "stat.ML cs.DS cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years it has become popular to study machine learning problems in a\nsetting of ordinal distance information rather than numerical distance\nmeasurements. By ordinal distance information we refer to binary answers to\ndistance comparisons such as $d(A,B)<d(C,D)$. For many problems in machine\nlearning and statistics it is unclear how to solve them in such a scenario. Up\nto now, the main approach is to explicitly construct an ordinal embedding of\nthe data points in the Euclidean space, an approach that has a number of\ndrawbacks. In this paper, we propose algorithms for the problems of medoid\nestimation, outlier identification, classification, and clustering when given\nonly ordinal data. They are based on estimating the lens depth function and the\n$k$-relative neighborhood graph on a data set. Our algorithms are simple, are\nmuch faster than an ordinal embedding approach and avoid some of its drawbacks,\nand can easily be parallelized.\n", "versions": [{"version": "v1", "created": "Tue, 23 Feb 2016 15:30:46 GMT"}, {"version": "v2", "created": "Mon, 24 Jul 2017 11:52:01 GMT"}], "update_date": "2017-07-25", "authors_parsed": [["Kleindessner", "Matth\u00e4us", ""], ["von Luxburg", "Ulrike", ""]]}, {"id": "1602.07264", "submitter": "Giancarlo Crocetti", "authors": "Giancarlo Crocetti, Michael Coakley, Phil Dressner, Wanda Kellum,\n  Tamba Lamin", "title": "A Multivariate Biomarker for Parkinson's Disease", "comments": "5 pages, 4 figures, 3 tables, published at the Research Day at Pace\n  University, New York. Proceedings of 12th Annual Research Day, 2014 - Pace\n  University", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this study, we executed a genomic analysis with the objective of selecting\na set of genes (possibly small) that would help in the detection and\nclassification of samples from patients affected by Parkinson Disease. We\nperformed a complete data analysis and during the exploratory phase, we\nselected a list of differentially expressed genes. Despite their association\nwith the diseased state, we could not use them as a biomarker tool. Therefore,\nour research was extended to include a multivariate analysis approach resulting\nin the identification and selection of a group of 20 genes that showed a clear\npotential in detecting and correctly classify Parkinson Disease samples even in\nthe presence of other neurodegenerative disorders.\n", "versions": [{"version": "v1", "created": "Fri, 15 May 2015 00:55:17 GMT"}], "update_date": "2016-02-24", "authors_parsed": [["Crocetti", "Giancarlo", ""], ["Coakley", "Michael", ""], ["Dressner", "Phil", ""], ["Kellum", "Wanda", ""], ["Lamin", "Tamba", ""]]}, {"id": "1602.07265", "submitter": "Chicheng Zhang", "authors": "Alina Beygelzimer, Daniel Hsu, John Langford, Chicheng Zhang", "title": "Search Improves Label for Active Learning", "comments": "32 pages; NIPS 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate active learning with access to two distinct oracles: Label\n(which is standard) and Search (which is not). The Search oracle models the\nsituation where a human searches a database to seed or counterexample an\nexisting solution. Search is stronger than Label while being natural to\nimplement in many situations. We show that an algorithm using both oracles can\nprovide exponentially large problem-dependent improvements over Label alone.\n", "versions": [{"version": "v1", "created": "Tue, 23 Feb 2016 19:05:09 GMT"}, {"version": "v2", "created": "Mon, 24 Oct 2016 06:29:08 GMT"}], "update_date": "2016-10-25", "authors_parsed": [["Beygelzimer", "Alina", ""], ["Hsu", "Daniel", ""], ["Langford", "John", ""], ["Zhang", "Chicheng", ""]]}, {"id": "1602.07280", "submitter": "Vaibhav Rajan", "authors": "Abhishek Sengupta, Vaibhav Rajan, Sakyajit Bhattacharya, G R K Sarma", "title": "A Statistical Model for Stroke Outcome Prediction and Treatment Planning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Stroke is a major cause of mortality and long--term disability in the world.\nPredictive outcome models in stroke are valuable for personalized treatment,\nrehabilitation planning and in controlled clinical trials. In this paper we\ndesign a new model to predict outcome in the short-term, the putative\ntherapeutic window for several treatments. Our regression-based model has a\nparametric form that is designed to address many challenges common in medical\ndatasets like highly correlated variables and class imbalance. Empirically our\nmodel outperforms the best--known previous models in predicting short--term\noutcomes and in inferring the most effective treatments that improve outcome.\n", "versions": [{"version": "v1", "created": "Mon, 22 Feb 2016 12:51:39 GMT"}], "update_date": "2016-02-24", "authors_parsed": [["Sengupta", "Abhishek", ""], ["Rajan", "Vaibhav", ""], ["Bhattacharya", "Sakyajit", ""], ["Sarma", "G R K", ""]]}, {"id": "1602.07320", "submitter": "Zachary Lipton", "authors": "Zachary C. Lipton", "title": "Stuck in a What? Adventures in Weight Space", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning researchers commonly suggest that converged models are stuck in\nlocal minima. More recently, some researchers observed that under reasonable\nassumptions, the vast majority of critical points are saddle points, not true\nminima. Both descriptions suggest that weights converge around a point in\nweight space, be it a local optima or merely a critical point. However, it's\npossible that neither interpretation is accurate. As neural networks are\ntypically over-complete, it's easy to show the existence of vast continuous\nregions through weight space with equal loss. In this paper, we build on recent\nwork empirically characterizing the error surfaces of neural networks. We\nanalyze training paths through weight space, presenting evidence that apparent\nconvergence of loss does not correspond to weights arriving at critical points,\nbut instead to large movements through flat regions of weight space. While it's\ntrivial to show that neural network error surfaces are globally non-convex, we\nshow that error surfaces are also locally non-convex, even after breaking\nsymmetry with a random initialization and also after partial training.\n", "versions": [{"version": "v1", "created": "Tue, 23 Feb 2016 21:23:24 GMT"}], "update_date": "2016-02-25", "authors_parsed": [["Lipton", "Zachary C.", ""]]}, {"id": "1602.07337", "submitter": "Hao Wu", "authors": "Hao Wu, Xinwei Deng and Naren Ramakrishnan", "title": "Sparse Estimation of Multivariate Poisson Log-Normal Models from Count\n  Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modeling data with multivariate count responses is a challenging problem due\nto the discrete nature of the responses. Existing methods for univariate count\nresponses cannot be easily extended to the multivariate case since the\ndependency among multiple responses needs to be properly accommodated. In this\npaper, we propose a multivariate Poisson log-normal regression model for\nmultivariate data with count responses. By simultaneously estimating the\nregression coefficients and inverse covariance matrix over the latent variables\nwith an efficient Monte Carlo EM algorithm, the proposed regression model takes\nadvantages of association among multiple count responses to improve the model\nprediction performance. Simulation studies and applications to real world data\nare conducted to systematically evaluate the performance of the proposed method\nin comparison with conventional methods.\n", "versions": [{"version": "v1", "created": "Mon, 22 Feb 2016 01:27:08 GMT"}, {"version": "v2", "created": "Thu, 25 Feb 2016 17:15:05 GMT"}, {"version": "v3", "created": "Fri, 12 Aug 2016 18:33:46 GMT"}], "update_date": "2016-08-15", "authors_parsed": [["Wu", "Hao", ""], ["Deng", "Xinwei", ""], ["Ramakrishnan", "Naren", ""]]}, {"id": "1602.07373", "submitter": "Song Wang", "authors": "Song Wang, Dongchun Ren, Li Chen, Wei Fan, Jun Sun, Satoshi Naoi", "title": "On Study of the Binarized Deep Neural Network for Image Classification", "comments": "9 pages, 6 figures. Rejected conference (CVPR 2015) submission.\n  Submission date: November, 2014. This work is patented in China (NO.\n  201410647710.3)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, the deep neural network (derived from the artificial neural\nnetwork) has attracted many researchers' attention by its outstanding\nperformance. However, since this network requires high-performance GPUs and\nlarge storage, it is very hard to use it on individual devices. In order to\nimprove the deep neural network, many trials have been made by refining the\nnetwork structure or training strategy. Unlike those trials, in this paper, we\nfocused on the basic propagation function of the artificial neural network and\nproposed the binarized deep neural network. This network is a pure binary\nsystem, in which all the values and calculations are binarized. As a result,\nour network can save a lot of computational resource and storage. Therefore, it\nis possible to use it on various devices. Moreover, the experimental results\nproved the feasibility of the proposed network.\n", "versions": [{"version": "v1", "created": "Wed, 24 Feb 2016 02:39:47 GMT"}], "update_date": "2016-02-25", "authors_parsed": [["Wang", "Song", ""], ["Ren", "Dongchun", ""], ["Chen", "Li", ""], ["Fan", "Wei", ""], ["Sun", "Jun", ""], ["Naoi", "Satoshi", ""]]}, {"id": "1602.07383", "submitter": "Weiguang Ding", "authors": "Weiguang Ding, Graham Taylor", "title": "Automatic Moth Detection from Trap Images for Pest Management", "comments": "Preprints accepted by Computers and electronics in agriculture", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Monitoring the number of insect pests is a crucial component in\npheromone-based pest management systems. In this paper, we propose an automatic\ndetection pipeline based on deep learning for identifying and counting pests in\nimages taken inside field traps. Applied to a commercial codling moth dataset,\nour method shows promising performance both qualitatively and quantitatively.\nCompared to previous attempts at pest detection, our approach uses no\npest-specific engineering which enables it to adapt to other species and\nenvironments with minimal human effort. It is amenable to implementation on\nparallel hardware and therefore capable of deployment in settings where\nreal-time performance is required.\n", "versions": [{"version": "v1", "created": "Wed, 24 Feb 2016 03:35:42 GMT"}], "update_date": "2016-02-25", "authors_parsed": [["Ding", "Weiguang", ""], ["Taylor", "Graham", ""]]}, {"id": "1602.07387", "submitter": "Peter Kairouz", "authors": "Peter Kairouz and Keith Bonawitz and Daniel Ramage", "title": "Discrete Distribution Estimation under Local Privacy", "comments": "23 pages, 12 figures, submitted to ICML 2016 (under review)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The collection and analysis of user data drives improvements in the app and\nweb ecosystems, but comes with risks to privacy. This paper examines discrete\ndistribution estimation under local privacy, a setting wherein service\nproviders can learn the distribution of a categorical statistic of interest\nwithout collecting the underlying data. We present new mechanisms, including\nhashed K-ary Randomized Response (KRR), that empirically meet or exceed the\nutility of existing mechanisms at all privacy levels. New theoretical results\ndemonstrate the order-optimality of KRR and the existing RAPPOR mechanism at\ndifferent privacy regimes.\n", "versions": [{"version": "v1", "created": "Wed, 24 Feb 2016 03:48:19 GMT"}, {"version": "v2", "created": "Wed, 2 Mar 2016 19:08:36 GMT"}, {"version": "v3", "created": "Wed, 15 Jun 2016 18:26:31 GMT"}], "update_date": "2016-06-16", "authors_parsed": [["Kairouz", "Peter", ""], ["Bonawitz", "Keith", ""], ["Ramage", "Daniel", ""]]}, {"id": "1602.07388", "submitter": "Keith Burghardt", "authors": "Keith Burghardt, Emanuel F. Alsina, Michelle Girvan, William Rand, and\n  Kristina Lerman", "title": "The Myopia of Crowds: A Study of Collective Evaluation on Stack Exchange", "comments": "10 pages, 9 figures", "journal-ref": "PLoS ONE 12(3): e0173610", "doi": "10.1371/journal.pone.0173610", "report-no": null, "categories": "cs.HC cs.CY cs.LG physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Crowds can often make better decisions than individuals or small groups of\nexperts by leveraging their ability to aggregate diverse information. Question\nanswering sites, such as Stack Exchange, rely on the \"wisdom of crowds\" effect\nto identify the best answers to questions asked by users. We analyze data from\n250 communities on the Stack Exchange network to pinpoint factors affecting\nwhich answers are chosen as the best answers. Our results suggest that, rather\nthan evaluate all available answers to a question, users rely on simple\ncognitive heuristics to choose an answer to vote for or accept. These cognitive\nheuristics are linked to an answer's salience, such as the order in which it is\nlisted and how much screen space it occupies. While askers appear to depend\nmore on heuristics, compared to voting users, when choosing an answer to accept\nas the most helpful one, voters use acceptance itself as a heuristic: they are\nmore likely to choose the answer after it is accepted than before that very\nsame answer was accepted. These heuristics become more important in explaining\nand predicting behavior as the number of available answers increases. Our\nfindings suggest that crowd judgments may become less reliable as the number of\nanswers grow.\n", "versions": [{"version": "v1", "created": "Wed, 24 Feb 2016 03:55:15 GMT"}], "update_date": "2017-04-04", "authors_parsed": [["Burghardt", "Keith", ""], ["Alsina", "Emanuel F.", ""], ["Girvan", "Michelle", ""], ["Rand", "William", ""], ["Lerman", "Kristina", ""]]}, {"id": "1602.07393", "submitter": "Zhenhao Ge", "authors": "Zhenhao Ge and Yufang Sun", "title": "Domain Specific Author Attribution Based on Feedforward Neural Network\n  Language Models", "comments": "International Conference on Pattern Recognition Application and\n  Methods (ICPRAM) 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Authorship attribution refers to the task of automatically determining the\nauthor based on a given sample of text. It is a problem with a long history and\nhas a wide range of application. Building author profiles using language models\nis one of the most successful methods to automate this task. New language\nmodeling methods based on neural networks alleviate the curse of dimensionality\nand usually outperform conventional N-gram methods. However, there have not\nbeen much research applying them to authorship attribution. In this paper, we\npresent a novel setup of a Neural Network Language Model (NNLM) and apply it to\na database of text samples from different authors. We investigate how the NNLM\nperforms on a task with moderate author set size and relatively limited\ntraining and test data, and how the topics of the text samples affect the\naccuracy. NNLM achieves nearly 2.5% reduction in perplexity, a measurement of\nfitness of a trained language model to the test data. Given 5 random test\nsentences, it also increases the author classification accuracy by 3.43% on\naverage, compared with the N-gram methods using SRILM tools. An open source\nimplementation of our methodology is freely available at\nhttps://github.com/zge/authorship-attribution/.\n", "versions": [{"version": "v1", "created": "Wed, 24 Feb 2016 04:32:34 GMT"}], "update_date": "2016-02-25", "authors_parsed": [["Ge", "Zhenhao", ""], ["Sun", "Yufang", ""]]}, {"id": "1602.07415", "submitter": "Christopher De Sa", "authors": "Christopher De Sa, Kunle Olukotun, and Christopher R\\'e", "title": "Ensuring Rapid Mixing and Low Bias for Asynchronous Gibbs Sampling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Gibbs sampling is a Markov chain Monte Carlo technique commonly used for\nestimating marginal distributions. To speed up Gibbs sampling, there has\nrecently been interest in parallelizing it by executing asynchronously. While\nempirical results suggest that many models can be efficiently sampled\nasynchronously, traditional Markov chain analysis does not apply to the\nasynchronous case, and thus asynchronous Gibbs sampling is poorly understood.\nIn this paper, we derive a better understanding of the two main challenges of\nasynchronous Gibbs: bias and mixing time. We show experimentally that our\ntheoretical results match practical outcomes.\n", "versions": [{"version": "v1", "created": "Wed, 24 Feb 2016 06:54:43 GMT"}, {"version": "v2", "created": "Mon, 30 May 2016 21:19:52 GMT"}, {"version": "v3", "created": "Thu, 16 Jun 2016 20:55:19 GMT"}], "update_date": "2016-06-20", "authors_parsed": [["De Sa", "Christopher", ""], ["Olukotun", "Kunle", ""], ["R\u00e9", "Christopher", ""]]}, {"id": "1602.07416", "submitter": "Chongxuan Li", "authors": "Chongxuan Li, Jun Zhu and Bo Zhang", "title": "Learning to Generate with Memory", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Memory units have been widely used to enrich the capabilities of deep\nnetworks on capturing long-term dependencies in reasoning and prediction tasks,\nbut little investigation exists on deep generative models (DGMs) which are good\nat inferring high-level invariant representations from unlabeled data. This\npaper presents a deep generative model with a possibly large external memory\nand an attention mechanism to capture the local detail information that is\noften lost in the bottom-up abstraction process in representation learning. By\nadopting a smooth attention model, the whole network is trained end-to-end by\noptimizing a variational bound of data likelihood via auto-encoding variational\nBayesian methods, where an asymmetric recognition network is learnt jointly to\ninfer high-level invariant representations. The asymmetric architecture can\nreduce the competition between bottom-up invariant feature extraction and\ntop-down generation of instance details. Our experiments on several datasets\ndemonstrate that memory can significantly boost the performance of DGMs and\neven achieve state-of-the-art results on various tasks, including density\nestimation, image generation, and missing value imputation.\n", "versions": [{"version": "v1", "created": "Wed, 24 Feb 2016 06:57:14 GMT"}, {"version": "v2", "created": "Sat, 28 May 2016 03:41:27 GMT"}], "update_date": "2016-05-31", "authors_parsed": [["Li", "Chongxuan", ""], ["Zhu", "Jun", ""], ["Zhang", "Bo", ""]]}, {"id": "1602.07428", "submitter": "Jun Zhu", "authors": "Jun Zhu and Jiaming Song and Bei Chen", "title": "Max-Margin Nonparametric Latent Feature Models for Link Prediction", "comments": "14 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.SI stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Link prediction is a fundamental task in statistical network analysis. Recent\nadvances have been made on learning flexible nonparametric Bayesian latent\nfeature models for link prediction. In this paper, we present a max-margin\nlearning method for such nonparametric latent feature relational models. Our\napproach attempts to unite the ideas of max-margin learning and Bayesian\nnonparametrics to discover discriminative latent features for link prediction.\nIt inherits the advances of nonparametric Bayesian methods to infer the unknown\nlatent social dimension, while for discriminative link prediction, it adopts\nthe max-margin learning principle by minimizing a hinge-loss using the linear\nexpectation operator, without dealing with a highly nonlinear link likelihood\nfunction. For posterior inference, we develop an efficient stochastic\nvariational inference algorithm under a truncated mean-field assumption. Our\nmethods can scale up to large-scale real networks with millions of entities and\ntens of millions of positive links. We also provide a full Bayesian\nformulation, which can avoid tuning regularization hyper-parameters.\nExperimental results on a diverse range of real datasets demonstrate the\nbenefits inherited from max-margin learning and Bayesian nonparametric\ninference.\n", "versions": [{"version": "v1", "created": "Wed, 24 Feb 2016 08:08:05 GMT"}], "update_date": "2016-02-25", "authors_parsed": [["Zhu", "Jun", ""], ["Song", "Jiaming", ""], ["Chen", "Bei", ""]]}, {"id": "1602.07464", "submitter": "Pawe{\\l} Teisseyre", "authors": "Pawe{\\l} Teisseyre", "title": "Feature ranking for multi-label classification using Markov Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a simple and efficient method for ranking features in multi-label\nclassification. The method produces a ranking of features showing their\nrelevance in predicting labels, which in turn allows to choose a final subset\nof features. The procedure is based on Markov Networks and allows to model the\ndependencies between labels and features in a direct way. In the first step we\nbuild a simple network using only labels and then we test how much adding a\nsingle feature affects the initial network. More specifically, in the first\nstep we use the Ising model whereas the second step is based on the score\nstatistic, which allows to test a significance of added features very quickly.\nThe proposed approach does not require transformation of label space, gives\ninterpretable results and allows for attractive visualization of dependency\nstructure. We give a theoretical justification of the procedure by discussing\nsome theoretical properties of the Ising model and the score statistic. We also\ndiscuss feature ranking procedure based on fitting Ising model using $l_1$\nregularized logistic regressions. Numerical experiments show that the proposed\nmethods outperform the conventional approaches on the considered artificial and\nreal datasets.\n", "versions": [{"version": "v1", "created": "Wed, 24 Feb 2016 11:11:10 GMT"}], "update_date": "2016-02-25", "authors_parsed": [["Teisseyre", "Pawe\u0142", ""]]}, {"id": "1602.07466", "submitter": "Pawe{\\l} Teisseyre", "authors": "Pawe{\\l} Teisseyre", "title": "Asymptotic consistency and order specification for logistic classifier\n  chains in multi-label learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Classifier chains are popular and effective method to tackle a multi-label\nclassification problem. The aim of this paper is to study the asymptotic\nproperties of the chain model in which the conditional probabilities are of the\nlogistic form. In particular we find conditions on the number of labels and the\ndistribution of feature vector under which the estimated mode of the joint\ndistribution of labels converges to the true mode. Best of our knowledge, this\nimportant issue has not yet been studied in the context of multi-label\nlearning. We also investigate how the order of model building in a chain\ninfluences the estimation of the joint distribution of labels. We establish the\nlink between the problem of incorrect ordering in the chain and incorrect model\nspecification. We propose a procedure of determining the optimal ordering of\nlabels in the chain, which is based on using measures of correct specification\nand allows to find the ordering such that the consecutive logistic models are\nbest possibly specified. The other important question raised in this paper is\nhow accurately can we estimate the joint posterior probability when the\nordering of labels is wrong or the logistic models in the chain are incorrectly\nspecified. The numerical experiments illustrate the theoretical results.\n", "versions": [{"version": "v1", "created": "Wed, 24 Feb 2016 11:15:03 GMT"}], "update_date": "2016-02-25", "authors_parsed": [["Teisseyre", "Pawe\u0142", ""]]}, {"id": "1602.07495", "submitter": "Alireza Ghasemi", "authors": "Alireza Ghasemi, Hamid R. Rabiee, Mohsen Fadaee, Mohammad T. Manzuri\n  and Mohammad H. Rohban", "title": "Active Learning from Positive and Unlabeled Data", "comments": "6 pages, presented at IEEE ICDM 2011 Workshops", "journal-ref": null, "doi": "10.1109/ICDMW.2011.20", "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  During recent years, active learning has evolved into a popular paradigm for\nutilizing user's feedback to improve accuracy of learning algorithms. Active\nlearning works by selecting the most informative sample among unlabeled data\nand querying the label of that point from user. Many different methods such as\nuncertainty sampling and minimum risk sampling have been utilized to select the\nmost informative sample in active learning. Although many active learning\nalgorithms have been proposed so far, most of them work with binary or\nmulti-class classification problems and therefore can not be applied to\nproblems in which only samples from one class as well as a set of unlabeled\ndata are available.\n  Such problems arise in many real-world situations and are known as the\nproblem of learning from positive and unlabeled data. In this paper we propose\nan active learning algorithm that can work when only samples of one class as\nwell as a set of unlabelled data are available. Our method works by separately\nestimating probability desnity of positive and unlabeled points and then\ncomputing expected value of informativeness to get rid of a hyper-parameter and\nhave a better measure of informativeness./ Experiments and empirical analysis\nshow promising results compared to other similar methods.\n", "versions": [{"version": "v1", "created": "Wed, 24 Feb 2016 13:32:36 GMT"}], "update_date": "2016-11-17", "authors_parsed": [["Ghasemi", "Alireza", ""], ["Rabiee", "Hamid R.", ""], ["Fadaee", "Mohsen", ""], ["Manzuri", "Mohammad T.", ""], ["Rohban", "Mohammad H.", ""]]}, {"id": "1602.07507", "submitter": "Alireza Ghasemi", "authors": "Alireza Ghasemi, Hamid R. Rabiee, Mohammad T. Manzuri, M. H. Rohban", "title": "A Bayesian Approach to the Data Description Problem", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we address the problem of data description using a Bayesian\nframework. The goal of data description is to draw a boundary around objects of\na certain class of interest to discriminate that class from the rest of the\nfeature space. Data description is also known as one-class learning and has a\nwide range of applications.\n  The proposed approach uses a Bayesian framework to precisely compute the\nclass boundary and therefore can utilize domain information in form of prior\nknowledge in the framework. It can also operate in the kernel space and\ntherefore recognize arbitrary boundary shapes. Moreover, the proposed method\ncan utilize unlabeled data in order to improve accuracy of discrimination.\n  We evaluate our method using various real-world datasets and compare it with\nother state of the art approaches of data description. Experiments show\npromising results and improved performance over other data description and\none-class learning algorithms.\n", "versions": [{"version": "v1", "created": "Wed, 24 Feb 2016 13:52:52 GMT"}], "update_date": "2016-02-26", "authors_parsed": [["Ghasemi", "Alireza", ""], ["Rabiee", "Hamid R.", ""], ["Manzuri", "Mohammad T.", ""], ["Rohban", "M. H.", ""]]}, {"id": "1602.07570", "submitter": "Aleksandrs Slivkins", "authors": "Yishay Mansour, Aleksandrs Slivkins, Vasilis Syrgkanis, Zhiwei Steven\n  Wu", "title": "Bayesian Exploration: Incentivizing Exploration in Bayesian Games", "comments": "All revisions focused on presentation; all results (except Appendix\n  C) have been present since the initial version", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GT cs.DS cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider a ubiquitous scenario in the Internet economy when individual\ndecision-makers (henceforth, agents) both produce and consume information as\nthey make strategic choices in an uncertain environment. This creates a\nthree-way tradeoff between exploration (trying out insufficiently explored\nalternatives to help others in the future), exploitation (making optimal\ndecisions given the information discovered by other agents), and incentives of\nthe agents (who are myopically interested in exploitation, while preferring the\nothers to explore). We posit a principal who controls the flow of information\nfrom agents that came before, and strives to coordinate the agents towards a\nsocially optimal balance between exploration and exploitation, not using any\nmonetary transfers. The goal is to design a recommendation policy for the\nprincipal which respects agents' incentives and minimizes a suitable notion of\nregret.\n  We extend prior work in this direction to allow the agents to interact with\none another in a shared environment: at each time step, multiple agents arrive\nto play a Bayesian game, receive recommendations, choose their actions, receive\ntheir payoffs, and then leave the game forever. The agents now face two sources\nof uncertainty: the actions of the other agents and the parameters of the\nuncertain game environment.\n  Our main contribution is to show that the principal can achieve constant\nregret when the utilities are deterministic (where the constant depends on the\nprior distribution, but not on the time horizon), and logarithmic regret when\nthe utilities are stochastic. As a key technical tool, we introduce the concept\nof explorable actions, the actions which some incentive-compatible policy can\nrecommend with non-zero probability. We show how the principal can identify\n(and explore) all explorable actions, and use the revealed information to\nperform optimally.\n", "versions": [{"version": "v1", "created": "Wed, 24 Feb 2016 15:57:28 GMT"}, {"version": "v2", "created": "Wed, 16 Nov 2016 00:53:19 GMT"}, {"version": "v3", "created": "Sun, 1 Jul 2018 17:48:48 GMT"}, {"version": "v4", "created": "Thu, 8 Apr 2021 02:23:26 GMT"}], "update_date": "2021-04-09", "authors_parsed": [["Mansour", "Yishay", ""], ["Slivkins", "Aleksandrs", ""], ["Syrgkanis", "Vasilis", ""], ["Wu", "Zhiwei Steven", ""]]}, {"id": "1602.07576", "submitter": "Taco Cohen", "authors": "Taco S. Cohen, Max Welling", "title": "Group Equivariant Convolutional Networks", "comments": null, "journal-ref": "Proceedings of the International Conference on Machine Learning\n  (ICML), 2016", "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce Group equivariant Convolutional Neural Networks (G-CNNs), a\nnatural generalization of convolutional neural networks that reduces sample\ncomplexity by exploiting symmetries. G-CNNs use G-convolutions, a new type of\nlayer that enjoys a substantially higher degree of weight sharing than regular\nconvolution layers. G-convolutions increase the expressive capacity of the\nnetwork without increasing the number of parameters. Group convolution layers\nare easy to use and can be implemented with negligible computational overhead\nfor discrete groups generated by translations, reflections and rotations.\nG-CNNs achieve state of the art results on CIFAR10 and rotated MNIST.\n", "versions": [{"version": "v1", "created": "Wed, 24 Feb 2016 16:17:15 GMT"}, {"version": "v2", "created": "Fri, 11 Mar 2016 18:26:26 GMT"}, {"version": "v3", "created": "Fri, 3 Jun 2016 10:54:16 GMT"}], "update_date": "2016-06-06", "authors_parsed": [["Cohen", "Taco S.", ""], ["Welling", "Max", ""]]}, {"id": "1602.07614", "submitter": "Daniele Ramazzotti", "authors": "Daniele Ramazzotti", "title": "A Model of Selective Advantage for the Efficient Inference of Cancer\n  Clonal Evolution", "comments": "Doctoral thesis, University of Milan", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, there has been a resurgence of interest in rigorous algorithms for\nthe inference of cancer progression from genomic data. The motivations are\nmanifold: (i) growing NGS and single cell data from cancer patients, (ii) need\nfor novel Data Science and Machine Learning algorithms to infer models of\ncancer progression, and (iii) a desire to understand the temporal and\nheterogeneous structure of tumor to tame its progression by efficacious\ntherapeutic intervention. This thesis presents a multi-disciplinary effort to\nmodel tumor progression involving successive accumulation of genetic\nalterations, each resulting populations manifesting themselves in a cancer\nphenotype. The framework presented in this work along with algorithms derived\nfrom it, represents a novel approach for inferring cancer progression, whose\naccuracy and convergence rates surpass the existing techniques. The approach\nderives its power from several fields including algorithms in machine learning,\ntheory of causality and cancer biology. Furthermore, a modular pipeline to\nextract ensemble-level progression models from sequenced cancer genomes is\nproposed. The pipeline combines state-of-the-art techniques for sample\nstratification, driver selection, identification of fitness-equivalent\nexclusive alterations and progression model inference. Furthermore, the results\nare validated by synthetic data with realistic generative models, and\nempirically interpreted in the context of real cancer datasets; in the later\ncase, biologically significant conclusions are also highlighted. Specifically,\nit demonstrates the pipeline's ability to reproduce much of the knowledge on\ncolorectal cancer, as well as to suggest novel hypotheses. Lastly, it also\nproves that the proposed framework can be applied to reconstruct the\nevolutionary history of cancer clones in single patients, as illustrated by an\nexample from clear cell renal carcinomas.\n", "versions": [{"version": "v1", "created": "Mon, 15 Feb 2016 16:33:39 GMT"}], "update_date": "2016-02-25", "authors_parsed": [["Ramazzotti", "Daniele", ""]]}, {"id": "1602.07616", "submitter": "Anindya De", "authors": "Anindya De and Michael Saks and Sijian Tang", "title": "Noisy population recovery in polynomial time", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.DS cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the noisy population recovery problem of Dvir et al., the goal is to learn\nan unknown distribution $f$ on binary strings of length $n$ from noisy samples.\nFor some parameter $\\mu \\in [0,1]$, a noisy sample is generated by flipping\neach coordinate of a sample from $f$ independently with probability\n$(1-\\mu)/2$. We assume an upper bound $k$ on the size of the support of the\ndistribution, and the goal is to estimate the probability of any string to\nwithin some given error $\\varepsilon$. It is known that the algorithmic\ncomplexity and sample complexity of this problem are polynomially related to\neach other.\n  We show that for $\\mu > 0$, the sample complexity (and hence the algorithmic\ncomplexity) is bounded by a polynomial in $k$, $n$ and $1/\\varepsilon$\nimproving upon the previous best result of $\\mathsf{poly}(k^{\\log\\log\nk},n,1/\\varepsilon)$ due to Lovett and Zhang.\n  Our proof combines ideas from Lovett and Zhang with a \\emph{noise attenuated}\nversion of M\\\"{o}bius inversion. In turn, the latter crucially uses the\nconstruction of \\emph{robust local inverse} due to Moitra and Saks.\n", "versions": [{"version": "v1", "created": "Wed, 24 Feb 2016 17:46:30 GMT"}], "update_date": "2016-02-25", "authors_parsed": [["De", "Anindya", ""], ["Saks", "Michael", ""], ["Tang", "Sijian", ""]]}, {"id": "1602.07630", "submitter": "Bicheng Ying", "authors": "Bicheng Ying, Kun Yuan, Ali H. Sayed", "title": "Online Dual Coordinate Ascent Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The stochastic dual coordinate-ascent (S-DCA) technique is a useful\nalternative to the traditional stochastic gradient-descent algorithm for\nsolving large-scale optimization problems due to its scalability to large data\nsets and strong theoretical guarantees. However, the available S-DCA\nformulation is limited to finite sample sizes and relies on performing multiple\npasses over the same data. This formulation is not well-suited for online\nimplementations where data keep streaming in. In this work, we develop an {\\em\nonline} dual coordinate-ascent (O-DCA) algorithm that is able to respond to\nstreaming data and does not need to revisit the past data. This feature embeds\nthe resulting construction with continuous adaptation, learning, and tracking\nabilities, which are particularly attractive for online learning scenarios.\n", "versions": [{"version": "v1", "created": "Wed, 24 Feb 2016 18:26:35 GMT"}], "update_date": "2016-02-25", "authors_parsed": [["Ying", "Bicheng", ""], ["Yuan", "Kun", ""], ["Sayed", "Ali H.", ""]]}, {"id": "1602.07714", "submitter": "Hado van Hasselt", "authors": "Hado van Hasselt and Arthur Guez and Matteo Hessel and Volodymyr Mnih\n  and David Silver", "title": "Learning values across many orders of magnitude", "comments": "Paper accepted for publication at NIPS 2016. This version includes\n  the appendix", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most learning algorithms are not invariant to the scale of the function that\nis being approximated. We propose to adaptively normalize the targets used in\nlearning. This is useful in value-based reinforcement learning, where the\nmagnitude of appropriate value approximations can change over time when we\nupdate the policy of behavior. Our main motivation is prior work on learning to\nplay Atari games, where the rewards were all clipped to a predetermined range.\nThis clipping facilitates learning across many different games with a single\nlearning algorithm, but a clipped reward function can result in qualitatively\ndifferent behavior. Using the adaptive normalization we can remove this\ndomain-specific heuristic without diminishing overall performance.\n", "versions": [{"version": "v1", "created": "Wed, 24 Feb 2016 21:14:52 GMT"}, {"version": "v2", "created": "Tue, 16 Aug 2016 05:27:17 GMT"}], "update_date": "2016-08-17", "authors_parsed": [["van Hasselt", "Hado", ""], ["Guez", "Arthur", ""], ["Hessel", "Matteo", ""], ["Mnih", "Volodymyr", ""], ["Silver", "David", ""]]}, {"id": "1602.07726", "submitter": "Zhiwei Steven Wu", "authors": "Rachel Cummings, Katrina Ligett, Kobbi Nissim, Aaron Roth, Zhiwei\n  Steven Wu", "title": "Adaptive Learning with Robust Generalization Guarantees", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The traditional notion of generalization---i.e., learning a hypothesis whose\nempirical error is close to its true error---is surprisingly brittle. As has\nrecently been noted in [DFH+15b], even if several algorithms have this\nguarantee in isolation, the guarantee need not hold if the algorithms are\ncomposed adaptively. In this paper, we study three notions of\ngeneralization---increasing in strength---that are robust to postprocessing and\namenable to adaptive composition, and examine the relationships between them.\nWe call the weakest such notion Robust Generalization. A second, intermediate,\nnotion is the stability guarantee known as differential privacy. The strongest\nguarantee we consider we call Perfect Generalization. We prove that every\nhypothesis class that is PAC learnable is also PAC learnable in a robustly\ngeneralizing fashion, with almost the same sample complexity. It was previously\nknown that differentially private algorithms satisfy robust generalization. In\nthis paper, we show that robust generalization is a strictly weaker concept,\nand that there is a learning task that can be carried out subject to robust\ngeneralization guarantees, yet cannot be carried out subject to differential\nprivacy. We also show that perfect generalization is a strictly stronger\nguarantee than differential privacy, but that, nevertheless, many learning\ntasks can be carried out subject to the guarantees of perfect generalization.\n", "versions": [{"version": "v1", "created": "Wed, 24 Feb 2016 21:59:30 GMT"}, {"version": "v2", "created": "Thu, 2 Jun 2016 00:07:01 GMT"}], "update_date": "2016-06-03", "authors_parsed": [["Cummings", "Rachel", ""], ["Ligett", "Katrina", ""], ["Nissim", "Kobbi", ""], ["Roth", "Aaron", ""], ["Wu", "Zhiwei Steven", ""]]}, {"id": "1602.07754", "submitter": "Kevin Xu", "authors": "Swayambhoo Jain, Urvashi Oswal, Kevin S. Xu, Brian Eriksson, Jarvis\n  Haupt", "title": "A Compressed Sensing Based Decomposition of Electrodermal Activity\n  Signals", "comments": "To appear in IEEE Transactions on Biomedical Engineering", "journal-ref": null, "doi": "10.1109/TBME.2016.2632523", "report-no": null, "categories": "stat.ML cs.LG stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The measurement and analysis of Electrodermal Activity (EDA) offers\napplications in diverse areas ranging from market research, to seizure\ndetection, to human stress analysis. Unfortunately, the analysis of EDA signals\nis made difficult by the superposition of numerous components which can obscure\nthe signal information related to a user's response to a stimulus. We show how\nsimple pre-processing followed by a novel compressed sensing based\ndecomposition can mitigate the effects of the undesired noise components and\nhelp reveal the underlying physiological signal. The proposed framework allows\nfor decomposition of EDA signals with provable bounds on the recovery of user\nresponses. We test our procedure on both synthetic and real-world EDA signals\nfrom wearable sensors and demonstrate that our approach allows for more\naccurate recovery of user responses as compared to the existing techniques.\n", "versions": [{"version": "v1", "created": "Wed, 24 Feb 2016 23:52:07 GMT"}, {"version": "v2", "created": "Thu, 26 Jan 2017 21:03:57 GMT"}], "update_date": "2017-02-01", "authors_parsed": [["Jain", "Swayambhoo", ""], ["Oswal", "Urvashi", ""], ["Xu", "Kevin S.", ""], ["Eriksson", "Brian", ""], ["Haupt", "Jarvis", ""]]}, {"id": "1602.07764", "submitter": "Kamyar Azizzadenesheli Ph.D.", "authors": "Kamyar Azizzadenesheli, Alessandro Lazaric, Animashree Anandkumar", "title": "Reinforcement Learning of POMDPs using Spectral Methods", "comments": null, "journal-ref": "29th Annual Conference on Learning Theory, PMLR 49:193-256, 2016", "doi": null, "report-no": null, "categories": "cs.AI cs.LG cs.NA math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new reinforcement learning algorithm for partially observable\nMarkov decision processes (POMDP) based on spectral decomposition methods.\nWhile spectral methods have been previously employed for consistent learning of\n(passive) latent variable models such as hidden Markov models, POMDPs are more\nchallenging since the learner interacts with the environment and possibly\nchanges the future observations in the process. We devise a learning algorithm\nrunning through episodes, in each episode we employ spectral techniques to\nlearn the POMDP parameters from a trajectory generated by a fixed policy. At\nthe end of the episode, an optimization oracle returns the optimal memoryless\nplanning policy which maximizes the expected reward based on the estimated\nPOMDP model. We prove an order-optimal regret bound with respect to the optimal\nmemoryless policy and efficient scaling with respect to the dimensionality of\nobservation and action spaces.\n", "versions": [{"version": "v1", "created": "Thu, 25 Feb 2016 01:25:36 GMT"}, {"version": "v2", "created": "Sun, 29 May 2016 07:15:21 GMT"}], "update_date": "2017-06-20", "authors_parsed": [["Azizzadenesheli", "Kamyar", ""], ["Lazaric", "Alessandro", ""], ["Anandkumar", "Animashree", ""]]}, {"id": "1602.07844", "submitter": "Shuai Zheng", "authors": "Shuai Zheng and Ruiliang Zhang and James T. Kwok", "title": "Fast Nonsmooth Regularized Risk Minimization with Continuation", "comments": "AAAI-2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In regularized risk minimization, the associated optimization problem becomes\nparticularly difficult when both the loss and regularizer are nonsmooth.\nExisting approaches either have slow or unclear convergence properties, are\nrestricted to limited problem subclasses, or require careful setting of a\nsmoothing parameter. In this paper, we propose a continuation algorithm that is\napplicable to a large class of nonsmooth regularized risk minimization\nproblems, can be flexibly used with a number of existing solvers for the\nunderlying smoothed subproblem, and with convergence results on the whole\nalgorithm rather than just one of its subproblems. In particular, when\naccelerated solvers are used, the proposed algorithm achieves the fastest known\nrates of $O(1/T^2)$ on strongly convex problems, and $O(1/T)$ on general convex\nproblems. Experiments on nonsmooth classification and regression tasks\ndemonstrate that the proposed algorithm outperforms the state-of-the-art.\n", "versions": [{"version": "v1", "created": "Thu, 25 Feb 2016 08:34:59 GMT"}], "update_date": "2016-10-14", "authors_parsed": [["Zheng", "Shuai", ""], ["Zhang", "Ruiliang", ""], ["Kwok", "James T.", ""]]}, {"id": "1602.07857", "submitter": "Daniele Ramazzotti", "authors": "Daniele Ramazzotti and Alex Graudenzi and Giulio Caravagna and Marco\n  Antoniotti", "title": "Modeling cumulative biological phenomena with Suppes-Bayes Causal\n  Networks", "comments": null, "journal-ref": null, "doi": "10.1177/1176934318785167", "report-no": null, "categories": "cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Several diseases related to cell proliferation are characterized by the\naccumulation of somatic DNA changes, with respect to wildtype conditions.\nCancer and HIV are two common examples of such diseases, where the mutational\nload in the cancerous/viral population increases over time. In these cases,\nselective pressures are often observed along with competition, cooperation and\nparasitism among distinct cellular clones. Recently, we presented a\nmathematical framework to model these phenomena, based on a combination of\nBayesian inference and Suppes' theory of probabilistic causation, depicted in\ngraphical structures dubbed Suppes-Bayes Causal Networks (SBCNs). SBCNs are\ngenerative probabilistic graphical models that recapitulate the potential\nordering of accumulation of such DNA changes during the progression of the\ndisease. Such models can be inferred from data by exploiting likelihood-based\nmodel-selection strategies with regularization. In this paper we discuss the\ntheoretical foundations of our approach and we investigate in depth the\ninfluence on the model-selection task of: (i) the poset based on Suppes' theory\nand (ii) different regularization strategies. Furthermore, we provide an\nexample of application of our framework to HIV genetic data highlighting the\nvaluable insights provided by the inferred.\n", "versions": [{"version": "v1", "created": "Thu, 25 Feb 2016 09:23:58 GMT"}, {"version": "v2", "created": "Thu, 31 Mar 2016 22:29:11 GMT"}, {"version": "v3", "created": "Wed, 8 Mar 2017 23:40:02 GMT"}, {"version": "v4", "created": "Thu, 5 Jul 2018 02:17:49 GMT"}], "update_date": "2018-07-06", "authors_parsed": [["Ramazzotti", "Daniele", ""], ["Graudenzi", "Alex", ""], ["Caravagna", "Giulio", ""], ["Antoniotti", "Marco", ""]]}, {"id": "1602.07860", "submitter": "Yash Satsangi", "authors": "Yash Satsangi, Shimon Whiteson, Frans A. Oliehoek", "title": "Probably Approximately Correct Greedy Maximization with Efficient Bounds\n  on Information Gain for Sensor Selection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Submodular function maximization finds application in a variety of real-world\ndecision-making problems. However, most existing methods, based on greedy\nmaximization, assume it is computationally feasible to evaluate F, the function\nbeing maximized. Unfortunately, in many realistic settings F is too expensive\nto evaluate exactly even once. We present probably approximately correct greedy\nmaximization, which requires access only to cheap anytime confidence bounds on\nF and uses them to prune elements. We show that, with high probability, our\nmethod returns an approximately optimal set. We propose novel, cheap confidence\nbounds for conditional entropy, which appears in many common choices of F and\nfor which it is difficult to find unbiased or bounded estimates. Finally,\nresults on a real-world dataset from a multi-camera tracking system in a\nshopping mall demonstrate that our approach performs comparably to existing\nmethods, but at a fraction of the computational cost.\n", "versions": [{"version": "v1", "created": "Thu, 25 Feb 2016 09:34:38 GMT"}, {"version": "v2", "created": "Mon, 10 Aug 2020 12:02:33 GMT"}], "update_date": "2020-08-11", "authors_parsed": [["Satsangi", "Yash", ""], ["Whiteson", "Shimon", ""], ["Oliehoek", "Frans A.", ""]]}, {"id": "1602.07863", "submitter": "Janne Lepp\\\"a-Aho", "authors": "Janne Lepp\\\"a-aho, Johan Pensar, Teemu Roos, Jukka Corander", "title": "Learning Gaussian Graphical Models With Fractional Marginal\n  Pseudo-likelihood", "comments": null, "journal-ref": null, "doi": "10.1016/j.ijar.2017.01.001", "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a Bayesian approximate inference method for learning the\ndependence structure of a Gaussian graphical model. Using pseudo-likelihood, we\nderive an analytical expression to approximate the marginal likelihood for an\narbitrary graph structure without invoking any assumptions about\ndecomposability. The majority of the existing methods for learning Gaussian\ngraphical models are either restricted to decomposable graphs or require\nspecification of a tuning parameter that may have a substantial impact on\nlearned structures. By combining a simple sparsity inducing prior for the graph\nstructures with a default reference prior for the model parameters, we obtain a\nfast and easily applicable scoring function that works well for even\nhigh-dimensional data. We demonstrate the favourable performance of our\napproach by large-scale comparisons against the leading methods for learning\nnon-decomposable Gaussian graphical models. A theoretical justification for our\nmethod is provided by showing that it yields a consistent estimator of the\ngraph structure.\n", "versions": [{"version": "v1", "created": "Thu, 25 Feb 2016 09:42:46 GMT"}], "update_date": "2017-04-13", "authors_parsed": [["Lepp\u00e4-aho", "Janne", ""], ["Pensar", "Johan", ""], ["Roos", "Teemu", ""], ["Corander", "Jukka", ""]]}, {"id": "1602.07865", "submitter": "Jesse Krijthe", "authors": "Jesse H. Krijthe and Marco Loog", "title": "Projected Estimators for Robust Semi-supervised Classification", "comments": "13 pages, 2 figures, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For semi-supervised techniques to be applied safely in practice we at least\nwant methods to outperform their supervised counterparts. We study this\nquestion for classification using the well-known quadratic surrogate loss\nfunction. Using a projection of the supervised estimate onto a set of\nconstraints imposed by the unlabeled data, we find we can safely improve over\nthe supervised solution in terms of this quadratic loss. Unlike other\napproaches to semi-supervised learning, the procedure does not rely on\nassumptions that are not intrinsic to the classifier at hand. It is\ntheoretically demonstrated that, measured on the labeled and unlabeled training\ndata, this semi-supervised procedure never gives a lower quadratic loss than\nthe supervised alternative. To our knowledge this is the first approach that\noffers such strong, albeit conservative, guarantees for improvement over the\nsupervised solution. The characteristics of our approach are explicated using\nbenchmark datasets to further understand the similarities and differences\nbetween the quadratic loss criterion used in the theoretical results and the\nclassification accuracy often considered in practice.\n", "versions": [{"version": "v1", "created": "Thu, 25 Feb 2016 09:57:42 GMT"}], "update_date": "2016-02-26", "authors_parsed": [["Krijthe", "Jesse H.", ""], ["Loog", "Marco", ""]]}, {"id": "1602.07868", "submitter": "Tim Salimans", "authors": "Tim Salimans and Diederik P. Kingma", "title": "Weight Normalization: A Simple Reparameterization to Accelerate Training\n  of Deep Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present weight normalization: a reparameterization of the weight vectors\nin a neural network that decouples the length of those weight vectors from\ntheir direction. By reparameterizing the weights in this way we improve the\nconditioning of the optimization problem and we speed up convergence of\nstochastic gradient descent. Our reparameterization is inspired by batch\nnormalization but does not introduce any dependencies between the examples in a\nminibatch. This means that our method can also be applied successfully to\nrecurrent models such as LSTMs and to noise-sensitive applications such as deep\nreinforcement learning or generative models, for which batch normalization is\nless well suited. Although our method is much simpler, it still provides much\nof the speed-up of full batch normalization. In addition, the computational\noverhead of our method is lower, permitting more optimization steps to be taken\nin the same amount of time. We demonstrate the usefulness of our method on\napplications in supervised image recognition, generative modelling, and deep\nreinforcement learning.\n", "versions": [{"version": "v1", "created": "Thu, 25 Feb 2016 10:13:45 GMT"}, {"version": "v2", "created": "Fri, 26 Feb 2016 08:53:23 GMT"}, {"version": "v3", "created": "Sat, 4 Jun 2016 01:21:52 GMT"}], "update_date": "2016-06-07", "authors_parsed": [["Salimans", "Tim", ""], ["Kingma", "Diederik P.", ""]]}, {"id": "1602.07905", "submitter": "Jan Leike", "authors": "Jan Leike and Tor Lattimore and Laurent Orseau and Marcus Hutter", "title": "Thompson Sampling is Asymptotically Optimal in General Environments", "comments": "UAI 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We discuss a variant of Thompson sampling for nonparametric reinforcement\nlearning in a countable classes of general stochastic environments. These\nenvironments can be non-Markov, non-ergodic, and partially observable. We show\nthat Thompson sampling learns the environment class in the sense that (1)\nasymptotically its value converges to the optimal value in mean and (2) given a\nrecoverability assumption regret is sublinear.\n", "versions": [{"version": "v1", "created": "Thu, 25 Feb 2016 12:37:21 GMT"}, {"version": "v2", "created": "Fri, 3 Jun 2016 10:59:36 GMT"}], "update_date": "2016-06-06", "authors_parsed": [["Leike", "Jan", ""], ["Lattimore", "Tor", ""], ["Orseau", "Laurent", ""], ["Hutter", "Marcus", ""]]}, {"id": "1602.07985", "submitter": "Alexandros A. Voudouris", "authors": "Ioannis Caragiannis, George A. Krimpas, Alexandros A. Voudouris", "title": "How effective can simple ordinal peer grading be?", "comments": "39 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.DS cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Ordinal peer grading has been proposed as a simple and scalable solution for\ncomputing reliable information about student performance in massive open online\ncourses. The idea is to outsource the grading task to the students themselves\nas follows. After the end of an exam, each student is asked to rank -- in terms\nof quality -- a bundle of exam papers by fellow students. An aggregation rule\nthen combines the individual rankings into a global one that contains all\nstudents. We define a broad class of simple aggregation rules, which we call\ntype-ordering aggregation rules, and present a theoretical framework for\nassessing their effectiveness. When statistical information about the grading\nbehaviour of students is available (in terms of a noise matrix that\ncharacterizes the grading behaviour of the average student from a student\npopulation), the framework can be used to compute the optimal rule from this\nclass with respect to a series of performance objectives that compare the\nranking returned by the aggregation rule to the underlying ground truth\nranking. For example, a natural rule known as Borda is proved to be optimal\nwhen students grade correctly. In addition, we present extensive simulations\nthat validate our theory and prove it to be extremely accurate in predicting\nthe performance of aggregation rules even when only rough information about\ngrading behaviour (i.e., an approximation of the noise matrix) is available.\nBoth in the application of our theoretical framework and in our simulations, we\nexploit data about grading behaviour of students that have been extracted from\ntwo field experiments in the University of Patras.\n", "versions": [{"version": "v1", "created": "Thu, 25 Feb 2016 16:37:57 GMT"}, {"version": "v2", "created": "Wed, 8 Apr 2020 16:23:56 GMT"}], "update_date": "2020-04-09", "authors_parsed": [["Caragiannis", "Ioannis", ""], ["Krimpas", "George A.", ""], ["Voudouris", "Alexandros A.", ""]]}, {"id": "1602.08007", "submitter": "Yann Ollivier", "authors": "Ga\\'etan Marceau-Caron, Yann Ollivier", "title": "Practical Riemannian Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We provide the first experimental results on non-synthetic datasets for the\nquasi-diagonal Riemannian gradient descents for neural networks introduced in\n[Ollivier, 2015]. These include the MNIST, SVHN, and FACE datasets as well as a\npreviously unpublished electroencephalogram dataset. The quasi-diagonal\nRiemannian algorithms consistently beat simple stochastic gradient gradient\ndescents by a varying margin. The computational overhead with respect to simple\nbackpropagation is around a factor $2$. Perhaps more interestingly, these\nmethods also reach their final performance quickly, thus requiring fewer\ntraining epochs and a smaller total computation time.\n  We also present an implementation guide to these Riemannian gradient descents\nfor neural networks, showing how the quasi-diagonal versions can be implemented\nwith minimal effort on top of existing routines which compute gradients.\n", "versions": [{"version": "v1", "created": "Thu, 25 Feb 2016 17:37:28 GMT"}], "update_date": "2016-02-26", "authors_parsed": [["Marceau-Caron", "Ga\u00e9tan", ""], ["Ollivier", "Yann", ""]]}, {"id": "1602.08017", "submitter": "Alexey Melnikov", "authors": "Adi Makmal, Alexey A. Melnikov, Vedran Dunjko, Hans J. Briegel", "title": "Meta-learning within Projective Simulation", "comments": "14 pages, 12 figures", "journal-ref": "IEEE Access 4, 2110-2122 (2016)", "doi": "10.1109/access.2016.2556579", "report-no": null, "categories": "cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning models of artificial intelligence can nowadays perform very well on\na large variety of tasks. However, in practice different task environments are\nbest handled by different learning models, rather than a single, universal,\napproach. Most non-trivial models thus require the adjustment of several to\nmany learning parameters, which is often done on a case-by-case basis by an\nexternal party. Meta-learning refers to the ability of an agent to autonomously\nand dynamically adjust its own learning parameters, or meta-parameters. In this\nwork we show how projective simulation, a recently developed model of\nartificial intelligence, can naturally be extended to account for meta-learning\nin reinforcement learning settings. The projective simulation approach is based\non a random walk process over a network of clips. The suggested meta-learning\nscheme builds upon the same design and employs clip networks to monitor the\nagent's performance and to adjust its meta-parameters \"on the fly\". We\ndistinguish between \"reflexive adaptation\" and \"adaptation through learning\",\nand show the utility of both approaches. In addition, a trade-off between\nflexibility and learning-time is addressed. The extended model is examined on\nthree different kinds of reinforcement learning tasks, in which the agent has\ndifferent optimal values of the meta-parameters, and is shown to perform well,\nreaching near-optimal to optimal success rates in all of them, without ever\nneeding to manually adjust any meta-parameter.\n", "versions": [{"version": "v1", "created": "Thu, 25 Feb 2016 18:07:53 GMT"}], "update_date": "2016-05-31", "authors_parsed": [["Makmal", "Adi", ""], ["Melnikov", "Alexey A.", ""], ["Dunjko", "Vedran", ""], ["Briegel", "Hans J.", ""]]}, {"id": "1602.08045", "submitter": "Zhenhao Ge", "authors": "Zhenhao Ge, Sudhendu R. Sharma, Mark J. T. Smith", "title": "PCA/LDA Approach for Text-Independent Speaker Recognition", "comments": "Society of Photo-Optical Instrumentation Engineers (SPIE) Conference\n  Series", "journal-ref": null, "doi": "10.1117/12.919235", "report-no": null, "categories": "cs.SD cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Various algorithms for text-independent speaker recognition have been\ndeveloped through the decades, aiming to improve both accuracy and efficiency.\nThis paper presents a novel PCA/LDA-based approach that is faster than\ntraditional statistical model-based methods and achieves competitive results.\nFirst, the performance based on only PCA and only LDA is measured; then a mixed\nmodel, taking advantages of both methods, is introduced. A subset of the TIMIT\ncorpus composed of 200 male speakers, is used for enrollment, validation and\ntesting. The best results achieve 100%; 96% and 95% classification rate at\npopulation level 50; 100 and 200, using 39-dimensional MFCC features with delta\nand double delta. These results are based on 12-second text-independent speech\nfor training and 4-second data for test. These are comparable to the\nconventional MFCC-GMM methods, but require significantly less time to train and\noperate.\n", "versions": [{"version": "v1", "created": "Thu, 25 Feb 2016 19:18:06 GMT"}], "update_date": "2016-10-04", "authors_parsed": [["Ge", "Zhenhao", ""], ["Sharma", "Sudhendu R.", ""], ["Smith", "Mark J. T.", ""]]}, {"id": "1602.08118", "submitter": "Andrew Simpson", "authors": "Andrew J.R. Simpson", "title": "Hierarchical Conflict Propagation: Sequence Learning in a Recurrent Deep\n  Neural Network", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recurrent neural networks (RNN) are capable of learning to encode and exploit\nactivation history over an arbitrary timescale. However, in practice, state of\nthe art gradient descent based training methods are known to suffer from\ndifficulties in learning long term dependencies. Here, we describe a novel\ntraining method that involves concurrent parallel cloned networks, each sharing\nthe same weights, each trained at different stimulus phase and each maintaining\nindependent activation histories. Training proceeds by recursively performing\nbatch-updates over the parallel clones as activation history is progressively\nincreased. This allows conflicts to propagate hierarchically from short-term\ncontexts towards longer-term contexts until they are resolved. We illustrate\nthe parallel clones method and hierarchical conflict propagation with a\ncharacter-level deep RNN tasked with memorizing a paragraph of Moby Dick (by\nHerman Melville).\n", "versions": [{"version": "v1", "created": "Thu, 25 Feb 2016 21:12:25 GMT"}], "update_date": "2016-02-29", "authors_parsed": [["Simpson", "Andrew J. R.", ""]]}, {"id": "1602.08124", "submitter": "Minsoo Rhu", "authors": "Minsoo Rhu, Natalia Gimelshein, Jason Clemons, Arslan Zulfiqar,\n  Stephen W. Keckler", "title": "vDNN: Virtualized Deep Neural Networks for Scalable, Memory-Efficient\n  Neural Network Design", "comments": "Published as a conference paper at the 49th IEEE/ACM International\n  Symposium on Microarchitecture (MICRO-49), 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The most widely used machine learning frameworks require users to carefully\ntune their memory usage so that the deep neural network (DNN) fits into the\nDRAM capacity of a GPU. This restriction hampers a researcher's flexibility to\nstudy different machine learning algorithms, forcing them to either use a less\ndesirable network architecture or parallelize the processing across multiple\nGPUs. We propose a runtime memory manager that virtualizes the memory usage of\nDNNs such that both GPU and CPU memory can simultaneously be utilized for\ntraining larger DNNs. Our virtualized DNN (vDNN) reduces the average GPU memory\nusage of AlexNet by up to 89%, OverFeat by 91%, and GoogLeNet by 95%, a\nsignificant reduction in memory requirements of DNNs. Similar experiments on\nVGG-16, one of the deepest and memory hungry DNNs to date, demonstrate the\nmemory-efficiency of our proposal. vDNN enables VGG-16 with batch size 256\n(requiring 28 GB of memory) to be trained on a single NVIDIA Titan X GPU card\ncontaining 12 GB of memory, with 18% performance loss compared to a\nhypothetical, oracular GPU with enough memory to hold the entire DNN.\n", "versions": [{"version": "v1", "created": "Thu, 25 Feb 2016 21:31:55 GMT"}, {"version": "v2", "created": "Tue, 1 Mar 2016 03:52:59 GMT"}, {"version": "v3", "created": "Thu, 28 Jul 2016 23:19:03 GMT"}], "update_date": "2016-08-01", "authors_parsed": [["Rhu", "Minsoo", ""], ["Gimelshein", "Natalia", ""], ["Clemons", "Jason", ""], ["Zulfiqar", "Arslan", ""], ["Keckler", "Stephen W.", ""]]}, {"id": "1602.08127", "submitter": "Xiping Fu", "authors": "Xiping Fu, Brendan McCane, Steven Mills, Michael Albert and Lech\n  Szymanski", "title": "Auto-JacoBin: Auto-encoder Jacobian Binary Hashing", "comments": "Submitting to journal (TPAMI). 17 pages, 11 figures. The Matlab codes\n  for AutoJacoBin and NOKMeans are available:\n  https://bitbucket.org/fxpfxp/autojacobin\n  https://bitbucket.org/fxpfxp/nokmeans The SIFT10M dataset is available at:\n  http://archive.ics.uci.edu/ml/datasets/SIFT10M", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Binary codes can be used to speed up nearest neighbor search tasks in large\nscale data sets as they are efficient for both storage and retrieval. In this\npaper, we propose a robust auto-encoder model that preserves the geometric\nrelationships of high-dimensional data sets in Hamming space. This is done by\nconsidering a noise-removing function in a region surrounding the manifold\nwhere the training data points lie. This function is defined with the property\nthat it projects the data points near the manifold into the manifold wisely,\nand we approximate this function by its first order approximation. Experimental\nresults show that the proposed method achieves better than state-of-the-art\nresults on three large scale high dimensional data sets.\n", "versions": [{"version": "v1", "created": "Thu, 25 Feb 2016 21:47:16 GMT"}, {"version": "v2", "created": "Tue, 1 Mar 2016 06:22:28 GMT"}], "update_date": "2016-03-02", "authors_parsed": [["Fu", "Xiping", ""], ["McCane", "Brendan", ""], ["Mills", "Steven", ""], ["Albert", "Michael", ""], ["Szymanski", "Lech", ""]]}, {"id": "1602.08128", "submitter": "Zhenhao Ge", "authors": "Zhenhao Ge, Sudhendu R. Sharma, Mark J. T. Smith", "title": "PCA Method for Automated Detection of Mispronounced Words", "comments": "SPIE Defense, Security, and Sensing", "journal-ref": null, "doi": "10.1117/12.884155", "report-no": null, "categories": "cs.SD cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a method for detecting mispronunciations with the aim of\nimproving Computer Assisted Language Learning (CALL) tools used by foreign\nlanguage learners. The algorithm is based on Principle Component Analysis\n(PCA). It is hierarchical with each successive step refining the estimate to\nclassify the test word as being either mispronounced or correct. Preprocessing\nbefore detection, like normalization and time-scale modification, is\nimplemented to guarantee uniformity of the feature vectors input to the\ndetection system. The performance using various features including spectrograms\nand Mel-Frequency Cepstral Coefficients (MFCCs) are compared and evaluated.\nBest results were obtained using MFCCs, achieving up to 99% accuracy in word\nverification and 93% in native/non-native classification. Compared with Hidden\nMarkov Models (HMMs) which are used pervasively in recognition application,\nthis particular approach is computational efficient and effective when training\ndata is limited.\n", "versions": [{"version": "v1", "created": "Thu, 25 Feb 2016 21:48:56 GMT"}], "update_date": "2016-02-29", "authors_parsed": [["Ge", "Zhenhao", ""], ["Sharma", "Sudhendu R.", ""], ["Smith", "Mark J. T.", ""]]}, {"id": "1602.08151", "submitter": "Akshay Balsubramani", "authors": "Akshay Balsubramani", "title": "Learning to Abstain from Binary Prediction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A binary classifier capable of abstaining from making a label prediction has\ntwo goals in tension: minimizing errors, and avoiding abstaining unnecessarily\noften. In this work, we exactly characterize the best achievable tradeoff\nbetween these two goals in a general semi-supervised setting, given an ensemble\nof predictors of varying competence as well as unlabeled data on which we wish\nto predict or abstain. We give an algorithm for learning a classifier in this\nsetting which trades off its errors with abstentions in a minimax optimal\nmanner, is as efficient as linear learning and prediction, and is demonstrably\npractical. Our analysis extends to a large class of loss functions and other\nscenarios, including ensembles comprised of specialists that can themselves\nabstain.\n", "versions": [{"version": "v1", "created": "Thu, 25 Feb 2016 23:46:57 GMT"}, {"version": "v2", "created": "Tue, 29 Nov 2016 17:05:38 GMT"}], "update_date": "2016-11-30", "authors_parsed": [["Balsubramani", "Akshay", ""]]}, {"id": "1602.08159", "submitter": "Keisuke Fujii", "authors": "Keisuke Fujii and Kohei Nakajima", "title": "Harnessing disordered quantum dynamics for machine learning", "comments": "19 pages, 13 figures", "journal-ref": "Phys. Rev. Applied 8, 024030 (2017)", "doi": "10.1103/PhysRevApplied.8.024030", "report-no": null, "categories": "quant-ph cs.AI cs.LG cs.NE nlin.CD", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Quantum computer has an amazing potential of fast information processing.\nHowever, realisation of a digital quantum computer is still a challenging\nproblem requiring highly accurate controls and key application strategies. Here\nwe propose a novel platform, quantum reservoir computing, to solve these issues\nsuccessfully by exploiting natural quantum dynamics, which is ubiquitous in\nlaboratories nowadays, for machine learning. In this framework, nonlinear\ndynamics including classical chaos can be universally emulated in quantum\nsystems. A number of numerical experiments show that quantum systems consisting\nof at most seven qubits possess computational capabilities comparable to\nconventional recurrent neural networks of 500 nodes. This discovery opens up a\nnew paradigm for information processing with artificial intelligence powered by\nquantum physics.\n", "versions": [{"version": "v1", "created": "Fri, 26 Feb 2016 00:57:59 GMT"}, {"version": "v2", "created": "Wed, 9 Nov 2016 16:05:22 GMT"}], "update_date": "2017-09-06", "authors_parsed": [["Fujii", "Keisuke", ""], ["Nakajima", "Kohei", ""]]}, {"id": "1602.08186", "submitter": "Viet Ha-Thuc", "authors": "Viet Ha-Thuc, Ye Xu, Satya Pradeep Kanduri, Xianren Wu, Vijay Dialani,\n  Yan Yan, Abhishek Gupta, Shakti Sinha", "title": "Search by Ideal Candidates: Next Generation of Talent Search at LinkedIn", "comments": "Demonstrated at WWW 2015, The 25th International World Wide Web\n  Conference (WWW 2015)", "journal-ref": null, "doi": "10.1145/2872518.2890549", "report-no": null, "categories": "cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One key challenge in talent search is how to translate complex criteria of a\nhiring position into a search query. This typically requires deep knowledge on\nwhich skills are typically needed for the position, what are their\nalternatives, which companies are likely to have such candidates, etc. However,\nlisting examples of suitable candidates for a given position is a relatively\neasy job. Therefore, in order to help searchers overcome this challenge, we\ndesign a next generation of talent search paradigm at LinkedIn: Search by Ideal\nCandidates. This new system only needs the searcher to input one or several\nexamples of suitable candidates for the position. The system will generate a\nquery based on the input candidates and then retrieve and rank results based on\nthe query as well as the input candidates. The query is also shown to the\nsearcher to make the system transparent and to allow the searcher to interact\nwith it. As the searcher modifies the initial query and makes it deviate from\nthe ideal candidates, the search ranking function dynamically adjusts an\nrefreshes the ranking results balancing between the roles of query and ideal\ncandidates. As of writing this paper, the new system is being launched to our\ncustomers.\n", "versions": [{"version": "v1", "created": "Fri, 26 Feb 2016 03:22:40 GMT"}], "update_date": "2016-02-29", "authors_parsed": [["Ha-Thuc", "Viet", ""], ["Xu", "Ye", ""], ["Kanduri", "Satya Pradeep", ""], ["Wu", "Xianren", ""], ["Dialani", "Vijay", ""], ["Yan", "Yan", ""], ["Gupta", "Abhishek", ""], ["Sinha", "Shakti", ""]]}, {"id": "1602.08191", "submitter": "Hanjoo Kim", "authors": "Hanjoo Kim, Jaehong Park, Jaehee Jang, and Sungroh Yoon", "title": "DeepSpark: A Spark-Based Distributed Deep Learning Framework for\n  Commodity Clusters", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The increasing complexity of deep neural networks (DNNs) has made it\nchallenging to exploit existing large-scale data processing pipelines for\nhandling massive data and parameters involved in DNN training. Distributed\ncomputing platforms and GPGPU-based acceleration provide a mainstream solution\nto this computational challenge. In this paper, we propose DeepSpark, a\ndistributed and parallel deep learning framework that exploits Apache Spark on\ncommodity clusters. To support parallel operations, DeepSpark automatically\ndistributes workloads and parameters to Caffe/Tensorflow-running nodes using\nSpark, and iteratively aggregates training results by a novel lock-free\nasynchronous variant of the popular elastic averaging stochastic gradient\ndescent based update scheme, effectively complementing the synchronized\nprocessing capabilities of Spark. DeepSpark is an on-going project, and the\ncurrent release is available at http://deepspark.snu.ac.kr.\n", "versions": [{"version": "v1", "created": "Fri, 26 Feb 2016 04:18:21 GMT"}, {"version": "v2", "created": "Tue, 8 Mar 2016 08:32:16 GMT"}, {"version": "v3", "created": "Sat, 1 Oct 2016 02:44:07 GMT"}], "update_date": "2016-10-04", "authors_parsed": [["Kim", "Hanjoo", ""], ["Park", "Jaehong", ""], ["Jang", "Jaehee", ""], ["Yoon", "Sungroh", ""]]}, {"id": "1602.08194", "submitter": "Ryan Spring", "authors": "Ryan Spring, Anshumali Shrivastava", "title": "Scalable and Sustainable Deep Learning via Randomized Hashing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Current deep learning architectures are growing larger in order to learn from\ncomplex datasets. These architectures require giant matrix multiplication\noperations to train millions of parameters. Conversely, there is another\ngrowing trend to bring deep learning to low-power, embedded devices. The matrix\noperations, associated with both training and testing of deep networks, are\nvery expensive from a computational and energy standpoint. We present a novel\nhashing based technique to drastically reduce the amount of computation needed\nto train and test deep networks. Our approach combines recent ideas from\nadaptive dropouts and randomized hashing for maximum inner product search to\nselect the nodes with the highest activation efficiently. Our new algorithm for\ndeep learning reduces the overall computational cost of forward and\nback-propagation by operating on significantly fewer (sparse) nodes. As a\nconsequence, our algorithm uses only 5% of the total multiplications, while\nkeeping on average within 1% of the accuracy of the original model. A unique\nproperty of the proposed hashing based back-propagation is that the updates are\nalways sparse. Due to the sparse gradient updates, our algorithm is ideally\nsuited for asynchronous and parallel training leading to near linear speedup\nwith increasing number of cores. We demonstrate the scalability and\nsustainability (energy efficiency) of our proposed algorithm via rigorous\nexperimental evaluations on several real datasets.\n", "versions": [{"version": "v1", "created": "Fri, 26 Feb 2016 05:07:23 GMT"}, {"version": "v2", "created": "Mon, 5 Dec 2016 04:52:36 GMT"}], "update_date": "2016-12-06", "authors_parsed": [["Spring", "Ryan", ""], ["Shrivastava", "Anshumali", ""]]}, {"id": "1602.08210", "submitter": "Yuhuai(Tony) Wu", "authors": "Saizheng Zhang, Yuhuai Wu, Tong Che, Zhouhan Lin, Roland Memisevic,\n  Ruslan Salakhutdinov, Yoshua Bengio", "title": "Architectural Complexity Measures of Recurrent Neural Networks", "comments": "17 pages, 8 figures; To appear in NIPS2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we systematically analyze the connecting architectures of\nrecurrent neural networks (RNNs). Our main contribution is twofold: first, we\npresent a rigorous graph-theoretic framework describing the connecting\narchitectures of RNNs in general. Second, we propose three architecture\ncomplexity measures of RNNs: (a) the recurrent depth, which captures the RNN's\nover-time nonlinear complexity, (b) the feedforward depth, which captures the\nlocal input-output nonlinearity (similar to the \"depth\" in feedforward neural\nnetworks (FNNs)), and (c) the recurrent skip coefficient which captures how\nrapidly the information propagates over time. We rigorously prove each\nmeasure's existence and computability. Our experimental results show that RNNs\nmight benefit from larger recurrent depth and feedforward depth. We further\ndemonstrate that increasing recurrent skip coefficient offers performance\nboosts on long term dependency problems.\n", "versions": [{"version": "v1", "created": "Fri, 26 Feb 2016 06:16:27 GMT"}, {"version": "v2", "created": "Mon, 29 Feb 2016 17:18:21 GMT"}, {"version": "v3", "created": "Sat, 12 Nov 2016 19:38:43 GMT"}], "update_date": "2016-11-15", "authors_parsed": [["Zhang", "Saizheng", ""], ["Wu", "Yuhuai", ""], ["Che", "Tong", ""], ["Lin", "Zhouhan", ""], ["Memisevic", "Roland", ""], ["Salakhutdinov", "Ruslan", ""], ["Bengio", "Yoshua", ""]]}, {"id": "1602.08225", "submitter": "Wei Liu", "authors": "Wei Liu, Wei-Long Zheng, Bao-Liang Lu", "title": "Multimodal Emotion Recognition Using Multimodal Deep Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To enhance the performance of affective models and reduce the cost of\nacquiring physiological signals for real-world applications, we adopt\nmultimodal deep learning approach to construct affective models from multiple\nphysiological signals. For unimodal enhancement task, we indicate that the best\nrecognition accuracy of 82.11% on SEED dataset is achieved with shared\nrepresentations generated by Deep AutoEncoder (DAE) model. For multimodal\nfacilitation tasks, we demonstrate that the Bimodal Deep AutoEncoder (BDAE)\nachieves the mean accuracies of 91.01% and 83.25% on SEED and DEAP datasets,\nrespectively, which are much superior to the state-of-the-art approaches. For\ncross-modal learning task, our experimental results demonstrate that the mean\naccuracy of 66.34% is achieved on SEED dataset through shared representations\ngenerated by EEG-based DAE as training samples and shared representations\ngenerated by eye-based DAE as testing sample, and vice versa.\n", "versions": [{"version": "v1", "created": "Fri, 26 Feb 2016 07:43:14 GMT"}], "update_date": "2016-02-29", "authors_parsed": [["Liu", "Wei", ""], ["Zheng", "Wei-Long", ""], ["Lu", "Bao-Liang", ""]]}, {"id": "1602.08254", "submitter": "Melanie Schmidt", "authors": "Johannes Bl\\\"omer, Christiane Lammersen, Melanie Schmidt, Christian\n  Sohler", "title": "Theoretical Analysis of the $k$-Means Algorithm - A Survey", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The $k$-means algorithm is one of the most widely used clustering heuristics.\nDespite its simplicity, analyzing its running time and quality of approximation\nis surprisingly difficult and can lead to deep insights that can be used to\nimprove the algorithm. In this paper we survey the recent results in this\ndirection as well as several extension of the basic $k$-means method.\n", "versions": [{"version": "v1", "created": "Fri, 26 Feb 2016 09:39:50 GMT"}], "update_date": "2016-02-29", "authors_parsed": [["Bl\u00f6mer", "Johannes", ""], ["Lammersen", "Christiane", ""], ["Schmidt", "Melanie", ""], ["Sohler", "Christian", ""]]}, {"id": "1602.08332", "submitter": "Felix Leibfried", "authors": "Felix Leibfried and Daniel Alexander Braun", "title": "Bounded Rational Decision-Making in Feedforward Neural Networks", "comments": "Proceedings of the 32nd Conference on Uncertainty in Artificial\n  Intelligence (UAI), New York City, NY, USA, 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bounded rational decision-makers transform sensory input into motor output\nunder limited computational resources. Mathematically, such decision-makers can\nbe modeled as information-theoretic channels with limited transmission rate.\nHere, we apply this formalism for the first time to multilayer feedforward\nneural networks. We derive synaptic weight update rules for two scenarios,\nwhere either each neuron is considered as a bounded rational decision-maker or\nthe network as a whole. In the update rules, bounded rationality translates\ninto information-theoretically motivated types of regularization in weight\nspace. In experiments on the MNIST benchmark classification task for\nhandwritten digits, we show that such information-theoretic regularization\nsuccessfully prevents overfitting across different architectures and attains\nresults that are competitive with other recent techniques like dropout,\ndropconnect and Bayes by backprop, for both ordinary and convolutional neural\nnetworks.\n", "versions": [{"version": "v1", "created": "Fri, 26 Feb 2016 14:15:03 GMT"}, {"version": "v2", "created": "Mon, 23 May 2016 15:51:07 GMT"}], "update_date": "2016-05-24", "authors_parsed": [["Leibfried", "Felix", ""], ["Braun", "Daniel Alexander", ""]]}, {"id": "1602.08350", "submitter": "Patrick O. Glauner", "authors": "Patrick O. Glauner, Andre Boechat, Lautaro Dolberg, Radu State, Franck\n  Bettinger, Yves Rangoni, Diogo Duarte", "title": "Large-Scale Detection of Non-Technical Losses in Imbalanced Data Sets", "comments": "Proceedings of the Seventh IEEE Conference on Innovative Smart Grid\n  Technologies (ISGT 2016)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Non-technical losses (NTL) such as electricity theft cause significant harm\nto our economies, as in some countries they may range up to 40% of the total\nelectricity distributed. Detecting NTLs requires costly on-site inspections.\nAccurate prediction of NTLs for customers using machine learning is therefore\ncrucial. To date, related research largely ignore that the two classes of\nregular and non-regular customers are highly imbalanced, that NTL proportions\nmay change and mostly consider small data sets, often not allowing to deploy\nthe results in production. In this paper, we present a comprehensive approach\nto assess three NTL detection models for different NTL proportions in large\nreal world data sets of 100Ks of customers: Boolean rules, fuzzy logic and\nSupport Vector Machine. This work has resulted in appreciable results that are\nabout to be deployed in a leading industry solution. We believe that the\nconsiderations and observations made in this contribution are necessary for\nfuture smart meter research in order to report their effectiveness on\nimbalanced and large real world data sets.\n", "versions": [{"version": "v1", "created": "Fri, 26 Feb 2016 14:49:29 GMT"}, {"version": "v2", "created": "Tue, 25 Jul 2017 04:44:12 GMT"}], "update_date": "2017-07-26", "authors_parsed": [["Glauner", "Patrick O.", ""], ["Boechat", "Andre", ""], ["Dolberg", "Lautaro", ""], ["State", "Radu", ""], ["Bettinger", "Franck", ""], ["Rangoni", "Yves", ""], ["Duarte", "Diogo", ""]]}, {"id": "1602.08448", "submitter": "Daniel Russo", "authors": "Daniel Russo", "title": "Simple Bayesian Algorithms for Best Arm Identification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper considers the optimal adaptive allocation of measurement effort\nfor identifying the best among a finite set of options or designs. An\nexperimenter sequentially chooses designs to measure and observes noisy signals\nof their quality with the goal of confidently identifying the best design after\na small number of measurements. This paper proposes three simple and intuitive\nBayesian algorithms for adaptively allocating measurement effort, and\nformalizes a sense in which these seemingly naive rules are the best possible.\nOne proposal is top-two probability sampling, which computes the two designs\nwith the highest posterior probability of being optimal, and then randomizes to\nselect among these two. One is a variant of top-two sampling which considers\nnot only the probability a design is optimal, but the expected amount by which\nits quality exceeds that of other designs. The final algorithm is a modified\nversion of Thompson sampling that is tailored for identifying the best design.\nWe prove that these simple algorithms satisfy a sharp optimality property. In a\nfrequentist setting where the true quality of the designs is fixed, one hopes\nthe posterior definitively identifies the optimal design, in the sense that\nthat the posterior probability assigned to the event that some other design is\noptimal converges to zero as measurements are collected. We show that under the\nproposed algorithms this convergence occurs at an exponential rate, and the\ncorresponding exponent is the best possible among all allocation\n", "versions": [{"version": "v1", "created": "Fri, 26 Feb 2016 19:39:01 GMT"}, {"version": "v2", "created": "Mon, 29 Feb 2016 07:28:19 GMT"}, {"version": "v3", "created": "Wed, 7 Dec 2016 21:30:55 GMT"}, {"version": "v4", "created": "Fri, 8 Jun 2018 13:41:30 GMT"}], "update_date": "2018-06-11", "authors_parsed": [["Russo", "Daniel", ""]]}, {"id": "1602.08486", "submitter": "Garrison Cottrell", "authors": "Honghao Shan, Matthew H. Tong, Garrison W. Cottrell", "title": "A Single Model Explains both Visual and Auditory Precortical Coding", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC cs.CV cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Precortical neural systems encode information collected by the senses, but\nthe driving principles of the encoding used have remained a subject of debate.\nWe present a model of retinal coding that is based on three constraints:\ninformation preservation, minimization of the neural wiring, and response\nequalization. The resulting novel version of sparse principal components\nanalysis successfully captures a number of known characteristics of the retinal\ncoding system, such as center-surround receptive fields, color opponency\nchannels, and spatiotemporal responses that correspond to magnocellular and\nparvocellular pathways. Furthermore, when trained on auditory data, the same\nmodel learns receptive fields well fit by gammatone filters, commonly used to\nmodel precortical auditory coding. This suggests that efficient coding may be a\nunifying principle of precortical encoding across modalities.\n", "versions": [{"version": "v1", "created": "Fri, 26 Feb 2016 10:17:53 GMT"}, {"version": "v2", "created": "Thu, 7 Apr 2016 19:19:11 GMT"}], "update_date": "2016-04-08", "authors_parsed": [["Shan", "Honghao", ""], ["Tong", "Matthew H.", ""], ["Cottrell", "Garrison W.", ""]]}, {"id": "1602.08671", "submitter": "Greg Yang", "authors": "Greg Yang", "title": "Lie Access Neural Turing Machine", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Following the recent trend in explicit neural memory structures, we present a\nnew design of an external memory, wherein memories are stored in an Euclidean\nkey space $\\mathbb R^n$. An LSTM controller performs read and write via\nspecialized read and write heads. It can move a head by either providing a new\naddress in the key space (aka random access) or moving from its previous\nposition via a Lie group action (aka Lie access). In this way, the \"L\" and \"R\"\ninstructions of a traditional Turing Machine are generalized to arbitrary\nelements of a fixed Lie group action. For this reason, we name this new model\nthe Lie Access Neural Turing Machine, or LANTM.\n  We tested two different configurations of LANTM against an LSTM baseline in\nseveral basic experiments. We found the right configuration of LANTM to\noutperform the baseline in all of our experiments. In particular, we trained\nLANTM on addition of $k$-digit numbers for $2 \\le k \\le 16$, but it was able to\ngeneralize almost perfectly to $17 \\le k \\le 32$, all with the number of\nparameters 2 orders of magnitude below the LSTM baseline.\n", "versions": [{"version": "v1", "created": "Sun, 28 Feb 2016 04:55:19 GMT"}, {"version": "v2", "created": "Tue, 23 Aug 2016 01:23:46 GMT"}, {"version": "v3", "created": "Tue, 6 Sep 2016 14:42:56 GMT"}], "update_date": "2016-09-07", "authors_parsed": [["Yang", "Greg", ""]]}, {"id": "1602.08734", "submitter": "Tim Salimans", "authors": "Tim Salimans", "title": "A Structured Variational Auto-encoder for Learning Deep Hierarchies of\n  Sparse Features", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this note we present a generative model of natural images consisting of a\ndeep hierarchy of layers of latent random variables, each of which follows a\nnew type of distribution that we call rectified Gaussian. These rectified\nGaussian units allow spike-and-slab type sparsity, while retaining the\ndifferentiability necessary for efficient stochastic gradient variational\ninference. To learn the parameters of the new model, we approximate the\nposterior of the latent variables with a variational auto-encoder. Rather than\nmaking the usual mean-field assumption however, the encoder parameterizes a new\ntype of structured variational approximation that retains the prior\ndependencies of the generative model. Using this structured posterior\napproximation, we are able to perform joint training of deep models with many\nlayers of latent random variables, without having to resort to stacking or\nother layerwise training procedures.\n", "versions": [{"version": "v1", "created": "Sun, 28 Feb 2016 16:10:40 GMT"}], "update_date": "2016-03-01", "authors_parsed": [["Salimans", "Tim", ""]]}, {"id": "1602.08761", "submitter": "Tolga Bolukbasi", "authors": "Tolga Bolukbasi, Kai-Wei Chang, Joseph Wang, Venkatesh Saligrama", "title": "Resource Constrained Structured Prediction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CL cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of structured prediction under test-time budget\nconstraints. We propose a novel approach applicable to a wide range of\nstructured prediction problems in computer vision and natural language\nprocessing. Our approach seeks to adaptively generate computationally costly\nfeatures during test-time in order to reduce the computational cost of\nprediction while maintaining prediction performance. We show that training the\nadaptive feature generation system can be reduced to a series of structured\nlearning problems, resulting in efficient training using existing structured\nlearning algorithms. This framework provides theoretical justification for\nseveral existing heuristic approaches found in literature. We evaluate our\nproposed adaptive system on two structured prediction tasks, optical character\nrecognition (OCR) and dependency parsing and show strong performance in\nreduction of the feature costs without degrading accuracy.\n", "versions": [{"version": "v1", "created": "Sun, 28 Feb 2016 19:44:57 GMT"}, {"version": "v2", "created": "Wed, 8 Jun 2016 01:31:01 GMT"}], "update_date": "2016-06-09", "authors_parsed": [["Bolukbasi", "Tolga", ""], ["Chang", "Kai-Wei", ""], ["Wang", "Joseph", ""], ["Saligrama", "Venkatesh", ""]]}, {"id": "1602.08771", "submitter": "Adam White", "authors": "Adam White, Martha White", "title": "Investigating practical linear temporal difference learning", "comments": "Autonomous Agents and Multi-agent Systems, 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Off-policy reinforcement learning has many applications including: learning\nfrom demonstration, learning multiple goal seeking policies in parallel, and\nrepresenting predictive knowledge. Recently there has been an proliferation of\nnew policy-evaluation algorithms that fill a longstanding algorithmic void in\nreinforcement learning: combining robustness to off-policy sampling, function\napproximation, linear complexity, and temporal difference (TD) updates. This\npaper contains two main contributions. First, we derive two new hybrid TD\npolicy-evaluation algorithms, which fill a gap in this collection of\nalgorithms. Second, we perform an empirical comparison to elicit which of these\nnew linear TD methods should be preferred in different situations, and make\nconcrete suggestions about practical use.\n", "versions": [{"version": "v1", "created": "Sun, 28 Feb 2016 21:23:54 GMT"}, {"version": "v2", "created": "Thu, 31 Mar 2016 01:10:08 GMT"}], "update_date": "2016-04-01", "authors_parsed": [["White", "Adam", ""], ["White", "Martha", ""]]}, {"id": "1602.08780", "submitter": "Dirk Tasche", "authors": "Dirk Tasche", "title": "Does quantification without adjustments work?", "comments": "20 pages, 2 figures, major update", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Classification is the task of predicting the class labels of objects based on\nthe observation of their features. In contrast, quantification has been defined\nas the task of determining the prevalences of the different sorts of class\nlabels in a target dataset. The simplest approach to quantification is Classify\n& Count where a classifier is optimised for classification on a training set\nand applied to the target dataset for the prediction of class labels. In the\ncase of binary quantification, the number of predicted positive labels is then\nused as an estimate of the prevalence of the positive class in the target\ndataset. Since the performance of Classify & Count for quantification is known\nto be inferior its results typically are subject to adjustments. However, some\nresearchers recently have suggested that Classify & Count might actually work\nwithout adjustments if it is based on a classifer that was specifically trained\nfor quantification. We discuss the theoretical foundation for this claim and\nexplore its potential and limitations with a numerical example based on the\nbinormal model with equal variances. In order to identify an optimal quantifier\nin the binormal setting, we introduce the concept of local Bayes optimality. As\na side remark, we present a complete proof of a theorem by Ye et al. (2012).\n", "versions": [{"version": "v1", "created": "Sun, 28 Feb 2016 22:29:25 GMT"}, {"version": "v2", "created": "Fri, 12 Aug 2016 16:24:05 GMT"}], "update_date": "2016-08-15", "authors_parsed": [["Tasche", "Dirk", ""]]}, {"id": "1602.08800", "submitter": "Vitaly Bulgakov", "authors": "Vitaly Bulgakov", "title": "Iterative Aggregation Method for Solving Principal Component Analysis\n  Problems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NA cs.IR cs.LG", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Motivated by the previously developed multilevel aggregation method for\nsolving structural analysis problems a novel two-level aggregation approach for\nefficient iterative solution of Principal Component Analysis (PCA) problems is\nproposed. The course aggregation model of the original covariance matrix is\nused in the iterative solution of the eigenvalue problem by a power iterations\nmethod. The method is tested on several data sets consisting of large number of\ntext documents.\n", "versions": [{"version": "v1", "created": "Mon, 29 Feb 2016 02:40:05 GMT"}], "update_date": "2016-03-01", "authors_parsed": [["Bulgakov", "Vitaly", ""]]}, {"id": "1602.08886", "submitter": "Ravi Kolla", "authors": "Ravi Kumar Kolla, Krishna Jagannathan, Aditya Gopalan", "title": "Collaborative Learning of Stochastic Bandits over a Social Network", "comments": "14 Pages, 6 Figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider a collaborative online learning paradigm, wherein a group of\nagents connected through a social network are engaged in playing a stochastic\nmulti-armed bandit game. Each time an agent takes an action, the corresponding\nreward is instantaneously observed by the agent, as well as its neighbours in\nthe social network. We perform a regret analysis of various policies in this\ncollaborative learning setting. A key finding of this paper is that natural\nextensions of widely-studied single agent learning policies to the network\nsetting need not perform well in terms of regret. In particular, we identify a\nclass of non-altruistic and individually consistent policies, and argue by\nderiving regret lower bounds that they are liable to suffer a large regret in\nthe networked setting. We also show that the learning performance can be\nsubstantially improved if the agents exploit the structure of the network, and\ndevelop a simple learning algorithm based on dominating sets of the network.\nSpecifically, we first consider a star network, which is a common motif in\nhierarchical social networks, and show analytically that the hub agent can be\nused as an information sink to expedite learning and improve the overall\nregret. We also derive networkwide regret bounds for the algorithm applied to\ngeneral networks. We conduct numerical experiments on a variety of networks to\ncorroborate our analytical results.\n", "versions": [{"version": "v1", "created": "Mon, 29 Feb 2016 09:53:28 GMT"}, {"version": "v2", "created": "Mon, 11 Jul 2016 06:31:57 GMT"}], "update_date": "2016-07-12", "authors_parsed": [["Kolla", "Ravi Kumar", ""], ["Jagannathan", "Krishna", ""], ["Gopalan", "Aditya", ""]]}, {"id": "1602.08927", "submitter": "Martin Spindler", "authors": "Ye Luo and Martin Spindler", "title": "High-Dimensional $L_2$Boosting: Rate of Convergence", "comments": "19 pages, 4 tables; AMS 2000 subject classifications: Primary 62J05,\n  62J07, 41A25; secondary 49M15, 68Q32", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG econ.EM math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Boosting is one of the most significant developments in machine learning.\nThis paper studies the rate of convergence of $L_2$Boosting, which is tailored\nfor regression, in a high-dimensional setting. Moreover, we introduce so-called\n\\textquotedblleft post-Boosting\\textquotedblright. This is a post-selection\nestimator which applies ordinary least squares to the variables selected in the\nfirst stage by $L_2$Boosting. Another variant is \\textquotedblleft Orthogonal\nBoosting\\textquotedblright\\ where after each step an orthogonal projection is\nconducted. We show that both post-$L_2$Boosting and the orthogonal boosting\nachieve the same rate of convergence as LASSO in a sparse, high-dimensional\nsetting. We show that the rate of convergence of the classical $L_2$Boosting\ndepends on the design matrix described by a sparse eigenvalue constant. To show\nthe latter results, we derive new approximation results for the pure greedy\nalgorithm, based on analyzing the revisiting behavior of $L_2$Boosting. We also\nintroduce feasible rules for early stopping, which can be easily implemented\nand used in applied work. Our results also allow a direct comparison between\nLASSO and boosting which has been missing from the literature. Finally, we\npresent simulation studies and applications to illustrate the relevance of our\ntheoretical results and to provide insights into the practical aspects of\nboosting. In these simulation studies, post-$L_2$Boosting clearly outperforms\nLASSO.\n", "versions": [{"version": "v1", "created": "Mon, 29 Feb 2016 12:05:53 GMT"}, {"version": "v2", "created": "Sat, 5 Nov 2016 14:35:38 GMT"}], "update_date": "2017-09-28", "authors_parsed": [["Luo", "Ye", ""], ["Spindler", "Martin", ""]]}, {"id": "1602.08952", "submitter": "\\'Akos K\\'ad\\'ar", "authors": "\\'Akos K\\'ad\\'ar, Grzegorz Chrupa{\\l}a, Afra Alishahi", "title": "Representation of linguistic form and function in recurrent neural\n  networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present novel methods for analyzing the activation patterns of RNNs from a\nlinguistic point of view and explore the types of linguistic structure they\nlearn. As a case study, we use a multi-task gated recurrent network\narchitecture consisting of two parallel pathways with shared word embeddings\ntrained on predicting the representations of the visual scene corresponding to\nan input sentence, and predicting the next word in the same sentence. Based on\nour proposed method to estimate the amount of contribution of individual tokens\nin the input to the final prediction of the networks we show that the image\nprediction pathway: a) is sensitive to the information structure of the\nsentence b) pays selective attention to lexical categories and grammatical\nfunctions that carry semantic information c) learns to treat the same input\ntoken differently depending on its grammatical functions in the sentence. In\ncontrast the language model is comparatively more sensitive to words with a\nsyntactic function. Furthermore, we propose methods to ex- plore the function\nof individual hidden units in RNNs and show that the two pathways of the\narchitecture in our case study contain specialized units tuned to patterns\ninformative for the task, some of which can carry activations to later time\nsteps to encode long-term dependencies.\n", "versions": [{"version": "v1", "created": "Mon, 29 Feb 2016 13:31:17 GMT"}, {"version": "v2", "created": "Wed, 8 Jun 2016 12:30:12 GMT"}], "update_date": "2016-06-09", "authors_parsed": [["K\u00e1d\u00e1r", "\u00c1kos", ""], ["Chrupa\u0142a", "Grzegorz", ""], ["Alishahi", "Afra", ""]]}, {"id": "1602.08986", "submitter": "G\\'eraud Le Falher", "authors": "G\\'eraud Le Falher and Fabio Vitale", "title": "Even Trolls Are Useful: Efficient Link Classification in Signed Networks", "comments": "17 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.SI physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We address the problem of classifying the links of signed social networks\ngiven their full structural topology. Motivated by a binary user behaviour\nassumption, which is supported by decades of research in psychology, we develop\nan efficient and surprisingly simple approach to solve this classification\nproblem. Our methods operate both within the active and batch settings. We\ndemonstrate that the algorithms we developed are extremely fast in both\ntheoretical and practical terms. Within the active setting, we provide a new\ncomplexity measure and a rigorous analysis of our methods that hold for\narbitrary signed networks. We validate our theoretical claims carrying out a\nset of experiments on three well known real-world datasets, showing that our\nmethods outperform the competitors while being much faster.\n", "versions": [{"version": "v1", "created": "Mon, 29 Feb 2016 14:45:18 GMT"}], "update_date": "2016-03-01", "authors_parsed": [["Falher", "G\u00e9raud Le", ""], ["Vitale", "Fabio", ""]]}, {"id": "1602.09013", "submitter": "Anastasia Podosinnikova", "authors": "Anastasia Podosinnikova, Francis Bach, and Simon Lacoste-Julien", "title": "Beyond CCA: Moment Matching for Multi-View Models", "comments": "Appears in: Proceedings of the 33rd International Conference on\n  Machine Learning (ICML 2016). 22 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce three novel semi-parametric extensions of probabilistic\ncanonical correlation analysis with identifiability guarantees. We consider\nmoment matching techniques for estimation in these models. For that, by drawing\nexplicit links between the new models and a discrete version of independent\ncomponent analysis (DICA), we first extend the DICA cumulant tensors to the new\ndiscrete version of CCA. By further using a close connection with independent\ncomponent analysis, we introduce generalized covariance matrices, which can\nreplace the cumulant tensors in the moment matching framework, and, therefore,\nimprove sample complexity and simplify derivations and algorithms\nsignificantly. As the tensor power method or orthogonal joint diagonalization\nare not applicable in the new setting, we use non-orthogonal joint\ndiagonalization techniques for matching the cumulants. We demonstrate\nperformance of the proposed models and estimation techniques on experiments\nwith both synthetic and real datasets.\n", "versions": [{"version": "v1", "created": "Mon, 29 Feb 2016 15:51:50 GMT"}, {"version": "v2", "created": "Fri, 3 Jun 2016 14:06:23 GMT"}], "update_date": "2016-06-06", "authors_parsed": [["Podosinnikova", "Anastasia", ""], ["Bach", "Francis", ""], ["Lacoste-Julien", "Simon", ""]]}, {"id": "1602.09118", "submitter": "Joshua Achiam", "authors": "Joshua Achiam", "title": "Easy Monotonic Policy Iteration", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A key problem in reinforcement learning for control with general function\napproximators (such as deep neural networks and other nonlinear functions) is\nthat, for many algorithms employed in practice, updates to the policy or\n$Q$-function may fail to improve performance---or worse, actually cause the\npolicy performance to degrade. Prior work has addressed this for policy\niteration by deriving tight policy improvement bounds; by optimizing the lower\nbound on policy improvement, a better policy is guaranteed. However, existing\napproaches suffer from bounds that are hard to optimize in practice because\nthey include sup norm terms which cannot be efficiently estimated or\ndifferentiated. In this work, we derive a better policy improvement bound where\nthe sup norm of the policy divergence has been replaced with an average\ndivergence; this leads to an algorithm, Easy Monotonic Policy Iteration, that\ngenerates sequences of policies with guaranteed non-decreasing returns and is\neasy to implement in a sample-based framework.\n", "versions": [{"version": "v1", "created": "Mon, 29 Feb 2016 19:59:16 GMT"}], "update_date": "2016-03-01", "authors_parsed": [["Achiam", "Joshua", ""]]}]