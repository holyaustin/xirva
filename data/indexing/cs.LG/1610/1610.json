[{"id": "1610.00054", "submitter": "Xuan Hong Dang", "authors": "Xuan-Hong Dang, Arlei Silva, Ambuj Singh, Ananthram Swami, Prithwish\n  Basu", "title": "Outlier Detection from Network Data with Subnetwork Interpretation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Detecting a small number of outliers from a set of data observations is\nalways challenging. This problem is more difficult in the setting of multiple\nnetwork samples, where computing the anomalous degree of a network sample is\ngenerally not sufficient. In fact, explaining why the network is exceptional,\nexpressed in the form of subnetwork, is also equally important. In this paper,\nwe develop a novel algorithm to address these two key problems. We treat each\nnetwork sample as a potential outlier and identify subnetworks that mostly\ndiscriminate it from nearby regular samples. The algorithm is developed in the\nframework of network regression combined with the constraints on both network\ntopology and L1-norm shrinkage to perform subnetwork discovery. Our method thus\ngoes beyond subspace/subgraph discovery and we show that it converges to a\nglobal optimum. Evaluation on various real-world network datasets demonstrates\nthat our algorithm not only outperforms baselines in both network and high\ndimensional setting, but also discovers highly relevant and interpretable local\nsubnetworks, further enhancing our understanding of anomalous networks.\n", "versions": [{"version": "v1", "created": "Fri, 30 Sep 2016 23:13:28 GMT"}], "update_date": "2016-10-04", "authors_parsed": [["Dang", "Xuan-Hong", ""], ["Silva", "Arlei", ""], ["Singh", "Ambuj", ""], ["Swami", "Ananthram", ""], ["Basu", "Prithwish", ""]]}, {"id": "1610.00064", "submitter": "Christopher Morris", "authors": "Christopher Morris, Nils M. Kriege, Kristian Kersting, Petra Mutzel", "title": "Faster Kernels for Graphs with Continuous Attributes via Hashing", "comments": "IEEE ICDM 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While state-of-the-art kernels for graphs with discrete labels scale well to\ngraphs with thousands of nodes, the few existing kernels for graphs with\ncontinuous attributes, unfortunately, do not scale well. To overcome this\nlimitation, we present hash graph kernels, a general framework to derive\nkernels for graphs with continuous attributes from discrete ones. The idea is\nto iteratively turn continuous attributes into discrete labels using randomized\nhash functions. We illustrate hash graph kernels for the Weisfeiler-Lehman\nsubtree kernel and for the shortest-path kernel. The resulting novel graph\nkernels are shown to be, both, able to handle graphs with continuous attributes\nand scalable to large graphs and data sets. This is supported by our\ntheoretical analysis and demonstrated by an extensive experimental evaluation.\n", "versions": [{"version": "v1", "created": "Sat, 1 Oct 2016 00:43:19 GMT"}], "update_date": "2016-10-04", "authors_parsed": [["Morris", "Christopher", ""], ["Kriege", "Nils M.", ""], ["Kersting", "Kristian", ""], ["Mutzel", "Petra", ""]]}, {"id": "1610.00081", "submitter": "Junbo Zhang", "authors": "Junbo Zhang, Yu Zheng, Dekang Qi", "title": "Deep Spatio-Temporal Residual Networks for Citywide Crowd Flows\n  Prediction", "comments": "AAAI 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Forecasting the flow of crowds is of great importance to traffic management\nand public safety, yet a very challenging task affected by many complex\nfactors, such as inter-region traffic, events and weather. In this paper, we\npropose a deep-learning-based approach, called ST-ResNet, to collectively\nforecast the in-flow and out-flow of crowds in each and every region through a\ncity. We design an end-to-end structure of ST-ResNet based on unique properties\nof spatio-temporal data. More specifically, we employ the framework of the\nresidual neural networks to model the temporal closeness, period, and trend\nproperties of the crowd traffic, respectively. For each property, we design a\nbranch of residual convolutional units, each of which models the spatial\nproperties of the crowd traffic. ST-ResNet learns to dynamically aggregate the\noutput of the three residual neural networks based on data, assigning different\nweights to different branches and regions. The aggregation is further combined\nwith external factors, such as weather and day of the week, to predict the\nfinal traffic of crowds in each and every region. We evaluate ST-ResNet based\non two types of crowd flows in Beijing and NYC, finding that its performance\nexceeds six well-know methods.\n", "versions": [{"version": "v1", "created": "Sat, 1 Oct 2016 03:56:13 GMT"}, {"version": "v2", "created": "Tue, 10 Jan 2017 09:53:16 GMT"}], "update_date": "2017-01-11", "authors_parsed": [["Zhang", "Junbo", ""], ["Zheng", "Yu", ""], ["Qi", "Dekang", ""]]}, {"id": "1610.00085", "submitter": "Nevin L.  Zhang", "authors": "Nevin L. Zhang and Leonard K. M. Poon", "title": "Latent Tree Analysis", "comments": "7 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Latent tree analysis seeks to model the correlations among a set of random\nvariables using a tree of latent variables. It was proposed as an improvement\nto latent class analysis --- a method widely used in social sciences and\nmedicine to identify homogeneous subgroups in a population. It provides new and\nfruitful perspectives on a number of machine learning areas, including cluster\nanalysis, topic detection, and deep probabilistic modeling. This paper gives an\noverview of the research on latent tree analysis and various ways it is used in\npractice.\n", "versions": [{"version": "v1", "created": "Sat, 1 Oct 2016 04:26:48 GMT"}], "update_date": "2016-10-04", "authors_parsed": [["Zhang", "Nevin L.", ""], ["Poon", "Leonard K. M.", ""]]}, {"id": "1610.00087", "submitter": "Wei Dai", "authors": "Wei Dai, Chia Dai, Shuhui Qu, Juncheng Li, Samarjit Das", "title": "Very Deep Convolutional Neural Networks for Raw Waveforms", "comments": "5 pages, 2 figures, under submission to International Conference on\n  Acoustics, Speech and Signal Processing (ICASSP) 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning acoustic models directly from the raw waveform data with minimal\nprocessing is challenging. Current waveform-based models have generally used\nvery few (~2) convolutional layers, which might be insufficient for building\nhigh-level discriminative features. In this work, we propose very deep\nconvolutional neural networks (CNNs) that directly use time-domain waveforms as\ninputs. Our CNNs, with up to 34 weight layers, are efficient to optimize over\nvery long sequences (e.g., vector of size 32000), necessary for processing\nacoustic waveforms. This is achieved through batch normalization, residual\nlearning, and a careful design of down-sampling in the initial layers. Our\nnetworks are fully convolutional, without the use of fully connected layers and\ndropout, to maximize representation learning. We use a large receptive field in\nthe first convolutional layer to mimic bandpass filters, but very small\nreceptive fields subsequently to control the model capacity. We demonstrate the\nperformance gains with the deeper models. Our evaluation shows that the CNN\nwith 18 weight layers outperform the CNN with 3 weight layers by over 15% in\nabsolute accuracy for an environmental sound recognition task and matches the\nperformance of models using log-mel features.\n", "versions": [{"version": "v1", "created": "Sat, 1 Oct 2016 05:15:15 GMT"}], "update_date": "2016-10-04", "authors_parsed": [["Dai", "Wei", ""], ["Dai", "Chia", ""], ["Qu", "Shuhui", ""], ["Li", "Juncheng", ""], ["Das", "Samarjit", ""]]}, {"id": "1610.00192", "submitter": "Tanay Kumar Saha", "authors": "Tanay Kumar Saha, Mourad Ouzzani, Hossam M. Hammady, Ahmed K.\n  Elmagarmid, Wajdi Dhifli, and Mohammad Al Hasan", "title": "A large scale study of SVM based methods for abstract screening in\n  systematic reviews", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A major task in systematic reviews is abstract screening, i.e., excluding,\noften hundreds or thousand of, irrelevant citations returned from a database\nsearch based on titles and abstracts. Thus, a systematic review platform that\ncan automate the abstract screening process is of huge importance. Several\nmethods have been proposed for this task. However, it is very hard to clearly\nunderstand the applicability of these methods in a systematic review platform\nbecause of the following challenges: (1) the use of non-overlapping metrics for\nthe evaluation of the proposed methods, (2) usage of features that are very\nhard to collect, (3) using a small set of reviews for the evaluation, and (4)\nno solid statistical testing or equivalence grouping of the methods. In this\npaper, we use feature representation that can be extracted per citation. We\nevaluate SVM-based methods (commonly used) on a large set of reviews ($61$) and\nmetrics ($11$) to provide equivalence grouping of methods based on a solid\nstatistical test. Our analysis also includes a strong variability of the\nmetrics using $500$x$2$ cross validation. While some methods shine for\ndifferent metrics and for different datasets, there is no single method that\ndominates the pack. Furthermore, we observe that in some cases relevant\n(included) citations can be found after screening only 15-20% of them via a\ncertainty based sampling. A few included citations present outlying\ncharacteristics and can only be found after a very large number of screening\nsteps. Finally, we present an ensemble algorithm for producing a $5$-star\nrating of citations based on their relevance. Such algorithm combines the best\nmethods from our evaluation and through its $5$-star rating outputs a more\neasy-to-consume prediction.\n", "versions": [{"version": "v1", "created": "Sat, 1 Oct 2016 21:11:38 GMT"}, {"version": "v2", "created": "Thu, 13 Oct 2016 12:56:11 GMT"}, {"version": "v3", "created": "Tue, 16 Jan 2018 00:52:51 GMT"}], "update_date": "2018-01-17", "authors_parsed": [["Saha", "Tanay Kumar", ""], ["Ouzzani", "Mourad", ""], ["Hammady", "Hossam M.", ""], ["Elmagarmid", "Ahmed K.", ""], ["Dhifli", "Wajdi", ""], ["Hasan", "Mohammad Al", ""]]}, {"id": "1610.00243", "submitter": "Elad Hoffer", "authors": "Elad Hoffer, Itay Hubara, Nir Ailon", "title": "Deep unsupervised learning through spatial contrasting", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolutional networks have marked their place over the last few years as the\nbest performing model for various visual tasks. They are, however, most suited\nfor supervised learning from large amounts of labeled data. Previous attempts\nhave been made to use unlabeled data to improve model performance by applying\nunsupervised techniques. These attempts require different architectures and\ntraining methods. In this work we present a novel approach for unsupervised\ntraining of Convolutional networks that is based on contrasting between spatial\nregions within images. This criterion can be employed within conventional\nneural networks and trained using standard techniques such as SGD and\nback-propagation, thus complementing supervised methods.\n", "versions": [{"version": "v1", "created": "Sun, 2 Oct 2016 08:42:59 GMT"}, {"version": "v2", "created": "Tue, 4 Dec 2018 15:38:31 GMT"}], "update_date": "2018-12-05", "authors_parsed": [["Hoffer", "Elad", ""], ["Hubara", "Itay", ""], ["Ailon", "Nir", ""]]}, {"id": "1610.00246", "submitter": "Abbas Hosseini", "authors": "Seyed Abbas Hosseini, Ali Khodadadi, Soheil Arabzade and Hamid R.\n  Rabiee", "title": "HNP3: A Hierarchical Nonparametric Point Process for Modeling Content\n  Diffusion over Social Media", "comments": "Accepted in IEEE International Conference on Data Mining (ICDM) 2016,\n  Barcelona", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces a novel framework for modeling temporal events with\ncomplex longitudinal dependency that are generated by dependent sources. This\nframework takes advantage of multidimensional point processes for modeling time\nof events. The intensity function of the proposed process is a mixture of\nintensities, and its complexity grows with the complexity of temporal patterns\nof data. Moreover, it utilizes a hierarchical dependent nonparametric approach\nto model marks of events. These capabilities allow the proposed model to adapt\nits temporal and topical complexity according to the complexity of data, which\nmakes it a suitable candidate for real world scenarios. An online inference\nalgorithm is also proposed that makes the framework applicable to a vast range\nof applications. The framework is applied to a real world application, modeling\nthe diffusion of contents over networks. Extensive experiments reveal the\neffectiveness of the proposed framework in comparison with state-of-the-art\nmethods.\n", "versions": [{"version": "v1", "created": "Sun, 2 Oct 2016 09:03:11 GMT"}], "update_date": "2016-10-04", "authors_parsed": [["Hosseini", "Seyed Abbas", ""], ["Khodadadi", "Ali", ""], ["Arabzade", "Soheil", ""], ["Rabiee", "Hamid R.", ""]]}, {"id": "1610.00270", "submitter": "Atilla Ozgur", "authors": "Atilla Ozgur, Hamit Erdem, Fatih Nar", "title": "Sparsity-driven weighted ensemble classifier", "comments": "Last version updated according to journal version but not edited by\n  journal", "journal-ref": "International Journal of Computational Intelligence Systems, 2018", "doi": "10.2991/ijcis.11.1.73", "report-no": null, "categories": "stat.ML cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this study, a novel sparsity-driven weighted ensemble classifier (SDWEC)\nthat improves classification accuracy and minimizes the number of classifiers\nis proposed. Using pre-trained classifiers, an ensemble in which base\nclassifiers votes according to assigned weights is formed. These assigned\nweights directly affect classifier accuracy. In the proposed method, ensemble\nweights finding problem is modeled as a cost function with the following terms:\n(a) a data fidelity term aiming to decrease misclassification rate, (b) a\nsparsity term aiming to decrease the number of classifiers, and (c) a\nnon-negativity constraint on the weights of the classifiers. As the proposed\ncost function is non-convex thus hard to solve, convex relaxation techniques\nand novel approximations are employed to obtain a numerically efficient\nsolution. Sparsity term of cost function allows trade-off between accuracy and\ntesting time when needed. The efficiency of SDWEC was tested on 11 datasets and\ncompared with the state-of-the art classifier ensemble methods. The results\nshow that SDWEC provides better or similar accuracy levels using fewer\nclassifiers and reduces testing time for ensemble.\n", "versions": [{"version": "v1", "created": "Sun, 2 Oct 2016 12:33:48 GMT"}, {"version": "v2", "created": "Thu, 17 Nov 2016 11:16:46 GMT"}, {"version": "v3", "created": "Fri, 21 Jun 2019 19:25:38 GMT"}], "update_date": "2019-06-25", "authors_parsed": [["Ozgur", "Atilla", ""], ["Erdem", "Hamit", ""], ["Nar", "Fatih", ""]]}, {"id": "1610.00324", "submitter": "Ganesh Venkatesh", "authors": "Ganesh Venkatesh, Eriko Nurvitadhi, Debbie Marr", "title": "Accelerating Deep Convolutional Networks using low-precision and\n  sparsity", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We explore techniques to significantly improve the compute efficiency and\nperformance of Deep Convolution Networks without impacting their accuracy. To\nimprove the compute efficiency, we focus on achieving high accuracy with\nextremely low-precision (2-bit) weight networks, and to accelerate the\nexecution time, we aggressively skip operations on zero-values. We achieve the\nhighest reported accuracy of 76.6% Top-1/93% Top-5 on the Imagenet object\nclassification challenge with low-precision network\\footnote{github release of\nthe source code coming soon} while reducing the compute requirement by ~3x\ncompared to a full-precision network that achieves similar accuracy.\nFurthermore, to fully exploit the benefits of our low-precision networks, we\nbuild a deep learning accelerator core, dLAC, that can achieve up to 1\nTFLOP/mm^2 equivalent for single-precision floating-point operations (~2\nTFLOP/mm^2 for half-precision).\n", "versions": [{"version": "v1", "created": "Sun, 2 Oct 2016 17:59:31 GMT"}], "update_date": "2016-10-04", "authors_parsed": [["Venkatesh", "Ganesh", ""], ["Nurvitadhi", "Eriko", ""], ["Marr", "Debbie", ""]]}, {"id": "1610.00366", "submitter": "Ruben Martinez-Cantin", "authors": "Ruben Martinez-Cantin", "title": "Funneled Bayesian Optimization for Design, Tuning and Control of\n  Autonomous Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bayesian optimization has become a fundamental global optimization algorithm\nin many problems where sample efficiency is of paramount importance. Recently,\nthere has been proposed a large number of new applications in fields such as\nrobotics, machine learning, experimental design, simulation, etc. In this\npaper, we focus on several problems that appear in robotics and autonomous\nsystems: algorithm tuning, automatic control and intelligent design. All those\nproblems can be mapped to global optimization problems. However, they become\nhard optimization problems. Bayesian optimization internally uses a\nprobabilistic surrogate model (e.g.: Gaussian process) to learn from the\nprocess and reduce the number of samples required. In order to generalize to\nunknown functions in a black-box fashion, the common assumption is that the\nunderlying function can be modeled with a stationary process. Nonstationary\nGaussian process regression cannot generalize easily and it typically requires\nprior knowledge of the function. Some works have designed techniques to\ngeneralize Bayesian optimization to nonstationary functions in an indirect way,\nbut using techniques originally designed for regression, where the objective is\nto improve the quality of the surrogate model everywhere. Instead optimization\nshould focus on improving the surrogate model near the optimum. In this paper,\nwe present a novel kernel function specially designed for Bayesian\noptimization, that allows nonstationary behavior of the surrogate model in an\nadaptive local region. In our experiments, we found that this new kernel\nresults in an improved local search (exploitation), without penalizing the\nglobal search (exploration). We provide results in well-known benchmarks and\nreal applications. The new method outperforms the state of the art in Bayesian\noptimization both in stationary and nonstationary problems.\n", "versions": [{"version": "v1", "created": "Sun, 2 Oct 2016 23:13:45 GMT"}, {"version": "v2", "created": "Tue, 5 Feb 2019 15:48:07 GMT"}], "update_date": "2019-02-06", "authors_parsed": [["Martinez-Cantin", "Ruben", ""]]}, {"id": "1610.00369", "submitter": "Asif Hassan", "authors": "A. Hassan, M. R. Amin, N. Mohammed, A. K. A. Azad", "title": "Sentiment Analysis on Bangla and Romanized Bangla Text (BRBT) using Deep\n  Recurrent models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sentiment Analysis (SA) is an action research area in the digital age. With\nrapid and constant growth of online social media sites and services, and the\nincreasing amount of textual data such as - statuses, comments, reviews etc.\navailable in them, application of automatic SA is on the rise. However, most of\nthe research works on SA in natural language processing (NLP) are based on\nEnglish language. Despite being the sixth most widely spoken language in the\nworld, Bangla still does not have a large and standard dataset. Because of\nthis, recent research works in Bangla have failed to produce results that can\nbe both comparable to works done by others and reusable as stepping stones for\nfuture researchers to progress in this field. Therefore, we first tried to\nprovide a textual dataset - that includes not just Bangla, but Romanized Bangla\ntexts as well, is substantial, post-processed and multiple validated, ready to\nbe used in SA experiments. We tested this dataset in Deep Recurrent model,\nspecifically, Long Short Term Memory (LSTM), using two types of loss functions\n- binary crossentropy and categorical crossentropy, and also did some\nexperimental pre-training by using data from one validation to pre-train the\nother and vice versa. Lastly, we documented the results along with some\nanalysis on them, which were promising.\n", "versions": [{"version": "v1", "created": "Sun, 2 Oct 2016 23:45:23 GMT"}, {"version": "v2", "created": "Thu, 24 Nov 2016 02:13:05 GMT"}], "update_date": "2016-11-28", "authors_parsed": [["Hassan", "A.", ""], ["Amin", "M. R.", ""], ["Mohammed", "N.", ""], ["Azad", "A. K. A.", ""]]}, {"id": "1610.00388", "submitter": "Jiatao Gu", "authors": "Jiatao Gu, Graham Neubig, Kyunghyun Cho and Victor O.K. Li", "title": "Learning to Translate in Real-time with Neural Machine Translation", "comments": "10 pages, camera ready", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Translating in real-time, a.k.a. simultaneous translation, outputs\ntranslation words before the input sentence ends, which is a challenging\nproblem for conventional machine translation methods. We propose a neural\nmachine translation (NMT) framework for simultaneous translation in which an\nagent learns to make decisions on when to translate from the interaction with a\npre-trained NMT environment. To trade off quality and delay, we extensively\nexplore various targets for delay and design a method for beam-search\napplicable in the simultaneous MT setting. Experiments against state-of-the-art\nbaselines on two language pairs demonstrate the efficacy of the proposed\nframework both quantitatively and qualitatively.\n", "versions": [{"version": "v1", "created": "Mon, 3 Oct 2016 02:11:03 GMT"}, {"version": "v2", "created": "Thu, 6 Oct 2016 00:46:39 GMT"}, {"version": "v3", "created": "Tue, 10 Jan 2017 21:07:56 GMT"}], "update_date": "2017-01-12", "authors_parsed": [["Gu", "Jiatao", ""], ["Neubig", "Graham", ""], ["Cho", "Kyunghyun", ""], ["Li", "Victor O. K.", ""]]}, {"id": "1610.00465", "submitter": "Bhanu Pratap Singh Rawat", "authors": "Harsh Nisar, Bhanu Pratap Singh Rawat", "title": "Can Evolutionary Sampling Improve Bagged Ensembles?", "comments": "3 pages, 1 table, Data Efficient Machine Learning Workshop (DEML'16),\n  ICML", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Perturb and Combine (P&C) group of methods generate multiple versions of the\npredictor by perturbing the training set or construction and then combining\nthem into a single predictor (Breiman, 1996b). The motive is to improve the\naccuracy in unstable classification and regression methods. One of the most\nwell known method in this group is Bagging. Arcing or Adaptive Resampling and\nCombining methods like AdaBoost are smarter variants of P&C methods. In this\nextended abstract, we lay the groundwork for a new family of methods under the\nP&C umbrella, known as Evolutionary Sampling (ES). We employ Evolutionary\nalgorithms to suggest smarter sampling in both the feature space (sub-spaces)\nas well as training samples. We discuss multiple fitness functions to assess\nensembles and empirically compare our performance against randomized sampling\nof training data and feature sub-spaces.\n", "versions": [{"version": "v1", "created": "Mon, 3 Oct 2016 09:53:06 GMT"}], "update_date": "2016-10-05", "authors_parsed": [["Nisar", "Harsh", ""], ["Rawat", "Bhanu Pratap Singh", ""]]}, {"id": "1610.00494", "submitter": "Ivan Y. Tyukin", "authors": "Alexander N. Gorban, Ilya Romanenko, Richard Burton, Ivan Y. Tyukin", "title": "One-Trial Correction of Legacy AI Systems and Stochastic Separation\n  Theorems", "comments": null, "journal-ref": "Information Sciences, 484, 237-254, 2019", "doi": "10.1016/j.ins.2019.02.001", "report-no": null, "categories": "stat.ML cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We consider the problem of efficient \"on the fly\" tuning of existing, or {\\it\nlegacy}, Artificial Intelligence (AI) systems. The legacy AI systems are\nallowed to be of arbitrary class, albeit the data they are using for computing\ninterim or final decision responses should posses an underlying structure of a\nhigh-dimensional topological real vector space. The tuning method that we\npropose enables dealing with errors without the need to re-train the system.\nInstead of re-training a simple cascade of perceptron nodes is added to the\nlegacy system. The added cascade modulates the AI legacy system's decisions. If\napplied repeatedly, the process results in a network of modulating rules\n\"dressing up\" and improving performance of existing AI systems. Mathematical\nrationale behind the method is based on the fundamental property of measure\nconcentration in high dimensional spaces. The method is illustrated with an\nexample of fine-tuning a deep convolutional network that has been pre-trained\nto detect pedestrians in images.\n", "versions": [{"version": "v1", "created": "Mon, 3 Oct 2016 11:15:12 GMT"}, {"version": "v2", "created": "Thu, 24 Nov 2016 18:52:25 GMT"}, {"version": "v3", "created": "Sun, 6 Aug 2017 15:24:27 GMT"}, {"version": "v4", "created": "Wed, 13 Feb 2019 09:14:55 GMT"}], "update_date": "2019-02-14", "authors_parsed": [["Gorban", "Alexander N.", ""], ["Romanenko", "Ilya", ""], ["Burton", "Richard", ""], ["Tyukin", "Ivan Y.", ""]]}, {"id": "1610.00520", "submitter": "Akash Kumar Dhaka", "authors": "Akash Kumar Dhaka and Giampiero Salvi", "title": "Semi-supervised Learning with Sparse Autoencoders in Phone\n  Classification", "comments": "5 pages, 1 figure, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose the application of a semi-supervised learning method to improve\nthe performance of acoustic modelling for automatic speech recognition based on\ndeep neural net- works. As opposed to unsupervised initialisation followed by\nsupervised fine tuning, our method takes advantage of both unlabelled and\nlabelled data simultaneously through mini- batch stochastic gradient descent.\nWe tested the method with varying proportions of labelled vs unlabelled\nobservations in frame-based phoneme classification on the TIMIT database. Our\nexperiments show that the method outperforms standard supervised training for\nan equal amount of labelled data and provides competitive error rates compared\nto state-of-the-art graph-based semi-supervised learning techniques.\n", "versions": [{"version": "v1", "created": "Mon, 3 Oct 2016 12:52:26 GMT"}], "update_date": "2016-10-04", "authors_parsed": [["Dhaka", "Akash Kumar", ""], ["Salvi", "Giampiero", ""]]}, {"id": "1610.00527", "submitter": "Nal Kalchbrenner", "authors": "Nal Kalchbrenner, Aaron van den Oord, Karen Simonyan, Ivo Danihelka,\n  Oriol Vinyals, Alex Graves, Koray Kavukcuoglu", "title": "Video Pixel Networks", "comments": "16 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a probabilistic video model, the Video Pixel Network (VPN), that\nestimates the discrete joint distribution of the raw pixel values in a video.\nThe model and the neural architecture reflect the time, space and color\nstructure of video tensors and encode it as a four-dimensional dependency\nchain. The VPN approaches the best possible performance on the Moving MNIST\nbenchmark, a leap over the previous state of the art, and the generated videos\nshow only minor deviations from the ground truth. The VPN also produces\ndetailed samples on the action-conditional Robotic Pushing benchmark and\ngeneralizes to the motion of novel objects.\n", "versions": [{"version": "v1", "created": "Mon, 3 Oct 2016 13:06:40 GMT"}], "update_date": "2016-10-05", "authors_parsed": [["Kalchbrenner", "Nal", ""], ["Oord", "Aaron van den", ""], ["Simonyan", "Karen", ""], ["Danihelka", "Ivo", ""], ["Vinyals", "Oriol", ""], ["Graves", "Alex", ""], ["Kavukcuoglu", "Koray", ""]]}, {"id": "1610.00529", "submitter": "Yevgen Chebotar", "authors": "Yevgen Chebotar, Mrinal Kalakrishnan, Ali Yahya, Adrian Li, Stefan\n  Schaal, Sergey Levine", "title": "Path Integral Guided Policy Search", "comments": "Published at the International Conference on Robotics and Automation\n  (ICRA), 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a policy search method for learning complex feedback control\npolicies that map from high-dimensional sensory inputs to motor torques, for\nmanipulation tasks with discontinuous contact dynamics. We build on a prior\ntechnique called guided policy search (GPS), which iteratively optimizes a set\nof local policies for specific instances of a task, and uses these to train a\ncomplex, high-dimensional global policy that generalizes across task instances.\nWe extend GPS in the following ways: (1) we propose the use of a model-free\nlocal optimizer based on path integral stochastic optimal control (PI2), which\nenables us to learn local policies for tasks with highly discontinuous contact\ndynamics; and (2) we enable GPS to train on a new set of task instances in\nevery iteration by using on-policy sampling: this increases the diversity of\nthe instances that the policy is trained on, and is crucial for achieving good\ngeneralization. We show that these contributions enable us to learn deep neural\nnetwork policies that can directly perform torque control from visual input. We\nvalidate the method on a challenging door opening task and a pick-and-place\ntask, and we demonstrate that our approach substantially outperforms the prior\nLQR-based local policy optimizer on these tasks. Furthermore, we show that\non-policy sampling significantly increases the generalization ability of these\npolicies.\n", "versions": [{"version": "v1", "created": "Mon, 3 Oct 2016 13:12:09 GMT"}, {"version": "v2", "created": "Thu, 11 Oct 2018 23:32:51 GMT"}], "update_date": "2018-10-15", "authors_parsed": [["Chebotar", "Yevgen", ""], ["Kalakrishnan", "Mrinal", ""], ["Yahya", "Ali", ""], ["Li", "Adrian", ""], ["Schaal", "Stefan", ""], ["Levine", "Sergey", ""]]}, {"id": "1610.00552", "submitter": "Minjae Lee", "authors": "Minjae Lee, Kyuyeon Hwang, Jinhwan Park, Sungwook Choi, Sungho Shin,\n  Wonyong Sung", "title": "FPGA-Based Low-Power Speech Recognition with Recurrent Neural Networks", "comments": "Accepted to SiPS 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG cs.SD", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, a neural network based real-time speech recognition (SR)\nsystem is developed using an FPGA for very low-power operation. The implemented\nsystem employs two recurrent neural networks (RNNs); one is a\nspeech-to-character RNN for acoustic modeling (AM) and the other is for\ncharacter-level language modeling (LM). The system also employs a statistical\nword-level LM to improve the recognition accuracy. The results of the AM, the\ncharacter-level LM, and the word-level LM are combined using a fairly simple\nN-best search algorithm instead of the hidden Markov model (HMM) based network.\nThe RNNs are implemented using massively parallel processing elements (PEs) for\nlow latency and high throughput. The weights are quantized to 6 bits to store\nall of them in the on-chip memory of an FPGA. The proposed algorithm is\nimplemented on a Xilinx XC7Z045, and the system can operate much faster than\nreal-time.\n", "versions": [{"version": "v1", "created": "Fri, 30 Sep 2016 10:44:32 GMT"}], "update_date": "2016-10-04", "authors_parsed": [["Lee", "Minjae", ""], ["Hwang", "Kyuyeon", ""], ["Park", "Jinhwan", ""], ["Choi", "Sungwook", ""], ["Shin", "Sungho", ""], ["Sung", "Wonyong", ""]]}, {"id": "1610.00564", "submitter": "Timothy O'Shea", "authors": "Timothy J. O'Shea, Seth Hitefield, Johnathan Corgan", "title": "End-to-End Radio Traffic Sequence Recognition with Deep Recurrent Neural\n  Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate sequence machine learning techniques on raw radio signal\ntime-series data. By applying deep recurrent neural networks we learn to\ndiscriminate between several application layer traffic types on top of a\nconstant envelope modulation without using an expert demodulation algorithm. We\nshow that complex protocol sequences can be learned and used for both\nclassification and generation tasks using this approach.\n", "versions": [{"version": "v1", "created": "Mon, 3 Oct 2016 14:22:19 GMT"}], "update_date": "2016-10-04", "authors_parsed": [["O'Shea", "Timothy J.", ""], ["Hitefield", "Seth", ""], ["Corgan", "Johnathan", ""]]}, {"id": "1610.00574", "submitter": "Sepehr Eghbali", "authors": "Sepehr Eghbali and Ladan Tahvildari", "title": "Fast Cosine Similarity Search in Binary Space with Angular Multi-index\n  Hashing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.DS cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given a large dataset of binary codes and a binary query point, we address\nhow to efficiently find $K$ codes in the dataset that yield the largest cosine\nsimilarities to the query. The straightforward answer to this problem is to\ncompare the query with all items in the dataset, but this is practical only for\nsmall datasets. One potential solution to enhance the search time and achieve\nsublinear cost is to use a hash table populated with binary codes of the\ndataset and then look up the nearby buckets to the query to retrieve the\nnearest neighbors. However, if codes are compared in terms of cosine similarity\nrather than the Hamming distance, then the main issue is that the order of\nbuckets to probe is not evident. To examine this issue, we first elaborate on\nthe connection between the Hamming distance and the cosine similarity. Doing\nthis allows us to systematically find the probing sequence in the hash table.\nHowever, solving the nearest neighbor search with a single table is only\npractical for short binary codes. To address this issue, we propose the angular\nmulti-index hashing search algorithm which relies on building multiple hash\ntables on binary code substrings. The proposed search algorithm solves the\nexact angular $K$ nearest neighbor problem in a time that is often orders of\nmagnitude faster than the linear scan baseline and even approximation methods.\n", "versions": [{"version": "v1", "created": "Wed, 14 Sep 2016 23:16:37 GMT"}, {"version": "v2", "created": "Wed, 18 Apr 2018 12:55:36 GMT"}], "update_date": "2018-04-19", "authors_parsed": [["Eghbali", "Sepehr", ""], ["Tahvildari", "Ladan", ""]]}, {"id": "1610.00579", "submitter": "Zhengyi Zhou", "authors": "Zhengyi Zhou (AT&T Labs Research), Philipp Meerkamp (Bloomberg LP),\n  Chris Volinsky (AT&T Labs Research)", "title": "Quantifying Urban Traffic Anomalies", "comments": "Presented at the Data For Good Exchange 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Detecting and quantifying anomalies in urban traffic is critical for\nreal-time alerting or re-routing in the short run and urban planning in the\nlong run. We describe a two-step framework that achieves these two goals in a\nrobust, fast, online, and unsupervised manner. First, we adapt stable principal\ncomponent pursuit to detect anomalies for each road segment. This allows us to\npinpoint traffic anomalies early and precisely in space. Then we group the\nroad-level anomalies across time and space into meaningful anomaly events using\na simple graph expansion procedure. These events can be easily clustered,\nvisualized, and analyzed by urban planners. We demonstrate the effectiveness of\nour system using 7 weeks of anonymized and aggregated cellular location data in\nDallas-Fort Worth. We suggest potential opportunities for urban planners and\npolicy makers to use our methodology to make informed changes. These\napplications include real-time re-routing of traffic in response to abnormally\nhigh traffic, or identifying candidates for high-impact infrastructure\nprojects.\n", "versions": [{"version": "v1", "created": "Fri, 30 Sep 2016 01:58:12 GMT"}], "update_date": "2016-10-04", "authors_parsed": [["Zhou", "Zhengyi", "", "AT&T Labs Research"], ["Meerkamp", "Philipp", "", "Bloomberg LP"], ["Volinsky", "Chris", "", "AT&T Labs Research"]]}, {"id": "1610.00580", "submitter": "Jacob Abernethy", "authors": "Jacob Abernethy (University of Michigan), Cyrus Anderson (University\n  of Michigan), Chengyu Dai (University of Michigan), Arya Farahi (University\n  of Michigan), Linh Nguyen (University of Michigan), Adam Rauh (University of\n  Michigan), Eric Schwartz (University of Michigan), Wenbo Shen (University of\n  Michigan), Guangsha Shi (University of Michigan), Jonathan Stroud (University\n  of Michigan), Xinyu Tan (University of Michigan), Jared Webb (University of\n  Michigan), Sheng Yang (University of Michigan)", "title": "Flint Water Crisis: Data-Driven Risk Assessment Via Residential Water\n  Testing", "comments": "Presented at the Data For Good Exchange 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recovery from the Flint Water Crisis has been hindered by uncertainty in both\nthe water testing process and the causes of contamination. In this work, we\ndevelop an ensemble of predictive models to assess the risk of lead\ncontamination in individual homes and neighborhoods. To train these models, we\nutilize a wide range of data sources, including voluntary residential water\ntests, historical records, and city infrastructure data. Additionally, we use\nour models to identify the most prominent factors that contribute to a high\nrisk of lead contamination. In this analysis, we find that lead service lines\nare not the only factor that is predictive of the risk of lead contamination of\nwater. These results could be used to guide the long-term recovery efforts in\nFlint, minimize the immediate damages, and improve resource-allocation\ndecisions for similar water infrastructure crises.\n", "versions": [{"version": "v1", "created": "Fri, 30 Sep 2016 14:31:11 GMT"}], "update_date": "2016-10-04", "authors_parsed": [["Abernethy", "Jacob", "", "University of Michigan"], ["Anderson", "Cyrus", "", "University\n  of Michigan"], ["Dai", "Chengyu", "", "University of Michigan"], ["Farahi", "Arya", "", "University\n  of Michigan"], ["Nguyen", "Linh", "", "University of Michigan"], ["Rauh", "Adam", "", "University of\n  Michigan"], ["Schwartz", "Eric", "", "University of Michigan"], ["Shen", "Wenbo", "", "University of\n  Michigan"], ["Shi", "Guangsha", "", "University of Michigan"], ["Stroud", "Jonathan", "", "University\n  of Michigan"], ["Tan", "Xinyu", "", "University of Michigan"], ["Webb", "Jared", "", "University of\n  Michigan"], ["Yang", "Sheng", "", "University of Michigan"]]}, {"id": "1610.00633", "submitter": "Shixiang Gu", "authors": "Shixiang Gu and Ethan Holly and Timothy Lillicrap and Sergey Levine", "title": "Deep Reinforcement Learning for Robotic Manipulation with Asynchronous\n  Off-Policy Updates", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Reinforcement learning holds the promise of enabling autonomous robots to\nlearn large repertoires of behavioral skills with minimal human intervention.\nHowever, robotic applications of reinforcement learning often compromise the\nautonomy of the learning process in favor of achieving training times that are\npractical for real physical systems. This typically involves introducing\nhand-engineered policy representations and human-supplied demonstrations. Deep\nreinforcement learning alleviates this limitation by training general-purpose\nneural network policies, but applications of direct deep reinforcement learning\nalgorithms have so far been restricted to simulated settings and relatively\nsimple tasks, due to their apparent high sample complexity. In this paper, we\ndemonstrate that a recent deep reinforcement learning algorithm based on\noff-policy training of deep Q-functions can scale to complex 3D manipulation\ntasks and can learn deep neural network policies efficiently enough to train on\nreal physical robots. We demonstrate that the training times can be further\nreduced by parallelizing the algorithm across multiple robots which pool their\npolicy updates asynchronously. Our experimental evaluation shows that our\nmethod can learn a variety of 3D manipulation skills in simulation and a\ncomplex door opening skill on real robots without any prior demonstrations or\nmanually designed representations.\n", "versions": [{"version": "v1", "created": "Mon, 3 Oct 2016 16:52:10 GMT"}, {"version": "v2", "created": "Wed, 23 Nov 2016 10:23:25 GMT"}], "update_date": "2016-11-24", "authors_parsed": [["Gu", "Shixiang", ""], ["Holly", "Ethan", ""], ["Lillicrap", "Timothy", ""], ["Levine", "Sergey", ""]]}, {"id": "1610.00660", "submitter": "Samik Banerjee", "authors": "Samik Banerjee, Sukhendu Das", "title": "Kernel Selection using Multiple Kernel Learning and Domain Adaptation in\n  Reproducing Kernel Hilbert Space, for Face Recognition under Surveillance\n  Scenario", "comments": "13 pages, 15 figures, 4 tables. Kernel Selection, Surveillance,\n  Multiple Kernel Learning, Domain Adaptation, RKHS, Hallucination", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Face Recognition (FR) has been the interest to several researchers over the\npast few decades due to its passive nature of biometric authentication. Despite\nhigh accuracy achieved by face recognition algorithms under controlled\nconditions, achieving the same performance for face images obtained in\nsurveillance scenarios, is a major hurdle. Some attempts have been made to\nsuper-resolve the low-resolution face images and improve the contrast, without\nconsiderable degree of success. The proposed technique in this paper tries to\ncope with the very low resolution and low contrast face images obtained from\nsurveillance cameras, for FR under surveillance conditions. For Support Vector\nMachine classification, the selection of appropriate kernel has been a widely\ndiscussed issue in the research community. In this paper, we propose a novel\nkernel selection technique termed as MFKL (Multi-Feature Kernel Learning) to\nobtain the best feature-kernel pairing. Our proposed technique employs a\neffective kernel selection by Multiple Kernel Learning (MKL) method, to choose\nthe optimal kernel to be used along with unsupervised domain adaptation method\nin the Reproducing Kernel Hilbert Space (RKHS), for a solution to the problem.\nRigorous experimentation has been performed on three real-world surveillance\nface datasets : FR\\_SURV, SCface and ChokePoint. Results have been shown using\nRank-1 Recognition Accuracy, ROC and CMC measures. Our proposed method\noutperforms all other recent state-of-the-art techniques by a considerable\nmargin.\n", "versions": [{"version": "v1", "created": "Mon, 3 Oct 2016 18:22:03 GMT"}], "update_date": "2016-10-05", "authors_parsed": [["Banerjee", "Samik", ""], ["Das", "Sukhendu", ""]]}, {"id": "1610.00673", "submitter": "Ali Yahya", "authors": "Ali Yahya, Adrian Li, Mrinal Kalakrishnan, Yevgen Chebotar, Sergey\n  Levine", "title": "Collective Robot Reinforcement Learning with Distributed Asynchronous\n  Guided Policy Search", "comments": "Submitted to the IEEE International Conference on Robotics and\n  Automation 2017", "journal-ref": null, "doi": "10.1109/IROS.2017.8202141", "report-no": null, "categories": "cs.LG cs.AI cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In principle, reinforcement learning and policy search methods can enable\nrobots to learn highly complex and general skills that may allow them to\nfunction amid the complexity and diversity of the real world. However, training\na policy that generalizes well across a wide range of real-world conditions\nrequires far greater quantity and diversity of experience than is practical to\ncollect with a single robot. Fortunately, it is possible for multiple robots to\nshare their experience with one another, and thereby, learn a policy\ncollectively. In this work, we explore distributed and asynchronous policy\nlearning as a means to achieve generalization and improved training times on\nchallenging, real-world manipulation tasks. We propose a distributed and\nasynchronous version of Guided Policy Search and use it to demonstrate\ncollective policy learning on a vision-based door opening task using four\nrobots. We show that it achieves better generalization, utilization, and\ntraining times than the single robot alternative.\n", "versions": [{"version": "v1", "created": "Mon, 3 Oct 2016 18:54:00 GMT"}], "update_date": "2019-05-29", "authors_parsed": [["Yahya", "Ali", ""], ["Li", "Adrian", ""], ["Kalakrishnan", "Mrinal", ""], ["Chebotar", "Yevgen", ""], ["Levine", "Sergey", ""]]}, {"id": "1610.00681", "submitter": "Muhammed Omer Sayin", "authors": "Muhammed O. Sayin, Suleyman S. Kozat, and Tamer Ba\\c{s}ar", "title": "Team-Optimal Distributed MMSE Estimation in General and Tree Networks", "comments": "Submitted to Digital Signal Processing", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SY cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We construct team-optimal estimation algorithms over distributed networks for\nstate estimation in the finite-horizon mean-square error (MSE) sense. Here, we\nhave a distributed collection of agents with processing and cooperation\ncapabilities. These agents observe noisy samples of a desired state through a\nlinear model and seek to learn this state by interacting with each other.\nAlthough this problem has attracted significant attention and been studied\nextensively in fields including machine learning and signal processing, all the\nwell-known strategies do not achieve team-optimal learning performance in the\nfinite-horizon MSE sense. To this end, we formulate the finite-horizon\ndistributed minimum MSE (MMSE) when there is no restriction on the size of the\ndisclosed information, i.e., oracle performance, over an arbitrary network\ntopology. Subsequently, we show that exchange of local estimates is sufficient\nto achieve the oracle performance only over certain network topologies. By\ninspecting these network structures, we propose recursive algorithms achieving\nthe oracle performance through the disclosure of local estimates. For practical\nimplementations we also provide approaches to reduce the complexity of the\nalgorithms through the time-windowing of the observations. Finally, in the\nnumerical examples, we demonstrate the superior performance of the introduced\nalgorithms in the finite-horizon MSE sense due to optimal estimation.\n", "versions": [{"version": "v1", "created": "Mon, 3 Oct 2016 19:09:01 GMT"}, {"version": "v2", "created": "Sun, 8 Jan 2017 19:28:02 GMT"}], "update_date": "2017-01-10", "authors_parsed": [["Sayin", "Muhammed O.", ""], ["Kozat", "Suleyman S.", ""], ["Ba\u015far", "Tamer", ""]]}, {"id": "1610.00696", "submitter": "Chelsea Finn", "authors": "Chelsea Finn and Sergey Levine", "title": "Deep Visual Foresight for Planning Robot Motion", "comments": "ICRA 2017. Supplementary video:\n  https://sites.google.com/site/robotforesight/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A key challenge in scaling up robot learning to many skills and environments\nis removing the need for human supervision, so that robots can collect their\nown data and improve their own performance without being limited by the cost of\nrequesting human feedback. Model-based reinforcement learning holds the promise\nof enabling an agent to learn to predict the effects of its actions, which\ncould provide flexible predictive models for a wide range of tasks and\nenvironments, without detailed human supervision. We develop a method for\ncombining deep action-conditioned video prediction models with model-predictive\ncontrol that uses entirely unlabeled training data. Our approach does not\nrequire a calibrated camera, an instrumented training set-up, nor precise\nsensing and actuation. Our results show that our method enables a real robot to\nperform nonprehensile manipulation -- pushing objects -- and can handle novel\nobjects not seen during training.\n", "versions": [{"version": "v1", "created": "Mon, 3 Oct 2016 19:54:17 GMT"}, {"version": "v2", "created": "Mon, 13 Mar 2017 00:18:49 GMT"}], "update_date": "2017-03-14", "authors_parsed": [["Finn", "Chelsea", ""], ["Levine", "Sergey", ""]]}, {"id": "1610.00732", "submitter": "Yao Xie", "authors": "Yao Xie and Lee Seversky", "title": "Sequential Low-Rank Change Detection", "comments": "Presented at Allerton Conference, 2016. Partially supported by a AFRI\n  Visiting Faculty Fellowship", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Detecting emergence of a low-rank signal from high-dimensional data is an\nimportant problem arising from many applications such as camera surveillance\nand swarm monitoring using sensors. We consider a procedure based on the\nlargest eigenvalue of the sample covariance matrix over a sliding window to\ndetect the change. To achieve dimensionality reduction, we present a\nsketching-based approach for rank change detection using the low-dimensional\nlinear sketches of the original high-dimensional observations. The premise is\nthat when the sketching matrix is a random Gaussian matrix, and the dimension\nof the sketching vector is sufficiently large, the rank of sample covariance\nmatrix for these sketches equals the rank of the original sample covariance\nmatrix with high probability. Hence, we may be able to detect the low-rank\nchange using sample covariance matrices of the sketches without having to\nrecover the original covariance matrix. We character the performance of the\nlargest eigenvalue statistic in terms of the false-alarm-rate and the expected\ndetection delay, and present an efficient online implementation via subspace\ntracking.\n", "versions": [{"version": "v1", "created": "Mon, 3 Oct 2016 20:29:37 GMT"}, {"version": "v2", "created": "Fri, 7 Oct 2016 16:44:49 GMT"}], "update_date": "2016-10-10", "authors_parsed": [["Xie", "Yao", ""], ["Seversky", "Lee", ""]]}, {"id": "1610.00768", "submitter": "Yash Sharma", "authors": "Nicolas Papernot, Fartash Faghri, Nicholas Carlini, Ian Goodfellow,\n  Reuben Feinman, Alexey Kurakin, Cihang Xie, Yash Sharma, Tom Brown, Aurko\n  Roy, Alexander Matyasko, Vahid Behzadan, Karen Hambardzumyan, Zhishuai Zhang,\n  Yi-Lin Juang, Zhi Li, Ryan Sheatsley, Abhibhav Garg, Jonathan Uesato, Willi\n  Gierke, Yinpeng Dong, David Berthelot, Paul Hendricks, Jonas Rauber, Rujun\n  Long, and Patrick McDaniel", "title": "Technical Report on the CleverHans v2.1.0 Adversarial Examples Library", "comments": "Technical report for https://github.com/tensorflow/cleverhans", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  CleverHans is a software library that provides standardized reference\nimplementations of adversarial example construction techniques and adversarial\ntraining. The library may be used to develop more robust machine learning\nmodels and to provide standardized benchmarks of models' performance in the\nadversarial setting. Benchmarks constructed without a standardized\nimplementation of adversarial example construction are not comparable to each\nother, because a good result may indicate a robust model or it may merely\nindicate a weak implementation of the adversarial example construction\nprocedure.\n  This technical report is structured as follows. Section 1 provides an\noverview of adversarial examples in machine learning and of the CleverHans\nsoftware. Section 2 presents the core functionalities of the library: namely\nthe attacks based on adversarial examples and defenses to improve the\nrobustness of machine learning models to these attacks. Section 3 describes how\nto report benchmark results using the library. Section 4 describes the\nversioning system.\n", "versions": [{"version": "v1", "created": "Mon, 3 Oct 2016 22:04:07 GMT"}, {"version": "v2", "created": "Wed, 5 Oct 2016 13:54:04 GMT"}, {"version": "v3", "created": "Wed, 14 Dec 2016 10:47:15 GMT"}, {"version": "v4", "created": "Thu, 5 Oct 2017 17:27:32 GMT"}, {"version": "v5", "created": "Thu, 17 May 2018 22:51:41 GMT"}, {"version": "v6", "created": "Wed, 27 Jun 2018 21:06:06 GMT"}], "update_date": "2018-06-29", "authors_parsed": [["Papernot", "Nicolas", ""], ["Faghri", "Fartash", ""], ["Carlini", "Nicholas", ""], ["Goodfellow", "Ian", ""], ["Feinman", "Reuben", ""], ["Kurakin", "Alexey", ""], ["Xie", "Cihang", ""], ["Sharma", "Yash", ""], ["Brown", "Tom", ""], ["Roy", "Aurko", ""], ["Matyasko", "Alexander", ""], ["Behzadan", "Vahid", ""], ["Hambardzumyan", "Karen", ""], ["Zhang", "Zhishuai", ""], ["Juang", "Yi-Lin", ""], ["Li", "Zhi", ""], ["Sheatsley", "Ryan", ""], ["Garg", "Abhibhav", ""], ["Uesato", "Jonathan", ""], ["Gierke", "Willi", ""], ["Dong", "Yinpeng", ""], ["Berthelot", "David", ""], ["Hendricks", "Paul", ""], ["Rauber", "Jonas", ""], ["Long", "Rujun", ""], ["McDaniel", "Patrick", ""]]}, {"id": "1610.00843", "submitter": "Avik Ray", "authors": "Avik Ray, Joe Neeman, Sujay Sanghavi, Sanjay Shakkottai", "title": "The Search Problem in Mixture Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the task of learning the parameters of a {\\em single} component\nof a mixture model, for the case when we are given {\\em side information} about\nthat component, we call this the \"search problem\" in mixture models. We would\nlike to solve this with computational and sample complexity lower than solving\nthe overall original problem, where one learns parameters of all components.\n  Our main contributions are the development of a simple but general model for\nthe notion of side information, and a corresponding simple matrix-based\nalgorithm for solving the search problem in this general setting. We then\nspecialize this model and algorithm to four common scenarios: Gaussian mixture\nmodels, LDA topic models, subspace clustering, and mixed linear regression. For\neach one of these we show that if (and only if) the side information is\ninformative, we obtain parameter estimates with greater accuracy, and also\nimproved computation complexity than existing moment based mixture model\nalgorithms (e.g. tensor methods). We also illustrate several natural ways one\ncan obtain such side information, for specific problem instances. Our\nexperiments on real data sets (NY Times, Yelp, BSDS500) further demonstrate the\npracticality of our algorithms showing significant improvement in runtime and\naccuracy.\n", "versions": [{"version": "v1", "created": "Tue, 4 Oct 2016 05:01:18 GMT"}, {"version": "v2", "created": "Sat, 24 Feb 2018 21:49:02 GMT"}], "update_date": "2018-02-27", "authors_parsed": [["Ray", "Avik", ""], ["Neeman", "Joe", ""], ["Sanghavi", "Sujay", ""], ["Shakkottai", "Sanjay", ""]]}, {"id": "1610.00844", "submitter": "Nesreen Ahmed", "authors": "Nesreen K. Ahmed, Ryan A. Rossi, Theodore L. Willke, Rong Zhou", "title": "Revisiting Role Discovery in Networks: From Node to Edge Roles", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Previous work in network analysis has focused on modeling the\nmixed-memberships of node roles in the graph, but not the roles of edges. We\nintroduce the edge role discovery problem and present a generalizable framework\nfor learning and extracting edge roles from arbitrary graphs automatically.\nFurthermore, while existing node-centric role models have mainly focused on\nsimple degree and egonet features, this work also explores graphlet features\nfor role discovery. In addition, we also develop an approach for automatically\nlearning and extracting important and useful edge features from an arbitrary\ngraph. The experimental results demonstrate the utility of edge roles for\nnetwork analysis tasks on a variety of graphs from various problem domains.\n", "versions": [{"version": "v1", "created": "Tue, 4 Oct 2016 05:04:30 GMT"}, {"version": "v2", "created": "Mon, 7 Nov 2016 22:10:31 GMT"}], "update_date": "2016-11-09", "authors_parsed": [["Ahmed", "Nesreen K.", ""], ["Rossi", "Ryan A.", ""], ["Willke", "Theodore L.", ""], ["Zhou", "Rong", ""]]}, {"id": "1610.00850", "submitter": "Michael Laskey", "authors": "Michael Laskey, Caleb Chuck, Jonathan Lee, Jeffrey Mahler, Sanjay\n  Krishnan, Kevin Jamieson, Anca Dragan, Ken Goldberg", "title": "Comparing Human-Centric and Robot-Centric Sampling for Robot Deep\n  Learning from Demonstrations", "comments": "Submitted to International Conference on Robotics and Automation\n  (ICRA) 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motivated by recent advances in Deep Learning for robot control, this paper\nconsiders two learning algorithms in terms of how they acquire demonstrations.\n\"Human-Centric\" (HC) sampling is the standard supervised learning algorithm,\nwhere a human supervisor demonstrates the task by teleoperating the robot to\nprovide trajectories consisting of state-control pairs. \"Robot-Centric\" (RC)\nsampling is an increasingly popular alternative used in algorithms such as\nDAgger, where a human supervisor observes the robot executing a learned policy\nand provides corrective control labels for each state visited. RC sampling can\nbe challenging for human supervisors and prone to mislabeling. RC sampling can\nalso induce error in policy performance because it repeatedly visits areas of\nthe state space that are harder to learn. Although policies learned with RC\nsampling can be superior to HC sampling for standard learning models such as\nlinear SVMs, policies learned with HC sampling may be comparable with\nhighly-expressive learning models such as deep learning and hyper-parametric\ndecision trees, which have little model error. We compare HC and RC using a\ngrid world and a physical robot singulation task, where in the latter the input\nis a binary image of a connected set of objects on a planar worksurface and the\npolicy generates a motion of the gripper to separate one object from the rest.\nWe observe in simulation that for linear SVMs, policies learned with RC\noutperformed those learned with HC but that with deep models this advantage\ndisappears. We also find that with RC, the corrective control labels provided\nby humans can be highly inconsistent. We prove there exists a class of examples\nwhere in the limit, HC is guaranteed to converge to an optimal policy while RC\nmay fail to converge.\n", "versions": [{"version": "v1", "created": "Tue, 4 Oct 2016 05:51:40 GMT"}, {"version": "v2", "created": "Fri, 7 Oct 2016 16:54:51 GMT"}, {"version": "v3", "created": "Wed, 29 Mar 2017 02:15:33 GMT"}], "update_date": "2017-03-30", "authors_parsed": [["Laskey", "Michael", ""], ["Chuck", "Caleb", ""], ["Lee", "Jonathan", ""], ["Mahler", "Jeffrey", ""], ["Krishnan", "Sanjay", ""], ["Jamieson", "Kevin", ""], ["Dragan", "Anca", ""], ["Goldberg", "Ken", ""]]}, {"id": "1610.00852", "submitter": "Joey Hong", "authors": "Joey Hong, Chris Mattmann, Paul Ramirez", "title": "Ensemble Maximum Entropy Classification and Linear Regression for Author\n  Age Prediction", "comments": "6 pages, 4 figures", "journal-ref": "2017 IEEE International Conference on Information Reuse and\n  Integration (IRI)", "doi": "10.1109/IRI.2017.48", "report-no": null, "categories": "cs.LG cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The evolution of the internet has created an abundance of unstructured data\non the web, a significant part of which is textual. The task of author\nprofiling seeks to find the demographics of people solely from their linguistic\nand content-based features in text. The ability to describe traits of authors\nclearly has applications in fields such as security and forensics, as well as\nmarketing. Instead of seeing age as just a classification problem, we also\nframe age as a regression one, but use an ensemble chain method that\nincorporates the power of both classification and regression to learn the\nauthors exact age.\n", "versions": [{"version": "v1", "created": "Tue, 4 Oct 2016 06:01:03 GMT"}], "update_date": "2019-06-24", "authors_parsed": [["Hong", "Joey", ""], ["Mattmann", "Chris", ""], ["Ramirez", "Paul", ""]]}, {"id": "1610.00946", "submitter": "Jean-Baptiste Mouret", "authors": "Jean-Baptiste Mouret (LORIA, LARSEN)", "title": "Micro-Data Learning: The Other End of the Spectrum", "comments": null, "journal-ref": "ERCIM News, ERCIM, 2016, pp.2", "doi": null, "report-no": null, "categories": "cs.AI cs.LG cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many fields are now snowed under with an avalanche of data, which raises\nconsiderable challenges for computer scientists. Meanwhile, robotics (among\nother fields) can often only use a few dozen data points because acquiring them\ninvolves a process that is expensive or time-consuming. How can an algorithm\nlearn with only a few data points?\n", "versions": [{"version": "v1", "created": "Tue, 4 Oct 2016 12:29:05 GMT"}], "update_date": "2017-09-20", "authors_parsed": [["Mouret", "Jean-Baptiste", "", "LORIA, LARSEN"]]}, {"id": "1610.00956", "submitter": "Ondrej Bajgar", "authors": "Ondrej Bajgar, Rudolf Kadlec and Jan Kleindienst", "title": "Embracing data abundance: BookTest Dataset for Reading Comprehension", "comments": "The first two authors contributed equally to this work. Submitted to\n  EACL 2017. Code and dataset are publicly available", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There is a practically unlimited amount of natural language data available.\nStill, recent work in text comprehension has focused on datasets which are\nsmall relative to current computing possibilities. This article is making a\ncase for the community to move to larger data and as a step in that direction\nit is proposing the BookTest, a new dataset similar to the popular Children's\nBook Test (CBT), however more than 60 times larger. We show that training on\nthe new data improves the accuracy of our Attention-Sum Reader model on the\noriginal CBT test data by a much larger margin than many recent attempts to\nimprove the model architecture. On one version of the dataset our ensemble even\nexceeds the human baseline provided by Facebook. We then show in our own human\nstudy that there is still space for further improvement.\n", "versions": [{"version": "v1", "created": "Tue, 4 Oct 2016 12:48:51 GMT"}], "update_date": "2016-10-05", "authors_parsed": [["Bajgar", "Ondrej", ""], ["Kadlec", "Rudolf", ""], ["Kleindienst", "Jan", ""]]}, {"id": "1610.00970", "submitter": "Alberto Bietti", "authors": "Alberto Bietti, Julien Mairal", "title": "Stochastic Optimization with Variance Reduction for Infinite Datasets\n  with Finite-Sum Structure", "comments": "Advances in Neural Information Processing Systems (NIPS), Dec 2017,\n  Long Beach, CA, United States", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Stochastic optimization algorithms with variance reduction have proven\nsuccessful for minimizing large finite sums of functions. Unfortunately, these\ntechniques are unable to deal with stochastic perturbations of input data,\ninduced for example by data augmentation. In such cases, the objective is no\nlonger a finite sum, and the main candidate for optimization is the stochastic\ngradient descent method (SGD). In this paper, we introduce a variance reduction\napproach for these settings when the objective is composite and strongly\nconvex. The convergence rate outperforms SGD with a typically much smaller\nconstant factor, which depends on the variance of gradient estimates only due\nto perturbations on a single example.\n", "versions": [{"version": "v1", "created": "Tue, 4 Oct 2016 13:08:42 GMT"}, {"version": "v2", "created": "Tue, 17 Jan 2017 12:22:56 GMT"}, {"version": "v3", "created": "Tue, 24 Jan 2017 10:21:02 GMT"}, {"version": "v4", "created": "Mon, 27 Feb 2017 13:45:38 GMT"}, {"version": "v5", "created": "Thu, 1 Jun 2017 10:30:01 GMT"}, {"version": "v6", "created": "Wed, 15 Nov 2017 13:48:57 GMT"}], "update_date": "2017-11-16", "authors_parsed": [["Bietti", "Alberto", ""], ["Mairal", "Julien", ""]]}, {"id": "1610.01030", "submitter": "Muhammad Imran", "authors": "Dat Tien Nguyen, Shafiq Joty, Muhammad Imran, Hassan Sajjad, Prasenjit\n  Mitra", "title": "Applications of Online Deep Learning for Crisis Response Using Social\n  Media Information", "comments": "Accepted at SWDM co-located with CIKM 2016. 6 pages, 2 figures. arXiv\n  admin note: text overlap with arXiv:1608.03902", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.CY cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  During natural or man-made disasters, humanitarian response organizations\nlook for useful information to support their decision-making processes. Social\nmedia platforms such as Twitter have been considered as a vital source of\nuseful information for disaster response and management. Despite advances in\nnatural language processing techniques, processing short and informal Twitter\nmessages is a challenging task. In this paper, we propose to use Deep Neural\nNetwork (DNN) to address two types of information needs of response\norganizations: 1) identifying informative tweets and 2) classifying them into\ntopical classes. DNNs use distributed representation of words and learn the\nrepresentation as well as higher level features automatically for the\nclassification task. We propose a new online algorithm based on stochastic\ngradient descent to train DNNs in an online fashion during disaster situations.\nWe test our models using a crisis-related real-world Twitter dataset.\n", "versions": [{"version": "v1", "created": "Tue, 4 Oct 2016 14:53:51 GMT"}, {"version": "v2", "created": "Wed, 5 Oct 2016 09:50:15 GMT"}], "update_date": "2016-10-06", "authors_parsed": [["Nguyen", "Dat Tien", ""], ["Joty", "Shafiq", ""], ["Imran", "Muhammad", ""], ["Sajjad", "Hassan", ""], ["Mitra", "Prasenjit", ""]]}, {"id": "1610.01076", "submitter": "Mateusz Malinowski", "authors": "Mateusz Malinowski and Mario Fritz", "title": "Tutorial on Answering Questions about Images with Deep Learning", "comments": "The tutorial was presented at '2nd Summer School on Integrating\n  Vision and Language: Deep Learning' in Malta, 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.CL cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Together with the development of more accurate methods in Computer Vision and\nNatural Language Understanding, holistic architectures that answer on questions\nabout the content of real-world images have emerged. In this tutorial, we build\na neural-based approach to answer questions about images. We base our tutorial\non two datasets: (mostly on) DAQUAR, and (a bit on) VQA. With small tweaks the\nmodels that we present here can achieve a competitive performance on both\ndatasets, in fact, they are among the best methods that use a combination of\nLSTM with a global, full frame CNN representation of an image. We hope that\nafter reading this tutorial, the reader will be able to use Deep Learning\nframeworks, such as Keras and introduced Kraino, to build various architectures\nthat will lead to a further performance improvement on this challenging task.\n", "versions": [{"version": "v1", "created": "Tue, 4 Oct 2016 16:29:28 GMT"}], "update_date": "2016-10-05", "authors_parsed": [["Malinowski", "Mateusz", ""], ["Fritz", "Mario", ""]]}, {"id": "1610.01096", "submitter": "Neil Shah", "authors": "Neil Shah", "title": "FLOCK: Combating Astroturfing on Livestreaming Platforms", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Livestreaming platforms have become increasingly popular in recent years as a\nmeans of sharing and advertising creative content. Popular content streamers\nwho attract large viewership to their live broadcasts can earn a living by\nmeans of ad revenue, donations and channel subscriptions. Unfortunately, this\nincentivized popularity has simultaneously resulted in incentive for fraudsters\nto provide services to astroturf, or artificially inflate viewership metrics by\nproviding fake \"live\" views to customers. Our work provides a number of major\ncontributions: (a) formulation: we are the first to introduce and characterize\nthe viewbot fraud problem in livestreaming platforms, (b) methodology: we\npropose FLOCK, a principled and unsupervised method which efficiently and\neffectively identifies botted broadcasts and their constituent botted views,\nand (c) practicality: our approach achieves over 98% precision in identifying\nbotted broadcasts and over 90% precision/recall against sizable synthetically\ngenerated viewbot attacks on a real-world livestreaming workload of over 16\nmillion views and 92 thousand broadcasts. FLOCK successfully operates on larger\ndatasets in practice and is regularly used at a large, undisclosed\nlivestreaming corporation.\n", "versions": [{"version": "v1", "created": "Tue, 4 Oct 2016 17:16:25 GMT"}], "update_date": "2016-10-05", "authors_parsed": [["Shah", "Neil", ""]]}, {"id": "1610.01101", "submitter": "Damek Davis", "authors": "Aleksandr Aravkin and Damek Davis", "title": "A SMART Stochastic Algorithm for Nonconvex Optimization with\n  Applications to Robust Machine Learning", "comments": "33 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we show how to transform any optimization problem that arises\nfrom fitting a machine learning model into one that (1) detects and removes\ncontaminated data from the training set while (2) simultaneously fitting the\ntrimmed model on the uncontaminated data that remains. To solve the resulting\nnonconvex optimization problem, we introduce a fast stochastic\nproximal-gradient algorithm that incorporates prior knowledge through nonsmooth\nregularization. For datasets of size $n$, our approach requires\n$O(n^{2/3}/\\varepsilon)$ gradient evaluations to reach $\\varepsilon$-accuracy\nand, when a certain error bound holds, the complexity improves to $O(\\kappa\nn^{2/3}\\log(1/\\varepsilon))$. These rates are $n^{1/3}$ times better than those\nachieved by typical, full gradient methods.\n", "versions": [{"version": "v1", "created": "Tue, 4 Oct 2016 17:24:43 GMT"}, {"version": "v2", "created": "Sun, 5 Feb 2017 15:24:39 GMT"}], "update_date": "2017-02-07", "authors_parsed": [["Aravkin", "Aleksandr", ""], ["Davis", "Damek", ""]]}, {"id": "1610.01112", "submitter": "William Montgomery Iv", "authors": "William Montgomery, Anurag Ajay, Chelsea Finn, Pieter Abbeel, Sergey\n  Levine", "title": "Reset-Free Guided Policy Search: Efficient Deep Reinforcement Learning\n  with Stochastic Initial States", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Autonomous learning of robotic skills can allow general-purpose robots to\nlearn wide behavioral repertoires without requiring extensive manual\nengineering. However, robotic skill learning methods typically make one of\nseveral trade-offs to enable practical real-world learning, such as requiring\nmanually designed policy or value function representations, initialization from\nhuman-provided demonstrations, instrumentation of the training environment, or\nextremely long training times. In this paper, we propose a new reinforcement\nlearning algorithm for learning manipulation skills that can train\ngeneral-purpose neural network policies with minimal human engineering, while\nstill allowing for fast, efficient learning in stochastic environments. Our\napproach builds on the guided policy search (GPS) algorithm, which transforms\nthe reinforcement learning problem into supervised learning from a\ncomputational teacher (without human demonstrations). In contrast to prior GPS\nmethods, which require a consistent set of initial states to which the system\nmust be reset after each episode, our approach can handle randomized initial\nstates, allowing it to be used in environments where deterministic resets are\nimpossible. We compare our method to existing policy search techniques in\nsimulation, showing that it can train high-dimensional neural network policies\nwith the same sample efficiency as prior GPS methods, and present real-world\nresults on a PR2 robotic manipulator.\n", "versions": [{"version": "v1", "created": "Tue, 4 Oct 2016 18:05:52 GMT"}, {"version": "v2", "created": "Thu, 6 Oct 2016 05:10:11 GMT"}], "update_date": "2016-10-07", "authors_parsed": [["Montgomery", "William", ""], ["Ajay", "Anurag", ""], ["Finn", "Chelsea", ""], ["Abbeel", "Pieter", ""], ["Levine", "Sergey", ""]]}, {"id": "1610.01132", "submitter": "Tengyu Ma", "authors": "Elad Hazan, Tengyu Ma", "title": "A Non-generative Framework and Convex Relaxations for Unsupervised\n  Learning", "comments": "NIPS 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We give a novel formal theoretical framework for unsupervised learning with\ntwo distinctive characteristics. First, it does not assume any generative model\nand based on a worst-case performance metric. Second, it is comparative, namely\nperformance is measured with respect to a given hypothesis class. This allows\nto avoid known computational hardness results and improper algorithms based on\nconvex relaxations. We show how several families of unsupervised learning\nmodels, which were previously only analyzed under probabilistic assumptions and\nare otherwise provably intractable, can be efficiently learned in our framework\nby convex optimization.\n", "versions": [{"version": "v1", "created": "Tue, 4 Oct 2016 19:22:44 GMT"}, {"version": "v2", "created": "Wed, 5 Oct 2016 00:30:02 GMT"}, {"version": "v3", "created": "Tue, 27 Dec 2016 20:59:01 GMT"}], "update_date": "2016-12-28", "authors_parsed": [["Hazan", "Elad", ""], ["Ma", "Tengyu", ""]]}, {"id": "1610.01145", "submitter": "Dmitry Yarotsky", "authors": "Dmitry Yarotsky", "title": "Error bounds for approximations with deep ReLU networks", "comments": "31 pages; major revision in v3; submitted to Neural Networks", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study expressive power of shallow and deep neural networks with piece-wise\nlinear activation functions. We establish new rigorous upper and lower bounds\nfor the network complexity in the setting of approximations in Sobolev spaces.\nIn particular, we prove that deep ReLU networks more efficiently approximate\nsmooth functions than shallow networks. In the case of approximations of 1D\nLipschitz functions we describe adaptive depth-6 network architectures more\nefficient than the standard shallow architecture.\n", "versions": [{"version": "v1", "created": "Mon, 3 Oct 2016 23:08:22 GMT"}, {"version": "v2", "created": "Mon, 7 Nov 2016 16:57:35 GMT"}, {"version": "v3", "created": "Mon, 1 May 2017 14:01:32 GMT"}], "update_date": "2017-05-02", "authors_parsed": [["Yarotsky", "Dmitry", ""]]}, {"id": "1610.01178", "submitter": "Peter Goldsborough", "authors": "Peter Goldsborough", "title": "A Tour of TensorFlow", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning is a branch of artificial intelligence employing deep neural\nnetwork architectures that has significantly advanced the state-of-the-art in\ncomputer vision, speech recognition, natural language processing and other\ndomains. In November 2015, Google released $\\textit{TensorFlow}$, an open\nsource deep learning software library for defining, training and deploying\nmachine learning models. In this paper, we review TensorFlow and put it in\ncontext of modern deep learning concepts and software. We discuss its basic\ncomputational paradigms and distributed execution model, its programming\ninterface as well as accompanying visualization toolkits. We then compare\nTensorFlow to alternative libraries such as Theano, Torch or Caffe on a\nqualitative as well as quantitative basis and finally comment on observed\nuse-cases of TensorFlow in academia and industry.\n", "versions": [{"version": "v1", "created": "Sat, 1 Oct 2016 11:32:03 GMT"}], "update_date": "2016-10-06", "authors_parsed": [["Goldsborough", "Peter", ""]]}, {"id": "1610.01206", "submitter": "Yingming Li", "authors": "Yingming Li, Ming Yang, and Zhongfei Zhang", "title": "A Survey of Multi-View Representation Learning", "comments": "Accepted by IEEE Transactions on Knowledge and Data Engineering", "journal-ref": null, "doi": "10.1109/TKDE.2018.2872063", "report-no": null, "categories": "cs.LG cs.CV cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, multi-view representation learning has become a rapidly growing\ndirection in machine learning and data mining areas. This paper introduces two\ncategories for multi-view representation learning: multi-view representation\nalignment and multi-view representation fusion. Consequently, we first review\nthe representative methods and theories of multi-view representation learning\nbased on the perspective of alignment, such as correlation-based alignment.\nRepresentative examples are canonical correlation analysis (CCA) and its\nseveral extensions. Then from the perspective of representation fusion we\ninvestigate the advancement of multi-view representation learning that ranges\nfrom generative methods including multi-modal topic learning, multi-view sparse\ncoding, and multi-view latent space Markov networks, to neural network-based\nmethods including multi-modal autoencoders, multi-view convolutional neural\nnetworks, and multi-modal recurrent neural networks. Further, we also\ninvestigate several important applications of multi-view representation\nlearning. Overall, this survey aims to provide an insightful overview of\ntheoretical foundation and state-of-the-art developments in the field of\nmulti-view representation learning and to help researchers find the most\nappropriate tools for particular applications.\n", "versions": [{"version": "v1", "created": "Mon, 3 Oct 2016 17:14:15 GMT"}, {"version": "v2", "created": "Sun, 27 Nov 2016 03:11:53 GMT"}, {"version": "v3", "created": "Thu, 24 Aug 2017 08:08:22 GMT"}, {"version": "v4", "created": "Fri, 1 Sep 2017 05:52:06 GMT"}, {"version": "v5", "created": "Wed, 24 Oct 2018 02:34:53 GMT"}], "update_date": "2018-10-25", "authors_parsed": [["Li", "Yingming", ""], ["Yang", "Ming", ""], ["Zhang", "Zhongfei", ""]]}, {"id": "1610.01234", "submitter": "Eric Bax", "authors": "Eric Bax and Farshad Kooti", "title": "Ensemble Validation: Selectivity has a Price, but Variety is Free", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Suppose some classifiers are selected from a set of hypothesis classifiers to\nform an equally-weighted ensemble that selects a member classifier at random\nfor each input example. Then the ensemble has an error bound consisting of the\naverage error bound for the member classifiers, a term for selectivity that\nvaries from zero (if all hypothesis classifiers are selected) to a standard\nuniform error bound (if only a single classifier is selected), and small\nconstants. There is no penalty for using a richer hypothesis set if the same\nfraction of the hypothesis classifiers are selected for the ensemble.\n", "versions": [{"version": "v1", "created": "Tue, 4 Oct 2016 23:44:20 GMT"}, {"version": "v2", "created": "Wed, 25 Apr 2018 19:48:13 GMT"}, {"version": "v3", "created": "Thu, 28 Mar 2019 22:13:52 GMT"}], "update_date": "2019-04-01", "authors_parsed": [["Bax", "Eric", ""], ["Kooti", "Farshad", ""]]}, {"id": "1610.01238", "submitter": "Dan Barnes", "authors": "Dan Barnes, Will Maddern and Ingmar Posner", "title": "Find Your Own Way: Weakly-Supervised Segmentation of Path Proposals for\n  Urban Autonomy", "comments": "International Conference on Robotics and Automation (ICRA), 2017.\n  Video summary: http://youtu.be/rbZ8ck_1nZk", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AI cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a weakly-supervised approach to segmenting proposed drivable paths\nin images with the goal of autonomous driving in complex urban environments.\nUsing recorded routes from a data collection vehicle, our proposed method\ngenerates vast quantities of labelled images containing proposed paths and\nobstacles without requiring manual annotation, which we then use to train a\ndeep semantic segmentation network. With the trained network we can segment\nproposed paths and obstacles at run-time using a vehicle equipped with only a\nmonocular camera without relying on explicit modelling of road or lane\nmarkings. We evaluate our method on the large-scale KITTI and Oxford RobotCar\ndatasets and demonstrate reliable path proposal and obstacle segmentation in a\nwide variety of environments under a range of lighting, weather and traffic\nconditions. We illustrate how the method can generalise to multiple path\nproposals at intersections and outline plans to incorporate the system into a\nframework for autonomous urban driving.\n", "versions": [{"version": "v1", "created": "Wed, 5 Oct 2016 00:44:49 GMT"}, {"version": "v2", "created": "Tue, 14 Mar 2017 22:05:10 GMT"}, {"version": "v3", "created": "Fri, 17 Nov 2017 16:54:44 GMT"}], "update_date": "2017-11-20", "authors_parsed": [["Barnes", "Dan", ""], ["Maddern", "Will", ""], ["Posner", "Ingmar", ""]]}, {"id": "1610.01239", "submitter": "Qinglong Wang", "authors": "Qinglong Wang, Wenbo Guo, Kaixuan Zhang, Alexander G. Ororbia II,\n  Xinyu Xing, C. Lee Giles, Xue Liu", "title": "Adversary Resistant Deep Neural Networks with an Application to Malware\n  Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Beyond its highly publicized victories in Go, there have been numerous\nsuccessful applications of deep learning in information retrieval, computer\nvision and speech recognition. In cybersecurity, an increasing number of\ncompanies have become excited about the potential of deep learning, and have\nstarted to use it for various security incidents, the most popular being\nmalware detection. These companies assert that deep learning (DL) could help\nturn the tide in the battle against malware infections. However, deep neural\nnetworks (DNNs) are vulnerable to adversarial samples, a flaw that plagues most\nif not all statistical learning models. Recent research has demonstrated that\nthose with malicious intent can easily circumvent deep learning-powered malware\ndetection by exploiting this flaw.\n  In order to address this problem, previous work has developed various defense\nmechanisms that either augmenting training data or enhance model's complexity.\nHowever, after a thorough analysis of the fundamental flaw in DNNs, we discover\nthat the effectiveness of current defenses is limited and, more importantly,\ncannot provide theoretical guarantees as to their robustness against\nadversarial sampled-based attacks. As such, we propose a new adversary\nresistant technique that obstructs attackers from constructing impactful\nadversarial samples by randomly nullifying features within samples. In this\nwork, we evaluate our proposed technique against a real world dataset with\n14,679 malware variants and 17,399 benign programs. We theoretically validate\nthe robustness of our technique, and empirically show that our technique\nsignificantly boosts DNN robustness to adversarial samples while maintaining\nhigh accuracy in classification. To demonstrate the general applicability of\nour proposed method, we also conduct experiments using the MNIST and CIFAR-10\ndatasets, generally used in image recognition research.\n", "versions": [{"version": "v1", "created": "Wed, 5 Oct 2016 00:46:03 GMT"}, {"version": "v2", "created": "Thu, 6 Oct 2016 16:20:56 GMT"}, {"version": "v3", "created": "Fri, 7 Oct 2016 17:24:13 GMT"}, {"version": "v4", "created": "Thu, 27 Apr 2017 17:25:30 GMT"}], "update_date": "2017-04-28", "authors_parsed": [["Wang", "Qinglong", ""], ["Guo", "Wenbo", ""], ["Zhang", "Kaixuan", ""], ["Ororbia", "Alexander G.", "II"], ["Xing", "Xinyu", ""], ["Giles", "C. Lee", ""], ["Liu", "Xue", ""]]}, {"id": "1610.01283", "submitter": "Aravind Rajeswaran", "authors": "Aravind Rajeswaran, Sarvjeet Ghotra, Balaraman Ravindran, Sergey\n  Levine", "title": "EPOpt: Learning Robust Neural Network Policies Using Model Ensembles", "comments": "Accepted for publication at the International Conference on Learning\n  Representations (ICLR) 2017. Supplementary video:\n  https://youtu.be/w1YJ9vwaoto", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sample complexity and safety are major challenges when learning policies with\nreinforcement learning for real-world tasks, especially when the policies are\nrepresented using rich function approximators like deep neural networks.\nModel-based methods where the real-world target domain is approximated using a\nsimulated source domain provide an avenue to tackle the above challenges by\naugmenting real data with simulated data. However, discrepancies between the\nsimulated source domain and the target domain pose a challenge for simulated\ntraining. We introduce the EPOpt algorithm, which uses an ensemble of simulated\nsource domains and a form of adversarial training to learn policies that are\nrobust and generalize to a broad range of possible target domains, including\nunmodeled effects. Further, the probability distribution over source domains in\nthe ensemble can be adapted using data from target domain and approximate\nBayesian methods, to progressively make it a better approximation. Thus,\nlearning on a model ensemble, along with source domain adaptation, provides the\nbenefit of both robustness and learning/adaptation.\n", "versions": [{"version": "v1", "created": "Wed, 5 Oct 2016 06:51:58 GMT"}, {"version": "v2", "created": "Mon, 10 Oct 2016 07:18:36 GMT"}, {"version": "v3", "created": "Fri, 16 Dec 2016 16:48:17 GMT"}, {"version": "v4", "created": "Fri, 3 Mar 2017 19:58:56 GMT"}], "update_date": "2017-03-07", "authors_parsed": [["Rajeswaran", "Aravind", ""], ["Ghotra", "Sarvjeet", ""], ["Ravindran", "Balaraman", ""], ["Levine", "Sergey", ""]]}, {"id": "1610.01374", "submitter": "Samik Banerjee", "authors": "Samik Banerjee, Sukhendu Das", "title": "Domain Adaptation with Soft-margin multiple feature-kernel learning\n  beats Deep Learning for surveillance face recognition", "comments": "This is an extended version of the paper accepted in CVPR Biometric\n  Workshop, 2016. arXiv admin note: text overlap with arXiv:1610.00660", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.HC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Face recognition (FR) is the most preferred mode for biometric-based\nsurveillance, due to its passive nature of detecting subjects, amongst all\ndifferent types of biometric traits. FR under surveillance scenario does not\ngive satisfactory performance due to low contrast, noise and poor illumination\nconditions on probes, as compared to the training samples. A state-of-the-art\ntechnology, Deep Learning, even fails to perform well in these scenarios. We\npropose a novel soft-margin based learning method for multiple feature-kernel\ncombinations, followed by feature transformed using Domain Adaptation, which\noutperforms many recent state-of-the-art techniques, when tested using three\nreal-world surveillance face datasets.\n", "versions": [{"version": "v1", "created": "Wed, 5 Oct 2016 11:48:56 GMT"}, {"version": "v2", "created": "Thu, 27 Oct 2016 13:14:49 GMT"}], "update_date": "2016-10-28", "authors_parsed": [["Banerjee", "Samik", ""], ["Das", "Sukhendu", ""]]}, {"id": "1610.01417", "submitter": "Igor Colin", "authors": "Igor Colin, Christophe Dupuy", "title": "Decentralized Topic Modelling with Latent Dirichlet Allocation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Privacy preserving networks can be modelled as decentralized networks (e.g.,\nsensors, connected objects, smartphones), where communication between nodes of\nthe network is not controlled by an all-knowing, central node. For this type of\nnetworks, the main issue is to gather/learn global information on the network\n(e.g., by optimizing a global cost function) while keeping the (sensitive)\ninformation at each node. In this work, we focus on text information that\nagents do not want to share (e.g., text messages, emails, confidential\nreports). We use recent advances on decentralized optimization and topic models\nto infer topics from a graph with limited communication. We propose a method to\nadapt latent Dirichlet allocation (LDA) model to decentralized optimization and\nshow on synthetic data that we still recover similar parameters and similar\nperformance at each node than with stochastic methods accessing to the whole\ninformation in the graph.\n", "versions": [{"version": "v1", "created": "Wed, 5 Oct 2016 13:45:53 GMT"}], "update_date": "2016-10-06", "authors_parsed": [["Colin", "Igor", ""], ["Dupuy", "Christophe", ""]]}, {"id": "1610.01476", "submitter": "Dominik Meyer", "authors": "Dominik Meyer, Hao Shen, Klaus Diepold", "title": "$\\ell_1$ Regularized Gradient Temporal-Difference Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we study the Temporal Difference (TD) learning with linear\nvalue function approximation. It is well known that most TD learning algorithms\nare unstable with linear function approximation and off-policy learning. Recent\ndevelopment of Gradient TD (GTD) algorithms has addressed this problem\nsuccessfully. However, the success of GTD algorithms requires a set of well\nchosen features, which are not always available. When the number of features is\nhuge, the GTD algorithms might face the problem of overfitting and being\ncomputationally expensive. To cope with this difficulty, regularization\ntechniques, in particular $\\ell_1$ regularization, have attracted significant\nattentions in developing TD learning algorithms. The present work combines the\nGTD algorithms with $\\ell_1$ regularization. We propose a family of $\\ell_1$\nregularized GTD algorithms, which employ the well known soft thresholding\noperator. We investigate convergence properties of the proposed algorithms, and\ndepict their performance with several numerical experiments.\n", "versions": [{"version": "v1", "created": "Wed, 5 Oct 2016 15:15:27 GMT"}], "update_date": "2016-10-06", "authors_parsed": [["Meyer", "Dominik", ""], ["Shen", "Hao", ""], ["Diepold", "Klaus", ""]]}, {"id": "1610.01546", "submitter": "Yueming Sun", "authors": "Yueming Sun, Yi Zhang, Yunfei Chen, Roger Jin", "title": "Conversational Recommendation System with Unsupervised Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We will demonstrate a conversational products recommendation agent. This\nsystem shows how we combine research in personalized recommendation systems\nwith research in dialogue systems to build a virtual sales agent. Based on new\ndeep learning technologies we developed, the virtual agent is capable of\nlearning how to interact with users, how to answer user questions, what is the\nnext question to ask, and what to recommend when chatting with a human user.\n  Normally a descent conversational agent for a particular domain requires tens\nof thousands of hand labeled conversational data or hand written rules. This is\na major barrier when launching a conversation agent for a new domain. We will\nexplore and demonstrate the effectiveness of the learning solution even when\nthere is no hand written rules or hand labeled training data.\n", "versions": [{"version": "v1", "created": "Thu, 22 Sep 2016 05:46:49 GMT"}], "update_date": "2016-10-06", "authors_parsed": [["Sun", "Yueming", ""], ["Zhang", "Yi", ""], ["Chen", "Yunfei", ""], ["Jin", "Roger", ""]]}, {"id": "1610.01642", "submitter": "Bharath Ramsundar", "authors": "Bharath Ramsundar and Vijay S. Pande", "title": "Learning Protein Dynamics with Metastable Switching Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a machine learning approach for extracting fine-grained\nrepresentations of protein evolution from molecular dynamics datasets.\nMetastable switching linear dynamical systems extend standard switching models\nwith a physically-inspired stability constraint. This constraint enables the\nlearning of nuanced representations of protein dynamics that closely match\nphysical reality. We derive an EM algorithm for learning, where the E-step\nextends the forward-backward algorithm for HMMs and the M-step requires the\nsolution of large biconvex optimization problems. We construct an approximate\nsemidefinite program solver based on the Frank-Wolfe algorithm and use it to\nsolve the M-step. We apply our EM algorithm to learn accurate dynamics from\nlarge simulation datasets for the opioid peptide met-enkephalin and the\nproto-oncogene Src-kinase. Our learned models demonstrate significant\nimprovements in temporal coherence over HMMs and standard switching models for\nmet-enkephalin, and sample transition paths (possibly useful in rational drug\ndesign) for Src-kinase.\n", "versions": [{"version": "v1", "created": "Wed, 5 Oct 2016 20:52:48 GMT"}], "update_date": "2016-10-07", "authors_parsed": [["Ramsundar", "Bharath", ""], ["Pande", "Vijay S.", ""]]}, {"id": "1610.01644", "submitter": "Guillaume Alain", "authors": "Guillaume Alain and Yoshua Bengio", "title": "Understanding intermediate layers using linear classifier probes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural network models have a reputation for being black boxes. We propose to\nmonitor the features at every layer of a model and measure how suitable they\nare for classification. We use linear classifiers, which we refer to as\n\"probes\", trained entirely independently of the model itself.\n  This helps us better understand the roles and dynamics of the intermediate\nlayers. We demonstrate how this can be used to develop a better intuition about\nmodels and to diagnose potential problems.\n  We apply this technique to the popular models Inception v3 and Resnet-50.\nAmong other things, we observe experimentally that the linear separability of\nfeatures increase monotonically along the depth of the model.\n", "versions": [{"version": "v1", "created": "Wed, 5 Oct 2016 20:59:01 GMT"}, {"version": "v2", "created": "Mon, 10 Oct 2016 02:33:57 GMT"}, {"version": "v3", "created": "Fri, 14 Oct 2016 18:47:19 GMT"}, {"version": "v4", "created": "Thu, 22 Nov 2018 23:40:00 GMT"}], "update_date": "2018-11-26", "authors_parsed": [["Alain", "Guillaume", ""], ["Bengio", "Yoshua", ""]]}, {"id": "1610.01675", "submitter": "Michael Lash", "authors": "Michael T. Lash, Qihang Lin, W. Nick Street, Jennifer G. Robinson,\n  Jeffrey Ohlmann", "title": "Generalized Inverse Classification", "comments": "Accepted to SDM 2017. Full paper + supplemental material", "journal-ref": null, "doi": "10.1137/1.9781611974973.19", "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Inverse classification is the process of perturbing an instance in a\nmeaningful way such that it is more likely to conform to a specific class.\nHistorical methods that address such a problem are often framed to leverage\nonly a single classifier, or specific set of classifiers. These works are often\naccompanied by naive assumptions. In this work we propose generalized inverse\nclassification (GIC), which avoids restricting the classification model that\ncan be used. We incorporate this formulation into a refined framework in which\nGIC takes place. Under this framework, GIC operates on features that are\nimmediately actionable. Each change incurs an individual cost, either linear or\nnon-linear. Such changes are subjected to occur within a specified level of\ncumulative change (budget). Furthermore, our framework incorporates the\nestimation of features that change as a consequence of direct actions taken\n(indirectly changeable features). To solve such a problem, we propose three\nreal-valued heuristic-based methods and two sensitivity analysis-based\ncomparison methods, each of which is evaluated on two freely available\nreal-world datasets. Our results demonstrate the validity and benefits of our\nformulation, framework, and methods.\n", "versions": [{"version": "v1", "created": "Wed, 5 Oct 2016 22:28:01 GMT"}, {"version": "v2", "created": "Thu, 12 Jan 2017 17:38:58 GMT"}], "update_date": "2017-06-13", "authors_parsed": [["Lash", "Michael T.", ""], ["Lin", "Qihang", ""], ["Street", "W. Nick", ""], ["Robinson", "Jennifer G.", ""], ["Ohlmann", "Jeffrey", ""]]}, {"id": "1610.01683", "submitter": "Orestis Tsinalis", "authors": "Orestis Tsinalis, Paul M. Matthews, Yike Guo, Stefanos Zafeiriou", "title": "Automatic Sleep Stage Scoring with Single-Channel EEG Using\n  Convolutional Neural Networks", "comments": "12 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  We used convolutional neural networks (CNNs) for automatic sleep stage\nscoring based on single-channel electroencephalography (EEG) to learn\ntask-specific filters for classification without using prior domain knowledge.\nWe used an openly available dataset from 20 healthy young adults for evaluation\nand applied 20-fold cross-validation. We used class-balanced random sampling\nwithin the stochastic gradient descent (SGD) optimization of the CNN to avoid\nskewed performance in favor of the most represented sleep stages. We achieved\nhigh mean F1-score (81%, range 79-83%), mean accuracy across individual sleep\nstages (82%, range 80-84%) and overall accuracy (74%, range 71-76%) over all\nsubjects. By analyzing and visualizing the filters that our CNN learns, we\nfound that rules learned by the filters correspond to sleep scoring criteria in\nthe American Academy of Sleep Medicine (AASM) manual that human experts follow.\nOur method's performance is balanced across classes and our results are\ncomparable to state-of-the-art methods with hand-engineered features. We show\nthat, without using prior domain knowledge, a CNN can automatically learn to\ndistinguish among different normal sleep stages.\n", "versions": [{"version": "v1", "created": "Wed, 5 Oct 2016 23:13:55 GMT"}], "update_date": "2016-10-07", "authors_parsed": [["Tsinalis", "Orestis", ""], ["Matthews", "Paul M.", ""], ["Guo", "Yike", ""], ["Zafeiriou", "Stefanos", ""]]}, {"id": "1610.01685", "submitter": "Lerrel Pinto Mr", "authors": "Lerrel Pinto, James Davidson and Abhinav Gupta", "title": "Supervision via Competition: Robot Adversaries for Learning Tasks", "comments": "Submission to ICRA 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There has been a recent paradigm shift in robotics to data-driven learning\nfor planning and control. Due to large number of experiences required for\ntraining, most of these approaches use a self-supervised paradigm: using\nsensors to measure success/failure. However, in most cases, these sensors\nprovide weak supervision at best. In this work, we propose an adversarial\nlearning framework that pits an adversary against the robot learning the task.\nIn an effort to defeat the adversary, the original robot learns to perform the\ntask with more robustness leading to overall improved performance. We show that\nthis adversarial framework forces the the robot to learn a better grasping\nmodel in order to overcome the adversary. By grasping 82% of presented novel\nobjects compared to 68% without an adversary, we demonstrate the utility of\ncreating adversaries. We also demonstrate via experiments that having robots in\nadversarial setting might be a better learning strategy as compared to having\ncollaborative multiple robots.\n", "versions": [{"version": "v1", "created": "Wed, 5 Oct 2016 23:28:12 GMT"}], "update_date": "2016-10-07", "authors_parsed": [["Pinto", "Lerrel", ""], ["Davidson", "James", ""], ["Gupta", "Abhinav", ""]]}, {"id": "1610.01687", "submitter": "Zifan Li", "authors": "Zifan Li, Ambuj Tewari", "title": "Sampled Fictitious Play is Hannan Consistent", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GT cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fictitious play is a simple and widely studied adaptive heuristic for playing\nrepeated games. It is well known that fictitious play fails to be Hannan\nconsistent. Several variants of fictitious play including regret matching,\ngeneralized regret matching and smooth fictitious play, are known to be Hannan\nconsistent. In this note, we consider sampled fictitious play: at each round,\nthe player samples past times and plays the best response to previous moves of\nother players at the sampled time points. We show that sampled fictitious play,\nusing Bernoulli sampling, is Hannan consistent. Unlike several existing Hannan\nconsistency proofs that rely on concentration of measure results, ours instead\nuses anti-concentration results from Littlewood-Offord theory.\n", "versions": [{"version": "v1", "created": "Wed, 5 Oct 2016 23:41:23 GMT"}, {"version": "v2", "created": "Tue, 11 Apr 2017 15:52:46 GMT"}], "update_date": "2017-04-12", "authors_parsed": [["Li", "Zifan", ""], ["Tewari", "Ambuj", ""]]}, {"id": "1610.01690", "submitter": "Vaneet Aggarwal", "authors": "Xiao-Yang Liu and Shuchin Aeron and Vaneet Aggarwal and Xiaodong Wang", "title": "Low-tubal-rank Tensor Completion using Alternating Minimization", "comments": null, "journal-ref": null, "doi": "10.1117/12.2224039", "report-no": null, "categories": "cs.LG cs.IT math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The low-tubal-rank tensor model has been recently proposed for real-world\nmultidimensional data. In this paper, we study the low-tubal-rank tensor\ncompletion problem, i.e., to recover a third-order tensor by observing a subset\nof its elements selected uniformly at random. We propose a fast iterative\nalgorithm, called {\\em Tubal-Alt-Min}, that is inspired by a similar approach\nfor low-rank matrix completion. The unknown low-tubal-rank tensor is\nrepresented as the product of two much smaller tensors with the low-tubal-rank\nproperty being automatically incorporated, and Tubal-Alt-Min alternates between\nestimating those two tensors using tensor least squares minimization. First, we\nnote that tensor least squares minimization is different from its matrix\ncounterpart and nontrivial as the circular convolution operator of the\nlow-tubal-rank tensor model is intertwined with the sub-sampling operator.\nSecond, the theoretical performance guarantee is challenging since\nTubal-Alt-Min is iterative and nonconvex in nature. We prove that 1)\nTubal-Alt-Min guarantees exponential convergence to the global optima, and 2)\nfor an $n \\times n \\times k$ tensor with tubal-rank $r \\ll n$, the required\nsampling complexity is $O(nr^2k \\log^3 n)$ and the computational complexity is\n$O(n^2rk^2 \\log^2 n)$. Third, on both synthetic data and real-world video data,\nevaluation results show that compared with tensor-nuclear norm minimization\n(TNN-ADMM), Tubal-Alt-Min improves the recovery error dramatically (by orders\nof magnitude). It is estimated that Tubal-Alt-Min converges at an exponential\nrate $10^{-0.4423 \\text{Iter}}$ where $\\text{Iter}$ denotes the number of\niterations, which is much faster than TNN-ADMM's $10^{-0.0332 \\text{Iter}}$,\nand the running time can be accelerated by more than $5$ times for a $200\n\\times 200 \\times 20$ tensor.\n", "versions": [{"version": "v1", "created": "Wed, 5 Oct 2016 23:44:37 GMT"}], "update_date": "2016-10-12", "authors_parsed": [["Liu", "Xiao-Yang", ""], ["Aeron", "Shuchin", ""], ["Aggarwal", "Vaneet", ""], ["Wang", "Xiaodong", ""]]}, {"id": "1610.01712", "submitter": "Sourangshu Bhattacharya", "authors": "Asis Roy, Sourangshu Bhattacharya, Kalyan Guin", "title": "A Methodology for Customizing Clinical Tests for Esophageal Cancer based\n  on Patient Preferences", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Tests for Esophageal cancer can be expensive, uncomfortable and can have side\neffects. For many patients, we can predict non-existence of disease with 100%\ncertainty, just using demographics, lifestyle, and medical history information.\nOur objective is to devise a general methodology for customizing tests using\nuser preferences so that expensive or uncomfortable tests can be avoided. We\npropose to use classifiers trained from electronic health records (EHR) for\nselection of tests. The key idea is to design classifiers with 100% false\nnormal rates, possibly at the cost higher false abnormals. We compare Naive\nBayes classification (NB), Random Forests (RF), Support Vector Machines (SVM)\nand Logistic Regression (LR), and find kernel Logistic regression to be most\nsuitable for the task. We propose an algorithm for finding the best probability\nthreshold for kernel LR, based on test set accuracy. Using the proposed\nalgorithm, we describe schemes for selecting tests, which appear as features in\nthe automatic classification algorithm, using preferences on costs and\ndiscomfort of the users. We test our methodology with EHRs collected for more\nthan 3000 patients, as a part of project carried out by a reputed hospital in\nMumbai, India. Kernel SVM and kernel LR with a polynomial kernel of degree 3,\nyields an accuracy of 99.8% and sensitivity 100%, without the MP features, i.e.\nusing only clinical tests. We demonstrate our test selection algorithm using\ntwo case studies, one using cost of clinical tests, and other using\n\"discomfort\" values for clinical tests. We compute the test sets corresponding\nto the lowest false abnormals for each criterion described above, using\nexhaustive enumeration of 15 clinical tests. The sets turn out to different,\nsubstantiating our claim that one can customize test sets based on user\npreferences.\n", "versions": [{"version": "v1", "created": "Thu, 6 Oct 2016 01:56:00 GMT"}], "update_date": "2016-10-07", "authors_parsed": [["Roy", "Asis", ""], ["Bhattacharya", "Sourangshu", ""], ["Guin", "Kalyan", ""]]}, {"id": "1610.01741", "submitter": "Mohamad Ivan Fanany", "authors": "Endang Purnama Giri, Mohamad Ivan Fanany, Aniati Murni Arymurthy", "title": "Combining Generative and Discriminative Neural Networks for Sleep Stages\n  Classification", "comments": "Submitted to Computational Intelligence and Neuroscience (Hindawi\n  Publishing). 13 pages", "journal-ref": null, "doi": null, "report-no": "3184843", "categories": "cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sleep stages pattern provides important clues in diagnosing the presence of\nsleep disorder. By analyzing sleep stages pattern and extracting its features\nfrom EEG, EOG, and EMG signals, we can classify sleep stages. This study\npresents a novel classification model for predicting sleep stages with a high\naccuracy. The main idea is to combine the generative capability of Deep Belief\nNetwork (DBN) with a discriminative ability and sequence pattern recognizing\ncapability of Long Short-term Memory (LSTM). We use DBN that is treated as an\nautomatic higher level features generator. The input to DBN is 28 \"handcrafted\"\nfeatures as used in previous sleep stages studies. We compared our method with\nother techniques which combined DBN with Hidden Markov Model (HMM).In this\nstudy, we exploit the sequence or time series characteristics of sleep dataset.\nTo the best of our knowledge, most of the present sleep analysis from\npolysomnogram relies only on single instanced label (nonsequence) for\nclassification. In this study, we used two datasets: an open data set that is\ntreated as a benchmark; the other dataset is our sleep stages dataset\n(available for download) to verify the results further. Our experiments showed\nthat the combination of DBN with LSTM gives better overall accuracy 98.75\\%\n(Fscore=0.9875) for benchmark dataset and 98.94\\% (Fscore=0.9894) for MKG\ndataset. This result is better than the state of the art of sleep stages\nclassification that was 91.31\\%.\n", "versions": [{"version": "v1", "created": "Thu, 6 Oct 2016 06:05:16 GMT"}], "update_date": "2016-10-07", "authors_parsed": [["Giri", "Endang Purnama", ""], ["Fanany", "Mohamad Ivan", ""], ["Arymurthy", "Aniati Murni", ""]]}, {"id": "1610.01757", "submitter": "Mohamad Ivan Fanany", "authors": "Endang Purnama Giri, Mohamad Ivan Fanany, Aniati Murni Arymurthy", "title": "Ischemic Stroke Identification Based on EEG and EOG using 1D\n  Convolutional Neural Network and Batch Normalization", "comments": "13 pages. To be published in ICACSIS 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In 2015, stroke was the number one cause of death in Indonesia. The majority\ntype of stroke is ischemic. The standard tool for diagnosing stroke is CT-Scan.\nFor developing countries like Indonesia, the availability of CT-Scan is very\nlimited and still relatively expensive. Because of the availability, another\ndevice that potential to diagnose stroke in Indonesia is EEG. Ischemic stroke\noccurs because of obstruction that can make the cerebral blood flow (CBF) on a\nperson with stroke has become lower than CBF on a normal person (control) so\nthat the EEG signal have a deceleration. On this study, we perform the ability\nof 1D Convolutional Neural Network (1DCNN) to construct classification model\nthat can distinguish the EEG and EOG stroke data from EEG and EOG control data.\nTo accelerate training process our model we use Batch Normalization. Involving\n62 person data object and from leave one out the scenario with five times\nrepetition of measurement we obtain the average of accuracy 0.86 (F-Score\n0.861) only at 200 epoch. This result is better than all over shallow and\npopular classifiers as the comparator (the best result of accuracy 0.69 and\nF-Score 0.72 ). The feature used in our study were only 24 handcrafted feature\nwith simple feature extraction process.\n", "versions": [{"version": "v1", "created": "Thu, 6 Oct 2016 07:19:27 GMT"}], "update_date": "2016-10-07", "authors_parsed": [["Giri", "Endang Purnama", ""], ["Fanany", "Mohamad Ivan", ""], ["Arymurthy", "Aniati Murni", ""]]}, {"id": "1610.01891", "submitter": "Mohamad Ivan Fanany", "authors": "Sadikin Mujiono, Mohamad Ivan Fanany, Chan Basaruddin", "title": "A New Data Representation Based on Training Data Characteristics to\n  Extract Drug Named-Entity in Medical Text", "comments": "Hindawi Publishing. Computational Intelligence and Neuroscience\n  Volume 2016 (2016), Article ID 3483528, 24 pages Received 27 May 2016;\n  Revised 8 August 2016; Accepted 18 September 2016. Special Issue on \"Smart\n  Data: Where the Big Data Meets the Semantics\". Academic Editor: Trong H.\n  Duong", "journal-ref": "Computational Intelligence and Neuroscience Volume 2016 (2016),\n  Article ID 3483528, 24 pages", "doi": null, "report-no": "3483528", "categories": "cs.CL cs.AI cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One essential task in information extraction from the medical corpus is drug\nname recognition. Compared with text sources come from other domains, the\nmedical text is special and has unique characteristics. In addition, the\nmedical text mining poses more challenges, e.g., more unstructured text, the\nfast growing of new terms addition, a wide range of name variation for the same\ndrug. The mining is even more challenging due to the lack of labeled dataset\nsources and external knowledge, as well as multiple token representations for a\nsingle drug name that is more common in the real application setting. Although\nmany approaches have been proposed to overwhelm the task, some problems\nremained with poor F-score performance (less than 0.75). This paper presents a\nnew treatment in data representation techniques to overcome some of those\nchallenges. We propose three data representation techniques based on the\ncharacteristics of word distribution and word similarities as a result of word\nembedding training. The first technique is evaluated with the standard NN\nmodel, i.e., MLP (Multi-Layer Perceptrons). The second technique involves two\ndeep network classifiers, i.e., DBN (Deep Belief Networks), and SAE (Stacked\nDenoising Encoders). The third technique represents the sentence as a sequence\nthat is evaluated with a recurrent NN model, i.e., LSTM (Long Short Term\nMemory). In extracting the drug name entities, the third technique gives the\nbest F-score performance compared to the state of the art, with its average\nF-score being 0.8645.\n", "versions": [{"version": "v1", "created": "Thu, 6 Oct 2016 14:38:09 GMT"}], "update_date": "2016-10-07", "authors_parsed": [["Mujiono", "Sadikin", ""], ["Fanany", "Mohamad Ivan", ""], ["Basaruddin", "Chan", ""]]}, {"id": "1610.01922", "submitter": "Mohamad Ivan Fanany", "authors": "Arif Budiman, Mohamad Ivan Fanany, Chan Basaruddin", "title": "Adaptive Online Sequential ELM for Concept Drift Tackling", "comments": "Hindawi Publishing. Computational Intelligence and Neuroscience\n  Volume 2016 (2016), Article ID 8091267, 17 pages Received 29 January 2016,\n  Accepted 17 May 2016. Special Issue on \"Advances in Neural Networks and\n  Hybrid-Metaheuristics: Theory, Algorithms, and Novel Engineering\n  Applications\". Academic Editor: Stefan Haufe", "journal-ref": "Computational Intelligence and Neuroscience Volume 2016 (2016),\n  Article ID 8091267, 17 pages", "doi": "10.1155/2016/8091267", "report-no": "8091267", "categories": "cs.AI cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A machine learning method needs to adapt to over time changes in the\nenvironment. Such changes are known as concept drift. In this paper, we propose\nconcept drift tackling method as an enhancement of Online Sequential Extreme\nLearning Machine (OS-ELM) and Constructive Enhancement OS-ELM (CEOS-ELM) by\nadding adaptive capability for classification and regression problem. The\nscheme is named as adaptive OS-ELM (AOS-ELM). It is a single classifier scheme\nthat works well to handle real drift, virtual drift, and hybrid drift. The\nAOS-ELM also works well for sudden drift and recurrent context change type. The\nscheme is a simple unified method implemented in simple lines of code. We\nevaluated AOS-ELM on regression and classification problem by using concept\ndrift public data set (SEA and STAGGER) and other public data sets such as\nMNIST, USPS, and IDS. Experiments show that our method gives higher kappa value\ncompared to the multiclassifier ELM ensemble. Even though AOS-ELM in practice\ndoes not need hidden nodes increase, we address some issues related to the\nincreasing of the hidden nodes such as error condition and rank values. We\npropose taking the rank of the pseudoinverse matrix as an indicator parameter\nto detect underfitting condition.\n", "versions": [{"version": "v1", "created": "Thu, 6 Oct 2016 16:08:52 GMT"}], "update_date": "2016-10-10", "authors_parsed": [["Budiman", "Arif", ""], ["Fanany", "Mohamad Ivan", ""], ["Basaruddin", "Chan", ""]]}, {"id": "1610.01934", "submitter": "Qinglong Wang", "authors": "Qinglong Wang, Wenbo Guo, Alexander G. Ororbia II, Xinyu Xing, Lin\n  Lin, C. Lee Giles, Xue Liu, Peng Liu, Gang Xiong", "title": "Using Non-invertible Data Transformations to Build Adversarial-Robust\n  Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural networks have proven to be quite effective in a wide variety of\nmachine learning tasks, ranging from improved speech recognition systems to\nadvancing the development of autonomous vehicles. However, despite their\nsuperior performance in many applications, these models have been recently\nshown to be susceptible to a particular type of attack possible through the\ngeneration of particular synthetic examples referred to as adversarial samples.\nThese samples are constructed by manipulating real examples from the training\ndata distribution in order to \"fool\" the original neural model, resulting in\nmisclassification (with high confidence) of previously correctly classified\nsamples. Addressing this weakness is of utmost importance if deep neural\narchitectures are to be applied to critical applications, such as those in the\ndomain of cybersecurity. In this paper, we present an analysis of this\nfundamental flaw lurking in all neural architectures to uncover limitations of\npreviously proposed defense mechanisms. More importantly, we present a unifying\nframework for protecting deep neural models using a non-invertible data\ntransformation--developing two adversary-resilient architectures utilizing both\nlinear and nonlinear dimensionality reduction. Empirical results indicate that\nour framework provides better robustness compared to state-of-art solutions\nwhile having negligible degradation in accuracy.\n", "versions": [{"version": "v1", "created": "Thu, 6 Oct 2016 16:20:45 GMT"}, {"version": "v2", "created": "Fri, 7 Oct 2016 14:08:07 GMT"}, {"version": "v3", "created": "Mon, 17 Oct 2016 15:45:44 GMT"}, {"version": "v4", "created": "Tue, 18 Oct 2016 19:53:00 GMT"}, {"version": "v5", "created": "Tue, 13 Dec 2016 20:13:33 GMT"}], "update_date": "2016-12-14", "authors_parsed": [["Wang", "Qinglong", ""], ["Guo", "Wenbo", ""], ["Ororbia", "Alexander G.", "II"], ["Xing", "Xinyu", ""], ["Lin", "Lin", ""], ["Giles", "C. Lee", ""], ["Liu", "Xue", ""], ["Liu", "Peng", ""], ["Xiong", "Gang", ""]]}, {"id": "1610.01935", "submitter": "Mohamad Ivan Fanany", "authors": "Intan Nurma Yulita, Mohamad Ivan Fanany, Aniati Murni Arymurthy", "title": "Sequence-based Sleep Stage Classification using Conditional Neural\n  Fields", "comments": "14 pages. Submitted to Computational and Mathematical Methods in\n  Medicine (Hindawi Publishin). Article ID 7163687", "journal-ref": null, "doi": null, "report-no": "7163687", "categories": "cs.NE cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sleep signals from a polysomnographic database are sequences in nature.\nCommonly employed analysis and classification methods, however, ignored this\nfact and treated the sleep signals as non-sequence data. Treating the sleep\nsignals as sequences, this paper compared two powerful unsupervised feature\nextractors and three sequence-based classifiers regarding accuracy and\ncomputational (training and testing) time after 10-folds cross-validation. The\ncompared feature extractors are Deep Belief Networks (DBN) and Fuzzy C-Means\n(FCM) clustering. Whereas the compared sequence-based classifiers are Hidden\nMarkov Models (HMM), Conditional Random Fields (CRF) and its variants, i.e.,\nHidden-state CRF (HCRF) and Latent-Dynamic CRF (LDCRF); and Conditional Neural\nFields (CNF) and its variant (LDCNF). In this study, we use two datasets. The\nfirst dataset is an open (public) polysomnographic dataset downloadable from\nthe Internet, while the second dataset is our polysomnographic dataset (also\navailable for download). For the first dataset, the combination of FCM and CNF\ngives the highest accuracy (96.75\\%) with relatively short training time (0.33\nhours). For the second dataset, the combination of DBN and CRF gives the\naccuracy of 99.96\\% but with 1.02 hours training time, whereas the combination\nof DBN and CNF gives slightly less accuracy (99.69\\%) but also less computation\ntime (0.89 hours).\n", "versions": [{"version": "v1", "created": "Thu, 6 Oct 2016 16:26:58 GMT"}], "update_date": "2016-10-07", "authors_parsed": [["Yulita", "Intan Nurma", ""], ["Fanany", "Mohamad Ivan", ""], ["Arymurthy", "Aniati Murni", ""]]}, {"id": "1610.01945", "submitter": "David Pfau", "authors": "David Pfau and Oriol Vinyals", "title": "Connecting Generative Adversarial Networks and Actor-Critic Methods", "comments": "Added comments on inverse reinforcement learning", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Both generative adversarial networks (GAN) in unsupervised learning and\nactor-critic methods in reinforcement learning (RL) have gained a reputation\nfor being difficult to optimize. Practitioners in both fields have amassed a\nlarge number of strategies to mitigate these instabilities and improve\ntraining. Here we show that GANs can be viewed as actor-critic methods in an\nenvironment where the actor cannot affect the reward. We review the strategies\nfor stabilizing training for each class of models, both those that generalize\nbetween the two and those that are particular to that model. We also review a\nnumber of extensions to GANs and RL algorithms with even more complicated\ninformation flow. We hope that by highlighting this formal connection we will\nencourage both GAN and RL communities to develop general, scalable, and stable\nalgorithms for multilevel optimization with deep networks, and to draw\ninspiration across communities.\n", "versions": [{"version": "v1", "created": "Thu, 6 Oct 2016 17:00:54 GMT"}, {"version": "v2", "created": "Fri, 7 Oct 2016 16:15:48 GMT"}, {"version": "v3", "created": "Wed, 18 Jan 2017 18:10:00 GMT"}], "update_date": "2017-01-19", "authors_parsed": [["Pfau", "David", ""], ["Vinyals", "Oriol", ""]]}, {"id": "1610.01959", "submitter": "Panos P. Markopoulos", "authors": "Panos P. Markopoulos, Sandipan Kundu, Shubham Chamadia, Dimitris A.\n  Pados", "title": "Efficient L1-Norm Principal-Component Analysis via Bit Flipping", "comments": null, "journal-ref": null, "doi": "10.1109/TSP.2017.2708023", "report-no": null, "categories": "cs.DS cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It was shown recently that the $K$ L1-norm principal components (L1-PCs) of a\nreal-valued data matrix $\\mathbf X \\in \\mathbb R^{D \\times N}$ ($N$ data\nsamples of $D$ dimensions) can be exactly calculated with cost\n$\\mathcal{O}(2^{NK})$ or, when advantageous, $\\mathcal{O}(N^{dK - K + 1})$\nwhere $d=\\mathrm{rank}(\\mathbf X)$, $K<d$ [1],[2]. In applications where\n$\\mathbf X$ is large (e.g., \"big\" data of large $N$ and/or \"heavy\" data of\nlarge $d$), these costs are prohibitive. In this work, we present a novel\nsuboptimal algorithm for the calculation of the $K < d$ L1-PCs of $\\mathbf X$\nof cost $\\mathcal O(ND \\mathrm{min} \\{ N,D\\} + N^2(K^4 + dK^2) + dNK^3)$, which\nis comparable to that of standard (L2-norm) PC analysis. Our theoretical and\nexperimental studies show that the proposed algorithm calculates the exact\noptimal L1-PCs with high frequency and achieves higher value in the L1-PC\noptimization metric than any known alternative algorithm of comparable\ncomputational cost. The superiority of the calculated L1-PCs over standard\nL2-PCs (singular vectors) in characterizing potentially faulty\ndata/measurements is demonstrated with experiments on data dimensionality\nreduction and disease diagnosis from genomic data.\n", "versions": [{"version": "v1", "created": "Thu, 6 Oct 2016 17:20:16 GMT"}], "update_date": "2017-08-02", "authors_parsed": [["Markopoulos", "Panos P.", ""], ["Kundu", "Sandipan", ""], ["Chamadia", "Shubham", ""], ["Pados", "Dimitris A.", ""]]}, {"id": "1610.01980", "submitter": "Tengyu Ma", "authors": "Tengyu Ma, Jonathan Shi, David Steurer", "title": "Polynomial-time Tensor Decompositions with Sum-of-Squares", "comments": "to appear in FOCS 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We give new algorithms based on the sum-of-squares method for tensor\ndecomposition. Our results improve the best known running times from\nquasi-polynomial to polynomial for several problems, including decomposing\nrandom overcomplete 3-tensors and learning overcomplete dictionaries with\nconstant relative sparsity. We also give the first robust analysis for\ndecomposing overcomplete 4-tensors in the smoothed analysis model. A key\ningredient of our analysis is to establish small spectral gaps in moment\nmatrices derived from solutions to sum-of-squares relaxations. To enable this\nanalysis we augment sum-of-squares relaxations with spectral analogs of maximum\nentropy constraints.\n", "versions": [{"version": "v1", "created": "Thu, 6 Oct 2016 18:18:54 GMT"}], "update_date": "2016-10-07", "authors_parsed": [["Ma", "Tengyu", ""], ["Shi", "Jonathan", ""], ["Steurer", "David", ""]]}, {"id": "1610.01986", "submitter": "Mehdi Khamassi", "authors": "Mehdi Khamassi, Costas Tzafestas", "title": "Active exploration in parameterized reinforcement learning", "comments": "Submitted to EWRL2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Online model-free reinforcement learning (RL) methods with continuous actions\nare playing a prominent role when dealing with real-world applications such as\nRobotics. However, when confronted to non-stationary environments, these\nmethods crucially rely on an exploration-exploitation trade-off which is rarely\ndynamically and automatically adjusted to changes in the environment. Here we\npropose an active exploration algorithm for RL in structured (parameterized)\ncontinuous action space. This framework deals with a set of discrete actions,\neach of which is parameterized with continuous variables. Discrete exploration\nis controlled through a Boltzmann softmax function with an inverse temperature\n$\\beta$ parameter. In parallel, a Gaussian exploration is applied to the\ncontinuous action parameters. We apply a meta-learning algorithm based on the\ncomparison between variations of short-term and long-term reward running\naverages to simultaneously tune $\\beta$ and the width of the Gaussian\ndistribution from which continuous action parameters are drawn. When applied to\na simple virtual human-robot interaction task, we show that this algorithm\noutperforms continuous parameterized RL both without active exploration and\nwith active exploration based on uncertainty variations measured by a\nKalman-Q-learning algorithm.\n", "versions": [{"version": "v1", "created": "Thu, 6 Oct 2016 18:34:04 GMT"}], "update_date": "2016-10-07", "authors_parsed": [["Khamassi", "Mehdi", ""], ["Tzafestas", "Costas", ""]]}, {"id": "1610.01989", "submitter": "Sakyasingha Dasgupta", "authors": "Sakyasingha Dasgupta and Takayuki Yoshizumi and Takayuki Osogami", "title": "Regularized Dynamic Boltzmann Machine with Delay Pruning for\n  Unsupervised Learning of Temporal Sequences", "comments": "6 pages, 5 figures, accepted full paper (oral presentation) at ICPR\n  2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce Delay Pruning, a simple yet powerful technique to regularize\ndynamic Boltzmann machines (DyBM). The recently introduced DyBM provides a\nparticularly structured Boltzmann machine, as a generative model of a\nmulti-dimensional time-series. This Boltzmann machine can have infinitely many\nlayers of units but allows exact inference and learning based on its\nbiologically motivated structure. DyBM uses the idea of conduction delays in\nthe form of fixed length first-in first-out (FIFO) queues, with a neuron\nconnected to another via this FIFO queue, and spikes from a pre-synaptic neuron\ntravel along the queue to the post-synaptic neuron with a constant period of\ndelay. Here, we present Delay Pruning as a mechanism to prune the lengths of\nthe FIFO queues (making them zero) by setting some delay lengths to one with a\nfixed probability, and finally selecting the best performing model with fixed\ndelays. The uniqueness of structure and a non-sampling based learning rule in\nDyBM, make the application of previously proposed regularization techniques\nlike Dropout or DropConnect difficult, leading to poor generalization. First,\nwe evaluate the performance of Delay Pruning to let DyBM learn a\nmultidimensional temporal sequence generated by a Markov chain. Finally, we\nshow the effectiveness of delay pruning in learning high dimensional sequences\nusing the moving MNIST dataset, and compare it with Dropout and DropConnect\nmethods.\n", "versions": [{"version": "v1", "created": "Thu, 22 Sep 2016 10:04:59 GMT"}], "update_date": "2016-10-07", "authors_parsed": [["Dasgupta", "Sakyasingha", ""], ["Yoshizumi", "Takayuki", ""], ["Osogami", "Takayuki", ""]]}, {"id": "1610.02067", "submitter": "S. Rasoul Etesami", "authors": "Seyed Rasoul Etesami, Walid Saad, Narayan Mandayam, and H. Vincent\n  Poor", "title": "Stochastic Games for Smart Grid Energy Management with Prospect\n  Prosumers", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GT cs.IT cs.LG cs.SY math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, the problem of smart grid energy management under stochastic\ndynamics is investigated. In the considered model, at the demand side, it is\nassumed that customers can act as prosumers who own renewable energy sources\nand can both produce and consume energy. Due to the coupling between the\nprosumers' decisions and the stochastic nature of renewable energy, the\ninteraction among prosumers is formulated as a stochastic game, in which each\nprosumer seeks to maximize its payoff, in terms of revenues, by controlling its\nenergy consumption and demand. In particular, the subjective behavior of\nprosumers is explicitly reflected into their payoff functions using prospect\ntheory, a powerful framework that allows modeling real-life human choices. For\nthis prospect-based stochastic game, it is shown that there always exists a\nstationary Nash equilibrium where the prosumers' trading policies in the\nequilibrium are independent of the time and their histories of the play.\nMoreover, a novel distributed algorithm with no information sharing among\nprosumers is proposed and shown to converge to an $\\epsilon$-Nash equilibrium.\nOn the other hand, at the supply side, the interaction between the utility\ncompany and the prosumers is formulated as an online optimization problem in\nwhich the utility company's goal is to learn its optimal energy allocation\nrules. For this case, it is shown that such an optimization problem admits a\nno-regret algorithm meaning that regardless of the actual outcome of the game\namong the prosumers, the utility company can follow a strategy that mitigates\nits allocation costs as if it knew the entire demand market a priori.\nSimulation results show the convergence of the proposed algorithms to their\npredicted outcomes and present new insights resulting from prospect theory that\ncontribute toward more efficient energy management in the smart grids.\n", "versions": [{"version": "v1", "created": "Thu, 6 Oct 2016 21:02:32 GMT"}, {"version": "v2", "created": "Mon, 7 Aug 2017 17:49:42 GMT"}], "update_date": "2017-08-08", "authors_parsed": [["Etesami", "Seyed Rasoul", ""], ["Saad", "Walid", ""], ["Mandayam", "Narayan", ""], ["Poor", "H. Vincent", ""]]}, {"id": "1610.02072", "submitter": "G\\'abor Braun", "authors": "G\\'abor Braun and Sebastian Pokutta", "title": "An efficient high-probability algorithm for Linear Bandits", "comments": "17 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For the linear bandit problem, we extend the analysis of algorithm CombEXP\nfrom [R. Combes, M. S. Talebi Mazraeh Shahi, A. Proutiere, and M. Lelarge.\nCombinatorial bandits revisited. In C. Cortes, N. D. Lawrence, D. D. Lee, M.\nSugiyama, and R. Garnett, editors, Advances in Neural Information Processing\nSystems 28, pages 2116--2124. Curran Associates, Inc., 2015. URL\nhttp://papers.nips.cc/paper/5831-combinatorial-bandits-revisited.pdf] to the\nhigh-probability case against adaptive adversaries, allowing actions to come\nfrom an arbitrary polytope. We prove a high-probability regret of\n\\(O(T^{2/3})\\) for time horizon \\(T\\). While this bound is weaker than the\noptimal \\(O(\\sqrt{T})\\) bound achieved by GeometricHedge in [P. L. Bartlett, V.\nDani, T. Hayes, S. Kakade, A. Rakhlin, and A. Tewari. High-probability regret\nbounds for bandit online linear optimization. In 21th Annual Conference on\nLearning Theory (COLT 2008), July 2008.\nhttp://eprints.qut.edu.au/45706/1/30-Bartlett.pdf], CombEXP is computationally\nefficient, requiring only an efficient linear optimization oracle over the\nconvex hull of the actions.\n", "versions": [{"version": "v1", "created": "Thu, 6 Oct 2016 21:14:16 GMT"}, {"version": "v2", "created": "Thu, 13 Oct 2016 16:09:41 GMT"}], "update_date": "2016-10-14", "authors_parsed": [["Braun", "G\u00e1bor", ""], ["Pokutta", "Sebastian", ""]]}, {"id": "1610.02132", "submitter": "Jerry Li", "authors": "Dan Alistarh, Demjan Grubic, Jerry Li, Ryota Tomioka, Milan Vojnovic", "title": "QSGD: Communication-Efficient SGD via Gradient Quantization and Encoding", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Parallel implementations of stochastic gradient descent (SGD) have received\nsignificant research attention, thanks to excellent scalability properties of\nthis algorithm, and to its efficiency in the context of training deep neural\nnetworks. A fundamental barrier for parallelizing large-scale SGD is the fact\nthat the cost of communicating the gradient updates between nodes can be very\nlarge. Consequently, lossy compression heuristics have been proposed, by which\nnodes only communicate quantized gradients. Although effective in practice,\nthese heuristics do not always provably converge, and it is not clear whether\nthey are optimal.\n  In this paper, we propose Quantized SGD (QSGD), a family of compression\nschemes which allow the compression of gradient updates at each node, while\nguaranteeing convergence under standard assumptions. QSGD allows the user to\ntrade off compression and convergence time: it can communicate a sublinear\nnumber of bits per iteration in the model dimension, and can achieve\nasymptotically optimal communication cost. We complement our theoretical\nresults with empirical data, showing that QSGD can significantly reduce\ncommunication cost, while being competitive with standard uncompressed\ntechniques on a variety of real tasks.\n  In particular, experiments show that gradient quantization applied to\ntraining of deep neural networks for image classification and automated speech\nrecognition can lead to significant reductions in communication cost, and\nend-to-end training time. For instance, on 16 GPUs, we are able to train a\nResNet-152 network on ImageNet 1.8x faster to full accuracy. Of note, we show\nthat there exist generic parameter settings under which all known network\narchitectures preserve or slightly improve their full accuracy when using\nquantization.\n", "versions": [{"version": "v1", "created": "Fri, 7 Oct 2016 03:44:34 GMT"}, {"version": "v2", "created": "Mon, 10 Apr 2017 22:25:06 GMT"}, {"version": "v3", "created": "Thu, 25 May 2017 08:05:19 GMT"}, {"version": "v4", "created": "Wed, 6 Dec 2017 18:28:32 GMT"}], "update_date": "2017-12-07", "authors_parsed": [["Alistarh", "Dan", ""], ["Grubic", "Demjan", ""], ["Li", "Jerry", ""], ["Tomioka", "Ryota", ""], ["Vojnovic", "Milan", ""]]}, {"id": "1610.02136", "submitter": "Dan Hendrycks", "authors": "Dan Hendrycks and Kevin Gimpel", "title": "A Baseline for Detecting Misclassified and Out-of-Distribution Examples\n  in Neural Networks", "comments": "Published as a conference paper at ICLR 2017. 1 Figure in 1 Appendix.\n  Minor changes from the previous version", "journal-ref": "International Conference on Learning Representations 2017", "doi": null, "report-no": null, "categories": "cs.NE cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the two related problems of detecting if an example is\nmisclassified or out-of-distribution. We present a simple baseline that\nutilizes probabilities from softmax distributions. Correctly classified\nexamples tend to have greater maximum softmax probabilities than erroneously\nclassified and out-of-distribution examples, allowing for their detection. We\nassess performance by defining several tasks in computer vision, natural\nlanguage processing, and automatic speech recognition, showing the\neffectiveness of this baseline across all. We then show the baseline can\nsometimes be surpassed, demonstrating the room for future research on these\nunderexplored detection tasks.\n", "versions": [{"version": "v1", "created": "Fri, 7 Oct 2016 04:06:01 GMT"}, {"version": "v2", "created": "Thu, 23 Mar 2017 18:11:25 GMT"}, {"version": "v3", "created": "Wed, 3 Oct 2018 07:32:57 GMT"}], "update_date": "2018-10-04", "authors_parsed": [["Hendrycks", "Dan", ""], ["Gimpel", "Kevin", ""]]}, {"id": "1610.02143", "submitter": "Tianyi Chen", "authors": "Tianyi Chen, Aryan Mokhtari, Xin Wang, Alejandro Ribeiro, and Georgios\n  B. Giannakis", "title": "Stochastic Averaging for Constrained Optimization with Application to\n  Online Resource Allocation", "comments": null, "journal-ref": null, "doi": "10.1109/TSP.2017.2679690", "report-no": null, "categories": "math.OC cs.DC cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Existing approaches to resource allocation for nowadays stochastic networks\nare challenged to meet fast convergence and tolerable delay requirements. The\npresent paper leverages online learning advances to facilitate stochastic\nresource allocation tasks. By recognizing the central role of Lagrange\nmultipliers, the underlying constrained optimization problem is formulated as a\nmachine learning task involving both training and operational modes, with the\ngoal of learning the sought multipliers in a fast and efficient manner. To this\nend, an order-optimal offline learning approach is developed first for batch\ntraining, and it is then generalized to the online setting with a procedure\ntermed learn-and-adapt. The novel resource allocation protocol permeates\nbenefits of stochastic approximation and statistical learning to obtain\nlow-complexity online updates with learning errors close to the statistical\naccuracy limits, while still preserving adaptation performance, which in the\nstochastic network optimization context guarantees queue stability. Analysis\nand simulated tests demonstrate that the proposed data-driven approach improves\nthe delay and convergence performance of existing resource allocation schemes.\n", "versions": [{"version": "v1", "created": "Fri, 7 Oct 2016 05:11:23 GMT"}, {"version": "v2", "created": "Sun, 26 Feb 2017 06:31:20 GMT"}], "update_date": "2017-05-24", "authors_parsed": [["Chen", "Tianyi", ""], ["Mokhtari", "Aryan", ""], ["Wang", "Xin", ""], ["Ribeiro", "Alejandro", ""], ["Giannakis", "Georgios B.", ""]]}, {"id": "1610.02164", "submitter": "Danijar Hafner", "authors": "Danijar Hafner", "title": "Deep Reinforcement Learning From Raw Pixels in Doom", "comments": "Bachelor's thesis", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Using current reinforcement learning methods, it has recently become possible\nto learn to play unknown 3D games from raw pixels. In this work, we study the\nchallenges that arise in such complex environments, and summarize current\nmethods to approach these. We choose a task within the Doom game, that has not\nbeen approached yet. The goal for the agent is to fight enemies in a 3D world\nconsisting of five rooms. We train the DQN and LSTM-A3C algorithms on this\ntask. Results show that both algorithms learn sensible policies, but fail to\nachieve high scores given the amount of training. We provide insights into the\nlearned behavior, which can serve as a valuable starting point for further\nresearch in the Doom domain.\n", "versions": [{"version": "v1", "created": "Fri, 7 Oct 2016 07:07:47 GMT"}], "update_date": "2016-10-10", "authors_parsed": [["Hafner", "Danijar", ""]]}, {"id": "1610.02242", "submitter": "Samuli Laine", "authors": "Samuli Laine, Timo Aila", "title": "Temporal Ensembling for Semi-Supervised Learning", "comments": "Final ICLR 2017 version. Includes new results for CIFAR-100 with\n  additional unlabeled data from Tiny Images dataset", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present a simple and efficient method for training deep\nneural networks in a semi-supervised setting where only a small portion of\ntraining data is labeled. We introduce self-ensembling, where we form a\nconsensus prediction of the unknown labels using the outputs of the\nnetwork-in-training on different epochs, and most importantly, under different\nregularization and input augmentation conditions. This ensemble prediction can\nbe expected to be a better predictor for the unknown labels than the output of\nthe network at the most recent training epoch, and can thus be used as a target\nfor training. Using our method, we set new records for two standard\nsemi-supervised learning benchmarks, reducing the (non-augmented)\nclassification error rate from 18.44% to 7.05% in SVHN with 500 labels and from\n18.63% to 16.55% in CIFAR-10 with 4000 labels, and further to 5.12% and 12.16%\nby enabling the standard augmentations. We additionally obtain a clear\nimprovement in CIFAR-100 classification accuracy by using random images from\nthe Tiny Images dataset as unlabeled extra inputs during training. Finally, we\ndemonstrate good tolerance to incorrect labels.\n", "versions": [{"version": "v1", "created": "Fri, 7 Oct 2016 12:15:42 GMT"}, {"version": "v2", "created": "Mon, 7 Nov 2016 13:27:40 GMT"}, {"version": "v3", "created": "Wed, 15 Mar 2017 14:22:41 GMT"}], "update_date": "2017-03-16", "authors_parsed": [["Laine", "Samuli", ""], ["Aila", "Timo", ""]]}, {"id": "1610.02273", "submitter": "Hyeokjun Choe", "authors": "Hyeokjun Choe, Seil Lee, Hyunha Nam, Seongsik Park, Seijoon Kim,\n  Eui-Young Chung, Sungroh Yoon", "title": "Near-Data Processing for Differentiable Machine Learning Models", "comments": "12 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR cs.DC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Near-data processing (NDP) refers to augmenting memory or storage with\nprocessing power. Despite its potential for acceleration computing and reducing\npower requirements, only limited progress has been made in popularizing NDP for\nvarious reasons. Recently, two major changes have occurred that have ignited\nrenewed interest and caused a resurgence of NDP. The first is the success of\nmachine learning (ML), which often demands a great deal of computation for\ntraining, requiring frequent transfers of big data. The second is the\npopularity of NAND flash-based solid-state drives (SSDs) containing multicore\nprocessors that can accommodate extra computation for data processing. In this\npaper, we evaluate the potential of NDP for ML using a new SSD platform that\nallows us to simulate instorage processing (ISP) of ML workloads. Our platform\n(named ISP-ML) is a full-fledged simulator of a realistic multi-channel SSD\nthat can execute various ML algorithms using data stored in the SSD. To conduct\na thorough performance analysis and an in-depth comparison with alternative\ntechniques, we focus on a specific algorithm: stochastic gradient descent\n(SGD), which is the de facto standard for training differentiable models such\nas logistic regression and neural networks. We implement and compare three SGD\nvariants (synchronous, Downpour, and elastic averaging) using ISP-ML,\nexploiting the multiple NAND channels to parallelize SGD. In addition, we\ncompare the performance of ISP and that of conventional in-host processing,\nrevealing the advantages of ISP. Based on the advantages and limitations\nidentified through our experiments, we further discuss directions for future\nresearch on ISP for accelerating ML.\n", "versions": [{"version": "v1", "created": "Thu, 6 Oct 2016 05:28:33 GMT"}, {"version": "v2", "created": "Wed, 9 Nov 2016 01:58:47 GMT"}, {"version": "v3", "created": "Fri, 28 Apr 2017 03:57:56 GMT"}], "update_date": "2017-05-01", "authors_parsed": [["Choe", "Hyeokjun", ""], ["Lee", "Seil", ""], ["Nam", "Hyunha", ""], ["Park", "Seongsik", ""], ["Kim", "Seijoon", ""], ["Chung", "Eui-Young", ""], ["Yoon", "Sungroh", ""]]}, {"id": "1610.02281", "submitter": "Jason T. L. Wang", "authors": "Ling Zhong and Jason T. L. Wang", "title": "Effective Classification of MicroRNA Precursors Using Combinatorial\n  Feature Mining and AdaBoost Algorithms", "comments": "26 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.GN cs.CE cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  MicroRNAs (miRNAs) are non-coding RNAs with approximately 22 nucleotides (nt)\nthat are derived from precursor molecules. These precursor molecules or\npre-miRNAs often fold into stem-loop hairpin structures. However, a large\nnumber of sequences with pre-miRNA-like hairpins can be found in genomes. It is\na challenge to distinguish the real pre-miRNAs from other hairpin sequences\nwith similar stem-loops (referred to as pseudo pre-miRNAs). Several\ncomputational methods have been developed to tackle this challenge. In this\npaper we propose a new method, called MirID, for identifying and classifying\nmicroRNA precursors. We collect 74 features from the sequences and secondary\nstructures of pre-miRNAs; some of these features are taken from our previous\nstudies on non-coding RNA prediction while others were suggested in the\nliterature. We develop a combinatorial feature mining algorithm to identify\nsuitable feature sets. These feature sets are then used to train support vector\nmachines to obtain classification models, based on which classifier ensemble is\nconstructed. Finally we use an AdaBoost algorithm to further enhance the\naccuracy of the classifier ensemble. Experimental results on a variety of\nspecies demonstrate the good performance of the proposed method, and its\nsuperiority over existing tools.\n", "versions": [{"version": "v1", "created": "Thu, 6 Oct 2016 04:35:37 GMT"}], "update_date": "2016-10-10", "authors_parsed": [["Zhong", "Ling", ""], ["Wang", "Jason T. L.", ""]]}, {"id": "1610.02348", "submitter": "Mohamad Ivan Fanany", "authors": "Arif Budiman, Mohamad Ivan Fanany, Chan Basaruddin", "title": "Adaptive Convolutional ELM For Concept Drift Handling in Online Stream\n  Data", "comments": "Submitted to IEEE Transactions on Systems, Man and Cybernetics:\n  Systems. Special Issue on Efficient and Rapid Machine Learning Algorithms for\n  Big Data and Dynamic Varying Systems", "journal-ref": null, "doi": null, "report-no": "SMCA-16-09-1038", "categories": "cs.AI cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In big data era, the data continuously generated and its distribution may\nkeep changes overtime. These challenges in online stream of data are known as\nconcept drift. In this paper, we proposed the Adaptive Convolutional ELM method\n(ACNNELM) as enhancement of Convolutional Neural Network (CNN) with a hybrid\nExtreme Learning Machine (ELM) model plus adaptive capability. This method is\naimed for concept drift handling. We enhanced the CNN as convolutional\nhiererchical features representation learner combined with Elastic ELM\n(E$^2$LM) as a parallel supervised classifier. We propose an Adaptive OS-ELM\n(AOS-ELM) for concept drift adaptability in classifier level (named ACNNELM-1)\nand matrices concatenation ensembles for concept drift adaptability in ensemble\nlevel (named ACNNELM-2). Our proposed Adaptive CNNELM is flexible that works\nwell in classifier level and ensemble level while most current methods only\nproposed to work on either one of the levels.\n  We verified our method in extended MNIST data set and not MNIST data set. We\nset the experiment to simulate virtual drift, real drift, and hybrid drift\nevent and we demonstrated how our CNNELM adaptability works. Our proposed\nmethod works well and gives better accuracy, computation scalability, and\nconcept drifts adaptability compared to the regular ELM and CNN. Further\nresearches are still required to study the optimum parameters and to use more\nvaried image data set.\n", "versions": [{"version": "v1", "created": "Fri, 7 Oct 2016 16:53:09 GMT"}], "update_date": "2016-10-10", "authors_parsed": [["Budiman", "Arif", ""], ["Fanany", "Mohamad Ivan", ""], ["Basaruddin", "Chan", ""]]}, {"id": "1610.02373", "submitter": "Mohamad Ivan Fanany", "authors": "Arif Budiman, Mohamad Ivan Fanany, Chan Basaruddin", "title": "Distributed Averaging CNN-ELM for Big Data", "comments": "Submitted to IEEE Transactions on Systems, Man and Cybernetics:\n  Systems", "journal-ref": null, "doi": null, "report-no": "SMCA-16-09-1039", "categories": "cs.LG cs.CV cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Increasing the scalability of machine learning to handle big volume of data\nis a challenging task. The scale up approach has some limitations. In this\npaper, we proposed a scale out approach for CNN-ELM based on MapReduce on\nclassifier level. Map process is the CNN-ELM training for certain partition of\ndata. It involves many CNN-ELM models that can be trained asynchronously.\nReduce process is the averaging of all CNN-ELM weights as final training\nresult. This approach can save a lot of training time than single CNN-ELM\nmodels trained alone. This approach also increased the scalability of machine\nlearning by combining scale out and scale up approaches. We verified our method\nin extended MNIST data set and not-MNIST data set experiment. However, it has\nsome drawbacks by additional iteration learning parameters that need to be\ncarefully taken and training data distribution that need to be carefully\nselected. Further researches to use more complex image data set are required.\n", "versions": [{"version": "v1", "created": "Fri, 7 Oct 2016 18:59:23 GMT"}], "update_date": "2016-10-10", "authors_parsed": [["Budiman", "Arif", ""], ["Fanany", "Mohamad Ivan", ""], ["Basaruddin", "Chan", ""]]}, {"id": "1610.02391", "submitter": "Ramprasaath R. Selvaraju", "authors": "Ramprasaath R. Selvaraju, Michael Cogswell, Abhishek Das, Ramakrishna\n  Vedantam, Devi Parikh, Dhruv Batra", "title": "Grad-CAM: Visual Explanations from Deep Networks via Gradient-based\n  Localization", "comments": "This version was published in International Journal of Computer\n  Vision (IJCV) in 2019; A previous version of the paper was published at\n  International Conference on Computer Vision (ICCV'17)", "journal-ref": null, "doi": "10.1007/s11263-019-01228-7", "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a technique for producing \"visual explanations\" for decisions from\na large class of CNN-based models, making them more transparent. Our approach -\nGradient-weighted Class Activation Mapping (Grad-CAM), uses the gradients of\nany target concept, flowing into the final convolutional layer to produce a\ncoarse localization map highlighting important regions in the image for\npredicting the concept. Grad-CAM is applicable to a wide variety of CNN\nmodel-families: (1) CNNs with fully-connected layers, (2) CNNs used for\nstructured outputs, (3) CNNs used in tasks with multimodal inputs or\nreinforcement learning, without any architectural changes or re-training. We\ncombine Grad-CAM with fine-grained visualizations to create a high-resolution\nclass-discriminative visualization and apply it to off-the-shelf image\nclassification, captioning, and visual question answering (VQA) models,\nincluding ResNet-based architectures. In the context of image classification\nmodels, our visualizations (a) lend insights into their failure modes, (b) are\nrobust to adversarial images, (c) outperform previous methods on localization,\n(d) are more faithful to the underlying model and (e) help achieve\ngeneralization by identifying dataset bias. For captioning and VQA, we show\nthat even non-attention based models can localize inputs. We devise a way to\nidentify important neurons through Grad-CAM and combine it with neuron names to\nprovide textual explanations for model decisions. Finally, we design and\nconduct human studies to measure if Grad-CAM helps users establish appropriate\ntrust in predictions from models and show that Grad-CAM helps untrained users\nsuccessfully discern a 'stronger' nodel from a 'weaker' one even when both make\nidentical predictions. Our code is available at\nhttps://github.com/ramprs/grad-cam/, along with a demo at\nhttp://gradcam.cloudcv.org, and a video at youtu.be/COjUB9Izk6E.\n", "versions": [{"version": "v1", "created": "Fri, 7 Oct 2016 19:54:24 GMT"}, {"version": "v2", "created": "Fri, 30 Dec 2016 07:19:35 GMT"}, {"version": "v3", "created": "Tue, 21 Mar 2017 23:48:00 GMT"}, {"version": "v4", "created": "Tue, 3 Dec 2019 02:13:03 GMT"}], "update_date": "2019-12-04", "authors_parsed": [["Selvaraju", "Ramprasaath R.", ""], ["Cogswell", "Michael", ""], ["Das", "Abhishek", ""], ["Vedantam", "Ramakrishna", ""], ["Parikh", "Devi", ""], ["Batra", "Dhruv", ""]]}, {"id": "1610.02413", "submitter": "Moritz Hardt", "authors": "Moritz Hardt and Eric Price and Nathan Srebro", "title": "Equality of Opportunity in Supervised Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a criterion for discrimination against a specified sensitive\nattribute in supervised learning, where the goal is to predict some target\nbased on available features. Assuming data about the predictor, target, and\nmembership in the protected group are available, we show how to optimally\nadjust any learned predictor so as to remove discrimination according to our\ndefinition. Our framework also improves incentives by shifting the cost of poor\nclassification from disadvantaged groups to the decision maker, who can respond\nby improving the classification accuracy.\n  In line with other studies, our notion is oblivious: it depends only on the\njoint statistics of the predictor, the target and the protected attribute, but\nnot on interpretation of individualfeatures. We study the inherent limits of\ndefining and identifying biases based on such oblivious measures, outlining\nwhat can and cannot be inferred from different oblivious tests.\n  We illustrate our notion using a case study of FICO credit scores.\n", "versions": [{"version": "v1", "created": "Fri, 7 Oct 2016 20:16:29 GMT"}], "update_date": "2016-10-11", "authors_parsed": [["Hardt", "Moritz", ""], ["Price", "Eric", ""], ["Srebro", "Nathan", ""]]}, {"id": "1610.02415", "submitter": "Jennifer N. Wei", "authors": "Rafael G\\'omez-Bombarelli, Jennifer N. Wei, David Duvenaud, Jos\\'e\n  Miguel Hern\\'andez-Lobato, Benjam\\'in S\\'anchez-Lengeling, Dennis Sheberla,\n  Jorge Aguilera-Iparraguirre, Timothy D. Hirzel, Ryan P. Adams and Al\\'an\n  Aspuru-Guzik", "title": "Automatic chemical design using a data-driven continuous representation\n  of molecules", "comments": "26 pages, 8 figures", "journal-ref": null, "doi": "10.1021/acscentsci.7b00572", "report-no": null, "categories": "cs.LG physics.chem-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We report a method to convert discrete representations of molecules to and\nfrom a multidimensional continuous representation. This model allows us to\ngenerate new molecules for efficient exploration and optimization through\nopen-ended spaces of chemical compounds. A deep neural network was trained on\nhundreds of thousands of existing chemical structures to construct three\ncoupled functions: an encoder, a decoder and a predictor. The encoder converts\nthe discrete representation of a molecule into a real-valued continuous vector,\nand the decoder converts these continuous vectors back to discrete molecular\nrepresentations. The predictor estimates chemical properties from the latent\ncontinuous vector representation of the molecule. Continuous representations\nallow us to automatically generate novel chemical structures by performing\nsimple operations in the latent space, such as decoding random vectors,\nperturbing known chemical structures, or interpolating between molecules.\nContinuous representations also allow the use of powerful gradient-based\noptimization to efficiently guide the search for optimized functional\ncompounds. We demonstrate our method in the domain of drug-like molecules and\nalso in the set of molecules with fewer that nine heavy atoms.\n", "versions": [{"version": "v1", "created": "Fri, 7 Oct 2016 20:26:34 GMT"}, {"version": "v2", "created": "Fri, 6 Jan 2017 20:48:48 GMT"}, {"version": "v3", "created": "Tue, 5 Dec 2017 06:22:03 GMT"}], "update_date": "2018-02-20", "authors_parsed": [["G\u00f3mez-Bombarelli", "Rafael", ""], ["Wei", "Jennifer N.", ""], ["Duvenaud", "David", ""], ["Hern\u00e1ndez-Lobato", "Jos\u00e9 Miguel", ""], ["S\u00e1nchez-Lengeling", "Benjam\u00edn", ""], ["Sheberla", "Dennis", ""], ["Aguilera-Iparraguirre", "Jorge", ""], ["Hirzel", "Timothy D.", ""], ["Adams", "Ryan P.", ""], ["Aspuru-Guzik", "Al\u00e1n", ""]]}, {"id": "1610.02483", "submitter": "Wan-Lei Zhao", "authors": "Wan-Lei Zhao, Cheng-Hao Deng, Chong-Wah Ngo", "title": "Boost K-Means", "comments": "11 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Due to its simplicity and versatility, k-means remains popular since it was\nproposed three decades ago. The performance of k-means has been enhanced from\ndifferent perspectives over the years. Unfortunately, a good trade-off between\nquality and efficiency is hardly reached. In this paper, a novel k-means\nvariant is presented. Different from most of k-means variants, the clustering\nprocedure is driven by an explicit objective function, which is feasible for\nthe whole l2-space. The classic egg-chicken loop in k-means has been simplified\nto a pure stochastic optimization procedure. The procedure of k-means becomes\nsimpler and converges to a considerably better local optima. The effectiveness\nof this new variant has been studied extensively in different contexts, such as\ndocument clustering, nearest neighbor search and image clustering. Superior\nperformance is observed across different scenarios.\n", "versions": [{"version": "v1", "created": "Sat, 8 Oct 2016 04:36:42 GMT"}, {"version": "v2", "created": "Sun, 4 Dec 2016 07:32:37 GMT"}], "update_date": "2016-12-06", "authors_parsed": [["Zhao", "Wan-Lei", ""], ["Deng", "Cheng-Hao", ""], ["Ngo", "Chong-Wah", ""]]}, {"id": "1610.02496", "submitter": "Kaiwei Li", "authors": "Kaiwei Li, Jianfei Chen, Wenguang Chen, Jun Zhu", "title": "SaberLDA: Sparsity-Aware Learning of Topic Models on GPUs", "comments": "13 pages, 12 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.IR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Latent Dirichlet Allocation (LDA) is a popular tool for analyzing discrete\ncount data such as text and images. Applications require LDA to handle both\nlarge datasets and a large number of topics. Though distributed CPU systems\nhave been used, GPU-based systems have emerged as a promising alternative\nbecause of the high computational power and memory bandwidth of GPUs. However,\nexisting GPU-based LDA systems cannot support a large number of topics because\nthey use algorithms on dense data structures whose time and space complexity is\nlinear to the number of topics. In this paper, we propose SaberLDA, a GPU-based\nLDA system that implements a sparsity-aware algorithm to achieve sublinear time\ncomplexity and scales well to learn a large number of topics. To address the\nchallenges introduced by sparsity, we propose a novel data layout, a new\nwarp-based sampling kernel, and an efficient sparse count matrix updating\nalgorithm that improves locality, makes efficient utilization of GPU warps, and\nreduces memory consumption. Experiments show that SaberLDA can learn from\nbillions-token-scale data with up to 10,000 topics, which is almost two orders\nof magnitude larger than that of the previous GPU-based systems. With a single\nGPU card, SaberLDA is able to learn 10,000 topics from a dataset of billions of\ntokens in a few hours, which is only achievable with clusters with tens of\nmachines before.\n", "versions": [{"version": "v1", "created": "Sat, 8 Oct 2016 07:57:00 GMT"}, {"version": "v2", "created": "Wed, 12 Oct 2016 12:39:07 GMT"}], "update_date": "2016-10-13", "authors_parsed": [["Li", "Kaiwei", ""], ["Chen", "Jianfei", ""], ["Chen", "Wenguang", ""], ["Zhu", "Jun", ""]]}, {"id": "1610.02501", "submitter": "Xinggang Wang", "authors": "Xinggang Wang, Yongluan Yan, Peng Tang, Xiang Bai, Wenyu Liu", "title": "Revisiting Multiple Instance Neural Networks", "comments": null, "journal-ref": null, "doi": "10.1016/j.patcog.2017.08.026", "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently neural networks and multiple instance learning are both attractive\ntopics in Artificial Intelligence related research fields. Deep neural networks\nhave achieved great success in supervised learning problems, and multiple\ninstance learning as a typical weakly-supervised learning method is effective\nfor many applications in computer vision, biometrics, nature language\nprocessing, etc. In this paper, we revisit the problem of solving multiple\ninstance learning problems using neural networks. Neural networks are appealing\nfor solving multiple instance learning problem. The multiple instance neural\nnetworks perform multiple instance learning in an end-to-end way, which take a\nbag with various number of instances as input and directly output bag label.\nAll of the parameters in a multiple instance network are able to be optimized\nvia back-propagation. We propose a new multiple instance neural network to\nlearn bag representations, which is different from the existing multiple\ninstance neural networks that focus on estimating instance label. In addition,\nrecent tricks developed in deep learning have been studied in multiple instance\nnetworks, we find deep supervision is effective for boosting bag classification\naccuracy. In the experiments, the proposed multiple instance networks achieve\nstate-of-the-art or competitive performance on several MIL benchmarks.\nMoreover, it is extremely fast for both testing and training, e.g., it takes\nonly 0.0003 second to predict a bag and a few seconds to train on a MIL\ndatasets on a moderate CPU.\n", "versions": [{"version": "v1", "created": "Sat, 8 Oct 2016 08:57:36 GMT"}], "update_date": "2020-04-08", "authors_parsed": [["Wang", "Xinggang", ""], ["Yan", "Yongluan", ""], ["Tang", "Peng", ""], ["Bai", "Xiang", ""], ["Liu", "Wenyu", ""]]}, {"id": "1610.02527", "submitter": "Jakub Kone\\v{c}n\\'y", "authors": "Jakub Kone\\v{c}n\\'y, H. Brendan McMahan, Daniel Ramage, Peter\n  Richt\\'arik", "title": "Federated Optimization: Distributed Machine Learning for On-Device\n  Intelligence", "comments": "38 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a new and increasingly relevant setting for distributed\noptimization in machine learning, where the data defining the optimization are\nunevenly distributed over an extremely large number of nodes. The goal is to\ntrain a high-quality centralized model. We refer to this setting as Federated\nOptimization. In this setting, communication efficiency is of the utmost\nimportance and minimizing the number of rounds of communication is the\nprincipal goal.\n  A motivating example arises when we keep the training data locally on users'\nmobile devices instead of logging it to a data center for training. In\nfederated optimziation, the devices are used as compute nodes performing\ncomputation on their local data in order to update a global model. We suppose\nthat we have extremely large number of devices in the network --- as many as\nthe number of users of a given service, each of which has only a tiny fraction\nof the total data available. In particular, we expect the number of data points\navailable locally to be much smaller than the number of devices. Additionally,\nsince different users generate data with different patterns, it is reasonable\nto assume that no device has a representative sample of the overall\ndistribution.\n  We show that existing algorithms are not suitable for this setting, and\npropose a new algorithm which shows encouraging experimental results for sparse\nconvex problems. This work also sets a path for future research needed in the\ncontext of \\federated optimization.\n", "versions": [{"version": "v1", "created": "Sat, 8 Oct 2016 13:25:15 GMT"}], "update_date": "2016-10-11", "authors_parsed": [["Kone\u010dn\u00fd", "Jakub", ""], ["McMahan", "H. Brendan", ""], ["Ramage", "Daniel", ""], ["Richt\u00e1rik", "Peter", ""]]}, {"id": "1610.02583", "submitter": "Gang Chen", "authors": "Gang Chen", "title": "A Gentle Tutorial of Recurrent Neural Network with Error Backpropagation", "comments": "9 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  We describe recurrent neural networks (RNNs), which have attracted great\nattention on sequential tasks, such as handwriting recognition, speech\nrecognition and image to text. However, compared to general feedforward neural\nnetworks, RNNs have feedback loops, which makes it a little hard to understand\nthe backpropagation step. Thus, we focus on basics, especially the error\nbackpropagation to compute gradients with respect to model parameters. Further,\nwe go into detail on how error backpropagation algorithm is applied on long\nshort-term memory (LSTM) by unfolding the memory unit.\n", "versions": [{"version": "v1", "created": "Sat, 8 Oct 2016 21:10:40 GMT"}, {"version": "v2", "created": "Tue, 17 Oct 2017 04:48:14 GMT"}, {"version": "v3", "created": "Sun, 14 Jan 2018 05:42:17 GMT"}], "update_date": "2018-01-16", "authors_parsed": [["Chen", "Gang", ""]]}, {"id": "1610.02649", "submitter": "Muhammad Yousefnezhad", "authors": "Muhammad Yousefnezhad, Ali Reihanian, Daoqiang Zhang, Behrouz\n  Minaei-Bidgoli", "title": "A new selection strategy for selective cluster ensemble based on\n  Diversity and Independency", "comments": "Accepted in Engineering Applications of Artificial Intelligence\n  (EAAI) Journal", "journal-ref": null, "doi": "10.1016/j.engappai.2016.10.005", "report-no": null, "categories": "stat.ML cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This research introduces a new strategy in cluster ensemble selection by\nusing Independency and Diversity metrics. In recent years, Diversity and\nQuality, which are two metrics in evaluation procedure, have been used for\nselecting basic clustering results in the cluster ensemble selection. Although\nquality can improve the final results in cluster ensemble, it cannot control\nthe procedures of generating basic results, which causes a gap in prediction of\nthe generated basic results' accuracy. Instead of quality, this paper\nintroduces Independency as a supplementary method to be used in conjunction\nwith Diversity. Therefore, this paper uses a heuristic metric, which is based\non the procedure of converting code to graph in Software Testing, in order to\ncalculate the Independency of two basic clustering algorithms. Moreover, a new\nmodeling language, which we called as \"Clustering Algorithms Independency\nLanguage\" (CAIL), is introduced in order to generate graphs which depict\nIndependency of algorithms. Also, Uniformity, which is a new similarity metric,\nhas been introduced for evaluating the diversity of basic results. As a\ncredential, our experimental results on varied different standard data sets\nshow that the proposed framework improves the accuracy of final results\ndramatically in comparison with other cluster ensemble methods.\n", "versions": [{"version": "v1", "created": "Sun, 9 Oct 2016 09:28:01 GMT"}], "update_date": "2016-10-11", "authors_parsed": [["Yousefnezhad", "Muhammad", ""], ["Reihanian", "Ali", ""], ["Zhang", "Daoqiang", ""], ["Minaei-Bidgoli", "Behrouz", ""]]}, {"id": "1610.02757", "submitter": "Maxime Voisin", "authors": "Maxime Voisin, Leo Dreyfus-Schmidt, Pierre Gutierrez, Samuel Ronsin\n  and Marc Beillevaire", "title": "Dataiku's Solution to SPHERE's Activity Recognition Challenge", "comments": "5 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Our team won the second prize of the Safe Aging with SPHERE Challenge\norganized by SPHERE, in conjunction with ECML-PKDD and Driven Data. The goal of\nthe competition was to recognize activities performed by humans, using sensor\ndata. This paper presents our solution. It is based on a rich pre-processing\nand state of the art machine learning methods. From the raw train data, we\ngenerate a synthetic train set with the same statistical characteristics as the\ntest set. We then perform feature engineering. The machine learning modeling\npart is based on stacking weak learners through a grid searched XGBoost\nalgorithm. Finally, we use post-processing to smooth our predictions over time.\n", "versions": [{"version": "v1", "created": "Mon, 10 Oct 2016 02:52:21 GMT"}], "update_date": "2016-10-11", "authors_parsed": [["Voisin", "Maxime", ""], ["Dreyfus-Schmidt", "Leo", ""], ["Gutierrez", "Pierre", ""], ["Ronsin", "Samuel", ""], ["Beillevaire", "Marc", ""]]}, {"id": "1610.02807", "submitter": "Qian Wan", "authors": "Qian Wan, Huiping Duan, Jun Fang, Hongbin Li", "title": "Robust Bayesian Compressed sensing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of robust compressed sensing whose objective is to\nrecover a high-dimensional sparse signal from compressed measurements corrupted\nby outliers. A new sparse Bayesian learning method is developed for robust\ncompressed sensing. The basic idea of the proposed method is to identify and\nremove the outliers from sparse signal recovery. To automatically identify the\noutliers, we employ a set of binary indicator hyperparameters to indicate which\nobservations are outliers. These indicator hyperparameters are treated as\nrandom variables and assigned a beta process prior such that their values are\nconfined to be binary. In addition, a Gaussian-inverse Gamma prior is imposed\non the sparse signal to promote sparsity. Based on this hierarchical prior\nmodel, we develop a variational Bayesian method to estimate the indicator\nhyperparameters as well as the sparse signal. Simulation results show that the\nproposed method achieves a substantial performance improvement over existing\nrobust compressed sensing techniques.\n", "versions": [{"version": "v1", "created": "Mon, 10 Oct 2016 08:53:01 GMT"}, {"version": "v2", "created": "Fri, 21 Oct 2016 08:13:25 GMT"}], "update_date": "2016-10-24", "authors_parsed": [["Wan", "Qian", ""], ["Duan", "Huiping", ""], ["Fang", "Jun", ""], ["Li", "Hongbin", ""]]}, {"id": "1610.02828", "submitter": "Jobin Wilson", "authors": "Jobin Wilson, Ram Mohan, Muhammad Arif, Santanu Chaudhury, Brejesh\n  Lall", "title": "Ranking academic institutions on potential paper acceptance in upcoming\n  conferences", "comments": "KDD 2016, KDD Cup 2016, Appeared in the KDD Cup Workshop\n  2016,https://kddcup2016.azurewebsites.net/Workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.DL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The crux of the problem in KDD Cup 2016 involves developing data mining\ntechniques to rank research institutions based on publications. Rank importance\nof research institutions are derived from predictions on the number of full\nresearch papers that would potentially get accepted in upcoming top-tier\nconferences, utilizing public information on the web. This paper describes our\nsolution to KDD Cup 2016. We used a two step approach in which we first\nidentify full research papers corresponding to each conference of interest and\nthen train two variants of exponential smoothing models to make predictions.\nOur solution achieves an overall score of 0.7508, while the winning submission\nscored 0.7656 in the overall results.\n", "versions": [{"version": "v1", "created": "Mon, 10 Oct 2016 09:55:14 GMT"}], "update_date": "2016-10-11", "authors_parsed": [["Wilson", "Jobin", ""], ["Mohan", "Ram", ""], ["Arif", "Muhammad", ""], ["Chaudhury", "Santanu", ""], ["Lall", "Brejesh", ""]]}, {"id": "1610.02876", "submitter": "Niek Tax", "authors": "Niek Tax, Natalia Sidorova, Wil M. P. van der Aalst, Reinder Haakma", "title": "Heuristic Approaches for Generating Local Process Models through Log\n  Projections", "comments": "paper accepted and to appear in the proceedings of the IEEE Symposium\n  on Computational Intelligence and Data Mining (CIDM), special session on\n  Process Mining, part of the Symposium Series on Computational Intelligence\n  (SSCI)", "journal-ref": "Proceedings of the IEEE Symposium Series on Computational\n  Intelligence (SSCI), (2016) 1-8", "doi": "10.1109/SSCI.2016.7849948", "report-no": null, "categories": "cs.LG cs.AI cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Local Process Model (LPM) discovery is focused on the mining of a set of\nprocess models where each model describes the behavior represented in the event\nlog only partially, i.e. subsets of possible events are taken into account to\ncreate so-called local process models. Often such smaller models provide\nvaluable insights into the behavior of the process, especially when no adequate\nand comprehensible single overall process model exists that is able to describe\nthe traces of the process from start to end. The practical application of LPM\ndiscovery is however hindered by computational issues in the case of logs with\nmany activities (problems may already occur when there are more than 17 unique\nactivities). In this paper, we explore three heuristics to discover subsets of\nactivities that lead to useful log projections with the goal of speeding up LPM\ndiscovery considerably while still finding high-quality LPMs. We found that a\nMarkov clustering approach to create projection sets results in the largest\nimprovement of execution time, with discovered LPMs still being better than\nwith the use of randomly generated activity sets of the same size. Another\nheuristic, based on log entropy, yields a more moderate speedup, but enables\nthe discovery of higher quality LPMs. The third heuristic, based on the\nrelative information gain, shows unstable performance: for some data sets the\nspeedup and LPM quality are higher than with the log entropy based method,\nwhile for other data sets there is no speedup at all.\n", "versions": [{"version": "v1", "created": "Mon, 10 Oct 2016 12:12:46 GMT"}], "update_date": "2017-12-20", "authors_parsed": [["Tax", "Niek", ""], ["Sidorova", "Natalia", ""], ["van der Aalst", "Wil M. P.", ""], ["Haakma", "Reinder", ""]]}, {"id": "1610.02891", "submitter": "Kaixiang Mo", "authors": "Kaixiang Mo, Shuangyin Li, Yu Zhang, Jiajun Li, Qiang Yang", "title": "Personalizing a Dialogue System with Transfer Reinforcement Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is difficult to train a personalized task-oriented dialogue system because\nthe data collected from each individual is often insufficient. Personalized\ndialogue systems trained on a small dataset can overfit and make it difficult\nto adapt to different user needs. One way to solve this problem is to consider\na collection of multiple users' data as a source domain and an individual\nuser's data as a target domain, and to perform a transfer learning from the\nsource to the target domain. By following this idea, we propose\n\"PETAL\"(PErsonalized Task-oriented diALogue), a transfer-learning framework\nbased on POMDP to learn a personalized dialogue system. The system first learns\ncommon dialogue knowledge from the source domain and then adapts this knowledge\nto the target user. This framework can avoid the negative transfer problem by\nconsidering differences between source and target users. The policy in the\npersonalized POMDP can learn to choose different actions appropriately for\ndifferent users. Experimental results on a real-world coffee-shopping data and\nsimulation data show that our personalized dialogue system can choose different\noptimal actions for different users, and thus effectively improve the dialogue\nquality under the personalized setting.\n", "versions": [{"version": "v1", "created": "Mon, 10 Oct 2016 12:51:05 GMT"}, {"version": "v2", "created": "Mon, 17 Oct 2016 14:08:42 GMT"}, {"version": "v3", "created": "Fri, 26 May 2017 14:05:07 GMT"}], "update_date": "2017-05-29", "authors_parsed": [["Mo", "Kaixiang", ""], ["Li", "Shuangyin", ""], ["Zhang", "Yu", ""], ["Li", "Jiajun", ""], ["Yang", "Qiang", ""]]}, {"id": "1610.02906", "submitter": "Xiaofei Sun", "authors": "Xiaofei Sun, Jiang Guo, Xiao Ding and Ting Liu", "title": "A General Framework for Content-enhanced Network Representation Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper investigates the problem of network embedding, which aims at\nlearning low-dimensional vector representation of nodes in networks. Most\nexisting network embedding methods rely solely on the network structure, i.e.,\nthe linkage relationships between nodes, but ignore the rich content\ninformation associated with it, which is common in real world networks and\nbeneficial to describing the characteristics of a node. In this paper, we\npropose content-enhanced network embedding (CENE), which is capable of jointly\nleveraging the network structure and the content information. Our approach\nintegrates text modeling and structure modeling in a general framework by\ntreating the content information as a special kind of node. Experiments on\nseveral real world net- works with application to node classification show that\nour models outperform all existing network embedding methods, demonstrating the\nmerits of content information and joint learning.\n", "versions": [{"version": "v1", "created": "Mon, 10 Oct 2016 13:27:01 GMT"}, {"version": "v2", "created": "Tue, 11 Oct 2016 02:06:55 GMT"}, {"version": "v3", "created": "Mon, 17 Oct 2016 13:55:02 GMT"}], "update_date": "2016-10-19", "authors_parsed": [["Sun", "Xiaofei", ""], ["Guo", "Jiang", ""], ["Ding", "Xiao", ""], ["Liu", "Ting", ""]]}, {"id": "1610.02967", "submitter": "Soeren Laue", "authors": "Joachim Giesen and S\\\"oren Laue", "title": "Distributed Convex Optimization with Many Convex Constraints", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.LG cs.NA stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We address the problem of solving convex optimization problems with many\nconvex constraints in a distributed setting. Our approach is based on an\nextension of the alternating direction method of multipliers (ADMM) that\nrecently gained a lot of attention in the Big Data context. Although it has\nbeen invented decades ago, ADMM so far can be applied only to unconstrained\nproblems and problems with linear equality or inequality constraints. Our\nextension can handle arbitrary inequality constraints directly. It combines the\nability of ADMM to solve convex optimization problems in a distributed setting\nwith the ability of the Augmented Lagrangian method to solve constrained\noptimization problems, and as we show, it inherits the convergence guarantees\nof ADMM and the Augmented Lagrangian method.\n", "versions": [{"version": "v1", "created": "Fri, 7 Oct 2016 12:59:46 GMT"}, {"version": "v2", "created": "Fri, 6 Apr 2018 10:52:50 GMT"}], "update_date": "2018-04-09", "authors_parsed": [["Giesen", "Joachim", ""], ["Laue", "S\u00f6ren", ""]]}, {"id": "1610.02987", "submitter": "Jelena Bradic", "authors": "Yinchu Zhu and Jelena Bradic", "title": "Linear Hypothesis Testing in Dense High-Dimensional Linear Models", "comments": "42 pages, 8 figures", "journal-ref": "Journal of the American Statistical Association: theory and\n  methods, 2017", "doi": "10.1080/01621459.2017.1356319", "report-no": null, "categories": "stat.ME cs.LG math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a methodology for testing linear hypothesis in high-dimensional\nlinear models. The proposed test does not impose any restriction on the size of\nthe model, i.e. model sparsity or the loading vector representing the\nhypothesis. Providing asymptotically valid methods for testing general linear\nfunctions of the regression parameters in high-dimensions is extremely\nchallenging -- especially without making restrictive or unverifiable\nassumptions on the number of non-zero elements. We propose to test the moment\nconditions related to the newly designed restructured regression, where the\ninputs are transformed and augmented features. These new features incorporate\nthe structure of the null hypothesis directly. The test statistics are\nconstructed in such a way that lack of sparsity in the original model parameter\ndoes not present a problem for the theoretical justification of our procedures.\nWe establish asymptotically exact control on Type I error without imposing any\nsparsity assumptions on model parameter or the vector representing the linear\nhypothesis. Our method is also shown to achieve certain optimality in detecting\ndeviations from the null hypothesis. We demonstrate the favorable finite-sample\nperformance of the proposed methods, via a number of numerical and a real data\nexample.\n", "versions": [{"version": "v1", "created": "Mon, 10 Oct 2016 16:30:27 GMT"}, {"version": "v2", "created": "Tue, 3 Jan 2017 01:10:12 GMT"}], "update_date": "2019-07-09", "authors_parsed": [["Zhu", "Yinchu", ""], ["Bradic", "Jelena", ""]]}, {"id": "1610.02995", "submitter": "Georg Martius", "authors": "Georg Martius and Christoph H. Lampert", "title": "Extrapolation and learning equations", "comments": "13 pages, 8 figures, 4 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In classical machine learning, regression is treated as a black box process\nof identifying a suitable function from a hypothesis set without attempting to\ngain insight into the mechanism connecting inputs and outputs. In the natural\nsciences, however, finding an interpretable function for a phenomenon is the\nprime goal as it allows to understand and generalize results. This paper\nproposes a novel type of function learning network, called equation learner\n(EQL), that can learn analytical expressions and is able to extrapolate to\nunseen domains. It is implemented as an end-to-end differentiable feed-forward\nnetwork and allows for efficient gradient based training. Due to sparsity\nregularization concise interpretable expressions can be obtained. Often the\ntrue underlying source expression is identified.\n", "versions": [{"version": "v1", "created": "Mon, 10 Oct 2016 16:47:36 GMT"}], "update_date": "2016-10-11", "authors_parsed": [["Martius", "Georg", ""], ["Lampert", "Christoph H.", ""]]}, {"id": "1610.03017", "submitter": "Jason Lee", "authors": "Jason Lee, Kyunghyun Cho and Thomas Hofmann", "title": "Fully Character-Level Neural Machine Translation without Explicit\n  Segmentation", "comments": "Transactions of the Association for Computational Linguistics (TACL),\n  2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most existing machine translation systems operate at the level of words,\nrelying on explicit segmentation to extract tokens. We introduce a neural\nmachine translation (NMT) model that maps a source character sequence to a\ntarget character sequence without any segmentation. We employ a character-level\nconvolutional network with max-pooling at the encoder to reduce the length of\nsource representation, allowing the model to be trained at a speed comparable\nto subword-level models while capturing local regularities. Our\ncharacter-to-character model outperforms a recently proposed baseline with a\nsubword-level encoder on WMT'15 DE-EN and CS-EN, and gives comparable\nperformance on FI-EN and RU-EN. We then demonstrate that it is possible to\nshare a single character-level encoder across multiple languages by training a\nmodel on a many-to-one translation task. In this multilingual setting, the\ncharacter-level encoder significantly outperforms the subword-level encoder on\nall the language pairs. We observe that on CS-EN, FI-EN and RU-EN, the quality\nof the multilingual character-level translation even surpasses the models\nspecifically trained on that language pair alone, both in terms of BLEU score\nand human judgment.\n", "versions": [{"version": "v1", "created": "Mon, 10 Oct 2016 18:19:34 GMT"}, {"version": "v2", "created": "Tue, 1 Nov 2016 17:51:32 GMT"}, {"version": "v3", "created": "Tue, 13 Jun 2017 03:32:34 GMT"}], "update_date": "2017-06-14", "authors_parsed": [["Lee", "Jason", ""], ["Cho", "Kyunghyun", ""], ["Hofmann", "Thomas", ""]]}, {"id": "1610.03035", "submitter": "William Chan", "authors": "William Chan, Yu Zhang, Quoc Le, Navdeep Jaitly", "title": "Latent Sequence Decompositions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present the Latent Sequence Decompositions (LSD) framework. LSD decomposes\nsequences with variable lengthed output units as a function of both the input\nsequence and the output sequence. We present a training algorithm which samples\nvalid extensions and an approximate decoding algorithm. We experiment with the\nWall Street Journal speech recognition task. Our LSD model achieves 12.9% WER\ncompared to a character baseline of 14.8% WER. When combined with a\nconvolutional network on the encoder, we achieve 9.6% WER.\n", "versions": [{"version": "v1", "created": "Mon, 10 Oct 2016 19:16:08 GMT"}, {"version": "v2", "created": "Mon, 17 Oct 2016 20:11:21 GMT"}, {"version": "v3", "created": "Sat, 5 Nov 2016 17:23:55 GMT"}, {"version": "v4", "created": "Wed, 30 Nov 2016 19:14:17 GMT"}, {"version": "v5", "created": "Thu, 19 Jan 2017 22:23:44 GMT"}, {"version": "v6", "created": "Tue, 7 Feb 2017 15:52:27 GMT"}], "update_date": "2017-02-08", "authors_parsed": [["Chan", "William", ""], ["Zhang", "Yu", ""], ["Le", "Quoc", ""], ["Jaitly", "Navdeep", ""]]}, {"id": "1610.03045", "submitter": "Jialei Wang", "authors": "Jialei Wang, Jason D. Lee, Mehrdad Mahdavi, Mladen Kolar, Nathan\n  Srebro", "title": "Sketching Meets Random Projection in the Dual: A Provable Recovery\n  Algorithm for Big and High-dimensional Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sketching techniques have become popular for scaling up machine learning\nalgorithms by reducing the sample size or dimensionality of massive data sets,\nwhile still maintaining the statistical power of big data. In this paper, we\nstudy sketching from an optimization point of view: we first show that the\niterative Hessian sketch is an optimization process with preconditioning, and\ndevelop accelerated iterative Hessian sketch via the searching the conjugate\ndirection; we then establish primal-dual connections between the Hessian sketch\nand dual random projection, and apply the preconditioned conjugate gradient\napproach on the dual problem, which leads to the accelerated iterative dual\nrandom projection methods. Finally to tackle the challenges from both large\nsample size and high-dimensionality, we propose the primal-dual sketch, which\niteratively sketches the primal and dual formulations. We show that using a\nlogarithmic number of calls to solvers of small scale problem, primal-dual\nsketch is able to recover the optimum of the original problem up to arbitrary\nprecision. The proposed algorithms are validated via extensive experiments on\nsynthetic and real data sets which complements our theoretical results.\n", "versions": [{"version": "v1", "created": "Mon, 10 Oct 2016 19:48:34 GMT"}], "update_date": "2016-10-11", "authors_parsed": [["Wang", "Jialei", ""], ["Lee", "Jason D.", ""], ["Mahdavi", "Mehrdad", ""], ["Kolar", "Mladen", ""], ["Srebro", "Nathan", ""]]}, {"id": "1610.03090", "submitter": "Kristjan Greenewald", "authors": "Kristjan Greenewald and Stephen Kelley and Alfred Hero III", "title": "Dynamic Metric Learning from Pairwise Comparisons", "comments": "to appear Allerton 2016. arXiv admin note: substantial text overlap\n  with arXiv:1603.03678", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent work in distance metric learning has focused on learning\ntransformations of data that best align with specified pairwise similarity and\ndissimilarity constraints, often supplied by a human observer. The learned\ntransformations lead to improved retrieval, classification, and clustering\nalgorithms due to the better adapted distance or similarity measures. Here, we\naddress the problem of learning these transformations when the underlying\nconstraint generation process is nonstationary. This nonstationarity can be due\nto changes in either the ground-truth clustering used to generate constraints\nor changes in the feature subspaces in which the class structure is apparent.\nWe propose Online Convex Ensemble StrongLy Adaptive Dynamic Learning (OCELAD),\na general adaptive, online approach for learning and tracking optimal metrics\nas they change over time that is highly robust to a variety of nonstationary\nbehaviors in the changing metric. We apply the OCELAD framework to an ensemble\nof online learners. Specifically, we create a retro-initialized composite\nobjective mirror descent (COMID) ensemble (RICE) consisting of a set of\nparallel COMID learners with different learning rates, demonstrate RICE-OCELAD\non both real and synthetic data sets and show significant performance\nimprovements relative to previously proposed batch and online distance metric\nlearning algorithms.\n", "versions": [{"version": "v1", "created": "Mon, 10 Oct 2016 20:39:25 GMT"}], "update_date": "2016-10-12", "authors_parsed": [["Greenewald", "Kristjan", ""], ["Kelley", "Stephen", ""], ["Hero", "Alfred", "III"]]}, {"id": "1610.03106", "submitter": "Hussam Hamdan", "authors": "Hussam Hamdan, Patrice Bellot, Frederic Bechet", "title": "Supervised Term Weighting Metrics for Sentiment Analysis in Short Text", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Term weighting metrics assign weights to terms in order to discriminate the\nimportant terms from the less crucial ones. Due to this characteristic, these\nmetrics have attracted growing attention in text classification and recently in\nsentiment analysis. Using the weights given by such metrics could lead to more\naccurate document representation which may improve the performance of the\nclassification. While previous studies have focused on proposing or comparing\ndifferent weighting metrics at two-classes document level sentiment analysis,\nthis study propose to analyse the results given by each metric in order to find\nout the characteristics of good and bad weighting metrics. Therefore we present\nan empirical study of fifteen global supervised weighting metrics with four\nlocal weighting metrics adopted from information retrieval, we also give an\nanalysis to understand the behavior of each metric by observing and analysing\nhow each metric distributes the terms and deduce some characteristics which may\ndistinguish the good and bad metrics. The evaluation has been done using\nSupport Vector Machine on three different datasets: Twitter, restaurant and\nlaptop reviews.\n", "versions": [{"version": "v1", "created": "Mon, 10 Oct 2016 21:52:47 GMT"}], "update_date": "2016-10-12", "authors_parsed": [["Hamdan", "Hussam", ""], ["Bellot", "Patrice", ""], ["Bechet", "Frederic", ""]]}, {"id": "1610.03147", "submitter": "Pan Zhou Prof.", "authors": "Yifan Hou, Pan Zhou, Ting Wang, Li Yu, Yuchong Hu, Dapeng Wu", "title": "Context-Aware Online Learning for Course Recommendation of MOOC Big Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CY cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Massive Open Online Course (MOOC) has expanded significantly in recent\nyears. With the widespread of MOOC, the opportunity to study the fascinating\ncourses for free has attracted numerous people of diverse educational\nbackgrounds all over the world. In the big data era, a key research topic for\nMOOC is how to mine the needed courses in the massive course databases in cloud\nfor each individual student accurately and rapidly as the number of courses is\nincreasing fleetly. In this respect, the key challenge is how to realize\npersonalized course recommendation as well as to reduce the computing and\nstorage costs for the tremendous course data. In this paper, we propose a big\ndata-supported, context-aware online learning-based course recommender system\nthat could handle the dynamic and infinitely massive datasets, which recommends\ncourses by using personalized context information and historical statistics.\nThe context-awareness takes the personal preferences into consideration, making\nthe recommendation suitable for people with different backgrounds. Besides, the\nalgorithm achieves the sublinear regret performance, which means it can\ngradually recommend the mostly preferred and matched courses to students. In\naddition, our storage module is expanded to the distributed-connected storage\nnodes, where the devised algorithm can handle massive course storage problems\nfrom heterogeneous sources of course datasets. Comparing to existing\nalgorithms, our proposed algorithms achieve the linear time complexity and\nspace complexity. Experiment results verify the superiority of our algorithms\nwhen comparing with existing ones in the MOOC big data setting.\n", "versions": [{"version": "v1", "created": "Tue, 11 Oct 2016 01:02:15 GMT"}, {"version": "v2", "created": "Sun, 16 Oct 2016 03:34:37 GMT"}], "update_date": "2016-10-18", "authors_parsed": [["Hou", "Yifan", ""], ["Zhou", "Pan", ""], ["Wang", "Ting", ""], ["Yu", "Li", ""], ["Hu", "Yuchong", ""], ["Wu", "Dapeng", ""]]}, {"id": "1610.03164", "submitter": "Andrea Daniele", "authors": "Andrea F. Daniele and Mohit Bansal and Matthew R. Walter", "title": "Navigational Instruction Generation as Inverse Reinforcement Learning\n  with Neural Machine Translation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AI cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modern robotics applications that involve human-robot interaction require\nrobots to be able to communicate with humans seamlessly and effectively.\nNatural language provides a flexible and efficient medium through which robots\ncan exchange information with their human partners. Significant advancements\nhave been made in developing robots capable of interpreting free-form\ninstructions, but less attention has been devoted to endowing robots with the\nability to generate natural language. We propose a navigational guide model\nthat enables robots to generate natural language instructions that allow humans\nto navigate a priori unknown environments. We first decide which information to\nshare with the user according to their preferences, using a policy trained from\nhuman demonstrations via inverse reinforcement learning. We then \"translate\"\nthis information into a natural language instruction using a neural\nsequence-to-sequence model that learns to generate free-form instructions from\nnatural language corpora. We evaluate our method on a benchmark route\ninstruction dataset and achieve a BLEU score of 72.18% when compared to\nhuman-generated reference instructions. We additionally conduct navigation\nexperiments with human participants that demonstrate that our method generates\ninstructions that people follow as accurately and easily as those produced by\nhumans.\n", "versions": [{"version": "v1", "created": "Tue, 11 Oct 2016 02:47:09 GMT"}], "update_date": "2016-10-12", "authors_parsed": [["Daniele", "Andrea F.", ""], ["Bansal", "Mohit", ""], ["Walter", "Matthew R.", ""]]}, {"id": "1610.03263", "submitter": "Patrick Bl\\\"obaum", "authors": "Patrick Bl\\\"obaum, Takashi Washio, Shohei Shimizu", "title": "Error Asymmetry in Causal and Anticausal Regression", "comments": null, "journal-ref": "Behaviormetrika, 2017, 10.1007/s41237-017-0022-z", "doi": "10.1007/s41237-017-0022-z", "report-no": null, "categories": "cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is generally difficult to make any statements about the expected\nprediction error in an univariate setting without further knowledge about how\nthe data were generated. Recent work showed that knowledge about the real\nunderlying causal structure of a data generation process has implications for\nvarious machine learning settings. Assuming an additive noise and an\nindependence between data generating mechanism and its input, we draw a novel\nconnection between the intrinsic causal relationship of two variables and the\nexpected prediction error. We formulate the theorem that the expected error of\nthe true data generating function as prediction model is generally smaller when\nthe effect is predicted from its cause and, on the contrary, greater when the\ncause is predicted from its effect. The theorem implies an asymmetry in the\nerror depending on the prediction direction. This is further corroborated with\nempirical evaluations in artificial and real-world data sets.\n", "versions": [{"version": "v1", "created": "Tue, 11 Oct 2016 10:15:15 GMT"}, {"version": "v2", "created": "Mon, 17 Apr 2017 12:25:44 GMT"}], "update_date": "2017-04-18", "authors_parsed": [["Bl\u00f6baum", "Patrick", ""], ["Washio", "Takashi", ""], ["Shimizu", "Shohei", ""]]}, {"id": "1610.03295", "submitter": "Shai Shalev-Shwartz", "authors": "Shai Shalev-Shwartz and Shaked Shammah and Amnon Shashua", "title": "Safe, Multi-Agent, Reinforcement Learning for Autonomous Driving", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Autonomous driving is a multi-agent setting where the host vehicle must apply\nsophisticated negotiation skills with other road users when overtaking, giving\nway, merging, taking left and right turns and while pushing ahead in\nunstructured urban roadways. Since there are many possible scenarios, manually\ntackling all possible cases will likely yield a too simplistic policy.\nMoreover, one must balance between unexpected behavior of other\ndrivers/pedestrians and at the same time not to be too defensive so that normal\ntraffic flow is maintained.\n  In this paper we apply deep reinforcement learning to the problem of forming\nlong term driving strategies. We note that there are two major challenges that\nmake autonomous driving different from other robotic tasks. First, is the\nnecessity for ensuring functional safety - something that machine learning has\ndifficulty with given that performance is optimized at the level of an\nexpectation over many instances. Second, the Markov Decision Process model\noften used in robotics is problematic in our case because of unpredictable\nbehavior of other agents in this multi-agent scenario. We make three\ncontributions in our work. First, we show how policy gradient iterations can be\nused without Markovian assumptions. Second, we decompose the problem into a\ncomposition of a Policy for Desires (which is to be learned) and trajectory\nplanning with hard constraints (which is not learned). The goal of Desires is\nto enable comfort of driving, while hard constraints guarantees the safety of\ndriving. Third, we introduce a hierarchical temporal abstraction we call an\n\"Option Graph\" with a gating mechanism that significantly reduces the effective\nhorizon and thereby reducing the variance of the gradient estimation even\nfurther.\n", "versions": [{"version": "v1", "created": "Tue, 11 Oct 2016 12:09:03 GMT"}], "update_date": "2016-10-12", "authors_parsed": [["Shalev-Shwartz", "Shai", ""], ["Shammah", "Shaked", ""], ["Shashua", "Amnon", ""]]}, {"id": "1610.03317", "submitter": "Hsiang-Fu Yu", "authors": "Hsiang-Fu Yu, Cho-Jui Hsieh, Qi Lei, and Inderjit S. Dhillon", "title": "A Greedy Approach for Budgeted Maximum Inner Product Search", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Maximum Inner Product Search (MIPS) is an important task in many machine\nlearning applications such as the prediction phase of a low-rank matrix\nfactorization model for a recommender system. There have been some works on how\nto perform MIPS in sub-linear time recently. However, most of them do not have\nthe flexibility to control the trade-off between search efficient and search\nquality. In this paper, we study the MIPS problem with a computational budget.\nBy carefully studying the problem structure of MIPS, we develop a novel\nGreedy-MIPS algorithm, which can handle budgeted MIPS by design. While simple\nand intuitive, Greedy-MIPS yields surprisingly superior performance compared to\nstate-of-the-art approaches. As a specific example, on a candidate set\ncontaining half a million vectors of dimension 200, Greedy-MIPS runs 200x\nfaster than the naive approach while yielding search results with the top-5\nprecision greater than 75\\%.\n", "versions": [{"version": "v1", "created": "Tue, 11 Oct 2016 13:10:48 GMT"}], "update_date": "2016-10-12", "authors_parsed": [["Yu", "Hsiang-Fu", ""], ["Hsieh", "Cho-Jui", ""], ["Lei", "Qi", ""], ["Dhillon", "Inderjit S.", ""]]}, {"id": "1610.03342", "submitter": "Grzegorz Chrupa{\\l}a", "authors": "Lieke Gelderloos and Grzegorz Chrupa{\\l}a", "title": "From phonemes to images: levels of representation in a recurrent neural\n  model of visually-grounded language learning", "comments": "Accepted at COLING 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We present a model of visually-grounded language learning based on stacked\ngated recurrent neural networks which learns to predict visual features given\nan image description in the form of a sequence of phonemes. The learning task\nresembles that faced by human language learners who need to discover both\nstructure and meaning from noisy and ambiguous data across modalities. We show\nthat our model indeed learns to predict features of the visual context given\nphonetically transcribed image descriptions, and show that it represents\nlinguistic information in a hierarchy of levels: lower layers in the stack are\ncomparatively more sensitive to form, whereas higher layers are more sensitive\nto meaning.\n", "versions": [{"version": "v1", "created": "Tue, 11 Oct 2016 14:00:28 GMT"}], "update_date": "2016-10-12", "authors_parsed": [["Gelderloos", "Lieke", ""], ["Chrupa\u0142a", "Grzegorz", ""]]}, {"id": "1610.03414", "submitter": "Jason Sakellariou", "authors": "Jason Sakellariou, Francesca Tria, Vittorio Loreto, Fran\\c{c}ois\n  Pachet", "title": "Maximum entropy models capture melodic styles", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a Maximum Entropy model able to capture the statistics of\nmelodies in music. The model can be used to generate new melodies that emulate\nthe style of the musical corpus which was used to train it. Instead of using\nthe $n-$body interactions of $(n-1)-$order Markov models, traditionally used in\nautomatic music generation, we use a $k-$nearest neighbour model with pairwise\ninteractions only. In that way, we keep the number of parameters low and avoid\nover-fitting problems typical of Markov models. We show that long-range musical\nphrases don't need to be explicitly enforced using high-order Markov\ninteractions, but can instead emerge from multiple, competing, pairwise\ninteractions. We validate our Maximum Entropy model by contrasting how much the\ngenerated sequences capture the style of the original corpus without\nplagiarizing it. To this end we use a data-compression approach to discriminate\nthe levels of borrowing and innovation featured by the artificial sequences.\nThe results show that our modelling scheme outperforms both fixed-order and\nvariable-order Markov models. This shows that, despite being based only on\npairwise interactions, this Maximum Entropy scheme opens the possibility to\ngenerate musically sensible alterations of the original phrases, providing a\nway to generate innovation.\n", "versions": [{"version": "v1", "created": "Tue, 11 Oct 2016 16:23:45 GMT"}], "update_date": "2016-10-12", "authors_parsed": [["Sakellariou", "Jason", ""], ["Tria", "Francesca", ""], ["Loreto", "Vittorio", ""], ["Pachet", "Fran\u00e7ois", ""]]}, {"id": "1610.03454", "submitter": "Weiran Wang", "authors": "Weiran Wang, Xinchen Yan, Honglak Lee, Karen Livescu", "title": "Deep Variational Canonical Correlation Analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present deep variational canonical correlation analysis (VCCA), a deep\nmulti-view learning model that extends the latent variable model interpretation\nof linear CCA to nonlinear observation models parameterized by deep neural\nnetworks. We derive variational lower bounds of the data likelihood by\nparameterizing the posterior probability of the latent variables from the view\nthat is available at test time. We also propose a variant of VCCA called\nVCCA-private that can, in addition to the \"common variables\" underlying both\nviews, extract the \"private variables\" within each view, and disentangles the\nshared and private information for multi-view data without hard supervision.\nExperimental results on real-world datasets show that our methods are\ncompetitive across domains.\n", "versions": [{"version": "v1", "created": "Tue, 11 Oct 2016 18:22:05 GMT"}, {"version": "v2", "created": "Mon, 14 Nov 2016 16:29:11 GMT"}, {"version": "v3", "created": "Sat, 25 Feb 2017 03:39:12 GMT"}], "update_date": "2017-02-28", "authors_parsed": [["Wang", "Weiran", ""], ["Yan", "Xinchen", ""], ["Lee", "Honglak", ""], ["Livescu", "Karen", ""]]}, {"id": "1610.03483", "submitter": "Balaji Lakshminarayanan", "authors": "Shakir Mohamed and Balaji Lakshminarayanan", "title": "Learning in Implicit Generative Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generative adversarial networks (GANs) provide an algorithmic framework for\nconstructing generative models with several appealing properties: they do not\nrequire a likelihood function to be specified, only a generating procedure;\nthey provide samples that are sharp and compelling; and they allow us to\nharness our knowledge of building highly accurate neural network classifiers.\nHere, we develop our understanding of GANs with the aim of forming a rich view\nof this growing area of machine learning---to build connections to the diverse\nset of statistical thinking on this topic, of which much can be gained by a\nmutual exchange of ideas. We frame GANs within the wider landscape of\nalgorithms for learning in implicit generative models--models that only specify\na stochastic procedure with which to generate data--and relate these ideas to\nmodelling problems in related fields, such as econometrics and approximate\nBayesian computation. We develop likelihood-free inference methods and\nhighlight hypothesis testing as a principle for learning in implicit generative\nmodels, using which we are able to derive the objective function used by GANs,\nand many other related objectives. The testing viewpoint directs our focus to\nthe general problem of density ratio estimation. There are four approaches for\ndensity ratio estimation, one of which is a solution using classifiers to\ndistinguish real from generated data. Other approaches such as divergence\nminimisation and moment matching have also been explored in the GAN literature,\nand we synthesise these views to form an understanding in terms of the\nrelationships between them and the wider literature, highlighting avenues for\nfuture exploration and cross-pollination.\n", "versions": [{"version": "v1", "created": "Tue, 11 Oct 2016 19:59:39 GMT"}, {"version": "v2", "created": "Fri, 4 Nov 2016 15:11:20 GMT"}, {"version": "v3", "created": "Fri, 13 Jan 2017 17:44:52 GMT"}, {"version": "v4", "created": "Mon, 27 Feb 2017 05:47:30 GMT"}], "update_date": "2017-02-28", "authors_parsed": [["Mohamed", "Shakir", ""], ["Lakshminarayanan", "Balaji", ""]]}, {"id": "1610.03518", "submitter": "Wojciech Zaremba", "authors": "Paul Christiano, Zain Shah, Igor Mordatch, Jonas Schneider, Trevor\n  Blackwell, Joshua Tobin, Pieter Abbeel, and Wojciech Zaremba", "title": "Transfer from Simulation to Real World through Learning Deep Inverse\n  Dynamics Model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AI cs.LG cs.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Developing control policies in simulation is often more practical and safer\nthan directly running experiments in the real world. This applies to policies\nobtained from planning and optimization, and even more so to policies obtained\nfrom reinforcement learning, which is often very data demanding. However, a\npolicy that succeeds in simulation often doesn't work when deployed on a real\nrobot. Nevertheless, often the overall gist of what the policy does in\nsimulation remains valid in the real world. In this paper we investigate such\nsettings, where the sequence of states traversed in simulation remains\nreasonable for the real world, even if the details of the controls are not, as\ncould be the case when the key differences lie in detailed friction, contact,\nmass and geometry properties. During execution, at each time step our approach\ncomputes what the simulation-based control policy would do, but then, rather\nthan executing these controls on the real robot, our approach computes what the\nsimulation expects the resulting next state(s) will be, and then relies on a\nlearned deep inverse dynamics model to decide which real-world action is most\nsuitable to achieve those next states. Deep models are only as good as their\ntraining data, and we also propose an approach for data collection to\n(incrementally) learn the deep inverse dynamics model. Our experiments shows\nour approach compares favorably with various baselines that have been developed\nfor dealing with simulation to real world model discrepancy, including output\nerror control and Gaussian dynamics adaptation.\n", "versions": [{"version": "v1", "created": "Tue, 11 Oct 2016 20:24:31 GMT"}], "update_date": "2016-10-13", "authors_parsed": [["Christiano", "Paul", ""], ["Shah", "Zain", ""], ["Mordatch", "Igor", ""], ["Schneider", "Jonas", ""], ["Blackwell", "Trevor", ""], ["Tobin", "Joshua", ""], ["Abbeel", "Pieter", ""], ["Zaremba", "Wojciech", ""]]}, {"id": "1610.03577", "submitter": "Jihun Hamm", "authors": "Jihun Hamm", "title": "Minimax Filter: Learning to Preserve Privacy from Inference Attacks", "comments": "Revision 2: minor revision", "journal-ref": "Journal of Machine Learning Research 18 (2017) 1-31", "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Preserving privacy of continuous and/or high-dimensional data such as images,\nvideos and audios, can be challenging with syntactic anonymization methods\nwhich are designed for discrete attributes. Differential privacy, which\nprovides a more formal definition of privacy, has shown more success in\nsanitizing continuous data. However, both syntactic and differential privacy\nare susceptible to inference attacks, i.e., an adversary can accurately infer\nsensitive attributes from sanitized data. The paper proposes a novel\nfilter-based mechanism which preserves privacy of continuous and\nhigh-dimensional attributes against inference attacks. Finding the optimal\nutility-privacy tradeoff is formulated as a min-diff-max optimization problem.\nThe paper provides an ERM-like analysis of the generalization error and also a\npractical algorithm to perform the optimization. In addition, the paper\nproposes an extension that combines minimax filter and differentially-private\nnoisy mechanism. Advantages of the method over purely noisy mechanisms is\nexplained and demonstrated with examples. Experiments with several real-world\ntasks including facial expression classification, speech emotion\nclassification, and activity classification from motion, show that the minimax\nfilter can simultaneously achieve similar or better target task accuracy and\nlower inference accuracy, often significantly lower than previous methods.\n", "versions": [{"version": "v1", "created": "Wed, 12 Oct 2016 01:57:20 GMT"}, {"version": "v2", "created": "Thu, 14 Sep 2017 18:40:30 GMT"}, {"version": "v3", "created": "Fri, 1 Dec 2017 05:07:49 GMT"}], "update_date": "2017-12-04", "authors_parsed": [["Hamm", "Jihun", ""]]}, {"id": "1610.03592", "submitter": "Shay Moran", "authors": "Ofir David and Shay Moran and Amir Yehudayoff", "title": "On statistical learning via the lens of compression", "comments": "Appeared in NIPS '16 (oral)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DM cs.LO math.CO math.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work continues the study of the relationship between sample compression\nschemes and statistical learning, which has been mostly investigated within the\nframework of binary classification. The central theme of this work is\nestablishing equivalences between learnability and compressibility, and\nutilizing these equivalences in the study of statistical learning theory.\n  We begin with the setting of multiclass categorization (zero/one loss). We\nprove that in this case learnability is equivalent to compression of\nlogarithmic sample size, and that uniform convergence implies compression of\nconstant size.\n  We then consider Vapnik's general learning setting: we show that in order to\nextend the compressibility-learnability equivalence to this case, it is\nnecessary to consider an approximate variant of compression.\n  Finally, we provide some applications of the compressibility-learnability\nequivalences:\n  (i) Agnostic-case learnability and realizable-case learnability are\nequivalent in multiclass categorization problems (in terms of sample\ncomplexity).\n  (ii) This equivalence between agnostic-case learnability and realizable-case\nlearnability does not hold for general learning problems: There exists a\nlearning problem whose loss function takes just three values, under which\nagnostic-case and realizable-case learnability are not equivalent.\n  (iii) Uniform convergence implies compression of constant size in multiclass\ncategorization problems. Part of the argument includes an analysis of the\nuniform convergence rate in terms of the graph dimension, in which we improve\nupon previous bounds.\n  (iv) A dichotomy for sample compression in multiclass categorization\nproblems: If a non-trivial compression exists then a compression of logarithmic\nsize exists.\n  (v) A compactness theorem for multiclass categorization problems.\n", "versions": [{"version": "v1", "created": "Wed, 12 Oct 2016 03:46:00 GMT"}, {"version": "v2", "created": "Fri, 30 Dec 2016 18:30:27 GMT"}], "update_date": "2017-01-02", "authors_parsed": [["David", "Ofir", ""], ["Moran", "Shay", ""], ["Yehudayoff", "Amir", ""]]}, {"id": "1610.03618", "submitter": "Chao Li", "authors": "Chao Li, Yi Yang, Min Feng, Srimat Chakradhar, Huiyang Zhou", "title": "Optimizing Memory Efficiency for Deep Convolutional Neural Networks on\n  GPUs", "comments": "Published as a conference paper International Conference on High\n  Performance Computing, Networking, Storage, and Analysis (SC'16), 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.LG cs.NE cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Leveraging large data sets, deep Convolutional Neural Networks (CNNs) achieve\nstate-of-the-art recognition accuracy. Due to the substantial compute and\nmemory operations, however, they require significant execution time. The\nmassive parallel computing capability of GPUs make them as one of the ideal\nplatforms to accelerate CNNs and a number of GPU-based CNN libraries have been\ndeveloped. While existing works mainly focus on the computational efficiency of\nCNNs, the memory efficiency of CNNs have been largely overlooked. Yet CNNs have\nintricate data structures and their memory behavior can have significant impact\non the performance. In this work, we study the memory efficiency of various CNN\nlayers and reveal the performance implication from both data layouts and memory\naccess patterns. Experiments show the universal effect of our proposed\noptimizations on both single layers and various networks, with up to 27.9x for\na single layer and up to 5.6x on the whole networks.\n", "versions": [{"version": "v1", "created": "Wed, 12 Oct 2016 07:02:48 GMT"}], "update_date": "2016-10-13", "authors_parsed": [["Li", "Chao", ""], ["Yang", "Yi", ""], ["Feng", "Min", ""], ["Chakradhar", "Srimat", ""], ["Zhou", "Huiyang", ""]]}, {"id": "1610.03628", "submitter": "Carlos Ciller Mr.", "authors": "Stefanos Apostolopoulos, Carlos Ciller, Sandro I. De Zanet, Sebastian\n  Wolf and Raphael Sznitman", "title": "RetiNet: Automatic AMD identification in OCT volumetric data", "comments": "14 pages, 10 figures, Code available", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Optical Coherence Tomography (OCT) provides a unique ability to image the eye\nretina in 3D at micrometer resolution and gives ophthalmologist the ability to\nvisualize retinal diseases such as Age-Related Macular Degeneration (AMD).\nWhile visual inspection of OCT volumes remains the main method for AMD\nidentification, doing so is time consuming as each cross-section within the\nvolume must be inspected individually by the clinician. In much the same way,\nacquiring ground truth information for each cross-section is expensive and time\nconsuming. This fact heavily limits the ability to acquire large amounts of\nground truth, which subsequently impacts the performance of learning-based\nmethods geared at automatic pathology identification. To avoid this burden, we\npropose a novel strategy for automatic analysis of OCT volumes where only\nvolume labels are needed. That is, we train a classifier in a semi-supervised\nmanner to conduct this task. Our approach uses a novel Convolutional Neural\nNetwork (CNN) architecture, that only needs volume-level labels to be trained\nto automatically asses whether an OCT volume is healthy or contains AMD. Our\narchitecture involves first learning a cross-section pathology classifier using\npseudo-labels that could be corrupted and then leverage these towards a more\naccurate volume-level classification. We then show that our approach provides\nexcellent performances on a publicly available dataset and outperforms a number\nof existing automatic techniques.\n", "versions": [{"version": "v1", "created": "Wed, 12 Oct 2016 07:56:24 GMT"}], "update_date": "2016-10-13", "authors_parsed": [["Apostolopoulos", "Stefanos", ""], ["Ciller", "Carlos", ""], ["De Zanet", "Sandro I.", ""], ["Wolf", "Sebastian", ""], ["Sznitman", "Raphael", ""]]}, {"id": "1610.03677", "submitter": "Suchet Bargoti", "authors": "Suchet Bargoti and James Underwood", "title": "Deep Fruit Detection in Orchards", "comments": "Submitted to the IEEE International Conference on Robotics and\n  Automation 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AI cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An accurate and reliable image based fruit detection system is critical for\nsupporting higher level agriculture tasks such as yield mapping and robotic\nharvesting. This paper presents the use of a state-of-the-art object detection\nframework, Faster R-CNN, in the context of fruit detection in orchards,\nincluding mangoes, almonds and apples. Ablation studies are presented to better\nunderstand the practical deployment of the detection network, including how\nmuch training data is required to capture variability in the dataset. Data\naugmentation techniques are shown to yield significant performance gains,\nresulting in a greater than two-fold reduction in the number of training images\nrequired. In contrast, transferring knowledge between orchards contributed to\nnegligible performance gain over initialising the Deep Convolutional Neural\nNetwork directly from ImageNet features. Finally, to operate over orchard data\ncontaining between 100-1000 fruit per image, a tiling approach is introduced\nfor the Faster R-CNN framework. The study has resulted in the best yet\ndetection performance for these orchards relative to previous works, with an\nF1-score of >0.9 achieved for apples and mangoes.\n", "versions": [{"version": "v1", "created": "Wed, 12 Oct 2016 11:40:24 GMT"}, {"version": "v2", "created": "Mon, 18 Sep 2017 01:03:55 GMT"}], "update_date": "2017-09-19", "authors_parsed": [["Bargoti", "Suchet", ""], ["Underwood", "James", ""]]}, {"id": "1610.03713", "submitter": "Jesse Krijthe", "authors": "Jesse H. Krijthe and Marco Loog", "title": "Optimistic Semi-supervised Least Squares Classification", "comments": "6 pages, 6 figures. International Conference on Pattern Recognition\n  (ICPR) 2016, Cancun, Mexico", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The goal of semi-supervised learning is to improve supervised classifiers by\nusing additional unlabeled training examples. In this work we study a simple\nself-learning approach to semi-supervised learning applied to the least squares\nclassifier. We show that a soft-label and a hard-label variant of self-learning\ncan be derived by applying block coordinate descent to two related but slightly\ndifferent objective functions. The resulting soft-label approach is related to\nan idea about dealing with missing data that dates back to the 1930s. We show\nthat the soft-label variant typically outperforms the hard-label variant on\nbenchmark datasets and partially explain this behaviour by studying the\nrelative difficulty of finding good local minima for the corresponding\nobjective functions.\n", "versions": [{"version": "v1", "created": "Wed, 12 Oct 2016 13:52:07 GMT"}], "update_date": "2016-10-13", "authors_parsed": [["Krijthe", "Jesse H.", ""], ["Loog", "Marco", ""]]}, {"id": "1610.03738", "submitter": "Daniel Wesierski", "authors": "Daniel Wesierski", "title": "Exploring the Entire Regularization Path for the Asymmetric Cost Linear\n  Support Vector Machine", "comments": "8 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose an algorithm for exploring the entire regularization path of\nasymmetric-cost linear support vector machines. Empirical evidence suggests the\npredictive power of support vector machines depends on the regularization\nparameters of the training algorithms. The algorithms exploring the entire\nregularization paths have been proposed for single-cost support vector machines\nthereby providing the complete knowledge on the behavior of the trained model\nover the hyperparameter space. Considering the problem in two-dimensional\nhyperparameter space though enables our algorithm to maintain greater\nflexibility in dealing with special cases and sheds light on problems\nencountered by algorithms building the paths in one-dimensional spaces. We\ndemonstrate two-dimensional regularization paths for linear support vector\nmachines that we train on synthetic and real data.\n", "versions": [{"version": "v1", "created": "Wed, 12 Oct 2016 14:57:10 GMT"}], "update_date": "2016-10-13", "authors_parsed": [["Wesierski", "Daniel", ""]]}, {"id": "1610.03761", "submitter": "Shehroz Khan", "authors": "Shehroz S. Khan, Babak Taati", "title": "Detecting Unseen Falls from Wearable Devices using Channel-wise Ensemble\n  of Autoencoders", "comments": "25 pages, 6 figures, 4 Tables", "journal-ref": "Expert Systems with Applications, Volume 87, 30 November 2017,\n  Pages 280-290", "doi": "10.1016/j.eswa.2017.06.011", "report-no": null, "categories": "cs.CV cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A fall is an abnormal activity that occurs rarely, so it is hard to collect\nreal data for falls. It is, therefore, difficult to use supervised learning\nmethods to automatically detect falls. Another challenge in using machine\nlearning methods to automatically detect falls is the choice of engineered\nfeatures. In this paper, we propose to use an ensemble of autoencoders to\nextract features from different channels of wearable sensor data trained only\non normal activities. We show that the traditional approach of choosing a\nthreshold as the maximum of the reconstruction error on the training normal\ndata is not the right way to identify unseen falls. We propose two methods for\nautomatic tightening of reconstruction error from only the normal activities\nfor better identification of unseen falls. We present our results on two\nactivity recognition datasets and show the efficacy of our proposed method\nagainst traditional autoencoder models and two standard one-class\nclassification methods.\n", "versions": [{"version": "v1", "created": "Wed, 12 Oct 2016 15:55:06 GMT"}, {"version": "v2", "created": "Mon, 17 Oct 2016 18:00:04 GMT"}, {"version": "v3", "created": "Wed, 22 Mar 2017 20:51:59 GMT"}], "update_date": "2018-02-07", "authors_parsed": [["Khan", "Shehroz S.", ""], ["Taati", "Babak", ""]]}, {"id": "1610.03774", "submitter": "Rahul Kidambi", "authors": "Prateek Jain, Sham M. Kakade, Rahul Kidambi, Praneeth Netrapalli,\n  Aaron Sidford", "title": "Parallelizing Stochastic Gradient Descent for Least Squares Regression:\n  mini-batching, averaging, and model misspecification", "comments": "39 pages. Published in the Journal of Machine Learning Research\n  (JMLR)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.DS cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work characterizes the benefits of averaging schemes widely used in\nconjunction with stochastic gradient descent (SGD). In particular, this work\nprovides a sharp analysis of: (1) mini-batching, a method of averaging many\nsamples of a stochastic gradient to both reduce the variance of the stochastic\ngradient estimate and for parallelizing SGD and (2) tail-averaging, a method\ninvolving averaging the final few iterates of SGD to decrease the variance in\nSGD's final iterate. This work presents non-asymptotic excess risk bounds for\nthese schemes for the stochastic approximation problem of least squares\nregression.\n  Furthermore, this work establishes a precise problem-dependent extent to\nwhich mini-batch SGD yields provable near-linear parallelization speedups over\nSGD with batch size one. This allows for understanding learning rate versus\nbatch size tradeoffs for the final iterate of an SGD method. These results are\nthen utilized in providing a highly parallelizable SGD method that obtains the\nminimax risk with nearly the same number of serial updates as batch gradient\ndescent, improving significantly over existing SGD methods. A non-asymptotic\nanalysis of communication efficient parallelization schemes such as\nmodel-averaging/parameter mixing methods is then provided.\n  Finally, this work sheds light on some fundamental differences in SGD's\nbehavior when dealing with agnostic noise in the (non-realizable) least squares\nregression problem. In particular, the work shows that the stepsizes that\nensure minimax risk for the agnostic case must be a function of the noise\nproperties.\n  This paper builds on the operator view of analyzing SGD methods, introduced\nby Defossez and Bach (2015), followed by developing a novel analysis in\nbounding these operators to characterize the excess risk. These techniques are\nof broader interest in analyzing computational aspects of stochastic\napproximation.\n", "versions": [{"version": "v1", "created": "Wed, 12 Oct 2016 16:30:11 GMT"}, {"version": "v2", "created": "Thu, 27 Oct 2016 17:09:30 GMT"}, {"version": "v3", "created": "Sun, 1 Apr 2018 16:20:07 GMT"}, {"version": "v4", "created": "Tue, 31 Jul 2018 17:50:00 GMT"}], "update_date": "2018-08-01", "authors_parsed": [["Jain", "Prateek", ""], ["Kakade", "Sham M.", ""], ["Kidambi", "Rahul", ""], ["Netrapalli", "Praneeth", ""], ["Sidford", "Aaron", ""]]}, {"id": "1610.03793", "submitter": "Daniel Hein", "authors": "Daniel Hein, Alexander Hentschel, Volkmar Sterzing, Michel Tokic,\n  Steffen Udluft", "title": "Introduction to the \"Industrial Benchmark\"", "comments": "11 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A novel reinforcement learning benchmark, called Industrial Benchmark, is\nintroduced. The Industrial Benchmark aims at being be realistic in the sense,\nthat it includes a variety of aspects that we found to be vital in industrial\napplications. It is not designed to be an approximation of any real system, but\nto pose the same hardness and complexity.\n", "versions": [{"version": "v1", "created": "Wed, 12 Oct 2016 17:18:01 GMT"}, {"version": "v2", "created": "Thu, 28 Sep 2017 11:28:26 GMT"}], "update_date": "2017-09-29", "authors_parsed": [["Hein", "Daniel", ""], ["Hentschel", "Alexander", ""], ["Sterzing", "Volkmar", ""], ["Tokic", "Michel", ""], ["Udluft", "Steffen", ""]]}, {"id": "1610.03899", "submitter": "Michael Rabadi", "authors": "Michael Rabadi", "title": "Generalization bound for kernel similarity learning", "comments": "9 pages", "journal-ref": "In NIPS 2016 Brains and Bits Workshop. Barcelona, Spain", "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Similarity learning has received a large amount of interest and is an\nimportant tool for many scientific and industrial applications. In this\nframework, we wish to infer the distance (similarity) between points with\nrespect to an arbitrary distance function $d$. Here, we formulate the problem\nas a regression from a feature space $\\mathcal{X}$ to an arbitrary vector space\n$\\mathcal{Y}$, where the Euclidean distance is proportional to $d$. We then\ngive Rademacher complexity bounds on the generalization error. We find that\nwith high probability, the complexity is bounded by the maximum of the radius\nof $\\mathcal{X}$ and the radius of $\\mathcal{Y}$.\n", "versions": [{"version": "v1", "created": "Wed, 12 Oct 2016 23:35:45 GMT"}, {"version": "v2", "created": "Thu, 20 Oct 2016 21:39:17 GMT"}], "update_date": "2016-11-30", "authors_parsed": [["Rabadi", "Michael", ""]]}, {"id": "1610.03914", "submitter": "Kiran Vodrahalli", "authors": "Kiran Vodrahalli, Po-Hsuan Chen, Yingyu Liang, Christopher Baldassano,\n  Janice Chen, Esther Yong, Christopher Honey, Uri Hasson, Peter Ramadge, Ken\n  Norman, Sanjeev Arora", "title": "Mapping Between fMRI Responses to Movies and their Natural Language\n  Annotations", "comments": "19 pages, 9 figures, in submission to NeuroImage. Prior version\n  presented at MLINI-2016 workshop, 2016 (arXiv:1701.01437) and ICML 2016\n  Workshop on Multi-view Representation Learning", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Several research groups have shown how to correlate fMRI responses to the\nmeanings of presented stimuli. This paper presents new methods for doing so\nwhen only a natural language annotation is available as the description of the\nstimulus. We study fMRI data gathered from subjects watching an episode of BBCs\nSherlock [1], and learn bidirectional mappings between fMRI responses and\nnatural language representations. We show how to leverage data from multiple\nsubjects watching the same movie to improve the accuracy of the mappings,\nallowing us to succeed at a scene classification task with 72% accuracy (random\nguessing would give 4%) and at a scene ranking task with average rank in the\ntop 4% (random guessing would give 50%). The key ingredients are (a) the use of\nthe Shared Response Model (SRM) and its variant SRM-ICA [2, 3] to aggregate\nfMRI data from multiple subjects, both of which are shown to be superior to\nstandard PCA in producing low-dimensional representations for the tasks in this\npaper; (b) a sentence embedding technique adapted from the natural language\nprocessing (NLP) literature [4] that produces semantic vector representation of\nthe annotations; (c) using previous timestep information in the featurization\nof the predictor data.\n", "versions": [{"version": "v1", "created": "Thu, 13 Oct 2016 02:20:45 GMT"}, {"version": "v2", "created": "Tue, 14 Mar 2017 20:29:41 GMT"}, {"version": "v3", "created": "Mon, 10 Apr 2017 08:41:51 GMT"}], "update_date": "2017-04-11", "authors_parsed": [["Vodrahalli", "Kiran", ""], ["Chen", "Po-Hsuan", ""], ["Liang", "Yingyu", ""], ["Baldassano", "Christopher", ""], ["Chen", "Janice", ""], ["Yong", "Esther", ""], ["Honey", "Christopher", ""], ["Hasson", "Uri", ""], ["Ramadge", "Peter", ""], ["Norman", "Ken", ""], ["Arora", "Sanjeev", ""]]}, {"id": "1610.03950", "submitter": "Lili Mou", "authors": "Yunchuan Chen, Lili Mou, Yan Xu, Ge Li, Zhi Jin", "title": "Compressing Neural Language Models by Sparse Word Representations", "comments": "ACL-16, pages 226--235", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural networks are among the state-of-the-art techniques for language\nmodeling. Existing neural language models typically map discrete words to\ndistributed, dense vector representations. After information processing of the\npreceding context words by hidden layers, an output layer estimates the\nprobability of the next word. Such approaches are time- and memory-intensive\nbecause of the large numbers of parameters for word embeddings and the output\nlayer. In this paper, we propose to compress neural language models by sparse\nword representations. In the experiments, the number of parameters in our model\nincreases very slowly with the growth of the vocabulary size, which is almost\nimperceptible. Moreover, our approach not only reduces the parameter space to a\nlarge extent, but also improves the performance in terms of the perplexity\nmeasure.\n", "versions": [{"version": "v1", "created": "Thu, 13 Oct 2016 06:55:54 GMT"}], "update_date": "2016-10-14", "authors_parsed": [["Chen", "Yunchuan", ""], ["Mou", "Lili", ""], ["Xu", "Yan", ""], ["Li", "Ge", ""], ["Jin", "Zhi", ""]]}, {"id": "1610.03988", "submitter": "Chin-Cheng Hsu", "authors": "Chin-Cheng Hsu, Hsin-Te Hwang, Yi-Chiao Wu, Yu Tsao, and Hsin-Min Wang", "title": "Dictionary Update for NMF-based Voice Conversion Using an\n  Encoder-Decoder Network", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG cs.SD", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a dictionary update method for Nonnegative Matrix\nFactorization (NMF) with high dimensional data in a spectral conversion (SC)\ntask. Voice conversion has been widely studied due to its potential\napplications such as personalized speech synthesis and speech enhancement.\nExemplar-based NMF (ENMF) emerges as an effective and probably the simplest\nchoice among all techniques for SC, as long as a source-target parallel speech\ncorpus is given. ENMF-based SC systems usually need a large amount of bases\n(exemplars) to ensure the quality of the converted speech. However, a small and\neffective dictionary is desirable but hard to obtain via dictionary update, in\nparticular when high-dimensional features such as STRAIGHT spectra are used.\nTherefore, we propose a dictionary update framework for NMF by means of an\nencoder-decoder reformulation. Regarding NMF as an encoder-decoder network\nmakes it possible to exploit the whole parallel corpus more effectively and\nefficiently when applied to SC. Our experiments demonstrate significant gains\nof the proposed system with small dictionaries over conventional ENMF-based\nsystems with dictionaries of same or much larger size.\n", "versions": [{"version": "v1", "created": "Thu, 13 Oct 2016 09:18:53 GMT"}], "update_date": "2016-10-14", "authors_parsed": [["Hsu", "Chin-Cheng", ""], ["Hwang", "Hsin-Te", ""], ["Wu", "Yi-Chiao", ""], ["Tsao", "Yu", ""], ["Wang", "Hsin-Min", ""]]}, {"id": "1610.03995", "submitter": "Adrian  Calma", "authors": "Tobias Reitmaier and Adrian Calma and Bernhard Sick", "title": "Semi-Supervised Active Learning for Support Vector Machines: A Novel\n  Approach that Exploits Structure Information in Data", "comments": "35 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In our today's information society more and more data emerges, e.g.~in social\nnetworks, technical applications, or business applications. Companies try to\ncommercialize these data using data mining or machine learning methods. For\nthis purpose, the data are categorized or classified, but often at high\n(monetary or temporal) costs. An effective approach to reduce these costs is to\napply any kind of active learning (AL) methods, as AL controls the training\nprocess of a classifier by specific querying individual data points (samples),\nwhich are then labeled (e.g., provided with class memberships) by a domain\nexpert. However, an analysis of current AL research shows that AL still has\nsome shortcomings. In particular, the structure information given by the\nspatial pattern of the (un)labeled data in the input space of a classification\nmodel (e.g.,~cluster information), is used in an insufficient way. In addition,\nmany existing AL techniques pay too little attention to their practical\napplicability. To meet these challenges, this article presents several\ntechniques that together build a new approach for combining AL and\nsemi-supervised learning (SSL) for support vector machines (SVM) in\nclassification tasks. Structure information is captured by means of\nprobabilistic models that are iteratively improved at runtime when label\ninformation becomes available. The probabilistic models are considered in a\nselection strategy based on distance, density, diversity, and distribution (4DS\nstrategy) information for AL and in a kernel function (Responsibility Weighted\nMahalanobis kernel) for SVM. The approach fuses generative and discriminative\nmodeling techniques. With 20 benchmark data sets and with the MNIST data set it\nis shown that our new solution yields significantly better results than\nstate-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Thu, 13 Oct 2016 09:36:56 GMT"}, {"version": "v2", "created": "Fri, 14 Oct 2016 09:43:12 GMT"}], "update_date": "2016-10-17", "authors_parsed": [["Reitmaier", "Tobias", ""], ["Calma", "Adrian", ""], ["Sick", "Bernhard", ""]]}, {"id": "1610.03996", "submitter": "Martin Wistuba", "authors": "Martin Wistuba, Nghia Duong-Trung, Nicolas Schilling, Lars\n  Schmidt-Thieme", "title": "Bank Card Usage Prediction Exploiting Geolocation Information", "comments": "Describes the winning solution for the ECML-PKDD 2016 Discovery\n  Challenge on Bank Card Usage Analysis. Final results on the private\n  leaderboard are available here:\n  https://dms.sztaki.hu/ecml-pkkd-2016/#/app/privateleaderboard", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe the solution of team ISMLL for the ECML-PKDD 2016 Discovery\nChallenge on Bank Card Usage for both tasks. Our solution is based on three\npillars. Gradient boosted decision trees as a strong regression and\nclassification model, an intensive search for good hyperparameter\nconfigurations and strong features that exploit geolocation information. This\napproach achieved the best performance on the public leaderboard for the first\ntask and a decent fourth position for the second task.\n", "versions": [{"version": "v1", "created": "Thu, 13 Oct 2016 09:44:03 GMT"}], "update_date": "2016-10-14", "authors_parsed": [["Wistuba", "Martin", ""], ["Duong-Trung", "Nghia", ""], ["Schilling", "Nicolas", ""], ["Schmidt-Thieme", "Lars", ""]]}, {"id": "1610.04019", "submitter": "Chin-Cheng Hsu", "authors": "Chin-Cheng Hsu, Hsin-Te Hwang, Yi-Chiao Wu, Yu Tsao and Hsin-Min Wang", "title": "Voice Conversion from Non-parallel Corpora Using Variational\n  Auto-encoder", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG cs.SD", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a flexible framework for spectral conversion (SC) that facilitates\ntraining with unaligned corpora. Many SC frameworks require parallel corpora,\nphonetic alignments, or explicit frame-wise correspondence for learning\nconversion functions or for synthesizing a target spectrum with the aid of\nalignments. However, these requirements gravely limit the scope of practical\napplications of SC due to scarcity or even unavailability of parallel corpora.\nWe propose an SC framework based on variational auto-encoder which enables us\nto exploit non-parallel corpora. The framework comprises an encoder that learns\nspeaker-independent phonetic representations and a decoder that learns to\nreconstruct the designated speaker. It removes the requirement of parallel\ncorpora or phonetic alignments to train a spectral conversion system. We report\nobjective and subjective evaluations to validate our proposed method and\ncompare it to SC methods that have access to aligned corpora.\n", "versions": [{"version": "v1", "created": "Thu, 13 Oct 2016 10:52:25 GMT"}], "update_date": "2016-10-14", "authors_parsed": [["Hsu", "Chin-Cheng", ""], ["Hwang", "Hsin-Te", ""], ["Wu", "Yi-Chiao", ""], ["Tsao", "Yu", ""], ["Wang", "Hsin-Min", ""]]}, {"id": "1610.04032", "submitter": "Francois Fleuret", "authors": "Fran\\c{c}ois Fleuret", "title": "Predicting the dynamics of 2d objects with a deep residual network", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  We investigate how a residual network can learn to predict the dynamics of\ninteracting shapes purely as an image-to-image regression task.\n  With a simple 2d physics simulator, we generate short sequences composed of\nrectangles put in motion by applying a pulling force at a point picked at\nrandom. The network is trained with a quadratic loss to predict the image of\nthe resulting configuration, given the image of the starting configuration and\nan image indicating the point of grasping.\n  Experiments show that the network learns to predict accurately the resulting\nimage, which implies in particular that (1) it segments rectangles as distinct\ncomponents, (2) it infers which one contains the grasping point, (3) it models\nproperly the dynamic of a single rectangle, including the torque, (4) it\ndetects and handles collisions to some extent, and (5) it re-synthesizes\nproperly the entire scene with displaced rectangles.\n", "versions": [{"version": "v1", "created": "Thu, 13 Oct 2016 11:27:07 GMT"}, {"version": "v2", "created": "Thu, 24 Nov 2016 11:12:52 GMT"}], "update_date": "2016-11-28", "authors_parsed": [["Fleuret", "Fran\u00e7ois", ""]]}, {"id": "1610.04042", "submitter": "Georgios Chasparis", "authors": "Thomas Grubinger, Georgios Chasparis, Thomas Natschlaeger", "title": "Generalized Online Transfer Learning for Climate Control in Residential\n  Buildings", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SY cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents an online transfer learning framework for improving\ntemperature predictions in residential buildings. In transfer learning,\nprediction models trained under a set of available data from a target domain\n(e.g., house with limited data) can be improved through the use of data\ngenerated from similar source domains (e.g., houses with rich data). Given also\nthe need for prediction models that can be trained online (e.g., as part of a\nmodel-predictive-control implementation), this paper introduces the generalized\nonline transfer learning algorithm (GOTL). It employs a weighted combination of\nthe available predictors (i.e., the target and source predictors) and\nguarantees convergence to the best weighted predictor. Furthermore, the use of\nTransfer Component Analysis (TCA) allows for using more than a single source\ndomains, since it may facilitate the fit of a single model on more than one\nsource domains (houses). This allows GOTL to transfer knowledge from more than\none source domains. We further validate our results through experiments in\nclimate control for residential buildings and show that GOTL may lead to\nnon-negligible energy savings for given comfort levels.\n", "versions": [{"version": "v1", "created": "Thu, 13 Oct 2016 11:54:43 GMT"}], "update_date": "2016-10-14", "authors_parsed": [["Grubinger", "Thomas", ""], ["Chasparis", "Georgios", ""], ["Natschlaeger", "Thomas", ""]]}, {"id": "1610.04086", "submitter": "Zhi-Hua Zhou", "authors": "Ming Pang and Wei Gao and Min Tao and Zhi-Hua Zhou", "title": "Unorganized Malicious Attacks Detection", "comments": null, "journal-ref": "NIPS 2018", "doi": null, "report-no": null, "categories": "cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recommender system has attracted much attention during the past decade. Many\nattack detection algorithms have been developed for better recommendations,\nmostly focusing on shilling attacks, where an attack organizer produces a large\nnumber of user profiles by the same strategy to promote or demote an item. This\nwork considers a different attack style: unorganized malicious attacks, where\nattackers individually utilize a small number of user profiles to attack\ndifferent items without any organizer. This attack style occurs in many real\napplications, yet relevant study remains open. We first formulate the\nunorganized malicious attacks detection as a matrix completion problem, and\npropose the Unorganized Malicious Attacks detection (UMA) approach, a proximal\nalternating splitting augmented Lagrangian method. We verify, both\ntheoretically and empirically, the effectiveness of our proposed approach.\n", "versions": [{"version": "v1", "created": "Thu, 13 Oct 2016 14:02:49 GMT"}, {"version": "v2", "created": "Wed, 14 Feb 2018 14:42:32 GMT"}, {"version": "v3", "created": "Sun, 18 Feb 2018 00:47:15 GMT"}], "update_date": "2020-07-07", "authors_parsed": [["Pang", "Ming", ""], ["Gao", "Wei", ""], ["Tao", "Min", ""], ["Zhou", "Zhi-Hua", ""]]}, {"id": "1610.04154", "submitter": "Sergio Ram\\'irez-Gallego", "authors": "Sergio Ram\\'irez-Gallego, H\\'ector Mouri\\~no-Tal\\'in, David\n  Mart\\'inez-Rego, Ver\\'onica Bol\\'on-Canedo, Jos\\'e Manuel Ben\\'itez, Amparo\n  Alonso-Betanzos, Francisco Herrera", "title": "An Information Theoretic Feature Selection Framework for Big Data under\n  Apache Spark", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.DC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the advent of extremely high dimensional datasets, dimensionality\nreduction techniques are becoming mandatory. Among many techniques, feature\nselection has been growing in interest as an important tool to identify\nrelevant features on huge datasets --both in number of instances and\nfeatures--. The purpose of this work is to demonstrate that standard feature\nselection methods can be parallelized in Big Data platforms like Apache Spark,\nboosting both performance and accuracy. We thus propose a distributed\nimplementation of a generic feature selection framework which includes a wide\ngroup of well-known Information Theoretic methods. Experimental results on a\nwide set of real-world datasets show that our distributed framework is capable\nof dealing with ultra-high dimensional datasets as well as those with a huge\nnumber of samples in a short period of time, outperforming the sequential\nversion in all the cases studied.\n", "versions": [{"version": "v1", "created": "Thu, 13 Oct 2016 16:17:07 GMT"}, {"version": "v2", "created": "Wed, 19 Oct 2016 16:46:28 GMT"}], "update_date": "2016-10-20", "authors_parsed": [["Ram\u00edrez-Gallego", "Sergio", ""], ["Mouri\u00f1o-Tal\u00edn", "H\u00e9ctor", ""], ["Mart\u00ednez-Rego", "David", ""], ["Bol\u00f3n-Canedo", "Ver\u00f3nica", ""], ["Ben\u00edtez", "Jos\u00e9 Manuel", ""], ["Alonso-Betanzos", "Amparo", ""], ["Herrera", "Francisco", ""]]}, {"id": "1610.04161", "submitter": "Shiyu Liang", "authors": "Shiyu Liang and R. Srikant", "title": "Why Deep Neural Networks for Function Approximation?", "comments": "The paper is published at the 5th International Conference on\n  Learning Representations (ICLR)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Recently there has been much interest in understanding why deep neural\nnetworks are preferred to shallow networks. We show that, for a large class of\npiecewise smooth functions, the number of neurons needed by a shallow network\nto approximate a function is exponentially larger than the corresponding number\nof neurons needed by a deep network for a given degree of function\napproximation. First, we consider univariate functions on a bounded interval\nand require a neural network to achieve an approximation error of $\\varepsilon$\nuniformly over the interval. We show that shallow networks (i.e., networks\nwhose depth does not depend on $\\varepsilon$) require\n$\\Omega(\\text{poly}(1/\\varepsilon))$ neurons while deep networks (i.e.,\nnetworks whose depth grows with $1/\\varepsilon$) require\n$\\mathcal{O}(\\text{polylog}(1/\\varepsilon))$ neurons. We then extend these\nresults to certain classes of important multivariate functions. Our results are\nderived for neural networks which use a combination of rectifier linear units\n(ReLUs) and binary step units, two of the most popular type of activation\nfunctions. Our analysis builds on a simple observation: the multiplication of\ntwo bits can be represented by a ReLU.\n", "versions": [{"version": "v1", "created": "Thu, 13 Oct 2016 16:34:30 GMT"}, {"version": "v2", "created": "Fri, 3 Mar 2017 20:43:04 GMT"}], "update_date": "2017-03-07", "authors_parsed": [["Liang", "Shiyu", ""], ["Srikant", "R.", ""]]}, {"id": "1610.04167", "submitter": "Or Sharir", "authors": "Or Sharir, Ronen Tamari, Nadav Cohen and Amnon Shashua", "title": "Tensorial Mixture Models", "comments": "A git repository for reproducing our experiments is available at:\n  https://github.com/HUJI-Deep/Generative-ConvACs", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Casting neural networks in generative frameworks is a highly sought-after\nendeavor these days. Contemporary methods, such as Generative Adversarial\nNetworks, capture some of the generative capabilities, but not all. In\nparticular, they lack the ability of tractable marginalization, and thus are\nnot suitable for many tasks. Other methods, based on arithmetic circuits and\nsum-product networks, do allow tractable marginalization, but their performance\nis challenged by the need to learn the structure of a circuit. Building on the\ntractability of arithmetic circuits, we leverage concepts from tensor analysis,\nand derive a family of generative models we call Tensorial Mixture Models\n(TMMs). TMMs assume a simple convolutional network structure, and in addition,\nlend themselves to theoretical analyses that allow comprehensive understanding\nof the relation between their structure and their expressive properties. We\nthus obtain a generative model that is tractable on one hand, and on the other\nhand, allows effective representation of rich distributions in an easily\ncontrolled manner. These two capabilities are brought together in the task of\nclassification under missing data, where TMMs deliver state of the art\naccuracies with seamless implementation and design.\n", "versions": [{"version": "v1", "created": "Thu, 13 Oct 2016 16:43:32 GMT"}, {"version": "v2", "created": "Thu, 17 Nov 2016 12:13:45 GMT"}, {"version": "v3", "created": "Mon, 6 Mar 2017 07:50:12 GMT"}, {"version": "v4", "created": "Tue, 21 Mar 2017 09:08:31 GMT"}, {"version": "v5", "created": "Sun, 25 Mar 2018 09:42:59 GMT"}], "update_date": "2018-03-28", "authors_parsed": [["Sharir", "Or", ""], ["Tamari", "Ronen", ""], ["Cohen", "Nadav", ""], ["Shashua", "Amnon", ""]]}, {"id": "1610.04210", "submitter": "Sohail Bahmani", "authors": "Sohail Bahmani and Justin Romberg", "title": "Phase Retrieval Meets Statistical Learning Theory: A Flexible Convex\n  Relaxation", "comments": "Accepted in AISTATS 2017. Extended the discussion of related work and\n  added a few more references. Clarified some of the statements and notations", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.LG math.FA math.IT math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a flexible convex relaxation for the phase retrieval problem that\noperates in the natural domain of the signal. Therefore, we avoid the\nprohibitive computational cost associated with \"lifting\" and semidefinite\nprogramming (SDP) in methods such as PhaseLift and compete with recently\ndeveloped non-convex techniques for phase retrieval. We relax the quadratic\nequations for phaseless measurements to inequality constraints each of which\nrepresenting a symmetric \"slab\". Through a simple convex program, our proposed\nestimator finds an extreme point of the intersection of these slabs that is\nbest aligned with a given anchor vector. We characterize geometric conditions\nthat certify success of the proposed estimator. Furthermore, using classic\nresults in statistical learning theory, we show that for random measurements\nthe geometric certificates hold with high probability at an optimal sample\ncomplexity. Phase transition of our estimator is evaluated through simulations.\nOur numerical experiments also suggest that the proposed method can solve phase\nretrieval problems with coded diffraction measurements as well.\n", "versions": [{"version": "v1", "created": "Thu, 13 Oct 2016 19:35:28 GMT"}, {"version": "v2", "created": "Thu, 16 Mar 2017 16:20:57 GMT"}], "update_date": "2017-03-17", "authors_parsed": [["Bahmani", "Sohail", ""], ["Romberg", "Justin", ""]]}, {"id": "1610.04286", "submitter": "Andrei Rusu", "authors": "Andrei A. Rusu, Mel Vecerik, Thomas Roth\\\"orl, Nicolas Heess, Razvan\n  Pascanu, Raia Hadsell", "title": "Sim-to-Real Robot Learning from Pixels with Progressive Nets", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Applying end-to-end learning to solve complex, interactive, pixel-driven\ncontrol tasks on a robot is an unsolved problem. Deep Reinforcement Learning\nalgorithms are too slow to achieve performance on a real robot, but their\npotential has been demonstrated in simulated environments. We propose using\nprogressive networks to bridge the reality gap and transfer learned policies\nfrom simulation to the real world. The progressive net approach is a general\nframework that enables reuse of everything from low-level visual features to\nhigh-level policies for transfer to new tasks, enabling a compositional, yet\nsimple, approach to building complex skills. We present an early demonstration\nof this approach with a number of experiments in the domain of robot\nmanipulation that focus on bridging the reality gap. Unlike other proposed\napproaches, our real-world experiments demonstrate successful task learning\nfrom raw visual input on a fully actuated robot manipulator. Moreover, rather\nthan relying on model-based trajectory optimisation, the task learning is\naccomplished using only deep reinforcement learning and sparse rewards.\n", "versions": [{"version": "v1", "created": "Thu, 13 Oct 2016 22:42:10 GMT"}, {"version": "v2", "created": "Tue, 22 May 2018 08:56:59 GMT"}], "update_date": "2018-05-23", "authors_parsed": [["Rusu", "Andrei A.", ""], ["Vecerik", "Mel", ""], ["Roth\u00f6rl", "Thomas", ""], ["Heess", "Nicolas", ""], ["Pascanu", "Razvan", ""], ["Hadsell", "Raia", ""]]}, {"id": "1610.04317", "submitter": "Ankur Moitra", "authors": "Ankur Moitra", "title": "Approximate Counting, the Lovasz Local Lemma and Inference in Graphical\n  Models", "comments": "25 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we introduce a new approach for approximately counting in\nbounded degree systems with higher-order constraints. Our main result is an\nalgorithm to approximately count the number of solutions to a CNF formula\n$\\Phi$ when the width is logarithmic in the maximum degree. This closes an\nexponential gap between the known upper and lower bounds.\n  Moreover our algorithm extends straightforwardly to approximate sampling,\nwhich shows that under Lov\\'asz Local Lemma-like conditions it is not only\npossible to find a satisfying assignment, it is also possible to generate one\napproximately uniformly at random from the set of all satisfying assignments.\nOur approach is a significant departure from earlier techniques in approximate\ncounting, and is based on a framework to bootstrap an oracle for computing\nmarginal probabilities on individual variables. Finally, we give an application\nof our results to show that it is algorithmically possible to sample from the\nposterior distribution in an interesting class of graphical models.\n", "versions": [{"version": "v1", "created": "Fri, 14 Oct 2016 03:44:12 GMT"}, {"version": "v2", "created": "Thu, 16 Mar 2017 01:30:09 GMT"}], "update_date": "2017-03-17", "authors_parsed": [["Moitra", "Ankur", ""]]}, {"id": "1610.04336", "submitter": "Michael Brand", "authors": "Michael Brand", "title": "MML is not consistent for Neyman-Scott", "comments": "16 pages, 0 tables, 0 figures", "journal-ref": null, "doi": "10.1109/TIT.2019.2943464", "report-no": null, "categories": "stat.ML cs.LG math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Strict Minimum Message Length (SMML) is an information-theoretic statistical\ninference method widely cited (but only with informal arguments) as providing\nestimations that are consistent for general estimation problems. It is,\nhowever, almost invariably intractable to compute, for which reason only\napproximations of it (known as MML algorithms) are ever used in practice. Using\nnovel techniques that allow for the first time direct, non-approximated\nanalysis of SMML solutions, we investigate the Neyman-Scott estimation problem,\nan oft-cited showcase for the consistency of MML, and show that even with a\nnatural choice of prior neither SMML nor its popular approximations are\nconsistent for it, thereby providing a counterexample to the general claim.\nThis is the first known explicit construction of an SMML solution for a\nnatural, high-dimensional problem.\n", "versions": [{"version": "v1", "created": "Fri, 14 Oct 2016 06:07:45 GMT"}, {"version": "v2", "created": "Thu, 9 Feb 2017 12:04:50 GMT"}, {"version": "v3", "created": "Tue, 2 May 2017 15:25:59 GMT"}, {"version": "v4", "created": "Wed, 19 Jul 2017 13:20:52 GMT"}, {"version": "v5", "created": "Mon, 6 Jan 2020 05:43:12 GMT"}], "update_date": "2020-01-07", "authors_parsed": [["Brand", "Michael", ""]]}, {"id": "1610.04337", "submitter": "Alaa Saade", "authors": "Alaa Saade", "title": "Spectral Inference Methods on Sparse Graphs: Theory and Applications", "comments": "PhD dissertation", "journal-ref": null, "doi": null, "report-no": null, "categories": "cond-mat.dis-nn cs.IT cs.LG math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In an era of unprecedented deluge of (mostly unstructured) data, graphs are\nproving more and more useful, across the sciences, as a flexible abstraction to\ncapture complex relationships between complex objects. One of the main\nchallenges arising in the study of such networks is the inference of\nmacroscopic, large-scale properties affecting a large number of objects, based\nsolely on the microscopic interactions between their elementary constituents.\nStatistical physics, precisely created to recover the macroscopic laws of\nthermodynamics from an idealized model of interacting particles, provides\nsignificant insight to tackle such complex networks.\n  In this dissertation, we use methods derived from the statistical physics of\ndisordered systems to design and study new algorithms for inference on graphs.\nOur focus is on spectral methods, based on certain eigenvectors of carefully\nchosen matrices, and sparse graphs, containing only a small amount of\ninformation. We develop an original theory of spectral inference based on a\nrelaxation of various mean-field free energy optimizations. Our approach is\ntherefore fully probabilistic, and contrasts with more traditional motivations\nbased on the optimization of a cost function. We illustrate the efficiency of\nour approach on various problems, including community detection, randomized\nsimilarity-based clustering, and matrix completion.\n", "versions": [{"version": "v1", "created": "Fri, 14 Oct 2016 06:18:33 GMT"}], "update_date": "2016-10-17", "authors_parsed": [["Saade", "Alaa", ""]]}, {"id": "1610.04351", "submitter": "Ryohei Hisano", "authors": "Ryohei Hisano", "title": "Semi-supervised Graph Embedding Approach to Dynamic Link Prediction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG cs.SI physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a simple discrete time semi-supervised graph embedding approach to\nlink prediction in dynamic networks. The learned embedding reflects information\nfrom both the temporal and cross-sectional network structures, which is\nperformed by defining the loss function as a weighted sum of the supervised\nloss from past dynamics and the unsupervised loss of predicting the\nneighborhood context in the current network. Our model is also capable of\nlearning different embeddings for both formation and dissolution dynamics.\nThese key aspects contributes to the predictive performance of our model and we\nprovide experiments with three real--world dynamic networks showing that our\nmethod is comparable to state of the art methods in link formation prediction\nand outperforms state of the art baseline methods in link dissolution\nprediction.\n", "versions": [{"version": "v1", "created": "Fri, 14 Oct 2016 07:44:33 GMT"}], "update_date": "2016-10-17", "authors_parsed": [["Hisano", "Ryohei", ""]]}, {"id": "1610.04420", "submitter": "Ievgen Redko", "authors": "Ievgen Redko, Amaury Habrard and Marc Sebban", "title": "Theoretical Analysis of Domain Adaptation with Optimal Transport", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Domain adaptation (DA) is an important and emerging field of machine learning\nthat tackles the problem occurring when the distributions of training (source\ndomain) and test (target domain) data are similar but different. Current\ntheoretical results show that the efficiency of DA algorithms depends on their\ncapacity of minimizing the divergence between source and target probability\ndistributions. In this paper, we provide a theoretical study on the advantages\nthat concepts borrowed from optimal transportation theory can bring to DA. In\nparticular, we show that the Wasserstein metric can be used as a divergence\nmeasure between distributions to obtain generalization guarantees for three\ndifferent learning settings: (i) classic DA with unsupervised target data (ii)\nDA combining source and target labeled data, (iii) multiple source DA. Based on\nthe obtained results, we provide some insights showing when this analysis can\nbe tighter than other existing frameworks.\n", "versions": [{"version": "v1", "created": "Fri, 14 Oct 2016 11:59:28 GMT"}, {"version": "v2", "created": "Wed, 16 Nov 2016 11:04:38 GMT"}, {"version": "v3", "created": "Tue, 25 Jul 2017 07:59:50 GMT"}, {"version": "v4", "created": "Fri, 28 Jul 2017 21:49:07 GMT"}], "update_date": "2017-08-01", "authors_parsed": [["Redko", "Ievgen", ""], ["Habrard", "Amaury", ""], ["Sebban", "Marc", ""]]}, {"id": "1610.04490", "submitter": "Casper Kaae S{\\o}nderby", "authors": "Casper Kaae S{\\o}nderby, Jose Caballero, Lucas Theis, Wenzhe Shi,\n  Ferenc Husz\\'ar", "title": "Amortised MAP Inference for Image Super-resolution", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image super-resolution (SR) is an underdetermined inverse problem, where a\nlarge number of plausible high-resolution images can explain the same\ndownsampled image. Most current single image SR methods use empirical risk\nminimisation, often with a pixel-wise mean squared error (MSE) loss. However,\nthe outputs from such methods tend to be blurry, over-smoothed and generally\nappear implausible. A more desirable approach would employ Maximum a Posteriori\n(MAP) inference, preferring solutions that always have a high probability under\nthe image prior, and thus appear more plausible. Direct MAP estimation for SR\nis non-trivial, as it requires us to build a model for the image prior from\nsamples. Furthermore, MAP inference is often performed via optimisation-based\niterative algorithms which don't compare well with the efficiency of\nneural-network-based alternatives. Here we introduce new methods for amortised\nMAP inference whereby we calculate the MAP estimate directly using a\nconvolutional neural network. We first introduce a novel neural network\narchitecture that performs a projection to the affine subspace of valid SR\nsolutions ensuring that the high resolution output of the network is always\nconsistent with the low resolution input. We show that, using this\narchitecture, the amortised MAP inference problem reduces to minimising the\ncross-entropy between two distributions, similar to training generative models.\nWe propose three methods to solve this optimisation problem: (1) Generative\nAdversarial Networks (GAN) (2) denoiser-guided SR which backpropagates\ngradient-estimates from denoising to train the network, and (3) a baseline\nmethod using a maximum-likelihood-trained image prior. Our experiments show\nthat the GAN based approach performs best on real image data. Lastly, we\nestablish a connection between GANs and amortised variational inference as in\ne.g. variational autoencoders.\n", "versions": [{"version": "v1", "created": "Fri, 14 Oct 2016 14:58:44 GMT"}, {"version": "v2", "created": "Wed, 15 Feb 2017 14:56:42 GMT"}, {"version": "v3", "created": "Tue, 21 Feb 2017 13:08:24 GMT"}], "update_date": "2017-02-28", "authors_parsed": [["S\u00f8nderby", "Casper Kaae", ""], ["Caballero", "Jose", ""], ["Theis", "Lucas", ""], ["Shi", "Wenzhe", ""], ["Husz\u00e1r", "Ferenc", ""]]}, {"id": "1610.04491", "submitter": "Tor Lattimore", "authors": "Tor Lattimore and Csaba Szepesvari", "title": "The End of Optimism? An Asymptotic Analysis of Finite-Armed Linear\n  Bandits", "comments": "13 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Stochastic linear bandits are a natural and simple generalisation of\nfinite-armed bandits with numerous practical applications. Current approaches\nfocus on generalising existing techniques for finite-armed bandits, notably the\noptimism principle and Thompson sampling. While prior work has mostly been in\nthe worst-case setting, we analyse the asymptotic instance-dependent regret and\nshow matching upper and lower bounds on what is achievable. Surprisingly, our\nresults show that no algorithm based on optimism or Thompson sampling will ever\nachieve the optimal rate, and indeed, can be arbitrarily far from optimal, even\nin very simple cases. This is a disturbing result because these techniques are\nstandard tools that are widely used for sequential optimisation. For example,\nfor generalised linear bandits and reinforcement learning.\n", "versions": [{"version": "v1", "created": "Fri, 14 Oct 2016 14:58:44 GMT"}], "update_date": "2016-10-17", "authors_parsed": [["Lattimore", "Tor", ""], ["Szepesvari", "Csaba", ""]]}, {"id": "1610.04574", "submitter": "Jure Sokolic", "authors": "Jure Sokolic, Raja Giryes, Guillermo Sapiro, Miguel R. D. Rodrigues", "title": "Generalization Error of Invariant Classifiers", "comments": "Accepted to AISTATS. This version has updated references", "journal-ref": "Conference on Artificial Intelligence and Statistics (AISTATS),\n  2017, pp. 1094-1103", "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper studies the generalization error of invariant classifiers. In\nparticular, we consider the common scenario where the classification task is\ninvariant to certain transformations of the input, and that the classifier is\nconstructed (or learned) to be invariant to these transformations. Our approach\nrelies on factoring the input space into a product of a base space and a set of\ntransformations. We show that whereas the generalization error of a\nnon-invariant classifier is proportional to the complexity of the input space,\nthe generalization error of an invariant classifier is proportional to the\ncomplexity of the base space. We also derive a set of sufficient conditions on\nthe geometry of the base space and the set of transformations that ensure that\nthe complexity of the base space is much smaller than the complexity of the\ninput space. Our analysis applies to general classifiers such as convolutional\nneural networks. We demonstrate the implications of the developed theory for\nsuch classifiers with experiments on the MNIST and CIFAR-10 datasets.\n", "versions": [{"version": "v1", "created": "Fri, 14 Oct 2016 18:40:52 GMT"}, {"version": "v2", "created": "Tue, 23 May 2017 11:32:03 GMT"}, {"version": "v3", "created": "Sun, 2 Jul 2017 18:58:21 GMT"}], "update_date": "2017-07-04", "authors_parsed": [["Sokolic", "Jure", ""], ["Giryes", "Raja", ""], ["Sapiro", "Guillermo", ""], ["Rodrigues", "Miguel R. D.", ""]]}, {"id": "1610.04576", "submitter": "Shuai Zheng", "authors": "Shuai Zheng, Chris Ding", "title": "Kernel Alignment Inspired Linear Discriminant Analysis", "comments": "Joint European Conference on Machine Learning and Knowledge Discovery\n  in Databases, ECML PKDD, 2014", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Kernel alignment measures the degree of similarity between two kernels. In\nthis paper, inspired from kernel alignment, we propose a new Linear\nDiscriminant Analysis (LDA) formulation, kernel alignment LDA (kaLDA). We first\ndefine two kernels, data kernel and class indicator kernel. The problem is to\nfind a subspace to maximize the alignment between subspace-transformed data\nkernel and class indicator kernel. Surprisingly, the kernel alignment induced\nkaLDA objective function is very similar to classical LDA and can be expressed\nusing between-class and total scatter matrices. This can be extended to\nmulti-label data. We use a Stiefel-manifold gradient descent algorithm to solve\nthis problem. We perform experiments on 8 single-label and 6 multi-label data\nsets. Results show that kaLDA has very good performance on many single-label\nand multi-label problems.\n", "versions": [{"version": "v1", "created": "Fri, 14 Oct 2016 18:48:03 GMT"}], "update_date": "2016-10-17", "authors_parsed": [["Zheng", "Shuai", ""], ["Ding", "Chris", ""]]}, {"id": "1610.04578", "submitter": "Kwang-Sung Jun", "authors": "Kwang-Sung Jun, Francesco Orabona, Rebecca Willett, Stephen Wright", "title": "Improved Strongly Adaptive Online Learning using Coin Betting", "comments": "fixed a few typos", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper describes a new parameter-free online learning algorithm for\nchanging environments. In comparing against algorithms with the same time\ncomplexity as ours, we obtain a strongly adaptive regret bound that is a factor\nof at least $\\sqrt{\\log(T)}$ better, where $T$ is the time horizon. Empirical\nresults show that our algorithm outperforms state-of-the-art methods in\nlearning with expert advice and metric learning scenarios.\n", "versions": [{"version": "v1", "created": "Fri, 14 Oct 2016 18:51:25 GMT"}, {"version": "v2", "created": "Wed, 8 Mar 2017 20:46:06 GMT"}, {"version": "v3", "created": "Mon, 7 Aug 2017 15:26:04 GMT"}], "update_date": "2017-08-08", "authors_parsed": [["Jun", "Kwang-Sung", ""], ["Orabona", "Francesco", ""], ["Willett", "Rebecca", ""], ["Wright", "Stephen", ""]]}, {"id": "1610.04599", "submitter": "Yao Xie", "authors": "Shuang Li, Yao Xie, and Le Song", "title": "Data-Driven Threshold Machine: Scan Statistics, Change-Point Detection,\n  and Extreme Bandits", "comments": "Submitted to conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a novel distribution-free approach, the data-driven threshold\nmachine (DTM), for a fundamental problem at the core of many learning tasks:\nchoose a threshold for a given pre-specified level that bounds the tail\nprobability of the maximum of a (possibly dependent but stationary) random\nsequence. We do not assume data distribution, but rather relying on the\nasymptotic distribution of extremal values, and reduce the problem to estimate\nthree parameters of the extreme value distributions and the extremal index. We\nspecially take care of data dependence via estimating extremal index since in\nmany settings, such as scan statistics, change-point detection, and extreme\nbandits, where dependence in the sequence of statistics can be significant. Key\nfeatures of our DTM also include robustness and the computational efficiency,\nand it only requires one sample path to form a reliable estimate of the\nthreshold, in contrast to the Monte Carlo sampling approach which requires\ndrawing a large number of sample paths. We demonstrate the good performance of\nDTM via numerical examples in various dependent settings.\n", "versions": [{"version": "v1", "created": "Fri, 14 Oct 2016 19:43:16 GMT"}], "update_date": "2016-10-17", "authors_parsed": [["Li", "Shuang", ""], ["Xie", "Yao", ""], ["Song", "Le", ""]]}, {"id": "1610.04658", "submitter": "Yacine Jernite", "authors": "Yacine Jernite, Anna Choromanska and David Sontag", "title": "Simultaneous Learning of Trees and Representations for Extreme\n  Classification and Density Estimation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider multi-class classification where the predictor has a hierarchical\nstructure that allows for a very large number of labels both at train and test\ntime. The predictive power of such models can heavily depend on the structure\nof the tree, and although past work showed how to learn the tree structure, it\nexpected that the feature vectors remained static. We provide a novel algorithm\nto simultaneously perform representation learning for the input data and\nlearning of the hierarchi- cal predictor. Our approach optimizes an objec- tive\nfunction which favors balanced and easily- separable multi-way node partitions.\nWe theoret- ically analyze this objective, showing that it gives rise to a\nboosting style property and a bound on classification error. We next show how\nto extend the algorithm to conditional density estimation. We empirically\nvalidate both variants of the al- gorithm on text classification and language\nmod- eling, respectively, and show that they compare favorably to common\nbaselines in terms of accu- racy and running time.\n", "versions": [{"version": "v1", "created": "Fri, 14 Oct 2016 22:03:15 GMT"}, {"version": "v2", "created": "Thu, 2 Mar 2017 20:33:14 GMT"}], "update_date": "2017-03-06", "authors_parsed": [["Jernite", "Yacine", ""], ["Choromanska", "Anna", ""], ["Sontag", "David", ""]]}, {"id": "1610.04668", "submitter": "Shuai Zheng", "authors": "Shuai Zheng, Xiao Cai, Chris Ding, Feiping Nie, Heng Huang", "title": "A Closed Form Solution to Multi-View Low-Rank Regression", "comments": "Twenty-Ninth AAAI Conference on Artificial Intelligence, AAAI, 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Real life data often includes information from different channels. For\nexample, in computer vision, we can describe an image using different image\nfeatures, such as pixel intensity, color, HOG, GIST feature, SIFT features,\netc.. These different aspects of the same objects are often called multi-view\n(or multi-modal) data. Low-rank regression model has been proved to be an\neffective learning mechanism by exploring the low-rank structure of real life\ndata. But previous low-rank regression model only works on single view data. In\nthis paper, we propose a multi-view low-rank regression model by imposing\nlow-rank constraints on multi-view regression model. Most importantly, we\nprovide a closed-form solution to the multi-view low-rank regression model.\nExtensive experiments on 4 multi-view datasets show that the multi-view\nlow-rank regression model outperforms single-view regression model and reveals\nthat multi-view low-rank structure is very helpful.\n", "versions": [{"version": "v1", "created": "Fri, 14 Oct 2016 23:43:47 GMT"}], "update_date": "2016-10-18", "authors_parsed": [["Zheng", "Shuai", ""], ["Cai", "Xiao", ""], ["Ding", "Chris", ""], ["Nie", "Feiping", ""], ["Huang", "Heng", ""]]}, {"id": "1610.04718", "submitter": "Andrey Vasnetsov", "authors": "Roman Samarev, Andrey Vasnetsov, Elizaveta Smelkova", "title": "Generalization of metric classification algorithms for sequences\n  classification and labelling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The article deals with the issue of modification of metric classification\nalgorithms. In particular, it studies the algorithm k-Nearest Neighbours for\nits application to sequential data. A method of generalization of metric\nclassification algorithms is proposed. As a part of it, there has been\ndeveloped an algorithm for solving the problem of classification and labelling\nof sequential data. The advantages of the developed algorithm of classification\nin comparison with the existing one are also discussed in the article. There is\na comparison of the effectiveness of the proposed algorithm with the algorithm\nof CRF in the task of chunking in the open data set CoNLL2000.\n", "versions": [{"version": "v1", "created": "Sat, 15 Oct 2016 11:04:44 GMT"}, {"version": "v2", "created": "Fri, 16 Dec 2016 05:32:34 GMT"}], "update_date": "2016-12-19", "authors_parsed": [["Samarev", "Roman", ""], ["Vasnetsov", "Andrey", ""], ["Smelkova", "Elizaveta", ""]]}, {"id": "1610.04782", "submitter": "Wittawat Jitkrittum", "authors": "Wittawat Jitkrittum, Zoltan Szabo, Arthur Gretton", "title": "An Adaptive Test of Independence with Analytic Kernel Embeddings", "comments": "8 pages of main text", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A new computationally efficient dependence measure, and an adaptive\nstatistical test of independence, are proposed. The dependence measure is the\ndifference between analytic embeddings of the joint distribution and the\nproduct of the marginals, evaluated at a finite set of locations (features).\nThese features are chosen so as to maximize a lower bound on the test power,\nresulting in a test that is data-efficient, and that runs in linear time (with\nrespect to the sample size n). The optimized features can be interpreted as\nevidence to reject the null hypothesis, indicating regions in the joint domain\nwhere the joint distribution and the product of the marginals differ most.\nConsistency of the independence test is established, for an appropriate choice\nof features. In real-world benchmarks, independence tests using the optimized\nfeatures perform comparably to the state-of-the-art quadratic-time HSIC test,\nand outperform competing O(n) and O(n log n) tests.\n", "versions": [{"version": "v1", "created": "Sat, 15 Oct 2016 20:19:48 GMT"}], "update_date": "2016-10-18", "authors_parsed": [["Jitkrittum", "Wittawat", ""], ["Szabo", "Zoltan", ""], ["Gretton", "Arthur", ""]]}, {"id": "1610.04783", "submitter": "Maria-Irina Nicolae", "authors": "Maria-Irina Nicolae, \\'Eric Gaussier, Amaury Habrard, Marc Sebban", "title": "Similarity Learning for Time Series Classification", "comments": "Techreport", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multivariate time series naturally exist in many fields, like energy,\nbioinformatics, signal processing, and finance. Most of these applications need\nto be able to compare these structured data. In this context, dynamic time\nwarping (DTW) is probably the most common comparison measure. However, not much\nresearch effort has been put into improving it by learning. In this paper, we\npropose a novel method for learning similarities based on DTW, in order to\nimprove time series classification. Making use of the uniform stability\nframework, we provide the first theoretical guarantees in the form of a\ngeneralization bound for linear classification. The experimental study shows\nthat the proposed approach is efficient, while yielding sparse classifiers.\n", "versions": [{"version": "v1", "created": "Sat, 15 Oct 2016 20:37:52 GMT"}], "update_date": "2016-10-18", "authors_parsed": [["Nicolae", "Maria-Irina", ""], ["Gaussier", "\u00c9ric", ""], ["Habrard", "Amaury", ""], ["Sebban", "Marc", ""]]}, {"id": "1610.04794", "submitter": "Bo Yang", "authors": "Bo Yang, Xiao Fu, Nicholas D. Sidiropoulos, Mingyi Hong", "title": "Towards K-means-friendly Spaces: Simultaneous Deep Learning and\n  Clustering", "comments": "Final ICML2017 version. Main paper: 10 pages; Supplementary material:\n  4 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most learning approaches treat dimensionality reduction (DR) and clustering\nseparately (i.e., sequentially), but recent research has shown that optimizing\nthe two tasks jointly can substantially improve the performance of both. The\npremise behind the latter genre is that the data samples are obtained via\nlinear transformation of latent representations that are easy to cluster; but\nin practice, the transformation from the latent space to the data can be more\ncomplicated. In this work, we assume that this transformation is an unknown and\npossibly nonlinear function. To recover the `clustering-friendly' latent\nrepresentations and to better cluster the data, we propose a joint DR and\nK-means clustering approach in which DR is accomplished via learning a deep\nneural network (DNN). The motivation is to keep the advantages of jointly\noptimizing the two tasks, while exploiting the deep neural network's ability to\napproximate any nonlinear function. This way, the proposed approach can work\nwell for a broad class of generative models. Towards this end, we carefully\ndesign the DNN structure and the associated joint optimization criterion, and\npropose an effective and scalable algorithm to handle the formulated\noptimization problem. Experiments using different real datasets are employed to\nshowcase the effectiveness of the proposed approach.\n", "versions": [{"version": "v1", "created": "Sat, 15 Oct 2016 22:51:06 GMT"}, {"version": "v2", "created": "Tue, 13 Jun 2017 22:40:26 GMT"}], "update_date": "2017-06-15", "authors_parsed": [["Yang", "Bo", ""], ["Fu", "Xiao", ""], ["Sidiropoulos", "Nicholas D.", ""], ["Hong", "Mingyi", ""]]}, {"id": "1610.04795", "submitter": "Rika Antonova", "authors": "Rika Antonova, Akshara Rai, Christopher G. Atkeson", "title": "Sample Efficient Optimization for Learning Controllers for Bipedal\n  Locomotion", "comments": "To appear in International Conference on Humanoid Robots (Humanoids\n  '2016), IEEE-RAS. (Rika Antonova and Akshara Rai contributed equally)", "journal-ref": null, "doi": "10.1109/HUMANOIDS.2016.7803249", "report-no": null, "categories": "cs.RO cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning policies for bipedal locomotion can be difficult, as experiments are\nexpensive and simulation does not usually transfer well to hardware. To counter\nthis, we need al- gorithms that are sample efficient and inherently safe.\nBayesian Optimization is a powerful sample-efficient tool for optimizing\nnon-convex black-box functions. However, its performance can degrade in higher\ndimensions. We develop a distance metric for bipedal locomotion that enhances\nthe sample-efficiency of Bayesian Optimization and use it to train a 16\ndimensional neuromuscular model for planar walking. This distance metric\nreflects some basic gait features of healthy walking and helps us quickly\neliminate a majority of unstable controllers. With our approach we can learn\npolicies for walking in less than 100 trials for a range of challenging\nsettings. In simulation, we show results on two different costs and on various\nterrains including rough ground and ramps, sloping upwards and downwards. We\nalso perturb our models with unknown inertial disturbances analogous with\ndifferences between simulation and hardware. These results are promising, as\nthey indicate that this method can potentially be used to learn control\npolicies on hardware.\n", "versions": [{"version": "v1", "created": "Sat, 15 Oct 2016 22:53:04 GMT"}], "update_date": "2018-10-11", "authors_parsed": [["Antonova", "Rika", ""], ["Rai", "Akshara", ""], ["Atkeson", "Christopher G.", ""]]}, {"id": "1610.04804", "submitter": "Zhen Han", "authors": "Zhen Han and Alyson Wilson", "title": "Dynamic Stacked Generalization for Node Classification on Networks", "comments": "9 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG cs.SI stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel stacked generalization (stacking) method as a dynamic\nensemble technique using a pool of heterogeneous classifiers for node label\nclassification on networks. The proposed method assigns component models a set\nof functional coefficients, which can vary smoothly with certain topological\nfeatures of a node. Compared to the traditional stacking model, the proposed\nmethod can dynamically adjust the weights of individual models as we move\nacross the graph and provide a more versatile and significantly more accurate\nstacking model for label prediction on a network. We demonstrate the benefits\nof the proposed model using both a simulation study and real data analysis.\n", "versions": [{"version": "v1", "created": "Sun, 16 Oct 2016 00:47:21 GMT"}], "update_date": "2016-10-18", "authors_parsed": [["Han", "Zhen", ""], ["Wilson", "Alyson", ""]]}, {"id": "1610.04900", "submitter": "Cheng Tang", "authors": "Cheng Tang, Claire Monteleoni", "title": "Convergence rate of stochastic k-means", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We analyze online and mini-batch k-means variants. Both scale up the widely\nused Lloyd 's algorithm via stochastic approximation, and have become popular\nfor large-scale clustering and unsupervised feature learning. We show, for the\nfirst time, that they have global convergence towards local optima at\n$O(\\frac{1}{t})$ rate under general conditions. In addition, we show if the\ndataset is clusterable, with suitable initialization, mini-batch k-means\nconverges to an optimal k-means solution with $O(\\frac{1}{t})$ convergence rate\nwith high probability. The k-means objective is non-convex and\nnon-differentiable: we exploit ideas from non-convex gradient-based\noptimization by providing a novel characterization of the trajectory of k-means\nalgorithm on its solution space, and circumvent its non-differentiability via\ngeometric insights about k-means update.\n", "versions": [{"version": "v1", "created": "Sun, 16 Oct 2016 18:59:59 GMT"}, {"version": "v2", "created": "Mon, 7 Nov 2016 18:20:06 GMT"}], "update_date": "2016-11-08", "authors_parsed": [["Tang", "Cheng", ""], ["Monteleoni", "Claire", ""]]}, {"id": "1610.04929", "submitter": "Li Wang", "authors": "Li Wang", "title": "Probabilistic Dimensionality Reduction via Structure Learning", "comments": "32 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel probabilistic dimensionality reduction framework that can\nnaturally integrate the generative model and the locality information of data.\nBased on this framework, we present a new model, which is able to learn a\nsmooth skeleton of embedding points in a low-dimensional space from\nhigh-dimensional noisy data. The formulation of the new model can be\nequivalently interpreted as two coupled learning problem, i.e., structure\nlearning and the learning of projection matrix. This interpretation motivates\nthe learning of the embedding points that can directly form an explicit graph\nstructure. We develop a new method to learn the embedding points that form a\nspanning tree, which is further extended to obtain a discriminative and compact\nfeature representation for clustering problems. Unlike traditional clustering\nmethods, we assume that centers of clusters should be close to each other if\nthey are connected in a learned graph, and other cluster centers should be\ndistant. This can greatly facilitate data visualization and scientific\ndiscovery in downstream analysis. Extensive experiments are performed that\ndemonstrate that the proposed framework is able to obtain discriminative\nfeature representations, and correctly recover the intrinsic structures of\nvarious real-world datasets.\n", "versions": [{"version": "v1", "created": "Sun, 16 Oct 2016 23:37:26 GMT"}], "update_date": "2016-10-18", "authors_parsed": [["Wang", "Li", ""]]}, {"id": "1610.05009", "submitter": "Saurav Gupta", "authors": "Saurav Gupta, Nitin Anand Shrivastava, Abbas Khosravi, Bijaya Ketan\n  Panigrahi", "title": "Wind ramp event prediction with parallelized Gradient Boosted Regression\n  Trees", "comments": "IJCNN 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Accurate prediction of wind ramp events is critical for ensuring the\nreliability and stability of the power systems with high penetration of wind\nenergy. This paper proposes a classification based approach for estimating the\nfuture class of wind ramp event based on certain thresholds. A parallelized\ngradient boosted regression tree based technique has been proposed to\naccurately classify the normal as well as rare extreme wind power ramp events.\nThe model has been validated using wind power data obtained from the National\nRenewable Energy Laboratory database. Performance comparison with several\nbenchmark techniques indicates the superiority of the proposed technique in\nterms of superior classification accuracy.\n", "versions": [{"version": "v1", "created": "Mon, 17 Oct 2016 08:29:57 GMT"}], "update_date": "2016-10-18", "authors_parsed": [["Gupta", "Saurav", ""], ["Shrivastava", "Nitin Anand", ""], ["Khosravi", "Abbas", ""], ["Panigrahi", "Bijaya Ketan", ""]]}, {"id": "1610.05036", "submitter": "Itir Onal Ertugrul", "authors": "Itir Onal Ertugrul and Mete Ozay and Fatos T. Yarman Vural", "title": "Encoding the Local Connectivity Patterns of fMRI for Cognitive State\n  Classification", "comments": "8 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we propose a novel framework to encode the local connectivity\npatterns of brain, using Fisher Vectors (FV), Vector of Locally Aggregated\nDescriptors (VLAD) and Bag-of-Words (BoW) methods. We first obtain local\ndescriptors, called Mesh Arc Descriptors (MADs) from fMRI data, by forming\nlocal meshes around anatomical regions, and estimating their relationship\nwithin a neighborhood. Then, we extract a dictionary of relationships, called\n\\textit{brain connectivity dictionary} by fitting a generative Gaussian mixture\nmodel (GMM) to a set of MADs, and selecting the codewords at the mean of each\ncomponent of the mixture. Codewords represent the connectivity patterns among\nanatomical regions. We also encode MADs by VLAD and BoW methods using the\nk-Means clustering.\n  We classify the cognitive states of Human Connectome Project (HCP) task fMRI\ndataset, where we train support vector machines (SVM) by the encoded MADs.\nResults demonstrate that, FV encoding of MADs can be successfully employed for\nclassification of cognitive tasks, and outperform the VLAD and BoW\nrepresentations. Moreover, we identify the significant Gaussians in mixture\nmodels by computing energy of their corresponding FV parts, and analyze their\neffect on classification accuracy. Finally, we suggest a new method to\nvisualize the codewords of brain connectivity dictionary.\n", "versions": [{"version": "v1", "created": "Mon, 17 Oct 2016 10:08:09 GMT"}], "update_date": "2016-10-18", "authors_parsed": [["Ertugrul", "Itir Onal", ""], ["Ozay", "Mete", ""], ["Vural", "Fatos T. Yarman", ""]]}, {"id": "1610.05083", "submitter": "Babak Hosseini", "authors": "Babak Hosseini, and Barbara Hammer", "title": "Efficient Metric Learning for the Analysis of Motion Data", "comments": "23 pages, 8 figures, DSAA 2015 conference", "journal-ref": null, "doi": "10.1109/DSAA.2015.7344819", "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate metric learning in the context of dynamic time warping (DTW),\nthe by far most popular dissimilarity measure used for the comparison and\nanalysis of motion capture data. While metric learning enables a\nproblem-adapted representation of data, the majority of methods has been\nproposed for vectorial data only. In this contribution, we extend the popular\nprinciple offered by the large margin nearest neighbors learner (LMNN) to DTW\nby treating the resulting component-wise dissimilarity values as features. We\ndemonstrate that this principle greatly enhances the classification accuracy in\nseveral benchmarks. Further, we show that recent auxiliary concepts such as\nmetric regularization can be transferred from the vectorial case to\ncomponent-wise DTW in a similar way. We illustrate that metric regularization\nconstitutes a crucial prerequisite for the interpretation of the resulting\nrelevance profiles.\n", "versions": [{"version": "v1", "created": "Mon, 17 Oct 2016 12:47:20 GMT"}, {"version": "v2", "created": "Tue, 18 Oct 2016 09:30:40 GMT"}, {"version": "v3", "created": "Tue, 12 Mar 2019 15:29:07 GMT"}], "update_date": "2019-03-13", "authors_parsed": [["Hosseini", "Babak", ""], ["Hammer", "Barbara", ""]]}, {"id": "1610.05120", "submitter": "G\\'abor Braun", "authors": "G\\'abor Braun, Sebastian Pokutta, Daniel Zink", "title": "Lazifying Conditional Gradient Algorithms", "comments": "25 pages and 31 pages of computational results", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Conditional gradient algorithms (also often called Frank-Wolfe algorithms)\nare popular due to their simplicity of only requiring a linear optimization\noracle and more recently they also gained significant traction for online\nlearning. While simple in principle, in many cases the actual implementation of\nthe linear optimization oracle is costly. We show a general method to lazify\nvarious conditional gradient algorithms, which in actual computations leads to\nseveral orders of magnitude of speedup in wall-clock time. This is achieved by\nusing a faster separation oracle instead of a linear optimization oracle,\nrelying only on few linear optimization oracle calls.\n", "versions": [{"version": "v1", "created": "Mon, 17 Oct 2016 14:01:25 GMT"}, {"version": "v2", "created": "Fri, 30 Dec 2016 16:53:44 GMT"}, {"version": "v3", "created": "Fri, 2 Mar 2018 17:43:43 GMT"}, {"version": "v4", "created": "Wed, 5 Sep 2018 15:24:14 GMT"}], "update_date": "2018-09-06", "authors_parsed": [["Braun", "G\u00e1bor", ""], ["Pokutta", "Sebastian", ""], ["Zink", "Daniel", ""]]}, {"id": "1610.05129", "submitter": "Wen Sun", "authors": "Wen Sun, Debadeepta Dey, and Ashish Kapoor", "title": "Risk-Aware Algorithms for Adversarial Contextual Bandits", "comments": "28 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work we consider adversarial contextual bandits with risk\nconstraints. At each round, nature prepares a context, a cost for each arm, and\nadditionally a risk for each arm. The learner leverages the context to pull an\narm and then receives the corresponding cost and risk associated with the\npulled arm. In addition to minimizing the cumulative cost, the learner also\nneeds to satisfy long-term risk constraints -- the average of the cumulative\nrisk from all pulled arms should not be larger than a pre-defined threshold. To\naddress this problem, we first study the full information setting where in each\nround the learner receives an adversarial convex loss and a convex constraint.\nWe develop a meta algorithm leveraging online mirror descent for the full\ninformation setting and extend it to contextual bandit with risk constraints\nsetting using expert advice. Our algorithms can achieve near-optimal regret in\nterms of minimizing the total cost, while successfully maintaining a sublinear\ngrowth of cumulative risk constraint violation.\n", "versions": [{"version": "v1", "created": "Mon, 17 Oct 2016 14:14:43 GMT"}], "update_date": "2016-10-18", "authors_parsed": [["Sun", "Wen", ""], ["Dey", "Debadeepta", ""], ["Kapoor", "Ashish", ""]]}, {"id": "1610.05160", "submitter": "Jesse Krijthe", "authors": "Jesse H. Krijthe and Marco Loog", "title": "The Peaking Phenomenon in Semi-supervised Learning", "comments": "11 pages, 5 figures. S+SSPR 2016, M\\'erida, Mexico", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For the supervised least squares classifier, when the number of training\nobjects is smaller than the dimensionality of the data, adding more data to the\ntraining set may first increase the error rate before decreasing it. This,\npossibly counterintuitive, phenomenon is known as peaking. In this work, we\nobserve that a similar but more pronounced version of this phenomenon also\noccurs in the semi-supervised setting, where instead of labeled objects,\nunlabeled objects are added to the training set. We explain why the learning\ncurve has a more steep incline and a more gradual decline in this setting\nthrough simulation studies and by applying an approximation of the learning\ncurve based on the work by Raudys & Duin.\n", "versions": [{"version": "v1", "created": "Mon, 17 Oct 2016 15:22:43 GMT"}], "update_date": "2016-10-18", "authors_parsed": [["Krijthe", "Jesse H.", ""], ["Loog", "Marco", ""]]}, {"id": "1610.05202", "submitter": "Aur\\'elien Bellet", "authors": "Paul Vanhaesebrouck, Aur\\'elien Bellet, Marc Tommasi", "title": "Decentralized Collaborative Learning of Personalized Models over\n  Networks", "comments": "To appear in the Proceedings of the 20th International Conference on\n  Artificial Intelligence and Statistics (AISTATS 2017)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.DC cs.SY stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider a set of learning agents in a collaborative peer-to-peer network,\nwhere each agent learns a personalized model according to its own learning\nobjective. The question addressed in this paper is: how can agents improve upon\ntheir locally trained model by communicating with other agents that have\nsimilar objectives? We introduce and analyze two asynchronous gossip algorithms\nrunning in a fully decentralized manner. Our first approach, inspired from\nlabel propagation, aims to smooth pre-trained local models over the network\nwhile accounting for the confidence that each agent has in its initial model.\nIn our second approach, agents jointly learn and propagate their model by\nmaking iterative updates based on both their local dataset and the behavior of\ntheir neighbors. To optimize this challenging objective, our decentralized\nalgorithm is based on ADMM.\n", "versions": [{"version": "v1", "created": "Mon, 17 Oct 2016 16:51:49 GMT"}, {"version": "v2", "created": "Wed, 15 Feb 2017 18:32:17 GMT"}], "update_date": "2019-01-25", "authors_parsed": [["Vanhaesebrouck", "Paul", ""], ["Bellet", "Aur\u00e9lien", ""], ["Tommasi", "Marc", ""]]}, {"id": "1610.05246", "submitter": "Kai Zhang", "authors": "Kai Zhang", "title": "BET on Independence", "comments": null, "journal-ref": null, "doi": "10.1080/01621459.2018.1537921", "report-no": null, "categories": "math.ST cs.LG stat.CO stat.ME stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of nonparametric dependence detection. Many existing\nmethods may suffer severe power loss due to non-uniform consistency, which we\nillustrate with a paradox. To avoid such power loss, we approach the\nnonparametric test of independence through the new framework of binary\nexpansion statistics (BEStat) and binary expansion testing (BET), which examine\ndependence through a novel binary expansion filtration approximation of the\ncopula. Through a Hadamard transform, we find that the symmetry statistics in\nthe filtration are complete sufficient statistics for dependence. These\nstatistics are also uncorrelated under the null. By utilizing symmetry\nstatistics, the BET avoids the problem of non-uniform consistency and improves\nupon a wide class of commonly used methods (a) by achieving the minimax rate in\nsample size requirement for reliable power and (b) by providing clear\ninterpretations of global relationships upon rejection of independence. The\nbinary expansion approach also connects the symmetry statistics with the\ncurrent computing system to facilitate efficient bitwise implementation. We\nillustrate the BET with a study of the distribution of stars in the night sky\nand with an exploratory data analysis of the TCGA breast cancer data.\n", "versions": [{"version": "v1", "created": "Mon, 17 Oct 2016 18:19:49 GMT"}, {"version": "v2", "created": "Thu, 12 Jan 2017 03:26:00 GMT"}, {"version": "v3", "created": "Thu, 26 Jan 2017 07:09:37 GMT"}, {"version": "v4", "created": "Sun, 23 Apr 2017 02:08:08 GMT"}, {"version": "v5", "created": "Mon, 20 Nov 2017 15:57:14 GMT"}, {"version": "v6", "created": "Sun, 13 May 2018 02:25:46 GMT"}, {"version": "v7", "created": "Mon, 15 Apr 2019 20:39:38 GMT"}], "update_date": "2020-04-14", "authors_parsed": [["Zhang", "Kai", ""]]}, {"id": "1610.05261", "submitter": "Michael Schober", "authors": "Michael Schober, Simo S\\\"arkk\\\"a, Philipp Hennig", "title": "A probabilistic model for the numerical solution of initial value\n  problems", "comments": "23 pages, 11 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.NA cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Like many numerical methods, solvers for initial value problems (IVPs) on\nordinary differential equations estimate an analytically intractable quantity,\nusing the results of tractable computations as inputs. This structure is\nclosely connected to the notion of inference on latent variables in statistics.\nWe describe a class of algorithms that formulate the solution to an IVP as\ninference on a latent path that is a draw from a Gaussian process probability\nmeasure (or equivalently, the solution of a linear stochastic differential\nequation). We then show that certain members of this class are connected\nprecisely to generalized linear methods for ODEs, a number of Runge--Kutta\nmethods, and Nordsieck methods. This probabilistic formulation of classic\nmethods is valuable in two ways: analytically, it highlights implicit prior\nassumptions favoring certain approximate solutions to the IVP over others, and\ngives a precise meaning to the old observation that these methods act like\nfilters. Practically, it endows the classic solvers with `docking points' for\nnotions of uncertainty and prior information about the initial value, the value\nof the ODE itself, and the solution of the problem.\n", "versions": [{"version": "v1", "created": "Mon, 17 Oct 2016 18:50:35 GMT"}, {"version": "v2", "created": "Wed, 25 Jan 2017 17:10:05 GMT"}, {"version": "v3", "created": "Thu, 10 Aug 2017 21:06:48 GMT"}], "update_date": "2017-08-14", "authors_parsed": [["Schober", "Michael", ""], ["S\u00e4rkk\u00e4", "Simo", ""], ["Hennig", "Philipp", ""]]}, {"id": "1610.05394", "submitter": "Venkatesh Saligrama", "authors": "Manjesh Hanawal and Csaba Szepesvari and Venkatesh Saligrama", "title": "Sequential Learning without Feedback", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In many security and healthcare systems a sequence of features/sensors/tests\nare used for detection and diagnosis. Each test outputs a prediction of the\nlatent state, and carries with it inherent costs. Our objective is to {\\it\nlearn} strategies for selecting tests to optimize accuracy \\& costs.\nUnfortunately it is often impossible to acquire in-situ ground truth\nannotations and we are left with the problem of unsupervised sensor selection\n(USS). We pose USS as a version of stochastic partial monitoring problem with\nan {\\it unusual} reward structure (even noisy annotations are unavailable).\nUnsurprisingly no learner can achieve sublinear regret without further\nassumptions. To this end we propose the notion of weak-dominance. This is a\ncondition on the joint probability distribution of test outputs and latent\nstate and says that whenever a test is accurate on an example, a later test in\nthe sequence is likely to be accurate as well. We empirically verify that weak\ndominance holds on real datasets and prove that it is a maximal condition for\nachieving sublinear regret. We reduce USS to a special case of multi-armed\nbandit problem with side information and develop polynomial time algorithms\nthat achieve sublinear regret.\n", "versions": [{"version": "v1", "created": "Tue, 18 Oct 2016 01:15:57 GMT"}], "update_date": "2016-10-19", "authors_parsed": [["Hanawal", "Manjesh", ""], ["Szepesvari", "Csaba", ""], ["Saligrama", "Venkatesh", ""]]}, {"id": "1610.05419", "submitter": "Ali Khalajmehrabadi", "authors": "Ali Khalajmehrabadi, Nikolaos Gatsis, Daniel Pack and David Akopian", "title": "A Joint Indoor WLAN Localization and Outlier Detection Scheme Using\n  LASSO and Elastic-Net Optimization Techniques", "comments": null, "journal-ref": null, "doi": "10.1109/TMC.2016.2616465", "report-no": null, "categories": "cs.NI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we introduce two indoor Wireless Local Area Network (WLAN)\npositioning methods using augmented sparse recovery algorithms. These schemes\nrender a sparse user's position vector, and in parallel, minimize the distance\nbetween the online measurement and radio map. The overall localization scheme\nfor both methods consists of three steps: 1) coarse localization, obtained from\ncomparing the online measurements with clustered radio map. A novel graph-based\nmethod is proposed to cluster the offline fingerprints. In the online phase, a\nRegion Of Interest (ROI) is selected within which we search for the user's\nlocation; 2) Access Point (AP) selection; and 3) fine localization through the\nnovel sparse recovery algorithms. Since the online measurements are subject to\ninordinate measurement readings, called outliers, the sparse recovery methods\nare modified in order to jointly estimate the outliers and user's position\nvector. The outlier detection procedure identifies the APs whose readings are\neither not available or erroneous. The proposed localization methods have been\ntested with Received Signal Strength (RSS) measurements in a typical office\nenvironment and the results show that they can localize the user with\nsignificantly high accuracy and resolution which is superior to the results\nfrom competing WLAN fingerprinting localization methods.\n", "versions": [{"version": "v1", "created": "Tue, 18 Oct 2016 03:37:11 GMT"}], "update_date": "2016-10-20", "authors_parsed": [["Khalajmehrabadi", "Ali", ""], ["Gatsis", "Nikolaos", ""], ["Pack", "Daniel", ""], ["Akopian", "David", ""]]}, {"id": "1610.05421", "submitter": "Ali Khalajmehrabadi", "authors": "Ali Khalajmehrabadi, Nikolaos Gatsis, and David Akopian", "title": "Structured Group Sparsity: A Novel Indoor WLAN Localization, Outlier\n  Detection, and Radio Map Interpolation Scheme", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces novel schemes for indoor localization, outlier\ndetection, and radio map interpolation using Wireless Local Area Networks\n(WLANs). The localization method consists of a novel multicomponent\noptimization technique that minimizes the squared $\\ell_{2}$-norm of the\nresiduals between the radio map and the online Received Signal Strength (RSS)\nmeasurements, the $\\ell_{1}$-norm of the user's location vector, and weighted\n$\\ell_{2}$-norms of layered groups of Reference Points (RPs). RPs are grouped\nusing a new criterion based on the similarity between the so-called Access\nPoint (AP) coverage vectors. In addition, since AP readings are prone to\ncontaining inordinate readings, called outliers, an augmented optimization\nproblem is proposed to detect the outliers and localize the user with cleaned\nonline measurements. Moreover, a novel scheme to record fingerprints from a\nsmaller number of RPs and estimate the radio map at RPs without recorded\nfingerprints is developed using sparse recovery techniques. All localization\nschemes are tested on RSS fingerprints collected from a real environment. The\noverall scheme has comparable complexity with competing approaches, while it\nperforms with high accuracy under a small number of APs and finer granularity\nof RPs.\n", "versions": [{"version": "v1", "created": "Tue, 18 Oct 2016 03:45:00 GMT"}], "update_date": "2016-10-19", "authors_parsed": [["Khalajmehrabadi", "Ali", ""], ["Gatsis", "Nikolaos", ""], ["Akopian", "David", ""]]}, {"id": "1610.05424", "submitter": "Ali Khalajmehrabadi", "authors": "Ali Khalajmehrabadi, Nikolaos Gatsis, and David Akopian", "title": "Modern WLAN Fingerprinting Indoor Positioning Methods and Deployment\n  Challenges", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Wireless Local Area Network (WLAN) has become a promising choice for indoor\npositioning as the only existing and established infrastructure, to localize\nthe mobile and stationary users indoors. However, since WLAN has been initially\ndesigned for wireless networking and not positioning, the localization task\nbased on WLAN signals has several challenges. Amongst the WLAN positioning\nmethods, WLAN fingerprinting localization has recently achieved great attention\ndue to its promising results. WLAN fingerprinting faces several challenges and\nhence, in this paper, our goal is to overview these challenges and the\nstate-of-the-art solutions. This paper consists of three main parts: 1)\nConventional localization schemes; 2) State-of-the-art approaches; 3) Practical\ndeployment challenges. Since all the proposed methods in WLAN literature have\nbeen conducted and tested in different settings, the reported results are not\nequally comparable. So, we compare some of the main localization schemes in a\nsingle real environment and assess their localization accuracy, positioning\nerror statistics, and complexity. Our results depict illustrative evaluation of\nWLAN localization systems and guide to future improvement opportunities.\n", "versions": [{"version": "v1", "created": "Tue, 18 Oct 2016 03:53:19 GMT"}], "update_date": "2016-10-19", "authors_parsed": [["Khalajmehrabadi", "Ali", ""], ["Gatsis", "Nikolaos", ""], ["Akopian", "David", ""]]}, {"id": "1610.05446", "submitter": "Haoyi Xiong", "authors": "Haoyi Xiong, Yanjie Fu, Wenqing Hu, Guanling Chen, Laura E. Barnes", "title": "Provably Good Early Detection of Diseases using Non-Sparse\n  Covariance-Regularized Linear Discriminant Analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To improve the performance of Linear Discriminant Analysis (LDA) for early\ndetection of diseases using Electronic Health Records (EHR) data, we propose\n\\TheName{} -- a novel framework for \\emph{\\underline{E}HR based\n\\underline{E}arly \\underline{D}etection of \\underline{D}iseases} on top of\n\\emph{Covariance-Regularized} LDA models. Specifically, \\TheName\\ employs a\n\\emph{non-sparse} inverse covariance matrix (or namely precision matrix)\nestimator derived from graphical lasso and incorporates the estimator into LDA\nclassifiers to improve classification accuracy. Theoretical analysis on\n\\TheName\\ shows that it can bound the expected error rate of LDA\nclassification, under certain assumptions. Finally, we conducted extensive\nexperiments using a large-scale real-world EHR dataset -- CHSN. We compared our\nsolution with other regularized LDA and downstream classifiers. The result\nshows \\TheName\\ outperforms all baselines and backups our theoretical analysis.\n", "versions": [{"version": "v1", "created": "Tue, 18 Oct 2016 06:11:23 GMT"}, {"version": "v2", "created": "Wed, 19 Oct 2016 01:34:27 GMT"}], "update_date": "2016-10-20", "authors_parsed": [["Xiong", "Haoyi", ""], ["Fu", "Yanjie", ""], ["Hu", "Wenqing", ""], ["Chen", "Guanling", ""], ["Barnes", "Laura E.", ""]]}, {"id": "1610.05463", "submitter": "Steve Chang", "authors": "Teng Lee, James Johnson, Steve Cheng", "title": "An Interactive Machine Learning Framework", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Machine learning (ML) is believed to be an effective and efficient tool to\nbuild reliable prediction model or extract useful structure from an avalanche\nof data. However, ML is also criticized by its difficulty in interpretation and\ncomplicated parameter tuning. In contrast, visualization is able to well\norganize and visually encode the entangled information in data and guild\naudiences to simpler perceptual inferences and analytic thinking. But large\nscale and high dimensional data will usually lead to the failure of many\nvisualization methods. In this paper, we close a loop between ML and\nvisualization via interaction between ML algorithm and users, so machine\nintelligence and human intelligence can cooperate and improve each other in a\nmutually rewarding way. In particular, we propose \"transparent boosting tree\n(TBT)\", which visualizes both the model structure and prediction statistics of\neach step in the learning process of gradient boosting tree to user, and\ninvolves user's feedback operations to trees into the learning process. In TBT,\nML is in charge of updating weights in learning model and filtering information\nshown to user from the big data, while visualization is in charge of providing\na visual understanding of ML model to facilitate user exploration. It combines\nthe advantages of both ML in big data statistics and human in decision making\nbased on domain knowledge. We develop a user friendly interface for this novel\nlearning method, and apply it to two datasets collected from real applications.\nOur study shows that making ML transparent by using interactive visualization\ncan significantly improve the exploration of ML algorithms, give rise to novel\ninsights of ML models, and integrates both machine and human intelligence.\n", "versions": [{"version": "v1", "created": "Tue, 18 Oct 2016 07:46:11 GMT"}], "update_date": "2016-10-19", "authors_parsed": [["Lee", "Teng", ""], ["Johnson", "James", ""], ["Cheng", "Steve", ""]]}, {"id": "1610.05492", "submitter": "Jakub Kone\\v{c}n\\'y", "authors": "Jakub Kone\\v{c}n\\'y, H. Brendan McMahan, Felix X. Yu, Peter\n  Richt\\'arik, Ananda Theertha Suresh, Dave Bacon", "title": "Federated Learning: Strategies for Improving Communication Efficiency", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Federated Learning is a machine learning setting where the goal is to train a\nhigh-quality centralized model while training data remains distributed over a\nlarge number of clients each with unreliable and relatively slow network\nconnections. We consider learning algorithms for this setting where on each\nround, each client independently computes an update to the current model based\non its local data, and communicates this update to a central server, where the\nclient-side updates are aggregated to compute a new global model. The typical\nclients in this setting are mobile phones, and communication efficiency is of\nthe utmost importance.\n  In this paper, we propose two ways to reduce the uplink communication costs:\nstructured updates, where we directly learn an update from a restricted space\nparametrized using a smaller number of variables, e.g. either low-rank or a\nrandom mask; and sketched updates, where we learn a full model update and then\ncompress it using a combination of quantization, random rotations, and\nsubsampling before sending it to the server. Experiments on both convolutional\nand recurrent networks show that the proposed methods can reduce the\ncommunication cost by two orders of magnitude.\n", "versions": [{"version": "v1", "created": "Tue, 18 Oct 2016 09:11:51 GMT"}, {"version": "v2", "created": "Mon, 30 Oct 2017 20:52:14 GMT"}], "update_date": "2017-11-01", "authors_parsed": [["Kone\u010dn\u00fd", "Jakub", ""], ["McMahan", "H. Brendan", ""], ["Yu", "Felix X.", ""], ["Richt\u00e1rik", "Peter", ""], ["Suresh", "Ananda Theertha", ""], ["Bacon", "Dave", ""]]}, {"id": "1610.05507", "submitter": "Arda Aytekin", "authors": "Arda Aytekin, Hamid Reza Feyzmahdavian, Mikael Johansson", "title": "Analysis and Implementation of an Asynchronous Optimization Algorithm\n  for the Parameter Server", "comments": "10 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.DC cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents an asynchronous incremental aggregated gradient algorithm\nand its implementation in a parameter server framework for solving regularized\noptimization problems. The algorithm can handle both general convex (possibly\nnon-smooth) regularizers and general convex constraints. When the empirical\ndata loss is strongly convex, we establish linear convergence rate, give\nexplicit expressions for step-size choices that guarantee convergence to the\noptimum, and bound the associated convergence factors. The expressions have an\nexplicit dependence on the degree of asynchrony and recover classical results\nunder synchronous operation. Simulations and implementations on commercial\ncompute clouds validate our findings.\n", "versions": [{"version": "v1", "created": "Tue, 18 Oct 2016 09:48:51 GMT"}], "update_date": "2016-10-19", "authors_parsed": [["Aytekin", "Arda", ""], ["Feyzmahdavian", "Hamid Reza", ""], ["Johansson", "Mikael", ""]]}, {"id": "1610.05555", "submitter": "Decebal Constantin Mocanu", "authors": "Decebal Constantin Mocanu and Maria Torres Vega and Eric Eaton and\n  Peter Stone and Antonio Liotta", "title": "Online Contrastive Divergence with Generative Replay: Experience Replay\n  without Storing Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Conceived in the early 1990s, Experience Replay (ER) has been shown to be a\nsuccessful mechanism to allow online learning algorithms to reuse past\nexperiences. Traditionally, ER can be applied to all machine learning paradigms\n(i.e., unsupervised, supervised, and reinforcement learning). Recently, ER has\ncontributed to improving the performance of deep reinforcement learning. Yet,\nits application to many practical settings is still limited by the memory\nrequirements of ER, necessary to explicitly store previous observations. To\nremedy this issue, we explore a novel approach, Online Contrastive Divergence\nwith Generative Replay (OCD_GR), which uses the generative capability of\nRestricted Boltzmann Machines (RBMs) instead of recorded past experiences. The\nRBM is trained online, and does not require the system to store any of the\nobserved data points. We compare OCD_GR to ER on 9 real-world datasets,\nconsidering a worst-case scenario (data points arriving in sorted order) as\nwell as a more realistic one (sequential random-order data points). Our results\nshow that in 64.28% of the cases OCD_GR outperforms ER and in the remaining\n35.72% it has an almost equal performance, while having a considerably reduced\nspace complexity (i.e., memory usage) at a comparable time complexity.\n", "versions": [{"version": "v1", "created": "Tue, 18 Oct 2016 12:06:14 GMT"}], "update_date": "2016-10-19", "authors_parsed": [["Mocanu", "Decebal Constantin", ""], ["Vega", "Maria Torres", ""], ["Eaton", "Eric", ""], ["Stone", "Peter", ""], ["Liotta", "Antonio", ""]]}, {"id": "1610.05670", "submitter": "Mark Eisen", "authors": "Mark Eisen, Santiago Segarra, Gabriel Egan, Alejandro Ribeiro", "title": "Stylometric Analysis of Early Modern Period English Plays", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Function word adjacency networks (WANs) are used to study the authorship of\nplays from the Early Modern English period. In these networks, nodes are\nfunction words and directed edges between two nodes represent the relative\nfrequency of directed co-appearance of the two words. For every analyzed play,\na WAN is constructed and these are aggregated to generate author profile\nnetworks. We first study the similarity of writing styles between Early English\nplaywrights by comparing the profile WANs. The accuracy of using WANs for\nauthorship attribution is then demonstrated by attributing known plays among\nsix popular playwrights. Moreover, the WAN method is shown to outperform other\nfrequency-based methods on attributing Early English plays. In addition, WANs\nare shown to be reliable classifiers even when attributing collaborative plays.\nFor several plays of disputed co-authorship, a deeper analysis is performed by\nattributing every act and scene separately, in which we both corroborate\nexisting breakdowns and provide evidence of new assignments.\n", "versions": [{"version": "v1", "created": "Tue, 18 Oct 2016 15:22:14 GMT"}, {"version": "v2", "created": "Thu, 3 Aug 2017 21:00:00 GMT"}], "update_date": "2017-08-07", "authors_parsed": [["Eisen", "Mark", ""], ["Segarra", "Santiago", ""], ["Egan", "Gabriel", ""], ["Ribeiro", "Alejandro", ""]]}, {"id": "1610.05672", "submitter": "Colin Wei", "authors": "Colin Wei and Iain Murray", "title": "Markov Chain Truncation for Doubly-Intractable Inference", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Computing partition functions, the normalizing constants of probability\ndistributions, is often hard. Variants of importance sampling give unbiased\nestimates of a normalizer Z, however, unbiased estimates of the reciprocal 1/Z\nare harder to obtain. Unbiased estimates of 1/Z allow Markov chain Monte Carlo\nsampling of \"doubly-intractable\" distributions, such as the parameter posterior\nfor Markov Random Fields or Exponential Random Graphs. We demonstrate how to\nconstruct unbiased estimates for 1/Z given access to black-box importance\nsampling estimators for Z. We adapt recent work on random series truncation and\nMarkov chain coupling, producing estimators with lower variance and a higher\npercentage of positive estimates than before. Our debiasing algorithms are\nsimple to implement, and have some theoretical and empirical advantages over\nexisting methods.\n", "versions": [{"version": "v1", "created": "Sat, 15 Oct 2016 20:14:52 GMT"}, {"version": "v2", "created": "Sat, 11 Mar 2017 22:21:42 GMT"}], "update_date": "2017-03-14", "authors_parsed": [["Wei", "Colin", ""], ["Murray", "Iain", ""]]}, {"id": "1610.05688", "submitter": "Pranay Dighe", "authors": "Pranay Dighe, Afsaneh Asaei and Herve Bourlard", "title": "Low-rank and Sparse Soft Targets to Learn Better DNN Acoustic Models", "comments": null, "journal-ref": null, "doi": "10.1109/ICASSP.2017.7953161", "report-no": null, "categories": "cs.CL cs.AI cs.HC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Conventional deep neural networks (DNN) for speech acoustic modeling rely on\nGaussian mixture models (GMM) and hidden Markov model (HMM) to obtain binary\nclass labels as the targets for DNN training. Subword classes in speech\nrecognition systems correspond to context-dependent tied states or senones. The\npresent work addresses some limitations of GMM-HMM senone alignments for DNN\ntraining. We hypothesize that the senone probabilities obtained from a DNN\ntrained with binary labels can provide more accurate targets to learn better\nacoustic models. However, DNN outputs bear inaccuracies which are exhibited as\nhigh dimensional unstructured noise, whereas the informative components are\nstructured and low-dimensional. We exploit principle component analysis (PCA)\nand sparse coding to characterize the senone subspaces. Enhanced probabilities\nobtained from low-rank and sparse reconstructions are used as soft-targets for\nDNN acoustic modeling, that also enables training with untranscribed data.\nExperiments conducted on AMI corpus shows 4.6% relative reduction in word error\nrate.\n", "versions": [{"version": "v1", "created": "Tue, 18 Oct 2016 16:02:10 GMT"}], "update_date": "2017-09-07", "authors_parsed": [["Dighe", "Pranay", ""], ["Asaei", "Afsaneh", ""], ["Bourlard", "Herve", ""]]}, {"id": "1610.05710", "submitter": "Babak Hosseini", "authors": "Babak Hosseini, Barbara Hammer", "title": "Feasibility Based Large Margin Nearest Neighbor Metric Learning", "comments": "This is the preprint of the conference paper published in ESANN2018", "journal-ref": "ESANN 2018 proceedings", "doi": null, "report-no": null, "categories": "cs.DS cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Large margin nearest neighbor (LMNN) is a metric learner which optimizes the\nperformance of the popular $k$NN classifier. However, its resulting metric\nrelies on pre-selected target neighbors. In this paper, we address the\nfeasibility of LMNN's optimization constraints regarding these target points,\nand introduce a mathematical measure to evaluate the size of the feasible\nregion of the optimization problem. We enhance the optimization framework of\nLMNN by a weighting scheme which prefers data triplets which yield a larger\nfeasible region. This increases the chances to obtain a good metric as the\nsolution of LMNN's problem. We evaluate the performance of the resulting\nfeasibility-based LMNN algorithm using synthetic and real datasets. The\nempirical results show an improved accuracy for different types of datasets in\ncomparison to regular LMNN.\n", "versions": [{"version": "v1", "created": "Tue, 18 Oct 2016 17:06:26 GMT"}, {"version": "v2", "created": "Wed, 2 May 2018 16:56:04 GMT"}], "update_date": "2018-05-03", "authors_parsed": [["Hosseini", "Babak", ""], ["Hammer", "Barbara", ""]]}, {"id": "1610.05712", "submitter": "Mariano Tepper", "authors": "Mariano Tepper and Guillermo Sapiro", "title": "Fast L1-NMF for Multiple Parametric Model Estimation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work we introduce a comprehensive algorithmic pipeline for multiple\nparametric model estimation. The proposed approach analyzes the information\nproduced by a random sampling algorithm (e.g., RANSAC) from a machine\nlearning/optimization perspective, using a \\textit{parameterless} biclustering\nalgorithm based on L1 nonnegative matrix factorization (L1-NMF). The proposed\nframework exploits consistent patterns that naturally arise during the RANSAC\nexecution, while explicitly avoiding spurious inconsistencies. Contrarily to\nthe main trends in the literature, the proposed technique does not impose\nnon-intersecting parametric models. A new accelerated algorithm to compute\nL1-NMFs allows to handle medium-sized problems faster while also extending the\nusability of the algorithm to much larger datasets. This accelerated algorithm\nhas applications in any other context where an L1-NMF is needed, beyond the\nbiclustering approach to parameter estimation here addressed. We accompany the\nalgorithmic presentation with theoretical foundations and numerous and diverse\nexamples.\n", "versions": [{"version": "v1", "created": "Tue, 18 Oct 2016 17:20:38 GMT"}, {"version": "v2", "created": "Fri, 11 Nov 2016 15:54:14 GMT"}], "update_date": "2016-11-14", "authors_parsed": [["Tepper", "Mariano", ""], ["Sapiro", "Guillermo", ""]]}, {"id": "1610.05735", "submitter": "Daniel Ritchie", "authors": "Daniel Ritchie, Paul Horsfall, Noah D. Goodman", "title": "Deep Amortized Inference for Probabilistic Programs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Probabilistic programming languages (PPLs) are a powerful modeling tool, able\nto represent any computable probability distribution. Unfortunately,\nprobabilistic program inference is often intractable, and existing PPLs mostly\nrely on expensive, approximate sampling-based methods. To alleviate this\nproblem, one could try to learn from past inferences, so that future inferences\nrun faster. This strategy is known as amortized inference; it has recently been\napplied to Bayesian networks and deep generative models. This paper proposes a\nsystem for amortized inference in PPLs. In our system, amortization comes in\nthe form of a parameterized guide program. Guide programs have similar\nstructure to the original program, but can have richer data flow, including\nneural network components. These networks can be optimized so that the guide\napproximately samples from the posterior distribution defined by the original\nprogram. We present a flexible interface for defining guide programs and a\nstochastic gradient-based scheme for optimizing guide parameters, as well as\nsome preliminary results on automatically deriving guide programs. We explore\nin detail the common machine learning pattern in which a 'local' model is\nspecified by 'global' random values and used to generate independent observed\ndata points; this gives rise to amortized local inference supporting global\nmodel learning.\n", "versions": [{"version": "v1", "created": "Tue, 18 Oct 2016 18:35:09 GMT"}], "update_date": "2016-10-19", "authors_parsed": [["Ritchie", "Daniel", ""], ["Horsfall", "Paul", ""], ["Goodman", "Noah D.", ""]]}, {"id": "1610.05755", "submitter": "Nicolas Papernot", "authors": "Nicolas Papernot, Mart\\'in Abadi, \\'Ulfar Erlingsson, Ian Goodfellow,\n  Kunal Talwar", "title": "Semi-supervised Knowledge Transfer for Deep Learning from Private\n  Training Data", "comments": "Accepted to ICLR 17 as an oral", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Some machine learning applications involve training data that is sensitive,\nsuch as the medical histories of patients in a clinical trial. A model may\ninadvertently and implicitly store some of its training data; careful analysis\nof the model may therefore reveal sensitive information.\n  To address this problem, we demonstrate a generally applicable approach to\nproviding strong privacy guarantees for training data: Private Aggregation of\nTeacher Ensembles (PATE). The approach combines, in a black-box fashion,\nmultiple models trained with disjoint datasets, such as records from different\nsubsets of users. Because they rely directly on sensitive data, these models\nare not published, but instead used as \"teachers\" for a \"student\" model. The\nstudent learns to predict an output chosen by noisy voting among all of the\nteachers, and cannot directly access an individual teacher or the underlying\ndata or parameters. The student's privacy properties can be understood both\nintuitively (since no single teacher and thus no single dataset dictates the\nstudent's training) and formally, in terms of differential privacy. These\nproperties hold even if an adversary can not only query the student but also\ninspect its internal workings.\n  Compared with previous work, the approach imposes only weak assumptions on\nhow teachers are trained: it applies to any model, including non-convex models\nlike DNNs. We achieve state-of-the-art privacy/utility trade-offs on MNIST and\nSVHN thanks to an improved privacy analysis and semi-supervised learning.\n", "versions": [{"version": "v1", "created": "Tue, 18 Oct 2016 19:37:37 GMT"}, {"version": "v2", "created": "Wed, 2 Nov 2016 13:18:56 GMT"}, {"version": "v3", "created": "Mon, 7 Nov 2016 00:18:03 GMT"}, {"version": "v4", "created": "Fri, 3 Mar 2017 18:56:43 GMT"}], "update_date": "2017-03-06", "authors_parsed": [["Papernot", "Nicolas", ""], ["Abadi", "Mart\u00edn", ""], ["Erlingsson", "\u00dalfar", ""], ["Goodfellow", "Ian", ""], ["Talwar", "Kunal", ""]]}, {"id": "1610.05773", "submitter": "Manuel Gomez Rodriguez", "authors": "Ali Zarezade and Utkarsh Upadhyay and Hamid Rabiee and Manuel Gomez\n  Rodriguez", "title": "RedQueen: An Online Algorithm for Smart Broadcasting in Social Networks", "comments": "To appear at the 10th ACM International Conference on Web Search and\n  Data Mining (WSDM)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.DS cs.LG cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Users in social networks whose posts stay at the top of their followers'{}\nfeeds the longest time are more likely to be noticed. Can we design an online\nalgorithm to help them decide when to post to stay at the top? In this paper,\nwe address this question as a novel optimal control problem for jump stochastic\ndifferential equations. For a wide variety of feed dynamics, we show that the\noptimal broadcasting intensity for any user is surprisingly simple -- it is\ngiven by the position of her most recent post on each of her follower's feeds.\nAs a consequence, we are able to develop a simple and highly efficient online\nalgorithm, RedQueen, to sample the optimal times for the user to post.\nExperiments on both synthetic and real data gathered from Twitter show that our\nalgorithm is able to consistently make a user's posts more visible over time,\nis robust to volume changes on her followers' feeds, and significantly\noutperforms the state of the art.\n", "versions": [{"version": "v1", "created": "Tue, 18 Oct 2016 20:00:05 GMT"}], "update_date": "2016-10-20", "authors_parsed": [["Zarezade", "Ali", ""], ["Upadhyay", "Utkarsh", ""], ["Rabiee", "Hamid", ""], ["Rodriguez", "Manuel Gomez", ""]]}, {"id": "1610.05775", "submitter": "Manuel Gomez Rodriguez", "authors": "Charalampos Mavroforakis and Isabel Valera and Manuel Gomez Rodriguez", "title": "Modeling the Dynamics of Online Learning Activity", "comments": "Python implementation of the proposed HDHP is available at\n  https://github.com/Networks-Learning/hdhp.py", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  People are increasingly relying on the Web and social media to find solutions\nto their problems in a wide range of domains. In this online setting, closely\nrelated problems often lead to the same characteristic learning pattern, in\nwhich people sharing these problems visit related pieces of information,\nperform almost identical queries or, more generally, take a series of similar\nactions. In this paper, we introduce a novel modeling framework for clustering\ncontinuous-time grouped streaming data, the hierarchical Dirichlet Hawkes\nprocess (HDHP), which allows us to automatically uncover a wide variety of\nlearning patterns from detailed traces of learning activity. Our model allows\nfor efficient inference, scaling to millions of actions taken by thousands of\nusers. Experiments on real data gathered from Stack Overflow reveal that our\nframework can recover meaningful learning patterns in terms of both content and\ntemporal dynamics, as well as accurately track users' interests and goals over\ntime.\n", "versions": [{"version": "v1", "created": "Tue, 18 Oct 2016 20:00:09 GMT"}], "update_date": "2016-10-20", "authors_parsed": [["Mavroforakis", "Charalampos", ""], ["Valera", "Isabel", ""], ["Rodriguez", "Manuel Gomez", ""]]}, {"id": "1610.05792", "submitter": "Soham De", "authors": "Soham De, Abhay Yadav, David Jacobs and Tom Goldstein", "title": "Big Batch SGD: Automated Inference using Adaptive Batch Sizes", "comments": "A preliminary version of this paper appears in AISTATS 2017\n  (International Conference on Artificial Intelligence and Statistics)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.NA math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Classical stochastic gradient methods for optimization rely on noisy gradient\napproximations that become progressively less accurate as iterates approach a\nsolution. The large noise and small signal in the resulting gradients makes it\ndifficult to use them for adaptive stepsize selection and automatic stopping.\nWe propose alternative \"big batch\" SGD schemes that adaptively grow the batch\nsize over time to maintain a nearly constant signal-to-noise ratio in the\ngradient approximation. The resulting methods have similar convergence rates to\nclassical SGD, and do not require convexity of the objective. The high fidelity\ngradients enable automated learning rate selection and do not require stepsize\ndecay. Big batch methods are thus easily automated and can run with little or\nno oversight.\n", "versions": [{"version": "v1", "created": "Tue, 18 Oct 2016 20:24:10 GMT"}, {"version": "v2", "created": "Tue, 8 Nov 2016 23:37:30 GMT"}, {"version": "v3", "created": "Fri, 3 Mar 2017 01:10:28 GMT"}, {"version": "v4", "created": "Thu, 6 Apr 2017 21:48:28 GMT"}], "update_date": "2017-04-10", "authors_parsed": [["De", "Soham", ""], ["Yadav", "Abhay", ""], ["Jacobs", "David", ""], ["Goldstein", "Tom", ""]]}, {"id": "1610.05796", "submitter": "Koray Mancuhan", "authors": "Koray Mancuhan and Chris Clifton", "title": "Decision Tree Classification on Outsourced Data", "comments": "Presented in the Data Ethics Workshop at the 20th ACM SIGKDD\n  Conference on Knowledge Discovery and Data Mining", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a client-server decision tree learning method for\noutsourced private data. The privacy model is anatomization/fragmentation: the\nserver sees data values, but the link between sensitive and identifying\ninformation is encrypted with a key known only to clients. Clients have limited\nprocessing and storage capability. Both sensitive and identifying information\nthus are stored on the server. The approach presented also retains most\nprocessing at the server, and client-side processing is amortized over\npredictions made by the clients. Experiments on various datasets show that the\nmethod produces decision trees approaching the accuracy of a non-private\ndecision tree, while substantially reducing the client's computing resource\nrequirements.\n", "versions": [{"version": "v1", "created": "Tue, 18 Oct 2016 20:49:21 GMT"}], "update_date": "2016-10-20", "authors_parsed": [["Mancuhan", "Koray", ""], ["Clifton", "Chris", ""]]}, {"id": "1610.05812", "submitter": "Liang Lu", "authors": "Liang Lu and Steve Renals", "title": "Small-footprint Highway Deep Neural Networks for Speech Recognition", "comments": "9 pages, 6 figures. Accepted to IEEE/ACM Transactions on Audio,\n  Speech and Language Processing, 2017. arXiv admin note: text overlap with\n  arXiv:1608.00892, arXiv:1607.01963", "journal-ref": null, "doi": "10.1109/TASLP.2017.2698723", "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  State-of-the-art speech recognition systems typically employ neural network\nacoustic models. However, compared to Gaussian mixture models, deep neural\nnetwork (DNN) based acoustic models often have many more model parameters,\nmaking it challenging for them to be deployed on resource-constrained\nplatforms, such as mobile devices. In this paper, we study the application of\nthe recently proposed highway deep neural network (HDNN) for training\nsmall-footprint acoustic models. HDNNs are a depth-gated feedforward neural\nnetwork, which include two types of gate functions to facilitate the\ninformation flow through different layers. Our study demonstrates that HDNNs\nare more compact than regular DNNs for acoustic modeling, i.e., they can\nachieve comparable recognition accuracy with many fewer model parameters.\nFurthermore, HDNNs are more controllable than DNNs: the gate functions of an\nHDNN can control the behavior of the whole network using a very small number of\nmodel parameters. Finally, we show that HDNNs are more adaptable than DNNs. For\nexample, simply updating the gate functions using adaptation data can result in\nconsiderable gains in accuracy. We demonstrate these aspects by experiments\nusing the publicly available AMI corpus, which has around 80 hours of training\ndata.\n", "versions": [{"version": "v1", "created": "Tue, 18 Oct 2016 22:06:01 GMT"}, {"version": "v2", "created": "Mon, 24 Oct 2016 21:12:56 GMT"}, {"version": "v3", "created": "Wed, 25 Jan 2017 15:45:22 GMT"}, {"version": "v4", "created": "Tue, 25 Apr 2017 19:48:41 GMT"}], "update_date": "2017-04-27", "authors_parsed": [["Lu", "Liang", ""], ["Renals", "Steve", ""]]}, {"id": "1610.05815", "submitter": "Koray Mancuhan", "authors": "Koray Mancuhan and Chris Clifton", "title": "Statistical Learning Theory Approach for Data Classification with\n  l-diversity", "comments": "Technical Report", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Corporations are retaining ever-larger corpuses of personal data; the\nfrequency or breaches and corresponding privacy impact have been rising\naccordingly. One way to mitigate this risk is through use of anonymized data,\nlimiting the exposure of individual data to only where it is absolutely needed.\nThis would seem particularly appropriate for data mining, where the goal is\ngeneralizable knowledge rather than data on specific individuals. In practice,\ncorporate data miners often insist on original data, for fear that they might\n\"miss something\" with anonymized or differentially private approaches. This\npaper provides a theoretical justification for the use of anonymized data.\nSpecifically, we show that a support vector classifier trained on anatomized\ndata satisfying l-diversity should be expected to do as well as on the original\ndata. Anatomy preserves all data values, but introduces uncertainty in the\nmapping between identifying and sensitive values, thus satisfying l-diversity.\nThe theoretical effectiveness of the proposed approach is validated using\nseveral publicly available datasets, showing that we outperform the state of\nthe art for support vector classification using training data protected by\nk-anonymity, and are comparable to learning on the original data.\n", "versions": [{"version": "v1", "created": "Tue, 18 Oct 2016 22:14:27 GMT"}], "update_date": "2016-10-20", "authors_parsed": [["Mancuhan", "Koray", ""], ["Clifton", "Chris", ""]]}, {"id": "1610.05820", "submitter": "Reza Shokri", "authors": "Reza Shokri, Marco Stronati, Congzheng Song, Vitaly Shmatikov", "title": "Membership Inference Attacks against Machine Learning Models", "comments": "In the proceedings of the IEEE Symposium on Security and Privacy,\n  2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We quantitatively investigate how machine learning models leak information\nabout the individual data records on which they were trained. We focus on the\nbasic membership inference attack: given a data record and black-box access to\na model, determine if the record was in the model's training dataset. To\nperform membership inference against a target model, we make adversarial use of\nmachine learning and train our own inference model to recognize differences in\nthe target model's predictions on the inputs that it trained on versus the\ninputs that it did not train on.\n  We empirically evaluate our inference techniques on classification models\ntrained by commercial \"machine learning as a service\" providers such as Google\nand Amazon. Using realistic datasets and classification tasks, including a\nhospital discharge dataset whose membership is sensitive from the privacy\nperspective, we show that these models can be vulnerable to membership\ninference attacks. We then investigate the factors that influence this leakage\nand evaluate mitigation strategies.\n", "versions": [{"version": "v1", "created": "Tue, 18 Oct 2016 22:38:33 GMT"}, {"version": "v2", "created": "Fri, 31 Mar 2017 22:17:07 GMT"}], "update_date": "2017-04-04", "authors_parsed": [["Shokri", "Reza", ""], ["Stronati", "Marco", ""], ["Song", "Congzheng", ""], ["Shmatikov", "Vitaly", ""]]}, {"id": "1610.05838", "submitter": "Xiaolong Xie", "authors": "Xiaolong Xie, Wei Tan, Liana L. Fong, Yun Liang", "title": "CuMF_SGD: Fast and Scalable Matrix Factorization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Matrix factorization (MF) has been widely used in e.g., recommender systems,\ntopic modeling and word embedding. Stochastic gradient descent (SGD) is popular\nin solving MF problems because it can deal with large data sets and is easy to\ndo incremental learning. We observed that SGD for MF is memory bound.\nMeanwhile, single-node CPU systems with caching performs well only for small\ndata sets; distributed systems have higher aggregated memory bandwidth but\nsuffer from relatively slow network connection. This observation inspires us to\naccelerate MF by utilizing GPUs's high memory bandwidth and fast intra-node\nconnection. We present cuMF_SGD, a CUDA-based SGD solution for large-scale MF\nproblems. On a single CPU, we design two workload schedule schemes, i.e.,\nbatch-Hogwild! and wavefront-update that fully exploit the massive amount of\ncores. Especially, batch-Hogwild! as a vectorized version of Hogwild! overcomes\nthe issue of memory discontinuity. We also develop highly-optimized kernels for\nSGD update, leveraging cache, warp-shuffle instructions and half-precision\nfloats. We also design a partition scheme to utilize multiple GPUs while\naddressing the well-known convergence issue when parallelizing SGD. On three\ndata sets with only one Maxwell or Pascal GPU, cuMF_SGD runs 3.1X-28.2X as fast\ncompared with state-of-art CPU solutions on 1-64 CPU nodes. Evaluations also\nshow that cuMF_SGD scales well on multiple GPUs in large data sets.\n", "versions": [{"version": "v1", "created": "Wed, 19 Oct 2016 01:28:11 GMT"}, {"version": "v2", "created": "Thu, 20 Oct 2016 13:38:34 GMT"}, {"version": "v3", "created": "Thu, 10 Nov 2016 01:16:40 GMT"}], "update_date": "2016-11-11", "authors_parsed": [["Xie", "Xiaolong", ""], ["Tan", "Wei", ""], ["Fong", "Liana L.", ""], ["Liang", "Yun", ""]]}, {"id": "1610.05925", "submitter": "Christophe Dupuy", "authors": "Christophe Dupuy (SIERRA), Francis Bach (SIERRA, LIENS)", "title": "Learning Determinantal Point Processes in Sublinear Time", "comments": "Under review for AISTATS 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new class of determinantal point processes (DPPs) which can be\nmanipulated for inference and parameter learning in potentially sublinear time\nin the number of items. This class, based on a specific low-rank factorization\nof the marginal kernel, is particularly suited to a subclass of continuous DPPs\nand DPPs defined on exponentially many items. We apply this new class to\nmodelling text documents as sampling a DPP of sentences, and propose a\nconditional maximum likelihood formulation to model topic proportions, which is\nmade possible with no approximation for our class of DPPs. We present an\napplication to document summarization with a DPP on $2^{500}$ items.\n", "versions": [{"version": "v1", "created": "Wed, 19 Oct 2016 09:18:10 GMT"}], "update_date": "2016-10-20", "authors_parsed": [["Dupuy", "Christophe", "", "SIERRA"], ["Bach", "Francis", "", "SIERRA, LIENS"]]}, {"id": "1610.05945", "submitter": "Xin Wang", "authors": "Xin Wang and Siu Ming Yiu", "title": "A multi-task learning model for malware classification with useful file\n  access pattern from API call sequence", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.CR cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Based on API call sequences, semantic-aware and machine learning (ML) based\nmalware classifiers can be built for malware detection or classification.\nPrevious works concentrate on crafting and extracting various features from\nmalware binaries, disassembled binaries or API calls via static or dynamic\nanalysis and resorting to ML to build classifiers. However, they tend to\ninvolve too much feature engineering and fail to provide interpretability. We\nsolve these two problems with the recent advances in deep learning: 1)\nRNN-based autoencoders (RNN-AEs) can automatically learn low-dimensional\nrepresentation of a malware from its raw API call sequence. 2) Multiple\ndecoders can be trained under different supervisions to give more information,\nother than the class or family label of a malware. Inspired by the works of\ndocument classification and automatic sentence summarization, each API call\nsequence can be regarded as a sentence. In this paper, we make the first\nattempt to build a multi-task malware learning model based on API call\nsequences. The model consists of two decoders, one for malware classification\nand one for $\\emph{file access pattern}$ (FAP) generation given the API call\nsequence of a malware. We base our model on the general seq2seq framework.\nExperiments show that our model can give competitive classification results as\nwell as insightful FAP information.\n", "versions": [{"version": "v1", "created": "Wed, 19 Oct 2016 10:06:14 GMT"}], "update_date": "2016-10-20", "authors_parsed": [["Wang", "Xin", ""], ["Yiu", "Siu Ming", ""]]}, {"id": "1610.05984", "submitter": "Daniel Hein", "authors": "Daniel Hein, Alexander Hentschel, Thomas Runkler, Steffen Udluft", "title": "Particle Swarm Optimization for Generating Interpretable Fuzzy\n  Reinforcement Learning Policies", "comments": null, "journal-ref": "Engineering Applications of Artificial Intelligence, Volume 65C,\n  October 2017, Pages 87-98", "doi": "10.1016/j.engappai.2017.07.005", "report-no": null, "categories": "cs.NE cs.AI cs.LG cs.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fuzzy controllers are efficient and interpretable system controllers for\ncontinuous state and action spaces. To date, such controllers have been\nconstructed manually or trained automatically either using expert-generated\nproblem-specific cost functions or incorporating detailed knowledge about the\noptimal control strategy. Both requirements for automatic training processes\nare not found in most real-world reinforcement learning (RL) problems. In such\napplications, online learning is often prohibited for safety reasons because\nonline learning requires exploration of the problem's dynamics during policy\ntraining. We introduce a fuzzy particle swarm reinforcement learning (FPSRL)\napproach that can construct fuzzy RL policies solely by training parameters on\nworld models that simulate real system dynamics. These world models are created\nby employing an autonomous machine learning technique that uses previously\ngenerated transition samples of a real system. To the best of our knowledge,\nthis approach is the first to relate self-organizing fuzzy controllers to\nmodel-based batch RL. Therefore, FPSRL is intended to solve problems in domains\nwhere online learning is prohibited, system dynamics are relatively easy to\nmodel from previously generated default policy transition samples, and it is\nexpected that a relatively easily interpretable control policy exists. The\nefficiency of the proposed approach with problems from such domains is\ndemonstrated using three standard RL benchmarks, i.e., mountain car, cart-pole\nbalancing, and cart-pole swing-up. Our experimental results demonstrate\nhigh-performing, interpretable fuzzy policies.\n", "versions": [{"version": "v1", "created": "Wed, 19 Oct 2016 12:41:52 GMT"}, {"version": "v2", "created": "Fri, 7 Apr 2017 07:22:21 GMT"}, {"version": "v3", "created": "Fri, 5 May 2017 09:01:41 GMT"}, {"version": "v4", "created": "Thu, 29 Jun 2017 07:13:09 GMT"}, {"version": "v5", "created": "Tue, 15 Aug 2017 21:41:03 GMT"}], "update_date": "2017-08-18", "authors_parsed": [["Hein", "Daniel", ""], ["Hentschel", "Alexander", ""], ["Runkler", "Thomas", ""], ["Udluft", "Steffen", ""]]}, {"id": "1610.06048", "submitter": "Koray Mancuhan", "authors": "Koray Mancuhan and Chris Clifton", "title": "K-Nearest Neighbor Classification Using Anatomized Data", "comments": "Technical Report. arXiv admin note: text overlap with arXiv:\n  1610.05815", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper analyzes k nearest neighbor classification with training data\nanonymized using anatomy. Anatomy preserves all data values, but introduces\nuncertainty in the mapping between identifying and sensitive values. We first\nstudy the theoretical effect of the anatomized training data on the k nearest\nneighbor error rate bounds, nearest neighbor convergence rate, and Bayesian\nerror. We then validate the derived bounds empirically. We show that 1)\nLearning from anatomized data approaches the limits of learning through the\nunprotected data (although requiring larger training data), and 2) nearest\nneighbor using anatomized data outperforms nearest neighbor on\ngeneralization-based anonymization.\n", "versions": [{"version": "v1", "created": "Wed, 19 Oct 2016 15:00:59 GMT"}], "update_date": "2016-10-30", "authors_parsed": [["Mancuhan", "Koray", ""], ["Clifton", "Chris", ""]]}, {"id": "1610.06072", "submitter": "Tom Bosc", "authors": "Tom Bosc", "title": "Learning to Learn Neural Networks", "comments": "presented at \"Reasoning, Attention, Memory\" workshop, NIPS 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Meta-learning consists in learning learning algorithms. We use a Long Short\nTerm Memory (LSTM) based network to learn to compute on-line updates of the\nparameters of another neural network. These parameters are stored in the cell\nstate of the LSTM. Our framework allows to compare learned algorithms to\nhand-made algorithms within the traditional train and test methodology. In an\nexperiment, we learn a learning algorithm for a one-hidden layer Multi-Layer\nPerceptron (MLP) on non-linearly separable datasets. The learned algorithm is\nable to update parameters of both layers and generalise well on similar\ndatasets.\n", "versions": [{"version": "v1", "created": "Wed, 19 Oct 2016 15:46:30 GMT"}], "update_date": "2016-10-20", "authors_parsed": [["Bosc", "Tom", ""]]}, {"id": "1610.06106", "submitter": "Edoardo Manino", "authors": "Edoardo Manino, Long Tran-Thanh, Nicholas R. Jennings", "title": "Efficiency of active learning for the allocation of workers on\n  crowdsourced classification tasks", "comments": "paper accepted in the CrowdML workshop at NIPS 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Crowdsourcing has been successfully employed in the past as an effective and\ncheap way to execute classification tasks and has therefore attracted the\nattention of the research community. However, we still lack a theoretical\nunderstanding of how to collect the labels from the crowd in an optimal way. In\nthis paper we focus on the problem of worker allocation and compare two active\nlearning policies proposed in the empirical literature with a uniform\nallocation of the available budget. To this end we make a thorough mathematical\nanalysis of the problem and derive a new bound on the performance of the\nsystem. Furthermore we run extensive simulations in a more realistic scenario\nand show that our theoretical results hold in practice.\n", "versions": [{"version": "v1", "created": "Wed, 19 Oct 2016 17:03:27 GMT"}], "update_date": "2016-10-20", "authors_parsed": [["Manino", "Edoardo", ""], ["Tran-Thanh", "Long", ""], ["Jennings", "Nicholas R.", ""]]}, {"id": "1610.06160", "submitter": "Qianli Liao", "authors": "Qianli Liao, Kenji Kawaguchi, Tomaso Poggio", "title": "Streaming Normalization: Towards Simpler and More Biologically-plausible\n  Normalizations for Online and Recurrent Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We systematically explored a spectrum of normalization algorithms related to\nBatch Normalization (BN) and propose a generalized formulation that\nsimultaneously solves two major limitations of BN: (1) online learning and (2)\nrecurrent learning. Our proposal is simpler and more biologically-plausible.\nUnlike previous approaches, our technique can be applied out of the box to all\nlearning scenarios (e.g., online learning, batch learning, fully-connected,\nconvolutional, feedforward, recurrent and mixed --- recurrent and\nconvolutional) and compare favorably with existing approaches. We also propose\nLp Normalization for normalizing by different orders of statistical moments. In\nparticular, L1 normalization is well-performing, simple to implement, fast to\ncompute, more biologically-plausible and thus ideal for GPU or hardware\nimplementations.\n", "versions": [{"version": "v1", "created": "Wed, 19 Oct 2016 19:34:48 GMT"}], "update_date": "2016-10-20", "authors_parsed": [["Liao", "Qianli", ""], ["Kawaguchi", "Kenji", ""], ["Poggio", "Tomaso", ""]]}, {"id": "1610.06209", "submitter": "Anna Choromanska", "authors": "Mariusz Bojarski, Anna Choromanska, Krzysztof Choromanski, Francois\n  Fagan, Cedric Gouy-Pailler, Anne Morvan, Nourhan Sakr, Tamas Sarlos, Jamal\n  Atif", "title": "Structured adaptive and random spinners for fast machine learning\n  computations", "comments": "arXiv admin note: substantial text overlap with arXiv:1605.09046", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider an efficient computational framework for speeding up several\nmachine learning algorithms with almost no loss of accuracy. The proposed\nframework relies on projections via structured matrices that we call Structured\nSpinners, which are formed as products of three structured matrix-blocks that\nincorporate rotations. The approach is highly generic, i.e. i) structured\nmatrices under consideration can either be fully-randomized or learned, ii) our\nstructured family contains as special cases all previously considered\nstructured schemes, iii) the setting extends to the non-linear case where the\nprojections are followed by non-linear functions, and iv) the method finds\nnumerous applications including kernel approximations via random feature maps,\ndimensionality reduction algorithms, new fast cross-polytope LSH techniques,\ndeep learning, convex optimization algorithms via Newton sketches, quantization\nwith random projection trees, and more. The proposed framework comes with\ntheoretical guarantees characterizing the capacity of the structured model in\nreference to its unstructured counterpart and is based on a general theoretical\nprinciple that we describe in the paper. As a consequence of our theoretical\nanalysis, we provide the first theoretical guarantees for one of the most\nefficient existing LSH algorithms based on the HD3HD2HD1 structured matrix\n[Andoni et al., 2015]. The exhaustive experimental evaluation confirms the\naccuracy and efficiency of structured spinners for a variety of different\napplications.\n", "versions": [{"version": "v1", "created": "Wed, 19 Oct 2016 20:48:25 GMT"}, {"version": "v2", "created": "Fri, 28 Oct 2016 01:25:06 GMT"}, {"version": "v3", "created": "Sat, 26 Nov 2016 17:14:56 GMT"}], "update_date": "2016-11-29", "authors_parsed": [["Bojarski", "Mariusz", ""], ["Choromanska", "Anna", ""], ["Choromanski", "Krzysztof", ""], ["Fagan", "Francois", ""], ["Gouy-Pailler", "Cedric", ""], ["Morvan", "Anne", ""], ["Sakr", "Nourhan", ""], ["Sarlos", "Tamas", ""], ["Atif", "Jamal", ""]]}, {"id": "1610.06249", "submitter": "Kien Do", "authors": "Kien Do and Truyen Tran and Svetha Venkatesh", "title": "Multilevel Anomaly Detection for Mixed Data", "comments": "9 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Anomalies are those deviating from the norm. Unsupervised anomaly detection\noften translates to identifying low density regions. Major problems arise when\ndata is high-dimensional and mixed of discrete and continuous attributes. We\npropose MIXMAD, which stands for MIXed data Multilevel Anomaly Detection, an\nensemble method that estimates the sparse regions across multiple levels of\nabstraction of mixed data. The hypothesis is for domains where multiple data\nabstractions exist, a data point may be anomalous with respect to the raw\nrepresentation or more abstract representations. To this end, our method\nsequentially constructs an ensemble of Deep Belief Nets (DBNs) with varying\ndepths. Each DBN is an energy-based detector at a predefined abstraction level.\nAt the bottom level of each DBN, there is a Mixed-variate Restricted Boltzmann\nMachine that models the density of mixed data. Predictions across the ensemble\nare finally combined via rank aggregation. The proposed MIXMAD is evaluated on\nhigh-dimensional realworld datasets of different characteristics. The results\ndemonstrate that for anomaly detection, (a) multilevel abstraction of\nhigh-dimensional and mixed data is a sensible strategy, and (b) empirically,\nMIXMAD is superior to popular unsupervised detection methods for both\nhomogeneous and mixed data.\n", "versions": [{"version": "v1", "created": "Thu, 20 Oct 2016 00:04:55 GMT"}], "update_date": "2016-10-21", "authors_parsed": [["Do", "Kien", ""], ["Tran", "Truyen", ""], ["Venkatesh", "Svetha", ""]]}, {"id": "1610.06251", "submitter": "Cheng Li", "authors": "Cheng Li, Xiaoxiao Guo and Qiaozhu Mei", "title": "DeepGraph: Graph Structure Predicts Network Growth", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The topological (or graph) structures of real-world networks are known to be\npredictive of multiple dynamic properties of the networks. Conventionally, a\ngraph structure is represented using an adjacency matrix or a set of\nhand-crafted structural features. These representations either fail to\nhighlight local and global properties of the graph or suffer from a severe loss\nof structural information. There lacks an effective graph representation, which\nhinges the realization of the predictive power of network structures.\n  In this study, we propose to learn the represention of a graph, or the\ntopological structure of a network, through a deep learning model. This\nend-to-end prediction model, named DeepGraph, takes the input of the raw\nadjacency matrix of a real-world network and outputs a prediction of the growth\nof the network. The adjacency matrix is first represented using a graph\ndescriptor based on the heat kernel signature, which is then passed through a\nmulti-column, multi-resolution convolutional neural network. Extensive\nexperiments on five large collections of real-world networks demonstrate that\nthe proposed prediction model significantly improves the effectiveness of\nexisting methods, including linear or nonlinear regressors that use\nhand-crafted features, graph kernels, and competing deep learning methods.\n", "versions": [{"version": "v1", "created": "Thu, 20 Oct 2016 00:16:05 GMT"}], "update_date": "2016-10-21", "authors_parsed": [["Li", "Cheng", ""], ["Guo", "Xiaoxiao", ""], ["Mei", "Qiaozhu", ""]]}, {"id": "1610.06258", "submitter": "Jimmy Ba", "authors": "Jimmy Ba, Geoffrey Hinton, Volodymyr Mnih, Joel Z. Leibo, Catalin\n  Ionescu", "title": "Using Fast Weights to Attend to the Recent Past", "comments": "Added [Schmidhuber 1993] citation to the last paragraph of the\n  introduction. Fixed typo appendix A.1 uniform initialization to 1/\\sqrt{H}", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Until recently, research on artificial neural networks was largely restricted\nto systems with only two types of variable: Neural activities that represent\nthe current or recent input and weights that learn to capture regularities\namong inputs, outputs and payoffs. There is no good reason for this\nrestriction. Synapses have dynamics at many different time-scales and this\nsuggests that artificial neural networks might benefit from variables that\nchange slower than activities but much faster than the standard weights. These\n\"fast weights\" can be used to store temporary memories of the recent past and\nthey provide a neurally plausible way of implementing the type of attention to\nthe past that has recently proved very helpful in sequence-to-sequence models.\nBy using fast weights we can avoid the need to store copies of neural activity\npatterns.\n", "versions": [{"version": "v1", "created": "Thu, 20 Oct 2016 01:03:20 GMT"}, {"version": "v2", "created": "Thu, 27 Oct 2016 19:53:07 GMT"}, {"version": "v3", "created": "Mon, 5 Dec 2016 00:14:01 GMT"}], "update_date": "2016-12-06", "authors_parsed": [["Ba", "Jimmy", ""], ["Hinton", "Geoffrey", ""], ["Mnih", "Volodymyr", ""], ["Leibo", "Joel Z.", ""], ["Ionescu", "Catalin", ""]]}, {"id": "1610.06276", "submitter": "Alexander Ulanov", "authors": "Alexander Ulanov, Andrey Simanovsky, Manish Marwah", "title": "Modeling Scalability of Distributed Machine Learning", "comments": "6 pages, 4 figures, appears at ICDE 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Present day machine learning is computationally intensive and processes large\namounts of data. It is implemented in a distributed fashion in order to address\nthese scalability issues. The work is parallelized across a number of computing\nnodes. It is usually hard to estimate in advance how many nodes to use for a\nparticular workload. We propose a simple framework for estimating the\nscalability of distributed machine learning algorithms. We measure the\nscalability by means of the speedup an algorithm achieves with more nodes. We\npropose time complexity models for gradient descent and graphical model\ninference. We validate our models with experiments on deep learning training\nand belief propagation. This framework was used to study the scalability of\nmachine learning algorithms in Apache Spark.\n", "versions": [{"version": "v1", "created": "Thu, 20 Oct 2016 03:28:40 GMT"}, {"version": "v2", "created": "Sat, 25 Mar 2017 02:17:04 GMT"}], "update_date": "2017-03-28", "authors_parsed": [["Ulanov", "Alexander", ""], ["Simanovsky", "Andrey", ""], ["Marwah", "Manish", ""]]}, {"id": "1610.06283", "submitter": "Mohamed K. Helwa", "authors": "Qiyang Li, Jingxing Qian, Zining Zhu, Xuchan Bao, Mohamed K. Helwa,\n  Angela P. Schoellig", "title": "Deep Neural Networks for Improved, Impromptu Trajectory Tracking of\n  Quadrotors", "comments": "7 pages, 8 figures. Accepted final version. To appear in the proc. of\n  the 2017 IEEE International Conference on Robotics and Automation", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.LG cs.NE cs.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Trajectory tracking control for quadrotors is important for applications\nranging from surveying and inspection, to film making. However, designing and\ntuning classical controllers, such as proportional-integral-derivative (PID)\ncontrollers, to achieve high tracking precision can be time-consuming and\ndifficult, due to hidden dynamics and other non-idealities. The Deep Neural\nNetwork (DNN), with its superior capability of approximating abstract,\nnonlinear functions, proposes a novel approach for enhancing trajectory\ntracking control. This paper presents a DNN-based algorithm as an add-on module\nthat improves the tracking performance of a classical feedback controller.\nGiven a desired trajectory, the DNNs provide a tailored reference input to the\ncontroller based on their gained experience. The input aims to achieve a unity\nmap between the desired and the output trajectory. The motivation for this work\nis an interactive \"fly-as-you-draw\" application, in which a user draws a\ntrajectory on a mobile device, and a quadrotor instantly flies that trajectory\nwith the DNN-enhanced control system. Experimental results demonstrate that the\nproposed approach improves the tracking precision for user-drawn trajectories\nafter the DNNs are trained on selected periodic trajectories, suggesting the\nmethod's potential in real-world applications. Tracking errors are reduced by\naround 40-50% for both training and testing trajectories from users,\nhighlighting the DNNs' capability of generalizing knowledge.\n", "versions": [{"version": "v1", "created": "Thu, 20 Oct 2016 04:00:27 GMT"}, {"version": "v2", "created": "Thu, 20 Jul 2017 00:47:25 GMT"}], "update_date": "2017-07-21", "authors_parsed": [["Li", "Qiyang", ""], ["Qian", "Jingxing", ""], ["Zhu", "Zining", ""], ["Bao", "Xuchan", ""], ["Helwa", "Mohamed K.", ""], ["Schoellig", "Angela P.", ""]]}, {"id": "1610.06402", "submitter": "Marc Pickett", "authors": "Marc Pickett and Rami Al-Rfou and Louis Shao and Chris Tar", "title": "A Growing Long-term Episodic & Semantic Memory", "comments": "Submission to NIPS workshop on Continual Learning. 4 page extended\n  abstract plus 5 more pages of references, figures, and supplementary material", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The long-term memory of most connectionist systems lies entirely in the\nweights of the system. Since the number of weights is typically fixed, this\nbounds the total amount of knowledge that can be learned and stored. Though\nthis is not normally a problem for a neural network designed for a specific\ntask, such a bound is undesirable for a system that continually learns over an\nopen range of domains. To address this, we describe a lifelong learning system\nthat leverages a fast, though non-differentiable, content-addressable memory\nwhich can be exploited to encode both a long history of sequential episodic\nknowledge and semantic knowledge over many episodes for an unbounded number of\ndomains. This opens the door for investigation into transfer learning, and\nleveraging prior knowledge that has been learned over a lifetime of experiences\nto new domains.\n", "versions": [{"version": "v1", "created": "Thu, 20 Oct 2016 13:29:56 GMT"}], "update_date": "2016-10-21", "authors_parsed": [["Pickett", "Marc", ""], ["Al-Rfou", "Rami", ""], ["Shao", "Louis", ""], ["Tar", "Chris", ""]]}, {"id": "1610.06421", "submitter": "Hao Dong", "authors": "Hao Dong, Akara Supratak, Wei Pan, Chao Wu, Paul M. Matthews and Yike\n  Guo", "title": "Mixed Neural Network Approach for Temporal Sleep Stage Classification", "comments": "THIS ARTICLE HAS BEEN PUBLISHED IN IEEE TRANSACTIONS ON NEURAL\n  SYSTEMS AND REHABILITATION ENGINEERING", "journal-ref": null, "doi": "10.1109/TNSRE.2017.2733220", "report-no": null, "categories": "q-bio.NC cs.CV cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a practical approach to addressing limitations posed by\nuse of single active electrodes in applications for sleep stage classification.\nElectroencephalography (EEG)-based characterizations of sleep stage progression\ncontribute the diagnosis and monitoring of the many pathologies of sleep.\nSeveral prior reports have explored ways of automating the analysis of sleep\nEEG and of reducing the complexity of the data needed for reliable\ndiscrimination of sleep stages in order to make it possible to perform sleep\nstudies at lower cost in the home (rather than only in specialized clinical\nfacilities). However, these reports have involved recordings from electrodes\nplaced on the cranial vertex or occiput, which can be uncomfortable or\ndifficult for subjects to position. Those that have utilized single EEG\nchannels which contain less sleep information, have showed poor classification\nperformance. We have taken advantage of Rectifier Neural Network for feature\ndetection and Long Short-Term Memory (LSTM) network for sequential data\nlearning to optimize classification performance with single electrode\nrecordings. After exploring alternative electrode placements, we found a\ncomfortable configuration of a single-channel EEG on the forehead and have\nshown that it can be integrated with additional electrodes for simultaneous\nrecording of the electroocuolgram (EOG). Evaluation of data from 62 people\n(with 494 hours sleep) demonstrated better performance of our analytical\nalgorithm for automated sleep classification than existing approaches using\nvertex or occipital electrode placements. Use of this recording configuration\nwith neural network deconvolution promises to make clinically indicated home\nsleep studies practical.\n", "versions": [{"version": "v1", "created": "Sat, 15 Oct 2016 18:48:00 GMT"}, {"version": "v2", "created": "Wed, 26 Jul 2017 17:39:53 GMT"}, {"version": "v3", "created": "Thu, 3 Aug 2017 15:00:48 GMT"}], "update_date": "2017-08-04", "authors_parsed": [["Dong", "Hao", ""], ["Supratak", "Akara", ""], ["Pan", "Wei", ""], ["Wu", "Chao", ""], ["Matthews", "Paul M.", ""], ["Guo", "Yike", ""]]}, {"id": "1610.06434", "submitter": "Ievgen Redko", "authors": "Ievgen Redko, Youn\\`es Bennani", "title": "Kernel Alignment for Unsupervised Transfer Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The ability of a human being to extrapolate previously gained knowledge to\nother domains inspired a new family of methods in machine learning called\ntransfer learning. Transfer learning is often based on the assumption that\nobjects in both target and source domains share some common feature and/or data\nspace. In this paper, we propose a simple and intuitive approach that minimizes\niteratively the distance between source and target task distributions by\noptimizing the kernel target alignment (KTA). We show that this procedure is\nsuitable for transfer learning by relating it to Hilbert-Schmidt Independence\nCriterion (HSIC) and Quadratic Mutual Information (QMI) maximization. We run\nour method on benchmark computer vision data sets and show that it can\noutperform some state-of-art methods.\n", "versions": [{"version": "v1", "created": "Thu, 20 Oct 2016 14:37:46 GMT"}], "update_date": "2016-10-21", "authors_parsed": [["Redko", "Ievgen", ""], ["Bennani", "Youn\u00e8s", ""]]}, {"id": "1610.06447", "submitter": "Nicolas Papadakis", "authors": "Arnaud Dessein and Nicolas Papadakis and Jean-Luc Rouas", "title": "Regularized Optimal Transport and the Rot Mover's Distance", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a unified framework for smooth convex regularization of\ndiscrete optimal transport problems. In this context, the regularized optimal\ntransport turns out to be equivalent to a matrix nearness problem with respect\nto Bregman divergences. Our framework thus naturally generalizes a previously\nproposed regularization based on the Boltzmann-Shannon entropy related to the\nKullback-Leibler divergence, and solved with the Sinkhorn-Knopp algorithm. We\ncall the regularized optimal transport distance the rot mover's distance in\nreference to the classical earth mover's distance. We develop two generic\nschemes that we respectively call the alternate scaling algorithm and the\nnon-negative alternate scaling algorithm, to compute efficiently the\nregularized optimal plans depending on whether the domain of the regularizer\nlies within the non-negative orthant or not. These schemes are based on\nDykstra's algorithm with alternate Bregman projections, and further exploit the\nNewton-Raphson method when applied to separable divergences. We enhance the\nseparable case with a sparse extension to deal with high data dimensions. We\nalso instantiate our proposed framework and discuss the inherent specificities\nfor well-known regularizers and statistical divergences in the machine learning\nand information geometry communities. Finally, we demonstrate the merits of our\nmethods with experiments using synthetic data to illustrate the effect of\ndifferent regularizers and penalties on the solutions, as well as real-world\ndata for a pattern recognition application to audio scene classification.\n", "versions": [{"version": "v1", "created": "Thu, 20 Oct 2016 14:49:36 GMT"}, {"version": "v2", "created": "Thu, 29 Jun 2017 11:05:10 GMT"}, {"version": "v3", "created": "Tue, 29 May 2018 11:47:37 GMT"}, {"version": "v4", "created": "Sat, 14 Jul 2018 20:01:46 GMT"}], "update_date": "2018-07-17", "authors_parsed": [["Dessein", "Arnaud", ""], ["Papadakis", "Nicolas", ""], ["Rouas", "Jean-Luc", ""]]}, {"id": "1610.06453", "submitter": "Ye Ye", "authors": "Stephanie Allen, David Madras, Ye Ye, Greg Zanotti", "title": "Change-point Detection Methods for Body-Worn Video", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Body-worn video (BWV) cameras are increasingly utilized by police departments\nto provide a record of police-public interactions. However, large-scale BWV\ndeployment produces terabytes of data per week, necessitating the development\nof effective computational methods to identify salient changes in video. In\nwork carried out at the 2016 RIPS program at IPAM, UCLA, we present a novel\ntwo-stage framework for video change-point detection. First, we employ\nstate-of-the-art machine learning methods including convolutional neural\nnetworks and support vector machines for scene classification. We then develop\nand compare change-point detection algorithms utilizing mean squared-error\nminimization, forecasting methods, hidden Markov models, and maximum likelihood\nestimation to identify noteworthy changes. We test our framework on detection\nof vehicle exits and entrances in a BWV data set provided by the Los Angeles\nPolice Department and achieve over 90% recall and nearly 70% precision --\ndemonstrating robustness to rapid scene changes, extreme luminance differences,\nand frequent camera occlusions.\n", "versions": [{"version": "v1", "created": "Thu, 20 Oct 2016 15:11:42 GMT"}], "update_date": "2016-10-21", "authors_parsed": [["Allen", "Stephanie", ""], ["Madras", "David", ""], ["Ye", "Ye", ""], ["Zanotti", "Greg", ""]]}, {"id": "1610.06492", "submitter": "Tomasz Kornuta", "authors": "Tomasz Kornuta and Kamil Rocki", "title": "Utilization of Deep Reinforcement Learning for saccadic-based object\n  visual search", "comments": "Paper submitted to special session on Machine Intelligence organized\n  during 23rd International AUTOMATION Conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The paper focuses on the problem of learning saccades enabling visual object\nsearch. The developed system combines reinforcement learning with a neural\nnetwork for learning to predict the possible outcomes of its actions. We\nvalidated the solution in three types of environment consisting of\n(pseudo)-randomly generated matrices of digits. The experimental verification\nis followed by the discussion regarding elements required by systems mimicking\nthe fovea movement and possible further research directions.\n", "versions": [{"version": "v1", "created": "Thu, 20 Oct 2016 16:34:08 GMT"}], "update_date": "2016-10-21", "authors_parsed": [["Kornuta", "Tomasz", ""], ["Rocki", "Kamil", ""]]}, {"id": "1610.06525", "submitter": "Lucas Maystre", "authors": "Lucas Maystre, Matthias Grossglauser", "title": "ChoiceRank: Identifying Preferences from Node Traffic in Networks", "comments": "Accepted at ICML 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG cs.SI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Understanding how users navigate in a network is of high interest in many\napplications. We consider a setting where only aggregate node-level traffic is\nobserved and tackle the task of learning edge transition probabilities. We cast\nit as a preference learning problem, and we study a model where choices follow\nLuce's axiom. In this case, the $O(n)$ marginal counts of node visits are a\nsufficient statistic for the $O(n^2)$ transition probabilities. We show how to\nmake the inference problem well-posed regardless of the network's structure,\nand we present ChoiceRank, an iterative algorithm that scales to networks that\ncontains billions of nodes and edges. We apply the model to two clickstream\ndatasets and show that it successfully recovers the transition probabilities\nusing only the network structure and marginal (node-level) traffic data.\nFinally, we also consider an application to mobility networks and apply the\nmodel to one year of rides on New York City's bicycle-sharing system.\n", "versions": [{"version": "v1", "created": "Thu, 20 Oct 2016 18:19:07 GMT"}, {"version": "v2", "created": "Thu, 15 Jun 2017 15:14:54 GMT"}], "update_date": "2017-06-16", "authors_parsed": [["Maystre", "Lucas", ""], ["Grossglauser", "Matthias", ""]]}, {"id": "1610.06534", "submitter": "Ugo Rosolia", "authors": "Ugo Rosolia, Ashwin Carvalho and Francesco Borrelli", "title": "Autonomous Racing using Learning Model Predictive Control", "comments": "Extended version of the paper accepted to ACC", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A novel learning Model Predictive Control technique is applied to the\nautonomous racing problem. The goal of the controller is to minimize the time\nto complete a lap. The proposed control strategy uses the data from previous\nlaps to improve its performance while satisfying safety requirements. Moreover,\na system identification technique is proposed to estimate the vehicle dynamics.\nSimulation results with the high fidelity simulator software CarSim show the\neffectiveness of the proposed control scheme.\n", "versions": [{"version": "v1", "created": "Thu, 20 Oct 2016 18:47:05 GMT"}, {"version": "v2", "created": "Mon, 19 Dec 2016 10:14:54 GMT"}, {"version": "v3", "created": "Wed, 22 Feb 2017 00:04:34 GMT"}, {"version": "v4", "created": "Sat, 11 Mar 2017 07:13:39 GMT"}, {"version": "v5", "created": "Fri, 14 Apr 2017 19:03:11 GMT"}, {"version": "v6", "created": "Thu, 9 Nov 2017 02:32:50 GMT"}], "update_date": "2017-11-10", "authors_parsed": [["Rosolia", "Ugo", ""], ["Carvalho", "Ashwin", ""], ["Borrelli", "Francesco", ""]]}, {"id": "1610.06603", "submitter": "Wei Hu", "authors": "Wei Chen, Wei Hu, Fu Li, Jian Li, Yu Liu, Pinyan Lu", "title": "Combinatorial Multi-Armed Bandit with General Reward Functions", "comments": "Published in Neural Information Processing Systems (NIPS) 2016. New\n  in this version: a minor bug fix", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we study the stochastic combinatorial multi-armed bandit\n(CMAB) framework that allows a general nonlinear reward function, whose\nexpected value may not depend only on the means of the input random variables\nbut possibly on the entire distributions of these variables. Our framework\nenables a much larger class of reward functions such as the $\\max()$ function\nand nonlinear utility functions. Existing techniques relying on accurate\nestimations of the means of random variables, such as the upper confidence\nbound (UCB) technique, do not work directly on these functions. We propose a\nnew algorithm called stochastically dominant confidence bound (SDCB), which\nestimates the distributions of underlying random variables and their\nstochastically dominant confidence bounds. We prove that SDCB can achieve\n$O(\\log{T})$ distribution-dependent regret and $\\tilde{O}(\\sqrt{T})$\ndistribution-independent regret, where $T$ is the time horizon. We apply our\nresults to the $K$-MAX problem and expected utility maximization problems. In\nparticular, for $K$-MAX, we provide the first polynomial-time approximation\nscheme (PTAS) for its offline problem, and give the first $\\tilde{O}(\\sqrt T)$\nbound on the $(1-\\epsilon)$-approximation regret of its online problem, for any\n$\\epsilon>0$.\n", "versions": [{"version": "v1", "created": "Thu, 20 Oct 2016 20:54:41 GMT"}, {"version": "v2", "created": "Tue, 31 Jan 2017 18:18:05 GMT"}, {"version": "v3", "created": "Wed, 1 Feb 2017 04:49:22 GMT"}, {"version": "v4", "created": "Fri, 20 Jul 2018 17:38:35 GMT"}], "update_date": "2018-07-23", "authors_parsed": [["Chen", "Wei", ""], ["Hu", "Wei", ""], ["Li", "Fu", ""], ["Li", "Jian", ""], ["Liu", "Yu", ""], ["Lu", "Pinyan", ""]]}, {"id": "1610.06633", "submitter": "Arun Kumar", "authors": "Arun Kumar, Paul Schrater", "title": "Novelty Learning via Collaborative Proximity Filtering", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The vast majority of recommender systems model preferences as static or\nslowly changing due to observable user experience. However, spontaneous changes\nin user preferences are ubiquitous in many domains like media consumption and\nkey factors that drive changes in preferences are not directly observable.\nThese latent sources of preference change pose new challenges. When systems do\nnot track and adapt to users' tastes, users lose confidence and trust,\nincreasing the risk of user churn. We meet these challenges by developing a\nmodel of novelty preferences that learns and tracks latent user tastes. We\ncombine three innovations: a new measure of item similarity based on patterns\nof consumption co-occurrence; model for {\\em spontaneous} changes in\npreferences; and a learning agent that tracks each user's dynamic preferences\nand learns individualized policies for variety. The resulting framework\nadaptively provides users with novelty tailored to their preferences for change\nper se.\n", "versions": [{"version": "v1", "created": "Fri, 21 Oct 2016 00:31:46 GMT"}], "update_date": "2016-10-24", "authors_parsed": [["Kumar", "Arun", ""], ["Schrater", "Paul", ""]]}, {"id": "1610.06656", "submitter": "Shanshan Wu", "authors": "Shanshan Wu, Srinadh Bhojanapalli, Sujay Sanghavi, Alexandros G.\n  Dimakis", "title": "Single Pass PCA of Matrix Products", "comments": "24 pages, 4 figures, NIPS 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.DS cs.IT cs.LG math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we present a new algorithm for computing a low rank\napproximation of the product $A^TB$ by taking only a single pass of the two\nmatrices $A$ and $B$. The straightforward way to do this is to (a) first sketch\n$A$ and $B$ individually, and then (b) find the top components using PCA on the\nsketch. Our algorithm in contrast retains additional summary information about\n$A,B$ (e.g. row and column norms etc.) and uses this additional information to\nobtain an improved approximation from the sketches. Our main analytical result\nestablishes a comparable spectral norm guarantee to existing two-pass methods;\nin addition we also provide results from an Apache Spark implementation that\nshows better computational and statistical performance on real-world and\nsynthetic evaluation datasets.\n", "versions": [{"version": "v1", "created": "Fri, 21 Oct 2016 02:45:46 GMT"}, {"version": "v2", "created": "Wed, 26 Oct 2016 13:58:24 GMT"}], "update_date": "2016-10-27", "authors_parsed": [["Wu", "Shanshan", ""], ["Bhojanapalli", "Srinadh", ""], ["Sanghavi", "Sujay", ""], ["Dimakis", "Alexandros G.", ""]]}, {"id": "1610.06664", "submitter": "Changyou Chen", "authors": "Changyou Chen and Nan Ding and Chunyuan Li and Yizhe Zhang and\n  Lawrence Carin", "title": "Stochastic Gradient MCMC with Stale Gradients", "comments": "NIPS2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Stochastic gradient MCMC (SG-MCMC) has played an important role in\nlarge-scale Bayesian learning, with well-developed theoretical convergence\nproperties. In such applications of SG-MCMC, it is becoming increasingly\npopular to employ distributed systems, where stochastic gradients are computed\nbased on some outdated parameters, yielding what are termed stale gradients.\nWhile stale gradients could be directly used in SG-MCMC, their impact on\nconvergence properties has not been well studied. In this paper we develop\ntheory to show that while the bias and MSE of an SG-MCMC algorithm depend on\nthe staleness of stochastic gradients, its estimation variance (relative to the\nexpected estimate, based on a prescribed number of samples) is independent of\nit. In a simple Bayesian distributed system with SG-MCMC, where stale gradients\nare computed asynchronously by a set of workers, our theory indicates a linear\nspeedup on the decrease of estimation variance w.r.t. the number of workers.\nExperiments on synthetic data and deep neural networks validate our theory,\ndemonstrating the effectiveness and scalability of SG-MCMC with stale\ngradients.\n", "versions": [{"version": "v1", "created": "Fri, 21 Oct 2016 04:18:11 GMT"}], "update_date": "2016-10-24", "authors_parsed": [["Chen", "Changyou", ""], ["Ding", "Nan", ""], ["Li", "Chunyuan", ""], ["Zhang", "Yizhe", ""], ["Carin", "Lawrence", ""]]}, {"id": "1610.06700", "submitter": "Hao Tang", "authors": "Hao Tang, Weiran Wang, Kevin Gimpel, Karen Livescu", "title": "End-to-End Training Approaches for Discriminative Segmental Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent work on discriminative segmental models has shown that they can\nachieve competitive speech recognition performance, using features based on\ndeep neural frame classifiers. However, segmental models can be more\nchallenging to train than standard frame-based approaches. While some segmental\nmodels have been successfully trained end to end, there is a lack of\nunderstanding of their training under different settings and with different\nlosses.\n  We investigate a model class based on recent successful approaches,\nconsisting of a linear model that combines segmental features based on an LSTM\nframe classifier. Similarly to hybrid HMM-neural network models, segmental\nmodels of this class can be trained in two stages (frame classifier training\nfollowed by linear segmental model weight training), end to end (joint training\nof both frame classifier and linear weights), or with end-to-end fine-tuning\nafter two-stage training.\n  We study segmental models trained end to end with hinge loss, log loss,\nlatent hinge loss, and marginal log loss. We consider several losses for the\ncase where training alignments are available as well as where they are not.\n  We find that in general, marginal log loss provides the most consistent\nstrong performance without requiring ground-truth alignments. We also find that\ntraining with dropout is very important in obtaining good performance with\nend-to-end training. Finally, the best results are typically obtained by a\ncombination of two-stage training and fine-tuning.\n", "versions": [{"version": "v1", "created": "Fri, 21 Oct 2016 08:45:35 GMT"}], "update_date": "2016-10-24", "authors_parsed": [["Tang", "Hao", ""], ["Wang", "Weiran", ""], ["Gimpel", "Kevin", ""], ["Livescu", "Karen", ""]]}, {"id": "1610.06761", "submitter": "Erik Rodner", "authors": "Erik Rodner, Bj\\\"orn Barz, Yanira Guanche, Milan Flach, Miguel\n  Mahecha, Paul Bodesheim, Markus Reichstein, Joachim Denzler", "title": "Maximally Divergent Intervals for Anomaly Detection", "comments": "ICML Workshop on Anomaly Detection", "journal-ref": null, "doi": "10.17871/BACI_ICML2016_Rodner", "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present new methods for batch anomaly detection in multivariate time\nseries. Our methods are based on maximizing the Kullback-Leibler divergence\nbetween the data distribution within and outside an interval of the time\nseries. An empirical analysis shows the benefits of our algorithms compared to\nmethods that treat each time step independently from each other without\noptimizing with respect to all possible intervals.\n", "versions": [{"version": "v1", "created": "Fri, 21 Oct 2016 12:30:30 GMT"}], "update_date": "2016-10-24", "authors_parsed": [["Rodner", "Erik", ""], ["Barz", "Bj\u00f6rn", ""], ["Guanche", "Yanira", ""], ["Flach", "Milan", ""], ["Mahecha", "Miguel", ""], ["Bodesheim", "Paul", ""], ["Reichstein", "Markus", ""], ["Denzler", "Joachim", ""]]}, {"id": "1610.06781", "submitter": "Fangyi Zhang", "authors": "Fangyi Zhang, J\\\"urgen Leitner, Michael Milford, Peter Corke", "title": "Modular Deep Q Networks for Sim-to-real Transfer of Visuo-motor Policies", "comments": "Australasian Conference on Robotics and Automation (ACRA) 2017,\n  Student Paper Award Finalist", "journal-ref": "The proceedings of the Australasian Conference on Robotics and\n  Automation (ACRA) 2017", "doi": null, "report-no": null, "categories": "cs.RO cs.AI cs.CV cs.LG cs.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While deep learning has had significant successes in computer vision thanks\nto the abundance of visual data, collecting sufficiently large real-world\ndatasets for robot learning can be costly. To increase the practicality of\nthese techniques on real robots, we propose a modular deep reinforcement\nlearning method capable of transferring models trained in simulation to a\nreal-world robotic task. We introduce a bottleneck between perception and\ncontrol, enabling the networks to be trained independently, but then merged and\nfine-tuned in an end-to-end manner to further improve hand-eye coordination. On\na canonical, planar visually-guided robot reaching task a fine-tuned accuracy\nof 1.6 pixels is achieved, a significant improvement over naive transfer (17.5\npixels), showing the potential for more complicated and broader applications.\nOur method provides a technique for more efficient learning and transfer of\nvisuo-motor policies for real robotic systems without relying entirely on large\nreal-world robot datasets.\n", "versions": [{"version": "v1", "created": "Fri, 21 Oct 2016 13:36:25 GMT"}, {"version": "v2", "created": "Wed, 1 Mar 2017 09:59:51 GMT"}, {"version": "v3", "created": "Mon, 17 Jul 2017 09:59:35 GMT"}, {"version": "v4", "created": "Tue, 19 Dec 2017 04:59:03 GMT"}], "update_date": "2017-12-20", "authors_parsed": [["Zhang", "Fangyi", ""], ["Leitner", "J\u00fcrgen", ""], ["Milford", "Michael", ""], ["Corke", "Peter", ""]]}, {"id": "1610.06806", "submitter": "Tianpei Xie", "authors": "Tianpei Xie, Nasser. M. Narabadi and Alfred O. Hero", "title": "Robust training on approximated minimal-entropy set", "comments": "13 pages; Accepted in Transaction on Signal Processing, 2016. arXiv\n  admin note: text overlap with arXiv:1507.04540", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a general framework to learn a robust large-margin\nbinary classifier when corrupt measurements, called anomalies, caused by sensor\nfailure might be present in the training set. The goal is to minimize the\ngeneralization error of the classifier on non-corrupted measurements while\ncontrolling the false alarm rate associated with anomalous samples. By\nincorporating a non-parametric regularizer based on an empirical entropy\nestimator, we propose a Geometric-Entropy-Minimization regularized Maximum\nEntropy Discrimination (GEM-MED) method to learn to classify and detect\nanomalies in a joint manner. We demonstrate using simulated data and a real\nmultimodal data set. Our GEM-MED method can yield improved performance over\nprevious robust classification methods in terms of both classification accuracy\nand anomaly detection rate.\n", "versions": [{"version": "v1", "created": "Fri, 21 Oct 2016 14:38:38 GMT"}], "update_date": "2016-10-24", "authors_parsed": [["Xie", "Tianpei", ""], ["Narabadi", "Nasser. M.", ""], ["Hero", "Alfred O.", ""]]}, {"id": "1610.06811", "submitter": "Carlos M. Ala\\'iz", "authors": "Carlos M. Ala\\'iz, Micha\\\"el Fanuel, Johan A. K. Suykens", "title": "Convex Formulation for Kernel PCA and its Use in Semi-Supervised\n  Learning", "comments": null, "journal-ref": "IEEE Transactions on Neural Networks and Learning Systems, 99, 1-7\n  (2017)", "doi": "10.1109/TNNLS.2017.2709838", "report-no": "ESAT-SISTA: 16-166", "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, Kernel PCA is reinterpreted as the solution to a convex\noptimization problem. Actually, there is a constrained convex problem for each\nprincipal component, so that the constraints guarantee that the principal\ncomponent is indeed a solution, and not a mere saddle point. Although these\ninsights do not imply any algorithmic improvement, they can be used to further\nunderstand the method, formulate possible extensions and properly address them.\nAs an example, a new convex optimization problem for semi-supervised\nclassification is proposed, which seems particularly well-suited whenever the\nnumber of known labels is small. Our formulation resembles a Least Squares SVM\nproblem with a regularization parameter multiplied by a negative sign, combined\nwith a variational principle for Kernel PCA. Our primal optimization principle\nfor semi-supervised learning is solved in terms of the Lagrange multipliers.\nNumerical experiments in several classification tasks illustrate the\nperformance of the proposed model in problems with only a few labeled data.\n", "versions": [{"version": "v1", "created": "Fri, 21 Oct 2016 14:55:48 GMT"}], "update_date": "2017-10-25", "authors_parsed": [["Ala\u00edz", "Carlos M.", ""], ["Fanuel", "Micha\u00ebl", ""], ["Suykens", "Johan A. K.", ""]]}, {"id": "1610.06848", "submitter": "Daniel Seita", "authors": "Daniel Seita, Xinlei Pan, Haoyu Chen, John Canny", "title": "An Efficient Minibatch Acceptance Test for Metropolis-Hastings", "comments": "Final version for UAI 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a novel Metropolis-Hastings method for large datasets that uses\nsmall expected-size minibatches of data. Previous work on reducing the cost of\nMetropolis-Hastings tests yield variable data consumed per sample, with only\nconstant factor reductions versus using the full dataset for each sample. Here\nwe present a method that can be tuned to provide arbitrarily small batch sizes,\nby adjusting either proposal step size or temperature. Our test uses the\nnoise-tolerant Barker acceptance test with a novel additive correction\nvariable. The resulting test has similar cost to a normal SGD update. Our\nexperiments demonstrate several order-of-magnitude speedups over previous work.\n", "versions": [{"version": "v1", "created": "Wed, 19 Oct 2016 00:19:25 GMT"}, {"version": "v2", "created": "Thu, 30 Mar 2017 21:08:51 GMT"}, {"version": "v3", "created": "Sun, 9 Jul 2017 16:36:03 GMT"}], "update_date": "2017-07-11", "authors_parsed": [["Seita", "Daniel", ""], ["Pan", "Xinlei", ""], ["Chen", "Haoyu", ""], ["Canny", "John", ""]]}, {"id": "1610.06918", "submitter": "David Andersen", "authors": "Mart\\'in Abadi and David G. Andersen (Google Brain)", "title": "Learning to Protect Communications with Adversarial Neural Cryptography", "comments": "15 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We ask whether neural networks can learn to use secret keys to protect\ninformation from other neural networks. Specifically, we focus on ensuring\nconfidentiality properties in a multiagent system, and we specify those\nproperties in terms of an adversary. Thus, a system may consist of neural\nnetworks named Alice and Bob, and we aim to limit what a third neural network\nnamed Eve learns from eavesdropping on the communication between Alice and Bob.\nWe do not prescribe specific cryptographic algorithms to these neural networks;\ninstead, we train end-to-end, adversarially. We demonstrate that the neural\nnetworks can learn how to perform forms of encryption and decryption, and also\nhow to apply these operations selectively in order to meet confidentiality\ngoals.\n", "versions": [{"version": "v1", "created": "Fri, 21 Oct 2016 19:58:29 GMT"}], "update_date": "2016-10-24", "authors_parsed": [["Abadi", "Mart\u00edn", "", "Google Brain"], ["Andersen", "David G.", "", "Google Brain"]]}, {"id": "1610.06920", "submitter": "Jorge Albericio", "authors": "J. Albericio, P. Judd, A. Delm\\'as, S. Sharify, A. Moshovos", "title": "Bit-pragmatic Deep Neural Network Computing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.AR cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We quantify a source of ineffectual computations when processing the\nmultiplications of the convolutional layers in Deep Neural Networks (DNNs) and\npropose Pragmatic (PRA), an architecture that exploits it improving performance\nand energy efficiency. The source of these ineffectual computations is best\nunderstood in the context of conventional multipliers which generate internally\nmultiple terms, that is, products of the multiplicand and powers of two, which\nadded together produce the final product [1]. At runtime, many of these terms\nare zero as they are generated when the multiplicand is combined with the\nzero-bits of the multiplicator. While conventional bit-parallel multipliers\ncalculate all terms in parallel to reduce individual product latency, PRA\ncalculates only the non-zero terms using a) on-the-fly conversion of the\nmultiplicator representation into an explicit list of powers of two, and b)\nhybrid bit-parallel multplicand/bit-serial multiplicator processing units. PRA\nexploits two sources of ineffectual computations: 1) the aforementioned zero\nproduct terms which are the result of the lack of explicitness in the\nmultiplicator representation, and 2) the excess in the representation precision\nused for both multiplicants and multiplicators, e.g., [2]. Measurements\ndemonstrate that for the convolutional layers, a straightforward variant of PRA\nimproves performance by 2.6x over the DaDiaNao (DaDN) accelerator [3] and by\n1.4x over STR [4]. Similarly, PRA improves energy efficiency by 28% and 10% on\naverage compared to DaDN and STR. An improved cross lane synchronication scheme\nboosts performance improvements to 3.1x over DaDN. Finally, Pragmatic benefits\npersist even with an 8-bit quantized representation [5].\n", "versions": [{"version": "v1", "created": "Thu, 20 Oct 2016 22:16:05 GMT"}], "update_date": "2016-10-25", "authors_parsed": [["Albericio", "J.", ""], ["Judd", "P.", ""], ["Delm\u00e1s", "A.", ""], ["Sharify", "S.", ""], ["Moshovos", "A.", ""]]}, {"id": "1610.06940", "submitter": "Xiaowei Huang", "authors": "Xiaowei Huang and Marta Kwiatkowska and Sen Wang and Min Wu", "title": "Safety Verification of Deep Neural Networks", "comments": "To appear as invited paper at CAV 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural networks have achieved impressive experimental results in image\nclassification, but can surprisingly be unstable with respect to adversarial\nperturbations, that is, minimal changes to the input image that cause the\nnetwork to misclassify it. With potential applications including perception\nmodules and end-to-end controllers for self-driving cars, this raises concerns\nabout their safety. We develop a novel automated verification framework for\nfeed-forward multi-layer neural networks based on Satisfiability Modulo Theory\n(SMT). We focus on safety of image classification decisions with respect to\nimage manipulations, such as scratches or changes to camera angle or lighting\nconditions that would result in the same class being assigned by a human, and\ndefine safety for an individual decision in terms of invariance of the\nclassification within a small neighbourhood of the original image. We enable\nexhaustive search of the region by employing discretisation, and propagate the\nanalysis layer by layer. Our method works directly with the network code and,\nin contrast to existing methods, can guarantee that adversarial examples, if\nthey exist, are found for the given region and family of manipulations. If\nfound, adversarial examples can be shown to human testers and/or used to\nfine-tune the network. We implement the techniques using Z3 and evaluate them\non state-of-the-art networks, including regularised and deep learning networks.\nWe also compare against existing techniques to search for adversarial examples\nand estimate network robustness.\n", "versions": [{"version": "v1", "created": "Fri, 21 Oct 2016 20:16:16 GMT"}, {"version": "v2", "created": "Sat, 5 Nov 2016 16:05:08 GMT"}, {"version": "v3", "created": "Fri, 5 May 2017 10:16:50 GMT"}], "update_date": "2017-05-08", "authors_parsed": [["Huang", "Xiaowei", ""], ["Kwiatkowska", "Marta", ""], ["Wang", "Sen", ""], ["Wu", "Min", ""]]}, {"id": "1610.06972", "submitter": "Himabindu Lakkaraju", "authors": "Himabindu Lakkaraju, Cynthia Rudin", "title": "Learning Cost-Effective Treatment Regimes using Markov Decision\n  Processes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Decision makers, such as doctors and judges, make crucial decisions such as\nrecommending treatments to patients, and granting bails to defendants on a\ndaily basis. Such decisions typically involve weighting the potential benefits\nof taking an action against the costs involved. In this work, we aim to\nautomate this task of learning \\emph{cost-effective, interpretable and\nactionable treatment regimes}. We formulate this as a problem of learning a\ndecision list -- a sequence of if-then-else rules -- which maps characteristics\nof subjects (eg., diagnostic test results of patients) to treatments. We\npropose a novel objective to construct a decision list which maximizes outcomes\nfor the population, and minimizes overall costs. We model the problem of\nlearning such a list as a Markov Decision Process (MDP) and employ a variant of\nthe Upper Confidence Bound for Trees (UCT) strategy which leverages customized\nchecks for pruning the search space effectively. Experimental results on real\nworld observational data capturing judicial bail decisions and treatment\nrecommendations for asthma patients demonstrate the effectiveness of our\napproach.\n", "versions": [{"version": "v1", "created": "Fri, 21 Oct 2016 23:17:03 GMT"}], "update_date": "2016-10-25", "authors_parsed": [["Lakkaraju", "Himabindu", ""], ["Rudin", "Cynthia", ""]]}, {"id": "1610.06998", "submitter": "Andre Pacheco", "authors": "Andre G. C. Pacheco and Renato A. Krohling", "title": "Ranking of classification algorithms in terms of mean-standard deviation\n  using A-TOPSIS", "comments": "16 pages, 8 figures and 11 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In classification problems when multiples algorithms are applied to different\nbenchmarks a difficult issue arises, i.e., how can we rank the algorithms? In\nmachine learning it is common run the algorithms several times and then a\nstatistic is calculated in terms of means and standard deviations. In order to\ncompare the performance of the algorithms, it is very common to employ\nstatistical tests. However, these tests may also present limitations, since\nthey consider only the means and not the standard deviations of the obtained\nresults. In this paper, we present the so called A-TOPSIS, based on TOPSIS\n(Technique for Order Preference by Similarity to Ideal Solution), to solve the\nproblem of ranking and comparing classification algorithms in terms of means\nand standard deviations. We use two case studies to illustrate the A-TOPSIS for\nranking classification algorithms and the results show the suitability of\nA-TOPSIS to rank the algorithms. The presented approach is general and can be\napplied to compare the performance of stochastic algorithms in machine\nlearning. Finally, to encourage researchers to use the A-TOPSIS for ranking\nalgorithms we also presented in this work an easy-to-use A-TOPSIS web\nframework.\n", "versions": [{"version": "v1", "created": "Sat, 22 Oct 2016 05:19:44 GMT"}], "update_date": "2016-10-25", "authors_parsed": [["Pacheco", "Andre G. C.", ""], ["Krohling", "Renato A.", ""]]}, {"id": "1610.07031", "submitter": "Terry Taewoong Um", "authors": "Terry Taewoong Um, Vahid Babakeshizadeh and Dana Kuli\\'c", "title": "Exercise Motion Classification from Large-Scale Wearable Sensor Data\n  Using Convolutional Neural Networks", "comments": "will appear in IROS2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The ability to accurately identify human activities is essential for\ndeveloping automatic rehabilitation and sports training systems. In this paper,\nlarge-scale exercise motion data obtained from a forearm-worn wearable sensor\nare classified with a convolutional neural network (CNN). Time-series data\nconsisting of accelerometer and orientation measurements are formatted as\nimages, allowing the CNN to automatically extract discriminative features. A\ncomparative study on the effects of image formatting and different CNN\narchitectures is also presented. The best performing configuration classifies\n50 gym exercises with 92.1% accuracy.\n", "versions": [{"version": "v1", "created": "Sat, 22 Oct 2016 10:46:01 GMT"}, {"version": "v2", "created": "Thu, 2 Mar 2017 04:35:49 GMT"}, {"version": "v3", "created": "Sat, 22 Jul 2017 22:09:17 GMT"}], "update_date": "2017-07-25", "authors_parsed": [["Um", "Terry Taewoong", ""], ["Babakeshizadeh", "Vahid", ""], ["Kuli\u0107", "Dana", ""]]}, {"id": "1610.07116", "submitter": "Bowei Yan", "authors": "Bowei Yan, Oluwasanmi Koyejo, Kai Zhong, Pradeep Ravikumar", "title": "Online Classification with Complex Metrics", "comments": "An error was found in the proof", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a framework and analysis of consistent binary classification for\ncomplex and non-decomposable performance metrics such as the F-measure and the\nJaccard measure. The proposed framework is general, as it applies to both batch\nand online learning, and to both linear and non-linear models. Our work follows\nrecent results showing that the Bayes optimal classifier for many complex\nmetrics is given by a thresholding of the conditional probability of the\npositive class. This manuscript extends this thresholding characterization --\nshowing that the utility is strictly locally quasi-concave with respect to the\nthreshold for a wide range of models and performance metrics. This, in turn,\nmotivates simple normalized gradient ascent updates for threshold estimation.\nWe present a finite-sample regret analysis for the resulting procedure. In\nparticular, the risk for the batch case converges to the Bayes risk at the same\nrate as that of the underlying conditional probability estimation, and the risk\nof proposed online algorithm converges at a rate that depends on the\nconditional probability estimation risk. For instance, in the special case\nwhere the conditional probability model is logistic regression, our procedure\nachieves $O(\\frac{1}{\\sqrt{n}})$ sample complexity, both for batch and online\ntraining. Empirical evaluation shows that the proposed algorithms out-perform\nalternatives in practice, with comparable or better prediction performance and\nreduced run time for various metrics and datasets.\n", "versions": [{"version": "v1", "created": "Sun, 23 Oct 2016 02:56:03 GMT"}, {"version": "v2", "created": "Sat, 10 Feb 2018 17:55:44 GMT"}], "update_date": "2018-02-13", "authors_parsed": [["Yan", "Bowei", ""], ["Koyejo", "Oluwasanmi", ""], ["Zhong", "Kai", ""], ["Ravikumar", "Pradeep", ""]]}, {"id": "1610.07119", "submitter": "Yi Tay", "authors": "Minh C. Phan, Yi Tay, Tuan-Anh Nguyen Pham", "title": "Cross Device Matching for Online Advertising with Neural Feature\n  Ensembles : First Place Solution at CIKM Cup 2016", "comments": "4 pages Competition Report for CIKM Cup", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.IR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe the 1st place winning approach for the CIKM Cup 2016 Challenge.\nIn this paper, we provide an approach to reasonably identify same users across\nmultiple devices based on browsing logs. Our approach regards a candidate\nranking problem as pairwise classification and utilizes an unsupervised neural\nfeature ensemble approach to learn latent features of users. Combined with\ntraditional hand crafted features, each user pair feature is fed into a\nsupervised classifier in order to perform pairwise classification. Lastly, we\npropose supervised and unsupervised inference techniques.\n", "versions": [{"version": "v1", "created": "Sun, 23 Oct 2016 03:25:05 GMT"}, {"version": "v2", "created": "Sun, 19 Feb 2017 03:33:03 GMT"}], "update_date": "2017-02-21", "authors_parsed": [["Phan", "Minh C.", ""], ["Tay", "Yi", ""], ["Pham", "Tuan-Anh Nguyen", ""]]}, {"id": "1610.07183", "submitter": "Tarun Kathuria", "authors": "L. Elisa Celis, Amit Deshpande, Tarun Kathuria, Nisheeth K. Vishnoi", "title": "How to be Fair and Diverse?", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Due to the recent cases of algorithmic bias in data-driven decision-making,\nmachine learning methods are being put under the microscope in order to\nunderstand the root cause of these biases and how to correct them. Here, we\nconsider a basic algorithmic task that is central in machine learning:\nsubsampling from a large data set. Subsamples are used both as an end-goal in\ndata summarization (where fairness could either be a legal, political or moral\nrequirement) and to train algorithms (where biases in the samples are often a\nsource of bias in the resulting model). Consequently, there is a growing effort\nto modify either the subsampling methods or the algorithms themselves in order\nto ensure fairness. However, in doing so, a question that seems to be\noverlooked is whether it is possible to produce fair subsamples that are also\nadequately representative of the feature space of the data set - an important\nand classic requirement in machine learning. Can diversity and fairness be\nsimultaneously ensured? We start by noting that, in some applications,\nguaranteeing one does not necessarily guarantee the other, and a new approach\nis required. Subsequently, we present an algorithmic framework which allows us\nto produce both fair and diverse samples. Our experimental results on an image\nsummarization task show marked improvements in fairness without compromising\nfeature diversity by much, giving us the best of both the worlds.\n", "versions": [{"version": "v1", "created": "Sun, 23 Oct 2016 15:13:46 GMT"}], "update_date": "2016-10-25", "authors_parsed": [["Celis", "L. Elisa", ""], ["Deshpande", "Amit", ""], ["Kathuria", "Tarun", ""], ["Vishnoi", "Nisheeth K.", ""]]}, {"id": "1610.07187", "submitter": "Adam Gonczarek", "authors": "Adam Gonczarek, Jakub M. Tomczak, Szymon Zar\\k{e}ba, Joanna Kaczmar,\n  Piotr D\\k{a}browski, Micha{\\l} J. Walczak", "title": "Learning Deep Architectures for Interaction Prediction in\n  Structure-based Virtual Screening", "comments": "Workshop on Machine Learning in Computational Biology. 30th\n  Conference on Neural Information Processing Systems (NIPS 2016), Barcelona,\n  Spain Extended version published in Computers in Biology and Medicine and\n  available online:\n  http://www.sciencedirect.com/science/article/pii/S0010482517302974", "journal-ref": null, "doi": "10.1016/j.compbiomed.2017.09.007", "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a deep learning architecture for structure-based virtual\nscreening that generates fixed-sized fingerprints of proteins and small\nmolecules by applying learnable atom convolution and softmax operations to each\ncompound separately. These fingerprints are further transformed non-linearly,\ntheir inner-product is calculated and used to predict the binding potential.\nMoreover, we show that widely used benchmark datasets may be insufficient for\ntesting structure-based virtual screening methods that utilize machine\nlearning. Therefore, we introduce a new benchmark dataset, which we constructed\nbased on DUD-E and PDBBind databases.\n", "versions": [{"version": "v1", "created": "Sun, 23 Oct 2016 15:51:46 GMT"}, {"version": "v2", "created": "Mon, 6 Feb 2017 14:14:51 GMT"}, {"version": "v3", "created": "Tue, 19 Sep 2017 09:52:06 GMT"}], "update_date": "2017-09-20", "authors_parsed": [["Gonczarek", "Adam", ""], ["Tomczak", "Jakub M.", ""], ["Zar\u0119ba", "Szymon", ""], ["Kaczmar", "Joanna", ""], ["D\u0105browski", "Piotr", ""], ["Walczak", "Micha\u0142 J.", ""]]}, {"id": "1610.07258", "submitter": "Zhiguang Wang", "authors": "Zhiguang Wang, Wei Song, Lu Liu, Fan Zhang, Junxiao Xue, Yangdong Ye,\n  Ming Fan, Mingliang Xu", "title": "Representation Learning with Deconvolution for Multivariate Time Series\n  Classification and Visualization", "comments": "arXiv admin note: text overlap with arXiv:1505.04366 by other authors", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new model based on the deconvolutional networks and SAX\ndiscretization to learn the representation for multivariate time series.\nDeconvolutional networks fully exploit the advantage the powerful\nexpressiveness of deep neural networks in the manner of unsupervised learning.\nWe design a network structure specifically to capture the cross-channel\ncorrelation with deconvolution, forcing the pooling operation to perform the\ndimension reduction along each position in the individual channel.\nDiscretization based on Symbolic Aggregate Approximation is applied on the\nfeature vectors to further extract the bag of features. We show how this\nrepresentation and bag of features helps on classification. A full comparison\nwith the sequence distance based approach is provided to demonstrate the\neffectiveness of our approach on the standard datasets. We further build the\nMarkov matrix from the discretized representation from the deconvolution to\nvisualize the time series as complex networks, which show more class-specific\nstatistical properties and clear structures with respect to different labels.\n", "versions": [{"version": "v1", "created": "Mon, 24 Oct 2016 01:53:12 GMT"}, {"version": "v2", "created": "Wed, 26 Oct 2016 00:17:45 GMT"}, {"version": "v3", "created": "Sat, 26 Nov 2016 21:02:49 GMT"}], "update_date": "2016-12-04", "authors_parsed": [["Wang", "Zhiguang", ""], ["Song", "Wei", ""], ["Liu", "Lu", ""], ["Zhang", "Fan", ""], ["Xue", "Junxiao", ""], ["Ye", "Yangdong", ""], ["Fan", "Ming", ""], ["Xu", "Mingliang", ""]]}, {"id": "1610.07273", "submitter": "Zhiguang Wang", "authors": "Lu Liu, Zhiguang Wang", "title": "Encoding Temporal Markov Dynamics in Graph for Visualizing and Mining\n  Time Series", "comments": "AAAI 2018 workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Time series and signals are attracting more attention across statistics,\nmachine learning and pattern recognition as it appears widely in the industry\nespecially in sensor and IoT related research and applications, but few\nadvances has been achieved in effective time series visual analytics and\ninteraction due to its temporal dimensionality and complex dynamics. Inspired\nby recent effort on using network metrics to characterize time series for\nclassification, we present an approach to visualize time series as complex\nnetworks based on the first order Markov process in its temporal ordering. In\ncontrast to the classical bar charts, line plots and other statistics based\ngraph, our approach delivers more intuitive visualization that better preserves\nboth the temporal dependency and frequency structures. It provides a natural\ninverse operation to map the graph back to raw signals, making it possible to\nuse graph statistics to characterize time series for better visual exploration\nand statistical analysis. Our experimental results suggest the effectiveness on\nvarious tasks such as pattern discovery and classification on both synthetic\nand the real time series and sensor data.\n", "versions": [{"version": "v1", "created": "Mon, 24 Oct 2016 03:39:35 GMT"}, {"version": "v2", "created": "Mon, 7 Nov 2016 00:05:25 GMT"}, {"version": "v3", "created": "Thu, 1 Dec 2016 19:52:10 GMT"}, {"version": "v4", "created": "Tue, 14 Aug 2018 16:54:22 GMT"}], "update_date": "2018-08-15", "authors_parsed": [["Liu", "Lu", ""], ["Wang", "Zhiguang", ""]]}, {"id": "1610.07379", "submitter": "Jonathan Scarlett", "authors": "Ilija Bogunovic and Jonathan Scarlett and Andreas Krause and Volkan\n  Cevher", "title": "Truncated Variance Reduction: A Unified Approach to Bayesian\n  Optimization and Level-Set Estimation", "comments": "Accepted to NIPS 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.IT cs.LG math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a new algorithm, truncated variance reduction (TruVaR), that\ntreats Bayesian optimization (BO) and level-set estimation (LSE) with Gaussian\nprocesses in a unified fashion. The algorithm greedily shrinks a sum of\ntruncated variances within a set of potential maximizers (BO) or unclassified\npoints (LSE), which is updated based on confidence bounds. TruVaR is effective\nin several important settings that are typically non-trivial to incorporate\ninto myopic algorithms, including pointwise costs and heteroscedastic noise. We\nprovide a general theoretical guarantee for TruVaR covering these aspects, and\nuse it to recover and strengthen existing results on BO and LSE. Moreover, we\nprovide a new result for a setting where one can select from a number of noise\nlevels having associated costs. We demonstrate the effectiveness of the\nalgorithm on both synthetic and real-world data sets.\n", "versions": [{"version": "v1", "created": "Mon, 24 Oct 2016 12:18:00 GMT"}], "update_date": "2016-10-25", "authors_parsed": [["Bogunovic", "Ilija", ""], ["Scarlett", "Jonathan", ""], ["Krause", "Andreas", ""], ["Cevher", "Volkan", ""]]}, {"id": "1610.07419", "submitter": "Bruno Ordozgoiti", "authors": "Udi Margolin, Alberto Mozo, Bruno Ordozgoiti, Danny Raz, Elisha\n  Rosensweig, Itai Segall", "title": "Using Machine Learning to Detect Noisy Neighbors in 5G Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  5G networks are expected to be more dynamic and chaotic in their structure\nthan current networks. With the advent of Network Function Virtualization\n(NFV), Network Functions (NF) will no longer be tightly coupled with the\nhardware they are running on, which poses new challenges in network management.\nNoisy neighbor is a term commonly used to describe situations in NFV\ninfrastructure where an application experiences degradation in performance due\nto the fact that some of the resources it needs are occupied by other\napplications in the same cloud node. These situations cannot be easily\nidentified using straightforward approaches, which calls for the use of\nsophisticated methods for NFV infrastructure management. In this paper we\ndemonstrate how Machine Learning (ML) techniques can be used to identify such\nevents. Through experiments using data collected at real NFV infrastructure, we\nshow that standard models for automated classification can detect the noisy\nneighbor phenomenon with an accuracy of more than 90% in a simple scenario.\n", "versions": [{"version": "v1", "created": "Mon, 24 Oct 2016 14:07:56 GMT"}], "update_date": "2016-10-25", "authors_parsed": [["Margolin", "Udi", ""], ["Mozo", "Alberto", ""], ["Ordozgoiti", "Bruno", ""], ["Raz", "Danny", ""], ["Rosensweig", "Elisha", ""], ["Segall", "Itai", ""]]}, {"id": "1610.07448", "submitter": "Simone Scardapane", "authors": "Simone Scardapane and Paolo Di Lorenzo", "title": "A Framework for Parallel and Distributed Training of Neural Networks", "comments": "Published on Neural Networks (Elsevier), in press", "journal-ref": null, "doi": "10.1016/j.neunet.2017.04.004", "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The aim of this paper is to develop a general framework for training neural\nnetworks (NNs) in a distributed environment, where training data is partitioned\nover a set of agents that communicate with each other through a sparse,\npossibly time-varying, connectivity pattern. In such distributed scenario, the\ntraining problem can be formulated as the (regularized) optimization of a\nnon-convex social cost function, given by the sum of local (non-convex) costs,\nwhere each agent contributes with a single error term defined with respect to\nits local dataset. To devise a flexible and efficient solution, we customize a\nrecently proposed framework for non-convex optimization over networks, which\nhinges on a (primal) convexification-decomposition technique to handle\nnon-convexity, and a dynamic consensus procedure to diffuse information among\nthe agents. Several typical choices for the training criterion (e.g., squared\nloss, cross entropy, etc.) and regularization (e.g., $\\ell_2$ norm, sparsity\ninducing penalties, etc.) are included in the framework and explored along the\npaper. Convergence to a stationary solution of the social non-convex problem is\nguaranteed under mild assumptions. Additionally, we show a principled way\nallowing each agent to exploit a possible multi-core architecture (e.g., a\nlocal cloud) in order to parallelize its local optimization step, resulting in\nstrategies that are both distributed (across the agents) and parallel (inside\neach agent) in nature. A comprehensive set of experimental results validate the\nproposed approach.\n", "versions": [{"version": "v1", "created": "Mon, 24 Oct 2016 14:58:56 GMT"}, {"version": "v2", "created": "Mon, 30 Jan 2017 11:00:58 GMT"}, {"version": "v3", "created": "Thu, 20 Apr 2017 08:55:19 GMT"}], "update_date": "2017-04-21", "authors_parsed": [["Scardapane", "Simone", ""], ["Di Lorenzo", "Paolo", ""]]}, {"id": "1610.07519", "submitter": "Emilie Chouzenoux", "authors": "Yosra Marnissi, Yuling Zheng, Emilie Chouzenoux, Jean-Christophe\n  Pesquet", "title": "A Variational Bayesian Approach for Image Restoration. Application to\n  Image Deblurring with Poisson-Gaussian Noise", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, a methodology is investigated for signal recovery in the\npresence of non-Gaussian noise. In contrast with regularized minimization\napproaches often adopted in the literature, in our algorithm the regularization\nparameter is reliably estimated from the observations. As the posterior density\nof the unknown parameters is analytically intractable, the estimation problem\nis derived in a variational Bayesian framework where the goal is to provide a\ngood approximation to the posterior distribution in order to compute posterior\nmean estimates. Moreover, a majorization technique is employed to circumvent\nthe difficulties raised by the intricate forms of the non-Gaussian likelihood\nand of the prior density. We demonstrate the potential of the proposed approach\nthrough comparisons with state-of-the-art techniques that are specifically\ntailored to signal recovery in the presence of mixed Poisson-Gaussian noise.\nResults show that the proposed approach is efficient and achieves performance\ncomparable with other methods where the regularization parameter is manually\ntuned from the ground truth.\n", "versions": [{"version": "v1", "created": "Mon, 24 Oct 2016 18:10:11 GMT"}, {"version": "v2", "created": "Fri, 20 Jan 2017 15:20:57 GMT"}], "update_date": "2017-01-23", "authors_parsed": [["Marnissi", "Yosra", ""], ["Zheng", "Yuling", ""], ["Chouzenoux", "Emilie", ""], ["Pesquet", "Jean-Christophe", ""]]}, {"id": "1610.07520", "submitter": "Felipe Pinheiro", "authors": "Felipe C. Pinheiro, Cassio G. Lopes", "title": "Nonlinear Adaptive Algorithms on Rank-One Tensor Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SY cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work proposes a low complexity nonlinearity model and develops adaptive\nalgorithms over it. The model is based on the decomposable---or rank-one, in\ntensor language---Volterra kernels. It may also be described as a product of\nFIR filters, which explains its low-complexity. The rank-one model is also\ninteresting because it comes from a well-posed problem in approximation theory.\nThe paper uses such model in an estimation theory context to develop an exact\ngradient-type algorithm, from which adaptive algorithms such as the least mean\nsquares (LMS) filter and its data-reuse version---the TRUE-LMS---are derived.\nStability and convergence issues are addressed. The algorithms are then tested\nin simulations, which show its good performance when compared to other\nnonlinear processing algorithms in the literature.\n", "versions": [{"version": "v1", "created": "Mon, 24 Oct 2016 18:12:18 GMT"}], "update_date": "2016-10-25", "authors_parsed": [["Pinheiro", "Felipe C.", ""], ["Lopes", "Cassio G.", ""]]}, {"id": "1610.07563", "submitter": "Jinbo Bi", "authors": "Xin Wang, Jinbo Bi, Shipeng Yu, Jiangwen Sun", "title": "On Multiplicative Multitask Feature Learning", "comments": "Advances in Neural Information Processing Systems 2014", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate a general framework of multiplicative multitask feature\nlearning which decomposes each task's model parameters into a multiplication of\ntwo components. One of the components is used across all tasks and the other\ncomponent is task-specific. Several previous methods have been proposed as\nspecial cases of our framework. We study the theoretical properties of this\nframework when different regularization conditions are applied to the two\ndecomposed components. We prove that this framework is mathematically\nequivalent to the widely used multitask feature learning methods that are based\non a joint regularization of all model parameters, but with a more general form\nof regularizers. Further, an analytical formula is derived for the across-task\ncomponent as related to the task-specific component for all these regularizers,\nleading to a better understanding of the shrinkage effect. Study of this\nframework motivates new multitask learning algorithms. We propose two new\nlearning formulations by varying the parameters in the proposed framework.\nEmpirical studies have revealed the relative advantages of the two new\nformulations by comparing with the state of the art, which provides instructive\ninsights into the feature learning problem with multiple tasks.\n", "versions": [{"version": "v1", "created": "Mon, 24 Oct 2016 19:27:52 GMT"}], "update_date": "2016-10-25", "authors_parsed": [["Wang", "Xin", ""], ["Bi", "Jinbo", ""], ["Yu", "Shipeng", ""], ["Sun", "Jiangwen", ""]]}, {"id": "1610.07569", "submitter": "Jiaqi Mu Jiaqi Mu", "authors": "Jiaqi Mu, Suma Bhat, Pramod Viswanath", "title": "Geometry of Polysemy", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Vector representations of words have heralded a transformational approach to\nclassical problems in NLP; the most popular example is word2vec. However, a\nsingle vector does not suffice to model the polysemous nature of many\n(frequent) words, i.e., words with multiple meanings. In this paper, we propose\na three-fold approach for unsupervised polysemy modeling: (a) context\nrepresentations, (b) sense induction and disambiguation and (c) lexeme (as a\nword and sense pair) representations. A key feature of our work is the finding\nthat a sentence containing a target word is well represented by a low rank\nsubspace, instead of a point in a vector space. We then show that the subspaces\nassociated with a particular sense of the target word tend to intersect over a\nline (one-dimensional subspace), which we use to disambiguate senses using a\nclustering algorithm that harnesses the Grassmannian geometry of the\nrepresentations. The disambiguation algorithm, which we call $K$-Grassmeans,\nleads to a procedure to label the different senses of the target word in the\ncorpus -- yielding lexeme vector representations, all in an unsupervised manner\nstarting from a large (Wikipedia) corpus in English. Apart from several\nprototypical target (word,sense) examples and a host of empirical studies to\nintuit and justify the various geometric representations, we validate our\nalgorithms on standard sense induction and disambiguation datasets and present\nnew state-of-the-art results.\n", "versions": [{"version": "v1", "created": "Mon, 24 Oct 2016 19:35:29 GMT"}], "update_date": "2016-10-25", "authors_parsed": [["Mu", "Jiaqi", ""], ["Bhat", "Suma", ""], ["Viswanath", "Pramod", ""]]}, {"id": "1610.07584", "submitter": "Jiajun Wu", "authors": "Jiajun Wu, Chengkai Zhang, Tianfan Xue, William T. Freeman, Joshua B.\n  Tenenbaum", "title": "Learning a Probabilistic Latent Space of Object Shapes via 3D\n  Generative-Adversarial Modeling", "comments": "NIPS 2016. The first two authors contributed equally to this work", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of 3D object generation. We propose a novel framework,\nnamely 3D Generative Adversarial Network (3D-GAN), which generates 3D objects\nfrom a probabilistic space by leveraging recent advances in volumetric\nconvolutional networks and generative adversarial nets. The benefits of our\nmodel are three-fold: first, the use of an adversarial criterion, instead of\ntraditional heuristic criteria, enables the generator to capture object\nstructure implicitly and to synthesize high-quality 3D objects; second, the\ngenerator establishes a mapping from a low-dimensional probabilistic space to\nthe space of 3D objects, so that we can sample objects without a reference\nimage or CAD models, and explore the 3D object manifold; third, the adversarial\ndiscriminator provides a powerful 3D shape descriptor which, learned without\nsupervision, has wide applications in 3D object recognition. Experiments\ndemonstrate that our method generates high-quality 3D objects, and our\nunsupervisedly learned features achieve impressive performance on 3D object\nrecognition, comparable with those of supervised learning methods.\n", "versions": [{"version": "v1", "created": "Mon, 24 Oct 2016 19:53:41 GMT"}, {"version": "v2", "created": "Wed, 4 Jan 2017 18:35:52 GMT"}], "update_date": "2017-01-05", "authors_parsed": [["Wu", "Jiajun", ""], ["Zhang", "Chengkai", ""], ["Xue", "Tianfan", ""], ["Freeman", "William T.", ""], ["Tenenbaum", "Joshua B.", ""]]}, {"id": "1610.07629", "submitter": "Jonathon Shlens", "authors": "Vincent Dumoulin, Jonathon Shlens, Manjunath Kudlur", "title": "A Learned Representation For Artistic Style", "comments": "9 pages. 15 pages of Appendix, International Conference on Learning\n  Representations (ICLR) 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The diversity of painting styles represents a rich visual vocabulary for the\nconstruction of an image. The degree to which one may learn and parsimoniously\ncapture this visual vocabulary measures our understanding of the higher level\nfeatures of paintings, if not images in general. In this work we investigate\nthe construction of a single, scalable deep network that can parsimoniously\ncapture the artistic style of a diversity of paintings. We demonstrate that\nsuch a network generalizes across a diversity of artistic styles by reducing a\npainting to a point in an embedding space. Importantly, this model permits a\nuser to explore new painting styles by arbitrarily combining the styles learned\nfrom individual paintings. We hope that this work provides a useful step\ntowards building rich models of paintings and offers a window on to the\nstructure of the learned representation of artistic style.\n", "versions": [{"version": "v1", "created": "Mon, 24 Oct 2016 20:06:54 GMT"}, {"version": "v2", "created": "Mon, 5 Dec 2016 16:24:40 GMT"}, {"version": "v3", "created": "Fri, 9 Dec 2016 01:20:17 GMT"}, {"version": "v4", "created": "Tue, 27 Dec 2016 23:05:51 GMT"}, {"version": "v5", "created": "Thu, 9 Feb 2017 16:29:09 GMT"}], "update_date": "2017-02-10", "authors_parsed": [["Dumoulin", "Vincent", ""], ["Shlens", "Jonathon", ""], ["Kudlur", "Manjunath", ""]]}, {"id": "1610.07650", "submitter": "Yining Wang", "authors": "Yining Wang, Yu-Xiang Wang and Aarti Singh", "title": "A Theoretical Analysis of Noisy Sparse Subspace Clustering on\n  Dimensionality-Reduced Data", "comments": "40 pages, 2 figures. A shorter version of this paper titled \"A\n  Deterministic Analysis of Noisy Sparse Subspace Clustering on\n  Dimensionality-Reduced Data\" with partial results appeared at Proceedings of\n  the 32nd International Conference on Machine Learning (ICML) held at Lille,\n  France in 2015", "journal-ref": "IEEE Transactions on Information Theory, 65(2):685-706, 2019", "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Subspace clustering is the problem of partitioning unlabeled data points into\na number of clusters so that data points within one cluster lie approximately\non a low-dimensional linear subspace. In many practical scenarios, the\ndimensionality of data points to be clustered are compressed due to constraints\nof measurement, computation or privacy. In this paper, we study the theoretical\nproperties of a popular subspace clustering algorithm named sparse subspace\nclustering (SSC) and establish formal success conditions of SSC on\ndimensionality-reduced data. Our analysis applies to the most general fully\ndeterministic model where both underlying subspaces and data points within each\nsubspace are deterministically positioned, and also a wide range of\ndimensionality reduction techniques (e.g., Gaussian random projection, uniform\nsubsampling, sketching) that fall into a subspace embedding framework (Meng &\nMahoney, 2013; Avron et al., 2014). Finally, we apply our analysis to a\ndifferentially private SSC algorithm and established both privacy and utility\nguarantees of the proposed method.\n", "versions": [{"version": "v1", "created": "Mon, 24 Oct 2016 20:54:07 GMT"}], "update_date": "2019-01-24", "authors_parsed": [["Wang", "Yining", ""], ["Wang", "Yu-Xiang", ""], ["Singh", "Aarti", ""]]}, {"id": "1610.07667", "submitter": "Nir Rosenfeld", "authors": "Nir Rosenfeld, Yishay Mansour, Elad Yom-Tov", "title": "Predicting Counterfactuals from Large Historical Data and Small\n  Randomized Trials", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When a new treatment is considered for use, whether a pharmaceutical drug or\na search engine ranking algorithm, a typical question that arises is, will its\nperformance exceed that of the current treatment? The conventional way to\nanswer this counterfactual question is to estimate the effect of the new\ntreatment in comparison to that of the conventional treatment by running a\ncontrolled, randomized experiment. While this approach theoretically ensures an\nunbiased estimator, it suffers from several drawbacks, including the difficulty\nin finding representative experimental populations as well as the cost of\nrunning such trials. Moreover, such trials neglect the huge quantities of\navailable control-condition data which are often completely ignored.\n  In this paper we propose a discriminative framework for estimating the\nperformance of a new treatment given a large dataset of the control condition\nand data from a small (and possibly unrepresentative) randomized trial\ncomparing new and old treatments. Our objective, which requires minimal\nassumptions on the treatments, models the relation between the outcomes of the\ndifferent conditions. This allows us to not only estimate mean effects but also\nto generate individual predictions for examples outside the randomized sample.\n  We demonstrate the utility of our approach through experiments in three\nareas: Search engine operation, treatments to diabetes patients, and market\nvalue estimation for houses. Our results demonstrate that our approach can\nreduce the number and size of the currently performed randomized controlled\nexperiments, thus saving significant time, money and effort on the part of\npractitioners.\n", "versions": [{"version": "v1", "created": "Mon, 24 Oct 2016 22:12:52 GMT"}, {"version": "v2", "created": "Wed, 26 Oct 2016 06:11:07 GMT"}], "update_date": "2016-10-27", "authors_parsed": [["Rosenfeld", "Nir", ""], ["Mansour", "Yishay", ""], ["Yom-Tov", "Elad", ""]]}, {"id": "1610.07675", "submitter": "Kamil Rocki", "authors": "Kamil Rocki, Tomasz Kornuta, Tegan Maharaj", "title": "Surprisal-Driven Zoneout", "comments": "Published at the Continual Learning and Deep Networks Workshop; NIPS\n  2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel method of regularization for recurrent neural networks\ncalled suprisal-driven zoneout. In this method, states zoneout (maintain their\nprevious value rather than updating), when the suprisal (discrepancy between\nthe last state's prediction and target) is small. Thus regularization is\nadaptive and input-driven on a per-neuron basis. We demonstrate the\neffectiveness of this idea by achieving state-of-the-art bits per character of\n1.31 on the Hutter Prize Wikipedia dataset, significantly reducing the gap to\nthe best known highly-engineered compression methods.\n", "versions": [{"version": "v1", "created": "Mon, 24 Oct 2016 22:38:52 GMT"}, {"version": "v2", "created": "Fri, 28 Oct 2016 19:55:16 GMT"}, {"version": "v3", "created": "Mon, 31 Oct 2016 15:18:11 GMT"}, {"version": "v4", "created": "Thu, 3 Nov 2016 17:09:23 GMT"}, {"version": "v5", "created": "Thu, 24 Nov 2016 06:40:26 GMT"}, {"version": "v6", "created": "Tue, 13 Dec 2016 23:32:24 GMT"}], "update_date": "2016-12-15", "authors_parsed": [["Rocki", "Kamil", ""], ["Kornuta", "Tomasz", ""], ["Maharaj", "Tegan", ""]]}, {"id": "1610.07677", "submitter": "Edward Yu", "authors": "Edward Yu, Parth Parekh", "title": "A Bayesian Ensemble for Unsupervised Anomaly Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Methods for unsupervised anomaly detection suffer from the fact that the data\nis unlabeled, making it difficult to assess the optimality of detection\nalgorithms. Ensemble learning has shown exceptional results in classification\nand clustering problems, but has not seen as much research in the context of\noutlier detection. Existing methods focus on combining output scores of\nindividual detectors, but this leads to outputs that are not easily\ninterpretable. In this paper, we introduce a theoretical foundation for\ncombining individual detectors with Bayesian classifier combination. Not only\nare posterior distributions easily interpreted as the probability distribution\nof anomalies, but bias, variance, and individual error rates of detectors are\nall easily obtained. Performance on real-world datasets shows high accuracy\nacross varied types of time series data.\n", "versions": [{"version": "v1", "created": "Mon, 24 Oct 2016 23:07:16 GMT"}], "update_date": "2016-10-26", "authors_parsed": [["Yu", "Edward", ""], ["Parekh", "Parth", ""]]}, {"id": "1610.07686", "submitter": "Youssef  Mroueh", "authors": "Youssef Mroueh, Etienne Marcheret, Vaibhava Goel", "title": "Co-Occuring Directions Sketching for Approximate Matrix Multiply", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce co-occurring directions sketching, a deterministic algorithm for\napproximate matrix product (AMM), in the streaming model. We show that\nco-occuring directions achieves a better error bound for AMM than other\nrandomized and deterministic approaches for AMM. Co-occurring directions gives\na $1 + \\epsilon$ -approximation of the optimal low rank approximation of a\nmatrix product. Empirically our algorithm outperforms competing methods for\nAMM, for a small sketch size. We validate empirically our theoretical findings\nand algorithms\n", "versions": [{"version": "v1", "created": "Tue, 25 Oct 2016 00:01:33 GMT"}], "update_date": "2016-10-26", "authors_parsed": [["Mroueh", "Youssef", ""], ["Marcheret", "Etienne", ""], ["Goel", "Vaibhava", ""]]}, {"id": "1610.07717", "submitter": "Andreas W. Kempa-Liehr", "authors": "Maximilian Christ, Andreas W. Kempa-Liehr, Michael Feindt", "title": "Distributed and parallel time series feature extraction for industrial\n  big data applications", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The all-relevant problem of feature selection is the identification of all\nstrongly and weakly relevant attributes. This problem is especially hard to\nsolve for time series classification and regression in industrial applications\nsuch as predictive maintenance or production line optimization, for which each\nlabel or regression target is associated with several time series and\nmeta-information simultaneously. Here, we are proposing an efficient, scalable\nfeature extraction algorithm for time series, which filters the available\nfeatures in an early stage of the machine learning pipeline with respect to\ntheir significance for the classification or regression task, while controlling\nthe expected percentage of selected but irrelevant features. The proposed\nalgorithm combines established feature extraction methods with a feature\nimportance filter. It has a low computational complexity, allows to start on a\nproblem with only limited domain knowledge available, can be trivially\nparallelized, is highly scalable and based on well studied non-parametric\nhypothesis tests. We benchmark our proposed algorithm on all binary\nclassification problems of the UCR time series classification archive as well\nas time series from a production line optimization project and simulated\nstochastic processes with underlying qualitative change of dynamics.\n", "versions": [{"version": "v1", "created": "Tue, 25 Oct 2016 03:31:58 GMT"}, {"version": "v2", "created": "Fri, 16 Dec 2016 02:07:07 GMT"}, {"version": "v3", "created": "Fri, 19 May 2017 21:20:18 GMT"}], "update_date": "2017-05-23", "authors_parsed": [["Christ", "Maximilian", ""], ["Kempa-Liehr", "Andreas W.", ""], ["Feindt", "Michael", ""]]}, {"id": "1610.07722", "submitter": "Ioakeim Perros", "authors": "Ioakeim Perros and Robert Chen and Richard Vuduc and Jimeng Sun", "title": "Sparse Hierarchical Tucker Factorization and its Application to\n  Healthcare", "comments": "This is an extended version of a paper presented at the 15th IEEE\n  International Conference on Data Mining (ICDM 2015)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new tensor factorization method, called the Sparse\nHierarchical-Tucker (Sparse H-Tucker), for sparse and high-order data tensors.\nSparse H-Tucker is inspired by its namesake, the classical Hierarchical Tucker\nmethod, which aims to compute a tree-structured factorization of an input data\nset that may be readily interpreted by a domain expert. However, Sparse\nH-Tucker uses a nested sampling technique to overcome a key scalability problem\nin Hierarchical Tucker, which is the creation of an unwieldy intermediate dense\ncore tensor; the result of our approach is a faster, more space-efficient, and\nmore accurate method. We extensively test our method on a real healthcare\ndataset, which is collected from 30K patients and results in an 18th order\nsparse data tensor. Unlike competing methods, Sparse H-Tucker can analyze the\nfull data set on a single multi-threaded machine. It can also do so more\naccurately and in less time than the state-of-the-art: on a 12th order subset\nof the input data, Sparse H-Tucker is 18x more accurate and 7.5x faster than a\npreviously state-of-the-art method. Even for analyzing low order tensors (e.g.,\n4-order), our method requires close to an order of magnitude less time and over\ntwo orders of magnitude less memory, as compared to traditional tensor\nfactorization methods such as CP and Tucker. Moreover, we observe that Sparse\nH-Tucker scales nearly linearly in the number of non-zero tensor elements. The\nresulting model also provides an interpretable disease hierarchy, which is\nconfirmed by a clinical expert.\n", "versions": [{"version": "v1", "created": "Tue, 25 Oct 2016 04:08:11 GMT"}], "update_date": "2016-10-26", "authors_parsed": [["Perros", "Ioakeim", ""], ["Chen", "Robert", ""], ["Vuduc", "Richard", ""], ["Sun", "Jimeng", ""]]}, {"id": "1610.07733", "submitter": "Yoshiyuki Kabashima", "authors": "Yoshiyuki Kabashima, Tomoyuki Obuchi, Makoto Uemura", "title": "Approximate cross-validation formula for Bayesian linear regression", "comments": "5 pages, 2 figures, invited paper for Allerton2016 conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cross-validation (CV) is a technique for evaluating the ability of\nstatistical models/learning systems based on a given data set. Despite its wide\napplicability, the rather heavy computational cost can prevent its use as the\nsystem size grows. To resolve this difficulty in the case of Bayesian linear\nregression, we develop a formula for evaluating the leave-one-out CV error\napproximately without actually performing CV. The usefulness of the developed\nformula is tested by statistical mechanical analysis for a synthetic model.\nThis is confirmed by application to a real-world supernova data set as well.\n", "versions": [{"version": "v1", "created": "Tue, 25 Oct 2016 05:10:49 GMT"}], "update_date": "2016-10-26", "authors_parsed": [["Kabashima", "Yoshiyuki", ""], ["Obuchi", "Tomoyuki", ""], ["Uemura", "Makoto", ""]]}, {"id": "1610.07752", "submitter": "Mrutyunjaya Panda", "authors": "Mrutyunjaya Panda", "title": "Big Models for Big Data using Multi objective averaged one dependence\n  estimators", "comments": "21 pages, 2 Figures, 10 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Even though, many researchers tried to explore the various possibilities on\nmulti objective feature selection, still it is yet to be explored with best of\nits capabilities in data mining applications rather than going for developing\nnew ones. In this paper, multi-objective evolutionary algorithm ENORA is used\nto select the features in a multi-class classification problem. The fusion of\nAnDE (averaged n-dependence estimators) with n=1, a variant of naive Bayes with\nefficient feature selection by ENORA is performed in order to obtain a fast\nhybrid classifier which can effectively learn from big data. This method aims\nat solving the problem of finding optimal feature subset from full data which\nat present still remains to be a difficult problem. The efficacy of the\nobtained classifier is extensively evaluated with a range of most popular 21\nreal world dataset, ranging from small to big. The results obtained are\nencouraging in terms of time, Root mean square error, zero-one loss and\nclassification accuracy.\n", "versions": [{"version": "v1", "created": "Tue, 25 Oct 2016 07:11:11 GMT"}], "update_date": "2016-10-26", "authors_parsed": [["Panda", "Mrutyunjaya", ""]]}, {"id": "1610.07797", "submitter": "Simon Lacoste-Julien", "authors": "Gauthier Gidel, Tony Jebara and Simon Lacoste-Julien", "title": "Frank-Wolfe Algorithms for Saddle Point Problems", "comments": "Appears in: Proceedings of the 20th International Conference on\n  Artificial Intelligence and Statistics (AISTATS 2017). 39 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We extend the Frank-Wolfe (FW) optimization algorithm to solve constrained\nsmooth convex-concave saddle point (SP) problems. Remarkably, the method only\nrequires access to linear minimization oracles. Leveraging recent advances in\nFW optimization, we provide the first proof of convergence of a FW-type saddle\npoint solver over polytopes, thereby partially answering a 30 year-old\nconjecture. We also survey other convergence results and highlight gaps in the\ntheoretical underpinnings of FW-style algorithms. Motivating applications\nwithout known efficient alternatives are explored through structured prediction\nwith combinatorial penalties as well as games over matching polytopes involving\nan exponential number of constraints.\n", "versions": [{"version": "v1", "created": "Tue, 25 Oct 2016 09:14:40 GMT"}, {"version": "v2", "created": "Thu, 2 Mar 2017 10:34:48 GMT"}, {"version": "v3", "created": "Fri, 3 Mar 2017 21:34:24 GMT"}], "update_date": "2017-03-07", "authors_parsed": [["Gidel", "Gauthier", ""], ["Jebara", "Tony", ""], ["Lacoste-Julien", "Simon", ""]]}, {"id": "1610.07857", "submitter": "Yevgeniy Bodyanskiy", "authors": "Yevgeniy Bodyanskiy, Olena Vynokurova, Volodymyr Savvo, Tatiana\n  Tverdokhlib, Pavlo Mulesa", "title": "Hybrid clustering-classification neural network in the medical\n  diagnostics of reactive arthritis", "comments": null, "journal-ref": "International Journal of Intelligent Systems and Applications,\n  2016, Vol. 8, No. 8, pp.1-9", "doi": null, "report-no": null, "categories": "cs.LG cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The hybrid clustering-classification neural network is proposed. This network\nallows increasing a quality of information processing under the condition of\noverlapping classes due to the rational choice of a learning rate parameter and\nintroducing a special procedure of fuzzy reasoning in the clustering process,\nwhich occurs both with an external learning signal (supervised) and without the\none (unsupervised). As similarity measure neighborhood function or membership\none, cosine structures are used, which allow to provide a high flexibility due\nto self-learning-learning process and to provide some new useful properties.\nMany realized experiments have confirmed the efficiency of proposed hybrid\nclustering-classification neural network; also, this network was used for\nsolving diagnostics task of reactive arthritis.\n", "versions": [{"version": "v1", "created": "Fri, 21 Oct 2016 09:11:53 GMT"}], "update_date": "2016-10-26", "authors_parsed": [["Bodyanskiy", "Yevgeniy", ""], ["Vynokurova", "Olena", ""], ["Savvo", "Volodymyr", ""], ["Tverdokhlib", "Tatiana", ""], ["Mulesa", "Pavlo", ""]]}, {"id": "1610.07883", "submitter": "Borja Balle", "authors": "Borja Balle and Mehryar Mohri", "title": "Generalization Bounds for Weighted Automata", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.FL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper studies the problem of learning weighted automata from a finite\nlabeled training sample. We consider several general families of weighted\nautomata defined in terms of three different measures: the norm of an\nautomaton's weights, the norm of the function computed by an automaton, or the\nnorm of the corresponding Hankel matrix. We present new data-dependent\ngeneralization guarantees for learning weighted automata expressed in terms of\nthe Rademacher complexity of these families. We further present upper bounds on\nthese Rademacher complexities, which reveal key new data-dependent terms\nrelated to the complexity of learning weighted automata.\n", "versions": [{"version": "v1", "created": "Tue, 25 Oct 2016 14:10:11 GMT"}], "update_date": "2016-10-26", "authors_parsed": [["Balle", "Borja", ""], ["Mohri", "Mehryar", ""]]}, {"id": "1610.08077", "submitter": "James Johndrow", "authors": "Kristian Lum and James Johndrow", "title": "A statistical framework for fair predictive algorithms", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Predictive modeling is increasingly being employed to assist human\ndecision-makers. One purported advantage of replacing human judgment with\ncomputer models in high stakes settings-- such as sentencing, hiring, policing,\ncollege admissions, and parole decisions-- is the perceived \"neutrality\" of\ncomputers. It is argued that because computer models do not hold personal\nprejudice, the predictions they produce will be equally free from prejudice.\nThere is growing recognition that employing algorithms does not remove the\npotential for bias, and can even amplify it, since training data were\ninevitably generated by a process that is itself biased. In this paper, we\nprovide a probabilistic definition of algorithmic bias. We propose a method to\nremove bias from predictive models by removing all information regarding\nprotected variables from the permitted training data. Unlike previous work in\nthis area, our framework is general enough to accommodate arbitrary data types,\ne.g. binary, continuous, etc. Motivated by models currently in use in the\ncriminal justice system that inform decisions on pre-trial release and\nparoling, we apply our proposed method to a dataset on the criminal histories\nof individuals at the time of sentencing to produce \"race-neutral\" predictions\nof re-arrest. In the process, we demonstrate that the most common approach to\ncreating \"race-neutral\" models-- omitting race as a covariate-- still results\nin racially disparate predictions. We then demonstrate that the application of\nour proposed method to these data removes racial disparities from predictions\nwith minimal impact on predictive accuracy.\n", "versions": [{"version": "v1", "created": "Tue, 25 Oct 2016 20:18:24 GMT"}], "update_date": "2016-10-27", "authors_parsed": [["Lum", "Kristian", ""], ["Johndrow", "James", ""]]}, {"id": "1610.08120", "submitter": "Suchet Bargoti", "authors": "Suchet Bargoti, James Underwood", "title": "Image Segmentation for Fruit Detection and Yield Estimation in Apple\n  Orchards", "comments": "This paper is the initial version of the manuscript submitted to The\n  Journal of Field Robotics in May 2016. Following reviews and revisions, the\n  paper has been accepted for publication. The reviewed version includes\n  extended comparison between the different classification frameworks and a\n  more in-depth literature review", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Ground vehicles equipped with monocular vision systems are a valuable source\nof high resolution image data for precision agriculture applications in\norchards. This paper presents an image processing framework for fruit detection\nand counting using orchard image data. A general purpose image segmentation\napproach is used, including two feature learning algorithms; multi-scale\nMulti-Layered Perceptrons (MLP) and Convolutional Neural Networks (CNN). These\nnetworks were extended by including contextual information about how the image\ndata was captured (metadata), which correlates with some of the appearance\nvariations and/or class distributions observed in the data. The pixel-wise\nfruit segmentation output is processed using the Watershed Segmentation (WS)\nand Circular Hough Transform (CHT) algorithms to detect and count individual\nfruits. Experiments were conducted in a commercial apple orchard near\nMelbourne, Australia. The results show an improvement in fruit segmentation\nperformance with the inclusion of metadata on the previously benchmarked MLP\nnetwork. We extend this work with CNNs, bringing agrovision closer to the\nstate-of-the-art in computer vision, where although metadata had negligible\ninfluence, the best pixel-wise F1-score of $0.791$ was achieved. The WS\nalgorithm produced the best apple detection and counting results, with a\ndetection F1-score of $0.858$. As a final step, image fruit counts were\naccumulated over multiple rows at the orchard and compared against the\npost-harvest fruit counts that were obtained from a grading and counting\nmachine. The count estimates using CNN and WS resulted in the best performance\nfor this dataset, with a squared correlation coefficient of $r^2=0.826$.\n", "versions": [{"version": "v1", "created": "Tue, 25 Oct 2016 23:38:02 GMT"}], "update_date": "2016-10-27", "authors_parsed": [["Bargoti", "Suchet", ""], ["Underwood", "James", ""]]}, {"id": "1610.08123", "submitter": "Paroma Varma", "authors": "Paroma Varma, Bryan He, Dan Iter, Peng Xu, Rose Yu, Christopher De Sa,\n  Christopher R\\'e", "title": "Socratic Learning: Augmenting Generative Models to Incorporate Latent\n  Subsets in Training Data", "comments": "4 figures; 18 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A challenge in training discriminative models like neural networks is\nobtaining enough labeled training data. Recent approaches use generative models\nto combine weak supervision sources, like user-defined heuristics or knowledge\nbases, to label training data. Prior work has explored learning accuracies for\nthese sources even without ground truth labels, but they assume that a single\naccuracy parameter is sufficient to model the behavior of these sources over\nthe entire training set. In particular, they fail to model latent subsets in\nthe training data in which the supervision sources perform differently than on\naverage. We present Socratic learning, a paradigm that uses feedback from a\ncorresponding discriminative model to automatically identify these subsets and\naugments the structure of the generative model accordingly. Experimentally, we\nshow that without any ground truth labels, the augmented generative model\nreduces error by up to 56.06% for a relation extraction task compared to a\nstate-of-the-art weak supervision technique that utilizes generative models.\n", "versions": [{"version": "v1", "created": "Tue, 25 Oct 2016 23:43:49 GMT"}, {"version": "v2", "created": "Wed, 9 Nov 2016 08:00:06 GMT"}, {"version": "v3", "created": "Fri, 3 Mar 2017 23:33:52 GMT"}, {"version": "v4", "created": "Thu, 28 Sep 2017 07:40:29 GMT"}], "update_date": "2017-09-29", "authors_parsed": [["Varma", "Paroma", ""], ["He", "Bryan", ""], ["Iter", "Dan", ""], ["Xu", "Peng", ""], ["Yu", "Rose", ""], ["De Sa", "Christopher", ""], ["R\u00e9", "Christopher", ""]]}, {"id": "1610.08127", "submitter": "Thomas Brouwer", "authors": "Thomas Brouwer, Jes Frellsen, Pietro Lio'", "title": "Fast Bayesian Non-Negative Matrix Factorisation and Tri-Factorisation", "comments": "NIPS 2016 Workshop on Advances in Approximate Bayesian Inference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.NA stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a fast variational Bayesian algorithm for performing non-negative\nmatrix factorisation and tri-factorisation. We show that our approach achieves\nfaster convergence per iteration and timestep (wall-clock) than Gibbs sampling\nand non-probabilistic approaches, and do not require additional samples to\nestimate the posterior. We show that in particular for matrix tri-factorisation\nconvergence is difficult, but our variational Bayesian approach offers a fast\nsolution, allowing the tri-factorisation approach to be used more effectively.\n", "versions": [{"version": "v1", "created": "Wed, 26 Oct 2016 00:10:44 GMT"}], "update_date": "2016-10-28", "authors_parsed": [["Brouwer", "Thomas", ""], ["Frellsen", "Jes", ""], ["Lio'", "Pietro", ""]]}, {"id": "1610.08166", "submitter": "Yossi Adi", "authors": "Yossi Adi, Joseph Keshet, Emily Cibelli, Erin Gustafson, Cynthia\n  Clopper, Matthew Goldrick", "title": "Automatic measurement of vowel duration via structured prediction", "comments": null, "journal-ref": null, "doi": "10.1121/1.4972527", "report-no": null, "categories": "stat.ML cs.LG cs.SD", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A key barrier to making phonetic studies scalable and replicable is the need\nto rely on subjective, manual annotation. To help meet this challenge, a\nmachine learning algorithm was developed for automatic measurement of a widely\nused phonetic measure: vowel duration. Manually-annotated data were used to\ntrain a model that takes as input an arbitrary length segment of the acoustic\nsignal containing a single vowel that is preceded and followed by consonants\nand outputs the duration of the vowel. The model is based on the structured\nprediction framework. The input signal and a hypothesized set of a vowel's\nonset and offset are mapped to an abstract vector space by a set of acoustic\nfeature functions. The learning algorithm is trained in this space to minimize\nthe difference in expectations between predicted and manually-measured vowel\ndurations. The trained model can then automatically estimate vowel durations\nwithout phonetic or orthographic transcription. Results comparing the model to\nthree sets of manually annotated data suggest it out-performed the current gold\nstandard for duration measurement, an HMM-based forced aligner (which requires\northographic or phonetic transcription as an input).\n", "versions": [{"version": "v1", "created": "Wed, 26 Oct 2016 04:50:35 GMT"}], "update_date": "2017-03-08", "authors_parsed": [["Adi", "Yossi", ""], ["Keshet", "Joseph", ""], ["Cibelli", "Emily", ""], ["Gustafson", "Erin", ""], ["Clopper", "Cynthia", ""], ["Goldrick", "Matthew", ""]]}, {"id": "1610.08229", "submitter": "Amit Mandelbaum", "authors": "Amit Mandelbaum and Adi Shalev", "title": "Word Embeddings and Their Use In Sentence Classification Tasks", "comments": "13 pages, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper have two parts. In the first part we discuss word embeddings. We\ndiscuss the need for them, some of the methods to create them, and some of\ntheir interesting properties. We also compare them to image embeddings and see\nhow word embedding and image embedding can be combined to perform different\ntasks. In the second part we implement a convolutional neural network trained\non top of pre-trained word vectors. The network is used for several\nsentence-level classification tasks, and achieves state-of-art (or comparable)\nresults, demonstrating the great power of pre-trainted word embeddings over\nrandom ones.\n", "versions": [{"version": "v1", "created": "Wed, 26 Oct 2016 08:48:10 GMT"}], "update_date": "2016-10-27", "authors_parsed": [["Mandelbaum", "Amit", ""], ["Shalev", "Adi", ""]]}, {"id": "1610.08239", "submitter": "Daniil Ryabko", "authors": "Daniil Ryabko", "title": "Things Bayes can't do", "comments": null, "journal-ref": "Proceedings of ALT, LNCS 9925, pp.253-260, Bari, Italy, 2016", "doi": "10.1007/978-3-319-46379-7_17", "report-no": null, "categories": "cs.LG math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The problem of forecasting conditional probabilities of the next event given\nthe past is considered in a general probabilistic setting. Given an arbitrary\n(large, uncountable) set C of predictors, we would like to construct a single\npredictor that performs asymptotically as well as the best predictor in C, on\nany data. Here we show that there are sets C for which such predictors exist,\nbut none of them is a Bayesian predictor with a prior concentrated on C. In\nother words, there is a predictor with sublinear regret, but every Bayesian\npredictor must have a linear regret. This negative finding is in sharp contrast\nwith previous results that establish the opposite for the case when one of the\npredictors in $C$ achieves asymptotically vanishing error. In such a case, if\nthere is a predictor that achieves asymptotically vanishing error for any\nmeasure in C, then there is a Bayesian predictor that also has this property,\nand whose prior is concentrated on (a countable subset of) C.\n", "versions": [{"version": "v1", "created": "Wed, 26 Oct 2016 09:13:28 GMT"}, {"version": "v2", "created": "Thu, 27 Oct 2016 12:13:37 GMT"}], "update_date": "2016-10-28", "authors_parsed": [["Ryabko", "Daniil", ""]]}, {"id": "1610.08249", "submitter": "Daniil Ryabko", "authors": "Daniil Ryabko", "title": "Universality of Bayesian mixture predictors", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.IT cs.LG math.IT stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The problem is that of sequential probability forecasting for finite-valued\ntime series. The data is generated by an unknown probability distribution over\nthe space of all one-way infinite sequences. It is known that this measure\nbelongs to a given set C, but the latter is completely arbitrary (uncountably\ninfinite, without any structure given). The performance is measured with\nasymptotic average log loss. In this work it is shown that the minimax\nasymptotic performance is always attainable, and it is attained by a convex\ncombination of a countably many measures from the set C (a Bayesian mixture).\nThis was previously only known for the case when the best achievable asymptotic\nerror is 0. This also contrasts previous results that show that in the\nnon-realizable case all Bayesian mixtures may be suboptimal, while there is a\npredictor that achieves the optimal performance.\n", "versions": [{"version": "v1", "created": "Wed, 26 Oct 2016 09:29:32 GMT"}, {"version": "v2", "created": "Tue, 1 Nov 2016 13:01:03 GMT"}], "update_date": "2016-11-02", "authors_parsed": [["Ryabko", "Daniil", ""]]}, {"id": "1610.08250", "submitter": "Kamal Nayan Reddy Challa", "authors": "Kamal Nayan Reddy Challa, Venkata Sasank Pagolu, Ganapati Panda,\n  Babita Majhi", "title": "An Improved Approach for Prediction of Parkinson's Disease using Machine\n  Learning Techniques", "comments": "Conference Paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Parkinson's disease (PD) is one of the major public health problems in the\nworld. It is a well-known fact that around one million people suffer from\nParkinson's disease in the United States whereas the number of people suffering\nfrom Parkinson's disease worldwide is around 5 million. Thus, it is important\nto predict Parkinson's disease in early stages so that early plan for the\nnecessary treatment can be made. People are mostly familiar with the motor\nsymptoms of Parkinson's disease, however, an increasing amount of research is\nbeing done to predict the Parkinson's disease from non-motor symptoms that\nprecede the motor ones. If an early and reliable prediction is possible then a\npatient can get a proper treatment at the right time. Nonmotor symptoms\nconsidered are Rapid Eye Movement (REM) sleep Behaviour Disorder (RBD) and\nolfactory loss. Developing machine learning models that can help us in\npredicting the disease can play a vital role in early prediction. In this\npaper, we extend a work which used the non-motor features such as RBD and\nolfactory loss. Along with this the extended work also uses important\nbiomarkers. In this paper, we try to model this classifier using different\nmachine learning models that have not been used before. We developed automated\ndiagnostic models using Multilayer Perceptron, BayesNet, Random Forest and\nBoosted Logistic Regression. It has been observed that Boosted Logistic\nRegression provides the best performance with an impressive accuracy of 97.159\n% and the area under the ROC curve was 98.9%. Thus, it is concluded that these\nmodels can be used for early prediction of Parkinson's disease.\n", "versions": [{"version": "v1", "created": "Wed, 26 Oct 2016 09:34:39 GMT"}], "update_date": "2016-10-27", "authors_parsed": [["Challa", "Kamal Nayan Reddy", ""], ["Pagolu", "Venkata Sasank", ""], ["Panda", "Ganapati", ""], ["Majhi", "Babita", ""]]}, {"id": "1610.08251", "submitter": "Vedran Dunjko", "authors": "Vedran Dunjko, Jacob M. Taylor, Hans J. Briegel", "title": "Quantum-enhanced machine learning", "comments": "5+15 pages. This paper builds upon and mostly supersedes\n  arXiv:1507.08482. In addition to results provided in this previous work, here\n  we achieve learning improvements in more general environments, and provide\n  connections to other work in quantum machine learning. Explicit constructions\n  of oracularized environments given in arXiv:1507.08482 are omitted in this\n  version", "journal-ref": "Phys. Rev. Lett. 117, 130501 (2016)", "doi": "10.1103/PhysRevLett.117.130501", "report-no": null, "categories": "quant-ph cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The emerging field of quantum machine learning has the potential to\nsubstantially aid in the problems and scope of artificial intelligence. This is\nonly enhanced by recent successes in the field of classical machine learning.\nIn this work we propose an approach for the systematic treatment of machine\nlearning, from the perspective of quantum information. Our approach is general\nand covers all three main branches of machine learning: supervised,\nunsupervised and reinforcement learning. While quantum improvements in\nsupervised and unsupervised learning have been reported, reinforcement learning\nhas received much less attention. Within our approach, we tackle the problem of\nquantum enhancements in reinforcement learning as well, and propose a\nsystematic scheme for providing improvements. As an example, we show that\nquadratic improvements in learning efficiency, and exponential improvements in\nperformance over limited time periods, can be obtained for a broad class of\nlearning problems.\n", "versions": [{"version": "v1", "created": "Wed, 26 Oct 2016 09:35:11 GMT"}], "update_date": "2016-10-27", "authors_parsed": [["Dunjko", "Vedran", ""], ["Taylor", "Jacob M.", ""], ["Briegel", "Hans J.", ""]]}, {"id": "1610.08401", "submitter": "Seyed-Mohsen Moosavi-Dezfooli", "authors": "Seyed-Mohsen Moosavi-Dezfooli, Alhussein Fawzi, Omar Fawzi, Pascal\n  Frossard", "title": "Universal adversarial perturbations", "comments": "Accepted at IEEE Conference on Computer Vision and Pattern\n  Recognition (CVPR), 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given a state-of-the-art deep neural network classifier, we show the\nexistence of a universal (image-agnostic) and very small perturbation vector\nthat causes natural images to be misclassified with high probability. We\npropose a systematic algorithm for computing universal perturbations, and show\nthat state-of-the-art deep neural networks are highly vulnerable to such\nperturbations, albeit being quasi-imperceptible to the human eye. We further\nempirically analyze these universal perturbations and show, in particular, that\nthey generalize very well across neural networks. The surprising existence of\nuniversal perturbations reveals important geometric correlations among the\nhigh-dimensional decision boundary of classifiers. It further outlines\npotential security breaches with the existence of single directions in the\ninput space that adversaries can possibly exploit to break a classifier on most\nnatural images.\n", "versions": [{"version": "v1", "created": "Wed, 26 Oct 2016 16:30:45 GMT"}, {"version": "v2", "created": "Thu, 17 Nov 2016 07:15:00 GMT"}, {"version": "v3", "created": "Thu, 9 Mar 2017 17:01:25 GMT"}], "update_date": "2017-03-10", "authors_parsed": [["Moosavi-Dezfooli", "Seyed-Mohsen", ""], ["Fawzi", "Alhussein", ""], ["Fawzi", "Omar", ""], ["Frossard", "Pascal", ""]]}, {"id": "1610.08424", "submitter": "Nantas Nardelli", "authors": "A. Bordallo, F. Previtali, N. Nardelli, S. Ramamoorthy", "title": "Counterfactual Reasoning about Intent for Interactive Navigation in\n  Dynamic Environments", "comments": null, "journal-ref": "2015 IEEE/RSJ International Conference on Intelligent Robots and\n  Systems (IROS 2015), pp. 2943-2950", "doi": "10.1109/IROS.2015.7353783", "report-no": null, "categories": "cs.RO cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many modern robotics applications require robots to function autonomously in\ndynamic environments including other decision making agents, such as people or\nother robots. This calls for fast and scalable interactive motion planning.\nThis requires models that take into consideration the other agent's intended\nactions in one's own planning. We present a real-time motion planning framework\nthat brings together a few key components including intention inference by\nreasoning counterfactually about potential motion of the other agents as they\nwork towards different goals. By using a light-weight motion model, we achieve\nefficient iterative planning for fluid motion when avoiding pedestrians, in\nparallel with goal inference for longer range movement prediction. This\ninference framework is coupled with a novel distributed visual tracking method\nthat provides reliable and robust models for the current belief-state of the\nmonitored environment. This combined approach represents a computationally\nefficient alternative to previously studied policy learning methods that often\nrequire significant offline training or calibration and do not yet scale to\ndensely populated environments. We validate this framework with experiments\ninvolving multi-robot and human-robot navigation. We further validate the\ntracker component separately on much larger scale unconstrained pedestrian data\nsets.\n", "versions": [{"version": "v1", "created": "Wed, 26 Oct 2016 17:00:23 GMT"}], "update_date": "2016-10-27", "authors_parsed": [["Bordallo", "A.", ""], ["Previtali", "F.", ""], ["Nardelli", "N.", ""], ["Ramamoorthy", "S.", ""]]}, {"id": "1610.08452", "submitter": "Muhammad Bilal Zafar", "authors": "Muhammad Bilal Zafar, Isabel Valera, Manuel Gomez Rodriguez, Krishna\n  P. Gummadi", "title": "Fairness Beyond Disparate Treatment & Disparate Impact: Learning\n  Classification without Disparate Mistreatment", "comments": "To appear in Proceedings of the 26th International World Wide Web\n  Conference (WWW), 2017. Code available at:\n  https://github.com/mbilalzafar/fair-classification", "journal-ref": null, "doi": "10.1145/3038912.3052660", "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automated data-driven decision making systems are increasingly being used to\nassist, or even replace humans in many settings. These systems function by\nlearning from historical decisions, often taken by humans. In order to maximize\nthe utility of these systems (or, classifiers), their training involves\nminimizing the errors (or, misclassifications) over the given historical data.\nHowever, it is quite possible that the optimally trained classifier makes\ndecisions for people belonging to different social groups with different\nmisclassification rates (e.g., misclassification rates for females are higher\nthan for males), thereby placing these groups at an unfair disadvantage. To\naccount for and avoid such unfairness, in this paper, we introduce a new notion\nof unfairness, disparate mistreatment, which is defined in terms of\nmisclassification rates. We then propose intuitive measures of disparate\nmistreatment for decision boundary-based classifiers, which can be easily\nincorporated into their formulation as convex-concave constraints. Experiments\non synthetic as well as real world datasets show that our methodology is\neffective at avoiding disparate mistreatment, often at a small cost in terms of\naccuracy.\n", "versions": [{"version": "v1", "created": "Wed, 26 Oct 2016 18:34:48 GMT"}, {"version": "v2", "created": "Wed, 8 Mar 2017 19:04:28 GMT"}], "update_date": "2017-03-10", "authors_parsed": [["Zafar", "Muhammad Bilal", ""], ["Valera", "Isabel", ""], ["Rodriguez", "Manuel Gomez", ""], ["Gummadi", "Krishna P.", ""]]}, {"id": "1610.08495", "submitter": "Tiep Vu Tiep Vu", "authors": "Tiep H. Vu, Hojjat S. Mousavi, Vishal Monga", "title": "Adaptive matching pursuit for sparse signal recovery", "comments": "ICASSP", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Spike and Slab priors have been of much recent interest in signal processing\nas a means of inducing sparsity in Bayesian inference. Applications domains\nthat benefit from the use of these priors include sparse recovery, regression\nand classification. It is well-known that solving for the sparse coefficient\nvector to maximize these priors results in a hard non-convex and mixed integer\nprogramming problem. Most existing solutions to this optimization problem\neither involve simplifying assumptions/relaxations or are computationally\nexpensive. We propose a new greedy and adaptive matching pursuit (AMP)\nalgorithm to directly solve this hard problem. Essentially, in each step of the\nalgorithm, the set of active elements would be updated by either adding or\nremoving one index, whichever results in better improvement. In addition, the\nintermediate steps of the algorithm are calculated via an inexpensive Cholesky\ndecomposition which makes the algorithm much faster. Results on simulated data\nsets as well as real-world image recovery challenges confirm the benefits of\nthe proposed AMP, particularly in providing a superior cost-quality trade-off\nover existing alternatives.\n", "versions": [{"version": "v1", "created": "Mon, 12 Sep 2016 17:48:38 GMT"}], "update_date": "2016-10-27", "authors_parsed": [["Vu", "Tiep H.", ""], ["Mousavi", "Hojjat S.", ""], ["Monga", "Vishal", ""]]}, {"id": "1610.08500", "submitter": "Nils Jansen", "authors": "Nils Jansen and Murat Cubuktepe and Ufuk Topcu", "title": "Synthesis of Shared Control Protocols with Provable Safety and\n  Performance Guarantees", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We formalize synthesis of shared control protocols with correctness\nguarantees for temporal logic specifications. More specifically, we introduce a\nmodeling formalism in which both a human and an autonomy protocol can issue\ncommands to a robot towards performing a certain task. These commands are\nblended into a joint input to the robot. The autonomy protocol is synthesized\nusing an abstraction of possible human commands accounting for randomness in\ndecisions caused by factors such as fatigue or incomprehensibility of the\nproblem at hand. The synthesis is designed to ensure that the resulting robot\nbehavior satisfies given safety and performance specifications, e.g., in\ntemporal logic. Our solution is based on nonlinear programming and we address\nthe inherent scalability issue by presenting alternative methods. We assess the\nfeasibility and the scalability of the approach by an experimental evaluation.\n", "versions": [{"version": "v1", "created": "Wed, 26 Oct 2016 19:49:09 GMT"}], "update_date": "2016-10-28", "authors_parsed": [["Jansen", "Nils", ""], ["Cubuktepe", "Murat", ""], ["Topcu", "Ufuk", ""]]}, {"id": "1610.08611", "submitter": "Yangbo He", "authors": "Yango He, Zhi Geng", "title": "Causal Network Learning from Multiple Interventions of Unknown\n  Manipulated Targets", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we discuss structure learning of causal networks from multiple\ndata sets obtained by external intervention experiments where we do not know\nwhat variables are manipulated. For example, the conditions in these\nexperiments are changed by changing temperature or using drugs, but we do not\nknow what target variables are manipulated by the external interventions. From\nsuch data sets, the structure learning becomes more difficult. For this case,\nwe first discuss the identifiability of causal structures. Next we present a\ngraph-merging method for learning causal networks for the case that the sample\nsizes are large for these interventions. Then for the case that the sample\nsizes of these interventions are relatively small, we propose a data-pooling\nmethod for learning causal networks in which we pool all data sets of these\ninterventions together for the learning. Further we propose a re-sampling\napproach to evaluate the edges of the causal network learned by the\ndata-pooling method. Finally we illustrate the proposed learning methods by\nsimulations.\n", "versions": [{"version": "v1", "created": "Thu, 27 Oct 2016 04:17:46 GMT"}], "update_date": "2016-10-28", "authors_parsed": [["He", "Yango", ""], ["Geng", "Zhi", ""]]}, {"id": "1610.08613", "submitter": "{\\L}ukasz Kaiser", "authors": "{\\L}ukasz Kaiser and Samy Bengio", "title": "Can Active Memory Replace Attention?", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Several mechanisms to focus attention of a neural network on selected parts\nof its input or memory have been used successfully in deep learning models in\nrecent years. Attention has improved image classification, image captioning,\nspeech recognition, generative models, and learning algorithmic tasks, but it\nhad probably the largest impact on neural machine translation.\n  Recently, similar improvements have been obtained using alternative\nmechanisms that do not focus on a single part of a memory but operate on all of\nit in parallel, in a uniform way. Such mechanism, which we call active memory,\nimproved over attention in algorithmic tasks, image processing, and in\ngenerative modelling.\n  So far, however, active memory has not improved over attention for most\nnatural language processing tasks, in particular for machine translation. We\nanalyze this shortcoming in this paper and propose an extended model of active\nmemory that matches existing attention models on neural machine translation and\ngeneralizes better to longer sentences. We investigate this model and explain\nwhy previous active memory models did not succeed. Finally, we discuss when\nactive memory brings most benefits and where attention can be a better choice.\n", "versions": [{"version": "v1", "created": "Thu, 27 Oct 2016 04:28:29 GMT"}, {"version": "v2", "created": "Tue, 7 Mar 2017 04:04:33 GMT"}], "update_date": "2017-03-08", "authors_parsed": [["Kaiser", "\u0141ukasz", ""], ["Bengio", "Samy", ""]]}, {"id": "1610.08628", "submitter": "Pierre Alquier", "authors": "Pierre Alquier and The Tien Mai and Massimiliano Pontil", "title": "Regret Bounds for Lifelong Learning", "comments": null, "journal-ref": "Proceedings of Machine Learning Research, 2017, vol. 54 (AISTAT\n  2017), pp. 261-269", "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of transfer learning in an online setting. Different\ntasks are presented sequentially and processed by a within-task algorithm. We\npropose a lifelong learning strategy which refines the underlying data\nrepresentation used by the within-task algorithm, thereby transferring\ninformation from one task to the next. We show that when the within-task\nalgorithm comes with some regret bound, our strategy inherits this good\nproperty. Our bounds are in expectation for a general loss function, and\nuniform for a convex loss. We discuss applications to dictionary learning and\nfinite set of predictors. In the latter case, we improve previous\n$O(1/\\sqrt{m})$ bounds to $O(1/m)$ where $m$ is the per task sample size.\n", "versions": [{"version": "v1", "created": "Thu, 27 Oct 2016 06:19:27 GMT"}], "update_date": "2019-10-14", "authors_parsed": [["Alquier", "Pierre", ""], ["Mai", "The Tien", ""], ["Pontil", "Massimiliano", ""]]}, {"id": "1610.08664", "submitter": "Luigi Leonardo Palese", "authors": "Luigi Leonardo Palese", "title": "A random version of principal component analysis in data clustering", "comments": "18 pages, 6 figures, 2 tables", "journal-ref": "Comput. Biol. Chem. 73 (2018) 57-64", "doi": "10.1016/j.compbiolchem.2018.01.009", "report-no": null, "categories": "q-bio.QM cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Principal component analysis (PCA) is a widespread technique for data\nanalysis that relies on the covariance-correlation matrix of the analyzed data.\nHowever to properly work with high-dimensional data, PCA poses severe\nmathematical constraints on the minimum number of different replicates or\nsamples that must be included in the analysis. Here we show that a modified\nalgorithm works not only on well dimensioned datasets, but also on degenerated\nones.\n", "versions": [{"version": "v1", "created": "Thu, 27 Oct 2016 08:52:19 GMT"}], "update_date": "2018-10-18", "authors_parsed": [["Palese", "Luigi Leonardo", ""]]}, {"id": "1610.08696", "submitter": "Wataru Kumagai", "authors": "Wataru Kumagai", "title": "Learning Bound for Parameter Transfer Learning", "comments": "This paper was accepted at NIPS 2016 as a poster presentation", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider a transfer-learning problem by using the parameter transfer\napproach, where a suitable parameter of feature mapping is learned through one\ntask and applied to another objective task. Then, we introduce the notion of\nthe local stability and parameter transfer learnability of parametric feature\nmapping,and thereby derive a learning bound for parameter transfer algorithms.\nAs an application of parameter transfer learning, we discuss the performance of\nsparse coding in self-taught learning. Although self-taught learning algorithms\nwith plentiful unlabeled data often show excellent empirical performance, their\ntheoretical analysis has not been studied. In this paper, we also provide the\nfirst theoretical learning bound for self-taught learning.\n", "versions": [{"version": "v1", "created": "Thu, 27 Oct 2016 10:50:55 GMT"}, {"version": "v2", "created": "Wed, 9 Nov 2016 01:08:40 GMT"}, {"version": "v3", "created": "Wed, 18 Jan 2017 04:41:17 GMT"}], "update_date": "2017-01-19", "authors_parsed": [["Kumagai", "Wataru", ""]]}, {"id": "1610.08738", "submitter": "Nicolas Keriven", "authors": "Nicolas Keriven (PANAMA), Nicolas Tremblay (GIPSA-CICS), Yann\n  Traonmilin (PANAMA), R\\'emi Gribonval (PANAMA)", "title": "Compressive K-means", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Lloyd-Max algorithm is a classical approach to perform K-means\nclustering. Unfortunately, its cost becomes prohibitive as the training dataset\ngrows large. We propose a compressive version of K-means (CKM), that estimates\ncluster centers from a sketch, i.e. from a drastically compressed\nrepresentation of the training dataset. We demonstrate empirically that CKM\nperforms similarly to Lloyd-Max, for a sketch size proportional to the number\nof cen-troids times the ambient dimension, and independent of the size of the\noriginal dataset. Given the sketch, the computational complexity of CKM is also\nindependent of the size of the dataset. Unlike Lloyd-Max which requires several\nreplicates, we further demonstrate that CKM is almost insensitive to\ninitialization. For a large dataset of 10^7 data points, we show that CKM can\nrun two orders of magnitude faster than five replicates of Lloyd-Max, with\nsimilar clustering performance on artificial data. Finally, CKM achieves lower\nclassification errors on handwritten digits classification.\n", "versions": [{"version": "v1", "created": "Thu, 27 Oct 2016 12:13:05 GMT"}, {"version": "v2", "created": "Wed, 30 Nov 2016 07:58:05 GMT"}, {"version": "v3", "created": "Mon, 9 Jan 2017 10:40:53 GMT"}, {"version": "v4", "created": "Fri, 10 Feb 2017 15:22:24 GMT"}], "update_date": "2017-02-13", "authors_parsed": [["Keriven", "Nicolas", "", "PANAMA"], ["Tremblay", "Nicolas", "", "GIPSA-CICS"], ["Traonmilin", "Yann", "", "PANAMA"], ["Gribonval", "R\u00e9mi", "", "PANAMA"]]}, {"id": "1610.08749", "submitter": "Antti Honkela", "authors": "Joonas J\\\"alk\\\"o and Onur Dikmen and Antti Honkela", "title": "Differentially Private Variational Inference for Non-conjugate Models", "comments": "10 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CR cs.LG stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many machine learning applications are based on data collected from people,\nsuch as their tastes and behaviour as well as biological traits and genetic\ndata. Regardless of how important the application might be, one has to make\nsure individuals' identities or the privacy of the data are not compromised in\nthe analysis. Differential privacy constitutes a powerful framework that\nprevents breaching of data subject privacy from the output of a computation.\nDifferentially private versions of many important Bayesian inference methods\nhave been proposed, but there is a lack of an efficient unified approach\napplicable to arbitrary models. In this contribution, we propose a\ndifferentially private variational inference method with a very wide\napplicability. It is built on top of doubly stochastic variational inference, a\nrecent advance which provides a variational solution to a large class of\nmodels. We add differential privacy into doubly stochastic variational\ninference by clipping and perturbing the gradients. The algorithm is made more\nefficient through privacy amplification from subsampling. We demonstrate the\nmethod can reach an accuracy close to non-private level under reasonably strong\nprivacy guarantees, clearly improving over previous sampling-based alternatives\nespecially in the strong privacy regime.\n", "versions": [{"version": "v1", "created": "Thu, 27 Oct 2016 12:34:36 GMT"}, {"version": "v2", "created": "Mon, 10 Apr 2017 12:59:55 GMT"}], "update_date": "2017-04-11", "authors_parsed": [["J\u00e4lk\u00f6", "Joonas", ""], ["Dikmen", "Onur", ""], ["Honkela", "Antti", ""]]}, {"id": "1610.08763", "submitter": "Xiang Ren", "authors": "Xiang Ren, Zeqiu Wu, Wenqi He, Meng Qu, Clare R. Voss, Heng Ji, Tarek\n  F. Abdelzaher, Jiawei Han", "title": "CoType: Joint Extraction of Typed Entities and Relations with Knowledge\n  Bases", "comments": "WWW 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Extracting entities and relations for types of interest from text is\nimportant for understanding massive text corpora. Traditionally, systems of\nentity relation extraction have relied on human-annotated corpora for training\nand adopted an incremental pipeline. Such systems require additional human\nexpertise to be ported to a new domain, and are vulnerable to errors cascading\ndown the pipeline. In this paper, we investigate joint extraction of typed\nentities and relations with labeled data heuristically obtained from knowledge\nbases (i.e., distant supervision). As our algorithm for type labeling via\ndistant supervision is context-agnostic, noisy training data poses unique\nchallenges for the task. We propose a novel domain-independent framework,\ncalled CoType, that runs a data-driven text segmentation algorithm to extract\nentity mentions, and jointly embeds entity mentions, relation mentions, text\nfeatures and type labels into two low-dimensional spaces (for entity and\nrelation mentions respectively), where, in each space, objects whose types are\nclose will also have similar representations. CoType, then using these learned\nembeddings, estimates the types of test (unlinkable) mentions. We formulate a\njoint optimization problem to learn embeddings from text corpora and knowledge\nbases, adopting a novel partial-label loss function for noisy labeled data and\nintroducing an object \"translation\" function to capture the cross-constraints\nof entities and relations on each other. Experiments on three public datasets\ndemonstrate the effectiveness of CoType across different domains (e.g., news,\nbiomedical), with an average of 25% improvement in F1 score compared to the\nnext best method.\n", "versions": [{"version": "v1", "created": "Thu, 27 Oct 2016 13:20:25 GMT"}, {"version": "v2", "created": "Fri, 2 Jun 2017 19:28:19 GMT"}], "update_date": "2017-06-06", "authors_parsed": [["Ren", "Xiang", ""], ["Wu", "Zeqiu", ""], ["He", "Wenqi", ""], ["Qu", "Meng", ""], ["Voss", "Clare R.", ""], ["Ji", "Heng", ""], ["Abdelzaher", "Tarek F.", ""], ["Han", "Jiawei", ""]]}, {"id": "1610.08838", "submitter": "Anand Rangarajan", "authors": "Anthony O. Smith and Anand Rangarajan", "title": "A Category Space Approach to Supervised Dimensionality Reduction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Supervised dimensionality reduction has emerged as an important theme in the\nlast decade. Despite the plethora of models and formulations, there is a lack\nof a simple model which aims to project the set of patterns into a space\ndefined by the classes (or categories). To this end, we set up a model in which\neach class is represented as a 1D subspace of the vector space formed by the\nfeatures. Assuming the set of classes does not exceed the cardinality of the\nfeatures, the model results in multi-class supervised learning in which the\nfeatures of each class are projected into the class subspace. Class\ndiscrimination is automatically guaranteed via the imposition of orthogonality\nof the 1D class sub-spaces. The resulting optimization problem - formulated as\nthe minimization of a sum of quadratic functions on a Stiefel manifold - while\nbeing non-convex (due to the constraints), nevertheless has a structure for\nwhich we can identify when we have reached a global minimum. After formulating\na version with standard inner products, we extend the formulation to\nreproducing kernel Hilbert spaces in a straightforward manner. The optimization\napproach also extends in a similar fashion to the kernel version. Results and\ncomparisons with the multi-class Fisher linear (and kernel) discriminants and\nprincipal component analysis (linear and kernel) showcase the relative merits\nof this approach to dimensionality reduction.\n", "versions": [{"version": "v1", "created": "Thu, 27 Oct 2016 15:30:35 GMT"}], "update_date": "2016-10-28", "authors_parsed": [["Smith", "Anthony O.", ""], ["Rangarajan", "Anand", ""]]}, {"id": "1610.08861", "submitter": "Jie Chen", "authors": "Jie Chen, Dehua Cheng, Yan Liu", "title": "On Bochner's and Polya's Characterizations of Positive-Definite Kernels\n  and the Respective Random Feature Maps", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Positive-definite kernel functions are fundamental elements of kernel methods\nand Gaussian processes. A well-known construction of such functions comes from\nBochner's characterization, which connects a positive-definite function with a\nprobability distribution. Another construction, which appears to have attracted\nless attention, is Polya's criterion that characterizes a subset of these\nfunctions. In this paper, we study the latter characterization and derive a\nnumber of novel kernels little known previously.\n  In the context of large-scale kernel machines, Rahimi and Recht (2007)\nproposed a random feature map (random Fourier) that approximates a kernel\nfunction, through independent sampling of the probability distribution in\nBochner's characterization. The authors also suggested another feature map\n(random binning), which, although not explicitly stated, comes from Polya's\ncharacterization. We show that with the same number of random samples, the\nrandom binning map results in an Euclidean inner product closer to the kernel\nthan does the random Fourier map. The superiority of the random binning map is\nconfirmed empirically through regressions and classifications in the\nreproducing kernel Hilbert space.\n", "versions": [{"version": "v1", "created": "Thu, 27 Oct 2016 16:09:30 GMT"}], "update_date": "2016-10-28", "authors_parsed": [["Chen", "Jie", ""], ["Cheng", "Dehua", ""], ["Liu", "Yan", ""]]}, {"id": "1610.08904", "submitter": "Chen Huang", "authors": "Chen Huang, Chen Change Loy, Xiaoou Tang", "title": "Local Similarity-Aware Deep Feature Embedding", "comments": "9 pages, 4 figures, 2 tables. Accepted to NIPS 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Existing deep embedding methods in vision tasks are capable of learning a\ncompact Euclidean space from images, where Euclidean distances correspond to a\nsimilarity metric. To make learning more effective and efficient, hard sample\nmining is usually employed, with samples identified through computing the\nEuclidean feature distance. However, the global Euclidean distance cannot\nfaithfully characterize the true feature similarity in a complex visual feature\nspace, where the intraclass distance in a high-density region may be larger\nthan the interclass distance in low-density regions. In this paper, we\nintroduce a Position-Dependent Deep Metric (PDDM) unit, which is capable of\nlearning a similarity metric adaptive to local feature structure. The metric\ncan be used to select genuinely hard samples in a local neighborhood to guide\nthe deep embedding learning in an online and robust manner. The new layer is\nappealing in that it is pluggable to any convolutional networks and is trained\nend-to-end. Our local similarity-aware feature embedding not only demonstrates\nfaster convergence and boosted performance on two complex image retrieval\ndatasets, its large margin nature also leads to superior generalization results\nunder the large and open set scenarios of transfer learning and zero-shot\nlearning on ImageNet 2010 and ImageNet-10K datasets.\n", "versions": [{"version": "v1", "created": "Thu, 27 Oct 2016 17:51:18 GMT"}], "update_date": "2016-10-28", "authors_parsed": [["Huang", "Chen", ""], ["Loy", "Chen Change", ""], ["Tang", "Xiaoou", ""]]}, {"id": "1610.08936", "submitter": "Maruan Al-Shedivat", "authors": "Maruan Al-Shedivat, Andrew Gordon Wilson, Yunus Saatchi, Zhiting Hu,\n  Eric P. Xing", "title": "Learning Scalable Deep Kernels with Recurrent Structure", "comments": "37 pages, 7 figures, 5 tables. Updated to the final version that\n  appears in JMLR, 18(82):1-37, 2017", "journal-ref": "Journal of Machine Learning Research (JMLR), JMLR 18(82):1-37,\n  2017", "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many applications in speech, robotics, finance, and biology deal with\nsequential data, where ordering matters and recurrent structures are common.\nHowever, this structure cannot be easily captured by standard kernel functions.\nTo model such structure, we propose expressive closed-form kernel functions for\nGaussian processes. The resulting model, GP-LSTM, fully encapsulates the\ninductive biases of long short-term memory (LSTM) recurrent networks, while\nretaining the non-parametric probabilistic advantages of Gaussian processes. We\nlearn the properties of the proposed kernels by optimizing the Gaussian process\nmarginal likelihood using a new provably convergent semi-stochastic gradient\nprocedure and exploit the structure of these kernels for scalable training and\nprediction. This approach provides a practical representation for Bayesian\nLSTMs. We demonstrate state-of-the-art performance on several benchmarks, and\nthoroughly investigate a consequential autonomous driving application, where\nthe predictive uncertainties provided by GP-LSTM are uniquely valuable.\n", "versions": [{"version": "v1", "created": "Thu, 27 Oct 2016 19:08:57 GMT"}, {"version": "v2", "created": "Mon, 26 Jun 2017 06:49:55 GMT"}, {"version": "v3", "created": "Thu, 5 Oct 2017 01:14:56 GMT"}], "update_date": "2017-10-06", "authors_parsed": [["Al-Shedivat", "Maruan", ""], ["Wilson", "Andrew Gordon", ""], ["Saatchi", "Yunus", ""], ["Hu", "Zhiting", ""], ["Xing", "Eric P.", ""]]}, {"id": "1610.09001", "submitter": "Yusuf Aytar", "authors": "Yusuf Aytar, Carl Vondrick, Antonio Torralba", "title": "SoundNet: Learning Sound Representations from Unlabeled Video", "comments": "NIPS 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.SD", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We learn rich natural sound representations by capitalizing on large amounts\nof unlabeled sound data collected in the wild. We leverage the natural\nsynchronization between vision and sound to learn an acoustic representation\nusing two-million unlabeled videos. Unlabeled video has the advantage that it\ncan be economically acquired at massive scales, yet contains useful signals\nabout natural sound. We propose a student-teacher training procedure which\ntransfers discriminative visual knowledge from well established visual\nrecognition models into the sound modality using unlabeled video as a bridge.\nOur sound representation yields significant performance improvements over the\nstate-of-the-art results on standard benchmarks for acoustic scene/object\nclassification. Visualizations suggest some high-level semantics automatically\nemerge in the sound network, even though it is trained without ground truth\nlabels.\n", "versions": [{"version": "v1", "created": "Thu, 27 Oct 2016 20:23:39 GMT"}], "update_date": "2016-10-31", "authors_parsed": [["Aytar", "Yusuf", ""], ["Vondrick", "Carl", ""], ["Torralba", "Antonio", ""]]}, {"id": "1610.09003", "submitter": "Carl Vondrick", "authors": "Yusuf Aytar, Lluis Castrejon, Carl Vondrick, Hamed Pirsiavash, Antonio\n  Torralba", "title": "Cross-Modal Scene Networks", "comments": "See more at http://cmplaces.csail.mit.edu/. arXiv admin note: text\n  overlap with arXiv:1607.07295", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  People can recognize scenes across many different modalities beyond natural\nimages. In this paper, we investigate how to learn cross-modal scene\nrepresentations that transfer across modalities. To study this problem, we\nintroduce a new cross-modal scene dataset. While convolutional neural networks\ncan categorize scenes well, they also learn an intermediate representation not\naligned across modalities, which is undesirable for cross-modal transfer\napplications. We present methods to regularize cross-modal convolutional neural\nnetworks so that they have a shared representation that is agnostic of the\nmodality. Our experiments suggest that our scene representation can help\ntransfer representations across modalities for retrieval. Moreover, our\nvisualizations suggest that units emerge in the shared representation that tend\nto activate on consistent concepts independently of the modality.\n", "versions": [{"version": "v1", "created": "Thu, 27 Oct 2016 20:24:36 GMT"}], "update_date": "2016-10-31", "authors_parsed": [["Aytar", "Yusuf", ""], ["Castrejon", "Lluis", ""], ["Vondrick", "Carl", ""], ["Pirsiavash", "Hamed", ""], ["Torralba", "Antonio", ""]]}, {"id": "1610.09027", "submitter": "Jonathan Hunt", "authors": "Jack W Rae, Jonathan J Hunt, Tim Harley, Ivo Danihelka, Andrew Senior,\n  Greg Wayne, Alex Graves, Timothy P Lillicrap", "title": "Scaling Memory-Augmented Neural Networks with Sparse Reads and Writes", "comments": "in 30th Conference on Neural Information Processing Systems (NIPS\n  2016), Barcelona, Spain", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural networks augmented with external memory have the ability to learn\nalgorithmic solutions to complex tasks. These models appear promising for\napplications such as language modeling and machine translation. However, they\nscale poorly in both space and time as the amount of memory grows --- limiting\ntheir applicability to real-world domains. Here, we present an end-to-end\ndifferentiable memory access scheme, which we call Sparse Access Memory (SAM),\nthat retains the representational power of the original approaches whilst\ntraining efficiently with very large memories. We show that SAM achieves\nasymptotic lower bounds in space and time complexity, and find that an\nimplementation runs $1,\\!000\\times$ faster and with $3,\\!000\\times$ less\nphysical memory than non-sparse models. SAM learns with comparable data\nefficiency to existing models on a range of synthetic tasks and one-shot\nOmniglot character recognition, and can scale to tasks requiring $100,\\!000$s\nof time steps and memories. As well, we show how our approach can be adapted\nfor models that maintain temporal associations between memories, as with the\nrecently introduced Differentiable Neural Computer.\n", "versions": [{"version": "v1", "created": "Thu, 27 Oct 2016 22:38:05 GMT"}], "update_date": "2016-10-31", "authors_parsed": [["Rae", "Jack W", ""], ["Hunt", "Jonathan J", ""], ["Harley", "Tim", ""], ["Danihelka", "Ivo", ""], ["Senior", "Andrew", ""], ["Wayne", "Greg", ""], ["Graves", "Alex", ""], ["Lillicrap", "Timothy P", ""]]}, {"id": "1610.09033", "submitter": "Jaan Altosaar", "authors": "Rajesh Ranganath, Jaan Altosaar, Dustin Tran, David M. Blei", "title": "Operator Variational Inference", "comments": "Appears in Neural Information Processing Systems, 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Variational inference is an umbrella term for algorithms which cast Bayesian\ninference as optimization. Classically, variational inference uses the\nKullback-Leibler divergence to define the optimization. Though this divergence\nhas been widely used, the resultant posterior approximation can suffer from\nundesirable statistical properties. To address this, we reexamine variational\ninference from its roots as an optimization problem. We use operators, or\nfunctions of functions, to design variational objectives. As one example, we\ndesign a variational objective with a Langevin-Stein operator. We develop a\nblack box algorithm, operator variational inference (OPVI), for optimizing any\noperator objective. Importantly, operators enable us to make explicit the\nstatistical and computational tradeoffs for variational inference. We can\ncharacterize different properties of variational objectives, such as objectives\nthat admit data subsampling---allowing inference to scale to massive data---as\nwell as objectives that admit variational programs---a rich class of posterior\napproximations that does not require a tractable density. We illustrate the\nbenefits of OPVI on a mixture model and a generative model of images.\n", "versions": [{"version": "v1", "created": "Thu, 27 Oct 2016 23:32:25 GMT"}, {"version": "v2", "created": "Mon, 31 Oct 2016 23:58:43 GMT"}, {"version": "v3", "created": "Thu, 15 Mar 2018 01:08:06 GMT"}], "update_date": "2018-03-16", "authors_parsed": [["Ranganath", "Rajesh", ""], ["Altosaar", "Jaan", ""], ["Tran", "Dustin", ""], ["Blei", "David M.", ""]]}, {"id": "1610.09038", "submitter": "Alex Lamb", "authors": "Alex Lamb, Anirudh Goyal, Ying Zhang, Saizheng Zhang, Aaron Courville,\n  Yoshua Bengio", "title": "Professor Forcing: A New Algorithm for Training Recurrent Networks", "comments": "NIPS 2016 Accepted Paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Teacher Forcing algorithm trains recurrent networks by supplying observed\nsequence values as inputs during training and using the network's own\none-step-ahead predictions to do multi-step sampling. We introduce the\nProfessor Forcing algorithm, which uses adversarial domain adaptation to\nencourage the dynamics of the recurrent network to be the same when training\nthe network and when sampling from the network over multiple time steps. We\napply Professor Forcing to language modeling, vocal synthesis on raw waveforms,\nhandwriting generation, and image generation. Empirically we find that\nProfessor Forcing acts as a regularizer, improving test likelihood on character\nlevel Penn Treebank and sequential MNIST. We also find that the model\nqualitatively improves samples, especially when sampling for a large number of\ntime steps. This is supported by human evaluation of sample quality. Trade-offs\nbetween Professor Forcing and Scheduled Sampling are discussed. We produce\nT-SNEs showing that Professor Forcing successfully makes the dynamics of the\nnetwork during training and sampling more similar.\n", "versions": [{"version": "v1", "created": "Thu, 27 Oct 2016 23:54:31 GMT"}], "update_date": "2016-10-31", "authors_parsed": [["Lamb", "Alex", ""], ["Goyal", "Anirudh", ""], ["Zhang", "Ying", ""], ["Zhang", "Saizheng", ""], ["Courville", "Aaron", ""], ["Bengio", "Yoshua", ""]]}, {"id": "1610.09072", "submitter": "Felix X. Yu", "authors": "Felix X. Yu, Ananda Theertha Suresh, Krzysztof Choromanski, Daniel\n  Holtmann-Rice, Sanjiv Kumar", "title": "Orthogonal Random Features", "comments": "NIPS 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an intriguing discovery related to Random Fourier Features: in\nGaussian kernel approximation, replacing the random Gaussian matrix by a\nproperly scaled random orthogonal matrix significantly decreases kernel\napproximation error. We call this technique Orthogonal Random Features (ORF),\nand provide theoretical and empirical justification for this behavior.\nMotivated by this discovery, we further propose Structured Orthogonal Random\nFeatures (SORF), which uses a class of structured discrete orthogonal matrices\nto speed up the computation. The method reduces the time cost from\n$\\mathcal{O}(d^2)$ to $\\mathcal{O}(d \\log d)$, where $d$ is the data\ndimensionality, with almost no compromise in kernel approximation quality\ncompared to ORF. Experiments on several datasets verify the effectiveness of\nORF and SORF over the existing methods. We also provide discussions on using\nthe same type of discrete orthogonal structure for a broader range of\napplications.\n", "versions": [{"version": "v1", "created": "Fri, 28 Oct 2016 03:50:00 GMT"}], "update_date": "2016-10-31", "authors_parsed": [["Yu", "Felix X.", ""], ["Suresh", "Ananda Theertha", ""], ["Choromanski", "Krzysztof", ""], ["Holtmann-Rice", "Daniel", ""], ["Kumar", "Sanjiv", ""]]}, {"id": "1610.09075", "submitter": "Jason Poulos", "authors": "Jason Poulos and Rafael Valle", "title": "Missing Data Imputation for Supervised Learning", "comments": null, "journal-ref": "Applied Artificial Intelligence, 32(2), 186-196 (2018)", "doi": "10.1080/08839514.2018.1448143", "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Missing data imputation can help improve the performance of prediction models\nin situations where missing data hide useful information. This paper compares\nmethods for imputing missing categorical data for supervised classification\ntasks. We experiment on two machine learning benchmark datasets with missing\ncategorical data, comparing classifiers trained on non-imputed (i.e., one-hot\nencoded) or imputed data with different levels of additional missing-data\nperturbation. We show imputation methods can increase predictive accuracy in\nthe presence of missing-data perturbation, which can actually improve\nprediction accuracy by regularizing the classifier. We achieve the\nstate-of-the-art on the Adult dataset with missing-data perturbation and\nk-nearest-neighbors (k-NN) imputation.\n", "versions": [{"version": "v1", "created": "Fri, 28 Oct 2016 04:06:59 GMT"}, {"version": "v2", "created": "Tue, 7 Aug 2018 01:44:58 GMT"}], "update_date": "2020-08-11", "authors_parsed": [["Poulos", "Jason", ""], ["Valle", "Rafael", ""]]}, {"id": "1610.09083", "submitter": "Steven C.H. Hoi", "authors": "Yue Wu, Steven C.H. Hoi, Chenghao Liu, Jing Lu, Doyen Sahoo, Nenghai\n  Yu", "title": "SOL: A Library for Scalable Online Learning Algorithms", "comments": "5 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  SOL is an open-source library for scalable online learning algorithms, and is\nparticularly suitable for learning with high-dimensional data. The library\nprovides a family of regular and sparse online learning algorithms for\nlarge-scale binary and multi-class classification tasks with high efficiency,\nscalability, portability, and extensibility. SOL was implemented in C++, and\nprovided with a collection of easy-to-use command-line tools, python wrappers\nand library calls for users and developers, as well as comprehensive documents\nfor both beginners and advanced users. SOL is not only a practical machine\nlearning toolbox, but also a comprehensive experimental platform for online\nlearning research. Experiments demonstrate that SOL is highly efficient and\nscalable for large-scale machine learning with high-dimensional data.\n", "versions": [{"version": "v1", "created": "Fri, 28 Oct 2016 05:47:51 GMT"}], "update_date": "2016-10-31", "authors_parsed": [["Wu", "Yue", ""], ["Hoi", "Steven C. H.", ""], ["Liu", "Chenghao", ""], ["Lu", "Jing", ""], ["Sahoo", "Doyen", ""], ["Yu", "Nenghai", ""]]}, {"id": "1610.09110", "submitter": "Igal Sason", "authors": "Igal Sason and Sergio Verd\\'u", "title": "$f$-Divergence Inequalities via Functional Domination", "comments": "A conference paper, 5 pages. To be presented in the 2016 ICSEE\n  International Conference on the Science of Electrical Engineering, Nov.\n  16--18, Eilat, Israel. See https://arxiv.org/abs/1508.00335 for the full\n  paper version, published as a journal paper in the IEEE Trans. on Information\n  Theory, vol. 62, no. 11, pp. 5973-6006, November 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.LG math.IT math.PR math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper considers derivation of $f$-divergence inequalities via the\napproach of functional domination. Bounds on an $f$-divergence based on one or\nseveral other $f$-divergences are introduced, dealing with pairs of probability\nmeasures defined on arbitrary alphabets. In addition, a variety of bounds are\nshown to hold under boundedness assumptions on the relative information. The\njournal paper, which includes more approaches for the derivation of\nf-divergence inequalities and proofs, is available on the arXiv at\nhttps://arxiv.org/abs/1508.00335, and it has been published in the IEEE Trans.\non Information Theory, vol. 62, no. 11, pp. 5973-6006, November 2016.\n", "versions": [{"version": "v1", "created": "Fri, 28 Oct 2016 08:11:26 GMT"}], "update_date": "2016-10-31", "authors_parsed": [["Sason", "Igal", ""], ["Verd\u00fa", "Sergio", ""]]}, {"id": "1610.09127", "submitter": "Ricardo Pio Monti", "authors": "Ricardo Pio Monti, Christoforos Anagnostopoulos, Giovanni Montana", "title": "Adaptive regularization for Lasso models in the context of\n  non-stationary data streams", "comments": "20 pages, 3 figures. arXiv admin note: text overlap with\n  arXiv:1511.02187", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Large scale, streaming datasets are ubiquitous in modern machine learning.\nStreaming algorithms must be scalable, amenable to incremental training and\nrobust to the presence of non-stationarity. In this work consider the problem\nof learning $\\ell_1$ regularized linear models in the context of streaming\ndata. In particular, the focus of this work revolves around how to select the\nregularization parameter when data arrives sequentially and the underlying\ndistribution is non-stationary (implying the choice of optimal regularization\nparameter is itself time-varying). We propose a framework through which to\ninfer an adaptive regularization parameter. Our approach employs an $\\ell_1$\npenalty constraint where the corresponding sparsity parameter is iteratively\nupdated via stochastic gradient descent. This serves to reformulate the choice\nof regularization parameter in a principled framework for online learning. The\nproposed method is derived for linear regression and subsequently extended to\ngeneralized linear models. We validate our approach using simulated and real\ndatasets and present an application to a neuroimaging dataset.\n", "versions": [{"version": "v1", "created": "Fri, 28 Oct 2016 09:04:30 GMT"}, {"version": "v2", "created": "Thu, 14 Dec 2017 11:14:32 GMT"}], "update_date": "2017-12-15", "authors_parsed": [["Monti", "Ricardo Pio", ""], ["Anagnostopoulos", "Christoforos", ""], ["Montana", "Giovanni", ""]]}, {"id": "1610.09158", "submitter": "Sebastian Ruder", "authors": "Sebastian Ruder, Parsa Ghaffari, and John G. Breslin", "title": "Towards a continuous modeling of natural language domains", "comments": "5 pages, 3 figures, published in Uphill Battles in Language\n  Processing workshop, EMNLP 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Humans continuously adapt their style and language to a variety of domains.\nHowever, a reliable definition of `domain' has eluded researchers thus far.\nAdditionally, the notion of discrete domains stands in contrast to the\nmultiplicity of heterogeneous domains that humans navigate, many of which\noverlap. In order to better understand the change and variation of human\nlanguage, we draw on research in domain adaptation and extend the notion of\ndiscrete domains to the continuous spectrum. We propose representation\nlearning-based models that can adapt to continuous domains and detail how these\ncan be used to investigate variation in language. To this end, we propose to\nuse dialogue modeling as a test bed due to its proximity to language modeling\nand its social component.\n", "versions": [{"version": "v1", "created": "Fri, 28 Oct 2016 10:26:40 GMT"}], "update_date": "2016-10-31", "authors_parsed": [["Ruder", "Sebastian", ""], ["Ghaffari", "Parsa", ""], ["Breslin", "John G.", ""]]}, {"id": "1610.09201", "submitter": "Maciej Wielgosz", "authors": "Matej Mertik and Maciej Wielgosz and Andrzej Skocze\\'n", "title": "A Conceptual Development of Quench Prediction App build on LSTM and ELQA\n  framework", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article presents a development of web application for quench prediction\nin \\gls{te-mpe-ee} at CERN. The authors describe an ELectrical Quality\nAssurance (ELQA) framework, a platform which was designed for rapid development\nof web integrated data analysis applications for different analysis needed\nduring the hardware commissioning of the Large Hadron Collider (LHC). In second\npart the article describes a research carried out with the data collected from\nQuench Detection System by means of using an LSTM recurrent neural network. The\narticle discusses and presents a conceptual work of implementing quench\nprediction application for \\gls{te-mpe-ee} based on the ELQA and quench\nprediction algorithm.\n", "versions": [{"version": "v1", "created": "Tue, 25 Oct 2016 20:19:35 GMT"}], "update_date": "2016-10-31", "authors_parsed": [["Mertik", "Matej", ""], ["Wielgosz", "Maciej", ""], ["Skocze\u0144", "Andrzej", ""]]}, {"id": "1610.09269", "submitter": "Aurko Roy", "authors": "Aurko Roy and Sebastian Pokutta", "title": "Hierarchical Clustering via Spreading Metrics", "comments": "Extended abstract in proceedings of NIPS 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the cost function for hierarchical clusterings introduced by\n[arXiv:1510.05043] where hierarchies are treated as first-class objects rather\nthan deriving their cost from projections into flat clusters. It was also shown\nin [arXiv:1510.05043] that a top-down algorithm returns a hierarchical\nclustering of cost at most $O\\left(\\alpha_n \\log n\\right)$ times the cost of\nthe optimal hierarchical clustering, where $\\alpha_n$ is the approximation\nratio of the Sparsest Cut subroutine used. Thus using the best known\napproximation algorithm for Sparsest Cut due to Arora-Rao-Vazirani, the top\ndown algorithm returns a hierarchical clustering of cost at most\n$O\\left(\\log^{3/2} n\\right)$ times the cost of the optimal solution. We improve\nthis by giving an $O(\\log{n})$-approximation algorithm for this problem. Our\nmain technical ingredients are a combinatorial characterization of ultrametrics\ninduced by this cost function, deriving an Integer Linear Programming (ILP)\nformulation for this family of ultrametrics, and showing how to iteratively\nround an LP relaxation of this formulation by using the idea of \\emph{sphere\ngrowing} which has been extensively used in the context of graph partitioning.\nWe also prove that our algorithm returns an $O(\\log{n})$-approximate\nhierarchical clustering for a generalization of this cost function also studied\nin [arXiv:1510.05043]. Experiments show that the hierarchies found by using the\nILP formulation as well as our rounding algorithm often have better projections\ninto flat clusters than the standard linkage based algorithms. We also give\nconstant factor inapproximability results for this problem.\n", "versions": [{"version": "v1", "created": "Fri, 28 Oct 2016 15:30:21 GMT"}], "update_date": "2016-10-31", "authors_parsed": [["Roy", "Aurko", ""], ["Pokutta", "Sebastian", ""]]}, {"id": "1610.09274", "submitter": "Guang-He Lee", "authors": "Guang-He Lee, Shao-Wen Yang, Shou-De Lin", "title": "Toward Implicit Sample Noise Modeling: Deviation-driven Matrix\n  Factorization", "comments": "6 pages + 1 reference page", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.IR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The objective function of a matrix factorization model usually aims to\nminimize the average of a regression error contributed by each element.\nHowever, given the existence of stochastic noises, the implicit deviations of\nsample data from their true values are almost surely diverse, which makes each\ndata point not equally suitable for fitting a model. In this case, simply\naveraging the cost among data in the objective function is not ideal.\nIntuitively we would like to emphasize more on the reliable instances (i.e.,\nthose contain smaller noise) while training a model. Motivated by such\nobservation, we derive our formula from a theoretical framework for optimal\nweighting under heteroscedastic noise distribution. Specifically, by modeling\nand learning the deviation of data, we design a novel matrix factorization\nmodel. Our model has two advantages. First, it jointly learns the deviation and\nconducts dynamic reweighting of instances, allowing the model to converge to a\nbetter solution. Second, during learning the deviated instances are assigned\nlower weights, which leads to faster convergence since the model does not need\nto overfit the noise. The experiments are conducted in clean recommendation and\nnoisy sensor datasets to test the effectiveness of the model in various\nscenarios. The results show that our model outperforms the state-of-the-art\nfactorization and deep learning models in both accuracy and efficiency.\n", "versions": [{"version": "v1", "created": "Fri, 28 Oct 2016 15:33:25 GMT"}], "update_date": "2016-10-31", "authors_parsed": [["Lee", "Guang-He", ""], ["Yang", "Shao-Wen", ""], ["Lin", "Shou-De", ""]]}, {"id": "1610.09296", "submitter": "Antonia Creswell", "authors": "Antonia Creswell, Kai Arulkumaran, Anil Anthony Bharath", "title": "Improving Sampling from Generative Autoencoders with Markov Chains", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We focus on generative autoencoders, such as variational or adversarial\nautoencoders, which jointly learn a generative model alongside an inference\nmodel. Generative autoencoders are those which are trained to softly enforce a\nprior on the latent distribution learned by the inference model. We call the\ndistribution to which the inference model maps observed samples, the learned\nlatent distribution, which may not be consistent with the prior. We formulate a\nMarkov chain Monte Carlo (MCMC) sampling process, equivalent to iteratively\ndecoding and encoding, which allows us to sample from the learned latent\ndistribution. Since, the generative model learns to map from the learned latent\ndistribution, rather than the prior, we may use MCMC to improve the quality of\nsamples drawn from the generative model, especially when the learned latent\ndistribution is far from the prior. Using MCMC sampling, we are able to reveal\npreviously unseen differences between generative autoencoders trained either\nwith or without a denoising criterion.\n", "versions": [{"version": "v1", "created": "Fri, 28 Oct 2016 16:17:03 GMT"}, {"version": "v2", "created": "Mon, 7 Nov 2016 15:17:32 GMT"}, {"version": "v3", "created": "Thu, 12 Jan 2017 16:13:14 GMT"}], "update_date": "2017-01-13", "authors_parsed": [["Creswell", "Antonia", ""], ["Arulkumaran", "Kai", ""], ["Bharath", "Anil Anthony", ""]]}, {"id": "1610.09300", "submitter": "Antoine Gautier", "authors": "Antoine Gautier, Quynh Nguyen and Matthias Hein", "title": "Globally Optimal Training of Generalized Polynomial Neural Networks with\n  Nonlinear Spectral Methods", "comments": "Long version of NIPS 2016 paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The optimization problem behind neural networks is highly non-convex.\nTraining with stochastic gradient descent and variants requires careful\nparameter tuning and provides no guarantee to achieve the global optimum. In\ncontrast we show under quite weak assumptions on the data that a particular\nclass of feedforward neural networks can be trained globally optimal with a\nlinear convergence rate with our nonlinear spectral method. Up to our knowledge\nthis is the first practically feasible method which achieves such a guarantee.\nWhile the method can in principle be applied to deep networks, we restrict\nourselves for simplicity in this paper to one and two hidden layer networks.\nOur experiments confirm that these models are rich enough to achieve good\nperformance on a series of real-world datasets.\n", "versions": [{"version": "v1", "created": "Fri, 28 Oct 2016 16:28:23 GMT"}], "update_date": "2016-10-31", "authors_parsed": [["Gautier", "Antoine", ""], ["Nguyen", "Quynh", ""], ["Hein", "Matthias", ""]]}, {"id": "1610.09307", "submitter": "Han Guo", "authors": "Namrata Vaswani and Han Guo", "title": "Correlated-PCA: Principal Components' Analysis when Data and Noise are\n  Correlated", "comments": "This paper has been withdrawn. It should have been a replacement of\n  arXiv 1608.04320", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given a matrix of observed data, Principal Components Analysis (PCA) computes\na small number of orthogonal directions that contain most of its variability.\nProvably accurate solutions for PCA have been in use for a long time. However,\nto the best of our knowledge, all existing theoretical guarantees for it assume\nthat the data and the corrupting noise are mutually independent, or at least\nuncorrelated. This is valid in practice often, but not always. In this paper,\nwe study the PCA problem in the setting where the data and noise can be\ncorrelated. Such noise is often also referred to as \"data-dependent noise\". We\nobtain a correctness result for the standard eigenvalue decomposition (EVD)\nbased solution to PCA under simple assumptions on the data-noise correlation.\nWe also develop and analyze a generalization of EVD, cluster-EVD, that improves\nupon EVD in certain regimes.\n", "versions": [{"version": "v1", "created": "Fri, 28 Oct 2016 16:37:39 GMT"}, {"version": "v2", "created": "Mon, 31 Oct 2016 19:44:44 GMT"}], "update_date": "2016-11-01", "authors_parsed": [["Vaswani", "Namrata", ""], ["Guo", "Han", ""]]}, {"id": "1610.09322", "submitter": "Yuan Deng", "authors": "Anima Anandkumar, Yuan Deng, Rong Ge, Hossein Mobahi", "title": "Homotopy Analysis for Tensor PCA", "comments": "Accepted to COLT 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Developing efficient and guaranteed nonconvex algorithms has been an\nimportant challenge in modern machine learning. Algorithms with good empirical\nperformance such as stochastic gradient descent often lack theoretical\nguarantees. In this paper, we analyze the class of homotopy or continuation\nmethods for global optimization of nonconvex functions. These methods start\nfrom an objective function that is efficient to optimize (e.g. convex), and\nprogressively modify it to obtain the required objective, and the solutions are\npassed along the homotopy path. For the challenging problem of tensor PCA, we\nprove global convergence of the homotopy method in the \"high noise\" regime. The\nsignal-to-noise requirement for our algorithm is tight in the sense that it\nmatches the recovery guarantee for the best degree-4 sum-of-squares algorithm.\nIn addition, we prove a phase transition along the homotopy path for tensor\nPCA. This allows to simplify the homotopy method to a local search algorithm,\nviz., tensor power iterations, with a specific initialization and a noise\ninjection procedure, while retaining the theoretical guarantees.\n", "versions": [{"version": "v1", "created": "Fri, 28 Oct 2016 17:24:45 GMT"}, {"version": "v2", "created": "Tue, 1 Nov 2016 02:52:41 GMT"}, {"version": "v3", "created": "Wed, 2 Nov 2016 13:00:58 GMT"}, {"version": "v4", "created": "Wed, 14 Jun 2017 00:11:55 GMT"}], "update_date": "2017-06-15", "authors_parsed": [["Anandkumar", "Anima", ""], ["Deng", "Yuan", ""], ["Ge", "Rong", ""], ["Mobahi", "Hossein", ""]]}, {"id": "1610.09369", "submitter": "Mathias Niepert", "authors": "Mathias Niepert", "title": "Discriminative Gaifman Models", "comments": "NIPS 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present discriminative Gaifman models, a novel family of relational\nmachine learning models. Gaifman models learn feature representations bottom up\nfrom representations of locally connected and bounded-size regions of knowledge\nbases (KBs). Considering local and bounded-size neighborhoods of knowledge\nbases renders logical inference and learning tractable, mitigates the problem\nof overfitting, and facilitates weight sharing. Gaifman models sample\nneighborhoods of knowledge bases so as to make the learned relational models\nmore robust to missing objects and relations which is a common situation in\nopen-world KBs. We present the core ideas of Gaifman models and apply them to\nlarge-scale relational learning problems. We also discuss the ways in which\nGaifman models relate to some existing relational machine learning approaches.\n", "versions": [{"version": "v1", "created": "Fri, 28 Oct 2016 11:57:26 GMT"}], "update_date": "2016-11-01", "authors_parsed": [["Niepert", "Mathias", ""]]}, {"id": "1610.09420", "submitter": "Liangbei Xu", "authors": "Liangbei Xu and Mark A. Davenport", "title": "Dynamic matrix recovery from incomplete observations under an exact\n  low-rank constraint", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Low-rank matrix factorizations arise in a wide variety of applications --\nincluding recommendation systems, topic models, and source separation, to name\njust a few. In these and many other applications, it has been widely noted that\nby incorporating temporal information and allowing for the possibility of\ntime-varying models, significant improvements are possible in practice.\nHowever, despite the reported superior empirical performance of these dynamic\nmodels over their static counterparts, there is limited theoretical\njustification for introducing these more complex models. In this paper we aim\nto address this gap by studying the problem of recovering a dynamically\nevolving low-rank matrix from incomplete observations. First, we propose the\nlocally weighted matrix smoothing (LOWEMS) framework as one possible approach\nto dynamic matrix recovery. We then establish error bounds for LOWEMS in both\nthe {\\em matrix sensing} and {\\em matrix completion} observation models. Our\nresults quantify the potential benefits of exploiting dynamic constraints both\nin terms of recovery accuracy and sample complexity. To illustrate these\nbenefits we provide both synthetic and real-world experimental results.\n", "versions": [{"version": "v1", "created": "Fri, 28 Oct 2016 22:44:29 GMT"}], "update_date": "2016-11-01", "authors_parsed": [["Xu", "Liangbei", ""], ["Davenport", "Mark A.", ""]]}, {"id": "1610.09428", "submitter": "Moontae Lee", "authors": "Moontae Lee, Seok Hyun Jin, David Mimno", "title": "Beyond Exchangeability: The Chinese Voting Process", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.IR cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many online communities present user-contributed responses such as reviews of\nproducts and answers to questions. User-provided helpfulness votes can\nhighlight the most useful responses, but voting is a social process that can\ngain momentum based on the popularity of responses and the polarity of existing\nvotes. We propose the Chinese Voting Process (CVP) which models the evolution\nof helpfulness votes as a self-reinforcing process dependent on position and\npresentation biases. We evaluate this model on Amazon product reviews and more\nthan 80 StackExchange forums, measuring the intrinsic quality of individual\nresponses and behavioral coefficients of different communities.\n", "versions": [{"version": "v1", "created": "Fri, 28 Oct 2016 23:38:22 GMT"}], "update_date": "2016-11-03", "authors_parsed": [["Lee", "Moontae", ""], ["Jin", "Seok Hyun", ""], ["Mimno", "David", ""]]}, {"id": "1610.09447", "submitter": "Bin Gu", "authors": "Bin Gu, Zhouyuan Huo, Heng Huang", "title": "Asynchronous Stochastic Block Coordinate Descent with Variance Reduction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Asynchronous parallel implementations for stochastic optimization have\nreceived huge successes in theory and practice recently. Asynchronous\nimplementations with lock-free are more efficient than the one with writing or\nreading lock. In this paper, we focus on a composite objective function\nconsisting of a smooth convex function $f$ and a block separable convex\nfunction, which widely exists in machine learning and computer vision. We\npropose an asynchronous stochastic block coordinate descent algorithm with the\naccelerated technology of variance reduction (AsySBCDVR), which are with\nlock-free in the implementation and analysis. AsySBCDVR is particularly\nimportant because it can scale well with the sample size and dimension\nsimultaneously. We prove that AsySBCDVR achieves a linear convergence rate when\nthe function $f$ is with the optimal strong convexity property, and a sublinear\nrate when $f$ is with the general convexity. More importantly, a near-linear\nspeedup on a parallel system with shared memory can be obtained.\n", "versions": [{"version": "v1", "created": "Sat, 29 Oct 2016 03:39:16 GMT"}, {"version": "v2", "created": "Mon, 7 Nov 2016 20:05:31 GMT"}, {"version": "v3", "created": "Mon, 14 Nov 2016 02:14:36 GMT"}], "update_date": "2016-11-15", "authors_parsed": [["Gu", "Bin", ""], ["Huo", "Zhouyuan", ""], ["Huang", "Heng", ""]]}, {"id": "1610.09451", "submitter": "Evan Sparks", "authors": "Evan R. Sparks, Shivaram Venkataraman, Tomer Kaftan, Michael J.\n  Franklin, Benjamin Recht", "title": "KeystoneML: Optimizing Pipelines for Large-Scale Advanced Analytics", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modern advanced analytics applications make use of machine learning\ntechniques and contain multiple steps of domain-specific and general-purpose\nprocessing with high resource requirements. We present KeystoneML, a system\nthat captures and optimizes the end-to-end large-scale machine learning\napplications for high-throughput training in a distributed environment with a\nhigh-level API. This approach offers increased ease of use and higher\nperformance over existing systems for large scale learning. We demonstrate the\neffectiveness of KeystoneML in achieving high quality statistical accuracy and\nscalable training using real world datasets in several domains. By optimizing\nexecution KeystoneML achieves up to 15x training throughput over unoptimized\nexecution on a real image classification application.\n", "versions": [{"version": "v1", "created": "Sat, 29 Oct 2016 04:21:24 GMT"}], "update_date": "2016-11-01", "authors_parsed": [["Sparks", "Evan R.", ""], ["Venkataraman", "Shivaram", ""], ["Kaftan", "Tomer", ""], ["Franklin", "Michael J.", ""], ["Recht", "Benjamin", ""]]}, {"id": "1610.09461", "submitter": "Quanming Yao", "authors": "Quanming Yao, James T. Kwok, Xiawei Guo", "title": "Fast Learning with Nonconvex L1-2 Regularization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NA math.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convex regularizers are often used for sparse learning. They are easy to\noptimize, but can lead to inferior prediction performance. The difference of\n$\\ell_1$ and $\\ell_2$ ($\\ell_{1-2}$) regularizer has been recently proposed as\na nonconvex regularizer. It yields better recovery than both $\\ell_0$ and\n$\\ell_1$ regularizers on compressed sensing. However, how to efficiently\noptimize its learning problem is still challenging. The main difficulty is that\nboth the $\\ell_1$ and $\\ell_2$ norms in $\\ell_{1-2}$ are not differentiable,\nand existing optimization algorithms cannot be applied. In this paper, we show\nthat a closed-form solution can be derived for the proximal step associated\nwith this regularizer. We further extend the result for low-rank matrix\nlearning and the total variation model. Experiments on both synthetic and real\ndata sets show that the resultant accelerated proximal gradient algorithm is\nmore efficient than other noncovex optimization algorithms.\n", "versions": [{"version": "v1", "created": "Sat, 29 Oct 2016 06:02:17 GMT"}, {"version": "v2", "created": "Fri, 4 Nov 2016 09:07:03 GMT"}, {"version": "v3", "created": "Tue, 20 Jun 2017 16:51:03 GMT"}], "update_date": "2017-06-21", "authors_parsed": [["Yao", "Quanming", ""], ["Kwok", "James T.", ""], ["Guo", "Xiawei", ""]]}, {"id": "1610.09463", "submitter": "Tadashi Wadayama", "authors": "Daisuke Ito and Tadashi Wadayama", "title": "Sparse Signal Recovery for Binary Compressed Sensing by Majority Voting\n  Neural Networks", "comments": "6 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.LG math.IT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose majority voting neural networks for sparse signal\nrecovery in binary compressed sensing. The majority voting neural network is\ncomposed of several independently trained feedforward neural networks employing\nthe sigmoid function as an activation function. Our empirical study shows that\na choice of a loss function used in training processes for the network is of\nprime importance. We found a loss function suitable for sparse signal recovery,\nwhich includes a cross entropy-like term and an $L_1$ regularized term. From\nthe experimental results, we observed that the majority voting neural network\nachieves excellent recovery performance, which is approaching the optimal\nperformance as the number of component nets grows. The simple architecture of\nthe majority voting neural networks would be beneficial for both software and\nhardware implementations.\n", "versions": [{"version": "v1", "created": "Sat, 29 Oct 2016 06:12:03 GMT"}], "update_date": "2016-11-01", "authors_parsed": [["Ito", "Daisuke", ""], ["Wadayama", "Tadashi", ""]]}, {"id": "1610.09491", "submitter": "Kiarash Shaloudegi", "authors": "Kiarash Shaloudegi, Andr\\'as Gy\\\"orgy, Csaba Szepesv\\'ari, and Wilsun\n  Xu", "title": "SDP Relaxation with Randomized Rounding for Energy Disaggregation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop a scalable, computationally efficient method for the task of\nenergy disaggregation for home appliance monitoring. In this problem the goal\nis to estimate the energy consumption of each appliance over time based on the\ntotal energy-consumption signal of a household. The current state of the art is\nto model the problem as inference in factorial HMMs, and use quadratic\nprogramming to find an approximate solution to the resulting quadratic integer\nprogram. Here we take a more principled approach, better suited to integer\nprogramming problems, and find an approximate optimum by combining convex\nsemidefinite relaxations randomized rounding, as well as a scalable ADMM method\nthat exploits the special structure of the resulting semidefinite program.\nSimulation results both in synthetic and real-world datasets demonstrate the\nsuperiority of our method.\n", "versions": [{"version": "v1", "created": "Sat, 29 Oct 2016 11:48:28 GMT"}], "update_date": "2016-11-01", "authors_parsed": [["Shaloudegi", "Kiarash", ""], ["Gy\u00f6rgy", "Andr\u00e1s", ""], ["Szepesv\u00e1ri", "Csaba", ""], ["Xu", "Wilsun", ""]]}, {"id": "1610.09512", "submitter": "Nan Jiang", "authors": "Nan Jiang, Akshay Krishnamurthy, Alekh Agarwal, John Langford, Robert\n  E. Schapire", "title": "Contextual Decision Processes with Low Bellman Rank are PAC-Learnable", "comments": "42 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper studies systematic exploration for reinforcement learning with\nrich observations and function approximation. We introduce a new model called\ncontextual decision processes, that unifies and generalizes most prior\nsettings. Our first contribution is a complexity measure, the Bellman rank,\nthat we show enables tractable learning of near-optimal behavior in these\nprocesses and is naturally small for many well-studied reinforcement learning\nsettings. Our second contribution is a new reinforcement learning algorithm\nthat engages in systematic exploration to learn contextual decision processes\nwith low Bellman rank. Our algorithm provably learns near-optimal behavior with\na number of samples that is polynomial in all relevant parameters but\nindependent of the number of unique observations. The approach uses Bellman\nerror minimization with optimistic exploration and provides new insights into\nefficient exploration for reinforcement learning with function approximation.\n", "versions": [{"version": "v1", "created": "Sat, 29 Oct 2016 14:01:58 GMT"}, {"version": "v2", "created": "Thu, 1 Dec 2016 19:21:45 GMT"}], "update_date": "2016-12-02", "authors_parsed": [["Jiang", "Nan", ""], ["Krishnamurthy", "Akshay", ""], ["Agarwal", "Alekh", ""], ["Langford", "John", ""], ["Schapire", "Robert E.", ""]]}, {"id": "1610.09513", "submitter": "Daniel Neil", "authors": "Daniel Neil, Michael Pfeiffer, and Shih-Chii Liu", "title": "Phased LSTM: Accelerating Recurrent Network Training for Long or\n  Event-based Sequences", "comments": "Selected for an oral presentation at NIPS, 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recurrent Neural Networks (RNNs) have become the state-of-the-art choice for\nextracting patterns from temporal sequences. However, current RNN models are\nill-suited to process irregularly sampled data triggered by events generated in\ncontinuous time by sensors or other neurons. Such data can occur, for example,\nwhen the input comes from novel event-driven artificial sensors that generate\nsparse, asynchronous streams of events or from multiple conventional sensors\nwith different update intervals. In this work, we introduce the Phased LSTM\nmodel, which extends the LSTM unit by adding a new time gate. This gate is\ncontrolled by a parametrized oscillation with a frequency range that produces\nupdates of the memory cell only during a small percentage of the cycle. Even\nwith the sparse updates imposed by the oscillation, the Phased LSTM network\nachieves faster convergence than regular LSTMs on tasks which require learning\nof long sequences. The model naturally integrates inputs from sensors of\narbitrary sampling rates, thereby opening new areas of investigation for\nprocessing asynchronous sensory events that carry timing information. It also\ngreatly improves the performance of LSTMs in standard RNN applications, and\ndoes so with an order-of-magnitude fewer computes at runtime.\n", "versions": [{"version": "v1", "created": "Sat, 29 Oct 2016 14:05:10 GMT"}], "update_date": "2016-11-01", "authors_parsed": [["Neil", "Daniel", ""], ["Pfeiffer", "Michael", ""], ["Liu", "Shih-Chii", ""]]}, {"id": "1610.09543", "submitter": "Pin-Yu Chen", "authors": "Pai-Shun Ting, Chun-Chen Tu, Pin-Yu Chen, Ya-Yun Lo, Shin-Ming Cheng", "title": "FEAST: An Automated Feature Selection Framework for Compilation Tasks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.LG cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The success of the application of machine-learning techniques to compilation\ntasks can be largely attributed to the recent development and advancement of\nprogram characterization, a process that numerically or structurally quantifies\na target program. While great achievements have been made in identifying key\nfeatures to characterize programs, choosing a correct set of features for a\nspecific compiler task remains an ad hoc procedure. In order to guarantee a\ncomprehensive coverage of features, compiler engineers usually need to select\nexcessive number of features. This, unfortunately, would potentially lead to a\nselection of multiple similar features, which in turn could create a new\nproblem of bias that emphasizes certain aspects of a program's characteristics,\nhence reducing the accuracy and performance of the target compiler task. In\nthis paper, we propose FEAture Selection for compilation Tasks (FEAST), an\nefficient and automated framework for determining the most relevant and\nrepresentative features from a feature pool. Specifically, FEAST utilizes\nwidely used statistics and machine-learning tools, including LASSO, sequential\nforward and backward selection, for automatic feature selection, and can in\ngeneral be applied to any numerical feature set. This paper further proposes an\nautomated approach to compiler parameter assignment for assessing the\nperformance of FEAST. Intensive experimental results demonstrate that, under\nthe compiler parameter assignment task, FEAST can achieve comparable results\nwith about 18% of features that are automatically selected from the entire\nfeature pool. We also inspect these selected features and discuss their roles\nin program execution.\n", "versions": [{"version": "v1", "created": "Sat, 29 Oct 2016 17:10:15 GMT"}], "update_date": "2016-11-01", "authors_parsed": [["Ting", "Pai-Shun", ""], ["Tu", "Chun-Chen", ""], ["Chen", "Pin-Yu", ""], ["Lo", "Ya-Yun", ""], ["Cheng", "Shin-Ming", ""]]}, {"id": "1610.09555", "submitter": "Jean Kossaifi", "authors": "Jean Kossaifi, Yannis Panagakis, Anima Anandkumar and Maja Pantic", "title": "TensorLy: Tensor Learning in Python", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Tensors are higher-order extensions of matrices. While matrix methods form\nthe cornerstone of machine learning and data analysis, tensor methods have been\ngaining increasing traction. However, software support for tensor operations is\nnot on the same footing. In order to bridge this gap, we have developed\n\\emph{TensorLy}, a high-level API for tensor methods and deep tensorized neural\nnetworks in Python. TensorLy aims to follow the same standards adopted by the\nmain projects of the Python scientific community, and seamlessly integrates\nwith them. Its BSD license makes it suitable for both academic and commercial\napplications. TensorLy's backend system allows users to perform computations\nwith NumPy, MXNet, PyTorch, TensorFlow and CuPy. They can be scaled on multiple\nCPU or GPU machines. In addition, using the deep-learning frameworks as backend\nallows users to easily design and train deep tensorized neural networks.\nTensorLy is available at https://github.com/tensorly/tensorly\n", "versions": [{"version": "v1", "created": "Sat, 29 Oct 2016 18:32:27 GMT"}, {"version": "v2", "created": "Wed, 9 May 2018 13:54:12 GMT"}], "update_date": "2018-05-10", "authors_parsed": [["Kossaifi", "Jean", ""], ["Panagakis", "Yannis", ""], ["Anandkumar", "Anima", ""], ["Pantic", "Maja", ""]]}, {"id": "1610.09559", "submitter": "Jamie Morgenstern", "authors": "Matthew Joseph, Michael Kearns, Jamie Morgenstern, Seth Neel, and\n  Aaron Roth", "title": "Fair Algorithms for Infinite and Contextual Bandits", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study fairness in linear bandit problems. Starting from the notion of\nmeritocratic fairness introduced in Joseph et al. [2016], we carry out a more\nrefined analysis of a more general problem, achieving better performance\nguarantees with fewer modelling assumptions on the number and structure of\navailable choices as well as the number selected. We also analyze the\npreviously-unstudied question of fairness in infinite linear bandit problems,\nobtaining instance-dependent regret upper bounds as well as lower bounds\ndemonstrating that this instance-dependence is necessary. The result is a\nframework for meritocratic fairness in an online linear setting that is\nsubstantially more powerful, general, and realistic than the current state of\nthe art.\n", "versions": [{"version": "v1", "created": "Sat, 29 Oct 2016 18:46:11 GMT"}, {"version": "v2", "created": "Tue, 1 Nov 2016 12:14:54 GMT"}, {"version": "v3", "created": "Fri, 14 Apr 2017 19:10:14 GMT"}, {"version": "v4", "created": "Thu, 29 Jun 2017 15:46:55 GMT"}], "update_date": "2017-06-30", "authors_parsed": [["Joseph", "Matthew", ""], ["Kearns", "Michael", ""], ["Morgenstern", "Jamie", ""], ["Neel", "Seth", ""], ["Roth", "Aaron", ""]]}, {"id": "1610.09608", "submitter": "Enmei Tu", "authors": "Enmei Tu, Guanghao Zhang, Lily Rachmawati, Eshan Rajabally and\n  Guang-Bin Huang", "title": "A Theoretical Study of The Relationship Between Whole An ELM Network and\n  Its Subnetworks", "comments": "3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A biological neural network is constituted by numerous subnetworks and\nmodules with different functionalities. For an artificial neural network, the\nrelationship between a network and its subnetworks is also important and useful\nfor both theoretical and algorithmic research, i.e. it can be exploited to\ndevelop incremental network training algorithm or parallel network training\nalgorithm. In this paper we explore the relationship between an ELM neural\nnetwork and its subnetworks. To the best of our knowledge, we are the first to\nprove a theorem that shows an ELM neural network can be scattered into\nsubnetworks and its optimal solution can be constructed recursively by the\noptimal solutions of these subnetworks. Based on the theorem we also present\ntwo algorithms to train a large ELM neural network efficiently: one is a\nparallel network training algorithm and the other is an incremental network\ntraining algorithm. The experimental results demonstrate the usefulness of the\ntheorem and the validity of the developed algorithms.\n", "versions": [{"version": "v1", "created": "Sun, 30 Oct 2016 06:34:19 GMT"}], "update_date": "2016-11-01", "authors_parsed": [["Tu", "Enmei", ""], ["Zhang", "Guanghao", ""], ["Rachmawati", "Lily", ""], ["Rajabally", "Eshan", ""], ["Huang", "Guang-Bin", ""]]}, {"id": "1610.09625", "submitter": "Daniel Harari", "authors": "Shimon Ullman, Nimrod Dorfman, Daniel Harari", "title": "Discovering containment: from infants to machines", "comments": null, "journal-ref": "Cognition 183 (2019) 67-81", "doi": "10.1016/j.cognition.2018.11.001", "report-no": null, "categories": "q-bio.NC cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Current artificial learning systems can recognize thousands of visual\ncategories, or play Go at a champion\"s level, but cannot explain infants\nlearning, in particular the ability to learn complex concepts without guidance,\nin a specific order. A notable example is the category of 'containers' and the\nnotion of containment, one of the earliest spatial relations to be learned,\nstarting already at 2.5 months, and preceding other common relations (e.g.,\nsupport). Such spontaneous unsupervised learning stands in contrast with\ncurrent highly successful computational models, which learn in a supervised\nmanner, that is, by using large data sets of labeled examples. How can\nmeaningful concepts be learned without guidance, and what determines the\ntrajectory of infant learning, making some notions appear consistently earlier\nthan others?\n", "versions": [{"version": "v1", "created": "Sun, 30 Oct 2016 10:26:22 GMT"}], "update_date": "2020-06-23", "authors_parsed": [["Ullman", "Shimon", ""], ["Dorfman", "Nimrod", ""], ["Harari", "Daniel", ""]]}, {"id": "1610.09639", "submitter": "Sajid Anwar", "authors": "Sajid Anwar, Wonyong Sung", "title": "Compact Deep Convolutional Neural Networks With Coarse Pruning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The learning capability of a neural network improves with increasing depth at\nhigher computational costs. Wider layers with dense kernel connectivity\npatterns furhter increase this cost and may hinder real-time inference. We\npropose feature map and kernel level pruning for reducing the computational\ncomplexity of a deep convolutional neural network. Pruning feature maps reduces\nthe width of a layer and hence does not need any sparse representation.\nFurther, kernel pruning converts the dense connectivity pattern into a sparse\none. Due to coarse nature, these pruning granularities can be exploited by GPUs\nand VLSI based implementations. We propose a simple and generic strategy to\nchoose the least adversarial pruning masks for both granularities. The pruned\nnetworks are retrained which compensates the loss in accuracy. We obtain the\nbest pruning ratios when we prune a network with both granularities.\nExperiments with the CIFAR-10 dataset show that more than 85% sparsity can be\ninduced in the convolution layers with less than 1% increase in the\nmissclassification rate of the baseline network.\n", "versions": [{"version": "v1", "created": "Sun, 30 Oct 2016 11:57:20 GMT"}], "update_date": "2016-11-01", "authors_parsed": [["Anwar", "Sajid", ""], ["Sung", "Wonyong", ""]]}, {"id": "1610.09650", "submitter": "Bharat Sau", "authors": "Bharat Bhusan Sau and Vineeth N. Balasubramanian", "title": "Deep Model Compression: Distilling Knowledge from Noisy Teachers", "comments": "9 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The remarkable successes of deep learning models across various applications\nhave resulted in the design of deeper networks that can solve complex problems.\nHowever, the increasing depth of such models also results in a higher storage\nand runtime complexity, which restricts the deployability of such very deep\nmodels on mobile and portable devices, which have limited storage and battery\ncapacity. While many methods have been proposed for deep model compression in\nrecent years, almost all of them have focused on reducing storage complexity.\nIn this work, we extend the teacher-student framework for deep model\ncompression, since it has the potential to address runtime and train time\ncomplexity too. We propose a simple methodology to include a noise-based\nregularizer while training the student from the teacher, which provides a\nhealthy improvement in the performance of the student network. Our experiments\non the CIFAR-10, SVHN and MNIST datasets show promising improvement, with the\nbest performance on the CIFAR-10 dataset. We also conduct a comprehensive\nempirical evaluation of the proposed method under related settings on the\nCIFAR-10 dataset to show the promise of the proposed approach.\n", "versions": [{"version": "v1", "created": "Sun, 30 Oct 2016 13:54:39 GMT"}, {"version": "v2", "created": "Wed, 2 Nov 2016 16:32:23 GMT"}], "update_date": "2016-11-03", "authors_parsed": [["Sau", "Bharat Bhusan", ""], ["Balasubramanian", "Vineeth N.", ""]]}, {"id": "1610.09716", "submitter": "Shuangfei Zhai", "authors": "Shuangfei Zhai, Yu Cheng, Weining Lu, Zhongfei Zhang", "title": "Doubly Convolutional Neural Networks", "comments": "To appear in NIPS 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Building large models with parameter sharing accounts for most of the success\nof deep convolutional neural networks (CNNs). In this paper, we propose doubly\nconvolutional neural networks (DCNNs), which significantly improve the\nperformance of CNNs by further exploring this idea. In stead of allocating a\nset of convolutional filters that are independently learned, a DCNN maintains\ngroups of filters where filters within each group are translated versions of\neach other. Practically, a DCNN can be easily implemented by a two-step\nconvolution procedure, which is supported by most modern deep learning\nlibraries. We perform extensive experiments on three image classification\nbenchmarks: CIFAR-10, CIFAR-100 and ImageNet, and show that DCNNs consistently\noutperform other competing architectures. We have also verified that replacing\na convolutional layer with a doubly convolutional layer at any depth of a CNN\ncan improve its performance. Moreover, various design choices of DCNNs are\ndemonstrated, which shows that DCNN can serve the dual purpose of building more\naccurate models and/or reducing the memory footprint without sacrificing the\naccuracy.\n", "versions": [{"version": "v1", "created": "Sun, 30 Oct 2016 22:07:16 GMT"}], "update_date": "2016-11-01", "authors_parsed": [["Zhai", "Shuangfei", ""], ["Cheng", "Yu", ""], ["Lu", "Weining", ""], ["Zhang", "Zhongfei", ""]]}, {"id": "1610.09726", "submitter": "Kirthevasan Kandasamy", "authors": "Kirthevasan Kandasamy and Gautam Dasarathy and Jeff Schneider and\n  Barnab\\'as P\\'oczos", "title": "The Multi-fidelity Multi-armed Bandit", "comments": "To appear at NIPS 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study a variant of the classical stochastic $K$-armed bandit where\nobserving the outcome of each arm is expensive, but cheap approximations to\nthis outcome are available. For example, in online advertising the performance\nof an ad can be approximated by displaying it for shorter time periods or to\nnarrower audiences. We formalise this task as a multi-fidelity bandit, where,\nat each time step, the forecaster may choose to play an arm at any one of $M$\nfidelities. The highest fidelity (desired outcome) expends cost\n$\\lambda^{(m)}$. The $m^{\\text{th}}$ fidelity (an approximation) expends\n$\\lambda^{(m)} < \\lambda^{(M)}$ and returns a biased estimate of the highest\nfidelity. We develop MF-UCB, a novel upper confidence bound procedure for this\nsetting and prove that it naturally adapts to the sequence of available\napproximations and costs thus attaining better regret than naive strategies\nwhich ignore the approximations. For instance, in the above online advertising\nexample, MF-UCB would use the lower fidelities to quickly eliminate suboptimal\nads and reserve the larger expensive experiments on a small set of promising\ncandidates. We complement this result with a lower bound and show that MF-UCB\nis nearly optimal under certain conditions.\n", "versions": [{"version": "v1", "created": "Sun, 30 Oct 2016 23:07:49 GMT"}], "update_date": "2016-11-01", "authors_parsed": [["Kandasamy", "Kirthevasan", ""], ["Dasarathy", "Gautam", ""], ["Schneider", "Jeff", ""], ["P\u00f3czos", "Barnab\u00e1s", ""]]}, {"id": "1610.09730", "submitter": "Songbai Yan", "authors": "Songbai Yan, Kamalika Chaudhuri and Tara Javidi", "title": "Active Learning from Imperfect Labelers", "comments": "To appear in NIPS 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study active learning where the labeler can not only return incorrect\nlabels but also abstain from labeling. We consider different noise and\nabstention conditions of the labeler. We propose an algorithm which utilizes\nabstention responses, and analyze its statistical consistency and query\ncomplexity under fairly natural assumptions on the noise and abstention rate of\nthe labeler. This algorithm is adaptive in a sense that it can automatically\nrequest less queries with a more informed or less noisy labeler. We couple our\nalgorithm with lower bounds to show that under some technical conditions, it\nachieves nearly optimal query complexity.\n", "versions": [{"version": "v1", "created": "Sun, 30 Oct 2016 23:39:18 GMT"}], "update_date": "2016-11-01", "authors_parsed": [["Yan", "Songbai", ""], ["Chaudhuri", "Kamalika", ""], ["Javidi", "Tara", ""]]}, {"id": "1610.09756", "submitter": "Vinayak Athavale", "authors": "Vinayak Athavale, Shreenivas Bharadwaj, Monik Pamecha, Ameya Prabhu\n  and Manish Shrivastava", "title": "Towards Deep Learning in Hindi NER: An approach to tackle the Labelled\n  Data Scarcity", "comments": "7 pages", "journal-ref": null, "doi": null, "report-no": "https://aclweb.org/anthology/W/W16/W16-6320.pdf", "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we describe an end to end Neural Model for Named Entity\nRecognition NER) which is based on Bi-Directional RNN-LSTM. Almost all NER\nsystems for Hindi use Language Specific features and handcrafted rules with\ngazetteers. Our model is language independent and uses no domain specific\nfeatures or any handcrafted rules. Our models rely on semantic information in\nthe form of word vectors which are learnt by an unsupervised learning algorithm\non an unannotated corpus. Our model attained state of the art performance in\nboth English and Hindi without the use of any morphological analysis or without\nusing gazetteers of any sort.\n", "versions": [{"version": "v1", "created": "Mon, 31 Oct 2016 01:31:52 GMT"}, {"version": "v2", "created": "Wed, 16 Nov 2016 17:15:14 GMT"}], "update_date": "2017-11-13", "authors_parsed": [["Athavale", "Vinayak", ""], ["Bharadwaj", "Shreenivas", ""], ["Pamecha", "Monik", ""], ["Prabhu", "Ameya", ""], ["Shrivastava", "Manish", ""]]}, {"id": "1610.09769", "submitter": "Jingbo Shang", "authors": "Jingbo Shang, Meng Qu, Jialu Liu, Lance M. Kaplan, Jiawei Han, Jian\n  Peng", "title": "Meta-Path Guided Embedding for Similarity Search in Large-Scale\n  Heterogeneous Information Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most real-world data can be modeled as heterogeneous information networks\n(HINs) consisting of vertices of multiple types and their relationships. Search\nfor similar vertices of the same type in large HINs, such as bibliographic\nnetworks and business-review networks, is a fundamental problem with broad\napplications. Although similarity search in HINs has been studied previously,\nmost existing approaches neither explore rich semantic information embedded in\nthe network structures nor take user's preference as a guidance.\n  In this paper, we re-examine similarity search in HINs and propose a novel\nembedding-based framework. It models vertices as low-dimensional vectors to\nexplore network structure-embedded similarity. To accommodate user preferences\nat defining similarity semantics, our proposed framework, ESim, accepts\nuser-defined meta-paths as guidance to learn vertex vectors in a user-preferred\nembedding space. Moreover, an efficient and parallel sampling-based\noptimization algorithm has been developed to learn embeddings in large-scale\nHINs. Extensive experiments on real-world large-scale HINs demonstrate a\nsignificant improvement on the effectiveness of ESim over several\nstate-of-the-art algorithms as well as its scalability.\n", "versions": [{"version": "v1", "created": "Mon, 31 Oct 2016 03:15:02 GMT"}], "update_date": "2016-11-01", "authors_parsed": [["Shang", "Jingbo", ""], ["Qu", "Meng", ""], ["Liu", "Jialu", ""], ["Kaplan", "Lance M.", ""], ["Han", "Jiawei", ""], ["Peng", "Jian", ""]]}, {"id": "1610.09778", "submitter": "Jingbo Shang", "authors": "Jingbo Shang, Meng Jiang, Wenzhu Tong, Jinfeng Xiao, Jian Peng, Jiawei\n  Han", "title": "DPPred: An Effective Prediction Framework with Concise Discriminative\n  Patterns", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the literature, two series of models have been proposed to address\nprediction problems including classification and regression. Simple models,\nsuch as generalized linear models, have ordinary performance but strong\ninterpretability on a set of simple features. The other series, including\ntree-based models, organize numerical, categorical and high dimensional\nfeatures into a comprehensive structure with rich interpretable information in\nthe data.\n  In this paper, we propose a novel Discriminative Pattern-based Prediction\nframework (DPPred) to accomplish the prediction tasks by taking their\nadvantages of both effectiveness and interpretability. Specifically, DPPred\nadopts the concise discriminative patterns that are on the prefix paths from\nthe root to leaf nodes in the tree-based models. DPPred selects a limited\nnumber of the useful discriminative patterns by searching for the most\neffective pattern combination to fit generalized linear models. Extensive\nexperiments show that in many scenarios, DPPred provides competitive accuracy\nwith the state-of-the-art as well as the valuable interpretability for\ndevelopers and experts. In particular, taking a clinical application dataset as\na case study, our DPPred outperforms the baselines by using only 40 concise\ndiscriminative patterns out of a potentially exponentially large set of\npatterns.\n", "versions": [{"version": "v1", "created": "Mon, 31 Oct 2016 03:43:04 GMT"}], "update_date": "2016-11-01", "authors_parsed": [["Shang", "Jingbo", ""], ["Jiang", "Meng", ""], ["Tong", "Wenzhu", ""], ["Xiao", "Jinfeng", ""], ["Peng", "Jian", ""], ["Han", "Jiawei", ""]]}, {"id": "1610.09887", "submitter": "Itay Safran", "authors": "Itay Safran, Ohad Shamir", "title": "Depth-Width Tradeoffs in Approximating Natural Functions with Neural\n  Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We provide several new depth-based separation results for feed-forward neural\nnetworks, proving that various types of simple and natural functions can be\nbetter approximated using deeper networks than shallower ones, even if the\nshallower networks are much larger. This includes indicators of balls and\nellipses; non-linear functions which are radial with respect to the $L_1$ norm;\nand smooth non-linear functions. We also show that these gaps can be observed\nexperimentally: Increasing the depth indeed allows better learning than\nincreasing width, when training neural networks to learn an indicator of a unit\nball.\n", "versions": [{"version": "v1", "created": "Mon, 31 Oct 2016 12:08:46 GMT"}, {"version": "v2", "created": "Thu, 9 Mar 2017 18:07:37 GMT"}, {"version": "v3", "created": "Wed, 13 May 2020 12:08:04 GMT"}], "update_date": "2020-05-14", "authors_parsed": [["Safran", "Itay", ""], ["Shamir", "Ohad", ""]]}, {"id": "1610.09893", "submitter": "Tao Qin Dr.", "authors": "Xiang Li and Tao Qin and Jian Yang and Tie-Yan Liu", "title": "LightRNN: Memory and Computation-Efficient Recurrent Neural Networks", "comments": "NIPS 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recurrent neural networks (RNNs) have achieved state-of-the-art performances\nin many natural language processing tasks, such as language modeling and\nmachine translation. However, when the vocabulary is large, the RNN model will\nbecome very big (e.g., possibly beyond the memory capacity of a GPU device) and\nits training will become very inefficient. In this work, we propose a novel\ntechnique to tackle this challenge. The key idea is to use 2-Component (2C)\nshared embedding for word representations. We allocate every word in the\nvocabulary into a table, each row of which is associated with a vector, and\neach column associated with another vector. Depending on its position in the\ntable, a word is jointly represented by two components: a row vector and a\ncolumn vector. Since the words in the same row share the row vector and the\nwords in the same column share the column vector, we only need $2 \\sqrt{|V|}$\nvectors to represent a vocabulary of $|V|$ unique words, which are far less\nthan the $|V|$ vectors required by existing approaches. Based on the\n2-Component shared embedding, we design a new RNN algorithm and evaluate it\nusing the language modeling task on several benchmark datasets. The results\nshow that our algorithm significantly reduces the model size and speeds up the\ntraining process, without sacrifice of accuracy (it achieves similar, if not\nbetter, perplexity as compared to state-of-the-art language models).\nRemarkably, on the One-Billion-Word benchmark Dataset, our algorithm achieves\ncomparable perplexity to previous language models, whilst reducing the model\nsize by a factor of 40-100, and speeding up the training process by a factor of\n2. We name our proposed algorithm \\emph{LightRNN} to reflect its very small\nmodel size and very high training speed.\n", "versions": [{"version": "v1", "created": "Mon, 31 Oct 2016 12:24:13 GMT"}], "update_date": "2016-11-01", "authors_parsed": [["Li", "Xiang", ""], ["Qin", "Tao", ""], ["Yang", "Jian", ""], ["Liu", "Tie-Yan", ""]]}, {"id": "1610.09900", "submitter": "Atilim Gunes Baydin", "authors": "Tuan Anh Le, Atilim Gunes Baydin, Frank Wood", "title": "Inference Compilation and Universal Probabilistic Programming", "comments": "11 pages, 6 figures", "journal-ref": "In Proceedings of the 20th International Conference on Artificial\n  Intelligence and Statistics (AISTATS 2017), 54:1338--1348. Proceedings of\n  Machine Learning Research. Fort Lauderdale, FL, USA: PMLR", "doi": null, "report-no": null, "categories": "cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a method for using deep neural networks to amortize the cost of\ninference in models from the family induced by universal probabilistic\nprogramming languages, establishing a framework that combines the strengths of\nprobabilistic programming and deep learning methods. We call what we do\n\"compilation of inference\" because our method transforms a denotational\nspecification of an inference problem in the form of a probabilistic program\nwritten in a universal programming language into a trained neural network\ndenoted in a neural network specification language. When at test time this\nneural network is fed observational data and executed, it performs approximate\ninference in the original model specified by the probabilistic program. Our\ntraining objective and learning procedure are designed to allow the trained\nneural network to be used as a proposal distribution in a sequential importance\nsampling inference engine. We illustrate our method on mixture models and\nCaptcha solving and show significant speedups in the efficiency of inference.\n", "versions": [{"version": "v1", "created": "Mon, 31 Oct 2016 12:53:20 GMT"}, {"version": "v2", "created": "Thu, 2 Mar 2017 17:11:01 GMT"}], "update_date": "2018-09-03", "authors_parsed": [["Le", "Tuan Anh", ""], ["Baydin", "Atilim Gunes", ""], ["Wood", "Frank", ""]]}, {"id": "1610.09903", "submitter": "Michael Schaarschmidt", "authors": "Michael Schaarschmidt, Felix Gessert, Valentin Dalibard, Eiko Yoneki", "title": "Learning Runtime Parameters in Computer Systems with Delayed Experience\n  Injection", "comments": "Deep Reinforcement Learning Workshop, NIPS 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning effective configurations in computer systems without hand-crafting\nmodels for every parameter is a long-standing problem. This paper investigates\nthe use of deep reinforcement learning for runtime parameters of cloud\ndatabases under latency constraints. Cloud services serve up to thousands of\nconcurrent requests per second and can adjust critical parameters by leveraging\nperformance metrics. In this work, we use continuous deep reinforcement\nlearning to learn optimal cache expirations for HTTP caching in content\ndelivery networks. To this end, we introduce a technique for asynchronous\nexperience management called delayed experience injection, which facilitates\ndelayed reward and next-state computation in concurrent environments where\nmeasurements are not immediately available. Evaluation results show that our\napproach based on normalized advantage functions and asynchronous CPU-only\ntraining outperforms a statistical estimator.\n", "versions": [{"version": "v1", "created": "Mon, 31 Oct 2016 12:57:25 GMT"}], "update_date": "2016-11-01", "authors_parsed": [["Schaarschmidt", "Michael", ""], ["Gessert", "Felix", ""], ["Dalibard", "Valentin", ""], ["Yoneki", "Eiko", ""]]}, {"id": "1610.09915", "submitter": "Rafael Boloix-Tortosa", "authors": "Rafael Boloix-Tortosa, Juan Jos\\'e Murillo-Fuentes, Irene Santos\n  Vel\\'azquez, and Fernando P\\'erez-Cruz", "title": "Complex-Valued Kernel Methods for Regression", "comments": "8 pages, 9 figures", "journal-ref": "IEEE Transactions on Signal Processing (Volume: 65, Issue: 19,\n  Oct.1, 1 2017)", "doi": "10.1109/TSP.2017.2726991", "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Usually, complex-valued RKHS are presented as an straightforward application\nof the real-valued case. In this paper we prove that this procedure yields a\nlimited solution for regression. We show that another kernel, here denoted as\npseudo kernel, is needed to learn any function in complex-valued fields.\nAccordingly, we derive a novel RKHS to include it, the widely RKHS (WRKHS).\nWhen the pseudo-kernel cancels, WRKHS reduces to complex-valued RKHS of\nprevious approaches. We address the kernel and pseudo-kernel design, paying\nattention to the kernel and the pseudo-kernel being complex-valued. In the\nexperiments included we report remarkable improvements in simple scenarios\nwhere real a imaginary parts have different similitude relations for given\ninputs or cases where real and imaginary parts are correlated. In the context\nof these novel results we revisit the problem of non-linear channel\nequalization, to show that the WRKHS helps to design more efficient solutions.\n", "versions": [{"version": "v1", "created": "Mon, 31 Oct 2016 13:36:53 GMT"}], "update_date": "2018-03-01", "authors_parsed": [["Boloix-Tortosa", "Rafael", ""], ["Murillo-Fuentes", "Juan Jos\u00e9", ""], ["Vel\u00e1zquez", "Irene Santos", ""], ["P\u00e9rez-Cruz", "Fernando", ""]]}, {"id": "1610.09932", "submitter": "Thomas Stevenson", "authors": "A. Bethani, A. J. Bevan, J. Hays and T. J. Stevenson", "title": "Support Vector Machines and Generalisation in HEP", "comments": "5 pages, 6 figures. Contribution to the proceedings of the 17th\n  International workshop on Advanced Computing and Analysis Techniques in\n  physics research - ACAT 2016, 18 - 22 January 2016, Valpara\\'iso, Chile", "journal-ref": null, "doi": "10.1088/1742-6596/762/1/012052", "report-no": null, "categories": "physics.data-an cs.LG hep-ex", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We review the concept of support vector machines (SVMs) and discuss examples\nof their use. One of the benefits of SVM algorithms, compared with neural\nnetworks and decision trees is that they can be less susceptible to over\nfitting than those other algorithms are to over training. This issue is related\nto the generalisation of a multivariate algorithm (MVA); a problem that has\noften been overlooked in particle physics. We discuss cross validation and how\nthis can be used to improve the generalisation of a MVA in the context of High\nEnergy Physics analyses. The examples presented use the Toolkit for\nMultivariate Analysis (TMVA) based on ROOT and describe our improvements to the\nSVM functionality and new tools introduced for cross validation within this\nframework.\n", "versions": [{"version": "v1", "created": "Wed, 19 Oct 2016 15:13:03 GMT"}], "update_date": "2016-12-21", "authors_parsed": [["Bethani", "A.", ""], ["Bevan", "A. J.", ""], ["Hays", "J.", ""], ["Stevenson", "T. J.", ""]]}, {"id": "1610.09975", "submitter": "Hasim Sak", "authors": "Hagen Soltau, Hank Liao, Hasim Sak", "title": "Neural Speech Recognizer: Acoustic-to-Word LSTM Model for Large\n  Vocabulary Speech Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present results that show it is possible to build a competitive, greatly\nsimplified, large vocabulary continuous speech recognition system with whole\nwords as acoustic units. We model the output vocabulary of about 100,000 words\ndirectly using deep bi-directional LSTM RNNs with CTC loss. The model is\ntrained on 125,000 hours of semi-supervised acoustic training data, which\nenables us to alleviate the data sparsity problem for word models. We show that\nthe CTC word models work very well as an end-to-end all-neural speech\nrecognition model without the use of traditional context-dependent sub-word\nphone units that require a pronunciation lexicon, and without any language\nmodel removing the need to decode. We demonstrate that the CTC word models\nperform better than a strong, more complex, state-of-the-art baseline with\nsub-word units.\n", "versions": [{"version": "v1", "created": "Mon, 31 Oct 2016 15:36:42 GMT"}], "update_date": "2016-11-01", "authors_parsed": [["Soltau", "Hagen", ""], ["Liao", "Hank", ""], ["Sak", "Hasim", ""]]}, {"id": "1610.10060", "submitter": "Alexandros Nathan", "authors": "Alexandros Nathan, Diego Klabjan", "title": "Optimization for Large-Scale Machine Learning with Distributed Features\n  and Observations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As the size of modern data sets exceeds the disk and memory capacities of a\nsingle computer, machine learning practitioners have resorted to parallel and\ndistributed computing. Given that optimization is one of the pillars of machine\nlearning and predictive modeling, distributed optimization methods have\nrecently garnered ample attention in the literature. Although previous research\nhas mostly focused on settings where either the observations, or features of\nthe problem at hand are stored in distributed fashion, the situation where both\nare partitioned across the nodes of a computer cluster (doubly distributed) has\nbarely been studied. In this work we propose two doubly distributed\noptimization algorithms. The first one falls under the umbrella of distributed\ndual coordinate ascent methods, while the second one belongs to the class of\nstochastic gradient/coordinate descent hybrid methods. We conduct numerical\nexperiments in Spark using real-world and simulated data sets and study the\nscaling properties of our methods. Our empirical evaluation of the proposed\nalgorithms demonstrates the out-performance of a block distributed ADMM method,\nwhich, to the best of our knowledge is the only other existing doubly\ndistributed optimization algorithm.\n", "versions": [{"version": "v1", "created": "Mon, 31 Oct 2016 18:43:21 GMT"}, {"version": "v2", "created": "Sat, 15 Apr 2017 01:10:43 GMT"}], "update_date": "2017-04-18", "authors_parsed": [["Nathan", "Alexandros", ""], ["Klabjan", "Diego", ""]]}, {"id": "1610.10087", "submitter": "Chuan-Yung Tsai", "authors": "Chuan-Yung Tsai, Andrew Saxe, David Cox", "title": "Tensor Switching Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a novel neural network algorithm, the Tensor Switching (TS)\nnetwork, which generalizes the Rectified Linear Unit (ReLU) nonlinearity to\ntensor-valued hidden units. The TS network copies its entire input vector to\ndifferent locations in an expanded representation, with the location determined\nby its hidden unit activity. In this way, even a simple linear readout from the\nTS representation can implement a highly expressive deep-network-like function.\nThe TS network hence avoids the vanishing gradient problem by construction, at\nthe cost of larger representation size. We develop several methods to train the\nTS network, including equivalent kernels for infinitely wide and deep TS\nnetworks, a one-pass linear learning algorithm, and two\nbackpropagation-inspired representation learning algorithms. Our experimental\nresults demonstrate that the TS network is indeed more expressive and\nconsistently learns faster than standard ReLU networks.\n", "versions": [{"version": "v1", "created": "Mon, 31 Oct 2016 19:44:50 GMT"}], "update_date": "2016-11-01", "authors_parsed": [["Tsai", "Chuan-Yung", ""], ["Saxe", "Andrew", ""], ["Cox", "David", ""]]}, {"id": "1610.10099", "submitter": "Nal Kalchbrenner", "authors": "Nal Kalchbrenner, Lasse Espeholt, Karen Simonyan, Aaron van den Oord,\n  Alex Graves, Koray Kavukcuoglu", "title": "Neural Machine Translation in Linear Time", "comments": "9 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a novel neural network for processing sequences. The ByteNet is a\none-dimensional convolutional neural network that is composed of two parts, one\nto encode the source sequence and the other to decode the target sequence. The\ntwo network parts are connected by stacking the decoder on top of the encoder\nand preserving the temporal resolution of the sequences. To address the\ndiffering lengths of the source and the target, we introduce an efficient\nmechanism by which the decoder is dynamically unfolded over the representation\nof the encoder. The ByteNet uses dilation in the convolutional layers to\nincrease its receptive field. The resulting network has two core properties: it\nruns in time that is linear in the length of the sequences and it sidesteps the\nneed for excessive memorization. The ByteNet decoder attains state-of-the-art\nperformance on character-level language modelling and outperforms the previous\nbest results obtained with recurrent networks. The ByteNet also achieves\nstate-of-the-art performance on character-to-character machine translation on\nthe English-to-German WMT translation task, surpassing comparable neural\ntranslation models that are based on recurrent networks with attentional\npooling and run in quadratic time. We find that the latent alignment structure\ncontained in the representations reflects the expected alignment between the\ntokens.\n", "versions": [{"version": "v1", "created": "Mon, 31 Oct 2016 19:56:39 GMT"}, {"version": "v2", "created": "Wed, 15 Mar 2017 18:09:51 GMT"}], "update_date": "2017-03-17", "authors_parsed": [["Kalchbrenner", "Nal", ""], ["Espeholt", "Lasse", ""], ["Simonyan", "Karen", ""], ["Oord", "Aaron van den", ""], ["Graves", "Alex", ""], ["Kavukcuoglu", "Koray", ""]]}]