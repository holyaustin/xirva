[{"id": "1411.0007", "submitter": "Michael Bloodgood", "authors": "John E. Miller, Michael Bloodgood, Manabu Torii and K. Vijay-Shanker", "title": "Rapid Adaptation of POS Tagging for Domain Specific Uses", "comments": "2 pages, 2 tables; appeared in Proceedings of the HLT-NAACL BioNLP\n  Workshop on Linking Natural Language and Biology, June 2006", "journal-ref": "In Proceedings of the HLT-NAACL BioNLP Workshop on Linking Natural\n  Language and Biology, pages 118-119, New York, New York, June 2006.\n  Association for Computational Linguistics", "doi": null, "report-no": null, "categories": "cs.CL cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Part-of-speech (POS) tagging is a fundamental component for performing\nnatural language tasks such as parsing, information extraction, and question\nanswering. When POS taggers are trained in one domain and applied in\nsignificantly different domains, their performance can degrade dramatically. We\npresent a methodology for rapid adaptation of POS taggers to new domains. Our\ntechnique is unsupervised in that a manually annotated corpus for the new\ndomain is not necessary. We use suffix information gathered from large amounts\nof raw text as well as orthographic information to increase the lexical\ncoverage. We present an experiment in the Biological domain where our POS\ntagger achieves results comparable to POS taggers specifically trained to this\ndomain.\n", "versions": [{"version": "v1", "created": "Fri, 31 Oct 2014 20:04:09 GMT"}], "update_date": "2014-11-04", "authors_parsed": [["Miller", "John E.", ""], ["Bloodgood", "Michael", ""], ["Torii", "Manabu", ""], ["Vijay-Shanker", "K.", ""]]}, {"id": "1411.0023", "submitter": "Eric Bax", "authors": "Ya Le, Eric Bax, Nicola Barbieri, David Garcia Soriano, Jitesh Mehta,\n  James Li", "title": "Validation of Matching", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a technique to compute probably approximately correct (PAC)\nbounds on precision and recall for matching algorithms. The bounds require some\nverified matches, but those matches may be used to develop the algorithms. The\nbounds can be applied to network reconciliation or entity resolution\nalgorithms, which identify nodes in different networks or values in a data set\nthat correspond to the same entity. For network reconciliation, the bounds do\nnot require knowledge of the network generation process.\n", "versions": [{"version": "v1", "created": "Fri, 31 Oct 2014 20:46:44 GMT"}, {"version": "v2", "created": "Mon, 11 Apr 2016 04:59:59 GMT"}], "update_date": "2016-04-12", "authors_parsed": [["Le", "Ya", ""], ["Bax", "Eric", ""], ["Barbieri", "Nicola", ""], ["Soriano", "David Garcia", ""], ["Mehta", "Jitesh", ""], ["Li", "James", ""]]}, {"id": "1411.0024", "submitter": "Vu Pham", "authors": "Vu Pham, Laurent El Ghaoui, Arturo Fernandez", "title": "Robust sketching for multiple square-root LASSO problems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.LG cs.SY stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many learning tasks, such as cross-validation, parameter search, or\nleave-one-out analysis, involve multiple instances of similar problems, each\ninstance sharing a large part of learning data with the others. We introduce a\nrobust framework for solving multiple square-root LASSO problems, based on a\nsketch of the learning data that uses low-rank approximations. Our approach\nallows a dramatic reduction in computational effort, in effect reducing the\nnumber of observations from $m$ (the number of observations to start with) to\n$k$ (the number of singular values retained in the low-rank model), while not\nsacrificing---sometimes even improving---the statistical performance.\nTheoretical analysis, as well as numerical experiments on both synthetic and\nreal data, illustrate the efficiency of the method in large scale applications.\n", "versions": [{"version": "v1", "created": "Thu, 30 Oct 2014 05:30:42 GMT"}], "update_date": "2014-11-04", "authors_parsed": [["Pham", "Vu", ""], ["Ghaoui", "Laurent El", ""], ["Fernandez", "Arturo", ""]]}, {"id": "1411.0161", "submitter": "Paul Honeine", "authors": "Paul Honeine", "title": "Entropy of Overcomplete Kernel Dictionaries", "comments": "10 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.CV cs.LG cs.NE math.IT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In signal analysis and synthesis, linear approximation theory considers a\nlinear decomposition of any given signal in a set of atoms, collected into a\nso-called dictionary. Relevant sparse representations are obtained by relaxing\nthe orthogonality condition of the atoms, yielding overcomplete dictionaries\nwith an extended number of atoms. More generally than the linear decomposition,\novercomplete kernel dictionaries provide an elegant nonlinear extension by\ndefining the atoms through a mapping kernel function (e.g., the gaussian\nkernel). Models based on such kernel dictionaries are used in neural networks,\ngaussian processes and online learning with kernels.\n  The quality of an overcomplete dictionary is evaluated with a diversity\nmeasure the distance, the approximation, the coherence and the Babel measures.\nIn this paper, we develop a framework to examine overcomplete kernel\ndictionaries with the entropy from information theory. Indeed, a higher value\nof the entropy is associated to a further uniform spread of the atoms over the\nspace. For each of the aforementioned diversity measures, we derive lower\nbounds on the entropy. Several definitions of the entropy are examined, with an\nextensive analysis in both the input space and the mapped feature space.\n", "versions": [{"version": "v1", "created": "Sat, 1 Nov 2014 19:41:14 GMT"}], "update_date": "2014-11-04", "authors_parsed": [["Honeine", "Paul", ""]]}, {"id": "1411.0169", "submitter": "Ilias Diakonikolas", "authors": "Siu-On Chan, Ilias Diakonikolas, Rocco A. Servedio, Xiaorui Sun", "title": "Near-Optimal Density Estimation in Near-Linear Time Using Variable-Width\n  Histograms", "comments": "conference version appears in NIPS 2014", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Let $p$ be an unknown and arbitrary probability distribution over $[0,1)$. We\nconsider the problem of {\\em density estimation}, in which a learning algorithm\nis given i.i.d. draws from $p$ and must (with high probability) output a\nhypothesis distribution that is close to $p$. The main contribution of this\npaper is a highly efficient density estimation algorithm for learning using a\nvariable-width histogram, i.e., a hypothesis distribution with a piecewise\nconstant probability density function.\n  In more detail, for any $k$ and $\\epsilon$, we give an algorithm that makes\n$\\tilde{O}(k/\\epsilon^2)$ draws from $p$, runs in $\\tilde{O}(k/\\epsilon^2)$\ntime, and outputs a hypothesis distribution $h$ that is piecewise constant with\n$O(k \\log^2(1/\\epsilon))$ pieces. With high probability the hypothesis $h$\nsatisfies $d_{\\mathrm{TV}}(p,h) \\leq C \\cdot \\mathrm{opt}_k(p) + \\epsilon$,\nwhere $d_{\\mathrm{TV}}$ denotes the total variation distance (statistical\ndistance), $C$ is a universal constant, and $\\mathrm{opt}_k(p)$ is the smallest\ntotal variation distance between $p$ and any $k$-piecewise constant\ndistribution. The sample size and running time of our algorithm are optimal up\nto logarithmic factors. The \"approximation factor\" $C$ in our result is\ninherent in the problem, as we prove that no algorithm with sample size bounded\nin terms of $k$ and $\\epsilon$ can achieve $C<2$ regardless of what kind of\nhypothesis distribution it uses.\n", "versions": [{"version": "v1", "created": "Sat, 1 Nov 2014 21:03:59 GMT"}], "update_date": "2014-11-04", "authors_parsed": [["Chan", "Siu-On", ""], ["Diakonikolas", "Ilias", ""], ["Servedio", "Rocco A.", ""], ["Sun", "Xiaorui", ""]]}, {"id": "1411.0189", "submitter": "Xinquan Chen", "authors": "Xinquan Chen", "title": "Synchronization Clustering based on a Linearized Version of Vicsek model", "comments": "37 pages, 9 figures, 3 tabels, 27 conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a kind of effective synchronization clustering method\nbased on a linearized version of Vicsek model. This method can be represented\nby an Effective Synchronization Clustering algorithm (ESynC), an Improved\nversion of ESynC algorithm (IESynC), a Shrinking Synchronization Clustering\nalgorithm based on another linear Vicsek model (SSynC), and an effective\nMulti-level Synchronization Clustering algorithm (MSynC). After some analysis\nand comparisions, we find that ESynC algorithm based on the Linearized version\nof the Vicsek model has better synchronization effect than SynC algorithm based\non an extensive Kuramoto model and a similar synchronization clustering\nalgorithm based on the original Vicsek model. By simulated experiments of some\nartificial data sets, we observe that ESynC algorithm, IESynC algorithm, and\nSSynC algorithm can get better synchronization effect although it needs less\niterative times and less time than SynC algorithm. In some simulations, we also\nobserve that IESynC algorithm and SSynC algorithm can get some improvements in\ntime cost than ESynC algorithm. At last, it gives some research expectations to\npopularize this algorithm.\n", "versions": [{"version": "v1", "created": "Sun, 2 Nov 2014 01:09:00 GMT"}], "update_date": "2014-11-04", "authors_parsed": [["Chen", "Xinquan", ""]]}, {"id": "1411.0292", "submitter": "Alp Kucukelbir", "authors": "Alp Kucukelbir, David M. Blei", "title": "Population Empirical Bayes", "comments": "UAI 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bayesian predictive inference analyzes a dataset to make predictions about\nnew observations. When a model does not match the data, predictive accuracy\nsuffers. We develop population empirical Bayes (POP-EB), a hierarchical\nframework that explicitly models the empirical population distribution as part\nof Bayesian analysis. We introduce a new concept, the latent dataset, as a\nhierarchical variable and set the empirical population as its prior. This leads\nto a new predictive density that mitigates model mismatch. We efficiently apply\nthis method to complex models by proposing a stochastic variational inference\nalgorithm, called bumping variational inference (BUMP-VI). We demonstrate\nimproved predictive accuracy over classical Bayesian inference in three models:\na linear regression model of health data, a Bayesian mixture model of natural\nimages, and a latent Dirichlet allocation topic model of scientific documents.\n", "versions": [{"version": "v1", "created": "Sun, 2 Nov 2014 18:50:14 GMT"}, {"version": "v2", "created": "Mon, 8 Jun 2015 21:36:22 GMT"}], "update_date": "2015-06-10", "authors_parsed": [["Kucukelbir", "Alp", ""], ["Blei", "David M.", ""]]}, {"id": "1411.0296", "submitter": "Aasa Feragen", "authors": "Aasa Feragen, Francois Lauze, S{\\o}ren Hauberg", "title": "Geodesic Exponential Kernels: When Curvature and Linearity Conflict", "comments": "13 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider kernel methods on general geodesic metric spaces and provide both\nnegative and positive results. First we show that the common Gaussian kernel\ncan only be generalized to a positive definite kernel on a geodesic metric\nspace if the space is flat. As a result, for data on a Riemannian manifold, the\ngeodesic Gaussian kernel is only positive definite if the Riemannian manifold\nis Euclidean. This implies that any attempt to design geodesic Gaussian kernels\non curved Riemannian manifolds is futile. However, we show that for spaces with\nconditionally negative definite distances the geodesic Laplacian kernel can be\ngeneralized while retaining positive definiteness. This implies that geodesic\nLaplacian kernels can be generalized to some curved spaces, including spheres\nand hyperbolic spaces. Our theoretical results are verified empirically.\n", "versions": [{"version": "v1", "created": "Sun, 2 Nov 2014 19:08:14 GMT"}, {"version": "v2", "created": "Mon, 17 Nov 2014 06:22:19 GMT"}], "update_date": "2014-11-18", "authors_parsed": [["Feragen", "Aasa", ""], ["Lauze", "Francois", ""], ["Hauberg", "S\u00f8ren", ""]]}, {"id": "1411.0306", "submitter": "Ahmed El Alaoui", "authors": "Ahmed El Alaoui, Michael W. Mahoney", "title": "Fast Randomized Kernel Methods With Statistical Guarantees", "comments": "Improved presentation. Technical details fixed. A conference version\n  of this paper appears in NIPS15 under the modified title \"Fast Randomized\n  Kernel Ridge Regression with Statistical Guarantees\"", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One approach to improving the running time of kernel-based machine learning\nmethods is to build a small sketch of the input and use it in lieu of the full\nkernel matrix in the machine learning task of interest. Here, we describe a\nversion of this approach that comes with running time guarantees as well as\nimproved guarantees on its statistical performance. By extending the notion of\n\\emph{statistical leverage scores} to the setting of kernel ridge regression,\nour main statistical result is to identify an importance sampling distribution\nthat reduces the size of the sketch (i.e., the required number of columns to be\nsampled) to the \\emph{effective dimensionality} of the problem. This quantity\nis often much smaller than previous bounds that depend on the \\emph{maximal\ndegrees of freedom}. Our main algorithmic result is to present a fast algorithm\nto compute approximations to these scores. This algorithm runs in time that is\nlinear in the number of samples---more precisely, the running time is\n$O(np^2)$, where the parameter $p$ depends only on the trace of the kernel\nmatrix and the regularization parameter---and it can be applied to the matrix\nof feature vectors, without having to form the full kernel matrix. This is\nobtained via a variant of length-squared sampling that we adapt to the kernel\nsetting in a way that is of independent interest. Lastly, we provide empirical\nresults illustrating our theory, and we discuss how this new notion of the\nstatistical leverage of a data point captures in a fine way the difficulty of\nthe original statistical learning problem.\n", "versions": [{"version": "v1", "created": "Sun, 2 Nov 2014 19:57:31 GMT"}, {"version": "v2", "created": "Mon, 12 Oct 2015 02:20:23 GMT"}, {"version": "v3", "created": "Sun, 8 Nov 2015 20:33:45 GMT"}], "update_date": "2015-11-10", "authors_parsed": [["Alaoui", "Ahmed El", ""], ["Mahoney", "Michael W.", ""]]}, {"id": "1411.0347", "submitter": "Mert Pilanci", "authors": "Mert Pilanci and Martin J. Wainwright", "title": "Iterative Hessian sketch: Fast and accurate solution approximation for\n  constrained least-squares", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.IT cs.LG math.IT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study randomized sketching methods for approximately solving least-squares\nproblem with a general convex constraint. The quality of a least-squares\napproximation can be assessed in different ways: either in terms of the value\nof the quadratic objective function (cost approximation), or in terms of some\ndistance measure between the approximate minimizer and the true minimizer\n(solution approximation). Focusing on the latter criterion, our first main\nresult provides a general lower bound on any randomized method that sketches\nboth the data matrix and vector in a least-squares problem; as a surprising\nconsequence, the most widely used least-squares sketch is sub-optimal for\nsolution approximation. We then present a new method known as the iterative\nHessian sketch, and show that it can be used to obtain approximations to the\noriginal least-squares problem using a projection dimension proportional to the\nstatistical complexity of the least-squares minimizer, and a logarithmic number\nof iterations. We illustrate our general theory with simulations for both\nunconstrained and constrained versions of least-squares, including\n$\\ell_1$-regularization and nuclear norm constraints. We also numerically\ndemonstrate the practicality of our approach in a real face expression\nclassification experiment.\n", "versions": [{"version": "v1", "created": "Mon, 3 Nov 2014 02:59:39 GMT"}], "update_date": "2014-11-04", "authors_parsed": [["Pilanci", "Mert", ""], ["Wainwright", "Martin J.", ""]]}, {"id": "1411.0541", "submitter": "Baharan Mirzasoleiman", "authors": "Baharan Mirzasoleiman, Amin Karbasi, Rik Sarkar, and Andreas Krause", "title": "Distributed Submodular Maximization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.DC cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many large-scale machine learning problems--clustering, non-parametric\nlearning, kernel machines, etc.--require selecting a small yet representative\nsubset from a large dataset. Such problems can often be reduced to maximizing a\nsubmodular set function subject to various constraints. Classical approaches to\nsubmodular optimization require centralized access to the full dataset, which\nis impractical for truly large-scale problems. In this paper, we consider the\nproblem of submodular function maximization in a distributed fashion. We\ndevelop a simple, two-stage protocol GreeDi, that is easily implemented using\nMapReduce style computations. We theoretically analyze our approach, and show\nthat under certain natural conditions, performance close to the centralized\napproach can be achieved. We begin with monotone submodular maximization\nsubject to a cardinality constraint, and then extend this approach to obtain\napproximation guarantees for (not necessarily monotone) submodular maximization\nsubject to more general constraints including matroid or knapsack constraints.\nIn our extensive experiments, we demonstrate the effectiveness of our approach\non several applications, including sparse Gaussian process inference and\nexemplar based clustering on tens of millions of examples using Hadoop.\n", "versions": [{"version": "v1", "created": "Mon, 3 Nov 2014 16:03:05 GMT"}, {"version": "v2", "created": "Mon, 27 Jun 2016 16:32:35 GMT"}], "update_date": "2016-06-28", "authors_parsed": [["Mirzasoleiman", "Baharan", ""], ["Karbasi", "Amin", ""], ["Sarkar", "Rik", ""], ["Krause", "Andreas", ""]]}, {"id": "1411.0547", "submitter": "Gregory Puleo", "authors": "Gregory J. Puleo, Olgica Milenkovic", "title": "Correlation Clustering with Constrained Cluster Sizes and Extended\n  Weights Bounds", "comments": "17 pages, simplified the last section and fixed some other minor\n  errors", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of correlation clustering on graphs with constraints\non both the cluster sizes and the positive and negative weights of edges. Our\ncontributions are twofold: First, we introduce the problem of correlation\nclustering with bounded cluster sizes. Second, we extend the regime of weight\nvalues for which the clustering may be performed with constant approximation\nguarantees in polynomial time and apply the results to the bounded cluster size\nproblem.\n", "versions": [{"version": "v1", "created": "Mon, 3 Nov 2014 16:17:31 GMT"}, {"version": "v2", "created": "Mon, 2 Feb 2015 15:26:44 GMT"}, {"version": "v3", "created": "Fri, 22 May 2015 16:41:01 GMT"}], "update_date": "2015-05-25", "authors_parsed": [["Puleo", "Gregory J.", ""], ["Milenkovic", "Olgica", ""]]}, {"id": "1411.0591", "submitter": "Charles Fisher", "authors": "Charles K. Fisher and Pankaj Mehta", "title": "Bayesian feature selection with strongly-regularizing priors maps to the\n  Ising Model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cond-mat.stat-mech cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Identifying small subsets of features that are relevant for prediction and/or\nclassification tasks is a central problem in machine learning and statistics.\nThe feature selection task is especially important, and computationally\ndifficult, for modern datasets where the number of features can be comparable\nto, or even exceed, the number of samples. Here, we show that feature selection\nwith Bayesian inference takes a universal form and reduces to calculating the\nmagnetizations of an Ising model, under some mild conditions. Our results\nexploit the observation that the evidence takes a universal form for\nstrongly-regularizing priors --- priors that have a large effect on the\nposterior probability even in the infinite data limit. We derive explicit\nexpressions for feature selection for generalized linear models, a large class\nof statistical techniques that include linear and logistic regression. We\nillustrate the power of our approach by analyzing feature selection in a\nlogistic regression-based classifier trained to distinguish between the letters\nB and D in the notMNIST dataset.\n", "versions": [{"version": "v1", "created": "Mon, 3 Nov 2014 18:15:29 GMT"}], "update_date": "2014-11-04", "authors_parsed": [["Fisher", "Charles K.", ""], ["Mehta", "Pankaj", ""]]}, {"id": "1411.0602", "submitter": "Venu  Satuluri", "authors": "Sebastian Schelter, Venu Satuluri, Reza Zadeh", "title": "Factorbird - a Parameter Server Approach to Distributed Matrix\n  Factorization", "comments": "10 pages. Submitted to the NIPS 2014 Workshop on Distributed Matrix\n  Computations", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present Factorbird, a prototype of a parameter server approach for\nfactorizing large matrices with Stochastic Gradient Descent-based algorithms.\nWe designed Factorbird to meet the following desiderata: (a) scalability to\ntall and wide matrices with dozens of billions of non-zeros, (b) extensibility\nto different kinds of models and loss functions as long as they can be\noptimized using Stochastic Gradient Descent (SGD), and (c) adaptability to both\nbatch and streaming scenarios. Factorbird uses a parameter server in order to\nscale to models that exceed the memory of an individual machine, and employs\nlock-free Hogwild!-style learning with a special partitioning scheme to\ndrastically reduce conflicting updates. We also discuss other aspects of the\ndesign of our system such as how to efficiently grid search for hyperparameters\nat scale. We present experiments of Factorbird on a matrix built from a subset\nof Twitter's interaction graph, consisting of more than 38 billion non-zeros\nand about 200 million rows and columns, which is to the best of our knowledge\nthe largest matrix on which factorization results have been reported in the\nliterature.\n", "versions": [{"version": "v1", "created": "Mon, 3 Nov 2014 18:49:25 GMT"}], "update_date": "2014-11-04", "authors_parsed": [["Schelter", "Sebastian", ""], ["Satuluri", "Venu", ""], ["Zadeh", "Reza", ""]]}, {"id": "1411.0630", "submitter": "Aram Galstyan", "authors": "Armen E. Allahverdyan and Aram Galstyan", "title": "Active Inference for Binary Symmetric Hidden Markov Models", "comments": "9 pages, 3 figures", "journal-ref": null, "doi": "10.1007/s10955-015-1321-y", "report-no": null, "categories": "stat.ML cond-mat.dis-nn cs.IT cs.LG math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider active maximum a posteriori (MAP) inference problem for Hidden\nMarkov Models (HMM), where, given an initial MAP estimate of the hidden\nsequence, we select to label certain states in the sequence to improve the\nestimation accuracy of the remaining states. We develop an analytical approach\nto this problem for the case of binary symmetric HMMs, and obtain a closed form\nsolution that relates the expected error reduction to model parameters under\nthe specified active inference scheme. We then use this solution to determine\nmost optimal active inference scheme in terms of error reduction, and examine\nthe relation of those schemes to heuristic principles of uncertainty reduction\nand solution unicity.\n", "versions": [{"version": "v1", "created": "Mon, 3 Nov 2014 19:46:07 GMT"}], "update_date": "2015-09-02", "authors_parsed": [["Allahverdyan", "Armen E.", ""], ["Galstyan", "Aram", ""]]}, {"id": "1411.0652", "submitter": "Emilio Ferrara", "authors": "Mohsen JafariAsbagh, Emilio Ferrara, Onur Varol, Filippo Menczer,\n  Alessandro Flammini", "title": "Clustering memes in social media streams", "comments": "25 pages, 8 figures, accepted on Social Network Analysis and Mining\n  (SNAM). The final publication is available at Springer via\n  http://dx.doi.org/10.1007/s13278-014-0237-x", "journal-ref": "Social Network Analysis and Mining, 4(1), 1-13. 2014", "doi": "10.1007/s13278-014-0237-x", "report-no": null, "categories": "cs.SI cs.CY cs.LG physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The problem of clustering content in social media has pervasive applications,\nincluding the identification of discussion topics, event detection, and content\nrecommendation. Here we describe a streaming framework for online detection and\nclustering of memes in social media, specifically Twitter. A pre-clustering\nprocedure, namely protomeme detection, first isolates atomic tokens of\ninformation carried by the tweets. Protomemes are thereafter aggregated, based\non multiple similarity measures, to obtain memes as cohesive groups of tweets\nreflecting actual concepts or topics of discussion. The clustering algorithm\ntakes into account various dimensions of the data and metadata, including\nnatural language, the social network, and the patterns of information\ndiffusion. As a result, our system can build clusters of semantically,\nstructurally, and topically related tweets. The clustering process is based on\na variant of Online K-means that incorporates a memory mechanism, used to\n\"forget\" old memes and replace them over time with the new ones. The evaluation\nof our framework is carried out by using a dataset of Twitter trending topics.\nOver a one-week period, we systematically determined whether our algorithm was\nable to recover the trending hashtags. We show that the proposed method\noutperforms baseline algorithms that only use content features, as well as a\nstate-of-the-art event detection method that assumes full knowledge of the\nunderlying follower network. We finally show that our online learning framework\nis flexible, due to its independence of the adopted clustering algorithm, and\nbest suited to work in a streaming scenario.\n", "versions": [{"version": "v1", "created": "Mon, 3 Nov 2014 20:41:00 GMT"}], "update_date": "2017-03-07", "authors_parsed": [["JafariAsbagh", "Mohsen", ""], ["Ferrara", "Emilio", ""], ["Varol", "Onur", ""], ["Menczer", "Filippo", ""], ["Flammini", "Alessandro", ""]]}, {"id": "1411.0728", "submitter": "Dileep Kalathil", "authors": "Dileep Kalathil, Vivek Borkar, Rahul Jain", "title": "Approachability in Stackelberg Stochastic Games with Vector Costs", "comments": "18 Pages, Submitted to Dynamic Games and Applications", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.GT cs.SY math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The notion of approachability was introduced by Blackwell [1] in the context\nof vector-valued repeated games. The famous Blackwell's approachability theorem\nprescribes a strategy for approachability, i.e., for `steering' the average\ncost of a given agent towards a given target set, irrespective of the\nstrategies of the other agents. In this paper, motivated by the multi-objective\noptimization/decision making problems in dynamically changing environments, we\naddress the approachability problem in Stackelberg stochastic games with vector\nvalued cost functions. We make two main contributions. Firstly, we give a\nsimple and computationally tractable strategy for approachability for\nStackelberg stochastic games along the lines of Blackwell's. Secondly, we give\na reinforcement learning algorithm for learning the approachable strategy when\nthe transition kernel is unknown. We also recover as a by-product Blackwell's\nnecessary and sufficient condition for approachability for convex sets in this\nset up and thus a complete characterization. We also give sufficient conditions\nfor non-convex sets.\n", "versions": [{"version": "v1", "created": "Mon, 3 Nov 2014 22:59:11 GMT"}, {"version": "v2", "created": "Mon, 20 Jun 2016 18:05:17 GMT"}, {"version": "v3", "created": "Tue, 21 Jun 2016 00:43:57 GMT"}], "update_date": "2016-06-22", "authors_parsed": [["Kalathil", "Dileep", ""], ["Borkar", "Vivek", ""], ["Jain", "Rahul", ""]]}, {"id": "1411.0860", "submitter": "Miao Xu", "authors": "Miao Xu, Rong Jin, Zhi-Hua Zhou", "title": "CUR Algorithm for Partially Observed Matrices", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  CUR matrix decomposition computes the low rank approximation of a given\nmatrix by using the actual rows and columns of the matrix. It has been a very\nuseful tool for handling large matrices. One limitation with the existing\nalgorithms for CUR matrix decomposition is that they need an access to the {\\it\nfull} matrix, a requirement that can be difficult to fulfill in many real world\napplications. In this work, we alleviate this limitation by developing a CUR\ndecomposition algorithm for partially observed matrices. In particular, the\nproposed algorithm computes the low rank approximation of the target matrix\nbased on (i) the randomly sampled rows and columns, and (ii) a subset of\nobserved entries that are randomly sampled from the matrix. Our analysis shows\nthe relative error bound, measured by spectral norm, for the proposed algorithm\nwhen the target matrix is of full rank. We also show that only $O(n r\\ln r)$\nobserved entries are needed by the proposed algorithm to perfectly recover a\nrank $r$ matrix of size $n\\times n$, which improves the sample complexity of\nthe existing algorithms for matrix completion. Empirical studies on both\nsynthetic and real-world datasets verify our theoretical claims and demonstrate\nthe effectiveness of the proposed algorithm.\n", "versions": [{"version": "v1", "created": "Tue, 4 Nov 2014 11:03:50 GMT"}], "update_date": "2014-11-05", "authors_parsed": [["Xu", "Miao", ""], ["Jin", "Rong", ""], ["Zhou", "Zhi-Hua", ""]]}, {"id": "1411.0972", "submitter": "Stephen Becker", "authors": "Volkan Cevher and Stephen Becker and Mark Schmidt", "title": "Convex Optimization for Big Data", "comments": "23 pages, 4 figurs, 8 algorithms", "journal-ref": "IEEE Signal Processing Magazine, Vol. 31(5), pages 32--43, 2014", "doi": "10.1109/MSP.2014.2329397", "report-no": null, "categories": "math.OC cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article reviews recent advances in convex optimization algorithms for\nBig Data, which aim to reduce the computational, storage, and communications\nbottlenecks. We provide an overview of this emerging field, describe\ncontemporary approximation techniques like first-order methods and\nrandomization for scalability, and survey the important role of parallel and\ndistributed computation. The new Big Data algorithms are based on surprisingly\nsimple principles and attain staggering accelerations even on classical\nproblems.\n", "versions": [{"version": "v1", "created": "Tue, 4 Nov 2014 17:14:27 GMT"}], "update_date": "2014-11-05", "authors_parsed": [["Cevher", "Volkan", ""], ["Becker", "Stephen", ""], ["Schmidt", "Mark", ""]]}, {"id": "1411.0997", "submitter": "Erin Pearse", "authors": "Chad Eckman, Jonathan A. Lindgren, Erin P. J. Pearse, David J. Sacco,\n  Zachariah Zhang", "title": "Iterated geometric harmonics for data imputation and reconstruction of\n  missing data", "comments": "13 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The method of geometric harmonics is adapted to the situation of incomplete\ndata by means of the iterated geometric harmonics (IGH) scheme. The method is\ntested on natural and synthetic data sets with 50--500 data points and\ndimensionality of 400--10,000. Experiments suggest that the algorithm converges\nto a near optimal solution within 4--6 iterations, at runtimes of less than 30\nminutes on a medium-grade desktop computer. The imputation of missing data\nvalues is applied to collections of damaged images (suffering from data\nannihilation rates of up to 70\\%) which are reconstructed with a surprising\ndegree of accuracy.\n", "versions": [{"version": "v1", "created": "Tue, 4 Nov 2014 18:46:34 GMT"}], "update_date": "2014-11-05", "authors_parsed": [["Eckman", "Chad", ""], ["Lindgren", "Jonathan A.", ""], ["Pearse", "Erin P. J.", ""], ["Sacco", "David J.", ""], ["Zhang", "Zachariah", ""]]}, {"id": "1411.1076", "submitter": "Andrea Montanari", "authors": "Andrea Montanari and Emile Richard", "title": "A statistical model for tensor PCA", "comments": "Neural Information Processing Systems (NIPS) 2014 (slightly expanded:\n  30 pages, 6 figures)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.IT math.IT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the Principal Component Analysis problem for large tensors of\narbitrary order $k$ under a single-spike (or rank-one plus noise) model. On the\none hand, we use information theory, and recent results in probability theory,\nto establish necessary and sufficient conditions under which the principal\ncomponent can be estimated using unbounded computational resources. It turns\nout that this is possible as soon as the signal-to-noise ratio $\\beta$ becomes\nlarger than $C\\sqrt{k\\log k}$ (and in particular $\\beta$ can remain bounded as\nthe problem dimensions increase).\n  On the other hand, we analyze several polynomial-time estimation algorithms,\nbased on tensor unfolding, power iteration and message passing ideas from\ngraphical models. We show that, unless the signal-to-noise ratio diverges in\nthe system dimensions, none of these approaches succeeds. This is possibly\nrelated to a fundamental limitation of computationally tractable estimators for\nthis problem.\n  We discuss various initializations for tensor power iteration, and show that\na tractable initialization based on the spectrum of the matricized tensor\noutperforms significantly baseline methods, statistically and computationally.\nFinally, we consider the case in which additional side information is available\nabout the unknown signal. We characterize the amount of side information that\nallows the iterative algorithms to converge to a good estimate.\n", "versions": [{"version": "v1", "created": "Tue, 4 Nov 2014 21:01:56 GMT"}], "update_date": "2014-11-06", "authors_parsed": [["Montanari", "Andrea", ""], ["Richard", "Emile", ""]]}, {"id": "1411.1087", "submitter": "Praneeth Netrapalli", "authors": "Prateek Jain and Praneeth Netrapalli", "title": "Fast Exact Matrix Completion with Finite Samples", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NA cs.DS cs.IT cs.LG math.IT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Matrix completion is the problem of recovering a low rank matrix by observing\na small fraction of its entries. A series of recent works [KOM12,JNS13,HW14]\nhave proposed fast non-convex optimization based iterative algorithms to solve\nthis problem. However, the sample complexity in all these results is\nsub-optimal in its dependence on the rank, condition number and the desired\naccuracy.\n  In this paper, we present a fast iterative algorithm that solves the matrix\ncompletion problem by observing $O(nr^5 \\log^3 n)$ entries, which is\nindependent of the condition number and the desired accuracy. The run time of\nour algorithm is $O(nr^7\\log^3 n\\log 1/\\epsilon)$ which is near linear in the\ndimension of the matrix. To the best of our knowledge, this is the first near\nlinear time algorithm for exact matrix completion with finite sample complexity\n(i.e. independent of $\\epsilon$).\n  Our algorithm is based on a well known projected gradient descent method,\nwhere the projection is onto the (non-convex) set of low rank matrices. There\nare two key ideas in our result: 1) our argument is based on a $\\ell_{\\infty}$\nnorm potential function (as opposed to the spectral norm) and provides a novel\nway to obtain perturbation bounds for it. 2) we prove and use a natural\nextension of the Davis-Kahan theorem to obtain perturbation bounds on the best\nlow rank approximation of matrices with good eigen-gap. Both of these ideas may\nbe of independent interest.\n", "versions": [{"version": "v1", "created": "Tue, 4 Nov 2014 21:16:23 GMT"}], "update_date": "2014-11-06", "authors_parsed": [["Jain", "Prateek", ""], ["Netrapalli", "Praneeth", ""]]}, {"id": "1411.1088", "submitter": "Jennifer Gillenwater", "authors": "Jennifer Gillenwater, Alex Kulesza, Emily Fox, Ben Taskar", "title": "Expectation-Maximization for Learning Determinantal Point Processes", "comments": null, "journal-ref": "Neural Information Processing Systems (NIPS), 2014", "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A determinantal point process (DPP) is a probabilistic model of set diversity\ncompactly parameterized by a positive semi-definite kernel matrix. To fit a DPP\nto a given task, we would like to learn the entries of its kernel matrix by\nmaximizing the log-likelihood of the available data. However, log-likelihood is\nnon-convex in the entries of the kernel matrix, and this learning problem is\nconjectured to be NP-hard. Thus, previous work has instead focused on more\nrestricted convex learning settings: learning only a single weight for each row\nof the kernel matrix, or learning weights for a linear combination of DPPs with\nfixed kernel matrices. In this work we propose a novel algorithm for learning\nthe full kernel matrix. By changing the kernel parameterization from matrix\nentries to eigenvalues and eigenvectors, and then lower-bounding the likelihood\nin the manner of expectation-maximization algorithms, we obtain an effective\noptimization procedure. We test our method on a real-world product\nrecommendation task, and achieve relative gains of up to 16.5% in test\nlog-likelihood compared to the naive approach of maximizing likelihood by\nprojected gradient ascent on the entries of the kernel matrix.\n", "versions": [{"version": "v1", "created": "Tue, 4 Nov 2014 21:23:35 GMT"}], "update_date": "2014-11-06", "authors_parsed": [["Gillenwater", "Jennifer", ""], ["Kulesza", "Alex", ""], ["Fox", "Emily", ""], ["Taskar", "Ben", ""]]}, {"id": "1411.1091", "submitter": "Jonathan Long", "authors": "Jonathan Long, Ning Zhang, Trevor Darrell", "title": "Do Convnets Learn Correspondence?", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolutional neural nets (convnets) trained from massive labeled datasets\nhave substantially improved the state-of-the-art in image classification and\nobject detection. However, visual understanding requires establishing\ncorrespondence on a finer level than object category. Given their large pooling\nregions and training from whole-image labels, it is not clear that convnets\nderive their success from an accurate correspondence model which could be used\nfor precise localization. In this paper, we study the effectiveness of convnet\nactivation features for tasks requiring correspondence. We present evidence\nthat convnet features localize at a much finer scale than their receptive field\nsizes, that they can be used to perform intraclass alignment as well as\nconventional hand-engineered features, and that they outperform conventional\nfeatures in keypoint prediction on objects from PASCAL VOC 2011.\n", "versions": [{"version": "v1", "created": "Tue, 4 Nov 2014 21:35:55 GMT"}], "update_date": "2015-03-10", "authors_parsed": [["Long", "Jonathan", ""], ["Zhang", "Ning", ""], ["Darrell", "Trevor", ""]]}, {"id": "1411.1119", "submitter": "Justin Domke", "authors": "Xianghang Liu and Justin Domke", "title": "Projecting Markov Random Field Parameters for Fast Mixing", "comments": "Neural Information Processing Systems 2014", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Markov chain Monte Carlo (MCMC) algorithms are simple and extremely powerful\ntechniques to sample from almost arbitrary distributions. The flaw in practice\nis that it can take a large and/or unknown amount of time to converge to the\nstationary distribution. This paper gives sufficient conditions to guarantee\nthat univariate Gibbs sampling on Markov Random Fields (MRFs) will be fast\nmixing, in a precise sense. Further, an algorithm is given to project onto this\nset of fast-mixing parameters in the Euclidean norm. Following recent work, we\ngive an example use of this to project in various divergence measures,\ncomparing univariate marginals obtained by sampling after projection to common\nvariational methods and Gibbs sampling on the original parameters.\n", "versions": [{"version": "v1", "created": "Wed, 5 Nov 2014 00:43:08 GMT"}, {"version": "v2", "created": "Fri, 7 Nov 2014 05:38:17 GMT"}, {"version": "v3", "created": "Wed, 12 Nov 2014 00:05:12 GMT"}], "update_date": "2014-11-13", "authors_parsed": [["Liu", "Xianghang", ""], ["Domke", "Justin", ""]]}, {"id": "1411.1125", "submitter": "Rodrigo de Lamare", "authors": "S. Xu, R. C. de Lamare and H. V. Poor", "title": "Distributed Low-Rank Estimation Based on Joint Iterative Optimization in\n  Wireless Sensor Networks", "comments": "5 figures, 6 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.LG math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a novel distributed reduced--rank scheme and an adaptive\nalgorithm for distributed estimation in wireless sensor networks. The proposed\ndistributed scheme is based on a transformation that performs dimensionality\nreduction at each agent of the network followed by a reduced-dimension\nparameter vector. A distributed reduced-rank joint iterative estimation\nalgorithm is developed, which has the ability to achieve significantly reduced\ncommunication overhead and improved performance when compared with existing\ntechniques. Simulation results illustrate the advantages of the proposed\nstrategy in terms of convergence rate and mean square error performance.\n", "versions": [{"version": "v1", "created": "Wed, 5 Nov 2014 01:12:17 GMT"}], "update_date": "2014-11-06", "authors_parsed": [["Xu", "S.", ""], ["de Lamare", "R. C.", ""], ["Poor", "H. V.", ""]]}, {"id": "1411.1134", "submitter": "Christopher De Sa", "authors": "Christopher De Sa, Kunle Olukotun, and Christopher R\\'e", "title": "Global Convergence of Stochastic Gradient Descent for Some Non-convex\n  Matrix Problems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Stochastic gradient descent (SGD) on a low-rank factorization is commonly\nemployed to speed up matrix problems including matrix completion, subspace\ntracking, and SDP relaxation. In this paper, we exhibit a step size scheme for\nSGD on a low-rank least-squares problem, and we prove that, under broad\nsampling conditions, our method converges globally from a random starting point\nwithin $O(\\epsilon^{-1} n \\log n)$ steps with constant probability for\nconstant-rank problems. Our modification of SGD relates it to stochastic power\niteration. We also show experiments to illustrate the runtime and convergence\nof the algorithm.\n", "versions": [{"version": "v1", "created": "Wed, 5 Nov 2014 03:05:43 GMT"}, {"version": "v2", "created": "Thu, 6 Nov 2014 03:10:29 GMT"}, {"version": "v3", "created": "Tue, 10 Feb 2015 20:19:28 GMT"}], "update_date": "2015-02-11", "authors_parsed": [["De Sa", "Christopher", ""], ["Olukotun", "Kunle", ""], ["R\u00e9", "Christopher", ""]]}, {"id": "1411.1147", "submitter": "Waleed Ammar", "authors": "Waleed Ammar, Chris Dyer, Noah A. Smith", "title": "Conditional Random Field Autoencoders for Unsupervised Structured\n  Prediction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a framework for unsupervised learning of structured predictors\nwith overlapping, global features. Each input's latent representation is\npredicted conditional on the observable data using a feature-rich conditional\nrandom field. Then a reconstruction of the input is (re)generated, conditional\non the latent structure, using models for which maximum likelihood estimation\nhas a closed-form. Our autoencoder formulation enables efficient learning\nwithout making unrealistic independence assumptions or restricting the kinds of\nfeatures that can be used. We illustrate insightful connections to traditional\nautoencoders, posterior regularization and multi-view learning. We show\ncompetitive results with instantiations of the model for two canonical NLP\ntasks: part-of-speech induction and bitext word alignment, and show that\ntraining our model can be substantially more efficient than comparable\nfeature-rich baselines.\n", "versions": [{"version": "v1", "created": "Wed, 5 Nov 2014 04:49:38 GMT"}, {"version": "v2", "created": "Mon, 10 Nov 2014 05:58:04 GMT"}], "update_date": "2014-11-11", "authors_parsed": [["Ammar", "Waleed", ""], ["Dyer", "Chris", ""], ["Smith", "Noah A.", ""]]}, {"id": "1411.1158", "submitter": "Ohad Shamir", "authors": "Nicol\\`o Cesa-Bianchi, Yishay Mansour and Ohad Shamir", "title": "On the Complexity of Learning with Kernels", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A well-recognized limitation of kernel learning is the requirement to handle\na kernel matrix, whose size is quadratic in the number of training examples.\nMany methods have been proposed to reduce this computational cost, mostly by\nusing a subset of the kernel matrix entries, or some form of low-rank matrix\napproximation, or a random projection method. In this paper, we study lower\nbounds on the error attainable by such methods as a function of the number of\nentries observed in the kernel matrix or the rank of an approximate kernel\nmatrix. We show that there are kernel learning problems where no such method\nwill lead to non-trivial computational savings. Our results also quantify how\nthe problem difficulty depends on parameters such as the nature of the loss\nfunction, the regularization parameter, the norm of the desired predictor, and\nthe kernel matrix rank. Our results also suggest cases where more efficient\nkernel learning might be possible.\n", "versions": [{"version": "v1", "created": "Wed, 5 Nov 2014 06:18:14 GMT"}], "update_date": "2014-11-06", "authors_parsed": [["Cesa-Bianchi", "Nicol\u00f2", ""], ["Mansour", "Yishay", ""], ["Shamir", "Ohad", ""]]}, {"id": "1411.1316", "submitter": "David Buckley", "authors": "David Buckley, Ke Chen, Joshua Knowles", "title": "Rapid Skill Capture in a First-Person Shooter", "comments": "16 pages, 28 figures, journal paper submission", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Various aspects of computer game design, including adaptive elements of game\nlevels, characteristics of 'bot' behavior, and player matching in multiplayer\ngames, would ideally be sensitive to a player's skill level. Yet, while\ndifficulty and player learning have been explored in the context of games,\nthere has been little work analyzing skill per se, and how it pertains to a\nplayer's input. To this end, we present a data set of 476 game logs from over\n40 players of a first-person shooter game (Red Eclipse) as a basis of a case\nstudy. We then analyze different metrics of skill and show that some of these\ncan be predicted using only a few seconds of keyboard and mouse input. We argue\nthat the techniques used here are useful for adapting games to match players'\nskill levels rapidly, perhaps more rapidly than solutions based on performance\naveraging such as TrueSkill.\n", "versions": [{"version": "v1", "created": "Wed, 5 Nov 2014 16:41:12 GMT"}, {"version": "v2", "created": "Thu, 6 Nov 2014 13:04:25 GMT"}], "update_date": "2014-11-07", "authors_parsed": [["Buckley", "David", ""], ["Chen", "Ke", ""], ["Knowles", "Joshua", ""]]}, {"id": "1411.1420", "submitter": "James Voss", "authors": "Mikhail Belkin, Luis Rademacher, James Voss", "title": "Eigenvectors of Orthogonally Decomposable Functions", "comments": "69 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Eigendecomposition of quadratic forms (symmetric matrices) guaranteed by\nthe spectral theorem is a foundational result in applied mathematics. Motivated\nby a shared structure found in inferential problems of recent interest---namely\northogonal tensor decompositions, Independent Component Analysis (ICA), topic\nmodels, spectral clustering, and Gaussian mixture learning---we generalize the\neigendecomposition from quadratic forms to a broad class of \"orthogonally\ndecomposable\" functions. We identify a key role of convexity in our extension,\nand we generalize two traditional characterizations of eigenvectors: First, the\neigenvectors of a quadratic form arise from the optima structure of the\nquadratic form on the sphere. Second, the eigenvectors are the fixed points of\nthe power iteration.\n  In our setting, we consider a simple first order generalization of the power\nmethod which we call gradient iteration. It leads to efficient and easily\nimplementable methods for basis recovery. It includes influential Machine\nLearning methods such as cumulant-based FastICA and the tensor power iteration\nfor orthogonally decomposable tensors as special cases.\n  We provide a complete theoretical analysis of gradient iteration using the\nstructure theory of discrete dynamical systems to show almost sure convergence\nand fast (super-linear) convergence rates. The analysis also extends to the\ncase when the observed function is only approximately orthogonally\ndecomposable, with bounds that are polynomial in dimension and other relevant\nparameters, such as perturbation size. Our perturbation results can be\nconsidered as a non-linear version of the classical Davis-Kahan theorem for\nperturbations of eigenvectors of symmetric matrices.\n", "versions": [{"version": "v1", "created": "Wed, 5 Nov 2014 21:07:20 GMT"}, {"version": "v2", "created": "Mon, 11 May 2015 16:08:28 GMT"}, {"version": "v3", "created": "Tue, 3 Nov 2015 17:22:20 GMT"}, {"version": "v4", "created": "Tue, 24 May 2016 18:10:04 GMT"}, {"version": "v5", "created": "Sat, 26 Nov 2016 20:03:30 GMT"}, {"version": "v6", "created": "Fri, 23 Feb 2018 02:55:26 GMT"}], "update_date": "2018-02-26", "authors_parsed": [["Belkin", "Mikhail", ""], ["Rademacher", "Luis", ""], ["Voss", "James", ""]]}, {"id": "1411.1434", "submitter": "Karthikeyan Shanmugam", "authors": "Karthikeyan Shanmugam, Rashish Tandon, Alexandros G. Dimakis, Pradeep\n  Ravikumar", "title": "On the Information Theoretic Limits of Learning Ising Models", "comments": "21 pages; to appear in NIPS 2014", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We provide a general framework for computing lower-bounds on the sample\ncomplexity of recovering the underlying graphs of Ising models, given i.i.d\nsamples. While there have been recent results for specific graph classes, these\ninvolve fairly extensive technical arguments that are specialized to each\nspecific graph class. In contrast, we isolate two key graph-structural\ningredients that can then be used to specify sample complexity lower-bounds.\nPresence of these structural properties makes the graph class hard to learn. We\nderive corollaries of our main result that not only recover existing recent\nresults, but also provide lower bounds for novel graph classes not considered\npreviously. We also extend our framework to the random graph setting and derive\ncorollaries for Erd\\H{o}s-R\\'{e}nyi graphs in a certain dense setting.\n", "versions": [{"version": "v1", "created": "Wed, 5 Nov 2014 22:28:00 GMT"}, {"version": "v2", "created": "Fri, 5 Dec 2014 22:02:55 GMT"}], "update_date": "2014-12-09", "authors_parsed": [["Shanmugam", "Karthikeyan", ""], ["Tandon", "Rashish", ""], ["Dimakis", "Alexandros G.", ""], ["Ravikumar", "Pradeep", ""]]}, {"id": "1411.1446", "submitter": "Wei Wang", "authors": "Wei Wang", "title": "Electrocardiography Separation of Mother and Baby", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  Extraction of Electrocardiography (ECG or EKG) signals of mother and baby is\na challenging task, because one single device is used and it receives a mixture\nof multiple heart beats. In this paper, we would like to design a filter to\nseparate the signals from each other.\n", "versions": [{"version": "v1", "created": "Wed, 5 Nov 2014 23:14:10 GMT"}], "update_date": "2014-11-07", "authors_parsed": [["Wang", "Wei", ""]]}, {"id": "1411.1488", "submitter": "Majid Janzamin", "authors": "Anima Anandkumar, Rong Ge, Majid Janzamin", "title": "Analyzing Tensor Power Method Dynamics in Overcomplete Regime", "comments": "38 pages; analysis of noise added to the previous version", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a novel analysis of the dynamics of tensor power iterations in the\novercomplete regime where the tensor CP rank is larger than the input\ndimension. Finding the CP decomposition of an overcomplete tensor is NP-hard in\ngeneral. We consider the case where the tensor components are randomly drawn,\nand show that the simple power iteration recovers the components with bounded\nerror under mild initialization conditions. We apply our analysis to\nunsupervised learning of latent variable models, such as multi-view mixture\nmodels and spherical Gaussian mixtures. Given the third order moment tensor, we\nlearn the parameters using tensor power iterations. We prove it can correctly\nlearn the model parameters when the number of hidden components $k$ is much\nlarger than the data dimension $d$, up to $k = o(d^{1.5})$. We initialize the\npower iterations with data samples and prove its success under mild conditions\non the signal-to-noise ratio of the samples. Our analysis significantly expands\nthe class of latent variable models where spectral methods are applicable. Our\nanalysis also deals with noise in the input tensor leading to sample complexity\nresult in the application to learning latent variable models.\n", "versions": [{"version": "v1", "created": "Thu, 6 Nov 2014 03:25:54 GMT"}, {"version": "v2", "created": "Mon, 14 Sep 2015 20:56:57 GMT"}], "update_date": "2015-09-16", "authors_parsed": [["Anandkumar", "Anima", ""], ["Ge", "Rong", ""], ["Janzamin", "Majid", ""]]}, {"id": "1411.1490", "submitter": "Maria Florina Balcan", "authors": "Maria-Florina Balcan, Avrim Blum, Santosh Vempala", "title": "Efficient Representations for Life-Long Learning and Autoencoding", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It has been a long-standing goal in machine learning, as well as in AI more\ngenerally, to develop life-long learning systems that learn many different\ntasks over time, and reuse insights from tasks learned, \"learning to learn\" as\nthey do so. In this work we pose and provide efficient algorithms for several\nnatural theoretical formulations of this goal. Specifically, we consider the\nproblem of learning many different target functions over time, that share\ncertain commonalities that are initially unknown to the learning algorithm. Our\naim is to learn new internal representations as the algorithm learns new target\nfunctions, that capture this commonality and allow subsequent learning tasks to\nbe solved more efficiently and from less data. We develop efficient algorithms\nfor two very different kinds of commonalities that target functions might\nshare: one based on learning common low-dimensional and unions of\nlow-dimensional subspaces and one based on learning nonlinear Boolean\ncombinations of features. Our algorithms for learning Boolean feature\ncombinations additionally have a dual interpretation, and can be viewed as\ngiving an efficient procedure for constructing near-optimal sparse Boolean\nautoencoders under a natural \"anchor-set\" assumption.\n", "versions": [{"version": "v1", "created": "Thu, 6 Nov 2014 03:51:39 GMT"}, {"version": "v2", "created": "Thu, 4 Dec 2014 22:59:04 GMT"}], "update_date": "2014-12-08", "authors_parsed": [["Balcan", "Maria-Florina", ""], ["Blum", "Avrim", ""], ["Vempala", "Santosh", ""]]}, {"id": "1411.1509", "submitter": "Zetao Chen", "authors": "Zetao Chen, Obadiah Lam, Adam Jacobson and Michael Milford", "title": "Convolutional Neural Network-based Place Recognition", "comments": "8 pages, 11 figures, this paper has been accepted by 2014\n  Australasian Conference on Robotics and Automation (ACRA 2014) to be held in\n  University of Melbourne, Dec 2~4", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently Convolutional Neural Networks (CNNs) have been shown to achieve\nstate-of-the-art performance on various classification tasks. In this paper, we\npresent for the first time a place recognition technique based on CNN models,\nby combining the powerful features learnt by CNNs with a spatial and sequential\nfilter. Applying the system to a 70 km benchmark place recognition dataset we\nachieve a 75% increase in recall at 100% precision, significantly outperforming\nall previous state of the art techniques. We also conduct a comprehensive\nperformance comparison of the utility of features from all 21 layers for place\nrecognition, both for the benchmark dataset and for a second dataset with more\nsignificant viewpoint changes.\n", "versions": [{"version": "v1", "created": "Thu, 6 Nov 2014 07:03:15 GMT"}], "update_date": "2014-11-07", "authors_parsed": [["Chen", "Zetao", ""], ["Lam", "Obadiah", ""], ["Jacobson", "Adam", ""], ["Milford", "Michael", ""]]}, {"id": "1411.1537", "submitter": "Fei Sha", "authors": "Boqing Gong, Wei-lun Chao, Kristen Grauman and Fei Sha", "title": "Large-Margin Determinantal Point Processes", "comments": "15 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Determinantal point processes (DPPs) offer a powerful approach to modeling\ndiversity in many applications where the goal is to select a diverse subset. We\nstudy the problem of learning the parameters (the kernel matrix) of a DPP from\nlabeled training data. We make two contributions. First, we show how to\nreparameterize a DPP's kernel matrix with multiple kernel functions, thus\nenhancing modeling flexibility. Second, we propose a novel parameter estimation\ntechnique based on the principle of large margin separation. In contrast to the\nstate-of-the-art method of maximum likelihood estimation, our large-margin loss\nfunction explicitly models errors in selecting the target subsets, and it can\nbe customized to trade off different types of errors (precision vs. recall).\nExtensive empirical studies validate our contributions, including applications\non challenging document and video summarization, where flexibility in modeling\nthe kernel matrix and balancing different errors is indispensable.\n", "versions": [{"version": "v1", "created": "Thu, 6 Nov 2014 09:14:02 GMT"}, {"version": "v2", "created": "Fri, 7 Nov 2014 05:21:03 GMT"}], "update_date": "2014-11-10", "authors_parsed": [["Gong", "Boqing", ""], ["Chao", "Wei-lun", ""], ["Grauman", "Kristen", ""], ["Sha", "Fei", ""]]}, {"id": "1411.1623", "submitter": "Siddharth Sigtia", "authors": "Siddharth Sigtia, Emmanouil Benetos, Nicolas Boulanger-Lewandowski,\n  Tillman Weyde, Artur S. d'Avila Garcez, Simon Dixon", "title": "A Hybrid Recurrent Neural Network For Music Transcription", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate the problem of incorporating higher-level symbolic score-like\ninformation into Automatic Music Transcription (AMT) systems to improve their\nperformance. We use recurrent neural networks (RNNs) and their variants as\nmusic language models (MLMs) and present a generative architecture for\ncombining these models with predictions from a frame level acoustic classifier.\nWe also compare different neural network architectures for acoustic modeling.\nThe proposed model computes a distribution over possible output sequences given\nthe acoustic input signal and we present an algorithm for performing a global\nsearch for good candidate transcriptions. The performance of the proposed model\nis evaluated on piano music from the MAPS dataset and we observe that the\nproposed model consistently outperforms existing transcription methods.\n", "versions": [{"version": "v1", "created": "Thu, 6 Nov 2014 14:18:39 GMT"}], "update_date": "2014-11-07", "authors_parsed": [["Sigtia", "Siddharth", ""], ["Benetos", "Emmanouil", ""], ["Boulanger-Lewandowski", "Nicolas", ""], ["Weyde", "Tillman", ""], ["Garcez", "Artur S. d'Avila", ""], ["Dixon", "Simon", ""]]}, {"id": "1411.1752", "submitter": "Adarsh Prasad", "authors": "Adarsh Prasad, Stefanie Jegelka and Dhruv Batra", "title": "Submodular meets Structured: Finding Diverse Subsets in\n  Exponentially-Large Structured Item Sets", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV cs.IR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To cope with the high level of ambiguity faced in domains such as Computer\nVision or Natural Language processing, robust prediction methods often search\nfor a diverse set of high-quality candidate solutions or proposals. In\nstructured prediction problems, this becomes a daunting task, as the solution\nspace (image labelings, sentence parses, etc.) is exponentially large. We study\ngreedy algorithms for finding a diverse subset of solutions in\nstructured-output spaces by drawing new connections between submodular\nfunctions over combinatorial item sets and High-Order Potentials (HOPs) studied\nfor graphical models. Specifically, we show via examples that when marginal\ngains of submodular diversity functions allow structured representations, this\nenables efficient (sub-linear time) approximate maximization by reducing the\ngreedy augmentation step to inference in a factor graph with appropriately\nconstructed HOPs. We discuss benefits, tradeoffs, and show that our\nconstructions lead to significantly better proposals.\n", "versions": [{"version": "v1", "created": "Thu, 6 Nov 2014 20:07:37 GMT"}], "update_date": "2014-11-10", "authors_parsed": [["Prasad", "Adarsh", ""], ["Jegelka", "Stefanie", ""], ["Batra", "Dhruv", ""]]}, {"id": "1411.1784", "submitter": "Mehdi Mirza", "authors": "Mehdi Mirza, Simon Osindero", "title": "Conditional Generative Adversarial Nets", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generative Adversarial Nets [8] were recently introduced as a novel way to\ntrain generative models. In this work we introduce the conditional version of\ngenerative adversarial nets, which can be constructed by simply feeding the\ndata, y, we wish to condition on to both the generator and discriminator. We\nshow that this model can generate MNIST digits conditioned on class labels. We\nalso illustrate how this model could be used to learn a multi-modal model, and\nprovide preliminary examples of an application to image tagging in which we\ndemonstrate how this approach can generate descriptive tags which are not part\nof training labels.\n", "versions": [{"version": "v1", "created": "Thu, 6 Nov 2014 22:33:22 GMT"}], "update_date": "2014-11-10", "authors_parsed": [["Mirza", "Mehdi", ""], ["Osindero", "Simon", ""]]}, {"id": "1411.1792", "submitter": "Jason Yosinski", "authors": "Jason Yosinski, Jeff Clune, Yoshua Bengio, Hod Lipson", "title": "How transferable are features in deep neural networks?", "comments": "To appear in Advances in Neural Information Processing Systems 27\n  (NIPS 2014)", "journal-ref": "Advances in Neural Information Processing Systems 27, pages\n  3320-3328. Dec. 2014", "doi": null, "report-no": null, "categories": "cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many deep neural networks trained on natural images exhibit a curious\nphenomenon in common: on the first layer they learn features similar to Gabor\nfilters and color blobs. Such first-layer features appear not to be specific to\na particular dataset or task, but general in that they are applicable to many\ndatasets and tasks. Features must eventually transition from general to\nspecific by the last layer of the network, but this transition has not been\nstudied extensively. In this paper we experimentally quantify the generality\nversus specificity of neurons in each layer of a deep convolutional neural\nnetwork and report a few surprising results. Transferability is negatively\naffected by two distinct issues: (1) the specialization of higher layer neurons\nto their original task at the expense of performance on the target task, which\nwas expected, and (2) optimization difficulties related to splitting networks\nbetween co-adapted neurons, which was not expected. In an example network\ntrained on ImageNet, we demonstrate that either of these two issues may\ndominate, depending on whether features are transferred from the bottom,\nmiddle, or top of the network. We also document that the transferability of\nfeatures decreases as the distance between the base task and target task\nincreases, but that transferring features even from distant tasks can be better\nthan using random features. A final surprising result is that initializing a\nnetwork with transferred features from almost any number of layers can produce\na boost to generalization that lingers even after fine-tuning to the target\ndataset.\n", "versions": [{"version": "v1", "created": "Thu, 6 Nov 2014 23:09:37 GMT"}], "update_date": "2014-12-09", "authors_parsed": [["Yosinski", "Jason", ""], ["Clune", "Jeff", ""], ["Bengio", "Yoshua", ""], ["Lipson", "Hod", ""]]}, {"id": "1411.1804", "submitter": "Dawen Liang", "authors": "Dawen Liang, Matthew D. Hoffman", "title": "Beta Process Non-negative Matrix Factorization with Stochastic\n  Structured Mean-Field Variational Inference", "comments": "6 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Beta process is the standard nonparametric Bayesian prior for latent factor\nmodel. In this paper, we derive a structured mean-field variational inference\nalgorithm for a beta process non-negative matrix factorization (NMF) model with\nPoisson likelihood. Unlike the linear Gaussian model, which is well-studied in\nthe nonparametric Bayesian literature, NMF model with beta process prior does\nnot enjoy the conjugacy. We leverage the recently developed stochastic\nstructured mean-field variational inference to relax the conjugacy constraint\nand restore the dependencies among the latent variables in the approximating\nvariational distribution. Preliminary results on both synthetic and real\nexamples demonstrate that the proposed inference algorithm can reasonably\nrecover the hidden structure of the data.\n", "versions": [{"version": "v1", "created": "Fri, 7 Nov 2014 00:51:03 GMT"}, {"version": "v2", "created": "Tue, 2 Dec 2014 05:23:23 GMT"}], "update_date": "2014-12-03", "authors_parsed": [["Liang", "Dawen", ""], ["Hoffman", "Matthew D.", ""]]}, {"id": "1411.1810", "submitter": "Stephan Mandt", "authors": "Stephan Mandt, James McInerney, Farhan Abrol, Rajesh Ranganath, and\n  David Blei", "title": "Variational Tempering", "comments": "published version, 8 pages, 4 figures", "journal-ref": "Proceedings of the 19th International Conference on Artificial\n  Intelligence and Statistics (AISTATS 2016), pages 704-712", "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Variational inference (VI) combined with data subsampling enables approximate\nposterior inference over large data sets, but suffers from poor local optima.\nWe first formulate a deterministic annealing approach for the generic class of\nconditionally conjugate exponential family models. This approach uses a\ndecreasing temperature parameter which deterministically deforms the objective\nduring the course of the optimization. A well-known drawback to this annealing\napproach is the choice of the cooling schedule. We therefore introduce\nvariational tempering, a variational algorithm that introduces a temperature\nlatent variable to the model. In contrast to related work in the Markov chain\nMonte Carlo literature, this algorithm results in adaptive annealing schedules.\nLastly, we develop local variational tempering, which assigns a latent\ntemperature to each data point; this allows for dynamic annealing that varies\nacross data. Compared to the traditional VI, all proposed approaches find\nimproved predictive likelihoods on held-out data.\n", "versions": [{"version": "v1", "created": "Fri, 7 Nov 2014 01:28:41 GMT"}, {"version": "v2", "created": "Tue, 27 Oct 2015 10:45:39 GMT"}, {"version": "v3", "created": "Fri, 30 Oct 2015 21:14:58 GMT"}, {"version": "v4", "created": "Sat, 28 May 2016 19:58:17 GMT"}], "update_date": "2016-05-31", "authors_parsed": [["Mandt", "Stephan", ""], ["McInerney", "James", ""], ["Abrol", "Farhan", ""], ["Ranganath", "Rajesh", ""], ["Blei", "David", ""]]}, {"id": "1411.1971", "submitter": "Jiaxin Zhang", "authors": "Xiangyang Zhou, Jiaxin Zhang, Brian Kulis", "title": "Power-Law Graph Cuts", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Algorithms based on spectral graph cut objectives such as normalized cuts,\nratio cuts and ratio association have become popular in recent years because\nthey are widely applicable and simple to implement via standard eigenvector\ncomputations. Despite strong performance for a number of clustering tasks,\nspectral graph cut algorithms still suffer from several limitations: first,\nthey require the number of clusters to be known in advance, but this\ninformation is often unknown a priori; second, they tend to produce clusters\nwith uniform sizes. In some cases, the true clusters exhibit a known size\ndistribution; in image segmentation, for instance, human-segmented images tend\nto yield segment sizes that follow a power-law distribution. In this paper, we\npropose a general framework of power-law graph cut algorithms that produce\nclusters whose sizes are power-law distributed, and also does not fix the\nnumber of clusters upfront. To achieve our goals, we treat the Pitman-Yor\nexchangeable partition probability function (EPPF) as a regularizer to graph\ncut objectives. Because the resulting objectives cannot be solved by relaxing\nvia eigenvectors, we derive a simple iterative algorithm to locally optimize\nthe objectives. Moreover, we show that our proposed algorithm can be viewed as\nperforming MAP inference on a particular Pitman-Yor mixture model. Our\nexperiments on various data sets show the effectiveness of our algorithms.\n", "versions": [{"version": "v1", "created": "Wed, 29 Oct 2014 20:46:20 GMT"}, {"version": "v2", "created": "Tue, 25 Nov 2014 21:41:07 GMT"}], "update_date": "2014-11-27", "authors_parsed": [["Zhou", "Xiangyang", ""], ["Zhang", "Jiaxin", ""], ["Kulis", "Brian", ""]]}, {"id": "1411.1990", "submitter": "Marwa El Halabi", "authors": "Marwa El Halabi and Volkan Cevher", "title": "A totally unimodular view of structured sparsity", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper describes a simple framework for structured sparse recovery based\non convex optimization. We show that many structured sparsity models can be\nnaturally represented by linear matrix inequalities on the support of the\nunknown parameters, where the constraint matrix has a totally unimodular (TU)\nstructure. For such structured models, tight convex relaxations can be obtained\nin polynomial time via linear programming. Our modeling framework unifies the\nprevalent structured sparsity norms in the literature, introduces new\ninteresting ones, and renders their tightness and tractability arguments\ntransparent.\n", "versions": [{"version": "v1", "created": "Fri, 7 Nov 2014 17:24:30 GMT"}, {"version": "v2", "created": "Tue, 3 Mar 2015 14:23:47 GMT"}], "update_date": "2015-03-04", "authors_parsed": [["Halabi", "Marwa El", ""], ["Cevher", "Volkan", ""]]}, {"id": "1411.2021", "submitter": "He Sun", "authors": "Richard Peng and He Sun and Luca Zanetti", "title": "Partitioning Well-Clustered Graphs: Spectral Clustering Works!", "comments": "A preliminary version of this paper appeared in COLT'15; the full\n  version is to appear in SIAM Journal on Computing", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we study variants of the widely used spectral clustering that\npartitions a graph into k clusters by (1) embedding the vertices of a graph\ninto a low-dimensional space using the bottom eigenvectors of the Laplacian\nmatrix, and (2) grouping the embedded points into k clusters via k-means\nalgorithms. We show that, for a wide class of graphs, spectral clustering gives\na good approximation of the optimal clustering. While this approach was\nproposed in the early 1990s and has comprehensive applications, prior to our\nwork similar results were known only for graphs generated from stochastic\nmodels.\n  We also give a nearly-linear time algorithm for partitioning well-clustered\ngraphs based on computing a matrix exponential and approximate nearest neighbor\ndata structures.\n", "versions": [{"version": "v1", "created": "Fri, 7 Nov 2014 20:23:50 GMT"}, {"version": "v2", "created": "Tue, 17 Nov 2015 17:08:00 GMT"}, {"version": "v3", "created": "Tue, 31 Jan 2017 15:07:33 GMT"}], "update_date": "2017-02-01", "authors_parsed": [["Peng", "Richard", ""], ["Sun", "He", ""], ["Zanetti", "Luca", ""]]}, {"id": "1411.2057", "submitter": "Siddhartha Banerjee", "authors": "Siddhartha Banerjee, Sujay Sanghavi, Sanjay Shakkottai", "title": "Online Collaborative-Filtering on Graphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A common phenomena in modern recommendation systems is the use of feedback\nfrom one user to infer the `value' of an item to other users. This results in\nan exploration vs. exploitation trade-off, in which items of possibly low value\nhave to be presented to users in order to ascertain their value. Existing\napproaches to solving this problem focus on the case where the number of items\nare small, or admit some underlying structure -- it is unclear, however, if\ngood recommendation is possible when dealing with content-rich settings with\nunstructured content.\n  We consider this problem under a simple natural model, wherein the number of\nitems and the number of item-views are of the same order, and an `access-graph'\nconstrains which user is allowed to see which item. Our main insight is that\nthe presence of the access-graph in fact makes good recommendation possible --\nhowever this requires the exploration policy to be designed to take advantage\nof the access-graph. Our results demonstrate the importance of `serendipity' in\nexploration, and how higher graph-expansion translates to a higher quality of\nrecommendations; it also suggests a reason why in some settings, simple\npolicies like Twitter's `Latest-First' policy achieve a good performance.\n  From a technical perspective, our model presents a way to study\nexploration-exploitation tradeoffs in settings where the number of `trials' and\n`strategies' are large (potentially infinite), and more importantly, of the\nsame order. Our algorithms admit competitive-ratio guarantees which hold for\nthe worst-case user, under both finite-population and infinite-horizon\nsettings, and are parametrized in terms of properties of the underlying graph.\nConversely, we also demonstrate that improperly-designed policies can be highly\nsub-optimal, and that in many settings, our results are order-wise optimal.\n", "versions": [{"version": "v1", "created": "Fri, 7 Nov 2014 22:52:19 GMT"}], "update_date": "2014-11-11", "authors_parsed": [["Banerjee", "Siddhartha", ""], ["Sanghavi", "Sujay", ""], ["Shakkottai", "Sanjay", ""]]}, {"id": "1411.2066", "submitter": "Zoltan Szabo", "authors": "Zoltan Szabo, Bharath Sriperumbudur, Barnabas Poczos, Arthur Gretton", "title": "Learning Theory for Distribution Regression", "comments": "Final version appeared at JMLR, with supplement. Code:\n  https://bitbucket.org/szzoli/ite/. arXiv admin note: text overlap with\n  arXiv:1402.1754", "journal-ref": "Journal of Machine Learning Research, 17(152):1-40, 2016", "doi": null, "report-no": null, "categories": "math.ST cs.LG math.FA stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We focus on the distribution regression problem: regressing to vector-valued\noutputs from probability measures. Many important machine learning and\nstatistical tasks fit into this framework, including multi-instance learning\nand point estimation problems without analytical solution (such as\nhyperparameter or entropy estimation). Despite the large number of available\nheuristics in the literature, the inherent two-stage sampled nature of the\nproblem makes the theoretical analysis quite challenging, since in practice\nonly samples from sampled distributions are observable, and the estimates have\nto rely on similarities computed between sets of points. To the best of our\nknowledge, the only existing technique with consistency guarantees for\ndistribution regression requires kernel density estimation as an intermediate\nstep (which often performs poorly in practice), and the domain of the\ndistributions to be compact Euclidean. In this paper, we study a simple,\nanalytically computable, ridge regression-based alternative to distribution\nregression, where we embed the distributions to a reproducing kernel Hilbert\nspace, and learn the regressor from the embeddings to the outputs. Our main\ncontribution is to prove that this scheme is consistent in the two-stage\nsampled setup under mild conditions (on separable topological domains enriched\nwith kernels): we present an exact computational-statistical efficiency\ntrade-off analysis showing that our estimator is able to match the one-stage\nsampled minimax optimal rate [Caponnetto and De Vito, 2007; Steinwart et al.,\n2009]. This result answers a 17-year-old open question, establishing the\nconsistency of the classical set kernel [Haussler, 1999; Gaertner et. al, 2002]\nin regression. We also cover consistency for more recent kernels on\ndistributions, including those due to [Christmann and Steinwart, 2010].\n", "versions": [{"version": "v1", "created": "Sat, 8 Nov 2014 01:16:44 GMT"}, {"version": "v2", "created": "Sat, 6 Dec 2014 23:49:00 GMT"}, {"version": "v3", "created": "Tue, 19 Jan 2016 22:03:20 GMT"}, {"version": "v4", "created": "Fri, 21 Oct 2016 15:46:35 GMT"}], "update_date": "2016-10-24", "authors_parsed": [["Szabo", "Zoltan", ""], ["Sriperumbudur", "Bharath", ""], ["Poczos", "Barnabas", ""], ["Gretton", "Arthur", ""]]}, {"id": "1411.2158", "submitter": "Norbert Binkiewicz", "authors": "Norbert Binkiewicz, Joshua T. Vogelstein, and Karl Rohe", "title": "Covariate-assisted spectral clustering", "comments": "28 pages, 4 figures, includes substantial changes to theoretical\n  results", "journal-ref": "Biometrika, Volume 104, Issue 2, 1 June 2017, Pages 361-377", "doi": "10.1093/biomet/asx008", "report-no": null, "categories": "stat.ML cs.LG math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Biological and social systems consist of myriad interacting units. The\ninteractions can be represented in the form of a graph or network. Measurements\nof these graphs can reveal the underlying structure of these interactions,\nwhich provides insight into the systems that generated the graphs. Moreover, in\napplications such as connectomics, social networks, and genomics, graph data\nare accompanied by contextualizing measures on each node. We utilize these node\ncovariates to help uncover latent communities in a graph, using a modification\nof spectral clustering. Statistical guarantees are provided under a joint\nmixture model that we call the node-contextualized stochastic blockmodel,\nincluding a bound on the mis-clustering rate. The bound is used to derive\nconditions for achieving perfect clustering. For most simulated cases,\ncovariate-assisted spectral clustering yields results superior to regularized\nspectral clustering without node covariates and to an adaptation of canonical\ncorrelation analysis. We apply our clustering method to large brain graphs\nderived from diffusion MRI data, using the node locations or neurological\nregion membership as covariates. In both cases, covariate-assisted spectral\nclustering yields clusters that are easier to interpret neurologically.\n", "versions": [{"version": "v1", "created": "Sat, 8 Nov 2014 20:14:59 GMT"}, {"version": "v2", "created": "Thu, 4 Jun 2015 19:14:01 GMT"}, {"version": "v3", "created": "Tue, 22 Dec 2015 04:51:04 GMT"}, {"version": "v4", "created": "Thu, 3 Mar 2016 04:07:22 GMT"}, {"version": "v5", "created": "Sun, 30 Oct 2016 04:22:47 GMT"}], "update_date": "2017-10-25", "authors_parsed": [["Binkiewicz", "Norbert", ""], ["Vogelstein", "Joshua T.", ""], ["Rohe", "Karl", ""]]}, {"id": "1411.2305", "submitter": "Xun Zheng", "authors": "Xun Zheng, Jin Kyu Kim, Qirong Ho, Eric P. Xing", "title": "Model-Parallel Inference for Big Topic Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In real world industrial applications of topic modeling, the ability to\ncapture gigantic conceptual space by learning an ultra-high dimensional topical\nrepresentation, i.e., the so-called \"big model\", is becoming the next\ndesideratum after enthusiasms on \"big data\", especially for fine-grained\ndownstream tasks such as online advertising, where good performances are\nusually achieved by regression-based predictors built on millions if not\nbillions of input features. The conventional data-parallel approach for\ntraining gigantic topic models turns out to be rather inefficient in utilizing\nthe power of parallelism, due to the heavy dependency on a centralized image of\n\"model\". Big model size also poses another challenge on the storage, where\navailable model size is bounded by the smallest RAM of nodes. To address these\nissues, we explore another type of parallelism, namely model-parallelism, which\nenables training of disjoint blocks of a big topic model in parallel. By\nintegrating data-parallelism with model-parallelism, we show that dependencies\nbetween distributed elements can be handled seamlessly, achieving not only\nfaster convergence but also an ability to tackle significantly bigger model\nsize. We describe an architecture for model-parallel inference of LDA, and\npresent a variant of collapsed Gibbs sampling algorithm tailored for it.\nExperimental results demonstrate the ability of this system to handle topic\nmodeling with unprecedented amount of 200 billion model variables only on a\nlow-end cluster with very limited computational resources and bandwidth.\n", "versions": [{"version": "v1", "created": "Mon, 10 Nov 2014 01:25:30 GMT"}], "update_date": "2014-11-11", "authors_parsed": [["Zheng", "Xun", ""], ["Kim", "Jin Kyu", ""], ["Ho", "Qirong", ""], ["Xing", "Eric P.", ""]]}, {"id": "1411.2331", "submitter": "Makoto Yamada", "authors": "Makoto Yamada, Avishek Saha, Hua Ouyang, Dawei Yin, Yi Chang", "title": "N$^3$LARS: Minimum Redundancy Maximum Relevance Feature Selection for\n  Large and High-dimensional Data", "comments": "arXiv admin note: text overlap with arXiv:1202.0515", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a feature selection method that finds non-redundant features from\na large and high-dimensional data in nonlinear way. Specifically, we propose a\nnonlinear extension of the non-negative least-angle regression (LARS) called\nN${}^3$LARS, where the similarity between input and output is measured through\nthe normalized version of the Hilbert-Schmidt Independence Criterion (HSIC). An\nadvantage of N${}^3$LARS is that it can easily incorporate with map-reduce\nframeworks such as Hadoop and Spark. Thus, with the help of distributed\ncomputing, a set of features can be efficiently selected from a large and\nhigh-dimensional data. Moreover, N${}^3$LARS is a convex method and can find a\nglobal optimum solution. The effectiveness of the proposed method is first\ndemonstrated through feature selection experiments for classification and\nregression with small and high-dimensional datasets. Finally, we evaluate our\nproposed method over a large and high-dimensional biology dataset.\n", "versions": [{"version": "v1", "created": "Mon, 10 Nov 2014 05:43:28 GMT"}], "update_date": "2014-11-11", "authors_parsed": [["Yamada", "Makoto", ""], ["Saha", "Avishek", ""], ["Ouyang", "Hua", ""], ["Yin", "Dawei", ""], ["Chang", "Yi", ""]]}, {"id": "1411.2337", "submitter": "Chen Fang", "authors": "Chen Fang and Daniel N. Rockmore", "title": "Multi-Task Metric Learning on Network Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multi-task learning (MTL) improves prediction performance in different\ncontexts by learning models jointly on multiple different, but related tasks.\nNetwork data, which are a priori data with a rich relational structure, provide\nan important context for applying MTL. In particular, the explicit relational\nstructure implies that network data is not i.i.d. data. Network data also often\ncomes with significant metadata (i.e., attributes) associated with each entity\n(node). Moreover, due to the diversity and variation in network data (e.g.,\nmulti-relational links or multi-category entities), various tasks can be\nperformed and often a rich correlation exists between them. Learning algorithms\nshould exploit all of these additional sources of information for better\nperformance. In this work we take a metric-learning point of view for the MTL\nproblem in the network context. Our approach builds on structure preserving\nmetric learning (SPML). In particular SPML learns a Mahalanobis distance metric\nfor node attributes using network structure as supervision, so that the learned\ndistance function encodes the structure and can be used to predict link\npatterns from attributes. SPML is described for single-task learning on single\nnetwork. Herein, we propose a multi-task version of SPML, abbreviated as\nMT-SPML, which is able to learn across multiple related tasks on multiple\nnetworks via shared intermediate parametrization. MT-SPML learns a specific\nmetric for each task and a common metric for all tasks. The task correlation is\ncarried through the common metric and the individual metrics encode task\nspecific information. When combined together, they are structure-preserving\nwith respect to individual tasks. MT-SPML works on general networks, thus is\nsuitable for a wide variety of problems. In experiments, we challenge MT-SPML\non two real-word problems, where MT-SPML achieves significant improvement.\n", "versions": [{"version": "v1", "created": "Mon, 10 Nov 2014 06:41:20 GMT"}], "update_date": "2014-11-11", "authors_parsed": [["Fang", "Chen", ""], ["Rockmore", "Daniel N.", ""]]}, {"id": "1411.2374", "submitter": "Aur\\'elien Bellet", "authors": "Kuan Liu and Aur\\'elien Bellet and Fei Sha", "title": "Similarity Learning for High-Dimensional Sparse Data", "comments": "14 pages. Proceedings of the 18th International Conference on\n  Artificial Intelligence and Statistics (AISTATS 2015). Matlab code:\n  https://github.com/bellet/HDSL", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A good measure of similarity between data points is crucial to many tasks in\nmachine learning. Similarity and metric learning methods learn such measures\nautomatically from data, but they do not scale well respect to the\ndimensionality of the data. In this paper, we propose a method that can learn\nefficiently similarity measure from high-dimensional sparse data. The core idea\nis to parameterize the similarity measure as a convex combination of rank-one\nmatrices with specific sparsity structures. The parameters are then optimized\nwith an approximate Frank-Wolfe procedure to maximally satisfy relative\nsimilarity constraints on the training data. Our algorithm greedily\nincorporates one pair of features at a time into the similarity measure,\nproviding an efficient way to control the number of active features and thus\nreduce overfitting. It enjoys very appealing convergence guarantees and its\ntime and memory complexity depends on the sparsity of the data instead of the\ndimension of the feature space. Our experiments on real-world high-dimensional\ndatasets demonstrate its potential for classification, dimensionality reduction\nand data exploration.\n", "versions": [{"version": "v1", "created": "Mon, 10 Nov 2014 10:40:47 GMT"}, {"version": "v2", "created": "Wed, 21 Oct 2015 13:45:00 GMT"}, {"version": "v3", "created": "Mon, 9 Sep 2019 16:53:40 GMT"}], "update_date": "2019-09-10", "authors_parsed": [["Liu", "Kuan", ""], ["Bellet", "Aur\u00e9lien", ""], ["Sha", "Fei", ""]]}, {"id": "1411.2539", "submitter": "Ryan Kiros", "authors": "Ryan Kiros, Ruslan Salakhutdinov, Richard S. Zemel", "title": "Unifying Visual-Semantic Embeddings with Multimodal Neural Language\n  Models", "comments": "13 pages. NIPS 2014 deep learning workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Inspired by recent advances in multimodal learning and machine translation,\nwe introduce an encoder-decoder pipeline that learns (a): a multimodal joint\nembedding space with images and text and (b): a novel language model for\ndecoding distributed representations from our space. Our pipeline effectively\nunifies joint image-text embedding models with multimodal neural language\nmodels. We introduce the structure-content neural language model that\ndisentangles the structure of a sentence to its content, conditioned on\nrepresentations produced by the encoder. The encoder allows one to rank images\nand sentences while the decoder can generate novel descriptions from scratch.\nUsing LSTM to encode sentences, we match the state-of-the-art performance on\nFlickr8K and Flickr30K without using object detections. We also set new best\nresults when using the 19-layer Oxford convolutional network. Furthermore we\nshow that with linear encoders, the learned embedding space captures multimodal\nregularities in terms of vector space arithmetic e.g. *image of a blue car* -\n\"blue\" + \"red\" is near images of red cars. Sample captions generated for 800\nimages are made available for comparison.\n", "versions": [{"version": "v1", "created": "Mon, 10 Nov 2014 19:09:41 GMT"}], "update_date": "2014-11-11", "authors_parsed": [["Kiros", "Ryan", ""], ["Salakhutdinov", "Ruslan", ""], ["Zemel", "Richard S.", ""]]}, {"id": "1411.2581", "submitter": "Rajesh Ranganath", "authors": "Rajesh Ranganath, Linpeng Tang, Laurent Charlin, David M. Blei", "title": "Deep Exponential Families", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe \\textit{deep exponential families} (DEFs), a class of latent\nvariable models that are inspired by the hidden structures used in deep neural\nnetworks. DEFs capture a hierarchy of dependencies between latent variables,\nand are easily generalized to many settings through exponential families. We\nperform inference using recent \"black box\" variational inference techniques. We\nthen evaluate various DEFs on text and combine multiple DEFs into a model for\npairwise recommendation data. In an extensive study, we show that going beyond\none layer improves predictions for DEFs. We demonstrate that DEFs find\ninteresting exploratory structure in large data sets, and give better\npredictive performance than state-of-the-art models.\n", "versions": [{"version": "v1", "created": "Mon, 10 Nov 2014 20:57:30 GMT"}], "update_date": "2014-11-11", "authors_parsed": [["Ranganath", "Rajesh", ""], ["Tang", "Linpeng", ""], ["Charlin", "Laurent", ""], ["Blei", "David M.", ""]]}, {"id": "1411.2635", "submitter": "Andreas Maurer", "authors": "Andreas Maurer", "title": "A chain rule for the expected suprema of Gaussian processes", "comments": null, "journal-ref": "Lecture Notes in Computer Science Volume 8776, 2014, pp 245-259", "doi": "10.1007/978-3-319-11662-4_18", "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The expected supremum of a Gaussian process indexed by the image of an index\nset under a function class is bounded in terms of separate properties of the\nindex set and the function class. The bound is relevant to the estimation of\nnonlinear transformations or the analysis of learning algorithms whenever\nhypotheses are chosen from composite classes, as is the case for multi-layer\nmodels.\n", "versions": [{"version": "v1", "created": "Mon, 10 Nov 2014 21:41:32 GMT"}], "update_date": "2014-11-12", "authors_parsed": [["Maurer", "Andreas", ""]]}, {"id": "1411.2664", "submitter": "Vitaly Feldman", "authors": "Cynthia Dwork and Vitaly Feldman and Moritz Hardt and Toniann Pitassi\n  and Omer Reingold and Aaron Roth", "title": "Preserving Statistical Validity in Adaptive Data Analysis", "comments": "Updated related work with recent developments", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A great deal of effort has been devoted to reducing the risk of spurious\nscientific discoveries, from the use of sophisticated validation techniques, to\ndeep statistical methods for controlling the false discovery rate in multiple\nhypothesis testing. However, there is a fundamental disconnect between the\ntheoretical results and the practice of data analysis: the theory of\nstatistical inference assumes a fixed collection of hypotheses to be tested, or\nlearning algorithms to be applied, selected non-adaptively before the data are\ngathered, whereas in practice data is shared and reused with hypotheses and new\nanalyses being generated on the basis of data exploration and the outcomes of\nprevious analyses.\n  In this work we initiate a principled study of how to guarantee the validity\nof statistical inference in adaptive data analysis. As an instance of this\nproblem, we propose and investigate the question of estimating the expectations\nof $m$ adaptively chosen functions on an unknown distribution given $n$ random\nsamples.\n  We show that, surprisingly, there is a way to estimate an exponential in $n$\nnumber of expectations accurately even if the functions are chosen adaptively.\nThis gives an exponential improvement over standard empirical estimators that\nare limited to a linear number of estimates. Our result follows from a general\ntechnique that counter-intuitively involves actively perturbing and\ncoordinating the estimates, using techniques developed for privacy\npreservation. We give additional applications of this technique to our\nquestion.\n", "versions": [{"version": "v1", "created": "Mon, 10 Nov 2014 23:44:49 GMT"}, {"version": "v2", "created": "Thu, 23 Apr 2015 20:57:38 GMT"}, {"version": "v3", "created": "Wed, 2 Mar 2016 07:04:07 GMT"}], "update_date": "2016-03-03", "authors_parsed": [["Dwork", "Cynthia", ""], ["Feldman", "Vitaly", ""], ["Hardt", "Moritz", ""], ["Pitassi", "Toniann", ""], ["Reingold", "Omer", ""], ["Roth", "Aaron", ""]]}, {"id": "1411.2674", "submitter": "Fangjian Guo", "authors": "Fangjian Guo, Charles Blundell, Hanna Wallach and Katherine Heller", "title": "The Bayesian Echo Chamber: Modeling Social Influence via Linguistic\n  Accommodation", "comments": "14 pages, 7 figures, to appear in AISTATS 2015. Fixed minor\n  formatting issues", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CL cs.LG cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present the Bayesian Echo Chamber, a new Bayesian generative model for\nsocial interaction data. By modeling the evolution of people's language usage\nover time, this model discovers latent influence relationships between them.\nUnlike previous work on inferring influence, which has primarily focused on\nsimple temporal dynamics evidenced via turn-taking behavior, our model captures\nmore nuanced influence relationships, evidenced via linguistic accommodation\npatterns in interaction content. The model, which is based on a discrete analog\nof the multivariate Hawkes process, permits a fully Bayesian inference\nalgorithm. We validate our model's ability to discover latent influence\npatterns using transcripts of arguments heard by the US Supreme Court and the\nmovie \"12 Angry Men.\" We showcase our model's capabilities by using it to infer\nlatent influence patterns from Federal Open Market Committee meeting\ntranscripts, demonstrating state-of-the-art performance at uncovering social\ndynamics in group discussions.\n", "versions": [{"version": "v1", "created": "Tue, 11 Nov 2014 01:02:20 GMT"}, {"version": "v2", "created": "Sun, 25 Jan 2015 18:39:13 GMT"}, {"version": "v3", "created": "Tue, 27 Jan 2015 19:37:09 GMT"}], "update_date": "2015-01-28", "authors_parsed": [["Guo", "Fangjian", ""], ["Blundell", "Charles", ""], ["Wallach", "Hanna", ""], ["Heller", "Katherine", ""]]}, {"id": "1411.2679", "submitter": "Jiwei Li", "authors": "Jiwei Li, Alan Ritter and Dan Jurafsky", "title": "Inferring User Preferences by Probabilistic Logical Reasoning over\n  Social Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.AI cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a framework for inferring the latent attitudes or preferences of\nusers by performing probabilistic first-order logical reasoning over the social\nnetwork graph. Our method answers questions about Twitter users like {\\em Does\nthis user like sushi?} or {\\em Is this user a New York Knicks fan?} by building\na probabilistic model that reasons over user attributes (the user's location or\ngender) and the social network (the user's friends and spouse), via inferences\nlike homophily (I am more likely to like sushi if spouse or friends like sushi,\nI am more likely to like the Knicks if I live in New York). The algorithm uses\ndistant supervision, semi-supervised data harvesting and vector space models to\nextract user attributes (e.g. spouse, education, location) and preferences\n(likes and dislikes) from text. The extracted propositions are then fed into a\nprobabilistic reasoner (we investigate both Markov Logic and Probabilistic Soft\nLogic). Our experiments show that probabilistic logical reasoning significantly\nimproves the performance on attribute and relation extraction, and also\nachieves an F-score of 0.791 at predicting a users likes or dislikes,\nsignificantly better than two strong baselines.\n", "versions": [{"version": "v1", "created": "Tue, 11 Nov 2014 01:53:21 GMT"}], "update_date": "2014-11-13", "authors_parsed": [["Li", "Jiwei", ""], ["Ritter", "Alan", ""], ["Jurafsky", "Dan", ""]]}, {"id": "1411.2795", "submitter": "Nitesh Kumar Chaudhary", "authors": "Nitesh Kumar Chaudhary", "title": "Speaker Identification From Youtube Obtained Data", "comments": "7 pages, 5 figures, 1 Table, Signal & Image Processing : An\n  International Journal (SIPIJ) Vol.5, No.5, October 2014", "journal-ref": null, "doi": "10.5121/sipij.2014.5503", "report-no": null, "categories": "cs.SD cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An efficient, and intuitive algorithm is presented for the identification of\nspeakers from a long dataset (like YouTube long discussion, Cocktail party\nrecorded audio or video).The goal of automatic speaker identification is to\nidentify the number of different speakers and prepare a model for that speaker\nby extraction, characterization and speaker-specific information contained in\nthe speech signal. It has many diverse application specially in the field of\nSurveillance, Immigrations at Airport, cyber security, transcription in\nmulti-source of similar sound source, where it is difficult to assign\ntranscription arbitrary. The most commonly speech parametrization used in\nspeaker verification, K-mean, cepstral analysis, is detailed. Gaussian mixture\nmodeling, which is the speaker modeling technique is then explained. Gaussian\nmixture models (GMM), perhaps the most robust machine learning algorithm has\nbeen introduced examine and judge carefully speaker identification in text\nindependent. The application or employment of Gaussian mixture models for\nmonitoring & Analysing speaker identity is encouraged by the familiarity,\nawareness, or understanding gained through experience that Gaussian spectrum\ndepict the characteristics of speaker's spectral conformational pattern and\nremarkable ability of GMM to construct capricious densities after that we\nillustrate 'Expectation maximization' an iterative algorithm which takes some\narbitrary value in initial estimation and carry on the iterative process until\nthe convergence of value is observed,so by doing various number of experiments\nwe are able to obtain 79 ~ 82% of identification rate using Vector quantization\nand 85 ~ 92.6% of identification rate using GMM modeling by Expectation\nmaximization parameter estimation depending on variation of parameter.\n", "versions": [{"version": "v1", "created": "Tue, 11 Nov 2014 13:20:19 GMT"}], "update_date": "2014-11-12", "authors_parsed": [["Chaudhary", "Nitesh Kumar", ""]]}, {"id": "1411.2883", "submitter": "Namita Jain Mrs", "authors": "Namita Jain and C.A. Murthy", "title": "A new estimate of mutual information based measure of dependence between\n  two variables: properties and fast implementation", "comments": "International Journal of Machine Learning and Cybernetics, Springer\n  Berlin Heidelberg, 10-Sep-2015", "journal-ref": null, "doi": "10.1007/s13042-015-0418-6", "report-no": null, "categories": "cs.IT cs.LG math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article proposes a new method to estimate an existing mutual information\nbased dependence measure using histogram density estimates. Finding a suitable\nbin length for histogram is an open problem. We propose a new way of computing\nthe bin length for histogram using a function of maximum separation between\npoints. The chosen bin length leads to consistent density estimates for\nhistogram method. The values of density thus obtained are used to calculate an\nestimate of an existing dependence measure. The proposed estimate is named as\nMutual Information Based Dependence Index (MIDI). Some important properties of\nMIDI have also been stated. The performance of the proposed method has been\ncompared to generally accepted measures like Distance Correlation (dcor),\nMaximal Information Coefficient (MINE) in terms of accuracy and computational\ncomplexity with the help of several artificial data sets with different amounts\nof noise. The proposed method is able to detect many types of relationships\nbetween variables, without making any assumption about the functional form of\nthe relationship. The power statistics of proposed method illustrate their\neffectiveness in detecting non linear relationship. Thus, it is able to achieve\ngenerality without a high rate of false positive cases. MIDI is found to work\nbetter on a real life data set than competing methods. The proposed method is\nfound to overcome some of the limitations which occur with dcor and MINE.\nComputationally, MIDI is found to be better than dcor and MINE, in terms of\ntime and memory, making it suitable for large data sets.\n", "versions": [{"version": "v1", "created": "Tue, 28 Oct 2014 12:49:24 GMT"}, {"version": "v2", "created": "Fri, 9 Jan 2015 02:31:35 GMT"}, {"version": "v3", "created": "Fri, 21 Aug 2015 09:04:18 GMT"}, {"version": "v4", "created": "Mon, 14 Sep 2015 02:36:58 GMT"}], "update_date": "2015-09-15", "authors_parsed": [["Jain", "Namita", ""], ["Murthy", "C. A.", ""]]}, {"id": "1411.2919", "submitter": "Tor Lattimore", "authors": "Tor Lattimore and Remi Munos", "title": "Bounded Regret for Finite-Armed Structured Bandits", "comments": "16 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study a new type of K-armed bandit problem where the expected return of\none arm may depend on the returns of other arms. We present a new algorithm for\nthis general class of problems and show that under certain circumstances it is\npossible to achieve finite expected cumulative regret. We also give\nproblem-dependent lower bounds on the cumulative regret showing that at least\nin special cases the new algorithm is nearly optimal.\n", "versions": [{"version": "v1", "created": "Tue, 11 Nov 2014 18:55:35 GMT"}], "update_date": "2014-11-12", "authors_parsed": [["Lattimore", "Tor", ""], ["Munos", "Remi", ""]]}, {"id": "1411.3128", "submitter": "Dimitrios Kotzias", "authors": "Dimitrios Kotzias, Misha Denil, Phil Blunsom, Nando de Freitas", "title": "Deep Multi-Instance Transfer Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a new approach for transferring knowledge from groups to\nindividuals that comprise them. We evaluate our method in text, by inferring\nthe ratings of individual sentences using full-review ratings. This approach,\nwhich combines ideas from transfer learning, deep learning and multi-instance\nlearning, reduces the need for laborious human labelling of fine-grained data\nwhen abundant labels are available at the group level.\n", "versions": [{"version": "v1", "created": "Wed, 12 Nov 2014 10:40:52 GMT"}, {"version": "v2", "created": "Wed, 10 Dec 2014 15:55:12 GMT"}], "update_date": "2014-12-11", "authors_parsed": [["Kotzias", "Dimitrios", ""], ["Denil", "Misha", ""], ["Blunsom", "Phil", ""], ["de Freitas", "Nando", ""]]}, {"id": "1411.3197", "submitter": "Karamjit Singh", "authors": "Karamjit Singh, Puneet Agarwal, Gautam Shroff", "title": "Warranty Cost Estimation Using Bayesian Network", "comments": "Selected for publication in Poster Proceedings of \"Industrial\n  Conference on Data Mining (ICDM 2014)\"\n  http://www.data-mining-forum.de/files/Program%202014%20DIN%20Lang.pdf", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  All multi-component product manufacturing companies face the problem of\nwarranty cost estimation. Failure rate analysis of components plays a key role\nin this problem. Data source used for failure rate analysis has traditionally\nbeen past failure data of components. However, failure rate analysis can be\nimproved by means of fusion of additional information, such as symptoms\nobserved during after-sale service of the product, geographical information\n(hilly or plains areas), and information from tele-diagnostic analytics. In\nthis paper, we propose an approach, which learns dependency between\npart-failures and symptoms gleaned from such diverse sources of information, to\npredict expected number of failures with better accuracy. We also indicate how\nthe optimum warranty period can be computed. We demonstrate, through empirical\nresults, that our method can improve the warranty cost estimates significantly.\n", "versions": [{"version": "v1", "created": "Tue, 11 Nov 2014 08:23:55 GMT"}], "update_date": "2014-11-13", "authors_parsed": [["Singh", "Karamjit", ""], ["Agarwal", "Puneet", ""], ["Shroff", "Gautam", ""]]}, {"id": "1411.3224", "submitter": "L.A. Prashanth", "authors": "Nathaniel Korda and L.A. Prashanth", "title": "On TD(0) with function approximation: Concentration bounds and a\n  centered variant with exponential convergence", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We provide non-asymptotic bounds for the well-known temporal difference\nlearning algorithm TD(0) with linear function approximators. These include\nhigh-probability bounds as well as bounds in expectation. Our analysis suggests\nthat a step-size inversely proportional to the number of iterations cannot\nguarantee optimal rate of convergence unless we assume (partial) knowledge of\nthe stationary distribution for the Markov chain underlying the policy\nconsidered. We also provide bounds for the iterate averaged TD(0) variant,\nwhich gets rid of the step-size dependency while exhibiting the optimal rate of\nconvergence. Furthermore, we propose a variant of TD(0) with linear\napproximators that incorporates a centering sequence, and establish that it\nexhibits an exponential rate of convergence in expectation. We demonstrate the\nusefulness of our bounds on two synthetic experimental settings.\n", "versions": [{"version": "v1", "created": "Wed, 12 Nov 2014 16:22:28 GMT"}, {"version": "v2", "created": "Tue, 1 Sep 2015 18:20:52 GMT"}], "update_date": "2015-09-02", "authors_parsed": [["Korda", "Nathaniel", ""], ["Prashanth", "L. A.", ""]]}, {"id": "1411.3302", "submitter": "Atanu Roy", "authors": "Chandrima Sarkar, Atanu Roy", "title": "Using Gaussian Measures for Efficient Constraint Based Clustering", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we present a novel iterative multiphase clustering technique\nfor efficiently clustering high dimensional data points. For this purpose we\nimplement clustering feature (CF) tree on a real data set and a Gaussian\ndensity distribution constraint on the resultant CF tree. The post processing\nby the application of Gaussian density distribution function on the\nmicro-clusters leads to refinement of the previously formed clusters thus\nimproving their quality. This algorithm also succeeds in overcoming the\ninherent drawbacks of conventional hierarchical methods of clustering like\ninability to undo the change made to the dendogram of the data points.\nMoreover, the constraint measure applied in the algorithm makes this clustering\ntechnique suitable for need driven data analysis. We provide veracity of our\nclaim by evaluating our algorithm with other similar clustering algorithms.\nIntroduction\n", "versions": [{"version": "v1", "created": "Wed, 12 Nov 2014 20:14:48 GMT"}], "update_date": "2014-11-13", "authors_parsed": [["Sarkar", "Chandrima", ""], ["Roy", "Atanu", ""]]}, {"id": "1411.3315", "submitter": "Rami Al-Rfou", "authors": "Vivek Kulkarni, Rami Al-Rfou, Bryan Perozzi, and Steven Skiena", "title": "Statistically Significant Detection of Linguistic Change", "comments": "11 pages, 7 figures, 4 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new computational approach for tracking and detecting\nstatistically significant linguistic shifts in the meaning and usage of words.\nSuch linguistic shifts are especially prevalent on the Internet, where the\nrapid exchange of ideas can quickly change a word's meaning. Our meta-analysis\napproach constructs property time series of word usage, and then uses\nstatistically sound change point detection algorithms to identify significant\nlinguistic shifts.\n  We consider and analyze three approaches of increasing complexity to generate\nsuch linguistic property time series, the culmination of which uses\ndistributional characteristics inferred from word co-occurrences. Using\nrecently proposed deep neural language models, we first train vector\nrepresentations of words for each time period. Second, we warp the vector\nspaces into one unified coordinate system. Finally, we construct a\ndistance-based distributional time series for each word to track it's\nlinguistic displacement over time.\n  We demonstrate that our approach is scalable by tracking linguistic change\nacross years of micro-blogging using Twitter, a decade of product reviews using\na corpus of movie reviews from Amazon, and a century of written books using the\nGoogle Book-ngrams. Our analysis reveals interesting patterns of language usage\nchange commensurate with each medium.\n", "versions": [{"version": "v1", "created": "Wed, 12 Nov 2014 20:37:08 GMT"}], "update_date": "2014-11-13", "authors_parsed": [["Kulkarni", "Vivek", ""], ["Al-Rfou", "Rami", ""], ["Perozzi", "Bryan", ""], ["Skiena", "Steven", ""]]}, {"id": "1411.3409", "submitter": "Paul Mineiro", "authors": "Paul Mineiro, Nikos Karampatziakis", "title": "A Randomized Algorithm for CCA", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present RandomizedCCA, a randomized algorithm for computing canonical\nanalysis, suitable for large datasets stored either out of core or on a\ndistributed file system. Accurate results can be obtained in as few as two data\npasses, which is relevant for distributed processing frameworks in which\niteration is expensive (e.g., Hadoop). The strategy also provides an excellent\ninitializer for standard iterative solutions.\n", "versions": [{"version": "v1", "created": "Thu, 13 Nov 2014 00:51:19 GMT"}], "update_date": "2014-11-14", "authors_parsed": [["Mineiro", "Paul", ""], ["Karampatziakis", "Nikos", ""]]}, {"id": "1411.3413", "submitter": "Tomoharu Iwata", "authors": "Tomoharu Iwata, Makoto Yamada", "title": "Multi-view Anomaly Detection via Probabilistic Latent Variable Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a nonparametric Bayesian probabilistic latent variable model for\nmulti-view anomaly detection, which is the task of finding instances that have\ninconsistent views. With the proposed model, all views of a non-anomalous\ninstance are assumed to be generated from a single latent vector. On the other\nhand, an anomalous instance is assumed to have multiple latent vectors, and its\ndifferent views are generated from different latent vectors. By inferring the\nnumber of latent vectors used for each instance with Dirichlet process priors,\nwe obtain multi-view anomaly scores. The proposed model can be seen as a robust\nextension of probabilistic canonical correlation analysis for noisy multi-view\ndata. We present Bayesian inference procedures for the proposed model based on\na stochastic EM algorithm. The effectiveness of the proposed model is\ndemonstrated in terms of performance when detecting multi-view anomalies and\nimputing missing values in multi-view data with anomalies.\n", "versions": [{"version": "v1", "created": "Thu, 13 Nov 2014 01:01:01 GMT"}], "update_date": "2014-11-14", "authors_parsed": [["Iwata", "Tomoharu", ""], ["Yamada", "Makoto", ""]]}, {"id": "1411.3436", "submitter": "Shai Shalev-Shwartz", "authors": "Shai Shalev-Shwartz", "title": "SelfieBoost: A Boosting Algorithm for Deep Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe and analyze a new boosting algorithm for deep learning called\nSelfieBoost. Unlike other boosting algorithms, like AdaBoost, which construct\nensembles of classifiers, SelfieBoost boosts the accuracy of a single network.\nWe prove a $\\log(1/\\epsilon)$ convergence rate for SelfieBoost under some \"SGD\nsuccess\" assumption which seems to hold in practice.\n", "versions": [{"version": "v1", "created": "Thu, 13 Nov 2014 03:34:32 GMT"}, {"version": "v2", "created": "Sat, 8 Apr 2017 06:06:38 GMT"}], "update_date": "2017-04-11", "authors_parsed": [["Shalev-Shwartz", "Shai", ""]]}, {"id": "1411.3553", "submitter": "Shaobo Lin", "authors": "Lin Xu, Shaobo Lin, Jinshan Zeng, Zongben Xu", "title": "Greedy metrics in orthogonal greedy learning", "comments": "33 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Orthogonal greedy learning (OGL) is a stepwise learning scheme that adds a\nnew atom from a dictionary via the steepest gradient descent and build the\nestimator via orthogonal projecting the target function to the space spanned by\nthe selected atoms in each greedy step. Here, \"greed\" means choosing a new atom\naccording to the steepest gradient descent principle. OGL then avoids the\noverfitting/underfitting by selecting an appropriate iteration number. In this\npaper, we point out that the overfitting/underfitting can also be avoided via\nredefining \"greed\" in OGL. To this end, we introduce a new greedy metric,\ncalled $\\delta$-greedy thresholds, to refine \"greed\" and theoretically verifies\nits feasibility. Furthermore, we reveals that such a greedy metric can bring an\nadaptive termination rule on the premise of maintaining the prominent learning\nperformance of OGL. Our results show that the steepest gradient descent is not\nthe unique greedy metric of OGL and some other more suitable metric may lessen\nthe hassle of model-selection of OGL.\n", "versions": [{"version": "v1", "created": "Thu, 13 Nov 2014 14:26:11 GMT"}], "update_date": "2014-11-14", "authors_parsed": [["Xu", "Lin", ""], ["Lin", "Shaobo", ""], ["Zeng", "Jinshan", ""], ["Xu", "Zongben", ""]]}, {"id": "1411.3652", "submitter": "SaiDhiraj Amuru", "authors": "SaiDhiraj Amuru, Cem Tekin, Mihaela van der Schaar, R. Michael Buehrer", "title": "Jamming Bandits", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.LG math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Can an intelligent jammer learn and adapt to unknown environments in an\nelectronic warfare-type scenario? In this paper, we answer this question in the\npositive, by developing a cognitive jammer that adaptively and optimally\ndisrupts the communication between a victim transmitter-receiver pair. We\nformalize the problem using a novel multi-armed bandit framework where the\njammer can choose various physical layer parameters such as the signaling\nscheme, power level and the on-off/pulsing duration in an attempt to obtain\npower efficient jamming strategies. We first present novel online learning\nalgorithms to maximize the jamming efficacy against static transmitter-receiver\npairs and prove that our learning algorithm converges to the optimal (in terms\nof the error rate inflicted at the victim and the energy used) jamming\nstrategy. Even more importantly, we prove that the rate of convergence to the\noptimal jamming strategy is sub-linear, i.e. the learning is fast in comparison\nto existing reinforcement learning algorithms, which is particularly important\nin dynamically changing wireless environments. Also, we characterize the\nperformance of the proposed bandit-based learning algorithm against multiple\nstatic and adaptive transmitter-receiver pairs.\n", "versions": [{"version": "v1", "created": "Thu, 13 Nov 2014 18:31:34 GMT"}], "update_date": "2014-11-14", "authors_parsed": [["Amuru", "SaiDhiraj", ""], ["Tekin", "Cem", ""], ["van der Schaar", "Mihaela", ""], ["Buehrer", "R. Michael", ""]]}, {"id": "1411.3698", "submitter": "Qingqing Huang", "authors": "Qingqing Huang, Rong Ge, Sham Kakade, Munther Dahleh", "title": "Minimal Realization Problems for Hidden Markov Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Consider a stationary discrete random process with alphabet size d, which is\nassumed to be the output process of an unknown stationary Hidden Markov Model\n(HMM). Given the joint probabilities of finite length strings of the process,\nwe are interested in finding a finite state generative model to describe the\nentire process. In particular, we focus on two classes of models: HMMs and\nquasi-HMMs, which is a strictly larger class of models containing HMMs. In the\nmain theorem, we show that if the random process is generated by an HMM of\norder less or equal than k, and whose transition and observation probability\nmatrix are in general position, namely almost everywhere on the parameter\nspace, both the minimal quasi-HMM realization and the minimal HMM realization\ncan be efficiently computed based on the joint probabilities of all the length\nN strings, for N > 4 lceil log_d(k) rceil +1. In this paper, we also aim to\ncompare and connect the two lines of literature: realization theory of HMMs,\nand the recent development in learning latent variable models with tensor\ndecomposition techniques.\n", "versions": [{"version": "v1", "created": "Thu, 13 Nov 2014 20:30:06 GMT"}, {"version": "v2", "created": "Mon, 14 Dec 2015 19:48:40 GMT"}], "update_date": "2015-12-15", "authors_parsed": [["Huang", "Qingqing", ""], ["Ge", "Rong", ""], ["Kakade", "Sham", ""], ["Dahleh", "Munther", ""]]}, {"id": "1411.3715", "submitter": "Daniele Barchiesi", "authors": "Daniele Barchiesi, Dimitrios Giannoulis, Dan Stowell, Mark D. Plumbley", "title": "Acoustic Scene Classification", "comments": null, "journal-ref": "IEEE Signal Processing Magazine 32(3) (May 2015) 16-34", "doi": "10.1109/MSP.2014.2326181", "report-no": null, "categories": "cs.SD cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this article we present an account of the state-of-the-art in acoustic\nscene classification (ASC), the task of classifying environments from the\nsounds they produce. Starting from a historical review of previous research in\nthis area, we define a general framework for ASC and present different imple-\nmentations of its components. We then describe a range of different algorithms\nsubmitted for a data challenge that was held to provide a general and fair\nbenchmark for ASC techniques. The dataset recorded for this purpose is\npresented, along with the performance metrics that are used to evaluate the\nalgorithms and statistical significance tests to compare the submitted methods.\nWe use a baseline method that employs MFCCS, GMMS and a maximum likelihood\ncriterion as a benchmark, and only find sufficient evidence to conclude that\nthree algorithms significantly outperform it. We also evaluate the human\nclassification accuracy in performing a similar classification task. The best\nperforming algorithm achieves a mean accuracy that matches the median accuracy\nobtained by humans, and common pairs of classes are misclassified by both\ncomputers and humans. However, all acoustic scenes are correctly classified by\nat least some individuals, while there are scenes that are misclassified by all\nalgorithms.\n", "versions": [{"version": "v1", "created": "Thu, 13 Nov 2014 16:03:09 GMT"}], "update_date": "2015-04-08", "authors_parsed": [["Barchiesi", "Daniele", ""], ["Giannoulis", "Dimitrios", ""], ["Stowell", "Dan", ""], ["Plumbley", "Mark D.", ""]]}, {"id": "1411.3784", "submitter": "Guido F.  Montufar", "authors": "Guido Montufar", "title": "Deep Narrow Boltzmann Machines are Universal Approximators", "comments": "Published as a conference paper at ICLR 2015", "journal-ref": "http://www.iclr.cc/doku.php?id=iclr2015:main", "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.PR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We show that deep narrow Boltzmann machines are universal approximators of\nprobability distributions on the activities of their visible units, provided\nthey have sufficiently many hidden layers, each containing the same number of\nunits as the visible layer. We show that, within certain parameter domains,\ndeep Boltzmann machines can be studied as feedforward networks. We provide\nupper and lower bounds on the sufficient depth and width of universal\napproximators. These results settle various intuitions regarding undirected\nnetworks and, in particular, they show that deep narrow Boltzmann machines are\nat least as compact universal approximators as narrow sigmoid belief networks\nand restricted Boltzmann machines, with respect to the currently available\nbounds for those models.\n", "versions": [{"version": "v1", "created": "Fri, 14 Nov 2014 03:50:30 GMT"}, {"version": "v2", "created": "Thu, 26 Feb 2015 18:59:27 GMT"}, {"version": "v3", "created": "Fri, 10 Apr 2015 12:22:14 GMT"}], "update_date": "2015-04-13", "authors_parsed": [["Montufar", "Guido", ""]]}, {"id": "1411.3787", "submitter": "Ping Li", "authors": "Anshumali Shrivastava, Ping Li", "title": "Asymmetric Minwise Hashing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.DB cs.DS cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Minwise hashing (Minhash) is a widely popular indexing scheme in practice.\nMinhash is designed for estimating set resemblance and is known to be\nsuboptimal in many applications where the desired measure is set overlap (i.e.,\ninner product between binary vectors) or set containment. Minhash has inherent\nbias towards smaller sets, which adversely affects its performance in\napplications where such a penalization is not desirable. In this paper, we\npropose asymmetric minwise hashing (MH-ALSH), to provide a solution to this\nproblem. The new scheme utilizes asymmetric transformations to cancel the bias\nof traditional minhash towards smaller sets, making the final \"collision\nprobability\" monotonic in the inner product. Our theoretical comparisons show\nthat for the task of retrieving with binary inner products asymmetric minhash\nis provably better than traditional minhash and other recently proposed hashing\nalgorithms for general inner products. Thus, we obtain an algorithmic\nimprovement over existing approaches in the literature. Experimental\nevaluations on four publicly available high-dimensional datasets validate our\nclaims and the proposed scheme outperforms, often significantly, other hashing\nalgorithms on the task of near neighbor retrieval with set containment. Our\nproposal is simple and easy to implement in practice.\n", "versions": [{"version": "v1", "created": "Fri, 14 Nov 2014 04:18:33 GMT"}], "update_date": "2014-11-17", "authors_parsed": [["Shrivastava", "Anshumali", ""], ["Li", "Ping", ""]]}, {"id": "1411.3815", "submitter": "Mingmin Zhao", "authors": "Mingmin Zhao, Chengxu Zhuang, Yizhou Wang, Tai Sing Lee", "title": "Predictive Encoding of Contextual Relationships for Perceptual\n  Inference, Interpolation and Prediction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new neurally-inspired model that can learn to encode the global\nrelationship context of visual events across time and space and to use the\ncontextual information to modulate the analysis by synthesis process in a\npredictive coding framework. The model learns latent contextual representations\nby maximizing the predictability of visual events based on local and global\ncontextual information through both top-down and bottom-up processes. In\ncontrast to standard predictive coding models, the prediction error in this\nmodel is used to update the contextual representation but does not alter the\nfeedforward input for the next layer, and is thus more consistent with\nneurophysiological observations. We establish the computational feasibility of\nthis model by demonstrating its ability in several aspects. We show that our\nmodel can outperform state-of-art performances of gated Boltzmann machines\n(GBM) in estimation of contextual information. Our model can also interpolate\nmissing events or predict future events in image sequences while simultaneously\nestimating contextual information. We show it achieves state-of-art\nperformances in terms of prediction accuracy in a variety of tasks and\npossesses the ability to interpolate missing frames, a function that is lacking\nin GBM.\n", "versions": [{"version": "v1", "created": "Fri, 14 Nov 2014 07:38:45 GMT"}, {"version": "v2", "created": "Sun, 21 Dec 2014 00:08:05 GMT"}, {"version": "v3", "created": "Wed, 24 Dec 2014 12:05:56 GMT"}, {"version": "v4", "created": "Mon, 2 Mar 2015 12:50:42 GMT"}, {"version": "v5", "created": "Fri, 10 Apr 2015 17:52:12 GMT"}, {"version": "v6", "created": "Thu, 16 Apr 2015 15:57:36 GMT"}], "update_date": "2015-04-17", "authors_parsed": [["Zhao", "Mingmin", ""], ["Zhuang", "Chengxu", ""], ["Wang", "Yizhou", ""], ["Lee", "Tai Sing", ""]]}, {"id": "1411.3895", "submitter": "Ismael Rodr\\'iguez-Fdez M.Sc", "authors": "I. Rodr\\'iguez-Fdez, M. Mucientes, A. Bugar\\'in", "title": "Learning Fuzzy Controllers in Mobile Robotics with Embedded\n  Preprocessing", "comments": null, "journal-ref": "Applied Soft Computing, vol 26, pp. 123-142, 2015", "doi": "10.1016/j.asoc.2014.09.021", "report-no": null, "categories": "cs.RO cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The automatic design of controllers for mobile robots usually requires two\nstages. In the first stage,sensorial data are preprocessed or transformed into\nhigh level and meaningful values of variables whichare usually defined from\nexpert knowledge. In the second stage, a machine learning technique is applied\ntoobtain a controller that maps these high level variables to the control\ncommands that are actually sent tothe robot. This paper describes an algorithm\nthat is able to embed the preprocessing stage into the learningstage in order\nto get controllers directly starting from sensorial raw data with no expert\nknowledgeinvolved. Due to the high dimensionality of the sensorial data, this\napproach uses Quantified Fuzzy Rules(QFRs), that are able to transform\nlow-level input variables into high-level input variables, reducingthe\ndimensionality through summarization. The proposed learning algorithm, called\nIterative QuantifiedFuzzy Rule Learning (IQFRL), is based on genetic\nprogramming. IQFRL is able to learn rules with differentstructures, and can\nmanage linguistic variables with multiple granularities. The algorithm has been\ntestedwith the implementation of the wall-following behavior both in several\nrealistic simulated environmentswith different complexity and on a Pioneer 3-AT\nrobot in two real environments. Results have beencompared with several\nwell-known learning algorithms combined with different data\npreprocessingtechniques, showing that IQFRL exhibits a better and statistically\nsignificant performance. Moreover,three real world applications for which IQFRL\nplays a central role are also presented: path and objecttracking with static\nand moving obstacles avoidance.\n", "versions": [{"version": "v1", "created": "Fri, 14 Nov 2014 13:11:32 GMT"}], "update_date": "2014-11-17", "authors_parsed": [["Rodr\u00edguez-Fdez", "I.", ""], ["Mucientes", "M.", ""], ["Bugar\u00edn", "A.", ""]]}, {"id": "1411.3919", "submitter": "Ognjen Arandjelovi\\'c PhD", "authors": "Ognjen Arandjelovic", "title": "Sample-targeted clinical trial adaptation", "comments": "AAAI Conference on Artificial Intelligence, 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Clinical trial adaptation refers to any adjustment of the trial protocol\nafter the onset of the trial. The main goal is to make the process of\nintroducing new medical interventions to patients more efficient by reducing\nthe cost and the time associated with evaluating their safety and efficacy. The\nprincipal question is how should adaptation be performed so as to minimize the\nchance of distorting the outcome of the trial. We propose a novel method for\nachieving this. Unlike previous work our approach focuses on trial adaptation\nby sample size adjustment. We adopt a recently proposed stratification\nframework based on collected auxiliary data and show that this information\ntogether with the primary measured variables can be used to make a\nprobabilistically informed choice of the particular sub-group a sample should\nbe removed from. Experiments on simulated data are used to illustrate the\neffectiveness of our method and its application in practice.\n", "versions": [{"version": "v1", "created": "Fri, 14 Nov 2014 14:30:27 GMT"}], "update_date": "2014-11-17", "authors_parsed": [["Arandjelovic", "Ognjen", ""]]}, {"id": "1411.4000", "submitter": "Fei Sha", "authors": "Zhiyun Lu and Avner May and Kuan Liu and Alireza Bagheri Garakani and\n  Dong Guo and Aur\\'elien Bellet and Linxi Fan and Michael Collins and Brian\n  Kingsbury and Michael Picheny and Fei Sha", "title": "How to Scale Up Kernel Methods to Be As Good As Deep Neural Nets", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The computational complexity of kernel methods has often been a major barrier\nfor applying them to large-scale learning problems. We argue that this barrier\ncan be effectively overcome. In particular, we develop methods to scale up\nkernel models to successfully tackle large-scale learning problems that are so\nfar only approachable by deep learning architectures. Based on the seminal work\nby Rahimi and Recht on approximating kernel functions with features derived\nfrom random projections, we advance the state-of-the-art by proposing methods\nthat can efficiently train models with hundreds of millions of parameters, and\nlearn optimal representations from multiple kernels. We conduct extensive\nempirical studies on problems from image recognition and automatic speech\nrecognition, and show that the performance of our kernel models matches that of\nwell-engineered deep neural nets (DNNs). To the best of our knowledge, this is\nthe first time that a direct comparison between these two methods on\nlarge-scale problems is reported. Our kernel methods have several appealing\nproperties: training with convex optimization, cost for training a single model\ncomparable to DNNs, and significantly reduced total cost due to fewer\nhyperparameters to tune for model selection. Our contrastive study between\nthese two very different but equally competitive models sheds light on\nfundamental questions such as how to learn good representations.\n", "versions": [{"version": "v1", "created": "Fri, 14 Nov 2014 18:24:20 GMT"}, {"version": "v2", "created": "Wed, 17 Jun 2015 10:31:35 GMT"}], "update_date": "2019-01-25", "authors_parsed": [["Lu", "Zhiyun", ""], ["May", "Avner", ""], ["Liu", "Kuan", ""], ["Garakani", "Alireza Bagheri", ""], ["Guo", "Dong", ""], ["Bellet", "Aur\u00e9lien", ""], ["Fan", "Linxi", ""], ["Collins", "Michael", ""], ["Kingsbury", "Brian", ""], ["Picheny", "Michael", ""], ["Sha", "Fei", ""]]}, {"id": "1411.4046", "submitter": "Mohammad Ali Keyvanrad", "authors": "Mohammad Ali Keyvanrad, Mohammad Mehdi Homayounpour", "title": "Deep Belief Network Training Improvement Using Elite Samples Minimizing\n  Free Energy", "comments": "18 pages. arXiv admin note: substantial text overlap with\n  arXiv:1408.3264", "journal-ref": "Int. J. Patt. Recogn. Artif. Intell. 29, 1551006 (2015)", "doi": "10.1142/S0218001415510064", "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Nowadays this is very popular to use deep architectures in machine learning.\nDeep Belief Networks (DBNs) are deep architectures that use stack of Restricted\nBoltzmann Machines (RBM) to create a powerful generative model using training\ndata. In this paper we present an improvement in a common method that is\nusually used in training of RBMs. The new method uses free energy as a\ncriterion to obtain elite samples from generative model. We argue that these\nsamples can more accurately compute gradient of log probability of training\ndata. According to the results, an error rate of 0.99% was achieved on MNIST\ntest set. This result shows that the proposed method outperforms the method\npresented in the first paper introducing DBN (1.25% error rate) and general\nclassification methods such as SVM (1.4% error rate) and KNN (with 1.6% error\nrate). In another test using ISOLET dataset, letter classification error\ndropped to 3.59% compared to 5.59% error rate achieved in those papers using\nthis dataset. The implemented method is available online at\n\"http://ceit.aut.ac.ir/~keyvanrad/DeeBNet Toolbox.html\".\n", "versions": [{"version": "v1", "created": "Fri, 14 Nov 2014 16:57:48 GMT"}], "update_date": "2015-08-21", "authors_parsed": [["Keyvanrad", "Mohammad Ali", ""], ["Homayounpour", "Mohammad Mehdi", ""]]}, {"id": "1411.4068", "submitter": "Anh Pham The", "authors": "Anh T. Pham, Raviv Raich, and Xiaoli Z. Fern", "title": "Dynamic Programming for Instance Annotation in Multi-instance\n  Multi-label Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Labeling data for classification requires significant human effort. To reduce\nlabeling cost, instead of labeling every instance, a group of instances (bag)\nis labeled by a single bag label. Computer algorithms are then used to infer\nthe label for each instance in a bag, a process referred to as instance\nannotation. This task is challenging due to the ambiguity regarding the\ninstance labels. We propose a discriminative probabilistic model for the\ninstance annotation problem and introduce an expectation maximization framework\nfor inference, based on the maximum likelihood approach. For many probabilistic\napproaches, brute-force computation of the instance label posterior probability\ngiven its bag label is exponential in the number of instances in the bag. Our\nkey contribution is a dynamic programming method for computing the posterior\nthat is linear in the number of instances. We evaluate our methods using both\nbenchmark and real world data sets, in the domain of bird song, image\nannotation, and activity recognition. In many cases, the proposed framework\noutperforms, sometimes significantly, the current state-of-the-art MIML\nlearning methods, both in instance label prediction and bag label prediction.\n", "versions": [{"version": "v1", "created": "Fri, 14 Nov 2014 21:59:38 GMT"}], "update_date": "2014-11-18", "authors_parsed": [["Pham", "Anh T.", ""], ["Raich", "Raviv", ""], ["Fern", "Xiaoli Z.", ""]]}, {"id": "1411.4070", "submitter": "Abigail Jacobs", "authors": "Abigail Z. Jacobs and Aaron Clauset", "title": "A unified view of generative models for networks: models, methods,\n  opportunities, and challenges", "comments": "10 pages. To appear at the NIPS 2014 Workshop on Networks: From\n  Graphs to Rich Data", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG cs.SI physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Research on probabilistic models of networks now spans a wide variety of\nfields, including physics, sociology, biology, statistics, and machine\nlearning. These efforts have produced a diverse ecology of models and methods.\nDespite this diversity, many of these models share a common underlying\nstructure: pairwise interactions (edges) are generated with probability\nconditional on latent vertex attributes. Differences between models generally\nstem from different philosophical choices about how to learn from data or\ndifferent empirically-motivated goals. The highly interdisciplinary nature of\nwork on these generative models, however, has inhibited the development of a\nunified view of their similarities and differences. For instance, novel\ntheoretical models and optimization techniques developed in machine learning\nare largely unknown within the social and biological sciences, which have\ninstead emphasized model interpretability. Here, we describe a unified view of\ngenerative models for networks that draws together many of these disparate\nthreads and highlights the fundamental similarities and differences that span\nthese fields. We then describe a number of opportunities and challenges for\nfuture work that are revealed by this view.\n", "versions": [{"version": "v1", "created": "Fri, 14 Nov 2014 22:00:41 GMT"}], "update_date": "2014-11-18", "authors_parsed": [["Jacobs", "Abigail Z.", ""], ["Clauset", "Aaron", ""]]}, {"id": "1411.4072", "submitter": "Bishan Yang", "authors": "Bishan Yang, Wen-tau Yih, Xiaodong He, Jianfeng Gao, Li Deng", "title": "Learning Multi-Relational Semantics Using Neural-Embedding Models", "comments": "7 pages, 2 figures, NIPS 2014 workshop on Learning Semantics", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we present a unified framework for modeling multi-relational\nrepresentations, scoring, and learning, and conduct an empirical study of\nseveral recent multi-relational embedding models under the framework. We\ninvestigate the different choices of relation operators based on linear and\nbilinear transformations, and also the effects of entity representations by\nincorporating unsupervised vectors pre-trained on extra textual resources. Our\nresults show several interesting findings, enabling the design of a simple\nembedding model that achieves the new state-of-the-art performance on a popular\nknowledge base completion task evaluated on Freebase.\n", "versions": [{"version": "v1", "created": "Fri, 14 Nov 2014 22:08:01 GMT"}], "update_date": "2014-11-18", "authors_parsed": [["Yang", "Bishan", ""], ["Yih", "Wen-tau", ""], ["He", "Xiaodong", ""], ["Gao", "Jianfeng", ""], ["Deng", "Li", ""]]}, {"id": "1411.4076", "submitter": "Amiraj Dhawan", "authors": "Amiraj Dhawan, Shruti Bhave, Amrita Aurora, Vishwanathan Iyer", "title": "Association Rule Based Flexible Machine Learning Module for Embedded\n  System Platforms like Android", "comments": "International Journal of Advanced Research in Artificial\n  Intelligence(IJARAI), Volume 3 Issue 1, 2014", "journal-ref": null, "doi": "10.14569/IJARAI.2014.030101", "report-no": null, "categories": "cs.CY cs.HC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The past few years have seen a tremendous growth in the popularity of\nsmartphones. As newer features continue to be added to smartphones to increase\ntheir utility, their significance will only increase in future. Combining\nmachine learning with mobile computing can enable smartphones to become\n'intelligent' devices, a feature which is hitherto unseen in them. Also, the\ncombination of machine learning and context aware computing can enable\nsmartphones to gauge user's requirements proactively, depending upon their\nenvironment and context. Accordingly, necessary services can be provided to\nusers.\n  In this paper, we have explored the methods and applications of integrating\nmachine learning and context aware computing on the Android platform, to\nprovide higher utility to the users. To achieve this, we define a Machine\nLearning (ML) module which is incorporated in the basic Android architecture.\nFirstly, we have outlined two major functionalities that the ML module should\nprovide. Then, we have presented three architectures, each of which\nincorporates the ML module at a different level in the Android architecture.\nThe advantages and shortcomings of each of these architectures have been\nevaluated. Lastly, we have explained a few applications in which our proposed\nsystem can be incorporated such that their functionality is improved.\n", "versions": [{"version": "v1", "created": "Fri, 14 Nov 2014 22:55:13 GMT"}], "update_date": "2014-11-18", "authors_parsed": [["Dhawan", "Amiraj", ""], ["Bhave", "Shruti", ""], ["Aurora", "Amrita", ""], ["Iyer", "Vishwanathan", ""]]}, {"id": "1411.4086", "submitter": "Hongwei Li", "authors": "Hongwei Li and Bin Yu", "title": "Error Rate Bounds and Iterative Weighted Majority Voting for\n  Crowdsourcing", "comments": "Journal Submission", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.HC cs.LG math.PR math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Crowdsourcing has become an effective and popular tool for human-powered\ncomputation to label large datasets. Since the workers can be unreliable, it is\ncommon in crowdsourcing to assign multiple workers to one task, and to\naggregate the labels in order to obtain results of high quality. In this paper,\nwe provide finite-sample exponential bounds on the error rate (in probability\nand in expectation) of general aggregation rules under the Dawid-Skene\ncrowdsourcing model. The bounds are derived for multi-class labeling, and can\nbe used to analyze many aggregation methods, including majority voting,\nweighted majority voting and the oracle Maximum A Posteriori (MAP) rule. We\nshow that the oracle MAP rule approximately optimizes our upper bound on the\nmean error rate of weighted majority voting in certain setting. We propose an\niterative weighted majority voting (IWMV) method that optimizes the error rate\nbound and approximates the oracle MAP rule. Its one step version has a provable\ntheoretical guarantee on the error rate. The IWMV method is intuitive and\ncomputationally simple. Experimental results on simulated and real data show\nthat IWMV performs at least on par with the state-of-the-art methods, and it\nhas a much lower computational cost (around one hundred times faster) than the\nstate-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Sat, 15 Nov 2014 00:02:34 GMT"}], "update_date": "2014-11-18", "authors_parsed": [["Li", "Hongwei", ""], ["Yu", "Bin", ""]]}, {"id": "1411.4101", "submitter": "Rahul Mohan Mr.", "authors": "Rahul Mohan", "title": "Deep Deconvolutional Networks for Scene Parsing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  Scene parsing is an important and challenging prob- lem in computer vision.\nIt requires labeling each pixel in an image with the category it belongs to.\nTradition- ally, it has been approached with hand-engineered features from\ncolor information in images. Recently convolutional neural networks (CNNs),\nwhich automatically learn hierar- chies of features, have achieved record\nperformance on the task. These approaches typically include a post-processing\ntechnique, such as superpixels, to produce the final label- ing. In this paper,\nwe propose a novel network architecture that combines deep deconvolutional\nneural networks with CNNs. Our experiments show that deconvolutional neu- ral\nnetworks are capable of learning higher order image structure beyond edge\nprimitives in comparison to CNNs. The new network architecture is employed for\nmulti-patch training, introduced as part of this work. Multi-patch train- ing\nmakes it possible to effectively learn spatial priors from scenes. The proposed\napproach yields state-of-the-art per- formance on four scene parsing datasets,\nnamely Stanford Background, SIFT Flow, CamVid, and KITTI. In addition, our\nsystem has the added advantage of having a training system that can be\ncompletely automated end-to-end with- out requiring any post-processing.\n", "versions": [{"version": "v1", "created": "Sat, 15 Nov 2014 02:03:14 GMT"}], "update_date": "2014-11-18", "authors_parsed": [["Mohan", "Rahul", ""]]}, {"id": "1411.4102", "submitter": "Rahul Sawhney", "authors": "Rahul Sawhney, Henrik I. Christensen and Gary R. Bradski", "title": "Anisotropic Agglomerative Adaptive Mean-Shift", "comments": "British Machine Vision Conference, 2014", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mean Shift today, is widely used for mode detection and clustering. The\ntechnique though, is challenged in practice due to assumptions of isotropicity\nand homoscedasticity. We present an adaptive Mean Shift methodology that allows\nfor full anisotropic clustering, through unsupervised local bandwidth\nselection. The bandwidth matrices evolve naturally, adapting locally through\nagglomeration, and in turn guiding further agglomeration. The online\nmethodology is practical and effecive for low-dimensional feature spaces,\npreserving better detail and clustering salience. Additionally, conventional\nMean Shift either critically depends on a per instance choice of bandwidth, or\nrelies on offline methods which are inflexible and/or again data instance\nspecific. The presented approach, due to its adaptive design, also alleviates\nthis issue - with a default form performing generally well. The methodology\nthough, allows for effective tuning of results.\n", "versions": [{"version": "v1", "created": "Sat, 15 Nov 2014 02:05:22 GMT"}], "update_date": "2014-11-18", "authors_parsed": [["Sawhney", "Rahul", ""], ["Christensen", "Henrik I.", ""], ["Bradski", "Gary R.", ""]]}, {"id": "1411.4114", "submitter": "Jongwon Ha", "authors": "Ha Jong Won, Li Gwang Chol, Kim Hyok Chol, Li Kum Song (College of\n  Computer Science, Kim Il Sung University)", "title": "Definition of Visual Speech Element and Research on a Method of\n  Extracting Feature Vector for Korean Lip-Reading", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.CV cs.LG", "license": "http://creativecommons.org/licenses/publicdomain/", "abstract": "  In this paper, we defined the viseme (visual speech element) and described\nabout the method of extracting visual feature vector. We defined the 10 visemes\nbased on vowel by analyzing of Korean utterance and proposed the method of\nextracting the 20-dimensional visual feature vector, combination of static\nfeatures and dynamic features. Lastly, we took an experiment in recognizing\nwords based on 3-viseme HMM and evaluated the efficiency.\n", "versions": [{"version": "v1", "created": "Sat, 15 Nov 2014 05:44:10 GMT"}], "update_date": "2014-11-19", "authors_parsed": [["Won", "Ha Jong", "", "College of\n  Computer Science, Kim Il Sung University"], ["Chol", "Li Gwang", "", "College of\n  Computer Science, Kim Il Sung University"], ["Chol", "Kim Hyok", "", "College of\n  Computer Science, Kim Il Sung University"], ["Song", "Li Kum", "", "College of\n  Computer Science, Kim Il Sung University"]]}, {"id": "1411.4116", "submitter": "Jack Cheng J", "authors": "Jianpeng Cheng, Dimitri Kartsaklis, Edward Grefenstette", "title": "Investigating the Role of Prior Disambiguation in Deep-learning\n  Compositional Models of Meaning", "comments": "NIPS 2014", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper aims to explore the effect of prior disambiguation on neural\nnetwork- based compositional models, with the hope that better semantic\nrepresentations for text compounds can be produced. We disambiguate the input\nword vectors before they are fed into a compositional deep net. A series of\nevaluations shows the positive effect of prior disambiguation for such deep\nmodels.\n", "versions": [{"version": "v1", "created": "Sat, 15 Nov 2014 06:32:49 GMT"}], "update_date": "2014-11-18", "authors_parsed": [["Cheng", "Jianpeng", ""], ["Kartsaklis", "Dimitri", ""], ["Grefenstette", "Edward", ""]]}, {"id": "1411.4199", "submitter": "Ke Jiang", "authors": "Ke Jiang, Qichao Que, Brian Kulis", "title": "Revisiting Kernelized Locality-Sensitive Hashing for Improved\n  Large-Scale Image Retrieval", "comments": "15 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a simple but powerful reinterpretation of kernelized\nlocality-sensitive hashing (KLSH), a general and popular method developed in\nthe vision community for performing approximate nearest-neighbor searches in an\narbitrary reproducing kernel Hilbert space (RKHS). Our new perspective is based\non viewing the steps of the KLSH algorithm in an appropriately projected space,\nand has several key theoretical and practical benefits. First, it eliminates\nthe problematic conceptual difficulties that are present in the existing\nmotivation of KLSH. Second, it yields the first formal retrieval performance\nbounds for KLSH. Third, our analysis reveals two techniques for boosting the\nempirical performance of KLSH. We evaluate these extensions on several\nlarge-scale benchmark image retrieval data sets, and show that our analysis\nleads to improved recall performance of at least 12%, and sometimes much\nhigher, over the standard KLSH method.\n", "versions": [{"version": "v1", "created": "Sun, 16 Nov 2014 00:08:24 GMT"}], "update_date": "2014-11-18", "authors_parsed": [["Jiang", "Ke", ""], ["Que", "Qichao", ""], ["Kulis", "Brian", ""]]}, {"id": "1411.4286", "submitter": "Zhiwei Qin", "authors": "Zhiwei Qin, Xiaocheng Tang, Ioannis Akrotirianakis, Amit Chakraborty", "title": "HIPAD - A Hybrid Interior-Point Alternating Direction algorithm for\n  knowledge-based SVM and feature selection", "comments": "Proceedings of 8th Learning and Intelligent OptimizatioN (LION8)\n  Conference, 2014", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider classification tasks in the regime of scarce labeled training\ndata in high dimensional feature space, where specific expert knowledge is also\navailable. We propose a new hybrid optimization algorithm that solves the\nelastic-net support vector machine (SVM) through an alternating direction\nmethod of multipliers in the first phase, followed by an interior-point method\nfor the classical SVM in the second phase. Both SVM formulations are adapted to\nknowledge incorporation. Our proposed algorithm addresses the challenges of\nautomatic feature selection, high optimization accuracy, and algorithmic\nflexibility for taking advantage of prior knowledge. We demonstrate the\neffectiveness and efficiency of our algorithm and compare it with existing\nmethods on a collection of synthetic and real-world data.\n", "versions": [{"version": "v1", "created": "Sun, 16 Nov 2014 17:58:18 GMT"}], "update_date": "2014-11-18", "authors_parsed": [["Qin", "Zhiwei", ""], ["Tang", "Xiaocheng", ""], ["Akrotirianakis", "Ioannis", ""], ["Chakraborty", "Amit", ""]]}, {"id": "1411.4342", "submitter": "Kirthevasan Kandasamy", "authors": "Kirthevasan Kandasamy, Akshay Krishnamurthy, Barnabas Poczos, Larry\n  Wasserman, James M. Robins", "title": "Influence Functions for Machine Learning: Nonparametric Estimators for\n  Entropies, Divergences and Mutual Informations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose and analyze estimators for statistical functionals of one or more\ndistributions under nonparametric assumptions. Our estimators are based on the\ntheory of influence functions, which appear in the semiparametric statistics\nliterature. We show that estimators based either on data-splitting or a\nleave-one-out technique enjoy fast rates of convergence and other favorable\ntheoretical properties. We apply this framework to derive estimators for\nseveral popular information theoretic quantities, and via empirical evaluation,\nshow the advantage of this approach over existing estimators.\n", "versions": [{"version": "v1", "created": "Mon, 17 Nov 2014 02:04:57 GMT"}, {"version": "v2", "created": "Tue, 10 Mar 2015 18:47:41 GMT"}, {"version": "v3", "created": "Fri, 19 Jun 2015 23:29:07 GMT"}], "update_date": "2015-07-21", "authors_parsed": [["Kandasamy", "Kirthevasan", ""], ["Krishnamurthy", "Akshay", ""], ["Poczos", "Barnabas", ""], ["Wasserman", "Larry", ""], ["Robins", "James M.", ""]]}, {"id": "1411.4455", "submitter": "Miao Fan", "authors": "Miao Fan, Deli Zhao, Qiang Zhou, Zhiyuan Liu, Thomas Fang Zheng,\n  Edward Y. Chang", "title": "Errata: Distant Supervision for Relation Extraction with Matrix\n  Completion", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The essence of distantly supervised relation extraction is that it is an\nincomplete multi-label classification problem with sparse and noisy features.\nTo tackle the sparsity and noise challenges, we propose solving the\nclassification problem using matrix completion on factorized matrix of\nminimized rank. We formulate relation classification as completing the unknown\nlabels of testing items (entity pairs) in a sparse matrix that concatenates\ntraining and testing textual features with training labels. Our algorithmic\nframework is based on the assumption that the rank of item-by-feature and\nitem-by-label joint matrix is low. We apply two optimization models to recover\nthe underlying low-rank matrix leveraging the sparsity of feature-label matrix.\nThe matrix completion problem is then solved by the fixed point continuation\n(FPC) algorithm, which can find the global optimum. Experiments on two widely\nused datasets with different dimensions of textual features demonstrate that\nour low-rank matrix completion approach significantly outperforms the baseline\nand the state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Mon, 17 Nov 2014 12:43:30 GMT"}], "update_date": "2014-11-18", "authors_parsed": [["Fan", "Miao", ""], ["Zhao", "Deli", ""], ["Zhou", "Qiang", ""], ["Liu", "Zhiyuan", ""], ["Zheng", "Thomas Fang", ""], ["Chang", "Edward Y.", ""]]}, {"id": "1411.4491", "submitter": "Basura Fernando", "authors": "Basura Fernando and Tatiana Tommasi and Tinne Tuytelaars", "title": "Joint cross-domain classification and subspace learning for unsupervised\n  adaptation", "comments": "Paper is under consideration at Pattern Recognition Letters", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Domain adaptation aims at adapting the knowledge acquired on a source domain\nto a new different but related target domain. Several approaches have\nbeenproposed for classification tasks in the unsupervised scenario, where no\nlabeled target data are available. Most of the attention has been dedicated to\nsearching a new domain-invariant representation, leaving the definition of the\nprediction function to a second stage. Here we propose to learn both jointly.\nSpecifically we learn the source subspace that best matches the target subspace\nwhile at the same time minimizing a regularized misclassification loss. We\nprovide an alternating optimization technique based on stochastic sub-gradient\ndescent to solve the learning problem and we demonstrate its performance on\nseveral domain adaptation tasks.\n", "versions": [{"version": "v1", "created": "Mon, 17 Nov 2014 14:29:35 GMT"}, {"version": "v2", "created": "Tue, 18 Nov 2014 15:55:50 GMT"}, {"version": "v3", "created": "Wed, 29 Apr 2015 02:51:00 GMT"}], "update_date": "2015-04-30", "authors_parsed": [["Fernando", "Basura", ""], ["Tommasi", "Tatiana", ""], ["Tuytelaars", "Tinne", ""]]}, {"id": "1411.4503", "submitter": "Itamar Katz", "authors": "Itamar Katz and Koby Crammer", "title": "Outlier-Robust Convex Segmentation", "comments": "* Accepted to AAAI-15, this version includes the\n  appendix/supplementary material referenced in the AAAI-15 submission, as well\n  as color figures * This version include some minor typos correction", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We derive a convex optimization problem for the task of segmenting sequential\ndata, which explicitly treats presence of outliers. We describe two algorithms\nfor solving this problem, one exact and one a top-down novel approach, and we\nderive a consistency results for the case of two segments and no outliers.\nRobustness to outliers is evaluated on two real-world tasks related to speech\nsegmentation. Our algorithms outperform baseline segmentation algorithms.\n", "versions": [{"version": "v1", "created": "Mon, 17 Nov 2014 14:59:25 GMT"}, {"version": "v2", "created": "Tue, 18 Nov 2014 07:59:33 GMT"}], "update_date": "2014-11-19", "authors_parsed": [["Katz", "Itamar", ""], ["Crammer", "Koby", ""]]}, {"id": "1411.4510", "submitter": "Kian Hsiang Low", "authors": "Kian Hsiang Low, Jiangbo Yu, Jie Chen, Patrick Jaillet", "title": "Parallel Gaussian Process Regression for Big Data: Low-Rank\n  Representation Meets Markov Approximation", "comments": "29th AAAI Conference on Artificial Intelligence (AAAI 2015), Extended\n  version with proofs, 10 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.DC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The expressive power of a Gaussian process (GP) model comes at a cost of poor\nscalability in the data size. To improve its scalability, this paper presents a\nlow-rank-cum-Markov approximation (LMA) of the GP model that is novel in\nleveraging the dual computational advantages stemming from complementing a\nlow-rank approximate representation of the full-rank GP based on a support set\nof inputs with a Markov approximation of the resulting residual process; the\nlatter approximation is guaranteed to be closest in the Kullback-Leibler\ndistance criterion subject to some constraint and is considerably more refined\nthan that of existing sparse GP models utilizing low-rank representations due\nto its more relaxed conditional independence assumption (especially with larger\ndata). As a result, our LMA method can trade off between the size of the\nsupport set and the order of the Markov property to (a) incur lower\ncomputational cost than such sparse GP models while achieving predictive\nperformance comparable to them and (b) accurately represent features/patterns\nof any scale. Interestingly, varying the Markov order produces a spectrum of\nLMAs with PIC approximation and full-rank GP at the two extremes. An advantage\nof our LMA method is that it is amenable to parallelization on multiple\nmachines/cores, thereby gaining greater scalability. Empirical evaluation on\nthree real-world datasets in clusters of up to 32 computing nodes shows that\nour centralized and parallel LMA methods are significantly more time-efficient\nand scalable than state-of-the-art sparse and full-rank GP regression methods\nwhile achieving comparable predictive performances.\n", "versions": [{"version": "v1", "created": "Mon, 17 Nov 2014 15:31:04 GMT"}], "update_date": "2014-11-18", "authors_parsed": [["Low", "Kian Hsiang", ""], ["Yu", "Jiangbo", ""], ["Chen", "Jie", ""], ["Jaillet", "Patrick", ""]]}, {"id": "1411.4521", "submitter": "Jesse Krijthe", "authors": "Jesse H. Krijthe and Marco Loog", "title": "Implicitly Constrained Semi-Supervised Linear Discriminant Analysis", "comments": "6 pages, 3 figures and 3 tables. International Conference on Pattern\n  Recognition (ICPR) 2014, Stockholm, Sweden", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Semi-supervised learning is an important and active topic of research in\npattern recognition. For classification using linear discriminant analysis\nspecifically, several semi-supervised variants have been proposed. Using any\none of these methods is not guaranteed to outperform the supervised classifier\nwhich does not take the additional unlabeled data into account. In this work we\ncompare traditional Expectation Maximization type approaches for\nsemi-supervised linear discriminant analysis with approaches based on intrinsic\nconstraints and propose a new principled approach for semi-supervised linear\ndiscriminant analysis, using so-called implicit constraints. We explore the\nrelationships between these methods and consider the question if and in what\nsense we can expect improvement in performance over the supervised procedure.\nThe constraint based approaches are more robust to misspecification of the\nmodel, and may outperform alternatives that make more assumptions on the data,\nin terms of the log-likelihood of unseen objects.\n", "versions": [{"version": "v1", "created": "Mon, 17 Nov 2014 15:57:11 GMT"}], "update_date": "2014-11-18", "authors_parsed": [["Krijthe", "Jesse H.", ""], ["Loog", "Marco", ""]]}, {"id": "1411.4738", "submitter": "Cuicui Kang", "authors": "Cuicui Kang, Shengcai Liao, Yonghao He, Jian Wang, Wenjia Niu, Shiming\n  Xiang, Chunhong Pan", "title": "Cross-Modal Similarity Learning : A Low Rank Bilinear Formulation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The cross-media retrieval problem has received much attention in recent years\ndue to the rapid increasing of multimedia data on the Internet. A new approach\nto the problem has been raised which intends to match features of different\nmodalities directly. In this research, there are two critical issues: how to\nget rid of the heterogeneity between different modalities and how to match the\ncross-modal features of different dimensions. Recently metric learning methods\nshow a good capability in learning a distance metric to explore the\nrelationship between data points. However, the traditional metric learning\nalgorithms only focus on single-modal features, which suffer difficulties in\naddressing the cross-modal features of different dimensions. In this paper, we\npropose a cross-modal similarity learning algorithm for the cross-modal feature\nmatching. The proposed method takes a bilinear formulation, and with the\nnuclear-norm penalization, it achieves low-rank representation. Accordingly,\nthe accelerated proximal gradient algorithm is successfully imported to find\nthe optimal solution with a fast convergence rate O(1/t^2). Experiments on\nthree well known image-text cross-media retrieval databases show that the\nproposed method achieves the best performance compared to the state-of-the-art\nalgorithms.\n", "versions": [{"version": "v1", "created": "Tue, 18 Nov 2014 05:53:06 GMT"}, {"version": "v2", "created": "Thu, 17 Dec 2015 01:25:27 GMT"}], "update_date": "2015-12-18", "authors_parsed": [["Kang", "Cuicui", ""], ["Liao", "Shengcai", ""], ["He", "Yonghao", ""], ["Wang", "Jian", ""], ["Niu", "Wenjia", ""], ["Xiang", "Shiming", ""], ["Pan", "Chunhong", ""]]}, {"id": "1411.5010", "submitter": "Noah Stein", "authors": "Noah D. Stein", "title": "Nonnegative Tensor Factorization for Directional Blind Audio Source\n  Separation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We augment the nonnegative matrix factorization method for audio source\nseparation with cues about directionality of sound propagation. This improves\nseparation quality greatly and removes the need for training data, with only a\ntwofold increase in run time. This is the first method which can exploit\ndirectional information from microphone arrays much smaller than the wavelength\nof sound, working both in simulation and in practice on millimeter-scale\nmicrophone arrays.\n", "versions": [{"version": "v1", "created": "Tue, 18 Nov 2014 20:52:52 GMT"}, {"version": "v2", "created": "Fri, 26 May 2017 18:43:59 GMT"}], "update_date": "2017-05-30", "authors_parsed": [["Stein", "Noah D.", ""]]}, {"id": "1411.5014", "submitter": "Shubhanshu Gupta", "authors": "Shubhanshu Gupta", "title": "Music Data Analysis: A State-of-the-art Survey", "comments": "10 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.LG cs.SD", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Music accounts for a significant chunk of interest among various online\nactivities. This is reflected by wide array of alternatives offered in music\nrelated web/mobile apps, information portals, featuring millions of artists,\nsongs and events attracting user activity at similar scale. Availability of\nlarge scale structured and unstructured data has attracted similar level of\nattention by data science community. This paper attempts to offer current\nstate-of-the-art in music related analysis. Various approaches involving\nmachine learning, information theory, social network analysis, semantic web and\nlinked open data are represented in the form of taxonomy along with data\nsources and use cases addressed by the research community.\n", "versions": [{"version": "v1", "created": "Tue, 18 Nov 2014 14:19:28 GMT"}], "update_date": "2014-11-20", "authors_parsed": [["Gupta", "Shubhanshu", ""]]}, {"id": "1411.5172", "submitter": "Markus Heinonen", "authors": "Markus Heinonen, Florence d'Alch\\'e-Buc", "title": "Learning nonparametric differential equations with operator-valued\n  kernels and gradient matching", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modeling dynamical systems with ordinary differential equations implies a\nmechanistic view of the process underlying the dynamics. However in many cases,\nthis knowledge is not available. To overcome this issue, we introduce a general\nframework for nonparametric ODE models using penalized regression in\nReproducing Kernel Hilbert Spaces (RKHS) based on operator-valued kernels.\nMoreover, we extend the scope of gradient matching approaches to nonparametric\nODE. A smooth estimate of the solution ODE is built to provide an approximation\nof the derivative of the ODE solution which is in turn used to learn the\nnonparametric ODE model. This approach benefits from the flexibility of\npenalized regression in RKHS allowing for ridge or (structured) sparse\nregression as well. Very good results are shown on 3 different ODE systems.\n", "versions": [{"version": "v1", "created": "Wed, 19 Nov 2014 10:48:50 GMT"}], "update_date": "2014-11-20", "authors_parsed": [["Heinonen", "Markus", ""], ["d'Alch\u00e9-Buc", "Florence", ""]]}, {"id": "1411.5260", "submitter": "Patrick Kimes", "authors": "Patrick K. Kimes, D. Neil Hayes, J. S. Marron and Yufeng Liu", "title": "Large-Margin Classification with Multiple Decision Rules", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Binary classification is a common statistical learning problem in which a\nmodel is estimated on a set of covariates for some outcome indicating the\nmembership of one of two classes. In the literature, there exists a distinction\nbetween hard and soft classification. In soft classification, the conditional\nclass probability is modeled as a function of the covariates. In contrast, hard\nclassification methods only target the optimal prediction boundary. While hard\nand soft classification methods have been studied extensively, not much work\nhas been done to compare the actual tasks of hard and soft classification. In\nthis paper we propose a spectrum of statistical learning problems which span\nthe hard and soft classification tasks based on fitting multiple decision rules\nto the data. By doing so, we reveal a novel collection of learning tasks of\nincreasing complexity. We study the problems using the framework of\nlarge-margin classifiers and a class of piecewise linear convex surrogates, for\nwhich we derive statistical properties and a corresponding sub-gradient descent\nalgorithm. We conclude by applying our approach to simulation settings and a\nmagnetic resonance imaging (MRI) dataset from the Alzheimer's Disease\nNeuroimaging Initiative (ADNI) study.\n", "versions": [{"version": "v1", "created": "Wed, 19 Nov 2014 15:45:54 GMT"}], "update_date": "2014-11-20", "authors_parsed": [["Kimes", "Patrick K.", ""], ["Hayes", "D. Neil", ""], ["Marron", "J. S.", ""], ["Liu", "Yufeng", ""]]}, {"id": "1411.5328", "submitter": "Robinson Piramuthu Robinson Piramuthu", "authors": "Bolei Zhou, Vignesh Jagadeesh, Robinson Piramuthu", "title": "ConceptLearner: Discovering Visual Concepts from Weakly Labeled Image\n  Collections", "comments": "9 pages, 8 figures, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Discovering visual knowledge from weakly labeled data is crucial to scale up\ncomputer vision recognition system, since it is expensive to obtain fully\nlabeled data for a large number of concept categories. In this paper, we\npropose ConceptLearner, which is a scalable approach to discover visual\nconcepts from weakly labeled image collections. Thousands of visual concept\ndetectors are learned automatically, without human in the loop for additional\nannotation. We show that these learned detectors could be applied to recognize\nconcepts at image-level and to detect concepts at image region-level\naccurately. Under domain-specific supervision, we further evaluate the learned\nconcepts for scene recognition on SUN database and for object detection on\nPascal VOC 2007. ConceptLearner shows promising performance compared to fully\nsupervised and weakly supervised methods.\n", "versions": [{"version": "v1", "created": "Wed, 19 Nov 2014 19:35:39 GMT"}], "update_date": "2014-11-20", "authors_parsed": [["Zhou", "Bolei", ""], ["Jagadeesh", "Vignesh", ""], ["Piramuthu", "Robinson", ""]]}, {"id": "1411.5371", "submitter": "Justin Kinney", "authors": "Justin B. Kinney", "title": "Unification of field theory and maximum entropy methods for learning\n  probability densities", "comments": "16 pages, 4 figures. Minor clarifying changes have been made\n  throughout. Software is available at https://github.com/jbkinney/14_maxent", "journal-ref": "Phys. Rev. E 92, 032107 (2015)", "doi": "10.1103/PhysRevE.92.032107", "report-no": null, "categories": "physics.data-an cs.LG q-bio.QM stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The need to estimate smooth probability distributions (a.k.a. probability\ndensities) from finite sampled data is ubiquitous in science. Many approaches\nto this problem have been described, but none is yet regarded as providing a\ndefinitive solution. Maximum entropy estimation and Bayesian field theory are\ntwo such approaches. Both have origins in statistical physics, but the\nrelationship between them has remained unclear. Here I unify these two methods\nby showing that every maximum entropy density estimate can be recovered in the\ninfinite smoothness limit of an appropriate Bayesian field theory. I also show\nthat Bayesian field theory estimation can be performed without imposing any\nboundary conditions on candidate densities, and that the infinite smoothness\nlimit of these theories recovers the most common types of maximum entropy\nestimates. Bayesian field theory is thus seen to provide a natural test of the\nvalidity of the maximum entropy null hypothesis. Bayesian field theory also\nreturns a lower entropy density estimate when the maximum entropy hypothesis is\nfalsified. The computations necessary for this approach can be performed\nrapidly for one-dimensional data, and software for doing this is provided.\nBased on these results, I argue that Bayesian field theory is poised to provide\na definitive solution to the density estimation problem in one dimension.\n", "versions": [{"version": "v1", "created": "Wed, 19 Nov 2014 21:00:58 GMT"}, {"version": "v2", "created": "Fri, 21 Nov 2014 16:58:05 GMT"}, {"version": "v3", "created": "Wed, 3 Dec 2014 14:57:30 GMT"}, {"version": "v4", "created": "Sat, 21 Mar 2015 22:07:18 GMT"}, {"version": "v5", "created": "Wed, 29 Jul 2015 02:29:54 GMT"}], "update_date": "2015-09-16", "authors_parsed": [["Kinney", "Justin B.", ""]]}, {"id": "1411.5404", "submitter": "Kevin Xu", "authors": "Kevin S. Xu", "title": "Stochastic Block Transition Models for Dynamic Networks", "comments": "To appear in proceedings of AISTATS 2015", "journal-ref": "Proceedings of the 18th International Conference on Artificial\n  Intelligence and Statistics (2015) 1079-1087", "doi": null, "report-no": null, "categories": "cs.SI cs.LG physics.soc-ph stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There has been great interest in recent years on statistical models for\ndynamic networks. In this paper, I propose a stochastic block transition model\n(SBTM) for dynamic networks that is inspired by the well-known stochastic block\nmodel (SBM) for static networks and previous dynamic extensions of the SBM.\nUnlike most existing dynamic network models, it does not make a hidden Markov\nassumption on the edge-level dynamics, allowing the presence or absence of\nedges to directly influence future edge probabilities while retaining the\ninterpretability of the SBM. I derive an approximate inference procedure for\nthe SBTM and demonstrate that it is significantly better at reproducing\ndurations of edges in real social network data.\n", "versions": [{"version": "v1", "created": "Wed, 19 Nov 2014 23:30:36 GMT"}, {"version": "v2", "created": "Thu, 29 Jan 2015 00:08:17 GMT"}], "update_date": "2016-07-11", "authors_parsed": [["Xu", "Kevin S.", ""]]}, {"id": "1411.5417", "submitter": "Abhradeep Guha Thakurta", "authors": "Kunal Talwar, Abhradeep Thakurta, Li Zhang", "title": "Private Empirical Risk Minimization Beyond the Worst Case: The Effect of\n  the Constraint Set Geometry", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Empirical Risk Minimization (ERM) is a standard technique in machine\nlearning, where a model is selected by minimizing a loss function over\nconstraint set. When the training dataset consists of private information, it\nis natural to use a differentially private ERM algorithm, and this problem has\nbeen the subject of a long line of work started with Chaudhuri and Monteleoni\n2008. A private ERM algorithm outputs an approximate minimizer of the loss\nfunction and its error can be measured as the difference from the optimal value\nof the loss function. When the constraint set is arbitrary, the required error\nbounds are fairly well understood \\cite{BassilyST14}. In this work, we show\nthat the geometric properties of the constraint set can be used to derive\nsignificantly better results. Specifically, we show that a differentially\nprivate version of Mirror Descent leads to error bounds of the form\n$\\tilde{O}(G_{\\mathcal{C}}/n)$ for a lipschitz loss function, improving on the\n$\\tilde{O}(\\sqrt{p}/n)$ bounds in Bassily, Smith and Thakurta 2014. Here $p$ is\nthe dimensionality of the problem, $n$ is the number of data points in the\ntraining set, and $G_{\\mathcal{C}}$ denotes the Gaussian width of the\nconstraint set that we optimize over. We show similar improvements for strongly\nconvex functions, and for smooth functions. In addition, we show that when the\nloss function is Lipschitz with respect to the $\\ell_1$ norm and $\\mathcal{C}$\nis $\\ell_1$-bounded, a differentially private version of the Frank-Wolfe\nalgorithm gives error bounds of the form $\\tilde{O}(n^{-2/3})$. This captures\nthe important and common case of sparse linear regression (LASSO), when the\ndata $x_i$ satisfies $|x_i|_{\\infty} \\leq 1$ and we optimize over the $\\ell_1$\nball. We show new lower bounds for this setting, that together with known\nbounds, imply that all our upper bounds are tight.\n", "versions": [{"version": "v1", "created": "Thu, 20 Nov 2014 01:33:53 GMT"}, {"version": "v2", "created": "Thu, 5 Mar 2015 02:38:52 GMT"}, {"version": "v3", "created": "Sun, 20 Nov 2016 22:40:46 GMT"}], "update_date": "2016-11-22", "authors_parsed": [["Talwar", "Kunal", ""], ["Thakurta", "Abhradeep", ""], ["Zhang", "Li", ""]]}, {"id": "1411.5428", "submitter": "Ben Stoddard", "authors": "Ben Stoddard and Yan Chen and Ashwin Machanavajjhala", "title": "Differentially Private Algorithms for Empirical Machine Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An important use of private data is to build machine learning classifiers.\nWhile there is a burgeoning literature on differentially private classification\nalgorithms, we find that they are not practical in real applications due to two\nreasons. First, existing differentially private classifiers provide poor\naccuracy on real world datasets. Second, there is no known differentially\nprivate algorithm for empirically evaluating the private classifier on a\nprivate test dataset.\n  In this paper, we develop differentially private algorithms that mirror real\nworld empirical machine learning workflows. We consider the private classifier\ntraining algorithm as a blackbox. We present private algorithms for selecting\nfeatures that are input to the classifier. Though adding a preprocessing step\ntakes away some of the privacy budget from the actual classification process\n(thus potentially making it noisier and less accurate), we show that our novel\npreprocessing techniques significantly increase classifier accuracy on three\nreal-world datasets. We also present the first private algorithms for\nempirically constructing receiver operating characteristic (ROC) curves on a\nprivate test set.\n", "versions": [{"version": "v1", "created": "Thu, 20 Nov 2014 03:10:47 GMT"}, {"version": "v2", "created": "Fri, 21 Nov 2014 20:41:04 GMT"}], "update_date": "2014-11-24", "authors_parsed": [["Stoddard", "Ben", ""], ["Chen", "Yan", ""], ["Machanavajjhala", "Ashwin", ""]]}, {"id": "1411.5595", "submitter": "Tianze Shi", "authors": "Tianze Shi, Zhiyuan Liu", "title": "Linking GloVe with word2vec", "comments": "5 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Global Vectors for word representation (GloVe), introduced by Jeffrey\nPennington et al. is reported to be an efficient and effective method for\nlearning vector representations of words. State-of-the-art performance is also\nprovided by skip-gram with negative-sampling (SGNS) implemented in the word2vec\ntool. In this note, we explain the similarities between the training objectives\nof the two models, and show that the objective of SGNS is similar to the\nobjective of a specialized form of GloVe, though their cost functions are\ndefined differently.\n", "versions": [{"version": "v1", "created": "Thu, 20 Nov 2014 16:39:28 GMT"}, {"version": "v2", "created": "Wed, 26 Nov 2014 06:46:18 GMT"}], "update_date": "2014-11-27", "authors_parsed": [["Shi", "Tianze", ""], ["Liu", "Zhiyuan", ""]]}, {"id": "1411.5649", "submitter": "Arthur Flajolet", "authors": "Arthur Flajolet, Patrick Jaillet", "title": "No-Regret Learnability for Piecewise Linear Losses", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the convex optimization approach to online regret minimization, many\nmethods have been developed to guarantee a $O(\\sqrt{T})$ bound on regret for\nsubdifferentiable convex loss functions with bounded subgradients, by using a\nreduction to linear loss functions. This suggests that linear loss functions\ntend to be the hardest ones to learn against, regardless of the underlying\ndecision spaces. We investigate this question in a systematic fashion looking\nat the interplay between the set of possible moves for both the decision maker\nand the adversarial environment. This allows us to highlight sharp distinctive\nbehaviors about the learnability of piecewise linear loss functions. On the one\nhand, when the decision set of the decision maker is a polyhedron, we establish\n$\\Omega(\\sqrt{T})$ lower bounds on regret for a large class of piecewise linear\nloss functions with important applications in online linear optimization,\nrepeated zero-sum Stackelberg games, online prediction with side information,\nand online two-stage optimization. On the other hand, we exhibit $o(\\sqrt{T})$\nlearning rates, achieved by the Follow-The-Leader algorithm, in online linear\noptimization when the boundary of the decision maker's decision set is curved\nand when $0$ does not lie in the convex hull of the environment's decision set.\nHence, the curvature of the decision maker's decision set is a determining\nfactor for the optimal learning rate. These results hold in a completely\nadversarial setting.\n", "versions": [{"version": "v1", "created": "Thu, 20 Nov 2014 19:38:11 GMT"}, {"version": "v2", "created": "Wed, 7 Jan 2015 01:41:37 GMT"}, {"version": "v3", "created": "Thu, 8 Jan 2015 08:40:58 GMT"}, {"version": "v4", "created": "Fri, 27 May 2016 21:29:52 GMT"}, {"version": "v5", "created": "Sat, 17 Sep 2016 16:14:50 GMT"}], "update_date": "2016-09-20", "authors_parsed": [["Flajolet", "Arthur", ""], ["Jaillet", "Patrick", ""]]}, {"id": "1411.5732", "submitter": "Suleyman Cetintas", "authors": "Suleyman Cetintas, Luo Si, Yan Ping Xin, Dake Zhang, Joo Young Park,\n  Ron Tzur", "title": "A Joint Probabilistic Classification Model of Relevant and Irrelevant\n  Sentences in Mathematical Word Problems", "comments": "appears in Journal of Educational Data Mining (JEDM, 2010)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Estimating the difficulty level of math word problems is an important task\nfor many educational applications. Identification of relevant and irrelevant\nsentences in math word problems is an important step for calculating the\ndifficulty levels of such problems. This paper addresses a novel application of\ntext categorization to identify two types of sentences in mathematical word\nproblems, namely relevant and irrelevant sentences. A novel joint probabilistic\nclassification model is proposed to estimate the joint probability of\nclassification decisions for all sentences of a math word problem by utilizing\nthe correlation among all sentences along with the correlation between the\nquestion sentence and other sentences, and sentence text. The proposed model is\ncompared with i) a SVM classifier which makes independent classification\ndecisions for individual sentences by only using the sentence text and ii) a\nnovel SVM classifier that considers the correlation between the question\nsentence and other sentences along with the sentence text. An extensive set of\nexperiments demonstrates the effectiveness of the joint probabilistic\nclassification model for identifying relevant and irrelevant sentences as well\nas the novel SVM classifier that utilizes the correlation between the question\nsentence and other sentences. Furthermore, empirical results and analysis show\nthat i) it is highly beneficial not to remove stopwords and ii) utilizing part\nof speech tagging does not make a significant improvement although it has been\nshown to be effective for the related task of math word problem type\nclassification.\n", "versions": [{"version": "v1", "created": "Fri, 21 Nov 2014 00:53:02 GMT"}], "update_date": "2014-11-24", "authors_parsed": [["Cetintas", "Suleyman", ""], ["Si", "Luo", ""], ["Xin", "Yan Ping", ""], ["Zhang", "Dake", ""], ["Park", "Joo Young", ""], ["Tzur", "Ron", ""]]}, {"id": "1411.5737", "submitter": "Steven Damelin Dr", "authors": "S. B. Damelin, Y. Gu, D. C. Wunsch II, R. Xu", "title": "Fuzzy Adaptive Resonance Theory, Diffusion Maps and their applications\n  to Clustering and Biclustering", "comments": "Accepted in Math.Model.Nat.Phenom", "journal-ref": "Math.Model.Nat.Phenom. Vol. 10, No 3, 2015, pp. 206-211", "doi": null, "report-no": null, "categories": "cs.NE cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we describe an algorithm FARDiff (Fuzzy Adaptive Resonance\nDif- fusion) which combines Diffusion Maps and Fuzzy Adaptive Resonance Theory\nto do clustering on high dimensional data. We describe some applications of\nthis method and some problems for future research.\n", "versions": [{"version": "v1", "created": "Fri, 21 Nov 2014 01:21:17 GMT"}, {"version": "v2", "created": "Wed, 26 Nov 2014 00:16:54 GMT"}, {"version": "v3", "created": "Sat, 20 Dec 2014 19:56:55 GMT"}, {"version": "v4", "created": "Wed, 18 Feb 2015 00:08:47 GMT"}, {"version": "v5", "created": "Tue, 6 Oct 2015 00:03:43 GMT"}], "update_date": "2015-10-07", "authors_parsed": [["Damelin", "S. B.", ""], ["Gu", "Y.", ""], ["Wunsch", "D. C.", "II"], ["Xu", "R.", ""]]}, {"id": "1411.5873", "submitter": "Zheng Qu", "authors": "Zheng Qu and Peter Richt\\'arik and Tong Zhang", "title": "Randomized Dual Coordinate Ascent with Arbitrary Sampling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.LG cs.NA math.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of minimizing the average of a large number of smooth\nconvex functions penalized with a strongly convex regularizer. We propose and\nanalyze a novel primal-dual method (Quartz) which at every iteration samples\nand updates a random subset of the dual variables, chosen according to an\narbitrary distribution. In contrast to typical analysis, we directly bound the\ndecrease of the primal-dual error (in expectation), without the need to first\nanalyze the dual error. Depending on the choice of the sampling, we obtain\nefficient serial, parallel and distributed variants of the method. In the\nserial case, our bounds match the best known bounds for SDCA (both with uniform\nand importance sampling). With standard mini-batching, our bounds predict\ninitial data-independent speedup as well as additional data-driven speedup\nwhich depends on spectral and sparsity properties of the data. We calculate\ntheoretical speedup factors and find that they are excellent predictors of\nactual speedup in practice. Moreover, we illustrate that it is possible to\ndesign an efficient mini-batch importance sampling. The distributed variant of\nQuartz is the first distributed SDCA-like method with an analysis for\nnon-separable data.\n", "versions": [{"version": "v1", "created": "Fri, 21 Nov 2014 13:55:31 GMT"}], "update_date": "2014-11-24", "authors_parsed": [["Qu", "Zheng", ""], ["Richt\u00e1rik", "Peter", ""], ["Zhang", "Tong", ""]]}, {"id": "1411.5899", "submitter": "Fulton Wang", "authors": "Fulton Wang, Cynthia Rudin", "title": "Falling Rule Lists", "comments": "Accepted at AISTATS 2015. Contains number of rules mined, running\n  times. in Proceedings of AISTATS 2015. JMLR: W&CP 38", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Falling rule lists are classification models consisting of an ordered list of\nif-then rules, where (i) the order of rules determines which example should be\nclassified by each rule, and (ii) the estimated probability of success\ndecreases monotonically down the list. These kinds of rule lists are inspired\nby healthcare applications where patients would be stratified into risk sets\nand the highest at-risk patients should be considered first. We provide a\nBayesian framework for learning falling rule lists that does not rely on\ntraditional greedy decision tree learning methods.\n", "versions": [{"version": "v1", "created": "Fri, 21 Nov 2014 15:01:56 GMT"}, {"version": "v2", "created": "Tue, 27 Jan 2015 04:41:38 GMT"}, {"version": "v3", "created": "Sun, 1 Feb 2015 06:46:47 GMT"}], "update_date": "2015-02-03", "authors_parsed": [["Wang", "Fulton", ""], ["Rudin", "Cynthia", ""]]}, {"id": "1411.5908", "submitter": "Karel Lenc", "authors": "Karel Lenc, Andrea Vedaldi", "title": "Understanding image representations by measuring their equivariance and\n  equivalence", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite the importance of image representations such as histograms of\noriented gradients and deep Convolutional Neural Networks (CNN), our\ntheoretical understanding of them remains limited. Aiming at filling this gap,\nwe investigate three key mathematical properties of representations:\nequivariance, invariance, and equivalence. Equivariance studies how\ntransformations of the input image are encoded by the representation,\ninvariance being a special case where a transformation has no effect.\nEquivalence studies whether two representations, for example two different\nparametrisations of a CNN, capture the same visual information or not. A number\nof methods to establish these properties empirically are proposed, including\nintroducing transformation and stitching layers in CNNs. These methods are then\napplied to popular representations to reveal insightful aspects of their\nstructure, including clarifying at which layers in a CNN certain geometric\ninvariances are achieved. While the focus of the paper is theoretical, direct\napplications to structured-output regression are demonstrated too.\n", "versions": [{"version": "v1", "created": "Fri, 21 Nov 2014 15:14:42 GMT"}, {"version": "v2", "created": "Sat, 20 Jun 2015 18:35:37 GMT"}], "update_date": "2015-06-23", "authors_parsed": [["Lenc", "Karel", ""], ["Vedaldi", "Andrea", ""]]}, {"id": "1411.5928", "submitter": "Alexey Dosovitskiy", "authors": "Alexey Dosovitskiy, Jost Tobias Springenberg, Maxim Tatarchenko and\n  Thomas Brox", "title": "Learning to Generate Chairs, Tables and Cars with Convolutional Networks", "comments": "v4: final PAMI version. New architecture figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We train generative 'up-convolutional' neural networks which are able to\ngenerate images of objects given object style, viewpoint, and color. We train\nthe networks on rendered 3D models of chairs, tables, and cars. Our experiments\nshow that the networks do not merely learn all images by heart, but rather find\na meaningful representation of 3D models allowing them to assess the similarity\nof different models, interpolate between given views to generate the missing\nones, extrapolate views, and invent new objects not present in the training set\nby recombining training instances, or even two different object classes.\nMoreover, we show that such generative networks can be used to find\ncorrespondences between different objects from the dataset, outperforming\nexisting approaches on this task.\n", "versions": [{"version": "v1", "created": "Fri, 21 Nov 2014 16:01:04 GMT"}, {"version": "v2", "created": "Mon, 5 Jan 2015 12:31:49 GMT"}, {"version": "v3", "created": "Thu, 3 Dec 2015 09:49:23 GMT"}, {"version": "v4", "created": "Wed, 2 Aug 2017 20:53:43 GMT"}], "update_date": "2017-08-04", "authors_parsed": [["Dosovitskiy", "Alexey", ""], ["Springenberg", "Jost Tobias", ""], ["Tatarchenko", "Maxim", ""], ["Brox", "Thomas", ""]]}, {"id": "1411.5977", "submitter": "Nihar Shah", "authors": "Nihar B. Shah and Dengyong Zhou", "title": "On the Impossibility of Convex Inference in Human Computation", "comments": "AAAI 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.HC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Human computation or crowdsourcing involves joint inference of the\nground-truth-answers and the worker-abilities by optimizing an objective\nfunction, for instance, by maximizing the data likelihood based on an assumed\nunderlying model. A variety of methods have been proposed in the literature to\naddress this inference problem. As far as we know, none of the objective\nfunctions in existing methods is convex. In machine learning and applied\nstatistics, a convex function such as the objective function of support vector\nmachines (SVMs) is generally preferred, since it can leverage the\nhigh-performance algorithms and rigorous guarantees established in the\nextensive literature on convex optimization. One may thus wonder if there\nexists a meaningful convex objective function for the inference problem in\nhuman computation. In this paper, we investigate this convexity issue for human\ncomputation. We take an axiomatic approach by formulating a set of axioms that\nimpose two mild and natural assumptions on the objective function for the\ninference. Under these axioms, we show that it is unfortunately impossible to\nensure convexity of the inference problem. On the other hand, we show that\ninterestingly, in the absence of a requirement to model \"spammers\", one can\nconstruct reasonable objective functions for crowdsourcing that guarantee\nconvex inference.\n", "versions": [{"version": "v1", "created": "Fri, 21 Nov 2014 18:51:10 GMT"}], "update_date": "2014-11-24", "authors_parsed": [["Shah", "Nihar B.", ""], ["Zhou", "Dengyong", ""]]}, {"id": "1411.5988", "submitter": "Rocco Langone", "authors": "Rocco Langone", "title": "Clustering evolving data using kernel-based methods", "comments": "PhD thesis, Faculty of Engineering, KU Leuven (Leuven, Belgium), July\n  2014", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this thesis, we propose several modelling strategies to tackle evolving\ndata in different contexts. In the framework of static clustering, we start by\nintroducing a soft kernel spectral clustering (SKSC) algorithm, which can\nbetter deal with overlapping clusters with respect to kernel spectral\nclustering (KSC) and provides more interpretable outcomes. Afterwards, a whole\nstrategy based upon KSC for community detection of static networks is proposed,\nwhere the extraction of a high quality training sub-graph, the choice of the\nkernel function, the model selection and the applicability to large-scale data\nare key aspects. This paves the way for the development of a novel clustering\nalgorithm for the analysis of evolving networks called kernel spectral\nclustering with memory effect (MKSC), where the temporal smoothness between\nclustering results in successive time steps is incorporated at the level of the\nprimal optimization problem, by properly modifying the KSC formulation. Later\non, an application of KSC to fault detection of an industrial machine is\npresented. Here, a smart pre-processing of the data by means of a proper\nwindowing operation is necessary to catch the ongoing degradation process\naffecting the machine. In this way, in a genuinely unsupervised manner, it is\npossible to raise an early warning when necessary, in an online fashion.\nFinally, we propose a new algorithm called incremental kernel spectral\nclustering (IKSC) for online learning of non-stationary data. This ambitious\nchallenge is faced by taking advantage of the out-of-sample property of kernel\nspectral clustering (KSC) to adapt the initial model, in order to tackle\nmerging, splitting or drifting of clusters across time. Real-world applications\nconsidered in this thesis include image segmentation, time-series clustering,\ncommunity detection of static and evolving networks.\n", "versions": [{"version": "v1", "created": "Thu, 20 Nov 2014 19:51:30 GMT"}], "update_date": "2014-11-24", "authors_parsed": [["Langone", "Rocco", ""]]}, {"id": "1411.6081", "submitter": "Cho-Jui Hsieh Cho-Jui Hsieh", "authors": "Cho-Jui Hsieh and Nagarajan Natarajan and Inderjit S. Dhillon", "title": "PU Learning for Matrix Completion", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NA stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we consider the matrix completion problem when the\nobservations are one-bit measurements of some underlying matrix M, and in\nparticular the observed samples consist only of ones and no zeros. This problem\nis motivated by modern applications such as recommender systems and social\nnetworks where only \"likes\" or \"friendships\" are observed. The problem of\nlearning from only positive and unlabeled examples, called PU\n(positive-unlabeled) learning, has been studied in the context of binary\nclassification. We consider the PU matrix completion problem, where an\nunderlying real-valued matrix M is first quantized to generate one-bit\nobservations and then a subset of positive entries is revealed. Under the\nassumption that M has bounded nuclear norm, we provide recovery guarantees for\ntwo different observation models: 1) M parameterizes a distribution that\ngenerates a binary matrix, 2) M is thresholded to obtain a binary matrix. For\nthe first case, we propose a \"shifted matrix completion\" method that recovers M\nusing only a subset of indices corresponding to ones, while for the second\ncase, we propose a \"biased matrix completion\" method that recovers the\n(thresholded) binary matrix. Both methods yield strong error bounds --- if M is\nn by n, the Frobenius error is bounded as O(1/((1-rho)n), where 1-rho denotes\nthe fraction of ones observed. This implies a sample complexity of O(n\\log n)\nones to achieve a small error, when M is dense and n is large. We extend our\nmethods and guarantees to the inductive matrix completion problem, where rows\nand columns of M have associated features. We provide efficient and scalable\noptimization procedures for both the methods and demonstrate the effectiveness\nof the proposed methods for link prediction (on real-world networks consisting\nof over 2 million nodes and 90 million links) and semi-supervised clustering\ntasks.\n", "versions": [{"version": "v1", "created": "Sat, 22 Nov 2014 04:37:15 GMT"}], "update_date": "2014-11-25", "authors_parsed": [["Hsieh", "Cho-Jui", ""], ["Natarajan", "Nagarajan", ""], ["Dhillon", "Inderjit S.", ""]]}, {"id": "1411.6156", "submitter": "Guy Bresler", "authors": "Guy Bresler", "title": "Efficiently learning Ising models on arbitrary graphs", "comments": "20 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.IT math.IT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of reconstructing the graph underlying an Ising model\nfrom i.i.d. samples. Over the last fifteen years this problem has been of\nsignificant interest in the statistics, machine learning, and statistical\nphysics communities, and much of the effort has been directed towards finding\nalgorithms with low computational cost for various restricted classes of\nmodels. Nevertheless, for learning Ising models on general graphs with $p$\nnodes of degree at most $d$, it is not known whether or not it is possible to\nimprove upon the $p^{d}$ computation needed to exhaustively search over all\npossible neighborhoods for each node.\n  In this paper we show that a simple greedy procedure allows to learn the\nstructure of an Ising model on an arbitrary bounded-degree graph in time on the\norder of $p^2$. We make no assumptions on the parameters except what is\nnecessary for identifiability of the model, and in particular the results hold\nat low-temperatures as well as for highly non-uniform models. The proof rests\non a new structural property of Ising models: we show that for any node there\nexists at least one neighbor with which it has a high mutual information. This\nstructural property may be of independent interest.\n", "versions": [{"version": "v1", "created": "Sat, 22 Nov 2014 19:05:11 GMT"}, {"version": "v2", "created": "Mon, 1 Dec 2014 01:24:41 GMT"}], "update_date": "2014-12-02", "authors_parsed": [["Bresler", "Guy", ""]]}, {"id": "1411.6160", "submitter": "Martin Copenhaver", "authors": "Dimitris Bertsimas and Martin S. Copenhaver", "title": "Characterization of the equivalence of robustification and\n  regularization in linear and matrix regression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.LG math.OC stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The notion of developing statistical methods in machine learning which are\nrobust to adversarial perturbations in the underlying data has been the subject\nof increasing interest in recent years. A common feature of this work is that\nthe adversarial robustification often corresponds exactly to regularization\nmethods which appear as a loss function plus a penalty. In this paper we deepen\nand extend the understanding of the connection between robustification and\nregularization (as achieved by penalization) in regression problems.\nSpecifically, (a) in the context of linear regression, we characterize\nprecisely under which conditions on the model of uncertainty used and on the\nloss function penalties robustification and regularization are equivalent, and\n(b) we extend the characterization of robustification and regularization to\nmatrix regression problems (matrix completion and Principal Component\nAnalysis).\n", "versions": [{"version": "v1", "created": "Sat, 22 Nov 2014 19:59:32 GMT"}, {"version": "v2", "created": "Sat, 25 Feb 2017 17:58:26 GMT"}], "update_date": "2017-02-28", "authors_parsed": [["Bertsimas", "Dimitris", ""], ["Copenhaver", "Martin S.", ""]]}, {"id": "1411.6191", "submitter": "David Balduzzi", "authors": "David Balduzzi, Hastagiri Vanchinathan, Joachim Buhmann", "title": "Kickback cuts Backprop's red-tape: Biologically plausible credit\n  assignment in neural networks", "comments": "7 pages. To appear, AAAI-15", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Error backpropagation is an extremely effective algorithm for assigning\ncredit in artificial neural networks. However, weight updates under Backprop\ndepend on lengthy recursive computations and require separate output and error\nmessages -- features not shared by biological neurons, that are perhaps\nunnecessary. In this paper, we revisit Backprop and the credit assignment\nproblem. We first decompose Backprop into a collection of interacting learning\nalgorithms; provide regret bounds on the performance of these sub-algorithms;\nand factorize Backprop's error signals. Using these results, we derive a new\ncredit assignment algorithm for nonparametric regression, Kickback, that is\nsignificantly simpler than Backprop. Finally, we provide a sufficient condition\nfor Kickback to follow error gradients, and show that Kickback matches\nBackprop's performance on real-world regression benchmarks.\n", "versions": [{"version": "v1", "created": "Sun, 23 Nov 2014 04:58:22 GMT"}], "update_date": "2014-11-25", "authors_parsed": [["Balduzzi", "David", ""], ["Vanchinathan", "Hastagiri", ""], ["Buhmann", "Joachim", ""]]}, {"id": "1411.6231", "submitter": "Xiaojun Chang", "authors": "Xiaojun Chang, Feiping Nie, Sen Wang, Yi Yang, Xiaofang Zhou and\n  Chengqi Zhang", "title": "Compound Rank-k Projections for Bilinear Analysis", "comments": "Accepted by IEEE Transactions on Neural Networks and Learning Systems\n  (IEEE T-NNLS), 2015", "journal-ref": null, "doi": "10.1109/TNNLS.2015.2441735", "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In many real-world applications, data are represented by matrices or\nhigh-order tensors. Despite the promising performance, the existing\ntwo-dimensional discriminant analysis algorithms employ a single projection\nmodel to exploit the discriminant information for projection, making the model\nless flexible. In this paper, we propose a novel Compound Rank-k Projection\n(CRP) algorithm for bilinear analysis. CRP deals with matrices directly without\ntransforming them into vectors, and it therefore preserves the correlations\nwithin the matrix and decreases the computation complexity. Different from the\nexisting two dimensional discriminant analysis algorithms, objective function\nvalues of CRP increase monotonically.In addition, CRP utilizes multiple rank-k\nprojection models to enable a larger search space in which the optimal solution\ncan be found. In this way, the discriminant ability is enhanced.\n", "versions": [{"version": "v1", "created": "Sun, 23 Nov 2014 12:50:20 GMT"}, {"version": "v2", "created": "Sun, 24 May 2015 07:58:29 GMT"}, {"version": "v3", "created": "Wed, 3 Jun 2015 04:26:25 GMT"}], "update_date": "2017-07-11", "authors_parsed": [["Chang", "Xiaojun", ""], ["Nie", "Feiping", ""], ["Wang", "Sen", ""], ["Yang", "Yi", ""], ["Zhou", "Xiaofang", ""], ["Zhang", "Chengqi", ""]]}, {"id": "1411.6232", "submitter": "Xiaojun Chang", "authors": "Xiaojun Chang and Yi Yang", "title": "Semi-supervised Feature Analysis by Mining Correlations among Multiple\n  Tasks", "comments": "11 pages, submitted to TNNLS", "journal-ref": null, "doi": "10.1109/TNNLS.2016.2582746", "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a novel semi-supervised feature selection framework\nby mining correlations among multiple tasks and apply it to different\nmultimedia applications. Instead of independently computing the importance of\nfeatures for each task, our algorithm leverages shared knowledge from multiple\nrelated tasks, thus, improving the performance of feature selection. Note that\nwe build our algorithm on assumption that different tasks share common\nstructures. The proposed algorithm selects features in a batch mode, by which\nthe correlations between different features are taken into consideration.\nBesides, considering the fact that labeling a large amount of training data in\nreal world is both time-consuming and tedious, we adopt manifold learning which\nexploits both labeled and unlabeled training data for feature space analysis.\nSince the objective function is non-smooth and difficult to solve, we propose\nan iterative algorithm with fast convergence. Extensive experiments on\ndifferent applications demonstrate that our algorithm outperforms other\nstate-of-the-art feature selection algorithms.\n", "versions": [{"version": "v1", "created": "Sun, 23 Nov 2014 13:00:18 GMT"}, {"version": "v2", "created": "Sun, 11 Jan 2015 18:47:26 GMT"}], "update_date": "2017-07-11", "authors_parsed": [["Chang", "Xiaojun", ""], ["Yang", "Yi", ""]]}, {"id": "1411.6233", "submitter": "Xiaojun Chang", "authors": "Xiaojun Chang, Feiping Nie, Yi Yang, and Heng Huang", "title": "A Convex Sparse PCA for Feature Analysis", "comments": null, "journal-ref": null, "doi": "10.1145/2910585", "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Principal component analysis (PCA) has been widely applied to dimensionality\nreduction and data pre-processing for different applications in engineering,\nbiology and social science. Classical PCA and its variants seek for linear\nprojections of the original variables to obtain a low dimensional feature\nrepresentation with maximal variance. One limitation is that it is very\ndifficult to interpret the results of PCA. In addition, the classical PCA is\nvulnerable to certain noisy data. In this paper, we propose a convex sparse\nprincipal component analysis (CSPCA) algorithm and apply it to feature\nanalysis. First we show that PCA can be formulated as a low-rank regression\noptimization problem. Based on the discussion, the l 2 , 1 -norm minimization\nis incorporated into the objective function to make the regression coefficients\nsparse, thereby robust to the outliers. In addition, based on the sparse model\nused in CSPCA, an optimal weight is assigned to each of the original feature,\nwhich in turn provides the output with good interpretability. With the output\nof our CSPCA, we can effectively analyze the importance of each feature under\nthe PCA criteria. The objective function is convex, and we propose an iterative\nalgorithm to optimize it. We apply the CSPCA algorithm to feature selection and\nconduct extensive experiments on six different benchmark datasets. Experimental\nresults demonstrate that the proposed algorithm outperforms state-of-the-art\nunsupervised feature selection algorithms.\n", "versions": [{"version": "v1", "created": "Sun, 23 Nov 2014 13:06:43 GMT"}], "update_date": "2017-07-11", "authors_parsed": [["Chang", "Xiaojun", ""], ["Nie", "Feiping", ""], ["Yang", "Yi", ""], ["Huang", "Heng", ""]]}, {"id": "1411.6235", "submitter": "Xiaojun Chang", "authors": "Xiaojun Chang, Feiping Nie, Zhigang Ma, and Yi Yang", "title": "Balanced k-Means and Min-Cut Clustering", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Clustering is an effective technique in data mining to generate groups that\nare the matter of interest. Among various clustering approaches, the family of\nk-means algorithms and min-cut algorithms gain most popularity due to their\nsimplicity and efficacy. The classical k-means algorithm partitions a number of\ndata points into several subsets by iteratively updating the clustering centers\nand the associated data points. By contrast, a weighted undirected graph is\nconstructed in min-cut algorithms which partition the vertices of the graph\ninto two sets. However, existing clustering algorithms tend to cluster minority\nof data points into a subset, which shall be avoided when the target dataset is\nbalanced. To achieve more accurate clustering for balanced dataset, we propose\nto leverage exclusive lasso on k-means and min-cut to regulate the balance\ndegree of the clustering results. By optimizing our objective functions that\nbuild atop the exclusive lasso, we can make the clustering result as much\nbalanced as possible. Extensive experiments on several large-scale datasets\nvalidate the advantage of the proposed algorithms compared to the\nstate-of-the-art clustering algorithms.\n", "versions": [{"version": "v1", "created": "Sun, 23 Nov 2014 13:16:25 GMT"}], "update_date": "2014-11-25", "authors_parsed": [["Chang", "Xiaojun", ""], ["Nie", "Feiping", ""], ["Ma", "Zhigang", ""], ["Yang", "Yi", ""]]}, {"id": "1411.6241", "submitter": "Xiaojun Chang", "authors": "Xiaojun Chang, Feiping Nie, Yi Yang and Heng Huang", "title": "Improved Spectral Clustering via Embedded Label Propagation", "comments": "Withdraw for a wrong formulation", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Spectral clustering is a key research topic in the field of machine learning\nand data mining. Most of the existing spectral clustering algorithms are built\nupon Gaussian Laplacian matrices, which are sensitive to parameters. We propose\na novel parameter free, distance consistent Locally Linear Embedding. The\nproposed distance consistent LLE promises that edges between closer data points\nhave greater weight.Furthermore, we propose a novel improved spectral\nclustering via embedded label propagation. Our algorithm is built upon two\nadvancements of the state of the art:1) label propagation,which propagates a\nnode\\'s labels to neighboring nodes according to their proximity; and 2)\nmanifold learning, which has been widely used in its capacity to leverage the\nmanifold structure of data points. First we perform standard spectral\nclustering on original data and assign each cluster to k nearest data points.\nNext, we propagate labels through dense, unlabeled data regions. Extensive\nexperiments with various datasets validate the superiority of the proposed\nalgorithm compared to current state of the art spectral algorithms.\n", "versions": [{"version": "v1", "created": "Sun, 23 Nov 2014 13:35:29 GMT"}, {"version": "v2", "created": "Tue, 6 Oct 2015 16:49:39 GMT"}], "update_date": "2015-10-07", "authors_parsed": [["Chang", "Xiaojun", ""], ["Nie", "Feiping", ""], ["Yang", "Yi", ""], ["Huang", "Heng", ""]]}, {"id": "1411.6243", "submitter": "Xu Sun", "authors": "Xu Sun", "title": "Structure Regularization for Structured Prediction: Theories and\n  Experiments", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While there are many studies on weight regularization, the study on structure\nregularization is rare. Many existing systems on structured prediction focus on\nincreasing the level of structural dependencies within the model. However, this\ntrend could have been misdirected, because our study suggests that complex\nstructures are actually harmful to generalization ability in structured\nprediction. To control structure-based overfitting, we propose a structure\nregularization framework via \\emph{structure decomposition}, which decomposes\ntraining samples into mini-samples with simpler structures, deriving a model\nwith better generalization power. We show both theoretically and empirically\nthat structure regularization can effectively control overfitting risk and lead\nto better accuracy. As a by-product, the proposed method can also substantially\naccelerate the training speed. The method and the theoretical results can apply\nto general graphical models with arbitrary structures. Experiments on\nwell-known tasks demonstrate that our method can easily beat the benchmark\nsystems on those highly-competitive tasks, achieving state-of-the-art\naccuracies yet with substantially faster training speed.\n", "versions": [{"version": "v1", "created": "Sun, 23 Nov 2014 14:11:01 GMT"}, {"version": "v2", "created": "Fri, 30 Jan 2015 07:41:21 GMT"}], "update_date": "2015-02-02", "authors_parsed": [["Sun", "Xu", ""]]}, {"id": "1411.6285", "submitter": "Avid Afzal", "authors": "Avid M. Afzal, Hamse Y. Mussa, Richard E. Turner, Andreas Bender,\n  Robert C. Glen", "title": "Target Fishing: A Single-Label or Multi-Label Problem?", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.BM cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  According to Cobanoglu et al and Murphy, it is now widely acknowledged that\nthe single target paradigm (one protein or target, one disease, one drug) that\nhas been the dominant premise in drug development in the recent past is\nuntenable. More often than not, a drug-like compound (ligand) can be\npromiscuous - that is, it can interact with more than one target protein. In\nrecent years, in in silico target prediction methods the promiscuity issue has\nbeen approached computationally in different ways. In this study we confine\nattention to the so-called ligand-based target prediction machine learning\napproaches, commonly referred to as target-fishing. With a few exceptions, the\ntarget-fishing approaches that are currently ubiquitous in cheminformatics\nliterature can be essentially viewed as single-label multi-classification\nschemes; these approaches inherently bank on the single target paradigm\nassumption that a ligand can home in on one specific target. In order to\naddress the ligand promiscuity issue, one might be able to cast target-fishing\nas a multi-label multi-class classification problem. For illustrative and\ncomparison purposes, single-label and multi-label Naive Bayes classification\nmodels (denoted here by SMM and MMM, respectively) for target-fishing were\nimplemented. The models were constructed and tested on 65,587 compounds and 308\ntargets retrieved from the ChEMBL17 database. SMM and MMM performed\ndifferently: for 16,344 test compounds, the MMM model returned recall and\nprecision values of 0.8058 and 0.6622, respectively; the corresponding recall\nand precision values yielded by the SMM model were 0.7805 and 0.7596,\nrespectively. However, at a significance level of 0.05 and one degree of\nfreedom McNemar test performed on the target prediction results returned by SMM\nand MMM for the 16,344 test ligands gave a chi-squared value of 15.656, in\nfavour of the MMM approach.\n", "versions": [{"version": "v1", "created": "Sun, 23 Nov 2014 18:50:42 GMT"}], "update_date": "2014-11-25", "authors_parsed": [["Afzal", "Avid M.", ""], ["Mussa", "Hamse Y.", ""], ["Turner", "Richard E.", ""], ["Bender", "Andreas", ""], ["Glen", "Robert C.", ""]]}, {"id": "1411.6305", "submitter": "Andres Munoz", "authors": "Mehryar Mohri and Andres Mu\\~noz Medina", "title": "Revenue Optimization in Posted-Price Auctions with Strategic Buyers", "comments": "At NIPS 2014", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study revenue optimization learning algorithms for posted-price auctions\nwith strategic buyers. We analyze a very broad family of monotone regret\nminimization algorithms for this problem, which includes the previously best\nknown algorithm, and show that no algorithm in that family admits a strategic\nregret more favorable than $\\Omega(\\sqrt{T})$. We then introduce a new\nalgorithm that achieves a strategic regret differing from the lower bound only\nby a factor in $O(\\log T)$, an exponential improvement upon the previous best\nalgorithm. Our new algorithm admits a natural analysis and simpler proofs, and\nthe ideas behind its design are general. We also report the results of\nempirical evaluations comparing our algorithm with the previous state of the\nart and show a consistent exponential improvement in several different\nscenarios.\n", "versions": [{"version": "v1", "created": "Sun, 23 Nov 2014 21:58:29 GMT"}], "update_date": "2014-11-25", "authors_parsed": [["Mohri", "Mehryar", ""], ["Medina", "Andres Mu\u00f1oz", ""]]}, {"id": "1411.6307", "submitter": "Nematollah Kayhan Batmanghelich", "authors": "Nematollah Kayhan Batmanghelich, Gerald Quon, Alex Kulesza, Manolis\n  Kellis, Polina Golland, Luke Bornn", "title": "Diversifying Sparsity Using Variational Determinantal Point Processes", "comments": "9 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel diverse feature selection method based on determinantal\npoint processes (DPPs). Our model enables one to flexibly define diversity\nbased on the covariance of features (similar to orthogonal matching pursuit) or\nalternatively based on side information. We introduce our approach in the\ncontext of Bayesian sparse regression, employing a DPP as a variational\napproximation to the true spike and slab posterior distribution. We\nsubsequently show how this variational DPP approximation generalizes and\nextends mean-field approximation, and can be learned efficiently by exploiting\nthe fast sampling properties of DPPs. Our motivating application comes from\nbioinformatics, where we aim to identify a diverse set of genes whose\nexpression profiles predict a tumor type where the diversity is defined with\nrespect to a gene-gene interaction network. We also explore an application in\nspatial statistics. In both cases, we demonstrate that the proposed method\nyields significantly more diverse feature sets than classic sparse methods,\nwithout compromising accuracy.\n", "versions": [{"version": "v1", "created": "Sun, 23 Nov 2014 22:11:34 GMT"}], "update_date": "2014-11-25", "authors_parsed": [["Batmanghelich", "Nematollah Kayhan", ""], ["Quon", "Gerald", ""], ["Kulesza", "Alex", ""], ["Kellis", "Manolis", ""], ["Golland", "Polina", ""], ["Bornn", "Luke", ""]]}, {"id": "1411.6308", "submitter": "Xiaojun Chang", "authors": "Xiaojun Chang, Feiping Nie, Zhigang Ma, Yi Yang and Xiaofang Zhou", "title": "A Convex Formulation for Spectral Shrunk Clustering", "comments": "AAAI2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Spectral clustering is a fundamental technique in the field of data mining\nand information processing. Most existing spectral clustering algorithms\nintegrate dimensionality reduction into the clustering process assisted by\nmanifold learning in the original space. However, the manifold in\nreduced-dimensional subspace is likely to exhibit altered properties in\ncontrast with the original space. Thus, applying manifold information obtained\nfrom the original space to the clustering process in a low-dimensional subspace\nis prone to inferior performance. Aiming to address this issue, we propose a\nnovel convex algorithm that mines the manifold structure in the low-dimensional\nsubspace. In addition, our unified learning process makes the manifold learning\nparticularly tailored for the clustering. Compared with other related methods,\nthe proposed algorithm results in more structured clustering result. To\nvalidate the efficacy of the proposed algorithm, we perform extensive\nexperiments on several benchmark datasets in comparison with some\nstate-of-the-art clustering approaches. The experimental results demonstrate\nthat the proposed algorithm has quite promising clustering performance.\n", "versions": [{"version": "v1", "created": "Sun, 23 Nov 2014 22:12:52 GMT"}], "update_date": "2014-11-25", "authors_parsed": [["Chang", "Xiaojun", ""], ["Nie", "Feiping", ""], ["Ma", "Zhigang", ""], ["Yang", "Yi", ""], ["Zhou", "Xiaofang", ""]]}, {"id": "1411.6314", "submitter": "Aaditya Ramdas", "authors": "Aaditya Ramdas, Sashank J. Reddi, Barnabas Poczos, Aarti Singh, Larry\n  Wasserman", "title": "On the High-dimensional Power of Linear-time Kernel Two-Sample Testing\n  under Mean-difference Alternatives", "comments": "25 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.AI cs.IT cs.LG math.IT stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Nonparametric two sample testing deals with the question of consistently\ndeciding if two distributions are different, given samples from both, without\nmaking any parametric assumptions about the form of the distributions. The\ncurrent literature is split into two kinds of tests - those which are\nconsistent without any assumptions about how the distributions may differ\n(\\textit{general} alternatives), and those which are designed to specifically\ntest easier alternatives, like a difference in means (\\textit{mean-shift}\nalternatives).\n  The main contribution of this paper is to explicitly characterize the power\nof a popular nonparametric two sample test, designed for general alternatives,\nunder a mean-shift alternative in the high-dimensional setting. Specifically,\nwe explicitly derive the power of the linear-time Maximum Mean Discrepancy\nstatistic using the Gaussian kernel, where the dimension and sample size can\nboth tend to infinity at any rate, and the two distributions differ in their\nmeans. As a corollary, we find that if the signal-to-noise ratio is held\nconstant, then the test's power goes to one if the number of samples increases\nfaster than the dimension increases. This is the first explicit power\nderivation for a general nonparametric test in the high-dimensional setting,\nand also the first analysis of how tests designed for general alternatives\nperform when faced with easier ones.\n", "versions": [{"version": "v1", "created": "Sun, 23 Nov 2014 23:32:02 GMT"}], "update_date": "2014-11-25", "authors_parsed": [["Ramdas", "Aaditya", ""], ["Reddi", "Sashank J.", ""], ["Poczos", "Barnabas", ""], ["Singh", "Aarti", ""], ["Wasserman", "Larry", ""]]}, {"id": "1411.6326", "submitter": "Shreyansh Daftry", "authors": "Debadeepta Dey, Kumar Shaurya Shankar, Sam Zeng, Rupesh Mehta, M.\n  Talha Agcayazi, Christopher Eriksen, Shreyansh Daftry, Martial Hebert, and J.\n  Andrew Bagnell", "title": "Vision and Learning for Deliberative Monocular Cluttered Flight", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cameras provide a rich source of information while being passive, cheap and\nlightweight for small and medium Unmanned Aerial Vehicles (UAVs). In this work\nwe present the first implementation of receding horizon control, which is\nwidely used in ground vehicles, with monocular vision as the only sensing mode\nfor autonomous UAV flight in dense clutter. We make it feasible on UAVs via a\nnumber of contributions: novel coupling of perception and control via relevant\nand diverse, multiple interpretations of the scene around the robot, leveraging\nrecent advances in machine learning to showcase anytime budgeted cost-sensitive\nfeature selection, and fast non-linear regression for monocular depth\nprediction. We empirically demonstrate the efficacy of our novel pipeline via\nreal world experiments of more than 2 kms through dense trees with a quadrotor\nbuilt from off-the-shelf parts. Moreover our pipeline is designed to combine\ninformation from other modalities like stereo and lidar as well if available.\n", "versions": [{"version": "v1", "created": "Mon, 24 Nov 2014 02:09:59 GMT"}], "update_date": "2014-11-25", "authors_parsed": [["Dey", "Debadeepta", ""], ["Shankar", "Kumar Shaurya", ""], ["Zeng", "Sam", ""], ["Mehta", "Rupesh", ""], ["Agcayazi", "M. Talha", ""], ["Eriksen", "Christopher", ""], ["Daftry", "Shreyansh", ""], ["Hebert", "Martial", ""], ["Bagnell", "J. Andrew", ""]]}, {"id": "1411.6358", "submitter": "Chenxu Zhao", "authors": "Junxiong Wang, Hongzhi Wang and Chenxu Zhao", "title": "A Hybrid Solution to improve Iteration Efficiency in the Distributed\n  Learning", "comments": "This paper has been withdrawn by the author due to a definition error", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Currently, many machine learning algorithms contain lots of iterations. When\nit comes to existing large-scale distributed systems, some slave nodes may\nbreak down or have lower efficiency. Therefore traditional machine learning\nalgorithm may fail because of the instability of distributed system.We presents\na hybrid approach which not only own a high fault-tolerant but also achieve a\nbalance of performance and efficiency.For each iteration, the result of slow\nmachines will be abandoned. Then, we discuss the relationship between accuracy\nand abandon rate. Next we debate the convergence speed of this process.\nFinally, our experiments demonstrate our idea can dramatically reduce\ncalculation time and be used in many platforms.\n", "versions": [{"version": "v1", "created": "Mon, 24 Nov 2014 06:42:03 GMT"}, {"version": "v2", "created": "Sun, 20 Dec 2020 11:20:04 GMT"}], "update_date": "2020-12-22", "authors_parsed": [["Wang", "Junxiong", ""], ["Wang", "Hongzhi", ""], ["Zhao", "Chenxu", ""]]}, {"id": "1411.6369", "submitter": "Yichong Xu", "authors": "Yichong Xu, Tianjun Xiao, Jiaxing Zhang, Kuiyuan Yang, Zheng Zhang", "title": "Scale-Invariant Convolutional Neural Networks", "comments": "This paper is submitted for CVPR2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Even though convolutional neural networks (CNN) has achieved near-human\nperformance in various computer vision tasks, its ability to tolerate scale\nvariations is limited. The popular practise is making the model bigger first,\nand then train it with data augmentation using extensive scale-jittering. In\nthis paper, we propose a scaleinvariant convolutional neural network (SiCNN), a\nmodeldesigned to incorporate multi-scale feature exaction and classification\ninto the network structure. SiCNN uses a multi-column architecture, with each\ncolumn focusing on a particular scale. Unlike previous multi-column strategies,\nthese columns share the same set of filter parameters by a scale transformation\namong them. This design deals with scale variation without blowing up the model\nsize. Experimental results show that SiCNN detects features at various scales,\nand the classification result exhibits strong robustness against object scale\nvariations.\n", "versions": [{"version": "v1", "created": "Mon, 24 Nov 2014 07:28:21 GMT"}], "update_date": "2014-11-25", "authors_parsed": [["Xu", "Yichong", ""], ["Xiao", "Tianjun", ""], ["Zhang", "Jiaxing", ""], ["Yang", "Kuiyuan", ""], ["Zhang", "Zheng", ""]]}, {"id": "1411.6370", "submitter": "Jun Zhu", "authors": "Jun Zhu, Jianfei Chen, Wenbo Hu, Bo Zhang", "title": "Big Learning with Bayesian Methods", "comments": "21 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.AP stat.CO stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Explosive growth in data and availability of cheap computing resources have\nsparked increasing interest in Big learning, an emerging subfield that studies\nscalable machine learning algorithms, systems, and applications with Big Data.\nBayesian methods represent one important class of statistic methods for machine\nlearning, with substantial recent developments on adaptive, flexible and\nscalable Bayesian learning. This article provides a survey of the recent\nadvances in Big learning with Bayesian methods, termed Big Bayesian Learning,\nincluding nonparametric Bayesian methods for adaptively inferring model\ncomplexity, regularized Bayesian inference for improving the flexibility via\nposterior regularization, and scalable algorithms and systems based on\nstochastic subsampling and distributed computing for dealing with large-scale\napplications.\n", "versions": [{"version": "v1", "created": "Mon, 24 Nov 2014 07:28:51 GMT"}, {"version": "v2", "created": "Wed, 1 Mar 2017 14:07:26 GMT"}], "update_date": "2017-03-02", "authors_parsed": [["Zhu", "Jun", ""], ["Chen", "Jianfei", ""], ["Hu", "Wenbo", ""], ["Zhang", "Bo", ""]]}, {"id": "1411.6400", "submitter": "Min Wei", "authors": "Min Wei, Tommy W. S. Chow, Rosa H. M. Chan", "title": "Mutual Information-Based Unsupervised Feature Transformation for\n  Heterogeneous Feature Subset Selection", "comments": "This paper has been withdrawn by the author due to the number of\n  datasets and classifiers are not sufficient to support the claim. Need more\n  simulation work", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Conventional mutual information (MI) based feature selection (FS) methods are\nunable to handle heterogeneous feature subset selection properly because of\ndata format differences or estimation methods of MI between feature subset and\nclass label. A way to solve this problem is feature transformation (FT). In\nthis study, a novel unsupervised feature transformation (UFT) which can\ntransform non-numerical features into numerical features is developed and\ntested. The UFT process is MI-based and independent of class label. MI-based FS\nalgorithms, such as Parzen window feature selector (PWFS), minimum redundancy\nmaximum relevance feature selection (mRMR), and normalized MI feature selection\n(NMIFS), can all adopt UFT for pre-processing of non-numerical features. Unlike\ntraditional FT methods, the proposed UFT is unbiased while PWFS is utilized to\nits full advantage. Simulations and analyses of large-scale datasets showed\nthat feature subset selected by the integrated method, UFT-PWFS, outperformed\nother FT-FS integrated methods in classification accuracy.\n", "versions": [{"version": "v1", "created": "Mon, 24 Nov 2014 10:15:17 GMT"}, {"version": "v2", "created": "Sun, 29 Mar 2015 05:32:50 GMT"}], "update_date": "2015-03-31", "authors_parsed": [["Wei", "Min", ""], ["Chow", "Tommy W. S.", ""], ["Chan", "Rosa H. M.", ""]]}, {"id": "1411.6520", "submitter": "Ilya Trofimov", "authors": "Ilya Trofimov, Alexander Genkin", "title": "Distributed Coordinate Descent for L1-regularized Logistic Regression", "comments": null, "journal-ref": "Analysis of Images, Social Networks and Texts. Fourth\n  International Conference, AIST 2015, Yekaterinburg, Russia, April 9-11, 2015,\n  Revised Selected Papers. Communications in Computer and Information Science,\n  Vol. 542, 243-254, Springer", "doi": "10.1007/978-3-319-26123-2_24", "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Solving logistic regression with L1-regularization in distributed settings is\nan important problem. This problem arises when training dataset is very large\nand cannot fit the memory of a single machine. We present d-GLMNET, a new\nalgorithm solving logistic regression with L1-regularization in the distributed\nsettings. We empirically show that it is superior over distributed online\nlearning via truncated gradient.\n", "versions": [{"version": "v1", "created": "Mon, 24 Nov 2014 16:40:33 GMT"}], "update_date": "2016-01-12", "authors_parsed": [["Trofimov", "Ilya", ""], ["Genkin", "Alexander", ""]]}, {"id": "1411.6590", "submitter": "Dejan Slep\\v{c}ev", "authors": "Nicolas Garcia Trillos, Dejan Slepcev, James von Brecht, Thomas\n  Laurent and Xavier Bresson", "title": "Consistency of Cheeger and Ratio Graph Cuts", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper establishes the consistency of a family of graph-cut-based\nalgorithms for clustering of data clouds. We consider point clouds obtained as\nsamples of a ground-truth measure. We investigate approaches to clustering\nbased on minimizing objective functionals defined on proximity graphs of the\ngiven sample. Our focus is on functionals based on graph cuts like the Cheeger\nand ratio cuts. We show that minimizers of the these cuts converge as the\nsample size increases to a minimizer of a corresponding continuum cut (which\npartitions the ground truth measure). Moreover, we obtain sharp conditions on\nhow the connectivity radius can be scaled with respect to the number of sample\npoints for the consistency to hold. We provide results for two-way and for\nmultiway cuts. Furthermore we provide numerical experiments that illustrate the\nresults and explore the optimality of scaling in dimension two.\n", "versions": [{"version": "v1", "created": "Mon, 24 Nov 2014 19:55:09 GMT"}], "update_date": "2014-11-25", "authors_parsed": [["Trillos", "Nicolas Garcia", ""], ["Slepcev", "Dejan", ""], ["von Brecht", "James", ""], ["Laurent", "Thomas", ""], ["Bresson", "Xavier", ""]]}, {"id": "1411.6591", "submitter": "George Chen", "authors": "Guy Bresler, George H. Chen, Devavrat Shah", "title": "A Latent Source Model for Online Collaborative Filtering", "comments": "Advances in Neural Information Processing Systems (NIPS 2014)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.IR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite the prevalence of collaborative filtering in recommendation systems,\nthere has been little theoretical development on why and how well it works,\nespecially in the \"online\" setting, where items are recommended to users over\ntime. We address this theoretical gap by introducing a model for online\nrecommendation systems, cast item recommendation under the model as a learning\nproblem, and analyze the performance of a cosine-similarity collaborative\nfiltering method. In our model, each of $n$ users either likes or dislikes each\nof $m$ items. We assume there to be $k$ types of users, and all the users of a\ngiven type share a common string of probabilities determining the chance of\nliking each item. At each time step, we recommend an item to each user, where a\nkey distinction from related bandit literature is that once a user consumes an\nitem (e.g., watches a movie), then that item cannot be recommended to the same\nuser again. The goal is to maximize the number of likable items recommended to\nusers over time. Our main result establishes that after nearly $\\log(km)$\ninitial learning time steps, a simple collaborative filtering algorithm\nachieves essentially optimal performance without knowing $k$. The algorithm has\nan exploitation step that uses cosine similarity and two types of exploration\nsteps, one to explore the space of items (standard in the literature) and the\nother to explore similarity between users (novel to this work).\n", "versions": [{"version": "v1", "created": "Fri, 31 Oct 2014 19:59:59 GMT"}], "update_date": "2014-11-25", "authors_parsed": [["Bresler", "Guy", ""], ["Chen", "George H.", ""], ["Shah", "Devavrat", ""]]}, {"id": "1411.6622", "submitter": "Osonde Osoba Ph.D.", "authors": "Osonde Adekorede Osoba", "title": "Noise Benefits in Expectation-Maximization Algorithms", "comments": "A Dissertation Presented to The Faculty of The USC Graduate School\n  University of Southern California In Partial Fulfillment of the Requirements\n  for the Degree Doctor of Philosophy (Electrical Engineering) August 2013.\n  (252 pages, 45 figures), Online:\n  http://digitallibrary.usc.edu/cdm/ref/collection/p15799coll3/id/294341", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This dissertation shows that careful injection of noise into sample data can\nsubstantially speed up Expectation-Maximization algorithms.\nExpectation-Maximization algorithms are a class of iterative algorithms for\nextracting maximum likelihood estimates from corrupted or incomplete data. The\nconvergence speed-up is an example of a noise benefit or \"stochastic resonance\"\nin statistical signal processing. The dissertation presents derivations of\nsufficient conditions for such noise-benefits and demonstrates the speed-up in\nsome ubiquitous signal-processing algorithms. These algorithms include\nparameter estimation for mixture models, the $k$-means clustering algorithm,\nthe Baum-Welch algorithm for training hidden Markov models, and backpropagation\nfor training feedforward artificial neural networks. This dissertation also\nanalyses the effects of data and model corruption on the more general Bayesian\ninference estimation framework. The main finding is a theorem guaranteeing that\nuniform approximators for Bayesian model functions produce uniform\napproximators for the posterior pdf via Bayes theorem. This result also applies\nto hierarchical and multidimensional Bayesian models.\n", "versions": [{"version": "v1", "created": "Mon, 24 Nov 2014 17:30:57 GMT"}], "update_date": "2014-11-26", "authors_parsed": [["Osoba", "Osonde Adekorede", ""]]}, {"id": "1411.6699", "submitter": "Yangfeng Ji", "authors": "Yangfeng Ji and Jacob Eisenstein", "title": "One Vector is Not Enough: Entity-Augmented Distributional Semantics for\n  Discourse Relations", "comments": "13 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Discourse relations bind smaller linguistic units into coherent texts.\nHowever, automatically identifying discourse relations is difficult, because it\nrequires understanding the semantics of the linked arguments. A more subtle\nchallenge is that it is not enough to represent the meaning of each argument of\na discourse relation, because the relation may depend on links between\nlower-level components, such as entity mentions. Our solution computes\ndistributional meaning representations by composition up the syntactic parse\ntree. A key difference from previous work on compositional distributional\nsemantics is that we also compute representations for entity mentions, using a\nnovel downward compositional pass. Discourse relations are predicted from the\ndistributional representations of the arguments, and also of their coreferent\nentity mentions. The resulting system obtains substantial improvements over the\nprevious state-of-the-art in predicting implicit discourse relations in the\nPenn Discourse Treebank.\n", "versions": [{"version": "v1", "created": "Tue, 25 Nov 2014 01:25:56 GMT"}], "update_date": "2014-11-26", "authors_parsed": [["Ji", "Yangfeng", ""], ["Eisenstein", "Jacob", ""]]}, {"id": "1411.6718", "submitter": "Mohamed Aly", "authors": "Mahmoud Nabil, Mohamed Aly, Amir Atiya", "title": "LABR: A Large Scale Arabic Sentiment Analysis Benchmark", "comments": "10 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce LABR, the largest sentiment analysis dataset to-date for the\nArabic language. It consists of over 63,000 book reviews, each rated on a scale\nof 1 to 5 stars. We investigate the properties of the dataset, and present its\nstatistics. We explore using the dataset for two tasks: (1) sentiment polarity\nclassification; and (2) ratings classification. Moreover, we provide standard\nsplits of the dataset into training, validation and testing, for both polarity\nand ratings classification, in both balanced and unbalanced settings. We extend\nour previous work by performing a comprehensive analysis on the dataset. In\nparticular, we perform an extended survey of the different classifiers\ntypically used for the sentiment polarity classification problem. We also\nconstruct a sentiment lexicon from the dataset that contains both single and\ncompound sentiment words and we explore its effectiveness. We make the dataset\nand experimental details publicly available.\n", "versions": [{"version": "v1", "created": "Tue, 25 Nov 2014 03:48:56 GMT"}, {"version": "v2", "created": "Sun, 3 May 2015 08:35:59 GMT"}], "update_date": "2015-05-05", "authors_parsed": [["Nabil", "Mahmoud", ""], ["Aly", "Mohamed", ""], ["Atiya", "Amir", ""]]}, {"id": "1411.6725", "submitter": "Haipeng Luo", "authors": "Haipeng Luo, Patrick Haffner and Jean-Francois Paiement", "title": "Accelerated Parallel Optimization Methods for Large Scale Machine\n  Learning", "comments": "Appear in the 7th NIPS Workshop on Optimization for Machine Learning", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The growing amount of high dimensional data in different machine learning\napplications requires more efficient and scalable optimization algorithms. In\nthis work, we consider combining two techniques, parallelism and Nesterov's\nacceleration, to design faster algorithms for L1-regularized loss. We first\nsimplify BOOM, a variant of gradient descent, and study it in a unified\nframework, which allows us to not only propose a refined measurement of\nsparsity to improve BOOM, but also show that BOOM is provably slower than\nFISTA. Moving on to parallel coordinate descent methods, we then propose an\nefficient accelerated version of Shotgun, improving the convergence rate from\n$O(1/t)$ to $O(1/t^2)$. Our algorithm enjoys a concise form and analysis\ncompared to previous work, and also allows one to study several connected work\nin a unified way.\n", "versions": [{"version": "v1", "created": "Tue, 25 Nov 2014 04:36:35 GMT"}], "update_date": "2014-11-26", "authors_parsed": [["Luo", "Haipeng", ""], ["Haffner", "Patrick", ""], ["Paiement", "Jean-Francois", ""]]}, {"id": "1411.7014", "submitter": "Guy Van den Broeck", "authors": "Guy Van den Broeck, Karthika Mohan, Arthur Choi, Judea Pearl", "title": "Efficient Algorithms for Bayesian Network Parameter Learning from\n  Incomplete Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose an efficient family of algorithms to learn the parameters of a\nBayesian network from incomplete data. In contrast to textbook approaches such\nas EM and the gradient method, our approach is non-iterative, yields closed\nform parameter estimates, and eliminates the need for inference in a Bayesian\nnetwork. Our approach provides consistent parameter estimates for missing data\nproblems that are MCAR, MAR, and in some cases, MNAR. Empirically, our approach\nis orders of magnitude faster than EM (as our approach requires no inference).\nGiven sufficient data, we learn parameters that can be orders of magnitude more\naccurate.\n", "versions": [{"version": "v1", "created": "Tue, 25 Nov 2014 20:39:51 GMT"}], "update_date": "2014-11-26", "authors_parsed": [["Broeck", "Guy Van den", ""], ["Mohan", "Karthika", ""], ["Choi", "Arthur", ""], ["Pearl", "Judea", ""]]}, {"id": "1411.7200", "submitter": "Ilya Tolstikhin", "authors": "Ilya Tolstikhin and Gilles Blanchard and Marius Kloft", "title": "Localized Complexities for Transductive Learning", "comments": "Appeared in Conference on Learning Theory 2014", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We show two novel concentration inequalities for suprema of empirical\nprocesses when sampling without replacement, which both take the variance of\nthe functions into account. While these inequalities may potentially have broad\napplications in learning theory in general, we exemplify their significance by\nstudying the transductive setting of learning theory. For which we provide the\nfirst excess risk bounds based on the localized complexity of the hypothesis\nclass, which can yield fast rates of convergence also in the transductive\nlearning setting. We give a preliminary analysis of the localized complexities\nfor the prominent case of kernel classes.\n", "versions": [{"version": "v1", "created": "Wed, 26 Nov 2014 12:14:22 GMT"}], "update_date": "2014-11-27", "authors_parsed": [["Tolstikhin", "Ilya", ""], ["Blanchard", "Gilles", ""], ["Kloft", "Marius", ""]]}, {"id": "1411.7245", "submitter": "Nicolas Gillis", "authors": "Arnaud Vandaele and Nicolas Gillis and Fran\\c{c}ois Glineur and Daniel\n  Tuyttens", "title": "Heuristics for Exact Nonnegative Matrix Factorization", "comments": "32 pages, 2 figures, 16 tables", "journal-ref": "Journal of Global Optimization 65 (2), pp 369-400, 2016", "doi": "10.1007/s10898-015-0350-z", "report-no": null, "categories": "math.OC cs.LG cs.NA stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The exact nonnegative matrix factorization (exact NMF) problem is the\nfollowing: given an $m$-by-$n$ nonnegative matrix $X$ and a factorization rank\n$r$, find, if possible, an $m$-by-$r$ nonnegative matrix $W$ and an $r$-by-$n$\nnonnegative matrix $H$ such that $X = WH$. In this paper, we propose two\nheuristics for exact NMF, one inspired from simulated annealing and the other\nfrom the greedy randomized adaptive search procedure. We show that these two\nheuristics are able to compute exact nonnegative factorizations for several\nclasses of nonnegative matrices (namely, linear Euclidean distance matrices,\nslack matrices, unique-disjointness matrices, and randomly generated matrices)\nand as such demonstrate their superiority over standard multi-start strategies.\nWe also consider a hybridization between these two heuristics that allows us to\ncombine the advantages of both methods. Finally, we discuss the use of these\nheuristics to gain insight on the behavior of the nonnegative rank, i.e., the\nminimum factorization rank such that an exact NMF exists. In particular, we\ndisprove a conjecture on the nonnegative rank of a Kronecker product, propose a\nnew upper bound on the extension complexity of generic $n$-gons and conjecture\nthe exact value of (i) the extension complexity of regular $n$-gons and (ii)\nthe nonnegative rank of a submatrix of the slack matrix of the correlation\npolytope.\n", "versions": [{"version": "v1", "created": "Wed, 26 Nov 2014 14:33:59 GMT"}], "update_date": "2016-10-07", "authors_parsed": [["Vandaele", "Arnaud", ""], ["Gillis", "Nicolas", ""], ["Glineur", "Fran\u00e7ois", ""], ["Tuyttens", "Daniel", ""]]}, {"id": "1411.7346", "submitter": "Gautam Kamath", "authors": "Jayadev Acharya, Cl\\'ement L. Canonne, Gautam Kamath", "title": "A Chasm Between Identity and Equivalence Testing with Conditional\n  Queries", "comments": "39 pages. To appear in Theory of Computing. Preliminary version\n  appeared in RANDOM 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC cs.LG math.PR math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A recent model for property testing of probability distributions (Chakraborty\net al., ITCS 2013, Canonne et al., SICOMP 2015) enables tremendous savings in\nthe sample complexity of testing algorithms, by allowing them to condition the\nsampling on subsets of the domain. In particular, Canonne, Ron, and Servedio\n(SICOMP 2015) showed that, in this setting, testing identity of an unknown\ndistribution $D$ (whether $D=D^\\ast$ for an explicitly known $D^\\ast$) can be\ndone with a constant number of queries, independent of the support size $n$ --\nin contrast to the required $\\Omega(\\sqrt{n})$ in the standard sampling model.\nIt was unclear whether the same stark contrast exists for the case of testing\nequivalence, where both distributions are unknown. While Canonne et al.\nestablished a $\\mathrm{poly}(\\log n)$-query upper bound for equivalence\ntesting, very recently brought down to $\\tilde O(\\log\\log n)$ by Falahatgar et\nal. (COLT 2015), whether a dependence on the domain size $n$ is necessary was\nstill open, and explicitly posed by Fischer at the Bertinoro Workshop on\nSublinear Algorithms (2014). We show that any testing algorithm for equivalence\nmust make $\\Omega(\\sqrt{\\log\\log n})$ queries in the conditional sampling\nmodel. This demonstrates a gap between identity and equivalence testing, absent\nin the standard sampling model (where both problems have sampling complexity\n$n^{\\Theta(1)}$).\n  We also obtain results on the query complexity of uniformity testing and\nsupport-size estimation with conditional samples. We answer a question of\nChakraborty et al. (ITCS 2013) showing that non-adaptive uniformity testing\nindeed requires $\\Omega(\\log n)$ queries in the conditional model. For the\nrelated problem of support-size estimation, we provide both adaptive and\nnon-adaptive algorithms, with query complexities $\\mathrm{poly}(\\log\\log n)$\nand $\\mathrm{poly}(\\log n)$, respectively.\n", "versions": [{"version": "v1", "created": "Wed, 26 Nov 2014 19:44:12 GMT"}, {"version": "v2", "created": "Sat, 18 Apr 2015 23:37:46 GMT"}, {"version": "v3", "created": "Fri, 7 Dec 2018 01:30:55 GMT"}], "update_date": "2018-12-10", "authors_parsed": [["Acharya", "Jayadev", ""], ["Canonne", "Cl\u00e9ment L.", ""], ["Kamath", "Gautam", ""]]}, {"id": "1411.7414", "submitter": "Siheng Chen", "authors": "Siheng Chen and Aliaksei Sandryhaila and Jos\\'e M. F. Moura and Jelena\n  Kova\\v{c}evi\\'c", "title": "Signal Recovery on Graphs: Variation Minimization", "comments": "To appear on IEEE Transactions on Signal Processing", "journal-ref": null, "doi": "10.1109/TSP.2015.2441042", "report-no": null, "categories": "cs.SI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of signal recovery on graphs as graphs model data\nwith complex structure as signals on a graph. Graph signal recovery implies\nrecovery of one or multiple smooth graph signals from noisy, corrupted, or\nincomplete measurements. We propose a graph signal model and formulate signal\nrecovery as a corresponding optimization problem. We provide a general solution\nby using the alternating direction methods of multipliers. We next show how\nsignal inpainting, matrix completion, robust principal component analysis, and\nanomaly detection all relate to graph signal recovery, and provide\ncorresponding specific solutions and theoretical analysis. Finally, we validate\nthe proposed methods on real-world recovery problems, including online blog\nclassification, bridge condition identification, temperature estimation,\nrecommender system, and expert opinion combination of online blog\nclassification.\n", "versions": [{"version": "v1", "created": "Wed, 26 Nov 2014 22:38:08 GMT"}, {"version": "v2", "created": "Sat, 6 Dec 2014 14:36:33 GMT"}, {"version": "v3", "created": "Fri, 29 May 2015 14:55:38 GMT"}], "update_date": "2015-10-28", "authors_parsed": [["Chen", "Siheng", ""], ["Sandryhaila", "Aliaksei", ""], ["Moura", "Jos\u00e9 M. F.", ""], ["Kova\u010devi\u0107", "Jelena", ""]]}, {"id": "1411.7432", "submitter": "Alessandra Tosi", "authors": "Alessandra Tosi, S{\\o}ren Hauberg, Alfredo Vellido, Neil D. Lawrence", "title": "Metrics for Probabilistic Geometries", "comments": "UAI 2014", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate the geometrical structure of probabilistic generative\ndimensionality reduction models using the tools of Riemannian geometry. We\nexplicitly define a distribution over the natural metric given by the models.\nWe provide the necessary algorithms to compute expected metric tensors where\nthe distribution over mappings is given by a Gaussian process. We treat the\ncorresponding latent variable model as a Riemannian manifold and we use the\nexpectation of the metric under the Gaussian process prior to define\ninterpolating paths and measure distance between latent points. We show how\ndistances that respect the expected metric lead to more appropriate generation\nof new data.\n", "versions": [{"version": "v1", "created": "Thu, 27 Nov 2014 00:27:05 GMT"}], "update_date": "2014-12-01", "authors_parsed": [["Tosi", "Alessandra", ""], ["Hauberg", "S\u00f8ren", ""], ["Vellido", "Alfredo", ""], ["Lawrence", "Neil D.", ""]]}, {"id": "1411.7441", "submitter": "Stefano Ermon", "authors": "Stefano Ermon, Ronan Le Bras, Santosh K. Suram, John M. Gregoire,\n  Carla Gomes, Bart Selman, Robert B. van Dover", "title": "Pattern Decomposition with Complex Combinatorial Constraints:\n  Application to Materials Discovery", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Identifying important components or factors in large amounts of noisy data is\na key problem in machine learning and data mining. Motivated by a pattern\ndecomposition problem in materials discovery, aimed at discovering new\nmaterials for renewable energy, e.g. for fuel and solar cells, we introduce\nCombiFD, a framework for factor based pattern decomposition that allows the\nincorporation of a-priori knowledge as constraints, including complex\ncombinatorial constraints. In addition, we propose a new pattern decomposition\nalgorithm, called AMIQO, based on solving a sequence of (mixed-integer)\nquadratic programs. Our approach considerably outperforms the state of the art\non the materials discovery problem, scaling to larger datasets and recovering\nmore precise and physically meaningful decompositions. We also show the\neffectiveness of our approach for enforcing background knowledge on other\napplication domains.\n", "versions": [{"version": "v1", "created": "Thu, 27 Nov 2014 02:31:41 GMT"}], "update_date": "2014-12-01", "authors_parsed": [["Ermon", "Stefano", ""], ["Bras", "Ronan Le", ""], ["Suram", "Santosh K.", ""], ["Gregoire", "John M.", ""], ["Gomes", "Carla", ""], ["Selman", "Bart", ""], ["van Dover", "Robert B.", ""]]}, {"id": "1411.7450", "submitter": "Chunhua Shen", "authors": "Hui Li, Chunhua Shen, Anton van den Hengel, Qinfeng Shi", "title": "Worst-Case Linear Discriminant Analysis as Scalable Semidefinite\n  Feasibility Problems", "comments": "14 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose an efficient semidefinite programming (SDP)\napproach to worst-case linear discriminant analysis (WLDA). Compared with the\ntraditional LDA, WLDA considers the dimensionality reduction problem from the\nworst-case viewpoint, which is in general more robust for classification.\nHowever, the original problem of WLDA is non-convex and difficult to optimize.\nIn this paper, we reformulate the optimization problem of WLDA into a sequence\nof semidefinite feasibility problems. To efficiently solve the semidefinite\nfeasibility problems, we design a new scalable optimization method with\nquasi-Newton methods and eigen-decomposition being the core components. The\nproposed method is orders of magnitude faster than standard interior-point\nbased SDP solvers.\n  Experiments on a variety of classification problems demonstrate that our\napproach achieves better performance than standard LDA. Our method is also much\nfaster and more scalable than standard interior-point SDP solvers based WLDA.\nThe computational complexity for an SDP with $m$ constraints and matrices of\nsize $d$ by $d$ is roughly reduced from $\\mathcal{O}(m^3+md^3+m^2d^2)$ to\n$\\mathcal{O}(d^3)$ ($m>d$ in our case).\n", "versions": [{"version": "v1", "created": "Thu, 27 Nov 2014 02:52:56 GMT"}], "update_date": "2014-12-01", "authors_parsed": [["Li", "Hui", ""], ["Shen", "Chunhua", ""], ["Hengel", "Anton van den", ""], ["Shi", "Qinfeng", ""]]}, {"id": "1411.7582", "submitter": "Zaeem Hussain", "authors": "Zaeem Hussain and Marina Meila", "title": "Graph Sensitive Indices for Comparing Clusterings", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This report discusses two new indices for comparing clusterings of a set of\npoints. The motivation for looking at new ways for comparing clusterings stems\nfrom the fact that the existing clustering indices are based on set cardinality\nalone and do not consider the positions of data points. The new indices,\nnamely, the Random Walk index (RWI) and Variation of Information with Neighbors\n(VIN), are both inspired by the clustering metric Variation of Information\n(VI). VI possesses some interesting theoretical properties which are also\ndesirable in a metric for comparing clusterings. We define our indices and\ndiscuss some of their explored properties which appear relevant for a\nclustering index. We also include the results of these indices on clusterings\nof some example data sets.\n", "versions": [{"version": "v1", "created": "Thu, 27 Nov 2014 13:19:15 GMT"}], "update_date": "2014-12-01", "authors_parsed": [["Hussain", "Zaeem", ""], ["Meila", "Marina", ""]]}, {"id": "1411.7610", "submitter": "Justin Bayer", "authors": "Justin Bayer and Christian Osendorfer", "title": "Learning Stochastic Recurrent Networks", "comments": "Submitted to conference track of ICLR 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Leveraging advances in variational inference, we propose to enhance recurrent\nneural networks with latent variables, resulting in Stochastic Recurrent\nNetworks (STORNs). The model i) can be trained with stochastic gradient\nmethods, ii) allows structured and multi-modal conditionals at each time step,\niii) features a reliable estimator of the marginal likelihood and iv) is a\ngeneralisation of deterministic recurrent neural networks. We evaluate the\nmethod on four polyphonic musical data sets and motion capture data.\n", "versions": [{"version": "v1", "created": "Thu, 27 Nov 2014 14:22:36 GMT"}, {"version": "v2", "created": "Fri, 2 Jan 2015 09:28:54 GMT"}, {"version": "v3", "created": "Thu, 5 Mar 2015 21:55:38 GMT"}], "update_date": "2015-03-09", "authors_parsed": [["Bayer", "Justin", ""], ["Osendorfer", "Christian", ""]]}, {"id": "1411.7717", "submitter": "James Martens", "authors": "James Martens, Venkatesh Medabalimi", "title": "On the Expressive Efficiency of Sum Product Networks", "comments": "Various minor revisions and corrections throughout", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sum Product Networks (SPNs) are a recently developed class of deep generative\nmodels which compute their associated unnormalized density functions using a\nspecial type of arithmetic circuit. When certain sufficient conditions, called\nthe decomposability and completeness conditions (or \"D&C\" conditions), are\nimposed on the structure of these circuits, marginal densities and other useful\nquantities, which are typically intractable for other deep generative models,\ncan be computed by what amounts to a single evaluation of the network (which is\na property known as \"validity\"). However, the effect that the D&C conditions\nhave on the capabilities of D&C SPNs is not well understood.\n  In this work we analyze the D&C conditions, expose the various connections\nthat D&C SPNs have with multilinear arithmetic circuits, and consider the\nquestion of how well they can capture various distributions as a function of\ntheir size and depth. Among our various contributions is a result which\nestablishes the existence of a relatively simple distribution with fully\ntractable marginal densities which cannot be efficiently captured by D&C SPNs\nof any depth, but which can be efficiently captured by various other deep\ngenerative models. We also show that with each additional layer of depth\npermitted, the set of distributions which can be efficiently captured by D&C\nSPNs grows in size. This kind of \"depth hierarchy\" property has been widely\nconjectured to hold for various deep models, but has never been proven for any\nof them. Some of our other contributions include a new characterization of the\nD&C conditions as sufficient and necessary ones for a slightly strengthened\nnotion of validity, and various state-machine characterizations of the types of\ncomputations that can be performed efficiently by D&C SPNs.\n", "versions": [{"version": "v1", "created": "Thu, 27 Nov 2014 23:02:41 GMT"}, {"version": "v2", "created": "Tue, 9 Dec 2014 02:00:39 GMT"}, {"version": "v3", "created": "Fri, 23 Jan 2015 03:28:47 GMT"}], "update_date": "2015-01-26", "authors_parsed": [["Martens", "James", ""], ["Medabalimi", "Venkatesh", ""]]}, {"id": "1411.7718", "submitter": "Dacheng Tao", "authors": "Tongliang Liu and Dacheng Tao", "title": "Classification with Noisy Labels by Importance Reweighting", "comments": null, "journal-ref": null, "doi": "10.1109/TPAMI.2015.2456899", "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we study a classification problem in which sample labels are\nrandomly corrupted. In this scenario, there is an unobservable sample with\nnoise-free labels. However, before being observed, the true labels are\nindependently flipped with a probability $\\rho\\in[0,0.5)$, and the random label\nnoise can be class-conditional. Here, we address two fundamental problems\nraised by this scenario. The first is how to best use the abundant surrogate\nloss functions designed for the traditional classification problem when there\nis label noise. We prove that any surrogate loss function can be used for\nclassification with noisy labels by using importance reweighting, with\nconsistency assurance that the label noise does not ultimately hinder the\nsearch for the optimal classifier of the noise-free sample. The other is the\nopen problem of how to obtain the noise rate $\\rho$. We show that the rate is\nupper bounded by the conditional probability $P(y|x)$ of the noisy sample.\nConsequently, the rate can be estimated, because the upper bound can be easily\nreached in classification problems. Experimental results on synthetic and real\ndatasets confirm the efficiency of our methods.\n", "versions": [{"version": "v1", "created": "Thu, 27 Nov 2014 23:18:51 GMT"}, {"version": "v2", "created": "Sat, 18 Jul 2015 04:03:44 GMT"}], "update_date": "2015-07-21", "authors_parsed": [["Liu", "Tongliang", ""], ["Tao", "Dacheng", ""]]}, {"id": "1411.7783", "submitter": "Harri Valpola", "authors": "Harri Valpola", "title": "From neural PCA to deep unsupervised learning", "comments": "A revised version of an article that has been accepted for\n  publication in Advances in Independent Component Analysis and Learning\n  Machines (2015), edited by Ella Bingham, Samuel Kaski, Jorma Laaksonen and\n  Jouko Lampinen", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A network supporting deep unsupervised learning is presented. The network is\nan autoencoder with lateral shortcut connections from the encoder to decoder at\neach level of the hierarchy. The lateral shortcut connections allow the higher\nlevels of the hierarchy to focus on abstract invariant features. While standard\nautoencoders are analogous to latent variable models with a single layer of\nstochastic variables, the proposed network is analogous to hierarchical latent\nvariables models. Learning combines denoising autoencoder and denoising sources\nseparation frameworks. Each layer of the network contributes to the cost\nfunction a term which measures the distance of the representations produced by\nthe encoder and the decoder. Since training signals originate from all levels\nof the network, all layers can learn efficiently even in deep networks. The\nspeedup offered by cost terms from higher levels of the hierarchy and the\nability to learn invariant features are demonstrated in experiments.\n", "versions": [{"version": "v1", "created": "Fri, 28 Nov 2014 09:03:24 GMT"}, {"version": "v2", "created": "Mon, 2 Feb 2015 12:58:05 GMT"}], "update_date": "2015-02-03", "authors_parsed": [["Valpola", "Harri", ""]]}, {"id": "1411.7817", "submitter": "Franz J. Kir\\'aly", "authors": "Franz J. Kir\\'aly, Andreas Ziehe, Klaus-Robert M\\\"uller", "title": "Learning with Algebraic Invariances, and the Invariant Kernel Trick", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When solving data analysis problems it is important to integrate prior\nknowledge and/or structural invariances. This paper contributes by a novel\nframework for incorporating algebraic invariance structure into kernels. In\nparticular, we show that algebraic properties such as sign symmetries in data,\nphase independence, scaling etc. can be included easily by essentially\nperforming the kernel trick twice. We demonstrate the usefulness of our theory\nin simulations on selected applications such as sign-invariant spectral\nclustering and underdetermined ICA.\n", "versions": [{"version": "v1", "created": "Fri, 28 Nov 2014 11:20:48 GMT"}], "update_date": "2014-12-01", "authors_parsed": [["Kir\u00e1ly", "Franz J.", ""], ["Ziehe", "Andreas", ""], ["M\u00fcller", "Klaus-Robert", ""]]}, {"id": "1411.7924", "submitter": "Bjarne {\\O}rum Fruergaard", "authors": "Bjarne {\\O}rum Fruergaard", "title": "Predicting clicks in online display advertising with latent features and\n  side-information", "comments": "Technical Report", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We review a method for click-through rate prediction based on the work of\nMenon et al. [11], which combines collaborative filtering and matrix\nfactorization with a side-information model and fuses the outputs to proper\nprobabilities in [0,1]. In addition we provide details, both for the modeling\nas well as the experimental part, that are not found elsewhere. We rigorously\ntest the performance on several test data sets from consecutive days in a\nclick-through rate prediction setup, in a manner which reflects a real-world\npipeline. Our results confirm that performance can be increased using latent\nfeatures, albeit the differences in the measures are small but significant.\n", "versions": [{"version": "v1", "created": "Fri, 28 Nov 2014 16:06:52 GMT"}], "update_date": "2014-12-01", "authors_parsed": [["Fruergaard", "Bjarne \u00d8rum", ""]]}, {"id": "1411.7973", "submitter": "Marcos R Vieira", "authors": "Matthias Kormaksson, Luciano Barbosa, Marcos R. Vieira, Bianca\n  Zadrozny", "title": "Bus Travel Time Predictions Using Additive Models", "comments": "11 pages, this is the technical report supporting the IEEE 2014\n  International Conference on Data Mining (ICDM) submission with the same title", "journal-ref": null, "doi": "10.1109/ICDM.2014.107", "report-no": null, "categories": "cs.LG stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many factors can affect the predictability of public bus services such as\ntraffic, weather and local events. Other aspects, such as day of week or hour\nof day, may influence bus travel times as well, either directly or in\nconjunction with other variables. However, the exact nature of such\nrelationships between travel times and predictor variables is, in most\nsituations, not known. In this paper we develop a framework that allows for\nflexible modeling of bus travel times through the use of Additive Models. In\nparticular, we model travel times as a sum of linear as well as nonlinear terms\nthat are modeled as smooth functions of predictor variables. The proposed class\nof models provides a principled statistical framework that is highly flexible\nin terms of model building. The experimental results demonstrate uniformly\nsuperior performance of our best model as compared to previous prediction\nmethods when applied to a very large GPS data set obtained from buses operating\nin the city of Rio de Janeiro.\n", "versions": [{"version": "v1", "created": "Fri, 28 Nov 2014 18:45:31 GMT"}], "update_date": "2016-11-15", "authors_parsed": [["Kormaksson", "Matthias", ""], ["Barbosa", "Luciano", ""], ["Vieira", "Marcos R.", ""], ["Zadrozny", "Bianca", ""]]}, {"id": "1411.8003", "submitter": "Ruoyu Sun", "authors": "Ruoyu Sun, Zhi-Quan Luo", "title": "Guaranteed Matrix Completion via Non-convex Factorization", "comments": "77 pages. Accepted to IEEE Transaction on Information theory. A\n  detailed description of the proof ideas is added, compared to version 2", "journal-ref": null, "doi": "10.1109/TIT.2016.2598574", "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Matrix factorization is a popular approach for large-scale matrix completion.\nThe optimization formulation based on matrix factorization can be solved very\nefficiently by standard algorithms in practice. However, due to the\nnon-convexity caused by the factorization model, there is a limited theoretical\nunderstanding of this formulation. In this paper, we establish a theoretical\nguarantee for the factorization formulation to correctly recover the underlying\nlow-rank matrix. In particular, we show that under similar conditions to those\nin previous works, many standard optimization algorithms converge to the global\noptima of a factorization formulation, and recover the true low-rank matrix. We\nstudy the local geometry of a properly regularized factorization formulation\nand prove that any stationary point in a certain local region is globally\noptimal. A major difference of our work from the existing results is that we do\nnot need resampling in either the algorithm or its analysis. Compared to other\nworks on nonconvex optimization, one extra difficulty lies in analyzing\nnonconvex constrained optimization when the constraint (or the corresponding\nregularizer) is not \"consistent\" with the gradient direction. One technical\ncontribution is the perturbation analysis for non-symmetric matrix\nfactorization.\n", "versions": [{"version": "v1", "created": "Fri, 28 Nov 2014 20:52:47 GMT"}, {"version": "v2", "created": "Fri, 29 May 2015 22:31:49 GMT"}, {"version": "v3", "created": "Tue, 11 Oct 2016 17:35:12 GMT"}], "update_date": "2016-11-18", "authors_parsed": [["Sun", "Ruoyu", ""], ["Luo", "Zhi-Quan", ""]]}]