[{"id": "1706.00005", "submitter": "Lars Ailo Bongo", "authors": "Morten Gr{\\o}nnesby, Juan Carlos Aviles Solis, Einar Holsb{\\o}, Hasse\n  Melbye, Lars Ailo Bongo", "title": "Feature Extraction for Machine Learning Based Crackle Detection in Lung\n  Sounds from a Health Survey", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In recent years, many innovative solutions for recording and viewing sounds\nfrom a stethoscope have become available. However, to fully utilize such\ndevices, there is a need for an automated approach for detecting abnormal lung\nsounds, which is better than the existing methods that typically have been\ndeveloped and evaluated using a small and non-diverse dataset.\n  We propose a machine learning based approach for detecting crackles in lung\nsounds recorded using a stethoscope in a large health survey. Our method is\ntrained and evaluated using 209 files with crackles classified by expert\nlisteners. Our analysis pipeline is based on features extracted from small\nwindows in audio files. We evaluated several feature extraction methods and\nclassifiers. We evaluated the pipeline using a training set of 175 crackle\nwindows and 208 normal windows. We did 100 cycles of cross validation where we\nshuffled training sets between cycles. For all the division between training\nand evaluation was 70%-30%.\n  We found and evaluated a 5-dimenstional vector with four features from the\ntime domain and one from the spectrum domain. We evaluated several classifiers\nand found SVM with a Radial Basis Function Kernel to perform best. Our approach\nhad a precision of 86% and recall of 84% for classifying a crackle in a window,\nwhich is more accurate than found in studies of health personnel. The\nlow-dimensional feature vector makes the SVM very fast. The model can be\ntrained on a regular computer in 1.44 seconds, and 319 crackles can be\nclassified in 1.08 seconds.\n  Our approach detects and visualizes individual crackles in recorded audio\nfiles. It is accurate, fast, and has low resource requirements. It can be used\nto train health personnel or as part of a smartphone application for Bluetooth\nstethoscopes.\n", "versions": [{"version": "v1", "created": "Wed, 31 May 2017 16:24:28 GMT"}, {"version": "v2", "created": "Sat, 23 Dec 2017 22:25:06 GMT"}], "update_date": "2017-12-27", "authors_parsed": [["Gr\u00f8nnesby", "Morten", ""], ["Solis", "Juan Carlos Aviles", ""], ["Holsb\u00f8", "Einar", ""], ["Melbye", "Hasse", ""], ["Bongo", "Lars Ailo", ""]]}, {"id": "1706.00038", "submitter": "Arash Vahdat", "authors": "Arash Vahdat", "title": "Toward Robustness against Label Noise in Training Deep Discriminative\n  Neural Networks", "comments": "To appear in Neural Information Processing Systems (NIPS) 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Collecting large training datasets, annotated with high-quality labels, is\ncostly and time-consuming. This paper proposes a novel framework for training\ndeep convolutional neural networks from noisy labeled datasets that can be\nobtained cheaply. The problem is formulated using an undirected graphical model\nthat represents the relationship between noisy and clean labels, trained in a\nsemi-supervised setting. In our formulation, the inference over latent clean\nlabels is tractable and is regularized during training using auxiliary sources\nof information. The proposed model is applied to the image labeling problem and\nis shown to be effective in labeling unseen images as well as reducing label\nnoise in training on CIFAR-10 and MS COCO datasets.\n", "versions": [{"version": "v1", "created": "Wed, 31 May 2017 18:16:49 GMT"}, {"version": "v2", "created": "Fri, 3 Nov 2017 01:47:50 GMT"}], "update_date": "2017-11-06", "authors_parsed": [["Vahdat", "Arash", ""]]}, {"id": "1706.00043", "submitter": "Angelos Katharopoulos", "authors": "Angelos Katharopoulos and Fran\\c{c}ois Fleuret", "title": "Biased Importance Sampling for Deep Neural Network Training", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Importance sampling has been successfully used to accelerate stochastic\noptimization in many convex problems. However, the lack of an efficient way to\ncalculate the importance still hinders its application to Deep Learning.\n  In this paper, we show that the loss value can be used as an alternative\nimportance metric, and propose a way to efficiently approximate it for a deep\nmodel, using a small model trained for that purpose in parallel.\n  This method allows in particular to utilize a biased gradient estimate that\nimplicitly optimizes a soft max-loss, and leads to better generalization\nperformance. While such method suffers from a prohibitively high variance of\nthe gradient estimate when using a standard stochastic optimizer, we show that\nwhen it is combined with our sampling mechanism, it results in a reliable\nprocedure.\n  We showcase the generality of our method by testing it on both image\nclassification and language modeling tasks using deep convolutional and\nrecurrent neural networks. In particular, our method results in 30% faster\ntraining of a CNN for CIFAR10 than when using uniform sampling.\n", "versions": [{"version": "v1", "created": "Wed, 31 May 2017 18:25:09 GMT"}, {"version": "v2", "created": "Wed, 13 Sep 2017 12:54:33 GMT"}], "update_date": "2017-09-14", "authors_parsed": [["Katharopoulos", "Angelos", ""], ["Fleuret", "Fran\u00e7ois", ""]]}, {"id": "1706.00046", "submitter": "Ludovic Denoyer", "authors": "Tom Veniat and Ludovic Denoyer", "title": "Learning Time/Memory-Efficient Deep Architectures with Budgeted Super\n  Networks", "comments": "2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition\n  (CVPR)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose to focus on the problem of discovering neural network\narchitectures efficient in terms of both prediction quality and cost. For\ninstance, our approach is able to solve the following tasks: learn a neural\nnetwork able to predict well in less than 100 milliseconds or learn an\nefficient model that fits in a 50 Mb memory. Our contribution is a novel family\nof models called Budgeted Super Networks (BSN). They are learned using gradient\ndescent techniques applied on a budgeted learning objective function which\nintegrates a maximum authorized cost, while making no assumption on the nature\nof this cost. We present a set of experiments on computer vision problems and\nanalyze the ability of our technique to deal with three different costs: the\ncomputation cost, the memory consumption cost and a distributed computation\ncost. We particularly show that our model can discover neural network\narchitectures that have a better accuracy than the ResNet and Convolutional\nNeural Fabrics architectures on CIFAR-10 and CIFAR-100, at a lower cost.\n", "versions": [{"version": "v1", "created": "Wed, 31 May 2017 18:34:50 GMT"}, {"version": "v2", "created": "Mon, 27 Nov 2017 09:09:34 GMT"}, {"version": "v3", "created": "Wed, 21 Feb 2018 17:05:14 GMT"}, {"version": "v4", "created": "Tue, 22 May 2018 21:28:48 GMT"}], "update_date": "2018-05-24", "authors_parsed": [["Veniat", "Tom", ""], ["Denoyer", "Ludovic", ""]]}, {"id": "1706.00051", "submitter": "Morteza Mardani", "authors": "Morteza Mardani, Enhao Gong, Joseph Y. Cheng, Shreyas Vasanawala, Greg\n  Zaharchuk, Marcus Alley, Neil Thakur, Song Han, William Dally, John M. Pauly,\n  and Lei Xing", "title": "Deep Generative Adversarial Networks for Compressed Sensing Automates\n  MRI", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Magnetic resonance image (MRI) reconstruction is a severely ill-posed linear\ninverse task demanding time and resource intensive computations that can\nsubstantially trade off {\\it accuracy} for {\\it speed} in real-time imaging. In\naddition, state-of-the-art compressed sensing (CS) analytics are not cognizant\nof the image {\\it diagnostic quality}. To cope with these challenges we put\nforth a novel CS framework that permeates benefits from generative adversarial\nnetworks (GAN) to train a (low-dimensional) manifold of diagnostic-quality MR\nimages from historical patients. Leveraging a mixture of least-squares (LS)\nGANs and pixel-wise $\\ell_1$ cost, a deep residual network with skip\nconnections is trained as the generator that learns to remove the {\\it\naliasing} artifacts by projecting onto the manifold. LSGAN learns the texture\ndetails, while $\\ell_1$ controls the high-frequency noise. A multilayer\nconvolutional neural network is then jointly trained based on diagnostic\nquality images to discriminate the projection quality. The test phase performs\nfeed-forward propagation over the generator network that demands a very low\ncomputational overhead. Extensive evaluations are performed on a large\ncontrast-enhanced MR dataset of pediatric patients. In particular, images rated\nbased on expert radiologists corroborate that GANCS retrieves high contrast\nimages with detailed texture relative to conventional CS, and pixel-wise\nschemes. In addition, it offers reconstruction under a few milliseconds, two\norders of magnitude faster than state-of-the-art CS-MRI schemes.\n", "versions": [{"version": "v1", "created": "Wed, 31 May 2017 19:12:14 GMT"}], "update_date": "2017-06-02", "authors_parsed": [["Mardani", "Morteza", ""], ["Gong", "Enhao", ""], ["Cheng", "Joseph Y.", ""], ["Vasanawala", "Shreyas", ""], ["Zaharchuk", "Greg", ""], ["Alley", "Marcus", ""], ["Thakur", "Neil", ""], ["Han", "Song", ""], ["Dally", "William", ""], ["Pauly", "John M.", ""], ["Xing", "Lei", ""]]}, {"id": "1706.00061", "submitter": "Reinhard Heckel", "authors": "Reinhard Heckel and Kannan Ramchandran", "title": "The Sample Complexity of Online One-Class Collaborative Filtering", "comments": "ICML 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.IT math.IT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the online one-class collaborative filtering (CF) problem that\nconsists of recommending items to users over time in an online fashion based on\npositive ratings only. This problem arises when users respond only occasionally\nto a recommendation with a positive rating, and never with a negative one. We\nstudy the impact of the probability of a user responding to a recommendation,\np_f, on the sample complexity, i.e., the number of ratings required to make\n`good' recommendations, and ask whether receiving positive and negative\nratings, instead of positive ratings only, improves the sample complexity. Both\nquestions arise in the design of recommender systems. We introduce a simple\nprobabilistic user model, and analyze the performance of an online user-based\nCF algorithm. We prove that after an initial cold start phase, where\nrecommendations are invested in exploring the user's preferences, this\nalgorithm makes---up to a fraction of the recommendations required for updating\nthe user's preferences---perfect recommendations. The number of ratings\nrequired for the cold start phase is nearly proportional to 1/p_f, and that for\nupdating the user's preferences is essentially independent of p_f. As a\nconsequence we find that, receiving positive and negative ratings instead of\nonly positive ones improves the number of ratings required for initial\nexploration by a factor of 1/p_f, which can be significant.\n", "versions": [{"version": "v1", "created": "Wed, 31 May 2017 19:37:12 GMT"}], "update_date": "2017-06-02", "authors_parsed": [["Heckel", "Reinhard", ""], ["Ramchandran", "Kannan", ""]]}, {"id": "1706.00074", "submitter": "Daniel Crawford", "authors": "Anna Levit, Daniel Crawford, Navid Ghadermarzy, Jaspreet S. Oberoi,\n  Ehsan Zahedinejad, Pooya Ronagh", "title": "Free energy-based reinforcement learning using a quantum processor", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.NE math.OC quant-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent theoretical and experimental results suggest the possibility of using\ncurrent and near-future quantum hardware in challenging sampling tasks. In this\npaper, we introduce free energy-based reinforcement learning (FERL) as an\napplication of quantum hardware. We propose a method for processing a quantum\nannealer's measured qubit spin configurations in approximating the free energy\nof a quantum Boltzmann machine (QBM). We then apply this method to perform\nreinforcement learning on the grid-world problem using the D-Wave 2000Q quantum\nannealer. The experimental results show that our technique is a promising\nmethod for harnessing the power of quantum sampling in reinforcement learning\ntasks.\n", "versions": [{"version": "v1", "created": "Mon, 29 May 2017 18:57:42 GMT"}], "update_date": "2017-06-02", "authors_parsed": [["Levit", "Anna", ""], ["Crawford", "Daniel", ""], ["Ghadermarzy", "Navid", ""], ["Oberoi", "Jaspreet S.", ""], ["Zahedinejad", "Ehsan", ""], ["Ronagh", "Pooya", ""]]}, {"id": "1706.00078", "submitter": "Nicolas Gillis", "authors": "Nicolas Gillis, Yaroslav Shitov", "title": "Low-Rank Matrix Approximation in the Infinity Norm", "comments": "12 pages, 3 tables", "journal-ref": "Linear Algebra and its Applications 581, pp. 367-382, 2019", "doi": "10.1016/j.laa.2019.07.017", "report-no": null, "categories": "cs.CC cs.LG math.NA math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The low-rank matrix approximation problem with respect to the entry-wise\n$\\ell_{\\infty}$-norm is the following: given a matrix $M$ and a factorization\nrank $r$, find a matrix $X$ whose rank is at most $r$ and that minimizes\n$\\max_{i,j} |M_{ij} - X_{ij}|$. In this paper, we prove that the decision\nvariant of this problem for $r=1$ is NP-complete using a reduction from the\nproblem `not all equal 3SAT'. We also analyze several cases when the problem\ncan be solved in polynomial time, and propose a simple practical heuristic\nalgorithm which we apply on the problem of the recovery of a quantized low-rank\nmatrix.\n", "versions": [{"version": "v1", "created": "Wed, 31 May 2017 20:32:51 GMT"}], "update_date": "2019-08-06", "authors_parsed": [["Gillis", "Nicolas", ""], ["Shitov", "Yaroslav", ""]]}, {"id": "1706.00082", "submitter": "Marco Marchesi", "authors": "Marco Marchesi", "title": "Megapixel Size Image Creation using Generative Adversarial Networks", "comments": "3 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Since its appearance, Generative Adversarial Networks (GANs) have received a\nlot of interest in the AI community. In image generation several projects\nshowed how GANs are able to generate photorealistic images but the results so\nfar did not look adequate for the quality standard of visual media production\nindustry. We present an optimized image generation process based on a Deep\nConvolutional Generative Adversarial Networks (DCGANs), in order to create\nphotorealistic high-resolution images (up to 1024x1024 pixels). Furthermore,\nthe system was fed with a limited dataset of images, less than two thousand\nimages. All these results give more clue about future exploitation of GANs in\nComputer Graphics and Visual Effects.\n", "versions": [{"version": "v1", "created": "Wed, 31 May 2017 20:43:19 GMT"}], "update_date": "2017-06-02", "authors_parsed": [["Marchesi", "Marco", ""]]}, {"id": "1706.00090", "submitter": "Jonathan Scarlett", "authors": "Jonathan Scarlett, Ilijia Bogunovic, Volkan Cevher", "title": "Lower Bounds on Regret for Noisy Gaussian Process Bandit Optimization", "comments": "Appearing in COLT 2017. This version corrects a few minor mistakes in\n  Table I, which summarizes the new and existing regret bounds", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.IT cs.LG math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we consider the problem of sequentially optimizing a black-box\nfunction $f$ based on noisy samples and bandit feedback. We assume that $f$ is\nsmooth in the sense of having a bounded norm in some reproducing kernel Hilbert\nspace (RKHS), yielding a commonly-considered non-Bayesian form of Gaussian\nprocess bandit optimization. We provide algorithm-independent lower bounds on\nthe simple regret, measuring the suboptimality of a single point reported after\n$T$ rounds, and on the cumulative regret, measuring the sum of regrets over the\n$T$ chosen points. For the isotropic squared-exponential kernel in $d$\ndimensions, we find that an average simple regret of $\\epsilon$ requires $T =\n\\Omega\\big(\\frac{1}{\\epsilon^2} (\\log\\frac{1}{\\epsilon})^{d/2}\\big)$, and the\naverage cumulative regret is at least $\\Omega\\big( \\sqrt{T(\\log T)^{d/2}}\n\\big)$, thus matching existing upper bounds up to the replacement of $d/2$ by\n$2d+O(1)$ in both cases. For the Mat\\'ern-$\\nu$ kernel, we give analogous\nbounds of the form $\\Omega\\big( (\\frac{1}{\\epsilon})^{2+d/\\nu}\\big)$ and\n$\\Omega\\big( T^{\\frac{\\nu + d}{2\\nu + d}} \\big)$, and discuss the resulting\ngaps to the existing upper bounds.\n", "versions": [{"version": "v1", "created": "Wed, 31 May 2017 21:14:06 GMT"}, {"version": "v2", "created": "Fri, 16 Jun 2017 13:45:34 GMT"}, {"version": "v3", "created": "Thu, 31 May 2018 08:05:38 GMT"}], "update_date": "2018-06-01", "authors_parsed": [["Scarlett", "Jonathan", ""], ["Bogunovic", "Ilijia", ""], ["Cevher", "Volkan", ""]]}, {"id": "1706.00095", "submitter": "Janis Keuper", "authors": "Martin Kuehn, Janis Keuper and Franz-Josef Pfreundt", "title": "Using GPI-2 for Distributed Memory Paralleliziation of the Caffe Toolbox\n  to Speed up Deep Neural Network Training", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DC", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Deep Neural Network (DNN) are currently of great inter- est in research and\napplication. The training of these net- works is a compute intensive and time\nconsuming task. To reduce training times to a bearable amount at reasonable\ncost we extend the popular Caffe toolbox for DNN with an efficient distributed\nmemory communication pattern. To achieve good scalability we emphasize the\noverlap of computation and communication and prefer fine granu- lar\nsynchronization patterns over global barriers. To im- plement these\ncommunication patterns we rely on the the Global address space Programming\nInterface version 2 (GPI-2) communication library. This interface provides a\nlight-weight set of asynchronous one-sided communica- tion primitives\nsupplemented by non-blocking fine gran- ular data synchronization mechanisms.\nTherefore, Caf- feGPI is the name of our parallel version of Caffe. First\nbenchmarks demonstrate better scaling behavior com- pared with other\nextensions, e.g., the Intel TM Caffe. Even within a single symmetric\nmultiprocessing machine with four graphics processing units, the CaffeGPI\nscales bet- ter than the standard Caffe toolbox. These first results\ndemonstrate that the use of standard High Performance Computing (HPC) hardware\nis a valid cost saving ap- proach to train large DDNs. I/O is an other\nbottleneck to work with DDNs in a standard parallel HPC setting, which we will\nconsider in more detail in a forthcoming paper.\n", "versions": [{"version": "v1", "created": "Wed, 31 May 2017 21:20:16 GMT"}, {"version": "v2", "created": "Fri, 18 Aug 2017 12:24:45 GMT"}], "update_date": "2017-08-21", "authors_parsed": [["Kuehn", "Martin", ""], ["Keuper", "Janis", ""], ["Pfreundt", "Franz-Josef", ""]]}, {"id": "1706.00119", "submitter": "Christos Dimitrakakis", "authors": "Christos Dimitrakakis and Yang Liu and David Parkes and Goran\n  Radanovic", "title": "Bayesian fairness", "comments": "13 pages, 8 figures, to appear at AAAI 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of how decision making can be fair when the\nunderlying probabilistic model of the world is not known with certainty. We\nargue that recent notions of fairness in machine learning need to explicitly\nincorporate parameter uncertainty, hence we introduce the notion of {\\em\nBayesian fairness} as a suitable candidate for fair decision rules. Using\nbalance, a definition of fairness introduced by Kleinberg et al (2016), we show\nhow a Bayesian perspective can lead to well-performing, fair decision rules\neven under high uncertainty.\n", "versions": [{"version": "v1", "created": "Wed, 31 May 2017 23:13:35 GMT"}, {"version": "v2", "created": "Mon, 9 Jul 2018 10:09:37 GMT"}, {"version": "v3", "created": "Sun, 4 Nov 2018 13:22:20 GMT"}], "update_date": "2018-11-06", "authors_parsed": [["Dimitrakakis", "Christos", ""], ["Liu", "Yang", ""], ["Parkes", "David", ""], ["Radanovic", "Goran", ""]]}, {"id": "1706.00136", "submitter": "Kwang-Sung Jun", "authors": "Kwang-Sung Jun, Aniruddha Bhargava, Robert Nowak, Rebecca Willett", "title": "Scalable Generalized Linear Bandits: Online Computation and Hashing", "comments": "accepted to NIPS'17 (typos fixed)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generalized Linear Bandits (GLBs), a natural extension of the stochastic\nlinear bandits, has been popular and successful in recent years. However,\nexisting GLBs scale poorly with the number of rounds and the number of arms,\nlimiting their utility in practice. This paper proposes new, scalable solutions\nto the GLB problem in two respects. First, unlike existing GLBs, whose\nper-time-step space and time complexity grow at least linearly with time $t$,\nwe propose a new algorithm that performs online computations to enjoy a\nconstant space and time complexity. At its heart is a novel Generalized Linear\nextension of the Online-to-confidence-set Conversion (GLOC method) that takes\n\\emph{any} online learning algorithm and turns it into a GLB algorithm. As a\nspecial case, we apply GLOC to the online Newton step algorithm, which results\nin a low-regret GLB algorithm with much lower time and memory complexity than\nprior work. Second, for the case where the number $N$ of arms is very large, we\npropose new algorithms in which each next arm is selected via an inner product\nsearch. Such methods can be implemented via hashing algorithms (i.e.,\n\"hash-amenable\") and result in a time complexity sublinear in $N$. While a\nThompson sampling extension of GLOC is hash-amenable, its regret bound for\n$d$-dimensional arm sets scales with $d^{3/2}$, whereas GLOC's regret bound\nscales with $d$. Towards closing this gap, we propose a new hash-amenable\nalgorithm whose regret bound scales with $d^{5/4}$. Finally, we propose a fast\napproximate hash-key computation (inner product) with a better accuracy than\nthe state-of-the-art, which can be of independent interest. We conclude the\npaper with preliminary experimental results confirming the merits of our\nmethods.\n", "versions": [{"version": "v1", "created": "Thu, 1 Jun 2017 00:41:42 GMT"}, {"version": "v2", "created": "Thu, 21 Sep 2017 03:18:58 GMT"}, {"version": "v3", "created": "Sat, 21 Oct 2017 17:40:05 GMT"}], "update_date": "2017-10-24", "authors_parsed": [["Jun", "Kwang-Sung", ""], ["Bhargava", "Aniruddha", ""], ["Nowak", "Robert", ""], ["Willett", "Rebecca", ""]]}, {"id": "1706.00153", "submitter": "Yuxin Peng", "authors": "Xin Huang, Yuxin Peng, and Mingkuan Yuan", "title": "Cross-modal Common Representation Learning by Hybrid Transfer Network", "comments": "To appear in the proceedings of 26th International Joint Conference\n  on Artificial Intelligence (IJCAI), Melbourne, Australia, Aug. 19-25, 2017. 8\n  pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  DNN-based cross-modal retrieval is a research hotspot to retrieve across\ndifferent modalities as image and text, but existing methods often face the\nchallenge of insufficient cross-modal training data. In single-modal scenario,\nsimilar problem is usually relieved by transferring knowledge from large-scale\nauxiliary datasets (as ImageNet). Knowledge from such single-modal datasets is\nalso very useful for cross-modal retrieval, which can provide rich general\nsemantic information that can be shared across different modalities. However,\nit is challenging to transfer useful knowledge from single-modal (as image)\nsource domain to cross-modal (as image/text) target domain. Knowledge in source\ndomain cannot be directly transferred to both two different modalities in\ntarget domain, and the inherent cross-modal correlation contained in target\ndomain provides key hints for cross-modal retrieval which should be preserved\nduring transfer process. This paper proposes Cross-modal Hybrid Transfer\nNetwork (CHTN) with two subnetworks: Modal-sharing transfer subnetwork utilizes\nthe modality in both source and target domains as a bridge, for transferring\nknowledge to both two modalities simultaneously; Layer-sharing correlation\nsubnetwork preserves the inherent cross-modal semantic correlation to further\nadapt to cross-modal retrieval task. Cross-modal data can be converted to\ncommon representation by CHTN for retrieval, and comprehensive experiment on 3\ndatasets shows its effectiveness.\n", "versions": [{"version": "v1", "created": "Thu, 1 Jun 2017 02:53:57 GMT"}, {"version": "v2", "created": "Sat, 24 Jun 2017 14:08:19 GMT"}], "update_date": "2017-06-27", "authors_parsed": [["Huang", "Xin", ""], ["Peng", "Yuxin", ""], ["Yuan", "Mingkuan", ""]]}, {"id": "1706.00241", "submitter": "Filip De Roos", "authors": "Filip de Roos and Philipp Hennig", "title": "Krylov Subspace Recycling for Fast Iterative Least-Squares in Machine\n  Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.NA stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Solving symmetric positive definite linear problems is a fundamental\ncomputational task in machine learning. The exact solution, famously, is\ncubicly expensive in the size of the matrix. To alleviate this problem, several\nlinear-time approximations, such as spectral and inducing-point methods, have\nbeen suggested and are now in wide use. These are low-rank approximations that\nchoose the low-rank space a priori and do not refine it over time. While this\nallows linear cost in the data-set size, it also causes a finite, uncorrected\napproximation error. Authors from numerical linear algebra have explored ways\nto iteratively refine such low-rank approximations, at a cost of a small number\nof matrix-vector multiplications. This idea is particularly interesting in the\nmany situations in machine learning where one has to solve a sequence of\nrelated symmetric positive definite linear problems. From the machine learning\nperspective, such deflation methods can be interpreted as transfer learning of\na low-rank approximation across a time-series of numerical tasks. We study the\nuse of such methods for our field. Our empirical results show that, on\nregression and classification problems of intermediate size, this approach can\ninterpolate between low computational cost and numerical precision.\n", "versions": [{"version": "v1", "created": "Thu, 1 Jun 2017 10:17:12 GMT"}], "update_date": "2017-06-02", "authors_parsed": [["de Roos", "Filip", ""], ["Hennig", "Philipp", ""]]}, {"id": "1706.00244", "submitter": "Jean-Philippe Vert", "authors": "Marine Le Morvan (CBIO), Jean-Philippe Vert (DMA, CBIO)", "title": "Supervised Quantile Normalisation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG q-bio.QM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Quantile normalisation is a popular normalisation method for data subject to\nunwanted variations such as images, speech, or genomic data. It applies a\nmonotonic transformation to the feature values of each sample to ensure that\nafter normalisation, they follow the same target distribution for each sample.\nChoosing a \"good\" target distribution remains however largely empirical and\nheuristic, and is usually done independently of the subsequent analysis of\nnormalised data. We propose instead to couple the quantile normalisation step\nwith the subsequent analysis, and to optimise the target distribution jointly\nwith the other parameters in the analysis. We illustrate this principle on the\nproblem of estimating a linear model over normalised data, and show that it\nleads to a particular low-rank matrix regression problem that can be solved\nefficiently. We illustrate the potential of our method, which we term SUQUAN,\non simulated data, images and genomic data, where it outperforms standard\nquantile normalisation.\n", "versions": [{"version": "v1", "created": "Thu, 1 Jun 2017 10:25:50 GMT"}], "update_date": "2017-06-02", "authors_parsed": [["Morvan", "Marine Le", "", "CBIO"], ["Vert", "Jean-Philippe", "", "DMA, CBIO"]]}, {"id": "1706.00286", "submitter": "Dzmitry Bahdanau", "authors": "Dzmitry Bahdanau, Tom Bosc, Stanis{\\l}aw Jastrz\\k{e}bski, Edward\n  Grefenstette, Pascal Vincent, Yoshua Bengio", "title": "Learning to Compute Word Embeddings On the Fly", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Words in natural language follow a Zipfian distribution whereby some words\nare frequent but most are rare. Learning representations for words in the \"long\ntail\" of this distribution requires enormous amounts of data. Representations\nof rare words trained directly on end tasks are usually poor, requiring us to\npre-train embeddings on external data, or treat all rare words as\nout-of-vocabulary words with a unique representation. We provide a method for\npredicting embeddings of rare words on the fly from small amounts of auxiliary\ndata with a network trained end-to-end for the downstream task. We show that\nthis improves results against baselines where embeddings are trained on the end\ntask for reading comprehension, recognizing textual entailment and language\nmodeling.\n", "versions": [{"version": "v1", "created": "Thu, 1 Jun 2017 13:12:15 GMT"}, {"version": "v2", "created": "Mon, 5 Jun 2017 20:18:27 GMT"}, {"version": "v3", "created": "Wed, 7 Mar 2018 16:07:10 GMT"}], "update_date": "2018-03-08", "authors_parsed": [["Bahdanau", "Dzmitry", ""], ["Bosc", "Tom", ""], ["Jastrz\u0119bski", "Stanis\u0142aw", ""], ["Grefenstette", "Edward", ""], ["Vincent", "Pascal", ""], ["Bengio", "Yoshua", ""]]}, {"id": "1706.00290", "submitter": "Julius Kunze", "authors": "Julius Kunze, Louis Kirsch, Ilia Kurenkov, Andreas Krug, Jens\n  Johannsmeier and Sebastian Stober", "title": "Transfer Learning for Speech Recognition on a Budget", "comments": "Accepted for 2nd ACL Workshop on Representation Learning for NLP", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  End-to-end training of automated speech recognition (ASR) systems requires\nmassive data and compute resources. We explore transfer learning based on model\nadaptation as an approach for training ASR models under constrained GPU memory,\nthroughput and training data. We conduct several systematic experiments\nadapting a Wav2Letter convolutional neural network originally trained for\nEnglish ASR to the German language. We show that this technique allows faster\ntraining on consumer-grade resources while requiring less training data in\norder to achieve the same accuracy, thereby lowering the cost of training ASR\nmodels in other languages. Model introspection revealed that small adaptations\nto the network's weights were sufficient for good performance, especially for\ninner layers.\n", "versions": [{"version": "v1", "created": "Thu, 1 Jun 2017 13:33:54 GMT"}], "update_date": "2017-06-02", "authors_parsed": [["Kunze", "Julius", ""], ["Kirsch", "Louis", ""], ["Kurenkov", "Ilia", ""], ["Krug", "Andreas", ""], ["Johannsmeier", "Jens", ""], ["Stober", "Sebastian", ""]]}, {"id": "1706.00326", "submitter": "Matthias Bauer", "authors": "Matthias Bauer, Mateo Rojas-Carulla, Jakub Bart{\\l}omiej\n  \\'Swi\\k{a}tkowski, Bernhard Sch\\\"olkopf, Richard E. Turner", "title": "Discriminative k-shot learning using probabilistic models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces a probabilistic framework for k-shot image\nclassification. The goal is to generalise from an initial large-scale\nclassification task to a separate task comprising new classes and small numbers\nof examples. The new approach not only leverages the feature-based\nrepresentation learned by a neural network from the initial task\n(representational transfer), but also information about the classes (concept\ntransfer). The concept information is encapsulated in a probabilistic model for\nthe final layer weights of the neural network which acts as a prior for\nprobabilistic k-shot learning. We show that even a simple probabilistic model\nachieves state-of-the-art on a standard k-shot learning dataset by a large\nmargin. Moreover, it is able to accurately model uncertainty, leading to well\ncalibrated classifiers, and is easily extensible and flexible, unlike many\nrecent approaches to k-shot learning.\n", "versions": [{"version": "v1", "created": "Thu, 1 Jun 2017 14:43:25 GMT"}, {"version": "v2", "created": "Sat, 9 Dec 2017 00:09:06 GMT"}], "update_date": "2017-12-12", "authors_parsed": [["Bauer", "Matthias", ""], ["Rojas-Carulla", "Mateo", ""], ["\u015awi\u0105tkowski", "Jakub Bart\u0142omiej", ""], ["Sch\u00f6lkopf", "Bernhard", ""], ["Turner", "Richard E.", ""]]}, {"id": "1706.00342", "submitter": "Francois Malgouyres", "authors": "Francois Malgouyres (IMT)", "title": "On the stable recovery of deep structured linear networks under sparsity\n  constraints", "comments": null, "journal-ref": "Mathematical and Scientific Machine Learning, Jul 2020, Princeton,\n  United States", "doi": null, "report-no": null, "categories": "math.OC cs.AI cs.LG math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider a deep structured linear network under sparsity constraints. We\nstudy sharp conditions guaranteeing the stability of the optimal parameters\ndefining the network. More precisely, we provide sharp conditions on the\nnetwork architecture and the sample under which the error on the parameters\ndefining the network scales linearly with the reconstruction error (i.e. the\nrisk). Therefore, under these conditions, the weights obtained with a\nsuccessful algorithms are well defined and only depend on the architecture of\nthe network and the sample. The features in the latent spaces are stably\ndefined. The stability property is required in order to interpret the features\ndefined in the latent spaces. It can also lead to a guarantee on the\nstatistical risk. This is what motivates this study. The analysis is based on\nthe recently proposed Tensorial Lifting. The particularity of this paper is to\nconsider a sparsity prior. This leads to a better stability constant. As an\nillustration, we detail the analysis and provide sharp stability guarantees for\nconvolutional linear network under sparsity prior. In this analysis, we\ndistinguish the role of the network architecture and the sample input. This\nhighlights the requirements on the data in connection to parameter stability.\n", "versions": [{"version": "v1", "created": "Wed, 31 May 2017 09:49:34 GMT"}, {"version": "v2", "created": "Tue, 20 Feb 2018 10:04:01 GMT"}, {"version": "v3", "created": "Wed, 20 May 2020 06:16:22 GMT"}], "update_date": "2020-05-21", "authors_parsed": [["Malgouyres", "Francois", "", "IMT"]]}, {"id": "1706.00359", "submitter": "Yishu Miao", "authors": "Yishu Miao, Edward Grefenstette, Phil Blunsom", "title": "Discovering Discrete Latent Topics with Neural Variational Inference", "comments": "ICML 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Topic models have been widely explored as probabilistic generative models of\ndocuments. Traditional inference methods have sought closed-form derivations\nfor updating the models, however as the expressiveness of these models grows,\nso does the difficulty of performing fast and accurate inference over their\nparameters. This paper presents alternative neural approaches to topic\nmodelling by providing parameterisable distributions over topics which permit\ntraining by backpropagation in the framework of neural variational inference.\nIn addition, with the help of a stick-breaking construction, we propose a\nrecurrent network that is able to discover a notionally unbounded number of\ntopics, analogous to Bayesian non-parametric topic models. Experimental results\non the MXM Song Lyrics, 20NewsGroups and Reuters News datasets demonstrate the\neffectiveness and efficiency of these neural topic models.\n", "versions": [{"version": "v1", "created": "Thu, 1 Jun 2017 15:55:42 GMT"}, {"version": "v2", "created": "Mon, 21 May 2018 19:00:21 GMT"}], "update_date": "2018-05-23", "authors_parsed": [["Miao", "Yishu", ""], ["Grefenstette", "Edward", ""], ["Blunsom", "Phil", ""]]}, {"id": "1706.00374", "submitter": "Nikola Mrk\\v{s}i\\'c", "authors": "Nikola Mrk\\v{s}i\\'c, Ivan Vuli\\'c, Diarmuid \\'O S\\'eaghdha, Ira\n  Leviant, Roi Reichart, Milica Ga\\v{s}i\\'c, Anna Korhonen and Steve Young", "title": "Semantic Specialisation of Distributional Word Vector Spaces using\n  Monolingual and Cross-Lingual Constraints", "comments": "Accepted for publication at TACL (to be presented at EMNLP 2017)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present Attract-Repel, an algorithm for improving the semantic quality of\nword vectors by injecting constraints extracted from lexical resources.\nAttract-Repel facilitates the use of constraints from mono- and cross-lingual\nresources, yielding semantically specialised cross-lingual vector spaces. Our\nevaluation shows that the method can make use of existing cross-lingual\nlexicons to construct high-quality vector spaces for a plethora of different\nlanguages, facilitating semantic transfer from high- to lower-resource ones.\nThe effectiveness of our approach is demonstrated with state-of-the-art results\non semantic similarity datasets in six languages. We next show that\nAttract-Repel-specialised vectors boost performance in the downstream task of\ndialogue state tracking (DST) across multiple languages. Finally, we show that\ncross-lingual vector spaces produced by our algorithm facilitate the training\nof multilingual DST models, which brings further performance improvements.\n", "versions": [{"version": "v1", "created": "Thu, 1 Jun 2017 16:29:47 GMT"}], "update_date": "2017-06-02", "authors_parsed": [["Mrk\u0161i\u0107", "Nikola", ""], ["Vuli\u0107", "Ivan", ""], ["S\u00e9aghdha", "Diarmuid \u00d3", ""], ["Leviant", "Ira", ""], ["Reichart", "Roi", ""], ["Ga\u0161i\u0107", "Milica", ""], ["Korhonen", "Anna", ""], ["Young", "Steve", ""]]}, {"id": "1706.00387", "submitter": "Shixiang Gu", "authors": "Shixiang Gu and Timothy Lillicrap and Zoubin Ghahramani and Richard E.\n  Turner and Bernhard Sch\\\"olkopf and Sergey Levine", "title": "Interpolated Policy Gradient: Merging On-Policy and Off-Policy Gradient\n  Estimation for Deep Reinforcement Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Off-policy model-free deep reinforcement learning methods using previously\ncollected data can improve sample efficiency over on-policy policy gradient\ntechniques. On the other hand, on-policy algorithms are often more stable and\neasier to use. This paper examines, both theoretically and empirically,\napproaches to merging on- and off-policy updates for deep reinforcement\nlearning. Theoretical results show that off-policy updates with a value\nfunction estimator can be interpolated with on-policy policy gradient updates\nwhilst still satisfying performance bounds. Our analysis uses control variate\nmethods to produce a family of policy gradient algorithms, with several\nrecently proposed algorithms being special cases of this family. We then\nprovide an empirical comparison of these techniques with the remaining\nalgorithmic details fixed, and show how different mixing of off-policy gradient\nestimates with on-policy samples contribute to improvements in empirical\nperformance. The final algorithm provides a generalization and unification of\nexisting deep policy gradient techniques, has theoretical guarantees on the\nbias introduced by off-policy updates, and improves on the state-of-the-art\nmodel-free deep RL methods on a number of OpenAI Gym continuous control\nbenchmarks.\n", "versions": [{"version": "v1", "created": "Thu, 1 Jun 2017 17:00:52 GMT"}], "update_date": "2017-06-02", "authors_parsed": [["Gu", "Shixiang", ""], ["Lillicrap", "Timothy", ""], ["Ghahramani", "Zoubin", ""], ["Turner", "Richard E.", ""], ["Sch\u00f6lkopf", "Bernhard", ""], ["Levine", "Sergey", ""]]}, {"id": "1706.00400", "submitter": "Narayanaswamy Siddharth", "authors": "N. Siddharth, Brooks Paige, Jan-Willem van de Meent, Alban Desmaison,\n  Noah D. Goodman, Pushmeet Kohli, Frank Wood, Philip H.S. Torr", "title": "Learning Disentangled Representations with Semi-Supervised Deep\n  Generative Models", "comments": "Accepted for publication at NIPS 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Variational autoencoders (VAEs) learn representations of data by jointly\ntraining a probabilistic encoder and decoder network. Typically these models\nencode all features of the data into a single variable. Here we are interested\nin learning disentangled representations that encode distinct aspects of the\ndata into separate variables. We propose to learn such representations using\nmodel architectures that generalise from standard VAEs, employing a general\ngraphical model structure in the encoder and decoder. This allows us to train\npartially-specified models that make relatively strong assumptions about a\nsubset of interpretable variables and rely on the flexibility of neural\nnetworks to learn representations for the remaining variables. We further\ndefine a general objective for semi-supervised learning in this model class,\nwhich can be approximated using an importance sampling procedure. We evaluate\nour framework's ability to learn disentangled representations, both by\nqualitative exploration of its generative capacity, and quantitative evaluation\nof its discriminative ability on a variety of models and datasets.\n", "versions": [{"version": "v1", "created": "Thu, 1 Jun 2017 17:23:07 GMT"}, {"version": "v2", "created": "Mon, 13 Nov 2017 17:55:15 GMT"}], "update_date": "2017-11-15", "authors_parsed": [["Siddharth", "N.", ""], ["Paige", "Brooks", ""], ["van de Meent", "Jan-Willem", ""], ["Desmaison", "Alban", ""], ["Goodman", "Noah D.", ""], ["Kohli", "Pushmeet", ""], ["Wood", "Frank", ""], ["Torr", "Philip H. S.", ""]]}, {"id": "1706.00439", "submitter": "Jean Kossaifi", "authors": "Jean Kossaifi, Aran Khanna, Zachary C. Lipton, Tommaso Furlanello and\n  Anima Anandkumar", "title": "Tensor Contraction Layers for Parsimonious Deep Nets", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Tensors offer a natural representation for many kinds of data frequently\nencountered in machine learning. Images, for example, are naturally represented\nas third order tensors, where the modes correspond to height, width, and\nchannels. Tensor methods are noted for their ability to discover\nmulti-dimensional dependencies, and tensor decompositions in particular, have\nbeen used to produce compact low-rank approximations of data. In this paper, we\nexplore the use of tensor contractions as neural network layers and investigate\nseveral ways to apply them to activation tensors. Specifically, we propose the\nTensor Contraction Layer (TCL), the first attempt to incorporate tensor\ncontractions as end-to-end trainable neural network layers. Applied to existing\nnetworks, TCLs reduce the dimensionality of the activation tensors and thus the\nnumber of model parameters. We evaluate the TCL on the task of image\nrecognition, augmenting two popular networks (AlexNet, VGG). The resulting\nmodels are trainable end-to-end. Applying the TCL to the task of image\nrecognition, using the CIFAR100 and ImageNet datasets, we evaluate the effect\nof parameter reduction via tensor contraction on performance. We demonstrate\nsignificant model compression without significant impact on the accuracy and,\nin some cases, improved performance.\n", "versions": [{"version": "v1", "created": "Thu, 1 Jun 2017 18:00:24 GMT"}], "update_date": "2017-06-05", "authors_parsed": [["Kossaifi", "Jean", ""], ["Khanna", "Aran", ""], ["Lipton", "Zachary C.", ""], ["Furlanello", "Tommaso", ""], ["Anandkumar", "Anima", ""]]}, {"id": "1706.00473", "submitter": "Vadim Sokolov", "authors": "Nicholas Polson and Vadim Sokolov", "title": "Deep Learning: A Bayesian Perspective", "comments": null, "journal-ref": null, "doi": "10.1214/17-BA1082", "report-no": null, "categories": "stat.ML cs.LG stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning is a form of machine learning for nonlinear high dimensional\npattern matching and prediction. By taking a Bayesian probabilistic\nperspective, we provide a number of insights into more efficient algorithms for\noptimisation and hyper-parameter tuning. Traditional high-dimensional data\nreduction techniques, such as principal component analysis (PCA), partial least\nsquares (PLS), reduced rank regression (RRR), projection pursuit regression\n(PPR) are all shown to be shallow learners. Their deep learning counterparts\nexploit multiple deep layers of data reduction which provide predictive\nperformance gains. Stochastic gradient descent (SGD) training optimisation and\nDropout (DO) regularization provide estimation and variable selection. Bayesian\nregularization is central to finding weights and connections in networks to\noptimize the predictive bias-variance trade-off. To illustrate our methodology,\nwe provide an analysis of international bookings on Airbnb. Finally, we\nconclude with directions for future research.\n", "versions": [{"version": "v1", "created": "Thu, 1 Jun 2017 19:50:37 GMT"}, {"version": "v2", "created": "Mon, 28 Aug 2017 00:57:42 GMT"}, {"version": "v3", "created": "Tue, 5 Sep 2017 01:33:09 GMT"}, {"version": "v4", "created": "Tue, 14 Nov 2017 03:36:51 GMT"}], "update_date": "2018-01-23", "authors_parsed": [["Polson", "Nicholas", ""], ["Sokolov", "Vadim", ""]]}, {"id": "1706.00476", "submitter": "Po-Wei Wang", "authors": "Po-Wei Wang, Wei-Cheng Chang, J. Zico Kolter", "title": "The Mixing method: low-rank coordinate descent for semidefinite\n  programming with diagonal constraints", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a low-rank coordinate descent approach to\nstructured semidefinite programming with diagonal constraints. The approach,\nwhich we call the Mixing method, is extremely simple to implement, has no free\nparameters, and typically attains an order of magnitude or better improvement\nin optimization performance over the current state of the art. We show that the\nmethod is strictly decreasing, converges to a critical point, and further that\nfor sufficient rank all non-optimal critical points are unstable. Moreover, we\nprove that with a step size, the Mixing method converges to the global optimum\nof the semidefinite program almost surely in a locally linear rate under random\ninitialization. This is the first low-rank semidefinite programming method that\nhas been shown to achieve a global optimum on the spherical manifold without\nassumption. We apply our algorithm to two related domains: solving the maximum\ncut semidefinite relaxation, and solving a maximum satisfiability relaxation\n(we also briefly consider additional applications such as learning word\nembeddings). In all settings, we demonstrate substantial improvement over the\nexisting state of the art along various dimensions, and in total, this work\nexpands the scope and scale of problems that can be solved using semidefinite\nprogramming methods.\n", "versions": [{"version": "v1", "created": "Thu, 1 Jun 2017 19:58:38 GMT"}, {"version": "v2", "created": "Sun, 1 Jul 2018 05:19:39 GMT"}, {"version": "v3", "created": "Wed, 4 Jul 2018 17:04:36 GMT"}], "update_date": "2018-07-05", "authors_parsed": [["Wang", "Po-Wei", ""], ["Chang", "Wei-Cheng", ""], ["Kolter", "J. Zico", ""]]}, {"id": "1706.00493", "submitter": "Ling Zhang", "authors": "Ling Zhang, Le Lu, Ronald M. Summers, Electron Kebebew, Jianhua Yao", "title": "Personalized Pancreatic Tumor Growth Prediction via Group Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Tumor growth prediction, a highly challenging task, has long been viewed as a\nmathematical modeling problem, where the tumor growth pattern is personalized\nbased on imaging and clinical data of a target patient. Though mathematical\nmodels yield promising results, their prediction accuracy may be limited by the\nabsence of population trend data and personalized clinical characteristics. In\nthis paper, we propose a statistical group learning approach to predict the\ntumor growth pattern that incorporates both the population trend and\npersonalized data, in order to discover high-level features from multimodal\nimaging data. A deep convolutional neural network approach is developed to\nmodel the voxel-wise spatio-temporal tumor progression. The deep features are\ncombined with the time intervals and the clinical factors to feed a process of\nfeature selection. Our predictive model is pretrained on a group data set and\npersonalized on the target patient data to estimate the future spatio-temporal\nprogression of the patient's tumor. Multimodal imaging data at multiple time\npoints are used in the learning, personalization and inference stages. Our\nmethod achieves a Dice coefficient of 86.8% +- 3.6% and RVD of 7.9% +- 5.4% on\na pancreatic tumor data set, outperforming the DSC of 84.4% +- 4.0% and RVD\n13.9% +- 9.8% obtained by a previous state-of-the-art model-based method.\n", "versions": [{"version": "v1", "created": "Thu, 1 Jun 2017 20:57:53 GMT"}], "update_date": "2017-06-05", "authors_parsed": [["Zhang", "Ling", ""], ["Lu", "Le", ""], ["Summers", "Ronald M.", ""], ["Kebebew", "Electron", ""], ["Yao", "Jianhua", ""]]}, {"id": "1706.00504", "submitter": "Patrick Judd", "authors": "Alberto Delmas, Patrick Judd, Sayeh Sharify, Andreas Moshovos", "title": "Dynamic Stripes: Exploiting the Dynamic Precision Requirements of\n  Activation Values in Neural Networks", "comments": "3 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Stripes is a Deep Neural Network (DNN) accelerator that uses bit-serial\ncomputation to offer performance that is proportional to the fixed-point\nprecision of the activation values. The fixed-point precisions are determined a\npriori using profiling and are selected at a per layer granularity. This paper\npresents Dynamic Stripes, an extension to Stripes that detects precision\nvariance at runtime and at a finer granularity. This extra level of precision\nreduction increases performance by 41% over Stripes.\n", "versions": [{"version": "v1", "created": "Thu, 1 Jun 2017 21:57:32 GMT"}], "update_date": "2017-06-05", "authors_parsed": [["Delmas", "Alberto", ""], ["Judd", "Patrick", ""], ["Sharify", "Sayeh", ""], ["Moshovos", "Andreas", ""]]}, {"id": "1706.00505", "submitter": "Bilal Farooq", "authors": "Melvin Wong and Bilal Farooq and Guillaume-Alexandre Bilodeau", "title": "Discriminative conditional restricted Boltzmann machine for discrete\n  choice and latent variable modelling", "comments": null, "journal-ref": null, "doi": "10.1016/j.jocm.2017.11.003", "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Conventional methods of estimating latent behaviour generally use attitudinal\nquestions which are subjective and these survey questions may not always be\navailable. We hypothesize that an alternative approach can be used for latent\nvariable estimation through an undirected graphical models. For instance,\nnon-parametric artificial neural networks. In this study, we explore the use of\ngenerative non-parametric modelling methods to estimate latent variables from\nprior choice distribution without the conventional use of measurement\nindicators. A restricted Boltzmann machine is used to represent latent\nbehaviour factors by analyzing the relationship information between the\nobserved choices and explanatory variables. The algorithm is adapted for latent\nbehaviour analysis in discrete choice scenario and we use a graphical approach\nto evaluate and understand the semantic meaning from estimated parameter vector\nvalues. We illustrate our methodology on a financial instrument choice dataset\nand perform statistical analysis on parameter sensitivity and stability. Our\nfindings show that through non-parametric statistical tests, we can extract\nuseful latent information on the behaviour of latent constructs through machine\nlearning methods and present strong and significant influence on the choice\nprocess. Furthermore, our modelling framework shows robustness in input\nvariability through sampling and validation.\n", "versions": [{"version": "v1", "created": "Thu, 1 Jun 2017 21:58:04 GMT"}], "update_date": "2018-09-18", "authors_parsed": [["Wong", "Melvin", ""], ["Farooq", "Bilal", ""], ["Bilodeau", "Guillaume-Alexandre", ""]]}, {"id": "1706.00517", "submitter": "Ardavan Pedram", "authors": "Yuanfang Li and Ardavan Pedram", "title": "CATERPILLAR: Coarse Grain Reconfigurable Architecture for Accelerating\n  the Training of Deep Neural Networks", "comments": "ASAP 2017: The 28th Annual IEEE International Conference on\n  Application-specific Systems, Architectures and Processors", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Accelerating the inference of a trained DNN is a well studied subject. In\nthis paper we switch the focus to the training of DNNs. The training phase is\ncompute intensive, demands complicated data communication, and contains\nmultiple levels of data dependencies and parallelism. This paper presents an\nalgorithm/architecture space exploration of efficient accelerators to achieve\nbetter network convergence rates and higher energy efficiency for training\nDNNs. We further demonstrate that an architecture with hierarchical support for\ncollective communication semantics provides flexibility in training various\nnetworks performing both stochastic and batched gradient descent based\ntechniques. Our results suggest that smaller networks favor non-batched\ntechniques while performance for larger networks is higher using batched\noperations. At 45nm technology, CATERPILLAR achieves performance efficiencies\nof 177 GFLOPS/W at over 80% utilization for SGD training on small networks and\n211 GFLOPS/W at over 90% utilization for pipelined SGD/CP training on larger\nnetworks using a total area of 103.2 mm$^2$ and 178.9 mm$^2$ respectively.\n", "versions": [{"version": "v1", "created": "Thu, 1 Jun 2017 22:58:37 GMT"}, {"version": "v2", "created": "Thu, 8 Jun 2017 15:30:54 GMT"}], "update_date": "2017-06-09", "authors_parsed": [["Li", "Yuanfang", ""], ["Pedram", "Ardavan", ""]]}, {"id": "1706.00531", "submitter": "Alireza Makhzani", "authors": "Alireza Makhzani, Brendan Frey", "title": "PixelGAN Autoencoders", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we describe the \"PixelGAN autoencoder\", a generative\nautoencoder in which the generative path is a convolutional autoregressive\nneural network on pixels (PixelCNN) that is conditioned on a latent code, and\nthe recognition path uses a generative adversarial network (GAN) to impose a\nprior distribution on the latent code. We show that different priors result in\ndifferent decompositions of information between the latent code and the\nautoregressive decoder. For example, by imposing a Gaussian distribution as the\nprior, we can achieve a global vs. local decomposition, or by imposing a\ncategorical distribution as the prior, we can disentangle the style and content\ninformation of images in an unsupervised fashion. We further show how the\nPixelGAN autoencoder with a categorical prior can be directly used in\nsemi-supervised settings and achieve competitive semi-supervised classification\nresults on the MNIST, SVHN and NORB datasets.\n", "versions": [{"version": "v1", "created": "Fri, 2 Jun 2017 00:53:14 GMT"}], "update_date": "2017-06-05", "authors_parsed": [["Makhzani", "Alireza", ""], ["Frey", "Brendan", ""]]}, {"id": "1706.00544", "submitter": "Pin-Yu Chen", "authors": "Pin-Yu Chen and Sijia Liu", "title": "Bias-Variance Tradeoff of Graph Laplacian Regularizer", "comments": "accepted by IEEE Signal Processing Letters", "journal-ref": null, "doi": "10.1109/LSP.2017.2712141", "report-no": null, "categories": "stat.ML cs.LG cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a bias-variance tradeoff of graph Laplacian regularizer,\nwhich is widely used in graph signal processing and semi-supervised learning\ntasks. The scaling law of the optimal regularization parameter is specified in\nterms of the spectral graph properties and a novel signal-to-noise ratio\nparameter, which suggests selecting a mediocre regularization parameter is\noften suboptimal. The analysis is applied to three applications, including\nrandom, band-limited, and multiple-sampled graph signals. Experiments on\nsynthetic and real-world graphs demonstrate near-optimal performance of the\nestablished analysis.\n", "versions": [{"version": "v1", "created": "Fri, 2 Jun 2017 03:28:50 GMT"}], "update_date": "2017-08-02", "authors_parsed": [["Chen", "Pin-Yu", ""], ["Liu", "Sijia", ""]]}, {"id": "1706.00550", "submitter": "Zhiting Hu", "authors": "Zhiting Hu, Zichao Yang, Ruslan Salakhutdinov, Eric P. Xing", "title": "On Unifying Deep Generative Models", "comments": "Polished and extended content over the ICLR conference version:\n  https://openreview.net/pdf?id=rylSzl-R-", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep generative models have achieved impressive success in recent years.\nGenerative Adversarial Networks (GANs) and Variational Autoencoders (VAEs), as\nemerging families for generative model learning, have largely been considered\nas two distinct paradigms and received extensive independent studies\nrespectively. This paper aims to establish formal connections between GANs and\nVAEs through a new formulation of them. We interpret sample generation in GANs\nas performing posterior inference, and show that GANs and VAEs involve\nminimizing KL divergences of respective posterior and inference distributions\nwith opposite directions, extending the two learning phases of classic\nwake-sleep algorithm, respectively. The unified view provides a powerful tool\nto analyze a diverse set of existing model variants, and enables to transfer\ntechniques across research lines in a principled way. For example, we apply the\nimportance weighting method in VAE literatures for improved GAN learning, and\nenhance VAEs with an adversarial mechanism that leverages generated samples.\nExperiments show generality and effectiveness of the transferred techniques.\n", "versions": [{"version": "v1", "created": "Fri, 2 Jun 2017 04:15:44 GMT"}, {"version": "v2", "created": "Thu, 8 Jun 2017 02:43:28 GMT"}, {"version": "v3", "created": "Sun, 16 Jul 2017 20:13:20 GMT"}, {"version": "v4", "created": "Sun, 1 Apr 2018 02:40:59 GMT"}, {"version": "v5", "created": "Wed, 11 Jul 2018 15:01:24 GMT"}], "update_date": "2018-07-12", "authors_parsed": [["Hu", "Zhiting", ""], ["Yang", "Zichao", ""], ["Salakhutdinov", "Ruslan", ""], ["Xing", "Eric P.", ""]]}, {"id": "1706.00587", "submitter": "Ralf Stauder", "authors": "Ralf Stauder, Erg\\\"un Kayis, Nassir Navab", "title": "Learning-based Surgical Workflow Detection from Intra-Operative Signals", "comments": "7 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A modern operating room (OR) provides a plethora of advanced medical devices.\nIn order to better facilitate the information offered by them, they need to\nautomatically react to the intra-operative context. To this end, the progress\nof the surgical workflow must be detected and interpreted, so that the current\nstatus can be given in machine-readable form. In this work, Random Forests (RF)\nand Hidden Markov Models (HMM) are compared and combined to detect the surgical\nworkflow phase of a laparoscopic cholecystectomy. Various combinations of data\nwere tested, from using only raw sensor data to filtered and augmented\ndatasets. Achieved accuracies ranged from 64% to 72% for the RF approach, and\nfrom 80% to 82% for the combination of RF and HMM.\n", "versions": [{"version": "v1", "created": "Fri, 2 Jun 2017 08:33:24 GMT"}], "update_date": "2017-06-05", "authors_parsed": [["Stauder", "Ralf", ""], ["Kayis", "Erg\u00fcn", ""], ["Navab", "Nassir", ""]]}, {"id": "1706.00633", "submitter": "Tianyu Pang", "authors": "Tianyu Pang, Chao Du, Yinpeng Dong, Jun Zhu", "title": "Towards Robust Detection of Adversarial Examples", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Although the recent progress is substantial, deep learning methods can be\nvulnerable to the maliciously generated adversarial examples. In this paper, we\npresent a novel training procedure and a thresholding test strategy, towards\nrobust detection of adversarial examples. In training, we propose to minimize\nthe reverse cross-entropy (RCE), which encourages a deep network to learn\nlatent representations that better distinguish adversarial examples from normal\nones. In testing, we propose to use a thresholding strategy as the detector to\nfilter out adversarial examples for reliable predictions. Our method is simple\nto implement using standard algorithms, with little extra training cost\ncompared to the common cross-entropy minimization. We apply our method to\ndefend various attacking methods on the widely used MNIST and CIFAR-10\ndatasets, and achieve significant improvements on robust predictions under all\nthe threat models in the adversarial setting.\n", "versions": [{"version": "v1", "created": "Fri, 2 Jun 2017 11:23:12 GMT"}, {"version": "v2", "created": "Mon, 18 Dec 2017 10:32:14 GMT"}, {"version": "v3", "created": "Mon, 26 Feb 2018 14:08:07 GMT"}, {"version": "v4", "created": "Wed, 7 Nov 2018 11:03:23 GMT"}], "update_date": "2018-11-08", "authors_parsed": [["Pang", "Tianyu", ""], ["Du", "Chao", ""], ["Dong", "Yinpeng", ""], ["Zhu", "Jun", ""]]}, {"id": "1706.00648", "submitter": "Michael Bukatin", "authors": "Michael Bukatin, Jon Anthony", "title": "Dataflow Matrix Machines as a Model of Computations with Linear Streams", "comments": "6 pages, accepted for presentation at LearnAut 2017: Learning and\n  Automata workshop at LICS (Logic in Computer Science) 2017 conference.\n  Preprint original version: April 9, 2017; minor correction: May 1, 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.LG cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We overview dataflow matrix machines as a Turing complete generalization of\nrecurrent neural networks and as a programming platform. We describe vector\nspace of finite prefix trees with numerical leaves which allows us to combine\nexpressive power of dataflow matrix machines with simplicity of traditional\nrecurrent neural networks.\n", "versions": [{"version": "v1", "created": "Wed, 3 May 2017 13:46:05 GMT"}], "update_date": "2017-06-05", "authors_parsed": [["Bukatin", "Michael", ""], ["Anthony", "Jon", ""]]}, {"id": "1706.00687", "submitter": "Shaked Shammah", "authors": "Shai Shalev-Shwartz, Ohad Shamir, Shaked Shammah", "title": "Weight Sharing is Crucial to Succesful Optimization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Exploiting the great expressive power of Deep Neural Network architectures,\nrelies on the ability to train them. While current theoretical work provides,\nmostly, results showing the hardness of this task, empirical evidence usually\ndiffers from this line, with success stories in abundance. A strong position\namong empirically successful architectures is captured by networks where\nextensive weight sharing is used, either by Convolutional or Recurrent layers.\nAdditionally, characterizing specific aspects of different tasks, making them\n\"harder\" or \"easier\", is an interesting direction explored both theoretically\nand empirically. We consider a family of ConvNet architectures, and prove that\nweight sharing can be crucial, from an optimization point of view. We explore\ndifferent notions of the frequency, of the target function, proving necessity\nof the target function having some low frequency components. This necessity is\nnot sufficient - only with weight sharing can it be exploited, thus\ntheoretically separating architectures using it, from others which do not. Our\ntheoretical results are aligned with empirical experiments in an even more\ngeneral setting, suggesting viability of examination of the role played by\ninterleaving those aspects in broader families of tasks.\n", "versions": [{"version": "v1", "created": "Fri, 2 Jun 2017 13:56:59 GMT"}], "update_date": "2017-06-05", "authors_parsed": [["Shalev-Shwartz", "Shai", ""], ["Shamir", "Ohad", ""], ["Shammah", "Shaked", ""]]}, {"id": "1706.00712", "submitter": "Jianming Liang PhD", "authors": "Nima Tajbakhsh, Jae Y. Shin, Suryakanth R. Gurudu, R. Todd Hurst,\n  Christopher B. Kendall, Michael B. Gotway, and Jianming Liang", "title": "Convolutional Neural Networks for Medical Image Analysis: Full Training\n  or Fine Tuning?", "comments": null, "journal-ref": "IEEE Transactions on Medical Imaging. 35(5):1299-1312 (2016)", "doi": "10.1109/TMI.2016.2535302", "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Training a deep convolutional neural network (CNN) from scratch is difficult\nbecause it requires a large amount of labeled training data and a great deal of\nexpertise to ensure proper convergence. A promising alternative is to fine-tune\na CNN that has been pre-trained using, for instance, a large set of labeled\nnatural images. However, the substantial differences between natural and\nmedical images may advise against such knowledge transfer. In this paper, we\nseek to answer the following central question in the context of medical image\nanalysis: \\emph{Can the use of pre-trained deep CNNs with sufficient\nfine-tuning eliminate the need for training a deep CNN from scratch?} To\naddress this question, we considered 4 distinct medical imaging applications in\n3 specialties (radiology, cardiology, and gastroenterology) involving\nclassification, detection, and segmentation from 3 different imaging\nmodalities, and investigated how the performance of deep CNNs trained from\nscratch compared with the pre-trained CNNs fine-tuned in a layer-wise manner.\nOur experiments consistently demonstrated that (1) the use of a pre-trained CNN\nwith adequate fine-tuning outperformed or, in the worst case, performed as well\nas a CNN trained from scratch; (2) fine-tuned CNNs were more robust to the size\nof training sets than CNNs trained from scratch; (3) neither shallow tuning nor\ndeep tuning was the optimal choice for a particular application; and (4) our\nlayer-wise fine-tuning scheme could offer a practical way to reach the best\nperformance for the application at hand based on the amount of available data.\n", "versions": [{"version": "v1", "created": "Fri, 2 Jun 2017 15:04:43 GMT"}], "update_date": "2017-06-05", "authors_parsed": [["Tajbakhsh", "Nima", ""], ["Shin", "Jae Y.", ""], ["Gurudu", "Suryakanth R.", ""], ["Hurst", "R. Todd", ""], ["Kendall", "Christopher B.", ""], ["Gotway", "Michael B.", ""], ["Liang", "Jianming", ""]]}, {"id": "1706.00719", "submitter": "Jianming Liang PhD", "authors": "Jae Y. Shin, Nima Tajbakhsh, R. Todd Hurst, Christopher B. Kendall,\n  and Jianming Liang", "title": "Automating Carotid Intima-Media Thickness Video Interpretation with\n  Convolutional Neural Networks", "comments": "J. Y. Shin, N. Tajbakhsh, R. T. Hurst, C. B. Kendall, and J. Liang.\n  Automating carotid intima-media thickness video interpretation with\n  convolutional neural networks. CVPR 2016, pp 2526-2535; N. Tajbakhsh, J. Y.\n  Shin, R. T. Hurst, C. B. Kendall, and J. Liang. Automatic interpretation of\n  CIMT videos using convolutional neural networks. Deep Learning for Medical\n  Image Analysis, Academic Press, 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cardiovascular disease (CVD) is the leading cause of mortality yet largely\npreventable, but the key to prevention is to identify at-risk individuals\nbefore adverse events. For predicting individual CVD risk, carotid intima-media\nthickness (CIMT), a noninvasive ultrasound method, has proven to be valuable,\noffering several advantages over CT coronary artery calcium score. However,\neach CIMT examination includes several ultrasound videos, and interpreting each\nof these CIMT videos involves three operations: (1) select three end-diastolic\nultrasound frames (EUF) in the video, (2) localize a region of interest (ROI)\nin each selected frame, and (3) trace the lumen-intima interface and the\nmedia-adventitia interface in each ROI to measure CIMT. These operations are\ntedious, laborious, and time consuming, a serious limitation that hinders the\nwidespread utilization of CIMT in clinical practice. To overcome this\nlimitation, this paper presents a new system to automate CIMT video\ninterpretation. Our extensive experiments demonstrate that the suggested system\nsignificantly outperforms the state-of-the-art methods. The superior\nperformance is attributable to our unified framework based on convolutional\nneural networks (CNNs) coupled with our informative image representation and\neffective post-processing of the CNN outputs, which are uniquely designed for\neach of the above three operations.\n", "versions": [{"version": "v1", "created": "Fri, 2 Jun 2017 15:21:09 GMT"}], "update_date": "2017-06-05", "authors_parsed": [["Shin", "Jae Y.", ""], ["Tajbakhsh", "Nima", ""], ["Hurst", "R. Todd", ""], ["Kendall", "Christopher B.", ""], ["Liang", "Jianming", ""]]}, {"id": "1706.00729", "submitter": "Daniel Hsu", "authors": "Arushi Gupta, Daniel Hsu", "title": "Parameter identification in Markov chain choice models", "comments": "10 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.LG stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work studies the parameter identification problem for the Markov chain\nchoice model of Blanchet, Gallego, and Goyal used in assortment planning. In\nthis model, the product selected by a customer is determined by a Markov chain\nover the products, where the products in the offered assortment are absorbing\nstates. The underlying parameters of the model were previously shown to be\nidentifiable from the choice probabilities for the all-products assortment,\ntogether with choice probabilities for assortments of all-but-one products.\nObtaining and estimating choice probabilities for such large assortments is not\ndesirable in many settings. The main result of this work is that the parameters\nmay be identified from assortments of sizes two and three, regardless of the\ntotal number of products. The result is obtained via a simple and efficient\nparameter recovery algorithm.\n", "versions": [{"version": "v1", "created": "Fri, 2 Jun 2017 15:47:17 GMT"}, {"version": "v2", "created": "Fri, 21 Jul 2017 18:21:52 GMT"}, {"version": "v3", "created": "Tue, 25 Jul 2017 21:03:33 GMT"}], "update_date": "2017-07-27", "authors_parsed": [["Gupta", "Arushi", ""], ["Hsu", "Daniel", ""]]}, {"id": "1706.00754", "submitter": "Kevin Bello", "authors": "Kevin Bello and Jean Honorio", "title": "Computationally and statistically efficient learning of causal Bayes\n  nets using path queries", "comments": null, "journal-ref": "Neural Information Processing Systems (NeurIPS) 2018", "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Causal discovery from empirical data is a fundamental problem in many\nscientific domains. Observational data allows for identifiability only up to\nMarkov equivalence class. In this paper we first propose a polynomial time\nalgorithm for learning the exact correctly-oriented structure of the transitive\nreduction of any causal Bayesian network with high probability, by using\ninterventional path queries. Each path query takes as input an origin node and\na target node, and answers whether there is a directed path from the origin to\nthe target. This is done by intervening on the origin node and observing\nsamples from the target node. We theoretically show the logarithmic sample\ncomplexity for the size of interventional data per path query, for continuous\nand discrete networks. We then show how to learn the transitive edges using\nalso logarithmic sample complexity (albeit in time exponential in the maximum\nnumber of parents for discrete networks), which allows us to learn the full\nnetwork. We further extend our work by reducing the number of interventional\npath queries for learning rooted trees. We also provide an analysis of\nimperfect interventions.\n", "versions": [{"version": "v1", "created": "Fri, 2 Jun 2017 17:00:01 GMT"}, {"version": "v2", "created": "Fri, 20 Oct 2017 20:56:02 GMT"}, {"version": "v3", "created": "Thu, 22 Feb 2018 19:49:03 GMT"}, {"version": "v4", "created": "Fri, 16 Aug 2019 04:09:36 GMT"}], "update_date": "2019-08-19", "authors_parsed": [["Bello", "Kevin", ""], ["Honorio", "Jean", ""]]}, {"id": "1706.00764", "submitter": "Yang Yuan", "authors": "Elad Hazan, Adam Klivans, Yang Yuan", "title": "Hyperparameter Optimization: A Spectral Approach", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We give a simple, fast algorithm for hyperparameter optimization inspired by\ntechniques from the analysis of Boolean functions. We focus on the\nhigh-dimensional regime where the canonical example is training a neural\nnetwork with a large number of hyperparameters. The algorithm --- an iterative\napplication of compressed sensing techniques for orthogonal polynomials ---\nrequires only uniform sampling of the hyperparameters and is thus easily\nparallelizable.\n  Experiments for training deep neural networks on Cifar-10 show that compared\nto state-of-the-art tools (e.g., Hyperband and Spearmint), our algorithm finds\nsignificantly improved solutions, in some cases better than what is attainable\nby hand-tuning. In terms of overall running time (i.e., time required to sample\nvarious settings of hyperparameters plus additional computation time), we are\nat least an order of magnitude faster than Hyperband and Bayesian Optimization.\nWe also outperform Random Search 8x.\n  Additionally, our method comes with provable guarantees and yields the first\nimprovements on the sample complexity of learning decision trees in over two\ndecades. In particular, we obtain the first quasi-polynomial time algorithm for\nlearning noisy decision trees with polynomial sample complexity.\n", "versions": [{"version": "v1", "created": "Fri, 2 Jun 2017 17:25:58 GMT"}, {"version": "v2", "created": "Wed, 7 Jun 2017 16:51:58 GMT"}, {"version": "v3", "created": "Fri, 27 Oct 2017 02:54:31 GMT"}, {"version": "v4", "created": "Sat, 20 Jan 2018 03:49:23 GMT"}], "update_date": "2018-01-23", "authors_parsed": [["Hazan", "Elad", ""], ["Klivans", "Adam", ""], ["Yuan", "Yang", ""]]}, {"id": "1706.00820", "submitter": "Adam Smith", "authors": "Adam Smith", "title": "Information, Privacy and Stability in Adaptive Data Analysis", "comments": "15 pages, first drafted February 2017. A version of this survey\n  appears in the Information Theory Society Newsletter", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Traditional statistical theory assumes that the analysis to be performed on a\ngiven data set is selected independently of the data themselves. This\nassumption breaks downs when data are re-used across analyses and the analysis\nto be performed at a given stage depends on the results of earlier stages. Such\ndependency can arise when the same data are used by several scientific studies,\nor when a single analysis consists of multiple stages.\n  How can we draw statistically valid conclusions when data are re-used? This\nis the focus of a recent and active line of work. At a high level, these\nresults show that limiting the information revealed by earlier stages of\nanalysis controls the bias introduced in later stages by adaptivity.\n  Here we review some known results in this area and highlight the role of\ninformation-theoretic concepts, notably several one-shot notions of mutual\ninformation.\n", "versions": [{"version": "v1", "created": "Fri, 2 Jun 2017 19:03:47 GMT"}], "update_date": "2017-06-06", "authors_parsed": [["Smith", "Adam", ""]]}, {"id": "1706.00834", "submitter": "Holakou Rahmanian", "authors": "Holakou Rahmanian, Manfred K. Warmuth", "title": "Online Dynamic Programming", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of repeatedly solving a variant of the same dynamic\nprogramming problem in successive trials. An instance of the type of problems\nwe consider is to find a good binary search tree in a changing environment.At\nthe beginning of each trial, the learner probabilistically chooses a tree with\nthe $n$ keys at the internal nodes and the $n+1$ gaps between keys at the\nleaves. The learner is then told the frequencies of the keys and gaps and is\ncharged by the average search cost for the chosen tree. The problem is online\nbecause the frequencies can change between trials. The goal is to develop\nalgorithms with the property that their total average search cost (loss) in all\ntrials is close to the total loss of the best tree chosen in hindsight for all\ntrials. The challenge, of course, is that the algorithm has to deal with\nexponential number of trees. We develop a general methodology for tackling such\nproblems for a wide class of dynamic programming algorithms. Our framework\nallows us to extend online learning algorithms like Hedge and Component Hedge\nto a significantly wider class of combinatorial objects than was possible\nbefore.\n", "versions": [{"version": "v1", "created": "Fri, 2 Jun 2017 20:02:19 GMT"}, {"version": "v2", "created": "Wed, 7 Jun 2017 22:30:44 GMT"}, {"version": "v3", "created": "Mon, 11 Dec 2017 20:02:19 GMT"}], "update_date": "2017-12-13", "authors_parsed": [["Rahmanian", "Holakou", ""], ["Warmuth", "Manfred K.", ""]]}, {"id": "1706.00856", "submitter": "Murat Seckin Ayhan", "authors": "Murat Seckin Ayhan and Vijay Raghavan and Alzheimer's disease\n  Neuroimaging Initiative", "title": "Multiple Kernel Learning and Automatic Subspace Relevance Determination\n  for High-dimensional Neuroimaging Data", "comments": "The material presented here is to promote the dissemination of\n  scholarly and technical work in a timely fashion. Data in this article are\n  from ADNI (adni.loni.usc.edu). As such, ADNI provided data but did not\n  participate in writing of this report", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG q-bio.NC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Alzheimer's disease is a major cause of dementia. Its diagnosis requires\naccurate biomarkers that are sensitive to disease stages. In this respect, we\nregard probabilistic classification as a method of designing a probabilistic\nbiomarker for disease staging. Probabilistic biomarkers naturally support the\ninterpretation of decisions and evaluation of uncertainty associated with them.\nIn this paper, we obtain probabilistic biomarkers via Gaussian Processes.\nGaussian Processes enable probabilistic kernel machines that offer flexible\nmeans to accomplish Multiple Kernel Learning. Exploiting this flexibility, we\npropose a new variation of Automatic Relevance Determination and tackle the\nchallenges of high dimensionality through multiple kernels. Our research\nresults demonstrate that the Gaussian Process models are competitive with or\nbetter than the well-known Support Vector Machine in terms of classification\nperformance even in the cases of single kernel learning. Extending the basic\nscheme towards the Multiple Kernel Learning, we improve the efficacy of the\nGaussian Process models and their interpretability in terms of the known\nanatomical correlates of the disease. For instance, the disease pathology\nstarts in and around the hippocampus and entorhinal cortex. Through the use of\nGaussian Processes and Multiple Kernel Learning, we have automatically and\nefficiently determined those portions of neuroimaging data. In addition to\ntheir interpretability, our Gaussian Process models are competitive with recent\ndeep learning solutions under similar settings.\n", "versions": [{"version": "v1", "created": "Fri, 2 Jun 2017 21:16:50 GMT"}], "update_date": "2017-06-06", "authors_parsed": [["Ayhan", "Murat Seckin", ""], ["Raghavan", "Vijay", ""], ["Initiative", "Alzheimer's disease Neuroimaging", ""]]}, {"id": "1706.00878", "submitter": "Qingqing Cao", "authors": "Qingqing Cao, Niranjan Balasubramanian, Aruna Balasubramanian", "title": "MobiRNN: Efficient Recurrent Neural Network Execution on Mobile GPU", "comments": "Published at 1st International Workshop on Embedded and Mobile Deep\n  Learning colocated with MobiSys 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.LG cs.NE", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  In this paper, we explore optimizations to run Recurrent Neural Network (RNN)\nmodels locally on mobile devices. RNN models are widely used for Natural\nLanguage Processing, Machine Translation, and other tasks. However, existing\nmobile applications that use RNN models do so on the cloud. To address privacy\nand efficiency concerns, we show how RNN models can be run locally on mobile\ndevices. Existing work on porting deep learning models to mobile devices focus\non Convolution Neural Networks (CNNs) and cannot be applied directly to RNN\nmodels. In response, we present MobiRNN, a mobile-specific optimization\nframework that implements GPU offloading specifically for mobile GPUs.\nEvaluations using an RNN model for activity recognition shows that MobiRNN does\nsignificantly decrease the latency of running RNN models on phones.\n", "versions": [{"version": "v1", "created": "Sat, 3 Jun 2017 00:48:12 GMT"}], "update_date": "2017-06-06", "authors_parsed": [["Cao", "Qingqing", ""], ["Balasubramanian", "Niranjan", ""], ["Balasubramanian", "Aruna", ""]]}, {"id": "1706.00884", "submitter": "Xintao Wu", "authors": "Shuhan Yuan, Xintao Wu, Yang Xiang", "title": "Task-specific Word Identification from Short Texts Using a Convolutional\n  Neural Network", "comments": "accepted by Intelligent Data Analysis, an International Journal", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Task-specific word identification aims to choose the task-related words that\nbest describe a short text. Existing approaches require well-defined seed words\nor lexical dictionaries (e.g., WordNet), which are often unavailable for many\napplications such as social discrimination detection and fake review detection.\nHowever, we often have a set of labeled short texts where each short text has a\ntask-related class label, e.g., discriminatory or non-discriminatory, specified\nby users or learned by classification algorithms. In this paper, we focus on\nidentifying task-specific words and phrases from short texts by exploiting\ntheir class labels rather than using seed words or lexical dictionaries. We\nconsider the task-specific word and phrase identification as feature learning.\nWe train a convolutional neural network over a set of labeled texts and use\nscore vectors to localize the task-specific words and phrases. Experimental\nresults on sentiment word identification show that our approach significantly\noutperforms existing methods. We further conduct two case studies to show the\neffectiveness of our approach. One case study on a crawled tweets dataset\ndemonstrates that our approach can successfully capture the\ndiscrimination-related words/phrases. The other case study on fake review\ndetection shows that our approach can identify the fake-review words/phrases.\n", "versions": [{"version": "v1", "created": "Sat, 3 Jun 2017 02:15:44 GMT"}], "update_date": "2017-06-06", "authors_parsed": [["Yuan", "Shuhan", ""], ["Wu", "Xintao", ""], ["Xiang", "Yang", ""]]}, {"id": "1706.00885", "submitter": "Xin Wang", "authors": "Xin Wang, Yujia Luo, Daniel Crankshaw, Alexey Tumanov, Fisher Yu,\n  Joseph E. Gonzalez", "title": "IDK Cascades: Fast Deep Learning by Learning not to Overthink", "comments": "UAI 2018 camera ready", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Advances in deep learning have led to substantial increases in prediction\naccuracy but have been accompanied by increases in the cost of rendering\npredictions. We conjecture that fora majority of real-world inputs, the recent\nadvances in deep learning have created models that effectively \"overthink\" on\nsimple inputs. In this paper, we revisit the classic question of building model\ncascades that primarily leverage class asymmetry to reduce cost. We introduce\nthe \"I Don't Know\"(IDK) prediction cascades framework, a general framework to\nsystematically compose a set of pre-trained models to accelerate inference\nwithout a loss in prediction accuracy. We propose two search based methods for\nconstructing cascades as well as a new cost-aware objective within this\nframework. The proposed IDK cascade framework can be easily adopted in the\nexisting model serving systems without additional model re-training. We\nevaluate the proposed techniques on a range of benchmarks to demonstrate the\neffectiveness of the proposed framework.\n", "versions": [{"version": "v1", "created": "Sat, 3 Jun 2017 02:29:12 GMT"}, {"version": "v2", "created": "Tue, 20 Jun 2017 21:03:43 GMT"}, {"version": "v3", "created": "Wed, 13 Sep 2017 17:19:04 GMT"}, {"version": "v4", "created": "Wed, 27 Jun 2018 07:11:26 GMT"}], "update_date": "2018-06-28", "authors_parsed": [["Wang", "Xin", ""], ["Luo", "Yujia", ""], ["Crankshaw", "Daniel", ""], ["Tumanov", "Alexey", ""], ["Yu", "Fisher", ""], ["Gonzalez", "Joseph E.", ""]]}, {"id": "1706.00891", "submitter": "Xintao Wu", "authors": "Shuhan Yuan, Xintao Wu, Jun Li, Aidong Lu", "title": "Spectrum-based deep neural networks for fraud detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.LG cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we focus on fraud detection on a signed graph with only a\nsmall set of labeled training data. We propose a novel framework that combines\ndeep neural networks and spectral graph analysis. In particular, we use the\nnode projection (called as spectral coordinate) in the low dimensional spectral\nspace of the graph's adjacency matrix as input of deep neural networks.\nSpectral coordinates in the spectral space capture the most useful topology\ninformation of the network. Due to the small dimension of spectral coordinates\n(compared with the dimension of the adjacency matrix derived from a graph),\ntraining deep neural networks becomes feasible. We develop and evaluate two\nneural networks, deep autoencoder and convolutional neural network, in our\nfraud detection framework. Experimental results on a real signed graph show\nthat our spectrum based deep neural networks are effective in fraud detection.\n", "versions": [{"version": "v1", "created": "Sat, 3 Jun 2017 03:08:34 GMT"}], "update_date": "2017-06-06", "authors_parsed": [["Yuan", "Shuhan", ""], ["Wu", "Xintao", ""], ["Li", "Jun", ""], ["Lu", "Aidong", ""]]}, {"id": "1706.00909", "submitter": "Philip H\\\"ausser", "authors": "Philip H\\\"ausser and Alexander Mordvintsev and Daniel Cremers", "title": "Learning by Association - A versatile semi-supervised training method\n  for neural networks", "comments": "IEEE Conference on Computer Vision and Pattern Recognition (CVPR)\n  2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In many real-world scenarios, labeled data for a specific machine learning\ntask is costly to obtain. Semi-supervised training methods make use of\nabundantly available unlabeled data and a smaller number of labeled examples.\nWe propose a new framework for semi-supervised training of deep neural networks\ninspired by learning in humans. \"Associations\" are made from embeddings of\nlabeled samples to those of unlabeled ones and back. The optimization schedule\nencourages correct association cycles that end up at the same class from which\nthe association was started and penalizes wrong associations ending at a\ndifferent class. The implementation is easy to use and can be added to any\nexisting end-to-end training setup. We demonstrate the capabilities of learning\nby association on several data sets and show that it can improve performance on\nclassification tasks tremendously by making use of additionally available\nunlabeled data. In particular, for cases with few labeled data, our training\nscheme outperforms the current state of the art on SVHN.\n", "versions": [{"version": "v1", "created": "Sat, 3 Jun 2017 08:08:56 GMT"}], "update_date": "2017-06-06", "authors_parsed": [["H\u00e4usser", "Philip", ""], ["Mordvintsev", "Alexander", ""], ["Cremers", "Daniel", ""]]}, {"id": "1706.00947", "submitter": "Annamalai Narayanan", "authors": "Annamalai Narayanan, Mahinthan Chandramohan, Lihui Chen, Yang Liu", "title": "Context-aware, Adaptive and Scalable Android Malware Detection through\n  Online Learning (extended version)", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.LG cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is well-known that Android malware constantly evolves so as to evade\ndetection. This causes the entire malware population to be non-stationary.\nContrary to this fact, most of the prior works on Machine Learning based\nAndroid malware detection have assumed that the distribution of the observed\nmalware characteristics (i.e., features) does not change over time. In this\nwork, we address the problem of malware population drift and propose a novel\nonline learning based framework to detect malware, named CASANDRA\n(Contextaware, Adaptive and Scalable ANDRoid mAlware detector). In order to\nperform accurate detection, a novel graph kernel that facilitates capturing\napps' security-sensitive behaviors along with their context information from\ndependency graphs is proposed. Besides being accurate and scalable, CASANDRA\nhas specific advantages: i) being adaptive to the evolution in malware features\nover time ii) explaining the significant features that led to an app's\nclassification as being malicious or benign. In a large-scale comparative\nanalysis, CASANDRA outperforms two state-of-the-art techniques on a benchmark\ndataset achieving 99.23% F-measure. When evaluated with more than 87,000 apps\ncollected in-the-wild, CASANDRA achieves 89.92% accuracy, outperforming\nexisting techniques by more than 25% in their typical batch learning setting\nand more than 7% when they are continuously retained, while maintaining\ncomparable efficiency.\n", "versions": [{"version": "v1", "created": "Sat, 3 Jun 2017 13:07:53 GMT"}, {"version": "v2", "created": "Thu, 6 Jul 2017 09:14:55 GMT"}], "update_date": "2017-07-07", "authors_parsed": [["Narayanan", "Annamalai", ""], ["Chandramohan", "Mahinthan", ""], ["Chen", "Lihui", ""], ["Liu", "Yang", ""]]}, {"id": "1706.00948", "submitter": "Xinyao Qian", "authors": "Xin-Yao Qian", "title": "Financial Series Prediction: Comparison Between Precision of Time Series\n  Models and Machine Learning Methods", "comments": "The successful result can be greatly attributed to overfitting. After\n  careful consideration, I think the conclusion is not that convincible and\n  valuable", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG q-fin.ST", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Precise financial series predicting has long been a difficult problem because\nof unstableness and many noises within the series. Although Traditional time\nseries models like ARIMA and GARCH have been researched and proved to be\neffective in predicting, their performances are still far from satisfying.\nMachine Learning, as an emerging research field in recent years, has brought\nabout many incredible improvements in tasks such as regressing and classifying,\nand it's also promising to exploit the methodology in financial time series\npredicting. In this paper, the predicting precision of financial time series\nbetween traditional time series models and mainstream machine learning models\nincluding some state-of-the-art ones of deep learning are compared through\nexperiment using real stock index data from history. The result shows that\nmachine learning as a modern method far surpasses traditional models in\nprecision.\n", "versions": [{"version": "v1", "created": "Sat, 3 Jun 2017 13:11:01 GMT"}, {"version": "v2", "created": "Thu, 23 Nov 2017 14:23:01 GMT"}, {"version": "v3", "created": "Thu, 30 Nov 2017 06:06:30 GMT"}, {"version": "v4", "created": "Mon, 25 Dec 2017 15:23:51 GMT"}, {"version": "v5", "created": "Sat, 8 Dec 2018 02:42:06 GMT"}], "update_date": "2018-12-11", "authors_parsed": [["Qian", "Xin-Yao", ""]]}, {"id": "1706.00977", "submitter": "Vashist Avadhanula", "authors": "Shipra Agrawal, Vashist Avadhanula, Vineet Goyal, Assaf Zeevi", "title": "Thompson Sampling for the MNL-Bandit", "comments": "Accepted for presentation at Conference on Learning Theory (COLT)\n  2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider a sequential subset selection problem under parameter\nuncertainty, where at each time step, the decision maker selects a subset of\ncardinality $K$ from $N$ possible items (arms), and observes a (bandit)\nfeedback in the form of the index of one of the items in said subset, or none.\nEach item in the index set is ascribed a certain value (reward), and the\nfeedback is governed by a Multinomial Logit (MNL) choice model whose parameters\nare a priori unknown. The objective of the decision maker is to maximize the\nexpected cumulative rewards over a finite horizon $T$, or alternatively,\nminimize the regret relative to an oracle that knows the MNL parameters. We\nrefer to this as the MNL-Bandit problem. This problem is representative of a\nlarger family of exploration-exploitation problems that involve a combinatorial\nobjective, and arise in several important application domains. We present an\napproach to adapt Thompson Sampling to this problem and show that it achieves\nnear-optimal regret as well as attractive numerical performance.\n", "versions": [{"version": "v1", "created": "Sat, 3 Jun 2017 16:48:34 GMT"}, {"version": "v2", "created": "Tue, 13 Jun 2017 09:47:40 GMT"}, {"version": "v3", "created": "Sat, 1 Jul 2017 17:36:16 GMT"}, {"version": "v4", "created": "Sat, 27 Oct 2018 09:53:17 GMT"}, {"version": "v5", "created": "Wed, 31 Oct 2018 06:57:46 GMT"}, {"version": "v6", "created": "Wed, 19 Dec 2018 23:14:39 GMT"}, {"version": "v7", "created": "Thu, 3 Jan 2019 19:45:01 GMT"}], "update_date": "2019-01-07", "authors_parsed": [["Agrawal", "Shipra", ""], ["Avadhanula", "Vashist", ""], ["Goyal", "Vineet", ""], ["Zeevi", "Assaf", ""]]}, {"id": "1706.00996", "submitter": "Shahira Azazy", "authors": "Shahira Shaaban Azab, Mohamed Farouk Abdel Hady, Hesham Ahmed Hefny", "title": "Semi-supervised Classification: Cluster and Label Approach using\n  Particle Swarm Optimization", "comments": null, "journal-ref": "International Journal of Computer Applications (09758887), Volume\n  160, No 3, February 2017", "doi": "10.5120/ijca2017913013", "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Classification predicts classes of objects using the knowledge learned during\nthe training phase. This process requires learning from labeled samples.\nHowever, the labeled samples usually limited. Annotation process is annoying,\ntedious, expensive, and requires human experts. Meanwhile, unlabeled data is\navailable and almost free. Semi-supervised learning approaches make use of both\nlabeled and unlabeled data. This paper introduces cluster and label approach\nusing PSO for semi-supervised classification. PSO is competitive to traditional\nclustering algorithms. A new local best PSO is presented to cluster the\nunlabeled data. The available labeled data guides the learning process. The\nexperiments are conducted using four state-of-the-art datasets from different\ndomains. The results compared with Label Propagation a popular semi-supervised\nclassifier and two state-of-the-art supervised classification models, namely\nk-nearest neighbors and decision trees. The experiments show the efficiency of\nthe proposed model.\n", "versions": [{"version": "v1", "created": "Sat, 3 Jun 2017 20:07:23 GMT"}], "update_date": "2017-06-06", "authors_parsed": [["Azab", "Shahira Shaaban", ""], ["Hady", "Mohamed Farouk Abdel", ""], ["Hefny", "Hesham Ahmed", ""]]}, {"id": "1706.00997", "submitter": "Shahira Azazy", "authors": "Shahira Shaaban Azab, Hesham Ahmed Hefny", "title": "Center of Gravity PSO for Partitioning Clustering", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents the local best model of PSO for partition-based\nclustering. The proposed model gets rid off the drawbacks of gbest PSO for\nclustering. The model uses a pre-specified number of clusters K. The LPOSC has\nK neighborhoods. Each neighborhood represents one of the clusters. The goal of\nthe particles in each neighborhood is optimizing the position of the centroid\nof the cluster. The performance of the proposed algorithms is measured using\nadjusted rand index. The results is compared with k-means and global best model\nof PSO.\n", "versions": [{"version": "v1", "created": "Sat, 3 Jun 2017 20:07:52 GMT"}, {"version": "v2", "created": "Wed, 14 Jun 2017 10:08:25 GMT"}], "update_date": "2017-06-15", "authors_parsed": [["Azab", "Shahira Shaaban", ""], ["Hefny", "Hesham Ahmed", ""]]}, {"id": "1706.00998", "submitter": "Shahira Azazy", "authors": "Shahira Shaaban Azab, Hesham Ahmed Hefny", "title": "Swarm Intelligence in Semi-supervised Classification", "comments": null, "journal-ref": "Data Mining And Knowledge Engineering, 9(5), 99-103 (2017)", "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This Paper represents a literature review of Swarm intelligence algorithm in\nthe area of semi-supervised classification. There are many research papers for\napplying swarm intelligence algorithms in the area of machine learning. Some\nalgorithms of SI are applied in the area of ML either solely or hybrid with\nother ML algorithms. SI algorithms are also used for tuning parameters of ML\nalgorithm, or as a backbone for ML algorithms. This paper introduces a brief\nliterature review for applying swarm intelligence algorithms in the field of\nsemi-supervised learning\n", "versions": [{"version": "v1", "created": "Sat, 3 Jun 2017 20:09:06 GMT"}], "update_date": "2017-06-06", "authors_parsed": [["Azab", "Shahira Shaaban", ""], ["Hefny", "Hesham Ahmed", ""]]}, {"id": "1706.01010", "submitter": "Jie Hou", "authors": "Jie Hou, Badri Adhikari, Jianlin Cheng", "title": "DeepSF: deep convolutional neural network for mapping protein sequences\n  to folds", "comments": "28 pages, 13 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG q-bio.BM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motivation\n  Protein fold recognition is an important problem in structural\nbioinformatics. Almost all traditional fold recognition methods use sequence\n(homology) comparison to indirectly predict the fold of a tar get protein based\non the fold of a template protein with known structure, which cannot explain\nthe relationship between sequence and fold. Only a few methods had been\ndeveloped to classify protein sequences into a small number of folds due to\nmethodological limitations, which are not generally useful in practice.\n  Results\n  We develop a deep 1D-convolution neural network (DeepSF) to directly classify\nany protein se quence into one of 1195 known folds, which is useful for both\nfold recognition and the study of se quence-structure relationship. Different\nfrom traditional sequence alignment (comparison) based methods, our method\nautomatically extracts fold-related features from a protein sequence of any\nlength and map it to the fold space. We train and test our method on the\ndatasets curated from SCOP1.75, yielding a classification accuracy of 80.4%. On\nthe independent testing dataset curated from SCOP2.06, the classification\naccuracy is 77.0%. We compare our method with a top profile profile alignment\nmethod - HHSearch on hard template-based and template-free modeling targets of\nCASP9-12 in terms of fold recognition accuracy. The accuracy of our method is\n14.5%-29.1% higher than HHSearch on template-free modeling targets and\n4.5%-16.7% higher on hard template-based modeling targets for top 1, 5, and 10\npredicted folds. The hidden features extracted from sequence by our method is\nrobust against sequence mutation, insertion, deletion and truncation, and can\nbe used for other protein pattern recognition problems such as protein\nclustering, comparison and ranking.\n", "versions": [{"version": "v1", "created": "Sun, 4 Jun 2017 01:33:08 GMT"}], "update_date": "2017-06-06", "authors_parsed": [["Hou", "Jie", ""], ["Adhikari", "Badri", ""], ["Cheng", "Jianlin", ""]]}, {"id": "1706.01014", "submitter": "Ming Yan", "authors": "Xiaolin Huang and Ming Yan", "title": "Nonconvex penalties with analytical solutions for one-bit compressive\n  sensing", "comments": null, "journal-ref": "X. Huang and M. Yan, Non-convex penalties with analytical\n  solutions for one-bit compressive sensing, Signal Processing, 144 (2018),\n  341-351", "doi": "10.1016/j.sigpro.2017.10.023", "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One-bit measurements widely exist in the real world, and they can be used to\nrecover sparse signals. This task is known as the problem of learning\nhalfspaces in learning theory and one-bit compressive sensing (1bit-CS) in\nsignal processing. In this paper, we propose novel algorithms based on both\nconvex and nonconvex sparsity-inducing penalties for robust 1bit-CS. We provide\na sufficient condition to verify whether a solution is globally optimal or not.\nThen we show that the globally optimal solution for positive homogeneous\npenalties can be obtained in two steps: a proximal operator and a normalization\nstep. For several nonconvex penalties, including minimax concave penalty (MCP),\n$\\ell_0$ norm, and sorted $\\ell_1$ penalty, we provide fast algorithms for\nfinding the analytical solutions by solving the dual problem. Specifically, our\nalgorithm is more than $200$ times faster than the existing algorithm for MCP.\nIts efficiency is comparable to the algorithm for the $\\ell_1$ penalty in time,\nwhile its performance is much better. Among these penalties, the sorted\n$\\ell_1$ penalty is most robust to noise in different settings.\n", "versions": [{"version": "v1", "created": "Sun, 4 Jun 2017 02:17:12 GMT"}, {"version": "v2", "created": "Mon, 6 Nov 2017 21:44:00 GMT"}], "update_date": "2017-11-08", "authors_parsed": [["Huang", "Xiaolin", ""], ["Yan", "Ming", ""]]}, {"id": "1706.01026", "submitter": "Yuan Zhou", "authors": "Jiecao Chen, Xi Chen, Qin Zhang, Yuan Zhou", "title": "Adaptive Multiple-Arm Identification", "comments": "30 pages, 5 figures, preliminary version to appear in ICML 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of selecting $K$ arms with the highest expected rewards\nin a stochastic $n$-armed bandit game. This problem has a wide range of\napplications, e.g., A/B testing, crowdsourcing, simulation optimization. Our\ngoal is to develop a PAC algorithm, which, with probability at least\n$1-\\delta$, identifies a set of $K$ arms with the aggregate regret at most\n$\\epsilon$. The notion of aggregate regret for multiple-arm identification was\nfirst introduced in \\cite{Zhou:14} , which is defined as the difference of the\naveraged expected rewards between the selected set of arms and the best $K$\narms. In contrast to \\cite{Zhou:14} that only provides instance-independent\nsample complexity, we introduce a new hardness parameter for characterizing the\ndifficulty of any given instance. We further develop two algorithms and\nestablish the corresponding sample complexity in terms of this hardness\nparameter. The derived sample complexity can be significantly smaller than\nstate-of-the-art results for a large class of instances and matches the\ninstance-independent lower bound upto a $\\log(\\epsilon^{-1})$ factor in the\nworst case. We also prove a lower bound result showing that the extra\n$\\log(\\epsilon^{-1})$ is necessary for instance-dependent algorithms using the\nintroduced hardness parameter.\n", "versions": [{"version": "v1", "created": "Sun, 4 Jun 2017 05:04:52 GMT"}], "update_date": "2017-06-06", "authors_parsed": [["Chen", "Jiecao", ""], ["Chen", "Xi", ""], ["Zhang", "Qin", ""], ["Zhou", "Yuan", ""]]}, {"id": "1706.01081", "submitter": "Mingda Qiao", "authors": "Lijie Chen, Anupam Gupta, Jian Li, Mingda Qiao, Ruosong Wang", "title": "Nearly Optimal Sampling Algorithms for Combinatorial Pure Exploration", "comments": "Accepted to COLT 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the combinatorial pure exploration problem Best-Set in stochastic\nmulti-armed bandits. In a Best-Set instance, we are given $n$ arms with unknown\nreward distributions, as well as a family $\\mathcal{F}$ of feasible subsets\nover the arms. Our goal is to identify the feasible subset in $\\mathcal{F}$\nwith the maximum total mean using as few samples as possible. The problem\ngeneralizes the classical best arm identification problem and the top-$k$ arm\nidentification problem, both of which have attracted significant attention in\nrecent years. We provide a novel instance-wise lower bound for the sample\ncomplexity of the problem, as well as a nontrivial sampling algorithm, matching\nthe lower bound up to a factor of $\\ln|\\mathcal{F}|$. For an important class of\ncombinatorial families, we also provide polynomial time implementation of the\nsampling algorithm, using the equivalence of separation and optimization for\nconvex program, and approximate Pareto curves in multi-objective optimization.\nWe also show that the $\\ln|\\mathcal{F}|$ factor is inevitable in general\nthrough a nontrivial lower bound construction. Our results significantly\nimprove several previous results for several important combinatorial\nconstraints, and provide a tighter understanding of the general Best-Set\nproblem.\n  We further introduce an even more general problem, formulated in geometric\nterms. We are given $n$ Gaussian arms with unknown means and unit variance.\nConsider the $n$-dimensional Euclidean space $\\mathbb{R}^n$, and a collection\n$\\mathcal{O}$ of disjoint subsets. Our goal is to determine the subset in\n$\\mathcal{O}$ that contains the $n$-dimensional vector of the means. The\nproblem generalizes most pure exploration bandit problems studied in the\nliterature. We provide the first nearly optimal sample complexity upper and\nlower bounds for the problem.\n", "versions": [{"version": "v1", "created": "Sun, 4 Jun 2017 14:27:17 GMT"}], "update_date": "2017-06-06", "authors_parsed": [["Chen", "Lijie", ""], ["Gupta", "Anupam", ""], ["Li", "Jian", ""], ["Qiao", "Mingda", ""], ["Wang", "Ruosong", ""]]}, {"id": "1706.01084", "submitter": "Ting Chen", "authors": "Ting Chen, Liangjie Hong, Yue Shi, Yizhou Sun", "title": "Joint Text Embedding for Personalized Content-based Recommendation", "comments": "typo fixes", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning a good representation of text is key to many recommendation\napplications. Examples include news recommendation where texts to be\nrecommended are constantly published everyday. However, most existing\nrecommendation techniques, such as matrix factorization based methods, mainly\nrely on interaction histories to learn representations of items. While latent\nfactors of items can be learned effectively from user interaction data, in many\ncases, such data is not available, especially for newly emerged items.\n  In this work, we aim to address the problem of personalized recommendation\nfor completely new items with text information available. We cast the problem\nas a personalized text ranking problem and propose a general framework that\ncombines text embedding with personalized recommendation. Users and textual\ncontent are embedded into latent feature space. The text embedding function can\nbe learned end-to-end by predicting user interactions with items. To alleviate\nsparsity in interaction data, and leverage large amount of text data with\nlittle or no user interactions, we further propose a joint text embedding model\nthat incorporates unsupervised text embedding with a combination module.\nExperimental results show that our model can significantly improve the\neffectiveness of recommendation systems on real-world datasets.\n", "versions": [{"version": "v1", "created": "Sun, 4 Jun 2017 14:48:28 GMT"}, {"version": "v2", "created": "Fri, 23 Jun 2017 21:55:56 GMT"}], "update_date": "2017-06-27", "authors_parsed": [["Chen", "Ting", ""], ["Hong", "Liangjie", ""], ["Shi", "Yue", ""], ["Sun", "Yizhou", ""]]}, {"id": "1706.01108", "submitter": "Peter Richt\\'arik", "authors": "Peter Richt\\'arik and Martin Tak\\'a\\v{c}", "title": "Stochastic Reformulations of Linear Systems: Algorithms and Convergence\n  Theory", "comments": "Accepted to SIAM Journal on Matrix Analysis and Applications. This\n  arXiv version has an additional section (Section 6.2), listing several\n  extensions done since the paper was first written. Statistics: 39 pages, 4\n  reformulations, 3 algorithms", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.NA cs.LG cs.NA stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop a family of reformulations of an arbitrary consistent linear\nsystem into a stochastic problem. The reformulations are governed by two\nuser-defined parameters: a positive definite matrix defining a norm, and an\narbitrary discrete or continuous distribution over random matrices. Our\nreformulation has several equivalent interpretations, allowing for researchers\nfrom various communities to leverage their domain specific insights. In\nparticular, our reformulation can be equivalently seen as a stochastic\noptimization problem, stochastic linear system, stochastic fixed point problem\nand a probabilistic intersection problem. We prove sufficient, and necessary\nand sufficient conditions for the reformulation to be exact. Further, we\npropose and analyze three stochastic algorithms for solving the reformulated\nproblem---basic, parallel and accelerated methods---with global linear\nconvergence rates. The rates can be interpreted as condition numbers of a\nmatrix which depends on the system matrix and on the reformulation parameters.\nThis gives rise to a new phenomenon which we call stochastic preconditioning,\nand which refers to the problem of finding parameters (matrix and distribution)\nleading to a sufficiently small condition number. Our basic method can be\nequivalently interpreted as stochastic gradient descent, stochastic Newton\nmethod, stochastic proximal point method, stochastic fixed point method, and\nstochastic projection method, with fixed stepsize (relaxation parameter),\napplied to the reformulations.\n", "versions": [{"version": "v1", "created": "Sun, 4 Jun 2017 17:04:15 GMT"}, {"version": "v2", "created": "Tue, 6 Jun 2017 04:44:19 GMT"}, {"version": "v3", "created": "Fri, 28 Jun 2019 11:21:42 GMT"}, {"version": "v4", "created": "Fri, 24 Jan 2020 16:50:14 GMT"}], "update_date": "2020-01-27", "authors_parsed": [["Richt\u00e1rik", "Peter", ""], ["Tak\u00e1\u010d", "Martin", ""]]}, {"id": "1706.01109", "submitter": "Tatiana Likhomanenko", "authors": "Alex Rogozhnikov and Tatiana Likhomanenko", "title": "InfiniteBoost: building infinite ensembles with gradient descent", "comments": "7 pages, 5 figures, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In machine learning ensemble methods have demonstrated high accuracy for the\nvariety of problems in different areas. Two notable ensemble methods widely\nused in practice are gradient boosting and random forests. In this paper we\npresent InfiniteBoost - a novel algorithm, which combines important properties\nof these two approaches. The algorithm constructs the ensemble of trees for\nwhich two properties hold: trees of the ensemble incorporate the mistakes done\nby others; at the same time the ensemble could contain the infinite number of\ntrees without the over-fitting effect. The proposed algorithm is evaluated on\nthe regression, classification, and ranking tasks using large scale, publicly\navailable datasets.\n", "versions": [{"version": "v1", "created": "Sun, 4 Jun 2017 17:06:18 GMT"}, {"version": "v2", "created": "Fri, 21 Sep 2018 17:20:36 GMT"}], "update_date": "2018-09-24", "authors_parsed": [["Rogozhnikov", "Alex", ""], ["Likhomanenko", "Tatiana", ""]]}, {"id": "1706.01120", "submitter": "Unai Garciarena", "authors": "Unai Garciarena, Roberto Santana, Alexander Mendiburu", "title": "Evolving imputation strategies for missing data in classification\n  problems with TPOT", "comments": "15 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Missing data has a ubiquitous presence in real-life applications of machine\nlearning techniques. Imputation methods are algorithms conceived for restoring\nmissing values in the data, based on other entries in the database. The choice\nof the imputation method has an influence on the performance of the machine\nlearning technique, e.g., it influences the accuracy of the classification\nalgorithm applied to the data. Therefore, selecting and applying the right\nimputation method is important and usually requires a substantial amount of\nhuman intervention. In this paper we propose the use of genetic programming\ntechniques to search for the right combination of imputation and classification\nalgorithms. We build our work on the recently introduced Python-based TPOT\nlibrary, and incorporate a heterogeneous set of imputation algorithms as part\nof the machine learning pipeline search. We show that genetic programming can\nautomatically find increasingly better pipelines that include the most\neffective combinations of imputation methods, feature pre-processing, and\nclassifiers for a variety of classification problems with missing data.\n", "versions": [{"version": "v1", "created": "Sun, 4 Jun 2017 18:20:07 GMT"}, {"version": "v2", "created": "Mon, 14 Aug 2017 22:38:19 GMT"}], "update_date": "2017-08-16", "authors_parsed": [["Garciarena", "Unai", ""], ["Santana", "Roberto", ""], ["Mendiburu", "Alexander", ""]]}, {"id": "1706.01151", "submitter": "Neev Samuel", "authors": "Neev Samuel, Tzvi Diskin and Ami Wiesel", "title": "Deep MIMO Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.IT cs.LG math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we consider the use of deep neural networks in the context of\nMultiple-Input-Multiple-Output (MIMO) detection. We give a brief introduction\nto deep learning and propose a modern neural network architecture suitable for\nthis detection task. First, we consider the case in which the MIMO channel is\nconstant, and we learn a detector for a specific system. Next, we consider the\nharder case in which the parameters are known yet changing and a single\ndetector must be learned for all multiple varying channels. We demonstrate the\nperformance of our deep MIMO detector using numerical simulations in comparison\nto competing methods including approximate message passing and semidefinite\nrelaxation. The results show that deep networks can achieve state of the art\naccuracy with significantly lower complexity while providing robustness against\nill conditioned channels and mis-specified noise variance.\n", "versions": [{"version": "v1", "created": "Sun, 4 Jun 2017 21:33:11 GMT"}], "update_date": "2017-06-06", "authors_parsed": [["Samuel", "Neev", ""], ["Diskin", "Tzvi", ""], ["Wiesel", "Ami", ""]]}, {"id": "1706.01177", "submitter": "Yu Shi", "authors": "Yu Shi, Po-Wei Chan, Honglei Zhuang, Huan Gui and Jiawei Han", "title": "PReP: Path-Based Relevance from a Probabilistic Perspective in\n  Heterogeneous Information Networks", "comments": "10 pages. In Proceedings of the 23nd ACM SIGKDD International\n  Conference on Knowledge Discovery and Data Mining, Halifax, Nova Scotia,\n  Canada, ACM, 2017", "journal-ref": null, "doi": "10.1145/3097983.3097990", "report-no": null, "categories": "cs.SI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As a powerful representation paradigm for networked and multi-typed data, the\nheterogeneous information network (HIN) is ubiquitous. Meanwhile, defining\nproper relevance measures has always been a fundamental problem and of great\npragmatic importance for network mining tasks. Inspired by our probabilistic\ninterpretation of existing path-based relevance measures, we propose to study\nHIN relevance from a probabilistic perspective. We also identify, from\nreal-world data, and propose to model cross-meta-path synergy, which is a\ncharacteristic important for defining path-based HIN relevance and has not been\nmodeled by existing methods. A generative model is established to derive a\nnovel path-based relevance measure, which is data-driven and tailored for each\nHIN. We develop an inference algorithm to find the maximum a posteriori (MAP)\nestimate of the model parameters, which entails non-trivial tricks. Experiments\non two real-world datasets demonstrate the effectiveness of the proposed model\nand relevance measure.\n", "versions": [{"version": "v1", "created": "Mon, 5 Jun 2017 02:28:15 GMT"}, {"version": "v2", "created": "Wed, 20 Feb 2019 22:21:18 GMT"}], "update_date": "2019-02-22", "authors_parsed": [["Shi", "Yu", ""], ["Chan", "Po-Wei", ""], ["Zhuang", "Honglei", ""], ["Gui", "Huan", ""], ["Han", "Jiawei", ""]]}, {"id": "1706.01214", "submitter": "Azad Naik", "authors": "Azad Naik, Huzefa Rangwala", "title": "Inconsistent Node Flattening for Improving Top-down Hierarchical\n  Classification", "comments": "IEEE International Conference on Data Science and Advanced Analytics\n  (DSAA), 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Large-scale classification of data where classes are structurally organized\nin a hierarchy is an important area of research. Top-down approaches that\nexploit the hierarchy during the learning and prediction phase are efficient\nfor large scale hierarchical classification. However, accuracy of top-down\napproaches is poor due to error propagation i.e., prediction errors made at\nhigher levels in the hierarchy cannot be corrected at lower levels. One of the\nmain reason behind errors at the higher levels is the presence of inconsistent\nnodes that are introduced due to the arbitrary process of creating these\nhierarchies by domain experts. In this paper, we propose two different\ndata-driven approaches (local and global) for hierarchical structure\nmodification that identifies and flattens inconsistent nodes present within the\nhierarchy. Our extensive empirical evaluation of the proposed approaches on\nseveral image and text datasets with varying distribution of features, classes\nand training instances per class shows improved classification performance over\ncompeting hierarchical modification approaches. Specifically, we see an\nimprovement upto 7% in Macro-F1 score with our approach over best TD baseline.\nSOURCE CODE: http://www.cs.gmu.edu/~mlbio/InconsistentNodeFlattening\n", "versions": [{"version": "v1", "created": "Mon, 5 Jun 2017 06:53:30 GMT"}], "update_date": "2017-06-06", "authors_parsed": [["Naik", "Azad", ""], ["Rangwala", "Huzefa", ""]]}, {"id": "1706.01215", "submitter": "Shuochao Yao", "authors": "Shuochao Yao, Yiran Zhao, Aston Zhang, Lu Su, Tarek Abdelzaher", "title": "DeepIoT: Compressing Deep Neural Network Structures for Sensing Systems\n  with a Compressor-Critic Framework", "comments": "Published in SenSys2017. Code is available on\n  https://github.com/yscacaca/DeepIoT", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent advances in deep learning motivate the use of deep neutral networks in\nsensing applications, but their excessive resource needs on constrained\nembedded devices remain an important impediment. A recently explored solution\nspace lies in compressing (approximating or simplifying) deep neural networks\nin some manner before use on the device. We propose a new compression solution,\ncalled DeepIoT, that makes two key contributions in that space. First, unlike\ncurrent solutions geared for compressing specific types of neural networks,\nDeepIoT presents a unified approach that compresses all commonly used deep\nlearning structures for sensing applications, including fully-connected,\nconvolutional, and recurrent neural networks, as well as their combinations.\nSecond, unlike solutions that either sparsify weight matrices or assume linear\nstructure within weight matrices, DeepIoT compresses neural network structures\ninto smaller dense matrices by finding the minimum number of non-redundant\nhidden elements, such as filters and dimensions required by each layer, while\nkeeping the performance of sensing applications the same. Importantly, it does\nso using an approach that obtains a global view of parameter redundancies,\nwhich is shown to produce superior compression. We conduct experiments with\nfive different sensing-related tasks on Intel Edison devices. DeepIoT\noutperforms all compared baseline algorithms with respect to execution time and\nenergy consumption by a significant margin. It reduces the size of deep neural\nnetworks by 90% to 98.9%. It is thus able to shorten execution time by 71.4% to\n94.5%, and decrease energy consumption by 72.2% to 95.7%. These improvements\nare achieved without loss of accuracy. The results underscore the potential of\nDeepIoT for advancing the exploitation of deep neural networks on\nresource-constrained embedded devices.\n", "versions": [{"version": "v1", "created": "Mon, 5 Jun 2017 06:55:44 GMT"}, {"version": "v2", "created": "Thu, 17 Aug 2017 14:48:44 GMT"}, {"version": "v3", "created": "Wed, 22 Nov 2017 23:27:25 GMT"}], "update_date": "2017-11-27", "authors_parsed": [["Yao", "Shuochao", ""], ["Zhao", "Yiran", ""], ["Zhang", "Aston", ""], ["Su", "Lu", ""], ["Abdelzaher", "Tarek", ""]]}, {"id": "1706.01242", "submitter": "Jos van der Westhuizen", "authors": "Jos van der Westhuizen and Joan Lasenby", "title": "Bayesian LSTMs in medicine", "comments": "11 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The medical field stands to see significant benefits from the recent advances\nin deep learning. Knowing the uncertainty in the decision made by any machine\nlearning algorithm is of utmost importance for medical practitioners. This\nstudy demonstrates the utility of using Bayesian LSTMs for classification of\nmedical time series. Four medical time series datasets are used to show the\naccuracy improvement Bayesian LSTMs provide over standard LSTMs. Moreover, we\nshow cherry-picked examples of confident and uncertain classifications of the\nmedical time series. With simple modifications of the common practice for deep\nlearning, significant improvements can be made for the medical practitioner and\npatient.\n", "versions": [{"version": "v1", "created": "Mon, 5 Jun 2017 09:04:07 GMT"}], "update_date": "2017-06-06", "authors_parsed": [["van der Westhuizen", "Jos", ""], ["Lasenby", "Joan", ""]]}, {"id": "1706.01284", "submitter": "Xinyun Chen", "authors": "Xinyun Chen, Chang Liu, Dawn Song", "title": "Towards Synthesizing Complex Programs from Input-Output Examples", "comments": "Published as a conference paper at ICLR 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, deep learning techniques have been developed to improve the\nperformance of program synthesis from input-output examples. Albeit its\nsignificant progress, the programs that can be synthesized by state-of-the-art\napproaches are still simple in terms of their complexity. In this work, we move\na significant step forward along this direction by proposing a new class of\nchallenging tasks in the domain of program synthesis from input-output\nexamples: learning a context-free parser from pairs of input programs and their\nparse trees. We show that this class of tasks are much more challenging than\npreviously studied tasks, and the test accuracy of existing approaches is\nalmost 0%.\n  We tackle the challenges by developing three novel techniques inspired by\nthree novel observations, which reveal the key ingredients of using deep\nlearning to synthesize a complex program. First, the use of a\nnon-differentiable machine is the key to effectively restrict the search space.\nThus our proposed approach learns a neural program operating a domain-specific\nnon-differentiable machine. Second, recursion is the key to achieve\ngeneralizability. Thus, we bake-in the notion of recursion in the design of our\nnon-differentiable machine. Third, reinforcement learning is the key to learn\nhow to operate the non-differentiable machine, but it is also hard to train the\nmodel effectively with existing reinforcement learning algorithms from a cold\nboot. We develop a novel two-phase reinforcement learning-based search\nalgorithm to overcome this issue. In our evaluation, we show that using our\nnovel approach, neural parsing programs can be learned to achieve 100% test\naccuracy on test inputs that are 500x longer than the training samples.\n", "versions": [{"version": "v1", "created": "Mon, 5 Jun 2017 11:44:35 GMT"}, {"version": "v2", "created": "Tue, 30 Jan 2018 04:54:32 GMT"}, {"version": "v3", "created": "Sun, 11 Feb 2018 04:33:30 GMT"}, {"version": "v4", "created": "Thu, 8 Mar 2018 00:22:59 GMT"}], "update_date": "2018-03-09", "authors_parsed": [["Chen", "Xinyun", ""], ["Liu", "Chang", ""], ["Song", "Dawn", ""]]}, {"id": "1706.01322", "submitter": "Alexander Kuhnle", "authors": "Alexander Kuhnle and Ann Copestake", "title": "Deep learning evaluation using deep linguistic processing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We discuss problems with the standard approaches to evaluation for tasks like\nvisual question answering, and argue that artificial data can be used to\naddress these as a complement to current practice. We demonstrate that with the\nhelp of existing 'deep' linguistic processing technology we are able to create\nchallenging abstract datasets, which enable us to investigate the language\nunderstanding abilities of multimodal deep learning models in detail, as\ncompared to a single performance value on a static and monolithic dataset.\n", "versions": [{"version": "v1", "created": "Mon, 5 Jun 2017 13:53:56 GMT"}, {"version": "v2", "created": "Sat, 12 May 2018 10:37:02 GMT"}], "update_date": "2018-05-15", "authors_parsed": [["Kuhnle", "Alexander", ""], ["Copestake", "Ann", ""]]}, {"id": "1706.01331", "submitter": "Mark Riedl", "authors": "Lara J. Martin, Prithviraj Ammanabrolu, Xinyu Wang, William Hancock,\n  Shruti Singh, Brent Harrison, Mark O. Riedl", "title": "Event Representations for Automated Story Generation with Deep Neural\n  Nets", "comments": "Submitted to AAAI'18", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automated story generation is the problem of automatically selecting a\nsequence of events, actions, or words that can be told as a story. We seek to\ndevelop a system that can generate stories by learning everything it needs to\nknow from textual story corpora. To date, recurrent neural networks that learn\nlanguage models at character, word, or sentence levels have had little success\ngenerating coherent stories. We explore the question of event representations\nthat provide a mid-level of abstraction between words and sentences in order to\nretain the semantic information of the original data while minimizing event\nsparsity. We present a technique for preprocessing textual story data into\nevent sequences. We then present a technique for automated story generation\nwhereby we decompose the problem into the generation of successive events\n(event2event) and the generation of natural language sentences from events\n(event2sentence). We give empirical results comparing different event\nrepresentations and their effects on event successor generation and the\ntranslation of events to natural language.\n", "versions": [{"version": "v1", "created": "Mon, 5 Jun 2017 14:04:48 GMT"}, {"version": "v2", "created": "Mon, 14 Aug 2017 18:14:02 GMT"}, {"version": "v3", "created": "Tue, 12 Sep 2017 14:45:22 GMT"}], "update_date": "2017-09-13", "authors_parsed": [["Martin", "Lara J.", ""], ["Ammanabrolu", "Prithviraj", ""], ["Wang", "Xinyu", ""], ["Hancock", "William", ""], ["Singh", "Shruti", ""], ["Harrison", "Brent", ""], ["Riedl", "Mark O.", ""]]}, {"id": "1706.01340", "submitter": "Robin Ruede", "authors": "Robin Ruede, Markus M\\\"uller, Sebastian St\\\"uker, Alex Waibel", "title": "Yeah, Right, Uh-Huh: A Deep Learning Backchannel Predictor", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.CV cs.HC cs.LG cs.SD", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Using supporting backchannel (BC) cues can make human-computer interaction\nmore social. BCs provide a feedback from the listener to the speaker indicating\nto the speaker that he is still listened to. BCs can be expressed in different\nways, depending on the modality of the interaction, for example as gestures or\nacoustic cues. In this work, we only considered acoustic cues. We are proposing\nan approach towards detecting BC opportunities based on acoustic input features\nlike power and pitch. While other works in the field rely on the use of a\nhand-written rule set or specialized features, we made use of artificial neural\nnetworks. They are capable of deriving higher order features from input\nfeatures themselves. In our setup, we first used a fully connected feed-forward\nnetwork to establish an updated baseline in comparison to our previously\nproposed setup. We also extended this setup by the use of Long Short-Term\nMemory (LSTM) networks which have shown to outperform feed-forward based setups\non various tasks. Our best system achieved an F1-Score of 0.37 using power and\npitch features. Adding linguistic information using word2vec, the score\nincreased to 0.39.\n", "versions": [{"version": "v1", "created": "Fri, 2 Jun 2017 17:05:26 GMT"}], "update_date": "2017-06-06", "authors_parsed": [["Ruede", "Robin", ""], ["M\u00fcller", "Markus", ""], ["St\u00fcker", "Sebastian", ""], ["Waibel", "Alex", ""]]}, {"id": "1706.01350", "submitter": "Alessandro Achille", "authors": "Alessandro Achille and Stefano Soatto", "title": "Emergence of Invariance and Disentanglement in Deep Representations", "comments": "Deep learning, neural network, representation, flat minima,\n  information bottleneck, overfitting, generalization, sufficiency, minimality,\n  sensitivity, information complexity, stochastic gradient descent,\n  regularization, total correlation, PAC-Bayes", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Using established principles from Statistics and Information Theory, we show\nthat invariance to nuisance factors in a deep neural network is equivalent to\ninformation minimality of the learned representation, and that stacking layers\nand injecting noise during training naturally bias the network towards learning\ninvariant representations. We then decompose the cross-entropy loss used during\ntraining and highlight the presence of an inherent overfitting term. We propose\nregularizing the loss by bounding such a term in two equivalent ways: One with\na Kullbach-Leibler term, which relates to a PAC-Bayes perspective; the other\nusing the information in the weights as a measure of complexity of a learned\nmodel, yielding a novel Information Bottleneck for the weights. Finally, we\nshow that invariance and independence of the components of the representation\nlearned by the network are bounded above and below by the information in the\nweights, and therefore are implicitly optimized during training. The theory\nenables us to quantify and predict sharp phase transitions between underfitting\nand overfitting of random labels when using our regularized loss, which we\nverify in experiments, and sheds light on the relation between the geometry of\nthe loss function, invariance properties of the learned representation, and\ngeneralization error.\n", "versions": [{"version": "v1", "created": "Mon, 5 Jun 2017 14:31:03 GMT"}, {"version": "v2", "created": "Mon, 16 Oct 2017 01:21:49 GMT"}, {"version": "v3", "created": "Thu, 28 Jun 2018 17:50:54 GMT"}], "update_date": "2018-06-29", "authors_parsed": [["Achille", "Alessandro", ""], ["Soatto", "Stefano", ""]]}, {"id": "1706.01380", "submitter": "Harish RaviPrakash", "authors": "Harish RaviPrakash, Milena Korostenskaja, Eduardo Castillo, Ki Lee,\n  James Baumgartner, Ulas Bagci", "title": "Automatic Response Assessment in Regions of Language Cortex in Epilepsy\n  Patients Using ECoG-based Functional Mapping and Machine Learning", "comments": "This paper will appear in the Proceedings of IEEE International\n  Conference on Systems, Man and Cybernetics (SMC) 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Accurate localization of brain regions responsible for language and cognitive\nfunctions in Epilepsy patients should be carefully determined prior to surgery.\nElectrocorticography (ECoG)-based Real Time Functional Mapping (RTFM) has been\nshown to be a safer alternative to the electrical cortical stimulation mapping\n(ESM), which is currently the clinical/gold standard. Conventional methods for\nanalyzing RTFM signals are based on statistical comparison of signal power at\ncertain frequency bands. Compared to gold standard (ESM), they have limited\naccuracies when assessing channel responses.\n  In this study, we address the accuracy limitation of the current RTFM signal\nestimation methods by analyzing the full frequency spectrum of the signal and\nreplacing signal power estimation methods with machine learning algorithms,\nspecifically random forest (RF), as a proof of concept. We train RF with power\nspectral density of the time-series RTFM signal in supervised learning\nframework where ground truth labels are obtained from the ESM. Results obtained\nfrom RTFM of six adult patients in a strictly controlled experimental setup\nreveal the state of the art detection accuracy of $\\approx 78\\%$ for the\nlanguage comprehension task, an improvement of $23\\%$ over the conventional\nRTFM estimation method. To the best of our knowledge, this is the first study\nexploring the use of machine learning approaches for determining RTFM signal\ncharacteristics, and using the whole-frequency band for better region\nlocalization. Our results demonstrate the feasibility of machine learning based\nRTFM signal analysis method over the full spectrum to be a clinical routine in\nthe near future.\n", "versions": [{"version": "v1", "created": "Fri, 26 May 2017 16:50:04 GMT"}, {"version": "v2", "created": "Sun, 6 Aug 2017 21:05:14 GMT"}], "update_date": "2017-08-08", "authors_parsed": [["RaviPrakash", "Harish", ""], ["Korostenskaja", "Milena", ""], ["Castillo", "Eduardo", ""], ["Lee", "Ki", ""], ["Baumgartner", "James", ""], ["Bagci", "Ulas", ""]]}, {"id": "1706.01383", "submitter": "Vianney Perchet", "authors": "Joon Kwon, Vianney Perchet, Claire Vernade", "title": "Sparse Stochastic Bandits", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the classical multi-armed bandit problem, d arms are available to the\ndecision maker who pulls them sequentially in order to maximize his cumulative\nreward. Guarantees can be obtained on a relative quantity called regret, which\nscales linearly with d (or with sqrt(d) in the minimax sense). We here consider\nthe sparse case of this classical problem in the sense that only a small number\nof arms, namely s < d, have a positive expected reward. We are able to leverage\nthis additional assumption to provide an algorithm whose regret scales with s\ninstead of d. Moreover, we prove that this algorithm is optimal by providing a\nmatching lower bound - at least for a wide and pertinent range of parameters\nthat we determine - and by evaluating its performance on simulated data.\n", "versions": [{"version": "v1", "created": "Mon, 5 Jun 2017 15:46:52 GMT"}], "update_date": "2017-06-06", "authors_parsed": [["Kwon", "Joon", ""], ["Perchet", "Vianney", ""], ["Vernade", "Claire", ""]]}, {"id": "1706.01394", "submitter": "Tom Morgan", "authors": "Sebastian Casalaina-Martin, Rafael Frongillo, Tom Morgan, Bo Waggoner", "title": "Multi-Observation Elicitation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study loss functions that measure the accuracy of a prediction based on\nmultiple data points simultaneously. To our knowledge, such loss functions have\nnot been studied before in the area of property elicitation or in machine\nlearning more broadly. As compared to traditional loss functions that take only\na single data point, these multi-observation loss functions can in some cases\ndrastically reduce the dimensionality of the hypothesis required. In\nelicitation, this corresponds to requiring many fewer reports; in empirical\nrisk minimization, it corresponds to algorithms on a hypothesis space of much\nsmaller dimension. We explore some examples of the tradeoff between\ndimensionality and number of observations, give some geometric\ncharacterizations and intuition for relating loss functions and the properties\nthat they elicit, and discuss some implications for both elicitation and\nmachine-learning contexts.\n", "versions": [{"version": "v1", "created": "Mon, 5 Jun 2017 16:07:26 GMT"}], "update_date": "2017-06-06", "authors_parsed": [["Casalaina-Martin", "Sebastian", ""], ["Frongillo", "Rafael", ""], ["Morgan", "Tom", ""], ["Waggoner", "Bo", ""]]}, {"id": "1706.01418", "submitter": "Steve Hanneke", "authors": "Steve Hanneke", "title": "Learning Whenever Learning is Possible: Universal Learning under General\n  Stochastic Processes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.PR math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work initiates a general study of learning and generalization without\nthe i.i.d. assumption, starting from first principles. While the traditional\napproach to statistical learning theory typically relies on standard\nassumptions from probability theory (e.g., i.i.d. or stationary ergodic), in\nthis work we are interested in developing a theory of learning based only on\nthe most fundamental and necessary assumptions implicit in the requirements of\nthe learning problem itself. We specifically study universally consistent\nfunction learning, where the objective is to obtain low long-run average loss\nfor any target function, when the data follow a given stochastic process. We\nare then interested in the question of whether there exist learning rules\nguaranteed to be universally consistent given only the assumption that\nuniversally consistent learning is possible for the given data process. The\nreasoning that motivates this criterion emanates from a kind of optimist's\ndecision theory, and so we refer to such learning rules as being optimistically\nuniversal. We study this question in three natural learning settings:\ninductive, self-adaptive, and online. Remarkably, as our strongest positive\nresult, we find that optimistically universal learning rules do indeed exist in\nthe self-adaptive learning setting. Establishing this fact requires us to\ndevelop new approaches to the design of learning algorithms. Along the way, we\nalso identify concise characterizations of the family of processes under which\nuniversally consistent learning is possible in the inductive and self-adaptive\nsettings. We additionally pose a number of enticing open problems, particularly\nfor the online learning setting.\n", "versions": [{"version": "v1", "created": "Mon, 5 Jun 2017 16:51:36 GMT"}, {"version": "v2", "created": "Tue, 20 Oct 2020 16:25:14 GMT"}], "update_date": "2020-10-21", "authors_parsed": [["Hanneke", "Steve", ""]]}, {"id": "1706.01427", "submitter": "Adam Santoro", "authors": "Adam Santoro, David Raposo, David G.T. Barrett, Mateusz Malinowski,\n  Razvan Pascanu, Peter Battaglia, Timothy Lillicrap", "title": "A simple neural network module for relational reasoning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Relational reasoning is a central component of generally intelligent\nbehavior, but has proven difficult for neural networks to learn. In this paper\nwe describe how to use Relation Networks (RNs) as a simple plug-and-play module\nto solve problems that fundamentally hinge on relational reasoning. We tested\nRN-augmented networks on three tasks: visual question answering using a\nchallenging dataset called CLEVR, on which we achieve state-of-the-art,\nsuper-human performance; text-based question answering using the bAbI suite of\ntasks; and complex reasoning about dynamic physical systems. Then, using a\ncurated dataset called Sort-of-CLEVR we show that powerful convolutional\nnetworks do not have a general capacity to solve relational questions, but can\ngain this capacity when augmented with RNs. Our work shows how a deep learning\narchitecture equipped with an RN module can implicitly discover and learn to\nreason about entities and their relations.\n", "versions": [{"version": "v1", "created": "Mon, 5 Jun 2017 17:17:18 GMT"}], "update_date": "2017-06-06", "authors_parsed": [["Santoro", "Adam", ""], ["Raposo", "David", ""], ["Barrett", "David G. T.", ""], ["Malinowski", "Mateusz", ""], ["Pascanu", "Razvan", ""], ["Battaglia", "Peter", ""], ["Lillicrap", "Timothy", ""]]}, {"id": "1706.01445", "submitter": "Zi Wang", "authors": "Zi Wang and Clement Gehring and Pushmeet Kohli and Stefanie Jegelka", "title": "Batched Large-scale Bayesian Optimization in High-dimensional Spaces", "comments": "Proceedings of the 21st International Conference on Artificial\n  Intelligence and Statistics (AISTATS) 2018, Lanzarote, Spain", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bayesian optimization (BO) has become an effective approach for black-box\nfunction optimization problems when function evaluations are expensive and the\noptimum can be achieved within a relatively small number of queries. However,\nmany cases, such as the ones with high-dimensional inputs, may require a much\nlarger number of observations for optimization. Despite an abundance of\nobservations thanks to parallel experiments, current BO techniques have been\nlimited to merely a few thousand observations. In this paper, we propose\nensemble Bayesian optimization (EBO) to address three current challenges in BO\nsimultaneously: (1) large-scale observations; (2) high dimensional input\nspaces; and (3) selections of batch queries that balance quality and diversity.\nThe key idea of EBO is to operate on an ensemble of additive Gaussian process\nmodels, each of which possesses a randomized strategy to divide and conquer. We\nshow unprecedented, previously impossible results of scaling up BO to tens of\nthousands of observations within minutes of computation.\n", "versions": [{"version": "v1", "created": "Mon, 5 Jun 2017 17:50:44 GMT"}, {"version": "v2", "created": "Tue, 2 Jan 2018 18:10:18 GMT"}, {"version": "v3", "created": "Sat, 6 Jan 2018 17:55:33 GMT"}, {"version": "v4", "created": "Wed, 16 May 2018 01:16:10 GMT"}], "update_date": "2018-08-06", "authors_parsed": [["Wang", "Zi", ""], ["Gehring", "Clement", ""], ["Kohli", "Pushmeet", ""], ["Jegelka", "Stefanie", ""]]}, {"id": "1706.01450", "submitter": "Tong Wang", "authors": "Tong Wang and Xingdi Yuan and Adam Trischler", "title": "A Joint Model for Question Answering and Question Generation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a generative machine comprehension model that learns jointly to\nask and answer questions based on documents. The proposed model uses a\nsequence-to-sequence framework that encodes the document and generates a\nquestion (answer) given an answer (question). Significant improvement in model\nperformance is observed empirically on the SQuAD corpus, confirming our\nhypothesis that the model benefits from jointly learning to perform both tasks.\nWe believe the joint model's novelty offers a new perspective on machine\ncomprehension beyond architectural engineering, and serves as a first step\ntowards autonomous information seeking.\n", "versions": [{"version": "v1", "created": "Mon, 5 Jun 2017 17:58:52 GMT"}], "update_date": "2017-06-06", "authors_parsed": [["Wang", "Tong", ""], ["Yuan", "Xingdi", ""], ["Trischler", "Adam", ""]]}, {"id": "1706.01498", "submitter": "Yizhe Zhang", "authors": "Yizhe Zhang, Changyou Chen, Zhe Gan, Ricardo Henao, Lawrence Carin", "title": "Stochastic Gradient Monomial Gamma Sampler", "comments": "Published on ICML 2017", "journal-ref": "Proceedings of the 34th International Conference on Machine\n  Learning, PMLR 70:3996-4005, 2017", "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent advances in stochastic gradient techniques have made it possible to\nestimate posterior distributions from large datasets via Markov Chain Monte\nCarlo (MCMC). However, when the target posterior is multimodal, mixing\nperformance is often poor. This results in inadequate exploration of the\nposterior distribution. A framework is proposed to improve the sampling\nefficiency of stochastic gradient MCMC, based on Hamiltonian Monte Carlo. A\ngeneralized kinetic function is leveraged, delivering superior stationary\nmixing, especially for multimodal distributions. Techniques are also discussed\nto overcome the practical issues introduced by this generalization. It is shown\nthat the proposed approach is better at exploring complex multimodal posterior\ndistributions, as demonstrated on multiple applications and in comparison with\nother stochastic gradient MCMC methods.\n", "versions": [{"version": "v1", "created": "Mon, 5 Jun 2017 18:48:31 GMT"}, {"version": "v2", "created": "Wed, 10 Jan 2018 19:38:23 GMT"}], "update_date": "2018-01-12", "authors_parsed": [["Zhang", "Yizhe", ""], ["Chen", "Changyou", ""], ["Gan", "Zhe", ""], ["Henao", "Ricardo", ""], ["Carin", "Lawrence", ""]]}, {"id": "1706.01502", "submitter": "Richard Y. Chen", "authors": "Richard Y. Chen, Szymon Sidor, Pieter Abbeel, John Schulman", "title": "UCB Exploration via Q-Ensembles", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We show how an ensemble of $Q^*$-functions can be leveraged for more\neffective exploration in deep reinforcement learning. We build on well\nestablished algorithms from the bandit setting, and adapt them to the\n$Q$-learning setting. We propose an exploration strategy based on\nupper-confidence bounds (UCB). Our experiments show significant gains on the\nAtari benchmark.\n", "versions": [{"version": "v1", "created": "Mon, 5 Jun 2017 19:01:26 GMT"}, {"version": "v2", "created": "Sun, 11 Jun 2017 18:54:53 GMT"}, {"version": "v3", "created": "Tue, 7 Nov 2017 20:45:59 GMT"}], "update_date": "2017-11-09", "authors_parsed": [["Chen", "Richard Y.", ""], ["Sidor", "Szymon", ""], ["Abbeel", "Pieter", ""], ["Schulman", "John", ""]]}, {"id": "1706.01513", "submitter": "Keith Feldman", "authors": "Keith Feldman, Louis Faust, Xian Wu, Chao Huang, and Nitesh V. Chawla", "title": "Beyond Volume: The Impact of Complex Healthcare Data on the Machine\n  Learning Pipeline", "comments": "Healthcare Informatics, Machine Learning, Knowledge Discovery: 20\n  Pages, 1 Figure", "journal-ref": "Towards Integrative Machine Learning and Knowledge Extraction,\n  LNCS vol 10344 (2017) 150-169", "doi": "10.1007/978-3-319-69775-8_9", "report-no": null, "categories": "cs.CY cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  From medical charts to national census, healthcare has traditionally operated\nunder a paper-based paradigm. However, the past decade has marked a long and\narduous transformation bringing healthcare into the digital age. Ranging from\nelectronic health records, to digitized imaging and laboratory reports, to\npublic health datasets, today, healthcare now generates an incredible amount of\ndigital information. Such a wealth of data presents an exciting opportunity for\nintegrated machine learning solutions to address problems across multiple\nfacets of healthcare practice and administration. Unfortunately, the ability to\nderive accurate and informative insights requires more than the ability to\nexecute machine learning models. Rather, a deeper understanding of the data on\nwhich the models are run is imperative for their success. While a significant\neffort has been undertaken to develop models able to process the volume of data\nobtained during the analysis of millions of digitalized patient records, it is\nimportant to remember that volume represents only one aspect of the data. In\nfact, drawing on data from an increasingly diverse set of sources, healthcare\ndata presents an incredibly complex set of attributes that must be accounted\nfor throughout the machine learning pipeline. This chapter focuses on\nhighlighting such challenges, and is broken down into three distinct\ncomponents, each representing a phase of the pipeline. We begin with attributes\nof the data accounted for during preprocessing, then move to considerations\nduring model building, and end with challenges to the interpretation of model\noutput. For each component, we present a discussion around data as it relates\nto the healthcare domain and offer insight into the challenges each may impose\non the efficiency of machine learning techniques.\n", "versions": [{"version": "v1", "created": "Thu, 1 Jun 2017 20:34:41 GMT"}, {"version": "v2", "created": "Fri, 26 Jan 2018 15:05:34 GMT"}], "update_date": "2018-01-29", "authors_parsed": [["Feldman", "Keith", ""], ["Faust", "Louis", ""], ["Wu", "Xian", ""], ["Huang", "Chao", ""], ["Chawla", "Nitesh V.", ""]]}, {"id": "1706.01531", "submitter": "Roghaiyeh Soleimani", "authors": "Roghayeh Soleymani, Eric Granger, Giorgio Fumera", "title": "Progressive Boosting for Class Imbalance", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Pattern recognition applications often suffer from skewed data distributions\nbetween classes, which may vary during operations w.r.t. the design data.\nTwo-class classification systems designed using skewed data tend to recognize\nthe majority class better than the minority class of interest. Several\ndata-level techniques have been proposed to alleviate this issue by up-sampling\nminority samples or under-sampling majority samples. However, some informative\nsamples may be neglected by random under-sampling and adding synthetic positive\nsamples through up-sampling adds to training complexity. In this paper, a new\nensemble learning algorithm called Progressive Boosting (PBoost) is proposed\nthat progressively inserts uncorrelated groups of samples into a Boosting\nprocedure to avoid loss of information while generating a diverse pool of\nclassifiers. Base classifiers in this ensemble are generated from one iteration\nto the next, using subsets from a validation set that grows gradually in size\nand imbalance. Consequently, PBoost is more robust to unknown and variable\nlevels of skew in operational data, and has lower computation complexity than\nBoosting ensembles in literature. In PBoost, a new loss factor is proposed to\navoid bias of performance towards the negative class. Using this loss factor,\nthe weight update of samples and classifier contribution in final predictions\nare set based on the ability to recognize both classes. Using the proposed loss\nfactor instead of standard accuracy can avoid biasing performance in any\nBoosting ensemble. The proposed approach was validated and compared using\nsynthetic data, videos from the FIA dataset that emulates face\nre-identification applications, and KEEL collection of datasets. Results show\nthat PBoost can outperform state of the art techniques in terms of both\naccuracy and complexity over different levels of imbalance and overlap between\nclasses.\n", "versions": [{"version": "v1", "created": "Mon, 5 Jun 2017 20:32:55 GMT"}], "update_date": "2019-12-02", "authors_parsed": [["Soleymani", "Roghayeh", ""], ["Granger", "Eric", ""], ["Fumera", "Giorgio", ""]]}, {"id": "1706.01556", "submitter": "Yifan Peng", "authors": "Yifan Peng and Zhiyong Lu", "title": "Deep learning for extracting protein-protein interactions from\n  biomedical literature", "comments": "Accepted for publication in Proceedings of the 2017 Workshop on\n  Biomedical Natural Language Processing, 10 pages, 2 figures, 6 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG q-bio.QM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  State-of-the-art methods for protein-protein interaction (PPI) extraction are\nprimarily feature-based or kernel-based by leveraging lexical and syntactic\ninformation. But how to incorporate such knowledge in the recent deep learning\nmethods remains an open question. In this paper, we propose a multichannel\ndependency-based convolutional neural network model (McDepCNN). It applies one\nchannel to the embedding vector of each word in the sentence, and another\nchannel to the embedding vector of the head of the corresponding word.\nTherefore, the model can use richer information obtained from different\nchannels. Experiments on two public benchmarking datasets, AIMed and BioInfer,\ndemonstrate that McDepCNN compares favorably to the state-of-the-art\nrich-feature and single-kernel based methods. In addition, McDepCNN achieves\n24.4% relative improvement in F1-score over the state-of-the-art methods on\ncross-corpus evaluation and 12% improvement in F1-score over kernel-based\nmethods on \"difficult\" instances. These results suggest that McDepCNN\ngeneralizes more easily over different corpora, and is capable of capturing\nlong distance features in the sentences.\n", "versions": [{"version": "v1", "created": "Mon, 5 Jun 2017 23:09:06 GMT"}, {"version": "v2", "created": "Wed, 7 Jun 2017 00:28:21 GMT"}], "update_date": "2017-06-08", "authors_parsed": [["Peng", "Yifan", ""], ["Lu", "Zhiyong", ""]]}, {"id": "1706.01566", "submitter": "Jesse Dodge", "authors": "Jesse Dodge, Kevin Jamieson, Noah A. Smith", "title": "Open Loop Hyperparameter Optimization and Determinantal Point Processes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Driven by the need for parallelizable hyperparameter optimization methods,\nthis paper studies \\emph{open loop} search methods: sequences that are\npredetermined and can be generated before a single configuration is evaluated.\nExamples include grid search, uniform random search, low discrepancy sequences,\nand other sampling distributions. In particular, we propose the use of\n$k$-determinantal point processes in hyperparameter optimization via random\nsearch. Compared to conventional uniform random search where hyperparameter\nsettings are sampled independently, a $k$-DPP promotes diversity. We describe\nan approach that transforms hyperparameter search spaces for efficient use with\na $k$-DPP. In addition, we introduce a novel Metropolis-Hastings algorithm\nwhich can sample from $k$-DPPs defined over any space from which uniform\nsamples can be drawn, including spaces with a mixture of discrete and\ncontinuous dimensions or tree structure. Our experiments show significant\nbenefits in realistic scenarios with a limited budget for training supervised\nlearners, whether in serial or parallel.\n", "versions": [{"version": "v1", "created": "Tue, 6 Jun 2017 00:14:05 GMT"}, {"version": "v2", "created": "Mon, 19 Jun 2017 21:29:38 GMT"}, {"version": "v3", "created": "Wed, 14 Feb 2018 02:00:13 GMT"}, {"version": "v4", "created": "Thu, 9 May 2019 01:32:23 GMT"}], "update_date": "2019-05-10", "authors_parsed": [["Dodge", "Jesse", ""], ["Jamieson", "Kevin", ""], ["Smith", "Noah A.", ""]]}, {"id": "1706.01581", "submitter": "Azad Naik", "authors": "Azad Naik and Huzefa Rangwala", "title": "Embedding Feature Selection for Large-scale Hierarchical Classification", "comments": "IEEE International Conference on Big Data (IEEE BigData 2016)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Large-scale Hierarchical Classification (HC) involves datasets consisting of\nthousands of classes and millions of training instances with high-dimensional\nfeatures posing several big data challenges. Feature selection that aims to\nselect the subset of discriminant features is an effective strategy to deal\nwith large-scale HC problem. It speeds up the training process, reduces the\nprediction time and minimizes the memory requirements by compressing the total\nsize of learned model weight vectors. Majority of the studies have also shown\nfeature selection to be competent and successful in improving the\nclassification accuracy by removing irrelevant features. In this work, we\ninvestigate various filter-based feature selection methods for dimensionality\nreduction to solve the large-scale HC problem. Our experimental evaluation on\ntext and image datasets with varying distribution of features, classes and\ninstances shows upto 3x order of speed-up on massive datasets and upto 45% less\nmemory requirements for storing the weight vectors of learned model without any\nsignificant loss (improvement for some datasets) in the classification\naccuracy. Source Code: https://cs.gmu.edu/~mlbio/featureselection.\n", "versions": [{"version": "v1", "created": "Tue, 6 Jun 2017 01:56:51 GMT"}], "update_date": "2017-06-07", "authors_parsed": [["Naik", "Azad", ""], ["Rangwala", "Huzefa", ""]]}, {"id": "1706.01583", "submitter": "Azad Naik", "authors": "Azad Naik, Anveshi Charuvaka and Huzefa Rangwala", "title": "Classifying Documents within Multiple Hierarchical Datasets using\n  Multi-Task Learning", "comments": "IEEE International Conference on Tools with Artificial Intelligence\n  (ICTAI), 2013", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Multi-task learning (MTL) is a supervised learning paradigm in which the\nprediction models for several related tasks are learned jointly to achieve\nbetter generalization performance. When there are only a few training examples\nper task, MTL considerably outperforms the traditional Single task learning\n(STL) in terms of prediction accuracy. In this work we develop an MTL based\napproach for classifying documents that are archived within dual concept\nhierarchies, namely, DMOZ and Wikipedia. We solve the multi-class\nclassification problem by defining one-versus-rest binary classification tasks\nfor each of the different classes across the two hierarchical datasets. Instead\nof learning a linear discriminant for each of the different tasks\nindependently, we use a MTL approach with relationships between the different\ntasks across the datasets established using the non-parametric, lazy, nearest\nneighbor approach. We also develop and evaluate a transfer learning (TL)\napproach and compare the MTL (and TL) methods against the standard single task\nlearning and semi-supervised learning approaches. Our empirical results\ndemonstrate the strength of our developed methods that show an improvement\nespecially when there are fewer number of training examples per classification\ntask.\n", "versions": [{"version": "v1", "created": "Tue, 6 Jun 2017 02:17:40 GMT"}], "update_date": "2017-06-07", "authors_parsed": [["Naik", "Azad", ""], ["Charuvaka", "Anveshi", ""], ["Rangwala", "Huzefa", ""]]}, {"id": "1706.01596", "submitter": "Abbas Mehrabian", "authors": "Hassan Ashtiani, Shai Ben-David and Abbas Mehrabian", "title": "Sample-Efficient Learning of Mixtures", "comments": "A bug from the previous version, which appeared in AAAI 2018\n  proceedings, is fixed. 18 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider PAC learning of probability distributions (a.k.a. density\nestimation), where we are given an i.i.d. sample generated from an unknown\ntarget distribution, and want to output a distribution that is close to the\ntarget in total variation distance. Let $\\mathcal F$ be an arbitrary class of\nprobability distributions, and let $\\mathcal{F}^k$ denote the class of\n$k$-mixtures of elements of $\\mathcal F$. Assuming the existence of a method\nfor learning $\\mathcal F$ with sample complexity $m_{\\mathcal{F}}(\\epsilon)$,\nwe provide a method for learning $\\mathcal F^k$ with sample complexity\n$O({k\\log k \\cdot m_{\\mathcal F}(\\epsilon) }/{\\epsilon^{2}})$. Our mixture\nlearning algorithm has the property that, if the $\\mathcal F$-learner is\nproper/agnostic, then the $\\mathcal F^k$-learner would be proper/agnostic as\nwell.\n  This general result enables us to improve the best known sample complexity\nupper bounds for a variety of important mixture classes. First, we show that\nthe class of mixtures of $k$ axis-aligned Gaussians in $\\mathbb{R}^d$ is\nPAC-learnable in the agnostic setting with $\\widetilde{O}({kd}/{\\epsilon ^ 4})$\nsamples, which is tight in $k$ and $d$ up to logarithmic factors. Second, we\nshow that the class of mixtures of $k$ Gaussians in $\\mathbb{R}^d$ is\nPAC-learnable in the agnostic setting with sample complexity\n$\\widetilde{O}({kd^2}/{\\epsilon ^ 4})$, which improves the previous known\nbounds of $\\widetilde{O}({k^3d^2}/{\\epsilon ^ 4})$ and\n$\\widetilde{O}(k^4d^4/\\epsilon ^ 2)$ in its dependence on $k$ and $d$. Finally,\nwe show that the class of mixtures of $k$ log-concave distributions over\n$\\mathbb{R}^d$ is PAC-learnable using\n$\\widetilde{O}(d^{(d+5)/2}\\epsilon^{-(d+9)/2}k)$ samples.\n", "versions": [{"version": "v1", "created": "Tue, 6 Jun 2017 03:47:28 GMT"}, {"version": "v2", "created": "Sun, 17 Sep 2017 17:12:36 GMT"}, {"version": "v3", "created": "Sun, 3 Jun 2018 18:58:10 GMT"}], "update_date": "2018-06-05", "authors_parsed": [["Ashtiani", "Hassan", ""], ["Ben-David", "Shai", ""], ["Mehrabian", "Abbas", ""]]}, {"id": "1706.01604", "submitter": "Manolis Tsakiris", "authors": "Manolis C. Tsakiris and Rene Vidal", "title": "Hyperplane Clustering Via Dual Principal Component Pursuit", "comments": null, "journal-ref": "Proceedings of the 34th International Conference on Machine\n  Learning, PMLR 70:3472-3481, 2017", "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We extend the theoretical analysis of a recently proposed single subspace\nlearning algorithm, called Dual Principal Component Pursuit (DPCP), to the case\nwhere the data are drawn from of a union of hyperplanes. To gain insight into\nthe properties of the $\\ell_1$ non-convex problem associated with DPCP, we\ndevelop a geometric analysis of a closely related continuous optimization\nproblem. Then transferring this analysis to the discrete problem, our results\nstate that as long as the hyperplanes are sufficiently separated, the dominant\nhyperplane is sufficiently dominant and the points are uniformly distributed\ninside the associated hyperplanes, then the non-convex DPCP problem has a\nunique global solution, equal to the normal vector of the dominant hyperplane.\nThis suggests the correctness of a sequential hyperplane learning algorithm\nbased on DPCP. A thorough experimental evaluation reveals that hyperplane\nlearning schemes based on DPCP dramatically improve over the state-of-the-art\nmethods for the case of synthetic data, while are competitive to the\nstate-of-the-art in the case of 3D plane clustering for Kinect data.\n", "versions": [{"version": "v1", "created": "Tue, 6 Jun 2017 04:27:24 GMT"}, {"version": "v2", "created": "Mon, 19 Jun 2017 19:20:23 GMT"}], "update_date": "2019-07-19", "authors_parsed": [["Tsakiris", "Manolis C.", ""], ["Vidal", "Rene", ""]]}, {"id": "1706.01606", "submitter": "Xiang Zhang", "authors": "Xiang Zhang, Lina Yao, Chaoran Huang, Tao Gu, Zheng Yang and Yunhao\n  Liu", "title": "DeepKey: An EEG and Gait Based Dual-Authentication System", "comments": "22 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Biometric authentication involves various technologies to identify\nindividuals by exploiting their unique, measurable physiological and behavioral\ncharacteristics. However, traditional biometric authentication systems (e.g.,\nface recognition, iris, retina, voice, and fingerprint) are facing an\nincreasing risk of being tricked by biometric tools such as anti-surveillance\nmasks, contact lenses, vocoder, or fingerprint films. In this paper, we design\na multimodal biometric authentication system named Deepkey, which uses both\nElectroencephalography (EEG) and gait signals to better protect against such\nrisk. Deepkey consists of two key components: an Invalid ID Filter Model to\nblock unauthorized subjects and an identification model based on\nattention-based Recurrent Neural Network (RNN) to identify a subject`s EEG IDs\nand gait IDs in parallel. The subject can only be granted access while all the\ncomponents produce consistent evidence to match the user`s proclaimed identity.\nWe implement Deepkey with a live deployment in our university and conduct\nextensive empirical experiments to study its technical feasibility in practice.\nDeepKey achieves the False Acceptance Rate (FAR) and the False Rejection Rate\n(FRR) of 0 and 1.0%, respectively. The preliminary results demonstrate that\nDeepkey is feasible, show consistent superior performance compared to a set of\nmethods, and has the potential to be applied to the authentication deployment\nin real world settings.\n", "versions": [{"version": "v1", "created": "Tue, 6 Jun 2017 04:54:21 GMT"}, {"version": "v2", "created": "Mon, 27 May 2019 08:46:03 GMT"}], "update_date": "2019-05-28", "authors_parsed": [["Zhang", "Xiang", ""], ["Yao", "Lina", ""], ["Huang", "Chaoran", ""], ["Gu", "Tao", ""], ["Yang", "Zheng", ""], ["Liu", "Yunhao", ""]]}, {"id": "1706.01643", "submitter": "Bowen Liu", "authors": "Bowen Liu, Bharath Ramsundar, Prasad Kawthekar, Jade Shi, Joseph\n  Gomes, Quang Luu Nguyen, Stephen Ho, Jack Sloane, Paul Wender, Vijay Pande", "title": "Retrosynthetic reaction prediction using neural sequence-to-sequence\n  models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG q-bio.QM stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe a fully data driven model that learns to perform a retrosynthetic\nreaction prediction task, which is treated as a sequence-to-sequence mapping\nproblem. The end-to-end trained model has an encoder-decoder architecture that\nconsists of two recurrent neural networks, which has previously shown great\nsuccess in solving other sequence-to-sequence prediction tasks such as machine\ntranslation. The model is trained on 50,000 experimental reaction examples from\nthe United States patent literature, which span 10 broad reaction types that\nare commonly used by medicinal chemists. We find that our model performs\ncomparably with a rule-based expert system baseline model, and also overcomes\ncertain limitations associated with rule-based expert systems and with any\nmachine learning approach that contains a rule-based expert system component.\nOur model provides an important first step towards solving the challenging\nproblem of computational retrosynthetic analysis.\n", "versions": [{"version": "v1", "created": "Tue, 6 Jun 2017 07:50:54 GMT"}], "update_date": "2017-06-07", "authors_parsed": [["Liu", "Bowen", ""], ["Ramsundar", "Bharath", ""], ["Kawthekar", "Prasad", ""], ["Shi", "Jade", ""], ["Gomes", "Joseph", ""], ["Nguyen", "Quang Luu", ""], ["Ho", "Stephen", ""], ["Sloane", "Jack", ""], ["Wender", "Paul", ""], ["Pande", "Vijay", ""]]}, {"id": "1706.01663", "submitter": "Alexis Linard", "authors": "Alexis Linard, Rick Smetsers, Frits Vaandrager, Umar Waqas, Joost van\n  Pinxten, Sicco Verwer", "title": "Learning Pairwise Disjoint Simple Languages from Positive Examples", "comments": "This paper has been accepted at the Learning and Automata (LearnAut)\n  Workshop, LICS 2017 (Reykjavik, Iceland)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.FL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A classical problem in grammatical inference is to identify a deterministic\nfinite automaton (DFA) from a set of positive and negative examples. In this\npaper, we address the related - yet seemingly novel - problem of identifying a\nset of DFAs from examples that belong to different unknown simple regular\nlanguages. We propose two methods based on compression for clustering the\nobserved positive examples. We apply our methods to a set of print jobs\nsubmitted to large industrial printers.\n", "versions": [{"version": "v1", "created": "Tue, 6 Jun 2017 09:03:17 GMT"}], "update_date": "2017-06-07", "authors_parsed": [["Linard", "Alexis", ""], ["Smetsers", "Rick", ""], ["Vaandrager", "Frits", ""], ["Waqas", "Umar", ""], ["van Pinxten", "Joost", ""], ["Verwer", "Sicco", ""]]}, {"id": "1706.01686", "submitter": "Yossi Arjevani", "authors": "Yossi Arjevani", "title": "Limitations on Variance-Reduction and Acceleration Schemes for Finite\n  Sum Optimization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the conditions under which one is able to efficiently apply\nvariance-reduction and acceleration schemes on finite sum optimization\nproblems. First, we show that, perhaps surprisingly, the finite sum structure\nby itself, is not sufficient for obtaining a complexity bound of\n$\\tilde{\\cO}((n+L/\\mu)\\ln(1/\\epsilon))$ for $L$-smooth and $\\mu$-strongly\nconvex individual functions - one must also know which individual function is\nbeing referred to by the oracle at each iteration. Next, we show that for a\nbroad class of first-order and coordinate-descent finite sum algorithms\n(including, e.g., SDCA, SVRG, SAG), it is not possible to get an `accelerated'\ncomplexity bound of $\\tilde{\\cO}((n+\\sqrt{n L/\\mu})\\ln(1/\\epsilon))$, unless\nthe strong convexity parameter is given explicitly. Lastly, we show that when\nthis class of algorithms is used for minimizing $L$-smooth and convex finite\nsums, the optimal complexity bound is $\\tilde{\\cO}(n+L/\\epsilon)$, assuming\nthat (on average) the same update rule is used in every iteration, and\n$\\tilde{\\cO}(n+\\sqrt{nL/\\epsilon})$, otherwise.\n", "versions": [{"version": "v1", "created": "Tue, 6 Jun 2017 10:35:33 GMT"}, {"version": "v2", "created": "Wed, 6 Dec 2017 19:16:39 GMT"}], "update_date": "2017-12-08", "authors_parsed": [["Arjevani", "Yossi", ""]]}, {"id": "1706.01724", "submitter": "Mingyuan Zhou", "authors": "Yulai Cong, Bo Chen, Hongwei Liu, Mingyuan Zhou", "title": "Deep Latent Dirichlet Allocation with Topic-Layer-Adaptive Stochastic\n  Gradient Riemannian MCMC", "comments": "Appearing in ICML 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is challenging to develop stochastic gradient based scalable inference for\ndeep discrete latent variable models (LVMs), due to the difficulties in not\nonly computing the gradients, but also adapting the step sizes to different\nlatent factors and hidden layers. For the Poisson gamma belief network (PGBN),\na recently proposed deep discrete LVM, we derive an alternative representation\nthat is referred to as deep latent Dirichlet allocation (DLDA). Exploiting data\naugmentation and marginalization techniques, we derive a block-diagonal Fisher\ninformation matrix and its inverse for the simplex-constrained global model\nparameters of DLDA. Exploiting that Fisher information matrix with stochastic\ngradient MCMC, we present topic-layer-adaptive stochastic gradient Riemannian\n(TLASGR) MCMC that jointly learns simplex-constrained global parameters across\nall layers and topics, with topic and layer specific learning rates.\nState-of-the-art results are demonstrated on big data sets.\n", "versions": [{"version": "v1", "created": "Tue, 6 Jun 2017 12:15:42 GMT"}], "update_date": "2017-06-07", "authors_parsed": [["Cong", "Yulai", ""], ["Chen", "Bo", ""], ["Liu", "Hongwei", ""], ["Zhou", "Mingyuan", ""]]}, {"id": "1706.01750", "submitter": "Ofir Lindenbaum", "authors": "Ofir Lindenbaum, Yuri Bregman, Neta Rabin, Amir Averbuch", "title": "Multi-View Kernels for Low-Dimensional Modeling of Seismic Events", "comments": null, "journal-ref": null, "doi": "10.1109/TGRS.2018.2797537", "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The problem of learning from seismic recordings has been studied for years.\nThere is a growing interest in developing automatic mechanisms for identifying\nthe properties of a seismic event. One main motivation is the ability have a\nreliable identification of man-made explosions. The availability of multiple\nhigh-dimensional observations has increased the use of machine learning\ntechniques in a variety of fields. In this work, we propose to use a\nkernel-fusion based dimensionality reduction framework for generating\nmeaningful seismic representations from raw data. The proposed method is tested\non 2023 events that were recorded in Israel and in Jordan. The method achieves\npromising results in classification of event type as well as in estimating the\nlocation of the event. The proposed fusion and dimensionality reduction tools\nmay be applied to other types of geophysical data.\n", "versions": [{"version": "v1", "created": "Tue, 6 Jun 2017 13:27:28 GMT"}], "update_date": "2018-07-04", "authors_parsed": [["Lindenbaum", "Ofir", ""], ["Bregman", "Yuri", ""], ["Rabin", "Neta", ""], ["Averbuch", "Amir", ""]]}, {"id": "1706.01763", "submitter": "Yanjun  Qi Dr.", "authors": "Andrew Norton and Yanjun Qi", "title": "Adversarial-Playground: A Visualization Suite for Adversarial Sample\n  Generation", "comments": "8 pages; 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With growing interest in adversarial machine learning, it is important for\nmachine learning practitioners and users to understand how their models may be\nattacked. We propose a web-based visualization tool, Adversarial-Playground, to\ndemonstrate the efficacy of common adversarial methods against a deep neural\nnetwork (DNN) model, built on top of the TensorFlow library.\nAdversarial-Playground provides users an efficient and effective experience in\nexploring techniques generating adversarial examples, which are inputs crafted\nby an adversary to fool a machine learning system. To enable\nAdversarial-Playground to generate quick and accurate responses for users, we\nuse two primary tactics: (1) We propose a faster variant of the\nstate-of-the-art Jacobian saliency map approach that maintains a comparable\nevasion rate. (2) Our visualization does not transmit the generated adversarial\nimages to the client, but rather only the matrix describing the sample and the\nvector representing classification likelihoods.\n  The source code along with the data from all of our experiments are available\nat \\url{https://github.com/QData/AdversarialDNN-Playground}.\n", "versions": [{"version": "v1", "created": "Tue, 6 Jun 2017 13:43:11 GMT"}, {"version": "v2", "created": "Fri, 16 Jun 2017 16:38:09 GMT"}], "update_date": "2017-06-19", "authors_parsed": [["Norton", "Andrew", ""], ["Qi", "Yanjun", ""]]}, {"id": "1706.01777", "submitter": "Lantian Li Mr.", "authors": "Dong Wang and Lantian Li and Ying Shi and Yixiang Chen and Zhiyuan\n  Tang", "title": "Deep Factorization for Speech Signal", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Speech signals are complex intermingling of various informative factors, and\nthis information blending makes decoding any of the individual factors\nextremely difficult. A natural idea is to factorize each speech frame into\nindependent factors, though it turns out to be even more difficult than\ndecoding each individual factor. A major encumbrance is that the speaker trait,\na major factor in speech signals, has been suspected to be a long-term\ndistributional pattern and so not identifiable at the frame level. In this\npaper, we demonstrated that the speaker factor is also a short-time spectral\npattern and can be largely identified with just a few frames using a simple\ndeep neural network (DNN). This discovery motivated a cascade deep\nfactorization (CDF) framework that infers speech factors in a sequential way,\nand factors previously inferred are used as conditional variables when\ninferring other factors. Our experiment on an automatic emotion recognition\n(AER) task demonstrated that this approach can effectively factorize speech\nsignals, and using these factors, the original speech spectrum can be recovered\nwith high accuracy. This factorization and reconstruction approach provides a\nnovel tool for many speech processing tasks.\n", "versions": [{"version": "v1", "created": "Mon, 5 Jun 2017 15:02:39 GMT"}, {"version": "v2", "created": "Sun, 25 Jun 2017 10:10:35 GMT"}], "update_date": "2017-06-27", "authors_parsed": [["Wang", "Dong", ""], ["Li", "Lantian", ""], ["Shi", "Ying", ""], ["Chen", "Yixiang", ""], ["Tang", "Zhiyuan", ""]]}, {"id": "1706.01824", "submitter": "Peng Yang", "authors": "Peng Yang, Peilin Zhao, Xin Gao", "title": "Robust Online Multi-Task Learning with Correlative and Personalized\n  Structures", "comments": null, "journal-ref": null, "doi": "10.1109/TKDE.2017.2703106", "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multi-Task Learning (MTL) can enhance a classifier's generalization\nperformance by learning multiple related tasks simultaneously. Conventional MTL\nworks under the offline or batch setting, and suffers from expensive training\ncost and poor scalability. To address such inefficiency issues, online learning\ntechniques have been applied to solve MTL problems. However, most existing\nalgorithms of online MTL constrain task relatedness into a presumed structure\nvia a single weight matrix, which is a strict restriction that does not always\nhold in practice. In this paper, we propose a robust online MTL framework that\novercomes this restriction by decomposing the weight matrix into two\ncomponents: the first one captures the low-rank common structure among tasks\nvia a nuclear norm and the second one identifies the personalized patterns of\noutlier tasks via a group lasso. Theoretical analysis shows the proposed\nalgorithm can achieve a sub-linear regret with respect to the best linear model\nin hindsight. Even though the above framework achieves good performance, the\nnuclear norm that simply adds all nonzero singular values together may not be a\ngood low-rank approximation. To improve the results, we use a log-determinant\nfunction as a non-convex rank approximation. The gradient scheme is applied to\noptimize log-determinant function and can obtain a closed-form solution for\nthis refined problem. Experimental results on a number of real-world\napplications verify the efficacy of our method.\n", "versions": [{"version": "v1", "created": "Tue, 6 Jun 2017 15:53:26 GMT"}], "update_date": "2017-06-28", "authors_parsed": [["Yang", "Peng", ""], ["Zhao", "Peilin", ""], ["Gao", "Xin", ""]]}, {"id": "1706.01826", "submitter": "Peter Sadowski", "authors": "Peter Sadowski, Balint Radics, Ananya, Yasunori Yamazaki, Pierre Baldi", "title": "Efficient Antihydrogen Detection in Antimatter Physics by Deep Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.ins-det cs.LG hep-ex", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Antihydrogen is at the forefront of antimatter research at the CERN\nAntiproton Decelerator. Experiments aiming to test the fundamental CPT symmetry\nand antigravity effects require the efficient detection of antihydrogen\nannihilation events, which is performed using highly granular tracking\ndetectors installed around an antimatter trap. Improving the efficiency of the\nantihydrogen annihilation detection plays a central role in the final\nsensitivity of the experiments. We propose deep learning as a novel technique\nto analyze antihydrogen annihilation data, and compare its performance with a\ntraditional track and vertex reconstruction method. We report that the deep\nlearning approach yields significant improvement, tripling event coverage while\nsimultaneously improving performance by over 5% in terms of Area Under Curve\n(AUC).\n", "versions": [{"version": "v1", "created": "Tue, 6 Jun 2017 16:00:36 GMT"}], "update_date": "2017-06-07", "authors_parsed": [["Sadowski", "Peter", ""], ["Radics", "Balint", ""], ["Ananya", "", ""], ["Yamazaki", "Yasunori", ""], ["Baldi", "Pierre", ""]]}, {"id": "1706.01833", "submitter": "Yaxiong Zeng", "authors": "Yaxiong Zeng, Diego Klabjan", "title": "Online Adaptive Machine Learning Based Algorithm for Implied Volatility\n  Surface Modeling", "comments": "34 Pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG q-fin.CP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we design a machine learning based method, online adaptive\nprimal support vector regression (SVR), to model the implied volatility surface\n(IVS). The algorithm proposed is the first derivation and implementation of an\nonline primal kernel SVR. It features enhancements that allow efficient online\nadaptive learning by embedding the idea of local fitness and budget maintenance\nto dynamically update support vectors upon pattern drifts. For algorithm\nacceleration, we implement its most computationally intensive parts in a Field\nProgrammable Gate Arrays hardware, where a 132x speedup over CPU is achieved\nduring online prediction. Using intraday tick data from the E-mini S&P 500\noptions market, we show that the Gaussian kernel outperforms the linear kernel\nin regulating the size of support vectors, and that our empirical IVS algorithm\nbeats two competing online methods with regards to model complexity and\nregression errors (the mean absolute percentage error of our algorithm is up to\n13%). Best results are obtained at the center of the IVS grid due to its larger\nnumber of adjacent support vectors than the edges of the grid. Sensitivity\nanalysis is also presented to demonstrate how hyper parameters affect the error\nrates and model complexity.\n", "versions": [{"version": "v1", "created": "Tue, 6 Jun 2017 16:09:57 GMT"}, {"version": "v2", "created": "Thu, 7 Jun 2018 15:28:49 GMT"}], "update_date": "2018-06-08", "authors_parsed": [["Zeng", "Yaxiong", ""], ["Klabjan", "Diego", ""]]}, {"id": "1706.01860", "submitter": "Jundong Li", "authors": "Jundong Li, Harsh Dani, Xia Hu, Jiliang Tang, Yi Chang, Huan Liu", "title": "Attributed Network Embedding for Learning in a Dynamic Environment", "comments": "10 pages", "journal-ref": null, "doi": "10.1145/3132847.3132919", "report-no": null, "categories": "cs.SI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Network embedding leverages the node proximity manifested to learn a\nlow-dimensional node vector representation for each node in the network. The\nlearned embeddings could advance various learning tasks such as node\nclassification, network clustering, and link prediction. Most, if not all, of\nthe existing works, are overwhelmingly performed in the context of plain and\nstatic networks. Nonetheless, in reality, network structure often evolves over\ntime with addition/deletion of links and nodes. Also, a vast majority of\nreal-world networks are associated with a rich set of node attributes, and\ntheir attribute values are also naturally changing, with the emerging of new\ncontent patterns and the fading of old content patterns. These changing\ncharacteristics motivate us to seek an effective embedding representation to\ncapture network and attribute evolving patterns, which is of fundamental\nimportance for learning in a dynamic environment. To our best knowledge, we are\nthe first to tackle this problem with the following two challenges: (1) the\ninherently correlated network and node attributes could be noisy and\nincomplete, it necessitates a robust consensus representation to capture their\nindividual properties and correlations; (2) the embedding learning needs to be\nperformed in an online fashion to adapt to the changes accordingly. In this\npaper, we tackle this problem by proposing a novel dynamic attributed network\nembedding framework - DANE. In particular, DANE first provides an offline\nmethod for a consensus embedding and then leverages matrix perturbation theory\nto maintain the freshness of the end embedding results in an online manner. We\nperform extensive experiments on both synthetic and real attributed networks to\ncorroborate the effectiveness and efficiency of the proposed framework.\n", "versions": [{"version": "v1", "created": "Tue, 6 Jun 2017 17:18:38 GMT"}, {"version": "v2", "created": "Sun, 26 Aug 2018 20:50:58 GMT"}], "update_date": "2018-08-28", "authors_parsed": [["Li", "Jundong", ""], ["Dani", "Harsh", ""], ["Hu", "Xia", ""], ["Tang", "Jiliang", ""], ["Chang", "Yi", ""], ["Liu", "Huan", ""]]}, {"id": "1706.01876", "submitter": "Ratha Pech", "authors": "Ratha Pech, Dong Hao, Yan-Li Lee, Maryna Po, Tao Zhou", "title": "A generalized method toward drug-target interaction prediction via\n  low-rank matrix projection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CE physics.bio-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Drug-target interaction (DTI) prediction plays a very important role in drug\ndevelopment and drug discovery. Biochemical experiments or \\textit{in vitro}\nmethods are very expensive, laborious and time-consuming. Therefore, \\textit{in\nsilico} approaches including docking simulation and machine learning have been\nproposed to solve this problem. In particular, machine learning approaches have\nattracted increasing attentions recently. However, in addition to the known\ndrug-target interactions, most of the machine learning methods require extra\ncharacteristic information such as chemical structures, genome sequences,\nbinding types and so on. Whenever such information is not available, they may\nperform poor. Very recently, the similarity-based link prediction methods were\nextended to bipartite networks, which can be applied to solve the DTI\nprediction problem by using topological information only. In this work, we\npropose a method based on low-rank matrix projection to solve the DTI\nprediction problem. On one hand, when there is no extra characteristic\ninformation of drugs or targets, the proposed method utilizes only the known\ninteractions. On the other hand, the proposed method can also utilize the extra\ncharacteristic information when it is available and the performances will be\nremarkably improved. Moreover, the proposed method can predict the interactions\nassociated with new drugs or targets of which we know nothing about their\nassociated interactions, but only some characteristic information. We compare\nthe proposed method with ten baseline methods, e.g., six similarity-based\nmethods that utilize only the known interactions and four methods that utilize\nthe extra characteristic information. The datasets and codes implementing the\nsimulations are available at https://github.com/rathapech/DTI_LMP.\n", "versions": [{"version": "v1", "created": "Tue, 6 Jun 2017 08:39:28 GMT"}, {"version": "v2", "created": "Thu, 3 May 2018 03:44:33 GMT"}], "update_date": "2018-05-04", "authors_parsed": [["Pech", "Ratha", ""], ["Hao", "Dong", ""], ["Lee", "Yan-Li", ""], ["Po", "Maryna", ""], ["Zhou", "Tao", ""]]}, {"id": "1706.01905", "submitter": "Matthias Plappert", "authors": "Matthias Plappert, Rein Houthooft, Prafulla Dhariwal, Szymon Sidor,\n  Richard Y. Chen, Xi Chen, Tamim Asfour, Pieter Abbeel, Marcin Andrychowicz", "title": "Parameter Space Noise for Exploration", "comments": "Updated to camera-ready ICLR submission", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.NE cs.RO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep reinforcement learning (RL) methods generally engage in exploratory\nbehavior through noise injection in the action space. An alternative is to add\nnoise directly to the agent's parameters, which can lead to more consistent\nexploration and a richer set of behaviors. Methods such as evolutionary\nstrategies use parameter perturbations, but discard all temporal structure in\nthe process and require significantly more samples. Combining parameter noise\nwith traditional RL methods allows to combine the best of both worlds. We\ndemonstrate that both off- and on-policy methods benefit from this approach\nthrough experimental comparison of DQN, DDPG, and TRPO on high-dimensional\ndiscrete action environments as well as continuous control tasks. Our results\nshow that RL with parameter noise learns more efficiently than traditional RL\nwith action space noise and evolutionary strategies individually.\n", "versions": [{"version": "v1", "created": "Tue, 6 Jun 2017 18:09:29 GMT"}, {"version": "v2", "created": "Wed, 31 Jan 2018 09:05:10 GMT"}], "update_date": "2018-02-01", "authors_parsed": [["Plappert", "Matthias", ""], ["Houthooft", "Rein", ""], ["Dhariwal", "Prafulla", ""], ["Sidor", "Szymon", ""], ["Chen", "Richard Y.", ""], ["Chen", "Xi", ""], ["Asfour", "Tamim", ""], ["Abbeel", "Pieter", ""], ["Andrychowicz", "Marcin", ""]]}, {"id": "1706.01983", "submitter": "Mrinal Haloi", "authors": "Mrinal Haloi", "title": "Deep Learning: Generalization Requires Deep Compositional Feature Space\n  Design", "comments": "fig added, with minor typo corrections", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generalization error defines the discriminability and the representation\npower of a deep model. In this work, we claim that feature space design using\ndeep compositional function plays a significant role in generalization along\nwith explicit and implicit regularizations. Our claims are being established\nwith several image classification experiments. We show that the information\nloss due to convolution and max pooling can be marginalized with the\ncompositional design, improving generalization performance. Also, we will show\nthat learning rate decay acts as an implicit regularizer in deep model\ntraining.\n", "versions": [{"version": "v1", "created": "Tue, 6 Jun 2017 21:10:07 GMT"}, {"version": "v2", "created": "Sat, 8 Jul 2017 22:31:36 GMT"}], "update_date": "2017-07-11", "authors_parsed": [["Haloi", "Mrinal", ""]]}, {"id": "1706.02047", "submitter": "Sharath Adavanne", "authors": "Sharath Adavanne, Konstantinos Drossos, Emre \\c{C}ak{\\i}r, Tuomas\n  Virtanen", "title": "Stacked Convolutional and Recurrent Neural Networks for Bird Audio\n  Detection", "comments": "Accepted for European Signal Processing Conference 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper studies the detection of bird calls in audio segments using\nstacked convolutional and recurrent neural networks. Data augmentation by\nblocks mixing and domain adaptation using a novel method of test mixing are\nproposed and evaluated in regard to making the method robust to unseen data.\nThe contributions of two kinds of acoustic features (dominant frequency and log\nmel-band energy) and their combinations are studied in the context of bird\naudio detection. Our best achieved AUC measure on five cross-validations of the\ndevelopment data is 95.5% and 88.1% on the unseen evaluation data.\n", "versions": [{"version": "v1", "created": "Wed, 7 Jun 2017 05:09:41 GMT"}], "update_date": "2017-06-08", "authors_parsed": [["Adavanne", "Sharath", ""], ["Drossos", "Konstantinos", ""], ["\u00c7ak\u0131r", "Emre", ""], ["Virtanen", "Tuomas", ""]]}, {"id": "1706.02052", "submitter": "Ravi Adepu Sankar", "authors": "Adepu Ravi Sankar, Vineeth N Balasubramanian", "title": "Are Saddles Good Enough for Deep Learning?", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent years have seen a growing interest in understanding deep neural\nnetworks from an optimization perspective. It is understood now that converging\nto low-cost local minima is sufficient for such models to become effective in\npractice. However, in this work, we propose a new hypothesis based on recent\ntheoretical findings and empirical studies that deep neural network models\nactually converge to saddle points with high degeneracy. Our findings from this\nwork are new, and can have a significant impact on the development of gradient\ndescent based methods for training deep networks. We validated our hypotheses\nusing an extensive experimental evaluation on standard datasets such as MNIST\nand CIFAR-10, and also showed that recent efforts that attempt to escape\nsaddles finally converge to saddles with high degeneracy, which we define as\n`good saddles'. We also verified the famous Wigner's Semicircle Law in our\nexperimental results.\n", "versions": [{"version": "v1", "created": "Wed, 7 Jun 2017 05:44:07 GMT"}], "update_date": "2017-06-08", "authors_parsed": [["Sankar", "Adepu Ravi", ""], ["Balasubramanian", "Vineeth N", ""]]}, {"id": "1706.02124", "submitter": "Marian Tietz", "authors": "Marian Tietz, Tayfun Alpay, Johannes Twiefel, Stefan Wermter", "title": "Semi-Supervised Phoneme Recognition with Recurrent Ladder Networks", "comments": null, "journal-ref": null, "doi": "10.1007/978-3-319-68600-4_1", "report-no": null, "categories": "cs.CL cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Ladder networks are a notable new concept in the field of semi-supervised\nlearning by showing state-of-the-art results in image recognition tasks while\nbeing compatible with many existing neural architectures. We present the\nrecurrent ladder network, a novel modification of the ladder network, for\nsemi-supervised learning of recurrent neural networks which we evaluate with a\nphoneme recognition task on the TIMIT corpus. Our results show that the model\nis able to consistently outperform the baseline and achieve fully-supervised\nbaseline performance with only 75% of all labels which demonstrates that the\nmodel is capable of using unsupervised data as an effective regulariser.\n", "versions": [{"version": "v1", "created": "Wed, 7 Jun 2017 10:50:47 GMT"}, {"version": "v2", "created": "Mon, 18 Sep 2017 18:49:26 GMT"}], "update_date": "2017-09-20", "authors_parsed": [["Tietz", "Marian", ""], ["Alpay", "Tayfun", ""], ["Twiefel", "Johannes", ""], ["Wermter", "Stefan", ""]]}, {"id": "1706.02216", "submitter": "William L Hamilton", "authors": "William L. Hamilton, Rex Ying, Jure Leskovec", "title": "Inductive Representation Learning on Large Graphs", "comments": "Published in NIPS 2017; version with full appendix and minor\n  corrections", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Low-dimensional embeddings of nodes in large graphs have proved extremely\nuseful in a variety of prediction tasks, from content recommendation to\nidentifying protein functions. However, most existing approaches require that\nall nodes in the graph are present during training of the embeddings; these\nprevious approaches are inherently transductive and do not naturally generalize\nto unseen nodes. Here we present GraphSAGE, a general, inductive framework that\nleverages node feature information (e.g., text attributes) to efficiently\ngenerate node embeddings for previously unseen data. Instead of training\nindividual embeddings for each node, we learn a function that generates\nembeddings by sampling and aggregating features from a node's local\nneighborhood. Our algorithm outperforms strong baselines on three inductive\nnode-classification benchmarks: we classify the category of unseen nodes in\nevolving information graphs based on citation and Reddit post data, and we show\nthat our algorithm generalizes to completely unseen graphs using a multi-graph\ndataset of protein-protein interactions.\n", "versions": [{"version": "v1", "created": "Wed, 7 Jun 2017 14:51:05 GMT"}, {"version": "v2", "created": "Wed, 8 Nov 2017 01:45:25 GMT"}, {"version": "v3", "created": "Tue, 10 Apr 2018 15:40:00 GMT"}, {"version": "v4", "created": "Mon, 10 Sep 2018 14:26:58 GMT"}], "update_date": "2018-09-11", "authors_parsed": [["Hamilton", "William L.", ""], ["Ying", "Rex", ""], ["Leskovec", "Jure", ""]]}, {"id": "1706.02222", "submitter": "Andros Tjandra", "authors": "Andros Tjandra, Sakriani Sakti, Ruli Manurung, Mirna Adriani and\n  Satoshi Nakamura", "title": "Gated Recurrent Neural Tensor Network", "comments": "Accepted at IJCNN 2016 URL :\n  http://ieeexplore.ieee.org/document/7727233/", "journal-ref": null, "doi": "10.1109/IJCNN.2016.7727233", "report-no": null, "categories": "cs.LG cs.CL stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recurrent Neural Networks (RNNs), which are a powerful scheme for modeling\ntemporal and sequential data need to capture long-term dependencies on datasets\nand represent them in hidden layers with a powerful model to capture more\ninformation from inputs. For modeling long-term dependencies in a dataset, the\ngating mechanism concept can help RNNs remember and forget previous\ninformation. Representing the hidden layers of an RNN with more expressive\noperations (i.e., tensor products) helps it learn a more complex relationship\nbetween the current input and the previous hidden layer information. These\nideas can generally improve RNN performances. In this paper, we proposed a\nnovel RNN architecture that combine the concepts of gating mechanism and the\ntensor product into a single model. By combining these two concepts into a\nsingle RNN, our proposed models learn long-term dependencies by modeling with\ngating units and obtain more expressive and direct interaction between input\nand hidden layers using a tensor product on 3-dimensional array (tensor) weight\nparameters. We use Long Short Term Memory (LSTM) RNN and Gated Recurrent Unit\n(GRU) RNN and combine them with a tensor product inside their formulations. Our\nproposed RNNs, which are called a Long-Short Term Memory Recurrent Neural\nTensor Network (LSTMRNTN) and Gated Recurrent Unit Recurrent Neural Tensor\nNetwork (GRURNTN), are made by combining the LSTM and GRU RNN models with the\ntensor product. We conducted experiments with our proposed models on word-level\nand character-level language modeling tasks and revealed that our proposed\nmodels significantly improved their performance compared to our baseline\nmodels.\n", "versions": [{"version": "v1", "created": "Wed, 7 Jun 2017 15:05:39 GMT"}], "update_date": "2017-06-08", "authors_parsed": [["Tjandra", "Andros", ""], ["Sakti", "Sakriani", ""], ["Manurung", "Ruli", ""], ["Adriani", "Mirna", ""], ["Nakamura", "Satoshi", ""]]}, {"id": "1706.02237", "submitter": "Sudeep Raja Putta", "authors": "Sudeep Raja Putta, Theja Tulabandhula", "title": "Efficient Reinforcement Learning via Initial Pure Exploration", "comments": "4 pages, 3 figures, Presented at Reinforcement Learning and Decision\n  Making 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In several realistic situations, an interactive learning agent can practice\nand refine its strategy before going on to be evaluated. For instance, consider\na student preparing for a series of tests. She would typically take a few\npractice tests to know which areas she needs to improve upon. Based of the\nscores she obtains in these practice tests, she would formulate a strategy for\nmaximizing her scores in the actual tests. We treat this scenario in the\ncontext of an agent exploring a fixed-horizon episodic Markov Decision Process\n(MDP), where the agent can practice on the MDP for some number of episodes (not\nnecessarily known in advance) before starting to incur regret for its actions.\n  During practice, the agent's goal must be to maximize the probability of\nfollowing an optimal policy. This is akin to the problem of Pure Exploration\n(PE). We extend the PE problem of Multi Armed Bandits (MAB) to MDPs and propose\na Bayesian algorithm called Posterior Sampling for Pure Exploration (PSPE),\nwhich is similar to its bandit counterpart. We show that the Bayesian simple\nregret converges at an optimal exponential rate when using PSPE.\n  When the agent starts being evaluated, its goal would be to minimize the\ncumulative regret incurred. This is akin to the problem of Reinforcement\nLearning (RL). The agent uses the Posterior Sampling for Reinforcement Learning\nalgorithm (PSRL) initialized with the posteriors of the practice phase. We\nhypothesize that this PSPE + PSRL combination is an optimal strategy for\nminimizing regret in RL problems with an initial practice phase. We show\nempirical results which prove that having a lower simple regret at the end of\nthe practice phase results in having lower cumulative regret during evaluation.\n", "versions": [{"version": "v1", "created": "Wed, 7 Jun 2017 16:18:44 GMT"}], "update_date": "2017-06-08", "authors_parsed": [["Putta", "Sudeep Raja", ""], ["Tulabandhula", "Theja", ""]]}, {"id": "1706.02240", "submitter": "Martin Schrimpf", "authors": "Hanlin Tang, Martin Schrimpf, Bill Lotter, Charlotte Moerman, Ana\n  Paredes, Josue Ortega Caro, Walter Hardesty, David Cox, Gabriel Kreiman", "title": "Recurrent computations for visual pattern completion", "comments": null, "journal-ref": null, "doi": "10.1073/pnas.1719397115", "report-no": null, "categories": "q-bio.NC cs.AI cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Making inferences from partial information constitutes a critical aspect of\ncognition. During visual perception, pattern completion enables recognition of\npoorly visible or occluded objects. We combined psychophysics, physiology and\ncomputational models to test the hypothesis that pattern completion is\nimplemented by recurrent computations and present three pieces of evidence that\nare consistent with this hypothesis. First, subjects robustly recognized\nobjects even when rendered <15% visible, but recognition was largely impaired\nwhen processing was interrupted by backward masking. Second, invasive\nphysiological responses along the human ventral cortex exhibited visually\nselective responses to partially visible objects that were delayed compared to\nwhole objects, suggesting the need for additional computations. These\nphysiological delays were correlated with the effects of backward masking.\nThird, state-of-the-art feed-forward computational architectures were not\nrobust to partial visibility. However, recognition performance was recovered\nwhen the model was augmented with attractor-based recurrent connectivity. These\nresults provide a strong argument of plausibility for the role of recurrent\ncomputations in making visual inferences from partial information.\n", "versions": [{"version": "v1", "created": "Wed, 7 Jun 2017 16:23:28 GMT"}, {"version": "v2", "created": "Fri, 6 Apr 2018 12:29:22 GMT"}], "update_date": "2018-08-15", "authors_parsed": [["Tang", "Hanlin", ""], ["Schrimpf", "Martin", ""], ["Lotter", "Bill", ""], ["Moerman", "Charlotte", ""], ["Paredes", "Ana", ""], ["Caro", "Josue Ortega", ""], ["Hardesty", "Walter", ""], ["Cox", "David", ""], ["Kreiman", "Gabriel", ""]]}, {"id": "1706.02248", "submitter": "Yuri G. Gordienko", "authors": "Yuriy Kochura, Sergii Stirenko, Anis Rojbi, Oleg Alienin, Michail\n  Novotarskiy, and Yuri Gordienko", "title": "Comparative Analysis of Open Source Frameworks for Machine Learning with\n  Use Case in Single-Threaded and Multi-Threaded Modes", "comments": "4 pages, 6 figures, 4 tables; XIIth International Scientific and\n  Technical Conference on Computer Sciences and Information Technologies (CSIT\n  2017), Lviv, Ukraine", "journal-ref": "Proceedings of 12th International Scientific and Technical\n  Conference on Computer Sciences and Information Technologies (CSIT), 5-8\n  Sept. 2017, (Lviv, Ukraine), vol.1, pp. 373-376, IEEE", "doi": "10.1109/STC-CSIT.2017.8098808", "report-no": null, "categories": "cs.LG cs.CV cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The basic features of some of the most versatile and popular open source\nframeworks for machine learning (TensorFlow, Deep Learning4j, and H2O) are\nconsidered and compared. Their comparative analysis was performed and\nconclusions were made as to the advantages and disadvantages of these\nplatforms. The performance tests for the de facto standard MNIST data set were\ncarried out on H2O framework for deep learning algorithms designed for CPU and\nGPU platforms for single-threaded and multithreaded modes of operation.\n", "versions": [{"version": "v1", "created": "Wed, 7 Jun 2017 16:41:21 GMT"}], "update_date": "2017-11-30", "authors_parsed": [["Kochura", "Yuriy", ""], ["Stirenko", "Sergii", ""], ["Rojbi", "Anis", ""], ["Alienin", "Oleg", ""], ["Novotarskiy", "Michail", ""], ["Gordienko", "Yuri", ""]]}, {"id": "1706.02257", "submitter": "Oluwatobi Olabiyi", "authors": "Oluwatobi Olabiyi, Eric Martinson, Vijay Chintalapudi, Rui Guo", "title": "Driver Action Prediction Using Deep (Bidirectional) Recurrent Neural\n  Network", "comments": "ITSC'17", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.CV cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Advanced driver assistance systems (ADAS) can be significantly improved with\neffective driver action prediction (DAP). Predicting driver actions early and\naccurately can help mitigate the effects of potentially unsafe driving\nbehaviors and avoid possible accidents. In this paper, we formulate driver\naction prediction as a timeseries anomaly prediction problem. While the anomaly\n(driver actions of interest) detection might be trivial in this context,\nfinding patterns that consistently precede an anomaly requires searching for or\nextracting features across multi-modal sensory inputs. We present such a driver\naction prediction system, including a real-time data acquisition, processing\nand learning framework for predicting future or impending driver action. The\nproposed system incorporates camera-based knowledge of the driving environment\nand the driver themselves, in addition to traditional vehicle dynamics. It then\nuses a deep bidirectional recurrent neural network (DBRNN) to learn the\ncorrelation between sensory inputs and impending driver behavior achieving\naccurate and high horizon action prediction. The proposed system performs\nbetter than other existing systems on driver action prediction tasks and can\naccurately predict key driver actions including acceleration, braking, lane\nchange and turning at durations of 5sec before the action is executed by the\ndriver.\n", "versions": [{"version": "v1", "created": "Wed, 7 Jun 2017 17:00:08 GMT"}], "update_date": "2018-06-01", "authors_parsed": [["Olabiyi", "Oluwatobi", ""], ["Martinson", "Eric", ""], ["Chintalapudi", "Vijay", ""], ["Guo", "Rui", ""]]}, {"id": "1706.02262", "submitter": "Shengjia Zhao", "authors": "Shengjia Zhao, Jiaming Song, Stefano Ermon", "title": "InfoVAE: Information Maximizing Variational Autoencoders", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A key advance in learning generative models is the use of amortized inference\ndistributions that are jointly trained with the models. We find that existing\ntraining objectives for variational autoencoders can lead to inaccurate\namortized inference distributions and, in some cases, improving the objective\nprovably degrades the inference quality. In addition, it has been observed that\nvariational autoencoders tend to ignore the latent variables when combined with\na decoding distribution that is too flexible. We again identify the cause in\nexisting training criteria and propose a new class of objectives (InfoVAE) that\nmitigate these problems. We show that our model can significantly improve the\nquality of the variational posterior and can make effective use of the latent\nfeatures regardless of the flexibility of the decoding distribution. Through\nextensive qualitative and quantitative analyses, we demonstrate that our models\noutperform competing approaches on multiple performance metrics.\n", "versions": [{"version": "v1", "created": "Wed, 7 Jun 2017 17:05:01 GMT"}, {"version": "v2", "created": "Fri, 17 Nov 2017 18:35:30 GMT"}, {"version": "v3", "created": "Wed, 30 May 2018 17:28:31 GMT"}], "update_date": "2018-05-31", "authors_parsed": [["Zhao", "Shengjia", ""], ["Song", "Jiaming", ""], ["Ermon", "Stefano", ""]]}, {"id": "1706.02263", "submitter": "Rianne van den Berg", "authors": "Rianne van den Berg, Thomas N. Kipf, Max Welling", "title": "Graph Convolutional Matrix Completion", "comments": "9 pages, 3 figures, updated with additional experimental evaluation", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.DB cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider matrix completion for recommender systems from the point of view\nof link prediction on graphs. Interaction data such as movie ratings can be\nrepresented by a bipartite user-item graph with labeled edges denoting observed\nratings. Building on recent progress in deep learning on graph-structured data,\nwe propose a graph auto-encoder framework based on differentiable message\npassing on the bipartite interaction graph. Our model shows competitive\nperformance on standard collaborative filtering benchmarks. In settings where\ncomplimentary feature information or structured data such as a social network\nis available, our framework outperforms recent state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Wed, 7 Jun 2017 17:05:19 GMT"}, {"version": "v2", "created": "Wed, 25 Oct 2017 19:20:03 GMT"}], "update_date": "2017-10-27", "authors_parsed": [["Berg", "Rianne van den", ""], ["Kipf", "Thomas N.", ""], ["Welling", "Max", ""]]}, {"id": "1706.02275", "submitter": "Ryan Lowe T.", "authors": "Ryan Lowe, Yi Wu, Aviv Tamar, Jean Harb, Pieter Abbeel, Igor Mordatch", "title": "Multi-Agent Actor-Critic for Mixed Cooperative-Competitive Environments", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We explore deep reinforcement learning methods for multi-agent domains. We\nbegin by analyzing the difficulty of traditional algorithms in the multi-agent\ncase: Q-learning is challenged by an inherent non-stationarity of the\nenvironment, while policy gradient suffers from a variance that increases as\nthe number of agents grows. We then present an adaptation of actor-critic\nmethods that considers action policies of other agents and is able to\nsuccessfully learn policies that require complex multi-agent coordination.\nAdditionally, we introduce a training regimen utilizing an ensemble of policies\nfor each agent that leads to more robust multi-agent policies. We show the\nstrength of our approach compared to existing methods in cooperative as well as\ncompetitive scenarios, where agent populations are able to discover various\nphysical and informational coordination strategies.\n", "versions": [{"version": "v1", "created": "Wed, 7 Jun 2017 17:35:00 GMT"}, {"version": "v2", "created": "Wed, 21 Jun 2017 22:18:54 GMT"}, {"version": "v3", "created": "Tue, 16 Jan 2018 23:37:25 GMT"}, {"version": "v4", "created": "Sat, 14 Mar 2020 20:33:00 GMT"}], "update_date": "2020-03-17", "authors_parsed": [["Lowe", "Ryan", ""], ["Wu", "Yi", ""], ["Tamar", "Aviv", ""], ["Harb", "Jean", ""], ["Abbeel", "Pieter", ""], ["Mordatch", "Igor", ""]]}, {"id": "1706.02289", "submitter": "Evgeny Burnaev", "authors": "Smolyakov Dmitry, Alexander Korotin, Pavel Erofeev, Artem Papanov,\n  Evgeny Burnaev", "title": "Meta-Learning for Resampling Recommendation Systems", "comments": "23 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.AP stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One possible approach to tackle the class imbalance in classification tasks\nis to resample a training dataset, i.e., to drop some of its elements or to\nsynthesize new ones. There exist several widely-used resampling methods. Recent\nresearch showed that the choice of resampling method significantly affects the\nquality of classification, which raises resampling selection problem.\nExhaustive search for optimal resampling is time-consuming and hence it is of\nlimited use. In this paper, we describe an alternative approach to the\nresampling selection. We follow the meta-learning concept to build resampling\nrecommendation systems, i.e., algorithms recommending resampling for datasets\non the basis of their properties.\n", "versions": [{"version": "v1", "created": "Tue, 6 Jun 2017 22:02:27 GMT"}, {"version": "v2", "created": "Wed, 21 Jun 2017 21:19:48 GMT"}, {"version": "v3", "created": "Mon, 19 Feb 2018 08:08:13 GMT"}, {"version": "v4", "created": "Mon, 17 Sep 2018 07:50:39 GMT"}], "update_date": "2018-09-18", "authors_parsed": [["Dmitry", "Smolyakov", ""], ["Korotin", "Alexander", ""], ["Erofeev", "Pavel", ""], ["Papanov", "Artem", ""], ["Burnaev", "Evgeny", ""]]}, {"id": "1706.02291", "submitter": "Sharath Adavanne", "authors": "Sharath Adavanne, Pasi Pertil\\\"a, Tuomas Virtanen", "title": "Sound Event Detection Using Spatial Features and Convolutional Recurrent\n  Neural Network", "comments": "Accepted for IEEE International Conference on Acoustics, Speech and\n  Signal Processing (ICASSP 2017)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes to use low-level spatial features extracted from\nmultichannel audio for sound event detection. We extend the convolutional\nrecurrent neural network to handle more than one type of these multichannel\nfeatures by learning from each of them separately in the initial stages. We\nshow that instead of concatenating the features of each channel into a single\nfeature vector the network learns sound events in multichannel audio better\nwhen they are presented as separate layers of a volume. Using the proposed\nspatial features over monaural features on the same network gives an absolute\nF-score improvement of 6.1% on the publicly available TUT-SED 2016 dataset and\n2.7% on the TUT-SED 2009 dataset that is fifteen times larger.\n", "versions": [{"version": "v1", "created": "Wed, 7 Jun 2017 06:01:48 GMT"}], "update_date": "2017-06-09", "authors_parsed": [["Adavanne", "Sharath", ""], ["Pertil\u00e4", "Pasi", ""], ["Virtanen", "Tuomas", ""]]}, {"id": "1706.02292", "submitter": "Sharath Adavanne", "authors": "Miroslav Malik, Sharath Adavanne, Konstantinos Drossos, Tuomas\n  Virtanen, Dasa Ticha, Roman Jarina", "title": "Stacked Convolutional and Recurrent Neural Networks for Music Emotion\n  Recognition", "comments": "Accepted for Sound and Music Computing (SMC 2017)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper studies the emotion recognition from musical tracks in the\n2-dimensional valence-arousal (V-A) emotional space. We propose a method based\non convolutional (CNN) and recurrent neural networks (RNN), having\nsignificantly fewer parameters compared with the state-of-the-art method for\nthe same task. We utilize one CNN layer followed by two branches of RNNs\ntrained separately for arousal and valence. The method was evaluated using the\n'MediaEval2015 emotion in music' dataset. We achieved an RMSE of 0.202 for\narousal and 0.268 for valence, which is the best result reported on this\ndataset.\n", "versions": [{"version": "v1", "created": "Wed, 7 Jun 2017 06:06:14 GMT"}], "update_date": "2017-06-09", "authors_parsed": [["Malik", "Miroslav", ""], ["Adavanne", "Sharath", ""], ["Drossos", "Konstantinos", ""], ["Virtanen", "Tuomas", ""], ["Ticha", "Dasa", ""], ["Jarina", "Roman", ""]]}, {"id": "1706.02293", "submitter": "Sharath Adavanne", "authors": "Sharath Adavanne, Giambattista Parascandolo, Pasi Pertil\\\"a, Toni\n  Heittola, Tuomas Virtanen", "title": "Sound Event Detection in Multichannel Audio Using Spatial and Harmonic\n  Features", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose the use of spatial and harmonic features in\ncombination with long short term memory (LSTM) recurrent neural network (RNN)\nfor automatic sound event detection (SED) task. Real life sound recordings\ntypically have many overlapping sound events, making it hard to recognize with\njust mono channel audio. Human listeners have been successfully recognizing the\nmixture of overlapping sound events using pitch cues and exploiting the stereo\n(multichannel) audio signal available at their ears to spatially localize these\nevents. Traditionally SED systems have only been using mono channel audio,\nmotivated by the human listener we propose to extend them to use multichannel\naudio. The proposed SED system is compared against the state of the art mono\nchannel method on the development subset of TUT sound events detection 2016\ndatabase. The usage of spatial and harmonic features are shown to improve the\nperformance of SED.\n", "versions": [{"version": "v1", "created": "Wed, 7 Jun 2017 06:11:32 GMT"}], "update_date": "2017-06-09", "authors_parsed": [["Adavanne", "Sharath", ""], ["Parascandolo", "Giambattista", ""], ["Pertil\u00e4", "Pasi", ""], ["Heittola", "Toni", ""], ["Virtanen", "Tuomas", ""]]}, {"id": "1706.02295", "submitter": "Chih-Kuan Yeh", "authors": "Chih-Kuan Yeh and Yao-Hung Hubert Tsai and Yu-Chiang Frank Wang", "title": "Generative-Discriminative Variational Model for Visual Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The paradigm shift from shallow classifiers with hand-crafted features to\nend-to-end trainable deep learning models has shown significant improvements on\nsupervised learning tasks. Despite the promising power of deep neural networks\n(DNN), how to alleviate overfitting during training has been a research topic\nof interest. In this paper, we present a Generative-Discriminative Variational\nModel (GDVM) for visual classification, in which we introduce a latent variable\ninferred from inputs for exhibiting generative abilities towards prediction. In\nother words, our GDVM casts the supervised learning task as a generative\nlearning process, with data discrimination to be jointly exploited for improved\nclassification. In our experiments, we consider the tasks of multi-class\nclassification, multi-label classification, and zero-shot learning. We show\nthat our GDVM performs favorably against the baselines or recent generative DNN\nmodels.\n", "versions": [{"version": "v1", "created": "Wed, 7 Jun 2017 10:19:30 GMT"}], "update_date": "2017-06-09", "authors_parsed": [["Yeh", "Chih-Kuan", ""], ["Tsai", "Yao-Hung Hubert", ""], ["Wang", "Yu-Chiang Frank", ""]]}, {"id": "1706.02332", "submitter": "Matthijs Douze", "authors": "Matthijs Douze and Arthur Szlam and Bharath Hariharan and Herv\\'e\n  J\\'egou", "title": "Low-shot learning with large-scale diffusion", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper considers the problem of inferring image labels from images when\nonly a few annotated examples are available at training time. This setup is\noften referred to as low-shot learning, where a standard approach is to\nre-train the last few layers of a convolutional neural network learned on\nseparate classes for which training examples are abundant. We consider a\nsemi-supervised setting based on a large collection of images to support label\npropagation. This is possible by leveraging the recent advances on large-scale\nsimilarity graph construction.\n  We show that despite its conceptual simplicity, scaling label propagation up\nto hundred millions of images leads to state of the art accuracy in the\nlow-shot learning regime.\n", "versions": [{"version": "v1", "created": "Wed, 7 Jun 2017 18:40:26 GMT"}, {"version": "v2", "created": "Tue, 17 Oct 2017 13:59:23 GMT"}, {"version": "v3", "created": "Fri, 15 Jun 2018 15:31:37 GMT"}], "update_date": "2018-06-18", "authors_parsed": [["Douze", "Matthijs", ""], ["Szlam", "Arthur", ""], ["Hariharan", "Bharath", ""], ["J\u00e9gou", "Herv\u00e9", ""]]}, {"id": "1706.02337", "submitter": "Xiao Yang", "authors": "Xiao Yang, Ersin Yumer, Paul Asente, Mike Kraley, Daniel Kifer, C. Lee\n  Giles", "title": "Learning to Extract Semantic Structure from Documents Using Multimodal\n  Fully Convolutional Neural Network", "comments": "CVPR 2017 Spotlight", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an end-to-end, multimodal, fully convolutional network for\nextracting semantic structures from document images. We consider document\nsemantic structure extraction as a pixel-wise segmentation task, and propose a\nunified model that classifies pixels based not only on their visual appearance,\nas in the traditional page segmentation task, but also on the content of\nunderlying text. Moreover, we propose an efficient synthetic document\ngeneration process that we use to generate pretraining data for our network.\nOnce the network is trained on a large set of synthetic documents, we fine-tune\nthe network on unlabeled real documents using a semi-supervised approach. We\nsystematically study the optimum network architecture and show that both our\nmultimodal approach and the synthetic data pretraining significantly boost the\nperformance.\n", "versions": [{"version": "v1", "created": "Wed, 7 Jun 2017 18:51:31 GMT"}], "update_date": "2017-06-09", "authors_parsed": [["Yang", "Xiao", ""], ["Yumer", "Ersin", ""], ["Asente", "Paul", ""], ["Kraley", "Mike", ""], ["Kifer", "Daniel", ""], ["Giles", "C. Lee", ""]]}, {"id": "1706.02361", "submitter": "Keunwoo Choi Mr", "authors": "Keunwoo Choi and George Fazekas and Kyunghyun Cho and Mark Sandler", "title": "The Effects of Noisy Labels on Deep Convolutional Neural Networks for\n  Music Tagging", "comments": "The section that overlapped with arXiv:1709.01922 is completely\n  removed since the earlier version. This is the camera-ready version", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG cs.MM cs.SD", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Deep neural networks (DNN) have been successfully applied to music\nclassification including music tagging. However, there are several open\nquestions regarding the training, evaluation, and analysis of DNNs. In this\narticle, we investigate specific aspects of neural networks, the effects of\nnoisy labels, to deepen our understanding of their properties. We analyse and\n(re-)validate a large music tagging dataset to investigate the reliability of\ntraining and evaluation. Using a trained network, we compute label vector\nsimilarities which is compared to groundtruth similarity.\n  The results highlight several important aspects of music tagging and neural\nnetworks. We show that networks can be effective despite relatively large error\nrates in groundtruth datasets, while conjecturing that label noise can be the\ncause of varying tag-wise performance differences. Lastly, the analysis of our\ntrained network provides valuable insight into the relationships between music\ntags. These results highlight the benefit of using data-driven methods to\naddress automatic music tagging.\n", "versions": [{"version": "v1", "created": "Wed, 7 Jun 2017 19:54:39 GMT"}, {"version": "v2", "created": "Sun, 10 Sep 2017 23:47:42 GMT"}, {"version": "v3", "created": "Tue, 14 Nov 2017 15:54:38 GMT"}], "update_date": "2017-11-15", "authors_parsed": [["Choi", "Keunwoo", ""], ["Fazekas", "George", ""], ["Cho", "Kyunghyun", ""], ["Sandler", "Mark", ""]]}, {"id": "1706.02375", "submitter": "Jeffrey Regier", "authors": "Jeffrey Regier and Michael I. Jordan and Jon McAuliffe", "title": "Fast Black-box Variational Inference through Stochastic Trust-Region\n  Optimization", "comments": "NIPS 2017 camera-ready", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce TrustVI, a fast second-order algorithm for black-box variational\ninference based on trust-region optimization and the reparameterization trick.\nAt each iteration, TrustVI proposes and assesses a step based on minibatches of\ndraws from the variational distribution. The algorithm provably converges to a\nstationary point. We implemented TrustVI in the Stan framework and compared it\nto two alternatives: Automatic Differentiation Variational Inference (ADVI) and\nHessian-free Stochastic Gradient Variational Inference (HFSGVI). The former is\nbased on stochastic first-order optimization. The latter uses second-order\ninformation, but lacks convergence guarantees. TrustVI typically converged at\nleast one order of magnitude faster than ADVI, demonstrating the value of\nstochastic second-order information. TrustVI often found substantially better\nvariational distributions than HFSGVI, demonstrating that our convergence\ntheory can matter in practice.\n", "versions": [{"version": "v1", "created": "Wed, 7 Jun 2017 20:37:09 GMT"}, {"version": "v2", "created": "Sun, 5 Nov 2017 00:28:47 GMT"}], "update_date": "2017-11-07", "authors_parsed": [["Regier", "Jeffrey", ""], ["Jordan", "Michael I.", ""], ["McAuliffe", "Jon", ""]]}, {"id": "1706.02379", "submitter": "Hao Li", "authors": "Hao Li, Soham De, Zheng Xu, Christoph Studer, Hanan Samet, Tom\n  Goldstein", "title": "Training Quantized Nets: A Deeper Understanding", "comments": "NIPS 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Currently, deep neural networks are deployed on low-power portable devices by\nfirst training a full-precision model using powerful hardware, and then\nderiving a corresponding low-precision model for efficient inference on such\nsystems. However, training models directly with coarsely quantized weights is a\nkey step towards learning on embedded platforms that have limited computing\nresources, memory capacity, and power consumption. Numerous recent publications\nhave studied methods for training quantized networks, but these studies have\nmostly been empirical. In this work, we investigate training methods for\nquantized neural networks from a theoretical viewpoint. We first explore\naccuracy guarantees for training methods under convexity assumptions. We then\nlook at the behavior of these algorithms for non-convex problems, and show that\ntraining algorithms that exploit high-precision representations have an\nimportant greedy search phase that purely quantized training methods lack,\nwhich explains the difficulty of training using low-precision arithmetic.\n", "versions": [{"version": "v1", "created": "Wed, 7 Jun 2017 21:01:15 GMT"}, {"version": "v2", "created": "Wed, 9 Aug 2017 10:28:36 GMT"}, {"version": "v3", "created": "Mon, 13 Nov 2017 16:32:39 GMT"}], "update_date": "2017-11-15", "authors_parsed": [["Li", "Hao", ""], ["De", "Soham", ""], ["Xu", "Zheng", ""], ["Studer", "Christoph", ""], ["Samet", "Hanan", ""], ["Goldstein", "Tom", ""]]}, {"id": "1706.02386", "submitter": "Daniele Ramazzotti", "authors": "Giulio Caravagna and Daniele Ramazzotti", "title": "Learning the structure of Bayesian Networks via the bootstrap", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning the structure of dependencies among multiple random variables is a\nproblem of considerable theoretical and practical interest. Within the context\nof Bayesian Networks, a practical and surprisingly successful solution to this\nlearning problem is achieved by adopting score-functions optimisation schema,\naugmented with multiple restarts to avoid local optima. Yet, the conditions\nunder which such strategies work well are poorly understood, and there are also\nsome intrinsic limitations to learning the directionality of the interaction\namong the variables. Following an early intuition of Friedman and Koller, we\npropose to decouple the learning problem into two steps: first, we identify a\npartial ordering among input variables which constrains the structural learning\nproblem, and then propose an effective bootstrap-based algorithm to simulate\naugmented data sets, and select the most important dependencies among the\nvariables. By using several synthetic data sets, we show that our algorithm\nyields better recovery performance than the state of the art, increasing the\nchances of identifying a globally-optimal solution to the learning problem, and\nsolving also well-known identifiability issues that affect the standard\napproach. We use our new algorithm to infer statistical dependencies between\ncancer driver somatic mutations detected by high-throughput genome sequencing\ndata of multiple colorectal cancer patients. In this way, we also show how the\nproposed methods can shade new insights about cancer initiation, and\nprogression. Code: https://github.com/caravagn/Bootstrap-based-Learning\n", "versions": [{"version": "v1", "created": "Wed, 7 Jun 2017 21:30:47 GMT"}, {"version": "v2", "created": "Tue, 19 Jan 2021 17:26:12 GMT"}], "update_date": "2021-01-20", "authors_parsed": [["Caravagna", "Giulio", ""], ["Ramazzotti", "Daniele", ""]]}, {"id": "1706.02390", "submitter": "Mustafa Mustafa", "authors": "Mustafa Mustafa, Deborah Bard, Wahid Bhimji, Zarija Luki\\'c, Rami\n  Al-Rfou, Jan M. Kratochvil", "title": "CosmoGAN: creating high-fidelity weak lensing convergence maps using\n  Generative Adversarial Networks", "comments": "11 pages, 8 figures", "journal-ref": "Computational Astrophysics and CosmologySimulations, Data Analysis\n  and Algorithms 2019 6:1", "doi": "10.1186/s40668-019-0029-9", "report-no": null, "categories": "astro-ph.IM cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Inferring model parameters from experimental data is a grand challenge in\nmany sciences, including cosmology. This often relies critically on high\nfidelity numerical simulations, which are prohibitively computationally\nexpensive. The application of deep learning techniques to generative modeling\nis renewing interest in using high dimensional density estimators as\ncomputationally inexpensive emulators of fully-fledged simulations. These\ngenerative models have the potential to make a dramatic shift in the field of\nscientific simulations, but for that shift to happen we need to study the\nperformance of such generators in the precision regime needed for science\napplications. To this end, in this work we apply Generative Adversarial\nNetworks to the problem of generating weak lensing convergence maps. We show\nthat our generator network produces maps that are described by, with high\nstatistical confidence, the same summary statistics as the fully simulated\nmaps.\n", "versions": [{"version": "v1", "created": "Wed, 7 Jun 2017 21:50:15 GMT"}, {"version": "v2", "created": "Fri, 17 Aug 2018 05:24:31 GMT"}, {"version": "v3", "created": "Fri, 21 Dec 2018 21:51:49 GMT"}, {"version": "v4", "created": "Sat, 29 Dec 2018 16:38:57 GMT"}, {"version": "v5", "created": "Mon, 6 May 2019 20:46:57 GMT"}, {"version": "v6", "created": "Wed, 22 May 2019 17:00:38 GMT"}], "update_date": "2019-05-23", "authors_parsed": [["Mustafa", "Mustafa", ""], ["Bard", "Deborah", ""], ["Bhimji", "Wahid", ""], ["Luki\u0107", "Zarija", ""], ["Al-Rfou", "Rami", ""], ["Kratochvil", "Jan M.", ""]]}, {"id": "1706.02409", "submitter": "Shahin Jabbari", "authors": "Richard Berk, Hoda Heidari, Shahin Jabbari, Matthew Joseph, Michael\n  Kearns, Jamie Morgenstern, Seth Neel, Aaron Roth", "title": "A Convex Framework for Fair Regression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a flexible family of fairness regularizers for (linear and\nlogistic) regression problems. These regularizers all enjoy convexity,\npermitting fast optimization, and they span the rang from notions of group\nfairness to strong individual fairness. By varying the weight on the fairness\nregularizer, we can compute the efficient frontier of the accuracy-fairness\ntrade-off on any given dataset, and we measure the severity of this trade-off\nvia a numerical quantity we call the Price of Fairness (PoF). The centerpiece\nof our results is an extensive comparative study of the PoF across six\ndifferent datasets in which fairness is a primary consideration.\n", "versions": [{"version": "v1", "created": "Wed, 7 Jun 2017 23:09:28 GMT"}], "update_date": "2017-06-09", "authors_parsed": [["Berk", "Richard", ""], ["Heidari", "Hoda", ""], ["Jabbari", "Shahin", ""], ["Joseph", "Matthew", ""], ["Kearns", "Michael", ""], ["Morgenstern", "Jamie", ""], ["Neel", "Seth", ""], ["Roth", "Aaron", ""]]}, {"id": "1706.02416", "submitter": "Sufeng Niu", "authors": "Sufeng Niu, Siheng Chen, Hanyu Guo, Colin Targonski, Melissa C. Smith,\n  Jelena Kova\\v{c}evi\\'c", "title": "Generalized Value Iteration Networks: Life Beyond Lattices", "comments": "14 pages, conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we introduce a generalized value iteration network (GVIN),\nwhich is an end-to-end neural network planning module. GVIN emulates the value\niteration algorithm by using a novel graph convolution operator, which enables\nGVIN to learn and plan on irregular spatial graphs. We propose three novel\ndifferentiable kernels as graph convolution operators and show that the\nembedding based kernel achieves the best performance. We further propose\nepisodic Q-learning, an improvement upon traditional n-step Q-learning that\nstabilizes training for networks that contain a planning module. Lastly, we\nevaluate GVIN on planning problems in 2D mazes, irregular graphs, and\nreal-world street networks, showing that GVIN generalizes well for both\narbitrary graphs and unseen graphs of larger scale and outperforms a naive\ngeneralization of VIN (discretizing a spatial graph into a 2D image).\n", "versions": [{"version": "v1", "created": "Thu, 8 Jun 2017 00:04:05 GMT"}, {"version": "v2", "created": "Thu, 26 Oct 2017 15:23:18 GMT"}], "update_date": "2017-10-27", "authors_parsed": [["Niu", "Sufeng", ""], ["Chen", "Siheng", ""], ["Guo", "Hanyu", ""], ["Targonski", "Colin", ""], ["Smith", "Melissa C.", ""], ["Kova\u010devi\u0107", "Jelena", ""]]}, {"id": "1706.02423", "submitter": "Jungsik Hwang", "authors": "Jungsik Hwang and Jun Tani", "title": "Seamless Integration and Coordination of Cognitive Skills in Humanoid\n  Robots: A Deep Learning Approach", "comments": "Accepted in the IEEE Transactions on Cognitive and Developmental\n  Systems (TCDS), 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This study investigates how adequate coordination among the different\ncognitive processes of a humanoid robot can be developed through end-to-end\nlearning of direct perception of visuomotor stream. We propose a deep dynamic\nneural network model built on a dynamic vision network, a motor generation\nnetwork, and a higher-level network. The proposed model was designed to process\nand to integrate direct perception of dynamic visuomotor patterns in a\nhierarchical model characterized by different spatial and temporal constraints\nimposed on each level. We conducted synthetic robotic experiments in which a\nrobot learned to read human's intention through observing the gestures and then\nto generate the corresponding goal-directed actions. Results verify that the\nproposed model is able to learn the tutored skills and to generalize them to\nnovel situations. The model showed synergic coordination of perception, action\nand decision making, and it integrated and coordinated a set of cognitive\nskills including visual perception, intention reading, attention switching,\nworking memory, action preparation and execution in a seamless manner. Analysis\nreveals that coherent internal representations emerged at each level of the\nhierarchy. Higher-level representation reflecting actional intention developed\nby means of continuous integration of the lower-level visuo-proprioceptive\nstream.\n", "versions": [{"version": "v1", "created": "Thu, 8 Jun 2017 01:15:00 GMT"}], "update_date": "2017-06-09", "authors_parsed": [["Hwang", "Jungsik", ""], ["Tani", "Jun", ""]]}, {"id": "1706.02444", "submitter": "Jungsik Hwang", "authors": "Jungsik Hwang, Jinhyung Kim, Ahmadreza Ahmadi, Minkyu Choi, Jun Tani", "title": "Predictive Coding-based Deep Dynamic Neural Network for Visuomotor\n  Learning", "comments": "Accepted at the 7th Joint IEEE International Conference of\n  Developmental Learning and Epigenetic Robotics (ICDL-EpiRob 2017)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG cs.RO q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This study presents a dynamic neural network model based on the predictive\ncoding framework for perceiving and predicting the dynamic visuo-proprioceptive\npatterns. In our previous study [1], we have shown that the deep dynamic neural\nnetwork model was able to coordinate visual perception and action generation in\na seamless manner. In the current study, we extended the previous model under\nthe predictive coding framework to endow the model with a capability of\nperceiving and predicting dynamic visuo-proprioceptive patterns as well as a\ncapability of inferring intention behind the perceived visuomotor information\nthrough minimizing prediction error. A set of synthetic experiments were\nconducted in which a robot learned to imitate the gestures of another robot in\na simulation environment. The experimental results showed that with given\nintention states, the model was able to mentally simulate the possible incoming\ndynamic visuo-proprioceptive patterns in a top-down process without the inputs\nfrom the external environment. Moreover, the results highlighted the role of\nminimizing prediction error in inferring underlying intention of the perceived\nvisuo-proprioceptive patterns, supporting the predictive coding account of the\nmirror neuron systems. The results also revealed that minimizing prediction\nerror in one modality induced the recall of the corresponding representation of\nanother modality acquired during the consolidative learning of raw-level\nvisuo-proprioceptive patterns.\n", "versions": [{"version": "v1", "created": "Thu, 8 Jun 2017 03:29:39 GMT"}], "update_date": "2017-06-09", "authors_parsed": [["Hwang", "Jungsik", ""], ["Kim", "Jinhyung", ""], ["Ahmadi", "Ahmadreza", ""], ["Choi", "Minkyu", ""], ["Tani", "Jun", ""]]}, {"id": "1706.02447", "submitter": "Raquel Aoki", "authors": "Raquel YS Aoki, Renato M Assuncao, Pedro OS Vaz de Melo", "title": "Luck is Hard to Beat: The Difficulty of Sports Prediction", "comments": "10 pages, KDD2017, Applied Data Science track", "journal-ref": "Proceedings of the 23rd ACM SIGKDD International Conference on\n  Knowledge Discovery and Data Mining, 2017", "doi": "10.1145/3097983.3098045", "report-no": null, "categories": "cs.LG stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Predicting the outcome of sports events is a hard task. We quantify this\ndifficulty with a coefficient that measures the distance between the observed\nfinal results of sports leagues and idealized perfectly balanced competitions\nin terms of skill. This indicates the relative presence of luck and skill. We\ncollected and analyzed all games from 198 sports leagues comprising 1503\nseasons from 84 countries of 4 different sports: basketball, soccer, volleyball\nand handball. We measured the competitiveness by countries and sports. We also\nidentify in each season which teams, if removed from its league, result in a\ncompletely random tournament. Surprisingly, not many of them are needed. As\nanother contribution of this paper, we propose a probabilistic graphical model\nto learn about the teams' skills and to decompose the relative weights of luck\nand skill in each game. We break down the skill component into factors\nassociated with the teams' characteristics. The model also allows to estimate\nas 0.36 the probability that an underdog team wins in the NBA league, with a\nhome advantage adding 0.09 to this probability. As shown in the first part of\nthe paper, luck is substantially present even in the most competitive\nchampionships, which partially explains why sophisticated and complex\nfeature-based models hardly beat simple models in the task of forecasting\nsports' outcomes.\n", "versions": [{"version": "v1", "created": "Thu, 8 Jun 2017 03:38:27 GMT"}], "update_date": "2017-11-27", "authors_parsed": [["Aoki", "Raquel YS", ""], ["Assuncao", "Renato M", ""], ["de Melo", "Pedro OS Vaz", ""]]}, {"id": "1706.02471", "submitter": "Zhi-Hua Zhou", "authors": "Peng Zhao and Zhi-Hua Zhou", "title": "Distribution-Free One-Pass Learning", "comments": null, "journal-ref": "IEEE Transactions on Knowledge and Data Engineering 2019", "doi": "10.1109/TKDE.2019.2937078", "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In many large-scale machine learning applications, data are accumulated with\ntime, and thus, an appropriate model should be able to update in an online\nparadigm. Moreover, as the whole data volume is unknown when constructing the\nmodel, it is desired to scan each data item only once with a storage\nindependent with the data volume. It is also noteworthy that the distribution\nunderlying may change during the data accumulation procedure. To handle such\ntasks, in this paper we propose DFOP, a distribution-free one-pass learning\napproach. This approach works well when distribution change occurs during data\naccumulation, without requiring prior knowledge about the change. Every data\nitem can be discarded once it has been scanned. Besides, theoretical guarantee\nshows that the estimate error, under a mild assumption, decreases until\nconvergence with high probability. The performance of DFOP for both regression\nand classification are validated in experiments.\n", "versions": [{"version": "v1", "created": "Thu, 8 Jun 2017 08:09:29 GMT"}], "update_date": "2020-07-07", "authors_parsed": [["Zhao", "Peng", ""], ["Zhou", "Zhi-Hua", ""]]}, {"id": "1706.02480", "submitter": "Jeffrey Humpherys", "authors": "Chris Hettinger, Tanner Christensen, Ben Ehlert, Jeffrey Humpherys,\n  Tyler Jarvis, and Sean Wade", "title": "Forward Thinking: Building and Training Neural Networks One Layer at a\n  Time", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a general framework for training deep neural networks without\nbackpropagation. This substantially decreases training time and also allows for\nconstruction of deep networks with many sorts of learners, including networks\nwhose layers are defined by functions that are not easily differentiated, like\ndecision trees. The main idea is that layers can be trained one at a time, and\nonce they are trained, the input data are mapped forward through the layer to\ncreate a new learning problem. The process is repeated, transforming the data\nthrough multiple layers, one at a time, rendering a new data set, which is\nexpected to be better behaved, and on which a final output layer can achieve\ngood performance. We call this forward thinking and demonstrate a proof of\nconcept by achieving state-of-the-art accuracy on the MNIST dataset for\nconvolutional neural networks. We also provide a general mathematical\nformulation of forward thinking that allows for other types of deep learning\nproblems to be considered.\n", "versions": [{"version": "v1", "created": "Thu, 8 Jun 2017 08:53:00 GMT"}], "update_date": "2017-06-09", "authors_parsed": [["Hettinger", "Chris", ""], ["Christensen", "Tanner", ""], ["Ehlert", "Ben", ""], ["Humpherys", "Jeffrey", ""], ["Jarvis", "Tyler", ""], ["Wade", "Sean", ""]]}, {"id": "1706.02490", "submitter": "Matej Hoffmann", "authors": "Karla Stepanova and Matej Hoffmann and Zdenek Straka and Frederico B.\n  Klein and Angelo Cangelosi and Michal Vavrecka", "title": "Where is my forearm? Clustering of body parts from simultaneous tactile\n  and linguistic input using sequential mapping", "comments": "pp. 155-162", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.AI cs.CL cs.LG cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Humans and animals are constantly exposed to a continuous stream of sensory\ninformation from different modalities. At the same time, they form more\ncompressed representations like concepts or symbols. In species that use\nlanguage, this process is further structured by this interaction, where a\nmapping between the sensorimotor concepts and linguistic elements needs to be\nestablished. There is evidence that children might be learning language by\nsimply disambiguating potential meanings based on multiple exposures to\nutterances in different contexts (cross-situational learning). In existing\nmodels, the mapping between modalities is usually found in a single step by\ndirectly using frequencies of referent and meaning co-occurrences. In this\npaper, we present an extension of this one-step mapping and introduce a newly\nproposed sequential mapping algorithm together with a publicly available Matlab\nimplementation. For demonstration, we have chosen a less typical scenario:\ninstead of learning to associate objects with their names, we focus on body\nrepresentations. A humanoid robot is receiving tactile stimulations on its\nbody, while at the same time listening to utterances of the body part names\n(e.g., hand, forearm and torso). With the goal at arriving at the correct \"body\ncategories\", we demonstrate how a sequential mapping algorithm outperforms\none-step mapping. In addition, the effect of data set size and noise in the\nlinguistic input are studied.\n", "versions": [{"version": "v1", "created": "Thu, 8 Jun 2017 09:31:42 GMT"}], "update_date": "2017-06-09", "authors_parsed": [["Stepanova", "Karla", ""], ["Hoffmann", "Matej", ""], ["Straka", "Zdenek", ""], ["Klein", "Frederico B.", ""], ["Cangelosi", "Angelo", ""], ["Vavrecka", "Michal", ""]]}, {"id": "1706.02496", "submitter": "Franziska Horn", "authors": "Franziska Horn", "title": "Context encoders as a simple but powerful extension of word2vec", "comments": "ACL 2017 2nd Workshop on Representation Learning for NLP", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With a simple architecture and the ability to learn meaningful word\nembeddings efficiently from texts containing billions of words, word2vec\nremains one of the most popular neural language models used today. However, as\nonly a single embedding is learned for every word in the vocabulary, the model\nfails to optimally represent words with multiple meanings. Additionally, it is\nnot possible to create embeddings for new (out-of-vocabulary) words on the\nspot. Based on an intuitive interpretation of the continuous bag-of-words\n(CBOW) word2vec model's negative sampling training objective in terms of\npredicting context based similarities, we motivate an extension of the model we\ncall context encoders (ConEc). By multiplying the matrix of trained word2vec\nembeddings with a word's average context vector, out-of-vocabulary (OOV)\nembeddings and representations for a word with multiple meanings can be created\nbased on the word's local contexts. The benefits of this approach are\nillustrated by using these word embeddings as features in the CoNLL 2003 named\nentity recognition (NER) task.\n", "versions": [{"version": "v1", "created": "Thu, 8 Jun 2017 09:56:11 GMT"}], "update_date": "2017-06-09", "authors_parsed": [["Horn", "Franziska", ""]]}, {"id": "1706.02501", "submitter": "Rika Antonova", "authors": "Rika Antonova, Silvia Cruciani", "title": "Unlocking the Potential of Simulators: Design with RL in Mind", "comments": "Extended abstract for RLDM17 (3rd Multidisciplinary Conference on\n  Reinforcement Learning and Decision Making)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Using Reinforcement Learning (RL) in simulation to construct policies useful\nin real life is challenging. This is often attributed to the sequential\ndecision making aspect: inaccuracies in simulation accumulate over multiple\nsteps, hence the simulated trajectories diverge from what would happen in\nreality.\n  In our work we show the need to consider another important aspect: the\nmismatch in simulating control. We bring attention to the need for modeling\ncontrol as well as dynamics, since oversimplifying assumptions about applying\nactions of RL policies could make the policies fail on real-world systems.\n  We design a simulator for solving a pivoting task (of interest in Robotics)\nand demonstrate that even a simple simulator designed with RL in mind\noutperforms high-fidelity simulators when it comes to learning a policy that is\nto be deployed on a real robotic system. We show that a phenomenon that is hard\nto model - friction - could be exploited successfully, even when RL is\nperformed using a simulator with a simple dynamics and noise model. Hence, we\ndemonstrate that as long as the main sources of uncertainty are identified, it\ncould be possible to learn policies applicable to real systems even using a\nsimple simulator.\n  RL-compatible simulators could open the possibilities for applying a wide\nrange of RL algorithms in various fields. This is important, since currently\ndata sparsity in fields like healthcare and education frequently forces\nresearchers and engineers to only consider sample-efficient RL approaches.\nSuccessful simulator-aided RL could increase flexibility of experimenting with\nRL algorithms and help applying RL policies to real-world settings in fields\nwhere data is scarce. We believe that lessons learned in Robotics could help\nother fields design RL-compatible simulators, so we summarize our experience\nand conclude with suggestions.\n", "versions": [{"version": "v1", "created": "Thu, 8 Jun 2017 10:10:44 GMT"}], "update_date": "2017-06-09", "authors_parsed": [["Antonova", "Rika", ""], ["Cruciani", "Silvia", ""]]}, {"id": "1706.02515", "submitter": "G\\\"unter Klambauer", "authors": "G\\\"unter Klambauer, Thomas Unterthiner, Andreas Mayr and Sepp\n  Hochreiter", "title": "Self-Normalizing Neural Networks", "comments": "9 pages (+ 93 pages appendix)", "journal-ref": "Advances in Neural Information Processing Systems 30 (NIPS 2017)", "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep Learning has revolutionized vision via convolutional neural networks\n(CNNs) and natural language processing via recurrent neural networks (RNNs).\nHowever, success stories of Deep Learning with standard feed-forward neural\nnetworks (FNNs) are rare. FNNs that perform well are typically shallow and,\ntherefore cannot exploit many levels of abstract representations. We introduce\nself-normalizing neural networks (SNNs) to enable high-level abstract\nrepresentations. While batch normalization requires explicit normalization,\nneuron activations of SNNs automatically converge towards zero mean and unit\nvariance. The activation function of SNNs are \"scaled exponential linear units\"\n(SELUs), which induce self-normalizing properties. Using the Banach fixed-point\ntheorem, we prove that activations close to zero mean and unit variance that\nare propagated through many network layers will converge towards zero mean and\nunit variance -- even under the presence of noise and perturbations. This\nconvergence property of SNNs allows to (1) train deep networks with many\nlayers, (2) employ strong regularization, and (3) to make learning highly\nrobust. Furthermore, for activations not close to unit variance, we prove an\nupper and lower bound on the variance, thus, vanishing and exploding gradients\nare impossible. We compared SNNs on (a) 121 tasks from the UCI machine learning\nrepository, on (b) drug discovery benchmarks, and on (c) astronomy tasks with\nstandard FNNs and other machine learning methods such as random forests and\nsupport vector machines. SNNs significantly outperformed all competing FNN\nmethods at 121 UCI tasks, outperformed all competing methods at the Tox21\ndataset, and set a new record at an astronomy data set. The winning SNN\narchitectures are often very deep. Implementations are available at:\ngithub.com/bioinf-jku/SNNs.\n", "versions": [{"version": "v1", "created": "Thu, 8 Jun 2017 11:14:24 GMT"}, {"version": "v2", "created": "Sat, 10 Jun 2017 12:01:44 GMT"}, {"version": "v3", "created": "Thu, 22 Jun 2017 10:46:17 GMT"}, {"version": "v4", "created": "Wed, 6 Sep 2017 13:33:53 GMT"}, {"version": "v5", "created": "Thu, 7 Sep 2017 10:39:00 GMT"}], "update_date": "2018-01-31", "authors_parsed": [["Klambauer", "G\u00fcnter", ""], ["Unterthiner", "Thomas", ""], ["Mayr", "Andreas", ""], ["Hochreiter", "Sepp", ""]]}, {"id": "1706.02524", "submitter": "Hyunjik Kim", "authors": "Hyunjik Kim and Yee Whye Teh", "title": "Scaling up the Automatic Statistician: Scalable Structure Discovery\n  using Gaussian Processes", "comments": "AISTATS 2018 (oral)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automating statistical modelling is a challenging problem in artificial\nintelligence. The Automatic Statistician takes a first step in this direction,\nby employing a kernel search algorithm with Gaussian Processes (GP) to provide\ninterpretable statistical models for regression problems. However this does not\nscale due to its $O(N^3)$ running time for the model selection. We propose\nScalable Kernel Composition (SKC), a scalable kernel search algorithm that\nextends the Automatic Statistician to bigger data sets. In doing so, we derive\na cheap upper bound on the GP marginal likelihood that sandwiches the marginal\nlikelihood with the variational lower bound . We show that the upper bound is\nsignificantly tighter than the lower bound and thus useful for model selection.\n", "versions": [{"version": "v1", "created": "Thu, 8 Jun 2017 11:41:51 GMT"}, {"version": "v2", "created": "Wed, 14 Feb 2018 12:56:33 GMT"}], "update_date": "2018-02-15", "authors_parsed": [["Kim", "Hyunjik", ""], ["Teh", "Yee Whye", ""]]}, {"id": "1706.02562", "submitter": "Benjamin Rubinstein", "authors": "Benjamin I. P. Rubinstein, Francesco Ald\\`a", "title": "Pain-Free Random Differential Privacy with Sensitivity Sampling", "comments": "12 pages, 9 figures, 1 table; full report of paper accepted into\n  ICML'2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR cs.DB stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Popular approaches to differential privacy, such as the Laplace and\nexponential mechanisms, calibrate randomised smoothing through global\nsensitivity of the target non-private function. Bounding such sensitivity is\noften a prohibitively complex analytic calculation. As an alternative, we\npropose a straightforward sampler for estimating sensitivity of non-private\nmechanisms. Since our sensitivity estimates hold with high probability, any\nmechanism that would be $(\\epsilon,\\delta)$-differentially private under\nbounded global sensitivity automatically achieves\n$(\\epsilon,\\delta,\\gamma)$-random differential privacy (Hall et al., 2012),\nwithout any target-specific calculations required. We demonstrate on worked\nexample learners how our usable approach adopts a naturally-relaxed privacy\nguarantee, while achieving more accurate releases even for non-private\nfunctions that are black-box computer programs.\n", "versions": [{"version": "v1", "created": "Thu, 8 Jun 2017 13:06:34 GMT"}], "update_date": "2017-06-12", "authors_parsed": [["Rubinstein", "Benjamin I. P.", ""], ["Ald\u00e0", "Francesco", ""]]}, {"id": "1706.02582", "submitter": "Stefan Steinerberger", "authors": "George C. Linderman, Stefan Steinerberger", "title": "Clustering with t-SNE, provably", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  t-distributed Stochastic Neighborhood Embedding (t-SNE), a clustering and\nvisualization method proposed by van der Maaten & Hinton in 2008, has rapidly\nbecome a standard tool in a number of natural sciences. Despite its\noverwhelming success, there is a distinct lack of mathematical foundations and\nthe inner workings of the algorithm are not well understood. The purpose of\nthis paper is to prove that t-SNE is able to recover well-separated clusters;\nmore precisely, we prove that t-SNE in the `early exaggeration' phase, an\noptimization technique proposed by van der Maaten & Hinton (2008) and van der\nMaaten (2014), can be rigorously analyzed. As a byproduct, the proof suggests\nnovel ways for setting the exaggeration parameter $\\alpha$ and step size $h$.\nNumerical examples illustrate the effectiveness of these rules: in particular,\nthe quality of embedding of topological structures (e.g. the swiss roll)\nimproves. We also discuss a connection to spectral clustering methods.\n", "versions": [{"version": "v1", "created": "Thu, 8 Jun 2017 13:44:15 GMT"}], "update_date": "2017-06-09", "authors_parsed": [["Linderman", "George C.", ""], ["Steinerberger", "Stefan", ""]]}, {"id": "1706.02613", "submitter": "Eran Malach", "authors": "Eran Malach, Shai Shalev-Shwartz", "title": "Decoupling \"when to update\" from \"how to update\"", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning requires data. A useful approach to obtain data is to be\ncreative and mine data from various sources, that were created for different\npurposes. Unfortunately, this approach often leads to noisy labels. In this\npaper, we propose a meta algorithm for tackling the noisy labels problem. The\nkey idea is to decouple \"when to update\" from \"how to update\". We demonstrate\nthe effectiveness of our algorithm by mining data for gender classification by\ncombining the Labeled Faces in the Wild (LFW) face recognition dataset with a\ntextual genderizing service, which leads to a noisy dataset. While our approach\nis very simple to implement, it leads to state-of-the-art results. We analyze\nsome convergence properties of the proposed algorithm.\n", "versions": [{"version": "v1", "created": "Thu, 8 Jun 2017 14:37:45 GMT"}, {"version": "v2", "created": "Mon, 26 Mar 2018 08:42:37 GMT"}], "update_date": "2018-03-28", "authors_parsed": [["Malach", "Eran", ""], ["Shalev-Shwartz", "Shai", ""]]}, {"id": "1706.02633", "submitter": "Stephanie L. Hyland", "authors": "Crist\\'obal Esteban, Stephanie L. Hyland, Gunnar R\\\"atsch", "title": "Real-valued (Medical) Time Series Generation with Recurrent Conditional\n  GANs", "comments": "13 pages, 4 figures, 3 tables (update with differential privacy)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generative Adversarial Networks (GANs) have shown remarkable success as a\nframework for training models to produce realistic-looking data. In this work,\nwe propose a Recurrent GAN (RGAN) and Recurrent Conditional GAN (RCGAN) to\nproduce realistic real-valued multi-dimensional time series, with an emphasis\non their application to medical data. RGANs make use of recurrent neural\nnetworks in the generator and the discriminator. In the case of RCGANs, both of\nthese RNNs are conditioned on auxiliary information. We demonstrate our models\nin a set of toy datasets, where we show visually and quantitatively (using\nsample likelihood and maximum mean discrepancy) that they can successfully\ngenerate realistic time-series. We also describe novel evaluation methods for\nGANs, where we generate a synthetic labelled training dataset, and evaluate on\na real test set the performance of a model trained on the synthetic data, and\nvice-versa. We illustrate with these metrics that RCGANs can generate\ntime-series data useful for supervised training, with only minor degradation in\nperformance on real test data. This is demonstrated on digit classification\nfrom 'serialised' MNIST and by training an early warning system on a medical\ndataset of 17,000 patients from an intensive care unit. We further discuss and\nanalyse the privacy concerns that may arise when using RCGANs to generate\nrealistic synthetic medical time series data.\n", "versions": [{"version": "v1", "created": "Thu, 8 Jun 2017 15:19:02 GMT"}, {"version": "v2", "created": "Mon, 4 Dec 2017 04:31:38 GMT"}], "update_date": "2017-12-05", "authors_parsed": [["Esteban", "Crist\u00f3bal", ""], ["Hyland", "Stephanie L.", ""], ["R\u00e4tsch", "Gunnar", ""]]}, {"id": "1706.02645", "submitter": "Tom Viering", "authors": "Tom J. Viering, Jesse H. Krijthe, Marco Loog", "title": "Nuclear Discrepancy for Active Learning", "comments": "32 pages, 5 figures, 4 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Active learning algorithms propose which unlabeled objects should be queried\nfor their labels to improve a predictive model the most. We study active\nlearners that minimize generalization bounds and uncover relationships between\nthese bounds that lead to an improved approach to active learning. In\nparticular we show the relation between the bound of the state-of-the-art\nMaximum Mean Discrepancy (MMD) active learner, the bound of the Discrepancy,\nand a new and looser bound that we refer to as the Nuclear Discrepancy bound.\nWe motivate this bound by a probabilistic argument: we show it considers\nsituations which are more likely to occur. Our experiments indicate that active\nlearning using the tightest Discrepancy bound performs the worst in terms of\nthe squared loss. Overall, our proposed loosest Nuclear Discrepancy\ngeneralization bound performs the best. We confirm our probabilistic argument\nempirically: the other bounds focus on more pessimistic scenarios that are\nrarer in practice. We conclude that tightness of bounds is not always of main\nimportance and that active learning methods should concentrate on realistic\nscenarios in order to improve performance.\n", "versions": [{"version": "v1", "created": "Thu, 8 Jun 2017 15:35:28 GMT"}], "update_date": "2017-06-09", "authors_parsed": [["Viering", "Tom J.", ""], ["Krijthe", "Jesse H.", ""], ["Loog", "Marco", ""]]}, {"id": "1706.02677", "submitter": "Ross Girshick", "authors": "Priya Goyal, Piotr Doll\\'ar, Ross Girshick, Pieter Noordhuis, Lukasz\n  Wesolowski, Aapo Kyrola, Andrew Tulloch, Yangqing Jia, Kaiming He", "title": "Accurate, Large Minibatch SGD: Training ImageNet in 1 Hour", "comments": "Tech report (v2: correct typos)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.DC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning thrives with large neural networks and large datasets. However,\nlarger networks and larger datasets result in longer training times that impede\nresearch and development progress. Distributed synchronous SGD offers a\npotential solution to this problem by dividing SGD minibatches over a pool of\nparallel workers. Yet to make this scheme efficient, the per-worker workload\nmust be large, which implies nontrivial growth in the SGD minibatch size. In\nthis paper, we empirically show that on the ImageNet dataset large minibatches\ncause optimization difficulties, but when these are addressed the trained\nnetworks exhibit good generalization. Specifically, we show no loss of accuracy\nwhen training with large minibatch sizes up to 8192 images. To achieve this\nresult, we adopt a hyper-parameter-free linear scaling rule for adjusting\nlearning rates as a function of minibatch size and develop a new warmup scheme\nthat overcomes optimization challenges early in training. With these simple\ntechniques, our Caffe2-based system trains ResNet-50 with a minibatch size of\n8192 on 256 GPUs in one hour, while matching small minibatch accuracy. Using\ncommodity hardware, our implementation achieves ~90% scaling efficiency when\nmoving from 8 to 256 GPUs. Our findings enable training visual recognition\nmodels on internet-scale data with high efficiency.\n", "versions": [{"version": "v1", "created": "Thu, 8 Jun 2017 16:51:53 GMT"}, {"version": "v2", "created": "Mon, 30 Apr 2018 21:53:41 GMT"}], "update_date": "2018-05-02", "authors_parsed": [["Goyal", "Priya", ""], ["Doll\u00e1r", "Piotr", ""], ["Girshick", "Ross", ""], ["Noordhuis", "Pieter", ""], ["Wesolowski", "Lukasz", ""], ["Kyrola", "Aapo", ""], ["Tulloch", "Andrew", ""], ["Jia", "Yangqing", ""], ["He", "Kaiming", ""]]}, {"id": "1706.02684", "submitter": "Jean-Charles Vialatte", "authors": "Jean-Charles Vialatte, Vincent Gripon, Gilles Coppin", "title": "Learning Local Receptive Fields and their Weight Sharing Scheme on\n  Graphs", "comments": "To appear in 2017, 5th IEEE Global Conference on Signal and\n  Information Processing, 5 pages, 3 figures, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a simple and generic layer formulation that extends the properties\nof convolutional layers to any domain that can be described by a graph. Namely,\nwe use the support of its adjacency matrix to design learnable weight sharing\nfilters able to exploit the underlying structure of signals in the same fashion\nas for images. The proposed formulation makes it possible to learn the weights\nof the filter as well as a scheme that controls how they are shared across the\ngraph. We perform validation experiments with image datasets and show that\nthese filters offer performances comparable with convolutional ones.\n", "versions": [{"version": "v1", "created": "Thu, 8 Jun 2017 17:03:34 GMT"}, {"version": "v2", "created": "Wed, 20 Sep 2017 14:56:59 GMT"}, {"version": "v3", "created": "Thu, 5 Oct 2017 16:32:20 GMT"}], "update_date": "2017-10-06", "authors_parsed": [["Vialatte", "Jean-Charles", ""], ["Gripon", "Vincent", ""], ["Coppin", "Gilles", ""]]}, {"id": "1706.02690", "submitter": "Yixuan Li", "authors": "Shiyu Liang, Yixuan Li and R. Srikant", "title": "Enhancing The Reliability of Out-of-distribution Image Detection in\n  Neural Networks", "comments": "ICLR 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of detecting out-of-distribution images in neural\nnetworks. We propose ODIN, a simple and effective method that does not require\nany change to a pre-trained neural network. Our method is based on the\nobservation that using temperature scaling and adding small perturbations to\nthe input can separate the softmax score distributions between in- and\nout-of-distribution images, allowing for more effective detection. We show in a\nseries of experiments that ODIN is compatible with diverse network\narchitectures and datasets. It consistently outperforms the baseline approach\nby a large margin, establishing a new state-of-the-art performance on this\ntask. For example, ODIN reduces the false positive rate from the baseline 34.7%\nto 4.3% on the DenseNet (applied to CIFAR-10) when the true positive rate is\n95%.\n", "versions": [{"version": "v1", "created": "Thu, 8 Jun 2017 17:43:56 GMT"}, {"version": "v2", "created": "Wed, 21 Jun 2017 17:48:16 GMT"}, {"version": "v3", "created": "Thu, 9 Nov 2017 18:33:49 GMT"}, {"version": "v4", "created": "Sun, 25 Feb 2018 18:51:13 GMT"}, {"version": "v5", "created": "Sun, 30 Aug 2020 16:50:36 GMT"}], "update_date": "2020-09-01", "authors_parsed": [["Liang", "Shiyu", ""], ["Li", "Yixuan", ""], ["Srikant", "R.", ""]]}, {"id": "1706.02733", "submitter": "Moritz Hardt", "authors": "Moritz Hardt", "title": "Climbing a shaky ladder: Better adaptive risk estimation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We revisit the \\emph{leaderboard problem} introduced by Blum and Hardt (2015)\nin an effort to reduce overfitting in machine learning benchmarks. We show that\na randomized version of their Ladder algorithm achieves leaderboard error\nO(1/n^{0.4}) compared with the previous best rate of O(1/n^{1/3}).\n  Short of proving that our algorithm is optimal, we point out a major obstacle\ntoward further progress. Specifically, any improvement to our upper bound would\nlead to asymptotic improvements in the general adaptive estimation setting as\nhave remained elusive in recent years. This connection also directly leads to\nlower bounds for specific classes of algorithms. In particular, we exhibit a\nnew attack on the leaderboard algorithm that both theoretically and empirically\ndistinguishes between our algorithm and previous leaderboard algorithms.\n", "versions": [{"version": "v1", "created": "Thu, 8 Jun 2017 18:48:38 GMT"}], "update_date": "2017-06-12", "authors_parsed": [["Hardt", "Moritz", ""]]}, {"id": "1706.02744", "submitter": "Niki Kilbertus", "authors": "Niki Kilbertus, Mateo Rojas-Carulla, Giambattista Parascandolo, Moritz\n  Hardt, Dominik Janzing, Bernhard Sch\\\"olkopf", "title": "Avoiding Discrimination through Causal Reasoning", "comments": "Advances in Neural Information Processing Systems 30, 2017\n  http://papers.nips.cc/paper/6668-avoiding-discrimination-through-causal-reasoning", "journal-ref": "Advances in Neural Information Processing Systems 30, 2017, p.\n  656--666", "doi": null, "report-no": null, "categories": "stat.ML cs.CY cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent work on fairness in machine learning has focused on various\nstatistical discrimination criteria and how they trade off. Most of these\ncriteria are observational: They depend only on the joint distribution of\npredictor, protected attribute, features, and outcome. While convenient to work\nwith, observational criteria have severe inherent limitations that prevent them\nfrom resolving matters of fairness conclusively.\n  Going beyond observational criteria, we frame the problem of discrimination\nbased on protected attributes in the language of causal reasoning. This\nviewpoint shifts attention from \"What is the right fairness criterion?\" to\n\"What do we want to assume about the causal data generating process?\" Through\nthe lens of causality, we make several contributions. First, we crisply\narticulate why and when observational criteria fail, thus formalizing what was\nbefore a matter of opinion. Second, our approach exposes previously ignored\nsubtleties and why they are fundamental to the problem. Finally, we put forward\nnatural causal non-discrimination criteria and develop algorithms that satisfy\nthem.\n", "versions": [{"version": "v1", "created": "Thu, 8 Jun 2017 19:50:56 GMT"}, {"version": "v2", "created": "Sun, 21 Jan 2018 16:39:51 GMT"}], "update_date": "2018-01-23", "authors_parsed": [["Kilbertus", "Niki", ""], ["Rojas-Carulla", "Mateo", ""], ["Parascandolo", "Giambattista", ""], ["Hardt", "Moritz", ""], ["Janzing", "Dominik", ""], ["Sch\u00f6lkopf", "Bernhard", ""]]}, {"id": "1706.02761", "submitter": "Li Jing", "authors": "Li Jing, Caglar Gulcehre, John Peurifoy, Yichen Shen, Max Tegmark,\n  Marin Solja\\v{c}i\\'c, Yoshua Bengio", "title": "Gated Orthogonal Recurrent Units: On Learning to Forget", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a novel recurrent neural network (RNN) based model that combines\nthe remembering ability of unitary RNNs with the ability of gated RNNs to\neffectively forget redundant/irrelevant information in its memory. We achieve\nthis by extending unitary RNNs with a gating mechanism. Our model is able to\noutperform LSTMs, GRUs and Unitary RNNs on several long-term dependency\nbenchmark tasks. We empirically both show the orthogonal/unitary RNNs lack the\nability to forget and also the ability of GORU to simultaneously remember long\nterm dependencies while forgetting irrelevant information. This plays an\nimportant role in recurrent neural networks. We provide competitive results\nalong with an analysis of our model on many natural sequential tasks including\nthe bAbI Question Answering, TIMIT speech spectrum prediction, Penn TreeBank,\nand synthetic tasks that involve long-term dependencies such as algorithmic,\nparenthesis, denoising and copying tasks.\n", "versions": [{"version": "v1", "created": "Thu, 8 Jun 2017 20:40:32 GMT"}, {"version": "v2", "created": "Thu, 6 Jul 2017 13:47:17 GMT"}, {"version": "v3", "created": "Wed, 25 Oct 2017 15:17:03 GMT"}], "update_date": "2017-10-26", "authors_parsed": [["Jing", "Li", ""], ["Gulcehre", "Caglar", ""], ["Peurifoy", "John", ""], ["Shen", "Yichen", ""], ["Tegmark", "Max", ""], ["Solja\u010di\u0107", "Marin", ""], ["Bengio", "Yoshua", ""]]}, {"id": "1706.02776", "submitter": "Matt Shannon", "authors": "Matt Shannon", "title": "Optimizing expected word error rate via sampling for speech recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  State-level minimum Bayes risk (sMBR) training has become the de facto\nstandard for sequence-level training of speech recognition acoustic models. It\nhas an elegant formulation using the expectation semiring, and gives large\nimprovements in word error rate (WER) over models trained solely using\ncross-entropy (CE) or connectionist temporal classification (CTC). sMBR\ntraining optimizes the expected number of frames at which the reference and\nhypothesized acoustic states differ. It may be preferable to optimize the\nexpected WER, but WER does not interact well with the expectation semiring, and\nprevious approaches based on computing expected WER exactly involve expanding\nthe lattices used during training. In this paper we show how to perform\noptimization of the expected WER by sampling paths from the lattices used\nduring conventional sMBR training. The gradient of the expected WER is itself\nan expectation, and so may be approximated using Monte Carlo sampling. We show\nexperimentally that optimizing WER during acoustic model training gives 5%\nrelative improvement in WER over a well-tuned sMBR baseline on a 2-channel\nquery recognition task (Google Home).\n", "versions": [{"version": "v1", "created": "Thu, 8 Jun 2017 21:14:48 GMT"}], "update_date": "2017-06-12", "authors_parsed": [["Shannon", "Matt", ""]]}, {"id": "1706.02780", "submitter": "Victor Silva", "authors": "Marcelo Souza Nery, Roque Anderson Teixeira, Victor do Nascimento\n  Silva, Adriano Alonso Veloso", "title": "Setting Players' Behaviors in World of Warcraft through Semi-Supervised\n  Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Digital games are one of the major and most important fields on the\nentertainment domain, which also involves cinema and music. Numerous attempts\nhave been done to improve the quality of the games including more realistic\nartistic production and computer science. Assessing the player's behavior, a\ntask known as player modeling, is currently the need of the hour which leads to\npossible improvements in terms of: (i) better game interaction experience, (ii)\nbetter exploitation of the relationship between players, and (iii)\nincreasing/maintaining the number of players interested in the game. In this\npaper we model players using the basic four behaviors proposed in\n\\cite{BartleArtigo}, namely: achiever, explorer, socializer and killer. Our\nanalysis is carried out using data obtained from the game \"World of Warcraft\"\nover 3 years (2006 $-$ 2009). We employ a semi-supervised learning technique in\norder to find out characteristics that possibly impact player's behavior.\n", "versions": [{"version": "v1", "created": "Thu, 8 Jun 2017 21:48:46 GMT"}], "update_date": "2017-06-12", "authors_parsed": [["Nery", "Marcelo Souza", ""], ["Teixeira", "Roque Anderson", ""], ["Silva", "Victor do Nascimento", ""], ["Veloso", "Adriano Alonso", ""]]}, {"id": "1706.02803", "submitter": "Shusen Wang", "authors": "Shusen Wang and Alex Gittens and Michael W. Mahoney", "title": "Scalable Kernel K-Means Clustering with Nystrom Approximation:\n  Relative-Error Bounds", "comments": null, "journal-ref": "Journal of Machine Learning Research 20 (2019) 1-49", "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Kernel $k$-means clustering can correctly identify and extract a far more\nvaried collection of cluster structures than the linear $k$-means clustering\nalgorithm. However, kernel $k$-means clustering is computationally expensive\nwhen the non-linear feature map is high-dimensional and there are many input\npoints. Kernel approximation, e.g., the Nystr\\\"om method, has been applied in\nprevious works to approximately solve kernel learning problems when both of the\nabove conditions are present. This work analyzes the application of this\nparadigm to kernel $k$-means clustering, and shows that applying the linear\n$k$-means clustering algorithm to $\\frac{k}{\\epsilon} (1 + o(1))$ features\nconstructed using a so-called rank-restricted Nystr\\\"om approximation results\nin cluster assignments that satisfy a $1 + \\epsilon$ approximation ratio in\nterms of the kernel $k$-means cost function, relative to the guarantee provided\nby the same algorithm without the use of the Nystr\\\"om method. As part of the\nanalysis, this work establishes a novel $1 + \\epsilon$ relative-error trace\nnorm guarantee for low-rank approximation using the rank-restricted Nystr\\\"om\napproximation. Empirical evaluations on the $8.1$ million instance MNIST8M\ndataset demonstrate the scalability and usefulness of kernel $k$-means\nclustering with Nystr\\\"om approximation. This work argues that spectral\nclustering using Nystr\\\"om approximation---a popular and computationally\nefficient, but theoretically unsound approach to non-linear clustering---should\nbe replaced with the efficient and theoretically sound combination of kernel\n$k$-means clustering with Nystr\\\"om approximation. The superior performance of\nthe latter approach is empirically verified.\n", "versions": [{"version": "v1", "created": "Fri, 9 Jun 2017 01:17:20 GMT"}, {"version": "v2", "created": "Sat, 24 Jun 2017 20:17:36 GMT"}, {"version": "v3", "created": "Thu, 10 Jan 2019 02:33:30 GMT"}, {"version": "v4", "created": "Sun, 10 Feb 2019 17:21:08 GMT"}], "update_date": "2019-02-12", "authors_parsed": [["Wang", "Shusen", ""], ["Gittens", "Alex", ""], ["Mahoney", "Michael W.", ""]]}, {"id": "1706.02815", "submitter": "Hao He", "authors": "Hao He, Bo Xin, David Wipf", "title": "From Bayesian Sparsity to Gated Recurrent Nets", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The iterations of many first-order algorithms, when applied to minimizing\ncommon regularized regression functions, often resemble neural network layers\nwith pre-specified weights. This observation has prompted the development of\nlearning-based approaches that purport to replace these iterations with\nenhanced surrogates forged as DNN models from available training data. For\nexample, important NP-hard sparse estimation problems have recently benefitted\nfrom this genre of upgrade, with simple feedforward or recurrent networks\nousting proximal gradient-based iterations. Analogously, this paper\ndemonstrates that more powerful Bayesian algorithms for promoting sparsity,\nwhich rely on complex multi-loop majorization-minimization techniques, mirror\nthe structure of more sophisticated long short-term memory (LSTM) networks, or\nalternative gated feedback networks previously designed for sequence\nprediction. As part of this development, we examine the parallels between\nlatent variable trajectories operating across multiple time-scales during\noptimization, and the activations within deep network structures designed to\nadaptively model such characteristic sequences. The resulting insights lead to\na novel sparse estimation system that, when granted training data, can estimate\noptimal solutions efficiently in regimes where other algorithms fail, including\npractical direction-of-arrival (DOA) and 3D geometry recovery problems. The\nunderlying principles we expose are also suggestive of a learning process for a\nricher class of multi-loop algorithms in other domains.\n", "versions": [{"version": "v1", "created": "Fri, 9 Jun 2017 02:56:54 GMT"}, {"version": "v2", "created": "Wed, 2 Aug 2017 17:03:43 GMT"}], "update_date": "2017-08-03", "authors_parsed": [["He", "Hao", ""], ["Xin", "Bo", ""], ["Wipf", "David", ""]]}, {"id": "1706.02857", "submitter": "Matthias Gall\\'e", "authors": "Ariadna Quattoni, Xavier Carreras, Matthias Gall\\'e", "title": "A Maximum Matching Algorithm for Basis Selection in Spectral Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.FL stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a solution to scale spectral algorithms for learning sequence\nfunctions. We are interested in the case where these functions are sparse (that\nis, for most sequences they return 0). Spectral algorithms reduce the learning\nproblem to the task of computing an SVD decomposition over a special type of\nmatrix called the Hankel matrix. This matrix is designed to capture the\nrelevant statistics of the training sequences. What is crucial is that to\ncapture long range dependencies we must consider very large Hankel matrices.\nThus the computation of the SVD becomes a critical bottleneck. Our solution\nfinds a subset of rows and columns of the Hankel that realizes a compact and\ninformative Hankel submatrix. The novelty lies in the way that this subset is\nselected: we exploit a maximal bipartite matching combinatorial algorithm to\nlook for a sub-block with full structural rank, and show how computation of\nthis sub-block can be further improved by exploiting the specific structure of\nHankel matrices.\n", "versions": [{"version": "v1", "created": "Fri, 9 Jun 2017 07:43:46 GMT"}], "update_date": "2017-06-12", "authors_parsed": [["Quattoni", "Ariadna", ""], ["Carreras", "Xavier", ""], ["Gall\u00e9", "Matthias", ""]]}, {"id": "1706.02869", "submitter": "Zheng Xu", "authors": "Zheng Xu, Gavin Taylor, Hao Li, Mario Figueiredo, Xiaoming Yuan, Tom\n  Goldstein", "title": "Adaptive Consensus ADMM for Distributed Optimization", "comments": "ICML 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NA cs.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The alternating direction method of multipliers (ADMM) is commonly used for\ndistributed model fitting problems, but its performance and reliability depend\nstrongly on user-defined penalty parameters. We study distributed ADMM methods\nthat boost performance by using different fine-tuned algorithm parameters on\neach worker node. We present a O(1/k) convergence rate for adaptive ADMM\nmethods with node-specific parameters, and propose adaptive consensus ADMM\n(ACADMM), which automatically tunes parameters without user oversight.\n", "versions": [{"version": "v1", "created": "Fri, 9 Jun 2017 08:52:37 GMT"}, {"version": "v2", "created": "Tue, 20 Jun 2017 05:22:11 GMT"}], "update_date": "2017-06-21", "authors_parsed": [["Xu", "Zheng", ""], ["Taylor", "Gavin", ""], ["Li", "Hao", ""], ["Figueiredo", "Mario", ""], ["Yuan", "Xiaoming", ""], ["Goldstein", "Tom", ""]]}, {"id": "1706.02899", "submitter": "Yanfei Zhang", "authors": "Yanfei Zhang and Junbin Gao", "title": "Assessing the Performance of Deep Learning Algorithms for Newsvendor\n  Problem", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In retailer management, the Newsvendor problem has widely attracted attention\nas one of basic inventory models. In the traditional approach to solving this\nproblem, it relies on the probability distribution of the demand. In theory, if\nthe probability distribution is known, the problem can be considered as fully\nsolved. However, in any real world scenario, it is almost impossible to even\napproximate or estimate a better probability distribution for the demand. In\nrecent years, researchers start adopting machine learning approach to learn a\ndemand prediction model by using other feature information. In this paper, we\npropose a supervised learning that optimizes the demand quantities for products\nbased on feature information. We demonstrate that the original Newsvendor loss\nfunction as the training objective outperforms the recently suggested quadratic\nloss function. The new algorithm has been assessed on both the synthetic data\nand real-world data, demonstrating better performance.\n", "versions": [{"version": "v1", "created": "Fri, 9 Jun 2017 11:21:35 GMT"}], "update_date": "2017-06-12", "authors_parsed": [["Zhang", "Yanfei", ""], ["Gao", "Junbin", ""]]}, {"id": "1706.02901", "submitter": "Che-Wei Huang", "authors": "Che-Wei Huang, Shrikanth. S. Narayanan", "title": "Characterizing Types of Convolution in Deep Convolutional Recurrent\n  Neural Networks for Robust Speech Emotion Recognition", "comments": "Revised Submission to IEEE Transactions", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL cs.MM cs.SD", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep convolutional neural networks are being actively investigated in a wide\nrange of speech and audio processing applications including speech recognition,\naudio event detection and computational paralinguistics, owing to their ability\nto reduce factors of variations, for learning from speech. However, studies\nhave suggested to favor a certain type of convolutional operations when\nbuilding a deep convolutional neural network for speech applications although\nthere has been promising results using different types of convolutional\noperations. In this work, we study four types of convolutional operations on\ndifferent input features for speech emotion recognition under noisy and clean\nconditions in order to derive a comprehensive understanding. Since affective\nbehavioral information has been shown to reflect temporally varying of mental\nstate and convolutional operation are applied locally in time, all deep neural\nnetworks share a deep recurrent sub-network architecture for further temporal\nmodeling. We present detailed quantitative module-wise performance analysis to\ngain insights into information flows within the proposed architectures. In\nparticular, we demonstrate the interplay of affective information and the other\nirrelevant information during the progression from one module to another.\nFinally we show that all of our deep neural networks provide state-of-the-art\nperformance on the eNTERFACE'05 corpus.\n", "versions": [{"version": "v1", "created": "Wed, 7 Jun 2017 15:17:21 GMT"}, {"version": "v2", "created": "Sat, 13 Jan 2018 19:38:21 GMT"}], "update_date": "2018-01-16", "authors_parsed": [["Huang", "Che-Wei", ""], ["Narayanan", "Shrikanth. S.", ""]]}, {"id": "1706.02921", "submitter": "Filip Korzeniowski", "authors": "Filip Korzeniowski, Gerhard Widmer", "title": "End-to-End Musical Key Estimation Using a Convolutional Neural Network", "comments": "First published in the Proceedings of the 25th European Signal\n  Processing Conference (EUSIPCO-2017) in 2017, published by EURASIP", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.SD", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an end-to-end system for musical key estimation, based on a\nconvolutional neural network. The proposed system not only out-performs\nexisting key estimation methods proposed in the academic literature; it is also\ncapable of learning a unified model for diverse musical genres that performs\ncomparably to existing systems specialised for specific genres. Our experiments\nconfirm that different genres do differ in their interpretation of tonality,\nand thus a system tuned e.g. for pop music performs subpar on pieces of\nelectronic music. They also reveal that such cross-genre setups evoke specific\ntypes of error (predicting the relative or parallel minor). However, using the\ndata-driven approach proposed in this paper, we can train models that deal with\nmultiple musical styles adequately, and without major losses in accuracy.\n", "versions": [{"version": "v1", "created": "Fri, 9 Jun 2017 12:33:23 GMT"}], "update_date": "2017-06-12", "authors_parsed": [["Korzeniowski", "Filip", ""], ["Widmer", "Gerhard", ""]]}, {"id": "1706.02949", "submitter": "Kumar Sankar Ray", "authors": "Srikanta Kolay, Kumar Sankar Ray, Abhoy Chand Mondal", "title": "K+ Means : An Enhancement Over K-Means Clustering Algorithm", "comments": "Authors: Co-author's name added Section 3: Step (a) and (b) of\n  K+Means algorithm are merged for simplicity. Section 3.1: K+ Means algorithm\n  complexity rectified. Section 4.3: Figure-7 and Figure-8 modified for clarity", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  K-means (MacQueen, 1967) [1] is one of the simplest unsupervised learning\nalgorithms that solve the well-known clustering problem. The procedure follows\na simple and easy way to classify a given data set to a predefined, say K\nnumber of clusters. Determination of K is a difficult job and it is not known\nthat which value of K can partition the objects as per our intuition. To\novercome this problem we proposed K+ Means algorithm. This algorithm is an\nenhancement over K-Means algorithm.\n", "versions": [{"version": "v1", "created": "Thu, 8 Jun 2017 10:06:25 GMT"}, {"version": "v2", "created": "Thu, 22 Jun 2017 13:25:51 GMT"}], "update_date": "2017-06-23", "authors_parsed": [["Kolay", "Srikanta", ""], ["Ray", "Kumar Sankar", ""], ["Mondal", "Abhoy Chand", ""]]}, {"id": "1706.02985", "submitter": "Ratthachat Chatpatanasiri", "authors": "Haizhen Wang, Ratthachat Chatpatanasiri, Pairote Sattayatham", "title": "Stock Trading Using PE ratio: A Dynamic Bayesian Network Modeling on\n  Behavioral Finance and Fundamental Investment", "comments": "34 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CE cs.AI cs.LG q-fin.GN", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  On a daily investment decision in a security market, the price earnings (PE)\nratio is one of the most widely applied methods being used as a firm valuation\ntool by investment experts. Unfortunately, recent academic developments in\nfinancial econometrics and machine learning rarely look at this tool. In\npractice, fundamental PE ratios are often estimated only by subjective expert\nopinions. The purpose of this research is to formalize a process of fundamental\nPE estimation by employing advanced dynamic Bayesian network (DBN) methodology.\nThe estimated PE ratio from our model can be used either as a information\nsupport for an expert to make investment decisions, or as an automatic trading\nsystem illustrated in experiments. Forward-backward inference and EM parameter\nestimation algorithms are derived with respect to the proposed DBN structure.\nUnlike existing works in literatures, the economic interpretation of our DBN\nmodel is well-justified by behavioral finance evidences of volatility. A simple\nbut practical trading strategy is invented based on the result of Bayesian\ninference. Extensive experiments show that our trading strategy equipped with\nthe inferenced PE ratios consistently outperforms standard investment\nbenchmarks.\n", "versions": [{"version": "v1", "created": "Thu, 25 May 2017 13:24:22 GMT"}], "update_date": "2017-06-12", "authors_parsed": [["Wang", "Haizhen", ""], ["Chatpatanasiri", "Ratthachat", ""], ["Sattayatham", "Pairote", ""]]}, {"id": "1706.02986", "submitter": "Emilie Kaufmann", "authors": "Emilie Kaufmann (CNRS, CRIStAL, SEQUEL), Wouter Koolen (CWI)", "title": "Monte-Carlo Tree Search by Best Arm Identification", "comments": "Advances in Neural Information Processing Systems (NIPS), Dec 2017,\n  Long Beach, United States", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent advances in bandit tools and techniques for sequential learning are\nsteadily enabling new applications and are promising the resolution of a range\nof challenging related problems. We study the game tree search problem, where\nthe goal is to quickly identify the optimal move in a given game tree by\nsequentially sampling its stochastic payoffs. We develop new algorithms for\ntrees of arbitrary depth, that operate by summarizing all deeper levels of the\ntree into confidence intervals at depth one, and applying a best arm\nidentification procedure at the root. We prove new sample complexity guarantees\nwith a refined dependence on the problem instance. We show experimentally that\nour algorithms outperform existing elimination-based algorithms and match\nprevious special-purpose methods for depth-two trees.\n", "versions": [{"version": "v1", "created": "Fri, 9 Jun 2017 14:58:10 GMT"}, {"version": "v2", "created": "Mon, 6 Nov 2017 07:48:45 GMT"}], "update_date": "2017-11-07", "authors_parsed": [["Kaufmann", "Emilie", "", "CNRS, CRIStAL, SEQUEL"], ["Koolen", "Wouter", "", "CWI"]]}, {"id": "1706.02999", "submitter": "Theja Tulabandhula", "authors": "Anuj Mahajan and Theja Tulabandhula", "title": "Symmetry Learning for Function Approximation in Reinforcement Learning", "comments": "12 pages, 3 figures. A preliminary version appears in AAMAS 2017.\n  Also presented at the 3rd Multidisciplinary Conference on Reinforcement\n  Learning and Decision Making", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we explore methods to exploit symmetries for ensuring sample\nefficiency in reinforcement learning (RL), this problem deserves ever\nincreasing attention with the recent advances in the use of deep networks for\ncomplex RL tasks which require large amount of training data. We introduce a\nnovel method to detect symmetries using reward trails observed during episodic\nexperience and prove its completeness. We also provide a framework to\nincorporate the discovered symmetries for functional approximation. Finally we\nshow that the use of potential based reward shaping is especially effective for\nour symmetry exploitation mechanism. Experiments on various classical problems\nshow that our method improves the learning performance significantly by\nutilizing symmetry information.\n", "versions": [{"version": "v1", "created": "Fri, 9 Jun 2017 15:30:32 GMT"}], "update_date": "2017-06-12", "authors_parsed": [["Mahajan", "Anuj", ""], ["Tulabandhula", "Theja", ""]]}, {"id": "1706.03041", "submitter": "Andreas S{\\o}gaard", "authors": "Andreas S{\\o}gaard", "title": "Learning optimal wavelet bases using a neural network approach", "comments": "11 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A novel method for learning optimal, orthonormal wavelet bases for\nrepresenting 1- and 2D signals, based on parallels between the wavelet\ntransform and fully connected artificial neural networks, is described. The\nstructural similarities between these two concepts are reviewed and combined to\na \"wavenet\", allowing for the direct learning of optimal wavelet filter\ncoefficient through stochastic gradient descent with back-propagation over\nensembles of training inputs, where conditions on the filter coefficients for\nconstituting orthonormal wavelet bases are cast as quadratic regularisations\nterms. We describe the practical implementation of this method, and study its\nperformance for high-energy physics collision events for QCD $2 \\to 2$\nprocesses. It is shown that an optimal solution is found, even in a\nhigh-dimensional search space, and the implications of the result are\ndiscussed.\n", "versions": [{"version": "v1", "created": "Sat, 25 Mar 2017 15:46:01 GMT"}, {"version": "v2", "created": "Fri, 31 Aug 2018 10:31:57 GMT"}], "update_date": "2018-09-03", "authors_parsed": [["S\u00f8gaard", "Andreas", ""]]}, {"id": "1706.03059", "submitter": "{\\L}ukasz Kaiser", "authors": "Lukasz Kaiser, Aidan N. Gomez, Francois Chollet", "title": "Depthwise Separable Convolutions for Neural Machine Translation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Depthwise separable convolutions reduce the number of parameters and\ncomputation used in convolutional operations while increasing representational\nefficiency. They have been shown to be successful in image classification\nmodels, both in obtaining better models than previously possible for a given\nparameter count (the Xception architecture) and considerably reducing the\nnumber of parameters required to perform at a given level (the MobileNets\nfamily of architectures). Recently, convolutional sequence-to-sequence networks\nhave been applied to machine translation tasks with good results. In this work,\nwe study how depthwise separable convolutions can be applied to neural machine\ntranslation. We introduce a new architecture inspired by Xception and ByteNet,\ncalled SliceNet, which enables a significant reduction of the parameter count\nand amount of computation needed to obtain results like ByteNet, and, with a\nsimilar parameter count, achieves new state-of-the-art results. In addition to\nshowing that depthwise separable convolutions perform well for machine\ntranslation, we investigate the architectural changes that they enable: we\nobserve that thanks to depthwise separability, we can increase the length of\nconvolution windows, removing the need for filter dilation. We also introduce a\nnew \"super-separable\" convolution operation that further reduces the number of\nparameters and computational cost for obtaining state-of-the-art results.\n", "versions": [{"version": "v1", "created": "Fri, 9 Jun 2017 17:59:16 GMT"}, {"version": "v2", "created": "Fri, 16 Jun 2017 02:35:48 GMT"}], "update_date": "2017-06-19", "authors_parsed": [["Kaiser", "Lukasz", ""], ["Gomez", "Aidan N.", ""], ["Chollet", "Francois", ""]]}, {"id": "1706.03078", "submitter": "Alberto Bietti", "authors": "Alberto Bietti and Julien Mairal", "title": "Group Invariance, Stability to Deformations, and Complexity of Deep\n  Convolutional Representations", "comments": null, "journal-ref": "Journal of Machine Learning Research 20 (2019) 1-49", "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The success of deep convolutional architectures is often attributed in part\nto their ability to learn multiscale and invariant representations of natural\nsignals. However, a precise study of these properties and how they affect\nlearning guarantees is still missing. In this paper, we consider deep\nconvolutional representations of signals; we study their invariance to\ntranslations and to more general groups of transformations, their stability to\nthe action of diffeomorphisms, and their ability to preserve signal\ninformation. This analysis is carried by introducing a multilayer kernel based\non convolutional kernel networks and by studying the geometry induced by the\nkernel mapping. We then characterize the corresponding reproducing kernel\nHilbert space (RKHS), showing that it contains a large class of convolutional\nneural networks with homogeneous activation functions. This analysis allows us\nto separate data representation from learning, and to provide a canonical\nmeasure of model complexity, the RKHS norm, which controls both stability and\ngeneralization of any learned model. In addition to models in the constructed\nRKHS, our stability analysis also applies to convolutional networks with\ngeneric activations such as rectified linear units, and we discuss its\nrelationship with recent generalization bounds based on spectral norms.\n", "versions": [{"version": "v1", "created": "Fri, 9 Jun 2017 18:02:47 GMT"}, {"version": "v2", "created": "Tue, 14 Nov 2017 12:08:20 GMT"}, {"version": "v3", "created": "Sun, 8 Apr 2018 14:36:45 GMT"}, {"version": "v4", "created": "Wed, 10 Oct 2018 21:17:35 GMT"}], "update_date": "2019-02-14", "authors_parsed": [["Bietti", "Alberto", ""], ["Mairal", "Julien", ""]]}, {"id": "1706.03100", "submitter": "Philip Thomas", "authors": "Philip S. Thomas and Christoph Dann and Emma Brunskill", "title": "Decoupling Learning Rules from Representations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the artificial intelligence field, learning often corresponds to changing\nthe parameters of a parameterized function. A learning rule is an algorithm or\nmathematical expression that specifies precisely how the parameters should be\nchanged. When creating an artificial intelligence system, we must make two\ndecisions: what representation should be used (i.e., what parameterized\nfunction should be used) and what learning rule should be used to search\nthrough the resulting set of representable functions. Using most learning\nrules, these two decisions are coupled in a subtle (and often unintentional)\nway. That is, using the same learning rule with two different representations\nthat can represent the same sets of functions can result in two different\noutcomes. After arguing that this coupling is undesirable, particularly when\nusing artificial neural networks, we present a method for partially decoupling\nthese two decisions for a broad class of learning rules that span unsupervised\nlearning, reinforcement learning, and supervised learning.\n", "versions": [{"version": "v1", "created": "Fri, 9 Jun 2017 19:34:03 GMT"}], "update_date": "2017-06-13", "authors_parsed": [["Thomas", "Philip S.", ""], ["Dann", "Christoph", ""], ["Brunskill", "Emma", ""]]}, {"id": "1706.03149", "submitter": "Peter Bloem", "authors": "Peter Bloem and Steven de Rooij", "title": "An Expectation-Maximization Algorithm for the Fractal Inverse Problem", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an Expectation-Maximization algorithm for the fractal inverse\nproblem: the problem of fitting a fractal model to data. In our setting the\nfractals are Iterated Function Systems (IFS), with similitudes as the family of\ntransformations. The data is a point cloud in ${\\mathbb R}^H$ with arbitrary\ndimension $H$. Each IFS defines a probability distribution on ${\\mathbb R}^H$,\nso that the fractal inverse problem can be cast as a problem of parameter\nestimation. We show that the algorithm reconstructs well-known fractals from\ndata, with the model converging to high precision parameters. We also show the\nutility of the model as an approximation for datasources outside the IFS model\nclass.\n", "versions": [{"version": "v1", "created": "Fri, 9 Jun 2017 22:50:32 GMT"}, {"version": "v2", "created": "Fri, 30 Jun 2017 14:05:00 GMT"}], "update_date": "2017-07-03", "authors_parsed": [["Bloem", "Peter", ""], ["de Rooij", "Steven", ""]]}, {"id": "1706.03161", "submitter": "David Hallac", "authors": "David Hallac, Sagar Vare, Stephen Boyd, Jure Leskovec", "title": "Toeplitz Inverse Covariance-Based Clustering of Multivariate Time Series\n  Data", "comments": "This revised version fixes two small typos in the published version", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.SI math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Subsequence clustering of multivariate time series is a useful tool for\ndiscovering repeated patterns in temporal data. Once these patterns have been\ndiscovered, seemingly complicated datasets can be interpreted as a temporal\nsequence of only a small number of states, or clusters. For example, raw sensor\ndata from a fitness-tracking application can be expressed as a timeline of a\nselect few actions (i.e., walking, sitting, running). However, discovering\nthese patterns is challenging because it requires simultaneous segmentation and\nclustering of the time series. Furthermore, interpreting the resulting clusters\nis difficult, especially when the data is high-dimensional. Here we propose a\nnew method of model-based clustering, which we call Toeplitz Inverse\nCovariance-based Clustering (TICC). Each cluster in the TICC method is defined\nby a correlation network, or Markov random field (MRF), characterizing the\ninterdependencies between different observations in a typical subsequence of\nthat cluster. Based on this graphical representation, TICC simultaneously\nsegments and clusters the time series data. We solve the TICC problem through\nalternating minimization, using a variation of the expectation maximization\n(EM) algorithm. We derive closed-form solutions to efficiently solve the two\nresulting subproblems in a scalable way, through dynamic programming and the\nalternating direction method of multipliers (ADMM), respectively. We validate\nour approach by comparing TICC to several state-of-the-art baselines in a\nseries of synthetic experiments, and we then demonstrate on an automobile\nsensor dataset how TICC can be used to learn interpretable clusters in\nreal-world scenarios.\n", "versions": [{"version": "v1", "created": "Sat, 10 Jun 2017 00:52:05 GMT"}, {"version": "v2", "created": "Tue, 15 May 2018 00:09:20 GMT"}], "update_date": "2018-05-16", "authors_parsed": [["Hallac", "David", ""], ["Vare", "Sagar", ""], ["Boyd", "Stephen", ""], ["Leskovec", "Jure", ""]]}, {"id": "1706.03175", "submitter": "Zhao Song", "authors": "Kai Zhong, Zhao Song, Prateek Jain, Peter L. Bartlett, Inderjit S.\n  Dhillon", "title": "Recovery Guarantees for One-hidden-layer Neural Networks", "comments": "ICML 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper, we consider regression problems with one-hidden-layer neural\nnetworks (1NNs). We distill some properties of activation functions that lead\nto $\\mathit{local~strong~convexity}$ in the neighborhood of the ground-truth\nparameters for the 1NN squared-loss objective. Most popular nonlinear\nactivation functions satisfy the distilled properties, including rectified\nlinear units (ReLUs), leaky ReLUs, squared ReLUs and sigmoids. For activation\nfunctions that are also smooth, we show $\\mathit{local~linear~convergence}$\nguarantees of gradient descent under a resampling rule. For homogeneous\nactivations, we show tensor methods are able to initialize the parameters to\nfall into the local strong convexity region. As a result, tensor initialization\nfollowed by gradient descent is guaranteed to recover the ground truth with\nsample complexity $ d \\cdot \\log(1/\\epsilon) \\cdot \\mathrm{poly}(k,\\lambda )$\nand computational complexity $n\\cdot d \\cdot \\mathrm{poly}(k,\\lambda) $ for\nsmooth homogeneous activations with high probability, where $d$ is the\ndimension of the input, $k$ ($k\\leq d$) is the number of hidden nodes,\n$\\lambda$ is a conditioning property of the ground-truth parameter matrix\nbetween the input layer and the hidden layer, $\\epsilon$ is the targeted\nprecision and $n$ is the number of samples. To the best of our knowledge, this\nis the first work that provides recovery guarantees for 1NNs with both sample\ncomplexity and computational complexity $\\mathit{linear}$ in the input\ndimension and $\\mathit{logarithmic}$ in the precision.\n", "versions": [{"version": "v1", "created": "Sat, 10 Jun 2017 02:56:39 GMT"}], "update_date": "2017-06-13", "authors_parsed": [["Zhong", "Kai", ""], ["Song", "Zhao", ""], ["Jain", "Prateek", ""], ["Bartlett", "Peter L.", ""], ["Dhillon", "Inderjit S.", ""]]}, {"id": "1706.03190", "submitter": "Donghao Luo", "authors": "Donghao Luo, Bingbing Ni, Yichao Yan, Xiaokang Yang", "title": "Image Matching via Loopy RNN", "comments": "6 pages, 5 figures, International Joint Conference on Artificial\n  Intelligence(IJCAI)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most existing matching algorithms are one-off algorithms, i.e., they usually\nmeasure the distance between the two image feature representation vectors for\nonly one time. In contrast, human's vision system achieves this task, i.e.,\nimage matching, by recursively looking at specific/related parts of both images\nand then making the final judgement. Towards this end, we propose a novel loopy\nrecurrent neural network (Loopy RNN), which is capable of aggregating\nrelationship information of two input images in a progressive/iterative manner\nand outputting the consolidated matching score in the final iteration. A Loopy\nRNN features two uniqueness. First, built on conventional long short-term\nmemory (LSTM) nodes, it links the output gate of the tail node to the input\ngate of the head node, thus it brings up symmetry property required for\nmatching. Second, a monotonous loss designed for the proposed network\nguarantees increasing confidence during the recursive matching process.\nExtensive experiments on several image matching benchmarks demonstrate the\ngreat potential of the proposed method.\n", "versions": [{"version": "v1", "created": "Sat, 10 Jun 2017 06:48:16 GMT"}, {"version": "v2", "created": "Wed, 14 Jun 2017 12:43:12 GMT"}, {"version": "v3", "created": "Sun, 18 Jun 2017 15:58:23 GMT"}], "update_date": "2017-06-20", "authors_parsed": [["Luo", "Donghao", ""], ["Ni", "Bingbing", ""], ["Yan", "Yichao", ""], ["Yang", "Xiaokang", ""]]}, {"id": "1706.03196", "submitter": "\\'Alvaro Peris", "authors": "\\'Alvaro Peris, Luis Cebri\\'an and Francisco Casacuberta", "title": "Online Learning for Neural Machine Translation Post-editing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural machine translation has meant a revolution of the field. Nevertheless,\npost-editing the outputs of the system is mandatory for tasks requiring high\ntranslation quality. Post-editing offers a unique opportunity for improving\nneural machine translation systems, using online learning techniques and\ntreating the post-edited translations as new, fresh training data. We review\nclassical learning methods and propose a new optimization algorithm. We\nthoroughly compare online learning algorithms in a post-editing scenario.\nResults show significant improvements in translation quality and effort\nreduction.\n", "versions": [{"version": "v1", "created": "Sat, 10 Jun 2017 07:41:22 GMT"}], "update_date": "2017-06-13", "authors_parsed": [["Peris", "\u00c1lvaro", ""], ["Cebri\u00e1n", "Luis", ""], ["Casacuberta", "Francisco", ""]]}, {"id": "1706.03199", "submitter": "Olivier Teytaud", "authors": "Olivier Bousquet, Sylvain Gelly, Karol Kurach, Marc Schoenauer,\n  Michele Sebag, Olivier Teytaud, Damien Vincent", "title": "Toward Optimal Run Racing: Application to Deep Learning Calibration", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper aims at one-shot learning of deep neural nets, where a highly\nparallel setting is considered to address the algorithm calibration problem -\nselecting the best neural architecture and learning hyper-parameter values\ndepending on the dataset at hand. The notoriously expensive calibration problem\nis optimally reduced by detecting and early stopping non-optimal runs. The\ntheoretical contribution regards the optimality guarantees within the multiple\nhypothesis testing framework. Experimentations on the Cifar10, PTB and Wiki\nbenchmarks demonstrate the relevance of the approach with a principled and\nconsistent improvement on the state of the art with no extra hyper-parameter.\n", "versions": [{"version": "v1", "created": "Sat, 10 Jun 2017 07:55:38 GMT"}, {"version": "v2", "created": "Tue, 20 Jun 2017 11:38:25 GMT"}], "update_date": "2017-06-21", "authors_parsed": [["Bousquet", "Olivier", ""], ["Gelly", "Sylvain", ""], ["Kurach", "Karol", ""], ["Schoenauer", "Marc", ""], ["Sebag", "Michele", ""], ["Teytaud", "Olivier", ""], ["Vincent", "Damien", ""]]}, {"id": "1706.03200", "submitter": "Olivier Teytaud", "authors": "Olivier Bousquet, Sylvain Gelly, Karol Kurach, Olivier Teytaud, Damien\n  Vincent", "title": "Critical Hyper-Parameters: No Random, No Cry", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The selection of hyper-parameters is critical in Deep Learning. Because of\nthe long training time of complex models and the availability of compute\nresources in the cloud, \"one-shot\" optimization schemes - where the sets of\nhyper-parameters are selected in advance (e.g. on a grid or in a random manner)\nand the training is executed in parallel - are commonly used. It is known that\ngrid search is sub-optimal, especially when only a few critical parameters\nmatter, and suggest to use random search instead. Yet, random search can be\n\"unlucky\" and produce sets of values that leave some part of the domain\nunexplored. Quasi-random methods, such as Low Discrepancy Sequences (LDS) avoid\nthese issues. We show that such methods have theoretical properties that make\nthem appealing for performing hyperparameter search, and demonstrate that, when\napplied to the selection of hyperparameters of complex Deep Learning models\n(such as state-of-the-art LSTM language models and image classification\nmodels), they yield suitable hyperparameters values with much fewer runs than\nrandom search. We propose a particularly simple LDS method which can be used as\na drop-in replacement for grid or random search in any Deep Learning pipeline,\nboth as a fully one-shot hyperparameter search or as an initializer in\niterative batch optimization.\n", "versions": [{"version": "v1", "created": "Sat, 10 Jun 2017 08:02:34 GMT"}], "update_date": "2017-06-13", "authors_parsed": [["Bousquet", "Olivier", ""], ["Gelly", "Sylvain", ""], ["Kurach", "Karol", ""], ["Teytaud", "Olivier", ""], ["Vincent", "Damien", ""]]}, {"id": "1706.03235", "submitter": "Hangyu Mao", "authors": "Hangyu Mao, Zhibo Gong, Yan Ni and Zhen Xiao", "title": "ACCNet: Actor-Coordinator-Critic Net for \"Learning-to-Communicate\" with\n  Deep Multi-agent Reinforcement Learning", "comments": "V3 of original submission. Actor-Critic Method for Multi-agent\n  Learning-to-Communicate based on Deep Reinforcement Learning, It is suitable\n  for both continuous and discrete action space environments", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Communication is a critical factor for the big multi-agent world to stay\norganized and productive. Typically, most previous multi-agent\n\"learning-to-communicate\" studies try to predefine the communication protocols\nor use technologies such as tabular reinforcement learning and evolutionary\nalgorithm, which can not generalize to changing environment or large collection\nof agents.\n  In this paper, we propose an Actor-Coordinator-Critic Net (ACCNet) framework\nfor solving \"learning-to-communicate\" problem. The ACCNet naturally combines\nthe powerful actor-critic reinforcement learning technology with deep learning\ntechnology. It can efficiently learn the communication protocols even from\nscratch under partially observable environment. We demonstrate that the ACCNet\ncan achieve better results than several baselines under both continuous and\ndiscrete action space environments. We also analyse the learned protocols and\ndiscuss some design considerations.\n", "versions": [{"version": "v1", "created": "Sat, 10 Jun 2017 13:50:23 GMT"}, {"version": "v2", "created": "Tue, 13 Jun 2017 02:00:14 GMT"}, {"version": "v3", "created": "Sun, 29 Oct 2017 05:09:39 GMT"}], "update_date": "2017-10-31", "authors_parsed": [["Mao", "Hangyu", ""], ["Gong", "Zhibo", ""], ["Ni", "Yan", ""], ["Xiao", "Zhen", ""]]}, {"id": "1706.03256", "submitter": "Soheil Khorram", "authors": "John Gideon, Soheil Khorram, Zakaria Aldeneh, Dimitrios Dimitriadis,\n  Emily Mower Provost", "title": "Progressive Neural Networks for Transfer Learning in Emotion Recognition", "comments": "5 pages, 4 figures, to appear in the proceedings of Interspeech 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Many paralinguistic tasks are closely related and thus representations\nlearned in one domain can be leveraged for another. In this paper, we\ninvestigate how knowledge can be transferred between three paralinguistic\ntasks: speaker, emotion, and gender recognition. Further, we extend this\nproblem to cross-dataset tasks, asking how knowledge captured in one emotion\ndataset can be transferred to another. We focus on progressive neural networks\nand compare these networks to the conventional deep learning method of\npre-training and fine-tuning. Progressive neural networks provide a way to\ntransfer knowledge and avoid the forgetting effect present when pre-training\nneural networks on different tasks. Our experiments demonstrate that: (1)\nemotion recognition can benefit from using representations originally learned\nfor different paralinguistic tasks and (2) transfer learning can effectively\nleverage additional datasets to improve the performance of emotion recognition\nsystems.\n", "versions": [{"version": "v1", "created": "Sat, 10 Jun 2017 17:26:20 GMT"}], "update_date": "2017-06-13", "authors_parsed": [["Gideon", "John", ""], ["Khorram", "Soheil", ""], ["Aldeneh", "Zakaria", ""], ["Dimitriadis", "Dimitrios", ""], ["Provost", "Emily Mower", ""]]}, {"id": "1706.03265", "submitter": "Jonathan Landy", "authors": "Jonathan Landy", "title": "Stepwise regression for unsupervised learning", "comments": "9 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  I consider unsupervised extensions of the fast stepwise linear regression\nalgorithm \\cite{efroymson1960multiple}. These extensions allow one to\nefficiently identify highly-representative feature variable subsets within a\ngiven set of jointly distributed variables. This in turn allows for the\nefficient dimensional reduction of large data sets via the removal of redundant\nfeatures. Fast search is effected here through the avoidance of repeat\ncomputations across trial fits, allowing for a full representative-importance\nranking of a set of feature variables to be carried out in $O(n^2 m)$ time,\nwhere $n$ is the number of variables and $m$ is the number of data samples\navailable. This runtime complexity matches that needed to carry out a single\nregression and is $O(n^2)$ faster than that of naive implementations. I present\npseudocode suitable for efficient forward, reverse, and forward-reverse\nunsupervised feature selection. To illustrate the algorithm's application, I\napply it to the problem of identifying representative stocks within a given\nfinancial market index -- a challenge relevant to the design of Exchange Traded\nFunds (ETFs). I also characterize the growth of numerical error with iteration\nstep in these algorithms, and finally demonstrate and rationalize the\nobservation that the forward and reverse algorithms return exactly inverted\nfeature orderings in the weakly-correlated feature set regime.\n", "versions": [{"version": "v1", "created": "Sat, 10 Jun 2017 18:13:42 GMT"}], "update_date": "2017-06-13", "authors_parsed": [["Landy", "Jonathan", ""]]}, {"id": "1706.03267", "submitter": "Reshad Hosseini", "authors": "Reshad Hosseini, Suvrit Sra", "title": "An Alternative to EM for Gaussian Mixture Models: Batch and Stochastic\n  Riemannian Optimization", "comments": "21 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider maximum likelihood estimation for Gaussian Mixture Models (Gmms).\nThis task is almost invariably solved (in theory and practice) via the\nExpectation Maximization (EM) algorithm. EM owes its success to various\nfactors, of which is its ability to fulfill positive definiteness constraints\nin closed form is of key importance. We propose an alternative to EM by\nappealing to the rich Riemannian geometry of positive definite matrices, using\nwhich we cast Gmm parameter estimation as a Riemannian optimization problem.\nSurprisingly, such an out-of-the-box Riemannian formulation completely fails\nand proves much inferior to EM. This motivates us to take a closer look at the\nproblem geometry, and derive a better formulation that is much more amenable to\nRiemannian optimization. We then develop (Riemannian) batch and stochastic\ngradient algorithms that outperform EM, often substantially. We provide a\nnon-asymptotic convergence analysis for our stochastic method, which is also\nthe first (to our knowledge) such global analysis for Riemannian stochastic\ngradient. Numerous empirical results are included to demonstrate the\neffectiveness of our methods.\n", "versions": [{"version": "v1", "created": "Sat, 10 Jun 2017 18:30:53 GMT"}], "update_date": "2018-06-04", "authors_parsed": [["Hosseini", "Reshad", ""], ["Sra", "Suvrit", ""]]}, {"id": "1706.03269", "submitter": "Aurelien Lucchi", "authors": "Paulina Grnarova and Kfir Y. Levy and Aurelien Lucchi and Thomas\n  Hofmann and Andreas Krause", "title": "An Online Learning Approach to Generative Adversarial Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of training generative models with a Generative\nAdversarial Network (GAN). Although GANs can accurately model complex\ndistributions, they are known to be difficult to train due to instabilities\ncaused by a difficult minimax optimization problem. In this paper, we view the\nproblem of training GANs as finding a mixed strategy in a zero-sum game.\nBuilding on ideas from online learning we propose a novel training method named\nChekhov GAN 1 . On the theory side, we show that our method provably converges\nto an equilibrium for semi-shallow GAN architectures, i.e. architectures where\nthe discriminator is a one layer network and the generator is arbitrary. On the\npractical side, we develop an efficient heuristic guided by our theoretical\nresults, which we apply to commonly used deep GAN architectures. On several\nreal world tasks our approach exhibits improved stability and performance\ncompared to standard GAN training.\n", "versions": [{"version": "v1", "created": "Sat, 10 Jun 2017 18:49:07 GMT"}], "update_date": "2017-06-13", "authors_parsed": [["Grnarova", "Paulina", ""], ["Levy", "Kfir Y.", ""], ["Lucchi", "Aurelien", ""], ["Hofmann", "Thomas", ""], ["Krause", "Andreas", ""]]}, {"id": "1706.03283", "submitter": "Sachin Talathi", "authors": "Sachin S. Talathi", "title": "Deep Recurrent Neural Networks for seizure detection and early seizure\n  detection systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.QM cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Epilepsy is common neurological diseases, affecting about 0.6-0.8 % of world\npopulation. Epileptic patients suffer from chronic unprovoked seizures, which\ncan result in broad spectrum of debilitating medical and social consequences.\nSince seizures, in general, occur infrequently and are unpredictable, automated\nseizure detection systems are recommended to screen for seizures during\nlong-term electroencephalogram (EEG) recordings. In addition, systems for early\nseizure detection can lead to the development of new types of intervention\nsystems that are designed to control or shorten the duration of seizure events.\nIn this article, we investigate the utility of recurrent neural networks (RNNs)\nin designing seizure detection and early seizure detection systems. We propose\na deep learning framework via the use of Gated Recurrent Unit (GRU) RNNs for\nseizure detection. We use publicly available data in order to evaluate our\nmethod and demonstrate very promising evaluation results with overall accuracy\nclose to 100 %. We also systematically investigate the application of our\nmethod for early seizure warning systems. Our method can detect about 98% of\nseizure events within the first 5 seconds of the overall epileptic seizure\nduration.\n", "versions": [{"version": "v1", "created": "Sat, 10 Jun 2017 21:29:09 GMT"}], "update_date": "2017-06-13", "authors_parsed": [["Talathi", "Sachin S.", ""]]}, {"id": "1706.03292", "submitter": "Hao Zhang", "authors": "Hao Zhang, Zeyu Zheng, Shizhen Xu, Wei Dai, Qirong Ho, Xiaodan Liang,\n  Zhiting Hu, Jinliang Wei, Pengtao Xie, Eric P. Xing", "title": "Poseidon: An Efficient Communication Architecture for Distributed Deep\n  Learning on GPU Clusters", "comments": "To appear in 2017 USENIX Annual Technical Conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.DC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning models can take weeks to train on a single GPU-equipped\nmachine, necessitating scaling out DL training to a GPU-cluster. However,\ncurrent distributed DL implementations can scale poorly due to substantial\nparameter synchronization over the network, because the high throughput of GPUs\nallows more data batches to be processed per unit time than CPUs, leading to\nmore frequent network synchronization. We present Poseidon, an efficient\ncommunication architecture for distributed DL on GPUs. Poseidon exploits the\nlayered model structures in DL programs to overlap communication and\ncomputation, reducing bursty network communication. Moreover, Poseidon uses a\nhybrid communication scheme that optimizes the number of bytes required to\nsynchronize each layer, according to layer properties and the number of\nmachines. We show that Poseidon is applicable to different DL frameworks by\nplugging Poseidon into Caffe and TensorFlow. We show that Poseidon enables\nCaffe and TensorFlow to achieve 15.5x speed-up on 16 single-GPU machines, even\nwith limited bandwidth (10GbE) and the challenging VGG19-22K network for image\nclassification. Moreover, Poseidon-enabled TensorFlow achieves 31.5x speed-up\nwith 32 single-GPU machines on Inception-V3, a 50% improvement over the\nopen-source TensorFlow (20x speed-up).\n", "versions": [{"version": "v1", "created": "Sun, 11 Jun 2017 01:11:06 GMT"}], "update_date": "2017-06-13", "authors_parsed": [["Zhang", "Hao", ""], ["Zheng", "Zeyu", ""], ["Xu", "Shizhen", ""], ["Dai", "Wei", ""], ["Ho", "Qirong", ""], ["Liang", "Xiaodan", ""], ["Hu", "Zhiting", ""], ["Wei", "Jinliang", ""], ["Xie", "Pengtao", ""], ["Xing", "Eric P.", ""]]}, {"id": "1706.03301", "submitter": "Matus Telgarsky", "authors": "Matus Telgarsky", "title": "Neural networks and rational functions", "comments": "To appear, ICML 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural networks and rational functions efficiently approximate each other. In\nmore detail, it is shown here that for any ReLU network, there exists a\nrational function of degree $O(\\text{polylog}(1/\\epsilon))$ which is\n$\\epsilon$-close, and similarly for any rational function there exists a ReLU\nnetwork of size $O(\\text{polylog}(1/\\epsilon))$ which is $\\epsilon$-close. By\ncontrast, polynomials need degree $\\Omega(\\text{poly}(1/\\epsilon))$ to\napproximate even a single ReLU. When converting a ReLU network to a rational\nfunction as above, the hidden constants depend exponentially on the number of\nlayers, which is shown to be tight; in other words, a compositional\nrepresentation can be beneficial even for rational functions.\n", "versions": [{"version": "v1", "created": "Sun, 11 Jun 2017 03:07:42 GMT"}], "update_date": "2017-06-13", "authors_parsed": [["Telgarsky", "Matus", ""]]}, {"id": "1706.03316", "submitter": "Kai Zheng", "authors": "Kai Zheng, Wenlong Mou, Liwei Wang", "title": "Collect at Once, Use Effectively: Making Non-interactive Locally Private\n  Learning Possible", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Non-interactive Local Differential Privacy (LDP) requires data analysts to\ncollect data from users through noisy channel at once. In this paper, we extend\nthe frontiers of Non-interactive LDP learning and estimation from several\naspects. For learning with smooth generalized linear losses, we propose an\napproximate stochastic gradient oracle estimated from non-interactive LDP\nchannel, using Chebyshev expansion. Combined with inexact gradient methods, we\nobtain an efficient algorithm with quasi-polynomial sample complexity bound.\nFor the high-dimensional world, we discover that under $\\ell_2$-norm assumption\non data points, high-dimensional sparse linear regression and mean estimation\ncan be achieved with logarithmic dependence on dimension, using random\nprojection and approximate recovery. We also extend our methods to Kernel Ridge\nRegression. Our work is the first one that makes learning and estimation\npossible for a broad range of learning tasks under non-interactive LDP model.\n", "versions": [{"version": "v1", "created": "Sun, 11 Jun 2017 07:04:50 GMT"}], "update_date": "2017-06-13", "authors_parsed": [["Zheng", "Kai", ""], ["Mou", "Wenlong", ""], ["Wang", "Liwei", ""]]}, {"id": "1706.03369", "submitter": "Francois-Xavier Briol", "authors": "Francois-Xavier Briol and Chris J. Oates and Jon Cockayne and Wilson\n  Ye Chen and Mark Girolami", "title": "On the Sampling Problem for Kernel Quadrature", "comments": "To appear at Thirty-fourth International Conference on Machine\n  Learning (ICML 2017)", "journal-ref": "Proceedings of the 34th International Conference on Machine\n  Learning, PMLR 70:586-595, 2017", "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.NA stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The standard Kernel Quadrature method for numerical integration with random\npoint sets (also called Bayesian Monte Carlo) is known to converge in root mean\nsquare error at a rate determined by the ratio $s/d$, where $s$ and $d$ encode\nthe smoothness and dimension of the integrand. However, an empirical\ninvestigation reveals that the rate constant $C$ is highly sensitive to the\ndistribution of the random points. In contrast to standard Monte Carlo\nintegration, for which optimal importance sampling is well-understood, the\nsampling distribution that minimises $C$ for Kernel Quadrature does not admit a\nclosed form. This paper argues that the practical choice of sampling\ndistribution is an important open problem. One solution is considered; a novel\nautomatic approach based on adaptive tempering and sequential Monte Carlo.\nEmpirical results demonstrate a dramatic reduction in integration error of up\nto 4 orders of magnitude can be achieved with the proposed method.\n", "versions": [{"version": "v1", "created": "Sun, 11 Jun 2017 16:08:17 GMT"}], "update_date": "2017-08-01", "authors_parsed": [["Briol", "Francois-Xavier", ""], ["Oates", "Chris J.", ""], ["Cockayne", "Jon", ""], ["Chen", "Wilson Ye", ""], ["Girolami", "Mark", ""]]}, {"id": "1706.03446", "submitter": "Benjamin Shickel", "authors": "Benjamin Shickel, Patrick Tighe, Azra Bihorac, Parisa Rashidi", "title": "Deep EHR: A Survey of Recent Advances in Deep Learning Techniques for\n  Electronic Health Record (EHR) Analysis", "comments": "Accepted for publication with Journal of Biomedical and Health\n  Informatics: http://ieeexplore.ieee.org/abstract/document/8086133/", "journal-ref": null, "doi": "10.1109/JBHI.2017.2767063", "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The past decade has seen an explosion in the amount of digital information\nstored in electronic health records (EHR). While primarily designed for\narchiving patient clinical information and administrative healthcare tasks,\nmany researchers have found secondary use of these records for various clinical\ninformatics tasks. Over the same period, the machine learning community has\nseen widespread advances in deep learning techniques, which also have been\nsuccessfully applied to the vast amount of EHR data. In this paper, we review\nthese deep EHR systems, examining architectures, technical aspects, and\nclinical applications. We also identify shortcomings of current techniques and\ndiscuss avenues of future research for EHR-based deep learning.\n", "versions": [{"version": "v1", "created": "Mon, 12 Jun 2017 03:03:15 GMT"}, {"version": "v2", "created": "Sat, 24 Feb 2018 01:41:18 GMT"}], "update_date": "2018-02-27", "authors_parsed": [["Shickel", "Benjamin", ""], ["Tighe", "Patrick", ""], ["Bihorac", "Azra", ""], ["Rashidi", "Parisa", ""]]}, {"id": "1706.03459", "submitter": "Zhe Feng", "authors": "Paul D\\\"utting and Zhe Feng and Harikrishna Narasimhan and David C.\n  Parkes and Sai Srivatsa Ravindranath", "title": "Optimal Auctions through Deep Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GT cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Designing an incentive compatible auction that maximizes expected revenue is\nan intricate task. The single-item case was resolved in a seminal piece of work\nby Myerson in 1981. Even after 30-40 years of intense research the problem\nremains unsolved for settings with two or more items. In this work, we initiate\nthe exploration of the use of tools from deep learning for the automated design\nof optimal auctions. We model an auction as a multi-layer neural network, frame\noptimal auction design as a constrained learning problem, and show how it can\nbe solved using standard machine learning pipelines. We prove generalization\nbounds and present extensive experiments, recovering essentially all known\nanalytical solutions for multi-item settings, and obtaining novel mechanisms\nfor settings in which the optimal mechanism is unknown.\n", "versions": [{"version": "v1", "created": "Mon, 12 Jun 2017 04:03:15 GMT"}, {"version": "v2", "created": "Mon, 19 Mar 2018 17:56:32 GMT"}, {"version": "v3", "created": "Mon, 1 Apr 2019 00:33:16 GMT"}, {"version": "v4", "created": "Sat, 29 Feb 2020 23:54:43 GMT"}, {"version": "v5", "created": "Fri, 21 Aug 2020 15:04:03 GMT"}], "update_date": "2020-08-24", "authors_parsed": [["D\u00fctting", "Paul", ""], ["Feng", "Zhe", ""], ["Narasimhan", "Harikrishna", ""], ["Parkes", "David C.", ""], ["Ravindranath", "Sai Srivatsa", ""]]}, {"id": "1706.03475", "submitter": "Kimin Lee", "authors": "Kimin Lee, Changho Hwang, KyoungSoo Park, Jinwoo Shin", "title": "Confident Multiple Choice Learning", "comments": "Accepted in ICML 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Ensemble methods are arguably the most trustworthy techniques for boosting\nthe performance of machine learning models. Popular independent ensembles (IE)\nrelying on naive averaging/voting scheme have been of typical choice for most\napplications involving deep neural networks, but they do not consider advanced\ncollaboration among ensemble models. In this paper, we propose new ensemble\nmethods specialized for deep neural networks, called confident multiple choice\nlearning (CMCL): it is a variant of multiple choice learning (MCL) via\naddressing its overconfidence issue.In particular, the proposed major\ncomponents of CMCL beyond the original MCL scheme are (i) new loss, i.e.,\nconfident oracle loss, (ii) new architecture, i.e., feature sharing and (iii)\nnew training method, i.e., stochastic labeling. We demonstrate the effect of\nCMCL via experiments on the image classification on CIFAR and SVHN, and the\nforeground-background segmentation on the iCoseg. In particular, CMCL using 5\nresidual networks provides 14.05% and 6.60% relative reductions in the top-1\nerror rates from the corresponding IE scheme for the classification task on\nCIFAR and SVHN, respectively.\n", "versions": [{"version": "v1", "created": "Mon, 12 Jun 2017 05:55:38 GMT"}, {"version": "v2", "created": "Fri, 22 Sep 2017 05:56:57 GMT"}], "update_date": "2017-09-25", "authors_parsed": [["Lee", "Kimin", ""], ["Hwang", "Changho", ""], ["Park", "KyoungSoo", ""], ["Shin", "Jinwoo", ""]]}, {"id": "1706.03492", "submitter": "Timothy Au", "authors": "Timothy C. Au", "title": "Random Forests, Decision Trees, and Categorical Predictors: The \"Absent\n  Levels\" Problem", "comments": "Updated to the published version that appears at\n  http://www.jmlr.org/papers/volume19/16-474/16-474.pdf", "journal-ref": "Journal of Machine Learning Research 19(45):1-30, 2018", "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One advantage of decision tree based methods like random forests is their\nability to natively handle categorical predictors without having to first\ntransform them (e.g., by using feature engineering techniques). However, in\nthis paper, we show how this capability can lead to an inherent \"absent levels\"\nproblem for decision tree based methods that has never been thoroughly\ndiscussed, and whose consequences have never been carefully explored. This\nproblem occurs whenever there is an indeterminacy over how to handle an\nobservation that has reached a categorical split which was determined when the\nobservation in question's level was absent during training. Although these\nincidents may appear to be innocuous, by using Leo Breiman and Adele Cutler's\nrandom forests FORTRAN code and the randomForest R package (Liaw and Wiener,\n2002) as motivating case studies, we examine how overlooking the absent levels\nproblem can systematically bias a model. Furthermore, by using three real data\nexamples, we illustrate how absent levels can dramatically alter a model's\nperformance in practice, and we empirically demonstrate how some simple\nheuristics can be used to help mitigate the effects of the absent levels\nproblem until a more robust theoretical solution is found.\n", "versions": [{"version": "v1", "created": "Mon, 12 Jun 2017 07:34:49 GMT"}, {"version": "v2", "created": "Sun, 28 Oct 2018 17:44:59 GMT"}], "update_date": "2018-10-30", "authors_parsed": [["Au", "Timothy C.", ""]]}, {"id": "1706.03533", "submitter": "Steven Van Vaerenbergh", "authors": "Steven Van Vaerenbergh, Simone Scardapane, Ignacio Santamaria", "title": "Recursive Multikernel Filters Exploiting Nonlinear Temporal Structure", "comments": "Eusipco 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In kernel methods, temporal information on the data is commonly included by\nusing time-delayed embeddings as inputs. Recently, an alternative formulation\nwas proposed by defining a gamma-filter explicitly in a reproducing kernel\nHilbert space, giving rise to a complex model where multiple kernels operate on\ndifferent temporal combinations of the input signal. In the original\nformulation, the kernels are then simply combined to obtain a single kernel\nmatrix (for instance by averaging), which provides computational benefits but\ndiscards important information on the temporal structure of the signal.\nInspired by works on multiple kernel learning, we overcome this drawback by\nconsidering the different kernels separately. We propose an efficient strategy\nto adaptively combine and select these kernels during the training phase. The\nresulting batch and online algorithms automatically learn to process highly\nnonlinear temporal information extracted from the input signal, which is\nimplicitly encoded in the kernel values. We evaluate our proposal on several\nartificial and real tasks, showing that it can outperform classical approaches\nboth in batch and online settings.\n", "versions": [{"version": "v1", "created": "Mon, 12 Jun 2017 09:24:42 GMT"}], "update_date": "2017-06-13", "authors_parsed": [["Van Vaerenbergh", "Steven", ""], ["Scardapane", "Simone", ""], ["Santamaria", "Ignacio", ""]]}, {"id": "1706.03581", "submitter": "Artsiom Ablavatski", "authors": "Artsiom Ablavatski, Shijian Lu and Jianfei Cai", "title": "Enriched Deep Recurrent Visual Attention Model for Multiple Object\n  Recognition", "comments": null, "journal-ref": null, "doi": "10.1109/WACV.2017.113", "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We design an Enriched Deep Recurrent Visual Attention Model (EDRAM) - an\nimproved attention-based architecture for multiple object recognition. The\nproposed model is a fully differentiable unit that can be optimized end-to-end\nby using Stochastic Gradient Descent (SGD). The Spatial Transformer (ST) was\nemployed as visual attention mechanism which allows to learn the geometric\ntransformation of objects within images. With the combination of the Spatial\nTransformer and the powerful recurrent architecture, the proposed EDRAM can\nlocalize and recognize objects simultaneously. EDRAM has been evaluated on two\npublicly available datasets including MNIST Cluttered (with 70K cluttered\ndigits) and SVHN (with up to 250k real world images of house numbers).\nExperiments show that it obtains superior performance as compared with the\nstate-of-the-art models.\n", "versions": [{"version": "v1", "created": "Mon, 12 Jun 2017 11:55:35 GMT"}], "update_date": "2017-06-13", "authors_parsed": [["Ablavatski", "Artsiom", ""], ["Lu", "Shijian", ""], ["Cai", "Jianfei", ""]]}, {"id": "1706.03595", "submitter": "Thomas Kirchner", "authors": "Thomas Kirchner, Janek Gr\\\"ohl and Lena Maier-Hein", "title": "Context encoding enables machine learning-based quantitative\n  photoacoustics", "comments": "under review JBO", "journal-ref": null, "doi": "10.1117/1.JBO.23.5.056008", "report-no": null, "categories": "physics.med-ph cs.LG physics.comp-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Real-time monitoring of functional tissue parameters, such as local blood\noxygenation, based on optical imaging could provide groundbreaking advances in\nthe diagnosis and interventional therapy of various diseases. While\nphotoacoustic (PA) imaging is a novel modality with great potential to measure\noptical absorption deep inside tissue, quantification of the measurements\nremains a major challenge. In this paper, we introduce the first machine\nlearning based approach to quantitative PA imaging (qPAI), which relies on\nlearning the fluence in a voxel to deduce the corresponding optical absorption.\nThe method encodes relevant information of the measured signal and the\ncharacteristics of the imaging system in voxel-based feature vectors, which\nallow the generation of thousands of training samples from a single simulated\nPA image. Comprehensive in silico experiments suggest that context encoding\n(CE)-qPAI enables highly accurate and robust quantification of the local\nfluence and thereby the optical absorption from PA images.\n", "versions": [{"version": "v1", "created": "Mon, 12 Jun 2017 12:26:19 GMT"}, {"version": "v2", "created": "Mon, 29 Jan 2018 19:58:19 GMT"}], "update_date": "2018-08-15", "authors_parsed": [["Kirchner", "Thomas", ""], ["Gr\u00f6hl", "Janek", ""], ["Maier-Hein", "Lena", ""]]}, {"id": "1706.03607", "submitter": "Edith Cohen", "authors": "Edith Cohen, Shiri Chechik, Haim Kaplan", "title": "Clustering Small Samples with Quality Guarantees: Adaptivity with\n  One2all pps", "comments": "17 pages, 2 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Clustering of data points is a fundamental tool in data analysis. We consider\npoints $X$ in a relaxed metric space, where the triangle inequality holds\nwithin a constant factor. The {\\em cost} of clustering $X$ by $Q$ is\n$V(Q)=\\sum_{x\\in X} d_{xQ}$. Two basic tasks, parametrized by $k \\geq 1$, are\n{\\em cost estimation}, which returns (approximate) $V(Q)$ for queries $Q$ such\nthat $|Q|=k$ and {\\em clustering}, which returns an (approximate) minimizer of\n$V(Q)$ of size $|Q|=k$. With very large data sets $X$, we seek efficient\nconstructions of small samples that act as surrogates to the full data for\nperforming these tasks. Existing constructions that provide quality guarantees\nare either worst-case, and unable to benefit from structure of real data sets,\nor make explicit strong assumptions on the structure. We show here how to avoid\nboth these pitfalls using adaptive designs.\n  At the core of our design is the {\\em one2all} construction of\nmulti-objective probability-proportional-to-size (pps) samples: Given a set $M$\nof centroids and $\\alpha \\geq 1$, one2all efficiently assigns probabilities to\npoints so that the clustering cost of {\\em each} $Q$ with cost $V(Q) \\geq\nV(M)/\\alpha$ can be estimated well from a sample of size $O(\\alpha\n|M|\\epsilon^{-2})$. For cost queries, we can obtain worst-case sample size\n$O(k\\epsilon^{-2})$ by applying one2all to a bicriteria approximation $M$, but\nwe adaptively balance $|M|$ and $\\alpha$ to further reduce sample size. For\nclustering, we design an adaptive wrapper that applies a base clustering\nalgorithm to a sample $S$. Our wrapper uses the smallest sample that provides\nstatistical guarantees that the quality of the clustering on the sample carries\nover to the full data set. We demonstrate experimentally the huge gains of\nusing our adaptive instead of worst-case methods.\n", "versions": [{"version": "v1", "created": "Mon, 12 Jun 2017 13:05:46 GMT"}, {"version": "v2", "created": "Sun, 29 Oct 2017 10:27:20 GMT"}], "update_date": "2017-10-31", "authors_parsed": [["Cohen", "Edith", ""], ["Chechik", "Shiri", ""], ["Kaplan", "Haim", ""]]}, {"id": "1706.03643", "submitter": "Serena Yeung", "authors": "Serena Yeung, Anitha Kannan, Yann Dauphin, Li Fei-Fei", "title": "Tackling Over-pruning in Variational Autoencoders", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Variational autoencoders (VAE) are directed generative models that learn\nfactorial latent variables. As noted by Burda et al. (2015), these models\nexhibit the problem of factor over-pruning where a significant number of\nstochastic factors fail to learn anything and become inactive. This can limit\ntheir modeling power and their ability to learn diverse and meaningful latent\nrepresentations. In this paper, we evaluate several methods to address this\nproblem and propose a more effective model-based approach called the epitomic\nvariational autoencoder (eVAE). The so-called epitomes of this model are groups\nof mutually exclusive latent factors that compete to explain the data. This\napproach helps prevent inactive units since each group is pressured to explain\nthe data. We compare the approaches with qualitative and quantitative results\non MNIST and TFD datasets. Our results show that eVAE makes efficient use of\nmodel capacity and generalizes better than VAE.\n", "versions": [{"version": "v1", "created": "Fri, 9 Jun 2017 10:13:00 GMT"}, {"version": "v2", "created": "Mon, 7 Aug 2017 01:13:29 GMT"}], "update_date": "2017-08-08", "authors_parsed": [["Yeung", "Serena", ""], ["Kannan", "Anitha", ""], ["Dauphin", "Yann", ""], ["Fei-Fei", "Li", ""]]}, {"id": "1706.03691", "submitter": "Jacob Steinhardt", "authors": "Jacob Steinhardt, Pang Wei Koh, Percy Liang", "title": "Certified Defenses for Data Poisoning Attacks", "comments": "Appeared at NIPS 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Machine learning systems trained on user-provided data are susceptible to\ndata poisoning attacks, whereby malicious users inject false training data with\nthe aim of corrupting the learned model. While recent work has proposed a\nnumber of attacks and defenses, little is understood about the worst-case loss\nof a defense in the face of a determined attacker. We address this by\nconstructing approximate upper bounds on the loss across a broad family of\nattacks, for defenders that first perform outlier removal followed by empirical\nrisk minimization. Our approximation relies on two assumptions: (1) that the\ndataset is large enough for statistical concentration between train and test\nerror to hold, and (2) that outliers within the clean (non-poisoned) data do\nnot have a strong effect on the model. Our bound comes paired with a candidate\nattack that often nearly matches the upper bound, giving us a powerful tool for\nquickly assessing defenses on a given dataset. Empirically, we find that even\nunder a simple defense, the MNIST-1-7 and Dogfish datasets are resilient to\nattack, while in contrast the IMDB sentiment dataset can be driven from 12% to\n23% test error by adding only 3% poisoned data.\n", "versions": [{"version": "v1", "created": "Fri, 9 Jun 2017 16:26:49 GMT"}, {"version": "v2", "created": "Fri, 24 Nov 2017 01:00:57 GMT"}], "update_date": "2017-11-27", "authors_parsed": [["Steinhardt", "Jacob", ""], ["Koh", "Pang Wei", ""], ["Liang", "Percy", ""]]}, {"id": "1706.03692", "submitter": "Vahid Noroozi", "authors": "Vahid Noroozi, Lei Zheng, Sara Bahaadini, Sihong Xie, Philip S. Yu", "title": "SEVEN: Deep Semi-supervised Verification Networks", "comments": "7 pages, 2 figures, accepted to the 2017 International Joint\n  Conference on Artificial Intelligence (IJCAI-17)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Verification determines whether two samples belong to the same class or not,\nand has important applications such as face and fingerprint verification, where\nthousands or millions of categories are present but each category has scarce\nlabeled examples, presenting two major challenges for existing deep learning\nmodels. We propose a deep semi-supervised model named SEmi-supervised\nVErification Network (SEVEN) to address these challenges. The model consists of\ntwo complementary components. The generative component addresses the lack of\nsupervision within each category by learning general salient structures from a\nlarge amount of data across categories. The discriminative component exploits\nthe learned general features to mitigate the lack of supervision within\ncategories, and also directs the generative component to find more informative\nstructures of the whole data manifold. The two components are tied together in\nSEVEN to allow an end-to-end training of the two components. Extensive\nexperiments on four verification tasks demonstrate that SEVEN significantly\noutperforms other state-of-the-art deep semi-supervised techniques when labeled\ndata are in short supply. Furthermore, SEVEN is competitive with fully\nsupervised baselines trained with a larger amount of labeled data. It indicates\nthe importance of the generative component in SEVEN.\n", "versions": [{"version": "v1", "created": "Mon, 12 Jun 2017 15:39:51 GMT"}, {"version": "v2", "created": "Wed, 14 Jun 2017 23:40:59 GMT"}], "update_date": "2017-06-16", "authors_parsed": [["Noroozi", "Vahid", ""], ["Zheng", "Lei", ""], ["Bahaadini", "Sara", ""], ["Xie", "Sihong", ""], ["Yu", "Philip S.", ""]]}, {"id": "1706.03729", "submitter": "Wenling Shang", "authors": "Wenling Shang and Kihyuk Sohn and Yuandong Tian", "title": "Channel-Recurrent Autoencoding for Image Modeling", "comments": "Code: https://github.com/WendyShang/crVAE. Supplementary Materials:\n  http://www-personal.umich.edu/~shangw/wacv18_supplementary_material.pdf", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite recent successes in synthesizing faces and bedrooms, existing\ngenerative models struggle to capture more complex image types, potentially due\nto the oversimplification of their latent space constructions. To tackle this\nissue, building on Variational Autoencoders (VAEs), we integrate recurrent\nconnections across channels to both inference and generation steps, allowing\nthe high-level features to be captured in global-to-local, coarse-to-fine\nmanners. Combined with adversarial loss, our channel-recurrent VAE-GAN\n(crVAE-GAN) outperforms VAE-GAN in generating a diverse spectrum of high\nresolution images while maintaining the same level of computational efficacy.\nOur model produces interpretable and expressive latent representations to\nbenefit downstream tasks such as image completion. Moreover, we propose two\nnovel regularizations, namely the KL objective weighting scheme over time steps\nand mutual information maximization between transformed latent variables and\nthe outputs, to enhance the training.\n", "versions": [{"version": "v1", "created": "Mon, 12 Jun 2017 17:01:34 GMT"}, {"version": "v2", "created": "Sun, 11 Mar 2018 09:00:51 GMT"}], "update_date": "2018-03-13", "authors_parsed": [["Shang", "Wenling", ""], ["Sohn", "Kihyuk", ""], ["Tian", "Yuandong", ""]]}, {"id": "1706.03736", "submitter": "Ignacio Heredia", "authors": "Ignacio Heredia", "title": "Large-Scale Plant Classification with Deep Neural Networks", "comments": "5 pages, 3 figures, 1 table. Published at Proocedings of ACM\n  Computing Frontiers Conference 2017", "journal-ref": "ACM CF'17 Proceedings of the Computing Frontiers Conference\n  (2017), 259-262", "doi": "10.1145/3075564.3075590", "report-no": null, "categories": "cs.LG cs.CV stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper discusses the potential of applying deep learning techniques for\nplant classification and its usage for citizen science in large-scale\nbiodiversity monitoring. We show that plant classification using near\nstate-of-the-art convolutional network architectures like ResNet50 achieves\nsignificant improvements in accuracy compared to the most widespread plant\nclassification application in test sets composed of thousands of different\nspecies labels. We find that the predictions can be confidently used as a\nbaseline classification in citizen science communities like iNaturalist (or its\nSpanish fork, Natusfera) which in turn can share their data with biodiversity\nportals like GBIF.\n", "versions": [{"version": "v1", "created": "Mon, 12 Jun 2017 17:16:20 GMT"}], "update_date": "2017-06-13", "authors_parsed": [["Heredia", "Ignacio", ""]]}, {"id": "1706.03741", "submitter": "Paul Christiano", "authors": "Paul Christiano, Jan Leike, Tom B. Brown, Miljan Martic, Shane Legg,\n  Dario Amodei", "title": "Deep reinforcement learning from human preferences", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.HC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For sophisticated reinforcement learning (RL) systems to interact usefully\nwith real-world environments, we need to communicate complex goals to these\nsystems. In this work, we explore goals defined in terms of (non-expert) human\npreferences between pairs of trajectory segments. We show that this approach\ncan effectively solve complex RL tasks without access to the reward function,\nincluding Atari games and simulated robot locomotion, while providing feedback\non less than one percent of our agent's interactions with the environment. This\nreduces the cost of human oversight far enough that it can be practically\napplied to state-of-the-art RL systems. To demonstrate the flexibility of our\napproach, we show that we can successfully train complex novel behaviors with\nabout an hour of human time. These behaviors and environments are considerably\nmore complex than any that have been previously learned from human feedback.\n", "versions": [{"version": "v1", "created": "Mon, 12 Jun 2017 17:23:59 GMT"}, {"version": "v2", "created": "Sun, 2 Jul 2017 20:25:56 GMT"}, {"version": "v3", "created": "Thu, 13 Jul 2017 20:18:41 GMT"}], "update_date": "2017-10-25", "authors_parsed": [["Christiano", "Paul", ""], ["Leike", "Jan", ""], ["Brown", "Tom B.", ""], ["Martic", "Miljan", ""], ["Legg", "Shane", ""], ["Amodei", "Dario", ""]]}, {"id": "1706.03762", "submitter": "Ashish Vaswani", "authors": "Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion\n  Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin", "title": "Attention Is All You Need", "comments": "15 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The dominant sequence transduction models are based on complex recurrent or\nconvolutional neural networks in an encoder-decoder configuration. The best\nperforming models also connect the encoder and decoder through an attention\nmechanism. We propose a new simple network architecture, the Transformer, based\nsolely on attention mechanisms, dispensing with recurrence and convolutions\nentirely. Experiments on two machine translation tasks show these models to be\nsuperior in quality while being more parallelizable and requiring significantly\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014\nEnglish-to-German translation task, improving over the existing best results,\nincluding ensembles by over 2 BLEU. On the WMT 2014 English-to-French\ntranslation task, our model establishes a new single-model state-of-the-art\nBLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction\nof the training costs of the best models from the literature. We show that the\nTransformer generalizes well to other tasks by applying it successfully to\nEnglish constituency parsing both with large and limited training data.\n", "versions": [{"version": "v1", "created": "Mon, 12 Jun 2017 17:57:34 GMT"}, {"version": "v2", "created": "Mon, 19 Jun 2017 16:49:45 GMT"}, {"version": "v3", "created": "Tue, 20 Jun 2017 05:20:02 GMT"}, {"version": "v4", "created": "Fri, 30 Jun 2017 17:29:30 GMT"}, {"version": "v5", "created": "Wed, 6 Dec 2017 03:30:32 GMT"}], "update_date": "2017-12-07", "authors_parsed": [["Vaswani", "Ashish", ""], ["Shazeer", "Noam", ""], ["Parmar", "Niki", ""], ["Uszkoreit", "Jakob", ""], ["Jones", "Llion", ""], ["Gomez", "Aidan N.", ""], ["Kaiser", "Lukasz", ""], ["Polosukhin", "Illia", ""]]}, {"id": "1706.03815", "submitter": "Grzegorz Chrupa{\\l}a", "authors": "Afra Alishahi, Marie Barking, Grzegorz Chrupa{\\l}a", "title": "Encoding of phonology in a recurrent neural model of grounded speech", "comments": "Accepted at CoNLL 2017", "journal-ref": null, "doi": "10.18653/v1/K17-1037", "report-no": null, "categories": "cs.CL cs.LG cs.SD", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We study the representation and encoding of phonemes in a recurrent neural\nnetwork model of grounded speech. We use a model which processes images and\ntheir spoken descriptions, and projects the visual and auditory representations\ninto the same semantic space. We perform a number of analyses on how\ninformation about individual phonemes is encoded in the MFCC features extracted\nfrom the speech signal, and the activations of the layers of the model. Via\nexperiments with phoneme decoding and phoneme discrimination we show that\nphoneme representations are most salient in the lower layers of the model,\nwhere low-level signals are processed at a fine-grained level, although a large\namount of phonological information is retain at the top recurrent layer. We\nfurther find out that the attention mechanism following the top recurrent layer\nsignificantly attenuates encoding of phonology and makes the utterance\nembeddings much more invariant to synonymy. Moreover, a hierarchical clustering\nof phoneme representations learned by the network shows an organizational\nstructure of phonemes similar to those proposed in linguistics.\n", "versions": [{"version": "v1", "created": "Mon, 12 Jun 2017 19:07:02 GMT"}, {"version": "v2", "created": "Fri, 16 Jun 2017 08:35:44 GMT"}], "update_date": "2018-10-30", "authors_parsed": [["Alishahi", "Afra", ""], ["Barking", "Marie", ""], ["Chrupa\u0142a", "Grzegorz", ""]]}, {"id": "1706.03825", "submitter": "Daniel Smilkov", "authors": "Daniel Smilkov, Nikhil Thorat, Been Kim, Fernanda Vi\\'egas, Martin\n  Wattenberg", "title": "SmoothGrad: removing noise by adding noise", "comments": "10 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Explaining the output of a deep network remains a challenge. In the case of\nan image classifier, one type of explanation is to identify pixels that\nstrongly influence the final decision. A starting point for this strategy is\nthe gradient of the class score function with respect to the input image. This\ngradient can be interpreted as a sensitivity map, and there are several\ntechniques that elaborate on this basic idea. This paper makes two\ncontributions: it introduces SmoothGrad, a simple method that can help visually\nsharpen gradient-based sensitivity maps, and it discusses lessons in the\nvisualization of these maps. We publish the code for our experiments and a\nwebsite with our results.\n", "versions": [{"version": "v1", "created": "Mon, 12 Jun 2017 19:53:30 GMT"}], "update_date": "2017-06-14", "authors_parsed": [["Smilkov", "Daniel", ""], ["Thorat", "Nikhil", ""], ["Kim", "Been", ""], ["Vi\u00e9gas", "Fernanda", ""], ["Wattenberg", "Martin", ""]]}, {"id": "1706.03847", "submitter": "Bal\\'azs Hidasi", "authors": "Bal\\'azs Hidasi, Alexandros Karatzoglou", "title": "Recurrent Neural Networks with Top-k Gains for Session-based\n  Recommendations", "comments": "CIKM'18, authors' version", "journal-ref": null, "doi": "10.1145/3269206.3271761", "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  RNNs have been shown to be excellent models for sequential data and in\nparticular for data that is generated by users in an session-based manner. The\nuse of RNNs provides impressive performance benefits over classical methods in\nsession-based recommendations. In this work we introduce novel ranking loss\nfunctions tailored to RNNs in the recommendation setting. The improved\nperformance of these losses over alternatives, along with further tricks and\nrefinements described in this work, allow for an overall improvement of up to\n35% in terms of MRR and Recall@20 over previous session-based RNN solutions and\nup to 53% over classical collaborative filtering approaches. Unlike data\naugmentation-based improvements, our method does not increase training times\nsignificantly. We further demonstrate the performance gain of the RNN over\nbaselines in an online A/B test.\n", "versions": [{"version": "v1", "created": "Mon, 12 Jun 2017 20:49:23 GMT"}, {"version": "v2", "created": "Mon, 29 Jan 2018 23:06:19 GMT"}, {"version": "v3", "created": "Tue, 28 Aug 2018 11:22:49 GMT"}], "update_date": "2018-08-29", "authors_parsed": [["Hidasi", "Bal\u00e1zs", ""], ["Karatzoglou", "Alexandros", ""]]}, {"id": "1706.03850", "submitter": "Yizhe Zhang", "authors": "Yizhe Zhang, Zhe Gan, Kai Fan, Zhi Chen, Ricardo Henao, Dinghan Shen,\n  Lawrence Carin", "title": "Adversarial Feature Matching for Text Generation", "comments": "Accepted by ICML 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Generative Adversarial Network (GAN) has achieved great success in\ngenerating realistic (real-valued) synthetic data. However, convergence issues\nand difficulties dealing with discrete data hinder the applicability of GAN to\ntext. We propose a framework for generating realistic text via adversarial\ntraining. We employ a long short-term memory network as generator, and a\nconvolutional network as discriminator. Instead of using the standard objective\nof GAN, we propose matching the high-dimensional latent feature distributions\nof real and synthetic sentences, via a kernelized discrepancy metric. This\neases adversarial training by alleviating the mode-collapsing problem. Our\nexperiments show superior performance in quantitative evaluation, and\ndemonstrate that our model can generate realistic-looking sentences.\n", "versions": [{"version": "v1", "created": "Mon, 12 Jun 2017 20:55:51 GMT"}, {"version": "v2", "created": "Sat, 29 Jul 2017 05:50:13 GMT"}, {"version": "v3", "created": "Sat, 18 Nov 2017 18:40:04 GMT"}], "update_date": "2017-11-21", "authors_parsed": [["Zhang", "Yizhe", ""], ["Gan", "Zhe", ""], ["Fan", "Kai", ""], ["Chen", "Zhi", ""], ["Henao", "Ricardo", ""], ["Shen", "Dinghan", ""], ["Carin", "Lawrence", ""]]}, {"id": "1706.03860", "submitter": "Mostafa Rahmani", "authors": "Mostafa Rahmani and George Atia", "title": "Subspace Clustering via Optimal Direction Search", "comments": null, "journal-ref": "IEEE Signal Processing Letters ( Volume: 24, Issue: 12, Dec. 2017\n  )", "doi": "10.1109/LSP.2017.2757901", "report-no": null, "categories": "cs.CV cs.IR cs.LG stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This letter presents a new spectral-clustering-based approach to the subspace\nclustering problem. Underpinning the proposed method is a convex program for\noptimal direction search, which for each data point d finds an optimal\ndirection in the span of the data that has minimum projection on the other data\npoints and non-vanishing projection on d. The obtained directions are\nsubsequently leveraged to identify a neighborhood set for each data point. An\nalternating direction method of multipliers framework is provided to\nefficiently solve for the optimal directions. The proposed method is shown to\nnotably outperform the existing subspace clustering methods, particularly for\nunwieldy scenarios involving high levels of noise and close subspaces, and\nyields the state-of-the-art results for the problem of face clustering using\nsubspace segmentation.\n", "versions": [{"version": "v1", "created": "Mon, 12 Jun 2017 21:52:57 GMT"}, {"version": "v2", "created": "Thu, 13 Jul 2017 22:56:21 GMT"}, {"version": "v3", "created": "Sun, 23 Jul 2017 20:36:57 GMT"}, {"version": "v4", "created": "Sun, 26 Nov 2017 15:43:15 GMT"}], "update_date": "2017-11-28", "authors_parsed": [["Rahmani", "Mostafa", ""], ["Atia", "George", ""]]}, {"id": "1706.03880", "submitter": "Vashist Avadhanula", "authors": "Shipra Agrawal, Vashist Avadhanula, Vineet Goyal and Assaf Zeevi", "title": "MNL-Bandit: A Dynamic Learning Approach to Assortment Selection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider a dynamic assortment selection problem, where in every round the\nretailer offers a subset (assortment) of $N$ substitutable products to a\nconsumer, who selects one of these products according to a multinomial logit\n(MNL) choice model. The retailer observes this choice and the objective is to\ndynamically learn the model parameters, while optimizing cumulative revenues\nover a selling horizon of length $T$. We refer to this exploration-exploitation\nformulation as the MNL-Bandit problem. Existing methods for this problem follow\nan \"explore-then-exploit\" approach, which estimate parameters to a desired\naccuracy and then, treating these estimates as if they are the correct\nparameter values, offers the optimal assortment based on these estimates. These\napproaches require certain a priori knowledge of \"separability\", determined by\nthe true parameters of the underlying MNL model, and this in turn is critical\nin determining the length of the exploration period. (Separability refers to\nthe distinguishability of the true optimal assortment from the other\nsub-optimal alternatives.) In this paper, we give an efficient algorithm that\nsimultaneously explores and exploits, achieving performance independent of the\nunderlying parameters. The algorithm can be implemented in a fully online\nmanner, without knowledge of the horizon length $T$. Furthermore, the algorithm\nis adaptive in the sense that its performance is near-optimal in both the \"well\nseparated\" case, as well as the general parameter setting where this separation\nneed not hold.\n", "versions": [{"version": "v1", "created": "Tue, 13 Jun 2017 00:54:47 GMT"}, {"version": "v2", "created": "Fri, 29 Jun 2018 22:35:34 GMT"}], "update_date": "2018-07-03", "authors_parsed": [["Agrawal", "Shipra", ""], ["Avadhanula", "Vashist", ""], ["Goyal", "Vineet", ""], ["Zeevi", "Assaf", ""]]}, {"id": "1706.03896", "submitter": "Tyler Maunu", "authors": "Tyler Maunu, Teng Zhang, Gilad Lerman", "title": "A Well-Tempered Landscape for Non-convex Robust Subspace Recovery", "comments": "58 pages, 6 figures, 1 table", "journal-ref": "Journal of Machine Learning Research, 20(37):1-59, 2019", "doi": null, "report-no": null, "categories": "cs.LG math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a mathematical analysis of a non-convex energy landscape for\nrobust subspace recovery. We prove that an underlying subspace is the only\nstationary point and local minimizer in a specified neighborhood under a\ndeterministic condition on a dataset. If the deterministic condition is\nsatisfied, we further show that a geodesic gradient descent method over the\nGrassmannian manifold can exactly recover the underlying subspace when the\nmethod is properly initialized. Proper initialization by principal component\nanalysis is guaranteed with a simple deterministic condition. Under slightly\nstronger assumptions, the gradient descent method with a piecewise constant\nstep-size scheme achieves linear convergence. The practicality of the\ndeterministic condition is demonstrated on some statistical models of data, and\nthe method achieves almost state-of-the-art recovery guarantees on the Haystack\nModel for different regimes of sample size and ambient dimension. In\nparticular, when the ambient dimension is fixed and the sample size is large\nenough, we show that our gradient method can exactly recover the underlying\nsubspace for any fixed fraction of outliers (less than 1).\n", "versions": [{"version": "v1", "created": "Tue, 13 Jun 2017 03:04:58 GMT"}, {"version": "v2", "created": "Wed, 10 Oct 2018 13:24:48 GMT"}, {"version": "v3", "created": "Thu, 28 Feb 2019 20:00:44 GMT"}], "update_date": "2019-10-18", "authors_parsed": [["Maunu", "Tyler", ""], ["Zhang", "Teng", ""], ["Lerman", "Gilad", ""]]}, {"id": "1706.03912", "submitter": "Zhe Li", "authors": "Zhe Li, Xiaoyu Wang, Xutao Lv, Tianbao Yang", "title": "SEP-Nets: Small and Effective Pattern Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While going deeper has been witnessed to improve the performance of\nconvolutional neural networks (CNN), going smaller for CNN has received\nincreasing attention recently due to its attractiveness for mobile/embedded\napplications. It remains an active and important topic how to design a small\nnetwork while retaining the performance of large and deep CNNs (e.g., Inception\nNets, ResNets). Albeit there are already intensive studies on compressing the\nsize of CNNs, the considerable drop of performance is still a key concern in\nmany designs. This paper addresses this concern with several new contributions.\nFirst, we propose a simple yet powerful method for compressing the size of deep\nCNNs based on parameter binarization. The striking difference from most\nprevious work on parameter binarization/quantization lies at different\ntreatments of $1\\times 1$ convolutions and $k\\times k$ convolutions ($k>1$),\nwhere we only binarize $k\\times k$ convolutions into binary patterns. The\nresulting networks are referred to as pattern networks. By doing this, we show\nthat previous deep CNNs such as GoogLeNet and Inception-type Nets can be\ncompressed dramatically with marginal drop in performance. Second, in light of\nthe different functionalities of $1\\times 1$ (data projection/transformation)\nand $k\\times k$ convolutions (pattern extraction), we propose a new block\nstructure codenamed the pattern residual block that adds transformed feature\nmaps generated by $1\\times 1$ convolutions to the pattern feature maps\ngenerated by $k\\times k$ convolutions, based on which we design a small network\nwith $\\sim 1$ million parameters. Combining with our parameter binarization, we\nachieve better performance on ImageNet than using similar sized networks\nincluding recently released Google MobileNets.\n", "versions": [{"version": "v1", "created": "Tue, 13 Jun 2017 06:07:26 GMT"}], "update_date": "2017-06-14", "authors_parsed": [["Li", "Zhe", ""], ["Wang", "Xiaoyu", ""], ["Lv", "Xutao", ""], ["Yang", "Tianbao", ""]]}, {"id": "1706.03922", "submitter": "Yizhen Wang", "authors": "Yizhen Wang, Somesh Jha, Kamalika Chaudhuri", "title": "Analyzing the Robustness of Nearest Neighbors to Adversarial Examples", "comments": null, "journal-ref": "International Conference on Machine Learning (ICML) 2018, Page\n  5133--5142", "doi": null, "report-no": null, "categories": "stat.ML cs.CR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motivated by safety-critical applications, test-time attacks on classifiers\nvia adversarial examples has recently received a great deal of attention.\nHowever, there is a general lack of understanding on why adversarial examples\narise; whether they originate due to inherent properties of data or due to lack\nof training samples remains ill-understood. In this work, we introduce a\ntheoretical framework analogous to bias-variance theory for understanding these\neffects.\n  We use our framework to analyze the robustness of a canonical non-parametric\nclassifier - the k-nearest neighbors. Our analysis shows that its robustness\nproperties depend critically on the value of k - the classifier may be\ninherently non-robust for small k, but its robustness approaches that of the\nBayes Optimal classifier for fast-growing k. We propose a novel modified\n1-nearest neighbor classifier, and guarantee its robustness in the large sample\nlimit. Our experiments suggest that this classifier may have good robustness\nproperties even for reasonable data set sizes.\n", "versions": [{"version": "v1", "created": "Tue, 13 Jun 2017 06:47:50 GMT"}, {"version": "v2", "created": "Thu, 7 Dec 2017 23:14:43 GMT"}, {"version": "v3", "created": "Sun, 11 Mar 2018 04:35:04 GMT"}, {"version": "v4", "created": "Wed, 13 Jun 2018 05:08:22 GMT"}, {"version": "v5", "created": "Sun, 15 Jul 2018 05:53:06 GMT"}, {"version": "v6", "created": "Wed, 19 Jun 2019 03:55:10 GMT"}], "update_date": "2019-06-20", "authors_parsed": [["Wang", "Yizhen", ""], ["Jha", "Somesh", ""], ["Chaudhuri", "Kamalika", ""]]}, {"id": "1706.03930", "submitter": "Chi Hong", "authors": "Chi Hong", "title": "Generative Models for Learning from Crowds", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.HC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose generative probabilistic models for label\naggregation. We use Gibbs sampling and a novel variational inference algorithm\nto perform the posterior inference. Empirical results show that our methods\nconsistently outperform state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Tue, 13 Jun 2017 07:28:26 GMT"}, {"version": "v2", "created": "Wed, 5 Jul 2017 07:28:14 GMT"}, {"version": "v3", "created": "Tue, 3 Oct 2017 09:18:04 GMT"}], "update_date": "2017-10-04", "authors_parsed": [["Hong", "Chi", ""]]}, {"id": "1706.03935", "submitter": "Nader Bshouty", "authors": "Nader H. Bshouty", "title": "Exact Learning from an Honest Teacher That Answers Membership Queries", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given a teacher that holds a function $f:X\\to R$ from some class of functions\n$C$. The teacher can receive from the learner an element~$d$ in the domain $X$\n(a query) and returns the value of the function in $d$, $f(d)\\in R$. The\nlearner goal is to find $f$ with a minimum number of queries, optimal time\ncomplexity, and optimal resources.\n  In this survey, we present some of the results known from the literature,\ndifferent techniques used, some new problems, and open problems.\n", "versions": [{"version": "v1", "created": "Tue, 13 Jun 2017 07:40:27 GMT"}], "update_date": "2017-06-14", "authors_parsed": [["Bshouty", "Nader H.", ""]]}, {"id": "1706.03958", "submitter": "Hadi Daneshmand", "authors": "Hadi Daneshmand, Hamed Hassani, Thomas Hofmann", "title": "Accelerated Dual Learning by Homotopic Initialization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Gradient descent and coordinate descent are well understood in terms of their\nasymptotic behavior, but less so in a transient regime often used for\napproximations in machine learning. We investigate how proper initialization\ncan have a profound effect on finding near-optimal solutions quickly. We show\nthat a certain property of a data set, namely the boundedness of the\ncorrelations between eigenfeatures and the response variable, can lead to\nfaster initial progress than expected by commonplace analysis. Convex\noptimization problems can tacitly benefit from that, but this automatism does\nnot apply to their dual formulation. We analyze this phenomenon and devise\nprovably good initialization strategies for dual optimization as well as\nheuristics for the non-convex case, relevant for deep learning. We find our\npredictions and methods to be experimentally well-supported.\n", "versions": [{"version": "v1", "created": "Tue, 13 Jun 2017 08:59:27 GMT"}], "update_date": "2017-06-14", "authors_parsed": [["Daneshmand", "Hadi", ""], ["Hassani", "Hamed", ""], ["Hofmann", "Thomas", ""]]}, {"id": "1706.03993", "submitter": "Joan Serr\\`a", "authors": "Joan Serr\\`a and Alexandros Karatzoglou", "title": "Getting deep recommenders fit: Bloom embeddings for sparse binary\n  input/output networks", "comments": "Accepted for publication at ACM RecSys 2017; previous version\n  submitted to ICLR 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.IR cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recommendation algorithms that incorporate techniques from deep learning are\nbecoming increasingly popular. Due to the structure of the data coming from\nrecommendation domains (i.e., one-hot-encoded vectors of item preferences),\nthese algorithms tend to have large input and output dimensionalities that\ndominate their overall size. This makes them difficult to train, due to the\nlimited memory of graphical processing units, and difficult to deploy on mobile\ndevices with limited hardware. To address these difficulties, we propose Bloom\nembeddings, a compression technique that can be applied to the input and output\nof neural network models dealing with sparse high-dimensional binary-coded\ninstances. Bloom embeddings are computationally efficient, and do not seriously\ncompromise the accuracy of the model up to 1/5 compression ratios. In some\ncases, they even improve over the original accuracy, with relative increases up\nto 12%. We evaluate Bloom embeddings on 7 data sets and compare it against 4\nalternative methods, obtaining favorable results. We also discuss a number of\nfurther advantages of Bloom embeddings, such as 'on-the-fly' constant-time\noperation, zero or marginal space requirements, training time speedups, or the\nfact that they do not require any change to the core model architecture or\ntraining configuration.\n", "versions": [{"version": "v1", "created": "Tue, 13 Jun 2017 10:50:25 GMT"}], "update_date": "2017-06-14", "authors_parsed": [["Serr\u00e0", "Joan", ""], ["Karatzoglou", "Alexandros", ""]]}, {"id": "1706.04026", "submitter": "Panayiotis Christodoulou", "authors": "Sotirios Chatzis, Panayiotis Christodoulou, Andreas S. Andreou", "title": "Recurrent Latent Variable Networks for Session-Based Recommendation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we attempt to ameliorate the impact of data sparsity in the\ncontext of session-based recommendation. Specifically, we seek to devise a\nmachine learning mechanism capable of extracting subtle and complex underlying\ntemporal dynamics in the observed session data, so as to inform the\nrecommendation algorithm. To this end, we improve upon systems that utilize\ndeep learning techniques with recurrently connected units; we do so by adopting\nconcepts from the field of Bayesian statistics, namely variational inference.\nOur proposed approach consists in treating the network recurrent units as\nstochastic latent variables with a prior distribution imposed over them. On\nthis basis, we proceed to infer corresponding posteriors; these can be used for\nprediction and recommendation generation, in a way that accounts for the\nuncertainty in the available sparse training data. To allow for our approach to\neasily scale to large real-world datasets, we perform inference under an\napproximate amortized variational inference (AVI) setup, whereby the learned\nposteriors are parameterized via (conventional) neural networks. We perform an\nextensive experimental evaluation of our approach using challenging benchmark\ndatasets, and illustrate its superiority over existing state-of-the-art\ntechniques.\n", "versions": [{"version": "v1", "created": "Tue, 13 Jun 2017 12:35:56 GMT"}], "update_date": "2017-06-14", "authors_parsed": [["Chatzis", "Sotirios", ""], ["Christodoulou", "Panayiotis", ""], ["Andreou", "Andreas S.", ""]]}, {"id": "1706.04052", "submitter": "Jinzhuo Wang", "authors": "Jinzhuo Wang, Wenmin Wang, Ronggang Wang, Wen Gao", "title": "Beyond Monte Carlo Tree Search: Playing Go with Deep Alternative Neural\n  Network and Long-Term Evaluation", "comments": "AAAI 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Monte Carlo tree search (MCTS) is extremely popular in computer Go which\ndetermines each action by enormous simulations in a broad and deep search tree.\nHowever, human experts select most actions by pattern analysis and careful\nevaluation rather than brute search of millions of future nteractions. In this\npaper, we propose a computer Go system that follows experts way of thinking and\nplaying. Our system consists of two parts. The first part is a novel deep\nalternative neural network (DANN) used to generate candidates of next move.\nCompared with existing deep convolutional neural network (DCNN), DANN inserts\nrecurrent layer after each convolutional layer and stacks them in an\nalternative manner. We show such setting can preserve more contexts of local\nfeatures and its evolutions which are beneficial for move prediction. The\nsecond part is a long-term evaluation (LTE) module used to provide a reliable\nevaluation of candidates rather than a single probability from move predictor.\nThis is consistent with human experts nature of playing since they can foresee\ntens of steps to give an accurate estimation of candidates. In our system, for\neach candidate, LTE calculates a cumulative reward after several future\ninteractions when local variations are settled. Combining criteria from the two\nparts, our system determines the optimal choice of next move. For more\ncomprehensive experiments, we introduce a new professional Go dataset (PGD),\nconsisting of 253233 professional records. Experiments on GoGoD and PGD\ndatasets show the DANN can substantially improve performance of move prediction\nover pure DCNN. When combining LTE, our system outperforms most relevant\napproaches and open engines based on MCTS.\n", "versions": [{"version": "v1", "created": "Tue, 13 Jun 2017 13:30:04 GMT"}], "update_date": "2017-06-14", "authors_parsed": [["Wang", "Jinzhuo", ""], ["Wang", "Wenmin", ""], ["Wang", "Ronggang", ""], ["Gao", "Wen", ""]]}, {"id": "1706.04074", "submitter": "Jian Du", "authors": "Jian Du, Shaodan Ma, Yik-Chung Wu, Soummya Kar and Jos\\'e M. F. Moura", "title": "Convergence analysis of belief propagation for pairwise linear Gaussian\n  models", "comments": "published in GlobalSIP 2017, Montreal, Canada. arXiv admin note: text\n  overlap with arXiv:1704.03969", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Gaussian belief propagation (BP) has been widely used for distributed\ninference in large-scale networks such as the smart grid, sensor networks, and\nsocial networks, where local measurements/observations are scattered over a\nwide geographical area. One particular case is when two neighboring agents\nshare a common observation. For example, to estimate voltage in the direct\ncurrent (DC) power flow model, the current measurement over a power line is\nproportional to the voltage difference between two neighboring buses. When\napplying the Gaussian BP algorithm to this type of problem, the convergence\ncondition remains an open issue. In this paper, we analyze the convergence\nproperties of Gaussian BP for this pairwise linear Gaussian model. We show\nanalytically that the updating information matrix converges at a geometric rate\nto a unique positive definite matrix with arbitrary positive semidefinite\ninitial value and further provide the necessary and sufficient convergence\ncondition for the belief mean vector to the optimal estimate.\n", "versions": [{"version": "v1", "created": "Mon, 12 Jun 2017 01:22:57 GMT"}, {"version": "v2", "created": "Wed, 2 Aug 2017 02:27:57 GMT"}, {"version": "v3", "created": "Sun, 3 Sep 2017 03:21:02 GMT"}, {"version": "v4", "created": "Sat, 18 Nov 2017 03:27:52 GMT"}], "update_date": "2017-11-21", "authors_parsed": [["Du", "Jian", ""], ["Ma", "Shaodan", ""], ["Wu", "Yik-Chung", ""], ["Kar", "Soummya", ""], ["Moura", "Jos\u00e9 M. F.", ""]]}, {"id": "1706.04081", "submitter": "Giuseppe Notarstefano", "authors": "Francesco Sasso and Angelo Coluccia and Giuseppe Notarstefano", "title": "Interaction-Based Distributed Learning in Cyber-Physical and Social\n  Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.LG math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we consider a network scenario in which agents can evaluate\neach other according to a score graph that models some physical or social\ninteraction. The goal is to design a distributed protocol, run by the agents,\nallowing them to learn their unknown state among a finite set of possible\nvalues. We propose a Bayesian framework in which scores and states are\nassociated to probabilistic events with unknown parameters and hyperparameters\nrespectively. We prove that each agent can learn its state by means of a local\nBayesian classifier and a (centralized) Maximum-Likelihood (ML) estimator of\nthe parameter-hyperparameter that combines plain ML and Empirical Bayes\napproaches. By using tools from graphical models, which allow us to gain\ninsight on conditional dependences of scores and states, we provide two relaxed\nprobabilistic models that ultimately lead to ML parameter-hyperparameter\nestimators amenable to distributed computation. In order to highlight the\nappropriateness of the proposed relaxations, we demonstrate the distributed\nestimators on a machine-to-machine testing set-up for anomaly detection and on\na social interaction set-up for user profiling.\n", "versions": [{"version": "v1", "created": "Tue, 13 Jun 2017 14:06:41 GMT"}], "update_date": "2017-06-14", "authors_parsed": [["Sasso", "Francesco", ""], ["Coluccia", "Angelo", ""], ["Notarstefano", "Giuseppe", ""]]}, {"id": "1706.04097", "submitter": "Yingyu Liang", "authors": "Yuanzhi Li, Yingyu Liang", "title": "Provable Alternating Gradient Descent for Non-negative Matrix\n  Factorization with Strong Correlations", "comments": "Accepted to the International Conference on Machine Learning (ICML),\n  2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS cs.NA stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Non-negative matrix factorization is a basic tool for decomposing data into\nthe feature and weight matrices under non-negativity constraints, and in\npractice is often solved in the alternating minimization framework. However, it\nis unclear whether such algorithms can recover the ground-truth feature matrix\nwhen the weights for different features are highly correlated, which is common\nin applications. This paper proposes a simple and natural alternating gradient\ndescent based algorithm, and shows that with a mild initialization it provably\nrecovers the ground-truth in the presence of strong correlations. In most\ninteresting cases, the correlation can be in the same order as the highest\npossible. Our analysis also reveals its several favorable features including\nrobustness to noise. We complement our theoretical results with empirical\nstudies on semi-synthetic datasets, demonstrating its advantage over several\npopular methods in recovering the ground-truth.\n", "versions": [{"version": "v1", "created": "Tue, 13 Jun 2017 14:39:59 GMT"}], "update_date": "2017-06-14", "authors_parsed": [["Li", "Yuanzhi", ""], ["Liang", "Yingyu", ""]]}, {"id": "1706.04115", "submitter": "Omer Levy", "authors": "Omer Levy, Minjoon Seo, Eunsol Choi, Luke Zettlemoyer", "title": "Zero-Shot Relation Extraction via Reading Comprehension", "comments": "CoNLL 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We show that relation extraction can be reduced to answering simple reading\ncomprehension questions, by associating one or more natural-language questions\nwith each relation slot. This reduction has several advantages: we can (1)\nlearn relation-extraction models by extending recent neural\nreading-comprehension techniques, (2) build very large training sets for those\nmodels by combining relation-specific crowd-sourced questions with distant\nsupervision, and even (3) do zero-shot learning by extracting new relation\ntypes that are only specified at test-time, for which we have no labeled\ntraining examples. Experiments on a Wikipedia slot-filling task demonstrate\nthat the approach can generalize to new questions for known relation types with\nhigh accuracy, and that zero-shot generalization to unseen relation types is\npossible, at lower accuracy levels, setting the bar for future work on this\ntask.\n", "versions": [{"version": "v1", "created": "Tue, 13 Jun 2017 15:17:42 GMT"}], "update_date": "2017-06-14", "authors_parsed": [["Levy", "Omer", ""], ["Seo", "Minjoon", ""], ["Choi", "Eunsol", ""], ["Zettlemoyer", "Luke", ""]]}, {"id": "1706.04125", "submitter": "Aadirupa Saha", "authors": "Siddharth Barman, Aditya Gopalan, and Aadirupa Saha", "title": "Online Learning for Structured Loss Spaces", "comments": "24 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider prediction with expert advice when the loss vectors are assumed\nto lie in a set described by the sum of atomic norm balls. We derive a regret\nbound for a general version of the online mirror descent (OMD) algorithm that\nuses a combination of regularizers, each adapted to the constituent atomic\nnorms. The general result recovers standard OMD regret bounds, and yields\nregret bounds for new structured settings where the loss vectors are (i) noisy\nversions of points from a low-rank subspace, (ii) sparse vectors corrupted with\nnoise, and (iii) sparse perturbations of low-rank vectors. For the problem of\nonline learning with structured losses, we also show lower bounds on regret in\nterms of rank and sparsity of the source set of the loss vectors, which implies\nlower bounds for the above additive loss settings as well.\n", "versions": [{"version": "v1", "created": "Tue, 13 Jun 2017 15:31:22 GMT"}, {"version": "v2", "created": "Sun, 12 Nov 2017 10:14:24 GMT"}], "update_date": "2017-11-15", "authors_parsed": [["Barman", "Siddharth", ""], ["Gopalan", "Aditya", ""], ["Saha", "Aadirupa", ""]]}, {"id": "1706.04148", "submitter": "Massimo Quadrana", "authors": "Massimo Quadrana, Alexandros Karatzoglou, Bal\\'azs Hidasi and Paolo\n  Cremonesi", "title": "Personalizing Session-based Recommendations with Hierarchical Recurrent\n  Neural Networks", "comments": null, "journal-ref": null, "doi": "10.1145/3109859.3109896", "report-no": null, "categories": "cs.LG cs.HC cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Session-based recommendations are highly relevant in many modern on-line\nservices (e.g. e-commerce, video streaming) and recommendation settings.\nRecently, Recurrent Neural Networks have been shown to perform very well in\nsession-based settings. While in many session-based recommendation domains user\nidentifiers are hard to come by, there are also domains in which user profiles\nare readily available. We propose a seamless way to personalize RNN models with\ncross-session information transfer and devise a Hierarchical RNN model that\nrelays end evolves latent hidden states of the RNNs across user sessions.\nResults on two industry datasets show large improvements over the session-only\nRNNs.\n", "versions": [{"version": "v1", "created": "Tue, 13 Jun 2017 16:33:52 GMT"}, {"version": "v2", "created": "Wed, 14 Jun 2017 08:02:51 GMT"}, {"version": "v3", "created": "Sat, 8 Jul 2017 16:52:18 GMT"}, {"version": "v4", "created": "Fri, 14 Jul 2017 16:42:43 GMT"}, {"version": "v5", "created": "Wed, 23 Aug 2017 18:42:56 GMT"}], "update_date": "2017-08-25", "authors_parsed": [["Quadrana", "Massimo", ""], ["Karatzoglou", "Alexandros", ""], ["Hidasi", "Bal\u00e1zs", ""], ["Cremonesi", "Paolo", ""]]}, {"id": "1706.04156", "submitter": "Vaishnavh Nagarajan", "authors": "Vaishnavh Nagarajan, J. Zico Kolter", "title": "Gradient descent GAN optimization is locally stable", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite the growing prominence of generative adversarial networks (GANs),\noptimization in GANs is still a poorly understood topic. In this paper, we\nanalyze the \"gradient descent\" form of GAN optimization i.e., the natural\nsetting where we simultaneously take small gradient steps in both generator and\ndiscriminator parameters. We show that even though GAN optimization does not\ncorrespond to a convex-concave game (even for simple parameterizations), under\nproper conditions, equilibrium points of this optimization procedure are still\n\\emph{locally asymptotically stable} for the traditional GAN formulation. On\nthe other hand, we show that the recently proposed Wasserstein GAN can have\nnon-convergent limit cycles near equilibrium. Motivated by this stability\nanalysis, we propose an additional regularization term for gradient descent GAN\nupdates, which \\emph{is} able to guarantee local stability for both the WGAN\nand the traditional GAN, and also shows practical promise in speeding up\nconvergence and addressing mode collapse.\n", "versions": [{"version": "v1", "created": "Tue, 13 Jun 2017 16:49:13 GMT"}, {"version": "v2", "created": "Wed, 15 Nov 2017 03:29:12 GMT"}, {"version": "v3", "created": "Sat, 13 Jan 2018 18:39:22 GMT"}], "update_date": "2018-01-16", "authors_parsed": [["Nagarajan", "Vaishnavh", ""], ["Kolter", "J. Zico", ""]]}, {"id": "1706.04161", "submitter": "Matej Balog", "authors": "Matej Balog, Nilesh Tripuraneni, Zoubin Ghahramani, Adrian Weller", "title": "Lost Relatives of the Gumbel Trick", "comments": "34th International Conference on Machine Learning (ICML 2017)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Gumbel trick is a method to sample from a discrete probability\ndistribution, or to estimate its normalizing partition function. The method\nrelies on repeatedly applying a random perturbation to the distribution in a\nparticular way, each time solving for the most likely configuration. We derive\nan entire family of related methods, of which the Gumbel trick is one member,\nand show that the new methods have superior properties in several settings with\nminimal additional computational cost. In particular, for the Gumbel trick to\nyield computational benefits for discrete graphical models, Gumbel\nperturbations on all configurations are typically replaced with so-called\nlow-rank perturbations. We show how a subfamily of our new methods adapts to\nthis setting, proving new upper and lower bounds on the log partition function\nand deriving a family of sequential samplers for the Gibbs distribution.\nFinally, we balance the discussion by showing how the simpler analytical form\nof the Gumbel trick enables additional theoretical results.\n", "versions": [{"version": "v1", "created": "Tue, 13 Jun 2017 17:01:54 GMT"}], "update_date": "2017-06-14", "authors_parsed": [["Balog", "Matej", ""], ["Tripuraneni", "Nilesh", ""], ["Ghahramani", "Zoubin", ""], ["Weller", "Adrian", ""]]}, {"id": "1706.04208", "submitter": "Harm van Seijen", "authors": "Harm van Seijen and Mehdi Fatemi and Joshua Romoff and Romain Laroche\n  and Tavian Barnes and Jeffrey Tsang", "title": "Hybrid Reward Architecture for Reinforcement Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One of the main challenges in reinforcement learning (RL) is generalisation.\nIn typical deep RL methods this is achieved by approximating the optimal value\nfunction with a low-dimensional representation using a deep network. While this\napproach works well in many domains, in domains where the optimal value\nfunction cannot easily be reduced to a low-dimensional representation, learning\ncan be very slow and unstable. This paper contributes towards tackling such\nchallenging domains, by proposing a new method, called Hybrid Reward\nArchitecture (HRA). HRA takes as input a decomposed reward function and learns\na separate value function for each component reward function. Because each\ncomponent typically only depends on a subset of all features, the corresponding\nvalue function can be approximated more easily by a low-dimensional\nrepresentation, enabling more effective learning. We demonstrate HRA on a\ntoy-problem and the Atari game Ms. Pac-Man, where HRA achieves above-human\nperformance.\n", "versions": [{"version": "v1", "created": "Tue, 13 Jun 2017 18:05:48 GMT"}, {"version": "v2", "created": "Tue, 28 Nov 2017 04:15:00 GMT"}], "update_date": "2017-11-29", "authors_parsed": [["van Seijen", "Harm", ""], ["Fatemi", "Mehdi", ""], ["Romoff", "Joshua", ""], ["Laroche", "Romain", ""], ["Barnes", "Tavian", ""], ["Tsang", "Jeffrey", ""]]}, {"id": "1706.04223", "submitter": "Junbo Zhao", "authors": "Jake Zhao (Junbo), Yoon Kim, Kelly Zhang, Alexander M. Rush and Yann\n  LeCun", "title": "Adversarially Regularized Autoencoders", "comments": "ICML 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep latent variable models, trained using variational autoencoders or\ngenerative adversarial networks, are now a key technique for representation\nlearning of continuous structures. However, applying similar methods to\ndiscrete structures, such as text sequences or discretized images, has proven\nto be more challenging. In this work, we propose a flexible method for training\ndeep latent variable models of discrete structures. Our approach is based on\nthe recently-proposed Wasserstein autoencoder (WAE) which formalizes the\nadversarial autoencoder (AAE) as an optimal transport problem. We first extend\nthis framework to model discrete sequences, and then further explore different\nlearned priors targeting a controllable representation. This adversarially\nregularized autoencoder (ARAE) allows us to generate natural textual outputs as\nwell as perform manipulations in the latent space to induce change in the\noutput space. Finally we show that the latent representation can be trained to\nperform unaligned textual style transfer, giving improvements both in\nautomatic/human evaluation compared to existing methods.\n", "versions": [{"version": "v1", "created": "Tue, 13 Jun 2017 19:00:53 GMT"}, {"version": "v2", "created": "Wed, 15 Nov 2017 05:41:04 GMT"}, {"version": "v3", "created": "Fri, 29 Jun 2018 00:07:16 GMT"}], "update_date": "2018-07-02", "authors_parsed": [["Zhao", "Jake", "", "Junbo"], ["Kim", "Yoon", ""], ["Zhang", "Kelly", ""], ["Rush", "Alexander M.", ""], ["LeCun", "Yann", ""]]}, {"id": "1706.04241", "submitter": "Ian Osband", "authors": "Ian Osband, Benjamin Van Roy", "title": "On Optimistic versus Randomized Exploration in Reinforcement Learning", "comments": "Extended abstract for RLDM 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We discuss the relative merits of optimistic and randomized approaches to\nexploration in reinforcement learning. Optimistic approaches presented in the\nliterature apply an optimistic boost to the value estimate at each state-action\npair and select actions that are greedy with respect to the resulting\noptimistic value function. Randomized approaches sample from among\nstatistically plausible value functions and select actions that are greedy with\nrespect to the random sample. Prior computational experience suggests that\nrandomized approaches can lead to far more statistically efficient learning. We\npresent two simple analytic examples that elucidate why this is the case. In\nprinciple, there should be optimistic approaches that fare well relative to\nrandomized approaches, but that would require intractable computation.\nOptimistic approaches that have been proposed in the literature sacrifice\nstatistical efficiency for the sake of computational efficiency. Randomized\napproaches, on the other hand, may enable simultaneous statistical and\ncomputational efficiency.\n", "versions": [{"version": "v1", "created": "Tue, 13 Jun 2017 20:22:54 GMT"}], "update_date": "2017-06-15", "authors_parsed": [["Osband", "Ian", ""], ["Van Roy", "Benjamin", ""]]}, {"id": "1706.04262", "submitter": "Abolfazl Ramezanpour", "authors": "A. Ramezanpour", "title": "Optimization by a quantum reinforcement algorithm", "comments": "14 pages, 5 figures", "journal-ref": "Phys. Rev. A 96, 052307 (2017)", "doi": "10.1103/PhysRevA.96.052307", "report-no": null, "categories": "cond-mat.dis-nn cond-mat.stat-mech cs.AI cs.LG quant-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A reinforcement algorithm solves a classical optimization problem by\nintroducing a feedback to the system which slowly changes the energy landscape\nand converges the algorithm to an optimal solution in the configuration space.\nHere, we use this strategy to concentrate (localize) preferentially the wave\nfunction of a quantum particle, which explores the configuration space of the\nproblem, on an optimal configuration. We examine the method by solving\nnumerically the equations governing the evolution of the system, which are\nsimilar to the nonlinear Schr\\\"odinger equations, for small problem sizes. In\nparticular, we observe that reinforcement increases the minimal energy gap of\nthe system in a quantum annealing algorithm. Our numerical simulations and the\nlatter observation show that such kind of quantum feedbacks might be helpful in\nsolving a computationally hard optimization problem by a quantum reinforcement\nalgorithm.\n", "versions": [{"version": "v1", "created": "Tue, 13 Jun 2017 21:32:54 GMT"}, {"version": "v2", "created": "Fri, 4 Aug 2017 12:00:47 GMT"}, {"version": "v3", "created": "Fri, 3 Nov 2017 17:19:47 GMT"}], "update_date": "2017-11-08", "authors_parsed": [["Ramezanpour", "A.", ""]]}, {"id": "1706.04265", "submitter": "Sebastian Herzog", "authors": "Sebastian Herzog, Christian Tetzlaff and Florentin W\\\"org\\\"otter", "title": "Transfer entropy-based feedback improves performance in artificial\n  neural networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.IT cs.NE math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The structure of the majority of modern deep neural networks is characterized\nby uni- directional feed-forward connectivity across a very large number of\nlayers. By contrast, the architecture of the cortex of vertebrates contains\nfewer hierarchical levels but many recurrent and feedback connections. Here we\nshow that a small, few-layer artificial neural network that employs feedback\nwill reach top level performance on a standard benchmark task, otherwise only\nobtained by large feed-forward structures. To achieve this we use feed-forward\ntransfer entropy between neurons to structure feedback connectivity. Transfer\nentropy can here intuitively be understood as a measure for the relevance of\ncertain pathways in the network, which are then amplified by feedback. Feedback\nmay therefore be key for high network performance in small brain-like\narchitectures.\n", "versions": [{"version": "v1", "created": "Tue, 13 Jun 2017 21:57:53 GMT"}, {"version": "v2", "created": "Thu, 22 Jun 2017 09:50:41 GMT"}], "update_date": "2017-06-23", "authors_parsed": [["Herzog", "Sebastian", ""], ["Tetzlaff", "Christian", ""], ["W\u00f6rg\u00f6tter", "Florentin", ""]]}, {"id": "1706.04289", "submitter": "Wray Buntine", "authors": "He Zhao, Lan Du, Wray Buntine", "title": "Leveraging Node Attributes for Incomplete Relational Data", "comments": "Appearing in ICML 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Relational data are usually highly incomplete in practice, which inspires us\nto leverage side information to improve the performance of community detection\nand link prediction. This paper presents a Bayesian probabilistic approach that\nincorporates various kinds of node attributes encoded in binary form in\nrelational models with Poisson likelihood. Our method works flexibly with both\ndirected and undirected relational networks. The inference can be done by\nefficient Gibbs sampling which leverages sparsity of both networks and node\nattributes. Extensive experiments show that our models achieve the\nstate-of-the-art link prediction results, especially with highly incomplete\nrelational data.\n", "versions": [{"version": "v1", "created": "Wed, 14 Jun 2017 00:37:07 GMT"}], "update_date": "2017-06-15", "authors_parsed": [["Zhao", "He", ""], ["Du", "Lan", ""], ["Buntine", "Wray", ""]]}, {"id": "1706.04304", "submitter": "Bangrui Chen", "authors": "Bangrui Chen, Peter I. Frazier", "title": "Dueling Bandits With Weak Regret", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider online content recommendation with implicit feedback through\npairwise comparisons, formalized as the so-called dueling bandit problem. We\nstudy the dueling bandit problem in the Condorcet winner setting, and consider\ntwo notions of regret: the more well-studied strong regret, which is 0 only\nwhen both arms pulled are the Condorcet winner; and the less well-studied weak\nregret, which is 0 if either arm pulled is the Condorcet winner. We propose a\nnew algorithm for this problem, Winner Stays (WS), with variations for each\nkind of regret: WS for weak regret (WS-W) has expected cumulative weak regret\nthat is $O(N^2)$, and $O(N\\log(N))$ if arms have a total order; WS for strong\nregret (WS-S) has expected cumulative strong regret of $O(N^2 + N \\log(T))$,\nand $O(N\\log(N)+N\\log(T))$ if arms have a total order. WS-W is the first\ndueling bandit algorithm with weak regret that is constant in time. WS is\nsimple to compute, even for problems with many arms, and we demonstrate through\nnumerical experiments on simulated and real data that WS has significantly\nsmaller regret than existing algorithms in both the weak- and strong-regret\nsettings.\n", "versions": [{"version": "v1", "created": "Wed, 14 Jun 2017 03:44:32 GMT"}], "update_date": "2017-06-15", "authors_parsed": [["Chen", "Bangrui", ""], ["Frazier", "Peter I.", ""]]}, {"id": "1706.04313", "submitter": "Huayan Wang", "authors": "Austin Stone, Huayan Wang, Michael Stark, Yi Liu, D. Scott Phoenix,\n  Dileep George", "title": "Teaching Compositionality to CNNs", "comments": "Preprint appearing in CVPR 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolutional neural networks (CNNs) have shown great success in computer\nvision, approaching human-level performance when trained for specific tasks via\napplication-specific loss functions. In this paper, we propose a method for\naugmenting and training CNNs so that their learned features are compositional.\nIt encourages networks to form representations that disentangle objects from\ntheir surroundings and from each other, thereby promoting better\ngeneralization. Our method is agnostic to the specific details of the\nunderlying CNN to which it is applied and can in principle be used with any\nCNN. As we show in our experiments, the learned representations lead to feature\nactivations that are more localized and improve performance over\nnon-compositional baselines in object recognition tasks.\n", "versions": [{"version": "v1", "created": "Wed, 14 Jun 2017 04:34:59 GMT"}], "update_date": "2017-06-15", "authors_parsed": [["Stone", "Austin", ""], ["Wang", "Huayan", ""], ["Stark", "Michael", ""], ["Liu", "Yi", ""], ["Phoenix", "D. Scott", ""], ["George", "Dileep", ""]]}, {"id": "1706.04326", "submitter": "Lambert Mathias", "authors": "Xing Fan, Emilio Monti, Lambert Mathias, Markus Dreyer", "title": "Transfer Learning for Neural Semantic Parsing", "comments": "Accepted for ACL Repl4NLP 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The goal of semantic parsing is to map natural language to a machine\ninterpretable meaning representation language (MRL). One of the constraints\nthat limits full exploration of deep learning technologies for semantic parsing\nis the lack of sufficient annotation training data. In this paper, we propose\nusing sequence-to-sequence in a multi-task setup for semantic parsing with a\nfocus on transfer learning. We explore three multi-task architectures for\nsequence-to-sequence modeling and compare their performance with an\nindependently trained model. Our experiments show that the multi-task setup\naids transfer learning from an auxiliary task with large labeled data to a\ntarget task with smaller labeled data. We see absolute accuracy gains ranging\nfrom 1.0% to 4.4% in our in- house data set, and we also see good gains ranging\nfrom 2.5% to 7.0% on the ATIS semantic parsing tasks with syntactic and\nsemantic auxiliary tasks.\n", "versions": [{"version": "v1", "created": "Wed, 14 Jun 2017 05:53:51 GMT"}], "update_date": "2017-06-15", "authors_parsed": [["Fan", "Xing", ""], ["Monti", "Emilio", ""], ["Mathias", "Lambert", ""], ["Dreyer", "Markus", ""]]}, {"id": "1706.04371", "submitter": "Haozhe Xie", "authors": "Haozhe Xie, Jie Li, Hanqing Xue", "title": "A survey of dimensionality reduction techniques based on random\n  projection", "comments": "10 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Dimensionality reduction techniques play important roles in the analysis of\nbig data. Traditional dimensionality reduction approaches, such as principal\ncomponent analysis (PCA) and linear discriminant analysis (LDA), have been\nstudied extensively in the past few decades. However, as the dimensionality of\ndata increases, the computational cost of traditional dimensionality reduction\nmethods grows exponentially, and the computation becomes prohibitively\nintractable. These drawbacks have triggered the development of random\nprojection (RP) techniques, which map high-dimensional data onto a\nlow-dimensional subspace with extremely reduced time cost. However, the RP\ntransformation matrix is generated without considering the intrinsic structure\nof the original data and usually leads to relatively high distortion.\nTherefore, in recent years, methods based on RP have been proposed to address\nthis problem. In this paper, we summarize the methods used in different\nsituations to help practitioners to employ the proper techniques for their\nspecific applications. Meanwhile, we enumerate the benefits and limitations of\nthe various methods and provide further references for researchers to develop\nnovel RP-based approaches.\n", "versions": [{"version": "v1", "created": "Wed, 14 Jun 2017 09:13:33 GMT"}, {"version": "v2", "created": "Sat, 17 Jun 2017 04:09:17 GMT"}, {"version": "v3", "created": "Fri, 27 Oct 2017 10:59:10 GMT"}, {"version": "v4", "created": "Wed, 30 May 2018 12:47:50 GMT"}], "update_date": "2018-05-31", "authors_parsed": [["Xie", "Haozhe", ""], ["Li", "Jie", ""], ["Xue", "Hanqing", ""]]}, {"id": "1706.04454", "submitter": "Levent Sagun", "authors": "Levent Sagun, Utku Evci, V. Ugur Guney, Yann Dauphin, Leon Bottou", "title": "Empirical Analysis of the Hessian of Over-Parametrized Neural Networks", "comments": "Minor update for ICLR 2018 Workshop Track presentation", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the properties of common loss surfaces through their Hessian matrix.\nIn particular, in the context of deep learning, we empirically show that the\nspectrum of the Hessian is composed of two parts: (1) the bulk centered near\nzero, (2) and outliers away from the bulk. We present numerical evidence and\nmathematical justifications to the following conjectures laid out by Sagun et\nal. (2016): Fixing data, increasing the number of parameters merely scales the\nbulk of the spectrum; fixing the dimension and changing the data (for instance\nadding more clusters or making the data less separable) only affects the\noutliers. We believe that our observations have striking implications for\nnon-convex optimization in high dimensions. First, the flatness of such\nlandscapes (which can be measured by the singularity of the Hessian) implies\nthat classical notions of basins of attraction may be quite misleading. And\nthat the discussion of wide/narrow basins may be in need of a new perspective\naround over-parametrization and redundancy that are able to create large\nconnected components at the bottom of the landscape. Second, the dependence of\nsmall number of large eigenvalues to the data distribution can be linked to the\nspectrum of the covariance matrix of gradients of model outputs. With this in\nmind, we may reevaluate the connections within the data-architecture-algorithm\nframework of a model, hoping that it would shed light into the geometry of\nhigh-dimensional and non-convex spaces in modern applications. In particular,\nwe present a case that links the two observations: small and large batch\ngradient descent appear to converge to different basins of attraction but we\nshow that they are in fact connected through their flat region and so belong to\nthe same basin.\n", "versions": [{"version": "v1", "created": "Wed, 14 Jun 2017 12:50:00 GMT"}, {"version": "v2", "created": "Wed, 8 Nov 2017 11:36:52 GMT"}, {"version": "v3", "created": "Mon, 7 May 2018 15:43:39 GMT"}], "update_date": "2018-05-08", "authors_parsed": [["Sagun", "Levent", ""], ["Evci", "Utku", ""], ["Guney", "V. Ugur", ""], ["Dauphin", "Yann", ""], ["Bottou", "Leon", ""]]}, {"id": "1706.04499", "submitter": "R\\'emi Leblond", "authors": "R\\'emi Leblond, Jean-Baptiste Alayrac, Anton Osokin and Simon\n  Lacoste-Julien", "title": "SEARNN: Training RNNs with Global-Local Losses", "comments": "Published as a conference paper at ICLR 2018, 16 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose SEARNN, a novel training algorithm for recurrent neural networks\n(RNNs) inspired by the \"learning to search\" (L2S) approach to structured\nprediction. RNNs have been widely successful in structured prediction\napplications such as machine translation or parsing, and are commonly trained\nusing maximum likelihood estimation (MLE). Unfortunately, this training loss is\nnot always an appropriate surrogate for the test error: by only maximizing the\nground truth probability, it fails to exploit the wealth of information offered\nby structured losses. Further, it introduces discrepancies between training and\npredicting (such as exposure bias) that may hurt test performance. Instead,\nSEARNN leverages test-alike search space exploration to introduce global-local\nlosses that are closer to the test error. We first demonstrate improved\nperformance over MLE on two different tasks: OCR and spelling correction. Then,\nwe propose a subsampling strategy to enable SEARNN to scale to large vocabulary\nsizes. This allows us to validate the benefits of our approach on a machine\ntranslation task.\n", "versions": [{"version": "v1", "created": "Wed, 14 Jun 2017 14:00:58 GMT"}, {"version": "v2", "created": "Mon, 29 Jan 2018 22:09:29 GMT"}, {"version": "v3", "created": "Sun, 4 Mar 2018 15:44:13 GMT"}], "update_date": "2018-03-06", "authors_parsed": [["Leblond", "R\u00e9mi", ""], ["Alayrac", "Jean-Baptiste", ""], ["Osokin", "Anton", ""], ["Lacoste-Julien", "Simon", ""]]}, {"id": "1706.04546", "submitter": "Theodoros Tsiligkaridis", "authors": "Theodoros Tsiligkaridis, David Romero", "title": "Reinforcement Learning with Budget-Constrained Nonparametric Function\n  Approximation for Opportunistic Spectrum Access", "comments": "6 pages, submitted", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.LG math.IT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Opportunistic spectrum access is one of the emerging techniques for\nmaximizing throughput in congested bands and is enabled by predicting idle\nslots in spectrum. We propose a kernel-based reinforcement learning approach\ncoupled with a novel budget-constrained sparsification technique that\nefficiently captures the environment to find the best channel access actions.\nThis approach allows learning and planning over the intrinsic state-action\nspace and extends well to large state spaces. We apply our methods to evaluate\ncoexistence of a reinforcement learning-based radio with a multi-channel\nadversarial radio and a single-channel CSMA-CA radio. Numerical experiments\nshow the performance gains over carrier-sense systems.\n", "versions": [{"version": "v1", "created": "Wed, 14 Jun 2017 15:44:52 GMT"}, {"version": "v2", "created": "Wed, 20 Jun 2018 19:46:21 GMT"}], "update_date": "2018-06-22", "authors_parsed": [["Tsiligkaridis", "Theodoros", ""], ["Romero", "David", ""]]}, {"id": "1706.04572", "submitter": "Miha Skalic", "authors": "Miha Skalic, Marcin Pekalski, Xingguo E. Pan", "title": "Deep Learning Methods for Efficient Large Scale Video Labeling", "comments": "7 pages, 5 tables, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We present a solution to \"Google Cloud and YouTube-8M Video Understanding\nChallenge\" that ranked 5th place. The proposed model is an ensemble of three\nmodel families, two frame level and one video level. The training was performed\non augmented dataset, with cross validation.\n", "versions": [{"version": "v1", "created": "Wed, 14 Jun 2017 16:24:18 GMT"}], "update_date": "2017-06-15", "authors_parsed": [["Skalic", "Miha", ""], ["Pekalski", "Marcin", ""], ["Pan", "Xingguo E.", ""]]}, {"id": "1706.04599", "submitter": "Geoff Pleiss", "authors": "Chuan Guo, Geoff Pleiss, Yu Sun, Kilian Q. Weinberger", "title": "On Calibration of Modern Neural Networks", "comments": "ICML 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Confidence calibration -- the problem of predicting probability estimates\nrepresentative of the true correctness likelihood -- is important for\nclassification models in many applications. We discover that modern neural\nnetworks, unlike those from a decade ago, are poorly calibrated. Through\nextensive experiments, we observe that depth, width, weight decay, and Batch\nNormalization are important factors influencing calibration. We evaluate the\nperformance of various post-processing calibration methods on state-of-the-art\narchitectures with image and document classification datasets. Our analysis and\nexperiments not only offer insights into neural network learning, but also\nprovide a simple and straightforward recipe for practical settings: on most\ndatasets, temperature scaling -- a single-parameter variant of Platt Scaling --\nis surprisingly effective at calibrating predictions.\n", "versions": [{"version": "v1", "created": "Wed, 14 Jun 2017 17:33:50 GMT"}, {"version": "v2", "created": "Thu, 3 Aug 2017 13:29:46 GMT"}], "update_date": "2017-08-04", "authors_parsed": [["Guo", "Chuan", ""], ["Pleiss", "Geoff", ""], ["Sun", "Yu", ""], ["Weinberger", "Kilian Q.", ""]]}, {"id": "1706.04601", "submitter": "Andrej Risteski", "authors": "Sanjeev Arora, Andrej Risteski", "title": "Provable benefits of representation learning", "comments": "22 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There is general consensus that learning representations is useful for a\nvariety of reasons, e.g. efficient use of labeled data (semi-supervised\nlearning), transfer learning and understanding hidden structure of data.\nPopular techniques for representation learning include clustering, manifold\nlearning, kernel-learning, autoencoders, Boltzmann machines, etc.\n  To study the relative merits of these techniques, it's essential to formalize\nthe definition and goals of representation learning, so that they are all\nbecome instances of the same definition. This paper introduces such a formal\nframework that also formalizes the utility of learning the representation. It\nis related to previous Bayesian notions, but with some new twists. We show the\nusefulness of our framework by exhibiting simple and natural settings -- linear\nmixture models and loglinear models, where the power of representation learning\ncan be formally shown. In these examples, representation learning can be\nperformed provably and efficiently under plausible assumptions (despite being\nNP-hard), and furthermore: (i) it greatly reduces the need for labeled data\n(semi-supervised learning) and (ii) it allows solving classification tasks when\nsimpler approaches like nearest neighbors require too much data (iii) it is\nmore powerful than manifold learning methods.\n", "versions": [{"version": "v1", "created": "Wed, 14 Jun 2017 17:35:21 GMT"}], "update_date": "2017-06-15", "authors_parsed": [["Arora", "Sanjeev", ""], ["Risteski", "Andrej", ""]]}, {"id": "1706.04635", "submitter": "Yan Zhang", "authors": "Yan Zhang and Mete Ozay and Zhun Sun and Takayuki Okatani", "title": "Information Potential Auto-Encoders", "comments": "Information Theory", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.IT math.IT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we suggest a framework to make use of mutual information as a\nregularization criterion to train Auto-Encoders (AEs). In the proposed\nframework, AEs are regularized by minimization of the mutual information\nbetween input and encoding variables of AEs during the training phase. In order\nto estimate the entropy of the encoding variables and the mutual information,\nwe propose a non-parametric method. We also give an information theoretic view\nof Variational AEs (VAEs), which suggests that VAEs can be considered as\nparametric methods that estimate entropy. Experimental results show that the\nproposed non-parametric models have more degree of freedom in terms of\nrepresentation learning of features drawn from complex distributions such as\nMixture of Gaussians, compared to methods which estimate entropy using\nparametric approaches, such as Variational AEs.\n", "versions": [{"version": "v1", "created": "Wed, 14 Jun 2017 18:52:54 GMT"}, {"version": "v2", "created": "Sun, 6 Aug 2017 06:44:19 GMT"}], "update_date": "2017-08-08", "authors_parsed": [["Zhang", "Yan", ""], ["Ozay", "Mete", ""], ["Sun", "Zhun", ""], ["Okatani", "Takayuki", ""]]}, {"id": "1706.04638", "submitter": "Thomas M\\\"ollenhoff", "authors": "Thomas Frerix, Thomas M\\\"ollenhoff, Michael Moeller, Daniel Cremers", "title": "Proximal Backpropagation", "comments": "Published as a conference paper at ICLR 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose proximal backpropagation (ProxProp) as a novel algorithm that\ntakes implicit instead of explicit gradient steps to update the network\nparameters during neural network training. Our algorithm is motivated by the\nstep size limitation of explicit gradient descent, which poses an impediment\nfor optimization. ProxProp is developed from a general point of view on the\nbackpropagation algorithm, currently the most common technique to train neural\nnetworks via stochastic gradient descent and variants thereof. Specifically, we\nshow that backpropagation of a prediction error is equivalent to sequential\ngradient descent steps on a quadratic penalty energy, which comprises the\nnetwork activations as variables of the optimization. We further analyze\ntheoretical properties of ProxProp and in particular prove that the algorithm\nyields a descent direction in parameter space and can therefore be combined\nwith a wide variety of convergent algorithms. Finally, we devise an efficient\nnumerical implementation that integrates well with popular deep learning\nframeworks. We conclude by demonstrating promising numerical results and show\nthat ProxProp can be effectively combined with common first order optimizers\nsuch as Adam.\n", "versions": [{"version": "v1", "created": "Wed, 14 Jun 2017 18:59:13 GMT"}, {"version": "v2", "created": "Fri, 3 Nov 2017 16:16:14 GMT"}, {"version": "v3", "created": "Tue, 20 Feb 2018 15:15:38 GMT"}], "update_date": "2018-02-21", "authors_parsed": [["Frerix", "Thomas", ""], ["M\u00f6llenhoff", "Thomas", ""], ["Moeller", "Michael", ""], ["Cremers", "Daniel", ""]]}, {"id": "1706.04646", "submitter": "Garrett Bernstein", "authors": "Garrett Bernstein, Ryan McKenna, Tao Sun, Daniel Sheldon, Michael Hay,\n  Gerome Miklau", "title": "Differentially Private Learning of Undirected Graphical Models using\n  Collective Graphical Models", "comments": "Accepted to ICML 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate the problem of learning discrete, undirected graphical models\nin a differentially private way. We show that the approach of releasing noisy\nsufficient statistics using the Laplace mechanism achieves a good trade-off\nbetween privacy, utility, and practicality. A naive learning algorithm that\nuses the noisy sufficient statistics \"as is\" outperforms general-purpose\ndifferentially private learning algorithms. However, it has three limitations:\nit ignores knowledge about the data generating process, rests on uncertain\ntheoretical foundations, and exhibits certain pathologies. We develop a more\nprincipled approach that applies the formalism of collective graphical models\nto perform inference over the true sufficient statistics within an\nexpectation-maximization framework. We show that this learns better models than\ncompeting approaches on both synthetic data and on real human mobility data\nused as a case study.\n", "versions": [{"version": "v1", "created": "Wed, 14 Jun 2017 19:27:25 GMT"}], "update_date": "2017-06-16", "authors_parsed": [["Bernstein", "Garrett", ""], ["McKenna", "Ryan", ""], ["Sun", "Tao", ""], ["Sheldon", "Daniel", ""], ["Hay", "Michael", ""], ["Miklau", "Gerome", ""]]}, {"id": "1706.04687", "submitter": "Ryan McNellis", "authors": "Adam N. Elmachtoub, Ryan McNellis, Sechan Oh, Marek Petrik", "title": "A Practical Method for Solving Contextual Bandit Problems Using Decision\n  Trees", "comments": "Proceedings of the 33rd Conference on Uncertainty in Artificial\n  Intelligence (UAI 2017)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many efficient algorithms with strong theoretical guarantees have been\nproposed for the contextual multi-armed bandit problem. However, applying these\nalgorithms in practice can be difficult because they require domain expertise\nto build appropriate features and to tune their parameters. We propose a new\nmethod for the contextual bandit problem that is simple, practical, and can be\napplied with little or no domain expertise. Our algorithm relies on decision\ntrees to model the context-reward relationship. Decision trees are\nnon-parametric, interpretable, and work well without hand-crafted features. To\nguide the exploration-exploitation trade-off, we use a bootstrapping approach\nwhich abstracts Thompson sampling to non-Bayesian settings. We also discuss\nseveral computational heuristics and demonstrate the performance of our method\non several datasets.\n", "versions": [{"version": "v1", "created": "Wed, 14 Jun 2017 22:38:06 GMT"}, {"version": "v2", "created": "Fri, 19 Oct 2018 19:45:03 GMT"}], "update_date": "2018-10-23", "authors_parsed": [["Elmachtoub", "Adam N.", ""], ["McNellis", "Ryan", ""], ["Oh", "Sechan", ""], ["Petrik", "Marek", ""]]}, {"id": "1706.04690", "submitter": "Satyen Kale", "authors": "Satyen Kale, Zohar Karnin, Tengyuan Liang and D\\'avid P\\'al", "title": "Adaptive Feature Selection: Computationally Efficient Online Sparse\n  Linear Regression under RIP", "comments": "Appearing in 34th International Conference on Machine Learning\n  (ICML), 2017", "journal-ref": "Proceedings of the 34th International Conference on Machine\n  Learning 70 (2017) 1780-1788", "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Online sparse linear regression is an online problem where an algorithm\nrepeatedly chooses a subset of coordinates to observe in an adversarially\nchosen feature vector, makes a real-valued prediction, receives the true label,\nand incurs the squared loss. The goal is to design an online learning algorithm\nwith sublinear regret to the best sparse linear predictor in hindsight. Without\nany assumptions, this problem is known to be computationally intractable. In\nthis paper, we make the assumption that data matrix satisfies restricted\nisometry property, and show that this assumption leads to computationally\nefficient algorithms with sublinear regret for two variants of the problem. In\nthe first variant, the true label is generated according to a sparse linear\nmodel with additive Gaussian noise. In the second, the true label is chosen\nadversarially.\n", "versions": [{"version": "v1", "created": "Wed, 14 Jun 2017 23:03:54 GMT"}], "update_date": "2020-07-27", "authors_parsed": [["Kale", "Satyen", ""], ["Karnin", "Zohar", ""], ["Liang", "Tengyuan", ""], ["P\u00e1l", "D\u00e1vid", ""]]}, {"id": "1706.04698", "submitter": "Dongsung Huh", "authors": "Dongsung Huh, Terrence J. Sejnowski", "title": "Gradient Descent for Spiking Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC cs.LG cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Much of studies on neural computation are based on network models of static\nneurons that produce analog output, despite the fact that information\nprocessing in the brain is predominantly carried out by dynamic neurons that\nproduce discrete pulses called spikes. Research in spike-based computation has\nbeen impeded by the lack of efficient supervised learning algorithm for spiking\nnetworks. Here, we present a gradient descent method for optimizing spiking\nnetwork models by introducing a differentiable formulation of spiking networks\nand deriving the exact gradient calculation. For demonstration, we trained\nrecurrent spiking networks on two dynamic tasks: one that requires optimizing\nfast (~millisecond) spike-based interactions for efficient encoding of\ninformation, and a delayed memory XOR task over extended duration (~second).\nThe results show that our method indeed optimizes the spiking network dynamics\non the time scale of individual spikes as well as behavioral time scales. In\nconclusion, our result offers a general purpose supervised learning algorithm\nfor spiking neural networks, thus advancing further investigations on\nspike-based computation.\n", "versions": [{"version": "v1", "created": "Wed, 14 Jun 2017 23:56:57 GMT"}, {"version": "v2", "created": "Mon, 19 Jun 2017 22:11:20 GMT"}], "update_date": "2017-06-21", "authors_parsed": [["Huh", "Dongsung", ""], ["Sejnowski", "Terrence J.", ""]]}, {"id": "1706.04701", "submitter": "Warren He", "authors": "Warren He and James Wei and Xinyun Chen and Nicholas Carlini and Dawn\n  Song", "title": "Adversarial Example Defenses: Ensembles of Weak Defenses are not Strong", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Ongoing research has proposed several methods to defend neural networks\nagainst adversarial examples, many of which researchers have shown to be\nineffective. We ask whether a strong defense can be created by combining\nmultiple (possibly weak) defenses. To answer this question, we study three\ndefenses that follow this approach. Two of these are recently proposed defenses\nthat intentionally combine components designed to work well together. A third\ndefense combines three independent defenses. For all the components of these\ndefenses and the combined defenses themselves, we show that an adaptive\nadversary can create adversarial examples successfully with low distortion.\nThus, our work implies that ensemble of weak defenses is not sufficient to\nprovide strong defense against adversarial examples.\n", "versions": [{"version": "v1", "created": "Thu, 15 Jun 2017 00:13:28 GMT"}], "update_date": "2017-06-16", "authors_parsed": [["He", "Warren", ""], ["Wei", "James", ""], ["Chen", "Xinyun", ""], ["Carlini", "Nicholas", ""], ["Song", "Dawn", ""]]}, {"id": "1706.04702", "submitter": "Arnulf Jentzen", "authors": "Weinan E and Jiequn Han and Arnulf Jentzen", "title": "Deep learning-based numerical methods for high-dimensional parabolic\n  partial differential equations and backward stochastic differential equations", "comments": "39 pages, 15 figures", "journal-ref": "Commun. Math. Stat. 5, 349-380 (2017)", "doi": "10.1007/s40304-017-0117-6", "report-no": null, "categories": "math.NA cs.LG cs.NE math.PR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new algorithm for solving parabolic partial differential\nequations (PDEs) and backward stochastic differential equations (BSDEs) in high\ndimension, by making an analogy between the BSDE and reinforcement learning\nwith the gradient of the solution playing the role of the policy function, and\nthe loss function given by the error between the prescribed terminal condition\nand the solution of the BSDE. The policy function is then approximated by a\nneural network, as is done in deep reinforcement learning. Numerical results\nusing TensorFlow illustrate the efficiency and accuracy of the proposed\nalgorithms for several 100-dimensional nonlinear PDEs from physics and finance\nsuch as the Allen-Cahn equation, the Hamilton-Jacobi-Bellman equation, and a\nnonlinear pricing model for financial derivatives.\n", "versions": [{"version": "v1", "created": "Thu, 15 Jun 2017 00:28:58 GMT"}], "update_date": "2020-07-14", "authors_parsed": [["E", "Weinan", ""], ["Han", "Jiequn", ""], ["Jentzen", "Arnulf", ""]]}, {"id": "1706.04711", "submitter": "Aurko Roy", "authors": "Aurko Roy, Huan Xu and Sebastian Pokutta", "title": "Reinforcement Learning under Model Mismatch", "comments": "To appear in Proceedings of NIPS 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study reinforcement learning under model misspecification, where we do not\nhave access to the true environment but only to a reasonably close\napproximation to it. We address this problem by extending the framework of\nrobust MDPs to the model-free Reinforcement Learning setting, where we do not\nhave access to the model parameters, but can only sample states from it. We\ndefine robust versions of Q-learning, SARSA, and TD-learning and prove\nconvergence to an approximately optimal robust policy and approximate value\nfunction respectively. We scale up the robust algorithms to large MDPs via\nfunction approximation and prove convergence under two different settings. We\nprove convergence of robust approximate policy iteration and robust approximate\nvalue iteration for linear architectures (under mild assumptions). We also\ndefine a robust loss function, the mean squared robust projected Bellman error\nand give stochastic gradient descent algorithms that are guaranteed to converge\nto a local minimum.\n", "versions": [{"version": "v1", "created": "Thu, 15 Jun 2017 01:06:05 GMT"}, {"version": "v2", "created": "Thu, 9 Nov 2017 01:09:29 GMT"}], "update_date": "2017-11-10", "authors_parsed": [["Roy", "Aurko", ""], ["Xu", "Huan", ""], ["Pokutta", "Sebastian", ""]]}, {"id": "1706.04719", "submitter": "Yiqing Guo", "authors": "Yiqing Guo, Xiuping Jia, and David Paull", "title": "Effective Sequential Classifier Training for SVM-based Multitemporal\n  Remote Sensing Image Classification", "comments": null, "journal-ref": null, "doi": "10.1109/TIP.2018.2808767", "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The explosive availability of remote sensing images has challenged supervised\nclassification algorithms such as Support Vector Machines (SVM), as training\nsamples tend to be highly limited due to the expensive and laborious task of\nground truthing. The temporal correlation and spectral similarity between\nmultitemporal images have opened up an opportunity to alleviate this problem.\nIn this study, a SVM-based Sequential Classifier Training (SCT-SVM) approach is\nproposed for multitemporal remote sensing image classification. The approach\nleverages the classifiers of previous images to reduce the required number of\ntraining samples for the classifier training of an incoming image. For each\nincoming image, a rough classifier is firstly predicted based on the temporal\ntrend of a set of previous classifiers. The predicted classifier is then\nfine-tuned into a more accurate position with current training samples. This\napproach can be applied progressively to sequential image data, with only a\nsmall number of training samples being required from each image. Experiments\nwere conducted with Sentinel-2A multitemporal data over an agricultural area in\nAustralia. Results showed that the proposed SCT-SVM achieved better\nclassification accuracies compared with two state-of-the-art model transfer\nalgorithms. When training data are insufficient, the overall classification\naccuracy of the incoming image was improved from 76.18% to 94.02% with the\nproposed SCT-SVM, compared with those obtained without the assistance from\nprevious images. These results demonstrate that the leverage of a priori\ninformation from previous images can provide advantageous assistance for later\nimages in multitemporal image classification.\n", "versions": [{"version": "v1", "created": "Thu, 15 Jun 2017 02:01:44 GMT"}, {"version": "v2", "created": "Wed, 17 Jan 2018 02:24:47 GMT"}], "update_date": "2018-07-17", "authors_parsed": [["Guo", "Yiqing", ""], ["Jia", "Xiuping", ""], ["Paull", "David", ""]]}, {"id": "1706.04721", "submitter": "Shannon Fenn", "authors": "Shannon Fenn, Pablo Moscato", "title": "Target Curricula via Selection of Minimum Feature Sets: a Case Study in\n  Boolean Networks", "comments": "Accepted for publication in JMLR issue 18", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the effect of introducing a curriculum of targets when training\nBoolean models on supervised Multi Label Classification (MLC) problems. In\nparticular, we consider how to order targets in the absence of prior knowledge,\nand how such a curriculum may be enforced when using meta-heuristics to train\ndiscrete non-linear models.\n  We show that hierarchical dependencies between targets can be exploited by\nenforcing an appropriate curriculum using hierarchical loss functions. On\nseveral multi output circuit-inference problems with known target difficulties,\nFeedforward Boolean Networks (FBNs) trained with such a loss function achieve\nsignificantly lower out-of-sample error, up to $10\\%$ in some cases. This\nimprovement increases as the loss places more emphasis on target order and is\nstrongly correlated with an easy-to-hard curricula. We also demonstrate the\nsame improvements on three real-world models and two Gene Regulatory Network\n(GRN) inference problems.\n  We posit a simple a-priori method for identifying an appropriate target order\nand estimating the strength of target relationships in Boolean MLCs. These\nmethods use intrinsic dimension as a proxy for target difficulty, which is\nestimated using optimal solutions to a combinatorial optimisation problem known\nas the Minimum-Feature-Set (minFS) problem. We also demonstrate that the same\ngeneralisation gains can be achieved without providing any knowledge of target\ndifficulty.\n", "versions": [{"version": "v1", "created": "Thu, 15 Jun 2017 02:08:54 GMT"}, {"version": "v2", "created": "Wed, 1 Nov 2017 02:57:13 GMT"}], "update_date": "2017-11-02", "authors_parsed": [["Fenn", "Shannon", ""], ["Moscato", "Pablo", ""]]}, {"id": "1706.04732", "submitter": "Andres Munoz Medina", "authors": "Andr\\'es Mu\\~noz Medina and Sergei Vassilvitskii", "title": "Revenue Optimization with Approximate Bid Predictions", "comments": "Accepted to NIPS 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.GT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the context of advertising auctions, finding good reserve prices is a\nnotoriously challenging learning problem. This is due to the heterogeneity of\nad opportunity types and the non-convexity of the objective function. In this\nwork, we show how to reduce reserve price optimization to the standard setting\nof prediction under squared loss, a well understood problem in the learning\ncommunity. We further bound the gap between the expected bid and revenue in\nterms of the average loss of the predictor. This is the first result that\nformally relates the revenue gained to the quality of a standard machine\nlearned model.\n", "versions": [{"version": "v1", "created": "Thu, 15 Jun 2017 04:05:56 GMT"}, {"version": "v2", "created": "Mon, 6 Nov 2017 13:39:38 GMT"}], "update_date": "2017-11-07", "authors_parsed": [["Medina", "Andr\u00e9s Mu\u00f1oz", ""], ["Vassilvitskii", "Sergei", ""]]}, {"id": "1706.04764", "submitter": "Yanhao Wang", "authors": "Yanhao Wang and Yuchen Li and Kian-Lee Tan", "title": "Efficient Representative Subset Selection over Sliding Windows", "comments": "26 pages, 9 figures, to appear in IEEE Transactions on Knowledge and\n  Data Engineering (TKDE). 2018", "journal-ref": null, "doi": "10.1109/TKDE.2018.2854182", "report-no": null, "categories": "cs.DS cs.LG cs.SI", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Representative subset selection (RSS) is an important tool for users to draw\ninsights from massive datasets. Existing literature models RSS as the\nsubmodular maximization problem to capture the \"diminishing returns\" property\nof the representativeness of selected subsets, but often only has a single\nconstraint (e.g., cardinality), which limits its applications in many\nreal-world problems. To capture the data recency issue and support different\ntypes of constraints, we formulate dynamic RSS in data streams as maximizing\nsubmodular functions subject to general $d$-knapsack constraints (SMDK) over\nsliding windows. We propose a \\textsc{KnapWindow} framework (KW) for SMDK. KW\nutilizes the \\textsc{KnapStream} algorithm (KS) for SMDK in append-only streams\nas a subroutine. It maintains a sequence of checkpoints and KS instances over\nthe sliding window. Theoretically, KW is\n$\\frac{1-\\varepsilon}{1+d}$-approximate for SMDK. Furthermore, we propose a\n\\textsc{KnapWindowPlus} framework (KW$^{+}$) to improve upon KW. KW$^{+}$\nbuilds an index \\textsc{SubKnapChk} to manage the checkpoints and KS instances.\n\\textsc{SubKnapChk} deletes a checkpoint whenever it can be approximated by its\nsuccessors. By keeping much fewer checkpoints, KW$^{+}$ achieves higher\nefficiency than KW while still guaranteeing a\n$\\frac{1-\\varepsilon'}{2+2d}$-approximate solution for SMDK. Finally, we\nevaluate the efficiency and solution quality of KW and KW$^{+}$ in real-world\ndatasets. The experimental results demonstrate that KW achieves more than two\norders of magnitude speedups over the batch baseline and preserves high-quality\nsolutions for SMDK over sliding windows. KW$^{+}$ further runs 5-10 times\nfaster than KW while providing solutions with equivalent or even better\nutilities.\n", "versions": [{"version": "v1", "created": "Thu, 15 Jun 2017 07:59:57 GMT"}, {"version": "v2", "created": "Mon, 3 Sep 2018 14:45:58 GMT"}], "update_date": "2018-09-05", "authors_parsed": [["Wang", "Yanhao", ""], ["Li", "Yuchen", ""], ["Tan", "Kian-Lee", ""]]}, {"id": "1706.04769", "submitter": "Simone Scardapane", "authors": "Simone Scardapane, Paolo Di Lorenzo", "title": "Stochastic Training of Neural Networks via Successive Convex\n  Approximations", "comments": "Preprint submitted to IEEE Transactions on Neural Networks and\n  Learning Systems", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a new family of algorithms for training neural networks\n(NNs). These are based on recent developments in the field of non-convex\noptimization, going under the general name of successive convex approximation\n(SCA) techniques. The basic idea is to iteratively replace the original\n(non-convex, highly dimensional) learning problem with a sequence of (strongly\nconvex) approximations, which are both accurate and simple to optimize.\nDifferently from similar ideas (e.g., quasi-Newton algorithms), the\napproximations can be constructed using only first-order information of the\nneural network function, in a stochastic fashion, while exploiting the overall\nstructure of the learning problem for a faster convergence. We discuss several\nuse cases, based on different choices for the loss function (e.g., squared loss\nand cross-entropy loss), and for the regularization of the NN's weights. We\nexperiment on several medium-sized benchmark problems, and on a large-scale\ndataset involving simulated physical data. The results show how the algorithm\noutperforms state-of-the-art techniques, providing faster convergence to a\nbetter minimum. Additionally, we show how the algorithm can be easily\nparallelized over multiple computational units without hindering its\nperformance. In particular, each computational unit can optimize a tailored\nsurrogate function defined on a randomly assigned subset of the input\nvariables, whose dimension can be selected depending entirely on the available\ncomputational power.\n", "versions": [{"version": "v1", "created": "Thu, 15 Jun 2017 08:11:22 GMT"}], "update_date": "2017-06-16", "authors_parsed": [["Scardapane", "Simone", ""], ["Di Lorenzo", "Paolo", ""]]}, {"id": "1706.04859", "submitter": "Wojciech Czarnecki", "authors": "Wojciech Marian Czarnecki, Simon Osindero, Max Jaderberg, Grzegorz\n  \\'Swirszcz and Razvan Pascanu", "title": "Sobolev Training for Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  At the heart of deep learning we aim to use neural networks as function\napproximators - training them to produce outputs from inputs in emulation of a\nground truth function or data creation process. In many cases we only have\naccess to input-output pairs from the ground truth, however it is becoming more\ncommon to have access to derivatives of the target output with respect to the\ninput - for example when the ground truth function is itself a neural network\nsuch as in network compression or distillation. Generally these target\nderivatives are not computed, or are ignored. This paper introduces Sobolev\nTraining for neural networks, which is a method for incorporating these target\nderivatives in addition the to target values while training. By optimising\nneural networks to not only approximate the function's outputs but also the\nfunction's derivatives we encode additional information about the target\nfunction within the parameters of the neural network. Thereby we can improve\nthe quality of our predictors, as well as the data-efficiency and\ngeneralization capabilities of our learned function approximation. We provide\ntheoretical justifications for such an approach as well as examples of\nempirical evidence on three distinct domains: regression on classical\noptimisation datasets, distilling policies of an agent playing Atari, and on\nlarge-scale applications of synthetic gradients. In all three domains the use\nof Sobolev Training, employing target derivatives in addition to target values,\nresults in models with higher accuracy and stronger generalisation.\n", "versions": [{"version": "v1", "created": "Thu, 15 Jun 2017 13:25:25 GMT"}, {"version": "v2", "created": "Tue, 4 Jul 2017 12:41:55 GMT"}, {"version": "v3", "created": "Wed, 26 Jul 2017 16:18:52 GMT"}], "update_date": "2017-07-27", "authors_parsed": [["Czarnecki", "Wojciech Marian", ""], ["Osindero", "Simon", ""], ["Jaderberg", "Max", ""], ["\u015awirszcz", "Grzegorz", ""], ["Pascanu", "Razvan", ""]]}, {"id": "1706.04892", "submitter": "Daniele Calandriello", "authors": "Daniele Calandriello, Alessandro Lazaric and Michal Valko", "title": "Second-Order Kernel Online Convex Optimization with Adaptive Sketching", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Kernel online convex optimization (KOCO) is a framework combining the\nexpressiveness of non-parametric kernel models with the regret guarantees of\nonline learning. First-order KOCO methods such as functional gradient descent\nrequire only $\\mathcal{O}(t)$ time and space per iteration, and, when the only\ninformation on the losses is their convexity, achieve a minimax optimal\n$\\mathcal{O}(\\sqrt{T})$ regret. Nonetheless, many common losses in kernel\nproblems, such as squared loss, logistic loss, and squared hinge loss posses\nstronger curvature that can be exploited. In this case, second-order KOCO\nmethods achieve $\\mathcal{O}(\\log(\\text{Det}(\\boldsymbol{K})))$ regret, which\nwe show scales as $\\mathcal{O}(d_{\\text{eff}}\\log T)$, where $d_{\\text{eff}}$\nis the effective dimension of the problem and is usually much smaller than\n$\\mathcal{O}(\\sqrt{T})$. The main drawback of second-order methods is their\nmuch higher $\\mathcal{O}(t^2)$ space and time complexity. In this paper, we\nintroduce kernel online Newton step (KONS), a new second-order KOCO method that\nalso achieves $\\mathcal{O}(d_{\\text{eff}}\\log T)$ regret. To address the\ncomputational complexity of second-order methods, we introduce a new matrix\nsketching algorithm for the kernel matrix $\\boldsymbol{K}_t$, and show that for\na chosen parameter $\\gamma \\leq 1$ our Sketched-KONS reduces the space and time\ncomplexity by a factor of $\\gamma^2$ to $\\mathcal{O}(t^2\\gamma^2)$ space and\ntime per iteration, while incurring only $1/\\gamma$ times more regret.\n", "versions": [{"version": "v1", "created": "Thu, 15 Jun 2017 14:33:08 GMT"}], "update_date": "2017-06-16", "authors_parsed": [["Calandriello", "Daniele", ""], ["Lazaric", "Alessandro", ""], ["Valko", "Michal", ""]]}, {"id": "1706.04902", "submitter": "Sebastian Ruder", "authors": "Sebastian Ruder, Ivan Vuli\\'c, Anders S{\\o}gaard", "title": "A Survey Of Cross-lingual Word Embedding Models", "comments": "Published in Journal of Artificial Intelligence Research", "journal-ref": "JAIR 65 (2019) 569-631", "doi": "10.1613/jair.1.11640", "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cross-lingual representations of words enable us to reason about word meaning\nin multilingual contexts and are a key facilitator of cross-lingual transfer\nwhen developing natural language processing models for low-resource languages.\nIn this survey, we provide a comprehensive typology of cross-lingual word\nembedding models. We compare their data requirements and objective functions.\nThe recurring theme of the survey is that many of the models presented in the\nliterature optimize for the same objectives, and that seemingly different\nmodels are often equivalent modulo optimization strategies, hyper-parameters,\nand such. We also discuss the different ways cross-lingual word embeddings are\nevaluated, as well as future challenges and research horizons.\n", "versions": [{"version": "v1", "created": "Thu, 15 Jun 2017 14:46:56 GMT"}, {"version": "v2", "created": "Wed, 18 Oct 2017 10:44:06 GMT"}, {"version": "v3", "created": "Thu, 30 May 2019 08:59:16 GMT"}, {"version": "v4", "created": "Sun, 6 Oct 2019 10:01:48 GMT"}], "update_date": "2019-10-08", "authors_parsed": [["Ruder", "Sebastian", ""], ["Vuli\u0107", "Ivan", ""], ["S\u00f8gaard", "Anders", ""]]}, {"id": "1706.04918", "submitter": "Jonathan Scarlett", "authors": "Ilija Bogunovic, Slobodan Mitrovi\\'c, Jonathan Scarlett, Volkan Cevher", "title": "Robust Submodular Maximization: A Non-Uniform Partitioning Approach", "comments": "Accepted to ICML 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of maximizing a monotone submodular function subject to\na cardinality constraint $k$, with the added twist that a number of items\n$\\tau$ from the returned set may be removed. We focus on the worst-case setting\nconsidered in (Orlin et al., 2016), in which a constant-factor approximation\nguarantee was given for $\\tau = o(\\sqrt{k})$. In this paper, we solve a key\nopen problem raised therein, presenting a new Partitioned Robust (PRo)\nsubmodular maximization algorithm that achieves the same guarantee for more\ngeneral $\\tau = o(k)$. Our algorithm constructs partitions consisting of\nbuckets with exponentially increasing sizes, and applies standard submodular\noptimization subroutines on the buckets in order to construct the robust\nsolution. We numerically demonstrate the performance of PRo in data\nsummarization and influence maximization, demonstrating gains over both the\ngreedy algorithm and the algorithm of (Orlin et al., 2016).\n", "versions": [{"version": "v1", "created": "Thu, 15 Jun 2017 15:15:10 GMT"}], "update_date": "2017-06-16", "authors_parsed": [["Bogunovic", "Ilija", ""], ["Mitrovi\u0107", "Slobodan", ""], ["Scarlett", "Jonathan", ""], ["Cevher", "Volkan", ""]]}, {"id": "1706.04933", "submitter": "Robert Busa-Fekete", "authors": "Robert Busa-Fekete, Balazs Szorenyi, Paul Weng, Shie Mannor", "title": "Multi-objective Bandits: Optimizing the Generalized Gini Index", "comments": "13 pages, 3 figures, draft version of ICML'17 paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We study the multi-armed bandit (MAB) problem where the agent receives a\nvectorial feedback that encodes many possibly competing objectives to be\noptimized. The goal of the agent is to find a policy, which can optimize these\nobjectives simultaneously in a fair way. This multi-objective online\noptimization problem is formalized by using the Generalized Gini Index (GGI)\naggregation function. We propose an online gradient descent algorithm which\nexploits the convexity of the GGI aggregation function, and controls the\nexploration in a careful way achieving a distribution-free regret\n$\\tilde{\\bigO} (T^{-1/2} )$ with high probability. We test our algorithm on\nsynthetic data as well as on an electric battery control problem where the goal\nis to trade off the use of the different cells of a battery in order to balance\ntheir respective degradation rates.\n", "versions": [{"version": "v1", "created": "Thu, 15 Jun 2017 15:43:21 GMT"}], "update_date": "2017-06-16", "authors_parsed": [["Busa-Fekete", "Robert", ""], ["Szorenyi", "Balazs", ""], ["Weng", "Paul", ""], ["Mannor", "Shie", ""]]}, {"id": "1706.04964", "submitter": "Furong Huang", "authors": "Furong Huang, Jordan Ash, John Langford, Robert Schapire", "title": "Learning Deep ResNet Blocks Sequentially using Boosting Theory", "comments": "Accepted to ICML 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural networks are known to be difficult to train due to the\ninstability of back-propagation. A deep \\emph{residual network} (ResNet) with\nidentity loops remedies this by stabilizing gradient computations. We prove a\nboosting theory for the ResNet architecture. We construct $T$ weak module\nclassifiers, each contains two of the $T$ layers, such that the combined strong\nlearner is a ResNet. Therefore, we introduce an alternative Deep ResNet\ntraining algorithm, \\emph{BoostResNet}, which is particularly suitable in\nnon-differentiable architectures. Our proposed algorithm merely requires a\nsequential training of $T$ \"shallow ResNets\" which are inexpensive. We prove\nthat the training error decays exponentially with the depth $T$ if the\n\\emph{weak module classifiers} that we train perform slightly better than some\nweak baseline. In other words, we propose a weak learning condition and prove a\nboosting theory for ResNet under the weak learning condition. Our results apply\nto general multi-class ResNets. A generalization error bound based on margin\ntheory is proved and suggests ResNet's resistant to overfitting under network\nwith $l_1$ norm bounded weights.\n", "versions": [{"version": "v1", "created": "Thu, 15 Jun 2017 16:59:07 GMT"}, {"version": "v2", "created": "Sat, 26 May 2018 01:44:53 GMT"}, {"version": "v3", "created": "Wed, 30 May 2018 03:35:50 GMT"}, {"version": "v4", "created": "Thu, 14 Jun 2018 17:45:52 GMT"}], "update_date": "2018-06-15", "authors_parsed": [["Huang", "Furong", ""], ["Ash", "Jordan", ""], ["Langford", "John", ""], ["Schapire", "Robert", ""]]}, {"id": "1706.04972", "submitter": "Hieu Pham", "authors": "Azalia Mirhoseini and Hieu Pham and Quoc V. Le and Benoit Steiner and\n  Rasmus Larsen and Yuefeng Zhou and Naveen Kumar and Mohammad Norouzi and Samy\n  Bengio and Jeff Dean", "title": "Device Placement Optimization with Reinforcement Learning", "comments": "To appear at ICML 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The past few years have witnessed a growth in size and computational\nrequirements for training and inference with neural networks. Currently, a\ncommon approach to address these requirements is to use a heterogeneous\ndistributed environment with a mixture of hardware devices such as CPUs and\nGPUs. Importantly, the decision of placing parts of the neural models on\ndevices is often made by human experts based on simple heuristics and\nintuitions. In this paper, we propose a method which learns to optimize device\nplacement for TensorFlow computational graphs. Key to our method is the use of\na sequence-to-sequence model to predict which subsets of operations in a\nTensorFlow graph should run on which of the available devices. The execution\ntime of the predicted placements is then used as the reward signal to optimize\nthe parameters of the sequence-to-sequence model. Our main result is that on\nInception-V3 for ImageNet classification, and on RNN LSTM, for language\nmodeling and neural machine translation, our model finds non-trivial device\nplacements that outperform hand-crafted heuristics and traditional algorithmic\nmethods.\n", "versions": [{"version": "v1", "created": "Tue, 13 Jun 2017 16:26:40 GMT"}, {"version": "v2", "created": "Sun, 25 Jun 2017 23:55:21 GMT"}], "update_date": "2017-11-15", "authors_parsed": [["Mirhoseini", "Azalia", ""], ["Pham", "Hieu", ""], ["Le", "Quoc V.", ""], ["Steiner", "Benoit", ""], ["Larsen", "Rasmus", ""], ["Zhou", "Yuefeng", ""], ["Kumar", "Naveen", ""], ["Norouzi", "Mohammad", ""], ["Bengio", "Samy", ""], ["Dean", "Jeff", ""]]}, {"id": "1706.04983", "submitter": "Andrew Brock", "authors": "Andrew Brock, Theodore Lim, J.M. Ritchie, Nick Weston", "title": "FreezeOut: Accelerate Training by Progressively Freezing Layers", "comments": "Extended Abstract", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The early layers of a deep neural net have the fewest parameters, but take up\nthe most computation. In this extended abstract, we propose to only train the\nhidden layers for a set portion of the training run, freezing them out\none-by-one and excluding them from the backward pass. Through experiments on\nCIFAR, we empirically demonstrate that FreezeOut yields savings of up to 20%\nwall-clock time during training with 3% loss in accuracy for DenseNets, a 20%\nspeedup without loss of accuracy for ResNets, and no improvement for VGG\nnetworks. Our code is publicly available at\nhttps://github.com/ajbrock/FreezeOut\n", "versions": [{"version": "v1", "created": "Thu, 15 Jun 2017 17:35:15 GMT"}, {"version": "v2", "created": "Sun, 18 Jun 2017 16:15:21 GMT"}], "update_date": "2017-06-20", "authors_parsed": [["Brock", "Andrew", ""], ["Lim", "Theodore", ""], ["Ritchie", "J. M.", ""], ["Weston", "Nick", ""]]}, {"id": "1706.04987", "submitter": "Mihaela Rosca", "authors": "Mihaela Rosca, Balaji Lakshminarayanan, David Warde-Farley, Shakir\n  Mohamed", "title": "Variational Approaches for Auto-Encoding Generative Adversarial Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Auto-encoding generative adversarial networks (GANs) combine the standard GAN\nalgorithm, which discriminates between real and model-generated data, with a\nreconstruction loss given by an auto-encoder. Such models aim to prevent mode\ncollapse in the learned generative model by ensuring that it is grounded in all\nthe available training data. In this paper, we develop a principle upon which\nauto-encoders can be combined with generative adversarial networks by\nexploiting the hierarchical structure of the generative model. The underlying\nprinciple shows that variational inference can be used a basic tool for\nlearning, but with the in- tractable likelihood replaced by a synthetic\nlikelihood, and the unknown posterior distribution replaced by an implicit\ndistribution; both synthetic likelihoods and implicit posterior distributions\ncan be learned using discriminators. This allows us to develop a natural fusion\nof variational auto-encoders and generative adversarial networks, combining the\nbest of both these methods. We describe a unified objective for optimization,\ndiscuss the constraints needed to guide learning, connect to the wide range of\nexisting work, and use a battery of tests to systematically and quantitatively\nassess the performance of our method.\n", "versions": [{"version": "v1", "created": "Thu, 15 Jun 2017 17:47:56 GMT"}, {"version": "v2", "created": "Sat, 21 Oct 2017 11:58:36 GMT"}], "update_date": "2017-10-24", "authors_parsed": [["Rosca", "Mihaela", ""], ["Lakshminarayanan", "Balaji", ""], ["Warde-Farley", "David", ""], ["Mohamed", "Shakir", ""]]}, {"id": "1706.05039", "submitter": "Rui Zhang", "authors": "Rui Zhang, Quanyan Zhu", "title": "Consensus-Based Transfer Linear Support Vector Machines for\n  Decentralized Multi-Task Multi-Agent Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Transfer learning has been developed to improve the performances of different\nbut related tasks in machine learning. However, such processes become less\nefficient with the increase of the size of training data and the number of\ntasks. Moreover, privacy can be violated as some tasks may contain sensitive\nand private data, which are communicated between nodes and tasks. We propose a\nconsensus-based distributed transfer learning framework, where several tasks\naim to find the best linear support vector machine (SVM) classifiers in a\ndistributed network. With alternating direction method of multipliers, tasks\ncan achieve better classification accuracies more efficiently and privately, as\neach node and each task train with their own data, and only decision variables\nare transferred between different tasks and nodes. Numerical experiments on\nMNIST datasets show that the knowledge transferred from the source tasks can be\nused to decrease the risks of the target tasks that lack training data or have\nunbalanced training labels. We show that the risks of the target tasks in the\nnodes without the data of the source tasks can also be reduced using the\ninformation transferred from the nodes who contain the data of the source\ntasks. We also show that the target tasks can enter and leave in real-time\nwithout rerunning the whole algorithm.\n", "versions": [{"version": "v1", "created": "Thu, 15 Jun 2017 18:53:11 GMT"}, {"version": "v2", "created": "Tue, 27 Mar 2018 13:45:06 GMT"}], "update_date": "2018-03-28", "authors_parsed": [["Zhang", "Rui", ""], ["Zhu", "Quanyan", ""]]}, {"id": "1706.05048", "submitter": "Ali Borji", "authors": "Ali Borji and Aysegul Dundar", "title": "Human-like Clustering with Deep Convolutional Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Classification and clustering have been studied separately in machine\nlearning and computer vision. Inspired by the recent success of deep learning\nmodels in solving various vision problems (e.g., object recognition, semantic\nsegmentation) and the fact that humans serve as the gold standard in assessing\nclustering algorithms, here, we advocate for a unified treatment of the two\nproblems and suggest that hierarchical frameworks that progressively build\ncomplex patterns on top of the simpler ones (e.g., convolutional neural\nnetworks) offer a promising solution. We do not dwell much on the learning\nmechanisms in these frameworks as they are still a matter of debate, with\nrespect to biological constraints. Instead, we emphasize on the\ncompositionality of the real world structures and objects. In particular, we\nshow that CNNs, trained end to end using back propagation with noisy labels,\nare able to cluster data points belonging to several overlapping shapes, and do\nso much better than the state of the art algorithms. The main takeaway lesson\nfrom our study is that mechanisms of human vision, particularly the hierarchal\norganization of the visual ventral stream should be taken into account in\nclustering algorithms (e.g., for learning representations in an unsupervised\nmanner or with minimum supervision) to reach human level clustering\nperformance. This, by no means, suggests that other methods do not hold merits.\nFor example, methods relying on pairwise affinities (e.g., spectral clustering)\nhave been very successful in many scenarios but still fail in some cases (e.g.,\noverlapping clusters).\n", "versions": [{"version": "v1", "created": "Thu, 15 Jun 2017 19:10:50 GMT"}, {"version": "v2", "created": "Mon, 11 Dec 2017 23:45:26 GMT"}], "update_date": "2017-12-13", "authors_parsed": [["Borji", "Ali", ""], ["Dundar", "Aysegul", ""]]}, {"id": "1706.05064", "submitter": "Junhyuk Oh", "authors": "Junhyuk Oh, Satinder Singh, Honglak Lee, Pushmeet Kohli", "title": "Zero-Shot Task Generalization with Multi-Task Deep Reinforcement\n  Learning", "comments": "ICML 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As a step towards developing zero-shot task generalization capabilities in\nreinforcement learning (RL), we introduce a new RL problem where the agent\nshould learn to execute sequences of instructions after learning useful skills\nthat solve subtasks. In this problem, we consider two types of generalizations:\nto previously unseen instructions and to longer sequences of instructions. For\ngeneralization over unseen instructions, we propose a new objective which\nencourages learning correspondences between similar subtasks by making\nanalogies. For generalization over sequential instructions, we present a\nhierarchical architecture where a meta controller learns to use the acquired\nskills for executing the instructions. To deal with delayed reward, we propose\na new neural architecture in the meta controller that learns when to update the\nsubtask, which makes learning more efficient. Experimental results on a\nstochastic 3D domain show that the proposed ideas are crucial for\ngeneralization to longer instructions as well as unseen instructions.\n", "versions": [{"version": "v1", "created": "Thu, 15 Jun 2017 20:04:35 GMT"}, {"version": "v2", "created": "Tue, 7 Nov 2017 00:37:51 GMT"}], "update_date": "2017-11-08", "authors_parsed": [["Oh", "Junhyuk", ""], ["Singh", "Satinder", ""], ["Lee", "Honglak", ""], ["Kohli", "Pushmeet", ""]]}, {"id": "1706.05069", "submitter": "Vitaly Feldman", "authors": "Vitaly Feldman and Thomas Steinke", "title": "Generalization for Adaptively-chosen Estimators via Stable Median", "comments": "To appear in Conference on Learning Theory (COLT) 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Datasets are often reused to perform multiple statistical analyses in an\nadaptive way, in which each analysis may depend on the outcomes of previous\nanalyses on the same dataset. Standard statistical guarantees do not account\nfor these dependencies and little is known about how to provably avoid\noverfitting and false discovery in the adaptive setting. We consider a natural\nformalization of this problem in which the goal is to design an algorithm that,\ngiven a limited number of i.i.d.~samples from an unknown distribution, can\nanswer adaptively-chosen queries about that distribution.\n  We present an algorithm that estimates the expectations of $k$ arbitrary\nadaptively-chosen real-valued estimators using a number of samples that scales\nas $\\sqrt{k}$. The answers given by our algorithm are essentially as accurate\nas if fresh samples were used to evaluate each estimator. In contrast, prior\nwork yields error guarantees that scale with the worst-case sensitivity of each\nestimator. We also give a version of our algorithm that can be used to verify\nanswers to such queries where the sample complexity depends logarithmically on\nthe number of queries $k$ (as in the reusable holdout technique).\n  Our algorithm is based on a simple approximate median algorithm that\nsatisfies the strong stability guarantees of differential privacy. Our\ntechniques provide a new approach for analyzing the generalization guarantees\nof differentially private algorithms.\n", "versions": [{"version": "v1", "created": "Thu, 15 Jun 2017 20:21:17 GMT"}], "update_date": "2017-06-19", "authors_parsed": [["Feldman", "Vitaly", ""], ["Steinke", "Thomas", ""]]}, {"id": "1706.05070", "submitter": "Dana Drachsler-Cohen", "authors": "Nader H. Bshouty, Dana Drachsler-Cohen, Martin Vechev, Eran Yahav", "title": "Learning Disjunctions of Predicates", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Let $F$ be a set of boolean functions. We present an algorithm for learning\n$F_\\vee := \\{\\vee_{f\\in S} f \\mid S \\subseteq F\\}$ from membership queries. Our\nalgorithm asks at most $|F| \\cdot OPT(F_\\vee)$ membership queries where\n$OPT(F_\\vee)$ is the minimum worst case number of membership queries for\nlearning $F_\\vee$. When $F$ is a set of halfspaces over a constant dimension\nspace or a set of variable inequalities, our algorithm runs in polynomial time.\n  The problem we address has practical importance in the field of program\nsynthesis, where the goal is to synthesize a program that meets some\nrequirements. Program synthesis has become popular especially in settings\naiming to help end users. In such settings, the requirements are not provided\nupfront and the synthesizer can only learn them by posing membership queries to\nthe end user. Our work enables such synthesizers to learn the exact\nrequirements while bounding the number of membership queries.\n", "versions": [{"version": "v1", "created": "Thu, 15 Jun 2017 20:21:38 GMT"}], "update_date": "2017-06-19", "authors_parsed": [["Bshouty", "Nader H.", ""], ["Drachsler-Cohen", "Dana", ""], ["Vechev", "Martin", ""], ["Yahav", "Eran", ""]]}, {"id": "1706.05075", "submitter": "Peng Zhou", "authors": "Suncong Zheng, Feng Wang, Hongyun Bao, Yuexing Hao, Peng Zhou, Bo Xu", "title": "Joint Extraction of Entities and Relations Based on a Novel Tagging\n  Scheme", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Joint extraction of entities and relations is an important task in\ninformation extraction. To tackle this problem, we firstly propose a novel\ntagging scheme that can convert the joint extraction task to a tagging problem.\nThen, based on our tagging scheme, we study different end-to-end models to\nextract entities and their relations directly, without identifying entities and\nrelations separately. We conduct experiments on a public dataset produced by\ndistant supervision method and the experimental results show that the tagging\nbased methods are better than most of the existing pipelined and joint learning\nmethods. What's more, the end-to-end model proposed in this paper, achieves the\nbest results on the public dataset.\n", "versions": [{"version": "v1", "created": "Wed, 7 Jun 2017 03:14:23 GMT"}], "update_date": "2017-06-19", "authors_parsed": [["Zheng", "Suncong", ""], ["Wang", "Feng", ""], ["Bao", "Hongyun", ""], ["Hao", "Yuexing", ""], ["Zhou", "Peng", ""], ["Xu", "Bo", ""]]}, {"id": "1706.05084", "submitter": "James Wilson", "authors": "Kelsey MacMillan and James D. Wilson", "title": "Topic supervised non-negative matrix factorization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Topic models have been extensively used to organize and interpret the\ncontents of large, unstructured corpora of text documents. Although topic\nmodels often perform well on traditional training vs. test set evaluations, it\nis often the case that the results of a topic model do not align with human\ninterpretation. This interpretability fallacy is largely due to the\nunsupervised nature of topic models, which prohibits any user guidance on the\nresults of a model. In this paper, we introduce a semi-supervised method called\ntopic supervised non-negative matrix factorization (TS-NMF) that enables the\nuser to provide labeled example documents to promote the discovery of more\nmeaningful semantic structure of a corpus. In this way, the results of TS-NMF\nbetter match the intuition and desired labeling of the user. The core of TS-NMF\nrelies on solving a non-convex optimization problem for which we derive an\niterative algorithm that is shown to be monotonic and convergent to a local\noptimum. We demonstrate the practical utility of TS-NMF on the Reuters and\nPubMed corpora, and find that TS-NMF is especially useful for conceptual or\nbroad topics, where topic key terms are not well understood. Although\nidentifying an optimal latent structure for the data is not a primary objective\nof the proposed approach, we find that TS-NMF achieves higher weighted Jaccard\nsimilarity scores than the contemporary methods, (unsupervised) NMF and latent\nDirichlet allocation, at supervision rates as low as 10% to 20%.\n", "versions": [{"version": "v1", "created": "Mon, 12 Jun 2017 04:20:04 GMT"}, {"version": "v2", "created": "Sun, 2 Jul 2017 16:00:27 GMT"}], "update_date": "2017-07-04", "authors_parsed": [["MacMillan", "Kelsey", ""], ["Wilson", "James D.", ""]]}, {"id": "1706.05098", "submitter": "Sebastian Ruder", "authors": "Sebastian Ruder", "title": "An Overview of Multi-Task Learning in Deep Neural Networks", "comments": "14 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multi-task learning (MTL) has led to successes in many applications of\nmachine learning, from natural language processing and speech recognition to\ncomputer vision and drug discovery. This article aims to give a general\noverview of MTL, particularly in deep neural networks. It introduces the two\nmost common methods for MTL in Deep Learning, gives an overview of the\nliterature, and discusses recent advances. In particular, it seeks to help ML\npractitioners apply MTL by shedding light on how MTL works and providing\nguidelines for choosing appropriate auxiliary tasks.\n", "versions": [{"version": "v1", "created": "Thu, 15 Jun 2017 21:38:12 GMT"}], "update_date": "2017-06-19", "authors_parsed": [["Ruder", "Sebastian", ""]]}, {"id": "1706.05123", "submitter": "Wenqing Xu", "authors": "Wenqing Xu, Mark Stalzer", "title": "Deriving Compact Laws Based on Algebraic Formulation of a Data Set", "comments": "16 pages, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In various subjects, there exist compact and consistent relationships between\ninput and output parameters. Discovering the relationships, or namely compact\nlaws, in a data set is of great interest in many fields, such as physics,\nchemistry, and finance. While data discovery has made great progress in\npractice thanks to the success of machine learning in recent years, the\ndevelopment of analytical approaches in finding the theory behind the data is\nrelatively slow. In this paper, we develop an innovative approach in\ndiscovering compact laws from a data set. By proposing a novel algebraic\nequation formulation, we convert the problem of deriving meaning from data into\nformulating a linear algebra model and searching for relationships that fit the\ndata. Rigorous proof is presented in validating the approach. The algebraic\nformulation allows the search of equation candidates in an explicit\nmathematical manner. Searching algorithms are also proposed for finding the\ngoverning equations with improved efficiency. For a certain type of compact\ntheory, our approach assures convergence and the discovery is computationally\nefficient and mathematically precise.\n", "versions": [{"version": "v1", "created": "Fri, 16 Jun 2017 01:13:04 GMT"}], "update_date": "2017-06-19", "authors_parsed": [["Xu", "Wenqing", ""], ["Stalzer", "Mark", ""]]}, {"id": "1706.05137", "submitter": "{\\L}ukasz Kaiser", "authors": "Lukasz Kaiser, Aidan N. Gomez, Noam Shazeer, Ashish Vaswani, Niki\n  Parmar, Llion Jones, Jakob Uszkoreit", "title": "One Model To Learn Them All", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning yields great results across many fields, from speech\nrecognition, image classification, to translation. But for each problem,\ngetting a deep model to work well involves research into the architecture and a\nlong period of tuning. We present a single model that yields good results on a\nnumber of problems spanning multiple domains. In particular, this single model\nis trained concurrently on ImageNet, multiple translation tasks, image\ncaptioning (COCO dataset), a speech recognition corpus, and an English parsing\ntask. Our model architecture incorporates building blocks from multiple\ndomains. It contains convolutional layers, an attention mechanism, and\nsparsely-gated layers. Each of these computational blocks is crucial for a\nsubset of the tasks we train on. Interestingly, even if a block is not crucial\nfor a task, we observe that adding it never hurts performance and in most cases\nimproves it on all tasks. We also show that tasks with less data benefit\nlargely from joint training with other tasks, while performance on large tasks\ndegrades only slightly if at all.\n", "versions": [{"version": "v1", "created": "Fri, 16 Jun 2017 03:10:03 GMT"}], "update_date": "2017-06-19", "authors_parsed": [["Kaiser", "Lukasz", ""], ["Gomez", "Aidan N.", ""], ["Shazeer", "Noam", ""], ["Vaswani", "Ashish", ""], ["Parmar", "Niki", ""], ["Jones", "Llion", ""], ["Uszkoreit", "Jakob", ""]]}, {"id": "1706.05148", "submitter": "Bin Dai", "authors": "Bin Dai and Yu Wang and John Aston and Gang Hua and David Wipf", "title": "Hidden Talents of the Variational Autoencoder", "comments": null, "journal-ref": "The Journal of Machine Learning Research, Volume 19 Issue 1,\n  January 2018 Pages 1573-1614", "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Variational autoencoders (VAE) represent a popular, flexible form of deep\ngenerative model that can be stochastically fit to samples from a given random\nprocess using an information-theoretic variational bound on the true underlying\ndistribution. Once so-obtained, the model can be putatively used to generate\nnew samples from this distribution, or to provide a low-dimensional latent\nrepresentation of existing samples. While quite effective in numerous\napplication domains, certain important mechanisms which govern the behavior of\nthe VAE are obfuscated by the intractable integrals and resulting stochastic\napproximations involved. Moreover, as a highly non-convex model, it remains\nunclear exactly how minima of the underlying energy relate to original design\npurposes. We attempt to better quantify these issues by analyzing a series of\ntractable special cases of increasing complexity. In doing so, we unveil\ninteresting connections with more traditional dimensionality reduction models,\nas well as an intrinsic yet underappreciated propensity for robustly dismissing\nsparse outliers when estimating latent manifolds. With respect to the latter,\nwe demonstrate that the VAE can be viewed as the natural evolution of recent\nrobust PCA models, capable of learning nonlinear manifolds of unknown dimension\nobscured by gross corruptions.\n", "versions": [{"version": "v1", "created": "Fri, 16 Jun 2017 05:31:10 GMT"}, {"version": "v2", "created": "Fri, 23 Jun 2017 10:32:13 GMT"}, {"version": "v3", "created": "Mon, 27 Nov 2017 09:12:44 GMT"}, {"version": "v4", "created": "Wed, 4 Apr 2018 11:09:42 GMT"}, {"version": "v5", "created": "Mon, 7 Oct 2019 06:10:39 GMT"}], "update_date": "2019-10-08", "authors_parsed": [["Dai", "Bin", ""], ["Wang", "Yu", ""], ["Aston", "John", ""], ["Hua", "Gang", ""], ["Wipf", "David", ""]]}, {"id": "1706.05198", "submitter": "Ruitong Huang", "authors": "Ruitong Huang, Mohammad M. Ajallooeian, Csaba Szepesv\\'ari, Martin\n  M\\\"uller", "title": "Structured Best Arm Identification with Fixed Confidence", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of identifying the best action among a set of possible\noptions when the value of each action is given by a mapping from a number of\nnoisy micro-observables in the so-called fixed confidence setting. Our main\nmotivation is the application to the minimax game search, which has been a\nmajor topic of interest in artificial intelligence. In this paper we introduce\nan abstract setting to clearly describe the essential properties of the\nproblem. While previous work only considered a two-move game tree search\nproblem, our abstract setting can be applied to the general minimax games where\nthe depth can be non-uniform and arbitrary, and transpositions are allowed. We\nintroduce a new algorithm (LUCB-micro) for the abstract setting, and give its\nlower and upper sample complexity results. Our bounds recover some previous\nresults, which were only available in more limited settings, while they also\nshed further light on how the structure of minimax problems influence sample\ncomplexity.\n", "versions": [{"version": "v1", "created": "Fri, 16 Jun 2017 09:51:36 GMT"}, {"version": "v2", "created": "Mon, 19 Jun 2017 05:47:31 GMT"}], "update_date": "2017-06-20", "authors_parsed": [["Huang", "Ruitong", ""], ["Ajallooeian", "Mohammad M.", ""], ["Szepesv\u00e1ri", "Csaba", ""], ["M\u00fcller", "Martin", ""]]}, {"id": "1706.05259", "submitter": "Zhi-Hua Zhou", "authors": "Bo-Jian Hou and Lijun Zhang and Zhi-Hua Zhou", "title": "Learning with Feature Evolvable Streams", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning with streaming data has attracted much attention during the past few\nyears. Though most studies consider data stream with fixed features, in real\npractice the features may be evolvable. For example, features of data gathered\nby limited-lifespan sensors will change when these sensors are substituted by\nnew ones. In this paper, we propose a novel learning paradigm: \\emph{Feature\nEvolvable Streaming Learning} where old features would vanish and new features\nwould occur. Rather than relying on only the current features, we attempt to\nrecover the vanished features and exploit it to improve performance.\nSpecifically, we learn two models from the recovered features and the current\nfeatures, respectively. To benefit from the recovered features, we develop two\nensemble methods. In the first method, we combine the predictions from two\nmodels and theoretically show that with the assistance of old features, the\nperformance on new features can be improved. In the second approach, we\ndynamically select the best single prediction and establish a better\nperformance guarantee when the best model switches. Experiments on both\nsynthetic and real data validate the effectiveness of our proposal.\n", "versions": [{"version": "v1", "created": "Fri, 16 Jun 2017 13:12:49 GMT"}, {"version": "v2", "created": "Mon, 8 Jan 2018 09:10:10 GMT"}], "update_date": "2018-01-09", "authors_parsed": [["Hou", "Bo-Jian", ""], ["Zhang", "Lijun", ""], ["Zhou", "Zhi-Hua", ""]]}, {"id": "1706.05335", "submitter": "Twan van Laarhoven", "authors": "Twan van Laarhoven and Elena Marchiori", "title": "Unsupervised Domain Adaptation with Random Walks on Target Labelings", "comments": "Source code to reproduce our experiments is available at\n  https://github.com/twanvl/rwa-da", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Unsupervised Domain Adaptation (DA) is used to automatize the task of\nlabeling data: an unlabeled dataset (target) is annotated using a labeled\ndataset (source) from a related domain. We cast domain adaptation as the\nproblem of finding stable labels for target examples. A new definition of label\nstability is proposed, motivated by a generalization error bound for large\nmargin linear classifiers: a target labeling is stable when, with high\nprobability, a classifier trained on a random subsample of the target with that\nlabeling yields the same labeling. We find stable labelings using a random walk\non a directed graph with transition probabilities based on labeling stability.\nThe majority vote of those labelings visited by the walk yields a stable label\nfor each target example. The resulting domain adaptation algorithm is\nstrikingly easy to implement and apply: It does not rely on data\ntransformations, which are in general computational prohibitive in the presence\nof many input features, and does not need to access the source data, which is\nadvantageous when data sharing is restricted. By acting on the original feature\nspace, our method is able to take full advantage of deep features from external\npre-trained neural networks, as demonstrated by the results of our experiments.\n", "versions": [{"version": "v1", "created": "Fri, 16 Jun 2017 16:21:10 GMT"}, {"version": "v2", "created": "Tue, 20 Mar 2018 18:59:38 GMT"}], "update_date": "2018-03-22", "authors_parsed": [["van Laarhoven", "Twan", ""], ["Marchiori", "Elena", ""]]}, {"id": "1706.05350", "submitter": "Twan van Laarhoven", "authors": "Twan van Laarhoven", "title": "L2 Regularization versus Batch and Weight Normalization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Batch Normalization is a commonly used trick to improve the training of deep\nneural networks. These neural networks use L2 regularization, also called\nweight decay, ostensibly to prevent overfitting. However, we show that L2\nregularization has no regularizing effect when combined with normalization.\nInstead, regularization has an influence on the scale of weights, and thereby\non the effective learning rate. We investigate this dependence, both in theory,\nand experimentally. We show that popular optimization methods such as ADAM only\npartially eliminate the influence of normalization on the learning rate. This\nleads to a discussion on other ways to mitigate this issue.\n", "versions": [{"version": "v1", "created": "Fri, 16 Jun 2017 17:08:08 GMT"}], "update_date": "2017-06-19", "authors_parsed": [["van Laarhoven", "Twan", ""]]}, {"id": "1706.05358", "submitter": "Chong Huang", "authors": "Chong Huang, Qiong Liu, Yan-Ying Chen, Kwang-Ting (Tim) Cheng", "title": "Local Feature Descriptor Learning with Adaptive Siamese Network", "comments": "4 pages, 2 tables, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Although the recent progress in the deep neural network has led to the\ndevelopment of learnable local feature descriptors, there is no explicit answer\nfor estimation of the necessary size of a neural network. Specifically, the\nlocal feature is represented in a low dimensional space, so the neural network\nshould have more compact structure. The small networks required for local\nfeature descriptor learning may be sensitive to initial conditions and learning\nparameters and more likely to become trapped in local minima. In order to\naddress the above problem, we introduce an adaptive pruning Siamese\nArchitecture based on neuron activation to learn local feature descriptors,\nmaking the network more computationally efficient with an improved recognition\nrate over more complex networks. Our experiments demonstrate that our learned\nlocal feature descriptors outperform the state-of-art methods in patch\nmatching.\n", "versions": [{"version": "v1", "created": "Fri, 16 Jun 2017 17:27:41 GMT"}], "update_date": "2017-06-19", "authors_parsed": [["Huang", "Chong", "", "Tim"], ["Liu", "Qiong", "", "Tim"], ["Chen", "Yan-Ying", "", "Tim"], ["Kwang-Ting", "", "", "Tim"], ["Cheng", "", ""]]}, {"id": "1706.05374", "submitter": "Kamil Ciosek", "authors": "Kamil Ciosek and Shimon Whiteson", "title": "Expected Policy Gradients", "comments": "Conference paper, AAAI-18, 12 pages including supplement", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose expected policy gradients (EPG), which unify stochastic policy\ngradients (SPG) and deterministic policy gradients (DPG) for reinforcement\nlearning. Inspired by expected sarsa, EPG integrates across the action when\nestimating the gradient, instead of relying only on the action in the sampled\ntrajectory. We establish a new general policy gradient theorem, of which the\nstochastic and deterministic policy gradient theorems are special cases. We\nalso prove that EPG reduces the variance of the gradient estimates without\nrequiring deterministic policies and, for the Gaussian case, with no\ncomputational overhead. Finally, we show that it is optimal in a certain sense\nto explore with a Gaussian policy such that the covariance is proportional to\nthe exponential of the scaled Hessian of the critic with respect to the\nactions. We present empirical results confirming that this new form of\nexploration substantially outperforms DPG with the Ornstein-Uhlenbeck heuristic\nin four challenging MuJoCo domains.\n", "versions": [{"version": "v1", "created": "Thu, 15 Jun 2017 18:27:03 GMT"}, {"version": "v2", "created": "Mon, 11 Sep 2017 16:58:25 GMT"}, {"version": "v3", "created": "Tue, 14 Nov 2017 09:58:50 GMT"}, {"version": "v4", "created": "Thu, 30 Nov 2017 20:08:18 GMT"}, {"version": "v5", "created": "Tue, 20 Mar 2018 10:58:12 GMT"}, {"version": "v6", "created": "Fri, 13 Apr 2018 19:25:12 GMT"}], "update_date": "2018-04-17", "authors_parsed": [["Ciosek", "Kamil", ""], ["Whiteson", "Shimon", ""]]}, {"id": "1706.05378", "submitter": "Fanny Yang", "authors": "Fanny Yang, Aaditya Ramdas, Kevin Jamieson, Martin J. Wainwright", "title": "A framework for Multi-A(rmed)/B(andit) testing with online FDR control", "comments": "Published as a conference paper at NIPS 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose an alternative framework to existing setups for controlling false\nalarms when multiple A/B tests are run over time. This setup arises in many\npractical applications, e.g. when pharmaceutical companies test new treatment\noptions against control pills for different diseases, or when internet\ncompanies test their default webpages versus various alternatives over time.\nOur framework proposes to replace a sequence of A/B tests by a sequence of\nbest-arm MAB instances, which can be continuously monitored by the data\nscientist. When interleaving the MAB tests with an an online false discovery\nrate (FDR) algorithm, we can obtain the best of both worlds: low sample\ncomplexity and any time online FDR control. Our main contributions are: (i) to\npropose reasonable definitions of a null hypothesis for MAB instances; (ii) to\ndemonstrate how one can derive an always-valid sequential p-value that allows\ncontinuous monitoring of each MAB test; and (iii) to show that using rejection\nthresholds of online-FDR algorithms as the confidence levels for the MAB\nalgorithms results in both sample-optimality, high power and low FDR at any\npoint in time. We run extensive simulations to verify our claims, and also\nreport results on real data collected from the New Yorker Cartoon Caption\ncontest.\n", "versions": [{"version": "v1", "created": "Fri, 16 Jun 2017 18:00:00 GMT"}, {"version": "v2", "created": "Sat, 18 Nov 2017 07:25:12 GMT"}], "update_date": "2017-11-21", "authors_parsed": [["Yang", "Fanny", ""], ["Ramdas", "Aaditya", ""], ["Jamieson", "Kevin", ""], ["Wainwright", "Martin J.", ""]]}, {"id": "1706.05394", "submitter": "Devansh Arpit", "authors": "Devansh Arpit, Stanis{\\l}aw Jastrz\\k{e}bski, Nicolas Ballas, David\n  Krueger, Emmanuel Bengio, Maxinder S. Kanwal, Tegan Maharaj, Asja Fischer,\n  Aaron Courville, Yoshua Bengio, Simon Lacoste-Julien", "title": "A Closer Look at Memorization in Deep Networks", "comments": "Appears in Proceedings of the 34th International Conference on\n  Machine Learning (ICML 2017), Devansh Arpit, Stanis{\\l}aw Jastrz\\k{e}bski,\n  Nicolas Ballas, and David Krueger contributed equally to this work", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We examine the role of memorization in deep learning, drawing connections to\ncapacity, generalization, and adversarial robustness. While deep networks are\ncapable of memorizing noise data, our results suggest that they tend to\nprioritize learning simple patterns first. In our experiments, we expose\nqualitative differences in gradient-based optimization of deep neural networks\n(DNNs) on noise vs. real data. We also demonstrate that for appropriately tuned\nexplicit regularization (e.g., dropout) we can degrade DNN training performance\non noise datasets without compromising generalization on real data. Our\nanalysis suggests that the notions of effective capacity which are dataset\nindependent are unlikely to explain the generalization performance of deep\nnetworks when trained with gradient based methods because training data itself\nplays an important role in determining the degree of memorization.\n", "versions": [{"version": "v1", "created": "Fri, 16 Jun 2017 18:11:09 GMT"}, {"version": "v2", "created": "Sat, 1 Jul 2017 14:26:51 GMT"}], "update_date": "2017-07-04", "authors_parsed": [["Arpit", "Devansh", ""], ["Jastrz\u0119bski", "Stanis\u0142aw", ""], ["Ballas", "Nicolas", ""], ["Krueger", "David", ""], ["Bengio", "Emmanuel", ""], ["Kanwal", "Maxinder S.", ""], ["Maharaj", "Tegan", ""], ["Fischer", "Asja", ""], ["Courville", "Aaron", ""], ["Bengio", "Yoshua", ""], ["Lacoste-Julien", "Simon", ""]]}, {"id": "1706.05439", "submitter": "Jack Baker", "authors": "Jack Baker, Paul Fearnhead, Emily B. Fox, Christopher Nemeth", "title": "Control Variates for Stochastic Gradient MCMC", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is well known that Markov chain Monte Carlo (MCMC) methods scale poorly\nwith dataset size. A popular class of methods for solving this issue is\nstochastic gradient MCMC. These methods use a noisy estimate of the gradient of\nthe log posterior, which reduces the per iteration computational cost of the\nalgorithm. Despite this, there are a number of results suggesting that\nstochastic gradient Langevin dynamics (SGLD), probably the most popular of\nthese methods, still has computational cost proportional to the dataset size.\nWe suggest an alternative log posterior gradient estimate for stochastic\ngradient MCMC, which uses control variates to reduce the variance. We analyse\nSGLD using this gradient estimate, and show that, under log-concavity\nassumptions on the target distribution, the computational cost required for a\ngiven level of accuracy is independent of the dataset size. Next we show that a\ndifferent control variate technique, known as zero variance control variates\ncan be applied to SGMCMC algorithms for free. This post-processing step\nimproves the inference of the algorithm by reducing the variance of the MCMC\noutput. Zero variance control variates rely on the gradient of the log\nposterior; we explore how the variance reduction is affected by replacing this\nwith the noisy gradient estimate calculated by SGMCMC.\n", "versions": [{"version": "v1", "created": "Fri, 16 Jun 2017 22:00:54 GMT"}, {"version": "v2", "created": "Thu, 14 Dec 2017 09:53:14 GMT"}], "update_date": "2017-12-15", "authors_parsed": [["Baker", "Jack", ""], ["Fearnhead", "Paul", ""], ["Fox", "Emily B.", ""], ["Nemeth", "Christopher", ""]]}, {"id": "1706.05477", "submitter": "Ehsan Abbasnejad M", "authors": "M. Ehsan Abbasnejad, Qinfeng Shi, Iman Abbasnejad, Anton van den\n  Hengel, Anthony Dick", "title": "Bayesian Conditional Generative Adverserial Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Traditional GANs use a deterministic generator function (typically a neural\nnetwork) to transform a random noise input $z$ to a sample $\\mathbf{x}$ that\nthe discriminator seeks to distinguish. We propose a new GAN called Bayesian\nConditional Generative Adversarial Networks (BC-GANs) that use a random\ngenerator function to transform a deterministic input $y'$ to a sample\n$\\mathbf{x}$. Our BC-GANs extend traditional GANs to a Bayesian framework, and\nnaturally handle unsupervised learning, supervised learning, and\nsemi-supervised learning problems. Experiments show that the proposed BC-GANs\noutperforms the state-of-the-arts.\n", "versions": [{"version": "v1", "created": "Sat, 17 Jun 2017 05:29:13 GMT"}], "update_date": "2017-06-20", "authors_parsed": [["Abbasnejad", "M. Ehsan", ""], ["Shi", "Qinfeng", ""], ["Abbasnejad", "Iman", ""], ["Hengel", "Anton van den", ""], ["Dick", "Anthony", ""]]}, {"id": "1706.05507", "submitter": "Mahesh Chandra Mukkamala", "authors": "Mahesh Chandra Mukkamala, Matthias Hein", "title": "Variants of RMSProp and Adagrad with Logarithmic Regret Bounds", "comments": "ICML 2017, 16 pages, 23 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Adaptive gradient methods have become recently very popular, in particular as\nthey have been shown to be useful in the training of deep neural networks. In\nthis paper we have analyzed RMSProp, originally proposed for the training of\ndeep neural networks, in the context of online convex optimization and show\n$\\sqrt{T}$-type regret bounds. Moreover, we propose two variants SC-Adagrad and\nSC-RMSProp for which we show logarithmic regret bounds for strongly convex\nfunctions. Finally, we demonstrate in the experiments that these new variants\noutperform other adaptive gradient techniques or stochastic gradient descent in\nthe optimization of strongly convex functions as well as in training of deep\nneural networks.\n", "versions": [{"version": "v1", "created": "Sat, 17 Jun 2017 09:48:55 GMT"}, {"version": "v2", "created": "Tue, 28 Nov 2017 18:47:37 GMT"}], "update_date": "2017-11-29", "authors_parsed": [["Mukkamala", "Mahesh Chandra", ""], ["Hein", "Matthias", ""]]}, {"id": "1706.05544", "submitter": "Charles Danko", "authors": "Zhong Wang, Tinyi Chu, Lauren A Choate, Charles G Danko", "title": "Rgtsvm: Support Vector Machines on a GPU in R", "comments": "6 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Rgtsvm provides a fast and flexible support vector machine (SVM)\nimplementation for the R language. The distinguishing feature of Rgtsvm is that\nsupport vector classification and support vector regression tasks are\nimplemented on a graphical processing unit (GPU), allowing the libraries to\nscale to millions of examples with >100-fold improvement in performance over\nexisting implementations. Nevertheless, Rgtsvm retains feature parity and has\nan interface that is compatible with the popular e1071 SVM package in R.\nAltogether, Rgtsvm enables large SVM models to be created by both experienced\nand novice practitioners.\n", "versions": [{"version": "v1", "created": "Sat, 17 Jun 2017 14:22:46 GMT"}], "update_date": "2017-06-20", "authors_parsed": [["Wang", "Zhong", ""], ["Chu", "Tinyi", ""], ["Choate", "Lauren A", ""], ["Danko", "Charles G", ""]]}, {"id": "1706.05554", "submitter": "Dan Feldman PhD", "authors": "Dan Feldman, Sedat Ozer, Daniela Rus", "title": "Coresets for Vector Summarization with Applications to Network Graphs", "comments": "ICML'2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We provide a deterministic data summarization algorithm that approximates the\nmean $\\bar{p}=\\frac{1}{n}\\sum_{p\\in P} p$ of a set $P$ of $n$ vectors in\n$\\REAL^d$, by a weighted mean $\\tilde{p}$ of a \\emph{subset} of $O(1/\\eps)$\nvectors, i.e., independent of both $n$ and $d$. We prove that the squared\nEuclidean distance between $\\bar{p}$ and $\\tilde{p}$ is at most $\\eps$\nmultiplied by the variance of $P$. We use this algorithm to maintain an\napproximated sum of vectors from an unbounded stream, using memory that is\nindependent of $d$, and logarithmic in the $n$ vectors seen so far. Our main\napplication is to extract and represent in a compact way friend groups and\nactivity summaries of users from underlying data exchanges. For example, in the\ncase of mobile networks, we can use GPS traces to identify meetings, in the\ncase of social networks, we can use information exchange to identify friend\ngroups. Our algorithm provably identifies the {\\it Heavy Hitter} entries in a\nproximity (adjacency) matrix. The Heavy Hitters can be used to extract and\nrepresent in a compact way friend groups and activity summaries of users from\nunderlying data exchanges. We evaluate the algorithm on several large data\nsets.\n", "versions": [{"version": "v1", "created": "Sat, 17 Jun 2017 16:00:44 GMT"}], "update_date": "2017-06-20", "authors_parsed": [["Feldman", "Dan", ""], ["Ozer", "Sedat", ""], ["Rus", "Daniela", ""]]}, {"id": "1706.05563", "submitter": "Timoleon Moraitis", "authors": "Timoleon Moraitis, Abu Sebastian, Irem Boybat, Manuel Le Gallo, Tomas\n  Tuma, Evangelos Eleftheriou", "title": "Fatiguing STDP: Learning from Spike-Timing Codes in the Presence of Rate\n  Codes", "comments": "8 pages, 8 figures, presented at IJCNN in May 2017", "journal-ref": "2017 International Joint Conference on Neural Networks (IJCNN),\n  Anchorage, AK, 2017, pp. 1823-1830", "doi": "10.1109/IJCNN.2017.7966072", "report-no": null, "categories": "cs.NE cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Spiking neural networks (SNNs) could play a key role in unsupervised machine\nlearning applications, by virtue of strengths related to learning from the fine\ntemporal structure of event-based signals. However, some spike-timing-related\nstrengths of SNNs are hindered by the sensitivity of spike-timing-dependent\nplasticity (STDP) rules to input spike rates, as fine temporal correlations may\nbe obstructed by coarser correlations between firing rates. In this article, we\npropose a spike-timing-dependent learning rule that allows a neuron to learn\nfrom the temporally-coded information despite the presence of rate codes. Our\nlong-term plasticity rule makes use of short-term synaptic fatigue dynamics. We\nshow analytically that, in contrast to conventional STDP rules, our fatiguing\nSTDP (FSTDP) helps learn the temporal code, and we derive the necessary\nconditions to optimize the learning process. We showcase the effectiveness of\nFSTDP in learning spike-timing correlations among processes of different rates\nin synthetic data. Finally, we use FSTDP to detect correlations in real-world\nweather data from the United States in an experimental realization of the\nalgorithm that uses a neuromorphic hardware platform comprising phase-change\nmemristive devices. Taken together, our analyses and demonstrations suggest\nthat FSTDP paves the way for the exploitation of the spike-based strengths of\nSNNs in real-world applications.\n", "versions": [{"version": "v1", "created": "Sat, 17 Jun 2017 17:11:27 GMT"}], "update_date": "2020-09-10", "authors_parsed": [["Moraitis", "Timoleon", ""], ["Sebastian", "Abu", ""], ["Boybat", "Irem", ""], ["Gallo", "Manuel Le", ""], ["Tuma", "Tomas", ""], ["Eleftheriou", "Evangelos", ""]]}, {"id": "1706.05598", "submitter": "Tengyu Ma", "authors": "Rong Ge and Tengyu Ma", "title": "On the Optimization Landscape of Tensor Decompositions", "comments": "Best paper in the NIPS 2016 Workshop on Nonconvex Optimization for\n  Machine Learning: Theory and Practice. In submission", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS math.OC math.PR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Non-convex optimization with local search heuristics has been widely used in\nmachine learning, achieving many state-of-art results. It becomes increasingly\nimportant to understand why they can work for these NP-hard problems on typical\ndata. The landscape of many objective functions in learning has been\nconjectured to have the geometric property that \"all local optima are\n(approximately) global optima\", and thus they can be solved efficiently by\nlocal search algorithms. However, establishing such property can be very\ndifficult.\n  In this paper, we analyze the optimization landscape of the random\nover-complete tensor decomposition problem, which has many applications in\nunsupervised learning, especially in learning latent variable models. In\npractice, it can be efficiently solved by gradient ascent on a non-convex\nobjective. We show that for any small constant $\\epsilon > 0$, among the set of\npoints with function values $(1+\\epsilon)$-factor larger than the expectation\nof the function, all the local maxima are approximate global maxima.\nPreviously, the best-known result only characterizes the geometry in small\nneighborhoods around the true components. Our result implies that even with an\ninitialization that is barely better than the random guess, the gradient ascent\nalgorithm is guaranteed to solve this problem.\n  Our main technique uses Kac-Rice formula and random matrix theory. To our\nbest knowledge, this is the first time when Kac-Rice formula is successfully\napplied to counting the number of local minima of a highly-structured random\npolynomial with dependent coefficients.\n", "versions": [{"version": "v1", "created": "Sun, 18 Jun 2017 01:18:42 GMT"}], "update_date": "2017-06-20", "authors_parsed": [["Ge", "Rong", ""], ["Ma", "Tengyu", ""]]}, {"id": "1706.05599", "submitter": "Mohammadhossein Chaghazardi", "authors": "Mohammadhossein Chaghazardi, Shuchin Aeron", "title": "Sample, computation vs storage tradeoffs for classification using tensor\n  subspace models", "comments": "5 Pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we exhibit the tradeoffs between the (training) sample,\ncomputation and storage complexity for the problem of supervised classification\nusing signal subspace estimation. Our main tool is the use of tensor subspaces,\ni.e. subspaces with a Kronecker structure, for embedding the data into lower\ndimensions. Among the subspaces with a Kronecker structure, we show that using\nsubspaces with a hierarchical structure for representing data leads to improved\ntradeoffs. One of the main reasons for the improvement is that embedding data\ninto these hierarchical Kronecker structured subspaces prevents overfitting at\nhigher latent dimensions.\n", "versions": [{"version": "v1", "created": "Sun, 18 Jun 2017 01:36:20 GMT"}, {"version": "v2", "created": "Wed, 21 Jun 2017 07:18:15 GMT"}, {"version": "v3", "created": "Tue, 27 Jun 2017 16:40:44 GMT"}], "update_date": "2017-06-28", "authors_parsed": [["Chaghazardi", "Mohammadhossein", ""], ["Aeron", "Shuchin", ""]]}, {"id": "1706.05648", "submitter": "Asish Ghoshal", "authors": "Asish Ghoshal and Jean Honorio", "title": "Learning Sparse Polymatrix Games in Polynomial Time and Sample\n  Complexity", "comments": null, "journal-ref": "International Conference on Artificial Intelligence and Statistics\n  (AISTATS) 2018", "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of learning sparse polymatrix games from observations\nof strategic interactions. We show that a polynomial time method based on\n$\\ell_{1,2}$-group regularized logistic regression recovers a game, whose Nash\nequilibria are the $\\epsilon$-Nash equilibria of the game from which the data\nwas generated (true game), in $\\mathcal{O}(m^4 d^4 \\log (pd))$ samples of\nstrategy profiles --- where $m$ is the maximum number of pure strategies of a\nplayer, $p$ is the number of players, and $d$ is the maximum degree of the game\ngraph. Under slightly more stringent separability conditions on the payoff\nmatrices of the true game, we show that our method learns a game with the exact\nsame Nash equilibria as the true game. We also show that $\\Omega(d \\log (pm))$\nsamples are necessary for any method to consistently recover a game, with the\nsame Nash-equilibria as the true game, from observations of strategic\ninteractions. We verify our theoretical results through simulation experiments.\n", "versions": [{"version": "v1", "created": "Sun, 18 Jun 2017 13:31:36 GMT"}, {"version": "v2", "created": "Mon, 20 Nov 2017 16:11:39 GMT"}], "update_date": "2019-01-30", "authors_parsed": [["Ghoshal", "Asish", ""], ["Honorio", "Jean", ""]]}, {"id": "1706.05681", "submitter": "Panayotis Mertikopoulos", "authors": "Zhengyuan Zhou and Panayotis Mertikopoulos and Nicholas Bambos and\n  Stephen Boyd and Peter Glynn", "title": "On the convergence of mirror descent beyond stochastic convex\n  programming", "comments": "30 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we examine the convergence of mirror descent in a class of\nstochastic optimization problems that are not necessarily convex (or even\nquasi-convex), and which we call variationally coherent. Since the standard\ntechnique of \"ergodic averaging\" offers no tangible benefits beyond convex\nprogramming, we focus directly on the algorithm's last generated sample (its\n\"last iterate\"), and we show that it converges with probabiility $1$ if the\nunderlying problem is coherent. We further consider a localized version of\nvariational coherence which ensures local convergence of stochastic mirror\ndescent (SMD) with high probability. These results contribute to the landscape\nof non-convex stochastic optimization by showing that (quasi-)convexity is not\nessential for convergence to a global minimum: rather, variational coherence, a\nmuch weaker requirement, suffices. Finally, building on the above, we reveal an\ninteresting insight regarding the convergence speed of SMD: in problems with\nsharp minima (such as generic linear programs or concave minimization\nproblems), SMD reaches a minimum point in a finite number of steps (a.s.), even\nin the presence of persistent gradient noise. This result is to be contrasted\nwith existing black-box convergence rate estimates that are only asymptotic.\n", "versions": [{"version": "v1", "created": "Sun, 18 Jun 2017 16:17:03 GMT"}, {"version": "v2", "created": "Mon, 16 Jul 2018 11:36:49 GMT"}], "update_date": "2018-07-17", "authors_parsed": [["Zhou", "Zhengyuan", ""], ["Mertikopoulos", "Panayotis", ""], ["Bambos", "Nicholas", ""], ["Boyd", "Stephen", ""], ["Glynn", "Peter", ""]]}, {"id": "1706.05683", "submitter": "Alfred Bourely", "authors": "Alfred Bourely, John Patrick Boueri, Krzysztof Choromonski", "title": "Sparse Neural Networks Topologies", "comments": "12 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose Sparse Neural Network architectures that are based on random or\nstructured bipartite graph topologies. Sparse architectures provide compression\nof the models learned and speed-ups of computations, they can also surpass\ntheir unstructured or fully connected counterparts. As we show, even more\ncompact topologies of the so-called SNN (Sparse Neural Network) can be achieved\nwith the use of structured graphs of connections between consecutive layers of\nneurons. In this paper, we investigate how the accuracy and training speed of\nthe models depend on the topology and sparsity of the neural network. Previous\napproaches using sparcity are all based on fully connected neural network\nmodels and create sparcity during training phase, instead we explicitly define\na sparse architectures of connections before the training. Building compact\nneural network models is coherent with empirical observations showing that\nthere is much redundancy in learned neural network models. We show\nexperimentally that the accuracy of the models learned with neural networks\ndepends on expander-like properties of the underlying topologies such as the\nspectral gap and algebraic connectivity rather than the density of the graphs\nof connections.\n", "versions": [{"version": "v1", "created": "Sun, 18 Jun 2017 16:30:25 GMT"}], "update_date": "2017-06-20", "authors_parsed": [["Bourely", "Alfred", ""], ["Boueri", "John Patrick", ""], ["Choromonski", "Krzysztof", ""]]}, {"id": "1706.05699", "submitter": "Dong Yin", "authors": "Dong Yin, Ashwin Pananjady, Max Lam, Dimitris Papailiopoulos, Kannan\n  Ramchandran, Peter Bartlett", "title": "Gradient Diversity: a Key Ingredient for Scalable Distributed Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It has been experimentally observed that distributed implementations of\nmini-batch stochastic gradient descent (SGD) algorithms exhibit speedup\nsaturation and decaying generalization ability beyond a particular batch-size.\nIn this work, we present an analysis hinting that high similarity between\nconcurrently processed gradients may be a cause of this performance\ndegradation. We introduce the notion of gradient diversity that measures the\ndissimilarity between concurrent gradient updates, and show its key role in the\nperformance of mini-batch SGD. We prove that on problems with high gradient\ndiversity, mini-batch SGD is amenable to better speedups, while maintaining the\ngeneralization performance of serial (one sample) SGD. We further establish\nlower bounds on convergence where mini-batch SGD slows down beyond a particular\nbatch-size, solely due to the lack of gradient diversity. We provide\nexperimental evidence indicating the key role of gradient diversity in\ndistributed learning, and discuss how heuristics like dropout, Langevin\ndynamics, and quantization can improve it.\n", "versions": [{"version": "v1", "created": "Sun, 18 Jun 2017 18:37:12 GMT"}, {"version": "v2", "created": "Thu, 17 Aug 2017 19:37:48 GMT"}, {"version": "v3", "created": "Sun, 7 Jan 2018 02:35:40 GMT"}], "update_date": "2018-01-09", "authors_parsed": [["Yin", "Dong", ""], ["Pananjady", "Ashwin", ""], ["Lam", "Max", ""], ["Papailiopoulos", "Dimitris", ""], ["Ramchandran", "Kannan", ""], ["Bartlett", "Peter", ""]]}, {"id": "1706.05730", "submitter": "Ivica Obadi\\'c", "authors": "Ivica Obadi\\'c, Gjorgji Madjarov (1), Ivica Dimitrovski (1), Dejan\n  Gjorgjevikj (1) ((1) Faculty of Computer Science and Engineering, Ss. Cyril\n  and Methodius University, Skopje, Macedonia)", "title": "Addressing Item-Cold Start Problem in Recommendation Systems using Model\n  Based Approach and Deep Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Traditional recommendation systems rely on past usage data in order to\ngenerate new recommendations. Those approaches fail to generate sensible\nrecommendations for new users and items into the system due to missing\ninformation about their past interactions. In this paper, we propose a solution\nfor successfully addressing item-cold start problem which uses model-based\napproach and recent advances in deep learning. In particular, we use latent\nfactor model for recommendation, and predict the latent factors from item's\ndescriptions using convolutional neural network when they cannot be obtained\nfrom usage data. Latent factors obtained by applying matrix factorization to\nthe available usage data are used as ground truth to train the convolutional\nneural network. To create latent factor representations for the new items, the\nconvolutional neural network uses their textual description. The results from\nthe experiments reveal that the proposed approach significantly outperforms\nseveral baseline estimators.\n", "versions": [{"version": "v1", "created": "Sun, 18 Jun 2017 21:51:10 GMT"}], "update_date": "2017-06-20", "authors_parsed": [["Obadi\u0107", "Ivica", ""], ["Madjarov", "Gjorgji", ""], ["Dimitrovski", "Ivica", ""], ["Gjorgjevikj", "Dejan", ""]]}, {"id": "1706.05744", "submitter": "Danijar Hafner", "authors": "Danijar Hafner, Alex Irpan, James Davidson, Nicolas Heess", "title": "Learning Hierarchical Information Flow with Recurrent Neural Modules", "comments": "NIPS 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose ThalNet, a deep learning model inspired by neocortical\ncommunication via the thalamus. Our model consists of recurrent neural modules\nthat send features through a routing center, endowing the modules with the\nflexibility to share features over multiple time steps. We show that our model\nlearns to route information hierarchically, processing input data by a chain of\nmodules. We observe common architectures, such as feed forward neural networks\nand skip connections, emerging as special cases of our architecture, while\nnovel connectivity patterns are learned for the text8 compression task. Our\nmodel outperforms standard recurrent neural networks on several sequential\nbenchmarks.\n", "versions": [{"version": "v1", "created": "Sun, 18 Jun 2017 23:20:12 GMT"}, {"version": "v2", "created": "Sat, 4 Nov 2017 01:20:06 GMT"}], "update_date": "2017-11-07", "authors_parsed": [["Hafner", "Danijar", ""], ["Irpan", "Alex", ""], ["Davidson", "James", ""], ["Heess", "Nicolas", ""]]}, {"id": "1706.05749", "submitter": "Nick Erickson", "authors": "Nick Erickson and Qi Zhao", "title": "Dex: Incremental Learning for Complex Environments in Deep Reinforcement\n  Learning", "comments": "NIPS 2017 submission, 10 pages, 26 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces Dex, a reinforcement learning environment toolkit\nspecialized for training and evaluation of continual learning methods as well\nas general reinforcement learning problems. We also present the novel continual\nlearning method of incremental learning, where a challenging environment is\nsolved using optimal weight initialization learned from first solving a similar\neasier environment. We show that incremental learning can produce vastly\nsuperior results than standard methods by providing a strong baseline method\nacross ten Dex environments. We finally develop a saliency method for\nqualitative analysis of reinforcement learning, which shows the impact\nincremental learning has on network attention.\n", "versions": [{"version": "v1", "created": "Mon, 19 Jun 2017 00:16:24 GMT"}], "update_date": "2017-06-20", "authors_parsed": [["Erickson", "Nick", ""], ["Zhao", "Qi", ""]]}, {"id": "1706.05764", "submitter": "Fenglong Ma", "authors": "Fenglong Ma, Radha Chitta, Jing Zhou, Quanzeng You, Tong Sun, Jing Gao", "title": "Dipole: Diagnosis Prediction in Healthcare via Attention-based\n  Bidirectional Recurrent Neural Networks", "comments": null, "journal-ref": null, "doi": "10.1145/3097983.3098088", "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Predicting the future health information of patients from the historical\nElectronic Health Records (EHR) is a core research task in the development of\npersonalized healthcare. Patient EHR data consist of sequences of visits over\ntime, where each visit contains multiple medical codes, including diagnosis,\nmedication, and procedure codes. The most important challenges for this task\nare to model the temporality and high dimensionality of sequential EHR data and\nto interpret the prediction results. Existing work solves this problem by\nemploying recurrent neural networks (RNNs) to model EHR data and utilizing\nsimple attention mechanism to interpret the results. However, RNN-based\napproaches suffer from the problem that the performance of RNNs drops when the\nlength of sequences is large, and the relationships between subsequent visits\nare ignored by current RNN-based approaches. To address these issues, we\npropose {\\sf Dipole}, an end-to-end, simple and robust model for predicting\npatients' future health information. Dipole employs bidirectional recurrent\nneural networks to remember all the information of both the past visits and the\nfuture visits, and it introduces three attention mechanisms to measure the\nrelationships of different visits for the prediction. With the attention\nmechanisms, Dipole can interpret the prediction results effectively. Dipole\nalso allows us to interpret the learned medical code representations which are\nconfirmed positively by medical experts. Experimental results on two real world\nEHR datasets show that the proposed Dipole can significantly improve the\nprediction accuracy compared with the state-of-the-art diagnosis prediction\napproaches and provide clinically meaningful interpretation.\n", "versions": [{"version": "v1", "created": "Mon, 19 Jun 2017 02:30:58 GMT"}], "update_date": "2017-06-20", "authors_parsed": [["Ma", "Fenglong", ""], ["Chitta", "Radha", ""], ["Zhou", "Jing", ""], ["You", "Quanzeng", ""], ["Sun", "Tong", ""], ["Gao", "Jing", ""]]}, {"id": "1706.05781", "submitter": "Keunwoo Choi Mr", "authors": "Keunwoo Choi, Deokjin Joo, Juho Kim", "title": "Kapre: On-GPU Audio Preprocessing Layers for a Quick Implementation of\n  Deep Neural Network Models with Keras", "comments": "ICML 2017 machine learning for music discovery", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.LG cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce Kapre, Keras layers for audio and music signal preprocessing.\nMusic research using deep neural networks requires a heavy and tedious\npreprocessing stage, for which audio processing parameters are often ignored in\nparameter optimisation. To solve this problem, Kapre implements time-frequency\nconversions, normalisation, and data augmentation as Keras layers. We report\nsimple benchmark results, showing real-time on-GPU preprocessing adds a\nreasonable amount of computation.\n", "versions": [{"version": "v1", "created": "Mon, 19 Jun 2017 04:42:14 GMT"}], "update_date": "2017-06-20", "authors_parsed": [["Choi", "Keunwoo", ""], ["Joo", "Deokjin", ""], ["Kim", "Juho", ""]]}, {"id": "1706.05801", "submitter": "Karim Abou-Moustafa", "authors": "Karim Abou-Moustafa and Csaba Szepesvari", "title": "An a Priori Exponential Tail Bound for k-Folds Cross-Validation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider a priori generalization bounds developed in terms of\ncross-validation estimates and the stability of learners. In particular, we\nfirst derive an exponential Efron-Stein type tail inequality for the\nconcentration of a general function of n independent random variables. Next,\nunder some reasonable notion of stability, we use this exponential tail bound\nto analyze the concentration of the k-fold cross-validation (KFCV) estimate\naround the true risk of a hypothesis generated by a general learning rule.\nWhile the accumulated literature has often attributed this concentration to the\nbias and variance of the estimator, our bound attributes this concentration to\nthe stability of the learning rule and the number of folds k. This insight\nraises valid concerns related to the practical use of KFCV and suggests\nresearch directions to obtain reliable empirical estimates of the actual risk.\n", "versions": [{"version": "v1", "created": "Mon, 19 Jun 2017 06:38:55 GMT"}], "update_date": "2017-06-20", "authors_parsed": [["Abou-Moustafa", "Karim", ""], ["Szepesvari", "Csaba", ""]]}, {"id": "1706.05806", "submitter": "Maithra Raghu", "authors": "Maithra Raghu, Justin Gilmer, Jason Yosinski, Jascha Sohl-Dickstein", "title": "SVCCA: Singular Vector Canonical Correlation Analysis for Deep Learning\n  Dynamics and Interpretability", "comments": "Accepted to NIPS 2017, code: https://github.com/google/svcca/ , new\n  plots on Imagenet", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new technique, Singular Vector Canonical Correlation Analysis\n(SVCCA), a tool for quickly comparing two representations in a way that is both\ninvariant to affine transform (allowing comparison between different layers and\nnetworks) and fast to compute (allowing more comparisons to be calculated than\nwith previous methods). We deploy this tool to measure the intrinsic\ndimensionality of layers, showing in some cases needless over-parameterization;\nto probe learning dynamics throughout training, finding that networks converge\nto final representations from the bottom up; to show where class-specific\ninformation in networks is formed; and to suggest new training regimes that\nsimultaneously save computation and overfit less. Code:\nhttps://github.com/google/svcca/\n", "versions": [{"version": "v1", "created": "Mon, 19 Jun 2017 07:09:20 GMT"}, {"version": "v2", "created": "Wed, 8 Nov 2017 08:36:27 GMT"}], "update_date": "2017-11-09", "authors_parsed": [["Raghu", "Maithra", ""], ["Gilmer", "Justin", ""], ["Yosinski", "Jason", ""], ["Sohl-Dickstein", "Jascha", ""]]}, {"id": "1706.05928", "submitter": "Carlos M. Ala\\'iz", "authors": "Carlos M. Ala\\'iz and Johan A. K. Suykens", "title": "Modified Frank-Wolfe Algorithm for Enhanced Sparsity in Support Vector\n  Machine Classifiers", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work proposes a new algorithm for training a re-weighted L2 Support\nVector Machine (SVM), inspired on the re-weighted Lasso algorithm of Cand\\`es\net al. and on the equivalence between Lasso and SVM shown recently by Jaggi. In\nparticular, the margin required for each training vector is set independently,\ndefining a new weighted SVM model. These weights are selected to be binary, and\nthey are automatically adapted during the training of the model, resulting in a\nvariation of the Frank-Wolfe optimization algorithm with essentially the same\ncomputational complexity as the original algorithm. As shown experimentally,\nthis algorithm is computationally cheaper to apply since it requires less\niterations to converge, and it produces models with a sparser representation in\nterms of support vectors and which are more stable with respect to the\nselection of the regularization hyper-parameter.\n", "versions": [{"version": "v1", "created": "Mon, 19 Jun 2017 13:31:09 GMT"}, {"version": "v2", "created": "Fri, 13 Apr 2018 13:28:15 GMT"}], "update_date": "2018-04-16", "authors_parsed": [["Ala\u00edz", "Carlos M.", ""], ["Suykens", "Johan A. K.", ""]]}, {"id": "1706.05966", "submitter": "Ahmed Alaa", "authors": "Ahmed M. Alaa, Michael Weisz, and Mihaela van der Schaar", "title": "Deep Counterfactual Networks with Propensity-Dropout", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel approach for inferring the individualized causal effects\nof a treatment (intervention) from observational data. Our approach\nconceptualizes causal inference as a multitask learning problem; we model a\nsubject's potential outcomes using a deep multitask network with a set of\nshared layers among the factual and counterfactual outcomes, and a set of\noutcome-specific layers. The impact of selection bias in the observational data\nis alleviated via a propensity-dropout regularization scheme, in which the\nnetwork is thinned for every training example via a dropout probability that\ndepends on the associated propensity score. The network is trained in\nalternating phases, where in each phase we use the training examples of one of\nthe two potential outcomes (treated and control populations) to update the\nweights of the shared layers and the respective outcome-specific layers.\nExperiments conducted on data based on a real-world observational study show\nthat our algorithm outperforms the state-of-the-art.\n", "versions": [{"version": "v1", "created": "Mon, 19 Jun 2017 14:12:12 GMT"}], "update_date": "2017-06-20", "authors_parsed": [["Alaa", "Ahmed M.", ""], ["Weisz", "Michael", ""], ["van der Schaar", "Mihaela", ""]]}, {"id": "1706.06028", "submitter": "Mariano Tepper", "authors": "Mariano Tepper, Anirvan M. Sengupta, and Dmitri Chklovskii", "title": "Clustering is semidefinitely not that hard: Nonnegative SDP for manifold\n  disentangling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In solving hard computational problems, semidefinite program (SDP)\nrelaxations often play an important role because they come with a guarantee of\noptimality. Here, we focus on a popular semidefinite relaxation of K-means\nclustering which yields the same solution as the non-convex original\nformulation for well segregated datasets. We report an unexpected finding: when\ndata contains (greater than zero-dimensional) manifolds, the SDP solution\ncaptures such geometrical structures. Unlike traditional manifold embedding\ntechniques, our approach does not rely on manually defining a kernel but rather\nenforces locality via a nonnegativity constraint. We thus call our approach\nNOnnegative MAnifold Disentangling, or NOMAD. To build an intuitive\nunderstanding of its manifold learning capabilities, we develop a theoretical\nanalysis of NOMAD on idealized datasets. While NOMAD is convex and the globally\noptimal solution can be found by generic SDP solvers with polynomial time\ncomplexity, they are too slow for modern datasets. To address this problem, we\nanalyze a non-convex heuristic and present a new, convex and yet efficient,\nalgorithm, based on the conditional gradient method. Our results render NOMAD a\nversatile, understandable, and powerful tool for manifold learning.\n", "versions": [{"version": "v1", "created": "Mon, 19 Jun 2017 15:57:12 GMT"}, {"version": "v2", "created": "Tue, 27 Jun 2017 17:36:03 GMT"}, {"version": "v3", "created": "Wed, 7 Mar 2018 18:12:21 GMT"}, {"version": "v4", "created": "Wed, 5 Sep 2018 19:25:46 GMT"}], "update_date": "2018-09-07", "authors_parsed": [["Tepper", "Mariano", ""], ["Sengupta", "Anirvan M.", ""], ["Chklovskii", "Dmitri", ""]]}, {"id": "1706.06054", "submitter": "Alyson K. Fletcher", "authors": "Alyson K. Fletcher, Mojtaba Sahraee-Ardakan, Philip Schniter, and\n  Sundeep Rangan", "title": "Rigorous Dynamics and Consistent Estimation in Arbitrarily Conditioned\n  Linear Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.LG math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The problem of estimating a random vector x from noisy linear measurements y\n= A x + w with unknown parameters on the distributions of x and w, which must\nalso be learned, arises in a wide range of statistical learning and linear\ninverse problems. We show that a computationally simple iterative\nmessage-passing algorithm can provably obtain asymptotically consistent\nestimates in a certain high-dimensional large-system limit (LSL) under very\ngeneral parameterizations. Previous message passing techniques have required\ni.i.d. sub-Gaussian A matrices and often fail when the matrix is\nill-conditioned. The proposed algorithm, called adaptive vector approximate\nmessage passing (Adaptive VAMP) with auto-tuning, applies to all\nright-rotationally random A. Importantly, this class includes matrices with\narbitrarily poor conditioning. We show that the parameter estimates and mean\nsquared error (MSE) of x in each iteration converge to deterministic limits\nthat can be precisely predicted by a simple set of state evolution (SE)\nequations. In addition, a simple testable condition is provided in which the\nMSE matches the Bayes-optimal value predicted by the replica method. The paper\nthus provides a computationally simple method with provable guarantees of\noptimality and consistency over a large class of linear inverse problems.\n", "versions": [{"version": "v1", "created": "Mon, 19 Jun 2017 16:57:11 GMT"}], "update_date": "2017-06-20", "authors_parsed": [["Fletcher", "Alyson K.", ""], ["Sahraee-Ardakan", "Mojtaba", ""], ["Schniter", "Philip", ""], ["Rangan", "Sundeep", ""]]}, {"id": "1706.06060", "submitter": "Scott Lundberg", "authors": "Scott M. Lundberg and Su-In Lee", "title": "Consistent feature attribution for tree ensembles", "comments": "presented at 2017 ICML Workshop on Human Interpretability in Machine\n  Learning (WHI 2017), Sydney, NSW, Australia", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Note that a newer expanded version of this paper is now available at:\narXiv:1802.03888\n  It is critical in many applications to understand what features are important\nfor a model, and why individual predictions were made. For tree ensemble\nmethods these questions are usually answered by attributing importance values\nto input features, either globally or for a single prediction. Here we show\nthat current feature attribution methods are inconsistent, which means changing\nthe model to rely more on a given feature can actually decrease the importance\nassigned to that feature. To address this problem we develop fast exact\nsolutions for SHAP (SHapley Additive exPlanation) values, which were recently\nshown to be the unique additive feature attribution method based on conditional\nexpectations that is both consistent and locally accurate. We integrate these\nimprovements into the latest version of XGBoost, demonstrate the\ninconsistencies of current methods, and show how using SHAP values results in\nsignificantly improved supervised clustering performance. Feature importance\nvalues are a key part of understanding widely used models such as gradient\nboosting trees and random forests, so improvements to them have broad practical\nimplications.\n", "versions": [{"version": "v1", "created": "Mon, 19 Jun 2017 17:03:46 GMT"}, {"version": "v2", "created": "Mon, 31 Jul 2017 19:28:56 GMT"}, {"version": "v3", "created": "Thu, 3 Aug 2017 21:33:01 GMT"}, {"version": "v4", "created": "Tue, 8 Aug 2017 01:18:43 GMT"}, {"version": "v5", "created": "Sun, 11 Feb 2018 21:44:49 GMT"}, {"version": "v6", "created": "Sat, 17 Feb 2018 01:11:50 GMT"}], "update_date": "2018-02-20", "authors_parsed": [["Lundberg", "Scott M.", ""], ["Lee", "Su-In", ""]]}, {"id": "1706.06066", "submitter": "Tuo Zhao", "authors": "Xingguo Li, Lin F. Yang, Jason Ge, Jarvis Haupt, Tong Zhang, Tuo Zhao", "title": "On Quadratic Convergence of DC Proximal Newton Algorithm for Nonconvex\n  Sparse Learning in High Dimensions", "comments": "36 pages, 5 figures, 1 table, Accepted at NIPS 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a DC proximal Newton algorithm for solving nonconvex regularized\nsparse learning problems in high dimensions. Our proposed algorithm integrates\nthe proximal Newton algorithm with multi-stage convex relaxation based on the\ndifference of convex (DC) programming, and enjoys both strong computational and\nstatistical guarantees. Specifically, by leveraging a sophisticated\ncharacterization of sparse modeling structures/assumptions (i.e., local\nrestricted strong convexity and Hessian smoothness), we prove that within each\nstage of convex relaxation, our proposed algorithm achieves (local) quadratic\nconvergence, and eventually obtains a sparse approximate local optimum with\noptimal statistical properties after only a few convex relaxations. Numerical\nexperiments are provided to support our theory.\n", "versions": [{"version": "v1", "created": "Mon, 19 Jun 2017 17:15:47 GMT"}, {"version": "v2", "created": "Tue, 12 Dec 2017 19:54:38 GMT"}, {"version": "v3", "created": "Thu, 15 Feb 2018 16:30:14 GMT"}], "update_date": "2018-02-16", "authors_parsed": [["Li", "Xingguo", ""], ["Yang", "Lin F.", ""], ["Ge", "Jason", ""], ["Haupt", "Jarvis", ""], ["Zhang", "Tong", ""], ["Zhao", "Tuo", ""]]}, {"id": "1706.06083", "submitter": "Dimitris Tsipras", "authors": "Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris\n  Tsipras, Adrian Vladu", "title": "Towards Deep Learning Models Resistant to Adversarial Attacks", "comments": "ICLR'18", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent work has demonstrated that deep neural networks are vulnerable to\nadversarial examples---inputs that are almost indistinguishable from natural\ndata and yet classified incorrectly by the network. In fact, some of the latest\nfindings suggest that the existence of adversarial attacks may be an inherent\nweakness of deep learning models. To address this problem, we study the\nadversarial robustness of neural networks through the lens of robust\noptimization. This approach provides us with a broad and unifying view on much\nof the prior work on this topic. Its principled nature also enables us to\nidentify methods for both training and attacking neural networks that are\nreliable and, in a certain sense, universal. In particular, they specify a\nconcrete security guarantee that would protect against any adversary. These\nmethods let us train networks with significantly improved resistance to a wide\nrange of adversarial attacks. They also suggest the notion of security against\na first-order adversary as a natural and broad security guarantee. We believe\nthat robustness against such well-defined classes of adversaries is an\nimportant stepping stone towards fully resistant deep learning models. Code and\npre-trained models are available at https://github.com/MadryLab/mnist_challenge\nand https://github.com/MadryLab/cifar10_challenge.\n", "versions": [{"version": "v1", "created": "Mon, 19 Jun 2017 17:53:11 GMT"}, {"version": "v2", "created": "Wed, 9 Aug 2017 17:34:00 GMT"}, {"version": "v3", "created": "Thu, 9 Nov 2017 01:16:40 GMT"}, {"version": "v4", "created": "Wed, 4 Sep 2019 18:53:10 GMT"}], "update_date": "2019-09-06", "authors_parsed": [["Madry", "Aleksander", ""], ["Makelov", "Aleksandar", ""], ["Schmidt", "Ludwig", ""], ["Tsipras", "Dimitris", ""], ["Vladu", "Adrian", ""]]}, {"id": "1706.06120", "submitter": "Junming Yin", "authors": "Xuan Wei, Daniel Dajun Zeng, Junming Yin", "title": "Multi-Label Annotation Aggregation in Crowdsourcing", "comments": "The paper needs more refinement", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As a means of human-based computation, crowdsourcing has been widely used to\nannotate large-scale unlabeled datasets. One of the obvious challenges is how\nto aggregate these possibly noisy labels provided by a set of heterogeneous\nannotators. Another challenge stems from the difficulty in evaluating the\nannotator reliability without even knowing the ground truth, which can be used\nto build incentive mechanisms in crowdsourcing platforms. When each instance is\nassociated with many possible labels simultaneously, the problem becomes even\nharder because of its combinatorial nature. In this paper, we present new\nflexible Bayesian models and efficient inference algorithms for multi-label\nannotation aggregation by taking both annotator reliability and label\ndependency into account. Extensive experiments on real-world datasets confirm\nthat the proposed methods outperform other competitive alternatives, and the\nmodel can recover the type of the annotators with high accuracy.\n", "versions": [{"version": "v1", "created": "Mon, 19 Jun 2017 18:03:15 GMT"}, {"version": "v2", "created": "Sun, 18 Oct 2020 03:14:53 GMT"}], "update_date": "2020-10-20", "authors_parsed": [["Wei", "Xuan", ""], ["Zeng", "Daniel Dajun", ""], ["Yin", "Junming", ""]]}, {"id": "1706.06122", "submitter": "Yedid Hoshen", "authors": "Yedid Hoshen", "title": "VAIN: Attentional Multi-agent Predictive Modeling", "comments": "NIPS 2017 Wrong sign fixed in Eqs:3-5", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multi-agent predictive modeling is an essential step for understanding\nphysical, social and team-play systems. Recently, Interaction Networks (INs)\nwere proposed for the task of modeling multi-agent physical systems, INs scale\nwith the number of interactions in the system (typically quadratic or higher\norder in the number of agents). In this paper we introduce VAIN, a novel\nattentional architecture for multi-agent predictive modeling that scales\nlinearly with the number of agents. We show that VAIN is effective for\nmulti-agent predictive modeling. Our method is evaluated on tasks from\nchallenging multi-agent prediction domains: chess and soccer, and outperforms\ncompeting multi-agent approaches.\n", "versions": [{"version": "v1", "created": "Mon, 19 Jun 2017 18:09:25 GMT"}, {"version": "v2", "created": "Fri, 28 Sep 2018 12:15:44 GMT"}], "update_date": "2018-10-01", "authors_parsed": [["Hoshen", "Yedid", ""]]}, {"id": "1706.06136", "submitter": "Alexander Gates", "authors": "Alexander J. Gates, Ian B. Wood, William P. Hetrick and Yong-Yeol Ahn", "title": "Element-centric clustering comparison unifies overlaps and hierarchy", "comments": null, "journal-ref": "Scientific Reports 9, 8574 (2019)", "doi": "10.1038/s41598-019-44892-y", "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Clustering is one of the most universal approaches for understanding complex\ndata. A pivotal aspect of clustering analysis is quantitatively comparing\nclusterings; clustering comparison is the basis for many tasks such as\nclustering evaluation, consensus clustering, and tracking the temporal\nevolution of clusters. In particular, the extrinsic evaluation of clustering\nmethods requires comparing the uncovered clusterings to planted clusterings or\nknown metadata. Yet, as we demonstrate, existing clustering comparison measures\nhave critical biases which undermine their usefulness, and no measure\naccommodates both overlapping and hierarchical clusterings. Here we unify the\ncomparison of disjoint, overlapping, and hierarchically structured clusterings\nby proposing a new element-centric framework: elements are compared based on\nthe relationships induced by the cluster structure, as opposed to the\ntraditional cluster-centric philosophy. We demonstrate that, in contrast to\nstandard clustering similarity measures, our framework does not suffer from\ncritical biases and naturally provides unique insights into how the clusterings\ndiffer. We illustrate the strengths of our framework by revealing new insights\ninto the organization of clusters in two applications: the improved\nclassification of schizophrenia based on the overlapping and hierarchical\ncommunity structure of fMRI brain networks, and the disentanglement of various\nsocial homophily factors in Facebook social networks. The universality of\nclustering suggests far-reaching impact of our framework throughout all areas\nof science.\n", "versions": [{"version": "v1", "created": "Mon, 19 Jun 2017 18:51:43 GMT"}, {"version": "v2", "created": "Wed, 12 Jun 2019 15:08:09 GMT"}], "update_date": "2019-06-13", "authors_parsed": [["Gates", "Alexander J.", ""], ["Wood", "Ian B.", ""], ["Hetrick", "William P.", ""], ["Ahn", "Yong-Yeol", ""]]}, {"id": "1706.06195", "submitter": "Ivo Gon\\c{c}alves", "authors": "Ivo Gon\\c{c}alves, Sara Silva, Carlos M. Fonseca, Mauro Castelli", "title": "Unsure When to Stop? Ask Your Semantic Neighbors", "comments": null, "journal-ref": null, "doi": "10.1145/3071178.3071328", "report-no": null, "categories": "cs.NE cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In iterative supervised learning algorithms it is common to reach a point in\nthe search where no further induction seems to be possible with the available\ndata. If the search is continued beyond this point, the risk of overfitting\nincreases significantly. Following the recent developments in inductive\nsemantic stochastic methods, this paper studies the feasibility of using\ninformation gathered from the semantic neighborhood to decide when to stop the\nsearch. Two semantic stopping criteria are proposed and experimentally assessed\nin Geometric Semantic Genetic Programming (GSGP) and in the Semantic Learning\nMachine (SLM) algorithm (the equivalent algorithm for neural networks). The\nexperiments are performed on real-world high-dimensional regression datasets.\nThe results show that the proposed semantic stopping criteria are able to\ndetect stopping points that result in a competitive generalization for both\nGSGP and SLM. This approach also yields computationally efficient algorithms as\nit allows the evolution of neural networks in less than 3 seconds on average,\nand of GP trees in at most 10 seconds. The usage of the proposed semantic\nstopping criteria in conjunction with the computation of optimal\nmutation/learning steps also results in small trees and neural networks.\n", "versions": [{"version": "v1", "created": "Mon, 19 Jun 2017 22:29:08 GMT"}], "update_date": "2017-06-21", "authors_parsed": [["Gon\u00e7alves", "Ivo", ""], ["Silva", "Sara", ""], ["Fonseca", "Carlos M.", ""], ["Castelli", "Mauro", ""]]}, {"id": "1706.06197", "submitter": "Xu Sun", "authors": "Xu Sun, Xuancheng Ren, Shuming Ma, Houfeng Wang", "title": "meProp: Sparsified Back Propagation for Accelerated Deep Learning with\n  Reduced Overfitting", "comments": "Accepted by the 34th International Conference on Machine Learning\n  (ICML 2017)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CL cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a simple yet effective technique for neural network learning. The\nforward propagation is computed as usual. In back propagation, only a small\nsubset of the full gradient is computed to update the model parameters. The\ngradient vectors are sparsified in such a way that only the top-$k$ elements\n(in terms of magnitude) are kept. As a result, only $k$ rows or columns\n(depending on the layout) of the weight matrix are modified, leading to a\nlinear reduction ($k$ divided by the vector dimension) in the computational\ncost. Surprisingly, experimental results demonstrate that we can update only\n1-4% of the weights at each back propagation pass. This does not result in a\nlarger number of training iterations. More interestingly, the accuracy of the\nresulting models is actually improved rather than degraded, and a detailed\nanalysis is given. The code is available at https://github.com/lancopku/meProp\n", "versions": [{"version": "v1", "created": "Mon, 19 Jun 2017 22:36:33 GMT"}, {"version": "v2", "created": "Wed, 5 Jul 2017 01:34:50 GMT"}, {"version": "v3", "created": "Mon, 30 Oct 2017 09:48:41 GMT"}, {"version": "v4", "created": "Tue, 31 Oct 2017 02:04:52 GMT"}, {"version": "v5", "created": "Mon, 11 Mar 2019 02:57:03 GMT"}], "update_date": "2019-03-12", "authors_parsed": [["Sun", "Xu", ""], ["Ren", "Xuancheng", ""], ["Ma", "Shuming", ""], ["Wang", "Houfeng", ""]]}, {"id": "1706.06216", "submitter": "Yujia Li", "authors": "Yujia Li, Alexander Schwing, Kuan-Chieh Wang, Richard Zemel", "title": "Dualing GANs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generative adversarial nets (GANs) are a promising technique for modeling a\ndistribution from samples. It is however well known that GAN training suffers\nfrom instability due to the nature of its maximin formulation. In this paper,\nwe explore ways to tackle the instability problem by dualizing the\ndiscriminator. We start from linear discriminators in which case conjugate\nduality provides a mechanism to reformulate the saddle point objective into a\nmaximization problem, such that both the generator and the discriminator of\nthis 'dualing GAN' act in concert. We then demonstrate how to extend this\nintuition to non-linear formulations. For GANs with linear discriminators our\napproach is able to remove the instability in training, while for GANs with\nnonlinear discriminators our approach provides an alternative to the commonly\nused GAN training algorithm.\n", "versions": [{"version": "v1", "created": "Mon, 19 Jun 2017 23:28:49 GMT"}], "update_date": "2017-06-21", "authors_parsed": [["Li", "Yujia", ""], ["Schwing", "Alexander", ""], ["Wang", "Kuan-Chieh", ""], ["Zemel", "Richard", ""]]}, {"id": "1706.06274", "submitter": "Raghu Meka", "authors": "Adam Klivans, Raghu Meka", "title": "Learning Graphical Models Using Multiplicative Weights", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We give a simple, multiplicative-weight update algorithm for learning\nundirected graphical models or Markov random fields (MRFs). The approach is\nnew, and for the well-studied case of Ising models or Boltzmann machines, we\nobtain an algorithm that uses a nearly optimal number of samples and has\nquadratic running time (up to logarithmic factors), subsuming and improving on\nall prior work. Additionally, we give the first efficient algorithm for\nlearning Ising models over general alphabets.\n  Our main application is an algorithm for learning the structure of t-wise\nMRFs with nearly-optimal sample complexity (up to polynomial losses in\nnecessary terms that depend on the weights) and running time that is\n$n^{O(t)}$. In addition, given $n^{O(t)}$ samples, we can also learn the\nparameters of the model and generate a hypothesis that is close in statistical\ndistance to the true MRF. All prior work runs in time $n^{\\Omega(d)}$ for\ngraphs of bounded degree d and does not generate a hypothesis close in\nstatistical distance even for t=3. We observe that our runtime has the correct\ndependence on n and t assuming the hardness of learning sparse parities with\nnoise.\n  Our algorithm--the Sparsitron-- is easy to implement (has only one parameter)\nand holds in the on-line setting. Its analysis applies a regret bound from\nFreund and Schapire's classic Hedge algorithm. It also gives the first solution\nto the problem of learning sparse Generalized Linear Models (GLMs).\n", "versions": [{"version": "v1", "created": "Tue, 20 Jun 2017 05:41:31 GMT"}], "update_date": "2017-06-21", "authors_parsed": [["Klivans", "Adam", ""], ["Meka", "Raghu", ""]]}, {"id": "1706.06279", "submitter": "Jintao Ke", "authors": "Jintao Ke, Hongyu Zheng, Hai Yang, Xiqun (Michael) Chen", "title": "Short-Term Forecasting of Passenger Demand under On-Demand Ride\n  Services: A Spatio-Temporal Deep Learning Approach", "comments": "39 pages, 10 figures", "journal-ref": "Transportation Research Part C: Emerging Technologies, Volume 85,\n  Pages 591-608, 2017", "doi": "10.1016/j.trc.2017.10.016", "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Short-term passenger demand forecasting is of great importance to the\non-demand ride service platform, which can incentivize vacant cars moving from\nover-supply regions to over-demand regions. The spatial dependences, temporal\ndependences, and exogenous dependences need to be considered simultaneously,\nhowever, which makes short-term passenger demand forecasting challenging. We\npropose a novel deep learning (DL) approach, named the fusion convolutional\nlong short-term memory network (FCL-Net), to address these three dependences\nwithin one end-to-end learning architecture. The model is stacked and fused by\nmultiple convolutional long short-term memory (LSTM) layers, standard LSTM\nlayers, and convolutional layers. The fusion of convolutional techniques and\nthe LSTM network enables the proposed DL approach to better capture the\nspatio-temporal characteristics and correlations of explanatory variables. A\ntailored spatially aggregated random forest is employed to rank the importance\nof the explanatory variables. The ranking is then used for feature selection.\nThe proposed DL approach is applied to the short-term forecasting of passenger\ndemand under an on-demand ride service platform in Hangzhou, China.\nExperimental results, validated on real-world data provided by DiDi Chuxing,\nshow that the FCL-Net achieves better predictive performance than traditional\napproaches including both classical time-series prediction models and neural\nnetwork based algorithms (e.g., artificial neural network and LSTM). This paper\nis one of the first DL studies to forecast the short-term passenger demand of\nan on-demand ride service platform by examining the spatio-temporal\ncorrelations.\n", "versions": [{"version": "v1", "created": "Tue, 20 Jun 2017 06:07:20 GMT"}], "update_date": "2018-02-13", "authors_parsed": [["Ke", "Jintao", "", "Michael"], ["Zheng", "Hongyu", "", "Michael"], ["Yang", "Hai", "", "Michael"], ["Xiqun", "", "", "Michael"], ["Chen", "", ""]]}, {"id": "1706.06341", "submitter": "Yao Wang", "authors": "Kaidong Wang, Yao Wang, Qian Zhao, Deyu Meng and Zongben Xu", "title": "SPLBoost: An Improved Robust Boosting Algorithm Based on Self-paced\n  Learning", "comments": "13 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is known that Boosting can be interpreted as a gradient descent technique\nto minimize an underlying loss function. Specifically, the underlying loss\nbeing minimized by the traditional AdaBoost is the exponential loss, which is\nproved to be very sensitive to random noise/outliers. Therefore, several\nBoosting algorithms, e.g., LogitBoost and SavageBoost, have been proposed to\nimprove the robustness of AdaBoost by replacing the exponential loss with some\ndesigned robust loss functions. In this work, we present a new way to robustify\nAdaBoost, i.e., incorporating the robust learning idea of Self-paced Learning\n(SPL) into Boosting framework. Specifically, we design a new robust Boosting\nalgorithm based on SPL regime, i.e., SPLBoost, which can be easily implemented\nby slightly modifying off-the-shelf Boosting packages. Extensive experiments\nand a theoretical characterization are also carried out to illustrate the\nmerits of the proposed SPLBoost.\n", "versions": [{"version": "v1", "created": "Tue, 20 Jun 2017 09:31:30 GMT"}, {"version": "v2", "created": "Fri, 23 Jun 2017 14:04:46 GMT"}], "update_date": "2017-06-26", "authors_parsed": [["Wang", "Kaidong", ""], ["Wang", "Yao", ""], ["Zhao", "Qian", ""], ["Meng", "Deyu", ""], ["Xu", "Zongben", ""]]}, {"id": "1706.06348", "submitter": "Han Zhao", "authors": "Han Zhao, Geoff Gordon", "title": "Frank-Wolfe Optimization for Symmetric-NMF under Simplicial Constraint", "comments": "In Proceedings of the Thirty-Fourth Conference on Uncertainty in\n  Artificial Intelligence, 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Symmetric nonnegative matrix factorization has found abundant applications in\nvarious domains by providing a symmetric low-rank decomposition of nonnegative\nmatrices. In this paper we propose a Frank-Wolfe (FW) solver to optimize the\nsymmetric nonnegative matrix factorization problem under a simplicial\nconstraint, which has recently been proposed for probabilistic clustering.\nCompared with existing solutions, this algorithm is simple to implement, and\nhas no hyperparameters to be tuned. Building on the recent advances of FW\nalgorithms in nonconvex optimization, we prove an $O(1/\\varepsilon^2)$\nconvergence rate to $\\varepsilon$-approximate KKT points, via a tight bound\n$\\Theta(n^2)$ on the curvature constant, which matches the best known result in\nunconstrained nonconvex setting using gradient methods. Numerical results\ndemonstrate the effectiveness of our algorithm. As a side contribution, we\nconstruct a simple nonsmooth convex problem where the FW algorithm fails to\nconverge to the optimum. This result raises an interesting question about\nnecessary conditions of the success of the FW algorithm on convex problems.\n", "versions": [{"version": "v1", "created": "Tue, 20 Jun 2017 10:03:29 GMT"}, {"version": "v2", "created": "Thu, 22 Jun 2017 06:05:16 GMT"}, {"version": "v3", "created": "Tue, 26 Jun 2018 04:12:29 GMT"}], "update_date": "2018-06-27", "authors_parsed": [["Zhao", "Han", ""], ["Gordon", "Geoff", ""]]}, {"id": "1706.06428", "submitter": "Chung-Cheng Chiu", "authors": "Chung-Cheng Chiu, Dieterich Lawson, Yuping Luo, George Tucker, Kevin\n  Swersky, Ilya Sutskever, Navdeep Jaitly", "title": "An online sequence-to-sequence model for noisy speech recognition", "comments": "arXiv admin note: substantial text overlap with arXiv:1608.01281", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generative models have long been the dominant approach for speech\nrecognition. The success of these models however relies on the use of\nsophisticated recipes and complicated machinery that is not easily accessible\nto non-practitioners. Recent innovations in Deep Learning have given rise to an\nalternative - discriminative models called Sequence-to-Sequence models, that\ncan almost match the accuracy of state of the art generative models. While\nthese models are easy to train as they can be trained end-to-end in a single\nstep, they have a practical limitation that they can only be used for offline\nrecognition. This is because the models require that the entirety of the input\nsequence be available at the beginning of inference, an assumption that is not\nvalid for instantaneous speech recognition. To address this problem, online\nsequence-to-sequence models were recently introduced. These models are able to\nstart producing outputs as data arrives, and the model feels confident enough\nto output partial transcripts. These models, like sequence-to-sequence are\ncausal - the output produced by the model until any time, $t$, affects the\nfeatures that are computed subsequently. This makes the model inherently more\npowerful than generative models that are unable to change features that are\ncomputed from the data. This paper highlights two main contributions - an\nimprovement to online sequence-to-sequence model training, and its application\nto noisy settings with mixed speech from two speakers.\n", "versions": [{"version": "v1", "created": "Fri, 16 Jun 2017 20:58:43 GMT"}], "update_date": "2017-06-21", "authors_parsed": [["Chiu", "Chung-Cheng", ""], ["Lawson", "Dieterich", ""], ["Luo", "Yuping", ""], ["Tucker", "George", ""], ["Swersky", "Kevin", ""], ["Sutskever", "Ilya", ""], ["Jaitly", "Navdeep", ""]]}, {"id": "1706.06474", "submitter": "Mark Herbster", "authors": "Stephen Pasteris, Fabio Vitale, Claudio Gentile, Mark Herbster", "title": "On Pairwise Clustering with Side Information", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Pairwise clustering, in general, partitions a set of items via a known\nsimilarity function. In our treatment, clustering is modeled as a transductive\nprediction problem. Thus rather than beginning with a known similarity\nfunction, the function instead is hidden and the learner only receives a random\nsample consisting of a subset of the pairwise similarities. An additional set\nof pairwise side-information may be given to the learner, which then determines\nthe inductive bias of our algorithms. We measure performance not based on the\nrecovery of the hidden similarity function, but instead on how well we classify\neach item. We give tight bounds on the number of misclassifications. We provide\ntwo algorithms. The first algorithm SACA is a simple agglomerative clustering\nalgorithm which runs in near linear time, and which serves as a baseline for\nour analyses. Whereas the second algorithm, RGCA, enables the incorporation of\nside-information which may lead to improved bounds at the cost of a longer\nrunning time.\n", "versions": [{"version": "v1", "created": "Mon, 19 Jun 2017 01:27:16 GMT"}], "update_date": "2017-06-21", "authors_parsed": [["Pasteris", "Stephen", ""], ["Vitale", "Fabio", ""], ["Gentile", "Claudio", ""], ["Herbster", "Mark", ""]]}, {"id": "1706.06516", "submitter": "Justin Eldridge", "authors": "Justin Eldridge, Mikhail Belkin, Yusu Wang", "title": "Unperturbed: spectral analysis beyond Davis-Kahan", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Classical matrix perturbation results, such as Weyl's theorem for eigenvalues\nand the Davis-Kahan theorem for eigenvectors, are general purpose. These\nclassical bounds are tight in the worst case, but in many settings sub-optimal\nin the typical case. In this paper, we present perturbation bounds which\nconsider the nature of the perturbation and its interaction with the\nunperturbed structure in order to obtain significant improvements over the\nclassical theory in many scenarios, such as when the perturbation is random. We\ndemonstrate the utility of these new results by analyzing perturbations in the\nstochastic blockmodel where we derive much tighter bounds than provided by the\nclassical theory. We use our new perturbation theory to show that a very simple\nand natural clustering algorithm -- whose analysis was difficult using the\nclassical tools -- nevertheless recovers the communities of the blockmodel\nexactly even in very sparse graphs.\n", "versions": [{"version": "v1", "created": "Tue, 20 Jun 2017 15:26:45 GMT"}], "update_date": "2017-06-21", "authors_parsed": [["Eldridge", "Justin", ""], ["Belkin", "Mikhail", ""], ["Wang", "Yusu", ""]]}, {"id": "1706.06525", "submitter": "Hamid Eghbal-Zadeh", "authors": "Hamid Eghbal-zadeh, Bernhard Lehner, Matthias Dorfer, Gerhard Widmer", "title": "A Hybrid Approach with Multi-channel I-Vectors and Convolutional Neural\n  Networks for Acoustic Scene Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In Acoustic Scene Classification (ASC) two major approaches have been\nfollowed . While one utilizes engineered features such as\nmel-frequency-cepstral-coefficients (MFCCs), the other uses learned features\nthat are the outcome of an optimization algorithm. I-vectors are the result of\na modeling technique that usually takes engineered features as input. It has\nbeen shown that standard MFCCs extracted from monaural audio signals lead to\ni-vectors that exhibit poor performance, especially on indoor acoustic scenes.\nAt the same time, Convolutional Neural Networks (CNNs) are well known for their\nability to learn features by optimizing their filters. They have been applied\non ASC and have shown promising results. In this paper, we first propose a\nnovel multi-channel i-vector extraction and scoring scheme for ASC, improving\ntheir performance on indoor and outdoor scenes. Second, we propose a CNN\narchitecture that achieves promising ASC results. Further, we show that\ni-vectors and CNNs capture complementary information from acoustic scenes.\nFinally, we propose a hybrid system for ASC using multi-channel i-vectors and\nCNNs by utilizing a score fusion technique. Using our method, we participated\nin the ASC task of the DCASE-2016 challenge. Our hybrid approach achieved 1 st\nrank among 49 submissions, substantially improving the previous state of the\nart.\n", "versions": [{"version": "v1", "created": "Tue, 20 Jun 2017 15:44:09 GMT"}], "update_date": "2017-11-15", "authors_parsed": [["Eghbal-zadeh", "Hamid", ""], ["Lehner", "Bernhard", ""], ["Dorfer", "Matthias", ""], ["Widmer", "Gerhard", ""]]}, {"id": "1706.06529", "submitter": "Justin Domke", "authors": "Justin Domke", "title": "A Divergence Bound for Hybrids of MCMC and Variational Inference and an\n  Application to Langevin Dynamics and SGVI", "comments": "International Conference on Machine Learning (ICML) 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Two popular classes of methods for approximate inference are Markov chain\nMonte Carlo (MCMC) and variational inference. MCMC tends to be accurate if run\nfor a long enough time, while variational inference tends to give better\napproximations at shorter time horizons. However, the amount of time needed for\nMCMC to exceed the performance of variational methods can be quite high,\nmotivating more fine-grained tradeoffs. This paper derives a distribution over\nvariational parameters, designed to minimize a bound on the divergence between\nthe resulting marginal distribution and the target, and gives an example of how\nto sample from this distribution in a way that interpolates between the\nbehavior of existing methods based on Langevin dynamics and stochastic gradient\nvariational inference (SGVI).\n", "versions": [{"version": "v1", "created": "Tue, 20 Jun 2017 16:06:50 GMT"}], "update_date": "2017-06-21", "authors_parsed": [["Domke", "Justin", ""]]}, {"id": "1706.06544", "submitter": "Taylor Killian", "authors": "Taylor Killian, Samuel Daulton, George Konidaris, Finale Doshi-Velez", "title": "Robust and Efficient Transfer Learning with Hidden-Parameter Markov\n  Decision Processes", "comments": "To appear at NIPS 2017, selected for an oral presentation. 17 pages\n  (incl references and appendix). Example code can be found at\n  http://github.com/dtak/hip-mdp-public", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a new formulation of the Hidden Parameter Markov Decision\nProcess (HiP-MDP), a framework for modeling families of related tasks using\nlow-dimensional latent embeddings. Our new framework correctly models the joint\nuncertainty in the latent parameters and the state space. We also replace the\noriginal Gaussian Process-based model with a Bayesian Neural Network, enabling\nmore scalable inference. Thus, we expand the scope of the HiP-MDP to\napplications with higher dimensions and more complex dynamics.\n", "versions": [{"version": "v1", "created": "Tue, 20 Jun 2017 16:51:46 GMT"}, {"version": "v2", "created": "Tue, 24 Oct 2017 01:35:08 GMT"}, {"version": "v3", "created": "Tue, 31 Oct 2017 02:50:56 GMT"}], "update_date": "2017-11-01", "authors_parsed": [["Killian", "Taylor", ""], ["Daulton", "Samuel", ""], ["Konidaris", "George", ""], ["Doshi-Velez", "Finale", ""]]}, {"id": "1706.06549", "submitter": "Sundeep Rangan", "authors": "Alyson K. Fletcher and Sundeep Rangan", "title": "Inference in Deep Networks in High Dimensions", "comments": "27 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.IT math.IT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep generative networks provide a powerful tool for modeling complex data in\na wide range of applications. In inverse problems that use these networks as\ngenerative priors on data, one must often perform inference of the inputs of\nthe networks from the outputs. Inference is also required for sampling during\nstochastic training on these generative models. This paper considers inference\nin a deep stochastic neural network where the parameters (e.g., weights, biases\nand activation functions) are known and the problem is to estimate the values\nof the input and hidden units from the output. While several approximate\nalgorithms have been proposed for this task, there are few analytic tools that\ncan provide rigorous guarantees in the reconstruction error. This work presents\na novel and computationally tractable output-to-input inference method called\nMulti-Layer Vector Approximate Message Passing (ML-VAMP). The proposed\nalgorithm, derived from expectation propagation, extends earlier AMP methods\nthat are known to achieve the replica predictions for optimality in simple\nlinear inverse problems. Our main contribution shows that the mean-squared\nerror (MSE) of ML-VAMP can be exactly predicted in a certain large system limit\n(LSL) where the numbers of layers is fixed and weight matrices are random and\northogonally-invariant with dimensions that grow to infinity. ML-VAMP is thus a\nprincipled method for output-to-input inference in deep networks with a\nrigorous and precise performance achievability result in high dimensions.\n", "versions": [{"version": "v1", "created": "Tue, 20 Jun 2017 17:04:33 GMT"}], "update_date": "2017-06-21", "authors_parsed": [["Fletcher", "Alyson K.", ""], ["Rangan", "Sundeep", ""]]}, {"id": "1706.06551", "submitter": "Karl Moritz Hermann", "authors": "Karl Moritz Hermann, Felix Hill, Simon Green, Fumin Wang, Ryan\n  Faulkner, Hubert Soyer, David Szepesvari, Wojciech Marian Czarnecki, Max\n  Jaderberg, Denis Teplyashin, Marcus Wainwright, Chris Apps, Demis Hassabis,\n  Phil Blunsom", "title": "Grounded Language Learning in a Simulated 3D World", "comments": "16 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We are increasingly surrounded by artificially intelligent technology that\ntakes decisions and executes actions on our behalf. This creates a pressing\nneed for general means to communicate with, instruct and guide artificial\nagents, with human language the most compelling means for such communication.\nTo achieve this in a scalable fashion, agents must be able to relate language\nto the world and to actions; that is, their understanding of language must be\ngrounded and embodied. However, learning grounded language is a notoriously\nchallenging problem in artificial intelligence research. Here we present an\nagent that learns to interpret language in a simulated 3D environment where it\nis rewarded for the successful execution of written instructions. Trained via a\ncombination of reinforcement and unsupervised learning, and beginning with\nminimal prior knowledge, the agent learns to relate linguistic symbols to\nemergent perceptual representations of its physical surroundings and to\npertinent sequences of actions. The agent's comprehension of language extends\nbeyond its prior experience, enabling it to apply familiar language to\nunfamiliar situations and to interpret entirely novel instructions. Moreover,\nthe speed with which this agent learns new words increases as its semantic\nknowledge grows. This facility for generalising and bootstrapping semantic\nknowledge indicates the potential of the present approach for reconciling\nambiguous natural language with the complexity of the physical world.\n", "versions": [{"version": "v1", "created": "Tue, 20 Jun 2017 17:09:29 GMT"}, {"version": "v2", "created": "Mon, 26 Jun 2017 09:47:36 GMT"}], "update_date": "2017-06-27", "authors_parsed": [["Hermann", "Karl Moritz", ""], ["Hill", "Felix", ""], ["Green", "Simon", ""], ["Wang", "Fumin", ""], ["Faulkner", "Ryan", ""], ["Soyer", "Hubert", ""], ["Szepesvari", "David", ""], ["Czarnecki", "Wojciech Marian", ""], ["Jaderberg", "Max", ""], ["Teplyashin", "Denis", ""], ["Wainwright", "Marcus", ""], ["Apps", "Chris", ""], ["Hassabis", "Demis", ""], ["Blunsom", "Phil", ""]]}, {"id": "1706.06569", "submitter": "Tomer Koren", "authors": "Vineet Gupta, Tomer Koren, Yoram Singer", "title": "A Unified Approach to Adaptive Regularization in Online and Stochastic\n  Optimization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe a framework for deriving and analyzing online optimization\nalgorithms that incorporate adaptive, data-dependent regularization, also\ntermed preconditioning. Such algorithms have been proven useful in stochastic\noptimization by reshaping the gradients according to the geometry of the data.\nOur framework captures and unifies much of the existing literature on adaptive\nonline methods, including the AdaGrad and Online Newton Step algorithms as well\nas their diagonal versions. As a result, we obtain new convergence proofs for\nthese algorithms that are substantially simpler than previous analyses. Our\nframework also exposes the rationale for the different preconditioned updates\nused in common stochastic optimization methods.\n", "versions": [{"version": "v1", "created": "Tue, 20 Jun 2017 17:51:00 GMT"}], "update_date": "2017-06-21", "authors_parsed": [["Gupta", "Vineet", ""], ["Koren", "Tomer", ""], ["Singer", "Yoram", ""]]}, {"id": "1706.06617", "submitter": "Olivier Pietquin", "authors": "Diana Borsa, Bilal Piot, R\\'emi Munos and Olivier Pietquin", "title": "Observational Learning by Reinforcement Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Observational learning is a type of learning that occurs as a function of\nobserving, retaining and possibly replicating or imitating the behaviour of\nanother agent. It is a core mechanism appearing in various instances of social\nlearning and has been found to be employed in several intelligent species,\nincluding humans. In this paper, we investigate to what extent the explicit\nmodelling of other agents is necessary to achieve observational learning\nthrough machine learning. Especially, we argue that observational learning can\nemerge from pure Reinforcement Learning (RL), potentially coupled with memory.\nThrough simple scenarios, we demonstrate that an RL agent can leverage the\ninformation provided by the observations of an other agent performing a task in\na shared environment. The other agent is only observed through the effect of\nits actions on the environment and never explicitly modeled. Two key aspects\nare borrowed from observational learning: i) the observer behaviour needs to\nchange as a result of viewing a 'teacher' (another agent) and ii) the observer\nneeds to be motivated somehow to engage in making use of the other agent's\nbehaviour. The later is naturally modeled by RL, by correlating the learning\nagent's reward with the teacher agent's behaviour.\n", "versions": [{"version": "v1", "created": "Tue, 20 Jun 2017 18:44:49 GMT"}], "update_date": "2017-06-22", "authors_parsed": [["Borsa", "Diana", ""], ["Piot", "Bilal", ""], ["Munos", "R\u00e9mi", ""], ["Pietquin", "Olivier", ""]]}, {"id": "1706.06619", "submitter": "Abraham Heifets", "authors": "Izhar Wallach and Abraham Heifets", "title": "Most Ligand-Based Classification Benchmarks Reward Memorization Rather\n  than Generalization", "comments": null, "journal-ref": null, "doi": "10.1021/acs.jcim.7b00403", "report-no": null, "categories": "q-bio.QM cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Undetected overfitting can occur when there are significant redundancies\nbetween training and validation data. We describe AVE, a new measure of\ntraining-validation redundancy for ligand-based classification problems that\naccounts for the similarity amongst inactive molecules as well as active. We\ninvestigated seven widely-used benchmarks for virtual screening and\nclassification, and show that the amount of AVE bias strongly correlates with\nthe performance of ligand-based predictive methods irrespective of the\npredicted property, chemical fingerprint, similarity measure, or\npreviously-applied unbiasing techniques. Therefore, it may be that the\npreviously-reported performance of most ligand-based methods can be explained\nby overfitting to benchmarks rather than good prospective accuracy.\n", "versions": [{"version": "v1", "created": "Tue, 20 Jun 2017 18:47:46 GMT"}, {"version": "v2", "created": "Wed, 9 May 2018 20:59:37 GMT"}], "update_date": "2018-05-11", "authors_parsed": [["Wallach", "Izhar", ""], ["Heifets", "Abraham", ""]]}, {"id": "1706.06620", "submitter": "Seyoung Kim", "authors": "Seyoung Kim, Tayfun Gokmen, Hyung-Min Lee and Wilfried E. Haensch", "title": "Analog CMOS-based Resistive Processing Unit for Deep Neural Network\n  Training", "comments": null, "journal-ref": null, "doi": "10.1109/MWSCAS.2017.8052950", "report-no": null, "categories": "cs.ET cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently we have shown that an architecture based on resistive processing\nunit (RPU) devices has potential to achieve significant acceleration in deep\nneural network (DNN) training compared to today's software-based DNN\nimplementations running on CPU/GPU. However, currently available device\ncandidates based on non-volatile memory technologies do not satisfy all the\nrequirements to realize the RPU concept. Here, we propose an analog CMOS-based\nRPU design (CMOS RPU) which can store and process data locally and can be\noperated in a massively parallel manner. We analyze various properties of the\nCMOS RPU to evaluate the functionality and feasibility for acceleration of DNN\ntraining.\n", "versions": [{"version": "v1", "created": "Tue, 20 Jun 2017 18:53:29 GMT"}], "update_date": "2017-10-27", "authors_parsed": [["Kim", "Seyoung", ""], ["Gokmen", "Tayfun", ""], ["Lee", "Hyung-Min", ""], ["Haensch", "Wilfried E.", ""]]}, {"id": "1706.06643", "submitter": "Philip Thomas", "authors": "Philip S. Thomas and Emma Brunskill", "title": "Policy Gradient Methods for Reinforcement Learning with Function\n  Approximation and Action-Dependent Baselines", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We show how an action-dependent baseline can be used by the policy gradient\ntheorem using function approximation, originally presented with\naction-independent baselines by (Sutton et al. 2000).\n", "versions": [{"version": "v1", "created": "Tue, 20 Jun 2017 19:47:44 GMT"}], "update_date": "2017-06-22", "authors_parsed": [["Thomas", "Philip S.", ""], ["Brunskill", "Emma", ""]]}, {"id": "1706.06660", "submitter": "Venkatesh Saligrama", "authors": "Yao Ma, Alex Olshevsky, Venkatesh Saligrama, Csaba Szepesvari", "title": "Crowdsourcing with Sparsely Interacting Workers", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider estimation of worker skills from worker-task interaction data\n(with unknown labels) for the single-coin crowd-sourcing binary classification\nmodel in symmetric noise. We define the (worker) interaction graph whose nodes\nare workers and an edge between two nodes indicates whether or not the two\nworkers participated in a common task. We show that skills are asymptotically\nidentifiable if and only if an appropriate limiting version of the interaction\ngraph is irreducible and has odd-cycles. We then formulate a weighted rank-one\noptimization problem to estimate skills based on observations on an\nirreducible, aperiodic interaction graph. We propose a gradient descent scheme\nand show that for such interaction graphs estimates converge asymptotically to\nthe global minimum. We characterize noise robustness of the gradient scheme in\nterms of spectral properties of signless Laplacians of the interaction graph.\nWe then demonstrate that a plug-in estimator based on the estimated skills\nachieves state-of-art performance on a number of real-world datasets. Our\nresults have implications for rank-one matrix completion problem in that\ngradient descent can provably recover $W \\times W$ rank-one matrices based on\n$W+1$ off-diagonal observations of a connected graph with a single odd-cycle.\n", "versions": [{"version": "v1", "created": "Tue, 20 Jun 2017 20:41:25 GMT"}], "update_date": "2017-06-22", "authors_parsed": [["Ma", "Yao", ""], ["Olshevsky", "Alex", ""], ["Saligrama", "Venkatesh", ""], ["Szepesvari", "Csaba", ""]]}, {"id": "1706.06664", "submitter": "Anshumali Shrivastava", "authors": "Chen Luo, Anshumali Shrivastava", "title": "Arrays of (locality-sensitive) Count Estimators (ACE): High-Speed\n  Anomaly Detection via Cache Lookups", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.LG stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Anomaly detection is one of the frequent and important subroutines deployed\nin large-scale data processing systems. Even being a well-studied topic,\nexisting techniques for unsupervised anomaly detection require storing\nsignificant amounts of data, which is prohibitive from memory and latency\nperspective. In the big-data world existing methods fail to address the new set\nof memory and latency constraints. In this paper, we propose ACE (Arrays of\n(locality-sensitive) Count Estimators) algorithm that can be 60x faster than\nthe ELKI package~\\cite{DBLP:conf/ssd/AchtertBKSZ09}, which has the fastest\nimplementation of the unsupervised anomaly detection algorithms. ACE algorithm\nrequires less than $4MB$ memory, to dynamically compress the full data\ninformation into a set of count arrays. These tiny $4MB$ arrays of counts are\nsufficient for unsupervised anomaly detection. At the core of the ACE\nalgorithm, there is a novel statistical estimator which is derived from the\nsampling view of Locality Sensitive Hashing(LSH). This view is significantly\ndifferent and efficient than the widely popular view of LSH for near-neighbor\nsearch. We show the superiority of ACE algorithm over 11 popular baselines on 3\nbenchmark datasets, including the KDD-Cup99 data which is the largest available\nbenchmark comprising of more than half a million entries with ground truth\nanomaly labels.\n", "versions": [{"version": "v1", "created": "Tue, 20 Jun 2017 21:09:22 GMT"}], "update_date": "2017-06-22", "authors_parsed": [["Luo", "Chen", ""], ["Shrivastava", "Anshumali", ""]]}, {"id": "1706.06681", "submitter": "Michihiro Yasunaga", "authors": "Michihiro Yasunaga, Rui Zhang, Kshitijh Meelu, Ayush Pareek, Krishnan\n  Srinivasan and Dragomir Radev", "title": "Graph-based Neural Multi-Document Summarization", "comments": "In CoNLL 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a neural multi-document summarization (MDS) system that\nincorporates sentence relation graphs. We employ a Graph Convolutional Network\n(GCN) on the relation graphs, with sentence embeddings obtained from Recurrent\nNeural Networks as input node features. Through multiple layer-wise\npropagation, the GCN generates high-level hidden sentence features for salience\nestimation. We then use a greedy heuristic to extract salient sentences while\navoiding redundancy. In our experiments on DUC 2004, we consider three types of\nsentence relation graphs and demonstrate the advantage of combining sentence\nrelations in graphs with the representation power of deep neural networks. Our\nmodel improves upon traditional graph-based extractive approaches and the\nvanilla GRU sequence model with no graph, and it achieves competitive results\nagainst other state-of-the-art multi-document summarization systems.\n", "versions": [{"version": "v1", "created": "Tue, 20 Jun 2017 22:12:14 GMT"}, {"version": "v2", "created": "Wed, 19 Jul 2017 01:11:00 GMT"}, {"version": "v3", "created": "Wed, 23 Aug 2017 08:46:52 GMT"}], "update_date": "2017-08-24", "authors_parsed": [["Yasunaga", "Michihiro", ""], ["Zhang", "Rui", ""], ["Meelu", "Kshitijh", ""], ["Pareek", "Ayush", ""], ["Srinivasan", "Krishnan", ""], ["Radev", "Dragomir", ""]]}, {"id": "1706.06689", "submitter": "Garrett Goh", "authors": "Garrett B. Goh, Charles Siegel, Abhinav Vishnu, Nathan O. Hodas,\n  Nathan Baker", "title": "Chemception: A Deep Neural Network with Minimal Chemistry Knowledge\n  Matches the Performance of Expert-developed QSAR/QSPR Models", "comments": "Submitted to a chemistry peer-reviewed journal", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.CE cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the last few years, we have seen the transformative impact of deep\nlearning in many applications, particularly in speech recognition and computer\nvision. Inspired by Google's Inception-ResNet deep convolutional neural network\n(CNN) for image classification, we have developed \"Chemception\", a deep CNN for\nthe prediction of chemical properties, using just the images of 2D drawings of\nmolecules. We develop Chemception without providing any additional explicit\nchemistry knowledge, such as basic concepts like periodicity, or advanced\nfeatures like molecular descriptors and fingerprints. We then show how\nChemception can serve as a general-purpose neural network architecture for\npredicting toxicity, activity, and solvation properties when trained on a\nmodest database of 600 to 40,000 compounds. When compared to multi-layer\nperceptron (MLP) deep neural networks trained with ECFP fingerprints,\nChemception slightly outperforms in activity and solvation prediction and\nslightly underperforms in toxicity prediction. Having matched the performance\nof expert-developed QSAR/QSPR deep learning models, our work demonstrates the\nplausibility of using deep neural networks to assist in computational chemistry\nresearch, where the feature engineering process is performed primarily by a\ndeep learning algorithm.\n", "versions": [{"version": "v1", "created": "Tue, 20 Jun 2017 22:25:57 GMT"}], "update_date": "2018-08-15", "authors_parsed": [["Goh", "Garrett B.", ""], ["Siegel", "Charles", ""], ["Vishnu", "Abhinav", ""], ["Hodas", "Nathan O.", ""], ["Baker", "Nathan", ""]]}, {"id": "1706.06714", "submitter": "Van-Khanh Tran", "authors": "Van-Khanh Tran and Le-Minh Nguyen", "title": "Neural-based Natural Language Generation in Dialogue using RNN\n  Encoder-Decoder with Semantic Aggregation", "comments": "To be appear at SIGDIAL 2017. arXiv admin note: text overlap with\n  arXiv:1706.00134, arXiv:1706.00139", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Natural language generation (NLG) is an important component in spoken\ndialogue systems. This paper presents a model called Encoder-Aggregator-Decoder\nwhich is an extension of an Recurrent Neural Network based Encoder-Decoder\narchitecture. The proposed Semantic Aggregator consists of two components: an\nAligner and a Refiner. The Aligner is a conventional attention calculated over\nthe encoded input information, while the Refiner is another attention or gating\nmechanism stacked over the attentive Aligner in order to further select and\naggregate the semantic elements. The proposed model can be jointly trained both\nsentence planning and surface realization to produce natural language\nutterances. The model was extensively assessed on four different NLG domains,\nin which the experimental results showed that the proposed generator\nconsistently outperforms the previous methods on all the NLG domains.\n", "versions": [{"version": "v1", "created": "Wed, 21 Jun 2017 01:07:02 GMT"}, {"version": "v2", "created": "Sun, 25 Jun 2017 09:31:34 GMT"}, {"version": "v3", "created": "Tue, 11 Jul 2017 14:47:13 GMT"}], "update_date": "2017-07-12", "authors_parsed": [["Tran", "Van-Khanh", ""], ["Nguyen", "Le-Minh", ""]]}, {"id": "1706.06783", "submitter": "Sina Sajadmanesh", "authors": "Sina Sajadmanesh, Jiawei Zhang, Hamid R. Rabiee", "title": "NPGLM: A Non-Parametric Method for Temporal Link Prediction", "comments": "7 pages, 5 figures, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we try to solve the problem of temporal link prediction in\ninformation networks. This implies predicting the time it takes for a link to\nappear in the future, given its features that have been extracted at the\ncurrent network snapshot. To this end, we introduce a probabilistic\nnon-parametric approach, called \"Non-Parametric Generalized Linear Model\"\n(NP-GLM), which infers the hidden underlying probability distribution of the\nlink advent time given its features. We then present a learning algorithm for\nNP-GLM and an inference method to answer time-related queries. Extensive\nexperiments conducted on both synthetic data and real-world Sina Weibo social\nnetwork demonstrate the effectiveness of NP-GLM in solving temporal link\nprediction problem vis-a-vis competitive baselines.\n", "versions": [{"version": "v1", "created": "Wed, 21 Jun 2017 08:16:47 GMT"}], "update_date": "2017-06-22", "authors_parsed": [["Sajadmanesh", "Sina", ""], ["Zhang", "Jiawei", ""], ["Rabiee", "Hamid R.", ""]]}, {"id": "1706.06810", "submitter": "Jongpil Lee", "authors": "Jongpil Lee, Juhan Nam", "title": "Multi-Level and Multi-Scale Feature Aggregation Using Sample-level Deep\n  Convolutional Neural Networks for Music Classification", "comments": "ICML Music Discovery Workshop 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.LG cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Music tag words that describe music audio by text have different levels of\nabstraction. Taking this issue into account, we propose a music classification\napproach that aggregates multi-level and multi-scale features using pre-trained\nfeature extractors. In particular, the feature extractors are trained in\nsample-level deep convolutional neural networks using raw waveforms. We show\nthat this approach achieves state-of-the-art results on several music\nclassification datasets.\n", "versions": [{"version": "v1", "created": "Wed, 21 Jun 2017 09:57:24 GMT"}], "update_date": "2017-06-22", "authors_parsed": [["Lee", "Jongpil", ""], ["Nam", "Juhan", ""]]}, {"id": "1706.06838", "submitter": "Federico Cabitza", "authors": "Federico Cabitza, Davide Ciucci, Raffaele Rasoini", "title": "A giant with feet of clay: on the validity of the data that feed machine\n  learning in medicine", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper considers the use of Machine Learning (ML) in medicine by focusing\non the main problem that this computational approach has been aimed at solving\nor at least minimizing: uncertainty. To this aim, we point out how uncertainty\nis so ingrained in medicine that it biases also the representation of clinical\nphenomena, that is the very input of ML models, thus undermining the clinical\nsignificance of their output. Recognizing this can motivate both medical\ndoctors, in taking more responsibility in the development and use of these\ndecision aids, and the researchers, in pursuing different ways to assess the\nvalue of these systems. In so doing, both designers and users could take this\nintrinsic characteristic of medicine more seriously and consider alternative\napproaches that do not \"sweep uncertainty under the rug\" within an objectivist\nfiction, which everyone can come up by believing as true.\n", "versions": [{"version": "v1", "created": "Wed, 21 Jun 2017 11:45:44 GMT"}, {"version": "v2", "created": "Mon, 26 Jun 2017 16:12:37 GMT"}, {"version": "v3", "created": "Mon, 14 May 2018 14:43:01 GMT"}], "update_date": "2018-05-15", "authors_parsed": [["Cabitza", "Federico", ""], ["Ciucci", "Davide", ""], ["Rasoini", "Raffaele", ""]]}, {"id": "1706.06859", "submitter": "Kazuyuki Hara", "authors": "Kazuyuki Hara, Daisuke Saitoh, and Hayaru Shouno", "title": "Analysis of dropout learning regarded as ensemble learning", "comments": "9 pages, 8 figures, submitted to Conference", "journal-ref": "A. E. P. VIlla et al. (Eds.): ICANN 2016 ( Part II, LNCS 9887, pp.\n  1-8, 2016)", "doi": "10.1007/978-3-319-44781-0_9", "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning is the state-of-the-art in fields such as visual object\nrecognition and speech recognition. This learning uses a large number of\nlayers, huge number of units, and connections. Therefore, overfitting is a\nserious problem. To avoid this problem, dropout learning is proposed. Dropout\nlearning neglects some inputs and hidden units in the learning process with a\nprobability, p, and then, the neglected inputs and hidden units are combined\nwith the learned network to express the final output. We find that the process\nof combining the neglected hidden units with the learned network can be\nregarded as ensemble learning, so we analyze dropout learning from this point\nof view.\n", "versions": [{"version": "v1", "created": "Tue, 20 Jun 2017 04:19:57 GMT"}], "update_date": "2017-06-22", "authors_parsed": [["Hara", "Kazuyuki", ""], ["Saitoh", "Daisuke", ""], ["Shouno", "Hayaru", ""]]}, {"id": "1706.06873", "submitter": "Minsik Cho Dr.", "authors": "Minsik Cho, Daniel Brand", "title": "MEC: Memory-efficient Convolution for Deep Neural Network", "comments": "ICML2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolution is a critical component in modern deep neural networks, thus\nseveral algorithms for convolution have been developed. Direct convolution is\nsimple but suffers from poor performance. As an alternative, multiple indirect\nmethods have been proposed including im2col-based convolution, FFT-based\nconvolution, or Winograd-based algorithm. However, all these indirect methods\nhave high memory-overhead, which creates performance degradation and offers a\npoor trade-off between performance and memory consumption. In this work, we\npropose a memory-efficient convolution or MEC with compact lowering, which\nreduces memory-overhead substantially and accelerates convolution process. MEC\nlowers the input matrix in a simple yet efficient/compact way (i.e., much less\nmemory-overhead), and then executes multiple small matrix multiplications in\nparallel to get convolution completed. Additionally, the reduced memory\nfootprint improves memory sub-system efficiency, improving performance. Our\nexperimental results show that MEC reduces memory consumption significantly\nwith good speedup on both mobile and server platforms, compared with other\nindirect convolution algorithms.\n", "versions": [{"version": "v1", "created": "Wed, 21 Jun 2017 13:00:39 GMT"}], "update_date": "2017-06-22", "authors_parsed": [["Cho", "Minsik", ""], ["Brand", "Daniel", ""]]}, {"id": "1706.06934", "submitter": "Nader Bshouty", "authors": "Nader H. Bshouty and Areej Costa", "title": "Exact Learning of Juntas from Membership Queries", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we study adaptive and non-adaptive exact learning of Juntas\nfrom membership queries. We use new techniques to find new bounds, narrow some\nof the gaps between the lower bounds and upper bounds and find new\ndeterministic and randomized algorithms with small query and time complexities.\n  Some of the bounds are tight in the sense that finding better ones either\ngives a breakthrough result in some long-standing combinatorial open problem or\nneeds a new technique that is beyond the existing ones.\n", "versions": [{"version": "v1", "created": "Wed, 21 Jun 2017 14:40:21 GMT"}], "update_date": "2017-06-22", "authors_parsed": [["Bshouty", "Nader H.", ""], ["Costa", "Areej", ""]]}, {"id": "1706.06941", "submitter": "Daniele Zambon", "authors": "Daniele Zambon, Cesare Alippi, Lorenzo Livi", "title": "Concept Drift and Anomaly Detection in Graph Streams", "comments": null, "journal-ref": "IEEE Transactions on Neural Networks and Learning Systems (Volume:\n  29, Issue: 11, Nov. 2018)", "doi": "10.1109/TNNLS.2018.2804443", "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Graph representations offer powerful and intuitive ways to describe data in a\nmultitude of application domains. Here, we consider stochastic processes\ngenerating graphs and propose a methodology for detecting changes in\nstationarity of such processes. The methodology is general and considers a\nprocess generating attributed graphs with a variable number of vertices/edges,\nwithout the need to assume one-to-one correspondence between vertices at\ndifferent time steps. The methodology acts by embedding every graph of the\nstream into a vector domain, where a conventional multivariate change detection\nprocedure can be easily applied. We ground the soundness of our proposal by\nproving several theoretical results. In addition, we provide a specific\nimplementation of the methodology and evaluate its effectiveness on several\ndetection problems involving attributed graphs representing biological\nmolecules and drawings. Experimental results are contrasted with respect to\nsuitable baseline methods, demonstrating the effectiveness of our approach.\n", "versions": [{"version": "v1", "created": "Wed, 21 Jun 2017 14:52:01 GMT"}, {"version": "v2", "created": "Tue, 7 Nov 2017 11:11:12 GMT"}, {"version": "v3", "created": "Fri, 9 Feb 2018 17:09:17 GMT"}], "update_date": "2021-02-11", "authors_parsed": [["Zambon", "Daniele", ""], ["Alippi", "Cesare", ""], ["Livi", "Lorenzo", ""]]}, {"id": "1706.06953", "submitter": "Kazuyuki Hara", "authors": "Kazuyuki Hara, Kentaro Katahira, and Masato Okada", "title": "Statistical Mechanics of Node-perturbation Learning with Noisy Baseline", "comments": "16 pages, 7 figures, submitted to JPSJ", "journal-ref": "Journal of the Physical Society of Japan 86, 024002 (2017)", "doi": "10.7566/JPSJ.86.024002", "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Node-perturbation learning is a type of statistical gradient descent\nalgorithm that can be applied to problems where the objective function is not\nexplicitly formulated, including reinforcement learning. It estimates the\ngradient of an objective function by using the change in the object function in\nresponse to the perturbation. The value of the objective function for an\nunperturbed output is called a baseline. Cho et al. proposed node-perturbation\nlearning with a noisy baseline. In this paper, we report on building the\nstatistical mechanics of Cho's model and on deriving coupled differential\nequations of order parameters that depict learning dynamics. We also show how\nto derive the generalization error by solving the differential equations of\norder parameters. On the basis of the results, we show that Cho's results are\nalso apply in general cases and show some general performances of Cho's model.\n", "versions": [{"version": "v1", "created": "Tue, 20 Jun 2017 04:46:56 GMT"}], "update_date": "2017-06-22", "authors_parsed": [["Hara", "Kazuyuki", ""], ["Katahira", "Kentaro", ""], ["Okada", "Masato", ""]]}, {"id": "1706.06974", "submitter": "Jon Kleinberg", "authors": "Jon Kleinberg, Annie Liang, Sendhil Mullainathan", "title": "The Theory is Predictive, but is it Complete? An Application to Human\n  Perception of Randomness", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.GT cs.SI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When we test a theory using data, it is common to focus on correctness: do\nthe predictions of the theory match what we see in the data? But we also care\nabout completeness: how much of the predictable variation in the data is\ncaptured by the theory? This question is difficult to answer, because in\ngeneral we do not know how much \"predictable variation\" there is in the\nproblem. In this paper, we consider approaches motivated by machine learning\nalgorithms as a means of constructing a benchmark for the best attainable level\nof prediction.\n  We illustrate our methods on the task of predicting human-generated random\nsequences. Relative to an atheoretical machine learning algorithm benchmark, we\nfind that existing behavioral models explain roughly 15 percent of the\npredictable variation in this problem. This fraction is robust across several\nvariations on the problem. We also consider a version of this approach for\nanalyzing field data from domains in which human perception and generation of\nrandomness has been used as a conceptual framework; these include sequential\ndecision-making and repeated zero-sum games. In these domains, our framework\nfor testing the completeness of theories provides a way of assessing their\neffectiveness over different contexts; we find that despite some differences,\nthe existing theories are fairly stable across our field domains in their\nperformance relative to the benchmark. Overall, our results indicate that (i)\nthere is a significant amount of structure in this problem that existing models\nhave yet to capture and (ii) there are rich domains in which machine learning\nmay provide a viable approach to testing completeness.\n", "versions": [{"version": "v1", "created": "Wed, 21 Jun 2017 15:56:06 GMT"}], "update_date": "2017-06-22", "authors_parsed": [["Kleinberg", "Jon", ""], ["Liang", "Annie", ""], ["Mullainathan", "Sendhil", ""]]}, {"id": "1706.06978", "submitter": "Xiaoqiang Zhu", "authors": "Guorui Zhou, Chengru Song, Xiaoqiang Zhu, Ying Fan, Han Zhu, Xiao Ma,\n  Yanghui Yan, Junqi Jin, Han Li, Kun Gai", "title": "Deep Interest Network for Click-Through Rate Prediction", "comments": "Accepted by KDD 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Click-through rate prediction is an essential task in industrial\napplications, such as online advertising. Recently deep learning based models\nhave been proposed, which follow a similar Embedding\\&MLP paradigm. In these\nmethods large scale sparse input features are first mapped into low dimensional\nembedding vectors, and then transformed into fixed-length vectors in a\ngroup-wise manner, finally concatenated together to fed into a multilayer\nperceptron (MLP) to learn the nonlinear relations among features. In this way,\nuser features are compressed into a fixed-length representation vector, in\nregardless of what candidate ads are. The use of fixed-length vector will be a\nbottleneck, which brings difficulty for Embedding\\&MLP methods to capture\nuser's diverse interests effectively from rich historical behaviors. In this\npaper, we propose a novel model: Deep Interest Network (DIN) which tackles this\nchallenge by designing a local activation unit to adaptively learn the\nrepresentation of user interests from historical behaviors with respect to a\ncertain ad. This representation vector varies over different ads, improving the\nexpressive ability of model greatly. Besides, we develop two techniques:\nmini-batch aware regularization and data adaptive activation function which can\nhelp training industrial deep networks with hundreds of millions of parameters.\nExperiments on two public datasets as well as an Alibaba real production\ndataset with over 2 billion samples demonstrate the effectiveness of proposed\napproaches, which achieve superior performance compared with state-of-the-art\nmethods. DIN now has been successfully deployed in the online display\nadvertising system in Alibaba, serving the main traffic.\n", "versions": [{"version": "v1", "created": "Wed, 21 Jun 2017 16:05:17 GMT"}, {"version": "v2", "created": "Fri, 23 Jun 2017 08:12:12 GMT"}, {"version": "v3", "created": "Mon, 7 May 2018 13:06:07 GMT"}, {"version": "v4", "created": "Thu, 13 Sep 2018 04:37:06 GMT"}], "update_date": "2018-09-14", "authors_parsed": [["Zhou", "Guorui", ""], ["Song", "Chengru", ""], ["Zhu", "Xiaoqiang", ""], ["Fan", "Ying", ""], ["Zhu", "Han", ""], ["Ma", "Xiao", ""], ["Yan", "Yanghui", ""], ["Jin", "Junqi", ""], ["Li", "Han", ""], ["Gai", "Kun", ""]]}, {"id": "1706.07001", "submitter": "Jialei Wang", "authors": "Jialei Wang, Tong Zhang", "title": "Improved Optimization of Finite Sums with Minibatch Stochastic Variance\n  Reduced Proximal Iterations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present novel minibatch stochastic optimization methods for empirical risk\nminimization problems, the methods efficiently leverage variance reduced\nfirst-order and sub-sampled higher-order information to accelerate the\nconvergence speed. For quadratic objectives, we prove improved iteration\ncomplexity over state-of-the-art under reasonable assumptions. We also provide\nempirical evidence of the advantages of our method compared to existing\napproaches in the literature.\n", "versions": [{"version": "v1", "created": "Wed, 21 Jun 2017 16:48:45 GMT"}, {"version": "v2", "created": "Wed, 11 Oct 2017 04:42:08 GMT"}], "update_date": "2017-10-12", "authors_parsed": [["Wang", "Jialei", ""], ["Zhang", "Tong", ""]]}, {"id": "1706.07036", "submitter": "Chen-Hsuan Lin", "authors": "Chen-Hsuan Lin, Chen Kong, Simon Lucey", "title": "Learning Efficient Point Cloud Generation for Dense 3D Object\n  Reconstruction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Conventional methods of 3D object generative modeling learn volumetric\npredictions using deep networks with 3D convolutional operations, which are\ndirect analogies to classical 2D ones. However, these methods are\ncomputationally wasteful in attempt to predict 3D shapes, where information is\nrich only on the surfaces. In this paper, we propose a novel 3D generative\nmodeling framework to efficiently generate object shapes in the form of dense\npoint clouds. We use 2D convolutional operations to predict the 3D structure\nfrom multiple viewpoints and jointly apply geometric reasoning with 2D\nprojection optimization. We introduce the pseudo-renderer, a differentiable\nmodule to approximate the true rendering operation, to synthesize novel depth\nmaps for optimization. Experimental results for single-image 3D object\nreconstruction tasks show that we outperforms state-of-the-art methods in terms\nof shape similarity and prediction density.\n", "versions": [{"version": "v1", "created": "Wed, 21 Jun 2017 17:56:59 GMT"}], "update_date": "2017-06-22", "authors_parsed": [["Lin", "Chen-Hsuan", ""], ["Kong", "Chen", ""], ["Lucey", "Simon", ""]]}, {"id": "1706.07094", "submitter": "Ben Letham", "authors": "Benjamin Letham, Brian Karrer, Guilherme Ottoni, Eytan Bakshy", "title": "Constrained Bayesian Optimization with Noisy Experiments", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Randomized experiments are the gold standard for evaluating the effects of\nchanges to real-world systems. Data in these tests may be difficult to collect\nand outcomes may have high variance, resulting in potentially large measurement\nerror. Bayesian optimization is a promising technique for efficiently\noptimizing multiple continuous parameters, but existing approaches degrade in\nperformance when the noise level is high, limiting its applicability to many\nrandomized experiments. We derive an expression for expected improvement under\ngreedy batch optimization with noisy observations and noisy constraints, and\ndevelop a quasi-Monte Carlo approximation that allows it to be efficiently\noptimized. Simulations with synthetic functions show that optimization\nperformance on noisy, constrained problems outperforms existing methods. We\nfurther demonstrate the effectiveness of the method with two real-world\nexperiments conducted at Facebook: optimizing a ranking system, and optimizing\nserver compiler flags.\n", "versions": [{"version": "v1", "created": "Wed, 21 Jun 2017 19:29:14 GMT"}, {"version": "v2", "created": "Tue, 26 Jun 2018 17:06:00 GMT"}], "update_date": "2018-06-27", "authors_parsed": [["Letham", "Benjamin", ""], ["Karrer", "Brian", ""], ["Ottoni", "Guilherme", ""], ["Bakshy", "Eytan", ""]]}, {"id": "1706.07101", "submitter": "Anthony Gamst", "authors": "Anthony Collins Gamst and Alden Walker", "title": "The energy landscape of a simple neural network", "comments": "17 pages, 15 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We explore the energy landscape of a simple neural network. In particular, we\nexpand upon previous work demonstrating that the empirical complexity of fitted\nneural networks is vastly less than a naive parameter count would suggest and\nthat this implicit regularization is actually beneficial for generalization\nfrom fitted models.\n", "versions": [{"version": "v1", "created": "Wed, 21 Jun 2017 19:51:43 GMT"}], "update_date": "2017-06-23", "authors_parsed": [["Gamst", "Anthony Collins", ""], ["Walker", "Alden", ""]]}, {"id": "1706.07103", "submitter": "Michele Piana", "authors": "Federico Benvenuto, Michele Piana, Cristina Campi, Anna Maria Massone", "title": "A hybrid supervised/unsupervised machine learning approach to solar\n  flare prediction", "comments": null, "journal-ref": null, "doi": "10.3847/1538-4357/aaa23c", "report-no": null, "categories": "astro-ph.SR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a hybrid approach to solar flare prediction, whereby a\nsupervised regularization method is used to realize feature importance and an\nunsupervised clustering method is used to realize the binary flare/no-flare\ndecision. The approach is validated against NOAA SWPC data.\n", "versions": [{"version": "v1", "created": "Wed, 21 Jun 2017 19:57:20 GMT"}], "update_date": "2018-02-07", "authors_parsed": [["Benvenuto", "Federico", ""], ["Piana", "Michele", ""], ["Campi", "Cristina", ""], ["Massone", "Anna Maria", ""]]}, {"id": "1706.07119", "submitter": "Antonio H. Ribeiro", "authors": "Ant\\^onio H. Ribeiro and Luis A. Aguirre", "title": "\"Parallel Training Considered Harmful?\": Comparing series-parallel and\n  parallel feedforward network training", "comments": null, "journal-ref": "Neurocomputing 316:222--231, (2018)", "doi": "10.1016/j.neucom.2018.07.071", "report-no": null, "categories": "cs.SY cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural network models for dynamic systems can be trained either in parallel\nor in series-parallel configurations. Influenced by early arguments, several\npapers justify the choice of series-parallel rather than parallel configuration\nclaiming it has a lower computational cost, better stability properties during\ntraining and provides more accurate results. Other published results, on the\nother hand, defend parallel training as being more robust and capable of\nyielding more accu- rate long-term predictions. The main contribution of this\npaper is to present a study comparing both methods under the same unified\nframework. We focus on three aspects: i) robustness of the estimation in the\npresence of noise; ii) computational cost; and, iii) convergence. A unifying\nmathematical framework and simulation studies show situations where each\ntraining method provides better validation results, being parallel training\nbetter in what is believed to be more realistic scenarios. An example using\nmeasured data seems to reinforce such claim. We also show, with a novel\ncomplexity analysis and numerical examples, that both methods have similar\ncomputational cost, being series series-parallel training, however, more\namenable to parallelization. Some informal discussion about stability and\nconvergence properties is presented and explored in the examples.\n", "versions": [{"version": "v1", "created": "Wed, 21 Jun 2017 21:00:44 GMT"}, {"version": "v2", "created": "Tue, 24 Oct 2017 15:48:27 GMT"}, {"version": "v3", "created": "Tue, 14 Aug 2018 13:46:31 GMT"}], "update_date": "2019-05-06", "authors_parsed": [["Ribeiro", "Ant\u00f4nio H.", ""], ["Aguirre", "Luis A.", ""]]}, {"id": "1706.07138", "submitter": "Stephan Zheng", "authors": "Stephan Zheng, Yisong Yue, Patrick Lucey", "title": "Generating Long-term Trajectories Using Deep Hierarchical Networks", "comments": "Published in NIPS 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of modeling spatiotemporal trajectories over long time\nhorizons using expert demonstrations. For instance, in sports, agents often\nchoose action sequences with long-term goals in mind, such as achieving a\ncertain strategic position. Conventional policy learning approaches, such as\nthose based on Markov decision processes, generally fail at learning cohesive\nlong-term behavior in such high-dimensional state spaces, and are only\neffective when myopic modeling lead to the desired behavior. The key difficulty\nis that conventional approaches are \"shallow\" models that only learn a single\nstate-action policy. We instead propose a hierarchical policy class that\nautomatically reasons about both long-term and short-term goals, which we\ninstantiate as a hierarchical neural network. We showcase our approach in a\ncase study on learning to imitate demonstrated basketball trajectories, and\nshow that it generates significantly more realistic trajectories compared to\nnon-hierarchical baselines as judged by professional sports analysts.\n", "versions": [{"version": "v1", "created": "Wed, 21 Jun 2017 23:05:52 GMT"}], "update_date": "2017-06-23", "authors_parsed": [["Zheng", "Stephan", ""], ["Yue", "Yisong", ""], ["Lucey", "Patrick", ""]]}, {"id": "1706.07145", "submitter": "Shuchang Zhou", "authors": "Shuchang Zhou, Yuzhi Wang, He Wen, Qinyao He and Yuheng Zou", "title": "Balanced Quantization: An Effective and Efficient Approach to Quantized\n  Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Quantized Neural Networks (QNNs), which use low bitwidth numbers for\nrepresenting parameters and performing computations, have been proposed to\nreduce the computation complexity, storage size and memory usage. In QNNs,\nparameters and activations are uniformly quantized, such that the\nmultiplications and additions can be accelerated by bitwise operations.\nHowever, distributions of parameters in Neural Networks are often imbalanced,\nsuch that the uniform quantization determined from extremal values may under\nutilize available bitwidth. In this paper, we propose a novel quantization\nmethod that can ensure the balance of distributions of quantized values. Our\nmethod first recursively partitions the parameters by percentiles into balanced\nbins, and then applies uniform quantization. We also introduce computationally\ncheaper approximations of percentiles to reduce the computation overhead\nintroduced. Overall, our method improves the prediction accuracies of QNNs\nwithout introducing extra computation during inference, has negligible impact\non training speed, and is applicable to both Convolutional Neural Networks and\nRecurrent Neural Networks. Experiments on standard datasets including ImageNet\nand Penn Treebank confirm the effectiveness of our method. On ImageNet, the\ntop-5 error rate of our 4-bit quantized GoogLeNet model is 12.7\\%, which is\nsuperior to the state-of-the-arts of QNNs.\n", "versions": [{"version": "v1", "created": "Thu, 22 Jun 2017 01:25:37 GMT"}], "update_date": "2017-06-23", "authors_parsed": [["Zhou", "Shuchang", ""], ["Wang", "Yuzhi", ""], ["Wen", "He", ""], ["He", "Qinyao", ""], ["Zou", "Yuheng", ""]]}, {"id": "1706.07147", "submitter": "Kevin Feigelis", "authors": "Kevin T. Feigelis and Daniel L. K. Yamins", "title": "A Useful Motif for Flexible Task Learning in an Embodied Two-Dimensional\n  Visual Environment", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI q-bio.NC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Animals (especially humans) have an amazing ability to learn new tasks\nquickly, and switch between them flexibly. How brains support this ability is\nlargely unknown, both neuroscientifically and algorithmically. One reasonable\nsupposition is that modules drawing on an underlying general-purpose sensory\nrepresentation are dynamically allocated on a per-task basis. Recent results\nfrom neuroscience and artificial intelligence suggest the role of the general\npurpose visual representation may be played by a deep convolutional neural\nnetwork, and give some clues how task modules based on such a representation\nmight be discovered and constructed. In this work, we investigate module\narchitectures in an embodied two-dimensional touchscreen environment, in which\nan agent's learning must occur via interactions with an environment that emits\nimages and rewards, and accepts touches as input. This environment is designed\nto capture the physical structure of the task environments that are commonly\ndeployed in visual neuroscience and psychophysics. We show that in this\ncontext, very simple changes in the nonlinear activations used by such a module\ncan significantly influence how fast it is at learning visual tasks and how\nsuitable it is for switching to new tasks.\n", "versions": [{"version": "v1", "created": "Thu, 22 Jun 2017 02:07:14 GMT"}], "update_date": "2017-06-23", "authors_parsed": [["Feigelis", "Kevin T.", ""], ["Yamins", "Daniel L. K.", ""]]}, {"id": "1706.07167", "submitter": "Yangyang Li", "authors": "Yangyang Li", "title": "Curvature-aware Manifold Learning", "comments": "24 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Traditional manifold learning algorithms assumed that the embedded manifold\nis globally or locally isometric to Euclidean space. Under this assumption,\nthey divided manifold into a set of overlapping local patches which are locally\nisometric to linear subsets of Euclidean space. By analyzing the global or\nlocal isometry assumptions it can be shown that the learnt manifold is a flat\nmanifold with zero Riemannian curvature tensor. In general, manifolds may not\nsatisfy these hypotheses. One major limitation of traditional manifold learning\nis that it does not consider the curvature information of manifold. In order to\nremove these limitations, we present our curvature-aware manifold learning\nalgorithm called CAML. The purpose of our algorithm is to break the local\nisometry assumption and to reduce the dimension of the general manifold which\nis not isometric to Euclidean space. Thus, our method adds the curvature\ninformation to the process of manifold learning. The experiments have shown\nthat our method CAML is more stable than other manifold learning algorithms by\ncomparing the neighborhood preserving ratios.\n", "versions": [{"version": "v1", "created": "Thu, 22 Jun 2017 04:37:31 GMT"}], "update_date": "2017-06-23", "authors_parsed": [["Li", "Yangyang", ""]]}, {"id": "1706.07179", "submitter": "Trapit Bansal", "authors": "Trapit Bansal, Arvind Neelakantan, Andrew McCallum", "title": "RelNet: End-to-End Modeling of Entities & Relations", "comments": "Accepted in AKBC 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce RelNet: a new model for relational reasoning. RelNet is a memory\naugmented neural network which models entities as abstract memory slots and is\nequipped with an additional relational memory which models relations between\nall memory pairs. The model thus builds an abstract knowledge graph on the\nentities and relations present in a document which can then be used to answer\nquestions about the document. It is trained end-to-end: only supervision to the\nmodel is in the form of correct answers to the questions. We test the model on\nthe 20 bAbI question-answering tasks with 10k examples per task and find that\nit solves all the tasks with a mean error of 0.3%, achieving 0% error on 11 of\nthe 20 tasks.\n", "versions": [{"version": "v1", "created": "Thu, 22 Jun 2017 06:59:07 GMT"}, {"version": "v2", "created": "Thu, 16 Nov 2017 02:39:58 GMT"}], "update_date": "2017-11-17", "authors_parsed": [["Bansal", "Trapit", ""], ["Neelakantan", "Arvind", ""], ["McCallum", "Andrew", ""]]}, {"id": "1706.07180", "submitter": "Remi Gribonval", "authors": "R\\'emi Gribonval (PANAMA, DANTE), Gilles Blanchard (DATASHAPE, LMO),\n  Nicolas Keriven (PANAMA, GIPSA-GAIA), Yann Traonmilin (PANAMA, IMB)", "title": "Compressive Statistical Learning with Random Feature Moments", "comments": "Main novelties between version 1 and version 2: improved\n  concentration bounds, improved sketch sizes for compressive k-means and\n  compressive GMM that now scale linearly with the ambient dimensionMain\n  novelties of version 3: all content on compressive clustering and compressive\n  GMM is now developed in the companion paper hal-02536818; improved\n  statistical guarantees in a generic framework with illustration of the\n  improvements on compressive PCA. Mathematical Statistics and Learning, EMS\n  Publishing House, In press", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.IT cs.LG math.IT math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe a general framework -- compressive statistical learning -- for\nresource-efficient large-scale learning: the training collection is compressed\nin one pass into a low-dimensional sketch (a vector of random empirical\ngeneralized moments) that captures the information relevant to the considered\nlearning task. A near-minimizer of the risk is computed from the sketch through\nthe solution of a nonlinear least squares problem. We investigate sufficient\nsketch sizes to control the generalization error of this procedure. The\nframework is illustrated on compressive PCA, compressive clustering, and\ncompressive Gaussian mixture Modeling with fixed known variance. The latter two\nare further developed in a companion paper.\n", "versions": [{"version": "v1", "created": "Thu, 22 Jun 2017 06:59:19 GMT"}, {"version": "v2", "created": "Thu, 7 Dec 2017 09:38:07 GMT"}, {"version": "v3", "created": "Fri, 17 Apr 2020 15:25:47 GMT"}, {"version": "v4", "created": "Tue, 22 Jun 2021 08:26:13 GMT"}], "update_date": "2021-06-23", "authors_parsed": [["Gribonval", "R\u00e9mi", "", "PANAMA, DANTE"], ["Blanchard", "Gilles", "", "DATASHAPE, LMO"], ["Keriven", "Nicolas", "", "PANAMA, GIPSA-GAIA"], ["Traonmilin", "Yann", "", "PANAMA, IMB"]]}, {"id": "1706.07230", "submitter": "Devendra Singh Chaplot", "authors": "Devendra Singh Chaplot, Kanthashree Mysore Sathyendra, Rama Kumar\n  Pasumarthi, Dheeraj Rajagopal, Ruslan Salakhutdinov", "title": "Gated-Attention Architectures for Task-Oriented Language Grounding", "comments": "To appear in AAAI-18", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CL cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To perform tasks specified by natural language instructions, autonomous\nagents need to extract semantically meaningful representations of language and\nmap it to visual elements and actions in the environment. This problem is\ncalled task-oriented language grounding. We propose an end-to-end trainable\nneural architecture for task-oriented language grounding in 3D environments\nwhich assumes no prior linguistic or perceptual knowledge and requires only raw\npixels from the environment and the natural language instruction as input. The\nproposed model combines the image and text representations using a\nGated-Attention mechanism and learns a policy to execute the natural language\ninstruction using standard reinforcement and imitation learning methods. We\nshow the effectiveness of the proposed model on unseen instructions as well as\nunseen maps, both quantitatively and qualitatively. We also introduce a novel\nenvironment based on a 3D game engine to simulate the challenges of\ntask-oriented language grounding over a rich set of instructions and\nenvironment states.\n", "versions": [{"version": "v1", "created": "Thu, 22 Jun 2017 09:39:17 GMT"}, {"version": "v2", "created": "Tue, 9 Jan 2018 03:24:06 GMT"}], "update_date": "2018-01-10", "authors_parsed": [["Chaplot", "Devendra Singh", ""], ["Sathyendra", "Kanthashree Mysore", ""], ["Pasumarthi", "Rama Kumar", ""], ["Rajagopal", "Dheeraj", ""], ["Salakhutdinov", "Ruslan", ""]]}, {"id": "1706.07276", "submitter": "Bei Shi", "authors": "Bei Shi, Wai Lam, Shoaib Jameel, Steven Schockaert, Kwun Ping Lai", "title": "Jointly Learning Word Embeddings and Latent Topics", "comments": "10 pagess, 2 figures, full paper. To appear in the proceedings of The\n  40th International ACM SIGIR Conference on Research and Development in\n  Information Retrieval (SIGIR '17)", "journal-ref": null, "doi": "10.1145/3077136.3080806", "report-no": null, "categories": "cs.CL cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Word embedding models such as Skip-gram learn a vector-space representation\nfor each word, based on the local word collocation patterns that are observed\nin a text corpus. Latent topic models, on the other hand, take a more global\nview, looking at the word distributions across the corpus to assign a topic to\neach word occurrence. These two paradigms are complementary in how they\nrepresent the meaning of word occurrences. While some previous works have\nalready looked at using word embeddings for improving the quality of latent\ntopics, and conversely, at using latent topics for improving word embeddings,\nsuch \"two-step\" methods cannot capture the mutual interaction between the two\nparadigms. In this paper, we propose STE, a framework which can learn word\nembeddings and latent topics in a unified manner. STE naturally obtains\ntopic-specific word embeddings, and thus addresses the issue of polysemy. At\nthe same time, it also learns the term distributions of the topics, and the\ntopic distributions of the documents. Our experimental results demonstrate that\nthe STE model can indeed generate useful topic-specific word embeddings and\ncoherent latent topics in an effective and efficient way.\n", "versions": [{"version": "v1", "created": "Wed, 21 Jun 2017 06:19:24 GMT"}], "update_date": "2017-06-23", "authors_parsed": [["Shi", "Bei", ""], ["Lam", "Wai", ""], ["Jameel", "Shoaib", ""], ["Schockaert", "Steven", ""], ["Lai", "Kwun Ping", ""]]}, {"id": "1706.07351", "submitter": "Lalit Maganti", "authors": "Alessio Lomuscio, Lalit Maganti", "title": "An approach to reachability analysis for feed-forward ReLU neural\n  networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the reachability problem for systems implemented as feed-forward\nneural networks whose activation function is implemented via ReLU functions. We\ndraw a correspondence between establishing whether some arbitrary output can\never be outputed by a neural system and linear problems characterising a neural\nsystem of interest. We present a methodology to solve cases of practical\ninterest by means of a state-of-the-art linear programs solver. We evaluate the\ntechnique presented by discussing the experimental results obtained by\nanalysing reachability properties for a number of benchmarks in the literature.\n", "versions": [{"version": "v1", "created": "Thu, 22 Jun 2017 14:59:49 GMT"}], "update_date": "2017-06-23", "authors_parsed": [["Lomuscio", "Alessio", ""], ["Maganti", "Lalit", ""]]}, {"id": "1706.07365", "submitter": "Alejandro Newell", "authors": "Alejandro Newell, Jia Deng", "title": "Pixels to Graphs by Associative Embedding", "comments": "Updated numbers. Code and pretrained models available at\n  https://github.com/umich-vl/px2graph", "journal-ref": "Advances in Neural Information Processing Systems 30 (NIPS 2017)", "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Graphs are a useful abstraction of image content. Not only can graphs\nrepresent details about individual objects in a scene but they can capture the\ninteractions between pairs of objects. We present a method for training a\nconvolutional neural network such that it takes in an input image and produces\na full graph definition. This is done end-to-end in a single stage with the use\nof associative embeddings. The network learns to simultaneously identify all of\nthe elements that make up a graph and piece them together. We benchmark on the\nVisual Genome dataset, and demonstrate state-of-the-art performance on the\nchallenging task of scene graph generation.\n", "versions": [{"version": "v1", "created": "Thu, 22 Jun 2017 15:20:25 GMT"}, {"version": "v2", "created": "Tue, 27 Mar 2018 17:13:31 GMT"}], "update_date": "2018-03-28", "authors_parsed": [["Newell", "Alejandro", ""], ["Deng", "Jia", ""]]}, {"id": "1706.07409", "submitter": "Vinay Praneeth Boda", "authors": "Vinay Praneeth Boda and Prakash Narayan", "title": "Universal Sampling Rate Distortion", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.LG math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We examine the coordinated and universal rate-efficient sampling of a subset\nof correlated discrete memoryless sources followed by lossy compression of the\nsampled sources. The goal is to reconstruct a predesignated subset of sources\nwithin a specified level of distortion. The combined sampling mechanism and\nrate distortion code are universal in that they are devised to perform robustly\nwithout exact knowledge of the underlying joint probability distribution of the\nsources. In Bayesian as well as nonBayesian settings, single-letter\ncharacterizations are provided for the universal sampling rate distortion\nfunction for fixed-set sampling, independent random sampling and memoryless\nrandom sampling. It is illustrated how these sampling mechanisms are\nsuccessively better. Our achievability proofs bring forth new schemes for joint\nsource distribution-learning and lossy compression.\n", "versions": [{"version": "v1", "created": "Thu, 22 Jun 2017 17:30:22 GMT"}], "update_date": "2017-06-23", "authors_parsed": [["Boda", "Vinay Praneeth", ""], ["Narayan", "Prakash", ""]]}, {"id": "1706.07446", "submitter": "Daniel George", "authors": "Daniel George, Hongyu Shen, E. A. Huerta", "title": "Deep Transfer Learning: A new deep learning glitch classification method\n  for advanced LIGO", "comments": null, "journal-ref": null, "doi": "10.1103/PhysRevD.97.101501", "report-no": null, "categories": "gr-qc astro-ph.IM cs.CV cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The exquisite sensitivity of the advanced LIGO detectors has enabled the\ndetection of multiple gravitational wave signals. The sophisticated design of\nthese detectors mitigates the effect of most types of noise. However, advanced\nLIGO data streams are contaminated by numerous artifacts known as glitches:\nnon-Gaussian noise transients with complex morphologies. Given their high rate\nof occurrence, glitches can lead to false coincident detections, obscure and\neven mimic gravitational wave signals. Therefore, successfully characterizing\nand removing glitches from advanced LIGO data is of utmost importance. Here, we\npresent the first application of Deep Transfer Learning for glitch\nclassification, showing that knowledge from deep learning algorithms trained\nfor real-world object recognition can be transferred for classifying glitches\nin time-series based on their spectrogram images. Using the Gravity Spy\ndataset, containing hand-labeled, multi-duration spectrograms obtained from\nreal LIGO data, we demonstrate that this method enables optimal use of very\ndeep convolutional neural networks for classification given small training\ndatasets, significantly reduces the time for training the networks, and\nachieves state-of-the-art accuracy above 98.8%, with perfect precision-recall\non 8 out of 22 classes. Furthermore, new types of glitches can be classified\naccurately given few labeled examples with this technique. Once trained via\ntransfer learning, we show that the convolutional neural networks can be\ntruncated and used as excellent feature extractors for unsupervised clustering\nmethods to identify new classes based on their morphology, without any labeled\nexamples. Therefore, this provides a new framework for dynamic glitch\nclassification for gravitational wave detectors, which are expected to\nencounter new types of noise as they undergo gradual improvements to attain\ndesign sensitivity.\n", "versions": [{"version": "v1", "created": "Thu, 22 Jun 2017 18:11:13 GMT"}], "update_date": "2018-07-15", "authors_parsed": [["George", "Daniel", ""], ["Shen", "Hongyu", ""], ["Huerta", "E. A.", ""]]}, {"id": "1706.07450", "submitter": "Soledad Villar", "authors": "Alex Nowak, Soledad Villar, Afonso S. Bandeira, Joan Bruna", "title": "Revised Note on Learning Algorithms for Quadratic Assignment with Graph\n  Neural Networks", "comments": "Revised note to arXiv:1706.07450v1 that appeared in IEEE Data Science\n  Workshop 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Inverse problems correspond to a certain type of optimization problems\nformulated over appropriate input distributions. Recently, there has been a\ngrowing interest in understanding the computational hardness of these\noptimization problems, not only in the worst case, but in an average-complexity\nsense under this same input distribution.\n  In this revised note, we are interested in studying another aspect of\nhardness, related to the ability to learn how to solve a problem by simply\nobserving a collection of previously solved instances. These 'planted\nsolutions' are used to supervise the training of an appropriate predictive\nmodel that parametrizes a broad class of algorithms, with the hope that the\nresulting model will provide good accuracy-complexity tradeoffs in the average\nsense.\n  We illustrate this setup on the Quadratic Assignment Problem, a fundamental\nproblem in Network Science. We observe that data-driven models based on Graph\nNeural Networks offer intriguingly good performance, even in regimes where\nstandard relaxation based techniques appear to suffer.\n", "versions": [{"version": "v1", "created": "Thu, 22 Jun 2017 18:18:58 GMT"}, {"version": "v2", "created": "Thu, 30 Aug 2018 20:27:20 GMT"}], "update_date": "2018-09-03", "authors_parsed": [["Nowak", "Alex", ""], ["Villar", "Soledad", ""], ["Bandeira", "Afonso S.", ""], ["Bruna", "Joan", ""]]}, {"id": "1706.07503", "submitter": "Chaitanya K. Joshi", "authors": "Chaitanya K. Joshi, Fei Mi and Boi Faltings", "title": "Personalization in Goal-Oriented Dialog", "comments": "Accepted at NIPS 2017 Conversational AI Workshop; Code and data at\n  https://github.com/chaitjo/personalized-dialog", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The main goal of modeling human conversation is to create agents which can\ninteract with people in both open-ended and goal-oriented scenarios. End-to-end\ntrained neural dialog systems are an important line of research for such\ngeneralized dialog models as they do not resort to any situation-specific\nhandcrafting of rules. However, incorporating personalization into such systems\nis a largely unexplored topic as there are no existing corpora to facilitate\nsuch work. In this paper, we present a new dataset of goal-oriented dialogs\nwhich are influenced by speaker profiles attached to them. We analyze the\nshortcomings of an existing end-to-end dialog system based on Memory Networks\nand propose modifications to the architecture which enable personalization. We\nalso investigate personalization in dialog as a multi-task learning problem,\nand show that a single model which shares features among various profiles\noutperforms separate models for each profile.\n", "versions": [{"version": "v1", "created": "Thu, 22 Jun 2017 22:09:14 GMT"}, {"version": "v2", "created": "Tue, 31 Oct 2017 11:19:48 GMT"}, {"version": "v3", "created": "Fri, 15 Dec 2017 06:28:24 GMT"}], "update_date": "2019-04-30", "authors_parsed": [["Joshi", "Chaitanya K.", ""], ["Mi", "Fei", ""], ["Faltings", "Boi", ""]]}, {"id": "1706.07510", "submitter": "Arya Mazumdar", "authors": "Arya Mazumdar, Barna Saha", "title": "Clustering with Noisy Queries", "comments": "Prior versions of some of the results have appeared before in\n  arXiv:1604.01839. In this version we rewrote several proofs for clarity, and\n  included many new results", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.DS cs.IT cs.LG math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we initiate a rigorous theoretical study of clustering with\nnoisy queries (or a faulty oracle). Given a set of $n$ elements, our goal is to\nrecover the true clustering by asking minimum number of pairwise queries to an\noracle. Oracle can answer queries of the form : \"do elements $u$ and $v$ belong\nto the same cluster?\" -- the queries can be asked interactively (adaptive\nqueries), or non-adaptively up-front, but its answer can be erroneous with\nprobability $p$. In this paper, we provide the first information theoretic\nlower bound on the number of queries for clustering with noisy oracle in both\nsituations. We design novel algorithms that closely match this query complexity\nlower bound, even when the number of clusters is unknown. Moreover, we design\ncomputationally efficient algorithms both for the adaptive and non-adaptive\nsettings. The problem captures/generalizes multiple application scenarios. It\nis directly motivated by the growing body of work that use crowdsourcing for\n{\\em entity resolution}, a fundamental and challenging data mining task aimed\nto identify all records in a database referring to the same entity. Here crowd\nrepresents the noisy oracle, and the number of queries directly relates to the\ncost of crowdsourcing. Another application comes from the problem of {\\em sign\nedge prediction} in social network, where social interactions can be both\npositive and negative, and one must identify the sign of all pair-wise\ninteractions by querying a few pairs. Furthermore, clustering with noisy oracle\nis intimately connected to correlation clustering, leading to improvement\ntherein. Finally, it introduces a new direction of study in the popular {\\em\nstochastic block model} where one has an incomplete stochastic block model\nmatrix to recover the clusters.\n", "versions": [{"version": "v1", "created": "Thu, 22 Jun 2017 22:22:04 GMT"}], "update_date": "2017-06-26", "authors_parsed": [["Mazumdar", "Arya", ""], ["Saha", "Barna", ""]]}, {"id": "1706.07535", "submitter": "Hemanth Venkateswara", "authors": "Hemanth Venkateswara, Prasanth Lade, Binbin Lin, Jieping Ye,\n  Sethuraman Panchanathan", "title": "Efficient Approximate Solutions to Mutual Information Based Global\n  Feature Selection", "comments": "ICDM 2015 Conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mutual Information (MI) is often used for feature selection when developing\nclassifier models. Estimating the MI for a subset of features is often\nintractable. We demonstrate, that under the assumptions of conditional\nindependence, MI between a subset of features can be expressed as the\nConditional Mutual Information (CMI) between pairs of features. But selecting\nfeatures with the highest CMI turns out to be a hard combinatorial problem. In\nthis work, we have applied two unique global methods, Truncated Power Method\n(TPower) and Low Rank Bilinear Approximation (LowRank), to solve the feature\nselection problem. These algorithms provide very good approximations to the\nNP-hard CMI based feature selection problem. We experimentally demonstrate the\neffectiveness of these procedures across multiple datasets and compare them\nwith existing MI based global and iterative feature selection procedures.\n", "versions": [{"version": "v1", "created": "Fri, 23 Jun 2017 01:08:59 GMT"}], "update_date": "2017-06-26", "authors_parsed": [["Venkateswara", "Hemanth", ""], ["Lade", "Prasanth", ""], ["Lin", "Binbin", ""], ["Ye", "Jieping", ""], ["Panchanathan", "Sethuraman", ""]]}, {"id": "1706.07555", "submitter": "Chengxu Zhuang", "authors": "Chengxu Zhuang, Jonas Kubilius, Mitra Hartmann, Daniel Yamins", "title": "Toward Goal-Driven Neural Network Models for the Rodent\n  Whisker-Trigeminal System", "comments": "17 pages including supplementary information, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In large part, rodents see the world through their whiskers, a powerful\ntactile sense enabled by a series of brain areas that form the\nwhisker-trigeminal system. Raw sensory data arrives in the form of mechanical\ninput to the exquisitely sensitive, actively-controllable whisker array, and is\nprocessed through a sequence of neural circuits, eventually arriving in\ncortical regions that communicate with decision-making and memory areas.\nAlthough a long history of experimental studies has characterized many aspects\nof these processing stages, the computational operations of the\nwhisker-trigeminal system remain largely unknown. In the present work, we take\na goal-driven deep neural network (DNN) approach to modeling these\ncomputations. First, we construct a biophysically-realistic model of the rat\nwhisker array. We then generate a large dataset of whisker sweeps across a wide\nvariety of 3D objects in highly-varying poses, angles, and speeds. Next, we\ntrain DNNs from several distinct architectural families to solve a shape\nrecognition task in this dataset. Each architectural family represents a\nstructurally-distinct hypothesis for processing in the whisker-trigeminal\nsystem, corresponding to different ways in which spatial and temporal\ninformation can be integrated. We find that most networks perform poorly on the\nchallenging shape recognition task, but that specific architectures from\nseveral families can achieve reasonable performance levels. Finally, we show\nthat Representational Dissimilarity Matrices (RDMs), a tool for comparing\npopulation codes between neural systems, can separate these higher-performing\nnetworks with data of a type that could plausibly be collected in a\nneurophysiological or imaging experiment. Our results are a proof-of-concept\nthat goal-driven DNN networks of the whisker-trigeminal system are potentially\nwithin reach.\n", "versions": [{"version": "v1", "created": "Fri, 23 Jun 2017 03:34:03 GMT"}], "update_date": "2017-06-27", "authors_parsed": [["Zhuang", "Chengxu", ""], ["Kubilius", "Jonas", ""], ["Hartmann", "Mitra", ""], ["Yamins", "Daniel", ""]]}, {"id": "1706.07561", "submitter": "Jiaming Song", "authors": "Jiaming Song and Shengjia Zhao and Stefano Ermon", "title": "A-NICE-MC: Adversarial Training for MCMC", "comments": "NIPS 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Existing Markov Chain Monte Carlo (MCMC) methods are either based on\ngeneral-purpose and domain-agnostic schemes which can lead to slow convergence,\nor hand-crafting of problem-specific proposals by an expert. We propose\nA-NICE-MC, a novel method to train flexible parametric Markov chain kernels to\nproduce samples with desired properties. First, we propose an efficient\nlikelihood-free adversarial training method to train a Markov chain and mimic a\ngiven data distribution. Then, we leverage flexible volume preserving flows to\nobtain parametric kernels for MCMC. Using a bootstrap approach, we show how to\ntrain efficient Markov chains to sample from a prescribed posterior\ndistribution by iteratively improving the quality of both the model and the\nsamples. A-NICE-MC provides the first framework to automatically design\nefficient domain-specific MCMC proposals. Empirical results demonstrate that\nA-NICE-MC combines the strong guarantees of MCMC with the expressiveness of\ndeep neural networks, and is able to significantly outperform competing methods\nsuch as Hamiltonian Monte Carlo.\n", "versions": [{"version": "v1", "created": "Fri, 23 Jun 2017 04:19:04 GMT"}, {"version": "v2", "created": "Tue, 14 Nov 2017 18:20:40 GMT"}, {"version": "v3", "created": "Wed, 14 Mar 2018 19:23:42 GMT"}], "update_date": "2018-03-16", "authors_parsed": [["Song", "Jiaming", ""], ["Zhao", "Shengjia", ""], ["Ermon", "Stefano", ""]]}, {"id": "1706.07637", "submitter": "Ding Zhao", "authors": "Wenshuo Wang, Chang Liu, Ding Zhao", "title": "How Much Data is Enough? A Statistical Approach with Case Study on\n  Longitudinal Driving Behavior", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Big data has shown its uniquely powerful ability to reveal, model, and\nunderstand driver behaviors. The amount of data affects the experiment cost and\nconclusions in the analysis. Insufficient data may lead to inaccurate models\nwhile excessive data waste resources. For projects that cost millions of\ndollars, it is critical to determine the right amount of data needed. However,\nhow to decide the appropriate amount has not been fully studied in the realm of\ndriver behaviors. This paper systematically investigates this issue to estimate\nhow much naturalistic driving data (NDD) is needed for understanding driver\nbehaviors from a statistical point of view. A general assessment method is\nproposed using a Gaussian kernel density estimation to catch the underlying\ncharacteristics of driver behaviors. We then apply the Kullback-Liebler\ndivergence method to measure the similarity between density functions with\ndiffering amounts of NDD. A max-minimum approach is used to compute the\nappropriate amount of NDD. To validate our proposed method, we investigated the\ncar-following case using NDD collected from the University of Michigan Safety\nPilot Model Deployment (SPMD) program. We demonstrate that from a statistical\nperspective, the proposed approach can provide an appropriate amount of NDD\ncapable of capturing most features of the normal car-following behavior, which\nis consistent with the experiment settings in many literatures.\n", "versions": [{"version": "v1", "created": "Fri, 23 Jun 2017 11:19:14 GMT"}], "update_date": "2017-06-26", "authors_parsed": [["Wang", "Wenshuo", ""], ["Liu", "Chang", ""], ["Zhao", "Ding", ""]]}, {"id": "1706.07642", "submitter": "Yazhou Yang", "authors": "Yazhou Yang, Marco Loog", "title": "A Variance Maximization Criterion for Active Learning", "comments": "Accepted by Pattern Recognition Journal", "journal-ref": "Pattern Recognition 78C (2018) pp. 358-370", "doi": "10.1016/j.patcog.2018.01.017", "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Active learning aims to train a classifier as fast as possible with as few\nlabels as possible. The core element in virtually any active learning strategy\nis the criterion that measures the usefulness of the unlabeled data based on\nwhich new points to be labeled are picked. We propose a novel approach which we\nrefer to as maximizing variance for active learning or MVAL for short. MVAL\nmeasures the value of unlabeled instances by evaluating the rate of change of\noutput variables caused by changes in the next sample to be queried and its\npotential labelling. In a sense, this criterion measures how unstable the\nclassifier's output is for the unlabeled data points under perturbations of the\ntraining data. MVAL maintains, what we refer to as, retraining information\nmatrices to keep track of these output scores and exploits two kinds of\nvariance to measure the informativeness and representativeness, respectively.\nBy fusing these variances, MVAL is able to select the instances which are both\ninformative and representative. We employ our technique both in combination\nwith logistic regression and support vector machines and demonstrate that MVAL\nachieves state-of-the-art performance in experiments on a large number of\nstandard benchmark datasets.\n", "versions": [{"version": "v1", "created": "Fri, 23 Jun 2017 11:38:00 GMT"}, {"version": "v2", "created": "Sun, 11 Feb 2018 13:01:51 GMT"}], "update_date": "2018-02-13", "authors_parsed": [["Yang", "Yazhou", ""], ["Loog", "Marco", ""]]}, {"id": "1706.07669", "submitter": "Steve Hanneke", "authors": "Steve Hanneke, Liu Yang", "title": "Testing Piecewise Functions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work explores the query complexity of property testing for general\npiecewise functions on the real line, in the active and passive property\ntesting settings. The results are proven under an abstract zero-measure\ncrossings condition, which has as special cases piecewise constant functions\nand piecewise polynomial functions. We find that, in the active testing\nsetting, the query complexity of testing general piecewise functions is\nindependent of the number of pieces. We also identify the optimal dependence on\nthe number of pieces in the query complexity of passive testing in the special\ncase of piecewise constant functions.\n", "versions": [{"version": "v1", "created": "Fri, 23 Jun 2017 12:29:52 GMT"}, {"version": "v2", "created": "Sun, 20 May 2018 18:25:43 GMT"}], "update_date": "2018-05-22", "authors_parsed": [["Hanneke", "Steve", ""], ["Yang", "Liu", ""]]}, {"id": "1706.07679", "submitter": "Hammad Afzal", "authors": "Maham Jahangir, Hammad Afzal, Mehreen Ahmed, Khawar Khurshid, Raheel\n  Nawaz", "title": "ECO-AMLP: A Decision Support System using an Enhanced Class Outlier with\n  Automatic Multilayer Perceptron for Diabetes Prediction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With advanced data analytical techniques, efforts for more accurate decision\nsupport systems for disease prediction are on rise. Surveys by World Health\nOrganization (WHO) indicate a great increase in number of diabetic patients and\nrelated deaths each year. Early diagnosis of diabetes is a major concern among\nresearchers and practitioners. The paper presents an application of\n\\textit{Automatic Multilayer Perceptron }which\\textit{ }is combined with an\noutlier detection method \\textit{Enhanced Class Outlier Detection using\ndistance based algorithm }to create a prediction framework named as Enhanced\nClass Outlier with Automatic Multi layer Perceptron (ECO-AMLP). A series of\nexperiments are performed on publicly available Pima Indian Diabetes Dataset to\ncompare ECO-AMLP with other individual classifiers as well as ensemble based\nmethods. The outlier technique used in our framework gave better results as\ncompared to other pre-processing and classification techniques. Finally, the\nresults are compared with other state-of-the-art methods reported in literature\nfor diabetes prediction on PIDD and achieved accuracy of 88.7\\% bests all other\nreported studies.\n", "versions": [{"version": "v1", "created": "Fri, 23 Jun 2017 13:01:09 GMT"}], "update_date": "2017-06-27", "authors_parsed": [["Jahangir", "Maham", ""], ["Afzal", "Hammad", ""], ["Ahmed", "Mehreen", ""], ["Khurshid", "Khawar", ""], ["Nawaz", "Raheel", ""]]}, {"id": "1706.07719", "submitter": "Arya Mazumdar", "authors": "Arya Mazumdar, Barna Saha", "title": "Query Complexity of Clustering with Side Information", "comments": "A prior version of this work appeared in arxiv previously, see\n  arxiv:1604.01839. This paper contains a new efficient Monte Carlo algorithm\n  that has not appeared before, and a stronger lower bound. Some proofs have\n  been rewritten for clarity", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.DS cs.IT cs.LG math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Suppose, we are given a set of $n$ elements to be clustered into $k$\n(unknown) clusters, and an oracle/expert labeler that can interactively answer\npair-wise queries of the form, \"do two elements $u$ and $v$ belong to the same\ncluster?\". The goal is to recover the optimum clustering by asking the minimum\nnumber of queries. In this paper, we initiate a rigorous theoretical study of\nthis basic problem of query complexity of interactive clustering, and provide\nstrong information theoretic lower bounds, as well as nearly matching upper\nbounds. Most clustering problems come with a similarity matrix, which is used\nby an automated process to cluster similar points together. Our main\ncontribution in this paper is to show the dramatic power of side information\naka similarity matrix on reducing the query complexity of clustering. A\nsimilarity matrix represents noisy pair-wise relationships such as one computed\nby some function on attributes of the elements. A natural noisy model is where\nsimilarity values are drawn independently from some arbitrary probability\ndistribution $f_+$ when the underlying pair of elements belong to the same\ncluster, and from some $f_-$ otherwise. We show that given such a similarity\nmatrix, the query complexity reduces drastically from $\\Theta(nk)$ (no\nsimilarity matrix) to $O(\\frac{k^2\\log{n}}{\\cH^2(f_+\\|f_-)})$ where $\\cH^2$\ndenotes the squared Hellinger divergence. Moreover, this is also\ninformation-theoretic optimal within an $O(\\log{n})$ factor. Our algorithms are\nall efficient, and parameter free, i.e., they work without any knowledge of $k,\nf_+$ and $f_-$, and only depend logarithmically with $n$. Along the way, our\nwork also reveals intriguing connection to popular community detection models\nsuch as the {\\em stochastic block model}, significantly generalizes them, and\nopens up many venues for interesting future research.\n", "versions": [{"version": "v1", "created": "Fri, 23 Jun 2017 14:24:32 GMT"}], "update_date": "2017-06-26", "authors_parsed": [["Mazumdar", "Arya", ""], ["Saha", "Barna", ""]]}, {"id": "1706.07853", "submitter": "Sayeh Sharify", "authors": "Sayeh Sharify, Alberto Delmas Lascorz, Kevin Siu, Patrick Judd,\n  Andreas Moshovos", "title": "Loom: Exploiting Weight and Activation Precisions to Accelerate\n  Convolutional Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.AR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Loom (LM), a hardware inference accelerator for Convolutional Neural Networks\n(CNNs) is presented. In LM every bit of data precision that can be saved\ntranslates to proportional performance gains. Specifically, for convolutional\nlayers LM's execution time scales inversely proportionally with the precisions\nof both weights and activations. For fully-connected layers LM's performance\nscales inversely proportionally with the precision of the weights. LM targets\narea- and bandwidth-constrained System-on-a-Chip designs such as those found on\nmobile devices that cannot afford the multi-megabyte buffers that would be\nneeded to store each layer on-chip. Accordingly, given a data bandwidth budget,\nLM boosts energy efficiency and performance over an equivalent bit-parallel\naccelerator. For both weights and activations LM can exploit profile-derived\nperlayer precisions. However, at runtime LM further trims activation precisions\nat a much smaller than a layer granularity. Moreover, it can naturally exploit\nweight precision variability at a smaller granularity than a layer. On average,\nacross several image classification CNNs and for a configuration that can\nperform the equivalent of 128 16b x 16b multiply-accumulate operations per\ncycle LM outperforms a state-of-the-art bit-parallel accelerator [1] by 4.38x\nwithout any loss in accuracy while being 3.54x more energy efficient. LM can\ntrade-off accuracy for additional improvements in execution performance and\nenergy efficiency and compares favorably to an accelerator that targeted only\nactivation precisions. We also study 2- and 4-bit LM variants and find the the\n2-bit per cycle variant is the most energy efficient.\n", "versions": [{"version": "v1", "created": "Fri, 23 Jun 2017 20:35:42 GMT"}, {"version": "v2", "created": "Wed, 16 May 2018 19:31:40 GMT"}], "update_date": "2018-05-18", "authors_parsed": [["Sharify", "Sayeh", ""], ["Lascorz", "Alberto Delmas", ""], ["Siu", "Kevin", ""], ["Judd", "Patrick", ""], ["Moshovos", "Andreas", ""]]}, {"id": "1706.07867", "submitter": "Abhilasha Ravichander", "authors": "Abhilasha Ravichander, Shruti Rijhwani, Rajat Kulshreshtha, Chirag\n  Nagpal, Tadas Baltru\\v{s}aitis, Louis-Philippe Morency", "title": "Preserving Intermediate Objectives: One Simple Trick to Improve Learning\n  for Hierarchical Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hierarchical models are utilized in a wide variety of problems which are\ncharacterized by task hierarchies, where predictions on smaller subtasks are\nuseful for trying to predict a final task. Typically, neural networks are first\ntrained for the subtasks, and the predictions of these networks are\nsubsequently used as additional features when training a model and doing\ninference for a final task. In this work, we focus on improving learning for\nsuch hierarchical models and demonstrate our method on the task of speaker\ntrait prediction. Speaker trait prediction aims to computationally identify\nwhich personality traits a speaker might be perceived to have, and has been of\ngreat interest to both the Artificial Intelligence and Social Science\ncommunities. Persuasiveness prediction in particular has been of interest, as\npersuasive speakers have a large amount of influence on our thoughts, opinions\nand beliefs. In this work, we examine how leveraging the relationship between\nrelated speaker traits in a hierarchical structure can help improve our ability\nto predict how persuasive a speaker is. We present a novel algorithm that\nallows us to backpropagate through this hierarchy. This hierarchical model\nachieves a 25% relative error reduction in classification accuracy over current\nstate-of-the art methods on the publicly available POM dataset.\n", "versions": [{"version": "v1", "created": "Fri, 23 Jun 2017 21:16:18 GMT"}], "update_date": "2017-06-27", "authors_parsed": [["Ravichander", "Abhilasha", ""], ["Rijhwani", "Shruti", ""], ["Kulshreshtha", "Rajat", ""], ["Nagpal", "Chirag", ""], ["Baltru\u0161aitis", "Tadas", ""], ["Morency", "Louis-Philippe", ""]]}, {"id": "1706.07880", "submitter": "Aditya Balu", "authors": "Zhanhong Jiang, Aditya Balu, Chinmay Hegde and Soumik Sarkar", "title": "Collaborative Deep Learning in Fixed Topology Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There is significant recent interest to parallelize deep learning algorithms\nin order to handle the enormous growth in data and model sizes. While most\nadvances focus on model parallelization and engaging multiple computing agents\nvia using a central parameter server, aspect of data parallelization along with\ndecentralized computation has not been explored sufficiently. In this context,\nthis paper presents a new consensus-based distributed SGD (CDSGD) (and its\nmomentum variant, CDMSGD) algorithm for collaborative deep learning over fixed\ntopology networks that enables data parallelization as well as decentralized\ncomputation. Such a framework can be extremely useful for learning agents with\naccess to only local/private data in a communication constrained environment.\nWe analyze the convergence properties of the proposed algorithm with strongly\nconvex and nonconvex objective functions with fixed and diminishing step sizes\nusing concepts of Lyapunov function construction. We demonstrate the efficacy\nof our algorithms in comparison with the baseline centralized SGD and the\nrecently proposed federated averaging algorithm (that also enables data\nparallelism) based on benchmark datasets such as MNIST, CIFAR-10 and CIFAR-100.\n", "versions": [{"version": "v1", "created": "Fri, 23 Jun 2017 22:30:17 GMT"}], "update_date": "2017-06-27", "authors_parsed": [["Jiang", "Zhanhong", ""], ["Balu", "Aditya", ""], ["Hegde", "Chinmay", ""], ["Sarkar", "Soumik", ""]]}, {"id": "1706.07881", "submitter": "Ting Chen", "authors": "Ting Chen, Yizhou Sun, Yue Shi, Liangjie Hong", "title": "On Sampling Strategies for Neural Network-based Collaborative Filtering", "comments": "This is a longer version (with supplementary attached) of the KDD'17\n  paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.IR cs.SI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent advances in neural networks have inspired people to design hybrid\nrecommendation algorithms that can incorporate both (1) user-item interaction\ninformation and (2) content information including image, audio, and text.\nDespite their promising results, neural network-based recommendation algorithms\npose extensive computational costs, making it challenging to scale and improve\nupon. In this paper, we propose a general neural network-based recommendation\nframework, which subsumes several existing state-of-the-art recommendation\nalgorithms, and address the efficiency issue by investigating sampling\nstrategies in the stochastic gradient descent training for the framework. We\ntackle this issue by first establishing a connection between the loss functions\nand the user-item interaction bipartite graph, where the loss function terms\nare defined on links while major computation burdens are located at nodes. We\ncall this type of loss functions \"graph-based\" loss functions, for which varied\nmini-batch sampling strategies can have different computational costs. Based on\nthe insight, three novel sampling strategies are proposed, which can\nsignificantly improve the training efficiency of the proposed framework (up to\n$\\times 30$ times speedup in our experiments), as well as improving the\nrecommendation performance. Theoretical analysis is also provided for both the\ncomputational cost and the convergence. We believe the study of sampling\nstrategies have further implications on general graph-based loss functions, and\nwould also enable more research under the neural network-based recommendation\nframework.\n", "versions": [{"version": "v1", "created": "Fri, 23 Jun 2017 22:56:40 GMT"}], "update_date": "2017-06-27", "authors_parsed": [["Chen", "Ting", ""], ["Sun", "Yizhou", ""], ["Shi", "Yue", ""], ["Hong", "Liangjie", ""]]}, {"id": "1706.07896", "submitter": "Mircea Andrecut Dr", "authors": "M. Andrecut", "title": "Reservoir Computing on the Hypersphere", "comments": "12 pages, 6 figures, Int. J. Mod. Phys. C, 2017", "journal-ref": null, "doi": "10.1142/S0129183117500954", "report-no": null, "categories": "cs.LG physics.data-an stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Reservoir Computing (RC) refers to a Recurrent Neural Networks (RNNs)\nframework, frequently used for sequence learning and time series prediction.\nThe RC system consists of a random fixed-weight RNN (the input-hidden reservoir\nlayer) and a classifier (the hidden-output readout layer). Here we focus on the\nsequence learning problem, and we explore a different approach to RC. More\nspecifically, we remove the non-linear neural activation function, and we\nconsider an orthogonal reservoir acting on normalized states on the unit\nhypersphere. Surprisingly, our numerical results show that the system's memory\ncapacity exceeds the dimensionality of the reservoir, which is the upper bound\nfor the typical RC approach based on Echo State Networks (ESNs). We also show\nhow the proposed system can be applied to symmetric cryptography problems, and\nwe include a numerical implementation.\n", "versions": [{"version": "v1", "created": "Sat, 24 Jun 2017 02:10:53 GMT"}], "update_date": "2017-06-27", "authors_parsed": [["Andrecut", "M.", ""]]}, {"id": "1706.07913", "submitter": "Mahamad Suhil", "authors": "Harsha S. Gowda, Mahamad Suhil, D.S. Guru, and Lavanya Narayana Raju", "title": "Semi-supervised Text Categorization Using Recursive K-means Clustering", "comments": "11 Pages, 8 Figures, Conference: RTIP2R", "journal-ref": null, "doi": "10.1007/978-981-10-4859-3_20", "report-no": null, "categories": "cs.LG cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present a semi-supervised learning algorithm for\nclassification of text documents. A method of labeling unlabeled text documents\nis presented. The presented method is based on the principle of divide and\nconquer strategy. It uses recursive K-means algorithm for partitioning both\nlabeled and unlabeled data collection. The K-means algorithm is applied\nrecursively on each partition till a desired level partition is achieved such\nthat each partition contains labeled documents of a single class. Once the\ndesired clusters are obtained, the respective cluster centroids are considered\nas representatives of the clusters and the nearest neighbor rule is used for\nclassifying an unknown text document. Series of experiments have been conducted\nto bring out the superiority of the proposed model over other recent state of\nthe art models on 20Newsgroups dataset.\n", "versions": [{"version": "v1", "created": "Sat, 24 Jun 2017 06:08:27 GMT"}], "update_date": "2017-06-27", "authors_parsed": [["Gowda", "Harsha S.", ""], ["Suhil", "Mahamad", ""], ["Guru", "D. S.", ""], ["Raju", "Lavanya Narayana", ""]]}, {"id": "1706.07927", "submitter": "Liming Shi", "authors": "Liming Shi, Jesper Kj{\\ae}r Nielsen, Jesper Rindom Jensen and Mads\n  Gr{\\ae}sb{\\o}ll Christensen", "title": "A Variational EM Method for Pole-Zero Modeling of Speech with Mixed\n  Block Sparse and Gaussian Excitation", "comments": "Accepted in the 25th European Signal Processing Conference (EUSIPCO\n  2017), published by EUROSIP, scheduled for Aug. 28 - Sep. 2 in Kos island,\n  Greece", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The modeling of speech can be used for speech synthesis and speech\nrecognition. We present a speech analysis method based on pole-zero modeling of\nspeech with mixed block sparse and Gaussian excitation. By using a pole-zero\nmodel, instead of the all-pole model, a better spectral fitting can be\nexpected. Moreover, motivated by the block sparse glottal flow excitation\nduring voiced speech and the white noise excitation for unvoiced speech, we\nmodel the excitation sequence as a combination of block sparse signals and\nwhite noise. A variational EM (VEM) method is proposed for estimating the\nposterior PDFs of the block sparse residuals and point estimates of mod- elling\nparameters within a sparse Bayesian learning framework. Compared to\nconventional pole-zero and all-pole based methods, experimental results show\nthat the proposed method has lower spectral distortion and good performance in\nreconstructing of the block sparse excitation.\n", "versions": [{"version": "v1", "created": "Sat, 24 Jun 2017 08:50:20 GMT"}], "update_date": "2017-06-27", "authors_parsed": [["Shi", "Liming", ""], ["Nielsen", "Jesper Kj\u00e6r", ""], ["Jensen", "Jesper Rindom", ""], ["Christensen", "Mads Gr\u00e6sb\u00f8ll", ""]]}, {"id": "1706.07979", "submitter": "Gr\\'egoire Montavon", "authors": "Gr\\'egoire Montavon, Wojciech Samek, Klaus-Robert M\\\"uller", "title": "Methods for Interpreting and Understanding Deep Neural Networks", "comments": "14 pages, 10 figures", "journal-ref": null, "doi": "10.1016/j.dsp.2017.10.011", "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper provides an entry point to the problem of interpreting a deep\nneural network model and explaining its predictions. It is based on a tutorial\ngiven at ICASSP 2017. It introduces some recently proposed techniques of\ninterpretation, along with theory, tricks and recommendations, to make most\nefficient use of these techniques on real data. It also discusses a number of\npractical applications.\n", "versions": [{"version": "v1", "created": "Sat, 24 Jun 2017 16:25:06 GMT"}], "update_date": "2017-11-15", "authors_parsed": [["Montavon", "Gr\u00e9goire", ""], ["Samek", "Wojciech", ""], ["M\u00fcller", "Klaus-Robert", ""]]}, {"id": "1706.08001", "submitter": "Zizhuang (Prince K) Wang Mr", "authors": "Zizhuang Wang", "title": "Temporal-related Convolutional-Restricted-Boltzmann-Machine capable of\n  learning relational order via reinforcement learning procedure?", "comments": "Keywords: Convolutional-Restricted-Boltzmann-Machine, Reinforcement\n  learning", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this article, we extend the conventional framework of\nconvolutional-Restricted-Boltzmann-Machine to learn highly abstract features\namong abitrary number of time related input maps by constructing a layer of\nmultiplicative units, which capture the relations among inputs. In many cases,\nmore than two maps are strongly related, so it is wise to make multiplicative\nunit learn relations among more input maps, in other words, to find the optimal\nrelational-order of each unit. In order to enable our machine to learn\nrelational order, we developed a reinforcement-learning method whose optimality\nis proven to train the network.\n", "versions": [{"version": "v1", "created": "Sat, 24 Jun 2017 20:56:27 GMT"}], "update_date": "2017-06-27", "authors_parsed": [["Wang", "Zizhuang", ""]]}, {"id": "1706.08082", "submitter": "Wouter Kouw", "authors": "Wouter M. Kouw, Marco Loog", "title": "Target contrastive pessimistic risk for robust domain adaptation", "comments": "35 pages, 3 figures, 6 tables, 2 algorithms, 1 theorem", "journal-ref": null, "doi": "10.1016/j.patrec.2021.05.005", "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In domain adaptation, classifiers with information from a source domain adapt\nto generalize to a target domain. However, an adaptive classifier can perform\nworse than a non-adaptive classifier due to invalid assumptions, increased\nsensitivity to estimation errors or model misspecification. Our goal is to\ndevelop a domain-adaptive classifier that is robust in the sense that it does\nnot rely on restrictive assumptions on how the source and target domains relate\nto each other and that it does not perform worse than the non-adaptive\nclassifier. We formulate a conservative parameter estimator that only deviates\nfrom the source classifier when a lower risk is guaranteed for all possible\nlabellings of the given target samples. We derive the classical least-squares\nand discriminant analysis cases and show that these perform on par with\nstate-of-the-art domain adaptive classifiers in sample selection bias settings,\nwhile outperforming them in more general domain adaptation settings.\n", "versions": [{"version": "v1", "created": "Sun, 25 Jun 2017 11:48:09 GMT"}], "update_date": "2021-06-18", "authors_parsed": [["Kouw", "Wouter M.", ""], ["Loog", "Marco", ""]]}, {"id": "1706.08146", "submitter": "Vatsal Sharan", "authors": "Vatsal Sharan, Kai Sheng Tai, Peter Bailis, Gregory Valiant", "title": "Compressed Factorization: Fast and Accurate Low-Rank Factorization of\n  Compressively-Sensed Data", "comments": "Updates for ICML'19 camera-ready", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  What learning algorithms can be run directly on compressively-sensed data? In\nthis work, we consider the question of accurately and efficiently computing\nlow-rank matrix or tensor factorizations given data compressed via random\nprojections. We examine the approach of first performing factorization in the\ncompressed domain, and then reconstructing the original high-dimensional\nfactors from the recovered (compressed) factors. In both the matrix and tensor\nsettings, we establish conditions under which this natural approach will\nprovably recover the original factors. While it is well-known that random\nprojections preserve a number of geometric properties of a dataset, our work\ncan be viewed as showing that they can also preserve certain solutions of\nnon-convex, NP-Hard problems like non-negative matrix factorization. We support\nthese theoretical results with experiments on synthetic data and demonstrate\nthe practical applicability of compressed factorization on real-world gene\nexpression and EEG time series datasets.\n", "versions": [{"version": "v1", "created": "Sun, 25 Jun 2017 17:47:16 GMT"}, {"version": "v2", "created": "Wed, 30 May 2018 09:13:19 GMT"}, {"version": "v3", "created": "Mon, 27 May 2019 08:27:04 GMT"}], "update_date": "2019-05-28", "authors_parsed": [["Sharan", "Vatsal", ""], ["Tai", "Kai Sheng", ""], ["Bailis", "Peter", ""], ["Valiant", "Gregory", ""]]}, {"id": "1706.08160", "submitter": "Shyam Upadhyay", "authors": "Shyam Upadhyay and Kai-Wei Chang and Matt Taddy and Adam Kalai and\n  James Zou", "title": "Beyond Bilingual: Multi-sense Word Embeddings using Multilingual Context", "comments": "ACL 2017 Repl4NLP workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Word embeddings, which represent a word as a point in a vector space, have\nbecome ubiquitous to several NLP tasks. A recent line of work uses bilingual\n(two languages) corpora to learn a different vector for each sense of a word,\nby exploiting crosslingual signals to aid sense identification. We present a\nmulti-view Bayesian non-parametric algorithm which improves multi-sense word\nembeddings by (a) using multilingual (i.e., more than two languages) corpora to\nsignificantly improve sense embeddings beyond what one achieves with bilingual\ninformation, and (b) uses a principled approach to learn a variable number of\nsenses per word, in a data-driven manner. Ours is the first approach with the\nability to leverage multilingual corpora efficiently for multi-sense\nrepresentation learning. Experiments show that multilingual training\nsignificantly improves performance over monolingual and bilingual training, by\nallowing us to combine different parallel corpora to leverage multilingual\ncontext. Multilingual training yields comparable performance to a state of the\nart mono-lingual model trained on five times more training data.\n", "versions": [{"version": "v1", "created": "Sun, 25 Jun 2017 20:00:54 GMT"}], "update_date": "2017-06-27", "authors_parsed": [["Upadhyay", "Shyam", ""], ["Chang", "Kai-Wei", ""], ["Taddy", "Matt", ""], ["Kalai", "Adam", ""], ["Zou", "James", ""]]}, {"id": "1706.08217", "submitter": "Shujiao Huang", "authors": "Zhenzhen Zhong, Shujiao Huang, Cheng Zhan, Licheng Zhang, Zhiwei Xiao,\n  Chang-Chun Wang, Pei Yang", "title": "An Effective Way to Improve YouTube-8M Classification Accuracy in Google\n  Cloud Platform", "comments": "5 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Large-scale datasets have played a significant role in progress of neural\nnetwork and deep learning areas. YouTube-8M is such a benchmark dataset for\ngeneral multi-label video classification. It was created from over 7 million\nYouTube videos (450,000 hours of video) and includes video labels from a\nvocabulary of 4716 classes (3.4 labels/video on average). It also comes with\npre-extracted audio & visual features from every second of video (3.2 billion\nfeature vectors in total). Google cloud recently released the datasets and\norganized 'Google Cloud & YouTube-8M Video Understanding Challenge' on Kaggle.\nCompetitors are challenged to develop classification algorithms that assign\nvideo-level labels using the new and improved Youtube-8M V2 dataset. Inspired\nby the competition, we started exploration of audio understanding and\nclassification using deep learning algorithms and ensemble methods. We built\nseveral baseline predictions according to the benchmark paper and public github\ntensorflow code. Furthermore, we improved global prediction accuracy (GAP) from\nbase level 77% to 80.7% through approaches of ensemble.\n", "versions": [{"version": "v1", "created": "Mon, 26 Jun 2017 03:50:51 GMT"}], "update_date": "2017-06-27", "authors_parsed": [["Zhong", "Zhenzhen", ""], ["Huang", "Shujiao", ""], ["Zhan", "Cheng", ""], ["Zhang", "Licheng", ""], ["Xiao", "Zhiwei", ""], ["Wang", "Chang-Chun", ""], ["Yang", "Pei", ""]]}, {"id": "1706.08224", "submitter": "Yi Zhang", "authors": "Sanjeev Arora, Yi Zhang", "title": "Do GANs actually learn the distribution? An empirical study", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Do GANS (Generative Adversarial Nets) actually learn the target distribution?\nThe foundational paper of (Goodfellow et al 2014) suggested they do, if they\nwere given sufficiently large deep nets, sample size, and computation time. A\nrecent theoretical analysis in Arora et al (to appear at ICML 2017) raised\ndoubts whether the same holds when discriminator has finite size. It showed\nthat the training objective can approach its optimum value even if the\ngenerated distribution has very low support ---in other words, the training\nobjective is unable to prevent mode collapse. The current note reports\nexperiments suggesting that such problems are not merely theoretical. It\npresents empirical evidence that well-known GANs approaches do learn\ndistributions of fairly low support, and thus presumably are not learning the\ntarget distribution. The main technical contribution is a new proposed test,\nbased upon the famous birthday paradox, for estimating the support size of the\ngenerated distribution.\n", "versions": [{"version": "v1", "created": "Mon, 26 Jun 2017 04:21:14 GMT"}, {"version": "v2", "created": "Sat, 1 Jul 2017 03:25:50 GMT"}], "update_date": "2017-07-04", "authors_parsed": [["Arora", "Sanjeev", ""], ["Zhang", "Yi", ""]]}, {"id": "1706.08323", "submitter": "Ruifeng Shao", "authors": "Ruifeng Shao, Ning Xu, Xin Geng", "title": "Multi-Label Learning with Label Enhancement", "comments": "ICDM 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The task of multi-label learning is to predict a set of relevant labels for\nthe unseen instance. Traditional multi-label learning algorithms treat each\nclass label as a logical indicator of whether the corresponding label is\nrelevant or irrelevant to the instance, i.e., +1 represents relevant to the\ninstance and -1 represents irrelevant to the instance. Such label represented\nby -1 or +1 is called logical label. Logical label cannot reflect different\nlabel importance. However, for real-world multi-label learning problems, the\nimportance of each possible label is generally different. For the real\napplications, it is difficult to obtain the label importance information\ndirectly. Thus we need a method to reconstruct the essential label importance\nfrom the logical multilabel data. To solve this problem, we assume that each\nmulti-label instance is described by a vector of latent real-valued labels,\nwhich can reflect the importance of the corresponding labels. Such label is\ncalled numerical label. The process of reconstructing the numerical labels from\nthe logical multi-label data via utilizing the logical label information and\nthe topological structure in the feature space is called Label Enhancement. In\nthis paper, we propose a novel multi-label learning framework called LEMLL,\ni.e., Label Enhanced Multi-Label Learning, which incorporates regression of the\nnumerical labels and label enhancement into a unified framework. Extensive\ncomparative studies validate that the performance of multi-label learning can\nbe improved significantly with label enhancement and LEMLL can effectively\nreconstruct latent label importance information from logical multi-label data.\n", "versions": [{"version": "v1", "created": "Mon, 26 Jun 2017 11:15:04 GMT"}, {"version": "v2", "created": "Fri, 7 Jul 2017 08:36:50 GMT"}, {"version": "v3", "created": "Wed, 9 Jan 2019 13:41:45 GMT"}, {"version": "v4", "created": "Tue, 16 Apr 2019 09:51:09 GMT"}], "update_date": "2019-04-17", "authors_parsed": [["Shao", "Ruifeng", ""], ["Xu", "Ning", ""], ["Geng", "Xin", ""]]}, {"id": "1706.08334", "submitter": "Gabriella Contardo", "authors": "Gabriella Contardo, Ludovic Denoyer, Thierry Artieres", "title": "A Meta-Learning Approach to One-Step Active Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of learning when obtaining the training labels is\ncostly, which is usually tackled in the literature using active-learning\ntechniques. These approaches provide strategies to choose the examples to label\nbefore or during training. These strategies are usually based on heuristics or\neven theoretical measures, but are not learned as they are directly used during\ntraining. We design a model which aims at \\textit{learning active-learning\nstrategies} using a meta-learning setting. More specifically, we consider a\npool-based setting, where the system observes all the examples of the dataset\nof a problem and has to choose the subset of examples to label in a single\nshot. Experiments show encouraging results.\n", "versions": [{"version": "v1", "created": "Mon, 26 Jun 2017 12:06:17 GMT"}, {"version": "v2", "created": "Mon, 17 Jul 2017 14:04:04 GMT"}], "update_date": "2017-07-18", "authors_parsed": [["Contardo", "Gabriella", ""], ["Denoyer", "Ludovic", ""], ["Artieres", "Thierry", ""]]}, {"id": "1706.08359", "submitter": "Huan Zhang", "authors": "Huan Zhang, Si Si, Cho-Jui Hsieh", "title": "GPU-acceleration for Large-scale Tree Boosting", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.DC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present a novel massively parallel algorithm for\naccelerating the decision tree building procedure on GPUs (Graphics Processing\nUnits), which is a crucial step in Gradient Boosted Decision Tree (GBDT) and\nrandom forests training. Previous GPU based tree building algorithms are based\non parallel multi-scan or radix sort to find the exact tree split, and thus\nsuffer from scalability and performance issues. We show that using a histogram\nbased algorithm to approximately find the best split is more efficient and\nscalable on GPU. By identifying the difference between classical GPU-based\nimage histogram construction and the feature histogram construction in decision\ntree training, we develop a fast feature histogram building kernel on GPU with\ncarefully designed computational and memory access sequence to reduce atomic\nupdate conflict and maximize GPU utilization. Our algorithm can be used as a\ndrop-in replacement for histogram construction in popular tree boosting systems\nto improve their scalability. As an example, to train GBDT on epsilon dataset,\nour method using a main-stream GPU is 7-8 times faster than histogram based\nalgorithm on CPU in LightGBM and 25 times faster than the exact-split finding\nalgorithm in XGBoost on a dual-socket 28-core Xeon server, while achieving\nsimilar prediction accuracy.\n", "versions": [{"version": "v1", "created": "Mon, 26 Jun 2017 13:27:29 GMT"}], "update_date": "2017-06-27", "authors_parsed": [["Zhang", "Huan", ""], ["Si", "Si", ""], ["Hsieh", "Cho-Jui", ""]]}, {"id": "1706.08427", "submitter": "Sebastian U. Stich", "authors": "Sebastian U. Stich, Anant Raj, Martin Jaggi", "title": "Approximate Steepest Coordinate Descent", "comments": "appearing at ICML 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new selection rule for the coordinate selection in coordinate\ndescent methods for huge-scale optimization. The efficiency of this novel\nscheme is provably better than the efficiency of uniformly random selection,\nand can reach the efficiency of steepest coordinate descent (SCD), enabling an\nacceleration of a factor of up to $n$, the number of coordinates. In many\npractical applications, our scheme can be implemented at no extra cost and\ncomputational efficiency very close to the faster uniform selection. Numerical\nexperiments with Lasso and Ridge regression show promising improvements, in\nline with our theoretical guarantees.\n", "versions": [{"version": "v1", "created": "Mon, 26 Jun 2017 15:07:02 GMT"}], "update_date": "2017-06-27", "authors_parsed": [["Stich", "Sebastian U.", ""], ["Raj", "Anant", ""], ["Jaggi", "Martin", ""]]}, {"id": "1706.08470", "submitter": "Carlo Baldassi", "authors": "Carlo Baldassi, Riccardo Zecchina", "title": "Efficiency of quantum versus classical annealing in non-convex learning\n  problems", "comments": "31 pages, 10 figures", "journal-ref": "Proceedings of the National Academy of Sciences Jan 2018,\n  201711456", "doi": "10.1073/pnas.1711456115", "report-no": null, "categories": "quant-ph cond-mat.dis-nn cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Quantum annealers aim at solving non-convex optimization problems by\nexploiting cooperative tunneling effects to escape local minima. The underlying\nidea consists in designing a classical energy function whose ground states are\nthe sought optimal solutions of the original optimization problem and add a\ncontrollable quantum transverse field to generate tunneling processes. A key\nchallenge is to identify classes of non-convex optimization problems for which\nquantum annealing remains efficient while thermal annealing fails. We show that\nthis happens for a wide class of problems which are central to machine\nlearning. Their energy landscapes is dominated by local minima that cause\nexponential slow down of classical thermal annealers while simulated quantum\nannealing converges efficiently to rare dense regions of optimal solutions.\n", "versions": [{"version": "v1", "created": "Mon, 26 Jun 2017 16:43:49 GMT"}, {"version": "v2", "created": "Tue, 27 Jun 2017 09:00:51 GMT"}, {"version": "v3", "created": "Mon, 16 Oct 2017 21:57:11 GMT"}], "update_date": "2018-09-12", "authors_parsed": [["Baldassi", "Carlo", ""], ["Zecchina", "Riccardo", ""]]}, {"id": "1706.08491", "submitter": "Lev Givon", "authors": "Lev E. Givon (1), Laura J. Mariano (1), David O'Dowd (1), John M.\n  Irvine (1), Abraham R. Schneider (1) ((1) The Charles Stark Draper\n  Laboratory, Inc.)", "title": "Cognitive Subscore Trajectory Prediction in Alzheimer's Disease", "comments": "Accepted for presentation in BioImage Informatics Conference 2017\n  (9/19/2017) and SIIM Conference on Machine Learning in Medical Imaging\n  (C-MIMI) 2017 (9/26/2017). Data used in the preparation of this article were\n  obtained from the Alzheimer's Disease Neuroimaging Initiative (ADNI) database\n  (http://adni.loni.usc.edu)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Accurate diagnosis of Alzheimer's Disease (AD) entails clinical evaluation of\nmultiple cognition metrics and biomarkers. Metrics such as the Alzheimer's\nDisease Assessment Scale - Cognitive test (ADAS-cog) comprise multiple\nsubscores that quantify different aspects of a patient's cognitive state such\nas learning, memory, and language production/comprehension. Although\ncomputer-aided diagnostic techniques for classification of a patient's current\ndisease state exist, they provide little insight into the relationship between\nchanges in brain structure and different aspects of a patient's cognitive state\nthat occur over time in AD. We have developed a Convolutional Neural Network\narchitecture that can concurrently predict the trajectories of the 13 subscores\ncomprised by a subject's ADAS-cog examination results from a current minimally\npreprocessed structural MRI scan up to 36 months from image acquisition time\nwithout resorting to manual feature extraction. Mean performance metrics are\nwithin range of those of existing techniques that require manual feature\nselection and are limited to predicting aggregate scores.\n", "versions": [{"version": "v1", "created": "Mon, 26 Jun 2017 17:29:42 GMT"}, {"version": "v2", "created": "Tue, 15 Aug 2017 14:24:28 GMT"}], "update_date": "2017-08-16", "authors_parsed": [["Givon", "Lev E.", ""], ["Mariano", "Laura J.", ""], ["O'Dowd", "David", ""], ["Irvine", "John M.", ""], ["Schneider", "Abraham R.", ""]]}, {"id": "1706.08498", "submitter": "Matus Telgarsky", "authors": "Peter Bartlett, Dylan J. Foster, Matus Telgarsky", "title": "Spectrally-normalized margin bounds for neural networks", "comments": "Comparison to arXiv v1: 1-norm in main bound refined to\n  (2,1)-group-norm. Comparison to NIPS camera ready: typo fixes", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a margin-based multiclass generalization bound for neural\nnetworks that scales with their margin-normalized \"spectral complexity\": their\nLipschitz constant, meaning the product of the spectral norms of the weight\nmatrices, times a certain correction factor. This bound is empirically\ninvestigated for a standard AlexNet network trained with SGD on the mnist and\ncifar10 datasets, with both original and random labels; the bound, the\nLipschitz constants, and the excess risks are all in direct correlation,\nsuggesting both that SGD selects predictors whose complexity scales with the\ndifficulty of the learning task, and secondly that the presented bound is\nsensitive to this complexity.\n", "versions": [{"version": "v1", "created": "Mon, 26 Jun 2017 17:43:48 GMT"}, {"version": "v2", "created": "Tue, 5 Dec 2017 06:08:38 GMT"}], "update_date": "2017-12-06", "authors_parsed": [["Bartlett", "Peter", ""], ["Foster", "Dylan J.", ""], ["Telgarsky", "Matus", ""]]}, {"id": "1706.08500", "submitter": "Martin Heusel", "authors": "Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler,\n  Sepp Hochreiter", "title": "GANs Trained by a Two Time-Scale Update Rule Converge to a Local Nash\n  Equilibrium", "comments": "Implementations are available at: https://github.com/bioinf-jku/TTUR", "journal-ref": "Advances in Neural Information Processing Systems 30 (NIPS 2017)", "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generative Adversarial Networks (GANs) excel at creating realistic images\nwith complex models for which maximum likelihood is infeasible. However, the\nconvergence of GAN training has still not been proved. We propose a two\ntime-scale update rule (TTUR) for training GANs with stochastic gradient\ndescent on arbitrary GAN loss functions. TTUR has an individual learning rate\nfor both the discriminator and the generator. Using the theory of stochastic\napproximation, we prove that the TTUR converges under mild assumptions to a\nstationary local Nash equilibrium. The convergence carries over to the popular\nAdam optimization, for which we prove that it follows the dynamics of a heavy\nball with friction and thus prefers flat minima in the objective landscape. For\nthe evaluation of the performance of GANs at image generation, we introduce the\n\"Fr\\'echet Inception Distance\" (FID) which captures the similarity of generated\nimages to real ones better than the Inception Score. In experiments, TTUR\nimproves learning for DCGANs and Improved Wasserstein GANs (WGAN-GP)\noutperforming conventional GAN training on CelebA, CIFAR-10, SVHN, LSUN\nBedrooms, and the One Billion Word Benchmark.\n", "versions": [{"version": "v1", "created": "Mon, 26 Jun 2017 17:45:23 GMT"}, {"version": "v2", "created": "Tue, 27 Jun 2017 15:06:08 GMT"}, {"version": "v3", "created": "Wed, 28 Jun 2017 16:36:56 GMT"}, {"version": "v4", "created": "Thu, 13 Jul 2017 09:15:29 GMT"}, {"version": "v5", "created": "Wed, 8 Nov 2017 16:25:21 GMT"}, {"version": "v6", "created": "Fri, 12 Jan 2018 14:05:44 GMT"}], "update_date": "2018-01-31", "authors_parsed": [["Heusel", "Martin", ""], ["Ramsauer", "Hubert", ""], ["Unterthiner", "Thomas", ""], ["Nessler", "Bernhard", ""], ["Hochreiter", "Sepp", ""]]}, {"id": "1706.08519", "submitter": "Ruofei Zhao", "authors": "Ya'acov Ritov, Yuekai Sun, Ruofei Zhao", "title": "On conditional parity as a notion of non-discrimination in machine\n  learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CY cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We identify conditional parity as a general notion of non-discrimination in\nmachine learning. In fact, several recently proposed notions of\nnon-discrimination, including a few counterfactual notions, are instances of\nconditional parity. We show that conditional parity is amenable to statistical\nanalysis by studying randomization as a general mechanism for achieving\nconditional parity and a kernel-based test of conditional parity.\n", "versions": [{"version": "v1", "created": "Mon, 26 Jun 2017 17:41:20 GMT"}], "update_date": "2017-06-28", "authors_parsed": [["Ritov", "Ya'acov", ""], ["Sun", "Yuekai", ""], ["Zhao", "Ruofei", ""]]}, {"id": "1706.08580", "submitter": "Angelos Katharopoulos", "authors": "Angelos Katharopoulos, Despoina Paschalidou, Christos Diou and\n  Anastasios Delopoulos", "title": "Learning Local Feature Aggregation Functions with Backpropagation", "comments": "In Proceedings of the 25th European Signal Processing Conference\n  (EUSIPCO 2017)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces a family of local feature aggregation functions and a\nnovel method to estimate their parameters, such that they generate optimal\nrepresentations for classification (or any task that can be expressed as a cost\nfunction minimization problem). To achieve that, we compose the local feature\naggregation function with the classifier cost function and we backpropagate the\ngradient of this cost function in order to update the local feature aggregation\nfunction parameters. Experiments on synthetic datasets indicate that our method\ndiscovers parameters that model the class-relevant information in addition to\nthe local feature space. Further experiments on a variety of motion and visual\ndescriptors, both on image and video datasets, show that our method outperforms\nother state-of-the-art local feature aggregation functions, such as Bag of\nWords, Fisher Vectors and VLAD, by a large margin.\n", "versions": [{"version": "v1", "created": "Mon, 26 Jun 2017 20:13:41 GMT"}], "update_date": "2017-06-28", "authors_parsed": [["Katharopoulos", "Angelos", ""], ["Paschalidou", "Despoina", ""], ["Diou", "Christos", ""], ["Delopoulos", "Anastasios", ""]]}, {"id": "1706.08606", "submitter": "David Barrett", "authors": "Samuel Ritter, David G.T. Barrett, Adam Santoro and Matt M. Botvinick", "title": "Cognitive Psychology for Deep Neural Networks: A Shape Bias Case Study", "comments": "ICML 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural networks (DNNs) have achieved unprecedented performance on a wide\nrange of complex tasks, rapidly outpacing our understanding of the nature of\ntheir solutions. This has caused a recent surge of interest in methods for\nrendering modern neural systems more interpretable. In this work, we propose to\naddress the interpretability problem in modern DNNs using the rich history of\nproblem descriptions, theories and experimental methods developed by cognitive\npsychologists to study the human mind. To explore the potential value of these\ntools, we chose a well-established analysis from developmental psychology that\nexplains how children learn word labels for objects, and applied that analysis\nto DNNs. Using datasets of stimuli inspired by the original cognitive\npsychology experiments, we find that state-of-the-art one shot learning models\ntrained on ImageNet exhibit a similar bias to that observed in humans: they\nprefer to categorize objects according to shape rather than color. The\nmagnitude of this shape bias varies greatly among architecturally identical,\nbut differently seeded models, and even fluctuates within seeds throughout\ntraining, despite nearly equivalent classification performance. These results\ndemonstrate the capability of tools from cognitive psychology for exposing\nhidden computational properties of DNNs, while concurrently providing us with a\ncomputational model for human word learning.\n", "versions": [{"version": "v1", "created": "Mon, 26 Jun 2017 21:31:18 GMT"}, {"version": "v2", "created": "Thu, 29 Jun 2017 17:52:55 GMT"}], "update_date": "2017-06-30", "authors_parsed": [["Ritter", "Samuel", ""], ["Barrett", "David G. T.", ""], ["Santoro", "Adam", ""], ["Botvinick", "Matt M.", ""]]}, {"id": "1706.08672", "submitter": "Tselil Schramm", "authors": "Tselil Schramm and David Steurer", "title": "Fast and robust tensor decomposition with applications to dictionary\n  learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop fast spectral algorithms for tensor decomposition that match the\nrobustness guarantees of the best known polynomial-time algorithms for this\nproblem based on the sum-of-squares (SOS) semidefinite programming hierarchy.\n  Our algorithms can decompose a 4-tensor with $n$-dimensional orthonormal\ncomponents in the presence of error with constant spectral norm (when viewed as\nan $n^2$-by-$n^2$ matrix). The running time is $n^5$ which is close to linear\nin the input size $n^4$.\n  We also obtain algorithms with similar running time to learn sparsely-used\northogonal dictionaries even when feature representations have constant\nrelative sparsity and non-independent coordinates.\n  The only previous polynomial-time algorithms to solve these problem are based\non solving large semidefinite programs. In contrast, our algorithms are easy to\nimplement directly and are based on spectral projections and tensor-mode\nrearrangements.\n  Or work is inspired by recent of Hopkins, Schramm, Shi, and Steurer (STOC'16)\nthat shows how fast spectral algorithms can achieve the guarantees of SOS for\naverage-case problems. In this work, we introduce general techniques to capture\nthe guarantees of SOS for worst-case problems.\n", "versions": [{"version": "v1", "created": "Tue, 27 Jun 2017 05:12:39 GMT"}], "update_date": "2017-06-28", "authors_parsed": [["Schramm", "Tselil", ""], ["Steurer", "David", ""]]}, {"id": "1706.08675", "submitter": "Dorien Herremans", "authors": "Dorien Herremans, Ching-Hua Chuan", "title": "Proceedings of the First International Workshop on Deep Learning and\n  Music", "comments": null, "journal-ref": "Proceedings of the First International Workshop on Deep Learning\n  and Music, joint with IJCNN, Anchorage, US, May 17-18, 2017", "doi": "10.13140/RG.2.2.22227.99364/1", "report-no": null, "categories": "cs.NE cs.LG cs.MM cs.SD", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Proceedings of the First International Workshop on Deep Learning and Music,\njoint with IJCNN, Anchorage, US, May 17-18, 2017\n", "versions": [{"version": "v1", "created": "Tue, 27 Jun 2017 05:28:06 GMT"}], "update_date": "2017-06-28", "authors_parsed": [["Herremans", "Dorien", ""], ["Chuan", "Ching-Hua", ""]]}, {"id": "1706.08697", "submitter": "Massimo Regoli", "authors": "Massimo Regoli, Nawid Jamali, Giorgio Metta and Lorenzo Natale", "title": "Controlled Tactile Exploration and Haptic Object Recognition", "comments": null, "journal-ref": "International Conference on Advanced Robotics (ICAR), 2017", "doi": "10.1109/ICAR.2017.8023495", "report-no": null, "categories": "cs.RO cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we propose a novel method for in-hand object recognition. The\nmethod is composed of a grasp stabilization controller and two exploratory\nbehaviours to capture the shape and the softness of an object. Grasp\nstabilization plays an important role in recognizing objects. First, it\nprevents the object from slipping and facilitates the exploration of the\nobject. Second, reaching a stable and repeatable position adds robustness to\nthe learning algorithm and increases invariance with respect to the way in\nwhich the robot grasps the object. The stable poses are estimated using a\nGaussian mixture model (GMM). We present experimental results showing that\nusing our method the classifier can successfully distinguish 30 objects.We also\ncompare our method with a benchmark experiment, in which the grasp\nstabilization is disabled. We show, with statistical significance, that our\nmethod outperforms the benchmark method.\n", "versions": [{"version": "v1", "created": "Tue, 27 Jun 2017 07:25:33 GMT"}], "update_date": "2021-06-30", "authors_parsed": [["Regoli", "Massimo", ""], ["Jamali", "Nawid", ""], ["Metta", "Giorgio", ""], ["Natale", "Lorenzo", ""]]}, {"id": "1706.08811", "submitter": "Magda Gregorova", "authors": "Magda Gregorov\\'a, Alexandros Kalousis, and St\\'ephane\n  Marchand-Maillet", "title": "Forecasting and Granger Modelling with Non-linear Dynamical Dependencies", "comments": "Accepted for ECML-PKDD 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Traditional linear methods for forecasting multivariate time series are not\nable to satisfactorily model the non-linear dependencies that may exist in\nnon-Gaussian series. We build on the theory of learning vector-valued functions\nin the reproducing kernel Hilbert space and develop a method for learning\nprediction functions that accommodate such non-linearities. The method not only\nlearns the predictive function but also the matrix-valued kernel underlying the\nfunction search space directly from the data. Our approach is based on learning\nmultiple matrix-valued kernels, each of those composed of a set of input\nkernels and a set of output kernels learned in the cone of positive\nsemi-definite matrices. In addition to superior predictive performance in the\npresence of strong non-linearities, our method also recovers the hidden dynamic\nrelationships between the series and thus is a new alternative to existing\ngraphical Granger techniques.\n", "versions": [{"version": "v1", "created": "Tue, 27 Jun 2017 12:19:39 GMT"}], "update_date": "2017-06-28", "authors_parsed": [["Gregorov\u00e1", "Magda", ""], ["Kalousis", "Alexandros", ""], ["Marchand-Maillet", "St\u00e9phane", ""]]}, {"id": "1706.08818", "submitter": "Roswitha Bammer", "authors": "Roswitha Bammer, Monika D\\\"orfler and Pavol Harar", "title": "Gabor frames and deep scattering networks in audio processing", "comments": "26 pages, 8 figures, 4 tables. Repository for reproducibility:\n  https://gitlab.com/hararticles/gs-gt . Keywords: machine learning; scattering\n  transform; Gabor transform; deep learning; time-frequency analysis; CNN.\n  Accepted and published after peer revision", "journal-ref": "Axioms 2019, 8(4), 106", "doi": "10.3390/axioms8040106", "report-no": null, "categories": "cs.SD cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces Gabor scattering, a feature extractor based on Gabor\nframes and Mallat's scattering transform. By using a simple signal model for\naudio signals specific properties of Gabor scattering are studied. It is shown\nthat for each layer, specific invariances to certain signal characteristics\noccur. Furthermore, deformation stability of the coefficient vector generated\nby the feature extractor is derived by using a decoupling technique which\nexploits the contractivity of general scattering networks. Deformations are\nintroduced as changes in spectral shape and frequency modulation. The\ntheoretical results are illustrated by numerical examples and experiments.\nNumerical evidence is given by evaluation on a synthetic and a \"real\" data set,\nthat the invariances encoded by the Gabor scattering transform lead to higher\nperformance in comparison with just using Gabor transform, especially when few\ntraining samples are available.\n", "versions": [{"version": "v1", "created": "Tue, 27 Jun 2017 12:39:10 GMT"}, {"version": "v2", "created": "Wed, 13 Feb 2019 15:38:34 GMT"}, {"version": "v3", "created": "Tue, 2 Jul 2019 12:57:37 GMT"}, {"version": "v4", "created": "Tue, 1 Oct 2019 12:48:14 GMT"}], "update_date": "2019-10-02", "authors_parsed": [["Bammer", "Roswitha", ""], ["D\u00f6rfler", "Monika", ""], ["Harar", "Pavol", ""]]}, {"id": "1706.08838", "submitter": "Pankaj Malhotra", "authors": "Pankaj Malhotra, Vishnu TV, Lovekesh Vig, Puneet Agarwal, Gautam\n  Shroff", "title": "TimeNet: Pre-trained deep recurrent neural network for time series\n  classification", "comments": "25th European Symposium on Artificial Neural Networks, Computational\n  Intelligence and Machine Learning, 2017, Bruges, Belgium", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Inspired by the tremendous success of deep Convolutional Neural Networks as\ngeneric feature extractors for images, we propose TimeNet: a deep recurrent\nneural network (RNN) trained on diverse time series in an unsupervised manner\nusing sequence to sequence (seq2seq) models to extract features from time\nseries. Rather than relying on data from the problem domain, TimeNet attempts\nto generalize time series representation across domains by ingesting time\nseries from several domains simultaneously. Once trained, TimeNet can be used\nas a generic off-the-shelf feature extractor for time series. The\nrepresentations or embeddings given by a pre-trained TimeNet are found to be\nuseful for time series classification (TSC). For several publicly available\ndatasets from UCR TSC Archive and an industrial telematics sensor data from\nvehicles, we observe that a classifier learned over the TimeNet embeddings\nyields significantly better performance compared to (i) a classifier learned\nover the embeddings given by a domain-specific RNN, as well as (ii) a nearest\nneighbor classifier based on Dynamic Time Warping.\n", "versions": [{"version": "v1", "created": "Fri, 23 Jun 2017 19:06:13 GMT"}], "update_date": "2017-06-28", "authors_parsed": [["Malhotra", "Pankaj", ""], ["TV", "Vishnu", ""], ["Vig", "Lovekesh", ""], ["Agarwal", "Puneet", ""], ["Shroff", "Gautam", ""]]}, {"id": "1706.08839", "submitter": "NhatHai Phan", "authors": "NhatHai Phan, Xintao Wu, and Dejing Dou", "title": "Preserving Differential Privacy in Convolutional Deep Belief Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The remarkable development of deep learning in medicine and healthcare domain\npresents obvious privacy issues, when deep neural networks are built on users'\npersonal and highly sensitive data, e.g., clinical records, user profiles,\nbiomedical images, etc. However, only a few scientific studies on preserving\nprivacy in deep learning have been conducted. In this paper, we focus on\ndeveloping a private convolutional deep belief network (pCDBN), which\nessentially is a convolutional deep belief network (CDBN) under differential\nprivacy. Our main idea of enforcing epsilon-differential privacy is to leverage\nthe functional mechanism to perturb the energy-based objective functions of\ntraditional CDBNs, rather than their results. One key contribution of this work\nis that we propose the use of Chebyshev expansion to derive the approximate\npolynomial representation of objective functions. Our theoretical analysis\nshows that we can further derive the sensitivity and error bounds of the\napproximate polynomial representation. As a result, preserving differential\nprivacy in CDBNs is feasible. We applied our model in a health social network,\ni.e., YesiWell data, and in a handwriting digit dataset, i.e., MNIST data, for\nhuman behavior prediction, human behavior classification, and handwriting digit\nrecognition tasks. Theoretical analysis and rigorous experimental evaluations\nshow that the pCDBN is highly effective. It significantly outperforms existing\nsolutions.\n", "versions": [{"version": "v1", "created": "Sun, 25 Jun 2017 22:29:23 GMT"}, {"version": "v2", "created": "Mon, 23 Apr 2018 02:55:39 GMT"}], "update_date": "2018-04-24", "authors_parsed": [["Phan", "NhatHai", ""], ["Wu", "Xintao", ""], ["Dou", "Dejing", ""]]}, {"id": "1706.08840", "submitter": "David Lopez-Paz", "authors": "David Lopez-Paz and Marc'Aurelio Ranzato", "title": "Gradient Episodic Memory for Continual Learning", "comments": "Published at NIPS 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One major obstacle towards AI is the poor ability of models to solve new\nproblems quicker, and without forgetting previously acquired knowledge. To\nbetter understand this issue, we study the problem of continual learning, where\nthe model observes, once and one by one, examples concerning a sequence of\ntasks. First, we propose a set of metrics to evaluate models learning over a\ncontinuum of data. These metrics characterize models not only by their test\naccuracy, but also in terms of their ability to transfer knowledge across\ntasks. Second, we propose a model for continual learning, called Gradient\nEpisodic Memory (GEM) that alleviates forgetting, while allowing beneficial\ntransfer of knowledge to previous tasks. Our experiments on variants of the\nMNIST and CIFAR-100 datasets demonstrate the strong performance of GEM when\ncompared to the state-of-the-art.\n", "versions": [{"version": "v1", "created": "Mon, 26 Jun 2017 14:53:34 GMT"}, {"version": "v2", "created": "Wed, 28 Jun 2017 16:19:12 GMT"}, {"version": "v3", "created": "Wed, 12 Jul 2017 17:14:59 GMT"}, {"version": "v4", "created": "Wed, 2 Aug 2017 20:37:51 GMT"}, {"version": "v5", "created": "Sat, 4 Nov 2017 13:11:18 GMT"}], "update_date": "2017-11-07", "authors_parsed": [["Lopez-Paz", "David", ""], ["Ranzato", "Marc'Aurelio", ""]]}, {"id": "1706.08877", "submitter": "Davide Zordan", "authors": "Davide Zordan, Michele Rossi, Michele Zorzi", "title": "Rate-Distortion Classification for Self-Tuning IoT Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many future wireless sensor networks and the Internet of Things are expected\nto follow a software defined paradigm, where protocol parameters and behaviors\nwill be dynamically tuned as a function of the signal statistics. New protocols\nwill be then injected as a software as certain events occur. For instance, new\ndata compressors could be (re)programmed on-the-fly as the monitored signal\ntype or its statistical properties change. We consider a lossy compression\nscenario, where the application tolerates some distortion of the gathered\nsignal in return for improved energy efficiency. To reap the full benefits of\nthis paradigm, we discuss an automatic sensor profiling approach where the\nsignal class, and in particular the corresponding rate-distortion curve, is\nautomatically assessed using machine learning tools (namely, support vector\nmachines and neural networks). We show that this curve can be reliably\nestimated on-the-fly through the computation of a small number (from ten to\ntwenty) of statistical features on time windows of a few hundreds samples.\n", "versions": [{"version": "v1", "created": "Tue, 27 Jun 2017 14:20:08 GMT"}], "update_date": "2017-06-28", "authors_parsed": [["Zordan", "Davide", ""], ["Rossi", "Michele", ""], ["Zorzi", "Michele", ""]]}, {"id": "1706.08894", "submitter": "Mohamed Laib", "authors": "Mohamed Laib and Mikhail Kanevski", "title": "Unsupervised Feature Selection Based on Space Filling Concept", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The paper deals with the adaptation of a new measure for the unsupervised\nfeature selection problems. The proposed measure is based on space filling\nconcept and is called the coverage measure. This measure was used for judging\nthe quality of an experimental space filling design. In the present work, the\ncoverage measure is adapted for selecting the smallest informative subset of\nvariables by reducing redundancy in data. This paper proposes a simple analogy\nto apply this measure. It is implemented in a filter algorithm for unsupervised\nfeature selection problems.\n  The proposed filter algorithm is robust with high dimensional data and can be\nimplemented without extra parameters. Further, it is tested with simulated data\nand real world case studies including environmental data and hyperspectral\nimage. Finally, the results are evaluated by using random forest algorithm.\n", "versions": [{"version": "v1", "created": "Tue, 27 Jun 2017 14:48:39 GMT"}], "update_date": "2017-06-28", "authors_parsed": [["Laib", "Mohamed", ""], ["Kanevski", "Mikhail", ""]]}, {"id": "1706.08915", "submitter": "Daniele Bellutta", "authors": "Daniele Bellutta", "title": "The Fog of War: A Machine Learning Approach to Forecasting Weather on\n  Mars", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "astro-ph.IM cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For over a decade, scientists at NASA's Jet Propulsion Laboratory (JPL) have\nbeen recording measurements from the Martian surface as a part of the Mars\nExploration Rovers mission. One quantity of interest has been the opacity of\nMars's atmosphere for its importance in day-to-day estimations of the amount of\npower available to the rover from its solar arrays. This paper proposes the use\nof neural networks as a method for forecasting Martian atmospheric opacity that\nis more effective than the current empirical model. The more accurate\nprediction provided by these networks would allow operators at JPL to make more\naccurate predictions of the amount of energy available to the rover when they\nplan activities for coming sols.\n", "versions": [{"version": "v1", "created": "Mon, 26 Jun 2017 05:05:00 GMT"}], "update_date": "2017-06-28", "authors_parsed": [["Bellutta", "Daniele", ""]]}, {"id": "1706.08928", "submitter": "Syed Arefinul Haque", "authors": "Xindi Wang and Syed Arefinul Haque", "title": "Classical Music Clustering Based on Acoustic Features", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG cs.SD", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we cluster 330 classical music pieces collected from MusicNet\ndatabase based on their musical note sequence. We use shingling and chord\ntrajectory matrices to create signature for each music piece and performed\nspectral clustering to find the clusters. Based on different resolution, the\noutput clusters distinctively indicate composition from different classical\nmusic era and different composing style of the musicians.\n", "versions": [{"version": "v1", "created": "Tue, 27 Jun 2017 16:25:00 GMT"}], "update_date": "2017-06-28", "authors_parsed": [["Wang", "Xindi", ""], ["Haque", "Syed Arefinul", ""]]}, {"id": "1706.08934", "submitter": "Carlo Ciliberto", "authors": "Carlo Ciliberto, Dimitris Stamos and Massimiliano Pontil", "title": "Reexamining Low Rank Matrix Factorization for Trace Norm Regularization", "comments": "22 pages, 4 figures, 1 Table", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Trace norm regularization is a widely used approach for learning low rank\nmatrices. A standard optimization strategy is based on formulating the problem\nas one of low rank matrix factorization which, however, leads to a non-convex\nproblem. In practice this approach works well, and it is often computationally\nfaster than standard convex solvers such as proximal gradient methods.\nNevertheless, it is not guaranteed to converge to a global optimum, and the\noptimization can be trapped at poor stationary points. In this paper we show\nthat it is possible to characterize all critical points of the non-convex\nproblem. This allows us to provide an efficient criterion to determine whether\na critical point is also a global minimizer. Our analysis suggests an iterative\nmeta-algorithm that dynamically expands the parameter space and allows the\noptimization to escape any non-global critical point, thereby converging to a\nglobal minimizer. The algorithm can be applied to problems such as matrix\ncompletion or multitask learning, and our analysis holds for any random\ninitialization of the factor matrices. Finally, we confirm the good performance\nof the algorithm on synthetic and real datasets.\n", "versions": [{"version": "v1", "created": "Tue, 27 Jun 2017 16:43:31 GMT"}, {"version": "v2", "created": "Wed, 26 Jul 2017 12:04:31 GMT"}, {"version": "v3", "created": "Mon, 31 Jul 2017 08:51:17 GMT"}], "update_date": "2017-08-01", "authors_parsed": [["Ciliberto", "Carlo", ""], ["Stamos", "Dimitris", ""], ["Pontil", "Massimiliano", ""]]}, {"id": "1706.08947", "submitter": "Behnam Neyshabur", "authors": "Behnam Neyshabur, Srinadh Bhojanapalli, David McAllester, Nathan\n  Srebro", "title": "Exploring Generalization in Deep Learning", "comments": "19 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With a goal of understanding what drives generalization in deep networks, we\nconsider several recently suggested explanations, including norm-based control,\nsharpness and robustness. We study how these measures can ensure\ngeneralization, highlighting the importance of scale normalization, and making\na connection between sharpness and PAC-Bayes theory. We then investigate how\nwell the measures explain different observed phenomena.\n", "versions": [{"version": "v1", "created": "Tue, 27 Jun 2017 17:20:06 GMT"}, {"version": "v2", "created": "Thu, 6 Jul 2017 17:10:40 GMT"}], "update_date": "2017-07-07", "authors_parsed": [["Neyshabur", "Behnam", ""], ["Bhojanapalli", "Srinadh", ""], ["McAllester", "David", ""], ["Srebro", "Nathan", ""]]}, {"id": "1706.08948", "submitter": "Sambhav R. Jain", "authors": "Sambhav R. Jain, Kye Okabe", "title": "Training a Fully Convolutional Neural Network to Route Integrated\n  Circuits", "comments": "Code released. 8 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a deep, fully convolutional neural network that learns to route a\ncircuit layout net with appropriate choice of metal tracks and wire class\ncombinations. Inputs to the network are the encoded layouts containing spatial\nlocation of pins to be routed. After 15 fully convolutional stages followed by\na score comparator, the network outputs 8 layout layers (corresponding to 4\nroute layers, 3 via layers and an identity-mapped pin layer) which are then\ndecoded to obtain the routed layouts. We formulate this as a binary\nsegmentation problem on a per-pixel per-layer basis, where the network is\ntrained to correctly classify pixels in each layout layer to be 'on' or 'off'.\nTo demonstrate learnability of layout design rules, we train the network on a\ndataset of 50,000 train and 10,000 validation samples that we generate based on\ncertain pre-defined layout constraints. Precision, recall and $F_1$ score\nmetrics are used to track the training progress. Our network achieves\n$F_1\\approx97\\%$ on the train set and $F_1\\approx92\\%$ on the validation set.\nWe use PyTorch for implementing our model. Code is made publicly available at\nhttps://github.com/sjain-stanford/deep-route .\n", "versions": [{"version": "v1", "created": "Tue, 27 Jun 2017 17:20:21 GMT"}, {"version": "v2", "created": "Mon, 11 Sep 2017 18:37:04 GMT"}], "update_date": "2017-09-13", "authors_parsed": [["Jain", "Sambhav R.", ""], ["Okabe", "Kye", ""]]}, {"id": "1706.09059", "submitter": "Bernd Fritzke", "authors": "Bernd Fritzke", "title": "The k-means-u* algorithm: non-local jumps and greedy retries improve\n  k-means++ clustering", "comments": "submitted to JMLR (38 pages, 36 figures, 4 algorithms)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a new clustering algorithm called k-means-u* which in many cases\nis able to significantly improve the clusterings found by k-means++, the\ncurrent de-facto standard for clustering in Euclidean spaces. First we\nintroduce the k-means-u algorithm which starts from a result of k-means++ and\nattempts to improve it with a sequence of non-local \"jumps\" alternated by runs\nof standard k-means. Each jump transfers the \"least useful\" center towards the\ncenter with the largest local error, offset by a small random vector. This is\ncontinued as long as the error decreases and often leads to an improved\nsolution. Occasionally k-means-u terminates despite obvious remaining\noptimization possibilities. By allowing a limited number of retries for the\nlast jump it is frequently possible to reach better local minima. The resulting\nalgorithm is called k-means-u* and dominates k-means++ wrt. solution quality\nwhich is demonstrated empirically using various data sets. By construction the\nlogarithmic quality bound established for k-means++ holds for k-means-u* as\nwell.\n", "versions": [{"version": "v1", "created": "Tue, 27 Jun 2017 21:53:50 GMT"}, {"version": "v2", "created": "Sat, 15 Jul 2017 17:02:41 GMT"}], "update_date": "2017-07-18", "authors_parsed": [["Fritzke", "Bernd", ""]]}, {"id": "1706.09090", "submitter": "Huitian Lei", "authors": "Huitian Lei, Ambuj Tewari, Susan A. Murphy", "title": "An Actor-Critic Contextual Bandit Algorithm for Personalized Mobile\n  Health Interventions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Increasing technological sophistication and widespread use of smartphones and\nwearable devices provide opportunities for innovative and highly personalized\nhealth interventions. A Just-In-Time Adaptive Intervention (JITAI) uses\nreal-time data collection and communication capabilities of modern mobile\ndevices to deliver interventions in real-time that are adapted to the\nin-the-moment needs of the user. The lack of methodological guidance in\nconstructing data-based JITAIs remains a hurdle in advancing JITAI research\ndespite the increasing popularity of JITAIs among clinical scientists. In this\narticle, we make a first attempt to bridge this methodological gap by\nformulating the task of tailoring interventions in real-time as a contextual\nbandit problem. Interpretability requirements in the domain of mobile health\nlead us to formulate the problem differently from existing formulations\nintended for web applications such as ad or news article placement. Under the\nassumption of linear reward function, we choose the reward function (the\n\"critic\") parameterization separately from a lower dimensional parameterization\nof stochastic policies (the \"actor\"). We provide an online actor-critic\nalgorithm that guides the construction and refinement of a JITAI. Asymptotic\nproperties of the actor-critic algorithm are developed and backed up by\nnumerical experiments. Additional numerical experiments are conducted to test\nthe robustness of the algorithm when idealized assumptions used in the analysis\nof contextual bandit algorithm are breached.\n", "versions": [{"version": "v1", "created": "Wed, 28 Jun 2017 00:56:22 GMT"}], "update_date": "2017-06-29", "authors_parsed": [["Lei", "Huitian", ""], ["Tewari", "Ambuj", ""], ["Murphy", "Susan A.", ""]]}, {"id": "1706.09152", "submitter": "Wenhu Chen", "authors": "Wenhu Chen, Guanlin Li, Shuo Ren, Shujie Liu, Zhirui Zhang, Mu Li,\n  Ming Zhou", "title": "Generative Bridging Network in Neural Sequence Prediction", "comments": "Accepted to NAACL 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In order to alleviate data sparsity and overfitting problems in maximum\nlikelihood estimation (MLE) for sequence prediction tasks, we propose the\nGenerative Bridging Network (GBN), in which a novel bridge module is introduced\nto assist the training of the sequence prediction model (the generator\nnetwork). Unlike MLE directly maximizing the conditional likelihood, the bridge\nextends the point-wise ground truth to a bridge distribution conditioned on it,\nand the generator is optimized to minimize their KL-divergence. Three different\nGBNs, namely uniform GBN, language-model GBN and coaching GBN, are proposed to\npenalize confidence, enhance language smoothness and relieve learning burden.\nExperiments conducted on two recognized sequence prediction tasks (machine\ntranslation and abstractive text summarization) show that our proposed GBNs can\nyield significant improvements over strong baselines. Furthermore, by analyzing\nsamples drawn from different bridges, expected influences on the generator are\nverified.\n", "versions": [{"version": "v1", "created": "Wed, 28 Jun 2017 07:44:17 GMT"}, {"version": "v2", "created": "Sun, 13 Aug 2017 16:24:41 GMT"}, {"version": "v3", "created": "Sun, 20 Aug 2017 11:17:13 GMT"}, {"version": "v4", "created": "Tue, 31 Oct 2017 17:49:11 GMT"}, {"version": "v5", "created": "Sat, 17 Mar 2018 22:03:58 GMT"}, {"version": "v6", "created": "Thu, 29 Nov 2018 22:29:53 GMT"}], "update_date": "2018-12-03", "authors_parsed": [["Chen", "Wenhu", ""], ["Li", "Guanlin", ""], ["Ren", "Shuo", ""], ["Liu", "Shujie", ""], ["Zhang", "Zhirui", ""], ["Li", "Mu", ""], ["Zhou", "Ming", ""]]}, {"id": "1706.09186", "submitter": "Claire Vernade", "authors": "Claire Vernade, Olivier Capp\\'e, Vianney Perchet", "title": "Stochastic Bandit Models for Delayed Conversions", "comments": "Conference on Uncertainty in Artificial Intelligence, Aug 2017,\n  Sydney, Australia", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Online advertising and product recommendation are important domains of\napplications for multi-armed bandit methods. In these fields, the reward that\nis immediately available is most often only a proxy for the actual outcome of\ninterest, which we refer to as a conversion. For instance, in web advertising,\nclicks can be observed within a few seconds after an ad display but the\ncorresponding sale --if any-- will take hours, if not days to happen. This\npaper proposes and investigates a new stochas-tic multi-armed bandit model in\nthe framework proposed by Chapelle (2014) --based on empirical studies in the\nfield of web advertising-- in which each action may trigger a future reward\nthat will then happen with a stochas-tic delay. We assume that the probability\nof conversion associated with each action is unknown while the distribution of\nthe conversion delay is known, distinguishing between the (idealized) case\nwhere the conversion events may be observed whatever their delay and the more\nrealistic setting in which late conversions are censored. We provide\nperformance lower bounds as well as two simple but efficient algorithms based\non the UCB and KLUCB frameworks. The latter algorithm, which is preferable when\nconversion rates are low, is based on a Poissonization argument, of independent\ninterest in other settings where aggregation of Bernoulli observations with\ndifferent success probabilities is required.\n", "versions": [{"version": "v1", "created": "Wed, 28 Jun 2017 09:43:46 GMT"}, {"version": "v2", "created": "Wed, 5 Jul 2017 10:51:21 GMT"}, {"version": "v3", "created": "Wed, 12 Jul 2017 09:12:56 GMT"}], "update_date": "2017-07-13", "authors_parsed": [["Vernade", "Claire", ""], ["Capp\u00e9", "Olivier", ""], ["Perchet", "Vianney", ""]]}, {"id": "1706.09200", "submitter": "Jaeyoon Yoo", "authors": "Jaeyoon Yoo, Heonseok Ha, Jihun Yi, Jongha Ryu, Chanju Kim, Jung-Woo\n  Ha, Young-Han Kim, and Sungroh Yoon", "title": "Energy-Based Sequence GANs for Recommendation and Their Connection to\n  Imitation Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recommender systems aim to find an accurate and efficient mapping from\nhistoric data of user-preferred items to a new item that is to be liked by a\nuser. Towards this goal, energy-based sequence generative adversarial nets\n(EB-SeqGANs) are adopted for recommendation by learning a generative model for\nthe time series of user-preferred items. By recasting the energy function as\nthe feature function, the proposed EB-SeqGANs is interpreted as an instance of\nmaximum-entropy imitation learning.\n", "versions": [{"version": "v1", "created": "Wed, 28 Jun 2017 10:12:01 GMT"}], "update_date": "2017-06-29", "authors_parsed": [["Yoo", "Jaeyoon", ""], ["Ha", "Heonseok", ""], ["Yi", "Jihun", ""], ["Ryu", "Jongha", ""], ["Kim", "Chanju", ""], ["Ha", "Jung-Woo", ""], ["Kim", "Young-Han", ""], ["Yoon", "Sungroh", ""]]}, {"id": "1706.09249", "submitter": "Michael Veale", "authors": "Michael Veale", "title": "Logics and practices of transparency and opacity in real-world\n  applications of public sector machine learning", "comments": "5 pages, 0 figures, presented as a talk at the 2017 Workshop on\n  Fairness, Accountability, and Transparency in Machine Learning (FAT/ML 2017),\n  Halifax, Canada, August 14, 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Machine learning systems are increasingly used to support public sector\ndecision-making across a variety of sectors. Given concerns around\naccountability in these domains, and amidst accusations of intentional or\nunintentional bias, there have been increased calls for transparency of these\ntechnologies. Few, however, have considered how logics and practices concerning\ntransparency have been understood by those involved in the machine learning\nsystems already being piloted and deployed in public bodies today. This short\npaper distils insights about transparency on the ground from interviews with 27\nsuch actors, largely public servants and relevant contractors, across 5 OECD\ncountries. Considering transparency and opacity in relation to trust and\nbuy-in, better decision-making, and the avoidance of gaming, it seeks to\nprovide useful insights for those hoping to develop socio-technical approaches\nto transparency that might be useful to practitioners on-the-ground.\n  An extended, archival version of this paper is available as Veale M., Van\nKleek M., & Binns R. (2018). `Fairness and accountability design needs for\nalgorithmic support in high-stakes public sector decision-making' Proceedings\nof the 2018 CHI Conference on Human Factors in Computing Systems (CHI'18),\nhttp://doi.org/10.1145/3173574.3174014.\n", "versions": [{"version": "v1", "created": "Mon, 19 Jun 2017 16:32:20 GMT"}, {"version": "v2", "created": "Thu, 6 Jul 2017 08:46:37 GMT"}, {"version": "v3", "created": "Mon, 5 Nov 2018 09:56:12 GMT"}], "update_date": "2018-11-06", "authors_parsed": [["Veale", "Michael", ""]]}, {"id": "1706.09293", "submitter": "James Ridgway", "authors": "Pierre Alquier and James Ridgway", "title": "Concentration of tempered posteriors and of their variational\n  approximations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.LG stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While Bayesian methods are extremely popular in statistics and machine\nlearning, their application to massive datasets is often challenging, when\npossible at all. Indeed, the classical MCMC algorithms are prohibitively slow\nwhen both the model dimension and the sample size are large. Variational\nBayesian methods aim at approximating the posterior by a distribution in a\ntractable family. Thus, MCMC are replaced by an optimization algorithm which is\norders of magnitude faster. VB methods have been applied in such\ncomputationally demanding applications as including collaborative filtering,\nimage and video processing, NLP and text processing... However, despite very\nnice results in practice, the theoretical properties of these approximations\nare usually not known. In this paper, we propose a general approach to prove\nthe concentration of variational approximations of fractional posteriors. We\napply our theory to two examples: matrix completion, and Gaussian VB.\n", "versions": [{"version": "v1", "created": "Wed, 28 Jun 2017 13:58:56 GMT"}, {"version": "v2", "created": "Thu, 7 Jun 2018 17:27:01 GMT"}, {"version": "v3", "created": "Mon, 22 Apr 2019 08:30:28 GMT"}], "update_date": "2019-04-23", "authors_parsed": [["Alquier", "Pierre", ""], ["Ridgway", "James", ""]]}, {"id": "1706.09317", "submitter": "Qian Wang", "authors": "Qian Wang and Ke Chen", "title": "Alternative Semantic Representations for Zero-Shot Human Action\n  Recognition", "comments": "Technical Report, School of Computer Science, The University of\n  Manchester, Accepted to ECML-PKDD 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.IR cs.LG cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A proper semantic representation for encoding side information is key to the\nsuccess of zero-shot learning. In this paper, we explore two alternative\nsemantic representations especially for zero-shot human action recognition:\ntextual descriptions of human actions and deep features extracted from still\nimages relevant to human actions. Such side information are accessible on Web\nwith little cost, which paves a new way in gaining side information for\nlarge-scale zero-shot human action recognition. We investigate different\nencoding methods to generate semantic representations for human actions from\nsuch side information. Based on our zero-shot visual recognition method, we\nconducted experiments on UCF101 and HMDB51 to evaluate two proposed semantic\nrepresentations . The results suggest that our proposed text- and image-based\nsemantic representations outperform traditional attributes and word vectors\nconsiderably for zero-shot human action recognition. In particular, the\nimage-based semantic representations yield the favourable performance even\nthough the representation is extracted from a small number of images per class.\n", "versions": [{"version": "v1", "created": "Wed, 28 Jun 2017 14:32:57 GMT"}], "update_date": "2017-06-29", "authors_parsed": [["Wang", "Qian", ""], ["Chen", "Ke", ""]]}, {"id": "1706.09318", "submitter": "Jaemin Son", "authors": "Jaemin Son, Sang Jun Park, and Kyu-Hwan Jung", "title": "Retinal Vessel Segmentation in Fundoscopic Images with Generative\n  Adversarial Networks", "comments": "9 pages, submitted to DLMIA 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Retinal vessel segmentation is an indispensable step for automatic detection\nof retinal diseases with fundoscopic images. Though many approaches have been\nproposed, existing methods tend to miss fine vessels or allow false positives\nat terminal branches. Let alone under-segmentation, over-segmentation is also\nproblematic when quantitative studies need to measure the precise width of\nvessels. In this paper, we present a method that generates the precise map of\nretinal vessels using generative adversarial training. Our methods achieve dice\ncoefficient of 0.829 on DRIVE dataset and 0.834 on STARE dataset which is the\nstate-of-the-art performance on both datasets.\n", "versions": [{"version": "v1", "created": "Wed, 28 Jun 2017 14:33:22 GMT"}], "update_date": "2017-07-03", "authors_parsed": [["Son", "Jaemin", ""], ["Park", "Sang Jun", ""], ["Jung", "Kyu-Hwan", ""]]}, {"id": "1706.09367", "submitter": "F\\'abio Pinto", "authors": "F\\'abio Pinto, V\\'itor Cerqueira, Carlos Soares, Jo\\~ao Mendes-Moreira", "title": "autoBagging: Learning to Rank Bagging Workflows with Metalearning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Machine Learning (ML) has been successfully applied to a wide range of\ndomains and applications. One of the techniques behind most of these successful\napplications is Ensemble Learning (EL), the field of ML that gave birth to\nmethods such as Random Forests or Boosting. The complexity of applying these\ntechniques together with the market scarcity on ML experts, has created the\nneed for systems that enable a fast and easy drop-in replacement for ML\nlibraries. Automated machine learning (autoML) is the field of ML that attempts\nto answers these needs. Typically, these systems rely on optimization\ntechniques such as bayesian optimization to lead the search for the best model.\nOur approach differs from these systems by making use of the most recent\nadvances on metalearning and a learning to rank approach to learn from\nmetadata. We propose autoBagging, an autoML system that automatically ranks 63\nbagging workflows by exploiting past performance and dataset characterization.\nResults on 140 classification datasets from the OpenML platform show that\nautoBagging can yield better performance than the Average Rank method and\nachieve results that are not statistically different from an ideal model that\nsystematically selects the best workflow for each dataset. For the purpose of\nreproducibility and generalizability, autoBagging is publicly available as an R\npackage on CRAN.\n", "versions": [{"version": "v1", "created": "Wed, 28 Jun 2017 17:13:47 GMT"}], "update_date": "2017-06-29", "authors_parsed": [["Pinto", "F\u00e1bio", ""], ["Cerqueira", "V\u00edtor", ""], ["Soares", "Carlos", ""], ["Mendes-Moreira", "Jo\u00e3o", ""]]}, {"id": "1706.09382", "submitter": "Sarah Marzen", "authors": "Sarah Marzen", "title": "The difference between memory and prediction in linear recurrent\n  networks", "comments": null, "journal-ref": "Phys. Rev. E 96, 032308 (2017)", "doi": "10.1103/PhysRevE.96.032308", "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recurrent networks are trained to memorize their input better, often in the\nhopes that such training will increase the ability of the network to predict.\nWe show that networks designed to memorize input can be arbitrarily bad at\nprediction. We also find, for several types of inputs, that one-node networks\noptimized for prediction are nearly at upper bounds on predictive capacity\ngiven by Wiener filters, and are roughly equivalent in performance to randomly\ngenerated five-node networks. Our results suggest that maximizing memory\ncapacity leads to very different networks than maximizing predictive capacity,\nand that optimizing recurrent weights can decrease reservoir size by half an\norder of magnitude.\n", "versions": [{"version": "v1", "created": "Mon, 26 Jun 2017 19:45:03 GMT"}, {"version": "v2", "created": "Mon, 14 Aug 2017 15:54:00 GMT"}], "update_date": "2017-09-20", "authors_parsed": [["Marzen", "Sarah", ""]]}, {"id": "1706.09395", "submitter": "Amirhossein Javaheri", "authors": "Amirhossein Javaheri, Hadi Zayyani, Farokh Marvasti", "title": "Recovery of Missing Samples Using Sparse Approximation via a Convex\n  Similarity Measure", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we study the missing sample recovery problem using methods\nbased on sparse approximation. In this regard, we investigate the algorithms\nused for solving the inverse problem associated with the restoration of missed\nsamples of image signal. This problem is also known as inpainting in the\ncontext of image processing and for this purpose, we suggest an iterative\nsparse recovery algorithm based on constrained $l_1$-norm minimization with a\nnew fidelity metric. The proposed metric called Convex SIMilarity (CSIM) index,\nis a simplified version of the Structural SIMilarity (SSIM) index, which is\nconvex and error-sensitive. The optimization problem incorporating this\ncriterion, is then solved via Alternating Direction Method of Multipliers\n(ADMM). Simulation results show the efficiency of the proposed method for\nmissing sample recovery of 1D patch vectors and inpainting of 2D image signals.\n", "versions": [{"version": "v1", "created": "Wed, 28 Jun 2017 17:59:33 GMT"}], "update_date": "2017-06-29", "authors_parsed": [["Javaheri", "Amirhossein", ""], ["Zayyani", "Hadi", ""], ["Marvasti", "Farokh", ""]]}, {"id": "1706.09516", "submitter": "Aleksandr Vorobev", "authors": "Liudmila Prokhorenkova, Gleb Gusev, Aleksandr Vorobev, Anna Veronika\n  Dorogush, Andrey Gulin", "title": "CatBoost: unbiased boosting with categorical features", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents the key algorithmic techniques behind CatBoost, a new\ngradient boosting toolkit. Their combination leads to CatBoost outperforming\nother publicly available boosting implementations in terms of quality on a\nvariety of datasets. Two critical algorithmic advances introduced in CatBoost\nare the implementation of ordered boosting, a permutation-driven alternative to\nthe classic algorithm, and an innovative algorithm for processing categorical\nfeatures. Both techniques were created to fight a prediction shift caused by a\nspecial kind of target leakage present in all currently existing\nimplementations of gradient boosting algorithms. In this paper, we provide a\ndetailed analysis of this problem and demonstrate that proposed algorithms\nsolve it effectively, leading to excellent empirical results.\n", "versions": [{"version": "v1", "created": "Wed, 28 Jun 2017 23:54:25 GMT"}, {"version": "v2", "created": "Mon, 23 Apr 2018 13:20:18 GMT"}, {"version": "v3", "created": "Tue, 11 Sep 2018 08:12:56 GMT"}, {"version": "v4", "created": "Wed, 31 Oct 2018 23:34:42 GMT"}, {"version": "v5", "created": "Sun, 20 Jan 2019 20:07:28 GMT"}], "update_date": "2019-01-23", "authors_parsed": [["Prokhorenkova", "Liudmila", ""], ["Gusev", "Gleb", ""], ["Vorobev", "Aleksandr", ""], ["Dorogush", "Anna Veronika", ""], ["Gulin", "Andrey", ""]]}, {"id": "1706.09520", "submitter": "Jingwei Zhang", "authors": "Jingwei Zhang, Lei Tai, Ming Liu, Joschka Boedecker, Wolfram Burgard", "title": "Neural SLAM: Learning to Explore with External Memory", "comments": "A video of our experiments can be found at: https://goo.gl/G2Vu5y", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an approach for agents to learn representations of a global map\nfrom sensor data, to aid their exploration in new environments. To achieve\nthis, we embed procedures mimicking that of traditional Simultaneous\nLocalization and Mapping (SLAM) into the soft attention based addressing of\nexternal memory architectures, in which the external memory acts as an internal\nrepresentation of the environment. This structure encourages the evolution of\nSLAM-like behaviors inside a completely differentiable deep neural network. We\nshow that this approach can help reinforcement learning agents to successfully\nexplore new environments where long-term memory is essential. We validate our\napproach in both challenging grid-world environments and preliminary Gazebo\nexperiments. A video of our experiments can be found at: https://goo.gl/G2Vu5y.\n", "versions": [{"version": "v1", "created": "Thu, 29 Jun 2017 00:14:15 GMT"}, {"version": "v2", "created": "Mon, 3 Jul 2017 09:07:06 GMT"}, {"version": "v3", "created": "Tue, 4 Jul 2017 08:55:44 GMT"}, {"version": "v4", "created": "Wed, 5 Jul 2017 09:23:05 GMT"}, {"version": "v5", "created": "Mon, 20 Nov 2017 10:16:03 GMT"}, {"version": "v6", "created": "Wed, 29 Nov 2017 11:40:51 GMT"}, {"version": "v7", "created": "Wed, 30 Dec 2020 13:57:28 GMT"}], "update_date": "2021-01-01", "authors_parsed": [["Zhang", "Jingwei", ""], ["Tai", "Lei", ""], ["Liu", "Ming", ""], ["Boedecker", "Joschka", ""], ["Burgard", "Wolfram", ""]]}, {"id": "1706.09529", "submitter": "Yongxin Yang", "authors": "Flood Sung, Li Zhang, Tao Xiang, Timothy Hospedales, Yongxin Yang", "title": "Learning to Learn: Meta-Critic Networks for Sample Efficient Learning", "comments": "Technical report, 12 pages, 3 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel and flexible approach to meta-learning for\nlearning-to-learn from only a few examples. Our framework is motivated by\nactor-critic reinforcement learning, but can be applied to both reinforcement\nand supervised learning. The key idea is to learn a meta-critic: an\naction-value function neural network that learns to criticise any actor trying\nto solve any specified task. For supervised learning, this corresponds to the\nnovel idea of a trainable task-parametrised loss generator. This meta-critic\napproach provides a route to knowledge transfer that can flexibly deal with\nfew-shot and semi-supervised conditions for both reinforcement and supervised\nlearning. Promising results are shown on both reinforcement and supervised\nlearning problems.\n", "versions": [{"version": "v1", "created": "Thu, 29 Jun 2017 00:54:47 GMT"}], "update_date": "2017-06-30", "authors_parsed": [["Sung", "Flood", ""], ["Zhang", "Li", ""], ["Xiang", "Tao", ""], ["Hospedales", "Timothy", ""], ["Yang", "Yongxin", ""]]}, {"id": "1706.09549", "submitter": "Chengtao Li", "authors": "Chengtao Li, David Alvarez-Melis, Keyulu Xu, Stefanie Jegelka, Suvrit\n  Sra", "title": "Distributional Adversarial Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a framework for adversarial training that relies on a sample\nrather than a single sample point as the fundamental unit of discrimination.\nInspired by discrepancy measures and two-sample tests between probability\ndistributions, we propose two such distributional adversaries that operate and\npredict on samples, and show how they can be easily implemented on top of\nexisting models. Various experimental results show that generators trained with\nour distributional adversaries are much more stable and are remarkably less\nprone to mode collapse than traditional models trained with pointwise\nprediction discriminators. The application of our framework to domain\nadaptation also results in considerable improvement over recent\nstate-of-the-art.\n", "versions": [{"version": "v1", "created": "Thu, 29 Jun 2017 02:33:12 GMT"}, {"version": "v2", "created": "Sun, 2 Jul 2017 05:07:53 GMT"}, {"version": "v3", "created": "Sun, 9 Jul 2017 15:47:29 GMT"}], "update_date": "2017-07-11", "authors_parsed": [["Li", "Chengtao", ""], ["Alvarez-Melis", "David", ""], ["Xu", "Keyulu", ""], ["Jegelka", "Stefanie", ""], ["Sra", "Suvrit", ""]]}, {"id": "1706.09553", "submitter": "Shija Geng", "authors": "S. Geng, G. Ren, M. Ogihara", "title": "Transforming Musical Signals through a Genre Classifying Convolutional\n  Neural Network", "comments": "Proceedings of the First International Conference on Deep Learning\n  and Music, Anchorage, US, May, 2017 (arXiv:1706.08675v1 [cs.NE])", "journal-ref": "Proc. of the First Int. Workshop on Deep Learning and Music joint\n  with IJCNN. Anchorage, US. 1(1). pp 48-49 (2017)", "doi": null, "report-no": "DLM/2017/4", "categories": "cs.SD cs.LG cs.MM cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolutional neural networks (CNNs) have been successfully applied on both\ndiscriminative and generative modeling for music-related tasks. For a\nparticular task, the trained CNN contains information representing the decision\nmaking or the abstracting process. One can hope to manipulate existing music\nbased on this 'informed' network and create music with new features\ncorresponding to the knowledge obtained by the network. In this paper, we\npropose a method to utilize the stored information from a CNN trained on\nmusical genre classification task. The network was composed of three\nconvolutional layers, and was trained to classify five-second song clips into\nfive different genres. After training, randomly selected clips were modified by\nmaximizing the sum of outputs from the network layers. In addition to the\npotential of such CNNs to produce interesting audio transformation, more\ninformation about the network and the original music could be obtained from the\nanalysis of the generated features since these features indicate how the\nnetwork 'understands' the music.\n", "versions": [{"version": "v1", "created": "Thu, 29 Jun 2017 02:39:00 GMT"}], "update_date": "2017-06-30", "authors_parsed": [["Geng", "S.", ""], ["Ren", "G.", ""], ["Ogihara", "M.", ""]]}, {"id": "1706.09555", "submitter": "Zhe-Cheng Fan", "authors": "Z.C. Fan, T.S. Chan, Y.H. Yang, and J.S. R. Jang", "title": "Music Signal Processing Using Vector Product Neural Networks", "comments": "Proceedings of the First International Conference on Deep Learning\n  and Music, Anchorage, US, May, 2017 (arXiv:1706.08675v1 [cs.NE])", "journal-ref": "Proc. of the First Int. Workshop on Deep Learning and Music joint\n  with IJCNN. Anchorage, US. 1(1). pp 36-30 (2017)", "doi": null, "report-no": "DLM/2017/5", "categories": "cs.SD cs.LG cs.MM cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel neural network model for music signal processing using\nvector product neurons and dimensionality transformations. Here, the inputs are\nfirst mapped from real values into three-dimensional vectors then fed into a\nthree-dimensional vector product neural network where the inputs, outputs, and\nweights are all three-dimensional values. Next, the final outputs are mapped\nback to the reals. Two methods for dimensionality transformation are proposed,\none via context windows and the other via spectral coloring. Experimental\nresults on the iKala dataset for blind singing voice separation confirm the\nefficacy of our model.\n", "versions": [{"version": "v1", "created": "Thu, 29 Jun 2017 02:41:30 GMT"}], "update_date": "2017-06-30", "authors_parsed": [["Fan", "Z. C.", ""], ["Chan", "T. S.", ""], ["Yang", "Y. H.", ""], ["Jang", "J. S. R.", ""]]}, {"id": "1706.09557", "submitter": "Carmine Cella", "authors": "C.E. Cella", "title": "Machine listening intelligence", "comments": "Proceedings of the First International Conference on Deep Learning\n  and Music, Anchorage, US, May, 2017 (arXiv:1706.08675v1 [cs.NE])", "journal-ref": "Proceedings of the First International Workshop on Deep Learning\n  and Music joint with IJCNN. Anchorage, US. 1(1). pp 50-55 (2017)", "doi": null, "report-no": "DLM/2017/6", "categories": "cs.SD cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This manifesto paper will introduce machine listening intelligence, an\nintegrated research framework for acoustic and musical signals modelling, based\non signal processing, deep learning and computational musicology.\n", "versions": [{"version": "v1", "created": "Thu, 29 Jun 2017 02:52:25 GMT"}], "update_date": "2017-06-30", "authors_parsed": [["Cella", "C. E.", ""]]}, {"id": "1706.09559", "submitter": "Lonce Wyse", "authors": "L. Wyse", "title": "Audio Spectrogram Representations for Processing with Convolutional\n  Neural Networks", "comments": "Proceedings of the First International Conference on Deep Learning\n  and Music, Anchorage, US, May, 2017 (arXiv:1706.08675v1 [cs.NE])", "journal-ref": "Proceedings of the First International Workshop on Deep Learning\n  and Music joint with IJCNN. Anchorage, US. 1(1). pp 37-41 (2017)", "doi": null, "report-no": "DLM/2017/9", "categories": "cs.SD cs.LG cs.MM cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One of the decisions that arise when designing a neural network for any\napplication is how the data should be represented in order to be presented to,\nand possibly generated by, a neural network. For audio, the choice is less\nobvious than it seems to be for visual images, and a variety of representations\nhave been used for different applications including the raw digitized sample\nstream, hand-crafted features, machine discovered features, MFCCs and variants\nthat include deltas, and a variety of spectral representations. This paper\nreviews some of these representations and issues that arise, focusing\nparticularly on spectrograms for generating audio using neural networks for\nstyle transfer.\n", "versions": [{"version": "v1", "created": "Thu, 29 Jun 2017 03:04:06 GMT"}], "update_date": "2017-06-30", "authors_parsed": [["Wyse", "L.", ""]]}, {"id": "1706.09563", "submitter": "Brendt Wohlberg", "authors": "Jialin Liu and Cristina Garcia-Cardona and Brendt Wohlberg and Wotao\n  Yin", "title": "Online Convolutional Dictionary Learning", "comments": "Accepted to be presented at ICIP 2017", "journal-ref": "Proceedings of IEEE International Conference on Image Processing\n  (ICIP), 2017, pp. 1707-1711", "doi": "10.1109/ICIP.2017.8296573", "report-no": null, "categories": "cs.LG cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While a number of different algorithms have recently been proposed for\nconvolutional dictionary learning, this remains an expensive problem. The\nsingle biggest impediment to learning from large training sets is the memory\nrequirements, which grow at least linearly with the size of the training set\nsince all existing methods are batch algorithms. The work reported here\naddresses this limitation by extending online dictionary learning ideas to the\nconvolutional context.\n", "versions": [{"version": "v1", "created": "Thu, 29 Jun 2017 03:25:32 GMT"}, {"version": "v2", "created": "Wed, 30 Aug 2017 20:14:46 GMT"}], "update_date": "2018-02-26", "authors_parsed": [["Liu", "Jialin", ""], ["Garcia-Cardona", "Cristina", ""], ["Wohlberg", "Brendt", ""], ["Yin", "Wotao", ""]]}, {"id": "1706.09585", "submitter": "Subhadip Mukherjee", "authors": "Subhadip Mukherjee, Deepak R., Huaijin Chen, Ashok Veeraraghavan, and\n  Chandra Sekhar Seelamantula", "title": "Online Reweighted Least Squares Algorithm for Sparse Recovery and\n  Application to Short-Wave Infrared Imaging", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We address the problem of sparse recovery in an online setting, where random\nlinear measurements of a sparse signal are revealed sequentially and the\nobjective is to recover the underlying signal. We propose a reweighted least\nsquares (RLS) algorithm to solve the problem of online sparse reconstruction,\nwherein a system of linear equations is solved using conjugate gradient with\nthe arrival of every new measurement. The proposed online algorithm is useful\nin a setting where one seeks to design a progressive decoding strategy to\nreconstruct a sparse signal from linear measurements so that one does not have\nto wait until all measurements are acquired. Moreover, the proposed algorithm\nis also useful in applications where it is infeasible to process all the\nmeasurements using a batch algorithm, owing to computational and storage\nconstraints. It is not needed a priori to collect a fixed number of\nmeasurements; rather one can keep collecting measurements until the quality of\nreconstruction is satisfactory and stop taking further measurements once the\nreconstruction is sufficiently accurate. We provide a proof-of-concept by\ncomparing the performance of our algorithm with the RLS-based batch\nreconstruction strategy, known as iteratively reweighted least squares (IRLS),\non natural images. Experiments on a recently proposed focal plane array-based\nimaging setup show up to 1 dB improvement in output peak signal-to-noise ratio\nas compared with the total variation-based reconstruction.\n", "versions": [{"version": "v1", "created": "Thu, 29 Jun 2017 05:53:39 GMT"}], "update_date": "2017-06-30", "authors_parsed": [["Mukherjee", "Subhadip", ""], ["R.", "Deepak", ""], ["Chen", "Huaijin", ""], ["Veeraraghavan", "Ashok", ""], ["Seelamantula", "Chandra Sekhar", ""]]}, {"id": "1706.09627", "submitter": "Giancarlo Nicola", "authors": "Paola Cerchiello, Giancarlo Nicola, Samuel Ronnqvist, Peter Sarlin", "title": "Deep learning bank distress from news and numerical financial data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we focus our attention on the exploitation of the information\ncontained in financial news to enhance the performance of a classifier of bank\ndistress. Such information should be analyzed and inserted into the predictive\nmodel in the most efficient way and this task deals with all the issues related\nto text analysis and specifically analysis of news media. Among the different\nmodels proposed for such purpose, we investigate one of the possible deep\nlearning approaches, based on a doc2vec representation of the textual data, a\nkind of neural network able to map the sequential and symbolic text input onto\na reduced latent semantic space. Afterwards, a second supervised neural network\nis trained combining news data with standard financial figures to classify\nbanks whether in distressed or tranquil states, based on a small set of known\ndistress events. Then the final aim is not only the improvement of the\npredictive performance of the classifier but also to assess the importance of\nnews data in the classification process. Does news data really bring more\nuseful information not contained in standard financial variables? Our results\nseem to confirm such hypothesis.\n", "versions": [{"version": "v1", "created": "Thu, 29 Jun 2017 08:42:44 GMT"}, {"version": "v2", "created": "Tue, 16 Jan 2018 18:06:14 GMT"}, {"version": "v3", "created": "Wed, 5 Sep 2018 09:08:43 GMT"}], "update_date": "2018-09-06", "authors_parsed": [["Cerchiello", "Paola", ""], ["Nicola", "Giancarlo", ""], ["Ronnqvist", "Samuel", ""], ["Sarlin", "Peter", ""]]}, {"id": "1706.09693", "submitter": "Elizabeth Newman", "authors": "Elizabeth Newman, Misha Kilmer, and Lior Horesh", "title": "Image classification using local tensor singular value decompositions", "comments": "Submitted to IEEE CAMSAP 2017 Conference, 5 pages, 9 figures and\n  tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  From linear classifiers to neural networks, image classification has been a\nwidely explored topic in mathematics, and many algorithms have proven to be\neffective classifiers. However, the most accurate classifiers typically have\nsignificantly high storage costs, or require complicated procedures that may be\ncomputationally expensive. We present a novel (nonlinear) classification\napproach using truncation of local tensor singular value decompositions (tSVD)\nthat robustly offers accurate results, while maintaining manageable storage\ncosts. Our approach takes advantage of the optimality of the representation\nunder the tensor algebra described to determine to which class an image\nbelongs. We extend our approach to a method that can determine specific\npairwise match scores, which could be useful in, for example, object\nrecognition problems where pose/position are different. We demonstrate the\npromise of our new techniques on the MNIST data set.\n", "versions": [{"version": "v1", "created": "Thu, 29 Jun 2017 11:45:54 GMT"}], "update_date": "2017-06-30", "authors_parsed": [["Newman", "Elizabeth", ""], ["Kilmer", "Misha", ""], ["Horesh", "Lior", ""]]}, {"id": "1706.09739", "submitter": "Sergio Oramas", "authors": "Sergio Oramas, Oriol Nieto, Mohamed Sordo, Xavier Serra", "title": "A Deep Multimodal Approach for Cold-start Music Recommendation", "comments": "In Proceedings of the 2nd Workshop on Deep Learning for Recommender\n  Systems (DLRS 2017), collocated with RecSys 2017", "journal-ref": null, "doi": "10.1145/3125486.3125492", "report-no": null, "categories": "cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An increasing amount of digital music is being published daily. Music\nstreaming services often ingest all available music, but this poses a\nchallenge: how to recommend new artists for which prior knowledge is scarce? In\nthis work we aim to address this so-called cold-start problem by combining text\nand audio information with user feedback data using deep network architectures.\nOur method is divided into three steps. First, artist embeddings are learned\nfrom biographies by combining semantics, text features, and aggregated usage\ndata. Second, track embeddings are learned from the audio signal and available\nfeedback data. Finally, artist and track embeddings are combined in a\nmultimodal network. Results suggest that both splitting the recommendation\nproblem between feature levels (i.e., artist metadata and audio track), and\nmerging feature embeddings in a multimodal approach improve the accuracy of the\nrecommendations.\n", "versions": [{"version": "v1", "created": "Thu, 29 Jun 2017 13:13:26 GMT"}, {"version": "v2", "created": "Mon, 24 Jul 2017 08:07:28 GMT"}], "update_date": "2017-07-25", "authors_parsed": [["Oramas", "Sergio", ""], ["Nieto", "Oriol", ""], ["Sordo", "Mohamed", ""], ["Serra", "Xavier", ""]]}, {"id": "1706.09763", "submitter": "Robin Nicole Mr", "authors": "Robin Nicole and Peter Sollich", "title": "Dynamical selection of Nash equilibria using Experience Weighted\n  Attraction Learning: emergence of heterogeneous mixed equilibria", "comments": "35 pages, 16 figures", "journal-ref": null, "doi": "10.1371/journal.pone.0196577", "report-no": null, "categories": "cs.GT cond-mat.stat-mech cs.LG q-fin.EC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the distribution of strategies in a large game that models how\nagents choose among different double auction markets. We classify the possible\nmean field Nash equilibria, which include potentially segregated states where\nan agent population can split into subpopulations adopting different\nstrategies. As the game is aggregative, the actual equilibrium strategy\ndistributions remain undetermined, however. We therefore compare with the\nresults of Experience-Weighted Attraction (EWA) learning, which at long times\nleads to Nash equilibria in the appropriate limits of large intensity of\nchoice, low noise (long agent memory) and perfect imputation of missing scores\n(fictitious play). The learning dynamics breaks the indeterminacy of the Nash\nequilibria. Non-trivially, depending on how the relevant limits are taken, more\nthan one type of equilibrium can be selected. These include the standard\nhomogeneous mixed and heterogeneous pure states, but also \\emph{heterogeneous\nmixed} states where different agents play different strategies that are not all\npure. The analysis of the EWA learning involves Fokker-Planck modeling combined\nwith large deviation methods. The theoretical results are confirmed by\nmulti-agent simulations.\n", "versions": [{"version": "v1", "created": "Thu, 29 Jun 2017 14:02:04 GMT"}], "update_date": "2018-09-05", "authors_parsed": [["Nicole", "Robin", ""], ["Sollich", "Peter", ""]]}, {"id": "1706.09773", "submitter": "Osbert Bastani", "authors": "Osbert Bastani and Carolyn Kim and Hamsa Bastani", "title": "Interpretability via Model Extraction", "comments": "Presented as a poster at the 2017 Workshop on Fairness,\n  Accountability, and Transparency in Machine Learning (FAT/ML 2017)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CY stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The ability to interpret machine learning models has become increasingly\nimportant now that machine learning is used to inform consequential decisions.\nWe propose an approach called model extraction for interpreting complex,\nblackbox models. Our approach approximates the complex model using a much more\ninterpretable model; as long as the approximation quality is good, then\nstatistical properties of the complex model are reflected in the interpretable\nmodel. We show how model extraction can be used to understand and debug random\nforests and neural nets trained on several datasets from the UCI Machine\nLearning Repository, as well as control policies learned for several classical\nreinforcement learning problems.\n", "versions": [{"version": "v1", "created": "Thu, 29 Jun 2017 14:30:40 GMT"}, {"version": "v2", "created": "Fri, 30 Jun 2017 02:02:44 GMT"}, {"version": "v3", "created": "Sun, 11 Mar 2018 02:08:24 GMT"}, {"version": "v4", "created": "Tue, 13 Mar 2018 00:56:59 GMT"}], "update_date": "2018-03-14", "authors_parsed": [["Bastani", "Osbert", ""], ["Kim", "Carolyn", ""], ["Bastani", "Hamsa", ""]]}, {"id": "1706.09795", "submitter": "Sophie Jan", "authors": "Nicolas Couellan and Sophie Jan", "title": "Feature uncertainty bounding schemes for large robust nonlinear SVM\n  classifiers", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the binary classification problem when data are large and subject\nto unknown but bounded uncertainties. We address the problem by formulating the\nnonlinear support vector machine training problem with robust optimization. To\ndo so, we analyze and propose two bounding schemes for uncertainties associated\nto random approximate features in low dimensional spaces. The proposed\ntechniques are based on Random Fourier Features and the Nystr\\\"om methods. The\nresulting formulations can be solved with efficient stochastic approximation\ntechniques such as stochastic (sub)-gradient, stochastic proximal gradient\ntechniques or their variants.\n", "versions": [{"version": "v1", "created": "Thu, 29 Jun 2017 15:08:26 GMT"}], "update_date": "2017-06-30", "authors_parsed": [["Couellan", "Nicolas", ""], ["Jan", "Sophie", ""]]}, {"id": "1706.09814", "submitter": "Yunwen Lei", "authors": "Yunwen Lei, Urun Dogan, Ding-Xuan Zhou, Marius Kloft", "title": "Data-dependent Generalization Bounds for Multi-class Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we study data-dependent generalization error bounds exhibiting\na mild dependency on the number of classes, making them suitable for\nmulti-class learning with a large number of label classes. The bounds generally\nhold for empirical multi-class risk minimization algorithms using an arbitrary\nnorm as regularizer. Key to our analysis are new structural results for\nmulti-class Gaussian complexities and empirical $\\ell_\\infty$-norm covering\nnumbers, which exploit the Lipschitz continuity of the loss function with\nrespect to the $\\ell_2$- and $\\ell_\\infty$-norm, respectively. We establish\ndata-dependent error bounds in terms of complexities of a linear function class\ndefined on a finite set induced by training examples, for which we show tight\nlower and upper bounds. We apply the results to several prominent multi-class\nlearning machines, exhibiting a tighter dependency on the number of classes\nthan the state of the art. For instance, for the multi-class SVM by Crammer and\nSinger (2002), we obtain a data-dependent bound with a logarithmic dependency\nwhich significantly improves the previous square-root dependency. Experimental\nresults are reported to verify the effectiveness of our theoretical findings.\n", "versions": [{"version": "v1", "created": "Thu, 29 Jun 2017 15:41:25 GMT"}, {"version": "v2", "created": "Fri, 29 Dec 2017 14:37:19 GMT"}], "update_date": "2018-01-01", "authors_parsed": [["Lei", "Yunwen", ""], ["Dogan", "Urun", ""], ["Zhou", "Ding-Xuan", ""], ["Kloft", "Marius", ""]]}, {"id": "1706.09865", "submitter": "C.H. Bryan Liu", "authors": "C.H. Bryan Liu, Benjamin Paul Chamberlain, Duncan A. Little, Angelo\n  Cardoso", "title": "Generalising Random Forest Parameter Optimisation to Include Stability\n  and Cost", "comments": "To appear in ECML-PKDD 2017", "journal-ref": "Machine Learning and Knowledge Discovery in Databases. ECML PKDD\n  2017. LNCS vol 10536, pp. 102-113 (2017)", "doi": "10.1007/978-3-319-71273-4_9", "report-no": null, "categories": "stat.ML cs.CY cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Random forests are among the most popular classification and regression\nmethods used in industrial applications. To be effective, the parameters of\nrandom forests must be carefully tuned. This is usually done by choosing values\nthat minimize the prediction error on a held out dataset. We argue that error\nreduction is only one of several metrics that must be considered when\noptimizing random forest parameters for commercial applications. We propose a\nnovel metric that captures the stability of random forests predictions, which\nwe argue is key for scenarios that require successive predictions. We motivate\nthe need for multi-criteria optimization by showing that in practical\napplications, simply choosing the parameters that lead to the lowest error can\nintroduce unnecessary costs and produce predictions that are not stable across\nindependent runs. To optimize this multi-criteria trade-off, we present a new\nframework that efficiently finds a principled balance between these three\nconsiderations using Bayesian optimisation. The pitfalls of optimising forest\nparameters purely for error reduction are demonstrated using two publicly\navailable real world datasets. We show that our framework leads to parameter\nsettings that are markedly different from the values discovered by error\nreduction metrics.\n", "versions": [{"version": "v1", "created": "Thu, 29 Jun 2017 17:23:44 GMT"}, {"version": "v2", "created": "Thu, 13 Jul 2017 15:43:33 GMT"}], "update_date": "2018-07-03", "authors_parsed": [["Liu", "C. H. Bryan", ""], ["Chamberlain", "Benjamin Paul", ""], ["Little", "Duncan A.", ""], ["Cardoso", "Angelo", ""]]}, {"id": "1706.09884", "submitter": "Jerry Li", "authors": "Jerry Li and Aleksander Madry and John Peebles and Ludwig Schmidt", "title": "On the Limitations of First-Order Approximation in GAN Dynamics", "comments": "18 pages, 4 figures, accepted to ICML 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While Generative Adversarial Networks (GANs) have demonstrated promising\nperformance on multiple vision tasks, their learning dynamics are not yet well\nunderstood, both in theory and in practice. To address this issue, we study GAN\ndynamics in a simple yet rich parametric model that exhibits several of the\ncommon problematic convergence behaviors such as vanishing gradients, mode\ncollapse, and diverging or oscillatory behavior. In spite of the non-convex\nnature of our model, we are able to perform a rigorous theoretical analysis of\nits convergence behavior. Our analysis reveals an interesting dichotomy: a GAN\nwith an optimal discriminator provably converges, while first order\napproximations of the discriminator steps lead to unstable GAN dynamics and\nmode collapse. Our result suggests that using first order discriminator steps\n(the de-facto standard in most existing GAN setups) might be one of the factors\nthat makes GAN training challenging in practice.\n", "versions": [{"version": "v1", "created": "Thu, 29 Jun 2017 17:52:44 GMT"}, {"version": "v2", "created": "Sun, 3 Jun 2018 23:00:03 GMT"}], "update_date": "2018-06-05", "authors_parsed": [["Li", "Jerry", ""], ["Madry", "Aleksander", ""], ["Peebles", "John", ""], ["Schmidt", "Ludwig", ""]]}, {"id": "1706.09916", "submitter": "Zhenpeng Zhou", "authors": "Zhenpeng Zhou, and Xiaocheng Li", "title": "Graph Convolution: A High-Order and Adaptive Approach", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we presented a novel convolutional neural network framework\nfor graph modeling, with the introduction of two new modules specially designed\nfor graph-structured data: the $k$-th order convolution operator and the\nadaptive filtering module. Importantly, our framework of High-order and\nAdaptive Graph Convolutional Network (HA-GCN) is a general-purposed\narchitecture that fits various applications on both node and graph centrics, as\nwell as graph generative models. We conducted extensive experiments on\ndemonstrating the advantages of our framework. Particularly, our HA-GCN\noutperforms the state-of-the-art models on node classification and molecule\nproperty prediction tasks. It also generates 32% more real molecules on the\nmolecule generation task, both of which will significantly benefit real-world\napplications such as material design and drug screening.\n", "versions": [{"version": "v1", "created": "Thu, 29 Jun 2017 18:33:06 GMT"}, {"version": "v2", "created": "Fri, 20 Oct 2017 04:25:23 GMT"}], "update_date": "2017-10-23", "authors_parsed": [["Zhou", "Zhenpeng", ""], ["Li", "Xiaocheng", ""]]}, {"id": "1706.09993", "submitter": "Yan Shuo Tan", "authors": "Yan Shuo Tan, Roman Vershynin", "title": "Phase Retrieval via Randomized Kaczmarz: Theoretical Guarantees", "comments": "Revised after comments from referees", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.NA cs.IT cs.LG math.IT math.PR math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of phase retrieval, i.e. that of solving systems of\nquadratic equations. A simple variant of the randomized Kaczmarz method was\nrecently proposed for phase retrieval, and it was shown numerically to have a\ncomputational edge over state-of-the-art Wirtinger flow methods. In this paper,\nwe provide the first theoretical guarantee for the convergence of the\nrandomized Kaczmarz method for phase retrieval. We show that it is sufficient\nto have as many Gaussian measurements as the dimension, up to a constant\nfactor. Along the way, we introduce a sufficient condition on measurement sets\nfor which the randomized Kaczmarz method is guaranteed to work. We show that\nGaussian sampling vectors satisfy this property with high probability; this is\nproved using a chaining argument coupled with bounds on VC dimension and metric\nentropy.\n", "versions": [{"version": "v1", "created": "Fri, 30 Jun 2017 01:21:55 GMT"}, {"version": "v2", "created": "Sat, 13 Jan 2018 20:02:48 GMT"}], "update_date": "2018-01-16", "authors_parsed": [["Tan", "Yan Shuo", ""], ["Vershynin", "Roman", ""]]}, {"id": "1706.10003", "submitter": "Sivaraman Balakrishnan", "authors": "Sivaraman Balakrishnan and Larry Wasserman", "title": "Hypothesis Testing For Densities and High-Dimensional Multinomials:\n  Sharp Local Minimax Rates", "comments": "60 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.IT cs.LG math.IT stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the goodness-of-fit testing problem of distinguishing whether the\ndata are drawn from a specified distribution, versus a composite alternative\nseparated from the null in the total variation metric. In the discrete case, we\nconsider goodness-of-fit testing when the null distribution has a possibly\ngrowing or unbounded number of categories. In the continuous case, we consider\ntesting a Lipschitz density, with possibly unbounded support, in the\nlow-smoothness regime where the Lipschitz parameter is not assumed to be\nconstant. In contrast to existing results, we show that the minimax rate and\ncritical testing radius in these settings depend strongly, and in a precise\nway, on the null distribution being tested and this motivates the study of the\n(local) minimax rate as a function of the null distribution. For multinomials\nthe local minimax rate was recently studied in the work of Valiant and Valiant.\nWe re-visit and extend their results and develop two modifications to the\nchi-squared test whose performance we characterize. For testing Lipschitz\ndensities, we show that the usual binning tests are inadequate in the\nlow-smoothness regime and we design a spatially adaptive partitioning scheme\nthat forms the basis for our locally minimax optimal tests. Furthermore, we\nprovide the first local minimax lower bounds for this problem which yield a\nsharp characterization of the dependence of the critical radius on the null\nhypothesis being tested. In the low-smoothness regime we also provide adaptive\ntests, that adapt to the unknown smoothness parameter. We illustrate our\nresults with a variety of simulations that demonstrate the practical utility of\nour proposed tests.\n", "versions": [{"version": "v1", "created": "Fri, 30 Jun 2017 02:34:12 GMT"}], "update_date": "2017-07-03", "authors_parsed": [["Balakrishnan", "Sivaraman", ""], ["Wasserman", "Larry", ""]]}, {"id": "1706.10006", "submitter": "Konstantinos Drossos", "authors": "Konstantinos Drossos, Sharath Adavanne, Tuomas Virtanen", "title": "Automated Audio Captioning with Recurrent Neural Networks", "comments": "Presented at the 11th IEEE Workshop on Applications of Signal\n  Processing to Audio and Acoustics (WASPAA), 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present the first approach to automated audio captioning. We employ an\nencoder-decoder scheme with an alignment model in between. The input to the\nencoder is a sequence of log mel-band energies calculated from an audio file,\nwhile the output is a sequence of words, i.e. a caption. The encoder is a\nmulti-layered, bi-directional gated recurrent unit (GRU) and the decoder a\nmulti-layered GRU with a classification layer connected to the last GRU of the\ndecoder. The classification layer and the alignment model are fully connected\nlayers with shared weights between timesteps. The proposed method is evaluated\nusing data drawn from a commercial sound effects library, ProSound Effects. The\nresulting captions were rated through metrics utilized in machine translation\nand image captioning fields. Results from metrics show that the proposed method\ncan predict words appearing in the original caption, but not always correctly\nordered.\n", "versions": [{"version": "v1", "created": "Fri, 30 Jun 2017 02:55:55 GMT"}, {"version": "v2", "created": "Tue, 24 Oct 2017 11:36:08 GMT"}], "update_date": "2017-10-25", "authors_parsed": [["Drossos", "Konstantinos", ""], ["Adavanne", "Sharath", ""], ["Virtanen", "Tuomas", ""]]}, {"id": "1706.10018", "submitter": "Ting Lan", "authors": "Jian Liu, Ting Lan, Hong Qin", "title": "Improvement of training set structure in fusion data cleaning using\n  Time-Domain Global Similarity method", "comments": null, "journal-ref": null, "doi": "10.1088/1748-0221/12/10/C10004", "report-no": null, "categories": "cs.LG physics.plasm-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Traditional data cleaning identifies dirty data by classifying original data\nsequences, which is a class$-$imbalanced problem since the proportion of\nincorrect data is much less than the proportion of correct ones for most\ndiagnostic systems in Magnetic Confinement Fusion (MCF) devices. When using\nmachine learning algorithms to classify diagnostic data based on\nclass$-$imbalanced training set, most classifiers are biased towards the major\nclass and show very poor classification rates on the minor class. By\ntransforming the direct classification problem about original data sequences\ninto a classification problem about the physical similarity between data\nsequences, the class$-$balanced effect of Time$-$Domain Global Similarity\n(TDGS) method on training set structure is investigated in this paper.\nMeanwhile, the impact of improved training set structure on data cleaning\nperformance of TDGS method is demonstrated with an application example in EAST\nPOlarimetry$-$INTerferometry (POINT) system.\n", "versions": [{"version": "v1", "created": "Fri, 30 Jun 2017 04:36:20 GMT"}], "update_date": "2017-10-25", "authors_parsed": [["Liu", "Jian", ""], ["Lan", "Ting", ""], ["Qin", "Hong", ""]]}, {"id": "1706.10020", "submitter": "Ting Lan", "authors": "Ting Lan, Jian Liu, Hong Qin", "title": "Preference-based performance measures for Time-Domain Global Similarity\n  method", "comments": null, "journal-ref": null, "doi": "10.1088/1748-0221/12/12/C12008", "report-no": null, "categories": "cs.LG physics.plasm-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For Time-Domain Global Similarity (TDGS) method, which transforms the data\ncleaning problem into a binary classification problem about the physical\nsimilarity between channels, directly adopting common performance measures\ncould only guarantee the performance for physical similarity. Nevertheless,\npractical data cleaning tasks have preferences for the correctness of original\ndata sequences. To obtain the general expressions of performance measures based\non the preferences of tasks, the mapping relations between performance of TDGS\nmethod about physical similarity and correctness of data sequences are\ninvestigated by probability theory in this paper. Performance measures for TDGS\nmethod in several common data cleaning tasks are set. Cases when these\npreference-based performance measures could be simplified are introduced.\n", "versions": [{"version": "v1", "created": "Fri, 30 Jun 2017 04:44:28 GMT"}, {"version": "v2", "created": "Wed, 8 Nov 2017 04:43:26 GMT"}], "update_date": "2018-01-17", "authors_parsed": [["Lan", "Ting", ""], ["Liu", "Jian", ""], ["Qin", "Hong", ""]]}, {"id": "1706.10031", "submitter": "Sotetsu Koyamada", "authors": "Sotetsu Koyamada, Yuta Kikuchi, Atsunori Kanemura, Shin-ichi Maeda,\n  Shin Ishii", "title": "Neural Sequence Model Training via $\\alpha$-divergence Minimization", "comments": "2017 ICML Workshop on Learning to Generate Natural Language (LGNL\n  2017)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new neural sequence model training method in which the objective\nfunction is defined by $\\alpha$-divergence. We demonstrate that the objective\nfunction generalizes the maximum-likelihood (ML)-based and reinforcement\nlearning (RL)-based objective functions as special cases (i.e., ML corresponds\nto $\\alpha \\to 0$ and RL to $\\alpha \\to1$). We also show that the gradient of\nthe objective function can be considered a mixture of ML- and RL-based\nobjective gradients. The experimental results of a machine translation task\nshow that minimizing the objective function with $\\alpha > 0$ outperforms\n$\\alpha \\to 0$, which corresponds to ML-based methods.\n", "versions": [{"version": "v1", "created": "Fri, 30 Jun 2017 06:09:47 GMT"}], "update_date": "2017-07-03", "authors_parsed": [["Koyamada", "Sotetsu", ""], ["Kikuchi", "Yuta", ""], ["Kanemura", "Atsunori", ""], ["Maeda", "Shin-ichi", ""], ["Ishii", "Shin", ""]]}, {"id": "1706.10036", "submitter": "Xingjun Ma", "authors": "Xingjun Ma, Sudanthi Wijewickrema, Yun Zhou, Shuo Zhou, Stephen\n  O'Leary, James Bailey", "title": "Providing Effective Real-time Feedback in Simulation-based Surgical\n  Training", "comments": "To appear in Proceedings of the 20th International Conference on\n  Medical Image Computing and Computer Assisted Intervention (MICCAI), Quebec\n  City, Canada, 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Virtual reality simulation is becoming popular as a training platform in\nsurgical education. However, one important aspect of simulation-based surgical\ntraining that has not received much attention is the provision of automated\nreal-time performance feedback to support the learning process. Performance\nfeedback is actionable advice that improves novice behaviour. In simulation,\nautomated feedback is typically extracted from prediction models trained using\ndata mining techniques. Existing techniques suffer from either low\neffectiveness or low efficiency resulting in their inability to be used in\nreal-time. In this paper, we propose a random forest based method that finds a\nbalance between effectiveness and efficiency. Experimental results in a\ntemporal bone surgery simulation show that the proposed method is able to\nextract highly effective feedback at a high level of efficiency.\n", "versions": [{"version": "v1", "created": "Fri, 30 Jun 2017 06:36:14 GMT"}], "update_date": "2017-07-03", "authors_parsed": [["Ma", "Xingjun", ""], ["Wijewickrema", "Sudanthi", ""], ["Zhou", "Yun", ""], ["Zhou", "Shuo", ""], ["O'Leary", "Stephen", ""], ["Bailey", "James", ""]]}, {"id": "1706.10082", "submitter": "Ippei Obayashi Mr.", "authors": "Ippei Obayashi and Yasuaki Hiraoka", "title": "Persistence Diagrams with Linear Machine Learning Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.AT cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Persistence diagrams have been widely recognized as a compact descriptor for\ncharacterizing multiscale topological features in data. When many datasets are\navailable, statistical features embedded in those persistence diagrams can be\nextracted by applying machine learnings. In particular, the ability for\nexplicitly analyzing the inverse in the original data space from those\nstatistical features of persistence diagrams is significantly important for\npractical applications. In this paper, we propose a unified method for the\ninverse analysis by combining linear machine learning models with persistence\nimages. The method is applied to point clouds and cubical sets, showing the\nability of the statistical inverse analysis and its advantages.\n", "versions": [{"version": "v1", "created": "Fri, 30 Jun 2017 09:33:50 GMT"}, {"version": "v2", "created": "Thu, 6 Jul 2017 07:19:30 GMT"}], "update_date": "2017-07-07", "authors_parsed": [["Obayashi", "Ippei", ""], ["Hiraoka", "Yasuaki", ""]]}, {"id": "1706.10172", "submitter": "M\\'arton Karsai", "authors": "Yongjun Liao, Wei Du, M\\'arton Karsai, Carlos Sarraute, Martin Minnoni\n  and Eric Fleury", "title": "Prepaid or Postpaid? That is the question. Novel Methods of Subscription\n  Type Prediction in Mobile Phone Services", "comments": "17 pages, 4 figures; chapter to appear in Lecture Notes in Social\n  Networks; Eds. R. Alhajj, U. Gl\\\"asser, Springer Nature (2017)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.LG physics.soc-ph stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we investigate the behavioural differences between mobile phone\ncustomers with prepaid and postpaid subscriptions. Our study reveals that (a)\npostpaid customers are more active in terms of service usage and (b) there are\nstrong structural correlations in the mobile phone call network as connections\nbetween customers of the same subscription type are much more frequent than\nthose between customers of different subscription types. Based on these\nobservations we provide methods to detect the subscription type of customers by\nusing information about their personal call statistics, and also their\negocentric networks simultaneously. The key of our first approach is to cast\nthis classification problem as a problem of graph labelling, which can be\nsolved by max-flow min-cut algorithms. Our experiments show that, by using both\nuser attributes and relationships, the proposed graph labelling approach is\nable to achieve a classification accuracy of $\\sim 87\\%$, which outperforms by\n$\\sim 7\\%$ supervised learning methods using only user attributes. In our\nsecond problem we aim to infer the subscription type of customers of external\noperators. We propose via approximate methods to solve this problem by using\nnode attributes, and a two-ways indirect inference method based on observed\nhomophilic structural correlations. Our results have straightforward\napplications in behavioural prediction and personal marketing.\n", "versions": [{"version": "v1", "created": "Fri, 30 Jun 2017 12:27:11 GMT"}], "update_date": "2017-07-03", "authors_parsed": [["Liao", "Yongjun", ""], ["Du", "Wei", ""], ["Karsai", "M\u00e1rton", ""], ["Sarraute", "Carlos", ""], ["Minnoni", "Martin", ""], ["Fleury", "Eric", ""]]}, {"id": "1706.10199", "submitter": "Margaux Luck", "authors": "Margaux Luck, Nicolas Pallet, Cecilia Damon", "title": "Rule-Mining based classification: a benchmark study", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This study proposed an exhaustive stable/reproducible rule-mining algorithm\ncombined to a classifier to generate both accurate and interpretable models.\nOur method first extracts rules (i.e., a conjunction of conditions about the\nvalues of a small number of input features) with our exhaustive rule-mining\nalgorithm, then constructs a new feature space based on the most relevant rules\ncalled \"local features\" and finally, builds a local predictive model by\ntraining a standard classifier on the new local feature space. This local\nfeature space is easy interpretable by providing a human-understandable\nexplanation under the explicit form of rules. Furthermore, our local predictive\napproach is as powerful as global classical ones like logistic regression (LR),\nsupport vector machine (SVM) and rules based methods like random forest (RF)\nand gradient boosted tree (GBT).\n", "versions": [{"version": "v1", "created": "Fri, 30 Jun 2017 13:51:23 GMT"}], "update_date": "2017-07-03", "authors_parsed": [["Luck", "Margaux", ""], ["Pallet", "Nicolas", ""], ["Damon", "Cecilia", ""]]}, {"id": "1706.10207", "submitter": "Frank E. Curtis", "authors": "Frank E. Curtis and Katya Scheinberg", "title": "Optimization Methods for Supervised Machine Learning: From Linear Models\n  to Deep Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The goal of this tutorial is to introduce key models, algorithms, and open\nquestions related to the use of optimization methods for solving problems\narising in machine learning. It is written with an INFORMS audience in mind,\nspecifically those readers who are familiar with the basics of optimization\nalgorithms, but less familiar with machine learning. We begin by deriving a\nformulation of a supervised learning problem and show how it leads to various\noptimization problems, depending on the context and underlying assumptions. We\nthen discuss some of the distinctive features of these optimization problems,\nfocusing on the examples of logistic regression and the training of deep neural\nnetworks. The latter half of the tutorial focuses on optimization algorithms,\nfirst for convex logistic regression, for which we discuss the use of\nfirst-order methods, the stochastic gradient method, variance reducing\nstochastic methods, and second-order methods. Finally, we discuss how these\napproaches can be employed to the training of deep neural networks, emphasizing\nthe difficulties that arise from the complex, nonconvex structure of these\nmodels.\n", "versions": [{"version": "v1", "created": "Fri, 30 Jun 2017 14:09:44 GMT"}], "update_date": "2017-07-03", "authors_parsed": [["Curtis", "Frank E.", ""], ["Scheinberg", "Katya", ""]]}, {"id": "1706.10208", "submitter": "Nina Grgi\\'c-Hla\\v{c}a", "authors": "Nina Grgi\\'c-Hla\\v{c}a, Muhammad Bilal Zafar, Krishna P. Gummadi,\n  Adrian Weller", "title": "On Fairness, Diversity and Randomness in Algorithmic Decision Making", "comments": "Presented as a poster at the 2017 Workshop on Fairness,\n  Accountability, and Transparency in Machine Learning (FAT/ML 2017)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Consider a binary decision making process where a single machine learning\nclassifier replaces a multitude of humans. We raise questions about the\nresulting loss of diversity in the decision making process. We study the\npotential benefits of using random classifier ensembles instead of a single\nclassifier in the context of fairness-aware learning and demonstrate various\nattractive properties: (i) an ensemble of fair classifiers is guaranteed to be\nfair, for several different measures of fairness, (ii) an ensemble of unfair\nclassifiers can still achieve fair outcomes, and (iii) an ensemble of\nclassifiers can achieve better accuracy-fairness trade-offs than a single\nclassifier. Finally, we introduce notions of distributional fairness to\ncharacterize further potential benefits of random classifier ensembles.\n", "versions": [{"version": "v1", "created": "Fri, 30 Jun 2017 14:10:34 GMT"}], "update_date": "2017-07-03", "authors_parsed": [["Grgi\u0107-Hla\u010da", "Nina", ""], ["Zafar", "Muhammad Bilal", ""], ["Gummadi", "Krishna P.", ""], ["Weller", "Adrian", ""]]}, {"id": "1706.10231", "submitter": "Alexander Dallmann", "authors": "Alexander Dallmann (1), Alexander Grimm (1), Christian P\\\"olitz (1),\n  Daniel Zoller (1), Andreas Hotho (1 and 2) ((1) University of W\\\"urzburg, (2)\n  L3S Research Center)", "title": "Improving Session Recommendation with Recurrent Neural Networks by\n  Exploiting Dwell Time", "comments": "6 pages, 3 figures, submission to DLRS workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, Recurrent Neural Networks (RNNs) have been applied to the task of\nsession-based recommendation. These approaches use RNNs to predict the next\nitem in a user session based on the previ- ously visited items. While some\napproaches consider additional item properties, we argue that item dwell time\ncan be used as an implicit measure of user interest to improve session-based\nitem recommen- dations. We propose an extension to existing RNN approaches that\ncaptures user dwell time in addition to the visited items and show that\nrecommendation performance can be improved. Additionally, we investigate the\nusefulness of a single validation split for model selection in the case of\nminor improvements and find that in our case the best model is not selected and\na fold-like study with different validation sets is necessary to ensure the\nselection of the best model.\n", "versions": [{"version": "v1", "created": "Fri, 30 Jun 2017 14:58:52 GMT"}], "update_date": "2017-07-03", "authors_parsed": [["Dallmann", "Alexander", "", "1 and 2"], ["Grimm", "Alexander", "", "1 and 2"], ["P\u00f6litz", "Christian", "", "1 and 2"], ["Zoller", "Daniel", "", "1 and 2"], ["Hotho", "Andreas", "", "1 and 2"]]}, {"id": "1706.10234", "submitter": "Paul Rubenstein", "authors": "Paul K. Rubenstein, Ilya Tolstikhin, Philipp Hennig, Bernhard\n  Schoelkopf", "title": "Probabilistic Active Learning of Functions in Structural Causal Models", "comments": "9 pages main text + 4 pages supplement", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of learning the functions computing children from\nparents in a Structural Causal Model once the underlying causal graph has been\nidentified. This is in some sense the second step after causal discovery.\nTaking a probabilistic approach to estimating these functions, we derive a\nnatural myopic active learning scheme that identifies the intervention which is\noptimally informative about all of the unknown functions jointly, given\npreviously observed data. We test the derived algorithms on simple examples, to\ndemonstrate that they produce a structured exploration policy that\nsignificantly improves on unstructured base-lines.\n", "versions": [{"version": "v1", "created": "Fri, 30 Jun 2017 15:06:57 GMT"}], "update_date": "2017-07-03", "authors_parsed": [["Rubenstein", "Paul K.", ""], ["Tolstikhin", "Ilya", ""], ["Hennig", "Philipp", ""], ["Schoelkopf", "Bernhard", ""]]}, {"id": "1706.10239", "submitter": "Lei Wu", "authors": "Lei Wu, Zhanxing Zhu, Weinan E", "title": "Towards Understanding Generalization of Deep Learning: Perspective of\n  Loss Landscapes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  It is widely observed that deep learning models with learned parameters\ngeneralize well, even with much more model parameters than the number of\ntraining samples. We systematically investigate the underlying reasons why deep\nneural networks often generalize well, and reveal the difference between the\nminima (with the same training error) that generalize well and those they\ndon't. We show that it is the characteristics the landscape of the loss\nfunction that explains the good generalization capability. For the landscape of\nloss function for deep networks, the volume of basin of attraction of good\nminima dominates over that of poor minima, which guarantees optimization\nmethods with random initialization to converge to good minima. We theoretically\njustify our findings through analyzing 2-layer neural networks; and show that\nthe low-complexity solutions have a small norm of Hessian matrix with respect\nto model parameters. For deeper networks, extensive numerical evidence helps to\nsupport our arguments.\n", "versions": [{"version": "v1", "created": "Fri, 30 Jun 2017 15:30:21 GMT"}, {"version": "v2", "created": "Tue, 28 Nov 2017 02:40:04 GMT"}], "update_date": "2017-11-29", "authors_parsed": [["Wu", "Lei", ""], ["Zhu", "Zhanxing", ""], ["E", "Weinan", ""]]}, {"id": "1706.10240", "submitter": "Ahmadreza Ahmadi", "authors": "Ahmadreza Ahmadi and Jun Tani", "title": "Bridging the Gap between Probabilistic and Deterministic Models: A\n  Simulation Study on a Variational Bayes Predictive Coding Recurrent Neural\n  Network Model", "comments": "This paper is accepted the 24th International Conference On Neural\n  Information Processing (ICONIP 2017). The previous submission to arXiv is\n  replaced by this version because there was an error in Equation 8", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The current paper proposes a novel variational Bayes predictive coding RNN\nmodel, which can learn to generate fluctuated temporal patterns from exemplars.\nThe model learns to maximize the lower bound of the weighted sum of the\nregularization and reconstruction error terms. We examined how this weighting\ncan affect development of different types of information processing while\nlearning fluctuated temporal patterns. Simulation results show that strong\nweighting of the reconstruction term causes the development of deterministic\nchaos for imitating the randomness observed in target sequences, while strong\nweighting of the regularization term causes the development of stochastic\ndynamics imitating probabilistic processes observed in targets. Moreover,\nresults indicate that the most generalized learning emerges between these two\nextremes. The paper concludes with implications in terms of the underlying\nneuronal mechanisms for autism spectrum disorder and for free action.\n", "versions": [{"version": "v1", "created": "Fri, 30 Jun 2017 15:31:17 GMT"}, {"version": "v2", "created": "Fri, 15 Sep 2017 14:35:31 GMT"}], "update_date": "2017-09-18", "authors_parsed": [["Ahmadi", "Ahmadreza", ""], ["Tani", "Jun", ""]]}, {"id": "1706.10268", "submitter": "Zahra Ghodsi", "authors": "Zahra Ghodsi, Tianyu Gu, Siddharth Garg", "title": "SafetyNets: Verifiable Execution of Deep Neural Networks on an Untrusted\n  Cloud", "comments": null, "journal-ref": "Advances in Neural Information Processing Systems 2017", "doi": null, "report-no": null, "categories": "cs.LG cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Inference using deep neural networks is often outsourced to the cloud since\nit is a computationally demanding task. However, this raises a fundamental\nissue of trust. How can a client be sure that the cloud has performed inference\ncorrectly? A lazy cloud provider might use a simpler but less accurate model to\nreduce its own computational load, or worse, maliciously modify the inference\nresults sent to the client. We propose SafetyNets, a framework that enables an\nuntrusted server (the cloud) to provide a client with a short mathematical\nproof of the correctness of inference tasks that they perform on behalf of the\nclient. Specifically, SafetyNets develops and implements a specialized\ninteractive proof (IP) protocol for verifiable execution of a class of deep\nneural networks, i.e., those that can be represented as arithmetic circuits.\nOur empirical results on three- and four-layer deep neural networks demonstrate\nthe run-time costs of SafetyNets for both the client and server are low.\nSafetyNets detects any incorrect computations of the neural network by the\nuntrusted server with high probability, while achieving state-of-the-art\naccuracy on the MNIST digit recognition (99.4%) and TIMIT speech recognition\ntasks (75.22%).\n", "versions": [{"version": "v1", "created": "Fri, 30 Jun 2017 16:47:16 GMT"}], "update_date": "2021-05-14", "authors_parsed": [["Ghodsi", "Zahra", ""], ["Gu", "Tianyu", ""], ["Garg", "Siddharth", ""]]}, {"id": "1706.10271", "submitter": "Vaishnavh Nagarajan", "authors": "Maria-Florina Balcan, Avrim Blum, Vaishnavh Nagarajan", "title": "Lifelong Learning in Costly Feature Spaces", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An important long-term goal in machine learning systems is to build learning\nagents that, like humans, can learn many tasks over their lifetime, and\nmoreover use information from these tasks to improve their ability to do so\nefficiently. In this work, our goal is to provide new theoretical insights into\nthe potential of this paradigm. In particular, we propose a lifelong learning\nframework that adheres to a novel notion of resource efficiency that is\ncritical in many real-world domains where feature evaluations are costly. That\nis, our learner aims to reuse information from previously learned related tasks\nto learn future tasks in a feature-efficient manner. Furthermore, we consider\nnovel combinatorial ways in which learning tasks can relate. Specifically, we\ndesign lifelong learning algorithms for two structurally different and widely\nused families of target functions: decision trees/lists and\nmonomials/polynomials. We also provide strong feature-efficiency guarantees for\nthese algorithms; in fact, we show that in order to learn future targets, we\nneed only slightly more feature evaluations per training example than what is\nneeded to predict on an arbitrary example using those targets. We also provide\nalgorithms with guarantees in an agnostic model where not all the targets are\nrelated to each other. Finally, we also provide lower bounds on the performance\nof a lifelong learner in these models, which are in fact tight under some\nconditions.\n", "versions": [{"version": "v1", "created": "Fri, 30 Jun 2017 16:56:47 GMT"}], "update_date": "2017-07-03", "authors_parsed": [["Balcan", "Maria-Florina", ""], ["Blum", "Avrim", ""], ["Nagarajan", "Vaishnavh", ""]]}, {"id": "1706.10295", "submitter": "Charles Blundell", "authors": "Meire Fortunato, Mohammad Gheshlaghi Azar, Bilal Piot, Jacob Menick,\n  Ian Osband, Alex Graves, Vlad Mnih, Remi Munos, Demis Hassabis, Olivier\n  Pietquin, Charles Blundell, Shane Legg", "title": "Noisy Networks for Exploration", "comments": "ICLR 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce NoisyNet, a deep reinforcement learning agent with parametric\nnoise added to its weights, and show that the induced stochasticity of the\nagent's policy can be used to aid efficient exploration. The parameters of the\nnoise are learned with gradient descent along with the remaining network\nweights. NoisyNet is straightforward to implement and adds little computational\noverhead. We find that replacing the conventional exploration heuristics for\nA3C, DQN and dueling agents (entropy reward and $\\epsilon$-greedy respectively)\nwith NoisyNet yields substantially higher scores for a wide range of Atari\ngames, in some cases advancing the agent from sub to super-human performance.\n", "versions": [{"version": "v1", "created": "Fri, 30 Jun 2017 17:56:19 GMT"}, {"version": "v2", "created": "Thu, 15 Feb 2018 16:00:54 GMT"}, {"version": "v3", "created": "Tue, 9 Jul 2019 09:57:23 GMT"}], "update_date": "2019-11-01", "authors_parsed": [["Fortunato", "Meire", ""], ["Azar", "Mohammad Gheshlaghi", ""], ["Piot", "Bilal", ""], ["Menick", "Jacob", ""], ["Osband", "Ian", ""], ["Graves", "Alex", ""], ["Mnih", "Vlad", ""], ["Munos", "Remi", ""], ["Hassabis", "Demis", ""], ["Pietquin", "Olivier", ""], ["Blundell", "Charles", ""], ["Legg", "Shane", ""]]}]