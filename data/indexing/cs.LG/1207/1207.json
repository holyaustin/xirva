[{"id": "1207.0057", "submitter": "Yoshua Bengio", "authors": "Yoshua Bengio and Guillaume Alain and Salah Rifai", "title": "Implicit Density Estimation by Local Moment Matching to Sample from\n  Auto-Encoders", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent work suggests that some auto-encoder variants do a good job of\ncapturing the local manifold structure of the unknown data generating density.\nThis paper contributes to the mathematical understanding of this phenomenon and\nhelps define better justified sampling algorithms for deep learning based on\nauto-encoder variants. We consider an MCMC where each step samples from a\nGaussian whose mean and covariance matrix depend on the previous state, defines\nthrough its asymptotic distribution a target density. First, we show that good\nchoices (in the sense of consistency) for these mean and covariance functions\nare the local expected value and local covariance under that target density.\nThen we show that an auto-encoder with a contractive penalty captures\nestimators of these local moments in its reconstruction function and its\nJacobian. A contribution of this work is thus a novel alternative to\nmaximum-likelihood density estimation, which we call local moment matching. It\nalso justifies a recently proposed sampling algorithm for the Contractive\nAuto-Encoder and extends it to the Denoising Auto-Encoder.\n", "versions": [{"version": "v1", "created": "Sat, 30 Jun 2012 07:45:11 GMT"}], "update_date": "2012-07-03", "authors_parsed": [["Bengio", "Yoshua", ""], ["Alain", "Guillaume", ""], ["Rifai", "Salah", ""]]}, {"id": "1207.0099", "submitter": "Masashi Sugiyama", "authors": "Masashi Sugiyama, Takafumi Kanamori, Taiji Suzuki, Marthinus\n  Christoffel du Plessis, Song Liu, Ichiro Takeuchi", "title": "Density-Difference Estimation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We address the problem of estimating the difference between two probability\ndensities. A naive approach is a two-step procedure of first estimating two\ndensities separately and then computing their difference. However, such a\ntwo-step procedure does not necessarily work well because the first step is\nperformed without regard to the second step and thus a small error incurred in\nthe first stage can cause a big error in the second stage. In this paper, we\npropose a single-shot procedure for directly estimating the density difference\nwithout separately estimating two densities. We derive a non-parametric\nfinite-sample error bound for the proposed single-shot density-difference\nestimator and show that it achieves the optimal convergence rate. The\nusefulness of the proposed method is also demonstrated experimentally.\n", "versions": [{"version": "v1", "created": "Sat, 30 Jun 2012 14:21:46 GMT"}], "update_date": "2012-07-03", "authors_parsed": [["Sugiyama", "Masashi", ""], ["Kanamori", "Takafumi", ""], ["Suzuki", "Taiji", ""], ["Plessis", "Marthinus Christoffel du", ""], ["Liu", "Song", ""], ["Takeuchi", "Ichiro", ""]]}, {"id": "1207.0151", "submitter": "Matthew Zeiler", "authors": "Matthew D. Zeiler and Rob Fergus", "title": "Differentiable Pooling for Hierarchical Feature Learning", "comments": "12 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a parametric form of pooling, based on a Gaussian, which can be\noptimized alongside the features in a single global objective function. By\ncontrast, existing pooling schemes are based on heuristics (e.g. local maximum)\nand have no clear link to the cost function of the model. Furthermore, the\nvariables of the Gaussian explicitly store location information, distinct from\nthe appearance captured by the features, thus providing a what/where\ndecomposition of the input signal. Although the differentiable pooling scheme\ncan be incorporated in a wide range of hierarchical models, we demonstrate it\nin the context of a Deconvolutional Network model (Zeiler et al. ICCV 2011). We\nalso explore a number of secondary issues within this model and present\ndetailed experiments on MNIST digits.\n", "versions": [{"version": "v1", "created": "Sat, 30 Jun 2012 21:04:13 GMT"}], "update_date": "2012-07-03", "authors_parsed": [["Zeiler", "Matthew D.", ""], ["Fergus", "Rob", ""]]}, {"id": "1207.0166", "submitter": "Claudio Gentile", "authors": "Claudio Gentile and Francesco Orabona", "title": "On Multilabel Classification and Ranking with Partial Feedback", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a novel multilabel/ranking algorithm working in partial\ninformation settings. The algorithm is based on 2nd-order descent methods, and\nrelies on upper-confidence bounds to trade-off exploration and exploitation. We\nanalyze this algorithm in a partial adversarial setting, where covariates can\nbe adversarial, but multilabel probabilities are ruled by (generalized) linear\nmodels. We show O(T^{1/2} log T) regret bounds, which improve in several ways\non the existing results. We test the effectiveness of our upper-confidence\nscheme by contrasting against full-information baselines on real-world\nmultilabel datasets, often obtaining comparable performance.\n", "versions": [{"version": "v1", "created": "Sat, 30 Jun 2012 23:07:03 GMT"}, {"version": "v2", "created": "Tue, 20 Nov 2012 16:48:22 GMT"}, {"version": "v3", "created": "Wed, 16 Jan 2013 19:19:34 GMT"}], "update_date": "2013-01-17", "authors_parsed": [["Gentile", "Claudio", ""], ["Orabona", "Francesco", ""]]}, {"id": "1207.0268", "submitter": "Shivani Agarwal", "authors": "Shivani Agarwal", "title": "Surrogate Regret Bounds for Bipartite Ranking via Strongly Proper Losses", "comments": "20 pages", "journal-ref": "Journal of Machine Learning Research, 15:1653-1674, 2014", "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The problem of bipartite ranking, where instances are labeled positive or\nnegative and the goal is to learn a scoring function that minimizes the\nprobability of mis-ranking a pair of positive and negative instances (or\nequivalently, that maximizes the area under the ROC curve), has been widely\nstudied in recent years. A dominant theoretical and algorithmic framework for\nthe problem has been to reduce bipartite ranking to pairwise classification; in\nparticular, it is well known that the bipartite ranking regret can be\nformulated as a pairwise classification regret, which in turn can be upper\nbounded using usual regret bounds for classification problems. Recently,\nKotlowski et al. (2011) showed regret bounds for bipartite ranking in terms of\nthe regret associated with balanced versions of the standard (non-pairwise)\nlogistic and exponential losses. In this paper, we show that such\n(non-pairwise) surrogate regret bounds for bipartite ranking can be obtained in\nterms of a broad class of proper (composite) losses that we term as strongly\nproper. Our proof technique is much simpler than that of Kotlowski et al.\n(2011), and relies on properties of proper (composite) losses as elucidated\nrecently by Reid and Williamson (2010, 2011) and others. Our result yields\nexplicit surrogate bounds (with no hidden balancing terms) in terms of a\nvariety of strongly proper losses, including for example logistic, exponential,\nsquared and squared hinge losses as special cases. We also obtain tighter\nsurrogate bounds under certain low-noise conditions via a recent result of\nClemencon and Robbiano (2011).\n", "versions": [{"version": "v1", "created": "Mon, 2 Jul 2012 02:57:30 GMT"}], "update_date": "2014-08-13", "authors_parsed": [["Agarwal", "Shivani", ""]]}, {"id": "1207.0396", "submitter": "Peratham Wiriyathammabhum Mr.", "authors": "Peratham Wiriyathammabhum, Boonserm Kijsirikul, Hiroya Takamura,\n  Manabu Okumura", "title": "Applying Deep Belief Networks to Word Sense Disambiguation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we applied a novel learning algorithm, namely, Deep Belief\nNetworks (DBN) to word sense disambiguation (WSD). DBN is a probabilistic\ngenerative model composed of multiple layers of hidden units. DBN uses\nRestricted Boltzmann Machine (RBM) to greedily train layer by layer as a\npretraining. Then, a separate fine tuning step is employed to improve the\ndiscriminative power. We compared DBN with various state-of-the-art supervised\nlearning algorithms in WSD such as Support Vector Machine (SVM), Maximum\nEntropy model (MaxEnt), Naive Bayes classifier (NB) and Kernel Principal\nComponent Analysis (KPCA). We used all words in the given paragraph,\nsurrounding context words and part-of-speech of surrounding words as our\nknowledge sources. We conducted our experiment on the SENSEVAL-2 data set. We\nobserved that DBN outperformed all other learning algorithms.\n", "versions": [{"version": "v1", "created": "Mon, 2 Jul 2012 14:19:21 GMT"}], "update_date": "2012-07-03", "authors_parsed": [["Wiriyathammabhum", "Peratham", ""], ["Kijsirikul", "Boonserm", ""], ["Takamura", "Hiroya", ""], ["Okumura", "Manabu", ""]]}, {"id": "1207.0560", "submitter": "Rishabh Iyer", "authors": "Rishabh Iyer and Jeff Bilmes", "title": "Algorithms for Approximate Minimization of the Difference Between\n  Submodular Functions, with Applications", "comments": "17 pages, 8 figures. A shorter version of this appeared in Proc.\n  Uncertainty in Artificial Intelligence (UAI), Catalina Islands, 2012", "journal-ref": "UAI-2012", "doi": null, "report-no": null, "categories": "cs.DS cs.LG", "license": "http://creativecommons.org/licenses/publicdomain/", "abstract": "  We extend the work of Narasimhan and Bilmes [30] for minimizing set functions\nrepresentable as a difference between submodular functions. Similar to [30],\nour new algorithms are guaranteed to monotonically reduce the objective\nfunction at every step. We empirically and theoretically show that the\nper-iteration cost of our algorithms is much less than [30], and our algorithms\ncan be used to efficiently minimize a difference between submodular functions\nunder various combinatorial constraints, a problem not previously addressed. We\nprovide computational bounds and a hardness result on the mul- tiplicative\ninapproximability of minimizing the difference between submodular functions. We\nshow, however, that it is possible to give worst-case additive bounds by\nproviding a polynomial time computable lower-bound on the minima. Finally we\nshow how a number of machine learning problems can be modeled as minimizing the\ndifference between submodular functions. We experimentally show the validity of\nour algorithms by testing them on the problem of feature selection with\nsubmodular cost features.\n", "versions": [{"version": "v1", "created": "Tue, 3 Jul 2012 01:25:10 GMT"}, {"version": "v2", "created": "Sun, 5 Aug 2012 15:41:46 GMT"}, {"version": "v3", "created": "Sat, 25 Aug 2012 16:04:53 GMT"}, {"version": "v4", "created": "Sat, 24 Aug 2013 05:35:11 GMT"}], "update_date": "2013-08-27", "authors_parsed": [["Iyer", "Rishabh", ""], ["Bilmes", "Jeff", ""]]}, {"id": "1207.0577", "submitter": "Ji Liu", "authors": "Ji Liu and Stephen J. Wright", "title": "Robust Dequantized Compressive Sensing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  We consider the reconstruction problem in compressed sensing in which the\nobservations are recorded in a finite number of bits. They may thus contain\nquantization errors (from being rounded to the nearest representable value) and\nsaturation errors (from being outside the range of representable values). Our\nformulation has an objective of weighted $\\ell_2$-$\\ell_1$ type, along with\nconstraints that account explicitly for quantization and saturation errors, and\nis solved with an augmented Lagrangian method. We prove a consistency result\nfor the recovered solution, stronger than those that have appeared to date in\nthe literature, showing in particular that asymptotic consistency can be\nobtained without oversampling. We present extensive computational comparisons\nwith formulations proposed previously, and variants thereof.\n", "versions": [{"version": "v1", "created": "Tue, 3 Jul 2012 06:07:13 GMT"}, {"version": "v2", "created": "Thu, 10 Oct 2013 17:19:31 GMT"}], "update_date": "2013-10-11", "authors_parsed": [["Liu", "Ji", ""], ["Wright", "Stephen J.", ""]]}, {"id": "1207.0580", "submitter": "Nitish Srivastava", "authors": "Geoffrey E. Hinton, Nitish Srivastava, Alex Krizhevsky, Ilya\n  Sutskever, Ruslan R. Salakhutdinov", "title": "Improving neural networks by preventing co-adaptation of feature\n  detectors", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When a large feedforward neural network is trained on a small training set,\nit typically performs poorly on held-out test data. This \"overfitting\" is\ngreatly reduced by randomly omitting half of the feature detectors on each\ntraining case. This prevents complex co-adaptations in which a feature detector\nis only helpful in the context of several other specific feature detectors.\nInstead, each neuron learns to detect a feature that is generally helpful for\nproducing the correct answer given the combinatorially large variety of\ninternal contexts in which it must operate. Random \"dropout\" gives big\nimprovements on many benchmark tasks and sets new records for speech and object\nrecognition.\n", "versions": [{"version": "v1", "created": "Tue, 3 Jul 2012 06:35:15 GMT"}], "update_date": "2012-07-04", "authors_parsed": [["Hinton", "Geoffrey E.", ""], ["Srivastava", "Nitish", ""], ["Krizhevsky", "Alex", ""], ["Sutskever", "Ilya", ""], ["Salakhutdinov", "Ruslan R.", ""]]}, {"id": "1207.0677", "submitter": "Romain Giot", "authors": "Romain Giot (GREYC), Christophe Charrier (GREYC), Maxime Descoteaux\n  (SCIL)", "title": "Local Water Diffusion Phenomenon Clustering From High Angular Resolution\n  Diffusion Imaging (HARDI)", "comments": "IAPR International Conference on Pattern Recognition (ICPR), Tsukuba,\n  Japan : France (2012)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The understanding of neurodegenerative diseases undoubtedly passes through\nthe study of human brain white matter fiber tracts. To date, diffusion magnetic\nresonance imaging (dMRI) is the unique technique to obtain information about\nthe neural architecture of the human brain, thus permitting the study of white\nmatter connections and their integrity. However, a remaining challenge of the\ndMRI community is to better characterize complex fiber crossing configurations,\nwhere diffusion tensor imaging (DTI) is limited but high angular resolution\ndiffusion imaging (HARDI) now brings solutions. This paper investigates the\ndevelopment of both identification and classification process of the local\nwater diffusion phenomenon based on HARDI data to automatically detect imaging\nvoxels where there are single and crossing fiber bundle populations. The\ntechnique is based on knowledge extraction processes and is validated on a dMRI\nphantom dataset with ground truth.\n", "versions": [{"version": "v1", "created": "Tue, 3 Jul 2012 13:52:19 GMT"}], "update_date": "2012-07-04", "authors_parsed": [["Giot", "Romain", "", "GREYC"], ["Charrier", "Christophe", "", "GREYC"], ["Descoteaux", "Maxime", "", "SCIL"]]}, {"id": "1207.0742", "submitter": "Marc Dymetman", "authors": "Marc Dymetman and Guillaume Bouchard and Simon Carter", "title": "The OS* Algorithm: a Joint Approach to Exact Optimization and Sampling", "comments": "21 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most current sampling algorithms for high-dimensional distributions are based\non MCMC techniques and are approximate in the sense that they are valid only\nasymptotically. Rejection sampling, on the other hand, produces valid samples,\nbut is unrealistically slow in high-dimension spaces. The OS* algorithm that we\npropose is a unified approach to exact optimization and sampling, based on\nincremental refinements of a functional upper bound, which combines ideas of\nadaptive rejection sampling and of A* optimization search. We show that the\nchoice of the refinement can be done in a way that ensures tractability in\nhigh-dimension spaces, and we present first experiments in two different\nsettings: inference in high-order HMMs and in large discrete graphical models.\n", "versions": [{"version": "v1", "created": "Tue, 3 Jul 2012 16:35:48 GMT"}], "update_date": "2012-07-04", "authors_parsed": [["Dymetman", "Marc", ""], ["Bouchard", "Guillaume", ""], ["Carter", "Simon", ""]]}, {"id": "1207.0783", "submitter": "Romain Giot", "authors": "Romain Giot (GREYC), Christophe Rosenberger (GREYC), Bernadette\n  Dorizzi (EPH, SAMOVAR)", "title": "Hybrid Template Update System for Unimodal Biometric Systems", "comments": "IEEE International Conference on Biometrics: Theory, Applications and\n  Systems (BTAS 2012), Washington, District of Columbia, USA : France (2012)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Semi-supervised template update systems allow to automatically take into\naccount the intra-class variability of the biometric data over time. Such\nsystems can be inefficient by including too many impostor's samples or skipping\ntoo many genuine's samples. In the first case, the biometric reference drifts\nfrom the real biometric data and attracts more often impostors. In the second\ncase, the biometric reference does not evolve quickly enough and also\nprogressively drifts from the real biometric data. We propose a hybrid system\nusing several biometric sub-references in order to increase per- formance of\nself-update systems by reducing the previously cited errors. The proposition is\nvalidated for a keystroke- dynamics authentication system (this modality\nsuffers of high variability over time) on two consequent datasets from the\nstate of the art.\n", "versions": [{"version": "v1", "created": "Tue, 3 Jul 2012 19:12:13 GMT"}], "update_date": "2012-07-04", "authors_parsed": [["Giot", "Romain", "", "GREYC"], ["Rosenberger", "Christophe", "", "GREYC"], ["Dorizzi", "Bernadette", "", "EPH, SAMOVAR"]]}, {"id": "1207.0784", "submitter": "Romain Giot", "authors": "Romain Giot (GREYC), Mohamad El-Abed (GREYC), Christophe Rosenberger\n  (GREYC)", "title": "Web-Based Benchmark for Keystroke Dynamics Biometric Systems: A\n  Statistical Analysis", "comments": "The Eighth International Conference on Intelligent Information Hiding\n  and Multimedia Signal Processing (IIHMSP 2012), Piraeus : Greece (2012)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most keystroke dynamics studies have been evaluated using a specific kind of\ndataset in which users type an imposed login and password. Moreover, these\nstudies are optimistics since most of them use different acquisition protocols,\nprivate datasets, controlled environment, etc. In order to enhance the accuracy\nof keystroke dynamics' performance, the main contribution of this paper is\ntwofold. First, we provide a new kind of dataset in which users have typed both\nan imposed and a chosen pairs of logins and passwords. In addition, the\nkeystroke dynamics samples are collected in a web-based uncontrolled\nenvironment (OS, keyboards, browser, etc.). Such kind of dataset is important\nsince it provides us more realistic results of keystroke dynamics' performance\nin comparison to the literature (controlled environment, etc.). Second, we\npresent a statistical analysis of well known assertions such as the\nrelationship between performance and password size, impact of fusion schemes on\nsystem overall performance, and others such as the relationship between\nperformance and entropy. We put into obviousness in this paper some new results\non keystroke dynamics in realistic conditions.\n", "versions": [{"version": "v1", "created": "Tue, 3 Jul 2012 19:12:56 GMT"}], "update_date": "2012-07-04", "authors_parsed": [["Giot", "Romain", "", "GREYC"], ["El-Abed", "Mohamad", "", "GREYC"], ["Rosenberger", "Christophe", "", "GREYC"]]}, {"id": "1207.1019", "submitter": "Emilie Morvant", "authors": "Emilie Morvant (LIF), Amaury Habrard (LAHC), St\\'ephane Ayache (LIF)", "title": "PAC-Bayesian Majority Vote for Late Classifier Fusion", "comments": "7 pages, Research report", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CV cs.LG cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A lot of attention has been devoted to multimedia indexing over the past few\nyears. In the literature, we often consider two kinds of fusion schemes: The\nearly fusion and the late fusion. In this paper we focus on late classifier\nfusion, where one combines the scores of each modality at the decision level.\nTo tackle this problem, we investigate a recent and elegant well-founded\nquadratic program named MinCq coming from the Machine Learning PAC-Bayes\ntheory. MinCq looks for the weighted combination, over a set of real-valued\nfunctions seen as voters, leading to the lowest misclassification rate, while\nmaking use of the voters' diversity. We provide evidence that this method is\nnaturally adapted to late fusion procedure. We propose an extension of MinCq by\nadding an order- preserving pairwise loss for ranking, helping to improve Mean\nAveraged Precision measure. We confirm the good behavior of the MinCq-based\nfusion approaches with experiments on a real image benchmark.\n", "versions": [{"version": "v1", "created": "Wed, 4 Jul 2012 15:09:05 GMT"}], "update_date": "2015-01-16", "authors_parsed": [["Morvant", "Emilie", "", "LIF"], ["Habrard", "Amaury", "", "LAHC"], ["Ayache", "St\u00e9phane", "", "LIF"]]}, {"id": "1207.1115", "submitter": "Jameson Toole", "authors": "Jameson L. Toole, Michael Ulm, Dietmar Bauer, Marta C. Gonzalez", "title": "Inferring land use from mobile phone activity", "comments": "To be presented at ACM UrbComp2012", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG physics.data-an physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Understanding the spatiotemporal distribution of people within a city is\ncrucial to many planning applications. Obtaining data to create required\nknowledge, currently involves costly survey methods. At the same time\nubiquitous mobile sensors from personal GPS devices to mobile phones are\ncollecting massive amounts of data on urban systems. The locations,\ncommunications, and activities of millions of people are recorded and stored by\nnew information technologies. This work utilizes novel dynamic data, generated\nby mobile phone users, to measure spatiotemporal changes in population. In the\nprocess, we identify the relationship between land use and dynamic population\nover the course of a typical week. A machine learning classification algorithm\nis used to identify clusters of locations with similar zoned uses and mobile\nphone activity patterns. It is shown that the mobile phone data is capable of\ndelivering useful information on actual land use that supplements zoning\nregulations.\n", "versions": [{"version": "v1", "created": "Tue, 3 Jul 2012 16:20:08 GMT"}], "update_date": "2012-07-06", "authors_parsed": [["Toole", "Jameson L.", ""], ["Ulm", "Michael", ""], ["Bauer", "Dietmar", ""], ["Gonzalez", "Marta C.", ""]]}, {"id": "1207.1358", "submitter": "Susan Shortreed", "authors": "Susan Shortreed, Marina Meila", "title": "Unsupervised spectral learning", "comments": "Appears in Proceedings of the Twenty-First Conference on Uncertainty\n  in Artificial Intelligence (UAI2005)", "journal-ref": null, "doi": null, "report-no": "UAI-P-2005-PG-534-541", "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In spectral clustering and spectral image segmentation, the data is partioned\nstarting from a given matrix of pairwise similarities S. the matrix S is\nconstructed by hand, or learned on a separate training set. In this paper we\nshow how to achieve spectral clustering in unsupervised mode. Our algorithm\nstarts with a set of observed pairwise features, which are possible components\nof an unknown, parametric similarity function. This function is learned\niteratively, at the same time as the clustering of the data. The algorithm\nshows promosing results on synthetic and real data.\n", "versions": [{"version": "v1", "created": "Wed, 4 Jul 2012 12:14:50 GMT"}], "update_date": "2012-07-09", "authors_parsed": [["Shortreed", "Susan", ""], ["Meila", "Marina", ""]]}, {"id": "1207.1364", "submitter": "Eric E. Altendorf", "authors": "Eric E. Altendorf, Angelo C. Restificar, Thomas G. Dietterich", "title": "Learning from Sparse Data by Exploiting Monotonicity Constraints", "comments": "Appears in Proceedings of the Twenty-First Conference on Uncertainty\n  in Artificial Intelligence (UAI2005)", "journal-ref": null, "doi": null, "report-no": "UAI-P-2005-PG-18-26", "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When training data is sparse, more domain knowledge must be incorporated into\nthe learning algorithm in order to reduce the effective size of the hypothesis\nspace. This paper builds on previous work in which knowledge about qualitative\nmonotonicities was formally represented and incorporated into learning\nalgorithms (e.g., Clark & Matwin's work with the CN2 rule learning algorithm).\nWe show how to interpret knowledge of qualitative influences, and in particular\nof monotonicities, as constraints on probability distributions, and to\nincorporate this knowledge into Bayesian network learning algorithms. We show\nthat this yields improved accuracy, particularly with very small training sets\n(e.g. less than 10 examples).\n", "versions": [{"version": "v1", "created": "Wed, 4 Jul 2012 16:03:10 GMT"}], "update_date": "2012-07-09", "authors_parsed": [["Altendorf", "Eric E.", ""], ["Restificar", "Angelo C.", ""], ["Dietterich", "Thomas G.", ""]]}, {"id": "1207.1366", "submitter": "Pieter Abbeel", "authors": "Pieter Abbeel, Daphne Koller, Andrew Y. Ng", "title": "Learning Factor Graphs in Polynomial Time & Sample Complexity", "comments": "Appears in Proceedings of the Twenty-First Conference on Uncertainty\n  in Artificial Intelligence (UAI2005)", "journal-ref": null, "doi": null, "report-no": "UAI-P-2005-PG-1-9", "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study computational and sample complexity of parameter and structure\nlearning in graphical models. Our main result shows that the class of factor\ngraphs with bounded factor size and bounded connectivity can be learned in\npolynomial time and polynomial number of samples, assuming that the data is\ngenerated by a network in this class. This result covers both parameter\nestimation for a known network structure and structure learning. It implies as\na corollary that we can learn factor graphs for both Bayesian networks and\nMarkov networks of bounded degree, in polynomial time and sample complexity.\nUnlike maximum likelihood estimation, our method does not require inference in\nthe underlying network, and so applies to networks where inference is\nintractable. We also show that the error of our learned model degrades\ngracefully when the generating distribution is not a member of the target class\nof networks.\n", "versions": [{"version": "v1", "created": "Wed, 4 Jul 2012 16:03:31 GMT"}], "update_date": "2012-07-09", "authors_parsed": [["Abbeel", "Pieter", ""], ["Koller", "Daphne", ""], ["Ng", "Andrew Y.", ""]]}, {"id": "1207.1379", "submitter": "Shen-Shyang Ho", "authors": "Shen-Shyang Ho, Harry Wechsler", "title": "On the Detection of Concept Changes in Time-Varying Data Stream by\n  Testing Exchangeability", "comments": "Appears in Proceedings of the Twenty-First Conference on Uncertainty\n  in Artificial Intelligence (UAI2005)", "journal-ref": null, "doi": null, "report-no": "UAI-P-2005-PG-267-274", "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A martingale framework for concept change detection based on testing data\nexchangeability was recently proposed (Ho, 2005). In this paper, we describe\nthe proposed change-detection test based on the Doob's Maximal Inequality and\nshow that it is an approximation of the sequential probability ratio test\n(SPRT). The relationship between the threshold value used in the proposed test\nand its size and power is deduced from the approximation. The mean delay time\nbefore a change is detected is estimated using the average sample number of a\nSPRT. The performance of the test using various threshold values is examined on\nfive different data stream scenarios simulated using two synthetic data sets.\nFinally, experimental results show that the test is effective in detecting\nchanges in time-varying data streams simulated using three benchmark data sets.\n", "versions": [{"version": "v1", "created": "Wed, 4 Jul 2012 16:10:01 GMT"}], "update_date": "2012-07-09", "authors_parsed": [["Ho", "Shen-Shyang", ""], ["Wechsler", "Harry", ""]]}, {"id": "1207.1380", "submitter": "Markus Harva", "authors": "Markus Harva, Tapani Raiko, Antti Honkela, Harri Valpola, Juha\n  Karhunen", "title": "Bayes Blocks: An Implementation of the Variational Bayesian Building\n  Blocks Framework", "comments": "Appears in Proceedings of the Twenty-First Conference on Uncertainty\n  in Artificial Intelligence (UAI2005)", "journal-ref": null, "doi": null, "report-no": "UAI-P-2005-PG-259-266", "categories": "cs.MS cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A software library for constructing and learning probabilistic models is\npresented. The library offers a set of building blocks from which a large\nvariety of static and dynamic models can be built. These include hierarchical\nmodels for variances of other variables and many nonlinear models. The\nunderlying variational Bayesian machinery, providing for fast and robust\nestimation but being mathematically rather involved, is almost completely\nhidden from the user thus making it very easy to use the library. The building\nblocks include Gaussian, rectified Gaussian and mixture-of-Gaussians variables\nand computational nodes which can be combined rather freely.\n", "versions": [{"version": "v1", "created": "Wed, 4 Jul 2012 16:10:18 GMT"}], "update_date": "2012-07-09", "authors_parsed": [["Harva", "Markus", ""], ["Raiko", "Tapani", ""], ["Honkela", "Antti", ""], ["Valpola", "Harri", ""], ["Karhunen", "Juha", ""]]}, {"id": "1207.1382", "submitter": "Yuhong Guo", "authors": "Yuhong Guo, Dana Wilkinson, Dale Schuurmans", "title": "Maximum Margin Bayesian Networks", "comments": "Appears in Proceedings of the Twenty-First Conference on Uncertainty\n  in Artificial Intelligence (UAI2005)", "journal-ref": null, "doi": null, "report-no": "UAI-P-2005-PG-233-242", "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of learning Bayesian network classifiers that\nmaximize the marginover a set of classification variables. We find that this\nproblem is harder for Bayesian networks than for undirected graphical models\nlike maximum margin Markov networks. The main difficulty is that the parameters\nin a Bayesian network must satisfy additional normalization constraints that an\nundirected graphical model need not respect. These additional constraints\ncomplicate the optimization task. Nevertheless, we derive an effective training\nalgorithm that solves the maximum margin training problem for a range of\nBayesian network topologies, and converges to an approximate solution for\narbitrary network topologies. Experimental results show that the method can\ndemonstrate improved generalization performance over Markov networks when the\ndirected graphical structure encodes relevant knowledge. In practice, the\ntraining technique allows one to combine prior knowledge expressed as a\ndirected (causal) model with state of the art discriminative learning methods.\n", "versions": [{"version": "v1", "created": "Wed, 4 Jul 2012 16:12:02 GMT"}], "update_date": "2012-07-09", "authors_parsed": [["Guo", "Yuhong", ""], ["Wilkinson", "Dana", ""], ["Schuurmans", "Dale", ""]]}, {"id": "1207.1387", "submitter": "Ad Feelders", "authors": "Ad Feelders, Linda C. van der Gaag", "title": "Learning Bayesian Network Parameters with Prior Knowledge about\n  Context-Specific Qualitative Influences", "comments": "Appears in Proceedings of the Twenty-First Conference on Uncertainty\n  in Artificial Intelligence (UAI2005)", "journal-ref": null, "doi": null, "report-no": "UAI-P-2005-PG-193-200", "categories": "cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a method for learning the parameters of a Bayesian network with\nprior knowledge about the signs of influences between variables. Our method\naccommodates not just the standard signs, but provides for context-specific\nsigns as well. We show how the various signs translate into order constraints\non the network parameters and how isotonic regression can be used to compute\norder-constrained estimates from the available data. Our experimental results\nshow that taking prior knowledge about the signs of influences into account\nleads to an improved fit of the true distribution, especially when only a small\nsample of data is available. Moreover, the computed estimates are guaranteed to\nbe consistent with the specified signs, thereby resulting in a network that is\nmore likely to be accepted by experts in its domain of application.\n", "versions": [{"version": "v1", "created": "Wed, 4 Jul 2012 16:13:39 GMT"}], "update_date": "2012-07-09", "authors_parsed": [["Feelders", "Ad", ""], ["van der Gaag", "Linda C.", ""]]}, {"id": "1207.1393", "submitter": "Hendrik Kuck", "authors": "Hendrik Kuck, Nando de Freitas", "title": "Learning about individuals from group statistics", "comments": "Appears in Proceedings of the Twenty-First Conference on Uncertainty\n  in Artificial Intelligence (UAI2005)", "journal-ref": null, "doi": null, "report-no": "UAI-P-2005-PG-332-339", "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new problem formulation which is similar to, but more\ninformative than, the binary multiple-instance learning problem. In this\nsetting, we are given groups of instances (described by feature vectors) along\nwith estimates of the fraction of positively-labeled instances per group. The\ntask is to learn an instance level classifier from this information. That is,\nwe are trying to estimate the unknown binary labels of individuals from\nknowledge of group statistics. We propose a principled probabilistic model to\nsolve this problem that accounts for uncertainty in the parameters and in the\nunknown individual labels. This model is trained with an efficient MCMC\nalgorithm. Its performance is demonstrated on both synthetic and real-world\ndata arising in general object recognition.\n", "versions": [{"version": "v1", "created": "Wed, 4 Jul 2012 16:16:02 GMT"}], "update_date": "2012-07-09", "authors_parsed": [["Kuck", "Hendrik", ""], ["de Freitas", "Nando", ""]]}, {"id": "1207.1396", "submitter": "Mike Klaas", "authors": "Mike Klaas, Nando de Freitas, Arnaud Doucet", "title": "Toward Practical N2 Monte Carlo: the Marginal Particle Filter", "comments": "Appears in Proceedings of the Twenty-First Conference on Uncertainty\n  in Artificial Intelligence (UAI2005)", "journal-ref": null, "doi": null, "report-no": "UAI-P-2005-PG-308-315", "categories": "stat.CO cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sequential Monte Carlo techniques are useful for state estimation in\nnon-linear, non-Gaussian dynamic models. These methods allow us to approximate\nthe joint posterior distribution using sequential importance sampling. In this\nframework, the dimension of the target distribution grows with each time step,\nthus it is necessary to introduce some resampling steps to ensure that the\nestimates provided by the algorithm have a reasonable variance. In many\napplications, we are only interested in the marginal filtering distribution\nwhich is defined on a space of fixed dimension. We present a Sequential Monte\nCarlo algorithm called the Marginal Particle Filter which operates directly on\nthe marginal distribution, hence avoiding having to perform importance sampling\non a space of growing dimension. Using this idea, we also derive an improved\nversion of the auxiliary particle filter. We show theoretic and empirical\nresults which demonstrate a reduction in variance over conventional particle\nfiltering, and present techniques for reducing the cost of the marginal\nparticle filter with N particles from O(N2) to O(N logN).\n", "versions": [{"version": "v1", "created": "Wed, 4 Jul 2012 16:17:01 GMT"}], "update_date": "2012-07-09", "authors_parsed": [["Klaas", "Mike", ""], ["de Freitas", "Nando", ""], ["Doucet", "Arnaud", ""]]}, {"id": "1207.1403", "submitter": "Alexandru Niculescu-Mizil", "authors": "Alexandru Niculescu-Mizil, Richard A. Caruana", "title": "Obtaining Calibrated Probabilities from Boosting", "comments": "Appears in Proceedings of the Twenty-First Conference on Uncertainty\n  in Artificial Intelligence (UAI2005)", "journal-ref": null, "doi": null, "report-no": "UAI-P-2005-PG-413-420", "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Boosted decision trees typically yield good accuracy, precision, and ROC\narea. However, because the outputs from boosting are not well calibrated\nposterior probabilities, boosting yields poor squared error and cross-entropy.\nWe empirically demonstrate why AdaBoost predicts distorted probabilities and\nexamine three calibration methods for correcting this distortion: Platt\nScaling, Isotonic Regression, and Logistic Correction. We also experiment with\nboosting using log-loss instead of the usual exponential loss. Experiments show\nthat Logistic Correction and boosting with log-loss work well when boosting\nweak models such as decision stumps, but yield poor performance when boosting\nmore complex models such as full decision trees. Platt Scaling and Isotonic\nRegression, however, significantly improve the probabilities predicted by\n", "versions": [{"version": "v1", "created": "Wed, 4 Jul 2012 16:19:55 GMT"}], "update_date": "2012-07-09", "authors_parsed": [["Niculescu-Mizil", "Alexandru", ""], ["Caruana", "Richard A.", ""]]}, {"id": "1207.1404", "submitter": "Mukund Narasimhan", "authors": "Mukund Narasimhan, Jeff A. Bilmes", "title": "A submodular-supermodular procedure with applications to discriminative\n  structure learning", "comments": "Appears in Proceedings of the Twenty-First Conference on Uncertainty\n  in Artificial Intelligence (UAI2005)", "journal-ref": null, "doi": null, "report-no": "UAI-P-2005-PG-404-412", "categories": "cs.LG cs.DS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present an algorithm for minimizing the difference between\ntwo submodular functions using a variational framework which is based on (an\nextension of) the concave-convex procedure [17]. Because several commonly used\nmetrics in machine learning, like mutual information and conditional mutual\ninformation, are submodular, the problem of minimizing the difference of two\nsubmodular problems arises naturally in many machine learning applications. Two\nsuch applications are learning discriminatively structured graphical models and\nfeature selection under computational complexity constraints. A commonly used\nmetric for measuring discriminative capacity is the EAR measure which is the\ndifference between two conditional mutual information terms. Feature selection\ntaking complexity considerations into account also fall into this framework\nbecause both the information that a set of features provide and the cost of\ncomputing and using the features can be modeled as submodular functions. This\nproblem is NP-hard, and we give a polynomial time heuristic for it. We also\npresent results on synthetic data to show that classifiers based on\ndiscriminative graphical models using this algorithm can significantly\noutperform classifiers based on generative graphical models.\n", "versions": [{"version": "v1", "created": "Wed, 4 Jul 2012 16:20:12 GMT"}], "update_date": "2012-07-09", "authors_parsed": [["Narasimhan", "Mukund", ""], ["Bilmes", "Jeff A.", ""]]}, {"id": "1207.1406", "submitter": "Andrew McCallum", "authors": "Andrew McCallum, Kedar Bellare, Fernando Pereira", "title": "A Conditional Random Field for Discriminatively-trained Finite-state\n  String Edit Distance", "comments": "Appears in Proceedings of the Twenty-First Conference on Uncertainty\n  in Artificial Intelligence (UAI2005)", "journal-ref": null, "doi": null, "report-no": "UAI-P-2005-PG-388-395", "categories": "cs.LG cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The need to measure sequence similarity arises in information extraction,\nobject identity, data mining, biological sequence analysis, and other domains.\nThis paper presents discriminative string-edit CRFs, a finitestate conditional\nrandom field model for edit sequences between strings. Conditional random\nfields have advantages over generative approaches to this problem, such as pair\nHMMs or the work of Ristad and Yianilos, because as conditionally-trained\nmethods, they enable the use of complex, arbitrary actions and features of the\ninput strings. As in generative models, the training data does not have to\nspecify the edit sequences between the given string pairs. Unlike generative\nmodels, however, our model is trained on both positive and negative instances\nof string pairs. We present positive experimental results on several data sets.\n", "versions": [{"version": "v1", "created": "Wed, 4 Jul 2012 16:20:45 GMT"}], "update_date": "2012-07-09", "authors_parsed": [["McCallum", "Andrew", ""], ["Bellare", "Kedar", ""], ["Pereira", "Fernando", ""]]}, {"id": "1207.1409", "submitter": "Charles Sutton", "authors": "Charles Sutton, Andrew McCallum", "title": "Piecewise Training for Undirected Models", "comments": "Appears in Proceedings of the Twenty-First Conference on Uncertainty\n  in Artificial Intelligence (UAI2005)", "journal-ref": null, "doi": null, "report-no": "UAI-P-2005-PG-568-575", "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For many large undirected models that arise in real-world applications, exact\nmaximumlikelihood training is intractable, because it requires computing\nmarginal distributions of the model. Conditional training is even more\ndifficult, because the partition function depends not only on the parameters,\nbut also on the observed input, requiring repeated inference over each training\nexample. An appealing idea for such models is to independently train a local\nundirected classifier over each clique, afterwards combining the learned\nweights into a single global model. In this paper, we show that this piecewise\nmethod can be justified as minimizing a new family of upper bounds on the log\npartition function. On three natural-language data sets, piecewise training is\nmore accurate than pseudolikelihood, and often performs comparably to global\ntraining using belief propagation.\n", "versions": [{"version": "v1", "created": "Wed, 4 Jul 2012 16:22:14 GMT"}], "update_date": "2012-07-09", "authors_parsed": [["Sutton", "Charles", ""], ["McCallum", "Andrew", ""]]}, {"id": "1207.1413", "submitter": "Shohei Shimizu", "authors": "Shohei Shimizu, Aapo Hyvarinen, Yutaka Kano, Patrik O. Hoyer", "title": "Discovery of non-gaussian linear causal models using ICA", "comments": "Appears in Proceedings of the Twenty-First Conference on Uncertainty\n  in Artificial Intelligence (UAI2005)", "journal-ref": null, "doi": null, "report-no": "UAI-P-2005-PG-525-533", "categories": "cs.LG cs.MS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, several methods have been proposed for the discovery of\ncausal structure from non-experimental data (Spirtes et al. 2000; Pearl 2000).\nSuch methods make various assumptions on the data generating process to\nfacilitate its identification from purely observational data. Continuing this\nline of research, we show how to discover the complete causal structure of\ncontinuous-valued data, under the assumptions that (a) the data generating\nprocess is linear, (b) there are no unobserved confounders, and (c) disturbance\nvariables have non-gaussian distributions of non-zero variances. The solution\nrelies on the use of the statistical method known as independent component\nanalysis (ICA), and does not require any pre-specified time-ordering of the\nvariables. We provide a complete Matlab package for performing this LiNGAM\nanalysis (short for Linear Non-Gaussian Acyclic Model), and demonstrate the\neffectiveness of the method using artificially generated data.\n", "versions": [{"version": "v1", "created": "Wed, 4 Jul 2012 16:23:35 GMT"}], "update_date": "2012-07-09", "authors_parsed": [["Shimizu", "Shohei", ""], ["Hyvarinen", "Aapo", ""], ["Kano", "Yutaka", ""], ["Hoyer", "Patrik O.", ""]]}, {"id": "1207.1414", "submitter": "Eerika Savia", "authors": "Eerika Savia, Kai Puolamaki, Janne Sinkkonen, Samuel Kaski", "title": "Two-Way Latent Grouping Model for User Preference Prediction", "comments": "Appears in Proceedings of the Twenty-First Conference on Uncertainty\n  in Artificial Intelligence (UAI2005)", "journal-ref": null, "doi": null, "report-no": "UAI-P-2005-PG-518-524", "categories": "cs.IR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a novel latent grouping model for predicting the relevance of a\nnew document to a user. The model assumes a latent group structure for both\nusers and documents. We compared the model against a state-of-the-art method,\nthe User Rating Profile model, where only users have a latent group structure.\nWe estimate both models by Gibbs sampling. The new method predicts relevance\nmore accurately for new documents that have few known ratings. The reason is\nthat generalization over documents then becomes necessary and hence the twoway\ngrouping is profitable.\n", "versions": [{"version": "v1", "created": "Wed, 4 Jul 2012 16:23:52 GMT"}], "update_date": "2012-07-09", "authors_parsed": [["Savia", "Eerika", ""], ["Puolamaki", "Kai", ""], ["Sinkkonen", "Janne", ""], ["Kaski", "Samuel", ""]]}, {"id": "1207.1417", "submitter": "Michal Rosen-Zvi", "authors": "Michal Rosen-Zvi, Michael I. Jordan, Alan Yuille", "title": "The DLR Hierarchy of Approximate Inference", "comments": "Appears in Proceedings of the Twenty-First Conference on Uncertainty\n  in Artificial Intelligence (UAI2005)", "journal-ref": null, "doi": null, "report-no": "UAI-P-2005-PG-493-500", "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a hierarchy for approximate inference based on the Dobrushin,\nLanford, Ruelle (DLR) equations. This hierarchy includes existing algorithms,\nsuch as belief propagation, and also motivates novel algorithms such as\nfactorized neighbors (FN) algorithms and variants of mean field (MF)\nalgorithms. In particular, we show that extrema of the Bethe free energy\ncorrespond to approximate solutions of the DLR equations. In addition, we\ndemonstrate a close connection between these approximate algorithms and Gibbs\nsampling. Finally, we compare and contrast various of the algorithms in the DLR\nhierarchy on spin-glass problems. The experiments show that algorithms higher\nup in the hierarchy give more accurate results when they converge but tend to\nbe less stable.\n", "versions": [{"version": "v1", "created": "Wed, 4 Jul 2012 16:25:12 GMT"}], "update_date": "2015-03-20", "authors_parsed": [["Rosen-Zvi", "Michal", ""], ["Jordan", "Michael I.", ""], ["Yuille", "Alan", ""]]}, {"id": "1207.1421", "submitter": "Huizhen Yu", "authors": "Huizhen Yu", "title": "A Function Approximation Approach to Estimation of Policy Gradient for\n  POMDP with Structured Policies", "comments": "Appears in Proceedings of the Twenty-First Conference on Uncertainty\n  in Artificial Intelligence (UAI2005)", "journal-ref": null, "doi": null, "report-no": "UAI-P-2005-PG-642-649", "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the estimation of the policy gradient in partially observable\nMarkov decision processes (POMDP) with a special class of structured policies\nthat are finite-state controllers. We show that the gradient estimation can be\ndone in the Actor-Critic framework, by making the critic compute a \"value\"\nfunction that does not depend on the states of POMDP. This function is the\nconditional mean of the true value function that depends on the states. We show\nthat the critic can be implemented using temporal difference (TD) methods with\nlinear function approximations, and the analytical results on TD and\nActor-Critic can be transfered to this case. Although Actor-Critic algorithms\nhave been used extensively in Markov decision processes (MDP), up to now they\nhave not been proposed for POMDP as an alternative to the earlier proposal\nGPOMDP algorithm, an actor-only method. Furthermore, we show that the same idea\napplies to semi-Markov problems with a subset of finite-state controllers.\n", "versions": [{"version": "v1", "created": "Wed, 4 Jul 2012 16:28:10 GMT"}], "update_date": "2012-07-09", "authors_parsed": [["Yu", "Huizhen", ""]]}, {"id": "1207.1423", "submitter": "Eric P. Xing", "authors": "Eric P. Xing, Rong Yan, Alexander G. Hauptmann", "title": "Mining Associated Text and Images with Dual-Wing Harmoniums", "comments": "Appears in Proceedings of the Twenty-First Conference on Uncertainty\n  in Artificial Intelligence (UAI2005)", "journal-ref": null, "doi": null, "report-no": "UAI-P-2005-PG-633-641", "categories": "cs.LG cs.DB stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a multi-wing harmonium model for mining multimedia data that\nextends and improves on earlier models based on two-layer random fields, which\ncapture bidirectional dependencies between hidden topic aspects and observed\ninputs. This model can be viewed as an undirected counterpart of the two-layer\ndirected models such as LDA for similar tasks, but bears significant difference\nin inference/learning cost tradeoffs, latent topic representations, and topic\nmixing mechanisms. In particular, our model facilitates efficient inference and\nrobust topic mixing, and potentially provides high flexibilities in modeling\nthe latent topic spaces. A contrastive divergence and a variational algorithm\nare derived for learning. We specialized our model to a dual-wing harmonium for\ncaptioned images, incorporating a multivariate Poisson for word-counts and a\nmultivariate Gaussian for color histogram. We present empirical results on the\napplications of this model to classification, retrieval and image annotation on\nnews video collections, and we report an extensive comparison with various\nextant models.\n", "versions": [{"version": "v1", "created": "Wed, 4 Jul 2012 16:28:40 GMT"}], "update_date": "2012-07-09", "authors_parsed": [["Xing", "Eric P.", ""], ["Yan", "Rong", ""], ["Hauptmann", "Alexander G.", ""]]}, {"id": "1207.1429", "submitter": "Marc Teyssier", "authors": "Marc Teyssier, Daphne Koller", "title": "Ordering-Based Search: A Simple and Effective Algorithm for Learning\n  Bayesian Networks", "comments": "Appears in Proceedings of the Twenty-First Conference on Uncertainty\n  in Artificial Intelligence (UAI2005)", "journal-ref": null, "doi": null, "report-no": "UAI-P-2005-PG-584-590", "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One of the basic tasks for Bayesian networks (BNs) is that of learning a\nnetwork structure from data. The BN-learning problem is NP-hard, so the\nstandard solution is heuristic search. Many approaches have been proposed for\nthis task, but only a very small number outperform the baseline of greedy\nhill-climbing with tabu lists; moreover, many of the proposed algorithms are\nquite complex and hard to implement. In this paper, we propose a very simple\nand easy-to-implement method for addressing this task. Our approach is based on\nthe well-known fact that the best network (of bounded in-degree) consistent\nwith a given node ordering can be found very efficiently. We therefore propose\na search not over the space of structures, but over the space of orderings,\nselecting for each ordering the best network consistent with it. This search\nspace is much smaller, makes more global search steps, has a lower branching\nfactor, and avoids costly acyclicity checks. We present results for this\nalgorithm on both synthetic and real data sets, evaluating both the score of\nthe network found and in the running time. We show that ordering-based search\noutperforms the standard baseline, and is competitive with recent algorithms\nthat are much harder to implement.\n", "versions": [{"version": "v1", "created": "Wed, 4 Jul 2012 16:31:04 GMT"}], "update_date": "2012-07-09", "authors_parsed": [["Teyssier", "Marc", ""], ["Koller", "Daphne", ""]]}, {"id": "1207.1655", "submitter": "Christopher E. Granade", "authors": "Christopher E. Granade, Christopher Ferrie, Nathan Wiebe, D. G. Cory", "title": "Robust Online Hamiltonian Learning", "comments": "24 pages, 12 figures; to appear in New Journal of Physics", "journal-ref": "2012 New J. Phys. 14 103013", "doi": "10.1088/1367-2630/14/10/103013", "report-no": null, "categories": "quant-ph cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  In this work we combine two distinct machine learning methodologies,\nsequential Monte Carlo and Bayesian experimental design, and apply them to the\nproblem of inferring the dynamical parameters of a quantum system. We design\nthe algorithm with practicality in mind by including parameters that control\ntrade-offs between the requirements on computational and experimental\nresources. The algorithm can be implemented online (during experimental data\ncollection), avoiding the need for storage and post-processing. Most\nimportantly, our algorithm is capable of learning Hamiltonian parameters even\nwhen the parameters change from experiment-to-experiment, and also when\nadditional noise processes are present and unknown. The algorithm also\nnumerically estimates the Cramer-Rao lower bound, certifying its own\nperformance.\n", "versions": [{"version": "v1", "created": "Fri, 6 Jul 2012 15:17:55 GMT"}, {"version": "v2", "created": "Tue, 18 Sep 2012 02:07:11 GMT"}], "update_date": "2012-10-10", "authors_parsed": [["Granade", "Christopher E.", ""], ["Ferrie", "Christopher", ""], ["Wiebe", "Nathan", ""], ["Cory", "D. G.", ""]]}, {"id": "1207.1965", "submitter": "Gilles Stoltz", "authors": "Marie Devaine (DMA), Pierre Gaillard (DMA, INRIA Paris -\n  Rocquencourt), Yannig Goude, Gilles Stoltz (DMA, INRIA Paris - Rocquencourt,\n  GREGH)", "title": "Forecasting electricity consumption by aggregating specialized experts", "comments": "33 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the setting of sequential prediction of arbitrary sequences based\non specialized experts. We first provide a review of the relevant literature\nand present two theoretical contributions: a general analysis of the specialist\naggregation rule of Freund et al. (1997) and an adaptation of fixed-share rules\nof Herbster and Warmuth (1998) in this setting. We then apply these rules to\nthe sequential short-term (one-day-ahead) forecasting of electricity\nconsumption; to do so, we consider two data sets, a Slovakian one and a French\none, respectively concerned with hourly and half-hourly predictions. We follow\na general methodology to perform the stated empirical studies and detail in\nparticular tuning issues of the learning parameters. The introduced aggregation\nrules demonstrate an improved accuracy on the data sets at hand; the\nimprovements lie in a reduced mean squared error but also in a more robust\nbehavior with respect to large occasional errors.\n", "versions": [{"version": "v1", "created": "Mon, 9 Jul 2012 06:47:39 GMT"}], "update_date": "2012-07-10", "authors_parsed": [["Devaine", "Marie", "", "DMA"], ["Gaillard", "Pierre", "", "DMA, INRIA Paris -\n  Rocquencourt"], ["Goude", "Yannig", "", "DMA, INRIA Paris - Rocquencourt,\n  GREGH"], ["Stoltz", "Gilles", "", "DMA, INRIA Paris - Rocquencourt,\n  GREGH"]]}, {"id": "1207.1977", "submitter": "Doris Entner", "authors": "Doris Entner, Patrik O. Hoyer", "title": "Estimating a Causal Order among Groups of Variables in Linear Models", "comments": "To appear at the International Conference on Artificial Neural\n  Networks 2012 (proceedings to be published in LNCS, Springer); To be\n  presented at the UAI Workshop on Causal Structure Learning 2012", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The machine learning community has recently devoted much attention to the\nproblem of inferring causal relationships from statistical data. Most of this\nwork has focused on uncovering connections among scalar random variables. We\ngeneralize existing methods to apply to collections of multi-dimensional random\nvectors, focusing on techniques applicable to linear models. The performance of\nthe resulting algorithms is evaluated and compared in simulations, which show\nthat our methods can, in many cases, provide useful information on causal\nrelationships even for relatively small sample sizes.\n", "versions": [{"version": "v1", "created": "Mon, 9 Jul 2012 08:05:44 GMT"}], "update_date": "2012-07-10", "authors_parsed": [["Entner", "Doris", ""], ["Hoyer", "Patrik O.", ""]]}, {"id": "1207.2328", "submitter": "Pan Zhang", "authors": "Pan Zhang and Florent Krzakala and J\\\"org Reichardt and Lenka\n  Zdeborov\\'a", "title": "Comparative Study for Inference of Hidden Classes in Stochastic Block\n  Models", "comments": "8 pages, 5 figures AIGM12", "journal-ref": "J. Stat. Mech. (2012) P12021", "doi": "10.1088/1742-5468/2012/12/P12021", "report-no": null, "categories": "cs.LG cond-mat.stat-mech physics.data-an stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Inference of hidden classes in stochastic block model is a classical problem\nwith important applications. Most commonly used methods for this problem\ninvolve na\\\"{\\i}ve mean field approaches or heuristic spectral methods.\nRecently, belief propagation was proposed for this problem. In this\ncontribution we perform a comparative study between the three methods on\nsynthetically created networks. We show that belief propagation shows much\nbetter performance when compared to na\\\"{\\i}ve mean field and spectral\napproaches. This applies to accuracy, computational efficiency and the tendency\nto overfit the data.\n", "versions": [{"version": "v1", "created": "Tue, 10 Jul 2012 12:22:21 GMT"}, {"version": "v2", "created": "Fri, 13 Jul 2012 09:41:10 GMT"}], "update_date": "2013-02-05", "authors_parsed": [["Zhang", "Pan", ""], ["Krzakala", "Florent", ""], ["Reichardt", "J\u00f6rg", ""], ["Zdeborov\u00e1", "Lenka", ""]]}, {"id": "1207.2340", "submitter": "Arash A. Amini", "authors": "Arash A. Amini, Aiyou Chen, Peter J. Bickel, Elizaveta Levina", "title": "Pseudo-likelihood methods for community detection in large sparse\n  networks", "comments": "Published in at http://dx.doi.org/10.1214/13-AOS1138 the Annals of\n  Statistics (http://www.imstat.org/aos/) by the Institute of Mathematical\n  Statistics (http://www.imstat.org)", "journal-ref": "Annals of Statistics 2013, Vol. 41, No. 4, 2097-2122", "doi": "10.1214/13-AOS1138", "report-no": "IMS-AOS-AOS1138", "categories": "cs.SI cs.LG math.ST physics.soc-ph stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many algorithms have been proposed for fitting network models with\ncommunities, but most of them do not scale well to large networks, and often\nfail on sparse networks. Here we propose a new fast pseudo-likelihood method\nfor fitting the stochastic block model for networks, as well as a variant that\nallows for an arbitrary degree distribution by conditioning on degrees. We show\nthat the algorithms perform well under a range of settings, including on very\nsparse networks, and illustrate on the example of a network of political blogs.\nWe also propose spectral clustering with perturbations, a method of independent\ninterest, which works well on sparse networks where regular spectral clustering\nfails, and use it to provide an initial value for pseudo-likelihood. We prove\nthat pseudo-likelihood provides consistent estimates of the communities under a\nmild condition on the starting value, for the case of a block model with two\ncommunities.\n", "versions": [{"version": "v1", "created": "Tue, 10 Jul 2012 13:28:32 GMT"}, {"version": "v2", "created": "Thu, 21 Feb 2013 18:52:23 GMT"}, {"version": "v3", "created": "Tue, 5 Nov 2013 15:49:54 GMT"}], "update_date": "2013-11-06", "authors_parsed": [["Amini", "Arash A.", ""], ["Chen", "Aiyou", ""], ["Bickel", "Peter J.", ""], ["Levina", "Elizaveta", ""]]}, {"id": "1207.2488", "submitter": "Mehrdad Gangeh", "authors": "Mehrdad J. Gangeh, Ali Ghodsi, Mohamed S. Kamel", "title": "Kernelized Supervised Dictionary Learning", "comments": "This paper has been withdrawn by the author as it has been already\n  published by the IEEE Trans. on Signal Processing and is now available online", "journal-ref": null, "doi": "10.1109/TSP.2013.2274276", "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose supervised dictionary learning (SDL) by\nincorporating information on class labels into the learning of the dictionary.\nTo this end, we propose to learn the dictionary in a space where the dependency\nbetween the signals and their corresponding labels is maximized. To maximize\nthis dependency, the recently introduced Hilbert Schmidt independence criterion\n(HSIC) is used. One of the main advantages of this novel approach for SDL is\nthat it can be easily kernelized by incorporating a kernel, particularly a\ndata-derived kernel such as normalized compression distance, into the\nformulation. The learned dictionary is compact and the proposed approach is\nfast. We show that it outperforms other unsupervised and supervised dictionary\nlearning approaches in the literature, using real-world data.\n", "versions": [{"version": "v1", "created": "Tue, 10 Jul 2012 20:52:46 GMT"}, {"version": "v2", "created": "Sat, 10 Nov 2012 17:44:27 GMT"}, {"version": "v3", "created": "Fri, 10 May 2013 17:57:55 GMT"}, {"version": "v4", "created": "Tue, 26 Nov 2013 17:29:04 GMT"}], "update_date": "2013-11-27", "authors_parsed": [["Gangeh", "Mehrdad J.", ""], ["Ghodsi", "Ali", ""], ["Kamel", "Mohamed S.", ""]]}, {"id": "1207.2491", "submitter": "Byron Boots", "authors": "Byron Boots and Geoffrey J. Gordon", "title": "A Spectral Learning Approach to Range-Only SLAM", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.RO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a novel spectral learning algorithm for simultaneous localization\nand mapping (SLAM) from range data with known correspondences. This algorithm\nis an instance of a general spectral system identification framework, from\nwhich it inherits several desirable properties, including statistical\nconsistency and no local optima. Compared with popular batch optimization or\nmultiple-hypothesis tracking (MHT) methods for range-only SLAM, our spectral\napproach offers guaranteed low computational requirements and good tracking\nperformance. Compared with popular extended Kalman filter (EKF) or extended\ninformation filter (EIF) approaches, and many MHT ones, our approach does not\nneed to linearize a transition or measurement model; such linearizations can\ncause severe errors in EKFs and EIFs, and to a lesser extent MHT, particularly\nfor the highly non-Gaussian posteriors encountered in range-only SLAM. We\nprovide a theoretical analysis of our method, including finite-sample error\nbounds. Finally, we demonstrate on a real-world robotic SLAM problem that our\nalgorithm is not only theoretically justified, but works well in practice: in a\ncomparison of multiple methods, the lowest errors come from a combination of\nour algorithm with batch optimization, but our method alone produces nearly as\ngood a result at far lower computational cost.\n", "versions": [{"version": "v1", "created": "Tue, 10 Jul 2012 21:19:33 GMT"}], "update_date": "2012-07-12", "authors_parsed": [["Boots", "Byron", ""], ["Gordon", "Geoffrey J.", ""]]}, {"id": "1207.2812", "submitter": "Anand Sarwate", "authors": "Kamalika Chaudhuri, Anand D. Sarwate, Kaushik Sinha", "title": "Near-Optimal Algorithms for Differentially-Private Principal Components", "comments": "37 pages, 8 figures; final version to appear in the Journal of\n  Machine Learning Research, preliminary version was at NIPS 2012", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Principal components analysis (PCA) is a standard tool for identifying good\nlow-dimensional approximations to data in high dimension. Many data sets of\ninterest contain private or sensitive information about individuals. Algorithms\nwhich operate on such data should be sensitive to the privacy risks in\npublishing their outputs. Differential privacy is a framework for developing\ntradeoffs between privacy and the utility of these outputs. In this paper we\ninvestigate the theory and empirical performance of differentially private\napproximations to PCA and propose a new method which explicitly optimizes the\nutility of the output. We show that the sample complexity of the proposed\nmethod differs from the existing procedure in the scaling with the data\ndimension, and that our method is nearly optimal in terms of this scaling. We\nfurthermore illustrate our results, showing that on real data there is a large\nperformance gap between the existing method and our method.\n", "versions": [{"version": "v1", "created": "Thu, 12 Jul 2012 00:05:02 GMT"}, {"version": "v2", "created": "Mon, 19 Nov 2012 00:29:51 GMT"}, {"version": "v3", "created": "Wed, 7 Aug 2013 21:48:35 GMT"}], "update_date": "2013-08-09", "authors_parsed": [["Chaudhuri", "Kamalika", ""], ["Sarwate", "Anand D.", ""], ["Sinha", "Kaushik", ""]]}, {"id": "1207.2940", "submitter": "Marc Deisenroth", "authors": "Marc Peter Deisenroth and Shakir Mohamed", "title": "Expectation Propagation in Gaussian Process Dynamical Systems: Extended\n  Version", "comments": null, "journal-ref": "Advances in Neural Information Processing Systems 25 (NIPS), pp.\n  2609-2617, 2012", "doi": null, "report-no": null, "categories": "stat.ML cs.LG cs.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Rich and complex time-series data, such as those generated from engineering\nsystems, financial markets, videos or neural recordings, are now a common\nfeature of modern data analysis. Explaining the phenomena underlying these\ndiverse data sets requires flexible and accurate models. In this paper, we\npromote Gaussian process dynamical systems (GPDS) as a rich model class that is\nappropriate for such analysis. In particular, we present a message passing\nalgorithm for approximate inference in GPDSs based on expectation propagation.\nBy posing inference as a general message passing problem, we iterate\nforward-backward smoothing. Thus, we obtain more accurate posterior\ndistributions over latent structures, resulting in improved predictive\nperformance compared to state-of-the-art GPDS smoothers, which are special\ncases of our general message passing algorithm. Hence, we provide a unifying\napproach within which to contextualize message passing in GPDSs.\n", "versions": [{"version": "v1", "created": "Thu, 12 Jul 2012 12:37:57 GMT"}, {"version": "v2", "created": "Fri, 13 Jul 2012 14:45:28 GMT"}, {"version": "v3", "created": "Mon, 12 Nov 2012 06:45:30 GMT"}, {"version": "v4", "created": "Fri, 25 Jul 2014 10:25:19 GMT"}, {"version": "v5", "created": "Wed, 17 Aug 2016 13:23:57 GMT"}], "update_date": "2016-08-18", "authors_parsed": [["Deisenroth", "Marc Peter", ""], ["Mohamed", "Shakir", ""]]}, {"id": "1207.3012", "submitter": "Aarti Singh", "authors": "Aaditya Ramdas and Aarti Singh", "title": "Optimal rates for first-order stochastic convex optimization under\n  Tsybakov noise condition", "comments": "Accepted for publication at ICML 2013", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We focus on the problem of minimizing a convex function $f$ over a convex set\n$S$ given $T$ queries to a stochastic first order oracle. We argue that the\ncomplexity of convex minimization is only determined by the rate of growth of\nthe function around its minimizer $x^*_{f,S}$, as quantified by a Tsybakov-like\nnoise condition. Specifically, we prove that if $f$ grows at least as fast as\n$\\|x-x^*_{f,S}\\|^\\kappa$ around its minimum, for some $\\kappa > 1$, then the\noptimal rate of learning $f(x^*_{f,S})$ is\n$\\Theta(T^{-\\frac{\\kappa}{2\\kappa-2}})$. The classic rate $\\Theta(1/\\sqrt T)$\nfor convex functions and $\\Theta(1/T)$ for strongly convex functions are\nspecial cases of our result for $\\kappa \\rightarrow \\infty$ and $\\kappa=2$, and\neven faster rates are attained for $\\kappa <2$. We also derive tight bounds for\nthe complexity of learning $x_{f,S}^*$, where the optimal rate is\n$\\Theta(T^{-\\frac{1}{2\\kappa-2}})$. Interestingly, these precise rates for\nconvex optimization also characterize the complexity of active learning and our\nresults further strengthen the connections between the two fields, both of\nwhich rely on feedback-driven queries.\n", "versions": [{"version": "v1", "created": "Thu, 12 Jul 2012 16:33:49 GMT"}, {"version": "v2", "created": "Fri, 8 Feb 2013 00:08:51 GMT"}], "update_date": "2015-03-20", "authors_parsed": [["Ramdas", "Aaditya", ""], ["Singh", "Aarti", ""]]}, {"id": "1207.3031", "submitter": "Konstantinos Tsianos", "authors": "Konstantinos I. Tsianos and Michael G. Rabbat", "title": "Distributed Strongly Convex Optimization", "comments": "18 pages single column draftcls format, 1 figure, Submitted to\n  Allerton 2012", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A lot of effort has been invested into characterizing the convergence rates\nof gradient based algorithms for non-linear convex optimization. Recently,\nmotivated by large datasets and problems in machine learning, the interest has\nshifted towards distributed optimization. In this work we present a distributed\nalgorithm for strongly convex constrained optimization. Each node in a network\nof n computers converges to the optimum of a strongly convex, L-Lipchitz\ncontinuous, separable objective at a rate O(log (sqrt(n) T) / T) where T is the\nnumber of iterations. This rate is achieved in the online setting where the\ndata is revealed one at a time to the nodes, and in the batch setting where\neach node has access to its full local dataset from the start. The same\nconvergence rate is achieved in expectation when the subgradients used at each\nnode are corrupted with additive zero-mean noise.\n", "versions": [{"version": "v1", "created": "Thu, 12 Jul 2012 17:38:46 GMT"}, {"version": "v2", "created": "Fri, 20 Jul 2012 03:08:51 GMT"}], "update_date": "2012-07-23", "authors_parsed": [["Tsianos", "Konstantinos I.", ""], ["Rabbat", "Michael G.", ""]]}, {"id": "1207.3071", "submitter": "Mehrdad Gangeh", "authors": "Mehrdad J. Gangeh, Ali Ghodsi, and Mohamed S. Kamel", "title": "Supervised Texture Classification Using a Novel Compression-Based\n  Similarity Measure", "comments": "This paper has been withdrawn by the author since it has already been\n  appeared in the proceedings of International Conference on Computer vision\n  and Graphics (ICCVG)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Supervised pixel-based texture classification is usually performed in the\nfeature space. We propose to perform this task in (dis)similarity space by\nintroducing a new compression-based (dis)similarity measure. The proposed\nmeasure utilizes two dimensional MPEG-1 encoder, which takes into consideration\nthe spatial locality and connectivity of pixels in the images. The proposed\nformulation has been carefully designed based on MPEG encoder functionality. To\nthis end, by design, it solely uses P-frame coding to find the (dis)similarity\namong patches/images. We show that the proposed measure works properly on both\nsmall and large patch sizes. Experimental results show that the proposed\napproach significantly improves the performance of supervised pixel-based\ntexture classification on Brodatz and outdoor images compared to other\ncompression-based dissimilarity measures as well as approaches performed in\nfeature space. It also improves the computation speed by about 40% compared to\nits rivals.\n", "versions": [{"version": "v1", "created": "Thu, 12 Jul 2012 19:37:13 GMT"}, {"version": "v2", "created": "Tue, 26 Nov 2013 17:32:56 GMT"}], "update_date": "2013-11-27", "authors_parsed": [["Gangeh", "Mehrdad J.", ""], ["Ghodsi", "Ali", ""], ["Kamel", "Mohamed S.", ""]]}, {"id": "1207.3127", "submitter": "Quan Wang", "authors": "Quan Wang, Yan Ou, A. Agung Julius, Kim L. Boyer, Min Jun Kim", "title": "Tracking Tetrahymena Pyriformis Cells using Decision Trees", "comments": "21st International Conference on Pattern Recognition, 2012", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV q-bio.CB stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Matching cells over time has long been the most difficult step in cell\ntracking. In this paper, we approach this problem by recasting it as a\nclassification problem. We construct a feature set for each cell, and compute a\nfeature difference vector between a cell in the current frame and a cell in a\nprevious frame. Then we determine whether the two cells represent the same cell\nover time by training decision trees as our binary classifiers. With the output\nof decision trees, we are able to formulate an assignment problem for our cell\nassociation task and solve it using a modified version of the Hungarian\nalgorithm.\n", "versions": [{"version": "v1", "created": "Fri, 13 Jul 2012 01:22:04 GMT"}], "update_date": "2017-11-01", "authors_parsed": [["Wang", "Quan", ""], ["Ou", "Yan", ""], ["Julius", "A. Agung", ""], ["Boyer", "Kim L.", ""], ["Kim", "Min Jun", ""]]}, {"id": "1207.3269", "submitter": "Siddhartha Banerjee", "authors": "Siddhartha Banerjee, Nidhi Hegde and Laurent Massouli\\'e", "title": "The Price of Privacy in Untrusted Recommendation Engines", "comments": "Preliminary version presented at the 50th Allerton Conference, 2012", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.IT math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent increase in online privacy concerns prompts the following question:\ncan a recommender system be accurate if users do not entrust it with their\nprivate data? To answer this, we study the problem of learning item-clusters\nunder local differential privacy, a powerful, formal notion of data privacy. We\ndevelop bounds on the sample-complexity of learning item-clusters from\nprivatized user inputs. Significantly, our results identify a sample-complexity\nseparation between learning in an information-rich and an information-scarce\nregime, thereby highlighting the interaction between privacy and the amount of\ninformation (ratings) available to each user.\n  In the information-rich regime, where each user rates at least a constant\nfraction of items, a spectral clustering approach is shown to achieve a\nsample-complexity lower bound derived from a simple information-theoretic\nargument based on Fano's inequality. However, the information-scarce regime,\nwhere each user rates only a vanishing fraction of items, is found to require a\nfundamentally different approach both for lower bounds and algorithms. To this\nend, we develop new techniques for bounding mutual information under a notion\nof channel-mismatch, and also propose a new algorithm, MaxSense, and show that\nit achieves optimal sample-complexity in this setting.\n  The techniques we develop for bounding mutual information may be of broader\ninterest. To illustrate this, we show their applicability to $(i)$ learning\nbased on 1-bit sketches, and $(ii)$ adaptive learning, where queries can be\nadapted based on answers to past queries.\n", "versions": [{"version": "v1", "created": "Fri, 13 Jul 2012 14:56:38 GMT"}, {"version": "v2", "created": "Mon, 27 Oct 2014 22:16:18 GMT"}], "update_date": "2014-10-29", "authors_parsed": [["Banerjee", "Siddhartha", ""], ["Hegde", "Nidhi", ""], ["Massouli\u00e9", "Laurent", ""]]}, {"id": "1207.3389", "submitter": "Chunhua Shen", "authors": "Xi Li and Anthony Dick and Chunhua Shen and Anton van den Hengel and\n  Hanzi Wang", "title": "Incremental Learning of 3D-DCT Compact Representations for Robust Visual\n  Tracking", "comments": "21 pages. Appearing in IEEE Transactions on Pattern Analysis and\n  Machine Intelligence", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Visual tracking usually requires an object appearance model that is robust to\nchanging illumination, pose and other factors encountered in video. In this\npaper, we construct an appearance model using the 3D discrete cosine transform\n(3D-DCT). The 3D-DCT is based on a set of cosine basis functions, which are\ndetermined by the dimensions of the 3D signal and thus independent of the input\nvideo data. In addition, the 3D-DCT can generate a compact energy spectrum\nwhose high-frequency coefficients are sparse if the appearance samples are\nsimilar. By discarding these high-frequency coefficients, we simultaneously\nobtain a compact 3D-DCT based object representation and a signal\nreconstruction-based similarity measure (reflecting the information loss from\nsignal reconstruction). To efficiently update the object representation, we\npropose an incremental 3D-DCT algorithm, which decomposes the 3D-DCT into\nsuccessive operations of the 2D discrete cosine transform (2D-DCT) and 1D\ndiscrete cosine transform (1D-DCT) on the input video data.\n", "versions": [{"version": "v1", "created": "Sat, 14 Jul 2012 04:44:17 GMT"}, {"version": "v2", "created": "Wed, 18 Jul 2012 07:36:12 GMT"}], "update_date": "2012-07-21", "authors_parsed": [["Li", "Xi", ""], ["Dick", "Anthony", ""], ["Shen", "Chunhua", ""], ["Hengel", "Anton van den", ""], ["Wang", "Hanzi", ""]]}, {"id": "1207.3394", "submitter": "Ali Shadvar", "authors": "Ali Shadvar", "title": "Dimension Reduction by Mutual Information Feature Extraction", "comments": "International Journal of Computer Science & Information Technology\n  (IJCSIT). arXiv admin note: substantial text overlap with arXiv:1206.2058", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://creativecommons.org/licenses/publicdomain/", "abstract": "  During the past decades, to study high-dimensional data in a large variety of\nproblems, researchers have proposed many Feature Extraction algorithms. One of\nthe most effective approaches for optimal feature extraction is based on mutual\ninformation (MI). However it is not always easy to get an accurate estimation\nfor high dimensional MI. In terms of MI, the optimal feature extraction is\ncreating a feature set from the data which jointly have the largest dependency\non the target class and minimum redundancy. In this paper, a\ncomponent-by-component gradient ascent method is proposed for feature\nextraction which is based on one-dimensional MI estimates. We will refer to\nthis algorithm as Mutual Information Feature Extraction (MIFX). The performance\nof this proposed method is evaluated using UCI databases. The results indicate\nthat MIFX provides a robust performance over different data sets which are\nalmost always the best or comparable to the best ones.\n", "versions": [{"version": "v1", "created": "Sat, 14 Jul 2012 06:13:48 GMT"}], "update_date": "2012-07-17", "authors_parsed": [["Shadvar", "Ali", ""]]}, {"id": "1207.3438", "submitter": "Dacheng Tao", "authors": "Naiyang Guan, Dacheng Tao, Zhigang Luo, John Shawe-Taylor", "title": "MahNMF: Manhattan Non-negative Matrix Factorization", "comments": "43 pages, 20 figures, 2 tables, submission to Journal of Machine\n  Learning Research", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG cs.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Non-negative matrix factorization (NMF) approximates a non-negative matrix\n$X$ by a product of two non-negative low-rank factor matrices $W$ and $H$. NMF\nand its extensions minimize either the Kullback-Leibler divergence or the\nEuclidean distance between $X$ and $W^T H$ to model the Poisson noise or the\nGaussian noise. In practice, when the noise distribution is heavy tailed, they\ncannot perform well. This paper presents Manhattan NMF (MahNMF) which minimizes\nthe Manhattan distance between $X$ and $W^T H$ for modeling the heavy tailed\nLaplacian noise. Similar to sparse and low-rank matrix decompositions, MahNMF\nrobustly estimates the low-rank part and the sparse part of a non-negative\nmatrix and thus performs effectively when data are contaminated by outliers. We\nextend MahNMF for various practical applications by developing box-constrained\nMahNMF, manifold regularized MahNMF, group sparse MahNMF, elastic net inducing\nMahNMF, and symmetric MahNMF. The major contribution of this paper lies in two\nfast optimization algorithms for MahNMF and its extensions: the rank-one\nresidual iteration (RRI) method and Nesterov's smoothing method. In particular,\nby approximating the residual matrix by the outer product of one row of W and\none row of $H$ in MahNMF, we develop an RRI method to iteratively update each\nvariable of $W$ and $H$ in a closed form solution. Although RRI is efficient\nfor small scale MahNMF and some of its extensions, it is neither scalable to\nlarge scale matrices nor flexible enough to optimize all MahNMF extensions.\nSince the objective functions of MahNMF and its extensions are neither convex\nnor smooth, we apply Nesterov's smoothing method to recursively optimize one\nfactor matrix with another matrix fixed. By setting the smoothing parameter\ninversely proportional to the iteration number, we improve the approximation\naccuracy iteratively for both MahNMF and its extensions.\n", "versions": [{"version": "v1", "created": "Sat, 14 Jul 2012 16:19:40 GMT"}], "update_date": "2012-07-17", "authors_parsed": [["Guan", "Naiyang", ""], ["Tao", "Dacheng", ""], ["Luo", "Zhigang", ""], ["Shawe-Taylor", "John", ""]]}, {"id": "1207.3520", "submitter": "Fabian Pedregosa", "authors": "Fabian Pedregosa (INRIA Paris - Rocquencourt), Alexandre Gramfort\n  (LNAO, INRIA Saclay - Ile de France), Ga\\\"el Varoquaux (LNAO, INRIA Saclay -\n  Ile de France), Bertrand Thirion (INRIA Saclay - Ile de France), Christophe\n  Pallier (NEUROSPIN), Elodie Cauvet (NEUROSPIN)", "title": "Improved brain pattern recovery through ranking approaches", "comments": null, "journal-ref": "Pattern Recognition in NeuroImaging (PRNI 2012) (2012)", "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Inferring the functional specificity of brain regions from functional\nMagnetic Resonance Images (fMRI) data is a challenging statistical problem.\nWhile the General Linear Model (GLM) remains the standard approach for brain\nmapping, supervised learning techniques (a.k.a.} decoding) have proven to be\nuseful to capture multivariate statistical effects distributed across voxels\nand brain regions. Up to now, much effort has been made to improve decoding by\nincorporating prior knowledge in the form of a particular regularization term.\nIn this paper we demonstrate that further improvement can be made by accounting\nfor non-linearities using a ranking approach rather than the commonly used\nleast-square regression. Through simulation, we compare the recovery properties\nof our approach to linear models commonly used in fMRI based decoding. We\ndemonstrate the superiority of ranking with a real fMRI dataset.\n", "versions": [{"version": "v1", "created": "Sun, 15 Jul 2012 15:06:35 GMT"}], "update_date": "2012-07-17", "authors_parsed": [["Pedregosa", "Fabian", "", "INRIA Paris - Rocquencourt"], ["Gramfort", "Alexandre", "", "LNAO, INRIA Saclay - Ile de France"], ["Varoquaux", "Ga\u00ebl", "", "LNAO, INRIA Saclay -\n  Ile de France"], ["Thirion", "Bertrand", "", "INRIA Saclay - Ile de France"], ["Pallier", "Christophe", "", "NEUROSPIN"], ["Cauvet", "Elodie", "", "NEUROSPIN"]]}, {"id": "1207.3560", "submitter": "Chathuranga Widanapathirana", "authors": "Chathuranga Widanapathirana, Y. Ahmet Sekercioglu, Paul G.\n  Fitzpatrick, Milosh V. Ivanovich, Jonathan C. Li", "title": "Diagnosing client faults using SVM-based intelligent inference from TCP\n  packet traces", "comments": "2011 6th International Conference on Broadband and Biomedical\n  Communications (IB2COM)", "journal-ref": null, "doi": "10.1109/IB2Com.2011.6217894", "report-no": null, "categories": "cs.NI cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present the Intelligent Automated Client Diagnostic (IACD) system, which\nonly relies on inference from Transmission Control Protocol (TCP) packet traces\nfor rapid diagnosis of client device problems that cause network performance\nissues. Using soft-margin Support Vector Machine (SVM) classifiers, the system\n(i) distinguishes link problems from client problems, and (ii) identifies\ncharacteristics unique to client faults to report the root cause of the client\ndevice problem. Experimental evaluation demonstrated the capability of the IACD\nsystem to distinguish between faulty and healthy links and to diagnose the\nclient faults with 98% accuracy in healthy links. The system can perform fault\ndiagnosis independent of the client's specific TCP implementation, enabling\ndiagnosis capability on diverse range of client computers.\n", "versions": [{"version": "v1", "created": "Mon, 16 Jul 2012 01:08:39 GMT"}], "update_date": "2012-07-17", "authors_parsed": [["Widanapathirana", "Chathuranga", ""], ["Sekercioglu", "Y. Ahmet", ""], ["Fitzpatrick", "Paul G.", ""], ["Ivanovich", "Milosh V.", ""], ["Li", "Jonathan C.", ""]]}, {"id": "1207.3598", "submitter": "Fabian Pedregosa", "authors": "Fabian Pedregosa (INRIA Paris - Rocquencourt, INRIA Saclay - Ile de\n  France), Alexandre Gramfort (INRIA Saclay - Ile de France, LNAO), Ga\\\"el\n  Varoquaux (INRIA Saclay - Ile de France, LNAO), Elodie Cauvet (NEUROSPIN),\n  Christophe Pallier (NEUROSPIN), Bertrand Thirion (INRIA Saclay - Ile de\n  France)", "title": "Learning to rank from medical imaging data", "comments": null, "journal-ref": "MLMI 2012 - 3rd International Workshop on Machine Learning in\n  Medical Imaging (2012)", "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Medical images can be used to predict a clinical score coding for the\nseverity of a disease, a pain level or the complexity of a cognitive task. In\nall these cases, the predicted variable has a natural order. While a standard\nclassifier discards this information, we would like to take it into account in\norder to improve prediction performance. A standard linear regression does\nmodel such information, however the linearity assumption is likely not be\nsatisfied when predicting from pixel intensities in an image. In this paper we\naddress these modeling challenges with a supervised learning procedure where\nthe model aims to order or rank images. We use a linear model for its\nrobustness in high dimension and its possible interpretation. We show on\nsimulations and two fMRI datasets that this approach is able to predict the\ncorrect ordering on pairs of images, yielding higher prediction accuracy than\nstandard regression and multiclass classification techniques.\n", "versions": [{"version": "v1", "created": "Mon, 16 Jul 2012 08:22:36 GMT"}, {"version": "v2", "created": "Sun, 30 Sep 2012 17:04:22 GMT"}], "update_date": "2012-10-02", "authors_parsed": [["Pedregosa", "Fabian", "", "INRIA Paris - Rocquencourt, INRIA Saclay - Ile de\n  France"], ["Gramfort", "Alexandre", "", "INRIA Saclay - Ile de France, LNAO"], ["Varoquaux", "Ga\u00ebl", "", "INRIA Saclay - Ile de France, LNAO"], ["Cauvet", "Elodie", "", "NEUROSPIN"], ["Pallier", "Christophe", "", "NEUROSPIN"], ["Thirion", "Bertrand", "", "INRIA Saclay - Ile de\n  France"]]}, {"id": "1207.3607", "submitter": "Hocine Cherifi", "authors": "Can Demirkesen (BIT Lab, LJK), Hocine Cherifi (BIT Lab, Le2i)", "title": "Fusing image representations for classification using support vector\n  machines", "comments": "Image and Vision Computing New Zealand, 2009. IVCNZ '09. 24th\n  International Conference, Wellington : Nouvelle-Z\\'elande (2009)", "journal-ref": null, "doi": "10.1109/IVCNZ.2009.5378367", "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In order to improve classification accuracy different image representations\nare usually combined. This can be done by using two different fusing schemes.\nIn feature level fusion schemes, image representations are combined before the\nclassification process. In classifier fusion, the decisions taken separately\nbased on individual representations are fused to make a decision. In this paper\nthe main methods derived for both strategies are evaluated. Our experimental\nresults show that classifier fusion performs better. Specifically Bayes belief\nintegration is the best performing strategy for image classification task.\n", "versions": [{"version": "v1", "created": "Mon, 16 Jul 2012 09:23:06 GMT"}], "update_date": "2012-07-17", "authors_parsed": [["Demirkesen", "Can", "", "BIT Lab, LJK"], ["Cherifi", "Hocine", "", "BIT Lab, Le2i"]]}, {"id": "1207.3760", "submitter": "\\\"Onder G\\\"urcan", "authors": "\\\"Onder G\\\"urcan, Carole Bernon, Kemal S. T\\\"urker", "title": "Towards a Self-Organized Agent-Based Simulation Model for Exploration of\n  Human Synaptic Connections", "comments": "4 pages, 1 figure, 2nd Computer Science Student Workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.AI cs.LG nlin.AO", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  In this paper, the early design of our self-organized agent-based simulation\nmodel for exploration of synaptic connections that faithfully generates what is\nobserved in natural situation is given. While we take inspiration from\nneuroscience, our intent is not to create a veridical model of processes in\nneurodevelopmental biology, nor to represent a real biological system. Instead,\nour goal is to design a simulation model that learns acting in the same way of\nhuman nervous system by using findings on human subjects using reflex\nmethodologies in order to estimate unknown connections.\n", "versions": [{"version": "v1", "created": "Mon, 16 Jul 2012 18:41:32 GMT"}], "update_date": "2012-07-17", "authors_parsed": [["G\u00fcrcan", "\u00d6nder", ""], ["Bernon", "Carole", ""], ["T\u00fcrker", "Kemal S.", ""]]}, {"id": "1207.3772", "submitter": "Steve Hanneke", "authors": "Steve Hanneke and Liu Yang", "title": "Surrogate Losses in Passive and Active Learning", "comments": null, "journal-ref": "Electronic Journal of Statistics, Volume 13, Number 2 (2019),\n  4646-4708", "doi": "10.1214/19-EJS1635", "report-no": null, "categories": "math.ST cs.LG stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Active learning is a type of sequential design for supervised machine\nlearning, in which the learning algorithm sequentially requests the labels of\nselected instances from a large pool of unlabeled data points. The objective is\nto produce a classifier of relatively low risk, as measured under the 0-1 loss,\nideally using fewer label requests than the number of random labeled data\npoints sufficient to achieve the same. This work investigates the potential\nuses of surrogate loss functions in the context of active learning.\nSpecifically, it presents an active learning algorithm based on an arbitrary\nclassification-calibrated surrogate loss function, along with an analysis of\nthe number of label requests sufficient for the classifier returned by the\nalgorithm to achieve a given risk under the 0-1 loss. Interestingly, these\nresults cannot be obtained by simply optimizing the surrogate risk via active\nlearning to an extent sufficient to provide a guarantee on the 0-1 loss, as is\ncommon practice in the analysis of surrogate losses for passive learning. Some\nof the results have additional implications for the use of surrogate losses in\npassive learning.\n", "versions": [{"version": "v1", "created": "Mon, 16 Jul 2012 19:26:24 GMT"}, {"version": "v2", "created": "Sat, 11 Jan 2014 02:51:59 GMT"}, {"version": "v3", "created": "Sat, 14 Mar 2015 15:25:54 GMT"}, {"version": "v4", "created": "Wed, 13 Nov 2019 17:30:55 GMT"}], "update_date": "2019-11-14", "authors_parsed": [["Hanneke", "Steve", ""], ["Yang", "Liu", ""]]}, {"id": "1207.3790", "submitter": "Hocine Cherifi", "authors": "Vincent Labatut (BIT Lab), Hocine Cherifi (Le2i)", "title": "Accuracy Measures for the Comparison of Classifiers", "comments": "The 5th International Conference on Information Technology, amman :\n  Jordanie (2011)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The selection of the best classification algorithm for a given dataset is a\nvery widespread problem. It is also a complex one, in the sense it requires to\nmake several important methodological choices. Among them, in this work we\nfocus on the measure used to assess the classification performance and rank the\nalgorithms. We present the most popular measures and discuss their properties.\nDespite the numerous measures proposed over the years, many of them turn out to\nbe equivalent in this specific case, to have interpretation problems, or to be\nunsuitable for our purpose. Consequently, classic overall success rate or\nmarginal rates should be preferred for this specific task.\n", "versions": [{"version": "v1", "created": "Mon, 16 Jul 2012 08:49:34 GMT"}], "update_date": "2012-07-18", "authors_parsed": [["Labatut", "Vincent", "", "BIT Lab"], ["Cherifi", "Hocine", "", "Le2i"]]}, {"id": "1207.3859", "submitter": "Sundeep Rangan", "authors": "Ulugbek S. Kamilov, Sundeep Rangan, Alyson K. Fletcher, Michael Unser", "title": "Approximate Message Passing with Consistent Parameter Estimation and\n  Applications to Sparse Learning", "comments": "14 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.LG math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the estimation of an i.i.d. (possibly non-Gaussian) vector $\\xbf\n\\in \\R^n$ from measurements $\\ybf \\in \\R^m$ obtained by a general cascade model\nconsisting of a known linear transform followed by a probabilistic\ncomponentwise (possibly nonlinear) measurement channel. A novel method, called\nadaptive generalized approximate message passing (Adaptive GAMP), that enables\njoint learning of the statistics of the prior and measurement channel along\nwith estimation of the unknown vector $\\xbf$ is presented. The proposed\nalgorithm is a generalization of a recently-developed EM-GAMP that uses\nexpectation-maximization (EM) iterations where the posteriors in the E-steps\nare computed via approximate message passing. The methodology can be applied to\na large class of learning problems including the learning of sparse priors in\ncompressed sensing or identification of linear-nonlinear cascade models in\ndynamical systems and neural spiking processes. We prove that for large i.i.d.\nGaussian transform matrices the asymptotic componentwise behavior of the\nadaptive GAMP algorithm is predicted by a simple set of scalar state evolution\nequations. In addition, we show that when a certain maximum-likelihood\nestimation can be performed in each step, the adaptive GAMP method can yield\nasymptotically consistent parameter estimates, which implies that the algorithm\nachieves a reconstruction quality equivalent to the oracle algorithm that knows\nthe correct parameter values. Remarkably, this result applies to essentially\narbitrary parametrizations of the unknown distributions, including ones that\nare nonlinear and non-Gaussian. The adaptive GAMP methodology thus provides a\nsystematic, general and computationally efficient method applicable to a large\nrange of complex linear-nonlinear models with provable guarantees.\n", "versions": [{"version": "v1", "created": "Tue, 17 Jul 2012 01:50:46 GMT"}, {"version": "v2", "created": "Thu, 19 Jul 2012 13:35:50 GMT"}, {"version": "v3", "created": "Sat, 1 Dec 2012 23:30:36 GMT"}], "update_date": "2012-12-04", "authors_parsed": [["Kamilov", "Ulugbek S.", ""], ["Rangan", "Sundeep", ""], ["Fletcher", "Alyson K.", ""], ["Unser", "Michael", ""]]}, {"id": "1207.3961", "submitter": "Deniz Akdemir", "authors": "Deniz Akdemir", "title": "Ensemble Clustering with Logic Rules", "comments": "Replacing two articles with one", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this article, the logic rule ensembles approach to supervised learning is\napplied to the unsupervised or semi-supervised clustering. Logic rules which\nwere obtained by combining simple conjunctive rules are used to partition the\ninput space and an ensemble of these rules is used to define a similarity\nmatrix. Similarity partitioning is used to partition the data in an\nhierarchical manner. We have used internal and external measures of cluster\nvalidity to evaluate the quality of clusterings or to identify the number of\nclusters.\n", "versions": [{"version": "v1", "created": "Tue, 17 Jul 2012 11:54:31 GMT"}, {"version": "v2", "created": "Thu, 23 Aug 2012 15:30:34 GMT"}, {"version": "v3", "created": "Thu, 15 Nov 2012 00:44:30 GMT"}], "update_date": "2012-11-16", "authors_parsed": [["Akdemir", "Deniz", ""]]}, {"id": "1207.4089", "submitter": "Mehrdad Gangeh", "authors": "Mehrdad J. Gangeh, Robert P. W. Duin, Bart M. ter Haar Romeny, Mohamed\n  S. Kamel", "title": "A Two-Stage Combined Classifier in Scale Space Texture Classification", "comments": "28 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Textures often show multiscale properties and hence multiscale techniques are\nconsidered useful for texture analysis. Scale-space theory as a biologically\nmotivated approach may be used to construct multiscale textures. In this paper\nvarious ways are studied to combine features on different scales for texture\nclassification of small image patches. We use the N-jet of derivatives up to\nthe second order at different scales to generate distinct pattern\nrepresentations (DPR) of feature subsets. Each feature subset in the DPR is\ngiven to a base classifier (BC) of a two-stage combined classifier. The\ndecisions made by these BCs are combined in two stages over scales and\nderivatives. Various combining systems and their significances and differences\nare discussed. The learning curves are used to evaluate the performances. We\nfound for small sample sizes combining classifiers performs significantly\nbetter than combining feature spaces (CFS). It is also shown that combining\nclassifiers performs better than the support vector machine on CFS in\nmultiscale texture classification.\n", "versions": [{"version": "v1", "created": "Tue, 17 Jul 2012 19:05:18 GMT"}], "update_date": "2015-03-20", "authors_parsed": [["Gangeh", "Mehrdad J.", ""], ["Duin", "Robert P. W.", ""], ["Romeny", "Bart M. ter Haar", ""], ["Kamel", "Mohamed S.", ""]]}, {"id": "1207.4110", "submitter": "Amir Globerson", "authors": "Amir Globerson, Naftali Tishby", "title": "The Minimum Information Principle for Discriminative Learning", "comments": "Appears in Proceedings of the Twentieth Conference on Uncertainty in\n  Artificial Intelligence (UAI2004)", "journal-ref": null, "doi": null, "report-no": "UAI-P-2004-PG-193-200", "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Exponential models of distributions are widely used in machine learning for\nclassiffication and modelling. It is well known that they can be interpreted as\nmaximum entropy models under empirical expectation constraints. In this work,\nwe argue that for classiffication tasks, mutual information is a more suitable\ninformation theoretic measure to be optimized. We show how the principle of\nminimum mutual information generalizes that of maximum entropy, and provides a\ncomprehensive framework for building discriminative classiffiers. A game\ntheoretic interpretation of our approach is then given, and several\ngeneralization bounds provided. We present iterative algorithms for solving the\nminimum information problem and its convex dual, and demonstrate their\nperformance on various classiffication tasks. The results show that minimum\ninformation classiffiers outperform the corresponding maximum entropy models.\n", "versions": [{"version": "v1", "created": "Wed, 11 Jul 2012 14:41:52 GMT"}], "update_date": "2012-07-19", "authors_parsed": [["Globerson", "Amir", ""], ["Tishby", "Naftali", ""]]}, {"id": "1207.4112", "submitter": "Luis David Garcia", "authors": "Luis David Garcia", "title": "Algebraic Statistics in Model Selection", "comments": "Appears in Proceedings of the Twentieth Conference on Uncertainty in\n  Artificial Intelligence (UAI2004)", "journal-ref": null, "doi": null, "report-no": "UAI-P-2004-PG-177-184", "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop the necessary theory in computational algebraic geometry to place\nBayesian networks into the realm of algebraic statistics. We present an\nalgebra{statistics dictionary focused on statistical modeling. In particular,\nwe link the notion of effiective dimension of a Bayesian network with the\nnotion of algebraic dimension of a variety. We also obtain the independence and\nnon{independence constraints on the distributions over the observable variables\nimplied by a Bayesian network with hidden variables, via a generating set of an\nideal of polynomials associated to the network. These results extend previous\nwork on the subject. Finally, the relevance of these results for model\nselection is discussed.\n", "versions": [{"version": "v1", "created": "Wed, 11 Jul 2012 14:42:26 GMT"}], "update_date": "2012-07-19", "authors_parsed": [["Garcia", "Luis David", ""]]}, {"id": "1207.4113", "submitter": "Alex Gammerman", "authors": "Alex Gammerman, Yuri Kalnishkan, Vladimir Vovk", "title": "On-line Prediction with Kernels and the Complexity Approximation\n  Principle", "comments": "Appears in Proceedings of the Twentieth Conference on Uncertainty in\n  Artificial Intelligence (UAI2004)", "journal-ref": null, "doi": null, "report-no": "UAI-P-2004-PG-170-176", "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The paper describes an application of Aggregating Algorithm to the problem of\nregression. It generalizes earlier results concerned with plain linear\nregression to kernel techniques and presents an on-line algorithm which\nperforms nearly as well as any oblivious kernel predictor. The paper contains\nthe derivation of an estimate on the performance of this algorithm. The\nestimate is then used to derive an application of the Complexity Approximation\nPrinciple to kernel methods.\n", "versions": [{"version": "v1", "created": "Wed, 11 Jul 2012 14:42:45 GMT"}], "update_date": "2012-07-19", "authors_parsed": [["Gammerman", "Alex", ""], ["Kalnishkan", "Yuri", ""], ["Vovk", "Vladimir", ""]]}, {"id": "1207.4118", "submitter": "Mathias Drton", "authors": "Mathias Drton, Thomas S. Richardson", "title": "Iterative Conditional Fitting for Gaussian Ancestral Graph Models", "comments": "Appears in Proceedings of the Twentieth Conference on Uncertainty in\n  Artificial Intelligence (UAI2004)", "journal-ref": null, "doi": null, "report-no": "UAI-P-2004-PG-130-137", "categories": "stat.ME cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Ancestral graph models, introduced by Richardson and Spirtes (2002),\ngeneralize both Markov random fields and Bayesian networks to a class of graphs\nwith a global Markov property that is closed under conditioning and\nmarginalization. By design, ancestral graphs encode precisely the conditional\nindependence structures that can arise from Bayesian networks with selection\nand unobserved (hidden/latent) variables. Thus, ancestral graph models provide\na potentially very useful framework for exploratory model selection when\nunobserved variables might be involved in the data-generating process but no\nparticular hidden structure can be specified. In this paper, we present the\nIterative Conditional Fitting (ICF) algorithm for maximum likelihood estimation\nin Gaussian ancestral graph models. The name reflects that in each step of the\nprocedure a conditional distribution is estimated, subject to constraints,\nwhile a marginal distribution is held fixed. This approach is in duality to the\nwell-known Iterative Proportional Fitting algorithm, in which marginal\ndistributions are fitted while conditional distributions are held fixed.\n", "versions": [{"version": "v1", "created": "Wed, 11 Jul 2012 14:44:26 GMT"}], "update_date": "2012-07-19", "authors_parsed": [["Drton", "Mathias", ""], ["Richardson", "Thomas S.", ""]]}, {"id": "1207.4125", "submitter": "Wray L. Buntine", "authors": "Wray L. Buntine, Aleks Jakulin", "title": "Applying Discrete PCA in Data Analysis", "comments": "Appears in Proceedings of the Twentieth Conference on Uncertainty in\n  Artificial Intelligence (UAI2004)", "journal-ref": null, "doi": null, "report-no": "UAI-P-2004-PG-59-66", "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Methods for analysis of principal components in discrete data have existed\nfor some time under various names such as grade of membership modelling,\nprobabilistic latent semantic analysis, and genotype inference with admixture.\nIn this paper we explore a number of extensions to the common theory, and\npresent some application of these methods to some common statistical tasks. We\nshow that these methods can be interpreted as a discrete version of ICA. We\ndevelop a hierarchical version yielding components at different levels of\ndetail, and additional techniques for Gibbs sampling. We compare the algorithms\non a text prediction task using support vector machines, and to information\nretrieval.\n", "versions": [{"version": "v1", "created": "Wed, 11 Jul 2012 14:46:50 GMT"}], "update_date": "2012-07-19", "authors_parsed": [["Buntine", "Wray L.", ""], ["Jakulin", "Aleks", ""]]}, {"id": "1207.4131", "submitter": "Yasemin Altun", "authors": "Yasemin Altun, Alex Smola, Thomas Hofmann", "title": "Exponential Families for Conditional Random Fields", "comments": "Appears in Proceedings of the Twentieth Conference on Uncertainty in\n  Artificial Intelligence (UAI2004)", "journal-ref": null, "doi": null, "report-no": "UAI-P-2004-PG-2-9", "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we de ne conditional random elds in reproducing kernel Hilbert\nspaces and show connections to Gaussian Process classi cation. More speci\ncally, we prove decomposition results for undirected graphical models and we\ngive constructions for kernels. Finally we present e cient means of solving the\noptimization problem using reduced rank decompositions and we show how\nstationarity can be exploited e ciently in the optimization process.\n", "versions": [{"version": "v1", "created": "Wed, 11 Jul 2012 14:48:54 GMT"}], "update_date": "2012-07-19", "authors_parsed": [["Altun", "Yasemin", ""], ["Smola", "Alex", ""], ["Hofmann", "Thomas", ""]]}, {"id": "1207.4132", "submitter": "Rodney Nielsen", "authors": "Rodney Nielsen", "title": "MOB-ESP and other Improvements in Probability Estimation", "comments": "Appears in Proceedings of the Twentieth Conference on Uncertainty in\n  Artificial Intelligence (UAI2004)", "journal-ref": null, "doi": null, "report-no": "UAI-P-2004-PG-418-425", "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A key prerequisite to optimal reasoning under uncertainty in intelligent\nsystems is to start with good class probability estimates. This paper improves\non the current best probability estimation trees (Bagged-PETs) and also\npresents a new ensemble-based algorithm (MOB-ESP). Comparisons are made using\nseveral benchmark datasets and multiple metrics. These experiments show that\nMOB-ESP outputs significantly more accurate class probabilities than either the\nbaseline BPETs algorithm or the enhanced version presented here (EB-PETs).\nThese results are based on metrics closely associated with the average accuracy\nof the predictions. MOB-ESP also provides much better probability rankings than\nB-PETs. The paper further suggests how these estimation techniques can be\napplied in concert with a broader category of classifiers.\n", "versions": [{"version": "v1", "created": "Wed, 11 Jul 2012 14:51:03 GMT"}], "update_date": "2012-07-19", "authors_parsed": [["Nielsen", "Rodney", ""]]}, {"id": "1207.4133", "submitter": "Iftach Nachman", "authors": "Iftach Nachman, Gal Elidan, Nir Friedman", "title": "\"Ideal Parent\" Structure Learning for Continuous Variable Networks", "comments": "Appears in Proceedings of the Twentieth Conference on Uncertainty in\n  Artificial Intelligence (UAI2004)", "journal-ref": null, "doi": null, "report-no": "UAI-P-2004-PG-400-409", "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, there is a growing interest in learning Bayesian networks\nwith continuous variables. Learning the structure of such networks is a\ncomputationally expensive procedure, which limits most applications to\nparameter learning. This problem is even more acute when learning networks with\nhidden variables. We present a general method for significantly speeding the\nstructure search algorithm for continuous variable networks with common\nparametric distributions. Importantly, our method facilitates the addition of\nnew hidden variables into the network structure efficiently. We demonstrate the\nmethod on several data sets, both for learning structure on fully observable\ndata, and for introducing new hidden variables during structure search.\n", "versions": [{"version": "v1", "created": "Wed, 11 Jul 2012 14:51:23 GMT"}], "update_date": "2012-07-19", "authors_parsed": [["Nachman", "Iftach", ""], ["Elidan", "Gal", ""], ["Friedman", "Nir", ""]]}, {"id": "1207.4134", "submitter": "Iain Murray", "authors": "Iain Murray, Zoubin Ghahramani", "title": "Bayesian Learning in Undirected Graphical Models: Approximate MCMC\n  algorithms", "comments": "Appears in Proceedings of the Twentieth Conference on Uncertainty in\n  Artificial Intelligence (UAI2004)", "journal-ref": null, "doi": null, "report-no": "UAI-P-2004-PG-392-399", "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bayesian learning in undirected graphical models|computing posterior\ndistributions over parameters and predictive quantities is exceptionally\ndifficult. We conjecture that for general undirected models, there are no\ntractable MCMC (Markov Chain Monte Carlo) schemes giving the correct\nequilibrium distribution over parameters. While this intractability, due to the\npartition function, is familiar to those performing parameter optimisation,\nBayesian learning of posterior distributions over undirected model parameters\nhas been unexplored and poses novel challenges. we propose several approximate\nMCMC schemes and test on fully observed binary models (Boltzmann machines) for\na small coronary heart disease data set and larger artificial systems. While\napproximations must perform well on the model, their interaction with the\nsampling scheme is also important. Samplers based on variational mean- field\napproximations generally performed poorly, more advanced methods using loopy\npropagation, brief sampling and stochastic dynamics lead to acceptable\nparameter posteriors. Finally, we demonstrate these techniques on a Markov\nrandom field with hidden variables.\n", "versions": [{"version": "v1", "created": "Wed, 11 Jul 2012 14:51:41 GMT"}], "update_date": "2012-07-19", "authors_parsed": [["Murray", "Iain", ""], ["Ghahramani", "Zoubin", ""]]}, {"id": "1207.4138", "submitter": "Omid Madani", "authors": "Omid Madani, Daniel J. Lizotte, Russell Greiner", "title": "Active Model Selection", "comments": "Appears in Proceedings of the Twentieth Conference on Uncertainty in\n  Artificial Intelligence (UAI2004)", "journal-ref": null, "doi": null, "report-no": "UAI-P-2004-PG-357-365", "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Classical learning assumes the learner is given a labeled data sample, from\nwhich it learns a model. The field of Active Learning deals with the situation\nwhere the learner begins not with a training sample, but instead with resources\nthat it can use to obtain information to help identify the optimal model. To\nbetter understand this task, this paper presents and analyses the simplified\n\"(budgeted) active model selection\" version, which captures the pure\nexploration aspect of many active learning problems in a clean and simple\nproblem formulation. Here the learner can use a fixed budget of \"model probes\"\n(where each probe evaluates the specified model on a random indistinguishable\ninstance) to identify which of a given set of possible models has the highest\nexpected accuracy. Our goal is a policy that sequentially determines which\nmodel to probe next, based on the information observed so far. We present a\nformal description of this task, and show that it is NPhard in general. We then\ninvestigate a number of algorithms for this task, including several existing\nones (eg, \"Round-Robin\", \"Interval Estimation\", \"Gittins\") as well as some\nnovel ones (e.g., \"Biased-Robin\"), describing first their approximation\nproperties and then their empirical performance on various problem instances.\nWe observe empirically that the simple biased-robin algorithm significantly\noutperforms the other algorithms in the case of identical costs and priors.\n", "versions": [{"version": "v1", "created": "Wed, 11 Jul 2012 14:52:51 GMT"}], "update_date": "2012-07-19", "authors_parsed": [["Madani", "Omid", ""], ["Lizotte", "Daniel J.", ""], ["Greiner", "Russell", ""]]}, {"id": "1207.4139", "submitter": "Guy Lebanon", "authors": "Guy Lebanon", "title": "An Extended Cencov-Campbell Characterization of Conditional Information\n  Geometry", "comments": "Appears in Proceedings of the Twentieth Conference on Uncertainty in\n  Artificial Intelligence (UAI2004)", "journal-ref": null, "doi": null, "report-no": "UAI-P-2004-PG-341-348", "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We formulate and prove an axiomatic characterization of conditional\ninformation geometry, for both the normalized and the nonnormalized cases. This\ncharacterization extends the axiomatic derivation of the Fisher geometry by\nCencov and Campbell to the cone of positive conditional models, and as a\nspecial case to the manifold of conditional distributions. Due to the close\nconnection between the conditional I-divergence and the product Fisher\ninformation metric the characterization provides a new axiomatic interpretation\nof the primal problems underlying logistic regression and AdaBoost.\n", "versions": [{"version": "v1", "created": "Wed, 11 Jul 2012 14:53:33 GMT"}], "update_date": "2012-07-19", "authors_parsed": [["Lebanon", "Guy", ""]]}, {"id": "1207.4142", "submitter": "Sergey Kirshner", "authors": "Sergey Kirshner, Padhraic Smyth, Andrew Robertson", "title": "Conditional Chow-Liu Tree Structures for Modeling Discrete-Valued Vector\n  Time Series", "comments": "Appears in Proceedings of the Twentieth Conference on Uncertainty in\n  Artificial Intelligence (UAI2004)", "journal-ref": null, "doi": null, "report-no": "UAI-P-2004-PG-317-324", "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of modeling discrete-valued vector time series data\nusing extensions of Chow-Liu tree models to capture both dependencies across\ntime and dependencies across variables. Conditional Chow-Liu tree models are\nintroduced, as an extension to standard Chow-Liu trees, for modeling\nconditional rather than joint densities. We describe learning algorithms for\nsuch models and show how they can be used to learn parsimonious representations\nfor the output distributions in hidden Markov models. These models are applied\nto the important problem of simulating and forecasting daily precipitation\noccurrence for networks of rain stations. To demonstrate the effectiveness of\nthe models, we compare their performance versus a number of alternatives using\nhistorical precipitation data from Southwestern Australia and the Western\nUnited States. We illustrate how the structure and parameters of the models can\nbe used to provide an improved meteorological interpretation of such data.\n", "versions": [{"version": "v1", "created": "Wed, 11 Jul 2012 14:54:25 GMT"}], "update_date": "2012-07-19", "authors_parsed": [["Kirshner", "Sergey", ""], ["Smyth", "Padhraic", ""], ["Robertson", "Andrew", ""]]}, {"id": "1207.4144", "submitter": "Joseph Kahn", "authors": "Joseph Kahn", "title": "A Generative Bayesian Model for Aggregating Experts' Probabilities", "comments": "Appears in Proceedings of the Twentieth Conference on Uncertainty in\n  Artificial Intelligence (UAI2004)", "journal-ref": null, "doi": null, "report-no": "UAI-P-2004-PG-301-308", "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In order to improve forecasts, a decisionmaker often combines probabilities\ngiven by various sources, such as human experts and machine learning\nclassifiers. When few training data are available, aggregation can be improved\nby incorporating prior knowledge about the event being forecasted and about\nsalient properties of the experts. To this end, we develop a generative\nBayesian aggregation model for probabilistic classi cation. The model includes\nan event-specific prior, measures of individual experts' bias, calibration,\naccuracy, and a measure of dependence betweeen experts. Rather than require\nabsolute measures, we show that aggregation may be expressed in terms of\nrelative accuracy between experts. The model results in a weighted logarithmic\nopinion pool (LogOps) that satis es consistency criteria such as the external\nBayesian property. We derive analytic solutions for independent and for\nexchangeable experts. Empirical tests demonstrate the model's use, comparing\nits accuracy with other aggregation methods.\n", "versions": [{"version": "v1", "created": "Wed, 11 Jul 2012 14:54:55 GMT"}], "update_date": "2012-07-19", "authors_parsed": [["Kahn", "Joseph", ""]]}, {"id": "1207.4146", "submitter": "Rong Jin", "authors": "Rong Jin, Luo Si", "title": "A Bayesian Approach toward Active Learning for Collaborative Filtering", "comments": "Appears in Proceedings of the Twentieth Conference on Uncertainty in\n  Artificial Intelligence (UAI2004)", "journal-ref": null, "doi": null, "report-no": "UAI-P-2004-PG-278-285", "categories": "cs.LG cs.IR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Collaborative filtering is a useful technique for exploiting the preference\npatterns of a group of users to predict the utility of items for the active\nuser. In general, the performance of collaborative filtering depends on the\nnumber of rated examples given by the active user. The more the number of rated\nexamples given by the active user, the more accurate the predicted ratings will\nbe. Active learning provides an effective way to acquire the most informative\nrated examples from active users. Previous work on active learning for\ncollaborative filtering only considers the expected loss function based on the\nestimated model, which can be misleading when the estimated model is\ninaccurate. This paper takes one step further by taking into account of the\nposterior distribution of the estimated model, which results in more robust\nactive learning algorithm. Empirical studies with datasets of movie ratings\nshow that when the number of ratings from the active user is restricted to be\nsmall, active learning methods only based on the estimated model don't perform\nwell while the active learning method using the model distribution achieves\nsubstantially better performance.\n", "versions": [{"version": "v1", "created": "Wed, 11 Jul 2012 14:55:41 GMT"}], "update_date": "2012-07-19", "authors_parsed": [["Jin", "Rong", ""], ["Si", "Luo", ""]]}, {"id": "1207.4148", "submitter": "Andrew Howard", "authors": "Andrew Howard, Tony S. Jebara", "title": "Dynamical Systems Trees", "comments": "Appears in Proceedings of the Twentieth Conference on Uncertainty in\n  Artificial Intelligence (UAI2004)", "journal-ref": null, "doi": null, "report-no": "UAI-P-2004-PG-260-267", "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose dynamical systems trees (DSTs) as a flexible class of models for\ndescribing multiple processes that interact via a hierarchy of aggregating\nparent chains. DSTs extend Kalman filters, hidden Markov models and nonlinear\ndynamical systems to an interactive group scenario. Various individual\nprocesses interact as communities and sub-communities in a tree structure that\nis unrolled in time. To accommodate nonlinear temporal activity, each\nindividual leaf process is modeled as a dynamical system containing discrete\nand/or continuous hidden states with discrete and/or Gaussian emissions.\nSubsequent higher level parent processes act like hidden Markov models and\nmediate the interaction between leaf processes or between other parent\nprocesses in the hierarchy. Aggregator chains are parents of child processes\nthat they combine and mediate, yielding a compact overall parameterization. We\nprovide tractable inference and learning algorithms for arbitrary DST\ntopologies via an efficient structured mean-field algorithm. The diverse\napplicability of DSTs is demonstrated by experiments on gene expression data\nand by modeling group behavior in the setting of an American football game.\n", "versions": [{"version": "v1", "created": "Wed, 11 Jul 2012 14:56:09 GMT"}], "update_date": "2012-07-19", "authors_parsed": [["Howard", "Andrew", ""], ["Jebara", "Tony S.", ""]]}, {"id": "1207.4149", "submitter": "Firas Hamze", "authors": "Firas Hamze, Nando de Freitas", "title": "From Fields to Trees", "comments": "Appears in Proceedings of the Twentieth Conference on Uncertainty in\n  Artificial Intelligence (UAI2004)", "journal-ref": null, "doi": null, "report-no": "UAI-P-2004-PG-243-250", "categories": "stat.CO cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present new MCMC algorithms for computing the posterior distributions and\nexpectations of the unknown variables in undirected graphical models with\nregular structure. For demonstration purposes, we focus on Markov Random Fields\n(MRFs). By partitioning the MRFs into non-overlapping trees, it is possible to\ncompute the posterior distribution of a particular tree exactly by conditioning\non the remaining tree. These exact solutions allow us to construct efficient\nblocked and Rao-Blackwellised MCMC algorithms. We show empirically that tree\nsampling is considerably more efficient than other partitioned sampling schemes\nand the naive Gibbs sampler, even in cases where loopy belief propagation fails\nto converge. We prove that tree sampling exhibits lower variance than the naive\nGibbs sampler and other naive partitioning schemes using the theoretical\nmeasure of maximal correlation. We also construct new information theory tools\nfor comparing different MCMC schemes and show that, under these, tree sampling\nis more efficient.\n", "versions": [{"version": "v1", "created": "Wed, 11 Jul 2012 14:56:43 GMT"}], "update_date": "2012-07-19", "authors_parsed": [["Hamze", "Firas", ""], ["de Freitas", "Nando", ""]]}, {"id": "1207.4151", "submitter": "Mukund Narasimhan", "authors": "Mukund Narasimhan, Jeff A. Bilmes", "title": "PAC-learning bounded tree-width Graphical Models", "comments": "Appears in Proceedings of the Twentieth Conference on Uncertainty in\n  Artificial Intelligence (UAI2004)", "journal-ref": null, "doi": null, "report-no": "UAI-P-2004-PG-410-417", "categories": "cs.LG cs.DS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We show that the class of strongly connected graphical models with treewidth\nat most k can be properly efficiently PAC-learnt with respect to the\nKullback-Leibler Divergence. Previous approaches to this problem, such as those\nof Chow ([1]), and Ho gen ([7]) have shown that this class is PAC-learnable by\nreducing it to a combinatorial optimization problem. However, for k > 1, this\nproblem is NP-complete ([15]), and so unless P=NP, these approaches will take\nexponential amounts of time. Our approach differs significantly from these, in\nthat it first attempts to find approximate conditional independencies by\nsolving (polynomially many) submodular optimization problems, and then using a\ndynamic programming formulation to combine the approximate conditional\nindependence information to derive a graphical model with underlying graph of\nthe tree-width specified. This gives us an efficient (polynomial time in the\nnumber of random variables) PAC-learning algorithm which requires only\npolynomial number of samples of the true distribution, and only polynomial\nrunning time.\n", "versions": [{"version": "v1", "created": "Wed, 11 Jul 2012 14:57:38 GMT"}], "update_date": "2012-07-19", "authors_parsed": [["Narasimhan", "Mukund", ""], ["Bilmes", "Jeff A.", ""]]}, {"id": "1207.4152", "submitter": "Lawrence Zitnick", "authors": "Lawrence Zitnick, Takeo Kanade", "title": "Maximum Entropy for Collaborative Filtering", "comments": "Appears in Proceedings of the Twentieth Conference on Uncertainty in\n  Artificial Intelligence (UAI2004)", "journal-ref": null, "doi": null, "report-no": "UAI-P-2004-PG-636-643", "categories": "cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Within the task of collaborative filtering two challenges for computing\nconditional probabilities exist. First, the amount of training data available\nis typically sparse with respect to the size of the domain. Thus, support for\nhigher-order interactions is generally not present. Second, the variables that\nwe are conditioning upon vary for each query. That is, users label different\nvariables during each query. For this reason, there is no consistent input to\noutput mapping. To address these problems we purpose a maximum entropy approach\nusing a non-standard measure of entropy. This approach can be simplified to\nsolving a set of linear equations that can be efficiently solved.\n", "versions": [{"version": "v1", "created": "Wed, 11 Jul 2012 14:59:15 GMT"}], "update_date": "2012-07-19", "authors_parsed": [["Zitnick", "Lawrence", ""], ["Kanade", "Takeo", ""]]}, {"id": "1207.4155", "submitter": "Xuejian Xiong", "authors": "Xuejian Xiong, Kap Chan, Kian Lee Tan", "title": "Similarity-Driven Cluster Merging Method for Unsupervised Fuzzy\n  Clustering", "comments": "Appears in Proceedings of the Twentieth Conference on Uncertainty in\n  Artificial Intelligence (UAI2004)", "journal-ref": null, "doi": null, "report-no": "UAI-P-2004-PG-611-618", "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, a similarity-driven cluster merging method is proposed for\nunsuper-vised fuzzy clustering. The cluster merging method is used to resolve\nthe problem of cluster validation. Starting with an overspecified number of\nclusters in the data, pairs of similar clusters are merged based on the\nproposed similarity-driven cluster merging criterion. The similarity between\nclusters is calculated by a fuzzy cluster similarity matrix, while an adaptive\nthreshold is used for merging. In addition, a modified generalized ob- jective\nfunction is used for prototype-based fuzzy clustering. The function includes\nthe p-norm distance measure as well as principal components of the clusters.\nThe number of the principal components is determined automatically from the\ndata being clustered. The properties of this unsupervised fuzzy clustering\nalgorithm are illustrated by several experiments.\n", "versions": [{"version": "v1", "created": "Wed, 11 Jul 2012 14:59:55 GMT"}], "update_date": "2012-07-19", "authors_parsed": [["Xiong", "Xuejian", ""], ["Chan", "Kap", ""], ["Tan", "Kian Lee", ""]]}, {"id": "1207.4156", "submitter": "Eric P. Xing", "authors": "Eric P. Xing, Michael I. Jordan, Stuart Russell", "title": "Graph partition strategies for generalized mean field inference", "comments": "Appears in Proceedings of the Twentieth Conference on Uncertainty in\n  Artificial Intelligence (UAI2004)", "journal-ref": null, "doi": null, "report-no": "UAI-P-2004-PG-602-610", "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An autonomous variational inference algorithm for arbitrary graphical models\nrequires the ability to optimize variational approximations over the space of\nmodel parameters as well as over the choice of tractable families used for the\nvariational approximation. In this paper, we present a novel combination of\ngraph partitioning algorithms with a generalized mean field (GMF) inference\nalgorithm. This combination optimizes over disjoint clustering of variables and\nperforms inference using those clusters. We provide a formal analysis of the\nrelationship between the graph cut and the GMF approximation, and explore\nseveral graph partition strategies empirically. Our empirical results provide\nrather clear support for a weighted version of MinCut as a useful clustering\nalgorithm for GMF inference, which is consistent with the implications from the\nformal analysis.\n", "versions": [{"version": "v1", "created": "Wed, 11 Jul 2012 15:00:11 GMT"}], "update_date": "2012-07-19", "authors_parsed": [["Xing", "Eric P.", ""], ["Jordan", "Michael I.", ""], ["Russell", "Stuart", ""]]}, {"id": "1207.4157", "submitter": "Ben Wellner", "authors": "Ben Wellner, Andrew McCallum, Fuchun Peng, Michael Hay", "title": "An Integrated, Conditional Model of Information Extraction and\n  Coreference with Applications to Citation Matching", "comments": "Appears in Proceedings of the Twentieth Conference on Uncertainty in\n  Artificial Intelligence (UAI2004)", "journal-ref": null, "doi": null, "report-no": "UAI-P-2004-PG-593-601", "categories": "cs.LG cs.DL cs.IR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Although information extraction and coreference resolution appear together in\nmany applications, most current systems perform them as ndependent steps. This\npaper describes an approach to integrated inference for extraction and\ncoreference based on conditionally-trained undirected graphical models. We\ndiscuss the advantages of conditional probability training, and of a\ncoreference model structure based on graph partitioning. On a data set of\nresearch paper citations, we show significant reduction in error by using\nextraction uncertainty to improve coreference citation matching accuracy, and\nusing coreference to improve the accuracy of the extracted fields.\n", "versions": [{"version": "v1", "created": "Wed, 11 Jul 2012 15:00:28 GMT"}], "update_date": "2012-07-19", "authors_parsed": [["Wellner", "Ben", ""], ["McCallum", "Andrew", ""], ["Peng", "Fuchun", ""], ["Hay", "Michael", ""]]}, {"id": "1207.4158", "submitter": "Max Welling", "authors": "Max Welling", "title": "On the Choice of Regions for Generalized Belief Propagation", "comments": "Appears in Proceedings of the Twentieth Conference on Uncertainty in\n  Artificial Intelligence (UAI2004)", "journal-ref": null, "doi": null, "report-no": "UAI-P-2004-PG-585-592", "categories": "cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generalized belief propagation (GBP) has proven to be a promising technique\nfor approximate inference tasks in AI and machine learning. However, the choice\nof a good set of clusters to be used in GBP has remained more of an art then a\nscience until this day. This paper proposes a sequential approach to adding new\nclusters of nodes and their interactions (i.e. \"regions\") to the approximation.\nWe first review and analyze the recently introduced region graphs and find that\nthree kinds of operations (\"split\", \"merge\" and \"death\") leave the free energy\nand (under some conditions) the fixed points of GBP invariant. This leads to\nthe notion of \"weakly irreducible\" regions as the natural candidates to be\nadded to the approximation. Computational complexity of the GBP algorithm is\ncontrolled by restricting attention to regions with small \"region-width\".\nCombining the above with an efficient (i.e. local in the graph) measure to\npredict the improved accuracy of GBP leads to the sequential \"region pursuit\"\nalgorithm for adding new regions bottom-up to the region graph. Experiments\nshow that this algorithm can indeed perform close to optimally.\n", "versions": [{"version": "v1", "created": "Wed, 11 Jul 2012 15:01:36 GMT"}], "update_date": "2012-07-19", "authors_parsed": [["Welling", "Max", ""]]}, {"id": "1207.4162", "submitter": "Bo Thiesson", "authors": "Bo Thiesson, David Maxwell Chickering, David Heckerman, Christopher\n  Meek", "title": "ARMA Time-Series Modeling with Graphical Models", "comments": "Appears in Proceedings of the Twentieth Conference on Uncertainty in\n  Artificial Intelligence (UAI2004)", "journal-ref": null, "doi": null, "report-no": "UAI-P-2004-PG-552-560", "categories": "stat.AP cs.LG stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We express the classic ARMA time-series model as a directed graphical model.\nIn doing so, we find that the deterministic relationships in the model make it\neffectively impossible to use the EM algorithm for learning model parameters.\nTo remedy this problem, we replace the deterministic relationships with\nGaussian distributions having a small variance, yielding the stochastic ARMA\n(ARMA) model. This modification allows us to use the EM algorithm to learn\nparmeters and to forecast,even in situations where some data is missing. This\nmodification, in conjunction with the graphicalmodel approach, also allows us\nto include cross predictors in situations where there are multiple times series\nand/or additional nontemporal covariates. More surprising,experiments suggest\nthat the move to stochastic ARMA yields improved accuracy through better\nsmoothing. We demonstrate improvements afforded by cross prediction and better\nsmoothing on real data.\n", "versions": [{"version": "v1", "created": "Wed, 11 Jul 2012 15:03:00 GMT"}, {"version": "v2", "created": "Wed, 8 Aug 2012 20:45:28 GMT"}], "update_date": "2012-08-10", "authors_parsed": [["Thiesson", "Bo", ""], ["Chickering", "David Maxwell", ""], ["Heckerman", "David", ""], ["Meek", "Christopher", ""]]}, {"id": "1207.4164", "submitter": "Chris Stauffer", "authors": "Chris Stauffer", "title": "Factored Latent Analysis for far-field tracking data", "comments": "Appears in Proceedings of the Twentieth Conference on Uncertainty in\n  Artificial Intelligence (UAI2004)", "journal-ref": null, "doi": null, "report-no": "UAI-P-2004-PG-536-543", "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper uses Factored Latent Analysis (FLA) to learn a factorized,\nsegmental representation for observations of tracked objects over time.\nFactored Latent Analysis is latent class analysis in which the observation\nspace is subdivided and each aspect of the original space is represented by a\nseparate latent class model. One could simply treat these factors as completely\nindependent and ignore their interdependencies or one could concatenate them\ntogether and attempt to learn latent class structure for the complete\nobservation space. Alternatively, FLA allows the interdependencies to be\nexploited in estimating an effective model, which is also capable of\nrepresenting a factored latent state. In this paper, FLA is used to learn a set\nof factored latent classes to represent different modalities of observations of\ntracked objects. Different characteristics of the state of tracked objects are\neach represented by separate latent class models, including normalized size,\nnormalized speed, normalized direction, and position. This model also enables\neffective temporal segmentation of these sequences. This method is data-driven,\nunsupervised using only pairwise observation statistics. This data-driven and\nunsupervised activity classi- fication technique exhibits good performance in\nmultiple challenging environments.\n", "versions": [{"version": "v1", "created": "Wed, 11 Jul 2012 15:03:34 GMT"}], "update_date": "2012-07-19", "authors_parsed": [["Stauffer", "Chris", ""]]}, {"id": "1207.4167", "submitter": "Satinder Singh", "authors": "Satinder Singh, Michael James, Matthew Rudary", "title": "Predictive State Representations: A New Theory for Modeling Dynamical\n  Systems", "comments": "Appears in Proceedings of the Twentieth Conference on Uncertainty in\n  Artificial Intelligence (UAI2004)", "journal-ref": null, "doi": null, "report-no": "UAI-P-2004-PG-512-519", "categories": "cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modeling dynamical systems, both for control purposes and to make predictions\nabout their behavior, is ubiquitous in science and engineering. Predictive\nstate representations (PSRs) are a recently introduced class of models for\ndiscrete-time dynamical systems. The key idea behind PSRs and the closely\nrelated OOMs (Jaeger's observable operator models) is to represent the state of\nthe system as a set of predictions of observable outcomes of experiments one\ncan do in the system. This makes PSRs rather different from history-based\nmodels such as nth-order Markov models and hidden-state-based models such as\nHMMs and POMDPs. We introduce an interesting construct, the systemdynamics\nmatrix, and show how PSRs can be derived simply from it. We also use this\nconstruct to show formally that PSRs are more general than both nth-order\nMarkov models and HMMs/POMDPs. Finally, we discuss the main difference between\nPSRs and OOMs and conclude with directions for future work.\n", "versions": [{"version": "v1", "created": "Wed, 11 Jul 2012 15:05:10 GMT"}], "update_date": "2012-07-19", "authors_parsed": [["Singh", "Satinder", ""], ["James", "Michael", ""], ["Rudary", "Matthew", ""]]}, {"id": "1207.4169", "submitter": "Michal Rosen-Zvi", "authors": "Michal Rosen-Zvi, Thomas Griffiths, Mark Steyvers, Padhraic Smyth", "title": "The Author-Topic Model for Authors and Documents", "comments": "Appears in Proceedings of the Twentieth Conference on Uncertainty in\n  Artificial Intelligence (UAI2004)", "journal-ref": null, "doi": null, "report-no": "UAI-P-2004-PG-487-494", "categories": "cs.IR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce the author-topic model, a generative model for documents that\nextends Latent Dirichlet Allocation (LDA; Blei, Ng, & Jordan, 2003) to include\nauthorship information. Each author is associated with a multinomial\ndistribution over topics and each topic is associated with a multinomial\ndistribution over words. A document with multiple authors is modeled as a\ndistribution over topics that is a mixture of the distributions associated with\nthe authors. We apply the model to a collection of 1,700 NIPS conference papers\nand 160,000 CiteSeer abstracts. Exact inference is intractable for these\ndatasets and we use Gibbs sampling to estimate the topic and author\ndistributions. We compare the performance with two other generative models for\ndocuments, which are special cases of the author-topic model: LDA (a topic\nmodel) and a simple author model in which each author is associated with a\ndistribution over words rather than a distribution over topics. We show topics\nrecovered by the author-topic model, and demonstrate applications to computing\nsimilarity between authors and entropy of author output.\n", "versions": [{"version": "v1", "created": "Wed, 11 Jul 2012 15:05:53 GMT"}], "update_date": "2012-07-19", "authors_parsed": [["Rosen-Zvi", "Michal", ""], ["Griffiths", "Thomas", ""], ["Steyvers", "Mark", ""], ["Smyth", "Padhraic", ""]]}, {"id": "1207.4172", "submitter": "Pradeep Ravikumar", "authors": "Pradeep Ravikumar, John Lafferty", "title": "Variational Chernoff Bounds for Graphical Models", "comments": "Appears in Proceedings of the Twentieth Conference on Uncertainty in\n  Artificial Intelligence (UAI2004)", "journal-ref": null, "doi": null, "report-no": "UAI-P-2004-PG-462-469", "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent research has made significant progress on the problem of bounding log\npartition functions for exponential family graphical models. Such bounds have\nassociated dual parameters that are often used as heuristic estimates of the\nmarginal probabilities required in inference and learning. However these\nvariational estimates do not give rigorous bounds on marginal probabilities,\nnor do they give estimates for probabilities of more general events than simple\nmarginals. In this paper we build on this recent work by deriving rigorous\nupper and lower bounds on event probabilities for graphical models. Our\napproach is based on the use of generalized Chernoff bounds to express bounds\non event probabilities in terms of convex optimization problems; these\noptimization problems, in turn, require estimates of generalized log partition\nfunctions. Simulations indicate that this technique can result in useful,\nrigorous bounds to complement the heuristic variational estimates, with\ncomparable computational cost.\n", "versions": [{"version": "v1", "created": "Wed, 11 Jul 2012 15:06:59 GMT"}], "update_date": "2012-07-19", "authors_parsed": [["Ravikumar", "Pradeep", ""], ["Lafferty", "John", ""]]}, {"id": "1207.4180", "submitter": "Pradeep Ravikumar", "authors": "Pradeep Ravikumar, William Cohen", "title": "A Hierarchical Graphical Model for Record Linkage", "comments": "Appears in Proceedings of the Twentieth Conference on Uncertainty in\n  Artificial Intelligence (UAI2004)", "journal-ref": null, "doi": null, "report-no": "UAI-P-2004-PG-454-461", "categories": "cs.LG cs.IR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The task of matching co-referent records is known among other names as rocord\nlinkage. For large record-linkage problems, often there is little or no labeled\ndata available, but unlabeled data shows a reasonable clear structure. For such\nproblems, unsupervised or semi-supervised methods are preferable to supervised\nmethods. In this paper, we describe a hierarchical graphical model framework\nfor the linakge-problem in an unsupervised setting. In addition to proposing\nnew methods, we also cast existing unsupervised probabilistic record-linkage\nmethods in this framework. Some of the techniques we propose to minimize\noverfitting in the above model are of interest in the general graphical model\nsetting. We describe a method for incorporating monotinicity constraints in a\ngraphical model. We also outline a bootstrapping approach of using\n\"single-field\" classifiers to noisily label latent variables in a hierarchical\nmodel. Experimental results show that our proposed unsupervised methods perform\nquite competitively even with fully supervised record-linkage methods.\n", "versions": [{"version": "v1", "created": "Thu, 12 Jul 2012 19:48:03 GMT"}], "update_date": "2012-07-19", "authors_parsed": [["Ravikumar", "Pradeep", ""], ["Cohen", "William", ""]]}, {"id": "1207.4255", "submitter": "Jean Honorio", "authors": "Jean Honorio, Tommi Jaakkola and Dimitris Samaras", "title": "On the Statistical Efficiency of $\\ell_{1,p}$ Multi-Task Learning of\n  Gaussian Graphical Models", "comments": "Submitted on October 21, 2015 to the Journal of Machine Learning\n  Research", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present $\\ell_{1,p}$ multi-task structure learning for\nGaussian graphical models. We analyze the sufficient number of samples for the\ncorrect recovery of the support union and edge signs. We also analyze the\nnecessary number of samples for any conceivable method by providing\ninformation-theoretic lower bounds. We compare the statistical efficiency of\nmulti-task learning versus that of single-task learning. For experiments, we\nuse a block coordinate descent method that is provably convergent and generates\na sequence of positive definite solutions. We provide experimental validation\non synthetic data as well as on two publicly available real-world data sets,\nincluding functional magnetic resonance imaging and gene expression data.\n", "versions": [{"version": "v1", "created": "Wed, 18 Jul 2012 02:53:02 GMT"}, {"version": "v2", "created": "Sat, 24 Oct 2015 08:11:13 GMT"}], "update_date": "2015-10-27", "authors_parsed": [["Honorio", "Jean", ""], ["Jaakkola", "Tommi", ""], ["Samaras", "Dimitris", ""]]}, {"id": "1207.4404", "submitter": "Yoshua Bengio", "authors": "Yoshua Bengio, Gr\\'egoire Mesnil, Yann Dauphin and Salah Rifai", "title": "Better Mixing via Deep Representations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It has previously been hypothesized, and supported with some experimental\nevidence, that deeper representations, when well trained, tend to do a better\njob at disentangling the underlying factors of variation. We study the\nfollowing related conjecture: better representations, in the sense of better\ndisentangling, can be exploited to produce faster-mixing Markov chains.\nConsequently, mixing would be more efficient at higher levels of\nrepresentation. To better understand why and how this is happening, we propose\na secondary conjecture: the higher-level samples fill more uniformly the space\nthey occupy and the high-density manifolds tend to unfold when represented at\nhigher levels. The paper discusses these hypotheses and tests them\nexperimentally through visualization and measurements of mixing and\ninterpolating between samples.\n", "versions": [{"version": "v1", "created": "Wed, 18 Jul 2012 16:07:36 GMT"}], "update_date": "2012-07-19", "authors_parsed": [["Bengio", "Yoshua", ""], ["Mesnil", "Gr\u00e9goire", ""], ["Dauphin", "Yann", ""], ["Rifai", "Salah", ""]]}, {"id": "1207.4421", "submitter": "Alekh Agarwal", "authors": "Alekh Agarwal, Sahand Negahban, Martin J. Wainwright", "title": "Stochastic optimization and sparse statistical recovery: An optimal\n  algorithm for high dimensions", "comments": "2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop and analyze stochastic optimization algorithms for problems in\nwhich the expected loss is strongly convex, and the optimum is (approximately)\nsparse. Previous approaches are able to exploit only one of these two\nstructures, yielding an $\\order(\\pdim/T)$ convergence rate for strongly convex\nobjectives in $\\pdim$ dimensions, and an $\\order(\\sqrt{(\\spindex \\log\n\\pdim)/T})$ convergence rate when the optimum is $\\spindex$-sparse. Our\nalgorithm is based on successively solving a series of $\\ell_1$-regularized\noptimization problems using Nesterov's dual averaging algorithm. We establish\nthat the error of our solution after $T$ iterations is at most\n$\\order((\\spindex \\log\\pdim)/T)$, with natural extensions to approximate\nsparsity. Our results apply to locally Lipschitz losses including the logistic,\nexponential, hinge and least-squares losses. By recourse to statistical minimax\nresults, we show that our convergence rates are optimal up to multiplicative\nconstant factors. The effectiveness of our approach is also confirmed in\nnumerical simulations, in which we compare to several baselines on a\nleast-squares regression problem.\n", "versions": [{"version": "v1", "created": "Wed, 18 Jul 2012 17:40:11 GMT"}], "update_date": "2012-07-19", "authors_parsed": [["Agarwal", "Alekh", ""], ["Negahban", "Sahand", ""], ["Wainwright", "Martin J.", ""]]}, {"id": "1207.4463", "submitter": "Jingwei  Liu", "authors": "Jingwei Liu", "title": "Protein Function Prediction Based on Kernel Logistic Regression with\n  2-order Graphic Neighbor Information", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.QM cs.LG q-bio.MN", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To enhance the accuracy of protein-protein interaction function prediction, a\n2-order graphic neighbor information feature extraction method based on\nundirected simple graph is proposed in this paper, which extends the 1-order\ngraphic neighbor featureextraction method. And the chi-square test statistical\nmethod is also involved in feature combination. To demonstrate the\neffectiveness of our 2-order graphic neighbor feature, four logistic regression\nmodels (logistic regression (abbrev. LR), diffusion kernel logistic regression\n(abbrev. DKLR), polynomial kernel logistic regression (abbrev. PKLR), and\nradial basis function (RBF) based kernel logistic regression (abbrev. RBF KLR))\nare investigated on the two feature sets. The experimental results of protein\nfunction prediction of Yeast Proteome Database (YPD) using the the\nprotein-protein interaction data of Munich Information Center for Protein\nSequences (MIPS) show that 2-order graphic neighbor information of proteins can\nsignificantly improve the average overall percentage of protein function\nprediction especially with RBF KLR. And, with a new 5-top chi-square feature\ncombination method, RBF KLR can achieve 99.05% average overall percentage on\n2-order neighbor feature combination set.\n", "versions": [{"version": "v1", "created": "Wed, 18 Jul 2012 19:45:28 GMT"}], "update_date": "2012-07-19", "authors_parsed": [["Liu", "Jingwei", ""]]}, {"id": "1207.4597", "submitter": "Victorin Martin", "authors": "Victorin Martin, Jean-Marc Lasgouttes and Cyril Furtlehner", "title": "Local stability of Belief Propagation algorithm with multiple fixed\n  points", "comments": "arXiv admin note: substantial text overlap with arXiv:1101.4170", "journal-ref": null, "doi": "10.3233/978-1-61499-096-3-180", "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A number of problems in statistical physics and computer science can be\nexpressed as the computation of marginal probabilities over a Markov random\nfield. Belief propagation, an iterative message-passing algorithm, computes\nexactly such marginals when the underlying graph is a tree. But it has gained\nits popularity as an efficient way to approximate them in the more general\ncase, even if it can exhibits multiple fixed points and is not guaranteed to\nconverge. In this paper, we express a new sufficient condition for local\nstability of a belief propagation fixed point in terms of the graph structure\nand the beliefs values at the fixed point. This gives credence to the usual\nunderstanding that Belief Propagation performs better on sparse graphs.\n", "versions": [{"version": "v1", "created": "Thu, 19 Jul 2012 09:49:54 GMT"}], "update_date": "2012-10-23", "authors_parsed": [["Martin", "Victorin", ""], ["Lasgouttes", "Jean-Marc", ""], ["Furtlehner", "Cyril", ""]]}, {"id": "1207.4676", "submitter": "Amir Globerson", "authors": "John Langford and Joelle Pineau (Editors)", "title": "Proceedings of the 29th International Conference on Machine Learning\n  (ICML-12)", "comments": "Proceedings of the 29th International Conference on Machine Learning\n  (ICML-12). Editors: John Langford and Joelle Pineau. Publisher: Omnipress,\n  2012", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This is an index to the papers that appear in the Proceedings of the 29th\nInternational Conference on Machine Learning (ICML-12). The conference was held\nin Edinburgh, Scotland, June 27th - July 3rd, 2012.\n", "versions": [{"version": "v1", "created": "Thu, 19 Jul 2012 14:08:22 GMT"}, {"version": "v2", "created": "Sun, 16 Sep 2012 11:24:54 GMT"}], "update_date": "2012-09-18", "authors_parsed": [["Langford", "John", "", "Editors"], ["Pineau", "Joelle", "", "Editors"]]}, {"id": "1207.4747", "submitter": "Simon Lacoste-Julien", "authors": "Simon Lacoste-Julien, Martin Jaggi, Mark Schmidt, Patrick Pletscher", "title": "Block-Coordinate Frank-Wolfe Optimization for Structural SVMs", "comments": "Appears in Proceedings of the 30th International Conference on\n  Machine Learning (ICML 2013). 9 pages main text + 22 pages appendix. Changes\n  from v3 to v4: 1) Re-organized appendix; improved & clarified duality gap\n  proofs; re-drew all plots; 2) Changed convention for Cf definition; 3) Added\n  weighted averaging experiments + convergence results; 4) Clarified main text\n  and relationship with appendix", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a randomized block-coordinate variant of the classic Frank-Wolfe\nalgorithm for convex optimization with block-separable constraints. Despite its\nlower iteration cost, we show that it achieves a similar convergence rate in\nduality gap as the full Frank-Wolfe algorithm. We also show that, when applied\nto the dual structural support vector machine (SVM) objective, this yields an\nonline algorithm that has the same low iteration complexity as primal\nstochastic subgradient methods. However, unlike stochastic subgradient methods,\nthe block-coordinate Frank-Wolfe algorithm allows us to compute the optimal\nstep-size and yields a computable duality gap guarantee. Our experiments\nindicate that this simple algorithm outperforms competing structural SVM\nsolvers.\n", "versions": [{"version": "v1", "created": "Thu, 19 Jul 2012 18:02:41 GMT"}, {"version": "v2", "created": "Mon, 29 Oct 2012 18:03:32 GMT"}, {"version": "v3", "created": "Tue, 30 Oct 2012 19:25:10 GMT"}, {"version": "v4", "created": "Mon, 14 Jan 2013 13:26:51 GMT"}], "update_date": "2013-01-15", "authors_parsed": [["Lacoste-Julien", "Simon", ""], ["Jaggi", "Martin", ""], ["Schmidt", "Mark", ""], ["Pletscher", "Patrick", ""]]}, {"id": "1207.4748", "submitter": "Brian Eriksson", "authors": "Brian Eriksson", "title": "Hierarchical Clustering using Randomly Selected Similarities", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.IT cs.LG math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The problem of hierarchical clustering items from pairwise similarities is\nfound across various scientific disciplines, from biology to networking. Often,\napplications of clustering techniques are limited by the cost of obtaining\nsimilarities between pairs of items. While prior work has been developed to\nreconstruct clustering using a significantly reduced set of pairwise\nsimilarities via adaptive measurements, these techniques are only applicable\nwhen choice of similarities are available to the user. In this paper, we\nexamine reconstructing hierarchical clustering under similarity observations\nat-random. We derive precise bounds which show that a significant fraction of\nthe hierarchical clustering can be recovered using fewer than all the pairwise\nsimilarities. We find that the correct hierarchical clustering down to a\nconstant fraction of the total number of items (i.e., clusters sized O(N)) can\nbe found using only O(N log N) randomly selected pairwise similarities in\nexpectation.\n", "versions": [{"version": "v1", "created": "Thu, 19 Jul 2012 18:06:37 GMT"}], "update_date": "2012-07-20", "authors_parsed": [["Eriksson", "Brian", ""]]}, {"id": "1207.4814", "submitter": "Hung Bui", "authors": "Hung Hai Bui and Tuyen N. Huynh and Sebastian Riedel", "title": "Automorphism Groups of Graphical Models and Lifted Variational Inference", "comments": "Extended version of the paper to appear in Statistical Relational AI\n  (StaRAI-12) workshop at UAI '12", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG math.CO stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Using the theory of group action, we first introduce the concept of the\nautomorphism group of an exponential family or a graphical model, thus\nformalizing the general notion of symmetry of a probabilistic model. This\nautomorphism group provides a precise mathematical framework for lifted\ninference in the general exponential family. Its group action partitions the\nset of random variables and feature functions into equivalent classes (called\norbits) having identical marginals and expectations. Then the inference problem\nis effectively reduced to that of computing marginals or expectations for each\nclass, thus avoiding the need to deal with each individual variable or feature.\nWe demonstrate the usefulness of this general framework in lifting two classes\nof variational approximation for MAP inference: local LP relaxation and local\nLP relaxation with cycle constraints; the latter yields the first lifted\ninference that operate on a bound tighter than local constraints. Initial\nexperimental results demonstrate that lifted MAP inference with cycle\nconstraints achieved the state of the art performance, obtaining much better\nobjective function values than local approximation while remaining relatively\nefficient.\n", "versions": [{"version": "v1", "created": "Thu, 19 Jul 2012 21:30:42 GMT"}], "update_date": "2012-07-24", "authors_parsed": [["Bui", "Hung Hai", ""], ["Huynh", "Tuyen N.", ""], ["Riedel", "Sebastian", ""]]}, {"id": "1207.4931", "submitter": "Gyanendra Nath  Tripathi", "authors": "G. N. Tripathi and V. Rihani", "title": "Motion Planning Of an Autonomous Mobile Robot Using Artificial Neural\n  Network", "comments": "7 pages, 4 figures, 1 table, 1 graph chart, ITCA-2012, Chennai, India", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AI cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The paper presents the electronic design and motion planning of a robot based\non decision making regarding its straight motion and precise turn using\nArtificial Neural Network (ANN). The ANN helps in learning of robot so that it\nperforms motion autonomously. The weights calculated are implemented in\nmicrocontroller. The performance has been tested to be excellent.\n", "versions": [{"version": "v1", "created": "Fri, 20 Jul 2012 12:15:12 GMT"}], "update_date": "2012-07-23", "authors_parsed": [["Tripathi", "G. N.", ""], ["Rihani", "V.", ""]]}, {"id": "1207.4992", "submitter": "Pavel Bazovkin", "authors": "Tatjana Lange, Karl Mosler and Pavlo Mozharovskyi", "title": "Fast nonparametric classification based on data depth", "comments": null, "journal-ref": "Statistical Papers 55 (2014), 49-69", "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  A new procedure, called DDa-procedure, is developed to solve the problem of\nclassifying d-dimensional objects into q >= 2 classes. The procedure is\ncompletely nonparametric; it uses q-dimensional depth plots and a very\nefficient algorithm for discrimination analysis in the depth space [0,1]^q.\nSpecifically, the depth is the zonoid depth, and the algorithm is the\nalpha-procedure. In case of more than two classes several binary\nclassifications are performed and a majority rule is applied. Special\ntreatments are discussed for 'outsiders', that is, data having zero depth\nvector. The DDa-classifier is applied to simulated as well as real data, and\nthe results are compared with those of similar procedures that have been\nrecently proposed. In most cases the new procedure has comparable error rates,\nbut is much faster than other classification approaches, including the SVM.\n", "versions": [{"version": "v1", "created": "Fri, 20 Jul 2012 16:28:57 GMT"}, {"version": "v2", "created": "Tue, 18 Dec 2012 00:10:49 GMT"}], "update_date": "2017-12-18", "authors_parsed": [["Lange", "Tatjana", ""], ["Mosler", "Karl", ""], ["Mozharovskyi", "Pavlo", ""]]}, {"id": "1207.5091", "submitter": "Anvesh Komuravelli", "authors": "Anvesh Komuravelli, Corina S. Pasareanu and Edmund M. Clarke", "title": "Learning Probabilistic Systems from Tree Samples", "comments": "14 pages, conference paper with full proofs", "journal-ref": "LICS, pp. 441-450, IEEE, 2012", "doi": "10.1109/LICS.2012.54", "report-no": null, "categories": "cs.LO cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of learning a non-deterministic probabilistic system\nconsistent with a given finite set of positive and negative tree samples.\nConsistency is defined with respect to strong simulation conformance. We\npropose learning algorithms that use traditional and a new \"stochastic\"\nstate-space partitioning, the latter resulting in the minimum number of states.\nWe then use them to solve the problem of \"active learning\", that uses a\nknowledgeable teacher to generate samples as counterexamples to simulation\nequivalence queries. We show that the problem is undecidable in general, but\nthat it becomes decidable under a suitable condition on the teacher which comes\nnaturally from the way samples are generated from failed simulation checks. The\nlatter problem is shown to be undecidable if we impose an additional condition\non the learner to always conjecture a \"minimum state\" hypothesis. We therefore\npropose a semi-algorithm using stochastic partitions. Finally, we apply the\nproposed (semi-) algorithms to infer intermediate assumptions in an automated\nassume-guarantee verification framework for probabilistic systems.\n", "versions": [{"version": "v1", "created": "Sat, 21 Jul 2012 02:34:25 GMT"}], "update_date": "2012-07-24", "authors_parsed": [["Komuravelli", "Anvesh", ""], ["Pasareanu", "Corina S.", ""], ["Clarke", "Edmund M.", ""]]}, {"id": "1207.5136", "submitter": "Jonas Peters", "authors": "Jonas Peters, Dominik Janzing and Bernhard Sch\\\"olkopf", "title": "Causal Inference on Time Series using Structural Equation Models", "comments": null, "journal-ref": "Advances in Neural Information Processing Systems 26, 154-162,\n  2014", "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Causal inference uses observations to infer the causal structure of the data\ngenerating system. We study a class of functional models that we call Time\nSeries Models with Independent Noise (TiMINo). These models require independent\nresidual time series, whereas traditional methods like Granger causality\nexploit the variance of residuals. There are two main contributions: (1)\nTheoretical: By restricting the model class (e.g. to additive noise) we can\nprovide a more general identifiability result than existing ones. This result\nincorporates lagged and instantaneous effects that can be nonlinear and do not\nneed to be faithful, and non-instantaneous feedbacks between the time series.\n(2) Practical: If there are no feedback loops between time series, we propose\nan algorithm based on non-linear independence tests of time series. When the\ndata are causally insufficient, or the data generating process does not satisfy\nthe model assumptions, this algorithm may still give partial results, but\nmostly avoids incorrect answers. An extension to (non-instantaneous) feedbacks\nis possible, but not discussed. It outperforms existing methods on artificial\nand real data. Code can be provided upon request.\n", "versions": [{"version": "v1", "created": "Sat, 21 Jul 2012 13:31:56 GMT"}], "update_date": "2016-08-18", "authors_parsed": [["Peters", "Jonas", ""], ["Janzing", "Dominik", ""], ["Sch\u00f6lkopf", "Bernhard", ""]]}, {"id": "1207.5208", "submitter": "Francis Maes", "authors": "Francis Maes and Damien Ernst and Louis Wehenkel", "title": "Meta-Learning of Exploration/Exploitation Strategies: The Multi-Armed\n  Bandit Case", "comments": "16 pages, Springer Selection of papers of ICAART'12", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The exploration/exploitation (E/E) dilemma arises naturally in many subfields\nof Science. Multi-armed bandit problems formalize this dilemma in its canonical\nform. Most current research in this field focuses on generic solutions that can\nbe applied to a wide range of problems. However, in practice, it is often the\ncase that a form of prior information is available about the specific class of\ntarget problems. Prior knowledge is rarely used in current solutions due to the\nlack of a systematic approach to incorporate it into the E/E strategy.\n  To address a specific class of E/E problems, we propose to proceed in three\nsteps: (i) model prior knowledge in the form of a probability distribution over\nthe target class of E/E problems; (ii) choose a large hypothesis space of\ncandidate E/E strategies; and (iii), solve an optimization problem to find a\ncandidate E/E strategy of maximal average performance over a sample of problems\ndrawn from the prior distribution.\n  We illustrate this meta-learning approach with two different hypothesis\nspaces: one where E/E strategies are numerically parameterized and another\nwhere E/E strategies are represented as small symbolic formulas. We propose\nappropriate optimization algorithms for both cases. Our experiments, with\ntwo-armed Bernoulli bandit problems and various playing budgets, show that the\nmeta-learnt E/E strategies outperform generic strategies of the literature\n(UCB1, UCB1-Tuned, UCB-v, KL-UCB and epsilon greedy); they also evaluate the\nrobustness of the learnt E/E strategies, by tests carried out on arms whose\nrewards follow a truncated Gaussian distribution.\n", "versions": [{"version": "v1", "created": "Sun, 22 Jul 2012 09:34:49 GMT"}], "update_date": "2012-07-24", "authors_parsed": [["Maes", "Francis", ""], ["Ernst", "Damien", ""], ["Wehenkel", "Louis", ""]]}, {"id": "1207.5259", "submitter": "Sebastien Bubeck", "authors": "Sebastien Bubeck and Damien Ernst and Aurelien Garivier", "title": "Optimal discovery with probabilistic expert advice: finite time analysis\n  and macroscopic optimality", "comments": "arXiv admin note: substantial text overlap with arXiv:1110.5447", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider an original problem that arises from the issue of security\nanalysis of a power system and that we name optimal discovery with\nprobabilistic expert advice. We address it with an algorithm based on the\noptimistic paradigm and on the Good-Turing missing mass estimator. We prove two\ndifferent regret bounds on the performance of this algorithm under weak\nassumptions on the probabilistic experts. Under more restrictive hypotheses, we\nalso prove a macroscopic optimality result, comparing the algorithm both with\nan oracle strategy and with uniform sampling. Finally, we provide numerical\nexperiments illustrating these theoretical findings.\n", "versions": [{"version": "v1", "created": "Sun, 22 Jul 2012 21:01:09 GMT"}, {"version": "v2", "created": "Wed, 7 Nov 2012 21:45:35 GMT"}, {"version": "v3", "created": "Fri, 29 Mar 2013 21:47:06 GMT"}], "update_date": "2013-04-02", "authors_parsed": [["Bubeck", "Sebastien", ""], ["Ernst", "Damien", ""], ["Garivier", "Aurelien", ""]]}, {"id": "1207.5342", "submitter": "Hanwen Cao", "authors": "Hanwen Cao and J\\\"urgen Peissig", "title": "A Robust Signal Classification Scheme for Cognitive Radio", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.LG cs.NI math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a robust signal classification scheme for achieving\ncomprehensive spectrum sensing of multiple coexisting wireless systems. It is\nbuilt upon a group of feature-based signal detection algorithms enhanced by the\nproposed dimension cancelation (DIC) method for mitigating the noise\nuncertainty problem. The classification scheme is implemented on our testbed\nconsisting real-world wireless devices. The simulation and experimental\nperformances agree with each other well and shows the effectiveness and\nrobustness of the proposed scheme.\n", "versions": [{"version": "v1", "created": "Mon, 23 Jul 2012 10:22:56 GMT"}], "update_date": "2016-11-26", "authors_parsed": [["Cao", "Hanwen", ""], ["Peissig", "J\u00fcrgen", ""]]}, {"id": "1207.5437", "submitter": "Yiming Ying", "authors": "Qiong Cao, Zheng-Chu Guo and Yiming Ying", "title": "Generalization Bounds for Metric and Similarity Learning", "comments": "20 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, metric learning and similarity learning have attracted a large\namount of interest. Many models and optimisation algorithms have been proposed.\nHowever, there is relatively little work on the generalization analysis of such\nmethods. In this paper, we derive novel generalization bounds of metric and\nsimilarity learning. In particular, we first show that the generalization\nanalysis reduces to the estimation of the Rademacher average over\n\"sums-of-i.i.d.\" sample-blocks related to the specific matrix norm. Then, we\nderive generalization bounds for metric/similarity learning with different\nmatrix-norm regularisers by estimating their specific Rademacher complexities.\nOur analysis indicates that sparse metric/similarity learning with $L^1$-norm\nregularisation could lead to significantly better bounds than those with\nFrobenius-norm regularisation. Our novel generalization analysis develops and\nrefines the techniques of U-statistics and Rademacher complexity analysis.\n", "versions": [{"version": "v1", "created": "Mon, 23 Jul 2012 16:20:05 GMT"}, {"version": "v2", "created": "Sun, 17 Mar 2013 10:56:35 GMT"}], "update_date": "2013-03-19", "authors_parsed": [["Cao", "Qiong", ""], ["Guo", "Zheng-Chu", ""], ["Ying", "Yiming", ""]]}, {"id": "1207.5536", "submitter": "David Tolpin", "authors": "David Tolpin and Solomon Eyal Shimony", "title": "MCTS Based on Simple Regret", "comments": "AAAI 2012, 7 pages. arXiv admin note: text overlap with\n  arXiv:1108.3711", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  UCT, a state-of-the art algorithm for Monte Carlo tree search (MCTS) in games\nand Markov decision processes, is based on UCB, a sampling policy for the\nMulti-armed Bandit problem (MAB) that minimizes the cumulative regret. However,\nsearch differs from MAB in that in MCTS it is usually only the final \"arm pull\"\n(the actual move selection) that collects a reward, rather than all \"arm\npulls\". Therefore, it makes more sense to minimize the simple regret, as\nopposed to the cumulative regret. We begin by introducing policies for\nmulti-armed bandits with lower finite-time and asymptotic simple regret than\nUCB, using it to develop a two-stage scheme (SR+CR) for MCTS which outperforms\nUCT empirically.\n  Optimizing the sampling process is itself a metareasoning problem, a solution\nof which can use value of information (VOI) techniques. Although the theory of\nVOI for search exists, applying it to MCTS is non-trivial, as typical myopic\nassumptions fail. Lacking a complete working VOI theory for MCTS, we\nnevertheless propose a sampling scheme that is \"aware\" of VOI, achieving an\nalgorithm that in empirical evaluation outperforms both UCT and the other\nproposed algorithms.\n", "versions": [{"version": "v1", "created": "Mon, 23 Jul 2012 21:13:40 GMT"}], "update_date": "2012-07-25", "authors_parsed": [["Tolpin", "David", ""], ["Shimony", "Solomon Eyal", ""]]}, {"id": "1207.5554", "submitter": "Mahdi Milani Fard", "authors": "Mahdi Milani Fard, Yuri Grinberg, Amir-massoud Farahmand, Joelle\n  Pineau, Doina Precup", "title": "Bellman Error Based Feature Generation using Random Projections on\n  Sparse Spaces", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We address the problem of automatic generation of features for value function\napproximation. Bellman Error Basis Functions (BEBFs) have been shown to improve\nthe error of policy evaluation with function approximation, with a convergence\nrate similar to that of value iteration. We propose a simple, fast and robust\nalgorithm based on random projections to generate BEBFs for sparse feature\nspaces. We provide a finite sample analysis of the proposed method, and prove\nthat projections logarithmic in the dimension of the original space are enough\nto guarantee contraction in the error. Empirical results demonstrate the\nstrength of this method.\n", "versions": [{"version": "v1", "created": "Mon, 23 Jul 2012 22:39:51 GMT"}, {"version": "v2", "created": "Tue, 11 Sep 2012 22:53:01 GMT"}, {"version": "v3", "created": "Fri, 21 Sep 2012 22:51:40 GMT"}], "update_date": "2012-09-25", "authors_parsed": [["Fard", "Mahdi Milani", ""], ["Grinberg", "Yuri", ""], ["Farahmand", "Amir-massoud", ""], ["Pineau", "Joelle", ""], ["Precup", "Doina", ""]]}, {"id": "1207.5589", "submitter": "David Tolpin", "authors": "David Tolpin and Solomon Eyal Shimony", "title": "VOI-aware MCTS", "comments": "2 pages, ECAI 2012", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  UCT, a state-of-the art algorithm for Monte Carlo tree search (MCTS) in games\nand Markov decision processes, is based on UCB1, a sampling policy for the\nMulti-armed Bandit problem (MAB) that minimizes the cumulative regret. However,\nsearch differs from MAB in that in MCTS it is usually only the final \"arm pull\"\n(the actual move selection) that collects a reward, rather than all \"arm\npulls\". In this paper, an MCTS sampling policy based on Value of Information\n(VOI) estimates of rollouts is suggested. Empirical evaluation of the policy\nand comparison to UCB1 and UCT is performed on random MAB instances as well as\non Computer Go.\n", "versions": [{"version": "v1", "created": "Tue, 24 Jul 2012 04:55:02 GMT"}], "update_date": "2012-07-25", "authors_parsed": [["Tolpin", "David", ""], ["Shimony", "Solomon Eyal", ""]]}, {"id": "1207.5774", "submitter": "Lou Marvin Caraig", "authors": "Lou Marvin Caraig", "title": "A New Training Algorithm for Kanerva's Sparse Distributed Memory", "comments": "5 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Sparse Distributed Memory proposed by Pentii Kanerva (SDM in short) was\nthought to be a model of human long term memory. The architecture of the SDM\npermits to store binary patterns and to retrieve them using partially matching\npatterns. However Kanerva's model is especially efficient only in handling\nrandom data. The purpose of this article is to introduce a new approach of\ntraining Kanerva's SDM that can handle efficiently non-random data, and to\nprovide it the capability to recognize inverted patterns. This approach uses a\nsignal model which is different from the one proposed for different purposes by\nHely, Willshaw and Hayes in [4]. This article additionally suggests a different\nway of creating hard locations in the memory despite the Kanerva's static\nmodel.\n", "versions": [{"version": "v1", "created": "Sun, 22 Jul 2012 16:30:07 GMT"}, {"version": "v2", "created": "Thu, 26 Jul 2012 05:04:18 GMT"}, {"version": "v3", "created": "Fri, 27 Jul 2012 08:24:51 GMT"}], "update_date": "2012-07-30", "authors_parsed": [["Caraig", "Lou Marvin", ""]]}, {"id": "1207.6076", "submitter": "Dino Sejdinovic", "authors": "Dino Sejdinovic, Bharath Sriperumbudur, Arthur Gretton, Kenji Fukumizu", "title": "Equivalence of distance-based and RKHS-based statistics in hypothesis\n  testing", "comments": "Published in at http://dx.doi.org/10.1214/13-AOS1140 the Annals of\n  Statistics (http://www.imstat.org/aos/) by the Institute of Mathematical\n  Statistics (http://www.imstat.org)", "journal-ref": "Annals of Statistics 2013, Vol. 41, No. 5, 2263-2291", "doi": "10.1214/13-AOS1140", "report-no": "IMS-AOS-AOS1140", "categories": "stat.ME cs.LG math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We provide a unifying framework linking two classes of statistics used in\ntwo-sample and independence testing: on the one hand, the energy distances and\ndistance covariances from the statistics literature; on the other, maximum mean\ndiscrepancies (MMD), that is, distances between embeddings of distributions to\nreproducing kernel Hilbert spaces (RKHS), as established in machine learning.\nIn the case where the energy distance is computed with a semimetric of negative\ntype, a positive definite kernel, termed distance kernel, may be defined such\nthat the MMD corresponds exactly to the energy distance. Conversely, for any\npositive definite kernel, we can interpret the MMD as energy distance with\nrespect to some negative-type semimetric. This equivalence readily extends to\ndistance covariance using kernels on the product space. We determine the class\nof probability distributions for which the test statistics are consistent\nagainst all alternatives. Finally, we investigate the performance of the family\nof distance kernels in two-sample and independence tests: we show in particular\nthat the energy distance most commonly employed in statistics is just one\nmember of a parametric family of kernels, and that other choices from this\nfamily can yield more powerful tests.\n", "versions": [{"version": "v1", "created": "Wed, 25 Jul 2012 18:17:20 GMT"}, {"version": "v2", "created": "Mon, 11 Mar 2013 11:29:37 GMT"}, {"version": "v3", "created": "Tue, 12 Nov 2013 12:22:53 GMT"}], "update_date": "2013-11-13", "authors_parsed": [["Sejdinovic", "Dino", ""], ["Sriperumbudur", "Bharath", ""], ["Gretton", "Arthur", ""], ["Fukumizu", "Kenji", ""]]}, {"id": "1207.6083", "submitter": "Alex Kulesza", "authors": "Alex Kulesza, Ben Taskar", "title": "Determinantal point processes for machine learning", "comments": "120 pages", "journal-ref": "Foundations and Trends in Machine Learning: Vol. 5: No 2-3, pp\n  123-286", "doi": "10.1561/2200000044", "report-no": null, "categories": "stat.ML cs.IR cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  Determinantal point processes (DPPs) are elegant probabilistic models of\nrepulsion that arise in quantum physics and random matrix theory. In contrast\nto traditional structured models like Markov random fields, which become\nintractable and hard to approximate in the presence of negative correlations,\nDPPs offer efficient and exact algorithms for sampling, marginalization,\nconditioning, and other inference tasks. We provide a gentle introduction to\nDPPs, focusing on the intuitions, algorithms, and extensions that are most\nrelevant to the machine learning community, and show how DPPs can be applied to\nreal-world applications like finding diverse sets of high-quality search\nresults, building informative summaries by selecting diverse sentences from\ndocuments, modeling non-overlapping human poses in images or video, and\nautomatically building timelines of important news stories.\n", "versions": [{"version": "v1", "created": "Wed, 25 Jul 2012 18:45:43 GMT"}, {"version": "v2", "created": "Tue, 13 Nov 2012 16:10:06 GMT"}, {"version": "v3", "created": "Tue, 11 Dec 2012 14:05:25 GMT"}, {"version": "v4", "created": "Thu, 10 Jan 2013 20:43:53 GMT"}], "update_date": "2013-01-11", "authors_parsed": [["Kulesza", "Alex", ""], ["Taskar", "Ben", ""]]}, {"id": "1207.6231", "submitter": "Mario Frank", "authors": "Mario Frank, Ralf Biedert, Eugene Ma, Ivan Martinovic, Dawn Song", "title": "Touchalytics: On the Applicability of Touchscreen Input as a Behavioral\n  Biometric for Continuous Authentication", "comments": "to appear at IEEE Transactions on Information Forensics & Security;\n  Download data from http://www.mariofrank.net/touchalytics/", "journal-ref": "IEEE Transactions on Information Forensics and Security (Vol. 8,\n  No. 1), pages 136-148, 2013", "doi": "10.1109/TIFS.2012.2225048", "report-no": null, "categories": "cs.CR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate whether a classifier can continuously authenticate users based\non the way they interact with the touchscreen of a smart phone. We propose a\nset of 30 behavioral touch features that can be extracted from raw touchscreen\nlogs and demonstrate that different users populate distinct subspaces of this\nfeature space. In a systematic experiment designed to test how this behavioral\npattern exhibits consistency over time, we collected touch data from users\ninteracting with a smart phone using basic navigation maneuvers, i.e., up-down\nand left-right scrolling. We propose a classification framework that learns the\ntouch behavior of a user during an enrollment phase and is able to accept or\nreject the current user by monitoring interaction with the touch screen. The\nclassifier achieves a median equal error rate of 0% for intra-session\nauthentication, 2%-3% for inter-session authentication and below 4% when the\nauthentication test was carried out one week after the enrollment phase. While\nour experimental findings disqualify this method as a standalone authentication\nmechanism for long-term authentication, it could be implemented as a means to\nextend screen-lock time or as a part of a multi-modal biometric authentication\nsystem.\n", "versions": [{"version": "v1", "created": "Thu, 26 Jul 2012 10:34:19 GMT"}, {"version": "v2", "created": "Mon, 8 Oct 2012 21:32:42 GMT"}], "update_date": "2013-01-08", "authors_parsed": [["Frank", "Mario", ""], ["Biedert", "Ralf", ""], ["Ma", "Eugene", ""], ["Martinovic", "Ivan", ""], ["Song", "Dawn", ""]]}, {"id": "1207.6253", "submitter": "Rui Henriques", "authors": "Rui Henriques and In\\^es Lynce and Vasco Manquinho", "title": "On When and How to use SAT to Mine Frequent Itemsets", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.DB cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A new stream of research was born in the last decade with the goal of mining\nitemsets of interest using Constraint Programming (CP). This has promoted a\nnatural way to combine complex constraints in a highly flexible manner.\nAlthough CP state-of-the-art solutions formulate the task using Boolean\nvariables, the few attempts to adopt propositional Satisfiability (SAT)\nprovided an unsatisfactory performance. This work deepens the study on when and\nhow to use SAT for the frequent itemset mining (FIM) problem by defining\ndifferent encodings with multiple task-driven enumeration options and search\nstrategies. Although for the majority of the scenarios SAT-based solutions\nappear to be non-competitive with CP peers, results show a variety of\ninteresting cases where SAT encodings are the best option.\n", "versions": [{"version": "v1", "created": "Thu, 26 Jul 2012 12:33:46 GMT"}], "update_date": "2012-07-27", "authors_parsed": [["Henriques", "Rui", ""], ["Lynce", "In\u00eas", ""], ["Manquinho", "Vasco", ""]]}, {"id": "1207.6379", "submitter": "Jose Bento", "authors": "Jos\\'e Bento, Nadia Fawaz, Andrea Montanari, Stratis Ioannidis", "title": "Identifying Users From Their Rating Patterns", "comments": "Winner of the 2011 Challenge on Context-Aware Movie Recommendation\n  (RecSys 2011 - CAMRa2011)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper reports on our analysis of the 2011 CAMRa Challenge dataset (Track\n2) for context-aware movie recommendation systems. The train dataset comprises\n4,536,891 ratings provided by 171,670 users on 23,974$ movies, as well as the\nhousehold groupings of a subset of the users. The test dataset comprises 5,450\nratings for which the user label is missing, but the household label is\nprovided. The challenge required to identify the user labels for the ratings in\nthe test set. Our main finding is that temporal information (time labels of the\nratings) is significantly more useful for achieving this objective than the\nuser preferences (the actual ratings). Using a model that leverages on this\nfact, we are able to identify users within a known household with an accuracy\nof approximately 96% (i.e. misclassification rate around 4%).\n", "versions": [{"version": "v1", "created": "Thu, 26 Jul 2012 19:27:03 GMT"}], "update_date": "2012-07-27", "authors_parsed": [["Bento", "Jos\u00e9", ""], ["Fawaz", "Nadia", ""], ["Montanari", "Andrea", ""], ["Ioannidis", "Stratis", ""]]}, {"id": "1207.6430", "submitter": "Christoph Brune", "authors": "Braxton Osting and Christoph Brune and Stanley J. Osher", "title": "Optimal Data Collection For Informative Rankings Expose Well-Connected\n  Graphs", "comments": "31 pages, 10 figures, 3 tables", "journal-ref": null, "doi": null, "report-no": "UCLA CAM report 12-32", "categories": "stat.ML cs.LG stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given a graph where vertices represent alternatives and arcs represent\npairwise comparison data, the statistical ranking problem is to find a\npotential function, defined on the vertices, such that the gradient of the\npotential function agrees with the pairwise comparisons. Our goal in this paper\nis to develop a method for collecting data for which the least squares\nestimator for the ranking problem has maximal Fisher information. Our approach,\nbased on experimental design, is to view data collection as a bi-level\noptimization problem where the inner problem is the ranking problem and the\nouter problem is to identify data which maximizes the informativeness of the\nranking. Under certain assumptions, the data collection problem decouples,\nreducing to a problem of finding multigraphs with large algebraic connectivity.\nThis reduction of the data collection problem to graph-theoretic questions is\none of the primary contributions of this work. As an application, we study the\nYahoo! Movie user rating dataset and demonstrate that the addition of a small\nnumber of well-chosen pairwise comparisons can significantly increase the\nFisher informativeness of the ranking. As another application, we study the\n2011-12 NCAA football schedule and propose schedules with the same number of\ngames which are significantly more informative. Using spectral clustering\nmethods to identify highly-connected communities within the division, we argue\nthat the NCAA could improve its notoriously poor rankings by simply scheduling\nmore out-of-conference games.\n", "versions": [{"version": "v1", "created": "Thu, 26 Jul 2012 23:14:34 GMT"}, {"version": "v2", "created": "Wed, 4 Jun 2014 08:31:57 GMT"}], "update_date": "2014-06-05", "authors_parsed": [["Osting", "Braxton", ""], ["Brune", "Christoph", ""], ["Osher", "Stanley J.", ""]]}, {"id": "1207.6910", "submitter": "Jakub Tomczak M.Sc.", "authors": "Jakub M. Tomczak, Jerzy Swiatek, Krzysztof Latawiec", "title": "Gaussian process regression as a predictive model for Quality-of-Service\n  in Web service systems", "comments": "9 pages, 4 figures, technical report", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present the Gaussian process regression as the predictive\nmodel for Quality-of-Service (QoS) attributes in Web service systems. The goal\nis to predict performance of the execution system expressed as QoS attributes\ngiven existing execution system, service repository, and inputs, e.g., streams\nof requests. In order to evaluate the performance of Gaussian process\nregression the simulation environment was developed. Two quality indexes were\nused, namely, Mean Absolute Error and Mean Squared Error. The results obtained\nwithin the experiment show that the Gaussian process performed the best with\nlinear kernel and statistically significantly better comparing to\nClassification and Regression Trees (CART) method.\n", "versions": [{"version": "v1", "created": "Mon, 30 Jul 2012 12:20:20 GMT"}, {"version": "v2", "created": "Wed, 8 May 2013 10:32:56 GMT"}], "update_date": "2013-05-09", "authors_parsed": [["Tomczak", "Jakub M.", ""], ["Swiatek", "Jerzy", ""], ["Latawiec", "Krzysztof", ""]]}, {"id": "1207.7035", "submitter": "Thomas Perry", "authors": "Thomas Perry and Hongyuan Zha and Patricio Frias and Dadan Zeng and\n  Mark Braunstein", "title": "Supervised Laplacian Eigenmaps with Applications in Clinical Diagnostics\n  for Pediatric Cardiology", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Electronic health records contain rich textual data which possess critical\npredictive information for machine-learning based diagnostic aids. However many\ntraditional machine learning methods fail to simultaneously integrate both\nvector space data and text. We present a supervised method using Laplacian\neigenmaps to augment existing machine-learning methods with low-dimensional\nrepresentations of textual predictors which preserve the local similarities.\nThe proposed implementation performs alternating optimization using gradient\ndescent. For the evaluation we applied our method to over 2,000 patient records\nfrom a large single-center pediatric cardiology practice to predict if patients\nwere diagnosed with cardiac disease. Our method was compared with latent\nsemantic indexing, latent Dirichlet allocation, and local Fisher discriminant\nanalysis. The results were assessed using AUC, MCC, specificity, and\nsensitivity. Results indicate supervised Laplacian eigenmaps was the highest\nperforming method in our study, achieving 0.782 and 0.374 for AUC and MCC\nrespectively. SLE showed an increase in 8.16% in AUC and 20.6% in MCC over the\nbaseline which excluded textual data and a 2.69% and 5.35% increase in AUC and\nMCC respectively over unsupervised Laplacian eigenmaps. This method allows many\nexisting machine learning predictors to effectively and efficiently utilize the\npotential of textual predictors.\n", "versions": [{"version": "v1", "created": "Fri, 27 Jul 2012 08:47:50 GMT"}], "update_date": "2012-07-31", "authors_parsed": [["Perry", "Thomas", ""], ["Zha", "Hongyuan", ""], ["Frias", "Patricio", ""], ["Zeng", "Dadan", ""], ["Braunstein", "Mark", ""]]}, {"id": "1207.7167", "submitter": "Wonchan Lee", "authors": "Wonchan Lee (Seoul National University), Yungbum Jung (Seoul National\n  University), Bow-yaw Wang (Academia Sinica), Kwangkuen Yi (Seoul National\n  University)", "title": "Predicate Generation for Learning-Based Quantifier-Free Loop Invariant\n  Inference", "comments": null, "journal-ref": "Logical Methods in Computer Science, Volume 8, Issue 3 (September\n  29, 2012) lmcs:1035", "doi": "10.2168/LMCS-8(3:25)2012", "report-no": null, "categories": "cs.LO cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We address the predicate generation problem in the context of loop invariant\ninference. Motivated by the interpolation-based abstraction refinement\ntechnique, we apply the interpolation theorem to synthesize predicates\nimplicitly implied by program texts. Our technique is able to improve the\neffectiveness and efficiency of the learning-based loop invariant inference\nalgorithm in [14]. We report experiment results of examples from Linux,\nSPEC2000, and Tar utility.\n", "versions": [{"version": "v1", "created": "Tue, 31 Jul 2012 04:55:45 GMT"}, {"version": "v2", "created": "Fri, 28 Sep 2012 11:03:50 GMT"}], "update_date": "2015-07-01", "authors_parsed": [["Lee", "Wonchan", "", "Seoul National University"], ["Jung", "Yungbum", "", "Seoul National\n  University"], ["Wang", "Bow-yaw", "", "Academia Sinica"], ["Yi", "Kwangkuen", "", "Seoul National\n  University"]]}, {"id": "1207.7253", "submitter": "S\\'ebastien Gigu\\`ere", "authors": "S\\'ebastien Gigu\\`ere, Mario Marchand, Fran\\c{c}ois Laviolette,\n  Alexandre Drouin and Jacques Corbeil", "title": "Learning a peptide-protein binding affinity predictor with kernel ridge\n  regression", "comments": "22 pages, 4 figures, 5 tables", "journal-ref": "BMC Bioinformatics 2013, 14:82", "doi": "10.1186/1471-2105-14-82", "report-no": null, "categories": "q-bio.QM cs.LG q-bio.BM stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a specialized string kernel for small bio-molecules, peptides and\npseudo-sequences of binding interfaces. The kernel incorporates\nphysico-chemical properties of amino acids and elegantly generalize eight\nkernels, such as the Oligo, the Weighted Degree, the Blended Spectrum, and the\nRadial Basis Function. We provide a low complexity dynamic programming\nalgorithm for the exact computation of the kernel and a linear time algorithm\nfor it's approximation. Combined with kernel ridge regression and SupCK, a\nnovel binding pocket kernel, the proposed kernel yields biologically relevant\nand good prediction accuracy on the PepX database. For the first time, a\nmachine learning predictor is capable of accurately predicting the binding\naffinity of any peptide to any protein. The method was also applied to both\nsingle-target and pan-specific Major Histocompatibility Complex class II\nbenchmark datasets and three Quantitative Structure Affinity Model benchmark\ndatasets.\n  On all benchmarks, our method significantly (p-value < 0.057) outperforms the\ncurrent state-of-the-art methods at predicting peptide-protein binding\naffinities. The proposed approach is flexible and can be applied to predict any\nquantitative biological activity. The method should be of value to a large\nsegment of the research community with the potential to accelerate\npeptide-based drug and vaccine development.\n", "versions": [{"version": "v1", "created": "Tue, 31 Jul 2012 14:11:31 GMT"}], "update_date": "2014-01-29", "authors_parsed": [["Gigu\u00e8re", "S\u00e9bastien", ""], ["Marchand", "Mario", ""], ["Laviolette", "Fran\u00e7ois", ""], ["Drouin", "Alexandre", ""], ["Corbeil", "Jacques", ""]]}]