[{"id": "1501.00037", "submitter": "Yuanli Pei", "authors": "Yuanli Pei, Xiaoli Z. Fern, R\\'omer Rosales, Teresa Vania Tjahja", "title": "Discriminative Clustering with Relative Constraints", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of clustering with relative constraints, where each\nconstraint specifies relative similarities among instances. In particular, each\nconstraint $(x_i, x_j, x_k)$ is acquired by posing a query: is instance $x_i$\nmore similar to $x_j$ than to $x_k$? We consider the scenario where answers to\nsuch queries are based on an underlying (but unknown) class concept, which we\naim to discover via clustering. Different from most existing methods that only\nconsider constraints derived from yes and no answers, we also incorporate don't\nknow responses. We introduce a Discriminative Clustering method with Relative\nConstraints (DCRC) which assumes a natural probabilistic relationship between\ninstances, their underlying cluster memberships, and the observed constraints.\nThe objective is to maximize the model likelihood given the constraints, and in\nthe meantime enforce cluster separation and cluster balance by also making use\nof the unlabeled instances. We evaluated the proposed method using constraints\ngenerated from ground-truth class labels, and from (noisy) human judgments from\na user study. Experimental results demonstrate: 1) the usefulness of relative\nconstraints, in particular when don't know answers are considered; 2) the\nimproved performance of the proposed method over state-of-the-art methods that\nutilize either relative or pairwise constraints; and 3) the robustness of our\nmethod in the presence of noisy constraints, such as those provided by human\njudgement.\n", "versions": [{"version": "v1", "created": "Tue, 30 Dec 2014 22:34:24 GMT"}], "update_date": "2015-01-05", "authors_parsed": [["Pei", "Yuanli", ""], ["Fern", "Xiaoli Z.", ""], ["Rosales", "R\u00f3mer", ""], ["Tjahja", "Teresa Vania", ""]]}, {"id": "1501.00052", "submitter": "Jonathan Huggins", "authors": "Jonathan H. Huggins, Ardavan Saeedi, and Matthew J. Johnson", "title": "Detailed Derivations of Small-Variance Asymptotics for some Hierarchical\n  Bayesian Nonparametric Models", "comments": "7 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this note we provide detailed derivations of two versions of\nsmall-variance asymptotics for hierarchical Dirichlet process (HDP) mixture\nmodels and the HDP hidden Markov model (HDP-HMM, a.k.a. the infinite HMM). We\ninclude derivations for the probabilities of certain CRP and CRF partitions,\nwhich are of more general interest.\n", "versions": [{"version": "v1", "created": "Wed, 31 Dec 2014 00:23:35 GMT"}], "update_date": "2015-01-05", "authors_parsed": [["Huggins", "Jonathan H.", ""], ["Saeedi", "Ardavan", ""], ["Johnson", "Matthew J.", ""]]}, {"id": "1501.00102", "submitter": "Natalia Neverova", "authors": "Natalia Neverova and Christian Wolf and Graham W. Taylor and Florian\n  Nebout", "title": "ModDrop: adaptive multi-modal gesture recognition", "comments": "14 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.HC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a method for gesture detection and localisation based on\nmulti-scale and multi-modal deep learning. Each visual modality captures\nspatial information at a particular spatial scale (such as motion of the upper\nbody or a hand), and the whole system operates at three temporal scales. Key to\nour technique is a training strategy which exploits: i) careful initialization\nof individual modalities; and ii) gradual fusion involving random dropping of\nseparate channels (dubbed ModDrop) for learning cross-modality correlations\nwhile preserving uniqueness of each modality-specific representation. We\npresent experiments on the ChaLearn 2014 Looking at People Challenge gesture\nrecognition track, in which we placed first out of 17 teams. Fusing multiple\nmodalities at several spatial and temporal scales leads to a significant\nincrease in recognition rates, allowing the model to compensate for errors of\nthe individual classifiers as well as noise in the separate channels.\nFuthermore, the proposed ModDrop training technique ensures robustness of the\nclassifier to missing signals in one or several channels to produce meaningful\npredictions from any number of available modalities. In addition, we\ndemonstrate the applicability of the proposed fusion scheme to modalities of\narbitrary nature by experiments on the same dataset augmented with audio.\n", "versions": [{"version": "v1", "created": "Wed, 31 Dec 2014 09:55:43 GMT"}, {"version": "v2", "created": "Sat, 6 Jun 2015 14:46:33 GMT"}], "update_date": "2015-07-21", "authors_parsed": [["Neverova", "Natalia", ""], ["Wolf", "Christian", ""], ["Taylor", "Graham W.", ""], ["Nebout", "Florian", ""]]}, {"id": "1501.00125", "submitter": "Hao Wu", "authors": "Hao Wu", "title": "Maximum Margin Clustering for State Decomposition of Metastable Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NA cs.SY math.NA physics.data-an", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When studying a metastable dynamical system, a prime concern is how to\ndecompose the phase space into a set of metastable states. Unfortunately, the\nmetastable state decomposition based on simulation or experimental data is\nstill a challenge. The most popular and simplest approach is geometric\nclustering which is developed based on the classical clustering technique.\nHowever, the prerequisites of this approach are: (1) data are obtained from\nsimulations or experiments which are in global equilibrium and (2) the\ncoordinate system is appropriately selected. Recently, the kinetic clustering\napproach based on phase space discretization and transition probability\nestimation has drawn much attention due to its applicability to more general\ncases, but the choice of discretization policy is a difficult task. In this\npaper, a new decomposition method designated as maximum margin metastable\nclustering is proposed, which converts the problem of metastable state\ndecomposition to a semi-supervised learning problem so that the large margin\ntechnique can be utilized to search for the optimal decomposition without phase\nspace discretization. Moreover, several simulation examples are given to\nillustrate the effectiveness of the proposed method.\n", "versions": [{"version": "v1", "created": "Wed, 31 Dec 2014 12:50:08 GMT"}], "update_date": "2015-01-05", "authors_parsed": [["Wu", "Hao", ""]]}, {"id": "1501.00199", "submitter": "Alex Beutel", "authors": "Alex Beutel, Amr Ahmed and Alexander J. Smola", "title": "ACCAMS: Additive Co-Clustering to Approximate Matrices Succinctly", "comments": "22 pages, under review for conference publication", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Matrix completion and approximation are popular tools to capture a user's\npreferences for recommendation and to approximate missing data. Instead of\nusing low-rank factorization we take a drastically different approach, based on\nthe simple insight that an additive model of co-clusterings allows one to\napproximate matrices efficiently. This allows us to build a concise model that,\nper bit of model learned, significantly beats all factorization approaches to\nmatrix approximation. Even more surprisingly, we find that summing over small\nco-clusterings is more effective in modeling matrices than classic\nco-clustering, which uses just one large partitioning of the matrix.\n  Following Occam's razor principle suggests that the simple structure induced\nby our model better captures the latent preferences and decision making\nprocesses present in the real world than classic co-clustering or matrix\nfactorization. We provide an iterative minimization algorithm, a collapsed\nGibbs sampler, theoretical guarantees for matrix approximation, and excellent\nempirical evidence for the efficacy of our approach. We achieve\nstate-of-the-art results on the Netflix problem with a fraction of the model\ncomplexity.\n", "versions": [{"version": "v1", "created": "Wed, 31 Dec 2014 19:36:55 GMT"}], "update_date": "2015-01-05", "authors_parsed": [["Beutel", "Alex", ""], ["Ahmed", "Amr", ""], ["Smola", "Alexander J.", ""]]}, {"id": "1501.00263", "submitter": "Lin Xiao", "authors": "Yuchen Zhang and Lin Xiao", "title": "Communication-Efficient Distributed Optimization of Self-Concordant\n  Empirical Loss", "comments": null, "journal-ref": null, "doi": null, "report-no": "MSR-TR-2015-1", "categories": "math.OC cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider distributed convex optimization problems originated from sample\naverage approximation of stochastic optimization, or empirical risk\nminimization in machine learning. We assume that each machine in the\ndistributed computing system has access to a local empirical loss function,\nconstructed with i.i.d. data sampled from a common distribution. We propose a\ncommunication-efficient distributed algorithm to minimize the overall empirical\nloss, which is the average of the local empirical losses. The algorithm is\nbased on an inexact damped Newton method, where the inexact Newton steps are\ncomputed by a distributed preconditioned conjugate gradient method. We analyze\nits iteration complexity and communication efficiency for minimizing\nself-concordant empirical loss functions, and discuss the results for\ndistributed ridge regression, logistic regression and binary classification\nwith a smoothed hinge loss. In a standard setting for supervised learning, the\nrequired number of communication rounds of the algorithm does not increase with\nthe sample size, and only grows slowly with the number of machines.\n", "versions": [{"version": "v1", "created": "Thu, 1 Jan 2015 09:21:57 GMT"}], "update_date": "2015-01-05", "authors_parsed": [["Zhang", "Yuchen", ""], ["Xiao", "Lin", ""]]}, {"id": "1501.00287", "submitter": "Harikrishna Narasimhan", "authors": "Harish G. Ramaswamy, Harikrishna Narasimhan, Shivani Agarwal", "title": "Consistent Classification Algorithms for Multi-class Non-Decomposable\n  Performance Metrics", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study consistency of learning algorithms for a multi-class performance\nmetric that is a non-decomposable function of the confusion matrix of a\nclassifier and cannot be expressed as a sum of losses on individual data\npoints; examples of such performance metrics include the macro F-measure\npopular in information retrieval and the G-mean metric used in class-imbalanced\nproblems. While there has been much work in recent years in understanding the\nconsistency properties of learning algorithms for `binary' non-decomposable\nmetrics, little is known either about the form of the optimal classifier for a\ngeneral multi-class non-decomposable metric, or about how these learning\nalgorithms generalize to the multi-class case. In this paper, we provide a\nunified framework for analysing a multi-class non-decomposable performance\nmetric, where the problem of finding the optimal classifier for the performance\nmetric is viewed as an optimization problem over the space of all confusion\nmatrices achievable under the given distribution. Using this framework, we show\nthat (under a continuous distribution) the optimal classifier for a multi-class\nperformance metric can be obtained as the solution of a cost-sensitive\nclassification problem, thus generalizing several previous results on specific\nbinary non-decomposable metrics. We then design a consistent learning algorithm\nfor concave multi-class performance metrics that proceeds via a sequence of\ncost-sensitive classification problems, and can be seen as applying the\nconditional gradient (CG) optimization method over the space of feasible\nconfusion matrices. To our knowledge, this is the first efficient learning\nalgorithm (whose running time is polynomial in the number of classes) that is\nconsistent for a large family of multi-class non-decomposable metrics. Our\nconsistency proof uses a novel technique based on the convergence analysis of\nthe CG method.\n", "versions": [{"version": "v1", "created": "Thu, 1 Jan 2015 16:22:58 GMT"}], "update_date": "2015-01-05", "authors_parsed": [["Ramaswamy", "Harish G.", ""], ["Narasimhan", "Harikrishna", ""], ["Agarwal", "Shivani", ""]]}, {"id": "1501.00299", "submitter": "Mohammad Pezeshki", "authors": "Mohammad Pezeshki", "title": "Sequence Modeling using Gated Recurrent Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we have used Recurrent Neural Networks to capture and model\nhuman motion data and generate motions by prediction of the next immediate data\npoint at each time-step. Our RNN is armed with recently proposed Gated\nRecurrent Units which has shown promising results in some sequence modeling\nproblems such as Machine Translation and Speech Synthesis. We demonstrate that\nthis model is able to capture long-term dependencies in data and generate\nrealistic motions.\n", "versions": [{"version": "v1", "created": "Thu, 1 Jan 2015 18:37:36 GMT"}], "update_date": "2015-01-05", "authors_parsed": [["Pezeshki", "Mohammad", ""]]}, {"id": "1501.00320", "submitter": "Sameer Pawar", "authors": "Sameer Pawar and Kannan Ramchandran", "title": "A robust sub-linear time R-FFAST algorithm for computing a sparse DFT", "comments": "35 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.LG math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Fast Fourier Transform (FFT) is the most efficiently known way to compute\nthe Discrete Fourier Transform (DFT) of an arbitrary n-length signal, and has a\ncomputational complexity of O(n log n). If the DFT X of the signal x has only k\nnon-zero coefficients (where k < n), can we do better? In [1], we addressed\nthis question and presented a novel FFAST (Fast Fourier Aliasing-based Sparse\nTransform) algorithm that cleverly induces sparse graph alias codes in the DFT\ndomain, via a Chinese-Remainder-Theorem (CRT)-guided sub-sampling operation of\nthe time-domain samples. The resulting sparse graph alias codes are then\nexploited to devise a fast and iterative onion-peeling style decoder that\ncomputes an n length DFT of a signal using only O(k) time-domain samples and\nO(klog k) computations. The FFAST algorithm is applicable whenever k is\nsub-linear in n (i.e. k = o(n)), but is obviously most attractive when k is\nmuch smaller than n.\n  In this paper, we adapt the FFAST framework of [1] to the case where the\ntime-domain samples are corrupted by a white Gaussian noise. In particular, we\nshow that the extended noise robust algorithm R-FFAST computes an n-length\nk-sparse DFT X using O(klog ^3 n) noise-corrupted time-domain samples, in\nO(klog^4n) computations, i.e., sub-linear time complexity. While our\ntheoretical results are for signals with a uniformly random support of the\nnon-zero DFT coefficients and additive white Gaussian noise, we provide\nsimulation results which demonstrates that the R-FFAST algorithm performs well\neven for signals like MR images, that have an approximately sparse Fourier\nspectrum with a non-uniform support for the dominant DFT coefficients.\n", "versions": [{"version": "v1", "created": "Thu, 1 Jan 2015 21:48:00 GMT"}], "update_date": "2015-01-05", "authors_parsed": [["Pawar", "Sameer", ""], ["Ramchandran", "Kannan", ""]]}, {"id": "1501.00329", "submitter": "Pol Blasco Moreno", "authors": "Pol Blasco and Deniz Gunduz", "title": "Multi-Access Communications with Energy Harvesting: A Multi-Armed Bandit\n  Model and the Optimality of the Myopic Policy", "comments": "accepted for publication in IEEE Journal of Selected Areas in\n  Communications (IEEE JSAC) Special Issue on Wireless Communications Powered\n  by Energy Harvesting and Wireless Energy Transfer", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.LG math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A multi-access wireless network with N transmitting nodes, each equipped with\nan energy harvesting (EH) device and a rechargeable battery of finite capacity,\nis studied. At each time slot (TS) a node is operative with a certain\nprobability, which may depend on the availability of data, or the state of its\nchannel. The energy arrival process at each node is modelled as an independent\ntwo-state Markov process, such that, at each TS, a node either harvests one\nunit of energy, or none. At each TS a subset of the nodes is scheduled by the\naccess point (AP). The scheduling policy that maximises the total throughput is\nstudied assuming that the AP does not know the states of either the EH\nprocesses or the batteries. The problem is identified as a restless multiarmed\nbandit (RMAB) problem, and an upper bound on the optimal scheduling policy is\nfound. Under certain assumptions regarding the EH processes and the battery\nsizes, the optimality of the myopic policy (MP) is proven. For the general\ncase, the performance of MP is compared numerically to the upper bound.\n", "versions": [{"version": "v1", "created": "Thu, 1 Jan 2015 23:05:09 GMT"}], "update_date": "2015-01-05", "authors_parsed": [["Blasco", "Pol", ""], ["Gunduz", "Deniz", ""]]}, {"id": "1501.00358", "submitter": "Cheng Yang", "authors": "Cheng Yang and Zhiyuan Liu", "title": "Comprehend DeepWalk as Matrix Factorization", "comments": "4 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Word2vec, as an efficient tool for learning vector representation of words\nhas shown its effectiveness in many natural language processing tasks. Mikolov\net al. issued Skip-Gram and Negative Sampling model for developing this\ntoolbox. Perozzi et al. introduced the Skip-Gram model into the study of social\nnetwork for the first time, and designed an algorithm named DeepWalk for\nlearning node embedding on a graph. We prove that the DeepWalk algorithm is\nactually factoring a matrix M where each entry M_{ij} is logarithm of the\naverage probability that node i randomly walks to node j in fix steps.\n", "versions": [{"version": "v1", "created": "Fri, 2 Jan 2015 07:57:14 GMT"}], "update_date": "2015-01-05", "authors_parsed": [["Yang", "Cheng", ""], ["Liu", "Zhiyuan", ""]]}, {"id": "1501.00375", "submitter": "Wittawat Jitkrittum", "authors": "Wittawat Jitkrittum, Arthur Gretton, Nicolas Heess", "title": "Passing Expectation Propagation Messages with Kernel Methods", "comments": "Accepted to Advances in Variational Inference, NIPS 2014 Workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose to learn a kernel-based message operator which takes as input all\nexpectation propagation (EP) incoming messages to a factor node and produces an\noutgoing message. In ordinary EP, computing an outgoing message involves\nestimating a multivariate integral which may not have an analytic expression.\nLearning such an operator allows one to bypass the expensive computation of the\nintegral during inference by directly mapping all incoming messages into an\noutgoing message. The operator can be learned from training data (examples of\ninput and output messages) which allows automated inference to be made on any\nkind of factor that can be sampled.\n", "versions": [{"version": "v1", "created": "Fri, 2 Jan 2015 10:00:07 GMT"}], "update_date": "2015-03-11", "authors_parsed": [["Jitkrittum", "Wittawat", ""], ["Gretton", "Arthur", ""], ["Heess", "Nicolas", ""]]}, {"id": "1501.00405", "submitter": "Puneet Agarwal", "authors": "Puneet Agarwal, Gautam Shroff, Sarmimala Saikia, and Zaigham Khan", "title": "Efficiently Discovering Frequent Motifs in Large-scale Sensor Data", "comments": "13 pages, 8 figures, Technical Report", "journal-ref": null, "doi": null, "report-no": "TR-DAIF-2015-1", "categories": "cs.DB cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  While analyzing vehicular sensor data, we found that frequently occurring\nwaveforms could serve as features for further analysis, such as rule mining,\nclassification, and anomaly detection. The discovery of waveform patterns, also\nknown as time-series motifs, has been studied extensively; however, available\ntechniques for discovering frequently occurring time-series motifs were found\nlacking in either efficiency or quality: Standard subsequence clustering\nresults in poor quality, to the extent that it has even been termed\n'meaningless'. Variants of hierarchical clustering using techniques for\nefficient discovery of 'exact pair motifs' find high-quality frequent motifs,\nbut at the cost of high computational complexity, making such techniques\nunusable for our voluminous vehicular sensor data. We show that good quality\nfrequent motifs can be discovered using bounded spherical clustering of\ntime-series subsequences, which we refer to as COIN clustering, with near\nlinear complexity in time-series size. COIN clustering addresses many of the\nchallenges that previously led to subsequence clustering being viewed as\nmeaningless. We describe an end-to-end motif-discovery procedure using a\nsequence of pre and post-processing techniques that remove trivial-matches and\nshifted-motifs, which also plagued previous subsequence-clustering approaches.\nWe demonstrate that our technique efficiently discovers frequent motifs in\nvoluminous vehicular sensor data as well as in publicly available data sets.\n", "versions": [{"version": "v1", "created": "Fri, 2 Jan 2015 14:09:46 GMT"}], "update_date": "2015-01-05", "authors_parsed": [["Agarwal", "Puneet", ""], ["Shroff", "Gautam", ""], ["Saikia", "Sarmimala", ""], ["Khan", "Zaigham", ""]]}, {"id": "1501.00437", "submitter": "Shai Ben-David", "authors": "Shai Ben-David", "title": "Computational Feasibility of Clustering under Clusterability Assumptions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is well known that most of the common clustering objectives are NP-hard to\noptimize. In practice, however, clustering is being routinely carried out. One\napproach for providing theoretical understanding of this seeming discrepancy is\nto come up with notions of clusterability that distinguish realistically\ninteresting input data from worst-case data sets. The hope is that there will\nbe clustering algorithms that are provably efficient on such 'clusterable'\ninstances. In other words, hope that \"Clustering is difficult only when it does\nnot matter\" (CDNM thesis, for short).\n  We believe that to some extent this may indeed be the case. This paper\nprovides a survey of recent papers along this line of research and a critical\nevaluation their results. Our bottom line conclusion is that that CDNM thesis\nis still far from being formally substantiated. We start by discussing which\nrequirements should be met in order to provide formal support the validity of\nthe CDNM thesis. In particular, we list some implied requirements for notions\nof clusterability. We then examine existing results in view of those\nrequirements and outline some research challenges and open questions.\n", "versions": [{"version": "v1", "created": "Fri, 2 Jan 2015 17:10:52 GMT"}], "update_date": "2015-01-05", "authors_parsed": [["Ben-David", "Shai", ""]]}, {"id": "1501.00503", "submitter": "Sebasti\\'an Basterrech", "authors": "Sebasti\\'an Basterrech", "title": "An Empirical Study of the L2-Boost technique with Echo State Networks", "comments": "To appear in Journal of Network and Innovative Computing, Volume 2,\n  Issue 1, pp. 120 - 127, 2014", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A particular case of Recurrent Neural Network (RNN) was introduced at the\nbeginning of the 2000s under the name of Echo State Networks (ESNs). The ESN\nmodel overcomes the limitations during the training of the RNNs while\nintroducing no significant disadvantages. Although the model presents some\nwell-identified drawbacks when the parameters are not well initialised. The\nperformance of an ESN is highly dependent on its internal parameters and\npattern of connectivity of the hidden-hidden weights Often, the tuning of the\nnetwork parameters can be hard and can impact in the accuracy of the models.\n  In this work, we investigate the performance of a specific boosting technique\n(called L2-Boost) with ESNs as single predictors. The L2-Boost technique has\nbeen shown to be an effective tool to combine \"weak\" predictors in regression\nproblems. In this study, we use an ensemble of random initialized ESNs (without\ncontrol their parameters) as \"weak\" predictors of the boosting procedure. We\nevaluate our approach on five well-know time-series benchmark problems.\nAdditionally, we compare this technique with a baseline approach that consists\nof averaging the prediction of an ensemble of ESNs.\n", "versions": [{"version": "v1", "created": "Fri, 2 Jan 2015 21:15:00 GMT"}], "update_date": "2015-01-06", "authors_parsed": [["Basterrech", "Sebasti\u00e1n", ""]]}, {"id": "1501.00559", "submitter": "Hao-Chung Cheng", "authors": "Hao-Chung Cheng, Min-Hsiu Hsieh, Ping-Cheng Yeh", "title": "The Learnability of Unknown Quantum Measurements", "comments": null, "journal-ref": "QIC, Vol. 16, No. 7-8, 0615-0656 (2016)", "doi": null, "report-no": null, "categories": "quant-ph cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Quantum machine learning has received significant attention in recent years,\nand promising progress has been made in the development of quantum algorithms\nto speed up traditional machine learning tasks. In this work, however, we focus\non investigating the information-theoretic upper bounds of sample complexity -\nhow many training samples are sufficient to predict the future behaviour of an\nunknown target function. This kind of problem is, arguably, one of the most\nfundamental problems in statistical learning theory and the bounds for\npractical settings can be completely characterised by a simple measure of\ncomplexity.\n  Our main result in the paper is that, for learning an unknown quantum\nmeasurement, the upper bound, given by the fat-shattering dimension, is\nlinearly proportional to the dimension of the underlying Hilbert space.\nLearning an unknown quantum state becomes a dual problem to ours, and as a\nbyproduct, we can recover Aaronson's famous result [Proc. R. Soc. A\n463:3089-3144 (2007)] solely using a classical machine learning technique. In\naddition, other famous complexity measures like covering numbers and Rademacher\ncomplexities are derived explicitly. We are able to connect measures of sample\ncomplexity with various areas in quantum information science, e.g. quantum\nstate/measurement tomography, quantum state discrimination and quantum random\naccess codes, which may be of independent interest. Lastly, with the assistance\nof general Bloch-sphere representation, we show that learning quantum\nmeasurements/states can be mathematically formulated as a neural network.\nConsequently, classical ML algorithms can be applied to efficiently accomplish\nthe two quantum learning tasks.\n", "versions": [{"version": "v1", "created": "Sat, 3 Jan 2015 12:26:10 GMT"}], "update_date": "2016-08-25", "authors_parsed": [["Cheng", "Hao-Chung", ""], ["Hsieh", "Min-Hsiu", ""], ["Yeh", "Ping-Cheng", ""]]}, {"id": "1501.00607", "submitter": "Kwetishe Danjuma", "authors": "Kwetishe Danjuma and Adenike O. Osofisan", "title": "Evaluation of Predictive Data Mining Algorithms in Erythemato-Squamous\n  Disease Diagnosis", "comments": "10 pages, 3 figures 2 tables", "journal-ref": "IJCSI International Journal of Computer Science Issues, 11(6),\n  85-94 (2014)", "doi": null, "report-no": null, "categories": "cs.LG cs.CE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A lot of time is spent searching for the most performing data mining\nalgorithms applied in clinical diagnosis. The study set out to identify the\nmost performing predictive data mining algorithms applied in the diagnosis of\nErythemato-squamous diseases. The study used Naive Bayes, Multilayer Perceptron\nand J48 decision tree induction to build predictive data mining models on 366\ninstances of Erythemato-squamous diseases datasets. Also, 10-fold\ncross-validation and sets of performance metrics were used to evaluate the\nbaseline predictive performance of the classifiers. The comparative analysis\nshows that the Naive Bayes performed best with accuracy of 97.4%, Multilayer\nPerceptron came out second with accuracy of 96.6%, and J48 came out the worst\nwith accuracy of 93.5%. The evaluation of these classifiers on clinical\ndatasets, gave an insight into the predictive ability of different data mining\nalgorithms applicable in clinical diagnosis especially in the diagnosis of\nErythemato-squamous diseases.\n", "versions": [{"version": "v1", "created": "Sat, 3 Jan 2015 21:34:35 GMT"}], "update_date": "2015-01-06", "authors_parsed": [["Danjuma", "Kwetishe", ""], ["Osofisan", "Adenike O.", ""]]}, {"id": "1501.00687", "submitter": "Ahmad Hassanat", "authors": "Mouhammd Alkasassbeh, Ghada A. Altarawneh, Ahmad B. A. Hassanat", "title": "On Enhancing The Performance Of Nearest Neighbour Classifiers Using\n  Hassanat Distance Metric", "comments": "Canadian Journal of Pure and Applied Sciences (CJPAS). volume 9,\n  issue 1, Feb 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We showed in this work how the Hassanat distance metric enhances the\nperformance of the nearest neighbour classifiers. The results demonstrate the\nsuperiority of this distance metric over the traditional and most-used\ndistances, such as Manhattan distance and Euclidian distance. Moreover, we\nproved that the Hassanat distance metric is invariant to data scale, noise and\noutliers. Throughout this work, it is clearly notable that both ENN and IINC\nperformed very well with the distance investigated, as their accuracy increased\nsignificantly by 3.3% and 3.1% respectively, with no significant advantage of\nthe ENN over the IINC in terms of accuracy. Correspondingly, it can be noted\nfrom our results that there is no optimal algorithm that can solve all\nreal-life problems perfectly; this is supported by the no-free-lunch theorem\n", "versions": [{"version": "v1", "created": "Sun, 4 Jan 2015 15:37:20 GMT"}], "update_date": "2015-01-06", "authors_parsed": [["Alkasassbeh", "Mouhammd", ""], ["Altarawneh", "Ghada A.", ""], ["Hassanat", "Ahmad B. A.", ""]]}, {"id": "1501.00728", "submitter": "Ahmed Ibrahim Taloba", "authors": "M. H. Marghny, Rasha M. Abd ElAziz, Ahmed I. Taloba", "title": "Differential Search Algorithm-based Parametric Optimization of Fuzzy\n  Generalized Eigenvalue Proximal Support Vector Machine", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Support Vector Machine (SVM) is an effective model for many classification\nproblems. However, SVM needs the solution of a quadratic program which require\nspecialized code. In addition, SVM has many parameters, which affects the\nperformance of SVM classifier. Recently, the Generalized Eigenvalue Proximal\nSVM (GEPSVM) has been presented to solve the SVM complexity. In real world\napplications data may affected by error or noise, working with this data is a\nchallenging problem. In this paper, an approach has been proposed to overcome\nthis problem. This method is called DSA-GEPSVM. The main improvements are\ncarried out based on the following: 1) a novel fuzzy values in the linear case.\n2) A new Kernel function in the nonlinear case. 3) Differential Search\nAlgorithm (DSA) is reformulated to find near optimal values of the GEPSVM\nparameters and its kernel parameters. The experimental results show that the\nproposed approach is able to find the suitable parameter values, and has higher\nclassification accuracy compared with some other algorithms.\n", "versions": [{"version": "v1", "created": "Sun, 4 Jan 2015 22:12:36 GMT"}], "update_date": "2015-01-06", "authors_parsed": [["Marghny", "M. H.", ""], ["ElAziz", "Rasha M. Abd", ""], ["Taloba", "Ahmed I.", ""]]}, {"id": "1501.00752", "submitter": "Alexander Wong", "authors": "Mohammad Shafiee, Zohreh Azimifar, and Alexander Wong", "title": "A Deep-structured Conditional Random Field Model for Object Silhouette\n  Tracking", "comments": "17 pages", "journal-ref": null, "doi": "10.1371/journal.pone.0133036", "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we introduce a deep-structured conditional random field\n(DS-CRF) model for the purpose of state-based object silhouette tracking. The\nproposed DS-CRF model consists of a series of state layers, where each state\nlayer spatially characterizes the object silhouette at a particular point in\ntime. The interactions between adjacent state layers are established by\ninter-layer connectivity dynamically determined based on inter-frame optical\nflow. By incorporate both spatial and temporal context in a dynamic fashion\nwithin such a deep-structured probabilistic graphical model, the proposed\nDS-CRF model allows us to develop a framework that can accurately and\nefficiently track object silhouettes that can change greatly over time, as well\nas under different situations such as occlusion and multiple targets within the\nscene. Experiment results using video surveillance datasets containing\ndifferent scenarios such as occlusion and multiple targets showed that the\nproposed DS-CRF approach provides strong object silhouette tracking performance\nwhen compared to baseline methods such as mean-shift tracking, as well as\nstate-of-the-art methods such as context tracking and boosted particle\nfiltering.\n", "versions": [{"version": "v1", "created": "Mon, 5 Jan 2015 03:09:34 GMT"}, {"version": "v2", "created": "Tue, 4 Aug 2015 18:27:20 GMT"}], "update_date": "2016-02-17", "authors_parsed": [["Shafiee", "Mohammad", ""], ["Azimifar", "Zohreh", ""], ["Wong", "Alexander", ""]]}, {"id": "1501.00756", "submitter": "Miguel \\'A. Carreira-Perpi\\~n\\'an", "authors": "Miguel \\'A. Carreira-Perpi\\~n\\'an, Ramin Raziperchikolaei", "title": "Hashing with binary autoencoders", "comments": "22 pages, 11 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An attractive approach for fast search in image databases is binary hashing,\nwhere each high-dimensional, real-valued image is mapped onto a\nlow-dimensional, binary vector and the search is done in this binary space.\nFinding the optimal hash function is difficult because it involves binary\nconstraints, and most approaches approximate the optimization by relaxing the\nconstraints and then binarizing the result. Here, we focus on the binary\nautoencoder model, which seeks to reconstruct an image from the binary code\nproduced by the hash function. We show that the optimization can be simplified\nwith the method of auxiliary coordinates. This reformulates the optimization as\nalternating two easier steps: one that learns the encoder and decoder\nseparately, and one that optimizes the code for each image. Image retrieval\nexperiments, using precision/recall and a measure of code utilization, show the\nresulting hash function outperforms or is competitive with state-of-the-art\nmethods for binary hashing.\n", "versions": [{"version": "v1", "created": "Mon, 5 Jan 2015 03:49:02 GMT"}], "update_date": "2015-01-23", "authors_parsed": [["Carreira-Perpi\u00f1\u00e1n", "Miguel \u00c1.", ""], ["Raziperchikolaei", "Ramin", ""]]}, {"id": "1501.00777", "submitter": "Jun Li", "authors": "Jun Li, Heyou Chang, Jian Yang", "title": "Sparse Deep Stacking Network for Image Classification", "comments": "8 pages, 3 figures, AAAI-2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.NE", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  Sparse coding can learn good robust representation to noise and model more\nhigher-order representation for image classification. However, the inference\nalgorithm is computationally expensive even though the supervised signals are\nused to learn compact and discriminative dictionaries in sparse coding\ntechniques. Luckily, a simplified neural network module (SNNM) has been\nproposed to directly learn the discriminative dictionaries for avoiding the\nexpensive inference. But the SNNM module ignores the sparse representations.\nTherefore, we propose a sparse SNNM module by adding the mixed-norm\nregularization (l1/l2 norm). The sparse SNNM modules are further stacked to\nbuild a sparse deep stacking network (S-DSN). In the experiments, we evaluate\nS-DSN with four databases, including Extended YaleB, AR, 15 scene and\nCaltech101. Experimental results show that our model outperforms related\nclassification methods with only a linear classifier. It is worth noting that\nwe reach 98.8% recognition accuracy on 15 scene.\n", "versions": [{"version": "v1", "created": "Mon, 5 Jan 2015 08:07:31 GMT"}], "update_date": "2015-01-06", "authors_parsed": [["Li", "Jun", ""], ["Chang", "Heyou", ""], ["Yang", "Jian", ""]]}, {"id": "1501.01209", "submitter": "Vikram Krishnamurthy", "authors": "Omid Namvar Gharehshiran and William Hoiles and Vikram Krishnamurthy", "title": "Reinforcement Learning and Nonparametric Detection of Game-Theoretic\n  Equilibrium Play in Social Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GT cs.LG cs.SI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper studies two important signal processing aspects of equilibrium\nbehavior in non-cooperative games arising in social networks, namely,\nreinforcement learning and detection of equilibrium play. The first part of the\npaper presents a reinforcement learning (adaptive filtering) algorithm that\nfacilitates learning an equilibrium by resorting to diffusion cooperation\nstrategies in a social network. Agents form homophilic social groups, within\nwhich they exchange past experiences over an undirected graph. It is shown\nthat, if all agents follow the proposed algorithm, their global behavior is\nattracted to the correlated equilibria set of the game. The second part of the\npaper provides a test to detect if the actions of agents are consistent with\nplay from the equilibrium of a concave potential game. The theory of revealed\npreference from microeconomics is used to construct a non-parametric decision\ntest and statistical test which only require the probe and associated actions\nof agents. A stochastic gradient algorithm is given to optimize the probe in\nreal time to minimize the Type-II error probabilities of the detection test\nsubject to specified Type-I error probability. We provide a real-world example\nusing the energy market, and a numerical example to detect malicious agents in\nan online social network.\n", "versions": [{"version": "v1", "created": "Thu, 11 Dec 2014 05:00:26 GMT"}], "update_date": "2015-01-07", "authors_parsed": [["Gharehshiran", "Omid Namvar", ""], ["Hoiles", "William", ""], ["Krishnamurthy", "Vikram", ""]]}, {"id": "1501.01242", "submitter": "Eric Heim", "authors": "Eric Heim (1), Matthew Berger (2), Lee M. Seversky (2), and Milos\n  Hauskrecht (1) ((1) University of Pittsburgh, (2) Air Force Research\n  Laboratory, Information Directorate)", "title": "Efficient Online Relative Comparison Kernel Learning", "comments": "Extended version of the paper appearing in The Proceedings of the\n  2015 SIAM International Conference on Data Mining (SDM15)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning a kernel matrix from relative comparison human feedback is an\nimportant problem with applications in collaborative filtering, object\nretrieval, and search. For learning a kernel over a large number of objects,\nexisting methods face significant scalability issues inhibiting the application\nof these methods to settings where a kernel is learned in an online and timely\nfashion. In this paper we propose a novel framework called Efficient online\nRelative comparison Kernel LEarning (ERKLE), for efficiently learning the\nsimilarity of a large set of objects in an online manner. We learn a kernel\nfrom relative comparisons via stochastic gradient descent, one query response\nat a time, by taking advantage of the sparse and low-rank properties of the\ngradient to efficiently restrict the kernel to lie in the space of positive\nsemidefinite matrices. In addition, we derive a passive-aggressive online\nupdate for minimally satisfying new relative comparisons as to not disrupt the\ninfluence of previously obtained comparisons. Experimentally, we demonstrate a\nconsiderable improvement in speed while obtaining improved or comparable\naccuracy compared to current methods in the online learning setting.\n", "versions": [{"version": "v1", "created": "Tue, 6 Jan 2015 17:19:06 GMT"}, {"version": "v2", "created": "Mon, 12 Jan 2015 14:10:03 GMT"}], "update_date": "2015-01-13", "authors_parsed": [["Heim", "Eric", ""], ["Berger", "Matthew", ""], ["Seversky", "Lee M.", ""], ["Hauskrecht", "Milos", ""]]}, {"id": "1501.01321", "submitter": "Silas Santiago Lopes Pereira Pereira", "authors": "Silas Santiago Lopes Pereira, Jos\\'e Everardo Bessa Maia and Jorge\n  Luiz de Castro e Silva", "title": "ITCM: A Real Time Internet Traffic Classifier Monitor", "comments": "16 pages, 3 figures, 7 tables, International Journal of Computer\n  Science & Information Technology (IJCSIT) Vol 6, No 6, December 2014", "journal-ref": "International Journal of Computer Science & Information Technology\n  (IJCSIT) Vol 6, No 6, December 2014", "doi": "10.5121/ijcsit.2014.6602", "report-no": null, "categories": "cs.NI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The continual growth of high speed networks is a challenge for real-time\nnetwork analysis systems. The real time traffic classification is an issue for\ncorporations and ISPs (Internet Service Providers). This work presents the\ndesign and implementation of a real time flow-based network traffic\nclassification system. The classifier monitor acts as a pipeline consisting of\nthree modules: packet capture and pre-processing, flow reassembly, and\nclassification with Machine Learning (ML). The modules are built as concurrent\nprocesses with well defined data interfaces between them so that any module can\nbe improved and updated independently. In this pipeline, the flow reassembly\nfunction becomes the bottleneck of the performance. In this implementation, was\nused a efficient method of reassembly which results in a average delivery delay\nof 0.49 seconds, approximately. For the classification module, the performances\nof the K-Nearest Neighbor (KNN), C4.5 Decision Tree, Naive Bayes (NB), Flexible\nNaive Bayes (FNB) and AdaBoost Ensemble Learning Algorithm are compared in\norder to validate our approach.\n", "versions": [{"version": "v1", "created": "Tue, 6 Jan 2015 21:27:38 GMT"}], "update_date": "2015-01-08", "authors_parsed": [["Pereira", "Silas Santiago Lopes", ""], ["Maia", "Jos\u00e9 Everardo Bessa", ""], ["Silva", "Jorge Luiz de Castro e", ""]]}, {"id": "1501.01348", "submitter": "Lee Zamparo", "authors": "Lee Zamparo and Zhaolei Zhang", "title": "Deep Autoencoders for Dimensionality Reduction of High-Content Screening\n  Data", "comments": "5 pages, 3 figures. Submitted to MLCB 2014 (NIPS workshop, Machine\n  Learning in Computational Biology)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  High-content screening uses large collections of unlabeled cell image data to\nreason about genetics or cell biology. Two important tasks are to identify\nthose cells which bear interesting phenotypes, and to identify sub-populations\nenriched for these phenotypes. This exploratory data analysis usually involves\ndimensionality reduction followed by clustering, in the hope that clusters\nrepresent a phenotype. We propose the use of stacked de-noising auto-encoders\nto perform dimensionality reduction for high-content screening. We demonstrate\nthe superior performance of our approach over PCA, Local Linear Embedding,\nKernel PCA and Isomap.\n", "versions": [{"version": "v1", "created": "Wed, 7 Jan 2015 02:13:09 GMT"}], "update_date": "2015-01-08", "authors_parsed": [["Zamparo", "Lee", ""], ["Zhang", "Zhaolei", ""]]}, {"id": "1501.01689", "submitter": "Ananda Theertha Suresh", "authors": "Aditya Bhaskara, Ananda Theertha Suresh, Morteza Zadimoghaddam", "title": "Sparse Solutions to Nonnegative Linear Systems and Applications", "comments": "22 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.IT cs.LG math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We give an efficient algorithm for finding sparse approximate solutions to\nlinear systems of equations with nonnegative coefficients. Unlike most known\nresults for sparse recovery, we do not require {\\em any} assumption on the\nmatrix other than non-negativity. Our algorithm is combinatorial in nature,\ninspired by techniques for the set cover problem, as well as the multiplicative\nweight update method.\n  We then present a natural application to learning mixture models in the PAC\nframework. For learning a mixture of $k$ axis-aligned Gaussians in $d$\ndimensions, we give an algorithm that outputs a mixture of $O(k/\\epsilon^3)$\nGaussians that is $\\epsilon$-close in statistical distance to the true\ndistribution, without any separation assumptions. The time and sample\ncomplexity is roughly $O(kd/\\epsilon^3)^{d}$. This is polynomial when $d$ is\nconstant -- precisely the regime in which known methods fail to identify the\ncomponents efficiently.\n  Given that non-negativity is a natural assumption, we believe that our result\nmay find use in other settings in which we wish to approximately explain data\nusing a small number of a (large) candidate set of components.\n", "versions": [{"version": "v1", "created": "Wed, 7 Jan 2015 23:38:46 GMT"}], "update_date": "2015-01-09", "authors_parsed": [["Bhaskara", "Aditya", ""], ["Suresh", "Ananda Theertha", ""], ["Zadimoghaddam", "Morteza", ""]]}, {"id": "1501.01924", "submitter": "Leman Akoglu", "authors": "Shebuti Rayana and Leman Akoglu", "title": "Less is More: Building Selective Anomaly Ensembles", "comments": "14 pages, 5 pages Appendix, 10 Figures, 15 Tables, to appear at SDM\n  2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Ensemble techniques for classification and clustering have long proven\neffective, yet anomaly ensembles have been barely studied. In this work, we tap\ninto this gap and propose a new ensemble approach for anomaly mining, with\napplication to event detection in temporal graphs. Our method aims to combine\nresults from heterogeneous detectors with varying outputs, and leverage the\nevidence from multiple sources to yield better performance. However, trusting\nall the results may deteriorate the overall ensemble accuracy, as some\ndetectors may fall short and provide inaccurate results depending on the nature\nof the data in hand. This suggests that being selective in which results to\ncombine is vital in building effective ensembles---hence \"less is more\".\n  In this paper we propose SELECT; an ensemble approach for anomaly mining that\nemploys novel techniques to automatically and systematically select the results\nto assemble in a fully unsupervised fashion. We apply our method to event\ndetection in temporal graphs, where SELECT successfully utilizes five base\ndetectors and seven consensus methods under a unified ensemble framework. We\nprovide extensive quantitative evaluation of our approach on five real-world\ndatasets (four with ground truth), including Enron email communications, New\nYork Times news corpus, and World Cup 2014 Twitter news feed. Thanks to its\nselection mechanism, SELECT yields superior performance compared to individual\ndetectors alone, the full ensemble (naively combining all results), and an\nexisting diversity-based ensemble.\n", "versions": [{"version": "v1", "created": "Thu, 8 Jan 2015 18:54:09 GMT"}], "update_date": "2015-01-12", "authors_parsed": [["Rayana", "Shebuti", ""], ["Akoglu", "Leman", ""]]}, {"id": "1501.02056", "submitter": "Simon Lacoste-Julien", "authors": "Simon Lacoste-Julien (LIENS, INRIA Paris - Rocquencourt, MSR - INRIA),\n  Fredrik Lindsten, Francis Bach (LIENS, INRIA Paris - Rocquencourt, MSR -\n  INRIA)", "title": "Sequential Kernel Herding: Frank-Wolfe Optimization for Particle\n  Filtering", "comments": "in 18th International Conference on Artificial Intelligence and\n  Statistics (AISTATS), May 2015, San Diego, United States. 38, JMLR Workshop\n  and Conference Proceedings", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, the Frank-Wolfe optimization algorithm was suggested as a procedure\nto obtain adaptive quadrature rules for integrals of functions in a reproducing\nkernel Hilbert space (RKHS) with a potentially faster rate of convergence than\nMonte Carlo integration (and \"kernel herding\" was shown to be a special case of\nthis procedure). In this paper, we propose to replace the random sampling step\nin a particle filter by Frank-Wolfe optimization. By optimizing the position of\nthe particles, we can obtain better accuracy than random or quasi-Monte Carlo\nsampling. In applications where the evaluation of the emission probabilities is\nexpensive (such as in robot localization), the additional computational cost to\ngenerate the particles through optimization can be justified. Experiments on\nstandard synthetic examples as well as on a robot localization task indicate\nindeed an improvement of accuracy over random and quasi-Monte Carlo sampling.\n", "versions": [{"version": "v1", "created": "Fri, 9 Jan 2015 07:17:27 GMT"}, {"version": "v2", "created": "Tue, 10 Feb 2015 07:19:38 GMT"}], "update_date": "2015-02-11", "authors_parsed": [["Lacoste-Julien", "Simon", "", "LIENS, INRIA Paris - Rocquencourt, MSR - INRIA"], ["Lindsten", "Fredrik", "", "LIENS, INRIA Paris - Rocquencourt, MSR -\n  INRIA"], ["Bach", "Francis", "", "LIENS, INRIA Paris - Rocquencourt, MSR -\n  INRIA"]]}, {"id": "1501.02058", "submitter": "Mouloud Kachouane", "authors": "M. Kachouane (USTHB), S. Sahki, M. Lakrouf (CDTA, USTHB), N. Ouadah\n  (CDTA)", "title": "HOG based Fast Human Detection", "comments": null, "journal-ref": "24th International Conference on Microelectronics (ICM), 2012, Dec\n  2012, Alger, Algeria. pp.1 - 4", "doi": "10.1109/ICM.2012.6471380", "report-no": null, "categories": "cs.RO cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Objects recognition in image is one of the most difficult problems in\ncomputer vision. It is also an important step for the implementation of several\nexisting applications that require high-level image interpretation. Therefore,\nthere is a growing interest in this research area during the last years. In\nthis paper, we present an algorithm for human detection and recognition in\nreal-time, from images taken by a CCD camera mounted on a car-like mobile\nrobot. The proposed technique is based on Histograms of Oriented Gradient (HOG)\nand SVM classifier. The implementation of our detector has provided good\nresults, and can be used in robotics tasks.\n", "versions": [{"version": "v1", "created": "Fri, 9 Jan 2015 07:46:36 GMT"}], "update_date": "2015-01-12", "authors_parsed": [["Kachouane", "M.", "", "USTHB"], ["Sahki", "S.", "", "CDTA, USTHB"], ["Lakrouf", "M.", "", "CDTA, USTHB"], ["Ouadah", "N.", "", "CDTA"]]}, {"id": "1501.02393", "submitter": "Raviteja Vemulapalli", "authors": "Raviteja Vemulapalli, David W. Jacobs", "title": "Riemannian Metric Learning for Symmetric Positive Definite Matrices", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Over the past few years, symmetric positive definite (SPD) matrices have been\nreceiving considerable attention from computer vision community. Though various\ndistance measures have been proposed in the past for comparing SPD matrices,\nthe two most widely-used measures are affine-invariant distance and\nlog-Euclidean distance. This is because these two measures are true geodesic\ndistances induced by Riemannian geometry. In this work, we focus on the\nlog-Euclidean Riemannian geometry and propose a data-driven approach for\nlearning Riemannian metrics/geodesic distances for SPD matrices. We show that\nthe geodesic distance learned using the proposed approach performs better than\nvarious existing distance measures when evaluated on face matching and\nclustering tasks.\n", "versions": [{"version": "v1", "created": "Sat, 10 Jan 2015 21:12:09 GMT"}], "update_date": "2015-01-13", "authors_parsed": [["Vemulapalli", "Raviteja", ""], ["Jacobs", "David W.", ""]]}, {"id": "1501.02411", "submitter": "Haojun Li", "authors": "Haojun Li", "title": "A Gaussian Particle Filter Approach for Sensors to Track Multiple Moving\n  Targets", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In a variety of problems, the number and state of multiple moving targets are\nunknown and are subject to be inferred from their measurements obtained by a\nsensor with limited sensing ability. This type of problems is raised in a\nvariety of applications, including monitoring of endangered species, cleaning,\nand surveillance. Particle filters are widely used to estimate target state\nfrom its prior information and its measurements that recently become available,\nespecially for the cases when the measurement model and the prior distribution\nof state of interest are non-Gaussian. However, the problem of estimating\nnumber of total targets and their state becomes intractable when the number of\ntotal targets and the measurement-target association are unknown. This paper\npresents a novel Gaussian particle filter technique that combines Kalman filter\nand particle filter for estimating the number and state of total targets based\non the measurement obtained online. The estimation is represented by a set of\nweighted particles, different from classical particle filter, where each\nparticle is a Gaussian distribution instead of a point mass.\n", "versions": [{"version": "v1", "created": "Sun, 11 Jan 2015 02:24:26 GMT"}], "update_date": "2015-01-13", "authors_parsed": [["Li", "Haojun", ""]]}, {"id": "1501.02432", "submitter": "Jayadeva", "authors": "Jayadeva, Sanjit Singh Batra, and Siddarth Sabharwal", "title": "Learning a Fuzzy Hyperplane Fat Margin Classifier with Minimum VC\n  dimension", "comments": "arXiv admin note: text overlap with arXiv:1410.4573", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Vapnik-Chervonenkis (VC) dimension measures the complexity of a learning\nmachine, and a low VC dimension leads to good generalization. The recently\nproposed Minimal Complexity Machine (MCM) learns a hyperplane classifier by\nminimizing an exact bound on the VC dimension. This paper extends the MCM\nclassifier to the fuzzy domain. The use of a fuzzy membership is known to\nreduce the effect of outliers, and to reduce the effect of noise on learning.\nExperimental results show, that on a number of benchmark datasets, the the\nfuzzy MCM classifier outperforms SVMs and the conventional MCM in terms of\ngeneralization, and that the fuzzy MCM uses fewer support vectors. On several\nbenchmark datasets, the fuzzy MCM classifier yields excellent test set\naccuracies while using one-tenth the number of support vectors used by SVMs.\n", "versions": [{"version": "v1", "created": "Sun, 11 Jan 2015 09:29:05 GMT"}], "update_date": "2015-01-13", "authors_parsed": [["Jayadeva", "", ""], ["Batra", "Sanjit Singh", ""], ["Sabharwal", "Siddarth", ""]]}, {"id": "1501.02484", "submitter": "Jihun Hamm", "authors": "Jihun Hamm, Adam Champion, Guoxing Chen, Mikhail Belkin, Dong Xuan", "title": "Crowd-ML: A Privacy-Preserving Learning Framework for a Crowd of Smart\n  Devices", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR cs.DC cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Smart devices with built-in sensors, computational capabilities, and network\nconnectivity have become increasingly pervasive. The crowds of smart devices\noffer opportunities to collectively sense and perform computing tasks in an\nunprecedented scale. This paper presents Crowd-ML, a privacy-preserving machine\nlearning framework for a crowd of smart devices, which can solve a wide range\nof learning problems for crowdsensing data with differential privacy\nguarantees. Crowd-ML endows a crowdsensing system with an ability to learn\nclassifiers or predictors online from crowdsensing data privately with minimal\ncomputational overheads on devices and servers, suitable for a practical and\nlarge-scale employment of the framework. We analyze the performance and the\nscalability of Crowd-ML, and implement the system with off-the-shelf\nsmartphones as a proof of concept. We demonstrate the advantages of Crowd-ML\nwith real and simulated experiments under various conditions.\n", "versions": [{"version": "v1", "created": "Sun, 11 Jan 2015 18:57:28 GMT"}], "update_date": "2015-01-13", "authors_parsed": [["Hamm", "Jihun", ""], ["Champion", "Adam", ""], ["Chen", "Guoxing", ""], ["Belkin", "Mikhail", ""], ["Xuan", "Dong", ""]]}, {"id": "1501.02592", "submitter": "Michiel Hermans", "authors": "Michiel Hermans, Miguel Soriano, Joni Dambre, Peter Bienstman, Ingo\n  Fischer", "title": "Photonic Delay Systems as Machine Learning Implementations", "comments": null, "journal-ref": "Journal of Machine Learning Research, vol. 16, pp. 2081-2097\n  (2015)", "doi": null, "report-no": null, "categories": "cs.NE cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Nonlinear photonic delay systems present interesting implementation platforms\nfor machine learning models. They can be extremely fast, offer great degrees of\nparallelism and potentially consume far less power than digital processors. So\nfar they have been successfully employed for signal processing using the\nReservoir Computing paradigm. In this paper we show that their range of\napplicability can be greatly extended if we use gradient descent with\nbackpropagation through time on a model of the system to optimize the input\nencoding of such systems. We perform physical experiments that demonstrate that\nthe obtained input encodings work well in reality, and we show that optimized\nsystems perform significantly better than the common Reservoir Computing\napproach. The results presented here demonstrate that common gradient descent\ntechniques from machine learning may well be applicable on physical\nneuro-inspired analog computers.\n", "versions": [{"version": "v1", "created": "Mon, 12 Jan 2015 10:25:31 GMT"}], "update_date": "2016-03-24", "authors_parsed": [["Hermans", "Michiel", ""], ["Soriano", "Miguel", ""], ["Dambre", "Joni", ""], ["Bienstman", "Peter", ""], ["Fischer", "Ingo", ""]]}, {"id": "1501.02598", "submitter": "Angeliki  Lazaridou", "authors": "Angeliki Lazaridou, Nghia The Pham, Marco Baroni", "title": "Combining Language and Vision with a Multimodal Skip-gram Model", "comments": "accepted at NAACL 2015, camera ready version, 11 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We extend the SKIP-GRAM model of Mikolov et al. (2013a) by taking visual\ninformation into account. Like SKIP-GRAM, our multimodal models (MMSKIP-GRAM)\nbuild vector-based word representations by learning to predict linguistic\ncontexts in text corpora. However, for a restricted set of words, the models\nare also exposed to visual representations of the objects they denote\n(extracted from natural images), and must predict linguistic and visual\nfeatures jointly. The MMSKIP-GRAM models achieve good performance on a variety\nof semantic benchmarks. Moreover, since they propagate visual information to\nall words, we use them to improve image labeling and retrieval in the zero-shot\nsetup, where the test concepts are never seen during model training. Finally,\nthe MMSKIP-GRAM models discover intriguing visual properties of abstract words,\npaving the way to realistic implementations of embodied theories of meaning.\n", "versions": [{"version": "v1", "created": "Mon, 12 Jan 2015 10:48:32 GMT"}, {"version": "v2", "created": "Tue, 13 Jan 2015 09:37:08 GMT"}, {"version": "v3", "created": "Thu, 12 Mar 2015 09:47:33 GMT"}], "update_date": "2015-03-13", "authors_parsed": [["Lazaridou", "Angeliki", ""], ["Pham", "Nghia The", ""], ["Baroni", "Marco", ""]]}, {"id": "1501.02629", "submitter": "Aur\\'elien Bellet", "authors": "St\\'ephan Cl\\'emen\\c{c}on, Aur\\'elien Bellet, Igor Colin", "title": "Scaling-up Empirical Risk Minimization: Optimization of Incomplete\n  U-statistics", "comments": "To appear in Journal of Machine Learning Research. 34 pages. v2:\n  minor correction to Theorem 4 and its proof, added 1 reference. v3: typo\n  corrected in Proposition 3. v4: improved presentation, added experiments on\n  model selection for clustering, fixed minor typos", "journal-ref": "Journal of Machine Learning Research 17(76):1-36, 2016", "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In a wide range of statistical learning problems such as ranking, clustering\nor metric learning among others, the risk is accurately estimated by\n$U$-statistics of degree $d\\geq 1$, i.e. functionals of the training data with\nlow variance that take the form of averages over $k$-tuples. From a\ncomputational perspective, the calculation of such statistics is highly\nexpensive even for a moderate sample size $n$, as it requires averaging\n$O(n^d)$ terms. This makes learning procedures relying on the optimization of\nsuch data functionals hardly feasible in practice. It is the major goal of this\npaper to show that, strikingly, such empirical risks can be replaced by\ndrastically computationally simpler Monte-Carlo estimates based on $O(n)$ terms\nonly, usually referred to as incomplete $U$-statistics, without damaging the\n$O_{\\mathbb{P}}(1/\\sqrt{n})$ learning rate of Empirical Risk Minimization (ERM)\nprocedures. For this purpose, we establish uniform deviation results describing\nthe error made when approximating a $U$-process by its incomplete version under\nappropriate complexity assumptions. Extensions to model selection, fast rate\nsituations and various sampling techniques are also considered, as well as an\napplication to stochastic gradient descent for ERM. Finally, numerical examples\nare displayed in order to provide strong empirical evidence that the approach\nwe promote largely surpasses more naive subsampling techniques.\n", "versions": [{"version": "v1", "created": "Mon, 12 Jan 2015 12:58:45 GMT"}, {"version": "v2", "created": "Tue, 14 Jul 2015 22:42:22 GMT"}, {"version": "v3", "created": "Wed, 21 Oct 2015 12:42:17 GMT"}, {"version": "v4", "created": "Tue, 19 Apr 2016 06:30:09 GMT"}], "update_date": "2019-01-25", "authors_parsed": [["Cl\u00e9men\u00e7on", "St\u00e9phan", ""], ["Bellet", "Aur\u00e9lien", ""], ["Colin", "Igor", ""]]}, {"id": "1501.02702", "submitter": "Feng Nan", "authors": "Feng Nan, Joseph Wang, Venkatesh Saligrama", "title": "Max-Cost Discrete Function Evaluation Problem under a Budget", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose novel methods for max-cost Discrete Function Evaluation Problem\n(DFEP) under budget constraints. We are motivated by applications such as\nclinical diagnosis where a patient is subjected to a sequence of (possibly\nexpensive) tests before a decision is made. Our goal is to develop strategies\nfor minimizing max-costs. The problem is known to be NP hard and greedy methods\nbased on specialized impurity functions have been proposed. We develop a broad\nclass of \\emph{admissible} impurity functions that admit monomials, classes of\npolynomials, and hinge-loss functions that allow for flexible impurity design\nwith provably optimal approximation bounds. This flexibility is important for\ndatasets when max-cost can be overly sensitive to \"outliers.\" Outliers bias\nmax-cost to a few examples that require a large number of tests for\nclassification. We design admissible functions that allow for accuracy-cost\ntrade-off and result in $O(\\log n)$ guarantees of the optimal cost among trees\nwith corresponding classification accuracy levels.\n", "versions": [{"version": "v1", "created": "Mon, 12 Jan 2015 16:33:47 GMT"}], "update_date": "2015-01-13", "authors_parsed": [["Nan", "Feng", ""], ["Wang", "Joseph", ""], ["Saligrama", "Venkatesh", ""]]}, {"id": "1501.02859", "submitter": "Saiprasad Ravishankar", "authors": "Saiprasad Ravishankar and Yoram Bresler", "title": "$\\ell_0$ Sparsifying Transform Learning with Efficient Optimal Updates\n  and Convergence Guarantees", "comments": "Accepted to IEEE Transactions on Signal Processing", "journal-ref": null, "doi": "10.1109/TSP.2015.2405503", "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many applications in signal processing benefit from the sparsity of signals\nin a certain transform domain or dictionary. Synthesis sparsifying dictionaries\nthat are directly adapted to data have been popular in applications such as\nimage denoising, inpainting, and medical image reconstruction. In this work, we\nfocus instead on the sparsifying transform model, and study the learning of\nwell-conditioned square sparsifying transforms. The proposed algorithms\nalternate between a $\\ell_0$ \"norm\"-based sparse coding step, and a non-convex\ntransform update step. We derive the exact analytical solution for each of\nthese steps. The proposed solution for the transform update step achieves the\nglobal minimum in that step, and also provides speedups over iterative\nsolutions involving conjugate gradients. We establish that our alternating\nalgorithms are globally convergent to the set of local minimizers of the\nnon-convex transform learning problems. In practice, the algorithms are\ninsensitive to initialization. We present results illustrating the promising\nperformance and significant speed-ups of transform learning over synthesis\nK-SVD in image denoising.\n", "versions": [{"version": "v1", "created": "Tue, 13 Jan 2015 01:34:40 GMT"}], "update_date": "2015-06-23", "authors_parsed": [["Ravishankar", "Saiprasad", ""], ["Bresler", "Yoram", ""]]}, {"id": "1501.02923", "submitter": "Saiprasad Ravishankar", "authors": "Saiprasad Ravishankar and Yoram Bresler", "title": "Efficient Blind Compressed Sensing Using Sparsifying Transforms with\n  Convergence Guarantees and Application to MRI", "comments": "This work has been accepted for publication in the SIAM Journal on\n  Imaging Sciences. It also appears in Saiprasad Ravishankar's PhD thesis, that\n  was deposited with the University of Illinois on December 05, 2014", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Natural signals and images are well-known to be approximately sparse in\ntransform domains such as Wavelets and DCT. This property has been heavily\nexploited in various applications in image processing and medical imaging.\nCompressed sensing exploits the sparsity of images or image patches in a\ntransform domain or synthesis dictionary to reconstruct images from\nundersampled measurements. In this work, we focus on blind compressed sensing,\nwhere the underlying sparsifying transform is a priori unknown, and propose a\nframework to simultaneously reconstruct the underlying image as well as the\nsparsifying transform from highly undersampled measurements. The proposed block\ncoordinate descent type algorithms involve highly efficient optimal updates.\nImportantly, we prove that although the proposed blind compressed sensing\nformulations are highly nonconvex, our algorithms are globally convergent\n(i.e., they converge from any initialization) to the set of critical points of\nthe objectives defining the formulations. These critical points are guaranteed\nto be at least partial global and partial local minimizers. The exact point(s)\nof convergence may depend on initialization. We illustrate the usefulness of\nthe proposed framework for magnetic resonance image reconstruction from highly\nundersampled k-space measurements. As compared to previous methods involving\nthe synthesis dictionary model, our approach is much faster, while also\nproviding promising reconstruction quality.\n", "versions": [{"version": "v1", "created": "Tue, 13 Jan 2015 09:19:27 GMT"}, {"version": "v2", "created": "Fri, 23 Oct 2015 01:58:24 GMT"}], "update_date": "2015-10-26", "authors_parsed": [["Ravishankar", "Saiprasad", ""], ["Bresler", "Yoram", ""]]}, {"id": "1501.02990", "submitter": "Yi  Li", "authors": "Yi Wang, Yi Li, Momiao Xiong, Li Jin", "title": "Random Bits Regression: a Strong General Predictor for Big Data", "comments": "20 pages,1 figure, 2 tables, research article", "journal-ref": "Big Data Analytics 2016 1:12", "doi": "10.1186/s41044-016-0010-4", "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To improve accuracy and speed of regressions and classifications, we present\na data-based prediction method, Random Bits Regression (RBR). This method first\ngenerates a large number of random binary intermediate/derived features based\non the original input matrix, and then performs regularized linear/logistic\nregression on those intermediate/derived features to predict the outcome.\nBenchmark analyses on a simulated dataset, UCI machine learning repository\ndatasets and a GWAS dataset showed that RBR outperforms other popular methods\nin accuracy and robustness. RBR (available on\nhttps://sourceforge.net/projects/rbr/) is very fast and requires reasonable\nmemories, therefore, provides a strong, robust and fast predictor in the big\ndata era.\n", "versions": [{"version": "v1", "created": "Tue, 13 Jan 2015 13:14:42 GMT"}], "update_date": "2016-11-04", "authors_parsed": [["Wang", "Yi", ""], ["Li", "Yi", ""], ["Xiong", "Momiao", ""], ["Jin", "Li", ""]]}, {"id": "1501.03001", "submitter": "Emilie Morvant", "authors": "Francois Laviolette, Emilie Morvant (LHC), Liva Ralaivola,\n  Jean-Francis Roy", "title": "On Generalizing the C-Bound to the Multiclass and Multi-label Settings", "comments": "NIPS 2014 Workshop on Representation and Learning Methods for Complex\n  Outputs, Dec 2014, Montr{\\'e}al, Canada", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The C-bound, introduced in Lacasse et al., gives a tight upper bound on the\nrisk of a binary majority vote classifier. In this work, we present a first\nstep towards extending this work to more complex outputs, by providing\ngeneralizations of the C-bound to the multiclass and multi-label settings.\n", "versions": [{"version": "v1", "created": "Tue, 13 Jan 2015 13:47:03 GMT"}], "update_date": "2015-01-14", "authors_parsed": [["Laviolette", "Francois", "", "LHC"], ["Morvant", "Emilie", "", "LHC"], ["Ralaivola", "Liva", ""], ["Roy", "Jean-Francis", ""]]}, {"id": "1501.03002", "submitter": "Emilie Morvant", "authors": "Pascal Germain, Amaury Habrard (LHC), Francois Laviolette, Emilie\n  Morvant (LHC)", "title": "An Improvement to the Domain Adaptation Bound in a PAC-Bayesian context", "comments": "NIPS 2014 Workshop on Transfer and Multi-task learning: Theory Meets\n  Practice, Dec 2014, Montr{\\'e}al, Canada", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper provides a theoretical analysis of domain adaptation based on the\nPAC-Bayesian theory. We propose an improvement of the previous domain\nadaptation bound obtained by Germain et al. in two ways. We first give another\ngeneralization bound tighter and easier to interpret. Moreover, we provide a\nnew analysis of the constant term appearing in the bound that can be of high\ninterest for developing new algorithmic solutions.\n", "versions": [{"version": "v1", "created": "Tue, 13 Jan 2015 13:50:58 GMT"}], "update_date": "2015-01-14", "authors_parsed": [["Germain", "Pascal", "", "LHC"], ["Habrard", "Amaury", "", "LHC"], ["Laviolette", "Francois", "", "LHC"], ["Morvant", "Emilie", "", "LHC"]]}, {"id": "1501.03015", "submitter": "Albrecht Zimmermann", "authors": "Albrecht Zimmermann, Bj\\\"orn Bringmann, Luc De Raedt", "title": "Exploring the efficacy of molecular fragments of different complexity in\n  computational SAR modeling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CE cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An important first step in computational SAR modeling is to transform the\ncompounds into a representation that can be processed by predictive modeling\ntechniques. This is typically a feature vector where each feature indicates the\npresence or absence of a molecular fragment. While the traditional approach to\nSAR modeling employed size restricted fingerprints derived from path fragments,\nmuch research in recent years focussed on mining more complex graph based\nfragments. Today, there seems to be a growing consensus in the data mining\ncommunity that these more expressive fragments should be more useful. We\nquestion this consensus and show experimentally that fragments of low\ncomplexity, i.e. sequences, perform better than equally large sets of more\ncomplex ones, an effect we explain by pairwise correlation among fragments and\nthe ability of a fragment set to encode compounds from different classes\ndistinctly. The size restriction on these sets is based on ordering the\nfragments by class-correlation scores. In addition, we also evaluate the\neffects of using a significance value instead of a length restriction for path\nfragments and find a significant reduction in the number of features with\nlittle loss in performance.\n", "versions": [{"version": "v1", "created": "Tue, 13 Jan 2015 14:24:58 GMT"}], "update_date": "2015-01-14", "authors_parsed": [["Zimmermann", "Albrecht", ""], ["Bringmann", "Bj\u00f6rn", ""], ["De Raedt", "Luc", ""]]}, {"id": "1501.03084", "submitter": "Gang Chen", "authors": "Gang Chen", "title": "Deep Learning with Nonparametric Clustering", "comments": "14 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  Clustering is an essential problem in machine learning and data mining. One\nvital factor that impacts clustering performance is how to learn or design the\ndata representation (or features). Fortunately, recent advances in deep\nlearning can learn unsupervised features effectively, and have yielded state of\nthe art performance in many classification problems, such as character\nrecognition, object recognition and document categorization. However, little\nattention has been paid to the potential of deep learning for unsupervised\nclustering problems. In this paper, we propose a deep belief network with\nnonparametric clustering. As an unsupervised method, our model first leverages\nthe advantages of deep learning for feature representation and dimension\nreduction. Then, it performs nonparametric clustering under a maximum margin\nframework -- a discriminative clustering model and can be trained online\nefficiently in the code space. Lastly model parameters are refined in the deep\nbelief network. Thus, this model can learn features for clustering and infer\nmodel complexity in an unified framework. The experimental results show the\nadvantage of our approach over competitive baselines.\n", "versions": [{"version": "v1", "created": "Tue, 13 Jan 2015 17:26:26 GMT"}], "update_date": "2015-01-14", "authors_parsed": [["Chen", "Gang", ""]]}, {"id": "1501.03227", "submitter": "Sylvain Chevallier", "authors": "Emmanuel K. Kalunga, Sylvain Chevallier, Quentin Barthelemy", "title": "Using Riemannian geometry for SSVEP-based Brain Computer Interface", "comments": "29 pages, 6 figures, 1 table, research report. Update on the overall\n  text, most of the figure are modified, the algorithm is explained more\n  clearly, updated comparisons with state of the art methods", "journal-ref": null, "doi": "10.1016/j.neucom.2016.01.007", "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Riemannian geometry has been applied to Brain Computer Interface (BCI) for\nbrain signals classification yielding promising results. Studying\nelectroencephalographic (EEG) signals from their associated covariance matrices\nallows a mitigation of common sources of variability (electronic, electrical,\nbiological) by constructing a representation which is invariant to these\nperturbations. While working in Euclidean space with covariance matrices is\nknown to be error-prone, one might take advantage of algorithmic advances in\ninformation geometry and matrix manifold to implement methods for Symmetric\nPositive-Definite (SPD) matrices. This paper proposes a comprehensive review of\nthe actual tools of information geometry and how they could be applied on\ncovariance matrices of EEG. In practice, covariance matrices should be\nestimated, thus a thorough study of all estimators is conducted on real EEG\ndataset. As a main contribution, this paper proposes an online implementation\nof a classifier in the Riemannian space and its subsequent assessment in\nSteady-State Visually Evoked Potential (SSVEP) experimentations.\n", "versions": [{"version": "v1", "created": "Wed, 14 Jan 2015 01:12:00 GMT"}, {"version": "v2", "created": "Wed, 21 Jan 2015 14:11:47 GMT"}, {"version": "v3", "created": "Tue, 24 Feb 2015 11:02:10 GMT"}], "update_date": "2021-02-11", "authors_parsed": [["Kalunga", "Emmanuel K.", ""], ["Chevallier", "Sylvain", ""], ["Barthelemy", "Quentin", ""]]}, {"id": "1501.03273", "submitter": "Roi Livni", "authors": "Elad Hazan and Roi Livni and Yishay Mansour", "title": "Classification with Low Rank and Missing Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider classification and regression tasks where we have missing data\nand assume that the (clean) data resides in a low rank subspace. Finding a\nhidden subspace is known to be computationally hard. Nevertheless, using a\nnon-proper formulation we give an efficient agnostic algorithm that classifies\nas good as the best linear classifier coupled with the best low-dimensional\nsubspace in which the data resides. A direct implication is that our algorithm\ncan linearly (and non-linearly through kernels) classify provably as well as\nthe best classifier that has access to the full data.\n", "versions": [{"version": "v1", "created": "Wed, 14 Jan 2015 08:16:50 GMT"}], "update_date": "2015-01-15", "authors_parsed": [["Hazan", "Elad", ""], ["Livni", "Roi", ""], ["Mansour", "Yishay", ""]]}, {"id": "1501.03302", "submitter": "Mateusz Malinowski", "authors": "Mateusz Malinowski and Mario Fritz", "title": "Hard to Cheat: A Turing Test based on Answering Questions about Images", "comments": "Presented in AAAI-15 Workshop: Beyond the Turing Test", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CL cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Progress in language and image understanding by machines has sparkled the\ninterest of the research community in more open-ended, holistic tasks, and\nrefueled an old AI dream of building intelligent machines. We discuss a few\nprominent challenges that characterize such holistic tasks and argue for\n\"question answering about images\" as a particular appealing instance of such a\nholistic task. In particular, we point out that it is a version of a Turing\nTest that is likely to be more robust to over-interpretations and contrast it\nwith tasks like grounding and generation of descriptions. Finally, we discuss\ntools to measure progress in this field.\n", "versions": [{"version": "v1", "created": "Wed, 14 Jan 2015 10:38:43 GMT"}, {"version": "v2", "created": "Thu, 15 Jan 2015 10:18:54 GMT"}], "update_date": "2015-01-16", "authors_parsed": [["Malinowski", "Mateusz", ""], ["Fritz", "Mario", ""]]}, {"id": "1501.03326", "submitter": "Heiko Strathmann", "authors": "Heiko Strathmann, Dino Sejdinovic, Mark Girolami", "title": "Unbiased Bayes for Big Data: Paths of Partial Posteriors", "comments": "18 pages, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A key quantity of interest in Bayesian inference are expectations of\nfunctions with respect to a posterior distribution. Markov Chain Monte Carlo is\na fundamental tool to consistently compute these expectations via averaging\nsamples drawn from an approximate posterior. However, its feasibility is being\nchallenged in the era of so called Big Data as all data needs to be processed\nin every iteration. Realising that such simulation is an unnecessarily hard\nproblem if the goal is estimation, we construct a computationally scalable\nmethodology that allows unbiased estimation of the required expectations --\nwithout explicit simulation from the full posterior. The scheme's variance is\nfinite by construction and straightforward to control, leading to algorithms\nthat are provably unbiased and naturally arrive at a desired error tolerance.\nThis is achieved at an average computational complexity that is sub-linear in\nthe size of the dataset and its free parameters are easy to tune. We\ndemonstrate the utility and generality of the methodology on a range of common\nstatistical models applied to large-scale benchmark and real-world datasets.\n", "versions": [{"version": "v1", "created": "Wed, 14 Jan 2015 12:15:14 GMT"}, {"version": "v2", "created": "Mon, 9 Feb 2015 16:21:20 GMT"}], "update_date": "2015-02-10", "authors_parsed": [["Strathmann", "Heiko", ""], ["Sejdinovic", "Dino", ""], ["Girolami", "Mark", ""]]}, {"id": "1501.03347", "submitter": "Faicel Chamroukhi", "authors": "Faicel Chamroukhi, Marius Bartcus, Herv\\'e Glotin", "title": "Dirichlet Process Parsimonious Mixtures for clustering", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The parsimonious Gaussian mixture models, which exploit an eigenvalue\ndecomposition of the group covariance matrices of the Gaussian mixture, have\nshown their success in particular in cluster analysis. Their estimation is in\ngeneral performed by maximum likelihood estimation and has also been considered\nfrom a parametric Bayesian prospective. We propose new Dirichlet Process\nParsimonious mixtures (DPPM) which represent a Bayesian nonparametric\nformulation of these parsimonious Gaussian mixture models. The proposed DPPM\nmodels are Bayesian nonparametric parsimonious mixture models that allow to\nsimultaneously infer the model parameters, the optimal number of mixture\ncomponents and the optimal parsimonious mixture structure from the data. We\ndevelop a Gibbs sampling technique for maximum a posteriori (MAP) estimation of\nthe developed DPMM models and provide a Bayesian model selection framework by\nusing Bayes factors. We apply them to cluster simulated data and real data\nsets, and compare them to the standard parsimonious mixture models. The\nobtained results highlight the effectiveness of the proposed nonparametric\nparsimonious mixture models as a good nonparametric alternative for the\nparametric parsimonious models.\n", "versions": [{"version": "v1", "created": "Wed, 14 Jan 2015 13:56:35 GMT"}, {"version": "v2", "created": "Wed, 17 Oct 2018 11:54:17 GMT"}], "update_date": "2018-10-18", "authors_parsed": [["Chamroukhi", "Faicel", ""], ["Bartcus", "Marius", ""], ["Glotin", "Herv\u00e9", ""]]}, {"id": "1501.03669", "submitter": "Giovanni Chierchia", "authors": "G. Chierchia, Nelly Pustelnik, Jean-Christophe Pesquet, B.\n  Pesquet-Popescu", "title": "A Proximal Approach for Sparse Multiclass SVM", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sparsity-inducing penalties are useful tools to design multiclass support\nvector machines (SVMs). In this paper, we propose a convex optimization\napproach for efficiently and exactly solving the multiclass SVM learning\nproblem involving a sparse regularization and the multiclass hinge loss\nformulated by Crammer and Singer. We provide two algorithms: the first one\ndealing with the hinge loss as a penalty term, and the other one addressing the\ncase when the hinge loss is enforced through a constraint. The related convex\noptimization problems can be efficiently solved thanks to the flexibility\noffered by recent primal-dual proximal algorithms and epigraphical splitting\ntechniques. Experiments carried out on several datasets demonstrate the\ninterest of considering the exact expression of the hinge loss rather than a\nsmooth approximation. The efficiency of the proposed algorithms w.r.t. several\nstate-of-the-art methods is also assessed through comparisons of execution\ntimes.\n", "versions": [{"version": "v1", "created": "Thu, 15 Jan 2015 13:23:14 GMT"}, {"version": "v2", "created": "Fri, 16 Jan 2015 09:26:32 GMT"}, {"version": "v3", "created": "Fri, 6 Feb 2015 23:36:03 GMT"}, {"version": "v4", "created": "Sun, 26 Apr 2015 15:33:36 GMT"}, {"version": "v5", "created": "Mon, 14 Dec 2015 09:49:32 GMT"}], "update_date": "2015-12-15", "authors_parsed": [["Chierchia", "G.", ""], ["Pustelnik", "Nelly", ""], ["Pesquet", "Jean-Christophe", ""], ["Pesquet-Popescu", "B.", ""]]}, {"id": "1501.03786", "submitter": "Jim Jing-Yan Wang", "authors": "Jim Jing-Yan Wang", "title": "Multi-view learning for multivariate performance measures optimization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose the problem of optimizing multivariate performance\nmeasures from multi-view data, and an effective method to solve it. This\nproblem has two features: the data points are presented by multiple views, and\nthe target of learning is to optimize complex multivariate performance\nmeasures. We propose to learn a linear discriminant functions for each view,\nand combine them to construct a overall multivariate mapping function for\nmult-view data. To learn the parameters of the linear dis- criminant functions\nof different views to optimize multivariate performance measures, we formulate\na optimization problem. In this problem, we propose to minimize the complexity\nof the linear discriminant functions of each view, encourage the consistences\nof the responses of different views over the same data points, and minimize the\nupper boundary of a given multivariate performance measure. To optimize this\nproblem, we employ the cutting-plane method in an iterative algorithm. In each\niteration, we update a set of constrains, and optimize the mapping function\nparameter of each view one by one.\n", "versions": [{"version": "v1", "created": "Thu, 15 Jan 2015 19:33:34 GMT"}, {"version": "v2", "created": "Fri, 16 Jan 2015 11:10:13 GMT"}], "update_date": "2015-01-19", "authors_parsed": [["Wang", "Jim Jing-Yan", ""]]}, {"id": "1501.03796", "submitter": "Akshay Balsubramani", "authors": "Akshay Balsubramani, Sanjoy Dasgupta, Yoav Freund", "title": "The Fast Convergence of Incremental PCA", "comments": "NIPS 2013", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider a situation in which we see samples in $\\mathbb{R}^d$ drawn\ni.i.d. from some distribution with mean zero and unknown covariance A. We wish\nto compute the top eigenvector of A in an incremental fashion - with an\nalgorithm that maintains an estimate of the top eigenvector in O(d) space, and\nincrementally adjusts the estimate with each new data point that arrives. Two\nclassical such schemes are due to Krasulina (1969) and Oja (1983). We give\nfinite-sample convergence rates for both.\n", "versions": [{"version": "v1", "created": "Thu, 15 Jan 2015 20:08:49 GMT"}], "update_date": "2015-01-16", "authors_parsed": [["Balsubramani", "Akshay", ""], ["Dasgupta", "Sanjoy", ""], ["Freund", "Yoav", ""]]}, {"id": "1501.03838", "submitter": "Akshay Balsubramani", "authors": "Akshay Balsubramani, Yoav Freund", "title": "PAC-Bayes with Minimax for Confidence-Rated Transduction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider using an ensemble of binary classifiers for transductive\nprediction, when unlabeled test data are known in advance. We derive minimax\noptimal rules for confidence-rated prediction in this setting. By using\nPAC-Bayes analysis on these rules, we obtain data-dependent performance\nguarantees without distributional assumptions on the data. Our analysis\ntechniques are readily extended to a setting in which the predictor is allowed\nto abstain.\n", "versions": [{"version": "v1", "created": "Thu, 15 Jan 2015 21:59:39 GMT"}], "update_date": "2015-01-19", "authors_parsed": [["Balsubramani", "Akshay", ""], ["Freund", "Yoav", ""]]}, {"id": "1501.03854", "submitter": "Kevin Vu", "authors": "Kevin Vu, John Snyder, Li Li, Matthias Rupp, Brandon F. Chen, Tarek\n  Khelif, Klaus-Robert M\\\"uller, Kieron Burke", "title": "Understanding Kernel Ridge Regression: Common behaviors from simple\n  functions to density functionals", "comments": "15 pages, 20 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.comp-ph cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Accurate approximations to density functionals have recently been obtained\nvia machine learning (ML). By applying ML to a simple function of one variable\nwithout any random sampling, we extract the qualitative dependence of errors on\nhyperparameters. We find universal features of the behavior in extreme limits,\nincluding both very small and very large length scales, and the noise-free\nlimit. We show how such features arise in ML models of density functionals.\n", "versions": [{"version": "v1", "created": "Fri, 16 Jan 2015 00:00:46 GMT"}, {"version": "v2", "created": "Wed, 28 Jan 2015 12:02:50 GMT"}], "update_date": "2015-01-29", "authors_parsed": [["Vu", "Kevin", ""], ["Snyder", "John", ""], ["Li", "Li", ""], ["Rupp", "Matthias", ""], ["Chen", "Brandon F.", ""], ["Khelif", "Tarek", ""], ["M\u00fcller", "Klaus-Robert", ""], ["Burke", "Kieron", ""]]}, {"id": "1501.03915", "submitter": "Massimo Brescia Dr", "authors": "Sabina Tangaro, Nicola Amoroso, Massimo Brescia, Stefano Cavuoti,\n  Andrea Chincarini, Rosangela Errico, Paolo Inglese, Giuseppe Longo, Rosalia\n  Maglietta, Andrea Tateo, Giuseppe Riccio, Roberto Bellotti", "title": "Feature Selection based on Machine Learning in MRIs for Hippocampal\n  Segmentation", "comments": "To appear on \"Computational and Mathematical Methods in Medicine\",\n  Hindawi Publishing Corporation. 19 pages, 7 figures", "journal-ref": "Computational and Mathematical Methods in Medicine Volume 2015,\n  Article ID 814104, 10 pages, Hindawi Publishing Corporation", "doi": "10.1155/2015/814104", "report-no": null, "categories": "physics.med-ph cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neurodegenerative diseases are frequently associated with structural changes\nin the brain. Magnetic Resonance Imaging (MRI) scans can show these variations\nand therefore be used as a supportive feature for a number of neurodegenerative\ndiseases. The hippocampus has been known to be a biomarker for Alzheimer\ndisease and other neurological and psychiatric diseases. However, it requires\naccurate, robust and reproducible delineation of hippocampal structures. Fully\nautomatic methods are usually the voxel based approach, for each voxel a number\nof local features were calculated. In this paper we compared four different\ntechniques for feature selection from a set of 315 features extracted for each\nvoxel: (i) filter method based on the Kolmogorov-Smirnov test; two wrapper\nmethods, respectively, (ii) Sequential Forward Selection and (iii) Sequential\nBackward Elimination; and (iv) embedded method based on the Random Forest\nClassifier on a set of 10 T1-weighted brain MRIs and tested on an independent\nset of 25 subjects. The resulting segmentations were compared with manual\nreference labelling. By using only 23 features for each voxel (sequential\nbackward elimination) we obtained comparable state of-the-art performances with\nrespect to the standard tool FreeSurfer.\n", "versions": [{"version": "v1", "created": "Fri, 16 Jan 2015 08:45:55 GMT"}], "update_date": "2015-07-27", "authors_parsed": [["Tangaro", "Sabina", ""], ["Amoroso", "Nicola", ""], ["Brescia", "Massimo", ""], ["Cavuoti", "Stefano", ""], ["Chincarini", "Andrea", ""], ["Errico", "Rosangela", ""], ["Inglese", "Paolo", ""], ["Longo", "Giuseppe", ""], ["Maglietta", "Rosalia", ""], ["Tateo", "Andrea", ""], ["Riccio", "Giuseppe", ""], ["Bellotti", "Roberto", ""]]}, {"id": "1501.03959", "submitter": "Kamil Ciosek", "authors": "Kamil Ciosek and David Silver", "title": "Value Iteration with Options and State Aggregation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a way of solving Markov Decision Processes that combines\nstate abstraction and temporal abstraction. Specifically, we combine state\naggregation with the options framework and demonstrate that they work well\ntogether and indeed it is only after one combines the two that the full benefit\nof each is realized. We introduce a hierarchical value iteration algorithm\nwhere we first coarsely solve subgoals and then use these approximate solutions\nto exactly solve the MDP. This algorithm solved several problems faster than\nvanilla value iteration.\n", "versions": [{"version": "v1", "created": "Fri, 16 Jan 2015 12:02:51 GMT"}], "update_date": "2015-01-19", "authors_parsed": [["Ciosek", "Kamil", ""], ["Silver", "David", ""]]}, {"id": "1501.03975", "submitter": "Vijay Manikandan Janakiraman", "authors": "Vijay Manikandan Janakiraman and XuanLong Nguyen and Dennis Assanis", "title": "Stochastic Gradient Based Extreme Learning Machines For Online Learning\n  of Advanced Combustion Engines", "comments": "This paper was written as an extract from my PhD thesis (July 2013)\n  and so references may not be to date as of this submission (Jan 2015). The\n  article is in review and contains 10 figures, 35 references", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.LG cs.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this article, a stochastic gradient based online learning algorithm for\nExtreme Learning Machines (ELM) is developed (SG-ELM). A stability criterion\nbased on Lyapunov approach is used to prove both asymptotic stability of\nestimation error and stability in the estimated parameters suitable for\nidentification of nonlinear dynamic systems. The developed algorithm not only\nguarantees stability, but also reduces the computational demand compared to the\nOS-ELM approach based on recursive least squares. In order to demonstrate the\neffectiveness of the algorithm on a real-world scenario, an advanced combustion\nengine identification problem is considered. The algorithm is applied to two\ncase studies: An online regression learning for system identification of a\nHomogeneous Charge Compression Ignition (HCCI) Engine and an online\nclassification learning (with class imbalance) for identifying the dynamic\noperating envelope of the HCCI Engine. The results indicate that the accuracy\nof the proposed SG-ELM is comparable to that of the state-of-the-art but adds\nstability and a reduction in computational effort.\n", "versions": [{"version": "v1", "created": "Fri, 16 Jan 2015 13:18:34 GMT"}], "update_date": "2015-01-19", "authors_parsed": [["Janakiraman", "Vijay Manikandan", ""], ["Nguyen", "XuanLong", ""], ["Assanis", "Dennis", ""]]}, {"id": "1501.04053", "submitter": "Dionissios Hristopulos Prof.", "authors": "Dionissios T. Hristopulos", "title": "Stochastic Local Interaction (SLI) Model: Interfacing Machine Learning\n  and Geostatistics", "comments": "31 pages, 7 figures", "journal-ref": null, "doi": "10.1016/j.cageo.2015.05.018", "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Machine learning and geostatistics are powerful mathematical frameworks for\nmodeling spatial data. Both approaches, however, suffer from poor scaling of\nthe required computational resources for large data applications. We present\nthe Stochastic Local Interaction (SLI) model, which employs a local\nrepresentation to improve computational efficiency. SLI combines geostatistics\nand machine learning with ideas from statistical physics and computational\ngeometry. It is based on a joint probability density function defined by an\nenergy functional which involves local interactions implemented by means of\nkernel functions with adaptive local kernel bandwidths. SLI is expressed in\nterms of an explicit, typically sparse, precision (inverse covariance) matrix.\nThis representation leads to a semi-analytical expression for interpolation\n(prediction), which is valid in any number of dimensions and avoids the\ncomputationally costly covariance matrix inversion.\n", "versions": [{"version": "v1", "created": "Fri, 16 Jan 2015 17:09:01 GMT"}, {"version": "v2", "created": "Tue, 5 May 2015 13:03:58 GMT"}], "update_date": "2015-07-15", "authors_parsed": [["Hristopulos", "Dionissios T.", ""]]}, {"id": "1501.04244", "submitter": "Miron Kursa", "authors": "Miron B. Kursa", "title": "Generalised Random Forest Space Overview", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Assuming a view of the Random Forest as a special case of a nested ensemble\nof interchangeable modules, we construct a generalisation space allowing one to\neasily develop novel methods based on this algorithm. We discuss the role and\nrequired properties of modules at each level, especially in context of some\nalready proposed RF generalisations.\n", "versions": [{"version": "v1", "created": "Sat, 17 Jan 2015 23:42:08 GMT"}], "update_date": "2015-01-20", "authors_parsed": [["Kursa", "Miron B.", ""]]}, {"id": "1501.04267", "submitter": "Shuliang Wang", "authors": "Shuliang Wang, Dakui Wang, Caoyuan Li, Yan Li", "title": "Comment on \"Clustering by fast search and find of density peaks\"", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In [1], a clustering algorithm was given to find the centers of clusters\nquickly. However, the accuracy of this algorithm heavily depend on the\nthreshold value of d-c. Furthermore, [1] has not provided any efficient way to\nselect the threshold value of d-c, that is, one can have to estimate the value\nof d_c depend on one's subjective experience. In this paper, based on the data\nfield [2], we propose a new way to automatically extract the threshold value of\nd_c from the original data set by using the potential entropy of data field.\nFor any data set to be clustered, the most reasonable value of d_c can be\nobjectively calculated from the data set by using our proposed method. The same\nexperiments in [1] are redone with our proposed method on the same experimental\ndata set used in [1], the results of which shows that the problem to calculate\nthe threshold value of d_c in [1] has been solved by using our method.\n", "versions": [{"version": "v1", "created": "Sun, 18 Jan 2015 05:15:55 GMT"}, {"version": "v2", "created": "Tue, 20 Jan 2015 03:41:28 GMT"}], "update_date": "2015-01-21", "authors_parsed": [["Wang", "Shuliang", ""], ["Wang", "Dakui", ""], ["Li", "Caoyuan", ""], ["Li", "Yan", ""]]}, {"id": "1501.04282", "submitter": "Jim Jing-Yan Wang", "authors": "Jim Jing-Yan Wang, Yunji Wang, Bing-Yi Jing, Xin Gao", "title": "Regularized maximum correntropy machine", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we investigate the usage of regularized correntropy framework\nfor learning of classifiers from noisy labels. The class label predictors\nlearned by minimizing transitional loss functions are sensitive to the noisy\nand outlying labels of training samples, because the transitional loss\nfunctions are equally applied to all the samples. To solve this problem, we\npropose to learn the class label predictors by maximizing the correntropy\nbetween the predicted labels and the true labels of the training samples, under\nthe regularized Maximum Correntropy Criteria (MCC) framework. Moreover, we\nregularize the predictor parameter to control the complexity of the predictor.\nThe learning problem is formulated by an objective function considering the\nparameter regularization and MCC simultaneously. By optimizing the objective\nfunction alternately, we develop a novel predictor learning algorithm. The\nexperiments on two chal- lenging pattern classification tasks show that it\nsignificantly outperforms the machines with transitional loss functions.\n", "versions": [{"version": "v1", "created": "Sun, 18 Jan 2015 11:46:30 GMT"}], "update_date": "2015-01-20", "authors_parsed": [["Wang", "Jim Jing-Yan", ""], ["Wang", "Yunji", ""], ["Jing", "Bing-Yi", ""], ["Gao", "Xin", ""]]}, {"id": "1501.04284", "submitter": "Zhiwu Lu", "authors": "Zhiwu Lu and Liwei Wang", "title": "Pairwise Constraint Propagation on Multi-View Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a graph-based learning approach to pairwise constraint\npropagation on multi-view data. Although pairwise constraint propagation has\nbeen studied extensively, pairwise constraints are usually defined over pairs\nof data points from a single view, i.e., only intra-view constraint propagation\nis considered for multi-view tasks. In fact, very little attention has been\npaid to inter-view constraint propagation, which is more challenging since\npairwise constraints are now defined over pairs of data points from different\nviews. In this paper, we propose to decompose the challenging inter-view\nconstraint propagation problem into semi-supervised learning subproblems so\nthat they can be efficiently solved based on graph-based label propagation. To\nthe best of our knowledge, this is the first attempt to give an efficient\nsolution to inter-view constraint propagation from a semi-supervised learning\nviewpoint. Moreover, since graph-based label propagation has been adopted for\nbasic optimization, we develop two constrained graph construction methods for\ninterview constraint propagation, which only differ in how the intra-view\npairwise constraints are exploited. The experimental results in cross-view\nretrieval have shown the promising performance of our inter-view constraint\npropagation.\n", "versions": [{"version": "v1", "created": "Sun, 18 Jan 2015 11:52:21 GMT"}], "update_date": "2015-01-20", "authors_parsed": [["Lu", "Zhiwu", ""], ["Wang", "Liwei", ""]]}, {"id": "1501.04309", "submitter": "Baogang Hu", "authors": "Bao-Gang Hu", "title": "Information Theory and its Relation to Machine Learning", "comments": "10 pages, 6 figures, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.LG math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this position paper, I first describe a new perspective on machine\nlearning (ML) by four basic problems (or levels), namely, \"What to learn?\",\n\"How to learn?\", \"What to evaluate?\", and \"What to adjust?\". The paper stresses\nmore on the first level of \"What to learn?\", or \"Learning Target Selection\".\nTowards this primary problem within the four levels, I briefly review the\nexisting studies about the connection between information theoretical learning\n(ITL [1]) and machine learning. A theorem is given on the relation between the\nempirically-defined similarity measure and information measures. Finally, a\nconjecture is proposed for pursuing a unified mathematical interpretation to\nlearning target selection.\n", "versions": [{"version": "v1", "created": "Sun, 18 Jan 2015 14:57:02 GMT"}], "update_date": "2015-01-20", "authors_parsed": [["Hu", "Bao-Gang", ""]]}, {"id": "1501.04318", "submitter": "Teng Qiu", "authors": "Teng Qiu, Yongjie Li", "title": "Clustering based on the In-tree Graph Structure and Affinity Propagation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  A recently proposed clustering method, called the Nearest Descent (ND), can\norganize the whole dataset into a sparsely connected graph, called the In-tree.\nThis ND-based Intree structure proves able to reveal the clustering structure\nunderlying the dataset, except one imperfect place, that is, there are some\nundesired edges in this In-tree which require to be removed. Here, we propose\nan effective way to automatically remove the undesired edges in In-tree via an\neffective combination of the In-tree structure with affinity propagation (AP).\nThe key for the combination is to add edges between the reachable nodes in\nIn-tree before using AP to remove the undesired edges. The experiments on both\nsynthetic and real datasets demonstrate the effectiveness of the proposed\nmethod.\n", "versions": [{"version": "v1", "created": "Sun, 18 Jan 2015 15:34:19 GMT"}, {"version": "v2", "created": "Mon, 29 Jan 2018 00:34:26 GMT"}], "update_date": "2018-01-30", "authors_parsed": [["Qiu", "Teng", ""], ["Li", "Yongjie", ""]]}, {"id": "1501.04325", "submitter": "Lars Maaloe", "authors": "Lars Maaloe and Morten Arngren and Ole Winther", "title": "Deep Belief Nets for Topic Modeling", "comments": "Accepted to the ICML-2014 Workshop on Knowledge-Powered Deep Learning\n  for Text Mining", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Applying traditional collaborative filtering to digital publishing is\nchallenging because user data is very sparse due to the high volume of\ndocuments relative to the number of users. Content based approaches, on the\nother hand, is attractive because textual content is often very informative. In\nthis paper we describe large-scale content based collaborative filtering for\ndigital publishing. To solve the digital publishing recommender problem we\ncompare two approaches: latent Dirichlet allocation (LDA) and deep belief nets\n(DBN) that both find low-dimensional latent representations for documents.\nEfficient retrieval can be carried out in the latent representation. We work\nboth on public benchmarks and digital media content provided by Issuu, an\nonline publishing platform. This article also comes with a newly developed deep\nbelief nets toolbox for topic modeling tailored towards performance evaluation\nof the DBN model and comparisons to the LDA model.\n", "versions": [{"version": "v1", "created": "Sun, 18 Jan 2015 17:12:59 GMT"}], "update_date": "2015-01-20", "authors_parsed": [["Maaloe", "Lars", ""], ["Arngren", "Morten", ""], ["Winther", "Ole", ""]]}, {"id": "1501.04346", "submitter": "Divyanshu Vats", "authors": "Andrew S. Lan and Divyanshu Vats and Andrew E. Waters and Richard G.\n  Baraniuk", "title": "Mathematical Language Processing: Automatic Grading and Feedback for\n  Open Response Mathematical Questions", "comments": "ACM Conference on Learning at Scale, March 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While computer and communication technologies have provided effective means\nto scale up many aspects of education, the submission and grading of\nassessments such as homework assignments and tests remains a weak link. In this\npaper, we study the problem of automatically grading the kinds of open response\nmathematical questions that figure prominently in STEM (science, technology,\nengineering, and mathematics) courses. Our data-driven framework for\nmathematical language processing (MLP) leverages solution data from a large\nnumber of learners to evaluate the correctness of their solutions, assign\npartial-credit scores, and provide feedback to each learner on the likely\nlocations of any errors. MLP takes inspiration from the success of natural\nlanguage processing for text data and comprises three main steps. First, we\nconvert each solution to an open response mathematical question into a series\nof numerical features. Second, we cluster the features from several solutions\nto uncover the structures of correct, partially correct, and incorrect\nsolutions. We develop two different clustering approaches, one that leverages\ngeneric clustering algorithms and one based on Bayesian nonparametrics. Third,\nwe automatically grade the remaining (potentially large number of) solutions\nbased on their assigned cluster and one instructor-provided grade per cluster.\nAs a bonus, we can track the cluster assignment of each step of a multistep\nsolution and determine when it departs from a cluster of correct solutions,\nwhich enables us to indicate the likely locations of errors to learners. We\ntest and validate MLP on real-world MOOC data to demonstrate how it can\nsubstantially reduce the human effort required in large-scale educational\nplatforms.\n", "versions": [{"version": "v1", "created": "Sun, 18 Jan 2015 20:50:39 GMT"}], "update_date": "2015-01-20", "authors_parsed": [["Lan", "Andrew S.", ""], ["Vats", "Divyanshu", ""], ["Waters", "Andrew E.", ""], ["Baraniuk", "Richard G.", ""]]}, {"id": "1501.04370", "submitter": "Ru He", "authors": "Ru He, Jin Tian, Huaiqing Wu", "title": "Structure Learning in Bayesian Networks of Moderate Size by Efficient\n  Sampling", "comments": "51 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the Bayesian model averaging approach to learning Bayesian network\nstructures (DAGs) from data. We develop new algorithms including the first\nalgorithm that is able to efficiently sample DAGs according to the exact\nstructure posterior. The DAG samples can then be used to construct estimators\nfor the posterior of any feature. We theoretically prove good properties of our\nestimators and empirically show that our estimators considerably outperform the\nestimators from the previous state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Mon, 19 Jan 2015 01:32:43 GMT"}], "update_date": "2015-01-20", "authors_parsed": [["He", "Ru", ""], ["Tian", "Jin", ""], ["Wu", "Huaiqing", ""]]}, {"id": "1501.04413", "submitter": "Masayuki Ohzeki", "authors": "Masayuki Ohzeki", "title": "Statistical-mechanical analysis of pre-training and fine tuning in deep\n  learning", "comments": "13 pages and 2 figures, to appear in JPSJ", "journal-ref": null, "doi": "10.7566/JPSJ.84.034003", "report-no": null, "categories": "stat.ML cond-mat.dis-nn cond-mat.stat-mech cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present a statistical-mechanical analysis of deep learning.\nWe elucidate some of the essential components of deep learning---pre-training\nby unsupervised learning and fine tuning by supervised learning. We formulate\nthe extraction of features from the training data as a margin criterion in a\nhigh-dimensional feature-vector space. The self-organized classifier is then\nsupplied with small amounts of labelled data, as in deep learning. Although we\nemploy a simple single-layer perceptron model, rather than directly analyzing a\nmulti-layer neural network, we find a nontrivial phase transition that is\ndependent on the number of unlabelled data in the generalization error of the\nresultant classifier. In this sense, we evaluate the efficacy of the\nunsupervised learning component of deep learning. The analysis is performed by\nthe replica method, which is a sophisticated tool in statistical mechanics. We\nvalidate our result in the manner of deep learning, using a simple iterative\nalgorithm to learn the weight vector on the basis of belief propagation.\n", "versions": [{"version": "v1", "created": "Mon, 19 Jan 2015 07:24:21 GMT"}], "update_date": "2015-06-23", "authors_parsed": [["Ohzeki", "Masayuki", ""]]}, {"id": "1501.04621", "submitter": "Rajib Rana", "authors": "Sajib Saha, Frank de Hoog, Ya.I. Nesterets, Rajib Rana, M. Tahtali and\n  T.E. Gureyev", "title": "Sparse Bayesian Learning for EEG Source Localization", "comments": "arXiv admin note: substantial text overlap with arXiv:1406.2434", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.QM cs.LG q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Purpose: Localizing the sources of electrical activity from\nelectroencephalographic (EEG) data has gained considerable attention over the\nlast few years. In this paper, we propose an innovative source localization\nmethod for EEG, based on Sparse Bayesian Learning (SBL). Methods: To better\nspecify the sparsity profile and to ensure efficient source localization, the\nproposed approach considers grouping of the electrical current dipoles inside\nhuman brain. SBL is used to solve the localization problem in addition with\nimposed constraint that the electric current dipoles associated with the brain\nactivity are isotropic. Results: Numerical experiments are conducted on a\nrealistic head model that is obtained by segmentation of MRI images of the head\nand includes four major components, namely the scalp, the skull, the\ncerebrospinal fluid (CSF) and the brain, with appropriate relative conductivity\nvalues. The results demonstrate that the isotropy constraint significantly\nimproves the performance of SBL. In a noiseless environment, the proposed\nmethod was 1 found to accurately (with accuracy of >75%) locate up to 6\nsimultaneously active sources, whereas for SBL without the isotropy constraint,\nthe accuracy of finding just 3 simultaneously active sources was <75%.\nConclusions: Compared to the state-of-the-art algorithms, the proposed method\nis potentially more consistent in specifying the sparsity profile of human\nbrain activity and is able to produce better source localization for EEG.\n", "versions": [{"version": "v1", "created": "Mon, 19 Jan 2015 16:11:03 GMT"}], "update_date": "2015-01-21", "authors_parsed": [["Saha", "Sajib", ""], ["de Hoog", "Frank", ""], ["Nesterets", "Ya. I.", ""], ["Rana", "Rajib", ""], ["Tahtali", "M.", ""], ["Gureyev", "T. E.", ""]]}, {"id": "1501.04656", "submitter": "Marcus A. Brubaker", "authors": "Ali Punjani and Marcus A. Brubaker", "title": "Microscopic Advances with Large-Scale Learning: Stochastic Optimization\n  for Cryo-EM", "comments": "Presented at NIPS 2014 Workshop on Machine Learning in Computational\n  Biology http://mlcb.org", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CV cs.LG q-bio.QM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Determining the 3D structures of biological molecules is a key problem for\nboth biology and medicine. Electron Cryomicroscopy (Cryo-EM) is a promising\ntechnique for structure estimation which relies heavily on computational\nmethods to reconstruct 3D structures from 2D images. This paper introduces the\nchallenging Cryo-EM density estimation problem as a novel application for\nstochastic optimization techniques. Structure discovery is formulated as MAP\nestimation in a probabilistic latent-variable model, resulting in an\noptimization problem to which an array of seven stochastic optimization methods\nare applied. The methods are tested on both real and synthetic data, with some\nmethods recovering reasonable structures in less than one epoch from a random\ninitialization. Complex quasi-Newton methods are found to converge more slowly\nthan simple gradient-based methods, but all stochastic methods are found to\nconverge to similar optima. This method represents a major improvement over\nexisting methods as it is significantly faster and is able to converge from a\nrandom initialization.\n", "versions": [{"version": "v1", "created": "Mon, 19 Jan 2015 22:07:27 GMT"}, {"version": "v2", "created": "Tue, 27 Jan 2015 21:13:28 GMT"}], "update_date": "2015-01-29", "authors_parsed": [["Punjani", "Ali", ""], ["Brubaker", "Marcus A.", ""]]}, {"id": "1501.04717", "submitter": "Yuting Zhang", "authors": "Yuting Zhang, Kui Jia, Yueming Wang, Gang Pan, Tsung-Han Chan, Yi Ma", "title": "Robust Face Recognition by Constrained Part-based Alignment", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Developing a reliable and practical face recognition system is a\nlong-standing goal in computer vision research. Existing literature suggests\nthat pixel-wise face alignment is the key to achieve high-accuracy face\nrecognition. By assuming a human face as piece-wise planar surfaces, where each\nsurface corresponds to a facial part, we develop in this paper a Constrained\nPart-based Alignment (CPA) algorithm for face recognition across pose and/or\nexpression. Our proposed algorithm is based on a trainable CPA model, which\nlearns appearance evidence of individual parts and a tree-structured shape\nconfiguration among different parts. Given a probe face, CPA simultaneously\naligns all its parts by fitting them to the appearance evidence with\nconsideration of the constraint from the tree-structured shape configuration.\nThis objective is formulated as a norm minimization problem regularized by\ngraph likelihoods. CPA can be easily integrated with many existing classifiers\nto perform part-based face recognition. Extensive experiments on benchmark face\ndatasets show that CPA outperforms or is on par with existing methods for\nrobust face recognition across pose, expression, and/or illumination changes.\n", "versions": [{"version": "v1", "created": "Tue, 20 Jan 2015 06:05:01 GMT"}], "update_date": "2015-01-21", "authors_parsed": [["Zhang", "Yuting", ""], ["Jia", "Kui", ""], ["Wang", "Yueming", ""], ["Pan", "Gang", ""], ["Chan", "Tsung-Han", ""], ["Ma", "Yi", ""]]}, {"id": "1501.04725", "submitter": "Siddharth Krishna", "authors": "Siddharth Krishna, Christian Puhrsch, Thomas Wies", "title": "Learning Invariants using Decision Trees", "comments": "15 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The problem of inferring an inductive invariant for verifying program safety\ncan be formulated in terms of binary classification. This is a standard problem\nin machine learning: given a sample of good and bad points, one is asked to\nfind a classifier that generalizes from the sample and separates the two sets.\nHere, the good points are the reachable states of the program, and the bad\npoints are those that reach a safety property violation. Thus, a learned\nclassifier is a candidate invariant. In this paper, we propose a new algorithm\nthat uses decision trees to learn candidate invariants in the form of arbitrary\nBoolean combinations of numerical inequalities. We have used our algorithm to\nverify C programs taken from the literature. The algorithm is able to infer\nsafe invariants for a range of challenging benchmarks and compares favorably to\nother ML-based invariant inference techniques. In particular, it scales well to\nlarge sample sets.\n", "versions": [{"version": "v1", "created": "Tue, 20 Jan 2015 07:20:30 GMT"}], "update_date": "2015-01-21", "authors_parsed": [["Krishna", "Siddharth", ""], ["Puhrsch", "Christian", ""], ["Wies", "Thomas", ""]]}, {"id": "1501.04826", "submitter": "Thorsten Wissmann", "authors": "Albert Atserias and Jos\\'e L. Balc\\'azar and Marie Ely Piceno", "title": "Relative Entailment Among Probabilistic Implications", "comments": null, "journal-ref": "Logical Methods in Computer Science, Volume 15, Issue 1 (February\n  6, 2019) lmcs:5171", "doi": "10.23638/LMCS-15(1:10)2019", "report-no": null, "categories": "cs.LO cs.DB cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We study a natural variant of the implicational fragment of propositional\nlogic. Its formulas are pairs of conjunctions of positive literals, related\ntogether by an implicational-like connective; the semantics of this sort of\nimplication is defined in terms of a threshold on a conditional probability of\nthe consequent, given the antecedent: we are dealing with what the data\nanalysis community calls confidence of partial implications or association\nrules. Existing studies of redundancy among these partial implications have\ncharacterized so far only entailment from one premise and entailment from two\npremises, both in the stand-alone case and in the case of presence of\nadditional classical implications (this is what we call \"relative entailment\").\nBy exploiting a previously noted alternative view of the entailment in terms of\nlinear programming duality, we characterize exactly the cases of entailment\nfrom arbitrary numbers of premises, again both in the stand-alone case and in\nthe case of presence of additional classical implications. As a result, we\nobtain decision algorithms of better complexity; additionally, for each\npotential case of entailment, we identify a critical confidence threshold and\nshow that it is, actually, intrinsic to each set of premises and antecedent of\nthe conclusion.\n", "versions": [{"version": "v1", "created": "Tue, 20 Jan 2015 14:41:36 GMT"}, {"version": "v2", "created": "Mon, 18 May 2015 12:02:40 GMT"}, {"version": "v3", "created": "Fri, 14 Jul 2017 09:37:28 GMT"}, {"version": "v4", "created": "Mon, 3 Sep 2018 17:02:21 GMT"}, {"version": "v5", "created": "Tue, 5 Feb 2019 11:44:54 GMT"}], "update_date": "2019-04-29", "authors_parsed": [["Atserias", "Albert", ""], ["Balc\u00e1zar", "Jos\u00e9 L.", ""], ["Piceno", "Marie Ely", ""]]}, {"id": "1501.04870", "submitter": "Luca Martino", "authors": "J. Read, L. Martino, P. Olmos, D. Luengo", "title": "Scalable Multi-Output Label Prediction: From Classifier Chains to\n  Classifier Trellises", "comments": "(accepted in Pattern Recognition)", "journal-ref": "Pattern Recognition, Volume 48, Issue 6, 2015, Pages 2096-2109", "doi": "10.1016/j.patcog.2015.01.004", "report-no": null, "categories": "stat.ML cs.CV cs.DS cs.LG stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multi-output inference tasks, such as multi-label classification, have become\nincreasingly important in recent years. A popular method for multi-label\nclassification is classifier chains, in which the predictions of individual\nclassifiers are cascaded along a chain, thus taking into account inter-label\ndependencies and improving the overall performance. Several varieties of\nclassifier chain methods have been introduced, and many of them perform very\ncompetitively across a wide range of benchmark datasets. However, scalability\nlimitations become apparent on larger datasets when modeling a fully-cascaded\nchain. In particular, the methods' strategies for discovering and modeling a\ngood chain structure constitutes a mayor computational bottleneck. In this\npaper, we present the classifier trellis (CT) method for scalable multi-label\nclassification. We compare CT with several recently proposed classifier chain\nmethods to show that it occupies an important niche: it is highly competitive\non standard multi-label problems, yet it can also scale up to thousands or even\ntens of thousands of labels.\n", "versions": [{"version": "v1", "created": "Tue, 20 Jan 2015 16:33:40 GMT"}], "update_date": "2015-06-01", "authors_parsed": [["Read", "J.", ""], ["Martino", "L.", ""], ["Olmos", "P.", ""], ["Luengo", "D.", ""]]}, {"id": "1501.05141", "submitter": "Philippe Giabbanelli", "authors": "Philippe J. Giabbanelli and Joseph G. Peters", "title": "An Algebra to Merge Heterogeneous Classifiers", "comments": "19 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DM cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  In distributed classification, each learner observes its environment and\ndeduces a classifier. As a learner has only a local view of its environment,\nclassifiers can be exchanged among the learners and integrated, or merged, to\nimprove accuracy. However, the operation of merging is not defined for most\nclassifiers. Furthermore, the classifiers that have to be merged may be of\ndifferent types in settings such as ad-hoc networks in which several\ngenerations of sensors may be creating classifiers. We introduce decision\nspaces as a framework for merging possibly different classifiers. We formally\nstudy the merging operation as an algebra, and prove that it satisfies a\ndesirable set of properties. The impact of time is discussed for the two main\ndata mining settings. Firstly, decision spaces can naturally be used with\nnon-stationary distributions, such as the data collected by sensor networks, as\nthe impact of a model decays over time. Secondly, we introduce an approach for\nstationary distributions, such as homogeneous databases partitioned over\ndifferent learners, which ensures that all models have the same impact. We also\npresent a method that uses storage flexibly to achieve different types of decay\nfor non-stationary distributions. Finally, we show that the algebraic approach\ndeveloped for merging can also be used to analyze the behaviour of other\noperators.\n", "versions": [{"version": "v1", "created": "Wed, 21 Jan 2015 11:42:58 GMT"}, {"version": "v2", "created": "Thu, 22 Jan 2015 12:18:56 GMT"}], "update_date": "2015-01-23", "authors_parsed": [["Giabbanelli", "Philippe J.", ""], ["Peters", "Joseph G.", ""]]}, {"id": "1501.05194", "submitter": "Guillaume Marrelec", "authors": "Guillaume Marrelec, Arnaud Mess\\'e, Pierre Bellec", "title": "A Bayesian alternative to mutual information for the hierarchical\n  clustering of dependent random variables", "comments": null, "journal-ref": "G. Marrelec, A. Messe, P. Bellec (2015) A Bayesian alternative to\n  mutual information for the hierarchical clustering of dependent random\n  variables. PLoS ONE 10(9): e0137278", "doi": "10.1371/journal.pone.0137278", "report-no": null, "categories": "stat.ML cs.LG q-bio.QM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The use of mutual information as a similarity measure in agglomerative\nhierarchical clustering (AHC) raises an important issue: some correction needs\nto be applied for the dimensionality of variables. In this work, we formulate\nthe decision of merging dependent multivariate normal variables in an AHC\nprocedure as a Bayesian model comparison. We found that the Bayesian\nformulation naturally shrinks the empirical covariance matrix towards a matrix\nset a priori (e.g., the identity), provides an automated stopping rule, and\ncorrects for dimensionality using a term that scales up the measure as a\nfunction of the dimensionality of the variables. Also, the resulting log Bayes\nfactor is asymptotically proportional to the plug-in estimate of mutual\ninformation, with an additive correction for dimensionality in agreement with\nthe Bayesian information criterion. We investigated the behavior of these\nBayesian alternatives (in exact and asymptotic forms) to mutual information on\nsimulated and real data. An encouraging result was first derived on\nsimulations: the hierarchical clustering based on the log Bayes factor\noutperformed off-the-shelf clustering techniques as well as raw and normalized\nmutual information in terms of classification accuracy. On a toy example, we\nfound that the Bayesian approaches led to results that were similar to those of\nmutual information clustering techniques, with the advantage of an automated\nthresholding. On real functional magnetic resonance imaging (fMRI) datasets\nmeasuring brain activity, it identified clusters consistent with the\nestablished outcome of standard procedures. On this application, normalized\nmutual information had a highly atypical behavior, in the sense that it\nsystematically favored very large clusters. These initial experiments suggest\nthat the proposed Bayesian alternatives to mutual information are a useful new\ntool for hierarchical clustering.\n", "versions": [{"version": "v1", "created": "Wed, 21 Jan 2015 15:22:13 GMT"}, {"version": "v2", "created": "Mon, 19 Oct 2015 11:31:38 GMT"}], "update_date": "2016-08-07", "authors_parsed": [["Marrelec", "Guillaume", ""], ["Mess\u00e9", "Arnaud", ""], ["Bellec", "Pierre", ""]]}, {"id": "1501.05222", "submitter": "Ryan Curtin", "authors": "Ryan R. Curtin, Dongryeol Lee, William B. March, Parikshit Ram", "title": "Plug-and-play dual-tree algorithm runtime analysis", "comments": "Submitted to JMLR", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Numerous machine learning algorithms contain pairwise statistical problems at\ntheir core---that is, tasks that require computations over all pairs of input\npoints if implemented naively. Often, tree structures are used to solve these\nproblems efficiently. Dual-tree algorithms can efficiently solve or approximate\nmany of these problems. Using cover trees, rigorous worst-case runtime\nguarantees have been proven for some of these algorithms. In this paper, we\npresent a problem-independent runtime guarantee for any dual-tree algorithm\nusing the cover tree, separating out the problem-dependent and the\nproblem-independent elements. This allows us to just plug in bounds for the\nproblem-dependent elements to get runtime guarantees for dual-tree algorithms\nfor any pairwise statistical problem without re-deriving the entire proof. We\ndemonstrate this plug-and-play procedure for nearest-neighbor search and\napproximate kernel density estimation to get improved runtime guarantees. Under\nmild assumptions, we also present the first linear runtime guarantee for\ndual-tree based range search.\n", "versions": [{"version": "v1", "created": "Wed, 21 Jan 2015 16:39:43 GMT"}], "update_date": "2015-01-22", "authors_parsed": [["Curtin", "Ryan R.", ""], ["Lee", "Dongryeol", ""], ["March", "William B.", ""], ["Ram", "Parikshit", ""]]}, {"id": "1501.05279", "submitter": "Wojciech Czarnecki", "authors": "Wojciech Marian Czarnecki, Jacek Tabor", "title": "Extreme Entropy Machines: Robust information theoretic classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most of the existing classification methods are aimed at minimization of\nempirical risk (through some simple point-based error measured with loss\nfunction) with added regularization. We propose to approach this problem in a\nmore information theoretic way by investigating applicability of entropy\nmeasures as a classification model objective function. We focus on quadratic\nRenyi's entropy and connected Cauchy-Schwarz Divergence which leads to the\nconstruction of Extreme Entropy Machines (EEM).\n  The main contribution of this paper is proposing a model based on the\ninformation theoretic concepts which on the one hand shows new, entropic\nperspective on known linear classifiers and on the other leads to a\nconstruction of very robust method competetitive with the state of the art\nnon-information theoretic ones (including Support Vector Machines and Extreme\nLearning Machines).\n  Evaluation on numerous problems spanning from small, simple ones from UCI\nrepository to the large (hundreads of thousands of samples) extremely\nunbalanced (up to 100:1 classes' ratios) datasets shows wide applicability of\nthe EEM in real life problems and that it scales well.\n", "versions": [{"version": "v1", "created": "Wed, 21 Jan 2015 19:54:26 GMT"}], "update_date": "2015-01-22", "authors_parsed": [["Czarnecki", "Wojciech Marian", ""], ["Tabor", "Jacek", ""]]}, {"id": "1501.05352", "submitter": "Miguel \\'A. Carreira-Perpi\\~n\\'an", "authors": "Ramin Raziperchikolaei and Miguel \\'A. Carreira-Perpi\\~n\\'an", "title": "Optimizing affinity-based binary hashing using auxiliary coordinates", "comments": "22 pages, 12 figures; added new experiments and references", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In supervised binary hashing, one wants to learn a function that maps a\nhigh-dimensional feature vector to a vector of binary codes, for application to\nfast image retrieval. This typically results in a difficult optimization\nproblem, nonconvex and nonsmooth, because of the discrete variables involved.\nMuch work has simply relaxed the problem during training, solving a continuous\noptimization, and truncating the codes a posteriori. This gives reasonable\nresults but is quite suboptimal. Recent work has tried to optimize the\nobjective directly over the binary codes and achieved better results, but the\nhash function was still learned a posteriori, which remains suboptimal. We\npropose a general framework for learning hash functions using affinity-based\nloss functions that uses auxiliary coordinates. This closes the loop and\noptimizes jointly over the hash functions and the binary codes so that they\ngradually match each other. The resulting algorithm can be seen as a corrected,\niterated version of the procedure of optimizing first over the codes and then\nlearning the hash function. Compared to this, our optimization is guaranteed to\nobtain better hash functions while being not much slower, as demonstrated\nexperimentally in various supervised datasets. In addition, our framework\nfacilitates the design of optimization algorithms for arbitrary types of loss\nand hash functions.\n", "versions": [{"version": "v1", "created": "Wed, 21 Jan 2015 23:53:47 GMT"}, {"version": "v2", "created": "Fri, 5 Feb 2016 01:25:26 GMT"}], "update_date": "2016-02-08", "authors_parsed": [["Raziperchikolaei", "Ramin", ""], ["Carreira-Perpi\u00f1\u00e1n", "Miguel \u00c1.", ""]]}, {"id": "1501.05396", "submitter": "Youssef  Mroueh", "authors": "Youssef Mroueh, Etienne Marcheret, Vaibhava Goel", "title": "Deep Multimodal Learning for Audio-Visual Speech Recognition", "comments": "ICASSP 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present methods in deep multimodal learning for fusing\nspeech and visual modalities for Audio-Visual Automatic Speech Recognition\n(AV-ASR). First, we study an approach where uni-modal deep networks are trained\nseparately and their final hidden layers fused to obtain a joint feature space\nin which another deep network is built. While the audio network alone achieves\na phone error rate (PER) of $41\\%$ under clean condition on the IBM large\nvocabulary audio-visual studio dataset, this fusion model achieves a PER of\n$35.83\\%$ demonstrating the tremendous value of the visual channel in phone\nclassification even in audio with high signal to noise ratio. Second, we\npresent a new deep network architecture that uses a bilinear softmax layer to\naccount for class specific correlations between modalities. We show that\ncombining the posteriors from the bilinear networks with those from the fused\nmodel mentioned above results in a further significant phone error rate\nreduction, yielding a final PER of $34.03\\%$.\n", "versions": [{"version": "v1", "created": "Thu, 22 Jan 2015 05:25:33 GMT"}], "update_date": "2015-01-23", "authors_parsed": [["Mroueh", "Youssef", ""], ["Marcheret", "Etienne", ""], ["Goel", "Vaibhava", ""]]}, {"id": "1501.05590", "submitter": "Panagiotis Traganitis", "authors": "Panagiotis A. Traganitis, Konstantinos Slavakis, Georgios B. Giannakis", "title": "Sketch and Validate for Big Data Clustering", "comments": "The present paper will appear on Signal Processing for Big Data\n  special issue (June 2015) of the IEEE Journal of Selected Topics in Signal\n  Processing", "journal-ref": null, "doi": "10.1109/JSTSP.2015.2396477", "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In response to the need for learning tools tuned to big data analytics, the\npresent paper introduces a framework for efficient clustering of huge sets of\n(possibly high-dimensional) data. Building on random sampling and consensus\n(RANSAC) ideas pursued earlier in a different (computer vision) context for\nrobust regression, a suite of novel dimensionality and set-reduction algorithms\nis developed. The advocated sketch-and-validate (SkeVa) family includes two\nalgorithms that rely on K-means clustering per iteration on reduced number of\ndimensions and/or feature vectors: The first operates in a batch fashion, while\nthe second sequential one offers computational efficiency and suitability with\nstreaming modes of operation. For clustering even nonlinearly separable\nvectors, the SkeVa family offers also a member based on user-selected kernel\nfunctions. Further trading off performance for reduced complexity, a fourth\nmember of the SkeVa family is based on a divergence criterion for selecting\nproper minimal subsets of feature variables and vectors, thus bypassing the\nneed for K-means clustering per iteration. Extensive numerical tests on\nsynthetic and real data sets highlight the potential of the proposed\nalgorithms, and demonstrate their competitive performance relative to\nstate-of-the-art random projection alternatives.\n", "versions": [{"version": "v1", "created": "Thu, 22 Jan 2015 18:16:30 GMT"}], "update_date": "2016-11-17", "authors_parsed": [["Traganitis", "Panagiotis A.", ""], ["Slavakis", "Konstantinos", ""], ["Giannakis", "Georgios B.", ""]]}, {"id": "1501.05624", "submitter": "John Paisley", "authors": "San Gultekin and John Paisley", "title": "A Collaborative Kalman Filter for Time-Evolving Dyadic Processes", "comments": "Appeared at 2014 IEEE International Conference on Data Mining (ICDM)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present the collaborative Kalman filter (CKF), a dynamic model for\ncollaborative filtering and related factorization models. Using the matrix\nfactorization approach to collaborative filtering, the CKF accounts for time\nevolution by modeling each low-dimensional latent embedding as a\nmultidimensional Brownian motion. Each observation is a random variable whose\ndistribution is parameterized by the dot product of the relevant Brownian\nmotions at that moment in time. This is naturally interpreted as a Kalman\nfilter with multiple interacting state space vectors. We also present a method\nfor learning a dynamically evolving drift parameter for each location by\nmodeling it as a geometric Brownian motion. We handle posterior intractability\nvia a mean-field variational approximation, which also preserves tractability\nfor downstream calculations in a manner similar to the Kalman filter. We\nevaluate the model on several large datasets, providing quantitative evaluation\non the 10 million Movielens and 100 million Netflix datasets and qualitative\nevaluation on a set of 39 million stock returns divided across roughly 6,500\ncompanies from the years 1962-2014.\n", "versions": [{"version": "v1", "created": "Thu, 22 Jan 2015 20:24:32 GMT"}], "update_date": "2015-01-23", "authors_parsed": [["Gultekin", "San", ""], ["Paisley", "John", ""]]}, {"id": "1501.05684", "submitter": "Paul Honeine", "authors": "Paul Honeine, Fei Zhu", "title": "Bi-Objective Nonnegative Matrix Factorization: Linear Versus\n  Kernel-Based Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CV cs.LG math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Nonnegative matrix factorization (NMF) is a powerful class of feature\nextraction techniques that has been successfully applied in many fields, namely\nin signal and image processing. Current NMF techniques have been limited to a\nsingle-objective problem in either its linear or nonlinear kernel-based\nformulation. In this paper, we propose to revisit the NMF as a multi-objective\nproblem, in particular a bi-objective one, where the objective functions\ndefined in both input and feature spaces are taken into account. By taking the\nadvantage of the sum-weighted method from the literature of multi-objective\noptimization, the proposed bi-objective NMF determines a set of nondominated,\nPareto optimal, solutions instead of a single optimal decomposition. Moreover,\nthe corresponding Pareto front is studied and approximated. Experimental\nresults on unmixing real hyperspectral images confirm the efficiency of the\nproposed bi-objective NMF compared with the state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Thu, 22 Jan 2015 22:59:47 GMT"}], "update_date": "2015-01-26", "authors_parsed": [["Honeine", "Paul", ""], ["Zhu", "Fei", ""]]}, {"id": "1501.05740", "submitter": "Martin Sundin", "authors": "Martin Sundin, Cristian R. Rojas, Magnus Jansson and Saikat Chatterjee", "title": "Bayesian Learning for Low-Rank matrix reconstruction", "comments": "Submitted to IEEE Transactions on Signal Processing", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG cs.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop latent variable models for Bayesian learning based low-rank matrix\ncompletion and reconstruction from linear measurements. For under-determined\nsystems, the developed methods are shown to reconstruct low-rank matrices when\nneither the rank nor the noise power is known a-priori. We derive relations\nbetween the latent variable models and several low-rank promoting penalty\nfunctions. The relations justify the use of Kronecker structured covariance\nmatrices in a Gaussian based prior. In the methods, we use evidence\napproximation and expectation-maximization to learn the model parameters. The\nperformance of the methods is evaluated through extensive numerical\nsimulations.\n", "versions": [{"version": "v1", "created": "Fri, 23 Jan 2015 08:52:35 GMT"}], "update_date": "2015-01-26", "authors_parsed": [["Sundin", "Martin", ""], ["Rojas", "Cristian R.", ""], ["Jansson", "Magnus", ""], ["Chatterjee", "Saikat", ""]]}, {"id": "1501.06060", "submitter": "Yi Wang", "authors": "Yi Wang", "title": "Consistency Analysis of Nearest Subspace Classifier", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Nearest subspace classifier (NSS) finds an estimation of the underlying\nsubspace within each class and assigns data points to the class that\ncorresponds to its nearest subspace. This paper mainly studies how well NSS can\nbe generalized to new samples. It is proved that NSS is strongly consistent\nunder certain assumptions. For completeness, NSS is evaluated through\nexperiments on various simulated and real data sets, in comparison with some\nother linear model based classifiers. It is also shown that NSS can obtain\neffective classification results and is very efficient, especially for large\nscale data sets.\n", "versions": [{"version": "v1", "created": "Sat, 24 Jan 2015 16:41:55 GMT"}], "update_date": "2015-01-27", "authors_parsed": [["Wang", "Yi", ""]]}, {"id": "1501.06095", "submitter": "Jonathan Ullman", "authors": "Thomas Steinke and Jonathan Ullman", "title": "Between Pure and Approximate Differential Privacy", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We show a new lower bound on the sample complexity of $(\\varepsilon,\n\\delta)$-differentially private algorithms that accurately answer statistical\nqueries on high-dimensional databases. The novelty of our bound is that it\ndepends optimally on the parameter $\\delta$, which loosely corresponds to the\nprobability that the algorithm fails to be private, and is the first to\nsmoothly interpolate between approximate differential privacy ($\\delta > 0$)\nand pure differential privacy ($\\delta = 0$).\n  Specifically, we consider a database $D \\in \\{\\pm1\\}^{n \\times d}$ and its\n\\emph{one-way marginals}, which are the $d$ queries of the form \"What fraction\nof individual records have the $i$-th bit set to $+1$?\" We show that in order\nto answer all of these queries to within error $\\pm \\alpha$ (on average) while\nsatisfying $(\\varepsilon, \\delta)$-differential privacy, it is necessary that\n$$ n \\geq \\Omega\\left( \\frac{\\sqrt{d \\log(1/\\delta)}}{\\alpha \\varepsilon}\n\\right), $$ which is optimal up to constant factors. To prove our lower bound,\nwe build on the connection between \\emph{fingerprinting codes} and lower bounds\nin differential privacy (Bun, Ullman, and Vadhan, STOC'14).\n  In addition to our lower bound, we give new purely and approximately\ndifferentially private algorithms for answering arbitrary statistical queries\nthat improve on the sample complexity of the standard Laplace and Gaussian\nmechanisms for achieving worst-case accuracy guarantees by a logarithmic\nfactor.\n", "versions": [{"version": "v1", "created": "Sat, 24 Jan 2015 23:26:21 GMT"}], "update_date": "2015-01-27", "authors_parsed": [["Steinke", "Thomas", ""], ["Ullman", "Jonathan", ""]]}, {"id": "1501.06115", "submitter": "Wentao Zhu", "authors": "Wentao Zhu, Jun Miao, Laiyun Qing", "title": "Constrained Extreme Learning Machines: A Study on Classification Cases", "comments": "14 pages, 6 figure, journel", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Extreme learning machine (ELM) is an extremely fast learning method and has a\npowerful performance for pattern recognition tasks proven by enormous\nresearches and engineers. However, its good generalization ability is built on\nlarge numbers of hidden neurons, which is not beneficial to real time response\nin the test process. In this paper, we proposed new ways, named \"constrained\nextreme learning machines\" (CELMs), to randomly select hidden neurons based on\nsample distribution. Compared to completely random selection of hidden nodes in\nELM, the CELMs randomly select hidden nodes from the constrained vector space\ncontaining some basic combinations of original sample vectors. The experimental\nresults show that the CELMs have better generalization ability than traditional\nELM, SVM and some other related methods. Additionally, the CELMs have a similar\nfast learning speed as ELM.\n", "versions": [{"version": "v1", "created": "Sun, 25 Jan 2015 05:11:34 GMT"}, {"version": "v2", "created": "Wed, 4 Feb 2015 11:42:01 GMT"}], "update_date": "2015-02-05", "authors_parsed": [["Zhu", "Wentao", ""], ["Miao", "Jun", ""], ["Qing", "Laiyun", ""]]}, {"id": "1501.06195", "submitter": "Yun Yang", "authors": "Yun Yang, Mert Pilanci, Martin J. Wainwright", "title": "Randomized sketches for kernels: Fast and optimal non-parametric\n  regression", "comments": "27 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.DS cs.LG stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Kernel ridge regression (KRR) is a standard method for performing\nnon-parametric regression over reproducing kernel Hilbert spaces. Given $n$\nsamples, the time and space complexity of computing the KRR estimate scale as\n$\\mathcal{O}(n^3)$ and $\\mathcal{O}(n^2)$ respectively, and so is prohibitive\nin many cases. We propose approximations of KRR based on $m$-dimensional\nrandomized sketches of the kernel matrix, and study how small the projection\ndimension $m$ can be chosen while still preserving minimax optimality of the\napproximate KRR estimate. For various classes of randomized sketches, including\nthose based on Gaussian and randomized Hadamard matrices, we prove that it\nsuffices to choose the sketch dimension $m$ proportional to the statistical\ndimension (modulo logarithmic factors). Thus, we obtain fast and minimax\noptimal approximations to the KRR estimate for non-parametric regression.\n", "versions": [{"version": "v1", "created": "Sun, 25 Jan 2015 19:06:59 GMT"}], "update_date": "2015-01-27", "authors_parsed": [["Yang", "Yun", ""], ["Pilanci", "Mert", ""], ["Wainwright", "Martin J.", ""]]}, {"id": "1501.06202", "submitter": "Yanwei  Fu", "authors": "Yanwei Fu, Timothy M. Hospedales, Tao Xiang, Jiechao Xiong, Shaogang\n  Gong, Yizhou Wang, and Yuan Yao", "title": "Robust Subjective Visual Property Prediction from Crowdsourced Pairwise\n  Labels", "comments": "14 pages, accepted by IEEE TPAMI", "journal-ref": null, "doi": "10.1109/TPAMI.2015.2456887", "report-no": null, "categories": "cs.CV cs.LG cs.MM cs.SI math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The problem of estimating subjective visual properties from image and video\nhas attracted increasing interest. A subjective visual property is useful\neither on its own (e.g. image and video interestingness) or as an intermediate\nrepresentation for visual recognition (e.g. a relative attribute). Due to its\nambiguous nature, annotating the value of a subjective visual property for\nlearning a prediction model is challenging. To make the annotation more\nreliable, recent studies employ crowdsourcing tools to collect pairwise\ncomparison labels because human annotators are much better at ranking two\nimages/videos (e.g. which one is more interesting) than giving an absolute\nvalue to each of them separately. However, using crowdsourced data also\nintroduces outliers. Existing methods rely on majority voting to prune the\nannotation outliers/errors. They thus require large amount of pairwise labels\nto be collected. More importantly as a local outlier detection method, majority\nvoting is ineffective in identifying outliers that can cause global ranking\ninconsistencies. In this paper, we propose a more principled way to identify\nannotation outliers by formulating the subjective visual property prediction\ntask as a unified robust learning to rank problem, tackling both the outlier\ndetection and learning to rank jointly. Differing from existing methods, the\nproposed method integrates local pairwise comparison labels together to\nminimise a cost that corresponds to global inconsistency of ranking order. This\nnot only leads to better detection of annotation outliers but also enables\nlearning with extremely sparse annotations. Extensive experiments on various\nbenchmark datasets demonstrate that our new approach significantly outperforms\nstate-of-the-arts alternatives.\n", "versions": [{"version": "v1", "created": "Sun, 25 Jan 2015 20:02:45 GMT"}, {"version": "v2", "created": "Fri, 30 Jan 2015 05:13:45 GMT"}, {"version": "v3", "created": "Fri, 24 Jul 2015 18:40:56 GMT"}, {"version": "v4", "created": "Mon, 27 Jul 2015 14:42:17 GMT"}], "update_date": "2015-07-28", "authors_parsed": [["Fu", "Yanwei", ""], ["Hospedales", "Timothy M.", ""], ["Xiang", "Tao", ""], ["Xiong", "Jiechao", ""], ["Gong", "Shaogang", ""], ["Wang", "Yizhou", ""], ["Yao", "Yuan", ""]]}, {"id": "1501.06225", "submitter": "Shahin Shahrampour", "authors": "Ali Jadbabaie, Alexander Rakhlin, Shahin Shahrampour and Karthik\n  Sridharan", "title": "Online Optimization : Competing with Dynamic Comparators", "comments": "23 pages, To appear in International Conference on Artificial\n  Intelligence and Statistics (AISTATS) 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent literature on online learning has focused on developing adaptive\nalgorithms that take advantage of a regularity of the sequence of observations,\nyet retain worst-case performance guarantees. A complementary direction is to\ndevelop prediction methods that perform well against complex benchmarks. In\nthis paper, we address these two directions together. We present a fully\nadaptive method that competes with dynamic benchmarks in which regret guarantee\nscales with regularity of the sequence of cost functions and comparators.\nNotably, the regret bound adapts to the smaller complexity measure in the\nproblem environment. Finally, we apply our results to drifting zero-sum,\ntwo-player games where both players achieve no regret guarantees against best\nsequences of actions in hindsight.\n", "versions": [{"version": "v1", "created": "Mon, 26 Jan 2015 00:40:08 GMT"}], "update_date": "2015-01-27", "authors_parsed": [["Jadbabaie", "Ali", ""], ["Rakhlin", "Alexander", ""], ["Shahrampour", "Shahin", ""], ["Sridharan", "Karthik", ""]]}, {"id": "1501.06237", "submitter": "Gang Chen", "authors": "Gang Chen", "title": "Deep Transductive Semi-supervised Maximum Margin Clustering", "comments": "14", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  Semi-supervised clustering is an very important topic in machine learning and\ncomputer vision. The key challenge of this problem is how to learn a metric,\nsuch that the instances sharing the same label are more likely close to each\nother on the embedded space. However, little attention has been paid to learn\nbetter representations when the data lie on non-linear manifold. Fortunately,\ndeep learning has led to great success on feature learning recently. Inspired\nby the advances of deep learning, we propose a deep transductive\nsemi-supervised maximum margin clustering approach. More specifically, given\npairwise constraints, we exploit both labeled and unlabeled data to learn a\nnon-linear mapping under maximum margin framework for clustering analysis.\nThus, our model unifies transductive learning, feature learning and maximum\nmargin techniques in the semi-supervised clustering framework. We pretrain the\ndeep network structure with restricted Boltzmann machines (RBMs) layer by layer\ngreedily, and optimize our objective function with gradient descent. By\nchecking the most violated constraints, our approach updates the model\nparameters through error backpropagation, in which deep features are learned\nautomatically. The experimental results shows that our model is significantly\nbetter than the state of the art on semi-supervised clustering.\n", "versions": [{"version": "v1", "created": "Mon, 26 Jan 2015 02:28:18 GMT"}], "update_date": "2015-01-27", "authors_parsed": [["Chen", "Gang", ""]]}, {"id": "1501.06241", "submitter": "Yao Xie", "authors": "Ruiyang Song, Yao Xie, Sebastian Pokutta", "title": "Sequential Sensing with Model Mismatch", "comments": "Submitted to IEEE for publication", "journal-ref": null, "doi": "10.1109/ISIT.2015.7282736", "report-no": null, "categories": "stat.ML cs.IT cs.LG math.IT math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We characterize the performance of sequential information guided sensing,\nInfo-Greedy Sensing, when there is a mismatch between the true signal model and\nthe assumed model, which may be a sample estimate. In particular, we consider a\nsetup where the signal is low-rank Gaussian and the measurements are taken in\nthe directions of eigenvectors of the covariance matrix in a decreasing order\nof eigenvalues. We establish a set of performance bounds when a mismatched\ncovariance matrix is used, in terms of the gap of signal posterior entropy, as\nwell as the additional amount of power required to achieve the same signal\nrecovery precision. Based on this, we further study how to choose an\ninitialization for Info-Greedy Sensing using the sample covariance matrix, or\nusing an efficient covariance sketching scheme.\n", "versions": [{"version": "v1", "created": "Mon, 26 Jan 2015 02:51:13 GMT"}, {"version": "v2", "created": "Mon, 16 Mar 2015 22:38:33 GMT"}], "update_date": "2016-11-18", "authors_parsed": [["Song", "Ruiyang", ""], ["Xie", "Yao", ""], ["Pokutta", "Sebastian", ""]]}, {"id": "1501.06243", "submitter": "Yang Cao", "authors": "Yang Cao and Yao Xie", "title": "Poisson Matrix Completion", "comments": "Submitted to IEEE for publication", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We extend the theory of matrix completion to the case where we make Poisson\nobservations for a subset of entries of a low-rank matrix. We consider the\n(now) usual matrix recovery formulation through maximum likelihood with proper\nconstraints on the matrix $M$, and establish theoretical upper and lower bounds\non the recovery error. Our bounds are nearly optimal up to a factor on the\norder of $\\mathcal{O}(\\log(d_1 d_2))$. These bounds are obtained by adapting\nthe arguments used for one-bit matrix completion \\cite{davenport20121}\n(although these two problems are different in nature) and the adaptation\nrequires new techniques exploiting properties of the Poisson likelihood\nfunction and tackling the difficulties posed by the locally sub-Gaussian\ncharacteristic of the Poisson distribution. Our results highlight a few\nimportant distinctions of Poisson matrix completion compared to the prior work\nin matrix completion including having to impose a minimum signal-to-noise\nrequirement on each observed entry. We also develop an efficient iterative\nalgorithm and demonstrate its good performance in recovering solar flare\nimages.\n", "versions": [{"version": "v1", "created": "Mon, 26 Jan 2015 03:02:51 GMT"}, {"version": "v2", "created": "Wed, 28 Jan 2015 05:14:30 GMT"}, {"version": "v3", "created": "Wed, 18 Feb 2015 16:52:34 GMT"}, {"version": "v4", "created": "Thu, 19 Feb 2015 04:04:57 GMT"}, {"version": "v5", "created": "Mon, 23 Mar 2015 16:24:58 GMT"}, {"version": "v6", "created": "Wed, 25 Mar 2015 18:03:15 GMT"}], "update_date": "2015-03-26", "authors_parsed": [["Cao", "Yang", ""], ["Xie", "Yao", ""]]}, {"id": "1501.06272", "submitter": "Fang Zhao", "authors": "Fang Zhao, Yongzhen Huang, Liang Wang, Tieniu Tan", "title": "Deep Semantic Ranking Based Hashing for Multi-Label Image Retrieval", "comments": "CVPR 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the rapid growth of web images, hashing has received increasing\ninterests in large scale image retrieval. Research efforts have been devoted to\nlearning compact binary codes that preserve semantic similarity based on\nlabels. However, most of these hashing methods are designed to handle simple\nbinary similarity. The complex multilevel semantic structure of images\nassociated with multiple labels have not yet been well explored. Here we\npropose a deep semantic ranking based method for learning hash functions that\npreserve multilevel semantic similarity between multi-label images. In our\napproach, deep convolutional neural network is incorporated into hash functions\nto jointly learn feature representations and mappings from them to hash codes,\nwhich avoids the limitation of semantic representation power of hand-crafted\nfeatures. Meanwhile, a ranking list that encodes the multilevel similarity\ninformation is employed to guide the learning of such deep hash functions. An\neffective scheme based on surrogate loss is used to solve the intractable\noptimization problem of nonsmooth and multivariate ranking measures involved in\nthe learning procedure. Experimental results show the superiority of our\nproposed approach over several state-of-the-art hashing methods in term of\nranking evaluation metrics when tested on multi-label image datasets.\n", "versions": [{"version": "v1", "created": "Mon, 26 Jan 2015 07:33:40 GMT"}, {"version": "v2", "created": "Sun, 19 Apr 2015 04:28:58 GMT"}], "update_date": "2015-04-21", "authors_parsed": [["Zhao", "Fang", ""], ["Huang", "Yongzhen", ""], ["Wang", "Liang", ""], ["Tan", "Tieniu", ""]]}, {"id": "1501.06284", "submitter": "Carl Henrik Ek", "authors": "Andrea Baisero, Florian T. Pokorny, Carl Henrik Ek", "title": "On a Family of Decomposable Kernels on Sequences", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In many applications data is naturally presented in terms of orderings of\nsome basic elements or symbols. Reasoning about such data requires a notion of\nsimilarity capable of handling sequences of different lengths. In this paper we\ndescribe a family of Mercer kernel functions for such sequentially structured\ndata. The family is characterized by a decomposable structure in terms of\nsymbol-level and structure-level similarities, representing a specific\ncombination of kernels which allows for efficient computation. We provide an\nexperimental evaluation on sequential classification tasks comparing kernels\nfrom our family of kernels to a state of the art sequence kernel called the\nGlobal Alignment kernel which has been shown to outperform Dynamic Time Warping\n", "versions": [{"version": "v1", "created": "Mon, 26 Jan 2015 08:30:55 GMT"}], "update_date": "2015-01-27", "authors_parsed": [["Baisero", "Andrea", ""], ["Pokorny", "Florian T.", ""], ["Ek", "Carl Henrik", ""]]}, {"id": "1501.06450", "submitter": "Teng Qiu", "authors": "Teng Qiu, Yongjie Li", "title": "IT-map: an Effective Nonlinear Dimensionality Reduction Method for\n  Interactive Clustering", "comments": "13 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  Scientists in many fields have the common and basic need of dimensionality\nreduction: visualizing the underlying structure of the massive multivariate\ndata in a low-dimensional space. However, many dimensionality reduction methods\nconfront the so-called \"crowding problem\" that clusters tend to overlap with\neach other in the embedding. Previously, researchers expect to avoid that\nproblem and seek to make clusters maximally separated in the embedding.\nHowever, the proposed in-tree (IT) based method, called IT-map, allows clusters\nin the embedding to be locally overlapped, while seeking to make them\ndistinguishable by some small yet key parts. IT-map provides a simple,\neffective and novel solution to cluster-preserving mapping, which makes it\npossible to cluster the original data points interactively and thus should be\nof general meaning in science and engineering.\n", "versions": [{"version": "v1", "created": "Mon, 26 Jan 2015 15:37:22 GMT"}, {"version": "v2", "created": "Wed, 18 Mar 2015 14:48:42 GMT"}], "update_date": "2015-03-19", "authors_parsed": [["Qiu", "Teng", ""], ["Li", "Yongjie", ""]]}, {"id": "1501.06478", "submitter": "Stephen Tyree", "authors": "Zhixiang Xu, Jacob R. Gardner, Stephen Tyree, Kilian Q. Weinberger", "title": "Compressed Support Vector Machines", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Support vector machines (SVM) can classify data sets along highly non-linear\ndecision boundaries because of the kernel-trick. This expressiveness comes at a\nprice: During test-time, the SVM classifier needs to compute the kernel\ninner-product between a test sample and all support vectors. With large\ntraining data sets, the time required for this computation can be substantial.\nIn this paper, we introduce a post-processing algorithm, which compresses the\nlearned SVM model by reducing and optimizing support vectors. We evaluate our\nalgorithm on several medium-scaled real-world data sets, demonstrating that it\nmaintains high test accuracy while reducing the test-time evaluation cost by\nseveral orders of magnitude---in some cases from hours to seconds. It is fair\nto say that most of the work in this paper was previously been invented by\nBurges and Sch\\\"olkopf almost 20 years ago. For most of the time during which\nwe conducted this research, we were unaware of this prior work. However, in the\npast two decades, computing power has increased drastically, and we can\ntherefore provide empirical insights that were not possible in their original\npaper.\n", "versions": [{"version": "v1", "created": "Mon, 26 Jan 2015 16:51:34 GMT"}, {"version": "v2", "created": "Mon, 2 Feb 2015 16:24:38 GMT"}], "update_date": "2015-02-03", "authors_parsed": [["Xu", "Zhixiang", ""], ["Gardner", "Jacob R.", ""], ["Tyree", "Stephen", ""], ["Weinberger", "Kilian Q.", ""]]}, {"id": "1501.06521", "submitter": "Ankur Moitra", "authors": "Boaz Barak and Ankur Moitra", "title": "Noisy Tensor Completion via the Sum-of-Squares Hierarchy", "comments": "24 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the noisy tensor completion problem we observe $m$ entries (whose location\nis chosen uniformly at random) from an unknown $n_1 \\times n_2 \\times n_3$\ntensor $T$. We assume that $T$ is entry-wise close to being rank $r$. Our goal\nis to fill in its missing entries using as few observations as possible. Let $n\n= \\max(n_1, n_2, n_3)$. We show that if $m = n^{3/2} r$ then there is a\npolynomial time algorithm based on the sixth level of the sum-of-squares\nhierarchy for completing it. Our estimate agrees with almost all of $T$'s\nentries almost exactly and works even when our observations are corrupted by\nnoise. This is also the first algorithm for tensor completion that works in the\novercomplete case when $r > n$, and in fact it works all the way up to $r =\nn^{3/2-\\epsilon}$.\n  Our proofs are short and simple and are based on establishing a new\nconnection between noisy tensor completion (through the language of Rademacher\ncomplexity) and the task of refuting random constant satisfaction problems.\nThis connection seems to have gone unnoticed even in the context of matrix\ncompletion. Furthermore, we use this connection to show matching lower bounds.\nOur main technical result is in characterizing the Rademacher complexity of the\nsequence of norms that arise in the sum-of-squares relaxations to the tensor\nnuclear norm. These results point to an interesting new direction: Can we\nexplore computational vs. sample complexity tradeoffs through the\nsum-of-squares hierarchy?\n", "versions": [{"version": "v1", "created": "Mon, 26 Jan 2015 18:48:55 GMT"}, {"version": "v2", "created": "Thu, 2 Apr 2015 19:54:52 GMT"}, {"version": "v3", "created": "Thu, 18 Feb 2016 16:37:39 GMT"}], "update_date": "2016-02-19", "authors_parsed": [["Barak", "Boaz", ""], ["Moitra", "Ankur", ""]]}, {"id": "1501.06587", "submitter": "Daniel Lemire", "authors": "Xiaodan Zhu, Peter Turney, Daniel Lemire, Andr\\'e Vellino", "title": "Measuring academic influence: Not all citations are equal", "comments": null, "journal-ref": "Journal of the Association for Information Science and Technology,\n  66: 408-427", "doi": "10.1002/asi.23179", "report-no": null, "categories": "cs.DL cs.CL cs.LG", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  The importance of a research article is routinely measured by counting how\nmany times it has been cited. However, treating all citations with equal weight\nignores the wide variety of functions that citations perform. We want to\nautomatically identify the subset of references in a bibliography that have a\ncentral academic influence on the citing paper. For this purpose, we examine\nthe effectiveness of a variety of features for determining the academic\ninfluence of a citation. By asking authors to identify the key references in\ntheir own work, we created a data set in which citations were labeled according\nto their academic influence. Using automatic feature selection with supervised\nmachine learning, we found a model for predicting academic influence that\nachieves good performance on this data set using only four features. The best\nfeatures, among those we evaluated, were those based on the number of times a\nreference is mentioned in the body of a citing paper. The performance of these\nfeatures inspired us to design an influence-primed h-index (the hip-index).\nUnlike the conventional h-index, it weights citations by how many times a\nreference is mentioned. According to our experiments, the hip-index is a better\nindicator of researcher performance than the conventional h-index.\n", "versions": [{"version": "v1", "created": "Mon, 26 Jan 2015 21:06:02 GMT"}], "update_date": "2015-01-28", "authors_parsed": [["Zhu", "Xiaodan", ""], ["Turney", "Peter", ""], ["Lemire", "Daniel", ""], ["Vellino", "Andr\u00e9", ""]]}, {"id": "1501.06598", "submitter": "Alexander Rakhlin", "authors": "Alexander Rakhlin and Karthik Sridharan", "title": "Online Nonparametric Regression with General Loss Functions", "comments": "arXiv admin note: substantial text overlap with arXiv:1402.2594", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.IT cs.LG math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper establishes minimax rates for online regression with arbitrary\nclasses of functions and general losses. We show that below a certain threshold\nfor the complexity of the function class, the minimax rates depend on both the\ncurvature of the loss function and the sequential complexities of the class.\nAbove this threshold, the curvature of the loss does not affect the rates.\nFurthermore, for the case of square loss, our results point to the interesting\nphenomenon: whenever sequential and i.i.d. empirical entropies match, the rates\nfor statistical and online learning are the same.\n  In addition to the study of minimax regret, we derive a generic forecaster\nthat enjoys the established optimal rates. We also provide a recipe for\ndesigning online prediction algorithms that can be computationally efficient\nfor certain problems. We illustrate the techniques by deriving existing and new\nforecasters for the case of finite experts and for online linear regression.\n", "versions": [{"version": "v1", "created": "Mon, 26 Jan 2015 21:52:41 GMT"}], "update_date": "2015-01-28", "authors_parsed": [["Rakhlin", "Alexander", ""], ["Sridharan", "Karthik", ""]]}, {"id": "1501.06633", "submitter": "Andrew Lavin", "authors": "Andrew Lavin", "title": "maxDNN: An Efficient Convolution Kernel for Deep Learning with Maxwell\n  GPUs", "comments": "7 pages, 2 figures, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.DC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper describes maxDNN, a computationally efficient convolution kernel\nfor deep learning with the NVIDIA Maxwell GPU. maxDNN reaches 96.3%\ncomputational efficiency on typical deep learning network architectures. The\ndesign combines ideas from cuda-convnet2 with the Maxas SGEMM assembly code. We\nonly address forward propagation (FPROP) operation of the network, but we\nbelieve that the same techniques used here will be effective for backward\npropagation (BPROP) as well.\n", "versions": [{"version": "v1", "created": "Tue, 27 Jan 2015 01:19:12 GMT"}, {"version": "v2", "created": "Wed, 28 Jan 2015 01:16:33 GMT"}, {"version": "v3", "created": "Fri, 30 Jan 2015 23:50:49 GMT"}], "update_date": "2015-02-03", "authors_parsed": [["Lavin", "Andrew", ""]]}, {"id": "1501.06794", "submitter": "Bernhard Sch\\\"olkopf", "authors": "Bernhard Sch\\\"olkopf, Krikamol Muandet, Kenji Fukumizu, Jonas Peters", "title": "Computing Functions of Random Variables via Reproducing Kernel Hilbert\n  Space Representations", "comments": null, "journal-ref": "Statistics and Computing 25:755-766 (2015)", "doi": null, "report-no": null, "categories": "stat.ML cs.DS cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe a method to perform functional operations on probability\ndistributions of random variables. The method uses reproducing kernel Hilbert\nspace representations of probability distributions, and it is applicable to all\noperations which can be applied to points drawn from the respective\ndistributions. We refer to our approach as {\\em kernel probabilistic\nprogramming}. We illustrate it on synthetic data, and show how it can be used\nfor nonparametric structural equation models, with an application to causal\ninference.\n", "versions": [{"version": "v1", "created": "Tue, 27 Jan 2015 15:36:22 GMT"}], "update_date": "2016-09-14", "authors_parsed": [["Sch\u00f6lkopf", "Bernhard", ""], ["Muandet", "Krikamol", ""], ["Fukumizu", "Kenji", ""], ["Peters", "Jonas", ""]]}, {"id": "1501.07093", "submitter": "Mallenahalli Naresh Kumar Prof. Dr.", "authors": "V. Sree Hari Rao and M. Naresh Kumar", "title": "Novel Approaches for Predicting Risk Factors of Atherosclerosis", "comments": "7 pages, 2 figures", "journal-ref": "Biomedical and Health Informatics, IEEE Journal of , vol.17, no.1,\n  pp.183,189, Jan. 2013", "doi": "10.1109/TITB.2012.2227271", "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Coronary heart disease (CHD) caused by hardening of artery walls due to\ncholesterol known as atherosclerosis is responsible for large number of deaths\nworld-wide. The disease progression is slow, asymptomatic and may lead to\nsudden cardiac arrest, stroke or myocardial infraction. Presently, imaging\ntechniques are being employed to understand the molecular and metabolic\nactivity of atherosclerotic plaques to estimate the risk. Though imaging\nmethods are able to provide some information on plaque metabolism they lack the\nrequired resolution and sensitivity for detection. In this paper we consider\nthe clinical observations and habits of individuals for predicting the risk\nfactors of CHD. The identification of risk factors helps in stratifying\npatients for further intensive tests such as nuclear imaging or coronary\nangiography. We present a novel approach for predicting the risk factors of\natherosclerosis with an in-built imputation algorithm and particle swarm\noptimization (PSO). We compare the performance of our methodology with other\nmachine learning techniques on STULONG dataset which is based on longitudinal\nstudy of middle aged individuals lasting for twenty years. Our methodology\npowered by PSO search has identified physical inactivity as one of the risk\nfactor for the onset of atherosclerosis in addition to other already known\nfactors. The decision rules extracted by our methodology are able to predict\nthe risk factors with an accuracy of $99.73%$ which is higher than the\naccuracies obtained by application of the state-of-the-art machine learning\ntechniques presently being employed in the identification of atherosclerosis\nrisk studies.\n", "versions": [{"version": "v1", "created": "Wed, 28 Jan 2015 13:26:02 GMT"}, {"version": "v2", "created": "Sat, 31 Jan 2015 03:22:15 GMT"}], "update_date": "2015-02-03", "authors_parsed": [["Rao", "V. Sree Hari", ""], ["Kumar", "M. Naresh", ""]]}, {"id": "1501.07227", "submitter": "Robert Murphy", "authors": "Robert A. Murphy", "title": "A Neural Network Anomaly Detector Using the Random Cluster Model", "comments": "These writings are part of a longer writing which has been submitted\n  for publication. I plan to replace this writing (and the other 2 writings)\n  with the single writing that has been submitted for publication. The other\n  writings to be withdrawn are 1503.03488 and 1412.4178", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The random cluster model is used to define an upper bound on a distance\nmeasure as a function of the number of data points to be classified and the\nexpected value of the number of classes to form in a hybrid K-means and\nregression classification methodology, with the intent of detecting anomalies.\nConditions are given for the identification of classes which contain anomalies\nand individual anomalies within identified classes. A neural network model\ndescribes the decision region-separating surface for offline storage and recall\nin any new anomaly detection.\n", "versions": [{"version": "v1", "created": "Wed, 28 Jan 2015 18:42:42 GMT"}, {"version": "v2", "created": "Sat, 7 Mar 2015 22:49:15 GMT"}, {"version": "v3", "created": "Wed, 11 Mar 2015 02:35:37 GMT"}, {"version": "v4", "created": "Sat, 14 Mar 2015 20:49:32 GMT"}, {"version": "v5", "created": "Wed, 10 Feb 2016 22:29:26 GMT"}], "update_date": "2016-02-12", "authors_parsed": [["Murphy", "Robert A.", ""]]}, {"id": "1501.07242", "submitter": "Tengyuan Liang", "authors": "Alexandre Belloni, Tengyuan Liang, Hariharan Narayanan, Alexander\n  Rakhlin", "title": "Escaping the Local Minima via Simulated Annealing: Optimization of\n  Approximately Convex Functions", "comments": "27 pages", "journal-ref": "Proceedings of the 28th Conference on Learning Theory 40 (2015)\n  240-265", "doi": null, "report-no": null, "categories": "cs.NA cs.LG math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of optimizing an approximately convex function over a\nbounded convex set in $\\mathbb{R}^n$ using only function evaluations. The\nproblem is reduced to sampling from an \\emph{approximately} log-concave\ndistribution using the Hit-and-Run method, which is shown to have the same\n$\\mathcal{O}^*$ complexity as sampling from log-concave distributions. In\naddition to extend the analysis for log-concave distributions to approximate\nlog-concave distributions, the implementation of the 1-dimensional sampler of\nthe Hit-and-Run walk requires new methods and analysis. The algorithm then is\nbased on simulated annealing which does not relies on first order conditions\nwhich makes it essentially immune to local minima.\n  We then apply the method to different motivating problems. In the context of\nzeroth order stochastic convex optimization, the proposed method produces an\n$\\epsilon$-minimizer after $\\mathcal{O}^*(n^{7.5}\\epsilon^{-2})$ noisy function\nevaluations by inducing a $\\mathcal{O}(\\epsilon/n)$-approximately log concave\ndistribution. We also consider in detail the case when the \"amount of\nnon-convexity\" decays towards the optimum of the function. Other applications\nof the method discussed in this work include private computation of empirical\nrisk minimizers, two-stage stochastic programming, and approximate dynamic\nprogramming for online learning.\n", "versions": [{"version": "v1", "created": "Wed, 28 Jan 2015 19:12:40 GMT"}, {"version": "v2", "created": "Mon, 15 Jun 2015 15:11:54 GMT"}], "update_date": "2020-07-27", "authors_parsed": [["Belloni", "Alexandre", ""], ["Liang", "Tengyuan", ""], ["Narayanan", "Hariharan", ""], ["Rakhlin", "Alexander", ""]]}, {"id": "1501.07315", "submitter": "Konstantinos Slavakis", "authors": "Konstantinos Slavakis and Georgios B. Giannakis", "title": "Per-Block-Convex Data Modeling by Accelerated Stochastic Approximation", "comments": "The paper has been withdrawn by the author due to the need for a\n  careful and very long revision process", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Applications involving dictionary learning, non-negative matrix\nfactorization, subspace clustering, and parallel factor tensor decomposition\ntasks motivate well algorithms for per-block-convex and non-smooth optimization\nproblems. By leveraging the stochastic approximation paradigm and first-order\nacceleration schemes, this paper develops an online and modular learning\nalgorithm for a large class of non-convex data models, where convexity is\nmanifested only per-block of variables whenever the rest of them are held\nfixed. The advocated algorithm incurs computational complexity that scales\nlinearly with the number of unknowns. Under minimal assumptions on the cost\nfunctions of the composite optimization task, without bounding constraints on\nthe optimization variables, or any explicit information on bounds of Lipschitz\ncoefficients, the expected cost evaluated online at the resultant iterates is\nprovably convergent with quadratic rate to an accumulation point of the\n(per-block) minima, while subgradients of the expected cost asymptotically\nvanish in the mean-squared sense. The merits of the general approach are\ndemonstrated in two online learning setups: (i) Robust linear regression using\na sparsity-cognizant total least-squares criterion; and (ii) semi-supervised\ndictionary learning for network-wide link load tracking and imputation with\nmissing entries. Numerical tests on synthetic and real data highlight the\npotential of the proposed framework for streaming data analytics by\ndemonstrating superior performance over block coordinate descent, and reduced\ncomplexity relative to the popular alternating-direction method of multipliers.\n", "versions": [{"version": "v1", "created": "Thu, 29 Jan 2015 00:15:28 GMT"}, {"version": "v2", "created": "Mon, 12 Dec 2016 22:18:49 GMT"}, {"version": "v3", "created": "Thu, 26 Jan 2017 15:49:01 GMT"}], "update_date": "2017-01-27", "authors_parsed": [["Slavakis", "Konstantinos", ""], ["Giannakis", "Georgios B.", ""]]}, {"id": "1501.07320", "submitter": "Arun Tejasvi Chaganty", "authors": "Volodymyr Kuleshov and Arun Tejasvi Chaganty and Percy Liang", "title": "Tensor Factorization via Matrix Factorization", "comments": "Appearing in Proceedings of the 18th International Conference on\n  Artificial Intelligence and Statistics (AISTATS) 2015, San Diego, CA, USA.\n  JMLR: W&CP volume 38", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Tensor factorization arises in many machine learning applications, such\nknowledge base modeling and parameter estimation in latent variable models.\nHowever, numerical methods for tensor factorization have not reached the level\nof maturity of matrix factorization methods. In this paper, we propose a new\nmethod for CP tensor factorization that uses random projections to reduce the\nproblem to simultaneous matrix diagonalization. Our method is conceptually\nsimple and also applies to non-orthogonal and asymmetric tensors of arbitrary\norder. We prove that a small number random projections essentially preserves\nthe spectral information in the tensor, allowing us to remove the dependence on\nthe eigengap that plagued earlier tensor-to-matrix reductions. Experimentally,\nour method outperforms existing tensor factorization methods on both simulated\ndata and two real datasets.\n", "versions": [{"version": "v1", "created": "Thu, 29 Jan 2015 01:01:34 GMT"}, {"version": "v2", "created": "Mon, 18 May 2015 21:53:34 GMT"}], "update_date": "2015-05-20", "authors_parsed": [["Kuleshov", "Volodymyr", ""], ["Chaganty", "Arun Tejasvi", ""], ["Liang", "Percy", ""]]}, {"id": "1501.07340", "submitter": "Alexander Rakhlin", "authors": "Alexander Rakhlin and Karthik Sridharan", "title": "Sequential Probability Assignment with Binary Alphabets and Large\n  Classes of Experts", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.LG math.IT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We analyze the problem of sequential probability assignment for binary\noutcomes with side information and logarithmic loss, where regret---or,\nredundancy---is measured with respect to a (possibly infinite) class of\nexperts. We provide upper and lower bounds for minimax regret in terms of\nsequential complexities of the class. These complexities were recently shown to\ngive matching (up to logarithmic factors) upper and lower bounds for sequential\nprediction with general convex Lipschitz loss functions (Rakhlin and Sridharan,\n2015). To deal with unbounded gradients of the logarithmic loss, we present a\nnew analysis that employs a sequential chaining technique with a Bernstein-type\nbound. The introduced complexities are intrinsic to the problem of sequential\nprobability assignment, as illustrated by our lower bound.\n  We also consider an example of a large class of experts parametrized by\nvectors in a high-dimensional Euclidean ball (or a Hilbert ball). The typical\ndiscretization approach fails, while our techniques give a non-trivial bound.\nFor this problem we also present an algorithm based on regularization with a\nself-concordant barrier. This algorithm is of an independent interest, as it\nrequires a bound on the function values rather than gradients.\n", "versions": [{"version": "v1", "created": "Thu, 29 Jan 2015 04:02:39 GMT"}], "update_date": "2015-01-30", "authors_parsed": [["Rakhlin", "Alexander", ""], ["Sridharan", "Karthik", ""]]}, {"id": "1501.07399", "submitter": "Joan Serr\\`a", "authors": "Joan Serr\\`a and Josep Lluis Arcos", "title": "Particle swarm optimization for time series motif discovery", "comments": "12 pages, 9 figures, 2 tables", "journal-ref": "Knowledge-Based Systems 92: 127-137. Jan 2016", "doi": "10.1016/j.knosys.2015.10.021", "report-no": null, "categories": "cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Efficiently finding similar segments or motifs in time series data is a\nfundamental task that, due to the ubiquity of these data, is present in a wide\nrange of domains and situations. Because of this, countless solutions have been\ndevised but, to date, none of them seems to be fully satisfactory and flexible.\nIn this article, we propose an innovative standpoint and present a solution\ncoming from it: an anytime multimodal optimization algorithm for time series\nmotif discovery based on particle swarms. By considering data from a variety of\ndomains, we show that this solution is extremely competitive when compared to\nthe state-of-the-art, obtaining comparable motifs in considerably less time\nusing minimal memory. In addition, we show that it is robust to different\nimplementation choices and see that it offers an unprecedented degree of\nflexibility with regard to the task. All these qualities make the presented\nsolution stand out as one of the most prominent candidates for motif discovery\nin long time series streams. Besides, we believe the proposed standpoint can be\nexploited in further time series analysis and mining tasks, widening the scope\nof research and potentially yielding novel effective solutions.\n", "versions": [{"version": "v1", "created": "Thu, 29 Jan 2015 10:18:46 GMT"}], "update_date": "2016-05-18", "authors_parsed": [["Serr\u00e0", "Joan", ""], ["Arcos", "Josep Lluis", ""]]}, {"id": "1501.07430", "submitter": "Seungjin Choi", "authors": "Juho Lee and Seungjin Choi", "title": "Bayesian Hierarchical Clustering with Exponential Family: Small-Variance\n  Asymptotics and Reducibility", "comments": "10 pages, 2 figures, AISTATS-2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bayesian hierarchical clustering (BHC) is an agglomerative clustering method,\nwhere a probabilistic model is defined and its marginal likelihoods are\nevaluated to decide which clusters to merge. While BHC provides a few\nadvantages over traditional distance-based agglomerative clustering algorithms,\nsuccessive evaluation of marginal likelihoods and careful hyperparameter tuning\nare cumbersome and limit the scalability. In this paper we relax BHC into a\nnon-probabilistic formulation, exploring small-variance asymptotics in\nconjugate-exponential models. We develop a novel clustering algorithm, referred\nto as relaxed BHC (RBHC), from the asymptotic limit of the BHC model that\nexhibits the scalability of distance-based agglomerative clustering algorithms\nas well as the flexibility of Bayesian nonparametric models. We also\ninvestigate the reducibility of the dissimilarity measure emerged from the\nasymptotic limit of the BHC model, allowing us to use scalable algorithms such\nas the nearest neighbor chain algorithm. Numerical experiments on both\nsynthetic and real-world datasets demonstrate the validity and high performance\nof our method.\n", "versions": [{"version": "v1", "created": "Thu, 29 Jan 2015 12:13:01 GMT"}, {"version": "v2", "created": "Wed, 3 Jun 2015 00:45:09 GMT"}], "update_date": "2015-06-04", "authors_parsed": [["Lee", "Juho", ""], ["Choi", "Seungjin", ""]]}, {"id": "1501.07467", "submitter": "Hamed Zamani", "authors": "Hamed Zamani, Azadeh Shakery, Pooya Moradi", "title": "Regression and Learning to Rank Aggregation for User Engagement\n  Evaluation", "comments": "In Proceedings of the 2014 ACM Recommender Systems Challenge,\n  RecSysChallenge '14", "journal-ref": null, "doi": "10.1145/2668067.2668077", "report-no": null, "categories": "cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  User engagement refers to the amount of interaction an instance (e.g., tweet,\nnews, and forum post) achieves. Ranking the items in social media websites\nbased on the amount of user participation in them, can be used in different\napplications, such as recommender systems. In this paper, we consider a tweet\ncontaining a rating for a movie as an instance and focus on ranking the\ninstances of each user based on their engagement, i.e., the total number of\nretweets and favorites it will gain.\n  For this task, we define several features which can be extracted from the\nmeta-data of each tweet. The features are partitioned into three categories:\nuser-based, movie-based, and tweet-based. We show that in order to obtain good\nresults, features from all categories should be considered. We exploit\nregression and learning to rank methods to rank the tweets and propose to\naggregate the results of regression and learning to rank methods to achieve\nbetter performance. We have run our experiments on an extended version of\nMovieTweeting dataset provided by ACM RecSys Challenge 2014. The results show\nthat learning to rank approach outperforms most of the regression models and\nthe combination can improve the performance significantly.\n", "versions": [{"version": "v1", "created": "Thu, 29 Jan 2015 14:54:12 GMT"}], "update_date": "2015-01-30", "authors_parsed": [["Zamani", "Hamed", ""], ["Shakery", "Azadeh", ""], ["Moradi", "Pooya", ""]]}, {"id": "1501.07584", "submitter": "Bo-Wei Chen", "authors": "Qi Guo, Bo-Wei Chen, Feng Jiang, Xiangyang Ji, and Sun-Yuan Kung", "title": "Efficient Divide-And-Conquer Classification Based on Feature-Space\n  Decomposition", "comments": "5 pages", "journal-ref": null, "doi": "10.1109/JSYST.2015.2478800", "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This study presents a divide-and-conquer (DC) approach based on feature space\ndecomposition for classification. When large-scale datasets are present,\ntypical approaches usually employed truncated kernel methods on the feature\nspace or DC approaches on the sample space. However, this did not guarantee\nseparability between classes, owing to overfitting. To overcome such problems,\nthis work proposes a novel DC approach on feature spaces consisting of three\nsteps. Firstly, we divide the feature space into several subspaces using the\ndecomposition method proposed in this paper. Subsequently, these feature\nsubspaces are sent into individual local classifiers for training. Finally, the\noutcomes of local classifiers are fused together to generate the final\nclassification results. Experiments on large-scale datasets are carried out for\nperformance evaluation. The results show that the error rates of the proposed\nDC method decreased comparing with the state-of-the-art fast SVM solvers, e.g.,\nreducing error rates by 10.53% and 7.53% on RCV1 and covtype datasets\nrespectively.\n", "versions": [{"version": "v1", "created": "Thu, 29 Jan 2015 20:41:29 GMT"}], "update_date": "2018-07-30", "authors_parsed": [["Guo", "Qi", ""], ["Chen", "Bo-Wei", ""], ["Jiang", "Feng", ""], ["Ji", "Xiangyang", ""], ["Kung", "Sun-Yuan", ""]]}, {"id": "1501.07627", "submitter": "Steve Gallant", "authors": "Stephen I. Gallant and T. Wendy Okaywe", "title": "Representing Objects, Relations, and Sequences", "comments": "41 pages", "journal-ref": "Neural Computation 25, 2038-2078 (August 2013)", "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Vector Symbolic Architectures (VSAs) are high-dimensional vector\nrepresentations of objects (eg., words, image parts), relations (eg., sentence\nstructures), and sequences for use with machine learning algorithms. They\nconsist of a vector addition operator for representing a collection of\nunordered objects, a Binding operator for associating groups of objects, and a\nmethodology for encoding complex structures.\n  We first develop Constraints that machine learning imposes upon VSAs: for\nexample, similar structures must be represented by similar vectors. The\nconstraints suggest that current VSAs should represent phrases (\"The smart\nBrazilian girl\") by binding sums of terms, in addition to simply binding the\nterms directly.\n  We show that matrix multiplication can be used as the binding operator for a\nVSA, and that matrix elements can be chosen at random. A consequence for living\nsystems is that binding is mathematically possible without the need to specify,\nin advance, precise neuron-to-neuron connection properties for large numbers of\nsynapses.\n  A VSA that incorporates these ideas, MBAT (Matrix Binding of Additive Terms),\nis described that satisfies all Constraints.\n  With respect to machine learning, for some types of problems appropriate VSA\nrepresentations permit us to prove learnability, rather than relying on\nsimulations. We also propose dividing machine (and neural) learning and\nrepresentation into three Stages, with differing roles for learning in each\nstage.\n  For neural modeling, we give \"representational reasons\" for nervous systems\nto have many recurrent connections, as well as for the importance of phrases in\nlanguage processing.\n  Sizing simulations and analyses suggest that VSAs in general, and MBAT in\nparticular, are ready for real-world applications.\n", "versions": [{"version": "v1", "created": "Thu, 29 Jan 2015 22:13:02 GMT"}], "update_date": "2015-02-02", "authors_parsed": [["Gallant", "Stephen I.", ""], ["Okaywe", "T. Wendy", ""]]}, {"id": "1501.07645", "submitter": "Sachin Talathi", "authors": "Sachin S. Talathi", "title": "Hyper-parameter optimization of Deep Convolutional Networks for object\n  recognition", "comments": "4 pages, 1 figure, 3 tables, Submitted to ICIP 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently sequential model based optimization (SMBO) has emerged as a\npromising hyper-parameter optimization strategy in machine learning. In this\nwork, we investigate SMBO to identify architecture hyper-parameters of deep\nconvolution networks (DCNs) object recognition. We propose a simple SMBO\nstrategy that starts from a set of random initial DCN architectures to generate\nnew architectures, which on training perform well on a given dataset. Using the\nproposed SMBO strategy we are able to identify a number of DCN architectures\nthat produce results that are comparable to state-of-the-art results on object\nrecognition benchmarks.\n", "versions": [{"version": "v1", "created": "Fri, 30 Jan 2015 02:08:51 GMT"}, {"version": "v2", "created": "Sun, 17 May 2015 03:32:22 GMT"}], "update_date": "2015-05-19", "authors_parsed": [["Talathi", "Sachin S.", ""]]}]