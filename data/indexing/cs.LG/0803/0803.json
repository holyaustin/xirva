[{"id": "0803.0924", "submitter": "Shiva Kasiviswanathan", "authors": "Shiva Prasad Kasiviswanathan, Homin K. Lee, Kobbi Nissim, Sofya\n  Raskhodnikova, and Adam Smith", "title": "What Can We Learn Privately?", "comments": "35 pages, 2 figures", "journal-ref": "SIAM Journal of Computing 40(3) (2011) 793-826", "doi": null, "report-no": null, "categories": "cs.LG cs.CC cs.CR cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning problems form an important category of computational tasks that\ngeneralizes many of the computations researchers apply to large real-life data\nsets. We ask: what concept classes can be learned privately, namely, by an\nalgorithm whose output does not depend too heavily on any one input or specific\ntraining example? More precisely, we investigate learning algorithms that\nsatisfy differential privacy, a notion that provides strong confidentiality\nguarantees in contexts where aggregate information is released about a database\ncontaining sensitive information about individuals. We demonstrate that,\nignoring computational constraints, it is possible to privately agnostically\nlearn any concept class using a sample size approximately logarithmic in the\ncardinality of the concept class. Therefore, almost anything learnable is\nlearnable privately: specifically, if a concept class is learnable by a\n(non-private) algorithm with polynomial sample complexity and output size, then\nit can be learned privately using a polynomial number of samples. We also\npresent a computationally efficient private PAC learner for the class of parity\nfunctions. Local (or randomized response) algorithms are a practical class of\nprivate algorithms that have received extensive investigation. We provide a\nprecise characterization of local private learning algorithms. We show that a\nconcept class is learnable by a local algorithm if and only if it is learnable\nin the statistical query (SQ) model. Finally, we present a separation between\nthe power of interactive and noninteractive local learning algorithms.\n", "versions": [{"version": "v1", "created": "Thu, 6 Mar 2008 17:50:07 GMT"}, {"version": "v2", "created": "Mon, 14 Apr 2008 16:18:44 GMT"}, {"version": "v3", "created": "Fri, 19 Feb 2010 01:47:02 GMT"}], "update_date": "2012-10-10", "authors_parsed": [["Kasiviswanathan", "Shiva Prasad", ""], ["Lee", "Homin K.", ""], ["Nissim", "Kobbi", ""], ["Raskhodnikova", "Sofya", ""], ["Smith", "Adam", ""]]}, {"id": "0803.1555", "submitter": "Bart Moelans", "authors": "Bart Kuijpers, Vanessa Lemmens, Bart Moelans and Karl Tuyls", "title": "Privacy Preserving ID3 over Horizontally, Vertically and Grid\n  Partitioned Data", "comments": "25 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider privacy preserving decision tree induction via ID3 in the case\nwhere the training data is horizontally or vertically distributed. Furthermore,\nwe consider the same problem in the case where the data is both horizontally\nand vertically distributed, a situation we refer to as grid partitioned data.\nWe give an algorithm for privacy preserving ID3 over horizontally partitioned\ndata involving more than two parties. For grid partitioned data, we discuss two\ndifferent evaluation methods for preserving privacy ID3, namely, first merging\nhorizontally and developing vertically or first merging vertically and next\ndeveloping horizontally. Next to introducing privacy preserving data mining\nover grid-partitioned data, the main contribution of this paper is that we\nshow, by means of a complexity analysis that the former evaluation method is\nthe more efficient.\n", "versions": [{"version": "v1", "created": "Tue, 11 Mar 2008 11:18:52 GMT"}], "update_date": "2008-12-18", "authors_parsed": [["Kuijpers", "Bart", ""], ["Lemmens", "Vanessa", ""], ["Moelans", "Bart", ""], ["Tuyls", "Karl", ""]]}, {"id": "0803.2856", "submitter": "Christoph Schommer", "authors": "T. Rothenberger, S. Oez, E. Tahirovic, C. Schommer", "title": "Figuring out Actors in Text Streams: Using Collocations to establish\n  Incremental Mind-maps", "comments": "10 pages, 3 Figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The recognition, involvement, and description of main actors influences the\nstory line of the whole text. This is of higher importance as the text per se\nrepresents a flow of words and expressions that once it is read it is lost. In\nthis respect, the understanding of a text and moreover on how the actor exactly\nbehaves is not only a major concern: as human beings try to store a given input\non short-term memory while associating diverse aspects and actors with\nincidents, the following approach represents a virtual architecture, where\ncollocations are concerned and taken as the associative completion of the\nactors' acting. Once that collocations are discovered, they become managed in\nseparated memory blocks broken down by the actors. As for human beings, the\nmemory blocks refer to associative mind-maps. We then present several priority\nfunctions to represent the actual temporal situation inside a mind-map to\nenable the user to reconstruct the recent events from the discovered temporal\nresults.\n", "versions": [{"version": "v1", "created": "Wed, 19 Mar 2008 18:00:19 GMT"}], "update_date": "2008-12-18", "authors_parsed": [["Rothenberger", "T.", ""], ["Oez", "S.", ""], ["Tahirovic", "E.", ""], ["Schommer", "C.", ""]]}, {"id": "0803.3490", "submitter": "Huan Xu Mr.", "authors": "Huan Xu, Constantine Caramanis and Shie Mannor", "title": "Robustness and Regularization of Support Vector Machines", "comments": null, "journal-ref": "Journal of Machine Learning Research, vol 10, 1485-1510, year 2009", "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider regularized support vector machines (SVMs) and show that they are\nprecisely equivalent to a new robust optimization formulation. We show that\nthis equivalence of robust optimization and regularization has implications for\nboth algorithms, and analysis. In terms of algorithms, the equivalence suggests\nmore general SVM-like algorithms for classification that explicitly build in\nprotection to noise, and at the same time control overfitting. On the analysis\nfront, the equivalence of robustness and regularization, provides a robust\noptimization interpretation for the success of regularized SVMs. We use the\nthis new robustness interpretation of SVMs to give a new proof of consistency\nof (kernelized) SVMs, thus establishing robustness as the reason regularized\nSVMs generalize well.\n", "versions": [{"version": "v1", "created": "Tue, 25 Mar 2008 03:51:59 GMT"}, {"version": "v2", "created": "Tue, 11 Nov 2008 22:36:47 GMT"}], "update_date": "2010-02-25", "authors_parsed": [["Xu", "Huan", ""], ["Caramanis", "Constantine", ""], ["Mannor", "Shie", ""]]}, {"id": "0803.3838", "submitter": "Ted Dunning", "authors": "Ted Dunning", "title": "Recorded Step Directional Mutation for Faster Convergence", "comments": "15 pages, 4 figures, presented at EP-98", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.LG", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  Two meta-evolutionary optimization strategies described in this paper\naccelerate the convergence of evolutionary programming algorithms while still\nretaining much of their ability to deal with multi-modal problems. The\nstrategies, called directional mutation and recorded step in this paper, can\noperate independently but together they greatly enhance the ability of\nevolutionary programming algorithms to deal with fitness landscapes\ncharacterized by long narrow valleys. The directional mutation aspect of this\ncombined method uses correlated meta-mutation but does not introduce a full\ncovariance matrix. These new methods are thus much more economical in terms of\nstorage for problems with high dimensionality. Additionally, directional\nmutation is rotationally invariant which is a substantial advantage over\nself-adaptive methods which use a single variance per coordinate for problems\nwhere the natural orientation of the problem is not oriented along the axes.\n", "versions": [{"version": "v1", "created": "Wed, 26 Mar 2008 22:49:40 GMT"}, {"version": "v2", "created": "Thu, 26 Mar 2009 20:37:59 GMT"}], "update_date": "2009-03-26", "authors_parsed": [["Dunning", "Ted", ""]]}]