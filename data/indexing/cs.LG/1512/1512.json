[{"id": "1512.00001", "submitter": "Stan Hatko", "authors": "Stan Hatko", "title": "k-Nearest Neighbour Classification of Datasets with a Family of\n  Distances", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The $k$-nearest neighbour ($k$-NN) classifier is one of the oldest and most\nimportant supervised learning algorithms for classifying datasets.\nTraditionally the Euclidean norm is used as the distance for the $k$-NN\nclassifier. In this thesis we investigate the use of alternative distances for\nthe $k$-NN classifier.\n  We start by introducing some background notions in statistical machine\nlearning. We define the $k$-NN classifier and discuss Stone's theorem and the\nproof that $k$-NN is universally consistent on the normed space $R^d$. We then\nprove that $k$-NN is universally consistent if we take a sequence of random\nnorms (that are independent of the sample and the query) from a family of norms\nthat satisfies a particular boundedness condition. We extend this result by\nreplacing norms with distances based on uniformly locally Lipschitz functions\nthat satisfy certain conditions. We discuss the limitations of Stone's lemma\nand Stone's theorem, particularly with respect to quasinorms and adaptively\nchoosing a distance for $k$-NN based on the labelled sample. We show the\nuniversal consistency of a two stage $k$-NN type classifier where we select the\ndistance adaptively based on a split labelled sample and the query. We conclude\nby giving some examples of improvements of the accuracy of classifying various\ndatasets using the above techniques.\n", "versions": [{"version": "v1", "created": "Sun, 29 Nov 2015 01:52:34 GMT"}], "update_date": "2015-12-02", "authors_parsed": [["Hatko", "Stan", ""]]}, {"id": "1512.00077", "submitter": "Gabriele Farina", "authors": "Massimo Cairo, Gabriele Farina, Romeo Rizzi", "title": "Decoding Hidden Markov Models Faster Than Viterbi Via Online\n  Matrix-Vector (max, +)-Multiplication", "comments": "AAAI 2016, to appear", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS cs.IT math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present a novel algorithm for the maximum a posteriori\ndecoding (MAPD) of time-homogeneous Hidden Markov Models (HMM), improving the\nworst-case running time of the classical Viterbi algorithm by a logarithmic\nfactor. In our approach, we interpret the Viterbi algorithm as a repeated\ncomputation of matrix-vector $(\\max, +)$-multiplications. On time-homogeneous\nHMMs, this computation is online: a matrix, known in advance, has to be\nmultiplied with several vectors revealed one at a time. Our main contribution\nis an algorithm solving this version of matrix-vector $(\\max,+)$-multiplication\nin subquadratic time, by performing a polynomial preprocessing of the matrix.\nEmploying this fast multiplication algorithm, we solve the MAPD problem in\n$O(mn^2/ \\log n)$ time for any time-homogeneous HMM of size $n$ and observation\nsequence of length $m$, with an extra polynomial preprocessing cost negligible\nfor $m > n$. To the best of our knowledge, this is the first algorithm for the\nMAPD problem requiring subquadratic time per observation, under the only\nassumption -- usually verified in practice -- that the transition probability\nmatrix does not change with time.\n", "versions": [{"version": "v1", "created": "Mon, 30 Nov 2015 22:38:07 GMT"}, {"version": "v2", "created": "Fri, 11 Dec 2015 10:40:51 GMT"}], "update_date": "2015-12-14", "authors_parsed": [["Cairo", "Massimo", ""], ["Farina", "Gabriele", ""], ["Rizzi", "Romeo", ""]]}, {"id": "1512.00165", "submitter": "Galit Bary", "authors": "Galit Bary", "title": "Learning Using 1-Local Membership Queries", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Classic machine learning algorithms learn from labelled examples. For\nexample, to design a machine translation system, a typical training set will\nconsist of English sentences and their translation. There is a stronger model,\nin which the algorithm can also query for labels of new examples it creates.\nE.g, in the translation task, the algorithm can create a new English sentence,\nand request its translation from the user during training. This combination of\nexamples and queries has been widely studied. Yet, despite many theoretical\nresults, query algorithms are almost never used. One of the main causes for\nthis is a report (Baum and Lang, 1992) on very disappointing empirical\nperformance of a query algorithm. These poor results were mainly attributed to\nthe fact that the algorithm queried for labels of examples that are artificial,\nand impossible to interpret by humans.\n  In this work we study a new model of local membership queries (Awasthi et\nal., 2012), which tries to resolve the problem of artificial queries. In this\nmodel, the algorithm is only allowed to query the labels of examples which are\nclose to examples from the training set. E.g., in translation, the algorithm\ncan change individual words in a sentence it has already seen, and then ask for\nthe translation. In this model, the examples queried by the algorithm will be\nclose to natural examples and hence, hopefully, will not appear as artificial\nor random. We focus on 1-local queries (i.e., queries of distance 1 from an\nexample in the training sample). We show that 1-local membership queries are\nalready stronger than the standard learning model. We also present an\nexperiment on a well known NLP task of sentiment analysis. In this experiment,\nthe users were asked to provide more information than merely indicating the\nlabel. We present results that illustrate that this extra information is\nbeneficial in practice.\n", "versions": [{"version": "v1", "created": "Tue, 1 Dec 2015 07:40:49 GMT"}], "update_date": "2015-12-02", "authors_parsed": [["Bary", "Galit", ""]]}, {"id": "1512.00228", "submitter": "Yunior Ram\\'irez-Cruz", "authors": "Henry Rosales-M\\'endez, Yunior Ram\\'irez-Cruz", "title": "MOCICE-BCubed F$_1$: A New Evaluation Measure for Biclustering\n  Algorithms", "comments": null, "journal-ref": "Pattern Recognition Letters 84 (2016) 142-148", "doi": "10.1016/j.patrec.2016.09.002", "report-no": null, "categories": "cs.LG cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The validation of biclustering algorithms remains a challenging task, even\nthough a number of measures have been proposed for evaluating the quality of\nthese algorithms. Although no criterion is universally accepted as the overall\nbest, a number of meta-evaluation conditions to be satisfied by biclustering\nalgorithms have been enunciated. In this work, we present MOCICE-BCubed F$_1$,\na new external measure for evaluating biclusterings, in the scenario where gold\nstandard annotations are available for both the object clusters and the\nassociated feature subspaces. Our proposal relies on the so-called\nmicro-objects transformation and satisfies the most comprehensive set of\nmeta-evaluation conditions so far enunciated for biclusterings. Additionally,\nthe proposed measure adequately handles the occurrence of overlapping in both\nthe object and feature spaces. Moreover, when used for evaluating traditional\nclusterings, which are viewed as a particular case of biclustering, the\nproposed measure also satisfies the most comprehensive set of meta-evaluation\nconditions so far enunciated for this task.\n", "versions": [{"version": "v1", "created": "Tue, 1 Dec 2015 11:26:50 GMT"}, {"version": "v2", "created": "Thu, 21 Jan 2016 11:35:22 GMT"}], "update_date": "2017-04-21", "authors_parsed": [["Rosales-M\u00e9ndez", "Henry", ""], ["Ram\u00edrez-Cruz", "Yunior", ""]]}, {"id": "1512.00242", "submitter": "Haibing Wu", "authors": "Haibing Wu and Xiaodong Gu", "title": "Towards Dropout Training for Convolutional Neural Networks", "comments": "This paper has been published in Neural Networks,\n  http://www.sciencedirect.com/science/article/pii/S0893608015001446", "journal-ref": "Neural Networks 71: 1-10 (2015)", "doi": "10.1016/j.neunet.2015.07.007", "report-no": null, "categories": "cs.LG cs.CV cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, dropout has seen increasing use in deep learning. For deep\nconvolutional neural networks, dropout is known to work well in fully-connected\nlayers. However, its effect in convolutional and pooling layers is still not\nclear. This paper demonstrates that max-pooling dropout is equivalent to\nrandomly picking activation based on a multinomial distribution at training\ntime. In light of this insight, we advocate employing our proposed\nprobabilistic weighted pooling, instead of commonly used max-pooling, to act as\nmodel averaging at test time. Empirical evidence validates the superiority of\nprobabilistic weighted pooling. We also empirically show that the effect of\nconvolutional dropout is not trivial, despite the dramatically reduced\npossibility of over-fitting due to the convolutional architecture. Elaborately\ndesigning dropout training simultaneously in max-pooling and fully-connected\nlayers, we achieve state-of-the-art performance on MNIST, and very competitive\nresults on CIFAR-10 and CIFAR-100, relative to other approaches without data\naugmentation. Finally, we compare max-pooling dropout and stochastic pooling,\nboth of which introduce stochasticity based on multinomial distributions at\npooling stage.\n", "versions": [{"version": "v1", "created": "Tue, 1 Dec 2015 12:46:11 GMT"}], "update_date": "2015-12-02", "authors_parsed": [["Wu", "Haibing", ""], ["Gu", "Xiaodong", ""]]}, {"id": "1512.00297", "submitter": "Lucas Lacasa", "authors": "Jacopo Iacovacci, Lucas Lacasa", "title": "Sequential visibility-graph motifs", "comments": null, "journal-ref": "Physical Review E 93, 042309 (2016)", "doi": "10.1103/PhysRevE.93.042309", "report-no": null, "categories": "physics.data-an cs.LG nlin.CD", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Visibility algorithms transform time series into graphs and encode dynamical\ninformation in their topology, paving the way for graph-theoretical time series\nanalysis as well as building a bridge between nonlinear dynamics and network\nscience. In this work we introduce and study the concept of sequential\nvisibility graph motifs, smaller substructures of n consecutive nodes that\nappear with characteristic frequencies. We develop a theory to compute in an\nexact way the motif profiles associated to general classes of deterministic and\nstochastic dynamics. We find that this simple property is indeed a highly\ninformative and computationally efficient feature capable to distinguish among\ndifferent dynamics and robust against noise contamination. We finally confirm\nthat it can be used in practice to perform unsupervised learning, by extracting\nmotif profiles from experimental heart-rate series and being able, accordingly,\nto disentangle meditative from other relaxation states. Applications of this\ngeneral theory include the automatic classification and description of\nphysical, biological, and financial time series.\n", "versions": [{"version": "v1", "created": "Tue, 1 Dec 2015 15:35:56 GMT"}, {"version": "v2", "created": "Mon, 2 May 2016 17:22:21 GMT"}], "update_date": "2016-05-11", "authors_parsed": [["Iacovacci", "Jacopo", ""], ["Lacasa", "Lucas", ""]]}, {"id": "1512.00355", "submitter": "Amrita Saha", "authors": "Amrita Saha, Sathish Indurthi, Shantanu Godbole, Subendhu Rongali and\n  Vikas C. Raykar", "title": "Taxonomy grounded aggregation of classifiers with different label sets", "comments": "Under review by AISTATS 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe the problem of aggregating the label predictions of diverse\nclassifiers using a class taxonomy. Such a taxonomy may not have been available\nor referenced when the individual classifiers were designed and trained, yet\nmapping the output labels into the taxonomy is desirable to integrate the\neffort spent in training the constituent classifiers. A hierarchical taxonomy\nrepresenting some domain knowledge may be different from, but partially\nmappable to, the label sets of the individual classifiers. We present a\nheuristic approach and a principled graphical model to aggregate the label\npredictions by grounding them into the available taxonomy. Our model aggregates\nthe labels using the taxonomy structure as constraints to find the most likely\nhierarchically consistent class. We experimentally validate our proposed method\non image and text classification tasks.\n", "versions": [{"version": "v1", "created": "Tue, 1 Dec 2015 17:32:16 GMT"}], "update_date": "2015-12-02", "authors_parsed": [["Saha", "Amrita", ""], ["Indurthi", "Sathish", ""], ["Godbole", "Shantanu", ""], ["Rongali", "Subendhu", ""], ["Raykar", "Vikas C.", ""]]}, {"id": "1512.00408", "submitter": "Frederik Ruelens", "authors": "Frederik Ruelens, Bert Claessens, Salman Quaiyum, Bart De Schutter,\n  Robert Babuska, and Ronnie Belmans", "title": "Reinforcement Learning Applied to an Electric Water Heater: From Theory\n  to Practice", "comments": "Submitted to IEEE transaction on smart grid", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Electric water heaters have the ability to store energy in their water buffer\nwithout impacting the comfort of the end user. This feature makes them a prime\ncandidate for residential demand response. However, the stochastic and\nnonlinear dynamics of electric water heaters, makes it challenging to harness\ntheir flexibility. Driven by this challenge, this paper formulates the\nunderlying sequential decision-making problem as a Markov decision process and\nuses techniques from reinforcement learning. Specifically, we apply an\nauto-encoder network to find a compact feature representation of the sensor\nmeasurements, which helps to mitigate the curse of dimensionality. A wellknown\nbatch reinforcement learning technique, fitted Q-iteration, is used to find a\ncontrol policy, given this feature representation. In a simulation-based\nexperiment using an electric water heater with 50 temperature sensors, the\nproposed method was able to achieve good policies much faster than when using\nthe full state information. In a lab experiment, we apply fitted Q-iteration to\nan electric water heater with eight temperature sensors. Further reducing the\nstate vector did not improve the results of fitted Q-iteration. The results of\nthe lab experiment, spanning 40 days, indicate that compared to a thermostat\ncontroller, the presented approach was able to reduce the total cost of energy\nconsumption of the electric water heater by 15%.\n", "versions": [{"version": "v1", "created": "Sun, 29 Nov 2015 18:03:13 GMT"}], "update_date": "2015-12-02", "authors_parsed": [["Ruelens", "Frederik", ""], ["Claessens", "Bert", ""], ["Quaiyum", "Salman", ""], ["De Schutter", "Bart", ""], ["Babuska", "Robert", ""], ["Belmans", "Ronnie", ""]]}, {"id": "1512.00442", "submitter": "Ke Li", "authors": "Ke Li, Jitendra Malik", "title": "Fast k-Nearest Neighbour Search via Dynamic Continuous Indexing", "comments": "13 pages, 6 figures; International Conference on Machine Learning\n  (ICML), 2016. This version corrects a typo in the pseudocode", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.AI cs.IR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Existing methods for retrieving k-nearest neighbours suffer from the curse of\ndimensionality. We argue this is caused in part by inherent deficiencies of\nspace partitioning, which is the underlying strategy used by most existing\nmethods. We devise a new strategy that avoids partitioning the vector space and\npresent a novel randomized algorithm that runs in time linear in dimensionality\nof the space and sub-linear in the intrinsic dimensionality and the size of the\ndataset and takes space constant in dimensionality of the space and linear in\nthe size of the dataset. The proposed algorithm allows fine-grained control\nover accuracy and speed on a per-query basis, automatically adapts to\nvariations in data density, supports dynamic updates to the dataset and is\neasy-to-implement. We show appealing theoretical properties and demonstrate\nempirically that the proposed algorithm outperforms locality-sensitivity\nhashing (LSH) in terms of approximation quality, speed and space efficiency.\n", "versions": [{"version": "v1", "created": "Tue, 1 Dec 2015 20:53:16 GMT"}, {"version": "v2", "created": "Fri, 10 Jun 2016 18:47:10 GMT"}, {"version": "v3", "created": "Thu, 6 Apr 2017 06:51:49 GMT"}], "update_date": "2017-04-07", "authors_parsed": [["Li", "Ke", ""], ["Malik", "Jitendra", ""]]}, {"id": "1512.00486", "submitter": "Maksim Lapin", "authors": "Maksim Lapin, Matthias Hein, Bernt Schiele", "title": "Loss Functions for Top-k Error: Analysis and Insights", "comments": "In Computer Vision and Pattern Recognition (CVPR), 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In order to push the performance on realistic computer vision tasks, the\nnumber of classes in modern benchmark datasets has significantly increased in\nrecent years. This increase in the number of classes comes along with increased\nambiguity between the class labels, raising the question if top-1 error is the\nright performance measure. In this paper, we provide an extensive comparison\nand evaluation of established multiclass methods comparing their top-k\nperformance both from a practical as well as from a theoretical perspective.\nMoreover, we introduce novel top-k loss functions as modifications of the\nsoftmax and the multiclass SVM losses and provide efficient optimization\nschemes for them. In the experiments, we compare on various datasets all of the\nproposed and established methods for top-k error optimization. An interesting\ninsight of this paper is that the softmax loss yields competitive top-k\nperformance for all k simultaneously. For a specific top-k error, our new top-k\nlosses lead typically to further improvements while being faster to train than\nthe softmax.\n", "versions": [{"version": "v1", "created": "Tue, 1 Dec 2015 21:22:35 GMT"}, {"version": "v2", "created": "Wed, 13 Apr 2016 15:12:01 GMT"}], "update_date": "2016-04-14", "authors_parsed": [["Lapin", "Maksim", ""], ["Hein", "Matthias", ""], ["Schiele", "Bernt", ""]]}, {"id": "1512.00570", "submitter": "Xinchen Yan", "authors": "Xinchen Yan, Jimei Yang, Kihyuk Sohn, Honglak Lee", "title": "Attribute2Image: Conditional Image Generation from Visual Attributes", "comments": "19 pages, accepted by ECCV 2016, The 14th European Conference on\n  Computer Vision (2016)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper investigates a novel problem of generating images from visual\nattributes. We model the image as a composite of foreground and background and\ndevelop a layered generative model with disentangled latent variables that can\nbe learned end-to-end using a variational auto-encoder. We experiment with\nnatural images of faces and birds and demonstrate that the proposed models are\ncapable of generating realistic and diverse samples with disentangled latent\nrepresentations. We use a general energy minimization algorithm for posterior\ninference of latent variables given novel images. Therefore, the learned\ngenerative models show excellent quantitative and visual results in the tasks\nof attribute-conditioned image reconstruction and completion.\n", "versions": [{"version": "v1", "created": "Wed, 2 Dec 2015 04:07:28 GMT"}, {"version": "v2", "created": "Sat, 8 Oct 2016 08:55:32 GMT"}], "update_date": "2016-10-11", "authors_parsed": [["Yan", "Xinchen", ""], ["Yang", "Jimei", ""], ["Sohn", "Kihyuk", ""], ["Lee", "Honglak", ""]]}, {"id": "1512.00573", "submitter": "Lawson Wong", "authors": "Lawson L.S. Wong, Thanard Kurutach, Leslie Pack Kaelbling, Tom\\'as\n  Lozano-P\\'erez", "title": "Object-based World Modeling in Semi-Static Environments with Dependent\n  Dirichlet-Process Mixtures", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To accomplish tasks in human-centric indoor environments, robots need to\nrepresent and understand the world in terms of objects and their attributes. We\nrefer to this attribute-based representation as a world model, and consider how\nto acquire it via noisy perception and maintain it over time, as objects are\nadded, changed, and removed in the world. Previous work has framed this as\nmultiple-target tracking problem, where objects are potentially in motion at\nall times. Although this approach is general, it is computationally expensive.\nWe argue that such generality is not needed in typical world modeling tasks,\nwhere objects only change state occasionally. More efficient approaches are\nenabled by restricting ourselves to such semi-static environments.\n  We consider a previously-proposed clustering-based world modeling approach\nthat assumed static environments, and extend it to semi-static domains by\napplying a dependent Dirichlet-process (DDP) mixture model. We derive a novel\nMAP inference algorithm under this model, subject to data association\nconstraints. We demonstrate our approach improves computational performance in\nsemi-static environments.\n", "versions": [{"version": "v1", "created": "Wed, 2 Dec 2015 04:32:02 GMT"}], "update_date": "2015-12-03", "authors_parsed": [["Wong", "Lawson L. S.", ""], ["Kurutach", "Thanard", ""], ["Kaelbling", "Leslie Pack", ""], ["Lozano-P\u00e9rez", "Tom\u00e1s", ""]]}, {"id": "1512.00659", "submitter": "Sanjay Sahay", "authors": "Aruna Govada, Bhavul Gauri and S.K.Sahay", "title": "Centroid Based Binary Tree Structured SVM for Multi Classification", "comments": "Presented in ICACCI, Kochi, India, 2015", "journal-ref": "IEEE Xplore, Advances in Computing, Communications and Informatics\n  (ICACCI), p.258 - 262, 2015", "doi": "10.1109/ICACCI.2015.7275618", "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Support Vector Machines (SVMs) were primarily designed for 2-class\nclassification. But they have been extended for N-class classification also\nbased on the requirement of multiclasses in the practical applications.\nAlthough N-class classification using SVM has considerable research attention,\ngetting minimum number of classifiers at the time of training and testing is\nstill a continuing research. We propose a new algorithm CBTS-SVM (Centroid\nbased Binary Tree Structured SVM) which addresses this issue. In this we build\na binary tree of SVM models based on the similarity of the class labels by\nfinding their distance from the corresponding centroids at the root level. The\nexperimental results demonstrates the comparable accuracy for CBTS with OVO\nwith reasonable gamma and cost values. On the other hand when CBTS is compared\nwith OVA, it gives the better accuracy with reduced training time and testing\ntime. Furthermore CBTS is also scalable as it is able to handle the large data\nsets.\n", "versions": [{"version": "v1", "created": "Wed, 2 Dec 2015 11:48:38 GMT"}], "update_date": "2015-12-03", "authors_parsed": [["Govada", "Aruna", ""], ["Gauri", "Bhavul", ""], ["Sahay", "S. K.", ""]]}, {"id": "1512.00743", "submitter": "Amogh Gudi", "authors": "Amogh Gudi", "title": "Recognizing Semantic Features in Faces using Deep Learning", "comments": "Thesis, M.Sc. Artificial Intelligence, University of Amsterdam, 2014", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The human face constantly conveys information, both consciously and\nsubconsciously. However, as basic as it is for humans to visually interpret\nthis information, it is quite a big challenge for machines. Conventional\nsemantic facial feature recognition and analysis techniques are already in use\nand are based on physiological heuristics, but they suffer from lack of\nrobustness and high computation time. This thesis aims to explore ways for\nmachines to learn to interpret semantic information available in faces in an\nautomated manner without requiring manual design of feature detectors, using\nthe approach of Deep Learning. This thesis provides a study of the effects of\nvarious factors and hyper-parameters of deep neural networks in the process of\ndetermining an optimal network configuration for the task of semantic facial\nfeature recognition. This thesis explores the effectiveness of the system to\nrecognize the various semantic features (like emotions, age, gender, ethnicity\netc.) present in faces. Furthermore, the relation between the effect of\nhigh-level concepts on low level features is explored through an analysis of\nthe similarities in low-level descriptors of different semantic features. This\nthesis also demonstrates a novel idea of using a deep network to generate 3-D\nActive Appearance Models of faces from real-world 2-D images.\n  For a more detailed report on this work, please see [arXiv:1512.00743v1].\n", "versions": [{"version": "v1", "created": "Wed, 2 Dec 2015 15:46:26 GMT"}, {"version": "v2", "created": "Wed, 19 Oct 2016 13:33:44 GMT"}], "update_date": "2016-10-21", "authors_parsed": [["Gudi", "Amogh", ""]]}, {"id": "1512.00818", "submitter": "Mohamed Elhoseiny Mohamed Elhoseiny", "authors": "Mohamed Elhoseiny, Jingen Liu, Hui Cheng, Harpreet Sawhney, Ahmed\n  Elgammal", "title": "Zero-Shot Event Detection by Multimodal Distributional Semantic\n  Embedding of Videos", "comments": "To appear in AAAI 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new zero-shot Event Detection method by Multi-modal\nDistributional Semantic embedding of videos. Our model embeds object and action\nconcepts as well as other available modalities from videos into a\ndistributional semantic space. To our knowledge, this is the first Zero-Shot\nevent detection model that is built on top of distributional semantics and\nextends it in the following directions: (a) semantic embedding of multimodal\ninformation in videos (with focus on the visual modalities), (b) automatically\ndetermining relevance of concepts/attributes to a free text query, which could\nbe useful for other applications, and (c) retrieving videos by free text event\nquery (e.g., \"changing a vehicle tire\") based on their content. We embed videos\ninto a distributional semantic space and then measure the similarity between\nvideos and the event query in a free text form. We validated our method on the\nlarge TRECVID MED (Multimedia Event Detection) challenge. Using only the event\ntitle as a query, our method outperformed the state-of-the-art that uses big\ndescriptions from 12.6% to 13.5% with MAP metric and 0.73 to 0.83 with ROC-AUC\nmetric. It is also an order of magnitude faster.\n", "versions": [{"version": "v1", "created": "Wed, 2 Dec 2015 19:34:00 GMT"}, {"version": "v2", "created": "Wed, 16 Dec 2015 00:58:49 GMT"}], "update_date": "2015-12-17", "authors_parsed": [["Elhoseiny", "Mohamed", ""], ["Liu", "Jingen", ""], ["Cheng", "Hui", ""], ["Sawhney", "Harpreet", ""], ["Elgammal", "Ahmed", ""]]}, {"id": "1512.00843", "submitter": "Sheng Wang", "authors": "Sheng Wang, Jian Peng, Jianzhu Ma and Jinbo Xu", "title": "Protein secondary structure prediction using deep convolutional neural\n  fields", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.BM cs.LG q-bio.QM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Protein secondary structure (SS) prediction is important for studying protein\nstructure and function. When only the sequence (profile) information is used as\ninput feature, currently the best predictors can obtain ~80% Q3 accuracy, which\nhas not been improved in the past decade. Here we present DeepCNF (Deep\nConvolutional Neural Fields) for protein SS prediction. DeepCNF is a Deep\nLearning extension of Conditional Neural Fields (CNF), which is an integration\nof Conditional Random Fields (CRF) and shallow neural networks. DeepCNF can\nmodel not only complex sequence-structure relationship by a deep hierarchical\narchitecture, but also interdependency between adjacent SS labels, so it is\nmuch more powerful than CNF. Experimental results show that DeepCNF can obtain\n~84% Q3 accuracy, ~85% SOV score, and ~72% Q8 accuracy, respectively, on the\nCASP and CAMEO test proteins, greatly outperforming currently popular\npredictors. As a general framework, DeepCNF can be used to predict other\nprotein structure properties such as contact number, disorder regions, and\nsolvent accessibility.\n", "versions": [{"version": "v1", "created": "Wed, 2 Dec 2015 01:53:57 GMT"}, {"version": "v2", "created": "Thu, 10 Dec 2015 17:01:59 GMT"}, {"version": "v3", "created": "Fri, 11 Dec 2015 03:16:55 GMT"}], "update_date": "2015-12-14", "authors_parsed": [["Wang", "Sheng", ""], ["Peng", "Jian", ""], ["Ma", "Jianzhu", ""], ["Xu", "Jinbo", ""]]}, {"id": "1512.00907", "submitter": "Mostafa Rahmani", "authors": "Mostafa Rahmani, George Atia", "title": "Innovation Pursuit: A New Approach to Subspace Clustering", "comments": null, "journal-ref": "IEEE Transactions on Signal Processing ( Volume: 65, Issue: 23,\n  Dec.1, 1 2017 )", "doi": "10.1109/TSP.2017.2749206", "report-no": null, "categories": "cs.CV cs.IR cs.IT cs.LG math.IT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In subspace clustering, a group of data points belonging to a union of\nsubspaces are assigned membership to their respective subspaces. This paper\npresents a new approach dubbed Innovation Pursuit (iPursuit) to the problem of\nsubspace clustering using a new geometrical idea whereby subspaces are\nidentified based on their relative novelties. We present two frameworks in\nwhich the idea of innovation pursuit is used to distinguish the subspaces.\nUnderlying the first framework is an iterative method that finds the subspaces\nconsecutively by solving a series of simple linear optimization problems, each\nsearching for a direction of innovation in the span of the data potentially\northogonal to all subspaces except for the one to be identified in one step of\nthe algorithm. A detailed mathematical analysis is provided establishing\nsufficient conditions for iPursuit to correctly cluster the data. The proposed\napproach can provably yield exact clustering even when the subspaces have\nsignificant intersections. It is shown that the complexity of the iterative\napproach scales only linearly in the number of data points and subspaces, and\nquadratically in the dimension of the subspaces. The second framework\nintegrates iPursuit with spectral clustering to yield a new variant of\nspectral-clustering-based algorithms. The numerical simulations with both real\nand synthetic data demonstrate that iPursuit can often outperform the\nstate-of-the-art subspace clustering algorithms, more so for subspaces with\nsignificant intersections, and that it significantly improves the\nstate-of-the-art result for subspace-segmentation-based face clustering.\n", "versions": [{"version": "v1", "created": "Wed, 2 Dec 2015 23:52:43 GMT"}, {"version": "v2", "created": "Fri, 2 Dec 2016 05:26:58 GMT"}, {"version": "v3", "created": "Wed, 17 May 2017 23:12:17 GMT"}, {"version": "v4", "created": "Wed, 14 Jun 2017 03:29:06 GMT"}, {"version": "v5", "created": "Sun, 26 Nov 2017 15:24:33 GMT"}], "update_date": "2017-11-28", "authors_parsed": [["Rahmani", "Mostafa", ""], ["Atia", "George", ""]]}, {"id": "1512.00965", "submitter": "Pengcheng Yin", "authors": "Pengcheng Yin, Zhengdong Lu, Hang Li, Ben Kao", "title": "Neural Enquirer: Learning to Query Tables with Natural Language", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CL cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We proposed Neural Enquirer as a neural network architecture to execute a\nnatural language (NL) query on a knowledge-base (KB) for answers. Basically,\nNeural Enquirer finds the distributed representation of a query and then\nexecutes it on knowledge-base tables to obtain the answer as one of the values\nin the tables. Unlike similar efforts in end-to-end training of semantic\nparsers, Neural Enquirer is fully \"neuralized\": it not only gives\ndistributional representation of the query and the knowledge-base, but also\nrealizes the execution of compositional queries as a series of differentiable\noperations, with intermediate results (consisting of annotations of the tables\nat different levels) saved on multiple layers of memory. Neural Enquirer can be\ntrained with gradient descent, with which not only the parameters of the\ncontrolling components and semantic parsing component, but also the embeddings\nof the tables and query words can be learned from scratch. The training can be\ndone in an end-to-end fashion, but it can take stronger guidance, e.g., the\nstep-by-step supervision for complicated queries, and benefit from it. Neural\nEnquirer is one step towards building neural network systems which seek to\nunderstand language by executing it on real-world. Our experiments show that\nNeural Enquirer can learn to execute fairly complicated NL queries on tables\nwith rich structures.\n", "versions": [{"version": "v1", "created": "Thu, 3 Dec 2015 06:46:27 GMT"}, {"version": "v2", "created": "Thu, 21 Jan 2016 02:46:25 GMT"}], "update_date": "2016-01-22", "authors_parsed": [["Yin", "Pengcheng", ""], ["Lu", "Zhengdong", ""], ["Li", "Hang", ""], ["Kao", "Ben", ""]]}, {"id": "1512.00984", "submitter": "Quanming Yao", "authors": "Quanming Yao, James T. Kwok, Wenliang Zhong", "title": "Fast Low-Rank Matrix Learning with Nonconvex Regularization", "comments": "Long version of conference paper appeared ICDM 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NA cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Low-rank modeling has a lot of important applications in machine learning,\ncomputer vision and social network analysis. While the matrix rank is often\napproximated by the convex nuclear norm, the use of nonconvex low-rank\nregularizers has demonstrated better recovery performance. However, the\nresultant optimization problem is much more challenging. A very recent\nstate-of-the-art is based on the proximal gradient algorithm. However, it\nrequires an expensive full SVD in each proximal step. In this paper, we show\nthat for many commonly-used nonconvex low-rank regularizers, a cutoff can be\nderived to automatically threshold the singular values obtained from the\nproximal operator. This allows the use of power method to approximate the SVD\nefficiently. Besides, the proximal operator can be reduced to that of a much\nsmaller matrix projected onto this leading subspace. Convergence, with a rate\nof O(1/T) where T is the number of iterations, can be guaranteed. Extensive\nexperiments are performed on matrix completion and robust principal component\nanalysis. The proposed method achieves significant speedup over the\nstate-of-the-art. Moreover, the matrix solution obtained is more accurate and\nhas a lower rank than that of the traditional nuclear norm regularizer.\n", "versions": [{"version": "v1", "created": "Thu, 3 Dec 2015 08:32:17 GMT"}], "update_date": "2016-05-02", "authors_parsed": [["Yao", "Quanming", ""], ["Kwok", "James T.", ""], ["Zhong", "Wenliang", ""]]}, {"id": "1512.00994", "submitter": "Xinggang Wang", "authors": "Hanqiang Song and Zhuotun Zhu and Xinggang Wang", "title": "Bag Reference Vector for Multi-instance Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multi-instance learning (MIL) has a wide range of applications due to its\ndistinctive characteristics. Although many state-of-the-art algorithms have\nachieved decent performances, a plurality of existing methods solve the problem\nonly in instance level rather than excavating relations among bags. In this\npaper, we propose an efficient algorithm to describe each bag by a\ncorresponding feature vector via comparing it with other bags. In other words,\nthe crucial information of a bag is extracted from the similarity between that\nbag and other reference bags. In addition, we apply extensions of Hausdorff\ndistance to representing the similarity, to a certain extent, overcoming the\nkey challenge of MIL problem, the ambiguity of instances' labels in positive\nbags. Experimental results on benchmarks and text categorization tasks show\nthat the proposed method outperforms the previous state-of-the-art by a large\nmargin.\n", "versions": [{"version": "v1", "created": "Thu, 3 Dec 2015 09:03:05 GMT"}], "update_date": "2015-12-04", "authors_parsed": [["Song", "Hanqiang", ""], ["Zhu", "Zhuotun", ""], ["Wang", "Xinggang", ""]]}, {"id": "1512.01110", "submitter": "Yang Song", "authors": "Yang Song, Jun Zhu", "title": "Bayesian Matrix Completion via Adaptive Relaxed Spectral Regularization", "comments": "Accepted to AAAI 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NA cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bayesian matrix completion has been studied based on a low-rank matrix\nfactorization formulation with promising results. However, little work has been\ndone on Bayesian matrix completion based on the more direct spectral\nregularization formulation. We fill this gap by presenting a novel Bayesian\nmatrix completion method based on spectral regularization. In order to\ncircumvent the difficulties of dealing with the orthonormality constraints of\nsingular vectors, we derive a new equivalent form with relaxed constraints,\nwhich then leads us to design an adaptive version of spectral regularization\nfeasible for Bayesian inference. Our Bayesian method requires no parameter\ntuning and can infer the number of latent factors automatically. Experiments on\nsynthetic and real datasets demonstrate encouraging results on rank recovery\nand collaborative filtering, with notably good results for very sparse\nmatrices.\n", "versions": [{"version": "v1", "created": "Thu, 3 Dec 2015 15:16:19 GMT"}, {"version": "v2", "created": "Fri, 25 Dec 2015 02:51:22 GMT"}], "update_date": "2016-05-31", "authors_parsed": [["Song", "Yang", ""], ["Zhu", "Jun", ""]]}, {"id": "1512.01124", "submitter": "Peter Sunehag", "authors": "Peter Sunehag, Richard Evans, Gabriel Dulac-Arnold, Yori Zwols, Daniel\n  Visentin and Ben Coppin", "title": "Deep Reinforcement Learning with Attention for Slate Markov Decision\n  Processes with High-Dimensional States and Actions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.HC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many real-world problems come with action spaces represented as feature\nvectors. Although high-dimensional control is a largely unsolved problem, there\nhas recently been progress for modest dimensionalities. Here we report on a\nsuccessful attempt at addressing problems of dimensionality as high as $2000$,\nof a particular form. Motivated by important applications such as\nrecommendation systems that do not fit the standard reinforcement learning\nframeworks, we introduce Slate Markov Decision Processes (slate-MDPs). A\nSlate-MDP is an MDP with a combinatorial action space consisting of slates\n(tuples) of primitive actions of which one is executed in an underlying MDP.\nThe agent does not control the choice of this executed action and the action\nmight not even be from the slate, e.g., for recommendation systems for which\nall recommendations can be ignored. We use deep Q-learning based on feature\nrepresentations of both the state and action to learn the value of whole\nslates. Unlike existing methods, we optimize for both the combinatorial and\nsequential aspects of our tasks. The new agent's superiority over agents that\neither ignore the combinatorial or sequential long-term value aspect is\ndemonstrated on a range of environments with dynamics from a real-world\nrecommendation system. Further, we use deep deterministic policy gradients to\nlearn a policy that for each position of the slate, guides attention towards\nthe part of the action space in which the value is the highest and we only\nevaluate actions in this area. The attention is used within a sequentially\ngreedy procedure leveraging submodularity. Finally, we show how introducing\nrisk-seeking can dramatically improve the agents performance and ability to\ndiscover more far reaching strategies.\n", "versions": [{"version": "v1", "created": "Thu, 3 Dec 2015 15:51:30 GMT"}, {"version": "v2", "created": "Wed, 16 Dec 2015 17:34:55 GMT"}], "update_date": "2015-12-17", "authors_parsed": [["Sunehag", "Peter", ""], ["Evans", "Richard", ""], ["Dulac-Arnold", "Gabriel", ""], ["Zwols", "Yori", ""], ["Visentin", "Daniel", ""], ["Coppin", "Ben", ""]]}, {"id": "1512.01173", "submitter": "Jiaxin Shi", "authors": "Jiaxin Shi, Jun Zhu", "title": "Building Memory with Concept Learning Capabilities from Large-scale\n  Knowledge Base", "comments": "Accepted to NIPS 2015 Cognitive Computation workshop (CoCo@NIPS 2015)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a new perspective on neural knowledge base (KB) embeddings, from\nwhich we build a framework that can model symbolic knowledge in the KB together\nwith its learning process. We show that this framework well regularizes\nprevious neural KB embedding model for superior performance in reasoning tasks,\nwhile having the capabilities of dealing with unseen entities, that is, to\nlearn their embeddings from natural language descriptions, which is very like\nhuman's behavior of learning semantic concepts.\n", "versions": [{"version": "v1", "created": "Thu, 3 Dec 2015 17:52:50 GMT"}], "update_date": "2015-12-04", "authors_parsed": [["Shi", "Jiaxin", ""], ["Zhu", "Jun", ""]]}, {"id": "1512.01274", "submitter": "Mu Li", "authors": "Tianqi Chen, Mu Li, Yutian Li, Min Lin, Naiyan Wang, Minjie Wang,\n  Tianjun Xiao, Bing Xu, Chiyuan Zhang and Zheng Zhang", "title": "MXNet: A Flexible and Efficient Machine Learning Library for\n  Heterogeneous Distributed Systems", "comments": "In Neural Information Processing Systems, Workshop on Machine\n  Learning Systems, 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.LG cs.MS cs.NE", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  MXNet is a multi-language machine learning (ML) library to ease the\ndevelopment of ML algorithms, especially for deep neural networks. Embedded in\nthe host language, it blends declarative symbolic expression with imperative\ntensor computation. It offers auto differentiation to derive gradients. MXNet\nis computation and memory efficient and runs on various heterogeneous systems,\nranging from mobile devices to distributed GPU clusters.\n  This paper describes both the API design and the system implementation of\nMXNet, and explains how embedding of both symbolic expression and tensor\noperation is handled in a unified fashion. Our preliminary experiments reveal\npromising results on large scale deep neural network applications using\nmultiple GPU machines.\n", "versions": [{"version": "v1", "created": "Thu, 3 Dec 2015 22:49:21 GMT"}], "update_date": "2015-12-07", "authors_parsed": [["Chen", "Tianqi", ""], ["Li", "Mu", ""], ["Li", "Yutian", ""], ["Lin", "Min", ""], ["Wang", "Naiyan", ""], ["Wang", "Minjie", ""], ["Xiao", "Tianjun", ""], ["Xu", "Bing", ""], ["Zhang", "Chiyuan", ""], ["Zhang", "Zheng", ""]]}, {"id": "1512.01283", "submitter": "Vivek Datla V", "authors": "Vivek Datla and Abhinav Vishnu", "title": "Predicting the top and bottom ranks of billboard songs using Machine\n  Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The music industry is a $130 billion industry. Predicting whether a song\ncatches the pulse of the audience impacts the industry. In this paper we\nanalyze language inside the lyrics of the songs using several computational\nlinguistic algorithms and predict whether a song would make to the top or\nbottom of the billboard rankings based on the language features. We trained and\ntested an SVM classifier with a radial kernel function on the linguistic\nfeatures. Results indicate that we can classify whether a song belongs to top\nand bottom of the billboard charts with a precision of 0.76.\n", "versions": [{"version": "v1", "created": "Thu, 3 Dec 2015 23:42:10 GMT"}], "update_date": "2015-12-07", "authors_parsed": [["Datla", "Vivek", ""], ["Vishnu", "Abhinav", ""]]}, {"id": "1512.01289", "submitter": "Edward Grant", "authors": "Edward Grant, Stephan Sahm, Mariam Zabihi, Marcel van Gerven", "title": "Predicting and visualizing psychological attributions with a deep neural\n  network", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Judgments about personality based on facial appearance are strong effectors\nin social decision making, and are known to have impact on areas from\npresidential elections to jury decisions. Recent work has shown that it is\npossible to predict perception of memorability, trustworthiness, intelligence\nand other attributes in human face images. The most successful of these\napproaches require face images expertly annotated with key facial landmarks. We\ndemonstrate a Convolutional Neural Network (CNN) model that is able to perform\nthe same task without the need for landmark features, thereby greatly\nincreasing efficiency. The model has high accuracy, surpassing human-level\nperformance in some cases. Furthermore, we use a deconvolutional approach to\nvisualize important features for perception of 22 attributes and demonstrate a\nnew method for separately visualizing positive and negative features.\n", "versions": [{"version": "v1", "created": "Fri, 4 Dec 2015 00:24:16 GMT"}, {"version": "v2", "created": "Sat, 24 Dec 2016 01:06:35 GMT"}], "update_date": "2016-12-28", "authors_parsed": [["Grant", "Edward", ""], ["Sahm", "Stephan", ""], ["Zabihi", "Mariam", ""], ["van Gerven", "Marcel", ""]]}, {"id": "1512.01322", "submitter": "Sungho Shin", "authors": "Sungho Shin, Kyuyeon Hwang, and Wonyong Sung", "title": "Fixed-Point Performance Analysis of Recurrent Neural Networks", "comments": null, "journal-ref": null, "doi": "10.1109/MSP.2015.2411564", "report-no": null, "categories": "cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recurrent neural networks have shown excellent performance in many\napplications, however they require increased complexity in hardware or software\nbased implementations. The hardware complexity can be much lowered by\nminimizing the word-length of weights and signals. This work analyzes the\nfixed-point performance of recurrent neural networks using a retrain based\nquantization method. The quantization sensitivity of each layer in RNNs is\nstudied, and the overall fixed-point optimization results minimizing the\ncapacity of weights while not sacrificing the performance are presented. A\nlanguage model and a phoneme recognition examples are used.\n", "versions": [{"version": "v1", "created": "Fri, 4 Dec 2015 06:07:28 GMT"}, {"version": "v2", "created": "Sat, 23 Jan 2016 11:53:34 GMT"}, {"version": "v3", "created": "Tue, 27 Sep 2016 11:42:34 GMT"}], "update_date": "2016-09-28", "authors_parsed": [["Shin", "Sungho", ""], ["Hwang", "Kyuyeon", ""], ["Sung", "Wonyong", ""]]}, {"id": "1512.01325", "submitter": "Babak Saleh", "authors": "Babak Saleh, Ahmed Elgammal, Jacob Feldman, Ali Farhadi", "title": "Toward a Taxonomy and Computational Models of Abnormalities in Images", "comments": "To appear in the Thirtieth AAAI Conference on Artificial Intelligence\n  (AAAI 2016)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.HC cs.IT cs.LG math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The human visual system can spot an abnormal image, and reason about what\nmakes it strange. This task has not received enough attention in computer\nvision. In this paper we study various types of atypicalities in images in a\nmore comprehensive way than has been done before. We propose a new dataset of\nabnormal images showing a wide range of atypicalities. We design human subject\nexperiments to discover a coarse taxonomy of the reasons for abnormality. Our\nexperiments reveal three major categories of abnormality: object-centric,\nscene-centric, and contextual. Based on this taxonomy, we propose a\ncomprehensive computational model that can predict all different types of\nabnormality in images and outperform prior arts in abnormality recognition.\n", "versions": [{"version": "v1", "created": "Fri, 4 Dec 2015 06:29:53 GMT"}], "update_date": "2015-12-07", "authors_parsed": [["Saleh", "Babak", ""], ["Elgammal", "Ahmed", ""], ["Feldman", "Jacob", ""], ["Farhadi", "Ali", ""]]}, {"id": "1512.01332", "submitter": "Naoto Yoshida", "authors": "Naoto Yoshida", "title": "Q-Networks for Binary Vector Actions", "comments": "9 pages, 5 figures, accepted for Deep Reinforcement Learning\n  Workshop, NIPS 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper reinforcement learning with binary vector actions was\ninvestigated. We suggest an effective architecture of the neural networks for\napproximating an action-value function with binary vector actions. The proposed\narchitecture approximates the action-value function by a linear function with\nrespect to the action vector, but is still non-linear with respect to the state\ninput. We show that this approximation method enables the efficient calculation\nof greedy action selection and softmax action selection. Using this\narchitecture, we suggest an online algorithm based on Q-learning. The empirical\nresults in the grid world and the blocker task suggest that our approximation\narchitecture would be effective for the RL problems with large discrete action\nsets.\n", "versions": [{"version": "v1", "created": "Fri, 4 Dec 2015 07:51:48 GMT"}], "update_date": "2015-12-07", "authors_parsed": [["Yoshida", "Naoto", ""]]}, {"id": "1512.01362", "submitter": "Collins Achepsah Leke", "authors": "Collins Leke, Tshilidzi Marwala and Satyakama Paul", "title": "Proposition of a Theoretical Model for Missing Data Imputation using\n  Deep Learning and Evolutionary Algorithms", "comments": "14 Pages, 4 figures, journal, experiments will be added testing the\n  hypotheses", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the last couple of decades, there has been major advancements in the\ndomain of missing data imputation. The techniques in the domain include amongst\nothers: Expectation Maximization, Neural Networks with Evolutionary Algorithms\nor optimization techniques and K-Nearest Neighbor approaches to solve the\nproblem. The presence of missing data entries in databases render the tasks of\ndecision-making and data analysis nontrivial. As a result this area has\nattracted a lot of research interest with the aim being to yield accurate and\ntime efficient and sensitive missing data imputation techniques especially when\ntime sensitive applications are concerned like power plants and winding\nprocesses. In this article, considering arbitrary and monotone missing data\npatterns, we hypothesize that the use of deep neural networks built using\nautoencoders and denoising autoencoders in conjunction with genetic algorithms,\nswarm intelligence and maximum likelihood estimator methods as novel data\nimputation techniques will lead to better imputed values than existing\ntechniques. Also considered are the missing at random, missing completely at\nrandom and missing not at random missing data mechanisms. We also intend to use\nfuzzy logic in tandem with deep neural networks to perform the missing data\nimputation tasks, as well as different building blocks for the deep neural\nnetworks like Stacked Restricted Boltzmann Machines and Deep Belief Networks to\ntest our hypothesis. The motivation behind this article is the need for missing\ndata imputation techniques that lead to better imputed values than existing\nmethods with higher accuracies and lower errors.\n", "versions": [{"version": "v1", "created": "Fri, 4 Dec 2015 10:39:59 GMT"}], "update_date": "2015-12-07", "authors_parsed": [["Leke", "Collins", ""], ["Marwala", "Tshilidzi", ""], ["Paul", "Satyakama", ""]]}, {"id": "1512.01400", "submitter": "Haibing Wu", "authors": "Haibing Wu and Xiaodong Gu", "title": "Max-Pooling Dropout for Regularization of Convolutional Neural Networks", "comments": "The journal version of this paper [arXiv:1512.00242] has been\n  published in Neural Networks,\n  http://www.sciencedirect.com/science/article/pii/S0893608015001446", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, dropout has seen increasing use in deep learning. For deep\nconvolutional neural networks, dropout is known to work well in fully-connected\nlayers. However, its effect in pooling layers is still not clear. This paper\ndemonstrates that max-pooling dropout is equivalent to randomly picking\nactivation based on a multinomial distribution at training time. In light of\nthis insight, we advocate employing our proposed probabilistic weighted\npooling, instead of commonly used max-pooling, to act as model averaging at\ntest time. Empirical evidence validates the superiority of probabilistic\nweighted pooling. We also compare max-pooling dropout and stochastic pooling,\nboth of which introduce stochasticity based on multinomial distributions at\npooling stage.\n", "versions": [{"version": "v1", "created": "Fri, 4 Dec 2015 13:18:37 GMT"}], "update_date": "2015-12-07", "authors_parsed": [["Wu", "Haibing", ""], ["Gu", "Xiaodong", ""]]}, {"id": "1512.01563", "submitter": "Marlos C. Machado", "authors": "Yitao Liang, Marlos C. Machado, Erik Talvitie, Michael Bowling", "title": "State of the Art Control of Atari Games Using Shallow Reinforcement\n  Learning", "comments": "A shorter version of this paper appears in the Proceedings of the\n  15th International Conference on Autonomous Agents and Multiagent Systems\n  (AAMAS 2016)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The recently introduced Deep Q-Networks (DQN) algorithm has gained attention\nas one of the first successful combinations of deep neural networks and\nreinforcement learning. Its promise was demonstrated in the Arcade Learning\nEnvironment (ALE), a challenging framework composed of dozens of Atari 2600\ngames used to evaluate general competency in AI. It achieved dramatically\nbetter results than earlier approaches, showing that its ability to learn good\nrepresentations is quite robust and general. This paper attempts to understand\nthe principles that underlie DQN's impressive performance and to better\ncontextualize its success. We systematically evaluate the importance of key\nrepresentational biases encoded by DQN's network by proposing simple linear\nrepresentations that make use of these concepts. Incorporating these\ncharacteristics, we obtain a computationally practical feature set that\nachieves competitive performance to DQN in the ALE. Besides offering insight\ninto the strengths and weaknesses of DQN, we provide a generic representation\nfor the ALE, significantly reducing the burden of learning a representation for\neach game. Moreover, we also provide a simple, reproducible benchmark for the\nsake of comparison to future work in the ALE.\n", "versions": [{"version": "v1", "created": "Fri, 4 Dec 2015 21:06:04 GMT"}, {"version": "v2", "created": "Thu, 21 Apr 2016 21:55:54 GMT"}], "update_date": "2016-04-25", "authors_parsed": [["Liang", "Yitao", ""], ["Machado", "Marlos C.", ""], ["Talvitie", "Erik", ""], ["Bowling", "Michael", ""]]}, {"id": "1512.01568", "submitter": "Sanjay Sahay", "authors": "Aruna Govada, Pravin Joshi, Sahil Mittal and Sanjay K Sahay", "title": "Hybrid Approach for Inductive Semi Supervised Learning using Label\n  Propagation and Support Vector Machine", "comments": "Presented in the 11th International Conference, MLDM, Germany, July\n  20 - 21, 2015. Springer, Machine Learning and Data Mining in Pattern\n  Recognition, LNAI Vol. 9166, p. 199-213, 2015", "journal-ref": null, "doi": "10.1007/978-3-319-21024-7_14", "report-no": null, "categories": "cs.LG cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Semi supervised learning methods have gained importance in today's world\nbecause of large expenses and time involved in labeling the unlabeled data by\nhuman experts. The proposed hybrid approach uses SVM and Label Propagation to\nlabel the unlabeled data. In the process, at each step SVM is trained to\nminimize the error and thus improve the prediction quality. Experiments are\nconducted by using SVM and logistic regression(Logreg). Results prove that SVM\nperforms tremendously better than Logreg. The approach is tested using 12\ndatasets of different sizes ranging from the order of 1000s to the order of\n10000s. Results show that the proposed approach outperforms Label Propagation\nby a large margin with F-measure of almost twice on average. The parallel\nversion of the proposed approach is also designed and implemented, the analysis\nshows that the training time decreases significantly when parallel version is\nused.\n", "versions": [{"version": "v1", "created": "Wed, 2 Dec 2015 12:04:30 GMT"}], "update_date": "2015-12-08", "authors_parsed": [["Govada", "Aruna", ""], ["Joshi", "Pravin", ""], ["Mittal", "Sahil", ""], ["Sahay", "Sanjay K", ""]]}, {"id": "1512.01587", "submitter": "Sahil Garg", "authors": "Sahil Garg, Aram Galstyan, Ulf Hermjakob, and Daniel Marcu", "title": "Extracting Biomolecular Interactions Using Semantic Parsing of\n  Biomedical Text", "comments": "Appearing in Proceedings of the Thirtieth AAAI Conference on\n  Artificial Intelligence (AAAI-16)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.IR cs.IT cs.LG math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We advance the state of the art in biomolecular interaction extraction with\nthree contributions: (i) We show that deep, Abstract Meaning Representations\n(AMR) significantly improve the accuracy of a biomolecular interaction\nextraction system when compared to a baseline that relies solely on surface-\nand syntax-based features; (ii) In contrast with previous approaches that infer\nrelations on a sentence-by-sentence basis, we expand our framework to enable\nconsistent predictions over sets of sentences (documents); (iii) We further\nmodify and expand a graph kernel learning framework to enable concurrent\nexploitation of automatically induced AMR (semantic) and dependency structure\n(syntactic) representations. Our experiments show that our approach yields\ninteraction extraction systems that are more robust in environments where there\nis a significant mismatch between training and test conditions.\n", "versions": [{"version": "v1", "created": "Fri, 4 Dec 2015 22:58:29 GMT"}], "update_date": "2015-12-08", "authors_parsed": [["Garg", "Sahil", ""], ["Galstyan", "Aram", ""], ["Hermjakob", "Ulf", ""], ["Marcu", "Daniel", ""]]}, {"id": "1512.01596", "submitter": "Volodymyr Turchenko", "authors": "Volodymyr Turchenko, Artur Luczak", "title": "Creation of a Deep Convolutional Auto-Encoder in Caffe", "comments": "9 pages, 7 figures, 5 tables, 34 references in the list; Added\n  references, corrected Table 3, changed several paragraphs in the text", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The development of a deep (stacked) convolutional auto-encoder in the Caffe\ndeep learning framework is presented in this paper. We describe simple\nprinciples which we used to create this model in Caffe. The proposed model of\nconvolutional auto-encoder does not have pooling/unpooling layers yet. The\nresults of our experimental research show comparable accuracy of dimensionality\nreduction in comparison with a classic auto-encoder on the example of MNIST\ndataset.\n", "versions": [{"version": "v1", "created": "Fri, 4 Dec 2015 23:58:47 GMT"}, {"version": "v2", "created": "Thu, 21 Apr 2016 01:51:14 GMT"}, {"version": "v3", "created": "Fri, 22 Apr 2016 03:20:41 GMT"}], "update_date": "2016-04-25", "authors_parsed": [["Turchenko", "Volodymyr", ""], ["Luczak", "Artur", ""]]}, {"id": "1512.01629", "submitter": "Yinlam Chow", "authors": "Yinlam Chow and Mohammad Ghavamzadeh and Lucas Janson and Marco Pavone", "title": "Risk-Constrained Reinforcement Learning with Percentile Risk Criteria", "comments": "arXiv admin note: substantial text overlap with arXiv:1406.3339", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In many sequential decision-making problems one is interested in minimizing\nan expected cumulative cost while taking into account \\emph{risk}, i.e.,\nincreased awareness of events of small probability and high consequences.\nAccordingly, the objective of this paper is to present efficient reinforcement\nlearning algorithms for risk-constrained Markov decision processes (MDPs),\nwhere risk is represented via a chance constraint or a constraint on the\nconditional value-at-risk (CVaR) of the cumulative cost. We collectively refer\nto such problems as percentile risk-constrained MDPs.\n  Specifically, we first derive a formula for computing the gradient of the\nLagrangian function for percentile risk-constrained MDPs. Then, we devise\npolicy gradient and actor-critic algorithms that (1) estimate such gradient,\n(2) update the policy in the descent direction, and (3) update the Lagrange\nmultiplier in the ascent direction. For these algorithms we prove convergence\nto locally optimal policies. Finally, we demonstrate the effectiveness of our\nalgorithms in an optimal stopping problem and an online marketing application.\n", "versions": [{"version": "v1", "created": "Sat, 5 Dec 2015 06:39:32 GMT"}, {"version": "v2", "created": "Fri, 19 Aug 2016 06:37:48 GMT"}, {"version": "v3", "created": "Thu, 6 Apr 2017 08:07:59 GMT"}], "update_date": "2017-04-07", "authors_parsed": [["Chow", "Yinlam", ""], ["Ghavamzadeh", "Mohammad", ""], ["Janson", "Lucas", ""], ["Pavone", "Marco", ""]]}, {"id": "1512.01655", "submitter": "Nicola Pezzotti", "authors": "Nicola Pezzotti, Boudewijn P.F. Lelieveldt, Laurens van der Maaten,\n  Thomas H\\\"ollt, Elmar Eisemann, and Anna Vilanova", "title": "Approximated and User Steerable tSNE for Progressive Visual Analytics", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Progressive Visual Analytics aims at improving the interactivity in existing\nanalytics techniques by means of visualization as well as interaction with\nintermediate results. One key method for data analysis is dimensionality\nreduction, for example, to produce 2D embeddings that can be visualized and\nanalyzed efficiently. t-Distributed Stochastic Neighbor Embedding (tSNE) is a\nwell-suited technique for the visualization of several high-dimensional data.\ntSNE can create meaningful intermediate results but suffers from a slow\ninitialization that constrains its application in Progressive Visual Analytics.\nWe introduce a controllable tSNE approximation (A-tSNE), which trades off speed\nand accuracy, to enable interactive data exploration. We offer real-time\nvisualization techniques, including a density-based solution and a Magic Lens\nto inspect the degree of approximation. With this feedback, the user can decide\non local refinements and steer the approximation level during the analysis. We\ndemonstrate our technique with several datasets, in a real-world research\nscenario and for the real-time analysis of high-dimensional streams to\nillustrate its effectiveness for interactive data analysis.\n", "versions": [{"version": "v1", "created": "Sat, 5 Dec 2015 12:05:52 GMT"}, {"version": "v2", "created": "Tue, 8 Dec 2015 14:56:25 GMT"}, {"version": "v3", "created": "Thu, 16 Jun 2016 09:36:40 GMT"}], "update_date": "2016-06-17", "authors_parsed": [["Pezzotti", "Nicola", ""], ["Lelieveldt", "Boudewijn P. F.", ""], ["van der Maaten", "Laurens", ""], ["H\u00f6llt", "Thomas", ""], ["Eisemann", "Elmar", ""], ["Vilanova", "Anna", ""]]}, {"id": "1512.01693", "submitter": "Ivan Sorokin", "authors": "Ivan Sorokin, Alexey Seleznev, Mikhail Pavlov, Aleksandr Fedorov,\n  Anastasiia Ignateva", "title": "Deep Attention Recurrent Q-Network", "comments": "7 pages, 5 figures, Deep Reinforcement Learning Workshop, NIPS 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A deep learning approach to reinforcement learning led to a general learner\nable to train on visual input to play a variety of arcade games at the human\nand superhuman levels. Its creators at the Google DeepMind's team called the\napproach: Deep Q-Network (DQN). We present an extension of DQN by \"soft\" and\n\"hard\" attention mechanisms. Tests of the proposed Deep Attention Recurrent\nQ-Network (DARQN) algorithm on multiple Atari 2600 games show level of\nperformance superior to that of DQN. Moreover, built-in attention mechanisms\nallow a direct online monitoring of the training process by highlighting the\nregions of the game screen the agent is focusing on when making decisions.\n", "versions": [{"version": "v1", "created": "Sat, 5 Dec 2015 18:35:40 GMT"}], "update_date": "2015-12-08", "authors_parsed": [["Sorokin", "Ivan", ""], ["Seleznev", "Alexey", ""], ["Pavlov", "Mikhail", ""], ["Fedorov", "Aleksandr", ""], ["Ignateva", "Anastasiia", ""]]}, {"id": "1512.01708", "submitter": "Soham De", "authors": "Soham De, Gavin Taylor, Tom Goldstein", "title": "Variance Reduction for Distributed Stochastic Gradient Descent", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DC math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Variance reduction (VR) methods boost the performance of stochastic gradient\ndescent (SGD) by enabling the use of larger, constant stepsizes and preserving\nlinear convergence rates. However, current variance reduced SGD methods require\neither high memory usage or an exact gradient computation (using the entire\ndataset) at the end of each epoch. This limits the use of VR methods in\npractical distributed settings. In this paper, we propose a variance reduction\nmethod, called VR-lite, that does not require full gradient computations or\nextra storage. We explore distributed synchronous and asynchronous variants\nthat are scalable and remain stable with low communication frequency. We\nempirically compare both the sequential and distributed algorithms to\nstate-of-the-art stochastic optimization methods, and find that our proposed\nalgorithms perform favorably to other stochastic methods.\n", "versions": [{"version": "v1", "created": "Sat, 5 Dec 2015 22:48:40 GMT"}, {"version": "v2", "created": "Fri, 7 Apr 2017 04:07:29 GMT"}], "update_date": "2017-04-10", "authors_parsed": [["De", "Soham", ""], ["Taylor", "Gavin", ""], ["Goldstein", "Tom", ""]]}, {"id": "1512.01712", "submitter": "Konstantin Lopyrev", "authors": "Konstantin Lopyrev", "title": "Generating News Headlines with Recurrent Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe an application of an encoder-decoder recurrent neural network\nwith LSTM units and attention to generating headlines from the text of news\narticles. We find that the model is quite effective at concisely paraphrasing\nnews articles. Furthermore, we study how the neural network decides which input\nwords to pay attention to, and specifically we identify the function of the\ndifferent neurons in a simplified attention mechanism. Interestingly, our\nsimplified attention mechanism performs better that the more complex attention\nmechanism on a held out set of articles.\n", "versions": [{"version": "v1", "created": "Sat, 5 Dec 2015 23:41:22 GMT"}], "update_date": "2015-12-08", "authors_parsed": [["Lopyrev", "Konstantin", ""]]}, {"id": "1512.01728", "submitter": "Qi Qian", "authors": "Qi Qian, Inci M. Baytas, Rong Jin, Anil Jain and Shenghuo Zhu", "title": "Similarity Learning via Adaptive Regression and Its Application to Image\n  Retrieval", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of similarity learning and its application to image\nretrieval with large-scale data. The similarity between pairs of images can be\nmeasured by the distances between their high dimensional representations, and\nthe problem of learning the appropriate similarity is often addressed by\ndistance metric learning. However, distance metric learning requires the\nlearned metric to be a PSD matrix, which is computational expensive and not\nnecessary for retrieval ranking problem. On the other hand, the bilinear model\nis shown to be more flexible for large-scale image retrieval task, hence, we\nadopt it to learn a matrix for estimating pairwise similarities under the\nregression framework. By adaptively updating the target matrix in regression,\nwe can mimic the hinge loss, which is more appropriate for similarity learning\nproblem. Although the regression problem can have the closed-form solution, the\ncomputational cost can be very expensive. The computational challenges come\nfrom two aspects: the number of images can be very large and image features\nhave high dimensionality. We address the first challenge by compressing the\ndata by a randomized algorithm with the theoretical guarantee. For the high\ndimensional issue, we address it by taking low rank assumption and applying\nalternating method to obtain the partial matrix, which has a global optimal\nsolution. Empirical studies on real world image datasets (i.e., Caltech and\nImageNet) demonstrate the effectiveness and efficiency of the proposed method.\n", "versions": [{"version": "v1", "created": "Sun, 6 Dec 2015 02:56:32 GMT"}], "update_date": "2015-12-08", "authors_parsed": [["Qian", "Qi", ""], ["Baytas", "Inci M.", ""], ["Jin", "Rong", ""], ["Jain", "Anil", ""], ["Zhu", "Shenghuo", ""]]}, {"id": "1512.01752", "submitter": "Sujith Ravi", "authors": "Sujith Ravi, Qiming Diao", "title": "Large Scale Distributed Semi-Supervised Learning Using Streaming\n  Approximation", "comments": "10 pages", "journal-ref": "Proceedings of the 19th International Conference on Artificial\n  Intelligence and Statistics (AISTATS), JMLR: W&CP volume 51, pp. 519-528,\n  2016", "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Traditional graph-based semi-supervised learning (SSL) approaches, even\nthough widely applied, are not suited for massive data and large label\nscenarios since they scale linearly with the number of edges $|E|$ and distinct\nlabels $m$. To deal with the large label size problem, recent works propose\nsketch-based methods to approximate the distribution on labels per node thereby\nachieving a space reduction from $O(m)$ to $O(\\log m)$, under certain\nconditions. In this paper, we present a novel streaming graph-based SSL\napproximation that captures the sparsity of the label distribution and ensures\nthe algorithm propagates labels accurately, and further reduces the space\ncomplexity per node to $O(1)$. We also provide a distributed version of the\nalgorithm that scales well to large data sizes. Experiments on real-world\ndatasets demonstrate that the new method achieves better performance than\nexisting state-of-the-art algorithms with significant reduction in memory\nfootprint. We also study different graph construction mechanisms for natural\nlanguage applications and propose a robust graph augmentation strategy trained\nusing state-of-the-art unsupervised deep learning architectures that yields\nfurther significant quality gains.\n", "versions": [{"version": "v1", "created": "Sun, 6 Dec 2015 06:58:57 GMT"}, {"version": "v2", "created": "Mon, 16 May 2016 19:40:37 GMT"}], "update_date": "2016-05-17", "authors_parsed": [["Ravi", "Sujith", ""], ["Diao", "Qiming", ""]]}, {"id": "1512.01845", "submitter": "Alex Beutel", "authors": "Chao-Yuan Wu, Alex Beutel, Amr Ahmed, Alexander J. Smola", "title": "Explaining reviews and ratings with PACO: Poisson Additive Co-Clustering", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Understanding a user's motivations provides valuable information beyond the\nability to recommend items. Quite often this can be accomplished by perusing\nboth ratings and review texts, since it is the latter where the reasoning for\nspecific preferences is explicitly expressed.\n  Unfortunately matrix factorization approaches to recommendation result in\nlarge, complex models that are difficult to interpret and give recommendations\nthat are hard to clearly explain to users. In contrast, in this paper, we\nattack this problem through succinct additive co-clustering. We devise a novel\nBayesian technique for summing co-clusterings of Poisson distributions. With\nthis novel technique we propose a new Bayesian model for joint collaborative\nfiltering of ratings and text reviews through a sum of simple co-clusterings.\nThe simple structure of our model yields easily interpretable recommendations.\nEven with a simple, succinct structure, our model outperforms competitors in\nterms of predicting ratings with reviews.\n", "versions": [{"version": "v1", "created": "Sun, 6 Dec 2015 22:13:46 GMT"}], "update_date": "2015-12-08", "authors_parsed": [["Wu", "Chao-Yuan", ""], ["Beutel", "Alex", ""], ["Ahmed", "Amr", ""], ["Smola", "Alexander J.", ""]]}, {"id": "1512.01914", "submitter": "Xiao Zhang", "authors": "Xiao Zhang", "title": "Rademacher Complexity of the Restricted Boltzmann Machine", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Boltzmann machine, as a fundamental construction block of deep belief network\nand deep Boltzmann machines, is widely used in deep learning community and\ngreat success has been achieved. However, theoretical understanding of many\naspects of it is still far from clear. In this paper, we studied the Rademacher\ncomplexity of both the asymptotic restricted Boltzmann machine and the\npractical implementation with single-step contrastive divergence (CD-1)\nprocedure. Our results disclose the fact that practical implementation training\nprocedure indeed increased the Rademacher complexity of restricted Boltzmann\nmachines. A further research direction might be the investigation of the VC\ndimension of a compositional function used in the CD-1 procedure.\n", "versions": [{"version": "v1", "created": "Mon, 7 Dec 2015 05:20:30 GMT"}], "update_date": "2015-12-08", "authors_parsed": [["Zhang", "Xiao", ""]]}, {"id": "1512.01926", "submitter": "Kamil Rocki", "authors": "Kamil Rocki", "title": "Thinking Required", "comments": "18 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There exists a theory of a single general-purpose learning algorithm which\ncould explain the principles its operation. It assumes the initial rough\narchitecture, a small library of simple innate circuits which are prewired at\nbirth. and proposes that all significant mental algorithms are learned. Given\ncurrent understanding and observations, this paper reviews and lists the\ningredients of such an algorithm from architectural and functional\nperspectives.\n", "versions": [{"version": "v1", "created": "Mon, 7 Dec 2015 06:37:49 GMT"}], "update_date": "2015-12-08", "authors_parsed": [["Rocki", "Kamil", ""]]}, {"id": "1512.01927", "submitter": "Haoran Chen", "authors": "Haoran Chen and Yanfeng Sun and Junbin Gao and Yongli Hu", "title": "Fast Optimization Algorithm on Riemannian Manifolds and Its Application\n  in Low-Rank Representation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NA cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The paper addresses the problem of optimizing a class of composite functions\non Riemannian manifolds and a new first order optimization algorithm (FOA) with\na fast convergence rate is proposed. Through the theoretical analysis for FOA,\nit has been proved that the algorithm has quadratic convergence. The\nexperiments in the matrix completion task show that FOA has better performance\nthan other first order optimization methods on Riemannian manifolds. A fast\nsubspace pursuit method based on FOA is proposed to solve the low-rank\nrepresentation model based on augmented Lagrange method on the low rank matrix\nvariety. Experimental results on synthetic and real data sets are presented to\ndemonstrate that both FOA and SP-RPRG(ALM) can achieve superior performance in\nterms of faster convergence and higher accuracy.\n", "versions": [{"version": "v1", "created": "Mon, 7 Dec 2015 06:44:23 GMT"}], "update_date": "2015-12-08", "authors_parsed": [["Chen", "Haoran", ""], ["Sun", "Yanfeng", ""], ["Gao", "Junbin", ""], ["Hu", "Yongli", ""]]}, {"id": "1512.01993", "submitter": "Sanjay Sahay", "authors": "Aruna Govada, Shree Ranjani, Aditi Viswanathan and S.K.Sahay", "title": "A Novel Approach to Distributed Multi-Class SVM", "comments": "8 Pages", "journal-ref": "Transactions on Machine Learning and Artificial Intelligence, Vol.\n  2, No. 5, p. 72, 2014", "doi": "10.14738/tmlai.25.562", "report-no": null, "categories": "cs.LG cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With data sizes constantly expanding, and with classical machine learning\nalgorithms that analyze such data requiring larger and larger amounts of\ncomputation time and storage space, the need to distribute computation and\nmemory requirements among several computers has become apparent. Although\nsubstantial work has been done in developing distributed binary SVM algorithms\nand multi-class SVM algorithms individually, the field of multi-class\ndistributed SVMs remains largely unexplored. This research proposes a novel\nalgorithm that implements the Support Vector Machine over a multi-class dataset\nand is efficient in a distributed environment (here, Hadoop). The idea is to\ndivide the dataset into half recursively and thus compute the optimal Support\nVector Machine for this half during the training phase, much like a divide and\nconquer approach. While testing, this structure has been effectively exploited\nto significantly reduce the prediction time. Our algorithm has shown better\ncomputation time during the prediction phase than the traditional sequential\nSVM methods (One vs. One, One vs. Rest) and out-performs them as the size of\nthe dataset grows. This approach also classifies the data with higher accuracy\nthan the traditional multi-class algorithms.\n", "versions": [{"version": "v1", "created": "Mon, 7 Dec 2015 11:44:35 GMT"}], "update_date": "2015-12-08", "authors_parsed": [["Govada", "Aruna", ""], ["Ranjani", "Shree", ""], ["Viswanathan", "Aditi", ""], ["Sahay", "S. K.", ""]]}, {"id": "1512.02009", "submitter": "Bei Chen", "authors": "Bei Chen, Jun Zhu, Nan Yang, Tian Tian, Ming Zhou, Bo Zhang", "title": "Jointly Modeling Topics and Intents with Global Order Structure", "comments": "Accepted by AAAI 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modeling document structure is of great importance for discourse analysis and\nrelated applications. The goal of this research is to capture the document\nintent structure by modeling documents as a mixture of topic words and\nrhetorical words. While the topics are relatively unchanged through one\ndocument, the rhetorical functions of sentences usually change following\ncertain orders in discourse. We propose GMM-LDA, a topic modeling based\nBayesian unsupervised model, to analyze the document intent structure\ncooperated with order information. Our model is flexible that has the ability\nto combine the annotations and do supervised learning. Additionally, entropic\nregularization can be introduced to model the significant divergence between\ntopics and intents. We perform experiments in both unsupervised and supervised\nsettings, results show the superiority of our model over several\nstate-of-the-art baselines.\n", "versions": [{"version": "v1", "created": "Mon, 7 Dec 2015 12:16:58 GMT"}], "update_date": "2015-12-08", "authors_parsed": [["Chen", "Bei", ""], ["Zhu", "Jun", ""], ["Yang", "Nan", ""], ["Tian", "Tian", ""], ["Zhou", "Ming", ""], ["Zhang", "Bo", ""]]}, {"id": "1512.02011", "submitter": "Vincent Francois-Lavet", "authors": "Vincent Fran\\c{c}ois-Lavet, Raphael Fonteneau, Damien Ernst", "title": "How to Discount Deep Reinforcement Learning: Towards New Dynamic\n  Strategies", "comments": "NIPS 2015 Deep Reinforcement Learning Workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Using deep neural nets as function approximator for reinforcement learning\ntasks have recently been shown to be very powerful for solving problems\napproaching real-world complexity. Using these results as a benchmark, we\ndiscuss the role that the discount factor may play in the quality of the\nlearning process of a deep Q-network (DQN). When the discount factor\nprogressively increases up to its final value, we empirically show that it is\npossible to significantly reduce the number of learning steps. When used in\nconjunction with a varying learning rate, we empirically show that it\noutperforms original DQN on several experiments. We relate this phenomenon with\nthe instabilities of neural networks when they are used in an approximate\nDynamic Programming setting. We also describe the possibility to fall within a\nlocal optimum during the learning process, thus connecting our discussion with\nthe exploration/exploitation dilemma.\n", "versions": [{"version": "v1", "created": "Mon, 7 Dec 2015 12:25:18 GMT"}, {"version": "v2", "created": "Wed, 20 Jan 2016 10:33:00 GMT"}], "update_date": "2016-01-21", "authors_parsed": [["Fran\u00e7ois-Lavet", "Vincent", ""], ["Fonteneau", "Raphael", ""], ["Ernst", "Damien", ""]]}, {"id": "1512.02016", "submitter": "Bei Chen", "authors": "Bei Chen, Ning Chen, Jun Zhu, Jiaming Song, Bo Zhang", "title": "Discriminative Nonparametric Latent Feature Relational Models with Data\n  Augmentation", "comments": "Accepted by AAAI 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a discriminative nonparametric latent feature relational model\n(LFRM) for link prediction to automatically infer the dimensionality of latent\nfeatures. Under the generic RegBayes (regularized Bayesian inference)\nframework, we handily incorporate the prediction loss with probabilistic\ninference of a Bayesian model; set distinct regularization parameters for\ndifferent types of links to handle the imbalance issue in real networks; and\nunify the analysis of both the smooth logistic log-loss and the piecewise\nlinear hinge loss. For the nonconjugate posterior inference, we present a\nsimple Gibbs sampler via data augmentation, without making restricting\nassumptions as done in variational methods. We further develop an approximate\nsampler using stochastic gradient Langevin dynamics to handle large networks\nwith hundreds of thousands of entities and millions of links, orders of\nmagnitude larger than what existing LFRM models can process. Extensive studies\non various real networks show promising performance.\n", "versions": [{"version": "v1", "created": "Mon, 7 Dec 2015 12:37:41 GMT"}], "update_date": "2015-12-08", "authors_parsed": [["Chen", "Bei", ""], ["Chen", "Ning", ""], ["Zhu", "Jun", ""], ["Song", "Jiaming", ""], ["Zhang", "Bo", ""]]}, {"id": "1512.02033", "submitter": "Danny Karmon", "authors": "Danny Karmon and Joseph Keshet", "title": "Risk Minimization in Structured Prediction using Orbit Loss", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a new surrogate loss function called orbit loss in the\nstructured prediction framework, which has good theoretical and practical\nadvantages. While the orbit loss is not convex, it has a simple analytical\ngradient and a simple perceptron-like learning rule. We analyze the new loss\ntheoretically and state a PAC-Bayesian generalization bound. We also prove that\nthe new loss is consistent in the strong sense; namely, the risk achieved by\nthe set of the trained parameters approaches the infimum risk achievable by any\nlinear decoder over the given features. Methods that are aimed at risk\nminimization, such as the structured ramp loss, the structured probit loss and\nthe direct loss minimization require at least two inference operations per\ntraining iteration. In this sense, the orbit loss is more efficient as it\nrequires only one inference operation per training iteration, while yields\nsimilar performance. We conclude the paper with an empirical comparison of the\nproposed loss function to the structured hinge loss, the structured ramp loss,\nthe structured probit loss and the direct loss minimization method on several\nbenchmark datasets and tasks.\n", "versions": [{"version": "v1", "created": "Mon, 7 Dec 2015 13:30:27 GMT"}, {"version": "v2", "created": "Wed, 9 Dec 2015 09:59:56 GMT"}], "update_date": "2015-12-10", "authors_parsed": [["Karmon", "Danny", ""], ["Keshet", "Joseph", ""]]}, {"id": "1512.02097", "submitter": "Teng Qiu", "authors": "Teng Qiu, Yongjie Li", "title": "Clustering by Deep Nearest Neighbor Descent (D-NND): A Density-based\n  Parameter-Insensitive Clustering Method", "comments": "28 pages, 14 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CV cs.LG stat.CO stat.ME", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Most density-based clustering methods largely rely on how well the underlying\ndensity is estimated. However, density estimation itself is also a challenging\nproblem, especially the determination of the kernel bandwidth. A large\nbandwidth could lead to the over-smoothed density estimation in which the\nnumber of density peaks could be less than the true clusters, while a small\nbandwidth could lead to the under-smoothed density estimation in which spurious\ndensity peaks, or called the \"ripple noise\", would be generated in the\nestimated density. In this paper, we propose a density-based hierarchical\nclustering method, called the Deep Nearest Neighbor Descent (D-NND), which\ncould learn the underlying density structure layer by layer and capture the\ncluster structure at the same time. The over-smoothed density estimation could\nbe largely avoided and the negative effect of the under-estimated cases could\nbe also largely reduced. Overall, D-NND presents not only the strong capability\nof discovering the underlying cluster structure but also the remarkable\nreliability due to its insensitivity to parameters.\n", "versions": [{"version": "v1", "created": "Mon, 7 Dec 2015 15:47:49 GMT"}], "update_date": "2015-12-08", "authors_parsed": [["Qiu", "Teng", ""], ["Li", "Yongjie", ""]]}, {"id": "1512.02109", "submitter": "Anmer Daskin", "authors": "Anmer Daskin", "title": "Obtaining A Linear Combination of the Principal Components of a Matrix\n  on Quantum Computers", "comments": "The title of the paper is changed. A couple of sections are extended.\n  8 pages and 3 figures", "journal-ref": "Quantum Inf Process (2016) 15: 4013", "doi": "10.1007/s11128-016-1388-7", "report-no": null, "categories": "quant-ph cs.LG math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Principal component analysis is a multivariate statistical method frequently\nused in science and engineering to reduce the dimension of a problem or extract\nthe most significant features from a dataset. In this paper, using a similar\nnotion to the quantum counting, we show how to apply the amplitude\namplification together with the phase estimation algorithm to an operator in\norder to procure the eigenvectors of the operator associated to the eigenvalues\ndefined in the range $\\left[a, b\\right]$, where $a$ and $b$ are real and $0\n\\leq a \\leq b \\leq 1$. This makes possible to obtain a combination of the\neigenvectors associated to the largest eigenvalues and so can be used to do\nprincipal component analysis on quantum computers.\n", "versions": [{"version": "v1", "created": "Thu, 26 Nov 2015 14:31:12 GMT"}, {"version": "v2", "created": "Wed, 9 Dec 2015 13:37:00 GMT"}, {"version": "v3", "created": "Thu, 28 Jan 2016 09:53:59 GMT"}], "update_date": "2016-11-09", "authors_parsed": [["Daskin", "Anmer", ""]]}, {"id": "1512.02134", "submitter": "Nikolaus Mayer", "authors": "Nikolaus Mayer, Eddy Ilg, Philip H\\\"ausser, Philipp Fischer, Daniel\n  Cremers, Alexey Dosovitskiy, Thomas Brox", "title": "A Large Dataset to Train Convolutional Networks for Disparity, Optical\n  Flow, and Scene Flow Estimation", "comments": "Includes supplementary material", "journal-ref": null, "doi": "10.1109/CVPR.2016.438", "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent work has shown that optical flow estimation can be formulated as a\nsupervised learning task and can be successfully solved with convolutional\nnetworks. Training of the so-called FlowNet was enabled by a large\nsynthetically generated dataset. The present paper extends the concept of\noptical flow estimation via convolutional networks to disparity and scene flow\nestimation. To this end, we propose three synthetic stereo video datasets with\nsufficient realism, variation, and size to successfully train large networks.\nOur datasets are the first large-scale datasets to enable training and\nevaluating scene flow methods. Besides the datasets, we present a convolutional\nnetwork for real-time disparity estimation that provides state-of-the-art\nresults. By combining a flow and disparity estimation network and training it\njointly, we demonstrate the first scene flow estimation with a convolutional\nnetwork.\n", "versions": [{"version": "v1", "created": "Mon, 7 Dec 2015 17:35:00 GMT"}], "update_date": "2018-01-18", "authors_parsed": [["Mayer", "Nikolaus", ""], ["Ilg", "Eddy", ""], ["H\u00e4usser", "Philip", ""], ["Fischer", "Philipp", ""], ["Cremers", "Daniel", ""], ["Dosovitskiy", "Alexey", ""], ["Brox", "Thomas", ""]]}, {"id": "1512.02181", "submitter": "Ji Liu", "authors": "Ji Liu and Xiaojin Zhu", "title": "The Teaching Dimension of Linear Learners", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Teaching dimension is a learning theoretic quantity that specifies the\nminimum training set size to teach a target model to a learner. Previous\nstudies on teaching dimension focused on version-space learners which maintain\nall hypotheses consistent with the training data, and cannot be applied to\nmodern machine learners which select a specific hypothesis via optimization.\nThis paper presents the first known teaching dimension for ridge regression,\nsupport vector machines, and logistic regression. We also exhibit optimal\ntraining sets that match these teaching dimensions. Our approach generalizes to\nother linear learners.\n", "versions": [{"version": "v1", "created": "Mon, 7 Dec 2015 19:24:55 GMT"}], "update_date": "2015-12-08", "authors_parsed": [["Liu", "Ji", ""], ["Zhu", "Xiaojin", ""]]}, {"id": "1512.02188", "submitter": "Tae-Hyun Oh", "authors": "Tae-Hyun Oh, Yasuyuki Matsushita, In So Kweon, David Wipf", "title": "Pseudo-Bayesian Robust PCA: Algorithms and Analyses", "comments": "Journal version of NIPS 2016. Submitted to TPAMI", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Commonly used in computer vision and other applications, robust PCA\nrepresents an algorithmic attempt to reduce the sensitivity of classical PCA to\noutliers. The basic idea is to learn a decomposition of some data matrix of\ninterest into low rank and sparse components, the latter representing unwanted\noutliers. Although the resulting optimization problem is typically NP-hard,\nconvex relaxations provide a computationally-expedient alternative with\ntheoretical support. However, in practical regimes performance guarantees break\ndown and a variety of non-convex alternatives, including Bayesian-inspired\nmodels, have been proposed to boost estimation quality. Unfortunately though,\nwithout additional a priori knowledge none of these methods can significantly\nexpand the critical operational range such that exact principal subspace\nrecovery is possible. Into this mix we propose a novel pseudo-Bayesian\nalgorithm that explicitly compensates for design weaknesses in many existing\nnon-convex approaches leading to state-of-the-art performance with a sound\nanalytical foundation. Surprisingly, our algorithm can even outperform convex\nmatrix completion despite the fact that the latter is provided with perfect\nknowledge of which entries are not corrupted.\n", "versions": [{"version": "v1", "created": "Mon, 7 Dec 2015 19:43:54 GMT"}, {"version": "v2", "created": "Fri, 7 Oct 2016 16:08:25 GMT"}], "update_date": "2016-10-10", "authors_parsed": [["Oh", "Tae-Hyun", ""], ["Matsushita", "Yasuyuki", ""], ["Kweon", "In So", ""], ["Wipf", "David", ""]]}, {"id": "1512.02337", "submitter": "David Steurer", "authors": "Samuel B. Hopkins, Tselil Schramm, Jonathan Shi, David Steurer", "title": "Fast spectral algorithms from sum-of-squares proofs: tensor\n  decomposition and planted sparse vectors", "comments": "62 pages, title changed, to appear at STOC 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider two problems that arise in machine learning applications: the\nproblem of recovering a planted sparse vector in a random linear subspace and\nthe problem of decomposing a random low-rank overcomplete 3-tensor. For both\nproblems, the best known guarantees are based on the sum-of-squares method. We\ndevelop new algorithms inspired by analyses of the sum-of-squares method. Our\nalgorithms achieve the same or similar guarantees as sum-of-squares for these\nproblems but the running time is significantly faster.\n  For the planted sparse vector problem, we give an algorithm with running time\nnearly linear in the input size that approximately recovers a planted sparse\nvector with up to constant relative sparsity in a random subspace of $\\mathbb\nR^n$ of dimension up to $\\tilde \\Omega(\\sqrt n)$. These recovery guarantees\nmatch the best known ones of Barak, Kelner, and Steurer (STOC 2014) up to\nlogarithmic factors.\n  For tensor decomposition, we give an algorithm with running time close to\nlinear in the input size (with exponent $\\approx 1.086$) that approximately\nrecovers a component of a random 3-tensor over $\\mathbb R^n$ of rank up to\n$\\tilde \\Omega(n^{4/3})$. The best previous algorithm for this problem due to\nGe and Ma (RANDOM 2015) works up to rank $\\tilde \\Omega(n^{3/2})$ but requires\nquasipolynomial time.\n", "versions": [{"version": "v1", "created": "Tue, 8 Dec 2015 05:49:07 GMT"}, {"version": "v2", "created": "Wed, 3 Feb 2016 18:01:12 GMT"}], "update_date": "2016-02-04", "authors_parsed": [["Hopkins", "Samuel B.", ""], ["Schramm", "Tselil", ""], ["Shi", "Jonathan", ""], ["Steurer", "David", ""]]}, {"id": "1512.02393", "submitter": "Changbo Zhu", "authors": "Changbo Zhu, Huan Xu, Shuicheng Yan", "title": "Online Crowdsourcing", "comments": "novelty not enough", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the success of modern internet based platform, such as Amazon Mechanical\nTurk, it is now normal to collect a large number of hand labeled samples from\nnon-experts. The Dawid- Skene algorithm, which is based on Expectation-\nMaximization update, has been widely used for inferring the true labels from\nnoisy crowdsourced labels. However, Dawid-Skene scheme requires all the data to\nperform each EM iteration, and can be infeasible for streaming data or large\nscale data. In this paper, we provide an online version of Dawid- Skene\nalgorithm that only requires one data frame for each iteration. Further, we\nprove that under mild conditions, the online Dawid-Skene scheme with projection\nconverges to a stationary point of the marginal log-likelihood of the observed\ndata. Our experiments demonstrate that the online Dawid- Skene scheme achieves\nstate of the art performance comparing with other methods based on the Dawid-\nSkene scheme.\n", "versions": [{"version": "v1", "created": "Tue, 8 Dec 2015 10:35:29 GMT"}, {"version": "v2", "created": "Fri, 8 Feb 2019 02:52:24 GMT"}], "update_date": "2019-02-11", "authors_parsed": [["Zhu", "Changbo", ""], ["Xu", "Huan", ""], ["Yan", "Shuicheng", ""]]}, {"id": "1512.02394", "submitter": "Changbo Zhu", "authors": "Changbo Zhu, Huan Xu", "title": "Online Gradient Descent in Function Space", "comments": "novelty not enough", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In many problems in machine learning and operations research, we need to\noptimize a function whose input is a random variable or a probability density\nfunction, i.e. to solve optimization problems in an infinite dimensional space.\nOn the other hand, online learning has the advantage of dealing with streaming\nexamples, and better model a changing environ- ment. In this paper, we extend\nthe celebrated online gradient descent algorithm to Hilbert spaces (function\nspaces), and analyze the convergence guarantee of the algorithm. Finally, we\ndemonstrate that our algorithms can be useful in several important problems.\n", "versions": [{"version": "v1", "created": "Tue, 8 Dec 2015 10:38:19 GMT"}, {"version": "v2", "created": "Fri, 8 Feb 2019 02:51:50 GMT"}], "update_date": "2019-02-11", "authors_parsed": [["Zhu", "Changbo", ""], ["Xu", "Huan", ""]]}, {"id": "1512.02406", "submitter": "Yi-Chun Chen", "authors": "Yi-Chun Chen, Tim Allan Wheeler, Mykel John Kochenderfer", "title": "Learning Discrete Bayesian Networks from Continuous Data", "comments": null, "journal-ref": "Journal of Artificial Intelligence Research 59 (2017) 103-132", "doi": null, "report-no": null, "categories": "cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning Bayesian networks from raw data can help provide insights into the\nrelationships between variables. While real data often contains a mixture of\ndiscrete and continuous-valued variables, many Bayesian network structure\nlearning algorithms assume all random variables are discrete. Thus, continuous\nvariables are often discretized when learning a Bayesian network. However, the\nchoice of discretization policy has significant impact on the accuracy, speed,\nand interpretability of the resulting models. This paper introduces a\nprincipled Bayesian discretization method for continuous variables in Bayesian\nnetworks with quadratic complexity instead of the cubic complexity of other\nstandard techniques. Empirical demonstrations show that the proposed method is\nsuperior to the established minimum description length algorithm. In addition,\nthis paper shows how to incorporate existing methods into the structure\nlearning process to discretize all continuous variables and simultaneously\nlearn Bayesian network structures.\n", "versions": [{"version": "v1", "created": "Tue, 8 Dec 2015 11:12:04 GMT"}, {"version": "v2", "created": "Tue, 15 Dec 2015 08:00:55 GMT"}, {"version": "v3", "created": "Tue, 18 Sep 2018 02:44:57 GMT"}], "update_date": "2018-09-19", "authors_parsed": [["Chen", "Yi-Chun", ""], ["Wheeler", "Tim Allan", ""], ["Kochenderfer", "Mykel John", ""]]}, {"id": "1512.02479", "submitter": "Gr\\'egoire Montavon", "authors": "Gr\\'egoire Montavon, Sebastian Bach, Alexander Binder, Wojciech Samek,\n  Klaus-Robert M\\\"uller", "title": "Explaining NonLinear Classification Decisions with Deep Taylor\n  Decomposition", "comments": "20 pages, 15 figures", "journal-ref": null, "doi": "10.1016/j.patcog.2016.11.008", "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Nonlinear methods such as Deep Neural Networks (DNNs) are the gold standard\nfor various challenging machine learning problems, e.g., image classification,\nnatural language processing or human action recognition. Although these methods\nperform impressively well, they have a significant disadvantage, the lack of\ntransparency, limiting the interpretability of the solution and thus the scope\nof application in practice. Especially DNNs act as black boxes due to their\nmultilayer nonlinear structure. In this paper we introduce a novel methodology\nfor interpreting generic multilayer neural networks by decomposing the network\nclassification decision into contributions of its input elements. Although our\nfocus is on image classification, the method is applicable to a broad set of\ninput data, learning tasks and network architectures. Our method is based on\ndeep Taylor decomposition and efficiently utilizes the structure of the network\nby backpropagating the explanations from the output to the input layer. We\nevaluate the proposed method empirically on the MNIST and ILSVRC data sets.\n", "versions": [{"version": "v1", "created": "Tue, 8 Dec 2015 14:25:29 GMT"}], "update_date": "2017-11-15", "authors_parsed": [["Montavon", "Gr\u00e9goire", ""], ["Bach", "Sebastian", ""], ["Binder", "Alexander", ""], ["Samek", "Wojciech", ""], ["M\u00fcller", "Klaus-Robert", ""]]}, {"id": "1512.02497", "submitter": "Francisco Massa", "authors": "Francisco Massa, Bryan Russell, Mathieu Aubry", "title": "Deep Exemplar 2D-3D Detection by Adapting from Real to Rendered Views", "comments": "To appear in CVPR 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents an end-to-end convolutional neural network (CNN) for\n2D-3D exemplar detection. We demonstrate that the ability to adapt the features\nof natural images to better align with those of CAD rendered views is critical\nto the success of our technique. We show that the adaptation can be learned by\ncompositing rendered views of textured object models on natural images. Our\napproach can be naturally incorporated into a CNN detection pipeline and\nextends the accuracy and speed benefits from recent advances in deep learning\nto 2D-3D exemplar detection. We applied our method to two tasks: instance\ndetection, where we evaluated on the IKEA dataset, and object category\ndetection, where we out-perform Aubry et al. for \"chair\" detection on a subset\nof the Pascal VOC dataset.\n", "versions": [{"version": "v1", "created": "Tue, 8 Dec 2015 15:04:46 GMT"}, {"version": "v2", "created": "Mon, 18 Apr 2016 13:14:22 GMT"}], "update_date": "2016-04-19", "authors_parsed": [["Massa", "Francisco", ""], ["Russell", "Bryan", ""], ["Aubry", "Mathieu", ""]]}, {"id": "1512.02560", "submitter": "Omid Ghahabi", "authors": "Omid Ghahabi and Javier Hernando", "title": "Deep Learning for Single and Multi-Session i-Vector Speaker Recognition", "comments": null, "journal-ref": "IEEE/ACM Transactions on Audio, Speech, and Language Processing,\n  Volume: 25, Issue: 4, April 2017", "doi": "10.1109/TASLP.2017.2661705", "report-no": null, "categories": "cs.SD cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The promising performance of Deep Learning (DL) in speech recognition has\nmotivated the use of DL in other speech technology applications such as speaker\nrecognition. Given i-vectors as inputs, the authors proposed an impostor\nselection algorithm and a universal model adaptation process in a hybrid system\nbased on Deep Belief Networks (DBN) and Deep Neural Networks (DNN) to\ndiscriminatively model each target speaker. In order to have more insight into\nthe behavior of DL techniques in both single and multi-session speaker\nenrollment tasks, some experiments have been carried out in this paper in both\nscenarios. Additionally, the parameters of the global model, referred to as\nuniversal DBN (UDBN), are normalized before adaptation. UDBN normalization\nfacilitates training DNNs specifically with more than one hidden layer.\nExperiments are performed on the NIST SRE 2006 corpus. It is shown that the\nproposed impostor selection algorithm and UDBN adaptation process enhance the\nperformance of conventional DNNs 8-20 % and 16-20 % in terms of EER for the\nsingle and multi-session tasks, respectively. In both scenarios, the proposed\narchitectures outperform the baseline systems obtaining up to 17 % reduction in\nEER.\n", "versions": [{"version": "v1", "created": "Tue, 8 Dec 2015 17:34:49 GMT"}], "update_date": "2017-04-24", "authors_parsed": [["Ghahabi", "Omid", ""], ["Hernando", "Javier", ""]]}, {"id": "1512.02673", "submitter": "Kangwook Lee", "authors": "Kangwook Lee, Maximilian Lam, Ramtin Pedarsani, Dimitris\n  Papailiopoulos, Kannan Ramchandran", "title": "Speeding Up Distributed Machine Learning Using Codes", "comments": "This work is published in IEEE Transactions on Information Theory and\n  presented in part at the NIPS 2015 Workshop on Machine Learning Systems and\n  the IEEE ISIT 2016", "journal-ref": null, "doi": "10.1109/TIT.2017.2736066", "report-no": null, "categories": "cs.DC cs.IT cs.LG cs.PF math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Codes are widely used in many engineering applications to offer robustness\nagainst noise. In large-scale systems there are several types of noise that can\naffect the performance of distributed machine learning algorithms -- straggler\nnodes, system failures, or communication bottlenecks -- but there has been\nlittle interaction cutting across codes, machine learning, and distributed\nsystems. In this work, we provide theoretical insights on how coded solutions\ncan achieve significant gains compared to uncoded ones. We focus on two of the\nmost basic building blocks of distributed learning algorithms: matrix\nmultiplication and data shuffling. For matrix multiplication, we use codes to\nalleviate the effect of stragglers, and show that if the number of homogeneous\nworkers is $n$, and the runtime of each subtask has an exponential tail, coded\ncomputation can speed up distributed matrix multiplication by a factor of $\\log\nn$. For data shuffling, we use codes to reduce communication bottlenecks,\nexploiting the excess in storage. We show that when a constant fraction\n$\\alpha$ of the data matrix can be cached at each worker, and $n$ is the number\nof workers, \\emph{coded shuffling} reduces the communication cost by a factor\nof $(\\alpha + \\frac{1}{n})\\gamma(n)$ compared to uncoded shuffling, where\n$\\gamma(n)$ is the ratio of the cost of unicasting $n$ messages to $n$ users to\nmulticasting a common message (of the same size) to $n$ users. For instance,\n$\\gamma(n) \\simeq n$ if multicasting a message to $n$ users is as cheap as\nunicasting a message to one user. We also provide experiment results,\ncorroborating our theoretical gains of the coded algorithms.\n", "versions": [{"version": "v1", "created": "Tue, 8 Dec 2015 21:54:04 GMT"}, {"version": "v2", "created": "Thu, 10 Dec 2015 19:34:37 GMT"}, {"version": "v3", "created": "Mon, 29 Jan 2018 03:04:14 GMT"}], "update_date": "2018-01-30", "authors_parsed": [["Lee", "Kangwook", ""], ["Lam", "Maximilian", ""], ["Pedarsani", "Ramtin", ""], ["Papailiopoulos", "Dimitris", ""], ["Ramchandran", "Kannan", ""]]}, {"id": "1512.02693", "submitter": "John Jameson", "authors": "John W. Jameson", "title": "Reinforcement Control with Hierarchical Backpropagated Adaptive Critics", "comments": "16 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.LG cs.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Present incremental learning methods are limited in the ability to achieve\nreliable credit assignment over a large number time steps (or events). However,\nthis situation is typical for cases where the dynamical system to be controlled\nrequires relatively frequent control updates in order to maintain stability or\nrobustness yet has some action-consequences which must be established over\nrelatively long periods of time. To address this problem, the learning\ncapabilities of a control architecture comprised of two Backpropagated Adaptive\nCritics (BACs) in a two-level hierarchy with continuous actions are explored.\nThe high-level BAC updates less frequently than the low-level BAC and controls\nthe latter to some degree. The response of the low-level to high-level signals\ncan either be determined a priori or it can emerge during learning. A general\napproach called Response Induction Learning is introduced to address the latter\ncase.\n", "versions": [{"version": "v1", "created": "Tue, 8 Dec 2015 23:11:54 GMT"}], "update_date": "2015-12-10", "authors_parsed": [["Jameson", "John W.", ""]]}, {"id": "1512.02728", "submitter": "Abhimanu Kumar", "authors": "Abhimanu Kumar and Pengtao Xie and Junming Yin and Eric P. Xing", "title": "Distributed Training of Deep Neural Networks with Theoretical Analysis:\n  Under SSP Setting", "comments": "The paper needs more refinement", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a distributed approach to train deep neural networks (DNNs), which\nhas guaranteed convergence theoretically and great scalability empirically:\nclose to 6 times faster on instance of ImageNet data set when run with 6\nmachines. The proposed scheme is close to optimally scalable in terms of number\nof machines, and guaranteed to converge to the same optima as the undistributed\nsetting. The convergence and scalability of the distributed setting is shown\nempirically across different datasets (TIMIT and ImageNet) and machine learning\ntasks (image classification and phoneme extraction). The convergence analysis\nprovides novel insights into this complex learning scheme, including: 1)\nlayerwise convergence, and 2) convergence of the weights in probability.\n", "versions": [{"version": "v1", "created": "Wed, 9 Dec 2015 02:46:28 GMT"}, {"version": "v2", "created": "Fri, 11 Dec 2015 21:51:51 GMT"}], "update_date": "2016-10-04", "authors_parsed": [["Kumar", "Abhimanu", ""], ["Xie", "Pengtao", ""], ["Yin", "Junming", ""], ["Xing", "Eric P.", ""]]}, {"id": "1512.02736", "submitter": "Xingyu Zeng", "authors": "Xingyu Zeng, Wanli Ouyang, Xiaogang Wang", "title": "Window-Object Relationship Guided Representation Learning for Generic\n  Object Detections", "comments": "9 pages, including 1 reference page, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In existing works that learn representation for object detection, the\nrelationship between a candidate window and the ground truth bounding box of an\nobject is simplified by thresholding their overlap. This paper shows\ninformation loss in this simplification and picks up the relative location/size\ninformation discarded by thresholding. We propose a representation learning\npipeline to use the relationship as supervision for improving the learned\nrepresentation in object detection. Such relationship is not limited to object\nof the target category, but also includes surrounding objects of other\ncategories. We show that image regions with multiple contexts and multiple\nrotations are effective in capturing such relationship during the\nrepresentation learning process and in handling the semantic and visual\nvariation caused by different window-object configurations. Experimental\nresults show that the representation learned by our approach can improve the\nobject detection accuracy by 6.4% in mean average precision (mAP) on\nILSVRC2014. On the challenging ILSVRC2014 test dataset, 48.6% mAP is achieved\nby our single model and it is the best among published results. On PASCAL VOC,\nit outperforms the state-of-the-art result of Fast RCNN by 3.3% in absolute\nmAP.\n", "versions": [{"version": "v1", "created": "Wed, 9 Dec 2015 03:32:21 GMT"}], "update_date": "2015-12-10", "authors_parsed": [["Zeng", "Xingyu", ""], ["Ouyang", "Wanli", ""], ["Wang", "Xiaogang", ""]]}, {"id": "1512.02743", "submitter": "Yuki Itoh", "authors": "Yuki Itoh, Marco F. Duarte, Mario Parente", "title": "Perfect Recovery Conditions For Non-Negative Sparse Modeling", "comments": "12 pages, 8 figures", "journal-ref": null, "doi": "10.1109/TSP.2016.2613067", "report-no": null, "categories": "cs.IT cs.LG math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sparse modeling has been widely and successfully used in many applications\nsuch as computer vision, machine learning, and pattern recognition. Accompanied\nwith those applications, significant research has studied the theoretical\nlimits and algorithm design for convex relaxations in sparse modeling. However,\ntheoretical analyses on non-negative versions of sparse modeling are limited in\nthe literature either to a noiseless setting or a scenario with a specific\nstatistical noise model such as Gaussian noise. This paper studies the\nperformance of non-negative sparse modeling in a more general scenario where\nthe observed signals have an unknown arbitrary distortion, especially focusing\non non-negativity constrained and L1-penalized least squares, and gives an\nexact bound for which this problem can recover the correct signal elements. We\npose two conditions to guarantee the correct signal recovery: minimum\ncoefficient condition (MCC) and nonlinearity vs. subset coherence condition\n(NSCC). The former defines the minimum weight for each of the correct atoms\npresent in the signal and the latter defines the tolerable deviation from the\nlinear model relative to the positive subset coherence (PSC), a novel type of\n\"coherence\" metric. We provide rigorous performance guarantees based on these\nconditions and experimentally verify their precise predictive power in a\nhyperspectral data unmixing application.\n", "versions": [{"version": "v1", "created": "Wed, 9 Dec 2015 03:51:19 GMT"}, {"version": "v2", "created": "Thu, 19 May 2016 18:28:18 GMT"}, {"version": "v3", "created": "Tue, 20 Sep 2016 02:39:03 GMT"}], "update_date": "2016-10-04", "authors_parsed": [["Itoh", "Yuki", ""], ["Duarte", "Marco F.", ""], ["Parente", "Mario", ""]]}, {"id": "1512.02752", "submitter": "Qi Mao", "authors": "Qi Mao, Li Wang, Ivor W. Tsang, Yijun Sun", "title": "A Novel Regularized Principal Graph Learning Framework on Explicit Graph\n  Representation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many scientific datasets are of high dimension, and the analysis usually\nrequires visual manipulation by retaining the most important structures of\ndata. Principal curve is a widely used approach for this purpose. However, many\nexisting methods work only for data with structures that are not\nself-intersected, which is quite restrictive for real applications. A few\nmethods can overcome the above problem, but they either require complicated\nhuman-made rules for a specific task with lack of convergence guarantee and\nadaption flexibility to different tasks, or cannot obtain explicit structures\nof data. To address these issues, we develop a new regularized principal graph\nlearning framework that captures the local information of the underlying graph\nstructure based on reversed graph embedding. As showcases, models that can\nlearn a spanning tree or a weighted undirected $\\ell_1$ graph are proposed, and\na new learning algorithm is developed that learns a set of principal points and\na graph structure from data, simultaneously. The new algorithm is simple with\nguaranteed convergence. We then extend the proposed framework to deal with\nlarge-scale data. Experimental results on various synthetic and six real world\ndatasets show that the proposed method compares favorably with baselines and\ncan uncover the underlying structure correctly.\n", "versions": [{"version": "v1", "created": "Wed, 9 Dec 2015 04:57:18 GMT"}, {"version": "v2", "created": "Sun, 17 Jan 2016 14:34:14 GMT"}], "update_date": "2016-01-19", "authors_parsed": [["Mao", "Qi", ""], ["Wang", "Li", ""], ["Tsang", "Ivor W.", ""], ["Sun", "Yijun", ""]]}, {"id": "1512.02767", "submitter": "Michael Maire", "authors": "Michael Maire, Takuya Narihira, Stella X. Yu", "title": "Affinity CNN: Learning Pixel-Centric Pairwise Relations for\n  Figure/Ground Embedding", "comments": "minor updates; extended version of CVPR 2016 conference paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Spectral embedding provides a framework for solving perceptual organization\nproblems, including image segmentation and figure/ground organization. From an\naffinity matrix describing pairwise relationships between pixels, it clusters\npixels into regions, and, using a complex-valued extension, orders pixels\naccording to layer. We train a convolutional neural network (CNN) to directly\npredict the pairwise relationships that define this affinity matrix. Spectral\nembedding then resolves these predictions into a globally-consistent\nsegmentation and figure/ground organization of the scene. Experiments\ndemonstrate significant benefit to this direct coupling compared to prior works\nwhich use explicit intermediate stages, such as edge detection, on the pathway\nfrom image to affinities. Our results suggest spectral embedding as a powerful\nalternative to the conditional random field (CRF)-based globalization schemes\ntypically coupled to deep neural networks.\n", "versions": [{"version": "v1", "created": "Wed, 9 Dec 2015 06:45:23 GMT"}, {"version": "v2", "created": "Mon, 11 Apr 2016 22:03:38 GMT"}], "update_date": "2016-04-13", "authors_parsed": [["Maire", "Michael", ""], ["Narihira", "Takuya", ""], ["Yu", "Stella X.", ""]]}, {"id": "1512.02831", "submitter": "Fabian Gieseke", "authors": "Fabian Gieseke and Cosmin Eugen Oancea and Ashish Mahabal and\n  Christian Igel and Tom Heskes", "title": "Bigger Buffer k-d Trees on Multi-Many-Core Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DS cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A buffer k-d tree is a k-d tree variant for massively-parallel nearest\nneighbor search. While providing valuable speed-ups on modern many-core devices\nin case both a large number of reference and query points are given, buffer k-d\ntrees are limited by the amount of points that can fit on a single device. In\nthis work, we show how to modify the original data structure and the associated\nworkflow to make the overall approach capable of dealing with massive data\nsets. We further provide a simple yet efficient way of using multiple devices\ngiven in a single workstation. The applicability of the modified framework is\ndemonstrated in the context of astronomy, a field that is faced with huge\namounts of data.\n", "versions": [{"version": "v1", "created": "Wed, 9 Dec 2015 12:28:12 GMT"}], "update_date": "2015-12-10", "authors_parsed": [["Gieseke", "Fabian", ""], ["Oancea", "Cosmin Eugen", ""], ["Mahabal", "Ashish", ""], ["Igel", "Christian", ""], ["Heskes", "Tom", ""]]}, {"id": "1512.02866", "submitter": "Liran Szlak", "authors": "Jonathan Rosenski, Ohad Shamir, Liran Szlak", "title": "Multi-Player Bandits -- a Musical Chairs Approach", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider a variant of the stochastic multi-armed bandit problem, where\nmultiple players simultaneously choose from the same set of arms and may\ncollide, receiving no reward. This setting has been motivated by problems\narising in cognitive radio networks, and is especially challenging under the\nrealistic assumption that communication between players is limited. We provide\na communication-free algorithm (Musical Chairs) which attains constant regret\nwith high probability, as well as a sublinear-regret, communication-free\nalgorithm (Dynamic Musical Chairs) for the more difficult setting of players\ndynamically entering and leaving throughout the game. Moreover, both algorithms\ndo not require prior knowledge of the number of players. To the best of our\nknowledge, these are the first communication-free algorithms with these types\nof formal guarantees. We also rigorously compare our algorithms to previous\nworks, and complement our theoretical findings with experiments.\n", "versions": [{"version": "v1", "created": "Wed, 9 Dec 2015 14:18:16 GMT"}], "update_date": "2015-12-10", "authors_parsed": [["Rosenski", "Jonathan", ""], ["Shamir", "Ohad", ""], ["Szlak", "Liran", ""]]}, {"id": "1512.02896", "submitter": "Farid M. Naini", "authors": "Farid M. Naini, Jayakrishnan Unnikrishnan, Patrick Thiran, Martin\n  Vetterli", "title": "Where You Are Is Who You Are: User Identification by Matching Statistics", "comments": null, "journal-ref": null, "doi": "10.1109/TIFS.2015.2498131", "report-no": null, "categories": "cs.LG cs.CR cs.SI stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most users of online services have unique behavioral or usage patterns. These\nbehavioral patterns can be exploited to identify and track users by using only\nthe observed patterns in the behavior. We study the task of identifying users\nfrom statistics of their behavioral patterns. Specifically, we focus on the\nsetting in which we are given histograms of users' data collected during two\ndifferent experiments. We assume that, in the first dataset, the users'\nidentities are anonymized or hidden and that, in the second dataset, their\nidentities are known. We study the task of identifying the users by matching\nthe histograms of their data in the first dataset with the histograms from the\nsecond dataset. In recent works, the optimal algorithm for this user\nidentification task is introduced. In this paper, we evaluate the effectiveness\nof this method on three different types of datasets and in multiple scenarios.\nUsing datasets such as call data records, web browsing histories, and GPS\ntrajectories, we show that a large fraction of users can be easily identified\ngiven only histograms of their data; hence these histograms can act as users'\nfingerprints. We also verify that simultaneous identification of users achieves\nbetter performance compared to one-by-one user identification. We show that\nusing the optimal method for identification gives higher identification\naccuracy than heuristics-based approaches in practical scenarios. The accuracy\nobtained under this optimal method can thus be used to quantify the maximum\nlevel of user identification that is possible in such settings. We show that\nthe key factors affecting the accuracy of the optimal identification algorithm\nare the duration of the data collection, the number of users in the anonymized\ndataset, and the resolution of the dataset. We analyze the effectiveness of\nk-anonymization in resisting user identification attacks on these datasets.\n", "versions": [{"version": "v1", "created": "Wed, 9 Dec 2015 15:23:33 GMT"}], "update_date": "2015-12-10", "authors_parsed": [["Naini", "Farid M.", ""], ["Unnikrishnan", "Jayakrishnan", ""], ["Thiran", "Patrick", ""], ["Vetterli", "Martin", ""]]}, {"id": "1512.02970", "submitter": "Soham De", "authors": "Soham De and Tom Goldstein", "title": "Efficient Distributed SGD with Variance Reduction", "comments": "In Proceedings of 2016 IEEE International Conference on Data Mining\n  (ICDM)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DC math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Stochastic Gradient Descent (SGD) has become one of the most popular\noptimization methods for training machine learning models on massive datasets.\nHowever, SGD suffers from two main drawbacks: (i) The noisy gradient updates\nhave high variance, which slows down convergence as the iterates approach the\noptimum, and (ii) SGD scales poorly in distributed settings, typically\nexperiencing rapidly decreasing marginal benefits as the number of workers\nincreases. In this paper, we propose a highly parallel method, CentralVR, that\nuses error corrections to reduce the variance of SGD gradient updates, and\nscales linearly with the number of worker nodes. CentralVR enjoys low iteration\ncomplexity, provably linear convergence rates, and exhibits linear performance\ngains up to hundreds of cores for massive datasets. We compare CentralVR to\nstate-of-the-art parallel stochastic optimization methods on a variety of\nmodels and datasets, and find that our proposed methods exhibit stronger\nscaling than other SGD variants.\n", "versions": [{"version": "v1", "created": "Wed, 9 Dec 2015 17:57:31 GMT"}, {"version": "v2", "created": "Tue, 4 Oct 2016 16:03:51 GMT"}, {"version": "v3", "created": "Fri, 7 Apr 2017 02:54:14 GMT"}], "update_date": "2017-04-10", "authors_parsed": [["De", "Soham", ""], ["Goldstein", "Tom", ""]]}, {"id": "1512.03025", "submitter": "Ilia Zintchenko", "authors": "Ilia Zintchenko, Matthew Hastings, Nathan Wiebe, Ethan Brown, Matthias\n  Troyer", "title": "Partial Reinitialisation for Optimisers", "comments": "8 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG cs.NE math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Heuristic optimisers which search for an optimal configuration of variables\nrelative to an objective function often get stuck in local optima where the\nalgorithm is unable to find further improvement. The standard approach to\ncircumvent this problem involves periodically restarting the algorithm from\nrandom initial configurations when no further improvement can be found. We\npropose a method of partial reinitialization, whereby, in an attempt to find a\nbetter solution, only sub-sets of variables are re-initialised rather than the\nwhole configuration. Much of the information gained from previous runs is hence\nretained. This leads to significant improvements in the quality of the solution\nfound in a given time for a variety of optimisation problems in machine\nlearning.\n", "versions": [{"version": "v1", "created": "Wed, 9 Dec 2015 20:08:43 GMT"}], "update_date": "2015-12-10", "authors_parsed": [["Zintchenko", "Ilia", ""], ["Hastings", "Matthew", ""], ["Wiebe", "Nathan", ""], ["Brown", "Ethan", ""], ["Troyer", "Matthias", ""]]}, {"id": "1512.03201", "submitter": "Olivier Sigaud", "authors": "Olivier Sigaud and Cl\\'ement Masson and David Filliat and Freek Stulp", "title": "Gated networks: an inventory", "comments": "Unpublished manuscript, 17 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Gated networks are networks that contain gating connections, in which the\noutputs of at least two neurons are multiplied. Initially, gated networks were\nused to learn relationships between two input sources, such as pixels from two\nimages. More recently, they have been applied to learning activity recognition\nor multi-modal representations. The aims of this paper are threefold: 1) to\nexplain the basic computations in gated networks to the non-expert, while\nadopting a standpoint that insists on their symmetric nature. 2) to serve as a\nquick reference guide to the recent literature, by providing an inventory of\napplications of these networks, as well as recent extensions to the basic\narchitecture. 3) to suggest future research directions and applications.\n", "versions": [{"version": "v1", "created": "Thu, 10 Dec 2015 10:31:13 GMT"}], "update_date": "2015-12-11", "authors_parsed": [["Sigaud", "Olivier", ""], ["Masson", "Cl\u00e9ment", ""], ["Filliat", "David", ""], ["Stulp", "Freek", ""]]}, {"id": "1512.03219", "submitter": "Vladislav Malyshkin", "authors": "Vladislav Gennadievich Malyshkin", "title": "Norm-Free Radon-Nikodym Approach to Machine Learning", "comments": "Cluster localization measure added. Quantum mechanics analogy\n  improved and expanded (density matrix exact expression added). Coverage\n  calculation via matrix spectrum added", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For Machine Learning (ML) classification problem, where a vector of\n$\\mathbf{x}$--observations (values of attributes) is mapped to a single $y$\nvalue (class label), a generalized Radon--Nikodym type of solution is proposed.\nQuantum--mechanics --like probability states $\\psi^2(\\mathbf{x})$ are\nconsidered and \"Cluster Centers\", corresponding to the extremums of\n$<y\\psi^2(\\mathbf{x})>/<\\psi^2(\\mathbf{x})>$, are found from generalized\neigenvalues problem. The eigenvalues give possible $y^{[i]}$ outcomes and\ncorresponding to them eigenvectors $\\psi^{[i]}(\\mathbf{x})$ define \"Cluster\nCenters\". The projection of a $\\psi$ state, localized at given $\\mathbf{x}$ to\nclassify, on these eigenvectors define the probability of $y^{[i]}$ outcome,\nthus avoiding using a norm ($L^2$ or other types), required for \"quality\ncriteria\" in a typical Machine Learning technique. A coverage of each `Cluster\nCenter\" is calculated, what potentially allows to separate system properties\n(described by $y^{[i]}$ outcomes) and system testing conditions (described by\n$C^{[i]}$ coverage). As an example of such application $y$ distribution\nestimator is proposed in a form of pairs $(y^{[i]},C^{[i]})$, that can be\nconsidered as Gauss quadratures generalization. This estimator allows to\nperform $y$ probability distribution estimation in a strongly non--Gaussian\ncase.\n", "versions": [{"version": "v1", "created": "Thu, 10 Dec 2015 11:24:26 GMT"}, {"version": "v2", "created": "Tue, 15 Dec 2015 19:01:18 GMT"}], "update_date": "2015-12-16", "authors_parsed": [["Malyshkin", "Vladislav Gennadievich", ""]]}, {"id": "1512.03375", "submitter": "Peter H Jin", "authors": "Peter H. Jin and Kurt Keutzer", "title": "Convolutional Monte Carlo Rollouts in Go", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we present a MCTS-based Go-playing program which uses\nconvolutional networks in all parts. Our method performs MCTS in batches,\nexplores the Monte Carlo search tree using Thompson sampling and a\nconvolutional network, and evaluates convnet-based rollouts on the GPU. We\nachieve strong win rates against open source Go programs and attain competitive\nresults against state of the art convolutional net-based Go-playing programs.\n", "versions": [{"version": "v1", "created": "Thu, 10 Dec 2015 19:32:48 GMT"}], "update_date": "2015-12-11", "authors_parsed": [["Jin", "Peter H.", ""], ["Keutzer", "Kurt", ""]]}, {"id": "1512.03396", "submitter": "Yuting Ma", "authors": "Yuting Ma, Tian Zheng", "title": "Boosted Sparse Non-linear Distance Metric Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a boosting-based solution addressing metric learning\nproblems for high-dimensional data. Distance measures have been used as natural\nmeasures of (dis)similarity and served as the foundation of various learning\nmethods. The efficiency of distance-based learning methods heavily depends on\nthe chosen distance metric. With increasing dimensionality and complexity of\ndata, however, traditional metric learning methods suffer from poor scalability\nand the limitation due to linearity as the true signals are usually embedded\nwithin a low-dimensional nonlinear subspace. In this paper, we propose a\nnonlinear sparse metric learning algorithm via boosting. We restructure a\nglobal optimization problem into a forward stage-wise learning of weak learners\nbased on a rank-one decomposition of the weight matrix in the Mahalanobis\ndistance metric. A gradient boosting algorithm is devised to obtain a sparse\nrank-one update of the weight matrix at each step. Nonlinear features are\nlearned by a hierarchical expansion of interactions incorporated within the\nboosting algorithm. Meanwhile, an early stopping rule is imposed to control the\noverall complexity of the learned metric. As a result, our approach guarantees\nthree desirable properties of the final metric: positive semi-definiteness, low\nrank and element-wise sparsity. Numerical experiments show that our learning\nmodel compares favorably with the state-of-the-art methods in the current\nliterature of metric learning.\n", "versions": [{"version": "v1", "created": "Thu, 10 Dec 2015 20:19:01 GMT"}], "update_date": "2015-12-11", "authors_parsed": [["Ma", "Yuting", ""], ["Zheng", "Tian", ""]]}, {"id": "1512.03423", "submitter": "Syvester Olubolu Orimaye Dr", "authors": "Sylvester Olubolu Orimaye, Foo Chuan Leong, Chen Hui Lee, Eddy Cheng\n  Han Ng", "title": "Predicting proximity with ambient mobile sensors for non-invasive health\n  diagnostics", "comments": "Accepted and presented at the 12th IEEE Malaysia International\n  Conference on Communications, 23-25 November, 2015, Kuching, Sarawak,\n  Malaysia", "journal-ref": null, "doi": "10.1109/MICC.2015.7725398", "report-no": null, "categories": "cs.CY cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modern smart phones are becoming helpful in the areas of Internet-Of-Things\n(IoT) and ambient health intelligence. By learning data from several mobile\nsensors, we detect nearness of the human body to a mobile device in a\nthree-dimensional space with no physical contact with the device for\nnon-invasive health diagnostics. We show that the human body generates wave\npatterns that interact with other naturally occurring ambient signals that\ncould be measured by mobile sensors, such as, temperature, humidity, magnetic\nfield, acceleration, gravity, and light. This interaction consequentially\nalters the patterns of the naturally occurring signals, and thus, exhibits\ncharacteristics that could be learned to predict the nearness of the human body\nto a mobile device, hence provide diagnostic information for medical\npractitioners. Our prediction technique achieved 88.75% accuracy and 88.3%\nspecificity.\n", "versions": [{"version": "v1", "created": "Thu, 10 Dec 2015 03:54:17 GMT"}], "update_date": "2016-11-18", "authors_parsed": [["Orimaye", "Sylvester Olubolu", ""], ["Leong", "Foo Chuan", ""], ["Lee", "Chen Hui", ""], ["Ng", "Eddy Cheng Han", ""]]}, {"id": "1512.03518", "submitter": "Zirui Zhou", "authors": "Zirui Zhou, Anthony Man-Cho So", "title": "A Unified Approach to Error Bounds for Structured Convex Optimization\n  Problems", "comments": "32 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.LG math.NA stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Error bounds, which refer to inequalities that bound the distance of vectors\nin a test set to a given set by a residual function, have proven to be\nextremely useful in analyzing the convergence rates of a host of iterative\nmethods for solving optimization problems. In this paper, we present a new\nframework for establishing error bounds for a class of structured convex\noptimization problems, in which the objective function is the sum of a smooth\nconvex function and a general closed proper convex function. Such a class\nencapsulates not only fairly general constrained minimization problems but also\nvarious regularized loss minimization formulations in machine learning, signal\nprocessing, and statistics. Using our framework, we show that a number of\nexisting error bound results can be recovered in a unified and transparent\nmanner. To further demonstrate the power of our framework, we apply it to a\nclass of nuclear-norm regularized loss minimization problems and establish a\nnew error bound for this class under a strict complementarity-type regularity\ncondition. We then complement this result by constructing an example to show\nthat the said error bound could fail to hold without the regularity condition.\nConsequently, we obtain a rather complete answer to a question raised by Tseng.\nWe believe that our approach will find further applications in the study of\nerror bounds for structured convex optimization problems.\n", "versions": [{"version": "v1", "created": "Fri, 11 Dec 2015 04:32:30 GMT"}], "update_date": "2015-12-14", "authors_parsed": [["Zhou", "Zirui", ""], ["So", "Anthony Man-Cho", ""]]}, {"id": "1512.03542", "submitter": "Zhengping Che", "authors": "Zhengping Che, Sanjay Purushotham, Robinder Khemani, Yan Liu", "title": "Distilling Knowledge from Deep Networks with Applications to Healthcare\n  Domain", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Exponential growth in Electronic Healthcare Records (EHR) has resulted in new\nopportunities and urgent needs for discovery of meaningful data-driven\nrepresentations and patterns of diseases in Computational Phenotyping research.\nDeep Learning models have shown superior performance for robust prediction in\ncomputational phenotyping tasks, but suffer from the issue of model\ninterpretability which is crucial for clinicians involved in decision-making.\nIn this paper, we introduce a novel knowledge-distillation approach called\nInterpretable Mimic Learning, to learn interpretable phenotype features for\nmaking robust prediction while mimicking the performance of deep learning\nmodels. Our framework uses Gradient Boosting Trees to learn interpretable\nfeatures from deep learning models such as Stacked Denoising Autoencoder and\nLong Short-Term Memory. Exhaustive experiments on a real-world clinical\ntime-series dataset show that our method obtains similar or better performance\nthan the deep learning models, and it provides interpretable phenotypes for\nclinical decision making.\n", "versions": [{"version": "v1", "created": "Fri, 11 Dec 2015 07:38:12 GMT"}], "update_date": "2015-12-14", "authors_parsed": [["Che", "Zhengping", ""], ["Purushotham", "Sanjay", ""], ["Khemani", "Robinder", ""], ["Liu", "Yan", ""]]}, {"id": "1512.03549", "submitter": "Pranjal Singh", "authors": "Pranjal Singh, Amitabha Mukerjee", "title": "Words are not Equal: Graded Weighting Model for building Composite\n  Document Vectors", "comments": "10 Pages, 2 Figures, 11 Tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite the success of distributional semantics, composing phrases from word\nvectors remains an important challenge. Several methods have been tried for\nbenchmark tasks such as sentiment classification, including word vector\naveraging, matrix-vector approaches based on parsing, and on-the-fly learning\nof paragraph vectors. Most models usually omit stop words from the composition.\nInstead of such an yes-no decision, we consider several graded schemes where\nwords are weighted according to their discriminatory relevance with respect to\nits use in the document (e.g., idf). Some of these methods (particularly\ntf-idf) are seen to result in a significant improvement in performance over\nprior state of the art. Further, combining such approaches into an ensemble\nbased on alternate classifiers such as the RNN model, results in an 1.6%\nperformance improvement on the standard IMDB movie review dataset, and a 7.01%\nimprovement on Amazon product reviews. Since these are language free models and\ncan be obtained in an unsupervised manner, they are of interest also for\nunder-resourced languages such as Hindi as well and many more languages. We\ndemonstrate the language free aspects by showing a gain of 12% for two review\ndatasets over earlier results, and also release a new larger dataset for future\ntesting (Singh,2015).\n", "versions": [{"version": "v1", "created": "Fri, 11 Dec 2015 08:44:45 GMT"}], "update_date": "2015-12-14", "authors_parsed": [["Singh", "Pranjal", ""], ["Mukerjee", "Amitabha", ""]]}, {"id": "1512.03844", "submitter": "Alexander Wong", "authors": "Mohammad Javad Shafiee, Parthipan Siva, Paul Fieguth, and Alexander\n  Wong", "title": "Efficient Deep Feature Learning and Extraction via StochasticNets", "comments": "10 pages. arXiv admin note: substantial text overlap with\n  arXiv:1508.05463", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural networks are a powerful tool for feature learning and extraction\ngiven their ability to model high-level abstractions in highly complex data.\nOne area worth exploring in feature learning and extraction using deep neural\nnetworks is efficient neural connectivity formation for faster feature learning\nand extraction. Motivated by findings of stochastic synaptic connectivity\nformation in the brain as well as the brain's uncanny ability to efficiently\nrepresent information, we propose the efficient learning and extraction of\nfeatures via StochasticNets, where sparsely-connected deep neural networks can\nbe formed via stochastic connectivity between neurons. To evaluate the\nfeasibility of such a deep neural network architecture for feature learning and\nextraction, we train deep convolutional StochasticNets to learn abstract\nfeatures using the CIFAR-10 dataset, and extract the learned features from\nimages to perform classification on the SVHN and STL-10 datasets. Experimental\nresults show that features learned using deep convolutional StochasticNets,\nwith fewer neural connections than conventional deep convolutional neural\nnetworks, can allow for better or comparable classification accuracy than\nconventional deep neural networks: relative test error decrease of ~4.5% for\nclassification on the STL-10 dataset and ~1% for classification on the SVHN\ndataset. Furthermore, it was shown that the deep features extracted using deep\nconvolutional StochasticNets can provide comparable classification accuracy\neven when only 10% of the training data is used for feature learning. Finally,\nit was also shown that significant gains in feature extraction speed can be\nachieved in embedded applications using StochasticNets. As such, StochasticNets\nallow for faster feature learning and extraction performance while facilitate\nfor better or comparable accuracy performances.\n", "versions": [{"version": "v1", "created": "Fri, 11 Dec 2015 22:47:34 GMT"}], "update_date": "2015-12-15", "authors_parsed": [["Shafiee", "Mohammad Javad", ""], ["Siva", "Parthipan", ""], ["Fieguth", "Paul", ""], ["Wong", "Alexander", ""]]}, {"id": "1512.03880", "submitter": "Jinyang  Gao", "authors": "Jinyang Gao, H.V.Jagadish, Beng Chin Ooi", "title": "Active Sampler: Light-weight Accelerator for Complex Data Analytics at\n  Scale", "comments": "12 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent years have witnessed amazing outcomes from \"Big Models\" trained by\n\"Big Data\". Most popular algorithms for model training are iterative. Due to\nthe surging volumes of data, we can usually afford to process only a fraction\nof the training data in each iteration. Typically, the data are either\nuniformly sampled or sequentially accessed.\n  In this paper, we study how the data access pattern can affect model\ntraining. We propose an Active Sampler algorithm, where training data with more\n\"learning value\" to the model are sampled more frequently. The goal is to focus\ntraining effort on valuable instances near the classification boundaries,\nrather than evident cases, noisy data or outliers. We show the correctness and\noptimality of Active Sampler in theory, and then develop a light-weight\nvectorized implementation. Active Sampler is orthogonal to most approaches\noptimizing the efficiency of large-scale data analytics, and can be applied to\nmost analytics models trained by stochastic gradient descent (SGD) algorithm.\nExtensive experimental evaluations demonstrate that Active Sampler can speed up\nthe training procedure of SVM, feature selection and deep learning, for\ncomparable training quality by 1.6-2.2x.\n", "versions": [{"version": "v1", "created": "Sat, 12 Dec 2015 06:32:33 GMT"}], "update_date": "2015-12-15", "authors_parsed": [["Gao", "Jinyang", ""], ["Jagadish", "H. V.", ""], ["Ooi", "Beng Chin", ""]]}, {"id": "1512.03929", "submitter": "Joseph Fitzsimons", "authors": "Zhikuan Zhao, Jack K. Fitzsimons and Joseph F. Fitzsimons", "title": "Quantum assisted Gaussian process regression", "comments": "4 pages. Comments welcome", "journal-ref": "Phys. Rev. A 99, 052331 (2019)", "doi": "10.1103/PhysRevA.99.052331", "report-no": null, "categories": "quant-ph cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Gaussian processes (GP) are a widely used model for regression problems in\nsupervised machine learning. Implementation of GP regression typically requires\n$O(n^3)$ logic gates. We show that the quantum linear systems algorithm [Harrow\net al., Phys. Rev. Lett. 103, 150502 (2009)] can be applied to Gaussian process\nregression (GPR), leading to an exponential reduction in computation time in\nsome instances. We show that even in some cases not ideally suited to the\nquantum linear systems algorithm, a polynomial increase in efficiency still\noccurs.\n", "versions": [{"version": "v1", "created": "Sat, 12 Dec 2015 16:19:35 GMT"}], "update_date": "2019-05-29", "authors_parsed": [["Zhao", "Zhikuan", ""], ["Fitzsimons", "Jack K.", ""], ["Fitzsimons", "Joseph F.", ""]]}, {"id": "1512.03953", "submitter": "Mehrdad Ghadiri", "authors": "Mehrdad Ghadiri, Amin Aghaee, Mahdieh Soleymani Baghshah", "title": "Active Distance-Based Clustering using K-medoids", "comments": "12 pages, 3 figures, PAKDD 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  k-medoids algorithm is a partitional, centroid-based clustering algorithm\nwhich uses pairwise distances of data points and tries to directly decompose\nthe dataset with $n$ points into a set of $k$ disjoint clusters. However,\nk-medoids itself requires all distances between data points that are not so\neasy to get in many applications. In this paper, we introduce a new method\nwhich requires only a small proportion of the whole set of distances and makes\nan effort to estimate an upper-bound for unknown distances using the inquired\nones. This algorithm makes use of the triangle inequality to calculate an\nupper-bound estimation of the unknown distances. Our method is built upon a\nrecursive approach to cluster objects and to choose some points actively from\neach bunch of data and acquire the distances between these prominent points\nfrom oracle. Experimental results show that the proposed method using only a\nsmall subset of the distances can find proper clustering on many real-world and\nsynthetic datasets.\n", "versions": [{"version": "v1", "created": "Sat, 12 Dec 2015 19:33:52 GMT"}], "update_date": "2015-12-15", "authors_parsed": [["Ghadiri", "Mehrdad", ""], ["Aghaee", "Amin", ""], ["Baghshah", "Mahdieh Soleymani", ""]]}, {"id": "1512.03965", "submitter": "Ohad Shamir", "authors": "Ronen Eldan and Ohad Shamir", "title": "The Power of Depth for Feedforward Neural Networks", "comments": "Accepted to COLT 2016; Fixed a bug in the proof of claim 2 (now\n  requiring the mild assumption that the activations are polynomially bounded);\n  Other minor revisions", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We show that there is a simple (approximately radial) function on $\\reals^d$,\nexpressible by a small 3-layer feedforward neural networks, which cannot be\napproximated by any 2-layer network, to more than a certain constant accuracy,\nunless its width is exponential in the dimension. The result holds for\nvirtually all known activation functions, including rectified linear units,\nsigmoids and thresholds, and formally demonstrates that depth -- even if\nincreased by 1 -- can be exponentially more valuable than width for standard\nfeedforward neural networks. Moreover, compared to related results in the\ncontext of Boolean functions, our result requires fewer assumptions, and the\nproof techniques and construction are very different.\n", "versions": [{"version": "v1", "created": "Sat, 12 Dec 2015 21:41:24 GMT"}, {"version": "v2", "created": "Thu, 24 Dec 2015 16:21:24 GMT"}, {"version": "v3", "created": "Fri, 25 Dec 2015 03:47:28 GMT"}, {"version": "v4", "created": "Mon, 9 May 2016 02:16:54 GMT"}], "update_date": "2016-05-10", "authors_parsed": [["Eldan", "Ronen", ""], ["Shamir", "Ohad", ""]]}, {"id": "1512.04009", "submitter": "Shenggang Ying", "authors": "Shenggang Ying, Mingsheng Ying and Yuan Feng", "title": "Quantum Privacy-Preserving Data Mining", "comments": "5 pages. Comments are welcome", "journal-ref": null, "doi": null, "report-no": null, "categories": "quant-ph cs.CR cs.DB cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data mining is a key technology in big data analytics and it can discover\nunderstandable knowledge (patterns) hidden in large data sets. Association rule\nis one of the most useful knowledge patterns, and a large number of algorithms\nhave been developed in the data mining literature to generate association rules\ncorresponding to different problems and situations. Privacy becomes a vital\nissue when data mining is used to sensitive data sets like medical records,\ncommercial data sets and national security. In this Letter, we present a\nquantum protocol for mining association rules on vertically partitioned\ndatabases. The quantum protocol can improve the privacy level preserved by\nknown classical protocols and at the same time it can exponentially reduce the\ncomputational complexity and communication cost.\n", "versions": [{"version": "v1", "created": "Sun, 13 Dec 2015 06:26:56 GMT"}, {"version": "v2", "created": "Thu, 14 Jan 2016 09:13:35 GMT"}], "update_date": "2016-01-15", "authors_parsed": [["Ying", "Shenggang", ""], ["Ying", "Mingsheng", ""], ["Feng", "Yuan", ""]]}, {"id": "1512.04011", "submitter": "Virginia Smith", "authors": "Virginia Smith, Simone Forte, Michael I. Jordan, Martin Jaggi", "title": "L1-Regularized Distributed Optimization: A Communication-Efficient\n  Primal-Dual Framework", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite the importance of sparsity in many large-scale applications, there\nare few methods for distributed optimization of sparsity-inducing objectives.\nIn this paper, we present a communication-efficient framework for\nL1-regularized optimization in the distributed environment. By viewing\nclassical objectives in a more general primal-dual setting, we develop a new\nclass of methods that can be efficiently distributed and applied to common\nsparsity-inducing models, such as Lasso, sparse logistic regression, and\nelastic net-regularized problems. We provide theoretical convergence guarantees\nfor our framework, and demonstrate its efficiency and flexibility with a\nthorough experimental comparison on Amazon EC2. Our proposed framework yields\nspeedups of up to 50x as compared to current state-of-the-art methods for\ndistributed L1-regularized optimization.\n", "versions": [{"version": "v1", "created": "Sun, 13 Dec 2015 06:49:00 GMT"}, {"version": "v2", "created": "Thu, 2 Jun 2016 23:45:03 GMT"}], "update_date": "2016-06-06", "authors_parsed": [["Smith", "Virginia", ""], ["Forte", "Simone", ""], ["Jordan", "Michael I.", ""], ["Jaggi", "Martin", ""]]}, {"id": "1512.04036", "submitter": "Shixia Liu", "authors": "Yangxin Zhong, Shixia Liu, Xiting Wang, Jiannan Xiao, and Yangqiu Song", "title": "Tracking Idea Flows between Social Groups", "comments": "8 pages, AAAI 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In many applications, ideas that are described by a set of words often flow\nbetween different groups. To facilitate users in analyzing the flow, we present\na method to model the flow behaviors that aims at identifying the lead-lag\nrelationships between word clusters of different user groups. In particular, an\nimproved Bayesian conditional cointegration based on dynamic time warping is\nemployed to learn links between words in different groups. A tensor-based\ntechnique is developed to cluster these linked words into different clusters\n(ideas) and track the flow of ideas. The main feature of the tensor\nrepresentation is that we introduce two additional dimensions to represent both\ntime and lead-lag relationships. Experiments on both synthetic and real\ndatasets show that our method is more effective than methods based on\ntraditional clustering techniques and achieves better accuracy. A case study\nwas conducted to demonstrate the usefulness of our method in helping users\nunderstand the flow of ideas between different user groups on social media\n", "versions": [{"version": "v1", "created": "Sun, 13 Dec 2015 11:33:44 GMT"}], "update_date": "2015-12-15", "authors_parsed": [["Zhong", "Yangxin", ""], ["Liu", "Shixia", ""], ["Wang", "Xiting", ""], ["Xiao", "Jiannan", ""], ["Song", "Yangqiu", ""]]}, {"id": "1512.04039", "submitter": "Martin Jaggi", "authors": "Chenxin Ma, Jakub Kone\\v{c}n\\'y, Martin Jaggi, Virginia Smith, Michael\n  I. Jordan, Peter Richt\\'arik and Martin Tak\\'a\\v{c}", "title": "Distributed Optimization with Arbitrary Local Solvers", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the growth of data and necessity for distributed optimization methods,\nsolvers that work well on a single machine must be re-designed to leverage\ndistributed computation. Recent work in this area has been limited by focusing\nheavily on developing highly specific methods for the distributed environment.\nThese special-purpose methods are often unable to fully leverage the\ncompetitive performance of their well-tuned and customized single machine\ncounterparts. Further, they are unable to easily integrate improvements that\ncontinue to be made to single machine methods. To this end, we present a\nframework for distributed optimization that both allows the flexibility of\narbitrary solvers to be used on each (single) machine locally, and yet\nmaintains competitive performance against other state-of-the-art\nspecial-purpose distributed methods. We give strong primal-dual convergence\nrate guarantees for our framework that hold for arbitrary local solvers. We\ndemonstrate the impact of local solver selection both theoretically and in an\nextensive experimental comparison. Finally, we provide thorough implementation\ndetails for our framework, highlighting areas for practical performance gains.\n", "versions": [{"version": "v1", "created": "Sun, 13 Dec 2015 11:58:03 GMT"}, {"version": "v2", "created": "Wed, 3 Aug 2016 17:14:21 GMT"}], "update_date": "2016-08-04", "authors_parsed": [["Ma", "Chenxin", ""], ["Kone\u010dn\u00fd", "Jakub", ""], ["Jaggi", "Martin", ""], ["Smith", "Virginia", ""], ["Jordan", "Michael I.", ""], ["Richt\u00e1rik", "Peter", ""], ["Tak\u00e1\u010d", "Martin", ""]]}, {"id": "1512.04052", "submitter": "Fionn Murtagh", "authors": "Fionn Murtagh", "title": "Big Data Scaling through Metric Mapping: Exploiting the Remarkable\n  Simplicity of Very High Dimensional Spaces using Correspondence Analysis", "comments": "13 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present new findings in regard to data analysis in very high dimensional\nspaces. We use dimensionalities up to around one million. A particular benefit\nof Correspondence Analysis is its suitability for carrying out an orthonormal\nmapping, or scaling, of power law distributed data. Power law distributed data\nare found in many domains. Correspondence factor analysis provides a latent\nsemantic or principal axes mapping. Our experiments use data from digital\nchemistry and finance, and other statistically generated data.\n", "versions": [{"version": "v1", "created": "Sun, 13 Dec 2015 13:34:32 GMT"}], "update_date": "2015-12-15", "authors_parsed": [["Murtagh", "Fionn", ""]]}, {"id": "1512.04087", "submitter": "Harm van Seijen", "authors": "Harm van Seijen and A. Rupam Mahmood and Patrick M. Pilarski and\n  Marlos C. Machado and Richard S. Sutton", "title": "True Online Temporal-Difference Learning", "comments": "This is the published JMLR version. It is a much improved version.\n  The main changes are: 1) re-structuring of the article; 2) additional\n  analysis on the forward view; 3) empirical comparison of traditional and new\n  forward view; 4) added discussion of other true online papers; 5) updated\n  discussion for non-linear function approximation", "journal-ref": "Journal of Machine Learning Research (JMLR), 17(145):1-40, 2016", "doi": null, "report-no": null, "categories": "cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The temporal-difference methods TD($\\lambda$) and Sarsa($\\lambda$) form a\ncore part of modern reinforcement learning. Their appeal comes from their good\nperformance, low computational cost, and their simple interpretation, given by\ntheir forward view. Recently, new versions of these methods were introduced,\ncalled true online TD($\\lambda$) and true online Sarsa($\\lambda$), respectively\n(van Seijen & Sutton, 2014). These new versions maintain an exact equivalence\nwith the forward view at all times, whereas the traditional versions only\napproximate it for small step-sizes. We hypothesize that these true online\nmethods not only have better theoretical properties, but also dominate the\nregular methods empirically. In this article, we put this hypothesis to the\ntest by performing an extensive empirical comparison. Specifically, we compare\nthe performance of true online TD($\\lambda$)/Sarsa($\\lambda$) with regular\nTD($\\lambda$)/Sarsa($\\lambda$) on random MRPs, a real-world myoelectric\nprosthetic arm, and a domain from the Arcade Learning Environment. We use\nlinear function approximation with tabular, binary, and non-binary features.\nOur results suggest that the true online methods indeed dominate the regular\nmethods. Across all domains/representations the learning speed of the true\nonline methods are often better, but never worse than that of the regular\nmethods. An additional advantage is that no choice between traces has to be\nmade for the true online methods. Besides the empirical results, we provide an\nin-depth analysis of the theory behind true online temporal-difference\nlearning. In addition, we show that new true online temporal-difference methods\ncan be derived by making changes to the online forward view and then rewriting\nthe update equations.\n", "versions": [{"version": "v1", "created": "Sun, 13 Dec 2015 17:13:33 GMT"}, {"version": "v2", "created": "Thu, 8 Sep 2016 18:56:23 GMT"}], "update_date": "2016-09-09", "authors_parsed": [["van Seijen", "Harm", ""], ["Mahmood", "A. Rupam", ""], ["Pilarski", "Patrick M.", ""], ["Machado", "Marlos C.", ""], ["Sutton", "Richard S.", ""]]}, {"id": "1512.04092", "submitter": "Shagun Sodhani", "authors": "Sanket Mehta, Shagun Sodhani", "title": "Stack Exchange Tagger", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The goal of our project is to develop an accurate tagger for questions posted\non Stack Exchange. Our problem is an instance of the more general problem of\ndeveloping accurate classifiers for large scale text datasets. We are tackling\nthe multilabel classification problem where each item (in this case, question)\ncan belong to multiple classes (in this case, tags). We are predicting the tags\n(or keywords) for a particular Stack Exchange post given only the question text\nand the title of the post. In the process, we compare the performance of\nSupport Vector Classification (SVC) for different kernel functions, loss\nfunction, etc. We found linear SVC with Crammer Singer technique produces best\nresults.\n", "versions": [{"version": "v1", "created": "Sun, 13 Dec 2015 17:52:44 GMT"}], "update_date": "2015-12-15", "authors_parsed": [["Mehta", "Sanket", ""], ["Sodhani", "Shagun", ""]]}, {"id": "1512.04105", "submitter": "Lucas Lehnert", "authors": "Lucas Lehnert and Doina Precup", "title": "Policy Gradient Methods for Off-policy Control", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Off-policy learning refers to the problem of learning the value function of a\nway of behaving, or policy, while following a different policy. Gradient-based\noff-policy learning algorithms, such as GTD and TDC/GQ, converge even when\nusing function approximation and incremental updates. However, they have been\ndeveloped for the case of a fixed behavior policy. In control problems, one\nwould like to adapt the behavior policy over time to become more greedy with\nrespect to the existing value function. In this paper, we present the first\ngradient-based learning algorithms for this problem, which rely on the\nframework of policy gradient in order to modify the behavior policy. We present\nderivations of the algorithms, a convergence theorem, and empirical evidence\nshowing that they compare favorably to existing approaches.\n", "versions": [{"version": "v1", "created": "Sun, 13 Dec 2015 19:20:14 GMT"}], "update_date": "2015-12-15", "authors_parsed": [["Lehnert", "Lucas", ""], ["Precup", "Doina", ""]]}, {"id": "1512.04152", "submitter": "Chansoo Lee", "authors": "Jacob Abernethy, Chansoo Lee, Ambuj Tewari", "title": "Fighting Bandits with a New Kind of Smoothness", "comments": "In Proceedings of NIPS, 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.GT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We define a novel family of algorithms for the adversarial multi-armed bandit\nproblem, and provide a simple analysis technique based on convex smoothing. We\nprove two main results. First, we show that regularization via the\n\\emph{Tsallis entropy}, which includes EXP3 as a special case, achieves the\n$\\Theta(\\sqrt{TN})$ minimax regret. Second, we show that a wide class of\nperturbation methods achieve a near-optimal regret as low as $O(\\sqrt{TN \\log\nN})$ if the perturbation distribution has a bounded hazard rate. For example,\nthe Gumbel, Weibull, Frechet, Pareto, and Gamma distributions all satisfy this\nkey property.\n", "versions": [{"version": "v1", "created": "Mon, 14 Dec 2015 01:57:02 GMT"}], "update_date": "2015-12-15", "authors_parsed": [["Abernethy", "Jacob", ""], ["Lee", "Chansoo", ""], ["Tewari", "Ambuj", ""]]}, {"id": "1512.04202", "submitter": "Xi-Lin Li", "authors": "Xi-Lin Li", "title": "Preconditioned Stochastic Gradient Descent", "comments": "13 pages, 9 figures. To appear in IEEE Transactions on Neural\n  Networks and Learning Systems. Supplemental materials on\n  https://sites.google.com/site/lixilinx/home/psgd", "journal-ref": null, "doi": "10.1109/TNNLS.2017.2672978", "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Stochastic gradient descent (SGD) still is the workhorse for many practical\nproblems. However, it converges slow, and can be difficult to tune. It is\npossible to precondition SGD to accelerate its convergence remarkably. But many\nattempts in this direction either aim at solving specialized problems, or\nresult in significantly more complicated methods than SGD. This paper proposes\na new method to estimate a preconditioner such that the amplitudes of\nperturbations of preconditioned stochastic gradient match that of the\nperturbations of parameters to be optimized in a way comparable to Newton\nmethod for deterministic optimization. Unlike the preconditioners based on\nsecant equation fitting as done in deterministic quasi-Newton methods, which\nassume positive definite Hessian and approximate its inverse, the new\npreconditioner works equally well for both convex and non-convex optimizations\nwith exact or noisy gradients. When stochastic gradient is used, it can\nnaturally damp the gradient noise to stabilize SGD. Efficient preconditioner\nestimation methods are developed, and with reasonable simplifications, they are\napplicable to large scaled problems. Experimental results demonstrate that\nequipped with the new preconditioner, without any tuning effort, preconditioned\nSGD can efficiently solve many challenging problems like the training of a deep\nneural network or a recurrent neural network requiring extremely long term\nmemories.\n", "versions": [{"version": "v1", "created": "Mon, 14 Dec 2015 07:14:09 GMT"}, {"version": "v2", "created": "Wed, 14 Sep 2016 21:51:02 GMT"}, {"version": "v3", "created": "Wed, 22 Feb 2017 06:58:30 GMT"}], "update_date": "2017-02-23", "authors_parsed": [["Li", "Xi-Lin", ""]]}, {"id": "1512.04280", "submitter": "Liang Lu", "authors": "Liang Lu and Steve Renals", "title": "Small-footprint Deep Neural Networks with Highway Connections for Speech\n  Recognition", "comments": "5 pages, 3 figures, fixed typo, accepted by Interspeech 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For speech recognition, deep neural networks (DNNs) have significantly\nimproved the recognition accuracy in most of benchmark datasets and application\ndomains. However, compared to the conventional Gaussian mixture models,\nDNN-based acoustic models usually have much larger number of model parameters,\nmaking it challenging for their applications in resource constrained platforms,\ne.g., mobile devices. In this paper, we study the application of the recently\nproposed highway network to train small-footprint DNNs, which are {\\it thinner}\nand {\\it deeper}, and have significantly smaller number of model parameters\ncompared to conventional DNNs. We investigated this approach on the AMI meeting\nspeech transcription corpus which has around 70 hours of audio data. The\nhighway neural networks constantly outperformed their plain DNN counterparts,\nand the number of model parameters can be reduced significantly without\nsacrificing the recognition accuracy.\n", "versions": [{"version": "v1", "created": "Mon, 14 Dec 2015 12:29:32 GMT"}, {"version": "v2", "created": "Thu, 3 Mar 2016 12:14:06 GMT"}, {"version": "v3", "created": "Mon, 20 Jun 2016 10:30:54 GMT"}, {"version": "v4", "created": "Wed, 14 Jun 2017 15:17:27 GMT"}], "update_date": "2017-06-15", "authors_parsed": [["Lu", "Liang", ""], ["Renals", "Steve", ""]]}, {"id": "1512.04295", "submitter": "Lukas Cavigelli", "authors": "Lukas Cavigelli, Luca Benini", "title": "Origami: A 803 GOp/s/W Convolutional Network Accelerator", "comments": "14 pages", "journal-ref": null, "doi": "10.1109/TCSVT.2016.2592330", "report-no": null, "categories": "cs.CV cs.AI cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An ever increasing number of computer vision and image/video processing\nchallenges are being approached using deep convolutional neural networks,\nobtaining state-of-the-art results in object recognition and detection,\nsemantic segmentation, action recognition, optical flow and superresolution.\nHardware acceleration of these algorithms is essential to adopt these\nimprovements in embedded and mobile computer vision systems. We present a new\narchitecture, design and implementation as well as the first reported silicon\nmeasurements of such an accelerator, outperforming previous work in terms of\npower-, area- and I/O-efficiency. The manufactured device provides up to 196\nGOp/s on 3.09 mm^2 of silicon in UMC 65nm technology and can achieve a power\nefficiency of 803 GOp/s/W. The massively reduced bandwidth requirements make it\nthe first architecture scalable to TOp/s performance.\n", "versions": [{"version": "v1", "created": "Mon, 14 Dec 2015 13:06:43 GMT"}, {"version": "v2", "created": "Tue, 19 Jan 2016 22:56:41 GMT"}], "update_date": "2016-11-11", "authors_parsed": [["Cavigelli", "Lukas", ""], ["Benini", "Luca", ""]]}, {"id": "1512.04392", "submitter": "Henry Y.T. Ngan", "authors": "Li-Li Wang, Henry Y.T. Ngan, Nelson H.C. Yung", "title": "Automatic Incident Classification for Big Traffic Data by Adaptive\n  Boosting SVM", "comments": "27 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modern cities experience heavy traffic flows and congestions regularly across\nspace and time. Monitoring traffic situations becomes an important challenge\nfor the Traffic Control and Surveillance Systems (TCSS). In advanced TCSS, it\nis helpful to automatically detect and classify different traffic incidents\nsuch as severity of congestion, abnormal driving pattern, abrupt or illegal\nstop on road, etc. Although most TCSS are equipped with basic incident\ndetection algorithms, they are however crude to be really useful as an\nautomated tool for further classification. In literature, there is a lack of\nresearch for Automated Incident Classification (AIC). Therefore, a novel AIC\nmethod is proposed in this paper to tackle such challenges. In the proposed\nmethod, traffic signals are firstly extracted from captured videos and\nconverted as spatial-temporal (ST) signals. Based on the characteristics of the\nST signals, a set of realistic simulation data are generated to construct an\nextended big traffic database to cover a variety of traffic situations. Next, a\nMean-Shift filter is introduced to suppress the effect of noise and extract\nsignificant features from the ST signals. The extracted features are then\nassociated with various types of traffic data: one normal type (inliers) and\nmultiple abnormal types (outliers). For the classification, an adaptive\nboosting classifier is trained to detect outliers in traffic data\nautomatically. Further, a Support Vector Machine (SVM) based method is adopted\nto train the model for identifying the categories of outliers. In short, this\nhybrid approach is called an Adaptive Boosting Support Vector Machines (AB-SVM)\nmethod. Experimental results show that the proposed AB-SVM method achieves a\nsatisfied result with more than 92% classification accuracy on average.\n", "versions": [{"version": "v1", "created": "Mon, 14 Dec 2015 16:23:59 GMT"}, {"version": "v2", "created": "Mon, 28 Dec 2015 08:28:16 GMT"}], "update_date": "2015-12-29", "authors_parsed": [["Wang", "Li-Li", ""], ["Ngan", "Henry Y. T.", ""], ["Yung", "Nelson H. C.", ""]]}, {"id": "1512.04407", "submitter": "Arjun Chandrasekaran", "authors": "Arjun Chandrasekaran, Ashwin K. Vijayakumar, Stanislaw Antol, Mohit\n  Bansal, Dhruv Batra, C. Lawrence Zitnick and Devi Parikh", "title": "We Are Humor Beings: Understanding and Predicting Visual Humor", "comments": "17 pages, 16 figures, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Humor is an integral part of human lives. Despite being tremendously\nimpactful, it is perhaps surprising that we do not have a detailed\nunderstanding of humor yet. As interactions between humans and AI systems\nincrease, it is imperative that these systems are taught to understand\nsubtleties of human expressions such as humor. In this work, we are interested\nin the question - what content in a scene causes it to be funny? As a first\nstep towards understanding visual humor, we analyze the humor manifested in\nabstract scenes and design computational models for them. We collect two\ndatasets of abstract scenes that facilitate the study of humor at both the\nscene-level and the object-level. We analyze the funny scenes and explore the\ndifferent types of humor depicted in them via human studies. We model two tasks\nthat we believe demonstrate an understanding of some aspects of visual humor.\nThe tasks involve predicting the funniness of a scene and altering the\nfunniness of a scene. We show that our models perform well quantitatively, and\nqualitatively through human studies. Our datasets are publicly available.\n", "versions": [{"version": "v1", "created": "Mon, 14 Dec 2015 16:59:35 GMT"}, {"version": "v2", "created": "Wed, 16 Dec 2015 02:12:49 GMT"}, {"version": "v3", "created": "Sun, 10 Apr 2016 22:15:43 GMT"}, {"version": "v4", "created": "Thu, 5 May 2016 21:36:13 GMT"}], "update_date": "2016-05-09", "authors_parsed": [["Chandrasekaran", "Arjun", ""], ["Vijayakumar", "Ashwin K.", ""], ["Antol", "Stanislaw", ""], ["Bansal", "Mohit", ""], ["Batra", "Dhruv", ""], ["Zitnick", "C. Lawrence", ""], ["Parikh", "Devi", ""]]}, {"id": "1512.04433", "submitter": "Samet Oymak", "authors": "Samet Oymak, Ben Recht", "title": "Near-Optimal Bounds for Binary Embeddings of Arbitrary Sets", "comments": "20 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS math.FA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study embedding a subset $K$ of the unit sphere to the Hamming cube\n$\\{-1,+1\\}^m$. We characterize the tradeoff between distortion and sample\ncomplexity $m$ in terms of the Gaussian width $\\omega(K)$ of the set. For\nsubspaces and several structured sets we show that Gaussian maps provide the\noptimal tradeoff $m\\sim \\delta^{-2}\\omega^2(K)$, in particular for $\\delta$\ndistortion one needs $m\\approx\\delta^{-2}{d}$ where $d$ is the subspace\ndimension. For general sets, we provide sharp characterizations which reduces\nto $m\\approx{\\delta^{-4}}{\\omega^2(K)}$ after simplification. We provide\nimproved results for local embedding of points that are in close proximity of\neach other which is related to locality sensitive hashing. We also discuss\nfaster binary embedding where one takes advantage of an initial sketching\nprocedure based on Fast Johnson-Lindenstauss Transform. Finally, we list\nseveral numerical observations and discuss open problems.\n", "versions": [{"version": "v1", "created": "Mon, 14 Dec 2015 17:57:41 GMT"}], "update_date": "2015-12-15", "authors_parsed": [["Oymak", "Samet", ""], ["Recht", "Ben", ""]]}, {"id": "1512.04455", "submitter": "Jonathan Hunt", "authors": "Nicolas Heess, Jonathan J Hunt, Timothy P Lillicrap, David Silver", "title": "Memory-based control with recurrent neural networks", "comments": "NIPS Deep Reinforcement Learning Workshop 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Partially observed control problems are a challenging aspect of reinforcement\nlearning. We extend two related, model-free algorithms for continuous control\n-- deterministic policy gradient and stochastic value gradient -- to solve\npartially observed domains using recurrent neural networks trained with\nbackpropagation through time.\n  We demonstrate that this approach, coupled with long-short term memory is\nable to solve a variety of physical control problems exhibiting an assortment\nof memory requirements. These include the short-term integration of information\nfrom noisy sensors and the identification of system parameters, as well as\nlong-term memory problems that require preserving information over many time\nsteps. We also demonstrate success on a combined exploration and memory problem\nin the form of a simplified version of the well-known Morris water maze task.\nFinally, we show that our approach can deal with high-dimensional observations\nby learning directly from pixels.\n  We find that recurrent deterministic and stochastic policies are able to\nlearn similarly good solutions to these tasks, including the water maze where\nthe agent must learn effective search strategies.\n", "versions": [{"version": "v1", "created": "Mon, 14 Dec 2015 18:44:48 GMT"}], "update_date": "2015-12-15", "authors_parsed": [["Heess", "Nicolas", ""], ["Hunt", "Jonathan J", ""], ["Lillicrap", "Timothy P", ""], ["Silver", "David", ""]]}, {"id": "1512.04466", "submitter": "Shuangfei Zhai", "authors": "Shuangfei Zhai, Zhongfei Zhang", "title": "Semisupervised Autoencoder for Sentiment Analysis", "comments": "To appear in AAAI 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we investigate the usage of autoencoders in modeling textual\ndata. Traditional autoencoders suffer from at least two aspects: scalability\nwith the high dimensionality of vocabulary size and dealing with\ntask-irrelevant words. We address this problem by introducing supervision via\nthe loss function of autoencoders. In particular, we first train a linear\nclassifier on the labeled data, then define a loss for the autoencoder with the\nweights learned from the linear classifier. To reduce the bias brought by one\nsingle classifier, we define a posterior probability distribution on the\nweights of the classifier, and derive the marginalized loss of the autoencoder\nwith Laplace approximation. We show that our choice of loss function can be\nrationalized from the perspective of Bregman Divergence, which justifies the\nsoundness of our model. We evaluate the effectiveness of our model on six\nsentiment analysis datasets, and show that our model significantly outperforms\nall the competing methods with respect to classification accuracy. We also show\nthat our model is able to take advantage of unlabeled dataset and get improved\nperformance. We further show that our model successfully learns highly\ndiscriminative feature maps, which explains its superior performance.\n", "versions": [{"version": "v1", "created": "Mon, 14 Dec 2015 19:09:53 GMT"}], "update_date": "2015-12-15", "authors_parsed": [["Zhai", "Shuangfei", ""], ["Zhang", "Zhongfei", ""]]}, {"id": "1512.04469", "submitter": "Martin Thoma", "authors": "Martin Thoma", "title": "\\\"Uber die Klassifizierung von Knoten in dynamischen Netzwerken mit\n  Inhalt", "comments": "in German. This term paper was handed in on 17.01.2014", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper explains the DYCOS-Algorithm as it was introduced in by Aggarwal\nand Li in 2011. It operates on graphs whichs nodes are partially labeled and\nautomatically adds missing labels to nodes. To do so, the DYCOS algorithm makes\nuse of the structure of the graph as well as content which is assigned to the\nnode. Aggarwal and Li measured in an experimental analysis that DYCOS adds the\nmissing labels to a Graph with 19396 nodes of which 14814 are labeled and\nanother Graph with 806635 nodes of which 18999 are labeld on one core of an\nIntel Xeon 2.5 GHz CPU with 32 G RAM within less than a minute. Additionally,\nextensions of the DYCOS algorithm are proposed.\n  -----\n  In dieser Arbeit wird der DYCOS-Algorithmus, wie er 2011 von Aggarwal und Li\nvorgestellt wurde, erkl\\\"art. Er arbeitet auf Graphen, deren Knoten teilweise\nmit Beschriftungen versehen sind und erg\\\"anzt automatisch Beschriftungen f\\\"ur\nKnoten, die bisher noch keine Beschriftung haben. Dieser Vorgang wird\n\"Klassifizierung\" genannt. Dazu verwendet er die Struktur des Graphen sowie\ntextuelle Informationen, die den Knoten zugeordnet sind. Die von Aggarwal und\nLi beschriebene experimentelle Analyse ergab, dass er auch auf dynamischen\nGraphen mit 19396 bzw. 806635 Knoten, von denen nur 14814 bzw. 18999\nbeschriftet waren, innerhalb von weniger als einer Minute auf einem Kern einer\nIntel Xeon 2.5 GHz CPU mit 32 G RAM ausgef\\\"uhrt werden kann. Zus\\\"atzlich wird\ndie Ver\\\"offentlichung von Aggarwal und Li kritisch er\\\"ortert und und es\nwerden m\\\"ogliche Erweiterungen des DYCOS-Algorithmus vorgeschlagen.\n", "versions": [{"version": "v1", "created": "Mon, 23 Nov 2015 13:28:11 GMT"}], "update_date": "2015-12-15", "authors_parsed": [["Thoma", "Martin", ""]]}, {"id": "1512.04483", "submitter": "Shuangfei Zhai", "authors": "Shuangfei Zhai, Zhongfei Zhang", "title": "Dropout Training of Matrix Factorization and Autoencoder for Link\n  Prediction in Sparse Graphs", "comments": "Published in SDM 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Matrix factorization (MF) and Autoencoder (AE) are among the most successful\napproaches of unsupervised learning. While MF based models have been\nextensively exploited in the graph modeling and link prediction literature, the\nAE family has not gained much attention. In this paper we investigate both MF\nand AE's application to the link prediction problem in sparse graphs. We show\nthe connection between AE and MF from the perspective of multiview learning,\nand further propose MF+AE: a model training MF and AE jointly with shared\nparameters. We apply dropout to training both the MF and AE parts, and show\nthat it can significantly prevent overfitting by acting as an adaptive\nregularization. We conduct experiments on six real world sparse graph datasets,\nand show that MF+AE consistently outperforms the competing methods, especially\non datasets that demonstrate strong non-cohesive structures.\n", "versions": [{"version": "v1", "created": "Mon, 14 Dec 2015 19:38:14 GMT"}], "update_date": "2015-12-15", "authors_parsed": [["Zhai", "Shuangfei", ""], ["Zhang", "Zhongfei", ""]]}, {"id": "1512.04509", "submitter": "Kumar Eswaran Dr.", "authors": "K.Eswaran and K.Damodhar Rao", "title": "On non-iterative training of a neural classifier", "comments": "18 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently an algorithm, was discovered, which separates points in n-dimension\nby planes in such a manner that no two points are left un-separated by at least\none plane{[}1-3{]}. By using this new algorithm we show that there are two ways\nof classification by a neural network, for a large dimension feature space,\nboth of which are non-iterative and deterministic. To demonstrate the power of\nboth these methods we apply them exhaustively to the classical pattern\nrecognition problem: The Fisher-Anderson's, IRIS flower data set and present\nthe results.\n  It is expected these methods will now be widely used for the training of\nneural networks for Deep Learning not only because of their non-iterative and\ndeterministic nature but also because of their efficiency and speed and will\nsupersede other classification methods which are iterative in nature and rely\non error minimization.\n", "versions": [{"version": "v1", "created": "Mon, 14 Dec 2015 20:44:12 GMT"}, {"version": "v2", "created": "Sun, 20 Dec 2015 04:32:01 GMT"}], "update_date": "2015-12-22", "authors_parsed": [["Eswaran", "K.", ""], ["Rao", "K. Damodhar", ""]]}, {"id": "1512.04564", "submitter": "Hung Nien", "authors": "Hung Nien and Jeffrey A. Fessler", "title": "Relaxed Linearized Algorithms for Faster X-Ray CT Image Reconstruction", "comments": "Submitted to IEEE Transactions on Medical Imaging", "journal-ref": "IEEE Transactions on Medical Imaging 35(4):1090-8 Apr 2016", "doi": "10.1109/TMI.2015.2508780", "report-no": null, "categories": "math.OC cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Statistical image reconstruction (SIR) methods are studied extensively for\nX-ray computed tomography (CT) due to the potential of acquiring CT scans with\nreduced X-ray dose while maintaining image quality. However, the longer\nreconstruction time of SIR methods hinders their use in X-ray CT in practice.\nTo accelerate statistical methods, many optimization techniques have been\ninvestigated. Over-relaxation is a common technique to speed up convergence of\niterative algorithms. For instance, using a relaxation parameter that is close\nto two in alternating direction method of multipliers (ADMM) has been shown to\nspeed up convergence significantly. This paper proposes a relaxed linearized\naugmented Lagrangian (AL) method that shows theoretical faster convergence rate\nwith over-relaxation and applies the proposed relaxed linearized AL method to\nX-ray CT image reconstruction problems. Experimental results with both\nsimulated and real CT scan data show that the proposed relaxed algorithm (with\nordered-subsets [OS] acceleration) is about twice as fast as the existing\nunrelaxed fast algorithms, with negligible computation and memory overhead.\n", "versions": [{"version": "v1", "created": "Mon, 14 Dec 2015 21:31:23 GMT"}], "update_date": "2019-06-14", "authors_parsed": [["Nien", "Hung", ""], ["Fessler", "Jeffrey A.", ""]]}, {"id": "1512.04754", "submitter": "Ulugbek Kamilov", "authors": "Ulugbek S. Kamilov and Hassan Mansour", "title": "Learning optimal nonlinearities for iterative thresholding algorithms", "comments": null, "journal-ref": null, "doi": "10.1109/LSP.2016.2548245", "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Iterative shrinkage/thresholding algorithm (ISTA) is a well-studied method\nfor finding sparse solutions to ill-posed inverse problems. In this letter, we\npresent a data-driven scheme for learning optimal thresholding functions for\nISTA. The proposed scheme is obtained by relating iterations of ISTA to layers\nof a simple deep neural network (DNN) and developing a corresponding error\nbackpropagation algorithm that allows to fine-tune the thresholding functions.\nSimulations on sparse statistical signals illustrate potential gains in\nestimation quality due to the proposed data adaptive ISTA.\n", "versions": [{"version": "v1", "created": "Tue, 15 Dec 2015 12:20:17 GMT"}], "update_date": "2016-05-04", "authors_parsed": [["Kamilov", "Ulugbek S.", ""], ["Mansour", "Hassan", ""]]}, {"id": "1512.04792", "submitter": "Han Xiao Bookman", "authors": "Han Xiao, Minlie Huang, Xiaoyan Zhu", "title": "From One Point to A Manifold: Knowledge Graph Embedding For Precise Link\n  Prediction", "comments": "arXiv admin note: text overlap with arXiv:1509.05488", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Knowledge graph embedding aims at offering a numerical knowledge\nrepresentation paradigm by transforming the entities and relations into\ncontinuous vector space. However, existing methods could not characterize the\nknowledge graph in a fine degree to make a precise prediction. There are two\nreasons: being an ill-posed algebraic system and applying an overstrict\ngeometric form. As precise prediction is critical, we propose an manifold-based\nembedding principle (\\textbf{ManifoldE}) which could be treated as a well-posed\nalgebraic system that expands the position of golden triples from one point in\ncurrent models to a manifold in ours. Extensive experiments show that the\nproposed models achieve substantial improvements against the state-of-the-art\nbaselines especially for the precise prediction task, and yet maintain high\nefficiency.\n", "versions": [{"version": "v1", "created": "Tue, 15 Dec 2015 14:24:44 GMT"}, {"version": "v2", "created": "Sun, 27 Dec 2015 14:14:51 GMT"}, {"version": "v3", "created": "Mon, 25 Jan 2016 09:47:10 GMT"}, {"version": "v4", "created": "Tue, 13 Jun 2017 06:38:13 GMT"}, {"version": "v5", "created": "Sat, 17 Jun 2017 03:59:43 GMT"}], "update_date": "2017-06-20", "authors_parsed": [["Xiao", "Han", ""], ["Huang", "Minlie", ""], ["Zhu", "Xiaoyan", ""]]}, {"id": "1512.04808", "submitter": "Sebastian Weichwald", "authors": "Sebastian Weichwald, Bernhard Sch\\\"olkopf, Tonio Ball, Moritz\n  Grosse-Wentrup", "title": "Causal and anti-causal learning in pattern recognition for neuroimaging", "comments": "accepted manuscript", "journal-ref": "Pattern Recognition in Neuroimaging, 2014 International Workshop\n  on, 1-4, 2014", "doi": "10.1109/PRNI.2014.6858551", "report-no": null, "categories": "stat.ML cs.LG q-bio.NC stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Pattern recognition in neuroimaging distinguishes between two types of\nmodels: encoding- and decoding models. This distinction is based on the insight\nthat brain state features, that are found to be relevant in an experimental\nparadigm, carry a different meaning in encoding- than in decoding models. In\nthis paper, we argue that this distinction is not sufficient: Relevant features\nin encoding- and decoding models carry a different meaning depending on whether\nthey represent causal- or anti-causal relations. We provide a theoretical\njustification for this argument and conclude that causal inference is essential\nfor interpretation in neuroimaging.\n", "versions": [{"version": "v1", "created": "Tue, 15 Dec 2015 15:05:00 GMT"}], "update_date": "2015-12-16", "authors_parsed": [["Weichwald", "Sebastian", ""], ["Sch\u00f6lkopf", "Bernhard", ""], ["Ball", "Tonio", ""], ["Grosse-Wentrup", "Moritz", ""]]}, {"id": "1512.04829", "submitter": "Wouter Kouw", "authors": "Wouter M. Kouw, Jesse H. Krijthe, Marco Loog and Laurens J.P. van der\n  Maaten", "title": "Feature-Level Domain Adaptation", "comments": "32 pages, 13 figures, 9 tables", "journal-ref": "JMLR 17:171 (2016) 1-32", "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Domain adaptation is the supervised learning setting in which the training\nand test data are sampled from different distributions: training data is\nsampled from a source domain, whilst test data is sampled from a target domain.\nThis paper proposes and studies an approach, called feature-level domain\nadaptation (FLDA), that models the dependence between the two domains by means\nof a feature-level transfer model that is trained to describe the transfer from\nsource to target domain. Subsequently, we train a domain-adapted classifier by\nminimizing the expected loss under the resulting transfer model. For linear\nclassifiers and a large family of loss functions and transfer models, this\nexpected loss can be computed or approximated analytically, and minimized\nefficiently. Our empirical evaluation of FLDA focuses on problems comprising\nbinary and count data in which the transfer can be naturally modeled via a\ndropout distribution, which allows the classifier to adapt to differences in\nthe marginal probability of features in the source and the target domain. Our\nexperiments on several real-world problems show that FLDA performs on par with\nstate-of-the-art domain-adaptation techniques.\n", "versions": [{"version": "v1", "created": "Tue, 15 Dec 2015 15:55:55 GMT"}, {"version": "v2", "created": "Tue, 7 Jun 2016 20:06:47 GMT"}], "update_date": "2016-10-21", "authors_parsed": [["Kouw", "Wouter M.", ""], ["Krijthe", "Jesse H.", ""], ["Loog", "Marco", ""], ["van der Maaten", "Laurens J. P.", ""]]}, {"id": "1512.04848", "submitter": "Travis Dick", "authors": "Travis Dick, Mu Li, Venkata Krishna Pillutla, Colin White, Maria\n  Florina Balcan, Alex Smola", "title": "Data Driven Resource Allocation for Distributed Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In distributed machine learning, data is dispatched to multiple machines for\nprocessing. Motivated by the fact that similar data points often belong to the\nsame or similar classes, and more generally, classification rules of high\naccuracy tend to be \"locally simple but globally complex\" (Vapnik & Bottou\n1993), we propose data dependent dispatching that takes advantage of such\nstructure. We present an in-depth analysis of this model, providing new\nalgorithms with provable worst-case guarantees, analysis proving existing\nscalable heuristics perform well in natural non worst-case conditions, and\ntechniques for extending a dispatching rule from a small sample to the entire\ndistribution. We overcome novel technical challenges to satisfy important\nconditions for accurate distributed learning, including fault tolerance and\nbalancedness. We empirically compare our approach with baselines based on\nrandom partitioning, balanced partition trees, and locality sensitive hashing,\nshowing that we achieve significantly higher accuracy on both synthetic and\nreal world image and advertising datasets. We also demonstrate that our\ntechnique strongly scales with the available computing power.\n", "versions": [{"version": "v1", "created": "Tue, 15 Dec 2015 16:41:42 GMT"}, {"version": "v2", "created": "Thu, 15 Dec 2016 20:45:52 GMT"}], "update_date": "2016-12-16", "authors_parsed": [["Dick", "Travis", ""], ["Li", "Mu", ""], ["Pillutla", "Venkata Krishna", ""], ["White", "Colin", ""], ["Balcan", "Maria Florina", ""], ["Smola", "Alex", ""]]}, {"id": "1512.04857", "submitter": "Mario Goldenbaum", "authors": "Kiril Ralinovski, Mario Goldenbaum, and S{\\l}awomir Sta\\'nczak", "title": "Energy-Efficient Classification for Anomaly Detection: The Wireless\n  Channel as a Helper", "comments": "submitted for possible conference publication", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.LG math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Anomaly detection has various applications including condition monitoring and\nfault diagnosis. The objective is to sense the environment, learn the normal\nsystem state, and then periodically classify whether the instantaneous state\ndeviates from the normal one or not. A flexible and cost-effective way of\nmonitoring a system state is to use a wireless sensor network. In the\ntraditional approach, the sensors encode their observations and transmit them\nto a fusion center by means of some interference avoiding channel access\nmethod. The fusion center then decodes all the data and classifies the\ncorresponding system state. As this approach can be highly inefficient in terms\nof energy consumption, in this paper we propose a transmission scheme that\nexploits interference for carrying out the anomaly detection directly in the\nair. In other words, the wireless channel helps the fusion center to retrieve\nthe sought classification outcome immediately from the channel output. To\nachieve this, the chosen learning model is linear support vector machines.\nAfter discussing the proposed scheme and proving its reliability, we present\nnumerical examples demonstrating that the scheme reduces the energy consumption\nfor anomaly detection by up to 53% compared to a strategy that uses time\ndivision multiple-access.\n", "versions": [{"version": "v1", "created": "Tue, 15 Dec 2015 17:05:38 GMT"}], "update_date": "2015-12-16", "authors_parsed": [["Ralinovski", "Kiril", ""], ["Goldenbaum", "Mario", ""], ["Sta\u0144czak", "S\u0142awomir", ""]]}, {"id": "1512.04860", "submitter": "Marc G. Bellemare", "authors": "Marc G. Bellemare, Georg Ostrovski, Arthur Guez, Philip S. Thomas and\n  R\\'emi Munos", "title": "Increasing the Action Gap: New Operators for Reinforcement Learning", "comments": null, "journal-ref": "Bellemare, Marc G., Ostrovski, G., Guez, A., Thomas, Philip S.,\n  and Munos, Remi. Increasing the Action Gap: New Operators for Reinforcement\n  Learning. Proceedings of the AAAI Conference on Artificial Intelligence, 2016", "doi": null, "report-no": null, "categories": "cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces new optimality-preserving operators on Q-functions. We\nfirst describe an operator for tabular representations, the consistent Bellman\noperator, which incorporates a notion of local policy consistency. We show that\nthis local consistency leads to an increase in the action gap at each state;\nincreasing this gap, we argue, mitigates the undesirable effects of\napproximation and estimation errors on the induced greedy policies. This\noperator can also be applied to discretized continuous space and time problems,\nand we provide empirical results evidencing superior performance in this\ncontext. Extending the idea of a locally consistent operator, we then derive\nsufficient conditions for an operator to preserve optimality, leading to a\nfamily of operators which includes our consistent Bellman operator. As\ncorollaries we provide a proof of optimality for Baird's advantage learning\nalgorithm and derive other gap-increasing operators with interesting\nproperties. We conclude with an empirical study on 60 Atari 2600 games\nillustrating the strong potential of these new operators.\n", "versions": [{"version": "v1", "created": "Tue, 15 Dec 2015 17:13:49 GMT"}], "update_date": "2015-12-16", "authors_parsed": [["Bellemare", "Marc G.", ""], ["Ostrovski", "Georg", ""], ["Guez", "Arthur", ""], ["Thomas", "Philip S.", ""], ["Munos", "R\u00e9mi", ""]]}, {"id": "1512.04906", "submitter": "David Grangier", "authors": "Welin Chen and David Grangier and Michael Auli", "title": "Strategies for Training Large Vocabulary Neural Language Models", "comments": "12 pages; journal paper; under review", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Training neural network language models over large vocabularies is still\ncomputationally very costly compared to count-based models such as Kneser-Ney.\nAt the same time, neural language models are gaining popularity for many\napplications such as speech recognition and machine translation whose success\ndepends on scalability. We present a systematic comparison of strategies to\nrepresent and train large vocabularies, including softmax, hierarchical\nsoftmax, target sampling, noise contrastive estimation and self normalization.\nWe further extend self normalization to be a proper estimator of likelihood and\nintroduce an efficient variant of softmax. We evaluate each method on three\npopular benchmarks, examining performance on rare words, the speed/accuracy\ntrade-off and complementarity to Kneser-Ney.\n", "versions": [{"version": "v1", "created": "Tue, 15 Dec 2015 19:29:01 GMT"}], "update_date": "2015-12-16", "authors_parsed": [["Chen", "Welin", ""], ["Grangier", "David", ""], ["Auli", "Michael", ""]]}, {"id": "1512.04960", "submitter": "Andrew Cotter", "authors": "Andrew Cotter, Maya Gupta, Jan Pfeifer", "title": "A Light Touch for Heavily Constrained SGD", "comments": null, "journal-ref": "29th Annual Conference on Learning Theory, pp. 729-771, 2016", "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Minimizing empirical risk subject to a set of constraints can be a useful\nstrategy for learning restricted classes of functions, such as monotonic\nfunctions, submodular functions, classifiers that guarantee a certain class\nlabel for some subset of examples, etc. However, these restrictions may result\nin a very large number of constraints. Projected stochastic gradient descent\n(SGD) is often the default choice for large-scale optimization in machine\nlearning, but requires a projection after each update. For heavily-constrained\nobjectives, we propose an efficient extension of SGD that stays close to the\nfeasible region while only applying constraints probabilistically at each\niteration. Theoretical analysis shows a compelling trade-off between\nper-iteration work and the number of iterations needed on problems with a large\nnumber of constraints.\n", "versions": [{"version": "v1", "created": "Tue, 15 Dec 2015 21:07:02 GMT"}, {"version": "v2", "created": "Mon, 24 Oct 2016 20:30:25 GMT"}], "update_date": "2016-10-26", "authors_parsed": [["Cotter", "Andrew", ""], ["Gupta", "Maya", ""], ["Pfeifer", "Jan", ""]]}, {"id": "1512.05059", "submitter": "Mina Ghashami", "authors": "Mina Ghashami, Daniel Perry, Jeff M. Phillips", "title": "Streaming Kernel Principal Component Analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Kernel principal component analysis (KPCA) provides a concise set of basis\nvectors which capture non-linear structures within large data sets, and is a\ncentral tool in data analysis and learning. To allow for non-linear relations,\ntypically a full $n \\times n$ kernel matrix is constructed over $n$ data\npoints, but this requires too much space and time for large values of $n$.\nTechniques such as the Nystr\\\"om method and random feature maps can help\ntowards this goal, but they do not explicitly maintain the basis vectors in a\nstream and take more space than desired. We propose a new approach for\nstreaming KPCA which maintains a small set of basis elements in a stream,\nrequiring space only logarithmic in $n$, and also improves the dependence on\nthe error parameter. Our technique combines together random feature maps with\nrecent advances in matrix sketching, it has guaranteed spectral norm error\nbounds with respect to the original kernel matrix, and it compares favorably in\npractice to state-of-the-art approaches.\n", "versions": [{"version": "v1", "created": "Wed, 16 Dec 2015 06:12:55 GMT"}], "update_date": "2015-12-17", "authors_parsed": [["Ghashami", "Mina", ""], ["Perry", "Daniel", ""], ["Phillips", "Jeff M.", ""]]}, {"id": "1512.05135", "submitter": "Byunghan Lee", "authors": "Byunghan Lee, Taehoon Lee, Byunggook Na, Sungroh Yoon", "title": "DNA-Level Splice Junction Prediction using Deep Recurrent Neural\n  Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG q-bio.GN", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A eukaryotic gene consists of multiple exons (protein coding regions) and\nintrons (non-coding regions), and a splice junction refers to the boundary\nbetween a pair of exon and intron. Precise identification of spice junctions on\na gene is important for deciphering its primary structure, function, and\ninteraction. Experimental techniques for determining exon/intron boundaries\ninclude RNA-seq, which is often accompanied by computational approaches.\nCanonical splicing signals are known, but computational junction prediction\nstill remains challenging because of a large number of false positives and\nother complications. In this paper, we exploit deep recurrent neural networks\n(RNNs) to model DNA sequences and to detect splice junctions thereon. We test\nvarious RNN units and architectures including long short-term memory units,\ngated recurrent units, and recently proposed iRNN for in-depth design space\nexploration. According to our experimental results, the proposed approach\nsignificantly outperforms not only conventional machine learning-based methods\nbut also a recent state-of-the-art deep belief network-based technique in terms\nof prediction accuracy.\n", "versions": [{"version": "v1", "created": "Wed, 16 Dec 2015 11:41:00 GMT"}], "update_date": "2015-12-17", "authors_parsed": [["Lee", "Byunghan", ""], ["Lee", "Taehoon", ""], ["Na", "Byunggook", ""], ["Yoon", "Sungroh", ""]]}, {"id": "1512.05244", "submitter": "Richard Nock", "authors": "Richard Nock", "title": "Learning Games and Rademacher Observations Losses", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It has recently been shown that supervised learning with the popular logistic\nloss is equivalent to optimizing the exponential loss over sufficient\nstatistics about the class: Rademacher observations (rados). We first show that\nthis unexpected equivalence can actually be generalized to other example / rado\nlosses, with necessary and sufficient conditions for the equivalence,\nexemplified on four losses that bear popular names in various fields:\nexponential (boosting), mean-variance (finance), Linear Hinge (on-line\nlearning), ReLU (deep learning), and unhinged (statistics). Second, we show\nthat the generalization unveils a surprising new connection to regularized\nlearning, and in particular a sufficient condition under which regularizing the\nloss over examples is equivalent to regularizing the rados (with Minkowski\nsums) in the equivalent rado loss. This brings simple and powerful rado-based\nlearning algorithms for sparsity-controlling regularization, that we exemplify\non a boosting algorithm for the regularized exponential rado-loss, which\nformally boosts over four types of regularization, including the popular ridge\nand lasso, and the recently coined slope --- we obtain the first proven\nboosting algorithm for this last regularization. Through our first contribution\non the equivalence of rado and example-based losses, Omega-R.AdaBoost~appears\nto be an efficient proxy to boost the regularized logistic loss over examples\nusing whichever of the four regularizers. Experiments display that\nregularization consistently improves performances of rado-based learning, and\nmay challenge or beat the state of the art of example-based learning even when\nlearning over small sets of rados. Finally, we connect regularization to\ndifferential privacy, and display how tiny budgets can be afforded on big\ndomains while beating (protected) example-based learning.\n", "versions": [{"version": "v1", "created": "Wed, 16 Dec 2015 16:56:02 GMT"}, {"version": "v2", "created": "Sat, 13 Feb 2016 00:33:22 GMT"}], "update_date": "2016-02-16", "authors_parsed": [["Nock", "Richard", ""]]}, {"id": "1512.05246", "submitter": "Calvin Murdock", "authors": "Calvin Murdock, Zhen Li, Howard Zhou, Tom Duerig", "title": "Blockout: Dynamic Model Selection for Hierarchical Deep Networks", "comments": null, "journal-ref": null, "doi": "10.1109/CVPR.2016.283", "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most deep architectures for image classification--even those that are trained\nto classify a large number of diverse categories--learn shared image\nrepresentations with a single model. Intuitively, however, categories that are\nmore similar should share more information than those that are very different.\nWhile hierarchical deep networks address this problem by learning separate\nfeatures for subsets of related categories, current implementations require\nsimplified models using fixed architectures specified via heuristic clustering\nmethods. Instead, we propose Blockout, a method for regularization and model\nselection that simultaneously learns both the model architecture and\nparameters. A generalization of Dropout, our approach gives a novel\nparametrization of hierarchical architectures that allows for structure\nlearning via back-propagation. To demonstrate its utility, we evaluate Blockout\non the CIFAR and ImageNet datasets, demonstrating improved classification\naccuracy, better regularization performance, faster training, and the clear\nemergence of hierarchical network structures.\n", "versions": [{"version": "v1", "created": "Wed, 16 Dec 2015 16:58:36 GMT"}], "update_date": "2018-03-20", "authors_parsed": [["Murdock", "Calvin", ""], ["Li", "Zhen", ""], ["Zhou", "Howard", ""], ["Duerig", "Tom", ""]]}, {"id": "1512.05294", "submitter": "Harini Suresh", "authors": "Harini Suresh", "title": "Feature Representation for ICU Mortality", "comments": "This article has been withdrawn due by the author due to the need for\n  more testing to verify results", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Good predictors of ICU Mortality have the potential to identify high-risk\npatients earlier, improve ICU resource allocation, or create more accurate\npopulation-level risk models. Machine learning practitioners typically make\nchoices about how to represent features in a particular model, but these\nchoices are seldom evaluated quantitatively. This study compares the\nperformance of different representations of clinical event data from MIMIC II\nin a logistic regression model to predict 36-hour ICU mortality. The most\ncommon representations are linear (normalized counts) and binary (yes/no).\nThese, along with a new representation termed \"hill\", are compared using both\nL1 and L2 regularization. Results indicate that the introduced \"hill\"\nrepresentation outperforms both the binary and linear representations, the hill\nrepresentation thus has the potential to improve existing models of ICU\nmortality.\n", "versions": [{"version": "v1", "created": "Wed, 16 Dec 2015 19:36:06 GMT"}, {"version": "v2", "created": "Sun, 7 Feb 2016 21:59:58 GMT"}], "update_date": "2016-02-09", "authors_parsed": [["Suresh", "Harini", ""]]}, {"id": "1512.05467", "submitter": "Marian-Andrei Rizoiu", "authors": "Marian-Andrei Rizoiu, Julien Velcin, St\\'ephane Lallich", "title": "Unsupervised Feature Construction for Improving Data Representation and\n  Semantics", "comments": null, "journal-ref": "Journal of Intelligent Information Systems, vol. 40, iss. 3, pp.\n  501-527, 2013", "doi": "10.1007/s10844-013-0235-x", "report-no": null, "categories": "cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Feature-based format is the main data representation format used by machine\nlearning algorithms. When the features do not properly describe the initial\ndata, performance starts to degrade. Some algorithms address this problem by\ninternally changing the representation space, but the newly-constructed\nfeatures are rarely comprehensible. We seek to construct, in an unsupervised\nway, new features that are more appropriate for describing a given dataset and,\nat the same time, comprehensible for a human user. We propose two algorithms\nthat construct the new features as conjunctions of the initial primitive\nfeatures or their negations. The generated feature sets have reduced\ncorrelations between features and succeed in catching some of the hidden\nrelations between individuals in a dataset. For example, a feature like $sky\n\\wedge \\neg building \\wedge panorama$ would be true for non-urban images and is\nmore informative than simple features expressing the presence or the absence of\nan object. The notion of Pareto optimality is used to evaluate feature sets and\nto obtain a balance between total correlation and the complexity of the\nresulted feature set. Statistical hypothesis testing is used in order to\nautomatically determine the values of the parameters used for constructing a\ndata-dependent feature set. We experimentally show that our approaches achieve\nthe construction of informative feature sets for multiple datasets.\n", "versions": [{"version": "v1", "created": "Thu, 17 Dec 2015 05:18:05 GMT"}], "update_date": "2015-12-18", "authors_parsed": [["Rizoiu", "Marian-Andrei", ""], ["Velcin", "Julien", ""], ["Lallich", "St\u00e9phane", ""]]}, {"id": "1512.05509", "submitter": "Denis Steckelmacher", "authors": "Denis Steckelmacher and Peter Vrancx", "title": "An Empirical Comparison of Neural Architectures for Reinforcement\n  Learning in Partially Observable Environments", "comments": "Presented at the 27th Benelux Conference on Artificial Intelligence", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper explores the performance of fitted neural Q iteration for\nreinforcement learning in several partially observable environments, using\nthree recurrent neural network architectures: Long Short-Term Memory, Gated\nRecurrent Unit and MUT1, a recurrent neural architecture evolved from a pool of\nseveral thousands candidate architectures. A variant of fitted Q iteration,\nbased on Advantage values instead of Q values, is also explored. The results\nshow that GRU performs significantly better than LSTM and MUT1 for most of the\nproblems considered, requiring less training episodes and less CPU time before\nlearning a very good policy. Advantage learning also tends to produce better\nresults.\n", "versions": [{"version": "v1", "created": "Thu, 17 Dec 2015 09:45:51 GMT"}], "update_date": "2015-12-18", "authors_parsed": [["Steckelmacher", "Denis", ""], ["Vrancx", "Peter", ""]]}, {"id": "1512.05616", "submitter": "Tony Beltramelli", "authors": "Tony Beltramelli, Sebastian Risi", "title": "Deep-Spying: Spying using Smartwatch and Deep Learning", "comments": "Security, Side-Channel Attack, Keystroke Inference, Motion Sensors,\n  Deep Learning, Recurrent Neural Network, Wearable Computing", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.CY cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Wearable technologies are today on the rise, becoming more common and broadly\navailable to mainstream users. In fact, wristband and armband devices such as\nsmartwatches and fitness trackers already took an important place in the\nconsumer electronics market and are becoming ubiquitous. By their very nature\nof being wearable, these devices, however, provide a new pervasive attack\nsurface threatening users privacy, among others.\n  In the meantime, advances in machine learning are providing unprecedented\npossibilities to process complex data efficiently. Allowing patterns to emerge\nfrom high dimensional unavoidably noisy data.\n  The goal of this work is to raise awareness about the potential risks related\nto motion sensors built-in wearable devices and to demonstrate abuse\nopportunities leveraged by advanced neural network architectures.\n  The LSTM-based implementation presented in this research can perform\ntouchlogging and keylogging on 12-keys keypads with above-average accuracy even\nwhen confronted with raw unprocessed data. Thus demonstrating that deep neural\nnetworks are capable of making keystroke inference attacks based on motion\nsensors easier to achieve by removing the need for non-trivial pre-processing\npipelines and carefully engineered feature extraction strategies. Our results\nsuggest that the complete technological ecosystem of a user can be compromised\nwhen a wearable wristband device is worn.\n", "versions": [{"version": "v1", "created": "Thu, 17 Dec 2015 14:58:26 GMT"}], "update_date": "2015-12-18", "authors_parsed": [["Beltramelli", "Tony", ""], ["Risi", "Sebastian", ""]]}, {"id": "1512.05665", "submitter": "Ulrich Schaechtle", "authors": "Ulrich Schaechtle, Ben Zinberg, Alexey Radul, Kostas Stathis and\n  Vikash K. Mansinghka", "title": "Probabilistic Programming with Gaussian Process Memoization", "comments": "36 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Gaussian Processes (GPs) are widely used tools in statistics, machine\nlearning, robotics, computer vision, and scientific computation. However,\ndespite their popularity, they can be difficult to apply; all but the simplest\nclassification or regression applications require specification and inference\nover complex covariance functions that do not admit simple analytical\nposteriors. This paper shows how to embed Gaussian processes in any\nhigher-order probabilistic programming language, using an idiom based on\nmemoization, and demonstrates its utility by implementing and extending classic\nand state-of-the-art GP applications. The interface to Gaussian processes,\ncalled gpmem, takes an arbitrary real-valued computational process as input and\nreturns a statistical emulator that automatically improve as the original\nprocess is invoked and its input-output behavior is recorded. The flexibility\nof gpmem is illustrated via three applications: (i) robust GP regression with\nhierarchical hyper-parameter learning, (ii) discovering symbolic expressions\nfrom time-series data by fully Bayesian structure learning over kernels\ngenerated by a stochastic grammar, and (iii) a bandit formulation of Bayesian\noptimization with automatic inference and action selection. All applications\nshare a single 50-line Python library and require fewer than 20 lines of\nprobabilistic code each.\n", "versions": [{"version": "v1", "created": "Thu, 17 Dec 2015 16:46:10 GMT"}, {"version": "v2", "created": "Tue, 5 Jan 2016 10:55:02 GMT"}], "update_date": "2016-01-06", "authors_parsed": [["Schaechtle", "Ulrich", ""], ["Zinberg", "Ben", ""], ["Radul", "Alexey", ""], ["Stathis", "Kostas", ""], ["Mansinghka", "Vikash K.", ""]]}, {"id": "1512.05742", "submitter": "Iulian Vlad Serban", "authors": "Iulian Vlad Serban, Ryan Lowe, Peter Henderson, Laurent Charlin,\n  Joelle Pineau", "title": "A Survey of Available Corpora for Building Data-Driven Dialogue Systems", "comments": "56 pages including references and appendix, 5 tables and 1 figure;\n  Under review for the Dialogue & Discourse journal. Update: paper has been\n  rewritten and now includes several new datasets", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.HC cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  During the past decade, several areas of speech and language understanding\nhave witnessed substantial breakthroughs from the use of data-driven models. In\nthe area of dialogue systems, the trend is less obvious, and most practical\nsystems are still built through significant engineering and expert knowledge.\nNevertheless, several recent results suggest that data-driven approaches are\nfeasible and quite promising. To facilitate research in this area, we have\ncarried out a wide survey of publicly available datasets suitable for\ndata-driven learning of dialogue systems. We discuss important characteristics\nof these datasets, how they can be used to learn diverse dialogue strategies,\nand their other potential uses. We also examine methods for transfer learning\nbetween datasets and the use of external knowledge. Finally, we discuss\nappropriate choice of evaluation metrics for the learning objective.\n", "versions": [{"version": "v1", "created": "Thu, 17 Dec 2015 19:52:39 GMT"}, {"version": "v2", "created": "Tue, 22 Dec 2015 04:58:05 GMT"}, {"version": "v3", "created": "Tue, 21 Mar 2017 01:15:32 GMT"}], "update_date": "2017-03-22", "authors_parsed": [["Serban", "Iulian Vlad", ""], ["Lowe", "Ryan", ""], ["Henderson", "Peter", ""], ["Charlin", "Laurent", ""], ["Pineau", "Joelle", ""]]}, {"id": "1512.05808", "submitter": "Jun Liu", "authors": "Jun Liu, Zheng Zhao, Ruiwen Zhang", "title": "Successive Ray Refinement and Its Application to Coordinate Descent for\n  LASSO", "comments": "26 pages, 6 figures, 6 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Coordinate descent is one of the most popular approaches for solving Lasso\nand its extensions due to its simplicity and efficiency. When applying\ncoordinate descent to solving Lasso, we update one coordinate at a time while\nfixing the remaining coordinates. Such an update, which is usually easy to\ncompute, greedily decreases the objective function value. In this paper, we aim\nto improve its computational efficiency by reducing the number of coordinate\ndescent iterations. To this end, we propose a novel technique called Successive\nRay Refinement (SRR). SRR makes use of the following ray continuation property\non the successive iterations: for a particular coordinate, the value obtained\nin the next iteration almost always lies on a ray that starts at its previous\niteration and passes through the current iteration. Motivated by this\nray-continuation property, we propose that coordinate descent be performed not\ndirectly on the previous iteration but on a refined search point that has the\nfollowing properties: on one hand, it lies on a ray that starts at a history\nsolution and passes through the previous iteration, and on the other hand, it\nachieves the minimum objective function value among all the points on the ray.\nWe propose two schemes for defining the search point and show that the refined\nsearch point can be efficiently obtained. Empirical results for real and\nsynthetic data sets show that the proposed SRR can significantly reduce the\nnumber of coordinate descent iterations, especially for small Lasso\nregularization parameters.\n", "versions": [{"version": "v1", "created": "Thu, 17 Dec 2015 21:47:02 GMT"}], "update_date": "2015-12-21", "authors_parsed": [["Liu", "Jun", ""], ["Zhao", "Zheng", ""], ["Zhang", "Ruiwen", ""]]}, {"id": "1512.05830", "submitter": "Zhouchen Lin", "authors": "Li Shen and Zhouchen Lin and Qingming Huang", "title": "Relay Backpropagation for Effective Learning of Deep Convolutional\n  Neural Networks", "comments": "Technical report for our submissions to the ILSVRC 2015 Scene\n  Classification Challenge, where we won the first place", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning deeper convolutional neural networks becomes a tendency in recent\nyears. However, many empirical evidences suggest that performance improvement\ncannot be gained by simply stacking more layers. In this paper, we consider the\nissue from an information theoretical perspective, and propose a novel method\nRelay Backpropagation, that encourages the propagation of effective information\nthrough the network in training stage. By virtue of the method, we achieved the\nfirst place in ILSVRC 2015 Scene Classification Challenge. Extensive\nexperiments on two challenging large scale datasets demonstrate the\neffectiveness of our method is not restricted to a specific dataset or network\narchitecture. Our models will be available to the research community later.\n", "versions": [{"version": "v1", "created": "Fri, 18 Dec 2015 00:13:10 GMT"}, {"version": "v2", "created": "Sun, 3 Apr 2016 07:47:28 GMT"}], "update_date": "2016-04-05", "authors_parsed": [["Shen", "Li", ""], ["Lin", "Zhouchen", ""], ["Huang", "Qingming", ""]]}, {"id": "1512.05840", "submitter": "Pau Perng-Hwa Kung", "authors": "Pau Perng-Hwa Kung", "title": "Deep Poisson Factorization Machines: factor analysis for mapping\n  behaviors in journalist ecosystem", "comments": "Incomplete work, will re-upload once the details and implementations\n  are straightened out", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Newsroom in online ecosystem is difficult to untangle. With prevalence of\nsocial media, interactions between journalists and individuals become visible,\nbut lack of understanding to inner processing of information feedback loop in\npublic sphere leave most journalists baffled. Can we provide an organized view\nto characterize journalist behaviors on individual level to know better of the\necosystem? To this end, I propose Poisson Factorization Machine (PFM), a\nBayesian analogue to matrix factorization that assumes Poisson distribution for\ngenerative process. The model generalizes recent studies on Poisson Matrix\nFactorization to account temporal interaction which involves tensor-like\nstructure, and label information. Two inference procedures are designed, one\nbased on batch variational EM and another stochastic variational inference\nscheme that efficiently scales with data size. An important novelty in this\nnote is that I show how to stack layers of PFM to introduce a deep\narchitecture. This work discusses some potential results applying the model and\nexplains how such latent factors may be useful for analyzing latent behaviors\nfor data exploration.\n", "versions": [{"version": "v1", "created": "Fri, 18 Dec 2015 01:25:47 GMT"}, {"version": "v2", "created": "Fri, 29 Dec 2017 21:35:25 GMT"}], "update_date": "2018-01-03", "authors_parsed": [["Kung", "Pau Perng-Hwa", ""]]}, {"id": "1512.05947", "submitter": "Kathrin Bujna", "authors": "Johannes Bl\\\"omer, Sascha Brauer, and Kathrin Bujna", "title": "Complexity and Approximation of the Fuzzy K-Means Problem", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The fuzzy $K$-means problem is a generalization of the classical $K$-means\nproblem to soft clusterings, i.e. clusterings where each points belongs to each\ncluster to some degree. Although popular in practice, prior to this work the\nfuzzy $K$-means problem has not been studied from a complexity theoretic or\nalgorithmic perspective. We show that optimal solutions for fuzzy $K$-means\ncannot, in general, be expressed by radicals over the input points.\nSurprisingly, this already holds for very simple inputs in one-dimensional\nspace. Hence, one cannot expect to compute optimal solutions exactly. We give\nthe first $(1+\\epsilon)$-approximation algorithms for the fuzzy $K$-means\nproblem. First, we present a deterministic approximation algorithm whose\nruntime is polynomial in $N$ and linear in the dimension $D$ of the input set,\ngiven that $K$ is constant, i.e. a polynomial time approximation algorithm\ngiven a fixed $K$. We achieve this result by showing that for each soft\nclustering there exists a hard clustering with comparable properties. Second,\nby using techniques known from coreset constructions for the $K$-means problem,\nwe develop a deterministic approximation algorithm that runs in time almost\nlinear in $N$ but exponential in the dimension $D$. We complement these results\nwith a randomized algorithm which imposes some natural restrictions on the\ninput set and whose runtime is comparable to some of the most efficient\napproximation algorithms for $K$-means, i.e. linear in the number of points and\nthe dimension, but exponential in the number of clusters.\n", "versions": [{"version": "v1", "created": "Fri, 18 Dec 2015 13:35:59 GMT"}], "update_date": "2015-12-21", "authors_parsed": [["Bl\u00f6mer", "Johannes", ""], ["Brauer", "Sascha", ""], ["Bujna", "Kathrin", ""]]}, {"id": "1512.06061", "submitter": "Brijnesh Jain", "authors": "Brijnesh Jain", "title": "Asymptotic Behavior of Mean Partitions in Consensus Clustering", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Although consistency is a minimum requirement of any estimator, little is\nknown about consistency of the mean partition approach in consensus clustering.\nThis contribution studies the asymptotic behavior of mean partitions. We show\nthat under normal assumptions, the mean partition approach is consistent and\nasymptotic normal. To derive both results, we represent partitions as points of\nsome geometric space, called orbit space. Then we draw on results from the\ntheory of Fr\\'echet means and stochastic programming. The asymptotic properties\nhold for continuous extensions of standard cluster criteria (indices). The\nresults justify consensus clustering using finite but sufficiently large sample\nsizes. Furthermore, the orbit space framework provides a mathematical\nfoundation for studying further statistical, geometrical, and analytical\nproperties of sets of partitions.\n", "versions": [{"version": "v1", "created": "Fri, 18 Dec 2015 17:59:49 GMT"}], "update_date": "2015-12-21", "authors_parsed": [["Jain", "Brijnesh", ""]]}, {"id": "1512.06173", "submitter": "Xuan Hong  Dang", "authors": "Xuan Hong Dang, Ambuj K. Singh, Petko Bogdanov, Hongyuan You and\n  Bayyuan Hsu", "title": "Discriminative Subnetworks with Regularized Spectral Learning for\n  Global-state Network Data", "comments": "manuscript for the ECML 2014 paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data mining practitioners are facing challenges from data with network\nstructure. In this paper, we address a specific class of global-state networks\nwhich comprises of a set of network instances sharing a similar structure yet\nhaving different values at local nodes. Each instance is associated with a\nglobal state which indicates the occurrence of an event. The objective is to\nuncover a small set of discriminative subnetworks that can optimally classify\nglobal network values. Unlike most existing studies which explore an\nexponential subnetwork space, we address this difficult problem by adopting a\nspace transformation approach. Specifically, we present an algorithm that\noptimizes a constrained dual-objective function to learn a low-dimensional\nsubspace that is capable of discriminating networks labelled by different\nglobal states, while reconciling with common network topology sharing across\ninstances. Our algorithm takes an appealing approach from spectral graph\nlearning and we show that the globally optimum solution can be achieved via\nmatrix eigen-decomposition.\n", "versions": [{"version": "v1", "created": "Sat, 19 Dec 2015 01:20:02 GMT"}], "update_date": "2015-12-22", "authors_parsed": [["Dang", "Xuan Hong", ""], ["Singh", "Ambuj K.", ""], ["Bogdanov", "Petko", ""], ["You", "Hongyuan", ""], ["Hsu", "Bayyuan", ""]]}, {"id": "1512.06216", "submitter": "Hao Zhang", "authors": "Hao Zhang, Zhiting Hu, Jinliang Wei, Pengtao Xie, Gunhee Kim, Qirong\n  Ho and Eric Xing", "title": "Poseidon: A System Architecture for Efficient GPU-based Deep Learning on\n  Multiple Machines", "comments": "14 pages, 8 figures, 6 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning (DL) has achieved notable successes in many machine learning\ntasks. A number of frameworks have been developed to expedite the process of\ndesigning and training deep neural networks (DNNs), such as Caffe, Torch and\nTheano. Currently they can harness multiple GPUs on a single machine, but are\nunable to use GPUs that are distributed across multiple machines; as even\naverage-sized DNNs can take days to train on a single GPU with 100s of GBs to\nTBs of data, distributed GPUs present a prime opportunity for scaling up DL.\nHowever, the limited bandwidth available on commodity Ethernet networks\npresents a bottleneck to distributed GPU training, and prevents its trivial\nrealization.\n  To investigate how to adapt existing frameworks to efficiently support\ndistributed GPUs, we propose Poseidon, a scalable system architecture for\ndistributed inter-machine communication in existing DL frameworks. We integrate\nPoseidon with Caffe and evaluate its performance at training DNNs for object\nrecognition. Poseidon features three key contributions that accelerate DNN\ntraining on clusters: (1) a three-level hybrid architecture that allows\nPoseidon to support both CPU-only and GPU-equipped clusters, (2) a distributed\nwait-free backpropagation (DWBP) algorithm to improve GPU utilization and to\nbalance communication, and (3) a structure-aware communication protocol (SACP)\nto minimize communication overheads. We empirically show that Poseidon\nconverges to same objectives as a single machine, and achieves state-of-art\ntraining speedup across multiple models and well-established datasets using a\ncommodity GPU cluster of 8 nodes (e.g. 4.5x speedup on AlexNet, 4x on\nGoogLeNet, 4x on CIFAR-10). On the much larger ImageNet22K dataset, Poseidon\nwith 8 nodes achieves better speedup and competitive accuracy to recent\nCPU-based distributed systems such as Adam and Le et al., which use 10s to\n1000s of nodes.\n", "versions": [{"version": "v1", "created": "Sat, 19 Dec 2015 09:55:37 GMT"}], "update_date": "2015-12-22", "authors_parsed": [["Zhang", "Hao", ""], ["Hu", "Zhiting", ""], ["Wei", "Jinliang", ""], ["Xie", "Pengtao", ""], ["Kim", "Gunhee", ""], ["Ho", "Qirong", ""], ["Xing", "Eric", ""]]}, {"id": "1512.06222", "submitter": "Dariush Kari", "authors": "Dariush Kari and Muhammed Omer Sayin and Suleyman Serdar Kozat", "title": "A new robust adaptive algorithm for underwater acoustic channel\n  equalization", "comments": "8 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.IT cs.LG math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a novel family of adaptive robust equalizers for highly\nchallenging underwater acoustic (UWA) channel equalization. Since the\nunderwater environment is highly non-stationary and subjected to impulsive\nnoise, we use adaptive filtering techniques based on a relative logarithmic\ncost function inspired by the competitive methods from the online learning\nliterature. To improve the convergence performance of the conventional linear\nequalization methods, while mitigating the stability issues, we intrinsically\ncombine different norms of the error in the cost function, using logarithmic\nfunctions. Hence, we achieve a comparable convergence performance to least mean\nfourth (LMF) equalizer, while significantly enhancing the stability performance\nin such an adverse communication medium. We demonstrate the performance of our\nalgorithms through highly realistic experiments performed on accurately\nsimulated underwater acoustic channels.\n", "versions": [{"version": "v1", "created": "Sat, 19 Dec 2015 10:27:59 GMT"}], "update_date": "2015-12-22", "authors_parsed": [["Kari", "Dariush", ""], ["Sayin", "Muhammed Omer", ""], ["Kozat", "Suleyman Serdar", ""]]}, {"id": "1512.06228", "submitter": "Abhijit Sharang", "authors": "Abhijit Sharang and Chetan Rao", "title": "Using machine learning for medium frequency derivative portfolio trading", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.TR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We use machine learning for designing a medium frequency trading strategy for\na portfolio of 5 year and 10 year US Treasury note futures. We formulate this\nas a classification problem where we predict the weekly direction of movement\nof the portfolio using features extracted from a deep belief network trained on\ntechnical indicators of the portfolio constituents. The experimentation shows\nthat the resulting pipeline is effective in making a profitable trade.\n", "versions": [{"version": "v1", "created": "Sat, 19 Dec 2015 11:45:13 GMT"}], "update_date": "2015-12-22", "authors_parsed": [["Sharang", "Abhijit", ""], ["Rao", "Chetan", ""]]}, {"id": "1512.06238", "submitter": "Eric Balkanski", "authors": "Eric Balkanski, Aviad Rubinstein, Yaron Singer", "title": "The Limitations of Optimization from Samples", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DM cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we consider the following question: can we optimize objective\nfunctions from the training data we use to learn them? We formalize this\nquestion through a novel framework we call optimization from samples (OPS). In\nOPS, we are given sampled values of a function drawn from some distribution and\nthe objective is to optimize the function under some constraint.\n  While there are interesting classes of functions that can be optimized from\nsamples, our main result is an impossibility. We show that there are classes of\nfunctions which are statistically learnable and optimizable, but for which no\nreasonable approximation for optimization from samples is achievable. In\nparticular, our main result shows that there is no constant factor\napproximation for maximizing coverage functions under a cardinality constraint\nusing polynomially-many samples drawn from any distribution.\n  We also show tight approximation guarantees for maximization under a\ncardinality constraint of several interesting classes of functions including\nunit-demand, additive, and general monotone submodular functions, as well as a\nconstant factor approximation for monotone submodular functions with bounded\ncurvature.\n", "versions": [{"version": "v1", "created": "Sat, 19 Dec 2015 12:35:36 GMT"}, {"version": "v2", "created": "Thu, 7 Apr 2016 21:25:06 GMT"}, {"version": "v3", "created": "Tue, 15 Nov 2016 21:41:09 GMT"}], "update_date": "2016-11-17", "authors_parsed": [["Balkanski", "Eric", ""], ["Rubinstein", "Aviad", ""], ["Singer", "Yaron", ""]]}, {"id": "1512.06293", "submitter": "Thomas Wiatowski", "authors": "Thomas Wiatowski and Helmut B\\\"olcskei", "title": "A Mathematical Theory of Deep Convolutional Neural Networks for Feature\n  Extraction", "comments": "IEEE Transactions on Information Theory, to appear", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.AI cs.LG math.FA math.IT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep convolutional neural networks have led to breakthrough results in\nnumerous practical machine learning tasks such as classification of images in\nthe ImageNet data set, control-policy-learning to play Atari games or the board\ngame Go, and image captioning. Many of these applications first perform feature\nextraction and then feed the results thereof into a trainable classifier. The\nmathematical analysis of deep convolutional neural networks for feature\nextraction was initiated by Mallat, 2012. Specifically, Mallat considered\nso-called scattering networks based on a wavelet transform followed by the\nmodulus non-linearity in each network layer, and proved translation invariance\n(asymptotically in the wavelet scale parameter) and deformation stability of\nthe corresponding feature extractor. This paper complements Mallat's results by\ndeveloping a theory that encompasses general convolutional transforms, or in\nmore technical parlance, general semi-discrete frames (including\nWeyl-Heisenberg filters, curvelets, shearlets, ridgelets, wavelets, and learned\nfilters), general Lipschitz-continuous non-linearities (e.g., rectified linear\nunits, shifted logistic sigmoids, hyperbolic tangents, and modulus functions),\nand general Lipschitz-continuous pooling operators emulating, e.g.,\nsub-sampling and averaging. In addition, all of these elements can be different\nin different network layers. For the resulting feature extractor we prove a\ntranslation invariance result of vertical nature in the sense of the features\nbecoming progressively more translation-invariant with increasing network\ndepth, and we establish deformation sensitivity bounds that apply to signal\nclasses such as, e.g., band-limited functions, cartoon functions, and Lipschitz\nfunctions.\n", "versions": [{"version": "v1", "created": "Sat, 19 Dec 2015 22:31:24 GMT"}, {"version": "v2", "created": "Thu, 1 Sep 2016 10:12:30 GMT"}, {"version": "v3", "created": "Tue, 24 Oct 2017 06:44:21 GMT"}], "update_date": "2017-10-25", "authors_parsed": [["Wiatowski", "Thomas", ""], ["B\u00f6lcskei", "Helmut", ""]]}, {"id": "1512.06337", "submitter": "Jiasong Wu", "authors": "Dan Wu, Jiasong Wu, Rui Zeng, Longyu Jiang, Lotfi Senhadji, Huazhong\n  Shu", "title": "Kernel principal component analysis network for image classification", "comments": "7 pages, 1 figure, 4 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In order to classify the nonlinear feature with linear classifier and improve\nthe classification accuracy, a deep learning network named kernel principal\ncomponent analysis network (KPCANet) is proposed. First, mapping the data into\nhigher space with kernel principal component analysis to make the data linearly\nseparable. Then building a two-layer KPCANet to obtain the principal components\nof image. Finally, classifying the principal components with linearly\nclassifier. Experimental results show that the proposed KPCANet is effective in\nface recognition, object recognition and hand-writing digits recognition, it\nalso outperforms principal component analysis network (PCANet) generally as\nwell. Besides, KPCANet is invariant to illumination and stable to occlusion and\nslight deformation.\n", "versions": [{"version": "v1", "created": "Sun, 20 Dec 2015 09:06:06 GMT"}], "update_date": "2015-12-22", "authors_parsed": [["Wu", "Dan", ""], ["Wu", "Jiasong", ""], ["Zeng", "Rui", ""], ["Jiang", "Longyu", ""], ["Senhadji", "Lotfi", ""], ["Shu", "Huazhong", ""]]}, {"id": "1512.06388", "submitter": "Xi Wu", "authors": "Xi Wu, Matthew Fredrikson, Wentao Wu, Somesh Jha, Jeffrey F. Naughton", "title": "Revisiting Differentially Private Regression: Lessons From Learning\n  Theory and their Consequences", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.DB cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Private regression has received attention from both database and security\ncommunities. Recent work by Fredrikson et al. (USENIX Security 2014) analyzed\nthe functional mechanism (Zhang et al. VLDB 2012) for training linear\nregression models over medical data. Unfortunately, they found that model\naccuracy is already unacceptable with differential privacy when $\\varepsilon =\n5$. We address this issue, presenting an explicit connection between\ndifferential privacy and stable learning theory through which a substantially\nbetter privacy/utility tradeoff can be obtained. Perhaps more importantly, our\ntheory reveals that the most basic mechanism in differential privacy, output\nperturbation, can be used to obtain a better tradeoff for all\nconvex-Lipschitz-bounded learning tasks. Since output perturbation is simple to\nimplement, it means that our approach is potentially widely applicable in\npractice. We go on to apply it on the same medical data as used by Fredrikson\net al. Encouragingly, we achieve accurate models even for $\\varepsilon = 0.1$.\nIn the last part of this paper, we study the impact of our improved\ndifferentially private mechanisms on model inversion attacks, a privacy attack\nintroduced by Fredrikson et al. We observe that the improved tradeoff makes the\nresulting differentially private model more susceptible to inversion attacks.\nWe analyze this phenomenon formally.\n", "versions": [{"version": "v1", "created": "Sun, 20 Dec 2015 15:53:29 GMT"}], "update_date": "2015-12-22", "authors_parsed": [["Wu", "Xi", ""], ["Fredrikson", "Matthew", ""], ["Wu", "Wentao", ""], ["Jha", "Somesh", ""], ["Naughton", "Jeffrey F.", ""]]}, {"id": "1512.06430", "submitter": "Muhammad R. Khan", "authors": "Muhammad R. Khan, Johua Manoj, Anikate Singh, Joshua Blumenstock", "title": "Behavioral Modeling for Churn Prediction: Early Indicators and Accurate\n  Predictors of Custom Defection and Loyalty", "comments": null, "journal-ref": null, "doi": "10.1109/BigDataCongress.2015.107", "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Churn prediction, or the task of identifying customers who are likely to\ndiscontinue use of a service, is an important and lucrative concern of firms in\nmany different industries. As these firms collect an increasing amount of\nlarge-scale, heterogeneous data on the characteristics and behaviors of\ncustomers, new methods become possible for predicting churn. In this paper, we\npresent a unified analytic framework for detecting the early warning signs of\nchurn, and assigning a \"Churn Score\" to each customer that indicates the\nlikelihood that the particular individual will churn within a predefined amount\nof time. This framework employs a brute force approach to feature engineering,\nthen winnows the set of relevant attributes via feature selection, before\nfeeding the final feature-set into a suite of supervised learning algorithms.\nUsing several terabytes of data from a large mobile phone network, our method\nidentifies several intuitive - and a few surprising - early warning signs of\nchurn, and our best model predicts whether a subscriber will churn with 89.4%\naccuracy.\n", "versions": [{"version": "v1", "created": "Sun, 20 Dec 2015 20:45:55 GMT"}], "update_date": "2015-12-22", "authors_parsed": [["Khan", "Muhammad R.", ""], ["Manoj", "Johua", ""], ["Singh", "Anikate", ""], ["Blumenstock", "Joshua", ""]]}, {"id": "1512.06452", "submitter": "Hossein Soleimani", "authors": "Hossein Soleimani, David J. Miller", "title": "ATD: Anomalous Topic Discovery in High Dimensional Discrete Data", "comments": null, "journal-ref": null, "doi": "10.1109/TKDE.2016.2561288", "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose an algorithm for detecting patterns exhibited by anomalous\nclusters in high dimensional discrete data. Unlike most anomaly detection (AD)\nmethods, which detect individual anomalies, our proposed method detects groups\n(clusters) of anomalies; i.e. sets of points which collectively exhibit\nabnormal patterns. In many applications this can lead to better understanding\nof the nature of the atypical behavior and to identifying the sources of the\nanomalies. Moreover, we consider the case where the atypical patterns exhibit\non only a small (salient) subset of the very high dimensional feature space.\nIndividual AD techniques and techniques that detect anomalies using all the\nfeatures typically fail to detect such anomalies, but our method can detect\nsuch instances collectively, discover the shared anomalous patterns exhibited\nby them, and identify the subsets of salient features. In this paper, we focus\non detecting anomalous topics in a batch of text documents, developing our\nalgorithm based on topic models. Results of our experiments show that our\nmethod can accurately detect anomalous topics and salient features (words)\nunder each such topic in a synthetic data set and two real-world text corpora\nand achieves better performance compared to both standard group AD and\nindividual AD techniques. All required code to reproduce our experiments is\navailable from https://github.com/hsoleimani/ATD\n", "versions": [{"version": "v1", "created": "Sun, 20 Dec 2015 22:55:39 GMT"}, {"version": "v2", "created": "Fri, 20 May 2016 17:39:57 GMT"}], "update_date": "2016-05-23", "authors_parsed": [["Soleimani", "Hossein", ""], ["Miller", "David J.", ""]]}, {"id": "1512.06612", "submitter": "Lili Mou", "authors": "Lili Mou, Rui Yan, Ge Li, Lu Zhang, Zhi Jin", "title": "Backward and Forward Language Modeling for Constrained Sentence\n  Generation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent language models, especially those based on recurrent neural networks\n(RNNs), make it possible to generate natural language from a learned\nprobability. Language generation has wide applications including machine\ntranslation, summarization, question answering, conversation systems, etc.\nExisting methods typically learn a joint probability of words conditioned on\nadditional information, which is (either statically or dynamically) fed to\nRNN's hidden layer. In many applications, we are likely to impose hard\nconstraints on the generated texts, i.e., a particular word must appear in the\nsentence. Unfortunately, existing approaches could not solve this problem. In\nthis paper, we propose a novel backward and forward language model. Provided a\nspecific word, we use RNNs to generate previous words and future words, either\nsimultaneously or asynchronously, resulting in two model variants. In this way,\nthe given word could appear at any position in the sentence. Experimental\nresults show that the generated texts are comparable to sequential LMs in\nquality.\n", "versions": [{"version": "v1", "created": "Mon, 21 Dec 2015 13:07:31 GMT"}, {"version": "v2", "created": "Sun, 3 Jan 2016 20:15:44 GMT"}], "update_date": "2016-01-05", "authors_parsed": [["Mou", "Lili", ""], ["Yan", "Rui", ""], ["Li", "Ge", ""], ["Zhang", "Lu", ""], ["Jin", "Zhi", ""]]}, {"id": "1512.06658", "submitter": "Haitian Zheng", "authors": "Haitian Zheng, Lu Fang, Mengqi Ji, Matti Strese, Yigitcan Ozer,\n  Eckehard Steinbach", "title": "Deep Learning for Surface Material Classification Using Haptic And\n  Visual Information", "comments": "8 pages, under review as a paper at Transactions on Multimedia", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When a user scratches a hand-held rigid tool across an object surface, an\nacceleration signal can be captured, which carries relevant information about\nthe surface. More importantly, such a haptic signal is complementary to the\nvisual appearance of the surface, which suggests the combination of both\nmodalities for the recognition of the surface material. In this paper, we\npresent a novel deep learning method dealing with the surface material\nclassification problem based on a Fully Convolutional Network (FCN), which\ntakes as input the aforementioned acceleration signal and a corresponding image\nof the surface texture. Compared to previous surface material classification\nsolutions, which rely on a careful design of hand-crafted domain-specific\nfeatures, our method automatically extracts discriminative features utilizing\nthe advanced deep learning methodologies. Experiments performed on the TUM\nsurface material database demonstrate that our method achieves state-of-the-art\nclassification accuracy robustly and efficiently.\n", "versions": [{"version": "v1", "created": "Mon, 21 Dec 2015 15:22:16 GMT"}, {"version": "v2", "created": "Sun, 1 May 2016 07:00:56 GMT"}], "update_date": "2016-05-03", "authors_parsed": [["Zheng", "Haitian", ""], ["Fang", "Lu", ""], ["Ji", "Mengqi", ""], ["Strese", "Matti", ""], ["Ozer", "Yigitcan", ""], ["Steinbach", "Eckehard", ""]]}, {"id": "1512.06730", "submitter": "Shuchin Aeron", "authors": "Eric Kernfeld, Nathan Majumder, Shuchin Aeron, Misha Kilmer", "title": "Multilinear Subspace Clustering", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.CV cs.LG math.IT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we present a new model and an algorithm for unsupervised\nclustering of 2-D data such as images. We assume that the data comes from a\nunion of multilinear subspaces (UOMS) model, which is a specific structured\ncase of the much studied union of subspaces (UOS) model. For segmentation under\nthis model, we develop Multilinear Subspace Clustering (MSC) algorithm and\nevaluate its performance on the YaleB and Olivietti image data sets. We show\nthat MSC is highly competitive with existing algorithms employing the UOS model\nin terms of clustering performance while enjoying improvement in computational\ncomplexity.\n", "versions": [{"version": "v1", "created": "Mon, 21 Dec 2015 17:53:35 GMT"}], "update_date": "2015-12-22", "authors_parsed": [["Kernfeld", "Eric", ""], ["Majumder", "Nathan", ""], ["Aeron", "Shuchin", ""], ["Kilmer", "Misha", ""]]}, {"id": "1512.06757", "submitter": "Jiaji Huang", "authors": "Jiaji Huang, Qiang Qiu, Robert Calderbank, Guillermo Sapiro", "title": "GraphConnect: A Regularization Framework for Neural Networks", "comments": "Theorems need more validation", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural networks have proved very successful in domains where large\ntraining sets are available, but when the number of training samples is small,\ntheir performance suffers from overfitting. Prior methods of reducing\noverfitting such as weight decay, Dropout and DropConnect are data-independent.\nThis paper proposes a new method, GraphConnect, that is data-dependent, and is\nmotivated by the observation that data of interest lie close to a manifold. The\nnew method encourages the relationships between the learned decisions to\nresemble a graph representing the manifold structure. Essentially GraphConnect\nis designed to learn attributes that are present in data samples in contrast to\nweight decay, Dropout and DropConnect which are simply designed to make it more\ndifficult to fit to random error or noise. Empirical Rademacher complexity is\nused to connect the generalization error of the neural network to spectral\nproperties of the graph learned from the input data. This framework is used to\nshow that GraphConnect is superior to weight decay. Experimental results on\nseveral benchmark datasets validate the theoretical analysis, and show that\nwhen the number of training samples is small, GraphConnect is able to\nsignificantly improve performance over weight decay.\n", "versions": [{"version": "v1", "created": "Mon, 21 Dec 2015 18:42:45 GMT"}, {"version": "v2", "created": "Wed, 27 Jan 2016 03:21:15 GMT"}], "update_date": "2016-01-28", "authors_parsed": [["Huang", "Jiaji", ""], ["Qiu", "Qiang", ""], ["Calderbank", "Robert", ""], ["Sapiro", "Guillermo", ""]]}, {"id": "1512.06900", "submitter": "Volker Tresp", "authors": "Crist\\'obal Esteban and Volker Tresp and Yinchong Yang and Stephan\n  Baier and Denis Krompa{\\ss}", "title": "Predicting the Co-Evolution of Event and Knowledge Graphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Embedding learning, a.k.a. representation learning, has been shown to be able\nto model large-scale semantic knowledge graphs. A key concept is a mapping of\nthe knowledge graph to a tensor representation whose entries are predicted by\nmodels using latent representations of generalized entities. Knowledge graphs\nare typically treated as static: A knowledge graph grows more links when more\nfacts become available but the ground truth values associated with links is\nconsidered time invariant. In this paper we address the issue of knowledge\ngraphs where triple states depend on time. We assume that changes in the\nknowledge graph always arrive in form of events, in the sense that the events\nare the gateway to the knowledge graph. We train an event prediction model\nwhich uses both knowledge graph background information and information on\nrecent events. By predicting future events, we also predict likely changes in\nthe knowledge graph and thus obtain a model for the evolution of the knowledge\ngraph as well. Our experiments demonstrate that our approach performs well in a\nclinical application, a recommendation engine and a sensor network application.\n", "versions": [{"version": "v1", "created": "Mon, 21 Dec 2015 22:49:43 GMT"}], "update_date": "2015-12-23", "authors_parsed": [["Esteban", "Crist\u00f3bal", ""], ["Tresp", "Volker", ""], ["Yang", "Yinchong", ""], ["Baier", "Stephan", ""], ["Krompa\u00df", "Denis", ""]]}, {"id": "1512.06927", "submitter": "Jian Jin", "authors": "Jian Jin", "title": "A C++ library for Multimodal Deep Learning", "comments": "27 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  MDL, Multimodal Deep Learning Library, is a deep learning framework that\nsupports multiple models, and this document explains its philosophy and\nfunctionality. MDL runs on Linux, Mac, and Unix platforms. It depends on\nOpenCV.\n", "versions": [{"version": "v1", "created": "Tue, 22 Dec 2015 01:27:23 GMT"}, {"version": "v2", "created": "Mon, 28 Dec 2015 20:00:20 GMT"}, {"version": "v3", "created": "Tue, 29 Dec 2015 13:39:52 GMT"}, {"version": "v4", "created": "Tue, 12 Apr 2016 17:34:29 GMT"}], "update_date": "2016-04-13", "authors_parsed": [["Jin", "Jian", ""]]}, {"id": "1512.06992", "submitter": "Christos Dimitrakakis", "authors": "Zuhe Zhang, Benjamin Rubinstein, Christos Dimitrakakis", "title": "On the Differential Privacy of Bayesian Inference", "comments": "AAAI 2016, Feb 2016, Phoenix, Arizona, United States", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CR cs.LG math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study how to communicate findings of Bayesian inference to third parties,\nwhile preserving the strong guarantee of differential privacy. Our main\ncontributions are four different algorithms for private Bayesian inference on\nproba-bilistic graphical models. These include two mechanisms for adding noise\nto the Bayesian updates, either directly to the posterior parameters, or to\ntheir Fourier transform so as to preserve update consistency. We also utilise a\nrecently introduced posterior sampling mechanism, for which we prove bounds for\nthe specific but general case of discrete Bayesian networks; and we introduce a\nmaximum-a-posteriori private mechanism. Our analysis includes utility and\nprivacy bounds, with a novel focus on the influence of graph structure on\nprivacy. Worked examples and experiments with Bayesian na{\\\"i}ve Bayes and\nBayesian linear regression illustrate the application of our mechanisms.\n", "versions": [{"version": "v1", "created": "Tue, 22 Dec 2015 09:22:39 GMT"}], "update_date": "2015-12-23", "authors_parsed": [["Zhang", "Zuhe", ""], ["Rubinstein", "Benjamin", ""], ["Dimitrakakis", "Christos", ""]]}, {"id": "1512.06999", "submitter": "Gael Varoquaux", "authors": "Ga\\\"el Varoquaux (PARIETAL), Michael Eickenberg (PARIETAL), Elvis\n  Dohmatob (PARIETAL), Bertand Thirion (PARIETAL)", "title": "FAASTA: A fast solver for total-variation regularization of\n  ill-conditioned problems with application to brain imaging", "comments": null, "journal-ref": "Colloque GRETSI, Sep 2015, Lyon, France. Gretsi, 2015,\n  http://www.gretsi.fr/colloque2015/myGretsi/programme.php", "doi": null, "report-no": null, "categories": "q-bio.NC cs.LG stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The total variation (TV) penalty, as many other analysis-sparsity problems,\ndoes not lead to separable factors or a proximal operatorwith a closed-form\nexpression, such as soft thresholding for the $\\ell\\_1$ penalty. As a result,\nin a variational formulation of an inverse problem or statisticallearning\nestimation, it leads to challenging non-smooth optimization problemsthat are\noften solved with elaborate single-step first-order methods. When thedata-fit\nterm arises from empirical measurements, as in brain imaging, it isoften very\nill-conditioned and without simple structure. In this situation, in proximal\nsplitting methods, the computation cost of thegradient step can easily dominate\neach iteration. Thus it is beneficialto minimize the number of gradient\nsteps.We present fAASTA, a variant of FISTA, that relies on an internal solver\nforthe TV proximal operator, and refines its tolerance to balance\ncomputationalcost of the gradient and the proximal steps. We give benchmarks\nandillustrations on \"brain decoding\": recovering brain maps from\nnoisymeasurements to predict observed behavior. The algorithm as well as\ntheempirical study of convergence speed are valuable for any non-exact\nproximaloperator, in particular analysis-sparsity problems.\n", "versions": [{"version": "v1", "created": "Tue, 22 Dec 2015 09:35:55 GMT"}], "update_date": "2015-12-23", "authors_parsed": [["Varoquaux", "Ga\u00ebl", "", "PARIETAL"], ["Eickenberg", "Michael", "", "PARIETAL"], ["Dohmatob", "Elvis", "", "PARIETAL"], ["Thirion", "Bertand", "", "PARIETAL"]]}, {"id": "1512.07041", "submitter": "Andrey Makarenko", "authors": "A.V. Makarenko, M.G. Volovik", "title": "Implementation of deep learning algorithm for automatic detection of\n  brain tumors using intraoperative IR-thermal mapping data", "comments": "7 pages, 5 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG q-bio.QM stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The efficiency of deep machine learning for automatic delineation of tumor\nareas has been demonstrated for intraoperative neuronavigation using active\nIR-mapping with the use of the cold test. The proposed approach employs a\nmatrix IR-imager to remotely register the space-time distribution of surface\ntemperature pattern, which is determined by the dynamics of local cerebral\nblood flow. The advantages of this technique are non-invasiveness, zero risks\nfor the health of patients and medical staff, low implementation and\noperational costs, ease and speed of use. Traditional IR-diagnostic technique\nhas a crucial limitation - it involves a diagnostician who determines the\nboundaries of tumor areas, which gives rise to considerable uncertainty, which\ncan lead to diagnosis errors that are difficult to control. The current study\ndemonstrates that implementing deep learning algorithms allows to eliminate the\nexplained drawback.\n", "versions": [{"version": "v1", "created": "Tue, 22 Dec 2015 11:52:26 GMT"}], "update_date": "2015-12-23", "authors_parsed": [["Makarenko", "A. V.", ""], ["Volovik", "M. G.", ""]]}, {"id": "1512.07074", "submitter": "Chunyang Xiao", "authors": "Chunyang Xiao", "title": "Move from Perturbed scheme to exponential weighting average", "comments": "10 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In an online decision problem, one makes decisions often with a pool of\ndecision sequence called experts but without knowledge of the future. After\neach step, one pays a cost based on the decision and observed rate. One\nreasonal goal would be to perform as well as the best expert in the pool. The\nmodern and well-known way to attain this goal is the algorithm of exponential\nweighting. However, recently, another algorithm called follow the perturbed\nleader is developed and achieved about the same performance. In our work, we\nfirst show the properties shared in common by the two algorithms which explain\nthe similarities on the performance. Next we will show that for a specific\nperturbation, the two algorithms are identical. Finally, we show with some\nexamples that follow-the-leader style algorithms extend naturally to a large\nclass of structured online problems for which the exponential algorithms are\ninefficient.\n", "versions": [{"version": "v1", "created": "Tue, 22 Dec 2015 13:18:17 GMT"}], "update_date": "2015-12-23", "authors_parsed": [["Xiao", "Chunyang", ""]]}, {"id": "1512.07108", "submitter": "Jiuxiang Gu Mr", "authors": "Jiuxiang Gu, Zhenhua Wang, Jason Kuen, Lianyang Ma, Amir Shahroudy,\n  Bing Shuai, Ting Liu, Xingxing Wang, Li Wang, Gang Wang, Jianfei Cai, Tsuhan\n  Chen", "title": "Recent Advances in Convolutional Neural Networks", "comments": "Pattern Recognition, Elsevier", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the last few years, deep learning has led to very good performance on a\nvariety of problems, such as visual recognition, speech recognition and natural\nlanguage processing. Among different types of deep neural networks,\nconvolutional neural networks have been most extensively studied. Leveraging on\nthe rapid growth in the amount of the annotated data and the great improvements\nin the strengths of graphics processor units, the research on convolutional\nneural networks has been emerged swiftly and achieved state-of-the-art results\non various tasks. In this paper, we provide a broad survey of the recent\nadvances in convolutional neural networks. We detailize the improvements of CNN\non different aspects, including layer design, activation function, loss\nfunction, regularization, optimization and fast computation. Besides, we also\nintroduce various applications of convolutional neural networks in computer\nvision, speech and natural language processing.\n", "versions": [{"version": "v1", "created": "Tue, 22 Dec 2015 14:54:34 GMT"}, {"version": "v2", "created": "Tue, 5 Jul 2016 11:39:16 GMT"}, {"version": "v3", "created": "Mon, 1 Aug 2016 01:54:59 GMT"}, {"version": "v4", "created": "Sat, 6 Aug 2016 12:38:35 GMT"}, {"version": "v5", "created": "Thu, 5 Jan 2017 05:45:53 GMT"}, {"version": "v6", "created": "Thu, 19 Oct 2017 16:34:35 GMT"}], "update_date": "2017-10-20", "authors_parsed": [["Gu", "Jiuxiang", ""], ["Wang", "Zhenhua", ""], ["Kuen", "Jason", ""], ["Ma", "Lianyang", ""], ["Shahroudy", "Amir", ""], ["Shuai", "Bing", ""], ["Liu", "Ting", ""], ["Wang", "Xingxing", ""], ["Wang", "Li", ""], ["Wang", "Gang", ""], ["Cai", "Jianfei", ""], ["Chen", "Tsuhan", ""]]}, {"id": "1512.07146", "submitter": "Steve Hanneke", "authors": "Steve Hanneke", "title": "Refined Error Bounds for Several Learning Algorithms", "comments": null, "journal-ref": "Journal of Machine Learning Research, Vol. 17 (2016), No. 135, pp.\n  1-55", "doi": null, "report-no": null, "categories": "cs.LG math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article studies the achievable guarantees on the error rates of certain\nlearning algorithms, with particular focus on refining logarithmic factors.\nMany of the results are based on a general technique for obtaining bounds on\nthe error rates of sample-consistent classifiers with monotonic error regions,\nin the realizable case. We prove bounds of this type expressed in terms of\neither the VC dimension or the sample compression size. This general technique\nalso enables us to derive several new bounds on the error rates of general\nsample-consistent learning algorithms, as well as refined bounds on the label\ncomplexity of the CAL active learning algorithm. Additionally, we establish a\nsimple necessary and sufficient condition for the existence of a\ndistribution-free bound on the error rates of all sample-consistent learning\nrules, converging at a rate inversely proportional to the sample size. We also\nstudy learning in the presence of classification noise, deriving a new excess\nerror rate guarantee for general VC classes under Tsybakov's noise condition,\nand establishing a simple and general necessary and sufficient condition for\nthe minimax excess risk under bounded noise to converge at a rate inversely\nproportional to the sample size.\n", "versions": [{"version": "v1", "created": "Tue, 22 Dec 2015 16:17:43 GMT"}, {"version": "v2", "created": "Sat, 10 Sep 2016 15:11:33 GMT"}], "update_date": "2016-09-13", "authors_parsed": [["Hanneke", "Steve", ""]]}, {"id": "1512.07158", "submitter": "Baichuan Zhang", "authors": "Baichuan Zhang, Noman Mohammed, Vachik Dave, Mohammad Al Hasan", "title": "Feature Selection for Classification under Anonymity Constraint", "comments": "Transactions on Data Privacy 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Over the last decade, proliferation of various online platforms and their\nincreasing adoption by billions of users have heightened the privacy risk of a\nuser enormously. In fact, security researchers have shown that sparse microdata\ncontaining information about online activities of a user although anonymous,\ncan still be used to disclose the identity of the user by cross-referencing the\ndata with other data sources. To preserve the privacy of a user, in existing\nworks several methods (k-anonymity, l-diversity, differential privacy) are\nproposed that ensure a dataset which is meant to share or publish bears small\nidentity disclosure risk. However, the majority of these methods modify the\ndata in isolation, without considering their utility in subsequent knowledge\ndiscovery tasks, which makes these datasets less informative. In this work, we\nconsider labeled data that are generally used for classification, and propose\ntwo methods for feature selection considering two goals: first, on the reduced\nfeature set the data has small disclosure risk, and second, the utility of the\ndata is preserved for performing a classification task. Experimental results on\nvarious real-world datasets show that the method is effective and useful in\npractice.\n", "versions": [{"version": "v1", "created": "Tue, 22 Dec 2015 17:06:01 GMT"}, {"version": "v2", "created": "Sat, 13 Feb 2016 03:05:36 GMT"}, {"version": "v3", "created": "Fri, 19 Feb 2016 02:01:57 GMT"}, {"version": "v4", "created": "Thu, 17 Mar 2016 02:30:33 GMT"}, {"version": "v5", "created": "Thu, 1 Dec 2016 01:05:59 GMT"}, {"version": "v6", "created": "Tue, 31 Jan 2017 15:47:47 GMT"}, {"version": "v7", "created": "Mon, 6 Feb 2017 01:14:37 GMT"}], "update_date": "2017-02-07", "authors_parsed": [["Zhang", "Baichuan", ""], ["Mohammed", "Noman", ""], ["Dave", "Vachik", ""], ["Hasan", "Mohammad Al", ""]]}, {"id": "1512.07336", "submitter": "Pengtao Xie", "authors": "Pengtao Xie, Yuntian Deng, Eric Xing", "title": "Latent Variable Modeling with Diversity-Inducing Mutual Angular\n  Regularization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Latent Variable Models (LVMs) are a large family of machine learning models\nproviding a principled and effective way to extract underlying patterns,\nstructure and knowledge from observed data. Due to the dramatic growth of\nvolume and complexity of data, several new challenges have emerged and cannot\nbe effectively addressed by existing LVMs: (1) How to capture long-tail\npatterns that carry crucial information when the popularity of patterns is\ndistributed in a power-law fashion? (2) How to reduce model complexity and\ncomputational cost without compromising the modeling power of LVMs? (3) How to\nimprove the interpretability and reduce the redundancy of discovered patterns?\nTo addresses the three challenges discussed above, we develop a novel\nregularization technique for LVMs, which controls the geometry of the latent\nspace during learning to enable the learned latent components of LVMs to be\ndiverse in the sense that they are favored to be mutually different from each\nother, to accomplish long-tail coverage, low redundancy, and better\ninterpretability. We propose a mutual angular regularizer (MAR) to encourage\nthe components in LVMs to have larger mutual angles. The MAR is non-convex and\nnon-smooth, entailing great challenges for optimization. To cope with this\nissue, we derive a smooth lower bound of the MAR and optimize the lower bound\ninstead. We show that the monotonicity of the lower bound is closely aligned\nwith the MAR to qualify the lower bound as a desirable surrogate of the MAR.\nUsing neural network (NN) as an instance, we analyze how the MAR affects the\ngeneralization performance of NN. On two popular latent variable models ---\nrestricted Boltzmann machine and distance metric learning, we demonstrate that\nMAR can effectively capture long-tail patterns, reduce model complexity without\nsacrificing expressivity and improve interpretability.\n", "versions": [{"version": "v1", "created": "Wed, 23 Dec 2015 02:29:39 GMT"}], "update_date": "2015-12-24", "authors_parsed": [["Xie", "Pengtao", ""], ["Deng", "Yuntian", ""], ["Xing", "Eric", ""]]}, {"id": "1512.07344", "submitter": "Xin Yuan", "authors": "Yunchen Pu, Xin Yuan, Andrew Stevens, Chunyuan Li, Lawrence Carin", "title": "A Deep Generative Deconvolutional Image Model", "comments": "10 pages, 7 figures. Appearing in Proceedings of the 19th\n  International Conference on Artificial Intelligence and Statistics (AISTATS)\n  2016, Cadiz, Spain. JMLR: W&CP volume 41", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A deep generative model is developed for representation and analysis of\nimages, based on a hierarchical convolutional dictionary-learning framework.\nStochastic {\\em unpooling} is employed to link consecutive layers in the model,\nyielding top-down image generation. A Bayesian support vector machine is linked\nto the top-layer features, yielding max-margin discrimination. Deep\ndeconvolutional inference is employed when testing, to infer the latent\nfeatures, and the top-layer features are connected with the max-margin\nclassifier for discrimination tasks. The model is efficiently trained using a\nMonte Carlo expectation-maximization (MCEM) algorithm, with implementation on\ngraphical processor units (GPUs) for efficient large-scale learning, and fast\ntesting. Excellent results are obtained on several benchmark datasets,\nincluding ImageNet, demonstrating that the proposed model achieves results that\nare highly competitive with similarly sized convolutional neural networks.\n", "versions": [{"version": "v1", "created": "Wed, 23 Dec 2015 03:10:29 GMT"}], "update_date": "2015-12-25", "authors_parsed": [["Pu", "Yunchen", ""], ["Yuan", "Xin", ""], ["Stevens", "Andrew", ""], ["Li", "Chunyuan", ""], ["Carin", "Lawrence", ""]]}, {"id": "1512.07422", "submitter": "Rodolphe Jenatton", "authors": "Rodolphe Jenatton, Jim Huang, C\\'edric Archambeau", "title": "Adaptive Algorithms for Online Convex Optimization with Long-term\n  Constraints", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an adaptive online gradient descent algorithm to solve online\nconvex optimization problems with long-term constraints , which are constraints\nthat need to be satisfied when accumulated over a finite number of rounds T ,\nbut can be violated in intermediate rounds. For some user-defined trade-off\nparameter $\\beta$ $\\in$ (0, 1), the proposed algorithm achieves cumulative\nregret bounds of O(T^max{$\\beta$,1--$\\beta$}) and O(T^(1--$\\beta$/2)) for the\nloss and the constraint violations respectively. Our results hold for convex\nlosses and can handle arbitrary convex constraints without requiring knowledge\nof the number of rounds in advance. Our contributions improve over the best\nknown cumulative regret bounds by Mahdavi, et al. (2012) that are respectively\nO(T^1/2) and O(T^3/4) for general convex domains, and respectively O(T^2/3) and\nO(T^2/3) when further restricting to polyhedral domains. We supplement the\nanalysis with experiments validating the performance of our algorithm in\npractice.\n", "versions": [{"version": "v1", "created": "Wed, 23 Dec 2015 10:32:09 GMT"}], "update_date": "2015-12-24", "authors_parsed": [["Jenatton", "Rodolphe", ""], ["Huang", "Jim", ""], ["Archambeau", "C\u00e9dric", ""]]}, {"id": "1512.07446", "submitter": "Cem Tekin", "authors": "Cem Tekin, Jinsung Yoon, Mihaela van der Schaar", "title": "Adaptive Ensemble Learning with Confidence Bounds", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Extracting actionable intelligence from distributed, heterogeneous,\ncorrelated and high-dimensional data sources requires run-time processing and\nlearning both locally and globally. In the last decade, a large number of\nmeta-learning techniques have been proposed in which local learners make online\npredictions based on their locally-collected data instances, and feed these\npredictions to an ensemble learner, which fuses them and issues a global\nprediction. However, most of these works do not provide performance guarantees\nor, when they do, these guarantees are asymptotic. None of these existing works\nprovide confidence estimates about the issued predictions or rate of learning\nguarantees for the ensemble learner. In this paper, we provide a systematic\nensemble learning method called Hedged Bandits, which comes with both long run\n(asymptotic) and short run (rate of learning) performance guarantees. Moreover,\nour approach yields performance guarantees with respect to the optimal local\nprediction strategy, and is also able to adapt its predictions in a data-driven\nmanner. We illustrate the performance of Hedged Bandits in the context of\nmedical informatics and show that it outperforms numerous online and offline\nensemble learning methods.\n", "versions": [{"version": "v1", "created": "Wed, 23 Dec 2015 12:08:15 GMT"}, {"version": "v2", "created": "Thu, 30 Jun 2016 21:36:37 GMT"}, {"version": "v3", "created": "Sun, 30 Oct 2016 12:55:05 GMT"}], "update_date": "2016-11-01", "authors_parsed": [["Tekin", "Cem", ""], ["Yoon", "Jinsung", ""], ["van der Schaar", "Mihaela", ""]]}, {"id": "1512.07587", "submitter": "Rajasekaran Masatran", "authors": "Rajasekaran Masatran", "title": "A Latent-Variable Lattice Model", "comments": "6 pages, with 4 figures, 8 algorithms, and 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Markov random field (MRF) learning is intractable, and its approximation\nalgorithms are computationally expensive. We target a small subset of MRF that\nis used frequently in computer vision. We characterize this subset with three\nconcepts: Lattice, Homogeneity, and Inertia; and design a non-markov model as\nan alternative. Our goal is robust learning from small datasets. Our learning\nalgorithm uses vector quantization and, at time complexity O(U log U) for a\ndataset of U pixels, is much faster than that of general-purpose MRF.\n", "versions": [{"version": "v1", "created": "Wed, 23 Dec 2015 19:01:03 GMT"}, {"version": "v2", "created": "Thu, 28 Jan 2016 16:57:50 GMT"}, {"version": "v3", "created": "Mon, 8 Feb 2016 08:48:46 GMT"}, {"version": "v4", "created": "Sat, 5 Mar 2016 13:07:09 GMT"}, {"version": "v5", "created": "Fri, 20 May 2016 08:30:02 GMT"}, {"version": "v6", "created": "Wed, 25 May 2016 09:17:23 GMT"}, {"version": "v7", "created": "Wed, 8 Jun 2016 03:25:09 GMT"}], "update_date": "2016-06-09", "authors_parsed": [["Masatran", "Rajasekaran", ""]]}, {"id": "1512.07638", "submitter": "Paul Reverdy", "authors": "Paul Reverdy and Vaibhav Srivastava and Naomi Ehrich Leonard", "title": "Satisficing in multi-armed bandit problems", "comments": "To appear in IEEE Transactions on Automatic Control", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Satisficing is a relaxation of maximizing and allows for less risky decision\nmaking in the face of uncertainty. We propose two sets of satisficing\nobjectives for the multi-armed bandit problem, where the objective is to\nachieve reward-based decision-making performance above a given threshold. We\nshow that these new problems are equivalent to various standard multi-armed\nbandit problems with maximizing objectives and use the equivalence to find\nbounds on performance. The different objectives can result in qualitatively\ndifferent behavior; for example, agents explore their options continually in\none case and only a finite number of times in another. For the case of Gaussian\nrewards we show an additional equivalence between the two sets of satisficing\nobjectives that allows algorithms developed for one set to be applied to the\nother. We then develop variants of the Upper Credible Limit (UCL) algorithm\nthat solve the problems with satisficing objectives and show that these\nmodified UCL algorithms achieve efficient satisficing performance.\n", "versions": [{"version": "v1", "created": "Wed, 23 Dec 2015 21:05:16 GMT"}, {"version": "v2", "created": "Mon, 19 Dec 2016 17:16:07 GMT"}], "update_date": "2016-12-20", "authors_parsed": [["Reverdy", "Paul", ""], ["Srivastava", "Vaibhav", ""], ["Leonard", "Naomi Ehrich", ""]]}, {"id": "1512.07650", "submitter": "Yahel David", "authors": "Yahel David and Nahum Shimkin", "title": "The Max $K$-Armed Bandit: PAC Lower Bounds and Efficient Algorithms", "comments": "arXiv admin note: substantial text overlap with arXiv:1508.05608", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the Max $K$-Armed Bandit problem, where a learning agent is faced\nwith several stochastic arms, each a source of i.i.d. rewards of unknown\ndistribution. At each time step the agent chooses an arm, and observes the\nreward of the obtained sample. Each sample is considered here as a separate\nitem with the reward designating its value, and the goal is to find an item\nwith the highest possible value. Our basic assumption is a known lower bound on\nthe {\\em tail function} of the reward distributions. Under the PAC framework,\nwe provide a lower bound on the sample complexity of any\n$(\\epsilon,\\delta)$-correct algorithm, and propose an algorithm that attains\nthis bound up to logarithmic factors. We analyze the robustness of the proposed\nalgorithm and in addition, we compare the performance of this algorithm to the\nvariant in which the arms are not distinguishable by the agent and are chosen\nrandomly at each stage. Interestingly, when the maximal rewards of the arms\nhappen to be similar, the latter approach may provide better performance.\n", "versions": [{"version": "v1", "created": "Wed, 23 Dec 2015 22:11:02 GMT"}], "update_date": "2015-12-25", "authors_parsed": [["David", "Yahel", ""], ["Shimkin", "Nahum", ""]]}, {"id": "1512.07679", "submitter": "Gabriel Dulac-Arnold", "authors": "Gabriel Dulac-Arnold and Richard Evans and Hado van Hasselt and Peter\n  Sunehag and Timothy Lillicrap and Jonathan Hunt and Timothy Mann and\n  Theophane Weber and Thomas Degris and Ben Coppin", "title": "Deep Reinforcement Learning in Large Discrete Action Spaces", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Being able to reason in an environment with a large number of discrete\nactions is essential to bringing reinforcement learning to a larger class of\nproblems. Recommender systems, industrial plants and language models are only\nsome of the many real-world tasks involving large numbers of discrete actions\nfor which current methods are difficult or even often impossible to apply. An\nability to generalize over the set of actions as well as sub-linear complexity\nrelative to the size of the set are both necessary to handle such tasks.\nCurrent approaches are not able to provide both of these, which motivates the\nwork in this paper. Our proposed approach leverages prior information about the\nactions to embed them in a continuous space upon which it can generalize.\nAdditionally, approximate nearest-neighbor methods allow for logarithmic-time\nlookup complexity relative to the number of actions, which is necessary for\ntime-wise tractable training. This combined approach allows reinforcement\nlearning methods to be applied to large-scale learning problems previously\nintractable with current methods. We demonstrate our algorithm's abilities on a\nseries of tasks having up to one million actions.\n", "versions": [{"version": "v1", "created": "Thu, 24 Dec 2015 01:31:40 GMT"}, {"version": "v2", "created": "Mon, 4 Apr 2016 11:27:36 GMT"}], "update_date": "2016-04-05", "authors_parsed": [["Dulac-Arnold", "Gabriel", ""], ["Evans", "Richard", ""], ["van Hasselt", "Hado", ""], ["Sunehag", "Peter", ""], ["Lillicrap", "Timothy", ""], ["Hunt", "Jonathan", ""], ["Mann", "Timothy", ""], ["Weber", "Theophane", ""], ["Degris", "Thomas", ""], ["Coppin", "Ben", ""]]}, {"id": "1512.07716", "submitter": "Hugh Perkins", "authors": "Hugh Perkins, Minjie Xu, Jun Zhu, Bo Zhang", "title": "Fast Parallel SVM using Data Augmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As one of the most popular classifiers, linear SVMs still have challenges in\ndealing with very large-scale problems, even though linear or sub-linear\nalgorithms have been developed recently on single machines. Parallel computing\nmethods have been developed for learning large-scale SVMs. However, existing\nmethods rely on solving local sub-optimization problems. In this paper, we\ndevelop a novel parallel algorithm for learning large-scale linear SVM. Our\napproach is based on a data augmentation equivalent formulation, which casts\nthe problem of learning SVM as a Bayesian inference problem, for which we can\ndevelop very efficient parallel sampling methods. We provide empirical results\nfor this parallel sampling SVM, and provide extensions for SVR, non-linear\nkernels, and provide a parallel implementation of the Crammer and Singer model.\nThis approach is very promising in its own right, and further is a very useful\ntechnique to parallelize a broader family of general maximum-margin models.\n", "versions": [{"version": "v1", "created": "Thu, 24 Dec 2015 04:56:28 GMT"}], "update_date": "2015-12-25", "authors_parsed": [["Perkins", "Hugh", ""], ["Xu", "Minjie", ""], ["Zhu", "Jun", ""], ["Zhang", "Bo", ""]]}, {"id": "1512.07748", "submitter": "Tomohiko Nakamura", "authors": "Tomohiko Nakamura, Eita Nakamura and Shigeki Sagayama", "title": "Real-Time Audio-to-Score Alignment of Music Performances Containing\n  Errors and Arbitrary Repeats and Skips", "comments": "12 pages, 8 figures, version accepted in IEEE/ACM Transactions on\n  Audio, Speech, and Language Processing", "journal-ref": null, "doi": "10.1109/TASLP.2015.2507862", "report-no": null, "categories": "cs.SD cs.LG cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper discusses real-time alignment of audio signals of music\nperformance to the corresponding score (a.k.a. score following) which can\nhandle tempo changes, errors and arbitrary repeats and/or skips (repeats/skips)\nin performances. This type of score following is particularly useful in\nautomatic accompaniment for practices and rehearsals, where errors and\nrepeats/skips are often made. Simple extensions of the algorithms previously\nproposed in the literature are not applicable in these situations for scores of\npractical length due to the problem of large computational complexity. To cope\nwith this problem, we present two hidden Markov models of monophonic\nperformance with errors and arbitrary repeats/skips, and derive efficient\nscore-following algorithms with an assumption that the prior probability\ndistributions of score positions before and after repeats/skips are independent\nfrom each other. We confirmed real-time operation of the algorithms with music\nscores of practical length (around 10000 notes) on a modern laptop and their\ntracking ability to the input performance within 0.7 s on average after\nrepeats/skips in clarinet performance data. Further improvements and extension\nfor polyphonic signals are also discussed.\n", "versions": [{"version": "v1", "created": "Thu, 24 Dec 2015 08:21:48 GMT"}], "update_date": "2015-12-25", "authors_parsed": [["Nakamura", "Tomohiko", ""], ["Nakamura", "Eita", ""], ["Sagayama", "Shigeki", ""]]}, {"id": "1512.07797", "submitter": "Jiaqian Yu", "authors": "Jiaqian Yu (CVC, GALEN), Matthew Blaschko", "title": "The Lov\\'asz Hinge: A Novel Convex Surrogate for Submodular Losses", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning with non-modular losses is an important problem when sets of\npredictions are made simultaneously. The main tools for constructing convex\nsurrogate loss functions for set prediction are margin rescaling and slack\nrescaling. In this work, we show that these strategies lead to tight convex\nsurrogates iff the underlying loss function is increasing in the number of\nincorrect predictions. However, gradient or cutting-plane computation for these\nfunctions is NP-hard for non-supermodular loss functions. We propose instead a\nnovel surrogate loss function for submodular losses, the Lov\\'asz hinge, which\nleads to O(p log p) complexity with O(p) oracle accesses to the loss function\nto compute a gradient or cutting-plane. We prove that the Lov\\'asz hinge is\nconvex and yields an extension. As a result, we have developed the first\ntractable convex surrogates in the literature for submodular losses. We\ndemonstrate the utility of this novel convex surrogate through several set\nprediction tasks, including on the PASCAL VOC and Microsoft COCO datasets.\n", "versions": [{"version": "v1", "created": "Thu, 24 Dec 2015 11:49:47 GMT"}, {"version": "v2", "created": "Mon, 15 May 2017 11:25:31 GMT"}], "update_date": "2017-05-16", "authors_parsed": [["Yu", "Jiaqian", "", "CVC, GALEN"], ["Blaschko", "Matthew", ""]]}, {"id": "1512.07807", "submitter": "Homayun Afrabandpey", "authors": "Seppo Virtanen, Homayun Afrabandpey, Samuel Kaski", "title": "Visualizations Relevant to The User By Multi-View Latent Variable\n  Factorization", "comments": "IEEE International Conference on Acoustic, Speech and Signal\n  Processing 2016", "journal-ref": null, "doi": "10.1109/ICASSP.2016.7472120", "report-no": null, "categories": "cs.LG cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A main goal of data visualization is to find, from among all the available\nalternatives, mappings to the 2D/3D display which are relevant to the user.\nAssuming user interaction data, or other auxiliary data about the items or\ntheir relationships, the goal is to identify which aspects in the primary data\nsupport the user\\'s input and, equally importantly, which aspects of the\nuser\\'s potentially noisy input have support in the primary data. For solving\nthe problem, we introduce a multi-view embedding in which a latent\nfactorization identifies which aspects in the two data views (primary data and\nuser data) are related and which are specific to only one of them. The\nfactorization is a generative model in which the display is parameterized as a\npart of the factorization and the other factors explain away the aspects not\nexpressible in a two-dimensional display. Functioning of the model is\ndemonstrated on several data sets.\n", "versions": [{"version": "v1", "created": "Thu, 24 Dec 2015 12:53:39 GMT"}, {"version": "v2", "created": "Mon, 25 Jan 2016 12:12:10 GMT"}], "update_date": "2016-09-28", "authors_parsed": [["Virtanen", "Seppo", ""], ["Afrabandpey", "Homayun", ""], ["Kaski", "Samuel", ""]]}, {"id": "1512.07839", "submitter": "Sacha Sokoloski", "authors": "Sacha Sokoloski", "title": "Implementing a Bayes Filter in a Neural Circuit: The Case of Unknown\n  Stimulus Dynamics", "comments": "This is the final version, and has been accepted for publication in\n  Neural Computation", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In order to interact intelligently with objects in the world, animals must\nfirst transform neural population responses into estimates of the dynamic,\nunknown stimuli which caused them. The Bayesian solution to this problem is\nknown as a Bayes filter, which applies Bayes' rule to combine population\nresponses with the predictions of an internal model. In this paper we present a\nmethod for learning to approximate a Bayes filter when the stimulus dynamics\nare unknown. To do this we use the inferential properties of probabilistic\npopulation codes to compute Bayes' rule, and train a neural network to compute\napproximate predictions by the method of maximum likelihood. In particular, we\nperform stochastic gradient descent on the negative log-likelihood with a novel\napproximation of the gradient. We demonstrate our methods on a finite-state, a\nlinear, and a nonlinear filtering problem, and show how the hidden layer of the\nneural network develops tuning curves which are consistent with findings in\nexperimental neuroscience.\n", "versions": [{"version": "v1", "created": "Tue, 22 Dec 2015 14:52:14 GMT"}, {"version": "v2", "created": "Mon, 16 May 2016 10:35:55 GMT"}, {"version": "v3", "created": "Thu, 17 Nov 2016 20:46:39 GMT"}, {"version": "v4", "created": "Wed, 7 Jun 2017 00:34:49 GMT"}], "update_date": "2017-06-08", "authors_parsed": [["Sokoloski", "Sacha", ""]]}, {"id": "1512.07851", "submitter": "Joseph Keshet", "authors": "Joseph Keshet, Adam Kariv, Arnon Dagan, Dvir Volk, Joey Simhon", "title": "Context-Based Prediction of App Usage", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There are around a hundred installed apps on an average smartphone. The high\nnumber of apps and the limited number of app icons that can be displayed on the\ndevice's screen requires a new paradigm to address their visibility to the\nuser. In this paper we propose a new online algorithm for dynamically\npredicting a set of apps that the user is likely to use. The algorithm runs on\nthe user's device and constantly learns the user's habits at a given time,\nlocation, and device state. It is designed to actively help the user to\nnavigate to the desired app as well as to provide a personalized feeling, and\nhence is aimed at maximizing the AUC. We show both theoretically and\nempirically that the algorithm maximizes the AUC, and yields good results on a\nset of 1,000 devices.\n", "versions": [{"version": "v1", "created": "Thu, 24 Dec 2015 16:27:40 GMT"}, {"version": "v2", "created": "Mon, 25 Jan 2016 19:39:40 GMT"}], "update_date": "2016-01-26", "authors_parsed": [["Keshet", "Joseph", ""], ["Kariv", "Adam", ""], ["Dagan", "Arnon", ""], ["Volk", "Dvir", ""], ["Simhon", "Joey", ""]]}, {"id": "1512.07876", "submitter": "Chao Liu", "authors": "Chao Liu, Sambuddha Ghosal, Zhanhong Jiang, Soumik Sarkar", "title": "An unsupervised spatiotemporal graphical modeling approach to anomaly\n  detection in distributed CPS", "comments": "ICCPS 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modern distributed cyber-physical systems (CPSs) encounter a large variety of\nphysical faults and cyber anomalies and in many cases, they are vulnerable to\ncatastrophic fault propagation scenarios due to strong connectivity among the\nsub-systems. This paper presents a new data-driven framework for system-wide\nanomaly detection for addressing such issues. The framework is based on a\nspatiotemporal feature extraction scheme built on the concept of symbolic\ndynamics for discovering and representing causal interactions among the\nsubsystems of a CPS. The extracted spatiotemporal features are then used to\nlearn system-wide patterns via a Restricted Boltzmann Machine (RBM). The\nresults show that: (1) the RBM free energy in the off-nominal conditions is\ndifferent from that in the nominal conditions and can be used for anomaly\ndetection; (2) the framework can capture multiple nominal modes with one\ngraphical model; (3) the case studies with simulated data and an integrated\nbuilding system validate the proposed approach.\n", "versions": [{"version": "v1", "created": "Thu, 24 Dec 2015 18:15:42 GMT"}, {"version": "v2", "created": "Fri, 20 May 2016 16:44:59 GMT"}], "update_date": "2016-05-23", "authors_parsed": [["Liu", "Chao", ""], ["Ghosal", "Sambuddha", ""], ["Jiang", "Zhanhong", ""], ["Sarkar", "Soumik", ""]]}, {"id": "1512.07962", "submitter": "Changyou Chen", "authors": "Changyou Chen, David Carlson, Zhe Gan, Chunyuan Li and Lawrence Carin", "title": "Bridging the Gap between Stochastic Gradient MCMC and Stochastic\n  Optimization", "comments": "Merry Christmas from the Santa (algorithm). AISTATS 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Stochastic gradient Markov chain Monte Carlo (SG-MCMC) methods are Bayesian\nanalogs to popular stochastic optimization methods; however, this connection is\nnot well studied. We explore this relationship by applying simulated annealing\nto an SGMCMC algorithm. Furthermore, we extend recent SG-MCMC methods with two\nkey components: i) adaptive preconditioners (as in ADAgrad or RMSprop), and ii)\nadaptive element-wise momentum weights. The zero-temperature limit gives a\nnovel stochastic optimization method with adaptive element-wise momentum\nweights, while conventional optimization methods only have a shared, static\nmomentum weight. Under certain assumptions, our theoretical analysis suggests\nthe proposed simulated annealing approach converges close to the global optima.\nExperiments on several deep neural network models show state-of-the-art results\ncompared to related stochastic optimization algorithms.\n", "versions": [{"version": "v1", "created": "Fri, 25 Dec 2015 06:01:44 GMT"}, {"version": "v2", "created": "Sat, 27 Feb 2016 16:26:22 GMT"}, {"version": "v3", "created": "Fri, 5 Aug 2016 14:49:57 GMT"}], "update_date": "2016-08-08", "authors_parsed": [["Chen", "Changyou", ""], ["Carlson", "David", ""], ["Gan", "Zhe", ""], ["Li", "Chunyuan", ""], ["Carin", "Lawrence", ""]]}, {"id": "1512.07982", "submitter": "Miltiadis Allamanis", "authors": "Fani A. Tzima, Miltiadis Allamanis, Alexandros Filotheou, Pericles A.\n  Mitkas", "title": "Inducing Generalized Multi-Label Rules with Learning Classifier Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, multi-label classification has attracted a significant body\nof research, motivated by real-life applications, such as text classification\nand medical diagnoses. Although sparsely studied in this context, Learning\nClassifier Systems are naturally well-suited to multi-label classification\nproblems, whose search space typically involves multiple highly specific\nniches. This is the motivation behind our current work that introduces a\ngeneralized multi-label rule format -- allowing for flexible label-dependency\nmodeling, with no need for explicit knowledge of which correlations to search\nfor -- and uses it as a guide for further adapting the general Michigan-style\nsupervised Learning Classifier System framework. The integration of the\naforementioned rule format and framework adaptations results in a novel\nalgorithm for multi-label classification whose behavior is studied through a\nset of properly defined artificial problems. The proposed algorithm is also\nthoroughly evaluated on a set of multi-label datasets and found competitive to\nother state-of-the-art multi-label classification methods.\n", "versions": [{"version": "v1", "created": "Fri, 25 Dec 2015 10:03:55 GMT"}], "update_date": "2015-12-29", "authors_parsed": [["Tzima", "Fani A.", ""], ["Allamanis", "Miltiadis", ""], ["Filotheou", "Alexandros", ""], ["Mitkas", "Pericles A.", ""]]}, {"id": "1512.08008", "submitter": "Ognjen Arandjelovi\\'c PhD", "authors": "Adham Beykikhoshk and Ognjen Arandjelovic and Dinh Phung and Svetha\n  Venkatesh", "title": "Discovering topic structures of a temporally evolving document corpus", "comments": "2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we describe a novel framework for the discovery of the topical\ncontent of a data corpus, and the tracking of its complex structural changes\nacross the temporal dimension. In contrast to previous work our model does not\nimpose a prior on the rate at which documents are added to the corpus nor does\nit adopt the Markovian assumption which overly restricts the type of changes\nthat the model can capture. Our key technical contribution is a framework based\non (i) discretization of time into epochs, (ii) epoch-wise topic discovery\nusing a hierarchical Dirichlet process-based model, and (iii) a temporal\nsimilarity graph which allows for the modelling of complex topic changes:\nemergence and disappearance, evolution, splitting, and merging. The power of\nthe proposed framework is demonstrated on two medical literature corpora\nconcerned with the autism spectrum disorder (ASD) and the metabolic syndrome\n(MetS) -- both increasingly important research subjects with significant social\nand healthcare consequences. In addition to the collected ASD and metabolic\nsyndrome literature corpora which we made freely available, our contribution\nalso includes an extensive empirical analysis of the proposed framework. We\ndescribe a detailed and careful examination of the effects that our\nalgorithms's free parameters have on its output, and discuss the significance\nof the findings both in the context of the practical application of our\nalgorithm as well as in the context of the existing body of work on temporal\ntopic analysis. Our quantitative analysis is followed by several qualitative\ncase studies highly relevant to the current research on ASD and MetS, on which\nour algorithm is shown to capture well the actual developments in these fields.\n", "versions": [{"version": "v1", "created": "Fri, 25 Dec 2015 15:18:11 GMT"}], "update_date": "2015-12-29", "authors_parsed": [["Beykikhoshk", "Adham", ""], ["Arandjelovic", "Ognjen", ""], ["Phung", "Dinh", ""], ["Venkatesh", "Svetha", ""]]}, {"id": "1512.08064", "submitter": "Steve Hanneke", "authors": "Steve Hanneke, Liu Yang", "title": "Statistical Learning under Nonstationary Mixing Processes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study a special case of the problem of statistical learning without the\ni.i.d. assumption. Specifically, we suppose a learning method is presented with\na sequence of data points, and required to make a prediction (e.g., a\nclassification) for each one, and can then observe the loss incurred by this\nprediction. We go beyond traditional analyses, which have focused on stationary\nmixing processes or nonstationary product processes, by combining these two\nrelaxations to allow nonstationary mixing processes. We are particularly\ninterested in the case of $\\beta$-mixing processes, with the sum of changes in\nmarginal distributions growing sublinearly in the number of samples. Under\nthese conditions, we propose a learning method, and establish that for bounded\nVC subgraph classes, the cumulative excess risk grows sublinearly in the number\nof predictions, at a quantified rate.\n", "versions": [{"version": "v1", "created": "Sat, 26 Dec 2015 01:33:55 GMT"}, {"version": "v2", "created": "Sun, 20 May 2018 18:14:27 GMT"}], "update_date": "2018-05-22", "authors_parsed": [["Hanneke", "Steve", ""], ["Yang", "Liu", ""]]}, {"id": "1512.08065", "submitter": "Ming Jin", "authors": "Ming Jin, Andreas Damianou, Pieter Abbeel, Costas Spanos", "title": "Inverse Reinforcement Learning via Deep Gaussian Process", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.RO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new approach to inverse reinforcement learning (IRL) based on\nthe deep Gaussian process (deep GP) model, which is capable of learning\ncomplicated reward structures with few demonstrations. Our model stacks\nmultiple latent GP layers to learn abstract representations of the state\nfeature space, which is linked to the demonstrations through the Maximum\nEntropy learning framework. Incorporating the IRL engine into the nonlinear\nlatent structure renders existing deep GP inference approaches intractable. To\ntackle this, we develop a non-standard variational approximation framework\nwhich extends previous inference schemes. This allows for approximate Bayesian\ntreatment of the feature space and guards against overfitting. Carrying out\nrepresentation and inverse reinforcement learning simultaneously within our\nmodel outperforms state-of-the-art approaches, as we demonstrate with\nexperiments on standard benchmarks (\"object world\",\"highway driving\") and a new\nbenchmark (\"binary world\").\n", "versions": [{"version": "v1", "created": "Sat, 26 Dec 2015 01:40:37 GMT"}, {"version": "v2", "created": "Thu, 30 Mar 2017 03:36:37 GMT"}, {"version": "v3", "created": "Tue, 2 May 2017 03:11:45 GMT"}, {"version": "v4", "created": "Thu, 4 May 2017 23:20:24 GMT"}], "update_date": "2017-05-08", "authors_parsed": [["Jin", "Ming", ""], ["Damianou", "Andreas", ""], ["Abbeel", "Pieter", ""], ["Spanos", "Costas", ""]]}, {"id": "1512.08120", "submitter": "Fanhua Shang", "authors": "Fanhua Shang and James Cheng and Hong Cheng", "title": "Regularized Orthogonal Tensor Decompositions for Multi-Relational\n  Learning", "comments": "18 pages, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multi-relational learning has received lots of attention from researchers in\nvarious research communities. Most existing methods either suffer from\nsuperlinear per-iteration cost, or are sensitive to the given ranks. To address\nboth issues, we propose a scalable core tensor trace norm Regularized\nOrthogonal Iteration Decomposition (ROID) method for full or incomplete tensor\nanalytics, which can be generalized as a graph Laplacian regularized version by\nusing auxiliary information or a sparse higher-order orthogonal iteration\n(SHOOI) version. We first induce the equivalence relation of the Schatten\np-norm (0<p<\\infty) of a low multi-linear rank tensor and its core tensor. Then\nwe achieve a much smaller matrix trace norm minimization problem. Finally, we\ndevelop two efficient augmented Lagrange multiplier algorithms to solve our\nproblems with convergence guarantees. Extensive experiments using both real and\nsynthetic datasets, even though with only a few observations, verified both the\nefficiency and effectiveness of our methods.\n", "versions": [{"version": "v1", "created": "Sat, 26 Dec 2015 15:26:05 GMT"}, {"version": "v2", "created": "Sat, 16 Jan 2016 15:32:15 GMT"}], "update_date": "2016-01-19", "authors_parsed": [["Shang", "Fanhua", ""], ["Cheng", "James", ""], ["Cheng", "Hong", ""]]}, {"id": "1512.08133", "submitter": "Akshay Balsubramani", "authors": "Akshay Balsubramani", "title": "The Utility of Abstaining in Binary Classification", "comments": "Short survey", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We explore the problem of binary classification in machine learning, with a\ntwist - the classifier is allowed to abstain on any datum, professing ignorance\nabout the true class label without committing to any prediction. This is\ndirectly motivated by applications like medical diagnosis and fraud risk\nassessment, in which incorrect predictions have potentially calamitous\nconsequences. We focus on a recent spate of theoretically driven work in this\narea that characterizes how allowing abstentions can lead to fewer errors in\nvery general settings. Two areas are highlighted: the surprising possibility of\nzero-error learning, and the fundamental tradeoff between predicting\nsufficiently often and avoiding incorrect predictions. We review efficient\nalgorithms with provable guarantees for each of these areas. We also discuss\nconnections to other scenarios, notably active learning, as they suggest\npromising directions of further inquiry in this emerging field.\n", "versions": [{"version": "v1", "created": "Sat, 26 Dec 2015 19:02:00 GMT"}], "update_date": "2015-12-29", "authors_parsed": [["Balsubramani", "Akshay", ""]]}, {"id": "1512.08169", "submitter": "Peter Radecki", "authors": "Peter Radecki and Brandon Hencey", "title": "Self-Excitation: An Enabler for Online Thermal Estimation and Model\n  Predictive Control of Buildings", "comments": "11 pages, 10 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SY cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper investigates a method to improve buildings' thermal predictive\ncontrol performance via online identification and excitation (active learning\nprocess) that minimally disrupts normal operations. In previous studies we have\ndemonstrated scalable methods to acquire multi-zone thermal models of passive\nbuildings using a gray-box approach that leverages building topology and\nmeasurement data. Here we extend the method to multi-zone actively controlled\nbuildings and examine how to improve the thermal model estimation by using the\ncontroller to excite unknown portions of the building's dynamics. Comparing\nagainst a baseline thermostat controller, we demonstrate the utility of both\nthe initially acquired and improved thermal models within a Model Predictive\nControl (MPC) framework, which anticipates weather uncertainty and time-varying\ntemperature set-points. A simulation study demonstrates self-excitation\nimproves model estimation, which corresponds to improved MPC energy savings and\noccupant comfort. By coupling building topology, estimation, and control\nroutines into a single online framework, we have demonstrated the potential for\nlow-cost scalable methods to actively learn and control buildings to ensure\noccupant comfort and minimize energy usage, all while using the existing\nbuilding's HVAC sensors and hardware.\n", "versions": [{"version": "v1", "created": "Sun, 27 Dec 2015 04:47:21 GMT"}], "update_date": "2015-12-29", "authors_parsed": [["Radecki", "Peter", ""], ["Hencey", "Brandon", ""]]}, {"id": "1512.08178", "submitter": "Francesco Dinuzzo", "authors": "Jean-Baptiste Fiot, Francesco Dinuzzo", "title": "Electricity Demand Forecasting by Multi-Task Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We explore the application of kernel-based multi-task learning techniques to\nforecast the demand of electricity in multiple nodes of a distribution network.\nWe show that recently developed output kernel learning techniques are\nparticularly well suited to solve this problem, as they allow to flexibly model\nthe complex seasonal effects that characterize electricity demand data, while\nlearning and exploiting correlations between multiple demand profiles. We also\ndemonstrate that kernels with a multiplicative structure yield superior\npredictive performance with respect to the widely adopted (generalized)\nadditive models. Our study is based on residential and industrial smart meter\ndata provided by the Irish Commission for Energy Regulation (CER).\n", "versions": [{"version": "v1", "created": "Sun, 27 Dec 2015 07:18:03 GMT"}], "update_date": "2015-12-29", "authors_parsed": [["Fiot", "Jean-Baptiste", ""], ["Dinuzzo", "Francesco", ""]]}, {"id": "1512.08204", "submitter": "Massimiliano Pontil", "authors": "Andrew M. McDonald, Massimiliano Pontil, Dimitris Stamos", "title": "New Perspectives on $k$-Support and Cluster Norms", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study a regularizer which is defined as a parameterized infimum of\nquadratics, and which we call the box-norm. We show that the k-support norm, a\nregularizer proposed by [Argyriou et al, 2012] for sparse vector prediction\nproblems, belongs to this family, and the box-norm can be generated as a\nperturbation of the former. We derive an improved algorithm to compute the\nproximity operator of the squared box-norm, and we provide a method to compute\nthe norm. We extend the norms to matrices, introducing the spectral k-support\nnorm and spectral box-norm. We note that the spectral box-norm is essentially\nequivalent to the cluster norm, a multitask learning regularizer introduced by\n[Jacob et al. 2009a], and which in turn can be interpreted as a perturbation of\nthe spectral k-support norm. Centering the norm is important for multitask\nlearning and we also provide a method to use centered versions of the norms as\nregularizers. Numerical experiments indicate that the spectral k-support and\nbox-norms and their centered variants provide state of the art performance in\nmatrix completion and multitask learning problems respectively.\n", "versions": [{"version": "v1", "created": "Sun, 27 Dec 2015 11:30:37 GMT"}], "update_date": "2016-01-12", "authors_parsed": [["McDonald", "Andrew M.", ""], ["Pontil", "Massimiliano", ""], ["Stamos", "Dimitris", ""]]}, {"id": "1512.08240", "submitter": "Jesse Krijthe", "authors": "Jesse H. Krijthe and Marco Loog", "title": "Robust Semi-supervised Least Squares Classification by Implicit\n  Constraints", "comments": "Appeared as Pattern Recognition Volume 63, March 2017, Pages 115-126.\n  This version of the manuscript fixes some typos in the equations on page 9\n  that are incorrect in the published version", "journal-ref": "Pattern Recognition Volume 63, March 2017, Pages 115-126", "doi": "10.1016/j.patcog.2016.09.009", "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce the implicitly constrained least squares (ICLS) classifier, a\nnovel semi-supervised version of the least squares classifier. This classifier\nminimizes the squared loss on the labeled data among the set of parameters\nimplied by all possible labelings of the unlabeled data. Unlike other\ndiscriminative semi-supervised methods, this approach does not introduce\nexplicit additional assumptions into the objective function, but leverages\nimplicit assumptions already present in the choice of the supervised least\nsquares classifier. This method can be formulated as a quadratic programming\nproblem and its solution can be found using a simple gradient descent\nprocedure. We prove that, in a limited 1-dimensional setting, this approach\nnever leads to performance worse than the supervised classifier. Experimental\nresults show that also in the general multidimensional case performance\nimprovements can be expected, both in terms of the squared loss that is\nintrinsic to the classifier, as well as in terms of the expected classification\nerror.\n", "versions": [{"version": "v1", "created": "Sun, 27 Dec 2015 16:44:06 GMT"}, {"version": "v2", "created": "Fri, 27 Jan 2017 20:42:38 GMT"}], "update_date": "2017-01-31", "authors_parsed": [["Krijthe", "Jesse H.", ""], ["Loog", "Marco", ""]]}, {"id": "1512.08279", "submitter": "Imme Ebert-Uphoff", "authors": "Imme Ebert-Uphoff and Yi Deng", "title": "Using Causal Discovery to Track Information Flow in Spatio-Temporal Data\n  - A Testbed and Experimental Results Using Advection-Diffusion Simulations", "comments": "40 pages, 19 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Causal discovery algorithms based on probabilistic graphical models have\nemerged in geoscience applications for the identification and visualization of\ndynamical processes. The key idea is to learn the structure of a graphical\nmodel from observed spatio-temporal data, which indicates information flow,\nthus pathways of interactions, in the observed physical system. Studying those\npathways allows geoscientists to learn subtle details about the underlying\ndynamical mechanisms governing our planet. Initial studies using this approach\non real-world atmospheric data have shown great potential for scientific\ndiscovery. However, in these initial studies no ground truth was available, so\nthat the resulting graphs have been evaluated only by whether a domain expert\nthinks they seemed physically plausible. This paper seeks to fill this gap. We\ndevelop a testbed that emulates two dynamical processes dominant in many\ngeoscience applications, namely advection and diffusion, in a 2D grid. Then we\napply the causal discovery based information tracking algorithms to the\nsimulation data to study how well the algorithms work for different scenarios\nand to gain a better understanding of the physical meaning of the graph\nresults, in particular of instantaneous connections. We make all data sets used\nin this study available to the community as a benchmark.\n  Keywords: Information flow, graphical model, structure learning, causal\ndiscovery, geoscience.\n", "versions": [{"version": "v1", "created": "Sun, 27 Dec 2015 21:42:06 GMT"}], "update_date": "2015-12-29", "authors_parsed": [["Ebert-Uphoff", "Imme", ""], ["Deng", "Yi", ""]]}, {"id": "1512.08422", "submitter": "Lili Mou", "authors": "Lili Mou, Rui Men, Ge Li, Yan Xu, Lu Zhang, Rui Yan, Zhi Jin", "title": "Natural Language Inference by Tree-Based Convolution and Heuristic\n  Matching", "comments": "Accepted by ACL'16 as a short paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose the TBCNN-pair model to recognize entailment and\ncontradiction between two sentences. In our model, a tree-based convolutional\nneural network (TBCNN) captures sentence-level semantics; then heuristic\nmatching layers like concatenation, element-wise product/difference combine the\ninformation in individual sentences. Experimental results show that our model\noutperforms existing sentence encoding-based approaches by a large margin.\n", "versions": [{"version": "v1", "created": "Mon, 28 Dec 2015 14:28:21 GMT"}, {"version": "v2", "created": "Fri, 22 Apr 2016 17:49:46 GMT"}, {"version": "v3", "created": "Fri, 13 May 2016 16:24:56 GMT"}], "update_date": "2016-05-16", "authors_parsed": [["Mou", "Lili", ""], ["Men", "Rui", ""], ["Li", "Ge", ""], ["Xu", "Yan", ""], ["Zhang", "Lu", ""], ["Yan", "Rui", ""], ["Jin", "Zhi", ""]]}, {"id": "1512.08425", "submitter": "Jiaming Xu", "authors": "Yudong Chen and Xiaodong Li and Jiaming Xu", "title": "Convexified Modularity Maximization for Degree-corrected Stochastic\n  Block Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.LG cs.SI stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The stochastic block model (SBM) is a popular framework for studying\ncommunity detection in networks. This model is limited by the assumption that\nall nodes in the same community are statistically equivalent and have equal\nexpected degrees. The degree-corrected stochastic block model (DCSBM) is a\nnatural extension of SBM that allows for degree heterogeneity within\ncommunities. This paper proposes a convexified modularity maximization approach\nfor estimating the hidden communities under DCSBM. Our approach is based on a\nconvex programming relaxation of the classical (generalized) modularity\nmaximization formulation, followed by a novel doubly-weighted $ \\ell_1 $-norm $\nk $-median procedure. We establish non-asymptotic theoretical guarantees for\nboth approximate clustering and perfect clustering. Our approximate clustering\nresults are insensitive to the minimum degree, and hold even in sparse regime\nwith bounded average degrees. In the special case of SBM, these theoretical\nresults match the best-known performance guarantees of computationally feasible\nalgorithms. Numerically, we provide an efficient implementation of our\nalgorithm, which is applied to both synthetic and real-world networks.\nExperiment results show that our method enjoys competitive performance compared\nto the state of the art in the literature.\n", "versions": [{"version": "v1", "created": "Mon, 28 Dec 2015 14:48:03 GMT"}, {"version": "v2", "created": "Tue, 19 Jan 2016 17:17:32 GMT"}], "update_date": "2016-01-20", "authors_parsed": [["Chen", "Yudong", ""], ["Li", "Xiaodong", ""], ["Xu", "Jiaming", ""]]}, {"id": "1512.08512", "submitter": "Andrew Owens", "authors": "Andrew Owens, Phillip Isola, Josh McDermott, Antonio Torralba, Edward\n  H. Adelson, William T. Freeman", "title": "Visually Indicated Sounds", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.SD", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Objects make distinctive sounds when they are hit or scratched. These sounds\nreveal aspects of an object's material properties, as well as the actions that\nproduced them. In this paper, we propose the task of predicting what sound an\nobject makes when struck as a way of studying physical interactions within a\nvisual scene. We present an algorithm that synthesizes sound from silent videos\nof people hitting and scratching objects with a drumstick. This algorithm uses\na recurrent neural network to predict sound features from videos and then\nproduces a waveform from these features with an example-based synthesis\nprocedure. We show that the sounds predicted by our model are realistic enough\nto fool participants in a \"real or fake\" psychophysical experiment, and that\nthey convey significant information about material properties and physical\ninteractions.\n", "versions": [{"version": "v1", "created": "Mon, 28 Dec 2015 20:56:50 GMT"}, {"version": "v2", "created": "Sat, 30 Apr 2016 03:03:04 GMT"}], "update_date": "2016-05-03", "authors_parsed": [["Owens", "Andrew", ""], ["Isola", "Phillip", ""], ["McDermott", "Josh", ""], ["Torralba", "Antonio", ""], ["Adelson", "Edward H.", ""], ["Freeman", "William T.", ""]]}, {"id": "1512.08562", "submitter": "Roy Fox", "authors": "Roy Fox, Ari Pakman, Naftali Tishby", "title": "Taming the Noise in Reinforcement Learning via Soft Updates", "comments": null, "journal-ref": "32nd Conference on Uncertainty in Artificial Intelligence (UAI\n  2016)", "doi": null, "report-no": null, "categories": "cs.LG cs.IT math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Model-free reinforcement learning algorithms, such as Q-learning, perform\npoorly in the early stages of learning in noisy environments, because much\neffort is spent unlearning biased estimates of the state-action value function.\nThe bias results from selecting, among several noisy estimates, the apparent\noptimum, which may actually be suboptimal. We propose G-learning, a new\noff-policy learning algorithm that regularizes the value estimates by\npenalizing deterministic policies in the beginning of the learning process. We\nshow that this method reduces the bias of the value-function estimation,\nleading to faster convergence to the optimal value and the optimal policy.\nMoreover, G-learning enables the natural incorporation of prior domain\nknowledge, when available. The stochastic nature of G-learning also makes it\navoid some exploration costs, a property usually attributed only to on-policy\nalgorithms. We illustrate these ideas in several examples, where G-learning\nresults in significant improvements of the convergence rate and the cost of the\nlearning process.\n", "versions": [{"version": "v1", "created": "Mon, 28 Dec 2015 23:59:12 GMT"}, {"version": "v2", "created": "Wed, 25 May 2016 20:33:03 GMT"}, {"version": "v3", "created": "Mon, 23 Jan 2017 18:21:49 GMT"}, {"version": "v4", "created": "Thu, 30 Mar 2017 05:00:30 GMT"}], "update_date": "2018-02-01", "authors_parsed": [["Fox", "Roy", ""], ["Pakman", "Ari", ""], ["Tishby", "Naftali", ""]]}, {"id": "1512.08571", "submitter": "Sajid Anwar", "authors": "Sajid Anwar, Kyuyeon Hwang and Wonyong Sung", "title": "Structured Pruning of Deep Convolutional Neural Networks", "comments": "11 pages, 8 figures, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Real time application of deep learning algorithms is often hindered by high\ncomputational complexity and frequent memory accesses. Network pruning is a\npromising technique to solve this problem. However, pruning usually results in\nirregular network connections that not only demand extra representation efforts\nbut also do not fit well on parallel computation. We introduce structured\nsparsity at various scales for convolutional neural networks, which are channel\nwise, kernel wise and intra kernel strided sparsity. This structured sparsity\nis very advantageous for direct computational resource savings on embedded\ncomputers, parallel computing environments and hardware based systems. To\ndecide the importance of network connections and paths, the proposed method\nuses a particle filtering approach. The importance weight of each particle is\nassigned by computing the misclassification rate with corresponding\nconnectivity pattern. The pruned network is re-trained to compensate for the\nlosses due to pruning. While implementing convolutions as matrix products, we\nparticularly show that intra kernel strided sparsity with a simple constraint\ncan significantly reduce the size of kernel and feature map matrices. The\npruned network is finally fixed point optimized with reduced word length\nprecision. This results in significant reduction in the total storage size\nproviding advantages for on-chip memory based implementations of deep neural\nnetworks.\n", "versions": [{"version": "v1", "created": "Tue, 29 Dec 2015 01:21:08 GMT"}], "update_date": "2015-12-31", "authors_parsed": [["Anwar", "Sajid", ""], ["Hwang", "Kyuyeon", ""], ["Sung", "Wonyong", ""]]}, {"id": "1512.08575", "submitter": "Roy Fox", "authors": "Roy Fox, Naftali Tishby", "title": "Optimal Selective Attention in Reactive Agents", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.IT math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In POMDPs, information about the hidden state, delivered through\nobservations, is both valuable to the agent, allowing it to base its actions on\nbetter informed internal states, and a \"curse\", exploding the size and\ndiversity of the internal state space. One attempt to deal with this is to\nfocus on reactive policies, that only base their actions on the most recent\nobservation. However, even reactive policies can be demanding on resources, and\nagents need to pay selective attention to only some of the information\navailable to them in observations. In this report we present the\nminimum-information principle for selective attention in reactive agents. We\nfurther motivate this approach by reducing the general problem of optimal\ncontrol in POMDPs, to reactive control with complex observations. Lastly, we\nexplore a newly discovered phenomenon of this optimization process - period\ndoubling bifurcations. This necessitates periodic policies, and raises many\nmore questions regarding stability, periodicity and chaos in optimal control.\n", "versions": [{"version": "v1", "created": "Tue, 29 Dec 2015 01:46:57 GMT"}], "update_date": "2015-12-31", "authors_parsed": [["Fox", "Roy", ""], ["Tishby", "Naftali", ""]]}, {"id": "1512.08580", "submitter": "Hongjian Wang", "authors": "Hongjian Wang, Zhenhui Li, Yu-Hsuan Kuo, Dan Kifer", "title": "A Simple Baseline for Travel Time Estimation using Large-Scale Trip Data", "comments": "12 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CY", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  The increased availability of large-scale trajectory data around the world\nprovides rich information for the study of urban dynamics. For example, New\nYork City Taxi Limousine Commission regularly releases source-destination\ninformation about trips in the taxis they regulate. Taxi data provide\ninformation about traffic patterns, and thus enable the study of urban flow --\nwhat will traffic between two locations look like at a certain date and time in\nthe future? Existing big data methods try to outdo each other in terms of\ncomplexity and algorithmic sophistication. In the spirit of \"big data beats\nalgorithms\", we present a very simple baseline which outperforms\nstate-of-the-art approaches, including Bing Maps and Baidu Maps (whose APIs\npermit large scale experimentation). Such a travel time estimation baseline has\nseveral important uses, such as navigation (fast travel time estimates can\nserve as approximate heuristics for A search variants for path finding) and\ntrip planning (which uses operating hours for popular destinations along with\ntravel time estimates to create an itinerary).\n", "versions": [{"version": "v1", "created": "Mon, 28 Dec 2015 20:31:38 GMT"}], "update_date": "2015-12-31", "authors_parsed": [["Wang", "Hongjian", ""], ["Li", "Zhenhui", ""], ["Kuo", "Yu-Hsuan", ""], ["Kifer", "Dan", ""]]}, {"id": "1512.08602", "submitter": "Adrian Vladu", "authors": "Vahab Mirrokni, Renato Paes Leme, Adrian Vladu, Sam Chiu-wai Wong", "title": "Tight Bounds for Approximate Carath\\'eodory and Beyond", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.LG math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We give a deterministic nearly-linear time algorithm for approximating any\npoint inside a convex polytope with a sparse convex combination of the\npolytope's vertices. Our result provides a constructive proof for the\nApproximate Carath\\'{e}odory Problem, which states that any point inside a\npolytope contained in the $\\ell_p$ ball of radius $D$ can be approximated to\nwithin $\\epsilon$ in $\\ell_p$ norm by a convex combination of only $O\\left(D^2\np/\\epsilon^2\\right)$ vertices of the polytope for $p \\geq 2$. We also show that\nthis bound is tight, using an argument based on anti-concentration for the\nbinomial distribution.\n  Along the way of establishing the upper bound, we develop a technique for\nminimizing norms over convex sets with complicated geometry; this is achieved\nby running Mirror Descent on a dual convex function obtained via Sion's\nTheorem.\n  As simple extensions of our method, we then provide new algorithms for\nsubmodular function minimization and SVM training. For submodular function\nminimization we obtain a simplification and (provable) speed-up over Wolfe's\nalgorithm, the method commonly found to be the fastest in practice. For SVM\ntraining, we obtain $O(1/\\epsilon^2)$ convergence for arbitrary kernels; each\niteration only requires matrix-vector operations involving the kernel matrix,\nso we overcome the obstacle of having to explicitly store the kernel or compute\nits Cholesky factorization.\n", "versions": [{"version": "v1", "created": "Tue, 29 Dec 2015 05:06:23 GMT"}], "update_date": "2015-12-31", "authors_parsed": [["Mirrokni", "Vahab", ""], ["Leme", "Renato Paes", ""], ["Vladu", "Adrian", ""], ["Wong", "Sam Chiu-wai", ""]]}, {"id": "1512.08756", "submitter": "Colin Raffel", "authors": "Colin Raffel and Daniel P. W. Ellis", "title": "Feed-Forward Networks with Attention Can Solve Some Long-Term Memory\n  Problems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We propose a simplified model of attention which is applicable to\nfeed-forward neural networks and demonstrate that the resulting model can solve\nthe synthetic \"addition\" and \"multiplication\" long-term memory problems for\nsequence lengths which are both longer and more widely varying than the best\npublished results for these tasks.\n", "versions": [{"version": "v1", "created": "Tue, 29 Dec 2015 19:03:43 GMT"}, {"version": "v2", "created": "Thu, 31 Dec 2015 17:52:46 GMT"}, {"version": "v3", "created": "Fri, 8 Jan 2016 01:52:06 GMT"}, {"version": "v4", "created": "Mon, 28 Mar 2016 02:41:21 GMT"}, {"version": "v5", "created": "Tue, 20 Sep 2016 05:02:22 GMT"}], "update_date": "2016-09-21", "authors_parsed": [["Raffel", "Colin", ""], ["Ellis", "Daniel P. W.", ""]]}, {"id": "1512.08787", "submitter": "Ravi Ganti", "authors": "Ravi Ganti, Laura Balzano, Rebecca Willett", "title": "Matrix Completion Under Monotonic Single Index Models", "comments": "21 pages, 5 figures, 1 table. Accepted for publication at NIPS 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most recent results in matrix completion assume that the matrix under\nconsideration is low-rank or that the columns are in a union of low-rank\nsubspaces. In real-world settings, however, the linear structure underlying\nthese models is distorted by a (typically unknown) nonlinear transformation.\nThis paper addresses the challenge of matrix completion in the face of such\nnonlinearities. Given a few observations of a matrix that are obtained by\napplying a Lipschitz, monotonic function to a low rank matrix, our task is to\nestimate the remaining unobserved entries. We propose a novel matrix completion\nmethod that alternates between low-rank matrix estimation and monotonic\nfunction estimation to estimate the missing matrix elements. Mean squared error\nbounds provide insight into how well the matrix can be estimated based on the\nsize, rank of the matrix and properties of the nonlinear transformation.\nEmpirical results on synthetic and real-world datasets demonstrate the\ncompetitiveness of the proposed approach.\n", "versions": [{"version": "v1", "created": "Tue, 29 Dec 2015 20:52:41 GMT"}], "update_date": "2015-12-31", "authors_parsed": [["Ganti", "Ravi", ""], ["Balzano", "Laura", ""], ["Willett", "Rebecca", ""]]}, {"id": "1512.08806", "submitter": "Uri Shaham", "authors": "Uri Shaham, Roy Lederman", "title": "Common Variable Learning and Invariant Representation Learning using\n  Siamese Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the statistical problem of learning common source of variability\nin data which are synchronously captured by multiple sensors, and demonstrate\nthat Siamese neural networks can be naturally applied to this problem. This\napproach is useful in particular in exploratory, data-driven applications,\nwhere neither a model nor label information is available. In recent years, many\nresearchers have successfully applied Siamese neural networks to obtain an\nembedding of data which corresponds to a \"semantic similarity\". We present an\ninterpretation of this \"semantic similarity\" as learning of equivalence\nclasses. We discuss properties of the embedding obtained by Siamese networks\nand provide empirical results that demonstrate the ability of Siamese networks\nto learn common variability.\n", "versions": [{"version": "v1", "created": "Tue, 29 Dec 2015 22:06:00 GMT"}, {"version": "v2", "created": "Tue, 26 Apr 2016 03:28:46 GMT"}, {"version": "v3", "created": "Wed, 11 May 2016 17:56:32 GMT"}], "update_date": "2016-05-12", "authors_parsed": [["Shaham", "Uri", ""], ["Lederman", "Roy", ""]]}, {"id": "1512.08808", "submitter": "Kerstin Bunte", "authors": "Kerstin Bunte, Eemeli Lepp\\\"aaho, Inka Saarinen, Samuel Kaski", "title": "Sparse group factor analysis for biclustering of multiple data sources", "comments": "7 pages, 5 figures, 1 table in Bioinformatics 2016", "journal-ref": "Bioinformatics Volume 32, Issue 16 Pp. 2457-2463, 2016", "doi": "10.1093/bioinformatics/btw207", "report-no": null, "categories": "cs.LG cs.IR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motivation: Modelling methods that find structure in data are necessary with\nthe current large volumes of genomic data, and there have been various efforts\nto find subsets of genes exhibiting consistent patterns over subsets of\ntreatments. These biclustering techniques have focused on one data source,\noften gene expression data. We present a Bayesian approach for joint\nbiclustering of multiple data sources, extending a recent method Group Factor\nAnalysis (GFA) to have a biclustering interpretation with additional sparsity\nassumptions. The resulting method enables data-driven detection of linear\nstructure present in parts of the data sources. Results: Our simulation studies\nshow that the proposed method reliably infers bi-clusters from heterogeneous\ndata sources. We tested the method on data from the NCI-DREAM drug sensitivity\nprediction challenge, resulting in an excellent prediction accuracy. Moreover,\nthe predictions are based on several biclusters which provide insight into the\ndata sources, in this case on gene expression, DNA methylation, protein\nabundance, exome sequence, functional connectivity fingerprints and drug\nsensitivity.\n", "versions": [{"version": "v1", "created": "Tue, 29 Dec 2015 22:07:35 GMT"}, {"version": "v2", "created": "Thu, 21 Apr 2016 10:23:53 GMT"}], "update_date": "2016-09-15", "authors_parsed": [["Bunte", "Kerstin", ""], ["Lepp\u00e4aho", "Eemeli", ""], ["Saarinen", "Inka", ""], ["Kaski", "Samuel", ""]]}, {"id": "1512.08836", "submitter": "Wen Sun", "authors": "Wen Sun, Arun Venkatraman, Byron Boots, J. Andrew Bagnell", "title": "Learning to Filter with Predictive State Inference Machines", "comments": "ICML 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Latent state space models are a fundamental and widely used tool for modeling\ndynamical systems. However, they are difficult to learn from data and learned\nmodels often lack performance guarantees on inference tasks such as filtering\nand prediction. In this work, we present the PREDICTIVE STATE INFERENCE MACHINE\n(PSIM), a data-driven method that considers the inference procedure on a\ndynamical system as a composition of predictors. The key idea is that rather\nthan first learning a latent state space model, and then using the learned\nmodel for inference, PSIM directly learns predictors for inference in\npredictive state space. We provide theoretical guarantees for inference, in\nboth realizable and agnostic settings, and showcase practical performance on a\nvariety of simulated and real world robotics benchmarks.\n", "versions": [{"version": "v1", "created": "Wed, 30 Dec 2015 03:17:00 GMT"}, {"version": "v2", "created": "Mon, 30 May 2016 17:20:32 GMT"}], "update_date": "2016-05-31", "authors_parsed": [["Sun", "Wen", ""], ["Venkatraman", "Arun", ""], ["Boots", "Byron", ""], ["Bagnell", "J. Andrew", ""]]}, {"id": "1512.08887", "submitter": "Farhad Pourkamali-Anaraki", "authors": "Farhad Pourkamali-Anaraki", "title": "Estimation of the sample covariance matrix from compressive measurements", "comments": "IET Signal Processing", "journal-ref": "IET Sig. Process. 10 (2016) 1089-1095", "doi": "10.1049/iet-spr.2016.0169", "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper focuses on the estimation of the sample covariance matrix from\nlow-dimensional random projections of data known as compressive measurements.\nIn particular, we present an unbiased estimator to extract the covariance\nstructure from compressive measurements obtained by a general class of random\nprojection matrices consisting of i.i.d. zero-mean entries and finite first\nfour moments. In contrast to previous works, we make no structural assumptions\nabout the underlying covariance matrix such as being low-rank. In fact, our\nanalysis is based on a non-Bayesian data setting which requires no\ndistributional assumptions on the set of data samples. Furthermore, inspired by\nthe generality of the projection matrices, we propose an approach to covariance\nestimation that utilizes sparse Rademacher matrices. Therefore, our algorithm\ncan be used to estimate the covariance matrix in applications with limited\nmemory and computation power at the acquisition devices. Experimental results\ndemonstrate that our approach allows for accurate estimation of the sample\ncovariance matrix on several real-world data sets, including video data.\n", "versions": [{"version": "v1", "created": "Wed, 30 Dec 2015 09:18:18 GMT"}, {"version": "v2", "created": "Fri, 19 Aug 2016 05:53:51 GMT"}, {"version": "v3", "created": "Fri, 28 Apr 2017 03:22:44 GMT"}], "update_date": "2017-05-01", "authors_parsed": [["Pourkamali-Anaraki", "Farhad", ""]]}, {"id": "1512.08903", "submitter": "Kyuyeon Hwang", "authors": "Kyuyeon Hwang, Minjae Lee, Wonyong Sung", "title": "Online Keyword Spotting with a Character-Level Recurrent Neural Network", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a context-aware keyword spotting model employing a\ncharacter-level recurrent neural network (RNN) for spoken term detection in\ncontinuous speech. The RNN is end-to-end trained with connectionist temporal\nclassification (CTC) to generate the probabilities of character and\nword-boundary labels. There is no need for the phonetic transcription, senone\nmodeling, or system dictionary in training and testing. Also, keywords can\neasily be added and modified by editing the text based keyword list without\nretraining the RNN. Moreover, the unidirectional RNN processes an infinitely\nlong input audio streams without pre-segmentation and keywords are detected\nwith low-latency before the utterance is finished. Experimental results show\nthat the proposed keyword spotter significantly outperforms the deep neural\nnetwork (DNN) and hidden Markov model (HMM) based keyword-filler model even\nwith less computations.\n", "versions": [{"version": "v1", "created": "Wed, 30 Dec 2015 10:32:12 GMT"}], "update_date": "2015-12-31", "authors_parsed": [["Hwang", "Kyuyeon", ""], ["Lee", "Minjae", ""], ["Sung", "Wonyong", ""]]}, {"id": "1512.08949", "submitter": "Nihar Shah", "authors": "Nihar B. Shah and Martin J. Wainwright", "title": "Simple, Robust and Optimal Ranking from Pairwise Comparisons", "comments": "Changes in version 2: In addition to recovery in the exact and\n  Hamming metrics, v2 analyzes a general, abstract recovery criterion based on\n  a notion of \"allowed sets\"", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.IT math.IT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider data in the form of pairwise comparisons of n items, with the\ngoal of precisely identifying the top k items for some value of k < n, or\nalternatively, recovering a ranking of all the items. We analyze the Copeland\ncounting algorithm that ranks the items in order of the number of pairwise\ncomparisons won, and show it has three attractive features: (a) its\ncomputational efficiency leads to speed-ups of several orders of magnitude in\ncomputation time as compared to prior work; (b) it is robust in that\ntheoretical guarantees impose no conditions on the underlying matrix of\npairwise-comparison probabilities, in contrast to some prior work that applies\nonly to the BTL parametric model; and (c) it is an optimal method up to\nconstant factors, meaning that it achieves the information-theoretic limits for\nrecovering the top k-subset. We extend our results to obtain sharp guarantees\nfor approximate recovery under the Hamming distortion metric, and more\ngenerally, to any arbitrary error requirement that satisfies a simple and\nnatural monotonicity condition.\n", "versions": [{"version": "v1", "created": "Wed, 30 Dec 2015 14:25:23 GMT"}, {"version": "v2", "created": "Wed, 27 Apr 2016 03:52:36 GMT"}], "update_date": "2016-04-28", "authors_parsed": [["Shah", "Nihar B.", ""], ["Wainwright", "Martin J.", ""]]}, {"id": "1512.09080", "submitter": "Emmanuel Abbe A", "authors": "Emmanuel Abbe and Colin Sandon", "title": "Detection in the stochastic block model with multiple clusters: proof of\n  the achievability conjectures, acyclic BP, and the information-computation\n  gap", "comments": "Extended version with further details on the algorithms and methods", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR cs.CC cs.IT cs.LG cs.SI math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In a paper that initiated the modern study of the stochastic block model,\nDecelle et al., backed by Mossel et al., made the following conjecture: Denote\nby $k$ the number of balanced communities, $a/n$ the probability of connecting\ninside communities and $b/n$ across, and set\n$\\mathrm{SNR}=(a-b)^2/(k(a+(k-1)b)$; for any $k \\geq 2$, it is possible to\ndetect communities efficiently whenever $\\mathrm{SNR}>1$ (the KS threshold),\nwhereas for $k\\geq 4$, it is possible to detect communities\ninformation-theoretically for some $\\mathrm{SNR}<1$. Massouli\\'e, Mossel et\nal.\\ and Bordenave et al.\\ succeeded in proving that the KS threshold is\nefficiently achievable for $k=2$, while Mossel et al.\\ proved that it cannot be\ncrossed information-theoretically for $k=2$. The above conjecture remained open\nfor $k \\geq 3$.\n  This paper proves this conjecture, further extending the efficient detection\nto non-symmetrical SBMs with a generalized notion of detection and KS\nthreshold. For the efficient part, a linearized acyclic belief propagation\n(ABP) algorithm is developed and proved to detect communities for any $k$ down\nto the KS threshold in time $O(n \\log n)$. Achieving this requires showing\noptimality of ABP in the presence of cycles, a challenge for message passing\nalgorithms. The paper further connects ABP to a power iteration method with a\nnonbacktracking operator of generalized order, formalizing the interplay\nbetween message passing and spectral methods. For the information-theoretic\n(IT) part, a non-efficient algorithm sampling a typical clustering is shown to\nbreak down the KS threshold at $k=4$. The emerging gap is shown to be large in\nsome cases; if $a=0$, the KS threshold reads $b \\gtrsim k^2$ whereas the IT\nbound reads $b \\gtrsim k \\ln(k)$, making the SBM a good study-case for\ninformation-computation gaps.\n", "versions": [{"version": "v1", "created": "Wed, 30 Dec 2015 19:49:28 GMT"}, {"version": "v2", "created": "Mon, 15 Feb 2016 20:34:42 GMT"}, {"version": "v3", "created": "Tue, 14 Jun 2016 15:16:39 GMT"}, {"version": "v4", "created": "Thu, 15 Sep 2016 02:04:27 GMT"}], "update_date": "2016-09-16", "authors_parsed": [["Abbe", "Emmanuel", ""], ["Sandon", "Colin", ""]]}, {"id": "1512.09156", "submitter": "Shashanka Ubaru", "authors": "Shashanka Ubaru, Arya Mazumdar and Yousef Saad", "title": "Low rank approximation and decomposition of large matrices using error\n  correcting codes", "comments": null, "journal-ref": "IEEE Transactions on Information Theory ( Volume: 63, Issue: 9,\n  Sept. 2017 ) Page(s): 5544 - 5558", "doi": "10.1109/TIT.2017.2723898", "report-no": null, "categories": "cs.IT cs.LG cs.NA math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Low rank approximation is an important tool used in many applications of\nsignal processing and machine learning. Recently, randomized sketching\nalgorithms were proposed to effectively construct low rank approximations and\nobtain approximate singular value decompositions of large matrices. Similar\nideas were used to solve least squares regression problems. In this paper, we\nshow how matrices from error correcting codes can be used to find such low rank\napproximations and matrix decompositions, and extend the framework to linear\nleast squares regression problems. The benefits of using these code matrices\nare the following: (i) They are easy to generate and they reduce randomness\nsignificantly. (ii) Code matrices with mild properties satisfy the subspace\nembedding property, and have a better chance of preserving the geometry of an\nentire subspace of vectors. (iii) For parallel and distributed applications,\ncode matrices have significant advantages over structured random matrices and\nGaussian random matrices. (iv) Unlike Fourier or Hadamard transform matrices,\nwhich require sampling $O(k\\log k)$ columns for a rank-$k$ approximation, the\nlog factor is not necessary for certain types of code matrices. That is,\n$(1+\\epsilon)$ optimal Frobenius norm error can be achieved for a rank-$k$\napproximation with $O(k/\\epsilon)$ samples. (v) Fast multiplication is possible\nwith structured code matrices, so fast approximations can be achieved for\ngeneral dense input matrices. (vi) For least squares regression problem\n$\\min\\|Ax-b\\|_2$ where $A\\in \\mathbb{R}^{n\\times d}$, the $(1+\\epsilon)$\nrelative error approximation can be achieved with $O(d/\\epsilon)$ samples, with\nhigh probability, when certain code matrices are used.\n", "versions": [{"version": "v1", "created": "Wed, 30 Dec 2015 21:29:57 GMT"}, {"version": "v2", "created": "Fri, 19 Aug 2016 18:14:33 GMT"}, {"version": "v3", "created": "Thu, 15 Jun 2017 22:54:25 GMT"}], "update_date": "2018-09-11", "authors_parsed": [["Ubaru", "Shashanka", ""], ["Mazumdar", "Arya", ""], ["Saad", "Yousef", ""]]}, {"id": "1512.09170", "submitter": "Vitaly Feldman", "authors": "Vitaly Feldman, Cristobal Guzman, Santosh Vempala", "title": "Statistical Query Algorithms for Mean Vector Estimation and Stochastic\n  Convex Optimization", "comments": "Substantial revision. To appear in SODA 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Stochastic convex optimization, where the objective is the expectation of a\nrandom convex function, is an important and widely used method with numerous\napplications in machine learning, statistics, operations research and other\nareas. We study the complexity of stochastic convex optimization given only\nstatistical query (SQ) access to the objective function. We show that\nwell-known and popular first-order iterative methods can be implemented using\nonly statistical queries. For many cases of interest we derive nearly matching\nupper and lower bounds on the estimation (sample) complexity including linear\noptimization in the most general setting. We then present several consequences\nfor machine learning, differential privacy and proving concrete lower bounds on\nthe power of convex optimization based methods.\n  The key ingredient of our work is SQ algorithms and lower bounds for\nestimating the mean vector of a distribution over vectors supported on a convex\nbody in $\\mathbb{R}^d$. This natural problem has not been previously studied\nand we show that our solutions can be used to get substantially improved SQ\nversions of Perceptron and other online algorithms for learning halfspaces.\n", "versions": [{"version": "v1", "created": "Wed, 30 Dec 2015 22:29:23 GMT"}, {"version": "v2", "created": "Tue, 22 Nov 2016 01:49:05 GMT"}], "update_date": "2016-11-23", "authors_parsed": [["Feldman", "Vitaly", ""], ["Guzman", "Cristobal", ""], ["Vempala", "Santosh", ""]]}, {"id": "1512.09176", "submitter": "Jie Xu", "authors": "Jie Xu, Tianwei Xing, Mihaela van der Schaar", "title": "Personalized Course Sequence Recommendations", "comments": null, "journal-ref": null, "doi": "10.1109/TSP.2016.2595495", "report-no": null, "categories": "cs.CY cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given the variability in student learning it is becoming increasingly\nimportant to tailor courses as well as course sequences to student needs. This\npaper presents a systematic methodology for offering personalized course\nsequence recommendations to students. First, a forward-search\nbackward-induction algorithm is developed that can optimally select course\nsequences to decrease the time required for a student to graduate. The\nalgorithm accounts for prerequisite requirements (typically present in higher\nlevel education) and course availability. Second, using the tools of\nmulti-armed bandits, an algorithm is developed that can optimally recommend a\ncourse sequence that both reduces the time to graduate while also increasing\nthe overall GPA of the student. The algorithm dynamically learns how students\nwith different contextual backgrounds perform for given course sequences and\nthen recommends an optimal course sequence for new students. Using real-world\nstudent data from the UCLA Mechanical and Aerospace Engineering department, we\nillustrate how the proposed algorithms outperform other methods that do not\ninclude student contextual information when making course sequence\nrecommendations.\n", "versions": [{"version": "v1", "created": "Wed, 30 Dec 2015 22:57:13 GMT"}, {"version": "v2", "created": "Tue, 12 Jan 2016 01:37:55 GMT"}], "update_date": "2016-09-21", "authors_parsed": [["Xu", "Jie", ""], ["Xing", "Tianwei", ""], ["van der Schaar", "Mihaela", ""]]}, {"id": "1512.09204", "submitter": "Weici Hu", "authors": "Weici Hu, Peter I. Frazier", "title": "Bayes-Optimal Effort Allocation in Crowdsourcing: Bounds and Index\n  Policies", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider effort allocation in crowdsourcing, where we wish to assign\nlabeling tasks to imperfect homogeneous crowd workers to maximize overall\naccuracy in a continuous-time Bayesian setting, subject to budget and time\nconstraints. The Bayes-optimal policy for this problem is the solution to a\npartially observable Markov decision process, but the curse of dimensionality\nrenders the computation infeasible. Based on the Lagrangian Relaxation\ntechnique in Adelman & Mersereau (2008), we provide a computationally tractable\ninstance-specific upper bound on the value of this Bayes-optimal policy, which\ncan in turn be used to bound the optimality gap of any other sub-optimal\npolicy. In an approach similar in spirit to the Whittle index for restless\nmultiarmed bandits, we provide an index policy for effort allocation in\ncrowdsourcing and demonstrate numerically that it outperforms other stateof-\narts and performs close to optimal solution.\n", "versions": [{"version": "v1", "created": "Thu, 31 Dec 2015 03:09:33 GMT"}], "update_date": "2016-01-01", "authors_parsed": [["Hu", "Weici", ""], ["Frazier", "Peter I.", ""]]}, {"id": "1512.09227", "submitter": "Zemin Zhang", "authors": "Zemin Zhang and Shuchin Aeron", "title": "Denoising and Completion of 3D Data via Multidimensional Dictionary\n  Learning", "comments": "9 pages, submitted to Conference on Computer Vision and Pattern\n  Recognition (CVPR) 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper a new dictionary learning algorithm for multidimensional data\nis proposed. Unlike most conventional dictionary learning methods which are\nderived for dealing with vectors or matrices, our algorithm, named KTSVD,\nlearns a multidimensional dictionary directly via a novel algebraic approach\nfor tensor factorization as proposed in [3, 12, 13]. Using this approach one\ncan define a tensor-SVD and we propose to extend K-SVD algorithm used for 1-D\ndata to a K-TSVD algorithm for handling 2-D and 3-D data. Our algorithm, based\non the idea of sparse coding (using group-sparsity over multidimensional\ncoefficient vectors), alternates between estimating a compact representation\nand dictionary learning. We analyze our KTSVD algorithm and demonstrate its\nresult on video completion and multispectral image denoising.\n", "versions": [{"version": "v1", "created": "Thu, 31 Dec 2015 06:37:54 GMT"}], "update_date": "2016-01-01", "authors_parsed": [["Zhang", "Zemin", ""], ["Aeron", "Shuchin", ""]]}, {"id": "1512.09295", "submitter": "Qirong Ho", "authors": "Eric P. Xing, Qirong Ho, Pengtao Xie, Wei Dai", "title": "Strategies and Principles of Distributed Machine Learning on Big Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.DC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The rise of Big Data has led to new demands for Machine Learning (ML) systems\nto learn complex models with millions to billions of parameters, that promise\nadequate capacity to digest massive datasets and offer powerful predictive\nanalytics thereupon. In order to run ML algorithms at such scales, on a\ndistributed cluster with 10s to 1000s of machines, it is often the case that\nsignificant engineering efforts are required --- and one might fairly ask if\nsuch engineering truly falls within the domain of ML research or not. Taking\nthe view that Big ML systems can benefit greatly from ML-rooted statistical and\nalgorithmic insights --- and that ML researchers should therefore not shy away\nfrom such systems design --- we discuss a series of principles and strategies\ndistilled from our recent efforts on industrial-scale ML solutions. These\nprinciples and strategies span a continuum from application, to engineering,\nand to theoretical research and development of Big ML systems and\narchitectures, with the goal of understanding how to make them efficient,\ngenerally-applicable, and supported with convergence and scaling guarantees.\nThey concern four key questions which traditionally receive little attention in\nML research: How to distribute an ML program over a cluster? How to bridge ML\ncomputation with inter-machine communication? How to perform such\ncommunication? What should be communicated between machines? By exposing\nunderlying statistical and algorithmic characteristics unique to ML programs\nbut not typically seen in traditional computer programs, and by dissecting\nsuccessful cases to reveal how we have harnessed these principles to design and\ndevelop both high-performance distributed ML software as well as\ngeneral-purpose ML frameworks, we present opportunities for ML researchers and\npractitioners to further shape and grow the area that lies between ML and\nsystems.\n", "versions": [{"version": "v1", "created": "Thu, 31 Dec 2015 14:33:53 GMT"}], "update_date": "2016-01-01", "authors_parsed": [["Xing", "Eric P.", ""], ["Ho", "Qirong", ""], ["Xie", "Pengtao", ""], ["Dai", "Wei", ""]]}, {"id": "1512.09300", "submitter": "Anders Boesen Lindbo Larsen", "authors": "Anders Boesen Lindbo Larsen, S{\\o}ren Kaae S{\\o}nderby, Hugo\n  Larochelle, Ole Winther", "title": "Autoencoding beyond pixels using a learned similarity metric", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an autoencoder that leverages learned representations to better\nmeasure similarities in data space. By combining a variational autoencoder with\na generative adversarial network we can use learned feature representations in\nthe GAN discriminator as basis for the VAE reconstruction objective. Thereby,\nwe replace element-wise errors with feature-wise errors to better capture the\ndata distribution while offering invariance towards e.g. translation. We apply\nour method to images of faces and show that it outperforms VAEs with\nelement-wise similarity measures in terms of visual fidelity. Moreover, we show\nthat the method learns an embedding in which high-level abstract visual\nfeatures (e.g. wearing glasses) can be modified using simple arithmetic.\n", "versions": [{"version": "v1", "created": "Thu, 31 Dec 2015 14:53:39 GMT"}, {"version": "v2", "created": "Wed, 10 Feb 2016 21:18:27 GMT"}], "update_date": "2016-02-12", "authors_parsed": [["Larsen", "Anders Boesen Lindbo", ""], ["S\u00f8nderby", "S\u00f8ren Kaae", ""], ["Larochelle", "Hugo", ""], ["Winther", "Ole", ""]]}, {"id": "1512.09327", "submitter": "Thibaut Lienart", "authors": "Leonard Hasenclever, Stefan Webb, Thibaut Lienart, Sebastian Vollmer,\n  Balaji Lakshminarayanan, Charles Blundell, Yee Whye Teh", "title": "Distributed Bayesian Learning with Stochastic Natural-gradient\n  Expectation Propagation and the Posterior Server", "comments": "37 pages, 7 figures", "journal-ref": "Journal of Machine Learning Research 18 (2017) 1-37", "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper makes two contributions to Bayesian machine learning algorithms.\nFirstly, we propose stochastic natural gradient expectation propagation (SNEP),\na novel alternative to expectation propagation (EP), a popular variational\ninference algorithm. SNEP is a black box variational algorithm, in that it does\nnot require any simplifying assumptions on the distribution of interest, beyond\nthe existence of some Monte Carlo sampler for estimating the moments of the EP\ntilted distributions. Further, as opposed to EP which has no guarantee of\nconvergence, SNEP can be shown to be convergent, even when using Monte Carlo\nmoment estimates. Secondly, we propose a novel architecture for distributed\nBayesian learning which we call the posterior server. The posterior server\nallows scalable and robust Bayesian learning in cases where a data set is\nstored in a distributed manner across a cluster, with each compute node\ncontaining a disjoint subset of data. An independent Monte Carlo sampler is run\non each compute node, with direct access only to the local data subset, but\nwhich targets an approximation to the global posterior distribution given all\ndata across the whole cluster. This is achieved by using a distributed\nasynchronous implementation of SNEP to pass messages across the cluster. We\ndemonstrate SNEP and the posterior server on distributed Bayesian learning of\nlogistic regression and neural networks.\n  Keywords: Distributed Learning, Large Scale Learning, Deep Learning, Bayesian\nLearn- ing, Variational Inference, Expectation Propagation, Stochastic\nApproximation, Natural Gradient, Markov chain Monte Carlo, Parameter Server,\nPosterior Server.\n", "versions": [{"version": "v1", "created": "Thu, 31 Dec 2015 17:30:45 GMT"}, {"version": "v2", "created": "Thu, 1 Sep 2016 16:02:19 GMT"}, {"version": "v3", "created": "Thu, 27 Oct 2016 12:45:58 GMT"}, {"version": "v4", "created": "Thu, 7 Sep 2017 18:36:51 GMT"}], "update_date": "2017-09-11", "authors_parsed": [["Hasenclever", "Leonard", ""], ["Webb", "Stefan", ""], ["Lienart", "Thibaut", ""], ["Vollmer", "Sebastian", ""], ["Lakshminarayanan", "Balaji", ""], ["Blundell", "Charles", ""], ["Teh", "Yee Whye", ""]]}, {"id": "1512.09328", "submitter": "Raouf Dridi Dr", "authors": "Raouf Dridi, Hedayat Alghassi", "title": "Homology Computation of Large Point Clouds using Quantum Annealing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "quant-ph cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Homology is a tool in topological data analysis which measures the shape of\nthe data. In many cases, these measurements translate into new insights which\nare not available by other means. To compute homology, we rely on mathematical\nconstructions which scale exponentially with the size of the data. Therefore,\nfor large point clouds, the computation is infeasible using classical\ncomputers. In this paper, we present a quantum annealing pipeline for\ncomputation of homology of large point clouds. The pipeline takes as input a\ngraph approximating the given point cloud. It uses quantum annealing to compute\na clique covering of the graph and then uses this cover to construct a\nMayer-Vietoris complex. The pipeline terminates by performing a simplified\nhomology computation of the Mayer-Vietoris complex. We have introduced three\ndifferent clique coverings and their quantum annealing formulation. Our\npipeline scales polynomially in the size of the data, once the covering step is\nsolved. To prove correctness of our algorithm, we have also included tests\nusing D-Wave 2X quantum processor.\n", "versions": [{"version": "v1", "created": "Wed, 23 Dec 2015 22:50:09 GMT"}, {"version": "v2", "created": "Tue, 22 Mar 2016 18:22:24 GMT"}, {"version": "v3", "created": "Mon, 6 Jun 2016 17:06:41 GMT"}], "update_date": "2016-06-07", "authors_parsed": [["Dridi", "Raouf", ""], ["Alghassi", "Hedayat", ""]]}]