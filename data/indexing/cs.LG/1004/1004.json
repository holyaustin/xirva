[{"id": "1004.0378", "submitter": "Mahmoud Khademi", "authors": "Mahmoud Khademi, Mohammad H. Kiapour, Mehran Safayani, Mohammad T.\n  Manzuri, and M. Shojaei", "title": "Facial Expression Representation and Recognition Using 2DHLDA, Gabor\n  Wavelets, and Ensemble Learning", "comments": "This paper has been withdrawn by the author due to an error in\n  experimental results", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, a novel method for representation and recognition of the\nfacial expressions in two-dimensional image sequences is presented. We apply a\nvariation of two-dimensional heteroscedastic linear discriminant analysis\n(2DHLDA) algorithm, as an efficient dimensionality reduction technique, to\nGabor representation of the input sequence. 2DHLDA is an extension of the\ntwo-dimensional linear discriminant analysis (2DLDA) approach and it removes\nthe equal within-class covariance. By applying 2DHLDA in two directions, we\neliminate the correlations between both image columns and image rows. Then, we\nperform a one-dimensional LDA on the new features. This combined method can\nalleviate the small sample size problem and instability encountered by HLDA.\nAlso, employing both geometric and appearance features and using an ensemble\nlearning scheme based on data fusion, we create a classifier which can\nefficiently classify the facial expressions. The proposed method is robust to\nillumination changes and it can properly represent temporal information as well\nas subtle changes in facial muscles. We provide experiments on Cohn-Kanade\ndatabase that show the superiority of the proposed method. KEYWORDS:\ntwo-dimensional heteroscedastic linear discriminant analysis (2DHLDA), subspace\nlearning, facial expression analysis, Gabor wavelets, ensemble learning.\n", "versions": [{"version": "v1", "created": "Fri, 2 Apr 2010 19:26:47 GMT"}, {"version": "v2", "created": "Sat, 10 Apr 2010 10:57:58 GMT"}, {"version": "v3", "created": "Mon, 21 Jun 2010 15:37:54 GMT"}, {"version": "v4", "created": "Wed, 20 Oct 2010 14:21:14 GMT"}, {"version": "v5", "created": "Tue, 9 Nov 2010 18:35:29 GMT"}, {"version": "v6", "created": "Tue, 8 Mar 2011 20:52:07 GMT"}, {"version": "v7", "created": "Fri, 20 Jul 2012 01:21:59 GMT"}], "update_date": "2012-07-23", "authors_parsed": [["Khademi", "Mahmoud", ""], ["Kiapour", "Mohammad H.", ""], ["Safayani", "Mehran", ""], ["Manzuri", "Mohammad T.", ""], ["Shojaei", "M.", ""]]}, {"id": "1004.0456", "submitter": "Fabrice Rossi", "authors": "Georges H\\'ebrail and Bernard Hugueney and Yves Lechevallier and\n  Fabrice Rossi", "title": "Exploratory Analysis of Functional Data via Clustering and Optimal\n  Segmentation", "comments": null, "journal-ref": "Neurocomputing, Volume 73, Issues 7-9, March 2010, Pages 1125-1141", "doi": "10.1016/j.neucom.2009.11.022", "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose in this paper an exploratory analysis algorithm for functional\ndata. The method partitions a set of functions into $K$ clusters and represents\neach cluster by a simple prototype (e.g., piecewise constant). The total number\nof segments in the prototypes, $P$, is chosen by the user and optimally\ndistributed among the clusters via two dynamic programming algorithms. The\npractical relevance of the method is shown on two real world datasets.\n", "versions": [{"version": "v1", "created": "Sat, 3 Apr 2010 16:28:47 GMT"}], "update_date": "2010-04-06", "authors_parsed": [["H\u00e9brail", "Georges", ""], ["Hugueney", "Bernard", ""], ["Lechevallier", "Yves", ""], ["Rossi", "Fabrice", ""]]}, {"id": "1004.0515", "submitter": "Mahmoud Khademi", "authors": "Mahmoud Khademi, Mohammad T. Manzuri-Shalmani, Mohammad H. Kiapour,\n  and Ali A. Kiaei", "title": "Recognizing Combinations of Facial Action Units with Different Intensity\n  Using a Mixture of Hidden Markov Models and Neural Network", "comments": null, "journal-ref": "LNCS vol. 5997, pp. 304--313, Springer, Heidelberg (Proc. of 9th\n  IAPR Workshop on Multiple Classifier Systems), 2010.", "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Facial Action Coding System consists of 44 action units (AUs) and more than\n7000 combinations. Hidden Markov models (HMMs) classifier has been used\nsuccessfully to recognize facial action units (AUs) and expressions due to its\nability to deal with AU dynamics. However, a separate HMM is necessary for each\nsingle AU and each AU combination. Since combinations of AU numbering in\nthousands, a more efficient method will be needed. In this paper an accurate\nreal-time sequence-based system for representation and recognition of facial\nAUs is presented. Our system has the following characteristics: 1) employing a\nmixture of HMMs and neural network, we develop a novel accurate classifier,\nwhich can deal with AU dynamics, recognize subtle changes, and it is also\nrobust to intensity variations, 2) although we use an HMM for each single AU\nonly, by employing a neural network we can recognize each single and\ncombination AU, and 3) using both geometric and appearance-based features, and\napplying efficient dimension reduction techniques, our system is robust to\nillumination changes and it can represent the temporal information involved in\nformation of the facial expressions. Extensive experiments on Cohn-Kanade\ndatabase show the superiority of the proposed method, in comparison with other\nclassifiers. Keywords: classifier design and evaluation, data fusion, facial\naction units (AUs), hidden Markov models (HMMs), neural network (NN).\n", "versions": [{"version": "v1", "created": "Sun, 4 Apr 2010 16:23:53 GMT"}], "update_date": "2010-04-06", "authors_parsed": [["Khademi", "Mahmoud", ""], ["Manzuri-Shalmani", "Mohammad T.", ""], ["Kiapour", "Mohammad H.", ""], ["Kiaei", "Ali A.", ""]]}, {"id": "1004.0517", "submitter": "Mahmoud Khademi", "authors": "Mahmoud Khademi, Mehran Safayani, and Mohammad T. Manzuri-Shalmani", "title": "Multilinear Biased Discriminant Analysis: A Novel Method for Facial\n  Action Unit Representation", "comments": "Proc. of 16th Korea-Japan Joint Workshop on Frontiers of Computer\n  Vision, Hiroshima, Japan, 2010.", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper a novel efficient method for representation of facial action\nunits by encoding an image sequence as a fourth-order tensor is presented. The\nmultilinear tensor-based extension of the biased discriminant analysis (BDA)\nalgorithm, called multilinear biased discriminant analysis (MBDA), is first\nproposed. Then, we apply the MBDA and two-dimensional BDA (2DBDA) algorithms,\nas the dimensionality reduction techniques, to Gabor representations and the\ngeometric features of the input image sequence respectively. The proposed\nscheme can deal with the asymmetry between positive and negative samples as\nwell as curse of dimensionality dilemma. Extensive experiments on Cohn-Kanade\ndatabase show the superiority of the proposed method for representation of the\nsubtle changes and the temporal information involved in formation of the facial\nexpressions. As an accurate tool, this representation can be applied to many\nareas such as recognition of spontaneous and deliberate facial expressions,\nmulti modal/media human computer interaction and lie detection efforts.\n", "versions": [{"version": "v1", "created": "Sun, 4 Apr 2010 16:40:39 GMT"}], "update_date": "2010-04-06", "authors_parsed": [["Khademi", "Mahmoud", ""], ["Safayani", "Mehran", ""], ["Manzuri-Shalmani", "Mohammad T.", ""]]}, {"id": "1004.0567", "submitter": "Secretary Aircc Journal", "authors": "Rung-Ching Chen, Kai-Fan Cheng and Chia-Fen Hsieh (Chaoyang University\n  of Technology, Taiwan)", "title": "Using Rough Set and Support Vector Machine for Network Intrusion\n  Detection", "comments": "13 Pages", "journal-ref": "International Journal of Network Security & Its Applications 1.1\n  (2009) 1-13", "doi": null, "report-no": null, "categories": "cs.LG cs.CR cs.NI", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  The main function of IDS (Intrusion Detection System) is to protect the\nsystem, analyze and predict the behaviors of users. Then these behaviors will\nbe considered an attack or a normal behavior. Though IDS has been developed for\nmany years, the large number of return alert messages makes managers maintain\nsystem inefficiently. In this paper, we use RST (Rough Set Theory) and SVM\n(Support Vector Machine) to detect intrusions. First, RST is used to preprocess\nthe data and reduce the dimensions. Next, the features were selected by RST\nwill be sent to SVM model to learn and test respectively. The method is\neffective to decrease the space density of data. The experiments will compare\nthe results with different methods and show RST and SVM schema could improve\nthe false positive rate and accuracy.\n", "versions": [{"version": "v1", "created": "Mon, 5 Apr 2010 06:12:47 GMT"}], "update_date": "2010-07-15", "authors_parsed": [["Chen", "Rung-Ching", "", "Chaoyang University\n  of Technology, Taiwan"], ["Cheng", "Kai-Fan", "", "Chaoyang University\n  of Technology, Taiwan"], ["Hsieh", "Chia-Fen", "", "Chaoyang University\n  of Technology, Taiwan"]]}, {"id": "1004.0755", "submitter": "Mahmoud Khademi", "authors": "Mehran Safayani, Mohammad T. Manzuri-Shalmani, Mahmoud Khademi", "title": "Extended Two-Dimensional PCA for Efficient Face Representation and\n  Recognition", "comments": "Proc. of 4th International Conference on Intelligent Computer\n  Communication and Processing (ICCP), Cluj-Napoca, Romania, pp. 295--298,\n  2008.", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper a novel method called Extended Two-Dimensional PCA (E2DPCA) is\nproposed which is an extension to the original 2DPCA. We state that the\ncovariance matrix of 2DPCA is equivalent to the average of the main diagonal of\nthe covariance matrix of PCA. This implies that 2DPCA eliminates some\ncovariance information that can be useful for recognition. E2DPCA instead of\njust using the main diagonal considers a radius of r diagonals around it and\nexpands the averaging so as to include the covariance information within those\ndiagonals. The parameter r unifies PCA and 2DPCA. r = 1 produces the covariance\nof 2DPCA, r = n that of PCA. Hence, by controlling r it is possible to control\nthe trade-offs between recognition accuracy and energy compression (fewer\ncoefficients), and between training and recognition complexity. Experiments on\nORL face database show improvement in both recognition accuracy and recognition\ntime over the original 2DPCA.\n", "versions": [{"version": "v1", "created": "Tue, 6 Apr 2010 02:27:58 GMT"}], "update_date": "2010-04-07", "authors_parsed": [["Safayani", "Mehran", ""], ["Manzuri-Shalmani", "Mohammad T.", ""], ["Khademi", "Mahmoud", ""]]}, {"id": "1004.1003", "submitter": "Byung-Hak Kim", "authors": "Byung-Hak Kim, Arvind Yedla, and Henry D. Pfister", "title": "Message-Passing Inference on a Factor Graph for Collaborative Filtering", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.LG math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces a novel message-passing (MP) framework for the\ncollaborative filtering (CF) problem associated with recommender systems. We\nmodel the movie-rating prediction problem popularized by the Netflix Prize,\nusing a probabilistic factor graph model and study the model by deriving\ngeneralization error bounds in terms of the training error. Based on the model,\nwe develop a new MP algorithm, termed IMP, for learning the model. To show\nsuperiority of the IMP algorithm, we compare it with the closely related\nexpectation-maximization (EM) based algorithm and a number of other matrix\ncompletion algorithms. Our simulation results on Netflix data show that, while\nthe methods perform similarly with large amounts of data, the IMP algorithm is\nsuperior for small amounts of data. This improves the cold-start problem of the\nCF systems in practice. Another advantage of the IMP algorithm is that it can\nbe analyzed using the technique of density evolution (DE) that was originally\ndeveloped for MP decoding of error-correcting codes.\n", "versions": [{"version": "v1", "created": "Wed, 7 Apr 2010 05:25:48 GMT"}], "update_date": "2010-04-08", "authors_parsed": [["Kim", "Byung-Hak", ""], ["Yedla", "Arvind", ""], ["Pfister", "Henry D.", ""]]}, {"id": "1004.1061", "submitter": "Yuexian Hou", "authors": "Yuexian Hou, Tingxu Yan, Peng Zhang, Dawei Song, Wenjie Li", "title": "On Tsallis Entropy Bias and Generalized Maximum Entropy Models", "comments": "29 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cond-mat.stat-mech cs.AI cs.IT math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In density estimation task, maximum entropy model (Maxent) can effectively\nuse reliable prior information via certain constraints, i.e., linear\nconstraints without empirical parameters. However, reliable prior information\nis often insufficient, and the selection of uncertain constraints becomes\nnecessary but poses considerable implementation complexity. Improper setting of\nuncertain constraints can result in overfitting or underfitting. To solve this\nproblem, a generalization of Maxent, under Tsallis entropy framework, is\nproposed. The proposed method introduces a convex quadratic constraint for the\ncorrection of (expected) Tsallis entropy bias (TEB). Specifically, we\ndemonstrate that the expected Tsallis entropy of sampling distributions is\nsmaller than the Tsallis entropy of the underlying real distribution. This\nexpected entropy reduction is exactly the (expected) TEB, which can be\nexpressed by a closed-form formula and act as a consistent and unbiased\ncorrection. TEB indicates that the entropy of a specific sampling distribution\nshould be increased accordingly. This entails a quantitative re-interpretation\nof the Maxent principle. By compensating TEB and meanwhile forcing the\nresulting distribution to be close to the sampling distribution, our\ngeneralized TEBC Maxent can be expected to alleviate the overfitting and\nunderfitting. We also present a connection between TEB and Lidstone estimator.\nAs a result, TEB-Lidstone estimator is developed by analytically identifying\nthe rate of probability correction in Lidstone. Extensive empirical evaluation\nshows promising performance of both TEBC Maxent and TEB-Lidstone in comparison\nwith various state-of-the-art density estimation methods.\n", "versions": [{"version": "v1", "created": "Wed, 7 Apr 2010 11:52:25 GMT"}], "update_date": "2010-04-08", "authors_parsed": [["Hou", "Yuexian", ""], ["Yan", "Tingxu", ""], ["Zhang", "Peng", ""], ["Song", "Dawei", ""], ["Li", "Wenjie", ""]]}, {"id": "1004.1230", "submitter": "Rdv Ijcsis", "authors": "Phanu Waraporn, Phayung Meesad, Gareth Clayton", "title": "Ontology-supported processing of clinical text using medical knowledge\n  integration for multi-label classification of diagnosis coding", "comments": "IEEE Publication format, ISSN 1947 5500,\n  http://sites.google.com/site/ijcsis/", "journal-ref": "IJCSIS, Vol. 7 No. 3, March 2010,", "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  This paper discusses the knowledge integration of clinical information\nextracted from distributed medical ontology in order to ameliorate a machine\nlearning-based multi-label coding assignment system. The proposed approach is\nimplemented using a decision tree based cascade hierarchical technique on the\nuniversity hospital data for patients with Coronary Heart Disease (CHD). The\npreliminary results obtained show a satisfactory finding.\n", "versions": [{"version": "v1", "created": "Thu, 8 Apr 2010 03:06:24 GMT"}], "update_date": "2010-04-09", "authors_parsed": [["Waraporn", "Phanu", ""], ["Meesad", "Phayung", ""], ["Clayton", "Gareth", ""]]}, {"id": "1004.1743", "submitter": "Rdv Ijcsis", "authors": "G. Nathiya, S. C. Punitha, M. Punithavalli", "title": "An Analytical Study on Behavior of Clusters Using K Means, EM and K*\n  Means Algorithm", "comments": "IEEE Publication format, ISSN 1947 5500,\n  http://sites.google.com/site/ijcsis/", "journal-ref": "IJCSIS, Vol. 7 No. 3, March 2010, 185-190", "doi": null, "report-no": null, "categories": "cs.LG cs.IR", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  Clustering is an unsupervised learning method that constitutes a cornerstone\nof an intelligent data analysis process. It is used for the exploration of\ninter-relationships among a collection of patterns, by organizing them into\nhomogeneous clusters. Clustering has been dynamically applied to a variety of\ntasks in the field of Information Retrieval (IR). Clustering has become one of\nthe most active area of research and the development. Clustering attempts to\ndiscover the set of consequential groups where those within each group are more\nclosely related to one another than the others assigned to different groups.\nThe resultant clusters can provide a structure for organizing large bodies of\ntext for efficient browsing and searching. There exists a wide variety of\nclustering algorithms that has been intensively studied in the clustering\nproblem. Among the algorithms that remain the most common and effectual, the\niterative optimization clustering algorithms have been demonstrated reasonable\nperformance for clustering, e.g. the Expectation Maximization (EM) algorithm\nand its variants, and the well known k-means algorithm. This paper presents an\nanalysis on how partition method clustering techniques - EM, K -means and K*\nMeans algorithm work on heartspect dataset with below mentioned features -\nPurity, Entropy, CPU time, Cluster wise analysis, Mean value analysis and inter\ncluster distance. Thus the paper finally provides the experimental results of\ndatasets for five clusters to strengthen the results that the quality of the\nbehavior in clusters in EM algorithm is far better than k-means algorithm and\nk*means algorithm.\n", "versions": [{"version": "v1", "created": "Sat, 10 Apr 2010 21:58:16 GMT"}], "update_date": "2010-04-13", "authors_parsed": [["Nathiya", "G.", ""], ["Punitha", "S. C.", ""], ["Punithavalli", "M.", ""]]}, {"id": "1004.1982", "submitter": "Dar\\'io Garc\\'ia-Garc\\'ia", "authors": "Dar\\'io Garc\\'ia-Garc\\'ia and Emilio Parrado-Hern\\'andez and Fernando\n  D\\'iaz-de-Mar\\'ia", "title": "State-Space Dynamics Distance for Clustering Sequential Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a novel similarity measure for clustering sequential\ndata. We first construct a common state-space by training a single\nprobabilistic model with all the sequences in order to get a unified\nrepresentation for the dataset. Then, distances are obtained attending to the\ntransition matrices induced by each sequence in that state-space. This approach\nsolves some of the usual overfitting and scalability issues of the existing\nsemi-parametric techniques, that rely on training a model for each sequence.\nEmpirical studies on both synthetic and real-world datasets illustrate the\nadvantages of the proposed similarity measure for clustering sequences.\n", "versions": [{"version": "v1", "created": "Fri, 9 Apr 2010 09:36:28 GMT"}], "update_date": "2010-04-13", "authors_parsed": [["Garc\u00eda-Garc\u00eda", "Dar\u00edo", ""], ["Parrado-Hern\u00e1ndez", "Emilio", ""], ["D\u00edaz-de-Mar\u00eda", "Fernando", ""]]}, {"id": "1004.1997", "submitter": "Daohang Sha", "authors": "Daohang Sha, Vladimir B. Bajic", "title": "An optimized recursive learning algorithm for three-layer feedforward\n  neural networks for mimo nonlinear system identifications", "comments": "15 pages, 5 figures", "journal-ref": "Intelligent Automation and Soft Computing, Vol. 17, No. 2, pp.\n  133-147, 2011", "doi": null, "report-no": null, "categories": "cs.NE cs.DC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Back-propagation with gradient method is the most popular learning algorithm\nfor feed-forward neural networks. However, it is critical to determine a proper\nfixed learning rate for the algorithm. In this paper, an optimized recursive\nalgorithm is presented for online learning based on matrix operation and\noptimization methods analytically, which can avoid the trouble to select a\nproper learning rate for the gradient method. The proof of weak convergence of\nthe proposed algorithm also is given. Although this approach is proposed for\nthree-layer, feed-forward neural networks, it could be extended to multiple\nlayer feed-forward neural networks. The effectiveness of the proposed\nalgorithms applied to the identification of behavior of a two-input and\ntwo-output non-linear dynamic system is demonstrated by simulation experiments.\n", "versions": [{"version": "v1", "created": "Mon, 12 Apr 2010 16:12:41 GMT"}], "update_date": "2011-08-10", "authors_parsed": [["Sha", "Daohang", ""], ["Bajic", "Vladimir B.", ""]]}, {"id": "1004.2027", "submitter": "Mohammad Gheshlaghi Azar", "authors": "Mohammad Gheshlaghi Azar, Vicenc Gomez and Hilbert J. Kappen", "title": "Dynamic Policy Programming", "comments": "Submitted to Journal of Machine Learning Research", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.SY math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a novel policy iteration method, called dynamic\npolicy programming (DPP), to estimate the optimal policy in the\ninfinite-horizon Markov decision processes. We prove the finite-iteration and\nasymptotic l\\infty-norm performance-loss bounds for DPP in the presence of\napproximation/estimation error. The bounds are expressed in terms of the\nl\\infty-norm of the average accumulated error as opposed to the l\\infty-norm of\nthe error in the case of the standard approximate value iteration (AVI) and the\napproximate policy iteration (API). This suggests that DPP can achieve a better\nperformance than AVI and API since it averages out the simulation noise caused\nby Monte-Carlo sampling throughout the learning process. We examine this\ntheoretical results numerically by com- paring the performance of the\napproximate variants of DPP with existing reinforcement learning (RL) methods\non different problem domains. Our results show that, in all cases, DPP-based\nalgorithms outperform other RL methods by a wide margin.\n", "versions": [{"version": "v1", "created": "Mon, 12 Apr 2010 19:09:43 GMT"}, {"version": "v2", "created": "Tue, 6 Sep 2011 20:23:59 GMT"}], "update_date": "2011-09-09", "authors_parsed": [["Azar", "Mohammad Gheshlaghi", ""], ["Gomez", "Vicenc", ""], ["Kappen", "Hilbert J.", ""]]}, {"id": "1004.2316", "submitter": "Sumio Watanabe", "authors": "Sumio Watanabe", "title": "Asymptotic Equivalence of Bayes Cross Validation and Widely Applicable\n  Information Criterion in Singular Learning Theory", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In regular statistical models, the leave-one-out cross-validation is\nasymptotically equivalent to the Akaike information criterion. However, since\nmany learning machines are singular statistical models, the asymptotic behavior\nof the cross-validation remains unknown. In previous studies, we established\nthe singular learning theory and proposed a widely applicable information\ncriterion, the expectation value of which is asymptotically equal to the\naverage Bayes generalization loss. In the present paper, we theoretically\ncompare the Bayes cross-validation loss and the widely applicable information\ncriterion and prove two theorems. First, the Bayes cross-validation loss is\nasymptotically equivalent to the widely applicable information criterion as a\nrandom variable. Therefore, model selection and hyperparameter optimization\nusing these two values are asymptotically equivalent. Second, the sum of the\nBayes generalization error and the Bayes cross-validation error is\nasymptotically equal to $2\\lambda/n$, where $\\lambda$ is the real log canonical\nthreshold and $n$ is the number of training samples. Therefore the relation\nbetween the cross-validation error and the generalization error is determined\nby the algebraic geometrical structure of a learning machine. We also clarify\nthat the deviance information criteria are different from the Bayes\ncross-validation and the widely applicable information criterion.\n", "versions": [{"version": "v1", "created": "Wed, 14 Apr 2010 05:08:48 GMT"}, {"version": "v2", "created": "Thu, 14 Oct 2010 01:55:02 GMT"}], "update_date": "2010-10-15", "authors_parsed": [["Watanabe", "Sumio", ""]]}, {"id": "1004.3334", "submitter": "Kamran Karimi", "authors": "Kamran Karimi and Howard J. Hamilton", "title": "Generation and Interpretation of Temporal Decision Rules", "comments": "17 pages, 3 figures, 4 tables. Accepted in the International Journal\n  of Computational Intelligence Research", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a solution to the problem of understanding a system that produces\na sequence of temporally ordered observations. Our solution is based on\ngenerating and interpreting a set of temporal decision rules. A temporal\ndecision rule is a decision rule that can be used to predict or retrodict the\nvalue of a decision attribute, using condition attributes that are observed at\ntimes other than the decision attribute's time of observation. A rule set,\nconsisting of a set of temporal decision rules with the same decision\nattribute, can be interpreted by our Temporal Investigation Method for\nEnregistered Record Sequences (TIMERS) to signify an instantaneous, an acausal\nor a possibly causal relationship between the condition attributes and the\ndecision attribute. We show the effectiveness of our method, by describing a\nnumber of experiments with both synthetic and real temporal data.\n", "versions": [{"version": "v1", "created": "Tue, 20 Apr 2010 02:52:27 GMT"}], "update_date": "2010-04-21", "authors_parsed": [["Karimi", "Kamran", ""], ["Hamilton", "Howard J.", ""]]}, {"id": "1004.3361", "submitter": "Stephane Nonnenmacher", "authors": "St\\'ephane Nonnenmacher (IPHT), Johannes Sjoestrand (IMB), Maciej\n  Zworski (UC Berkeley Maths)", "title": "From open quantum systems to open quantum maps", "comments": "53 pages, 8 figures", "journal-ref": null, "doi": "10.1007/s00220-011-1214-0", "report-no": null, "categories": "math.AP cs.LG math-ph math.DS math.MP nlin.CD", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For a class of quantized open chaotic systems satisfying a natural dynamical\nassumption, we show that the study of the resolvent, and hence of scattering\nand resonances, can be reduced to the study of a family of open quantum maps,\nthat is of finite dimensional operators obtained by quantizing the Poincar\\'e\nmap associated with the flow near the set of trapped trajectories.\n", "versions": [{"version": "v1", "created": "Tue, 20 Apr 2010 07:02:20 GMT"}, {"version": "v2", "created": "Tue, 11 May 2010 17:34:00 GMT"}, {"version": "v3", "created": "Mon, 28 Feb 2011 11:21:36 GMT"}], "update_date": "2015-05-18", "authors_parsed": [["Nonnenmacher", "St\u00e9phane", "", "IPHT"], ["Sjoestrand", "Johannes", "", "IMB"], ["Zworski", "Maciej", "", "UC Berkeley Maths"]]}, {"id": "1004.3814", "submitter": "Mithun Das Gupta", "authors": "Mithun Das Gupta, Thomas S. Huang", "title": "Bregman Distance to L1 Regularized Logistic Regression", "comments": "8 pages, 3 images, shorter version published in ICPR 2008 by same\n  authors.", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work we investigate the relationship between Bregman distances and\nregularized Logistic Regression model. We present a detailed study of Bregman\nDistance minimization, a family of generalized entropy measures associated with\nconvex functions. We convert the L1-regularized logistic regression into this\nmore general framework and propose a primal-dual method based algorithm for\nlearning the parameters. We pose L1-regularized logistic regression into\nBregman distance minimization and then apply non-linear constrained\noptimization techniques to estimate the parameters of the logistic model.\n", "versions": [{"version": "v1", "created": "Wed, 21 Apr 2010 23:09:06 GMT"}], "update_date": "2010-04-23", "authors_parsed": [["Gupta", "Mithun Das", ""], ["Huang", "Thomas S.", ""]]}, {"id": "1004.4223", "submitter": "Gregory Valiant", "authors": "Ankur Moitra and Gregory Valiant", "title": "Settling the Polynomial Learnability of Mixtures of Gaussians", "comments": "43 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given data drawn from a mixture of multivariate Gaussians, a basic problem is\nto accurately estimate the mixture parameters. We give an algorithm for this\nproblem that has a running time, and data requirement polynomial in the\ndimension and the inverse of the desired accuracy, with provably minimal\nassumptions on the Gaussians. As simple consequences of our learning algorithm,\nwe can perform near-optimal clustering of the sample points and density\nestimation for mixtures of k Gaussians, efficiently. The building blocks of our\nalgorithm are based on the work Kalai et al. [STOC 2010] that gives an\nefficient algorithm for learning mixtures of two Gaussians by considering a\nseries of projections down to one dimension, and applying the method of moments\nto each univariate projection. A major technical hurdle in Kalai et al. is\nshowing that one can efficiently learn univariate mixtures of two Gaussians. In\ncontrast, because pathological scenarios can arise when considering univariate\nprojections of mixtures of more than two Gaussians, the bulk of the work in\nthis paper concerns how to leverage an algorithm for learning univariate\nmixtures (of many Gaussians) to yield an efficient algorithm for learning in\nhigh dimensions. Our algorithm employs hierarchical clustering and rescaling,\ntogether with delicate methods for backtracking and recovering from failures\nthat can occur in our univariate algorithm. Finally, while the running time and\ndata requirements of our algorithm depend exponentially on the number of\nGaussians in the mixture, we prove that such a dependence is necessary.\n", "versions": [{"version": "v1", "created": "Fri, 23 Apr 2010 20:46:26 GMT"}], "update_date": "2010-04-27", "authors_parsed": [["Moitra", "Ankur", ""], ["Valiant", "Gregory", ""]]}, {"id": "1004.4421", "submitter": "Ohad Shamir", "authors": "Nicol\\`o Cesa-Bianchi, Shai Shalev-Shwartz and Ohad Shamir", "title": "Efficient Learning with Partially Observed Attributes", "comments": "This is a full version of the paper appearing in The 27th\n  International Conference on Machine Learning (ICML 2010)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe and analyze efficient algorithms for learning a linear predictor\nfrom examples when the learner can only view a few attributes of each training\nexample. This is the case, for instance, in medical research, where each\npatient participating in the experiment is only willing to go through a small\nnumber of tests. Our analysis bounds the number of additional examples\nsufficient to compensate for the lack of full information on each training\nexample. We demonstrate the efficiency of our algorithms by showing that when\nrunning on digit recognition data, they obtain a high prediction accuracy even\nwhen the learner gets to see only four pixels of each image.\n", "versions": [{"version": "v1", "created": "Mon, 26 Apr 2010 07:41:50 GMT"}, {"version": "v2", "created": "Wed, 28 Apr 2010 14:38:13 GMT"}], "update_date": "2010-04-29", "authors_parsed": [["Cesa-Bianchi", "Nicol\u00f2", ""], ["Shalev-Shwartz", "Shai", ""], ["Shamir", "Ohad", ""]]}, {"id": "1004.4668", "submitter": "John Moriarty", "authors": "Nick S. Jones and John Moriarty", "title": "Evolutionary Inference for Function-valued Traits: Gaussian Process\n  Regression on Phylogenies", "comments": "7 pages, 1 figure", "journal-ref": "Journal of the Royal Society Interface vol. 10 no. 78 20120616\n  (2013)", "doi": "10.1098/rsif.2012.0616", "report-no": null, "categories": "q-bio.QM cs.LG physics.data-an stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Biological data objects often have both of the following features: (i) they\nare functions rather than single numbers or vectors, and (ii) they are\ncorrelated due to phylogenetic relationships. In this paper we give a flexible\nstatistical model for such data, by combining assumptions from phylogenetics\nwith Gaussian processes. We describe its use as a nonparametric Bayesian prior\ndistribution, both for prediction (placing posterior distributions on ancestral\nfunctions) and model selection (comparing rates of evolution across a\nphylogeny, or identifying the most likely phylogenies consistent with the\nobserved data). Our work is integrative, extending the popular phylogenetic\nBrownian Motion and Ornstein-Uhlenbeck models to functional data and Bayesian\ninference, and extending Gaussian Process regression to phylogenies. We provide\na brief illustration of the application of our method.\n", "versions": [{"version": "v1", "created": "Mon, 26 Apr 2010 22:22:18 GMT"}, {"version": "v2", "created": "Thu, 3 Nov 2011 00:04:22 GMT"}, {"version": "v3", "created": "Fri, 3 Aug 2012 07:42:47 GMT"}], "update_date": "2012-12-20", "authors_parsed": [["Jones", "Nick S.", ""], ["Moriarty", "John", ""]]}, {"id": "1004.4864", "submitter": "Kaushik Sinha", "authors": "Mikhail Belkin and Kaushik Sinha", "title": "Polynomial Learning of Distribution Families", "comments": "24 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The question of polynomial learnability of probability distributions,\nparticularly Gaussian mixture distributions, has recently received significant\nattention in theoretical computer science and machine learning. However,\ndespite major progress, the general question of polynomial learnability of\nGaussian mixture distributions still remained open. The current work resolves\nthe question of polynomial learnability for Gaussian mixtures in high dimension\nwith an arbitrary fixed number of components. The result on learning Gaussian\nmixtures relies on an analysis of distributions belonging to what we call\n\"polynomial families\" in low dimension. These families are characterized by\ntheir moments being polynomial in parameters and include almost all common\nprobability distributions as well as their mixtures and products. Using tools\nfrom real algebraic geometry, we show that parameters of any distribution\nbelonging to such a family can be learned in polynomial time and using a\npolynomial number of sample points. The result on learning polynomial families\nis quite general and is of independent interest. To estimate parameters of a\nGaussian mixture distribution in high dimensions, we provide a deterministic\nalgorithm for dimensionality reduction. This allows us to reduce learning a\nhigh-dimensional mixture to a polynomial number of parameter estimations in low\ndimension. Combining this reduction with the results on polynomial families\nyields our result on learning arbitrary Gaussian mixtures in high dimensions.\n", "versions": [{"version": "v1", "created": "Tue, 27 Apr 2010 16:59:43 GMT"}], "update_date": "2010-05-13", "authors_parsed": [["Belkin", "Mikhail", ""], ["Sinha", "Kaushik", ""]]}, {"id": "1004.5194", "submitter": "Daniil Ryabko", "authors": "Daniil Ryabko (INRIA Lille - Nord Europe)", "title": "Clustering processes", "comments": "in proceedings of ICML 2010. arXiv-admin note: for version 2 of this\n  article please see: arXiv:1005.0826v1", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.IT math.IT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The problem of clustering is considered, for the case when each data point is\na sample generated by a stationary ergodic process. We propose a very natural\nasymptotic notion of consistency, and show that simple consistent algorithms\nexist, under most general non-parametric assumptions. The notion of consistency\nis as follows: two samples should be put into the same cluster if and only if\nthey were generated by the same distribution. With this notion of consistency,\nclustering generalizes such classical statistical problems as homogeneity\ntesting and process classification. We show that, for the case of a known\nnumber of clusters, consistency can be achieved under the only assumption that\nthe joint distribution of the data is stationary ergodic (no parametric or\nMarkovian assumptions, no assumptions of independence, neither between nor\nwithin the samples). If the number of clusters is unknown, consistency can be\nachieved under appropriate assumptions on the mixing rates of the processes.\n(again, no parametric or independence assumptions). In both cases we give\nexamples of simple (at most quadratic in each argument) algorithms which are\nconsistent.\n", "versions": [{"version": "v1", "created": "Thu, 29 Apr 2010 06:38:47 GMT"}], "update_date": "2010-05-31", "authors_parsed": [["Ryabko", "Daniil", "", "INRIA Lille - Nord Europe"]]}, {"id": "1004.5229", "submitter": "Sarah Filippi", "authors": "Sarah Filippi (LTCI), Olivier Capp\\'e (LTCI), Aur\\'elien Garivier\n  (LTCI)", "title": "Optimism in Reinforcement Learning and Kullback-Leibler Divergence", "comments": "This work has been accepted and presented at ALLERTON 2010;\n  Communication, Control, and Computing (Allerton), 2010 48th Annual Allerton\n  Conference on, Monticello (Illinois) : \\'Etats-Unis (2010)", "journal-ref": null, "doi": "10.1109/ALLERTON.2010.5706896", "report-no": null, "categories": "cs.LG math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider model-based reinforcement learning in finite Markov De- cision\nProcesses (MDPs), focussing on so-called optimistic strategies. In MDPs,\noptimism can be implemented by carrying out extended value it- erations under a\nconstraint of consistency with the estimated model tran- sition probabilities.\nThe UCRL2 algorithm by Auer, Jaksch and Ortner (2009), which follows this\nstrategy, has recently been shown to guarantee near-optimal regret bounds. In\nthis paper, we strongly argue in favor of using the Kullback-Leibler (KL)\ndivergence for this purpose. By studying the linear maximization problem under\nKL constraints, we provide an ef- ficient algorithm, termed KL-UCRL, for\nsolving KL-optimistic extended value iteration. Using recent deviation bounds\non the KL divergence, we prove that KL-UCRL provides the same guarantees as\nUCRL2 in terms of regret. However, numerical experiments on classical\nbenchmarks show a significantly improved behavior, particularly when the MDP\nhas reduced connectivity. To support this observation, we provide elements of\ncom- parison between the two algorithms based on geometric considerations.\n", "versions": [{"version": "v1", "created": "Thu, 29 Apr 2010 09:31:55 GMT"}, {"version": "v2", "created": "Thu, 17 Jun 2010 09:56:58 GMT"}, {"version": "v3", "created": "Wed, 13 Oct 2010 10:11:39 GMT"}], "update_date": "2011-09-22", "authors_parsed": [["Filippi", "Sarah", "", "LTCI"], ["Capp\u00e9", "Olivier", "", "LTCI"], ["Garivier", "Aur\u00e9lien", "", "LTCI"]]}, {"id": "1004.5326", "submitter": "Michael J. Barber", "authors": "Michael J. Barber and John W. Clark", "title": "Designing neural networks that process mean values of random variables", "comments": "13 pages, elsarticle", "journal-ref": null, "doi": null, "report-no": null, "categories": "cond-mat.dis-nn cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a class of neural networks derived from probabilistic models in\nthe form of Bayesian networks. By imposing additional assumptions about the\nnature of the probabilistic models represented in the networks, we derive\nneural networks with standard dynamics that require no training to determine\nthe synaptic weights, that perform accurate calculation of the mean values of\nthe random variables, that can pool multiple sources of evidence, and that deal\ncleanly and consistently with inconsistent or contradictory evidence. The\npresented neural networks capture many properties of Bayesian networks,\nproviding distributed versions of probabilistic models.\n", "versions": [{"version": "v1", "created": "Thu, 29 Apr 2010 15:35:32 GMT"}], "update_date": "2010-04-30", "authors_parsed": [["Barber", "Michael J.", ""], ["Clark", "John W.", ""]]}]