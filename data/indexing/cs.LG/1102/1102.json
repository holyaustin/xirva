[{"id": "1102.0026", "submitter": "Parasaran Raman", "authors": "Parasaran Raman, Jeff M. Phillips and Suresh Venkatasubramanian", "title": "Spatially-Aware Comparison and Consensus for Clusterings", "comments": "12 Pages, 9 figures, Proceedings of 2011 Siam International\n  Conference on Data Mining", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CG cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a new distance metric between clusterings that\nincorporates information about the spatial distribution of points and clusters.\nOur approach builds on the idea of a Hilbert space-based representation of\nclusters as a combination of the representations of their constituent points.\nWe use this representation and the underlying metric to design a\nspatially-aware consensus clustering procedure. This consensus procedure is\nimplemented via a novel reduction to Euclidean clustering, and is both simple\nand efficient. All of our results apply to both soft and hard clusterings. We\naccompany these algorithms with a detailed experimental evaluation that\ndemonstrates the efficiency and quality of our techniques.\n", "versions": [{"version": "v1", "created": "Mon, 31 Jan 2011 22:21:19 GMT"}], "update_date": "2015-03-18", "authors_parsed": [["Raman", "Parasaran", ""], ["Phillips", "Jeff M.", ""], ["Venkatasubramanian", "Suresh", ""]]}, {"id": "1102.0059", "submitter": "Donghui Yan", "authors": "Donghui Yan, Pei Wang, Michael Linden, Beatrice Knudsen, Timothy\n  Randolph", "title": "Statistical methods for tissue array images - algorithmic scoring and\n  co-training", "comments": "Published in at http://dx.doi.org/10.1214/12-AOAS543 the Annals of\n  Applied Statistics (http://www.imstat.org/aoas/) by the Institute of\n  Mathematical Statistics (http://www.imstat.org)", "journal-ref": "Annals of Applied Statistics 2012, Vol. 6, No. 3, 1280-1305", "doi": "10.1214/12-AOAS543", "report-no": "IMS-AOAS-AOAS543", "categories": "stat.ME cs.CE cs.CV cs.LG q-bio.QM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent advances in tissue microarray technology have allowed\nimmunohistochemistry to become a powerful medium-to-high throughput analysis\ntool, particularly for the validation of diagnostic and prognostic biomarkers.\nHowever, as study size grows, the manual evaluation of these assays becomes a\nprohibitive limitation; it vastly reduces throughput and greatly increases\nvariability and expense. We propose an algorithm - Tissue Array Co-Occurrence\nMatrix Analysis (TACOMA) - for quantifying cellular phenotypes based on\ntextural regularity summarized by local inter-pixel relationships. The\nalgorithm can be easily trained for any staining pattern, is absent of\nsensitive tuning parameters and has the ability to report salient pixels in an\nimage that contribute to its score. Pathologists' input via informative\ntraining patches is an important aspect of the algorithm that allows the\ntraining for any specific marker or cell type. With co-training, the error rate\nof TACOMA can be reduced substantially for a very small training sample (e.g.,\nwith size 30). We give theoretical insights into the success of co-training via\nthinning of the feature set in a high-dimensional setting when there is\n\"sufficient\" redundancy among the features. TACOMA is flexible, transparent and\nprovides a scoring process that can be evaluated with clarity and confidence.\nIn a study based on an estrogen receptor (ER) marker, we show that TACOMA is\ncomparable to, or outperforms, pathologists' performance in terms of accuracy\nand repeatability.\n", "versions": [{"version": "v1", "created": "Tue, 1 Feb 2011 02:08:00 GMT"}, {"version": "v2", "created": "Mon, 1 Oct 2012 09:20:39 GMT"}], "update_date": "2015-03-18", "authors_parsed": [["Yan", "Donghui", ""], ["Wang", "Pei", ""], ["Linden", "Michael", ""], ["Knudsen", "Beatrice", ""], ["Randolph", "Timothy", ""]]}, {"id": "1102.0836", "submitter": "Feng Yan", "authors": "Yuan Qi, Feng Yan", "title": "EigenNet: A Bayesian hybrid of generative and conditional models for\n  sparse learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is a challenging task to select correlated variables in a high dimensional\nspace. To address this challenge, the elastic net has been developed and\nsuccessfully applied to many applications. Despite its great success, the\nelastic net does not explicitly use correlation information embedded in data to\nselect correlated variables. To overcome this limitation, we present a novel\nBayesian hybrid model, the EigenNet, that uses the eigenstructures of data to\nguide variable selection. Specifically, it integrates a sparse conditional\nclassification model with a generative model capturing variable correlations in\na principled Bayesian framework. We reparameterize the hybrid model in the\neigenspace to avoid overfiting and to increase the computational efficiency of\nits MCMC sampler. Furthermore, we provide an alternative view to the EigenNet\nfrom a regularization perspective: the EigenNet has an adaptive\neigenspace-based composite regularizer, which naturally generalizes the\n$l_{1/2}$ regularizer used by the elastic net. Experiments on synthetic and\nreal data show that the EigenNet significantly outperforms the lasso, the\nelastic net, and the Bayesian lasso in terms of prediction accuracy, especially\nwhen the number of training samples is smaller than the number of variables.\n", "versions": [{"version": "v1", "created": "Fri, 4 Feb 2011 04:40:07 GMT"}, {"version": "v2", "created": "Tue, 8 Feb 2011 04:03:50 GMT"}], "update_date": "2011-02-09", "authors_parsed": [["Qi", "Yuan", ""], ["Yan", "Feng", ""]]}, {"id": "1102.0899", "submitter": "Michael Del Rose", "authors": "Michael DelRose, Christian Wagner, Philip Frederick", "title": "Evidence Feed Forward Hidden Markov Model: A New Type of Hidden Markov\n  Model", "comments": "19 pages, International Journal of Artificial Intelligence and\n  Applications", "journal-ref": "International Journal of Artificial Intelligence and Applications\n  (IJAIA), Vol. 2, No. 1, Jan 2011", "doi": "10.5121/ijaia.2011.2101", "report-no": null, "categories": "cs.AI cs.CV cs.LG math.NA math.PR", "license": "http://creativecommons.org/licenses/publicdomain/", "abstract": "  The ability to predict the intentions of people based solely on their visual\nactions is a skill only performed by humans and animals. The intelligence of\ncurrent computer algorithms has not reached this level of complexity, but there\nare several research efforts that are working towards it. With the number of\nclassification algorithms available, it is hard to determine which algorithm\nworks best for a particular situation. In classification of visual human intent\ndata, Hidden Markov Models (HMM), and their variants, are leading candidates.\n  The inability of HMMs to provide a probability in the observation to\nobservation linkages is a big downfall in this classification technique. If a\nperson is visually identifying an action of another person, they monitor\npatterns in the observations. By estimating the next observation, people have\nthe ability to summarize the actions, and thus determine, with pretty good\naccuracy, the intention of the person performing the action. These visual cues\nand linkages are important in creating intelligent algorithms for determining\nhuman actions based on visual observations.\n  The Evidence Feed Forward Hidden Markov Model is a newly developed algorithm\nwhich provides observation to observation linkages. The following research\naddresses the theory behind Evidence Feed Forward HMMs, provides mathematical\nproofs of their learning of these parameters to optimize the likelihood of\nobservations with a Evidence Feed Forwards HMM, which is important in all\ncomputational intelligence algorithm, and gives comparative examples with\nstandard HMMs in classification of both visual action data and measurement\ndata; thus providing a strong base for Evidence Feed Forward HMMs in\nclassification of many types of problems.\n", "versions": [{"version": "v1", "created": "Fri, 4 Feb 2011 13:00:06 GMT"}], "update_date": "2011-02-07", "authors_parsed": [["DelRose", "Michael", ""], ["Wagner", "Christian", ""], ["Frederick", "Philip", ""]]}, {"id": "1102.1027", "submitter": "Alaa Abi Haidar", "authors": "Alaa Abi-Haidar and Luis M. Rocha", "title": "Collective Classification of Textual Documents by Guided\n  Self-Organization in T-Cell Cross-Regulation Dynamics", "comments": null, "journal-ref": "Evolutionary Intelligence. 2011. Volume 4, Number 2, 69-80", "doi": "10.1007/s12065-011-0052-5", "report-no": null, "categories": "cs.IR cs.AI cs.LG nlin.AO q-bio.OT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present and study an agent-based model of T-Cell cross-regulation in the\nadaptive immune system, which we apply to binary classification. Our method\nexpands an existing analytical model of T-cell cross-regulation (Carneiro et\nal. in Immunol Rev 216(1):48-68, 2007) that was used to study the\nself-organizing dynamics of a single population of T-Cells in interaction with\nan idealized antigen presenting cell capable of presenting a single antigen.\nWith agent-based modeling we are able to study the self-organizing dynamics of\nmultiple populations of distinct T-cells which interact via antigen presenting\ncells that present hundreds of distinct antigens. Moreover, we show that such\nself-organizing dynamics can be guided to produce an effective binary\nclassification of antigens, which is competitive with existing machine learning\nmethods when applied to biomedical text classification. More specifically, here\nwe test our model on a dataset of publicly available full-text biomedical\narticles provided by the BioCreative challenge (Krallinger in The biocreative\nii. 5 challenge overview, p 19, 2009). We study the robustness of our model's\nparameter configurations, and show that it leads to encouraging results\ncomparable to state-of-the-art classifiers. Our results help us understand both\nT-cell cross-regulation as a general principle of guided self-organization, as\nwell as its applicability to document classification. Therefore, we show that\nour bio-inspired algorithm is a promising novel method for biomedical article\nclassification and for binary document classification in general.\n", "versions": [{"version": "v1", "created": "Fri, 4 Feb 2011 22:10:45 GMT"}], "update_date": "2015-03-18", "authors_parsed": [["Abi-Haidar", "Alaa", ""], ["Rocha", "Luis M.", ""]]}, {"id": "1102.1182", "submitter": "Lenka Zdeborova", "authors": "Aurelien Decelle, Florent Krzakala, Cristopher Moore and Lenka\n  Zdeborov\\'a", "title": "Phase transition in the detection of modules in sparse networks", "comments": "4 pages, 4 figures", "journal-ref": "Phys. Rev. Lett. 107, 065701 (2011)", "doi": "10.1103/PhysRevLett.107.065701", "report-no": null, "categories": "cond-mat.stat-mech cs.LG cs.SI physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an asymptotically exact analysis of the problem of detecting\ncommunities in sparse random networks. Our results are also applicable to\ndetection of functional modules, partitions, and colorings in noisy planted\nmodels. Using a cavity method analysis, we unveil a phase transition from a\nregion where the original group assignment is undetectable to one where\ndetection is possible. In some cases, the detectable region splits into an\nalgorithmically hard region and an easy one. Our approach naturally translates\ninto a practical algorithm for detecting modules in sparse networks, and\nlearning the parameters of the underlying model.\n", "versions": [{"version": "v1", "created": "Sun, 6 Feb 2011 18:43:03 GMT"}], "update_date": "2011-08-04", "authors_parsed": [["Decelle", "Aurelien", ""], ["Krzakala", "Florent", ""], ["Moore", "Cristopher", ""], ["Zdeborov\u00e1", "Lenka", ""]]}, {"id": "1102.1324", "submitter": "Haizhang Zhang", "authors": "Yuesheng Xu, Haizhang Zhang, Qinghui Zhang", "title": "Refinement of Operator-valued Reproducing Kernels", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.FA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper studies the construction of a refinement kernel for a given\noperator-valued reproducing kernel such that the vector-valued reproducing\nkernel Hilbert space of the refinement kernel contains that of the given one as\na subspace. The study is motivated from the need of updating the current\noperator-valued reproducing kernel in multi-task learning when underfitting or\noverfitting occurs. Numerical simulations confirm that the established\nrefinement kernel method is able to meet this need. Various characterizations\nare provided based on feature maps and vector-valued integral representations\nof operator-valued reproducing kernels. Concrete examples of refining\ntranslation invariant and finite Hilbert-Schmidt operator-valued reproducing\nkernels are provided. Other examples include refinement of Hessian of\nscalar-valued translation-invariant kernels and transformation kernels.\nExistence and properties of operator-valued reproducing kernels preserved\nduring the refinement process are also investigated.\n", "versions": [{"version": "v1", "created": "Mon, 7 Feb 2011 14:41:30 GMT"}], "update_date": "2011-02-08", "authors_parsed": [["Xu", "Yuesheng", ""], ["Zhang", "Haizhang", ""], ["Zhang", "Qinghui", ""]]}, {"id": "1102.1465", "submitter": "Adrian Barbu", "authors": "Adrian Barbu, Nathan Lay", "title": "An Introduction to Artificial Prediction Markets for Classification", "comments": "29 pages, 8 figures", "journal-ref": "Journal of Machine Learning Research, 13, 2177-2204, 2012", "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Prediction markets are used in real life to predict outcomes of interest such\nas presidential elections. This paper presents a mathematical theory of\nartificial prediction markets for supervised learning of conditional\nprobability estimators. The artificial prediction market is a novel method for\nfusing the prediction information of features or trained classifiers, where the\nfusion result is the contract price on the possible outcomes. The market can be\ntrained online by updating the participants' budgets using training examples.\nInspired by the real prediction markets, the equations that govern the market\nare derived from simple and reasonable assumptions. Efficient numerical\nalgorithms are presented for solving these equations. The obtained artificial\nprediction market is shown to be a maximum likelihood estimator. It generalizes\nlinear aggregation, existent in boosting and random forest, as well as logistic\nregression and some kernel methods. Furthermore, the market mechanism allows\nthe aggregation of specialized classifiers that participate only on specific\ninstances. Experimental comparisons show that the artificial prediction markets\noften outperform random forest and implicit online learning on synthetic data\nand real UCI datasets. Moreover, an extensive evaluation for pelvic and\nabdominal lymph node detection in CT data shows that the prediction market\nimproves adaboost's detection rate from 79.6% to 81.2% at 3 false\npositives/volume.\n", "versions": [{"version": "v1", "created": "Mon, 7 Feb 2011 23:25:47 GMT"}, {"version": "v2", "created": "Wed, 9 Feb 2011 15:48:12 GMT"}, {"version": "v3", "created": "Mon, 14 Feb 2011 21:02:49 GMT"}, {"version": "v4", "created": "Thu, 22 Sep 2011 20:23:30 GMT"}, {"version": "v5", "created": "Sun, 26 Feb 2012 21:54:27 GMT"}, {"version": "v6", "created": "Mon, 9 Jul 2012 19:24:19 GMT"}], "update_date": "2015-03-18", "authors_parsed": [["Barbu", "Adrian", ""], ["Lay", "Nathan", ""]]}, {"id": "1102.1808", "submitter": "L\\'eon Bottou", "authors": "Leon Bottou", "title": "From Machine Learning to Machine Reasoning", "comments": "15 pages - fix broken pagination in v2", "journal-ref": null, "doi": null, "report-no": "tr-2011-02-08", "categories": "cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A plausible definition of \"reasoning\" could be \"algebraically manipulating\npreviously acquired knowledge in order to answer a new question\". This\ndefinition covers first-order logical inference or probabilistic inference. It\nalso includes much simpler manipulations commonly used to build large learning\nsystems. For instance, we can build an optical character recognition system by\nfirst training a character segmenter, an isolated character recognizer, and a\nlanguage model, using appropriate labeled training sets. Adequately\nconcatenating these modules and fine tuning the resulting system can be viewed\nas an algebraic operation in a space of models. The resulting model answers a\nnew question, that is, converting the image of a text page into a computer\nreadable text.\n  This observation suggests a conceptual continuity between algebraically rich\ninference systems, such as logical or probabilistic inference, and simple\nmanipulations, such as the mere concatenation of trainable learning systems.\nTherefore, instead of trying to bridge the gap between machine learning systems\nand sophisticated \"all-purpose\" inference mechanisms, we can instead\nalgebraically enrich the set of manipulations applicable to training systems,\nand build reasoning capabilities from the ground up.\n", "versions": [{"version": "v1", "created": "Wed, 9 Feb 2011 08:25:36 GMT"}, {"version": "v2", "created": "Thu, 10 Feb 2011 03:18:15 GMT"}, {"version": "v3", "created": "Fri, 11 Feb 2011 05:10:57 GMT"}], "update_date": "2011-02-14", "authors_parsed": [["Bottou", "Leon", ""]]}, {"id": "1102.2467", "submitter": "Marcus Hutter", "authors": "Marcus Hutter", "title": "Universal Learning Theory", "comments": "12 LaTeX pages", "journal-ref": "Encyclopedia of Machine Learning (2011) pages 1001-1008", "doi": null, "report-no": null, "categories": "cs.LG cs.IT math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This encyclopedic article gives a mini-introduction into the theory of\nuniversal learning, founded by Ray Solomonoff in the 1960s and significantly\ndeveloped and extended in the last decade. It explains the spirit of universal\nlearning, but necessarily glosses over technical subtleties.\n", "versions": [{"version": "v1", "created": "Sat, 12 Feb 2011 01:34:52 GMT"}], "update_date": "2012-02-10", "authors_parsed": [["Hutter", "Marcus", ""]]}, {"id": "1102.2490", "submitter": "Aur\\'elien Garivier", "authors": "Aur\\'elien Garivier and Olivier Capp\\'e", "title": "The KL-UCB Algorithm for Bounded Stochastic Bandits and Beyond", "comments": "18 pages, 3 figures; Conf. Comput. Learning Theory (COLT) 2011 in\n  Budapest, Hungary", "journal-ref": "Conference On Learning Theory n{\\deg}24 Jul. 2011 pp.359-376", "doi": null, "report-no": null, "categories": "math.ST cs.LG cs.SY math.OC stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a finite-time analysis of the KL-UCB algorithm, an\nonline, horizon-free index policy for stochastic bandit problems. We prove two\ndistinct results: first, for arbitrary bounded rewards, the KL-UCB algorithm\nsatisfies a uniformly better regret bound than UCB or UCB2; second, in the\nspecial case of Bernoulli rewards, it reaches the lower bound of Lai and\nRobbins. Furthermore, we show that simple adaptations of the KL-UCB algorithm\nare also optimal for specific classes of (possibly unbounded) rewards,\nincluding those generated from exponential families of distributions. A\nlarge-scale numerical study comparing KL-UCB with its main competitors (UCB,\nUCB2, UCB-Tuned, UCB-V, DMED) shows that KL-UCB is remarkably efficient and\nstable, including for short time horizons. KL-UCB is also the only method that\nalways performs better than the basic UCB policy. Our regret bounds rely on\ndeviations results of independent interest which are stated and proved in the\nAppendix. As a by-product, we also obtain an improved regret bound for the\nstandard UCB algorithm.\n", "versions": [{"version": "v1", "created": "Sat, 12 Feb 2011 10:03:21 GMT"}, {"version": "v2", "created": "Tue, 15 Mar 2011 17:22:01 GMT"}, {"version": "v3", "created": "Thu, 19 May 2011 10:07:35 GMT"}, {"version": "v4", "created": "Mon, 30 May 2011 08:53:45 GMT"}, {"version": "v5", "created": "Thu, 29 Aug 2013 15:37:53 GMT"}], "update_date": "2013-08-30", "authors_parsed": [["Garivier", "Aur\u00e9lien", ""], ["Capp\u00e9", "Olivier", ""]]}, {"id": "1102.2739", "submitter": "Sergey Tarasenko", "authors": "Sergey S. Tarasenko", "title": "A General Framework for Development of the Cortex-like Visual Object\n  Recognition System: Waves of Spikes, Predictive Coding and Universal\n  Dictionary of Features", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This study is focused on the development of the cortex-like visual object\nrecognition system. We propose a general framework, which consists of three\nhierarchical levels (modules). These modules functionally correspond to the V1,\nV4 and IT areas. Both bottom-up and top-down connections between the\nhierarchical levels V4 and IT are employed. The higher the degree of matching\nbetween the input and the preferred stimulus, the shorter the response time of\nthe neuron. Therefore information about a single stimulus is distributed in\ntime and is transmitted by the waves of spikes. The reciprocal connections and\nwaves of spikes implement predictive coding: an initial hypothesis is generated\non the basis of information delivered by the first wave of spikes and is tested\nwith the information carried by the consecutive waves. The development is\nconsidered as extraction and accumulation of features in V4 and objects in IT.\nOnce stored a feature can be disposed, if rarely activated. This cause update\nof feature repository. Consequently, objects in IT are also updated. This\nillustrates the growing process and dynamical change of topological structures\nof V4, IT and connections between these areas.\n", "versions": [{"version": "v1", "created": "Mon, 14 Feb 2011 11:40:08 GMT"}], "update_date": "2011-02-15", "authors_parsed": [["Tarasenko", "Sergey S.", ""]]}, {"id": "1102.2808", "submitter": "Chun Wei Seah", "authors": "Chun-Wei Seah, Ivor W. Tsang, Yew-Soon Ong", "title": "Transductive Ordinal Regression", "comments": null, "journal-ref": "IEEE Transactions on Neural Networks and Learning Systems,\n  23(7):1074 - 1086, 2012", "doi": "10.1109/TNNLS.2012.2198240", "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Ordinal regression is commonly formulated as a multi-class problem with\nordinal constraints. The challenge of designing accurate classifiers for\nordinal regression generally increases with the number of classes involved, due\nto the large number of labeled patterns that are needed. The availability of\nordinal class labels, however, is often costly to calibrate or difficult to\nobtain. Unlabeled patterns, on the other hand, often exist in much greater\nabundance and are freely available. To take benefits from the abundance of\nunlabeled patterns, we present a novel transductive learning paradigm for\nordinal regression in this paper, namely Transductive Ordinal Regression (TOR).\nThe key challenge of the present study lies in the precise estimation of both\nthe ordinal class label of the unlabeled data and the decision functions of the\nordinal classes, simultaneously. The core elements of the proposed TOR include\nan objective function that caters to several commonly used loss functions\ncasted in transductive settings, for general ordinal regression. A label\nswapping scheme that facilitates a strictly monotonic decrease in the objective\nfunction value is also introduced. Extensive numerical studies on commonly used\nbenchmark datasets including the real world sentiment prediction problem are\nthen presented to showcase the characteristics and efficacies of the proposed\ntransductive ordinal regression. Further, comparisons to recent\nstate-of-the-art ordinal regression methods demonstrate the introduced\ntransductive learning paradigm for ordinal regression led to the robust and\nimproved performance.\n", "versions": [{"version": "v1", "created": "Mon, 14 Feb 2011 15:53:06 GMT"}, {"version": "v2", "created": "Tue, 15 Feb 2011 12:46:46 GMT"}, {"version": "v3", "created": "Thu, 30 Aug 2012 02:23:16 GMT"}, {"version": "v4", "created": "Fri, 31 Aug 2012 02:54:05 GMT"}, {"version": "v5", "created": "Mon, 3 Sep 2012 02:17:30 GMT"}], "update_date": "2015-03-18", "authors_parsed": [["Seah", "Chun-Wei", ""], ["Tsang", "Ivor W.", ""], ["Ong", "Yew-Soon", ""]]}, {"id": "1102.2975", "submitter": "Keqin Liu", "authors": "Haoyang Liu, Keqin Liu, Qing Zhao", "title": "Decentralized Restless Bandit with Multiple Players and Unknown Dynamics", "comments": "7 pages, 2 figures, in Proc. of Information Theory and Applications\n  Workshop (ITA), January, 2011", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.LG cs.SY math.PR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider decentralized restless multi-armed bandit problems with unknown\ndynamics and multiple players. The reward state of each arm transits according\nto an unknown Markovian rule when it is played and evolves according to an\narbitrary unknown random process when it is passive. Players activating the\nsame arm at the same time collide and suffer from reward loss. The objective is\nto maximize the long-term reward by designing a decentralized arm selection\npolicy to address unknown reward models and collisions among players. A\ndecentralized policy is constructed that achieves a regret with logarithmic\norder when an arbitrary nontrivial bound on certain system parameters is known.\nWhen no knowledge about the system is available, we extend the policy to\nachieve a regret arbitrarily close to the logarithmic order. The result finds\napplications in communication networks, financial investment, and industrial\nengineering.\n", "versions": [{"version": "v1", "created": "Tue, 15 Feb 2011 06:12:44 GMT"}], "update_date": "2011-02-16", "authors_parsed": [["Liu", "Haoyang", ""], ["Liu", "Keqin", ""], ["Zhao", "Qing", ""]]}, {"id": "1102.3176", "submitter": "Mario Frank", "authors": "Mario Frank and Joachim M. Buhmann", "title": "Selecting the rank of truncated SVD by Maximum Approximation Capacity", "comments": "7 pages, 5 figures; Will be presented at the IEEE International\n  Symposium on Information Theory (ISIT) 2011. The conference version has only\n  5 pages. This version has an extended appendix", "journal-ref": "Information Theory Proceedings (ISIT), 2011 IEEE International\n  Symposium on, 2011, pages 1036-1040", "doi": "10.1109/ISIT.2011.6033687", "report-no": null, "categories": "cs.IT cs.LG math.IT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Truncated Singular Value Decomposition (SVD) calculates the closest rank-$k$\napproximation of a given input matrix. Selecting the appropriate rank $k$\ndefines a critical model order choice in most applications of SVD. To obtain a\nprincipled cut-off criterion for the spectrum, we convert the underlying\noptimization problem into a noisy channel coding problem. The optimal\napproximation capacity of this channel controls the appropriate strength of\nregularization to suppress noise. In simulation experiments, this information\ntheoretic method to determine the optimal rank competes with state-of-the art\nmodel selection techniques.\n", "versions": [{"version": "v1", "created": "Tue, 15 Feb 2011 20:49:37 GMT"}, {"version": "v2", "created": "Wed, 9 Mar 2011 13:18:10 GMT"}, {"version": "v3", "created": "Wed, 8 Jun 2011 10:08:51 GMT"}], "update_date": "2013-01-08", "authors_parsed": [["Frank", "Mario", ""], ["Buhmann", "Joachim M.", ""]]}, {"id": "1102.3260", "submitter": "Simona Cocco", "authors": "Simona Cocco (LPS), R\\'emi Monasson (LPTENS)", "title": "Adaptive Cluster Expansion for Inferring Boltzmann Machines with Noisy\n  Data", "comments": "Accepted for publication in Physical Review Letters (2011)", "journal-ref": null, "doi": "10.1103/PhysRevLett.106.090601", "report-no": null, "categories": "physics.data-an cond-mat.stat-mech cs.LG q-bio.NC q-bio.QM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a procedure to infer the interactions among a set of binary\nvariables, based on their sampled frequencies and pairwise correlations. The\nalgorithm builds the clusters of variables contributing most to the entropy of\nthe inferred Ising model, and rejects the small contributions due to the\nsampling noise. Our procedure successfully recovers benchmark Ising models even\nat criticality and in the low temperature phase, and is applied to\nneurobiological data.\n", "versions": [{"version": "v1", "created": "Wed, 16 Feb 2011 08:15:42 GMT"}], "update_date": "2015-05-27", "authors_parsed": [["Cocco", "Simona", "", "LPS"], ["Monasson", "R\u00e9mi", "", "LPTENS"]]}, {"id": "1102.3508", "submitter": "Cem Tekin", "authors": "Cem Tekin and Mingyan Liu", "title": "Online Learning of Rested and Restless Bandits", "comments": null, "journal-ref": "Information Theory, IEEE Transactions on , vol.58, no.8,\n  pp.5588,5611, Aug. 2012", "doi": "10.1109/TIT.2012.2198613", "report-no": null, "categories": "math.OC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we study the online learning problem involving rested and\nrestless multiarmed bandits with multiple plays. The system consists of a\nsingle player/user and a set of K finite-state discrete-time Markov chains\n(arms) with unknown state spaces and statistics. At each time step the player\ncan play M arms. The objective of the user is to decide for each step which M\nof the K arms to play over a sequence of trials so as to maximize its long term\nreward. The restless multiarmed bandit is particularly relevant to the\napplication of opportunistic spectrum access (OSA), where a (secondary) user\nhas access to a set of K channels, each of time-varying condition as a result\nof random fading and/or certain primary users' activities.\n", "versions": [{"version": "v1", "created": "Thu, 17 Feb 2011 07:08:37 GMT"}], "update_date": "2015-03-25", "authors_parsed": [["Tekin", "Cem", ""], ["Liu", "Mingyan", ""]]}, {"id": "1102.3887", "submitter": "Brian Eriksson", "authors": "Brian Eriksson, Gautam Dasarathy, Aarti Singh, Robert Nowak", "title": "Active Clustering: Robust and Efficient Hierarchical Clustering using\n  Adaptively Selected Similarities", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.LG math.IT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hierarchical clustering based on pairwise similarities is a common tool used\nin a broad range of scientific applications. However, in many problems it may\nbe expensive to obtain or compute similarities between the items to be\nclustered. This paper investigates the hierarchical clustering of N items based\non a small subset of pairwise similarities, significantly less than the\ncomplete set of N(N-1)/2 similarities. First, we show that if the intracluster\nsimilarities exceed intercluster similarities, then it is possible to correctly\ndetermine the hierarchical clustering from as few as 3N log N similarities. We\ndemonstrate this order of magnitude savings in the number of pairwise\nsimilarities necessitates sequentially selecting which similarities to obtain\nin an adaptive fashion, rather than picking them at random. We then propose an\nactive clustering method that is robust to a limited fraction of anomalous\nsimilarities, and show how even in the presence of these noisy similarity\nvalues we can resolve the hierarchical clustering using only O(N log^2 N)\npairwise similarities.\n", "versions": [{"version": "v1", "created": "Fri, 18 Feb 2011 19:05:49 GMT"}], "update_date": "2015-03-19", "authors_parsed": [["Eriksson", "Brian", ""], ["Dasarathy", "Gautam", ""], ["Singh", "Aarti", ""], ["Nowak", "Robert", ""]]}, {"id": "1102.3919", "submitter": "TaeHyun Hwang", "authors": "TaeHyun Hwang, Wei Zhang, Maoqiang Xie, Rui Kuang", "title": "Inferring Disease and Gene Set Associations with Rank Coherence in\n  Networks", "comments": "16 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.GN cs.AI cs.LG q-bio.MN", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A computational challenge to validate the candidate disease genes identified\nin a high-throughput genomic study is to elucidate the associations between the\nset of candidate genes and disease phenotypes. The conventional gene set\nenrichment analysis often fails to reveal associations between disease\nphenotypes and the gene sets with a short list of poorly annotated genes,\nbecause the existing annotations of disease causative genes are incomplete. We\npropose a network-based computational approach called rcNet to discover the\nassociations between gene sets and disease phenotypes. Assuming coherent\nassociations between the genes ranked by their relevance to the query gene set,\nand the disease phenotypes ranked by their relevance to the hidden target\ndisease phenotypes of the query gene set, we formulate a learning framework\nmaximizing the rank coherence with respect to the known disease phenotype-gene\nassociations. An efficient algorithm coupling ridge regression with label\npropagation, and two variants are introduced to find the optimal solution of\nthe framework. We evaluated the rcNet algorithms and existing baseline methods\nwith both leave-one-out cross-validation and a task of predicting recently\ndiscovered disease-gene associations in OMIM. The experiments demonstrated that\nthe rcNet algorithms achieved the best overall rankings compared to the\nbaselines. To further validate the reproducibility of the performance, we\napplied the algorithms to identify the target diseases of novel candidate\ndisease genes obtained from recent studies of GWAS, DNA copy number variation\nanalysis, and gene expression profiling. The algorithms ranked the target\ndisease of the candidate genes at the top of the rank list in many cases across\nall the three case studies. The rcNet algorithms are available as a webtool for\ndisease and gene set association analysis at\nhttp://compbio.cs.umn.edu/dgsa_rcNet.\n", "versions": [{"version": "v1", "created": "Fri, 18 Feb 2011 21:01:38 GMT"}], "update_date": "2011-02-22", "authors_parsed": [["Hwang", "TaeHyun", ""], ["Zhang", "Wei", ""], ["Xie", "Maoqiang", ""], ["Kuang", "Rui", ""]]}, {"id": "1102.3923", "submitter": "Rina Foygel", "authors": "Rina Foygel, Nathan Srebro", "title": "Concentration-Based Guarantees for Low-Rank Matrix Reconstruction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of approximately reconstructing a partially-observed,\napproximately low-rank matrix. This problem has received much attention lately,\nmostly using the trace-norm as a surrogate to the rank. Here we study low-rank\nmatrix reconstruction using both the trace-norm, as well as the less-studied\nmax-norm, and present reconstruction guarantees based on existing analysis on\nthe Rademacher complexity of the unit balls of these norms. We show how these\nare superior in several ways to recently published guarantees based on\nspecialized analysis.\n", "versions": [{"version": "v1", "created": "Fri, 18 Feb 2011 21:26:16 GMT"}, {"version": "v2", "created": "Thu, 26 May 2011 19:26:27 GMT"}], "update_date": "2011-05-27", "authors_parsed": [["Foygel", "Rina", ""], ["Srebro", "Nathan", ""]]}, {"id": "1102.3949", "submitter": "Zhilin Zhang", "authors": "Zhilin Zhang and Bhaskar D. Rao", "title": "Sparse Signal Recovery with Temporally Correlated Source Vectors Using\n  Sparse Bayesian Learning", "comments": "The final version with some typos corrected. Codes can be downloaded\n  at: http://dsp.ucsd.edu/~zhilin/TSBL_code.zip", "journal-ref": "IEEE Journal of Selected Topics in Signal Processing, vol.5, no.\n  5, pp. 912-926, 2011", "doi": "10.1109/JSTSP.2011.2159773", "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We address the sparse signal recovery problem in the context of multiple\nmeasurement vectors (MMV) when elements in each nonzero row of the solution\nmatrix are temporally correlated. Existing algorithms do not consider such\ntemporal correlations and thus their performance degrades significantly with\nthe correlations. In this work, we propose a block sparse Bayesian learning\nframework which models the temporal correlations. In this framework we derive\ntwo sparse Bayesian learning (SBL) algorithms, which have superior recovery\nperformance compared to existing algorithms, especially in the presence of high\ntemporal correlations. Furthermore, our algorithms are better at handling\nhighly underdetermined problems and require less row-sparsity on the solution\nmatrix. We also provide analysis of the global and local minima of their cost\nfunction, and show that the SBL cost function has the very desirable property\nthat the global minimum is at the sparsest solution to the MMV problem.\nExtensive experiments also provide some interesting results that motivate\nfuture theoretical research on the MMV model.\n", "versions": [{"version": "v1", "created": "Sat, 19 Feb 2011 01:41:35 GMT"}, {"version": "v2", "created": "Wed, 17 Aug 2011 00:03:36 GMT"}], "update_date": "2011-08-18", "authors_parsed": [["Zhang", "Zhilin", ""], ["Rao", "Bhaskar D.", ""]]}, {"id": "1102.4021", "submitter": "Manas Pathak", "authors": "Manas A. Pathak, Mehrbod Sharifi, Bhiksha Raj", "title": "Privacy Preserving Spam Filtering", "comments": "9 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Email is a private medium of communication, and the inherent privacy\nconstraints form a major obstacle in developing effective spam filtering\nmethods which require access to a large amount of email data belonging to\nmultiple users. To mitigate this problem, we envision a privacy preserving spam\nfiltering system, where the server is able to train and evaluate a logistic\nregression based spam classifier on the combined email data of all users\nwithout being able to observe any emails using primitives such as homomorphic\nencryption and randomization. We analyze the protocols for correctness and\nsecurity, and perform experiments of a prototype system on a large scale spam\nfiltering task.\n  State of the art spam filters often use character n-grams as features which\nresult in large sparse data representation, which is not feasible to be used\ndirectly with our training and evaluation protocols. We explore various data\nindependent dimensionality reduction which decrease the running time of the\nprotocol making it feasible to use in practice while achieving high accuracy.\n", "versions": [{"version": "v1", "created": "Sat, 19 Feb 2011 20:40:56 GMT"}, {"version": "v2", "created": "Sun, 18 Sep 2011 05:37:43 GMT"}], "update_date": "2011-09-20", "authors_parsed": [["Pathak", "Manas A.", ""], ["Sharifi", "Mehrbod", ""], ["Raj", "Bhiksha", ""]]}, {"id": "1102.4240", "submitter": "Vincent Gripon", "authors": "Vincent Gripon and Claude Berrou", "title": "Sparse neural networks with large learning diversity", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Coded recurrent neural networks with three levels of sparsity are introduced.\nThe first level is related to the size of messages, much smaller than the\nnumber of available neurons. The second one is provided by a particular coding\nrule, acting as a local constraint in the neural activity. The third one is a\ncharacteristic of the low final connection density of the network after the\nlearning phase. Though the proposed network is very simple since it is based on\nbinary neurons and binary connections, it is able to learn a large number of\nmessages and recall them, even in presence of strong erasures. The performance\nof the network is assessed as a classifier and as an associative memory.\n", "versions": [{"version": "v1", "created": "Mon, 21 Feb 2011 14:48:20 GMT"}], "update_date": "2011-02-22", "authors_parsed": [["Gripon", "Vincent", ""], ["Berrou", "Claude", ""]]}, {"id": "1102.4374", "submitter": "Benjamin Rubinstein", "authors": "Arvind Narayanan, Elaine Shi, Benjamin I. P. Rubinstein", "title": "Link Prediction by De-anonymization: How We Won the Kaggle Social\n  Network Challenge", "comments": "11 pages, 13 figures; submitted to IJCNN'2011", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper describes the winning entry to the IJCNN 2011 Social Network\nChallenge run by Kaggle.com. The goal of the contest was to promote research on\nreal-world link prediction, and the dataset was a graph obtained by crawling\nthe popular Flickr social photo sharing website, with user identities scrubbed.\nBy de-anonymizing much of the competition test set using our own Flickr crawl,\nwe were able to effectively game the competition. Our attack represents a new\napplication of de-anonymization to gaming machine learning contests, suggesting\nchanges in how future competitions should be run.\n  We introduce a new simulated annealing-based weighted graph matching\nalgorithm for the seeding step of de-anonymization. We also show how to combine\nde-anonymization with link prediction---the latter is required to achieve good\nperformance on the portion of the test set not de-anonymized---for example by\ntraining the predictor on the de-anonymized portion of the test set, and\ncombining probabilistic predictions from de-anonymization and link prediction.\n", "versions": [{"version": "v1", "created": "Tue, 22 Feb 2011 00:11:14 GMT"}], "update_date": "2015-03-19", "authors_parsed": [["Narayanan", "Arvind", ""], ["Shi", "Elaine", ""], ["Rubinstein", "Benjamin I. P.", ""]]}, {"id": "1102.4442", "submitter": "Vianney Perchet", "authors": "Vianney Perchet", "title": "Internal Regret with Partial Monitoring. Calibration-Based Optimal\n  Algorithms", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.GT math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We provide consistent random algorithms for sequential decision under partial\nmonitoring, i.e. when the decision maker does not observe the outcomes but\nreceives instead random feedback signals. Those algorithms have no internal\nregret in the sense that, on the set of stages where the decision maker chose\nhis action according to a given law, the average payoff could not have been\nimproved in average by using any other fixed law.\n  They are based on a generalization of calibration, no longer defined in terms\nof a Voronoi diagram but instead of a Laguerre diagram (a more general\nconcept). This allows us to bound, for the first time in this general\nframework, the expected average internal -- as well as the usual external --\nregret at stage $n$ by $O(n^{-1/3})$, which is known to be optimal.\n", "versions": [{"version": "v1", "created": "Tue, 22 Feb 2011 09:56:28 GMT"}], "update_date": "2011-02-23", "authors_parsed": [["Perchet", "Vianney", ""]]}, {"id": "1102.4807", "submitter": "Alekh Agarwal", "authors": "Alekh Agarwal and Sahand N. Negahban and Martin J. Wainwright", "title": "Noisy matrix decomposition via convex relaxation: Optimal rates in high\n  dimensions", "comments": "41 pages, 2 figures", "journal-ref": "Annals of Statistics 2012, Vol. 40, No. 2, 1171-1197", "doi": "10.1214/12-AOS1000", "report-no": "IMS-AOS-AOS1000", "categories": "stat.ML cs.IT cs.LG math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We analyze a class of estimators based on convex relaxation for solving\nhigh-dimensional matrix decomposition problems. The observations are noisy\nrealizations of a linear transformation $\\mathfrak{X}$ of the sum of an\napproximately) low rank matrix $\\Theta^\\star$ with a second matrix\n$\\Gamma^\\star$ endowed with a complementary form of low-dimensional structure;\nthis set-up includes many statistical models of interest, including factor\nanalysis, multi-task regression, and robust covariance estimation. We derive a\ngeneral theorem that bounds the Frobenius norm error for an estimate of the\npair $(\\Theta^\\star, \\Gamma^\\star)$ obtained by solving a convex optimization\nproblem that combines the nuclear norm with a general decomposable regularizer.\nOur results utilize a \"spikiness\" condition that is related to but milder than\nsingular vector incoherence. We specialize our general result to two cases that\nhave been studied in past work: low rank plus an entrywise sparse matrix, and\nlow rank plus a columnwise sparse matrix. For both models, our theory yields\nnon-asymptotic Frobenius error bounds for both deterministic and stochastic\nnoise matrices, and applies to matrices $\\Theta^\\star$ that can be exactly or\napproximately low rank, and matrices $\\Gamma^\\star$ that can be exactly or\napproximately sparse. Moreover, for the case of stochastic noise matrices and\nthe identity observation operator, we establish matching lower bounds on the\nminimax error. The sharpness of our predictions is confirmed by numerical\nsimulations.\n", "versions": [{"version": "v1", "created": "Wed, 23 Feb 2011 18:02:53 GMT"}, {"version": "v2", "created": "Mon, 18 Jul 2011 02:25:47 GMT"}, {"version": "v3", "created": "Tue, 6 Mar 2012 06:59:59 GMT"}], "update_date": "2012-08-09", "authors_parsed": [["Agarwal", "Alekh", ""], ["Negahban", "Sahand N.", ""], ["Wainwright", "Martin J.", ""]]}, {"id": "1102.5288", "submitter": "Derin Babacan", "authors": "S. Derin Babacan, Martin Luessi, Rafael Molina, Aggelos K. Katsaggelos", "title": "Sparse Bayesian Methods for Low-Rank Matrix Estimation", "comments": "This paper has been withdrawn by the author due to significant\n  revisions in the paper. The new version will be uploaded soon", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG cs.SY math.OC stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recovery of low-rank matrices has recently seen significant activity in many\nareas of science and engineering, motivated by recent theoretical results for\nexact reconstruction guarantees and interesting practical applications. A\nnumber of methods have been developed for this recovery problem. However, a\nprincipled method for choosing the unknown target rank is generally not\nprovided. In this paper, we present novel recovery algorithms for estimating\nlow-rank matrices in matrix completion and robust principal component analysis\nbased on sparse Bayesian learning (SBL) principles. Starting from a matrix\nfactorization formulation and enforcing the low-rank constraint in the\nestimates as a sparsity constraint, we develop an approach that is very\neffective in determining the correct rank while providing high recovery\nperformance. We provide connections with existing methods in other similar\nproblems and empirical results and comparisons with current state-of-the-art\nmethods that illustrate the effectiveness of this approach.\n", "versions": [{"version": "v1", "created": "Fri, 25 Feb 2011 17:13:00 GMT"}, {"version": "v2", "created": "Fri, 9 Sep 2011 19:06:10 GMT"}], "update_date": "2011-09-12", "authors_parsed": [["Babacan", "S. Derin", ""], ["Luessi", "Martin", ""], ["Molina", "Rafael", ""], ["Katsaggelos", "Aggelos K.", ""]]}, {"id": "1102.5396", "submitter": "Ravi Venkatesan", "authors": "R. C. Venkatesan and A. Plastino", "title": "Deformed Statistics Free Energy Model for Source Separation using\n  Unsupervised Learning", "comments": "5 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cond-mat.stat-mech cs.IT cs.LG math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A generalized-statistics variational principle for source separation is\nformulated by recourse to Tsallis' entropy subjected to the additive duality\nand employing constraints described by normal averages. The variational\nprinciple is amalgamated with Hopfield-like learning rules resulting in an\nunsupervised learning model. The update rules are formulated with the aid of\nq-deformed calculus. Numerical examples exemplify the efficacy of this model.\n", "versions": [{"version": "v1", "created": "Sat, 26 Feb 2011 09:31:25 GMT"}], "update_date": "2011-03-01", "authors_parsed": [["Venkatesan", "R. C.", ""], ["Plastino", "A.", ""]]}, {"id": "1102.5561", "submitter": "Andras Lorincz", "authors": "Gabor Matuz and Andras Lorincz", "title": "Decision Making Agent Searching for Markov Models in Near-Deterministic\n  World", "comments": "Draft", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Reinforcement learning has solid foundations, but becomes inefficient in\npartially observed (non-Markovian) environments. Thus, a learning agent -born\nwith a representation and a policy- might wish to investigate to what extent\nthe Markov property holds. We propose a learning architecture that utilizes\ncombinatorial policy optimization to overcome non-Markovity and to develop\nefficient behaviors, which are easy to inherit, tests the Markov property of\nthe behavioral states, and corrects against non-Markovity by running a\ndeterministic factored Finite State Model, which can be learned. We illustrate\nthe properties of architecture in the near deterministic Ms. Pac-Man game. We\nanalyze the architecture from the point of view of evolutionary, individual,\nand social learning.\n", "versions": [{"version": "v1", "created": "Sun, 27 Feb 2011 23:47:13 GMT"}, {"version": "v2", "created": "Tue, 1 Mar 2011 07:35:59 GMT"}], "update_date": "2011-03-02", "authors_parsed": [["Matuz", "Gabor", ""], ["Lorincz", "Andras", ""]]}, {"id": "1102.5593", "submitter": "Fanggang Wang", "authors": "Fanggang Wang, Rongtao Xu, Zhangdui Zhong", "title": "Low Complexity Kolmogorov-Smirnov Modulation Classification", "comments": "This paper is accepted by IEEE WCNC 2011", "journal-ref": null, "doi": "10.1109/WCNC.2011.5779375", "report-no": null, "categories": "cs.IT cs.LG math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Kolmogorov-Smirnov (K-S) test-a non-parametric method to measure the goodness\nof fit, is applied for automatic modulation classification (AMC) in this paper.\nThe basic procedure involves computing the empirical cumulative distribution\nfunction (ECDF) of some decision statistic derived from the received signal,\nand comparing it with the CDFs of the signal under each candidate modulation\nformat. The K-S-based modulation classifier is first developed for AWGN\nchannel, then it is applied to OFDM-SDMA systems to cancel multiuser\ninterference. Regarding the complexity issue of K-S modulation classification,\nwe propose a low-complexity method based on the robustness of the K-S\nclassifier. Extensive simulation results demonstrate that compared with the\ntraditional cumulant-based classifiers, the proposed K-S classifier offers\nsuperior classification performance and requires less number of signal samples\n(thus is fast).\n", "versions": [{"version": "v1", "created": "Mon, 28 Feb 2011 04:16:24 GMT"}, {"version": "v2", "created": "Thu, 3 Mar 2011 13:10:48 GMT"}], "update_date": "2016-11-17", "authors_parsed": [["Wang", "Fanggang", ""], ["Xu", "Rongtao", ""], ["Zhong", "Zhangdui", ""]]}, {"id": "1102.5597", "submitter": "Radim v{R}eh{u}v{r}ek", "authors": "Radim \\v{R}eh{\\r{u}}\\v{r}ek", "title": "Fast and Faster: A Comparison of Two Streamed Matrix Decomposition\n  Algorithms", "comments": null, "journal-ref": "NIPS Workshop on Low-Rank Methods for Large-Scale Machine\n  Learning, 2010", "doi": null, "report-no": null, "categories": "cs.NA cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the explosion of the size of digital dataset, the limiting factor for\ndecomposition algorithms is the \\emph{number of passes} over the input, as the\ninput is often stored out-of-core or even off-site. Moreover, we're only\ninterested in algorithms that operate in \\emph{constant memory} w.r.t. to the\ninput size, so that arbitrarily large input can be processed. In this paper, we\npresent a practical comparison of two such algorithms: a distributed method\nthat operates in a single pass over the input vs. a streamed two-pass\nstochastic algorithm. The experiments track the effect of distributed\ncomputing, oversampling and memory trade-offs on the accuracy and performance\nof the two algorithms. To ensure meaningful results, we choose the input to be\na real dataset, namely the whole of the English Wikipedia, in the application\nsettings of Latent Semantic Analysis.\n", "versions": [{"version": "v1", "created": "Mon, 28 Feb 2011 05:26:58 GMT"}], "update_date": "2016-08-14", "authors_parsed": [["\u0158eh{\u016f}\u0159ek", "Radim", ""]]}, {"id": "1102.5728", "submitter": "Wahiba Ben abdessalem Karaa", "authors": "Wahiba Ben Abdessalem Karaa", "title": "Named Entity Recognition Using Web Document Corpus", "comments": "11 pages 4 figures, 2 tables", "journal-ref": "International Journal of Managing Information Technology (IJMIT)\n  Vol.3, No.1, February 2011", "doi": "10.5121/ijmit.2011.3104", "report-no": null, "categories": "cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces a named entity recognition approach in textual corpus.\nThis Named Entity (NE) can be a named: location, person, organization, date,\ntime, etc., characterized by instances. A NE is found in texts accompanied by\ncontexts: words that are left or right of the NE. The work mainly aims at\nidentifying contexts inducing the NE's nature. As such, The occurrence of the\nword \"President\" in a text, means that this word or context may be followed by\nthe name of a president as President \"Obama\". Likewise, a word preceded by the\nstring \"footballer\" induces that this is the name of a footballer. NE\nrecognition may be viewed as a classification method, where every word is\nassigned to a NE class, regarding the context. The aim of this study is then to\nidentify and classify the contexts that are most relevant to recognize a NE,\nthose which are frequently found with the NE. A learning approach using\ntraining corpus: web documents, constructed from learning examples is then\nsuggested. Frequency representations and modified tf-idf representations are\nused to calculate the context weights associated to context frequency, learning\nexample frequency, and document frequency in the corpus.\n", "versions": [{"version": "v1", "created": "Mon, 28 Feb 2011 18:33:09 GMT"}], "update_date": "2011-03-01", "authors_parsed": [["Karaa", "Wahiba Ben Abdessalem", ""]]}, {"id": "1102.5750", "submitter": "Philippe Rigollet", "authors": "Philippe Rigollet and Xin Tong", "title": "Neyman-Pearson classification, convexity and stochastic constraints", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motivated by problems of anomaly detection, this paper implements the\nNeyman-Pearson paradigm to deal with asymmetric errors in binary classification\nwith a convex loss. Given a finite collection of classifiers, we combine them\nand obtain a new classifier that satisfies simultaneously the two following\nproperties with high probability: (i) its probability of type I error is below\na pre-specified level and (ii), it has probability of type II error close to\nthe minimum possible. The proposed classifier is obtained by solving an\noptimization problem with an empirical objective and an empirical constraint.\nNew techniques to handle such problems are developed and have consequences on\nchance constrained programming.\n", "versions": [{"version": "v1", "created": "Mon, 28 Feb 2011 19:31:41 GMT"}], "update_date": "2011-03-01", "authors_parsed": [["Rigollet", "Philippe", ""], ["Tong", "Xin", ""]]}]