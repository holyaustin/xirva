[{"id": "1309.0003", "submitter": "Xinjia Chen", "authors": "Xinjia Chen", "title": "Concentration Inequalities for Bounded Random Vectors", "comments": "9 pages, no figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR cs.LG math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We derive simple concentration inequalities for bounded random vectors, which\ngeneralize Hoeffding's inequalities for bounded scalar random variables. As\napplications, we apply the general results to multinomial and Dirichlet\ndistributions to obtain multivariate concentration inequalities.\n", "versions": [{"version": "v1", "created": "Fri, 30 Aug 2013 18:27:01 GMT"}], "update_date": "2013-11-05", "authors_parsed": [["Chen", "Xinjia", ""]]}, {"id": "1309.0113", "submitter": "Anthony Man-Cho So", "authors": "Anthony Man-Cho So", "title": "Non-Asymptotic Convergence Analysis of Inexact Gradient Methods for\n  Machine Learning Without Strong Convexity", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many recent applications in machine learning and data fitting call for the\nalgorithmic solution of structured smooth convex optimization problems.\nAlthough the gradient descent method is a natural choice for this task, it\nrequires exact gradient computations and hence can be inefficient when the\nproblem size is large or the gradient is difficult to evaluate. Therefore,\nthere has been much interest in inexact gradient methods (IGMs), in which an\nefficiently computable approximate gradient is used to perform the update in\neach iteration. Currently, non-asymptotic linear convergence results for IGMs\nare typically established under the assumption that the objective function is\nstrongly convex, which is not satisfied in many applications of interest; while\nlinear convergence results that do not require the strong convexity assumption\nare usually asymptotic in nature. In this paper, we combine the best of these\ntwo types of results and establish---under the standard assumption that the\ngradient approximation errors decrease linearly to zero---the non-asymptotic\nlinear convergence of IGMs when applied to a class of structured convex\noptimization problems. Such a class covers settings where the objective\nfunction is not necessarily strongly convex and includes the least squares and\nlogistic regression problems. We believe that our techniques will find further\napplications in the non-asymptotic convergence analysis of other first-order\nmethods.\n", "versions": [{"version": "v1", "created": "Sat, 31 Aug 2013 13:39:00 GMT"}], "update_date": "2013-09-03", "authors_parsed": [["So", "Anthony Man-Cho", ""]]}, {"id": "1309.0238", "submitter": "Gael Varoquaux", "authors": "Lars Buitinck (ILPS), Gilles Louppe, Mathieu Blondel, Fabian Pedregosa\n  (INRIA Saclay - Ile de France), Andreas Mueller, Olivier Grisel, Vlad\n  Niculae, Peter Prettenhofer, Alexandre Gramfort (INRIA Saclay - Ile de\n  France, LTCI), Jaques Grobler (INRIA Saclay - Ile de France), Robert Layton,\n  Jake Vanderplas, Arnaud Joly, Brian Holt, Ga\\\"el Varoquaux (INRIA Saclay -\n  Ile de France)", "title": "API design for machine learning software: experiences from the\n  scikit-learn project", "comments": null, "journal-ref": "European Conference on Machine Learning and Principles and\n  Practices of Knowledge Discovery in Databases (2013)", "doi": null, "report-no": null, "categories": "cs.LG cs.MS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Scikit-learn is an increasingly popular machine learning li- brary. Written\nin Python, it is designed to be simple and efficient, accessible to\nnon-experts, and reusable in various contexts. In this paper, we present and\ndiscuss our design choices for the application programming interface (API) of\nthe project. In particular, we describe the simple and elegant interface shared\nby all learning and processing units in the library and then discuss its\nadvantages in terms of composition and reusability. The paper also comments on\nimplementation details specific to the Python ecosystem and analyzes obstacles\nfaced by users and developers of the library.\n", "versions": [{"version": "v1", "created": "Sun, 1 Sep 2013 16:22:48 GMT"}], "update_date": "2013-09-03", "authors_parsed": [["Buitinck", "Lars", "", "ILPS"], ["Louppe", "Gilles", "", "INRIA Saclay - Ile de France"], ["Blondel", "Mathieu", "", "INRIA Saclay - Ile de France"], ["Pedregosa", "Fabian", "", "INRIA Saclay - Ile de France"], ["Mueller", "Andreas", "", "INRIA Saclay - Ile de\n  France, LTCI"], ["Grisel", "Olivier", "", "INRIA Saclay - Ile de\n  France, LTCI"], ["Niculae", "Vlad", "", "INRIA Saclay - Ile de\n  France, LTCI"], ["Prettenhofer", "Peter", "", "INRIA Saclay - Ile de\n  France, LTCI"], ["Gramfort", "Alexandre", "", "INRIA Saclay - Ile de\n  France, LTCI"], ["Grobler", "Jaques", "", "INRIA Saclay - Ile de France"], ["Layton", "Robert", "", "INRIA Saclay -\n  Ile de France"], ["Vanderplas", "Jake", "", "INRIA Saclay -\n  Ile de France"], ["Joly", "Arnaud", "", "INRIA Saclay -\n  Ile de France"], ["Holt", "Brian", "", "INRIA Saclay -\n  Ile de France"], ["Varoquaux", "Ga\u00ebl", "", "INRIA Saclay -\n  Ile de France"]]}, {"id": "1309.0242", "submitter": "Pontus Svenson", "authors": "Johan Dahlin and Pontus Svenson", "title": "Ensemble approaches for improving community detection methods", "comments": "12 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.soc-ph cs.LG cs.SI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Statistical estimates can often be improved by fusion of data from several\ndifferent sources. One example is so-called ensemble methods which have been\nsuccessfully applied in areas such as machine learning for classification and\nclustering. In this paper, we present an ensemble method to improve community\ndetection by aggregating the information found in an ensemble of community\nstructures. This ensemble can found by re-sampling methods, multiple runs of a\nstochastic community detection method, or by several different community\ndetection algorithms applied to the same network. The proposed method is\nevaluated using random networks with community structures and compared with two\ncommonly used community detection methods. The proposed method when applied on\na stochastic community detection algorithm performs well with low computational\ncomplexity, thus offering both a new approach to community detection and an\nadditional community detection method.\n", "versions": [{"version": "v1", "created": "Sun, 1 Sep 2013 16:59:55 GMT"}], "update_date": "2013-09-03", "authors_parsed": [["Dahlin", "Johan", ""], ["Svenson", "Pontus", ""]]}, {"id": "1309.0302", "submitter": "Dacheng Tao", "authors": "Tianyi Zhou and Dacheng Tao", "title": "Unmixing Incoherent Structures of Big Data by Randomized or Greedy\n  Decomposition", "comments": "42 pages, 5 figures, 4 tables, 5 algorithms", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.DS cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  Learning big data by matrix decomposition always suffers from expensive\ncomputation, mixing of complicated structures and noise. In this paper, we\nstudy more adaptive models and efficient algorithms that decompose a data\nmatrix as the sum of semantic components with incoherent structures. We firstly\nintroduce \"GO decomposition (GoDec)\", an alternating projection method\nestimating the low-rank part $L$ and the sparse part $S$ from data matrix\n$X=L+S+G$ corrupted by noise $G$. Two acceleration strategies are proposed to\nobtain scalable unmixing algorithm on big data: 1) Bilateral random projection\n(BRP) is developed to speed up the update of $L$ in GoDec by a closed-form\nbuilt from left and right random projections of $X-S$ in lower dimensions; 2)\nGreedy bilateral (GreB) paradigm updates the left and right factors of $L$ in a\nmutually adaptive and greedy incremental manner, and achieve significant\nimprovement in both time and sample complexities. Then we proposes three\nnontrivial variants of GoDec that generalizes GoDec to more general data type\nand whose fast algorithms can be derived from the two strategies......\n", "versions": [{"version": "v1", "created": "Mon, 2 Sep 2013 05:07:31 GMT"}], "update_date": "2013-09-03", "authors_parsed": [["Zhou", "Tianyi", ""], ["Tao", "Dacheng", ""]]}, {"id": "1309.0337", "submitter": "Neil Houlsby", "authors": "Neil Houlsby, Massimiliano Ciaramita", "title": "Scalable Probabilistic Entity-Topic Modeling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an LDA approach to entity disambiguation. Each topic is associated\nwith a Wikipedia article and topics generate either content words or entity\nmentions. Training such models is challenging because of the topic and\nvocabulary size, both in the millions. We tackle these problems using a novel\ndistributed inference and representation framework based on a parallel Gibbs\nsampler guided by the Wikipedia link graph, and pipelines of MapReduce allowing\nfast and memory-frugal processing of large datasets. We report state-of-the-art\nperformance on a public dataset.\n", "versions": [{"version": "v1", "created": "Mon, 2 Sep 2013 09:34:50 GMT"}], "update_date": "2013-09-03", "authors_parsed": [["Houlsby", "Neil", ""], ["Ciaramita", "Massimiliano", ""]]}, {"id": "1309.0489", "submitter": "Eric Heim", "authors": "Eric Heim (University of Pittsburgh), Hamed Valizadegan (NASA Ames\n  Research Center), and Milos Hauskrecht (University of Pittsburgh)", "title": "Relative Comparison Kernel Learning with Auxiliary Kernels", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work we consider the problem of learning a positive semidefinite\nkernel matrix from relative comparisons of the form: \"object A is more similar\nto object B than it is to C\", where comparisons are given by humans. Existing\nsolutions to this problem assume many comparisons are provided to learn a high\nquality kernel. However, this can be considered unrealistic for many real-world\ntasks since relative assessments require human input, which is often costly or\ndifficult to obtain. Because of this, only a limited number of these\ncomparisons may be provided. In this work, we explore methods for aiding the\nprocess of learning a kernel with the help of auxiliary kernels built from more\neasily extractable information regarding the relationships among objects. We\npropose a new kernel learning approach in which the target kernel is defined as\na conic combination of auxiliary kernels and a kernel whose elements are\nlearned directly. We formulate a convex optimization to solve for this target\nkernel that adds only minor overhead to methods that use no auxiliary\ninformation. Empirical results show that in the presence of few training\nrelative comparisons, our method can learn kernels that generalize to more\nout-of-sample comparisons than methods that do not utilize auxiliary\ninformation, as well as similar methods that learn metrics over objects.\n", "versions": [{"version": "v1", "created": "Mon, 2 Sep 2013 19:29:34 GMT"}, {"version": "v2", "created": "Tue, 12 Nov 2013 16:12:29 GMT"}, {"version": "v3", "created": "Tue, 15 Apr 2014 20:32:08 GMT"}], "update_date": "2014-04-17", "authors_parsed": [["Heim", "Eric", "", "University of Pittsburgh"], ["Valizadegan", "Hamed", "", "NASA Ames\n  Research Center"], ["Hauskrecht", "Milos", "", "University of Pittsburgh"]]}, {"id": "1309.0671", "submitter": "Ruben Martinez-Cantin", "authors": "Ruben Martinez-Cantin", "title": "BayesOpt: A Library for Bayesian optimization with Robotics Applications", "comments": "Robotics: Science and Systems, Workshop on Active Learning in\n  Robotics: Exploration, Curiosity, and Interaction", "journal-ref": "Journal of Machine Learning Research, 15(Nov), 3915-3919, 2014", "doi": null, "report-no": null, "categories": "cs.RO cs.AI cs.LG cs.MS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The purpose of this paper is twofold. On one side, we present a general\nframework for Bayesian optimization and we compare it with some related fields\nin active learning and Bayesian numerical analysis. On the other hand, Bayesian\noptimization and related problems (bandits, sequential experimental design) are\nhighly dependent on the surrogate model that is selected. However, there is no\nclear standard in the literature. Thus, we present a fast and flexible toolbox\nthat allows to test and combine different models and criteria with little\neffort. It includes most of the state-of-the-art contributions, algorithms and\nmodels. Its speed also removes part of the stigma that Bayesian optimization\nmethods are only good for \"expensive functions\". The software is free and it\ncan be used in many operating systems and computer languages.\n", "versions": [{"version": "v1", "created": "Tue, 3 Sep 2013 13:38:05 GMT"}], "update_date": "2018-02-13", "authors_parsed": [["Martinez-Cantin", "Ruben", ""]]}, {"id": "1309.0787", "submitter": "Furong Huang", "authors": "Furong Huang, U. N. Niranjan, Mohammad Umar Hakeem, Animashree\n  Anandkumar", "title": "Online Tensor Methods for Learning Latent Variable Models", "comments": "JMLR 2014", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DC cs.SI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce an online tensor decomposition based approach for two latent\nvariable modeling problems namely, (1) community detection, in which we learn\nthe latent communities that the social actors in social networks belong to, and\n(2) topic modeling, in which we infer hidden topics of text articles. We\nconsider decomposition of moment tensors using stochastic gradient descent. We\nconduct optimization of multilinear operations in SGD and avoid directly\nforming the tensors, to save computational and storage costs. We present\noptimized algorithm in two platforms. Our GPU-based implementation exploits the\nparallelism of SIMD architectures to allow for maximum speed-up by a careful\noptimization of storage and data transfer, whereas our CPU-based implementation\nuses efficient sparse matrix computations and is suitable for large sparse\ndatasets. For the community detection problem, we demonstrate accuracy and\ncomputational efficiency on Facebook, Yelp and DBLP datasets, and for the topic\nmodeling problem, we also demonstrate good performance on the New York Times\ndataset. We compare our results to the state-of-the-art algorithms such as the\nvariational method, and report a gain of accuracy and a gain of several orders\nof magnitude in the execution time.\n", "versions": [{"version": "v1", "created": "Tue, 3 Sep 2013 19:30:55 GMT"}, {"version": "v2", "created": "Tue, 10 Sep 2013 20:56:08 GMT"}, {"version": "v3", "created": "Wed, 16 Oct 2013 01:58:14 GMT"}, {"version": "v4", "created": "Mon, 31 Mar 2014 17:24:07 GMT"}, {"version": "v5", "created": "Sat, 3 Oct 2015 04:26:19 GMT"}], "update_date": "2015-10-06", "authors_parsed": [["Huang", "Furong", ""], ["Niranjan", "U. N.", ""], ["Hakeem", "Mohammad Umar", ""], ["Anandkumar", "Animashree", ""]]}, {"id": "1309.0790", "submitter": "Philip Graff", "authors": "Philip Graff, Farhan Feroz, Michael P. Hobson, Anthony N. Lasenby", "title": "SKYNET: an efficient and robust neural network training tool for machine\n  learning in astronomy", "comments": "19 pages, 21 figures, 7 tables; this version is re-submission to\n  MNRAS in response to referee comments; software available at\n  http://www.mrao.cam.ac.uk/software/skynet/", "journal-ref": null, "doi": "10.1093/mnras/stu642", "report-no": null, "categories": "astro-ph.IM cs.LG cs.NE physics.data-an stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present the first public release of our generic neural network training\nalgorithm, called SkyNet. This efficient and robust machine learning tool is\nable to train large and deep feed-forward neural networks, including\nautoencoders, for use in a wide range of supervised and unsupervised learning\napplications, such as regression, classification, density estimation,\nclustering and dimensionality reduction. SkyNet uses a `pre-training' method to\nobtain a set of network parameters that has empirically been shown to be close\nto a good solution, followed by further optimisation using a regularised\nvariant of Newton's method, where the level of regularisation is determined and\nadjusted automatically; the latter uses second-order derivative information to\nimprove convergence, but without the need to evaluate or store the full Hessian\nmatrix, by using a fast approximate method to calculate Hessian-vector\nproducts. This combination of methods allows for the training of complicated\nnetworks that are difficult to optimise using standard backpropagation\ntechniques. SkyNet employs convergence criteria that naturally prevent\noverfitting, and also includes a fast algorithm for estimating the accuracy of\nnetwork outputs. The utility and flexibility of SkyNet are demonstrated by\napplication to a number of toy problems, and to astronomical problems focusing\non the recovery of structure from blurred and noisy images, the identification\nof gamma-ray bursters, and the compression and denoising of galaxy images. The\nSkyNet software, which is implemented in standard ANSI C and fully parallelised\nusing MPI, is available at http://www.mrao.cam.ac.uk/software/skynet/.\n", "versions": [{"version": "v1", "created": "Tue, 3 Sep 2013 19:33:28 GMT"}, {"version": "v2", "created": "Mon, 27 Jan 2014 19:23:30 GMT"}], "update_date": "2015-06-17", "authors_parsed": [["Graff", "Philip", ""], ["Feroz", "Farhan", ""], ["Hobson", "Michael P.", ""], ["Lasenby", "Anthony N.", ""]]}, {"id": "1309.0866", "submitter": "EPTCS", "authors": "Ezio Bartocci (TU Wien, Austria), Luca Bortolussi (University of\n  Trieste, Italy), Laura Nenzi (IMT Lucca, Italy), Guido Sanguinetti\n  (University of Edinburgh, UK)", "title": "On the Robustness of Temporal Properties for Stochastic Models", "comments": "In Proceedings HSB 2013, arXiv:1308.5724", "journal-ref": "EPTCS 125, 2013, pp. 3-19", "doi": "10.4204/EPTCS.125.1", "report-no": null, "categories": "cs.LO cs.AI cs.LG cs.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Stochastic models such as Continuous-Time Markov Chains (CTMC) and Stochastic\nHybrid Automata (SHA) are powerful formalisms to model and to reason about the\ndynamics of biological systems, due to their ability to capture the\nstochasticity inherent in biological processes. A classical question in formal\nmodelling with clear relevance to biological modelling is the model checking\nproblem. i.e. calculate the probability that a behaviour, expressed for\ninstance in terms of a certain temporal logic formula, may occur in a given\nstochastic process. However, one may not only be interested in the notion of\nsatisfiability, but also in the capacity of a system to mantain a particular\nemergent behaviour unaffected by the perturbations, caused e.g. from extrinsic\nnoise, or by possible small changes in the model parameters. To address this\nissue, researchers from the verification community have recently proposed\nseveral notions of robustness for temporal logic providing suitable definitions\nof distance between a trajectory of a (deterministic) dynamical system and the\nboundaries of the set of trajectories satisfying the property of interest. The\ncontributions of this paper are twofold. First, we extend the notion of\nrobustness to stochastic systems, showing that this naturally leads to a\ndistribution of robustness scores. By discussing two examples, we show how to\napproximate the distribution of the robustness score and its key indicators:\nthe average robustness and the conditional average robustness. Secondly, we\nshow how to combine these indicators with the satisfaction probability to\naddress the system design problem, where the goal is to optimize some control\nparameters of a stochastic model in order to best maximize robustness of the\ndesired specifications.\n", "versions": [{"version": "v1", "created": "Tue, 3 Sep 2013 23:40:49 GMT"}], "update_date": "2013-09-05", "authors_parsed": [["Bartocci", "Ezio", "", "TU Wien, Austria"], ["Bortolussi", "Luca", "", "University of\n  Trieste, Italy"], ["Nenzi", "Laura", "", "IMT Lucca, Italy"], ["Sanguinetti", "Guido", "", "University of Edinburgh, UK"]]}, {"id": "1309.1007", "submitter": "Aryeh Kontorovich", "authors": "Aryeh Kontorovich", "title": "Concentration in unbounded metric spaces and algorithmic stability", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR cs.LG math.FA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We prove an extension of McDiarmid's inequality for metric spaces with\nunbounded diameter. To this end, we introduce the notion of the {\\em\nsubgaussian diameter}, which is a distribution-dependent refinement of the\nmetric diameter. Our technique provides an alternative approach to that of\nKutin and Niyogi's method of weakly difference-bounded functions, and yields\nnontrivial, dimension-free results in some interesting cases where the former\ndoes not. As an application, we give apparently the first generalization bound\nin the algorithmic stability setting that holds for unbounded loss functions.\nWe furthermore extend our concentration inequality to strongly mixing\nprocesses.\n", "versions": [{"version": "v1", "created": "Wed, 4 Sep 2013 12:40:31 GMT"}, {"version": "v2", "created": "Wed, 11 Sep 2013 16:24:52 GMT"}], "update_date": "2013-09-12", "authors_parsed": [["Kontorovich", "Aryeh", ""]]}, {"id": "1309.1193", "submitter": "Evgenia Chunikhina", "authors": "E. Chunikhina, R. Raich, and T. Nguyen", "title": "Confidence-constrained joint sparsity recovery under the Poisson noise\n  model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Our work is focused on the joint sparsity recovery problem where the common\nsparsity pattern is corrupted by Poisson noise. We formulate the\nconfidence-constrained optimization problem in both least squares (LS) and\nmaximum likelihood (ML) frameworks and study the conditions for perfect\nreconstruction of the original row sparsity and row sparsity pattern. However,\nthe confidence-constrained optimization problem is non-convex. Using convex\nrelaxation, an alternative convex reformulation of the problem is proposed. We\nevaluate the performance of the proposed approach using simulation results on\nsynthetic data and show the effectiveness of proposed row sparsity and row\nsparsity pattern recovery framework.\n", "versions": [{"version": "v1", "created": "Wed, 4 Sep 2013 21:46:55 GMT"}, {"version": "v2", "created": "Wed, 9 Oct 2013 17:59:10 GMT"}], "update_date": "2013-10-10", "authors_parsed": [["Chunikhina", "E.", ""], ["Raich", "R.", ""], ["Nguyen", "T.", ""]]}, {"id": "1309.1369", "submitter": "Aleksandr Aravkin", "authors": "Aleksandr Y. Aravkin, Anna Choromanska, Tony Jebara, and Dimitri\n  Kanevsky", "title": "Semistochastic Quadratic Bound Methods", "comments": "11 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.NA stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Partition functions arise in a variety of settings, including conditional\nrandom fields, logistic regression, and latent gaussian models. In this paper,\nwe consider semistochastic quadratic bound (SQB) methods for maximum likelihood\ninference based on partition function optimization. Batch methods based on the\nquadratic bound were recently proposed for this class of problems, and\nperformed favorably in comparison to state-of-the-art techniques.\nSemistochastic methods fall in between batch algorithms, which use all the\ndata, and stochastic gradient type methods, which use small random selections\nat each iteration. We build semistochastic quadratic bound-based methods, and\nprove both global convergence (to a stationary point) under very weak\nassumptions, and linear convergence rate under stronger assumptions on the\nobjective. To make the proposed methods faster and more stable, we consider\ninexact subproblem minimization and batch-size selection schemes. The efficacy\nof SQB methods is demonstrated via comparison with several state-of-the-art\ntechniques on commonly used datasets.\n", "versions": [{"version": "v1", "created": "Thu, 5 Sep 2013 15:12:11 GMT"}, {"version": "v2", "created": "Sat, 21 Dec 2013 02:42:50 GMT"}, {"version": "v3", "created": "Sat, 25 Jan 2014 21:00:34 GMT"}, {"version": "v4", "created": "Mon, 17 Feb 2014 22:18:34 GMT"}], "update_date": "2014-02-19", "authors_parsed": [["Aravkin", "Aleksandr Y.", ""], ["Choromanska", "Anna", ""], ["Jebara", "Tony", ""], ["Kanevsky", "Dimitri", ""]]}, {"id": "1309.1392", "submitter": "James P. Crutchfield", "authors": "Christopher C. Strelioff and James P. Crutchfield", "title": "Bayesian Structural Inference for Hidden Processes", "comments": "20 pages, 11 figures, 1 table; supplementary materials, 15 pages, 11\n  figures, 6 tables; http://csc.ucdavis.edu/~cmg/compmech/pubs/bsihp.htm", "journal-ref": "Phys. Rev. E 89, 042119 (2014)", "doi": "10.1103/PhysRevE.89.042119", "report-no": "Santa Fe Institute Working Paper 13-09-027", "categories": "stat.ML cs.LG math.ST nlin.CD physics.data-an stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a Bayesian approach to discovering patterns in structurally\ncomplex processes. The proposed method of Bayesian Structural Inference (BSI)\nrelies on a set of candidate unifilar HMM (uHMM) topologies for inference of\nprocess structure from a data series. We employ a recently developed exact\nenumeration of topological epsilon-machines. (A sequel then removes the\ntopological restriction.) This subset of the uHMM topologies has the added\nbenefit that inferred models are guaranteed to be epsilon-machines,\nirrespective of estimated transition probabilities. Properties of\nepsilon-machines and uHMMs allow for the derivation of analytic expressions for\nestimating transition probabilities, inferring start states, and comparing the\nposterior probability of candidate model topologies, despite process internal\nstructure being only indirectly present in data. We demonstrate BSI's\neffectiveness in estimating a process's randomness, as reflected by the Shannon\nentropy rate, and its structure, as quantified by the statistical complexity.\nWe also compare using the posterior distribution over candidate models and the\nsingle, maximum a posteriori model for point estimation and show that the\nformer more accurately reflects uncertainty in estimated values. We apply BSI\nto in-class examples of finite- and infinite-order Markov processes, as well to\nan out-of-class, infinite-state hidden process.\n", "versions": [{"version": "v1", "created": "Thu, 5 Sep 2013 16:18:35 GMT"}, {"version": "v2", "created": "Mon, 9 Dec 2013 05:21:31 GMT"}], "update_date": "2014-04-23", "authors_parsed": [["Strelioff", "Christopher C.", ""], ["Crutchfield", "James P.", ""]]}, {"id": "1309.1501", "submitter": "Aleksandr Aravkin", "authors": "Tara N. Sainath, Brian Kingsbury, Abdel-rahman Mohamed, George E.\n  Dahl, George Saon, Hagen Soltau, Tomas Beran, Aleksandr Y. Aravkin, Bhuvana\n  Ramabhadran", "title": "Improvements to deep convolutional neural networks for LVCSR", "comments": "6 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL cs.NE math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep Convolutional Neural Networks (CNNs) are more powerful than Deep Neural\nNetworks (DNN), as they are able to better reduce spectral variation in the\ninput signal. This has also been confirmed experimentally, with CNNs showing\nimprovements in word error rate (WER) between 4-12% relative compared to DNNs\nacross a variety of LVCSR tasks. In this paper, we describe different methods\nto further improve CNN performance. First, we conduct a deep analysis comparing\nlimited weight sharing and full weight sharing with state-of-the-art features.\nSecond, we apply various pooling strategies that have shown improvements in\ncomputer vision to an LVCSR speech task. Third, we introduce a method to\neffectively incorporate speaker adaptation, namely fMLLR, into log-mel\nfeatures. Fourth, we introduce an effective strategy to use dropout during\nHessian-free sequence training. We find that with these improvements,\nparticularly with fMLLR and dropout, we are able to achieve an additional 2-3%\nrelative improvement in WER on a 50-hour Broadcast News task over our previous\nbest CNN baseline. On a larger 400-hour BN task, we find an additional 4-5%\nrelative improvement over our previous best CNN baseline.\n", "versions": [{"version": "v1", "created": "Thu, 5 Sep 2013 22:06:58 GMT"}, {"version": "v2", "created": "Wed, 11 Sep 2013 14:33:09 GMT"}, {"version": "v3", "created": "Tue, 10 Dec 2013 11:51:39 GMT"}], "update_date": "2013-12-11", "authors_parsed": [["Sainath", "Tara N.", ""], ["Kingsbury", "Brian", ""], ["Mohamed", "Abdel-rahman", ""], ["Dahl", "George E.", ""], ["Saon", "George", ""], ["Soltau", "Hagen", ""], ["Beran", "Tomas", ""], ["Aravkin", "Aleksandr Y.", ""], ["Ramabhadran", "Bhuvana", ""]]}, {"id": "1309.1508", "submitter": "Aleksandr Aravkin", "authors": "Tara N. Sainath, Lior Horesh, Brian Kingsbury, Aleksandr Y. Aravkin,\n  Bhuvana Ramabhadran", "title": "Accelerating Hessian-free optimization for deep neural networks by\n  implicit preconditioning and sampling", "comments": "this paper is not supposed to be posted publically before the\n  conference in December due to company policy. another co-author was not\n  informed of this and posted without the permission of the first author. pls\n  remove", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL cs.NE math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hessian-free training has become a popular parallel second or- der\noptimization technique for Deep Neural Network training. This study aims at\nspeeding up Hessian-free training, both by means of decreasing the amount of\ndata used for training, as well as through reduction of the number of Krylov\nsubspace solver iterations used for implicit estimation of the Hessian. In this\npaper, we develop an L-BFGS based preconditioning scheme that avoids the need\nto access the Hessian explicitly. Since L-BFGS cannot be regarded as a\nfixed-point iteration, we further propose the employment of flexible Krylov\nsubspace solvers that retain the desired theoretical convergence guarantees of\ntheir conventional counterparts. Second, we propose a new sampling algorithm,\nwhich geometrically increases the amount of data utilized for gradient and\nKrylov subspace iteration calculations. On a 50-hr English Broadcast News task,\nwe find that these methodologies provide roughly a 1.5x speed-up, whereas, on a\n300-hr Switchboard task, these techniques provide over a 2.3x speedup, with no\nloss in WER. These results suggest that even further speed-up is expected, as\nproblems scale and complexity grows.\n", "versions": [{"version": "v1", "created": "Thu, 5 Sep 2013 23:21:02 GMT"}, {"version": "v2", "created": "Wed, 11 Sep 2013 14:34:31 GMT"}, {"version": "v3", "created": "Tue, 10 Dec 2013 12:05:51 GMT"}], "update_date": "2013-12-11", "authors_parsed": [["Sainath", "Tara N.", ""], ["Horesh", "Lior", ""], ["Kingsbury", "Brian", ""], ["Aravkin", "Aleksandr Y.", ""], ["Ramabhadran", "Bhuvana", ""]]}, {"id": "1309.1541", "submitter": "Weiran Wang", "authors": "Weiran Wang, Miguel \\'A. Carreira-Perpi\\~n\\'an", "title": "Projection onto the probability simplex: An efficient algorithm with a\n  simple proof, and an application", "comments": "5 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We provide an elementary proof of a simple, efficient algorithm for computing\nthe Euclidean projection of a point onto the probability simplex. We also show\nan application in Laplacian K-modes clustering.\n", "versions": [{"version": "v1", "created": "Fri, 6 Sep 2013 05:48:40 GMT"}], "update_date": "2013-09-09", "authors_parsed": [["Wang", "Weiran", ""], ["Carreira-Perpi\u00f1\u00e1n", "Miguel \u00c1.", ""]]}, {"id": "1309.1543", "submitter": "Oluwagbenga Ogunduyile O", "authors": "O.A. Randle, O. O. Ogunduyile, T. Zuva, N. A. Fashola", "title": "A Comparism of the Performance of Supervised and Unsupervised Machine\n  Learning Techniques in evolving Awale/Mancala/Ayo Game Player", "comments": "10 pages, 2 figures", "journal-ref": "International Journal of Game Theory and Technology (IJGTT),\n  Vol.1, No.1, June 2013", "doi": null, "report-no": null, "categories": "cs.LG cs.GT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Awale games have become widely recognized across the world, for their\ninnovative strategies and techniques which were used in evolving the agents\n(player) and have produced interesting results under various conditions. This\npaper will compare the results of the two major machine learning techniques by\nreviewing their performance when using minimax, endgame database, a combination\nof both techniques or other techniques, and will determine which are the best\ntechniques.\n", "versions": [{"version": "v1", "created": "Fri, 6 Sep 2013 06:06:15 GMT"}], "update_date": "2013-09-09", "authors_parsed": [["Randle", "O. A.", ""], ["Ogunduyile", "O. O.", ""], ["Zuva", "T.", ""], ["Fashola", "N. A.", ""]]}, {"id": "1309.1761", "submitter": "Shaun Joseph", "authors": "Shaun N. Joseph and Seif Omar Abu Bakr and Gabriel Lugo", "title": "Convergence of Nearest Neighbor Pattern Classification with Selective\n  Sampling", "comments": "18 pages, 2 figures, mZeal Communications Technical Report", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the panoply of pattern classification techniques, few enjoy the intuitive\nappeal and simplicity of the nearest neighbor rule: given a set of samples in\nsome metric domain space whose value under some function is known, we estimate\nthe function anywhere in the domain by giving the value of the nearest sample\nper the metric. More generally, one may use the modal value of the m nearest\nsamples, where m is a fixed positive integer (although m=1 is known to be\nadmissible in the sense that no larger value is asymptotically superior in\nterms of prediction error). The nearest neighbor rule is nonparametric and\nextremely general, requiring in principle only that the domain be a metric\nspace. The classic paper on the technique, proving convergence under\nindependent, identically-distributed (iid) sampling, is due to Cover and Hart\n(1967). Because taking samples is costly, there has been much research in\nrecent years on selective sampling, in which each sample is selected from a\npool of candidates ranked by a heuristic; the heuristic tries to guess which\ncandidate would be the most \"informative\" sample. Lindenbaum et al. (2004)\napply selective sampling to the nearest neighbor rule, but their approach\nsacrifices the austere generality of Cover and Hart; furthermore, their\nheuristic algorithm is complex and computationally expensive. Here we report\nrecent results that enable selective sampling in the original Cover-Hart\nsetting. Our results pose three selection heuristics and prove that their\nnearest neighbor rule predictions converge to the true pattern. Two of the\nalgorithms are computationally cheap, with complexity growing linearly in the\nnumber of samples. We believe that these results constitute an important\nadvance in the art.\n", "versions": [{"version": "v1", "created": "Fri, 6 Sep 2013 18:52:16 GMT"}], "update_date": "2013-09-10", "authors_parsed": [["Joseph", "Shaun N.", ""], ["Bakr", "Seif Omar Abu", ""], ["Lugo", "Gabriel", ""]]}, {"id": "1309.1853", "submitter": "Chunhua Shen", "authors": "Guosheng Lin, Chunhua Shen, David Suter, Anton van den Hengel", "title": "A General Two-Step Approach to Learning-Based Hashing", "comments": "13 pages. Appearing in Int. Conf. Computer Vision (ICCV) 2013", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most existing approaches to hashing apply a single form of hash function, and\nan optimization process which is typically deeply coupled to this specific\nform. This tight coupling restricts the flexibility of the method to respond to\nthe data, and can result in complex optimization problems that are difficult to\nsolve. Here we propose a flexible yet simple framework that is able to\naccommodate different types of loss functions and hash functions. This\nframework allows a number of existing approaches to hashing to be placed in\ncontext, and simplifies the development of new problem-specific hashing\nmethods. Our framework decomposes hashing learning problem into two steps: hash\nbit learning and hash function learning based on the learned bits. The first\nstep can typically be formulated as binary quadratic problems, and the second\nstep can be accomplished by training standard binary classifiers. Both problems\nhave been extensively studied in the literature. Our extensive experiments\ndemonstrate that the proposed framework is effective, flexible and outperforms\nthe state-of-the-art.\n", "versions": [{"version": "v1", "created": "Sat, 7 Sep 2013 11:33:36 GMT"}], "update_date": "2013-09-10", "authors_parsed": [["Lin", "Guosheng", ""], ["Shen", "Chunhua", ""], ["Suter", "David", ""], ["Hengel", "Anton van den", ""]]}, {"id": "1309.1952", "submitter": "Alekh Agarwal", "authors": "Alekh Agarwal and Animashree Anandkumar and Praneeth Netrapalli", "title": "A Clustering Approach to Learn Sparsely-Used Overcomplete Dictionaries", "comments": "Part of this work appears in COLT 2014", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of learning overcomplete dictionaries in the context\nof sparse coding, where each sample selects a sparse subset of dictionary\nelements. Our main result is a strategy to approximately recover the unknown\ndictionary using an efficient algorithm. Our algorithm is a clustering-style\nprocedure, where each cluster is used to estimate a dictionary element. The\nresulting solution can often be further cleaned up to obtain a high accuracy\nestimate, and we provide one simple scenario where $\\ell_1$-regularized\nregression can be used for such a second stage.\n", "versions": [{"version": "v1", "created": "Sun, 8 Sep 2013 12:55:39 GMT"}, {"version": "v2", "created": "Mon, 7 Jul 2014 05:10:23 GMT"}], "update_date": "2014-07-08", "authors_parsed": [["Agarwal", "Alekh", ""], ["Anandkumar", "Animashree", ""], ["Netrapalli", "Praneeth", ""]]}, {"id": "1309.2074", "submitter": "Qiang Qiu", "authors": "Qiang Qiu, Guillermo Sapiro", "title": "Learning Transformations for Clustering and Classification", "comments": "arXiv admin note: substantial text overlap with arXiv:1308.0273,\n  arXiv:1308.0275", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A low-rank transformation learning framework for subspace clustering and\nclassification is here proposed. Many high-dimensional data, such as face\nimages and motion sequences, approximately lie in a union of low-dimensional\nsubspaces. The corresponding subspace clustering problem has been extensively\nstudied in the literature to partition such high-dimensional data into clusters\ncorresponding to their underlying low-dimensional subspaces. However,\nlow-dimensional intrinsic structures are often violated for real-world\nobservations, as they can be corrupted by errors or deviate from ideal models.\nWe propose to address this by learning a linear transformation on subspaces\nusing matrix rank, via its convex surrogate nuclear norm, as the optimization\ncriteria. The learned linear transformation restores a low-rank structure for\ndata from the same subspace, and, at the same time, forces a a maximally\nseparated structure for data from different subspaces. In this way, we reduce\nvariations within subspaces, and increase separation between subspaces for a\nmore robust subspace clustering. This proposed learned robust subspace\nclustering framework significantly enhances the performance of existing\nsubspace clustering methods. Basic theoretical results here presented help to\nfurther support the underlying framework. To exploit the low-rank structures of\nthe transformed subspaces, we further introduce a fast subspace clustering\ntechnique, which efficiently combines robust PCA with sparse modeling. When\nclass labels are present at the training stage, we show this low-rank\ntransformation framework also significantly enhances classification\nperformance. Extensive experiments using public datasets are presented, showing\nthat the proposed approach significantly outperforms state-of-the-art methods\nfor subspace clustering and classification.\n", "versions": [{"version": "v1", "created": "Mon, 9 Sep 2013 09:16:02 GMT"}, {"version": "v2", "created": "Sun, 9 Mar 2014 18:50:35 GMT"}], "update_date": "2014-03-11", "authors_parsed": [["Qiu", "Qiang", ""], ["Sapiro", "Guillermo", ""]]}, {"id": "1309.2080", "submitter": "Elena Bellodi", "authors": "Elena Bellodi, Fabrizio Riguzzi", "title": "Structure Learning of Probabilistic Logic Programs by Searching the\n  Clause Space", "comments": "44 pages, 12 figures", "journal-ref": "Theory and Practice of Logic Programming 15 (2015) 169-212", "doi": "10.1017/S1471068413000689", "report-no": null, "categories": "cs.LG cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning probabilistic logic programming languages is receiving an increasing\nattention and systems are available for learning the parameters (PRISM,\nLeProbLog, LFI-ProbLog and EMBLEM) or both the structure and the parameters\n(SEM-CP-logic and SLIPCASE) of these languages. In this paper we present the\nalgorithm SLIPCOVER for \"Structure LearnIng of Probabilistic logic programs by\nsearChing OVER the clause space\". It performs a beam search in the space of\nprobabilistic clauses and a greedy search in the space of theories, using the\nlog likelihood of the data as the guiding heuristics. To estimate the log\nlikelihood SLIPCOVER performs Expectation Maximization with EMBLEM. The\nalgorithm has been tested on five real world datasets and compared with\nSLIPCASE, SEM-CP-logic, Aleph and two algorithms for learning Markov Logic\nNetworks (Learning using Structural Motifs (LSM) and ALEPH++ExactL1). SLIPCOVER\nachieves higher areas under the precision-recall and ROC curves in most cases.\n", "versions": [{"version": "v1", "created": "Mon, 9 Sep 2013 09:24:44 GMT"}], "update_date": "2020-02-19", "authors_parsed": [["Bellodi", "Elena", ""], ["Riguzzi", "Fabrizio", ""]]}, {"id": "1309.2168", "submitter": "Pablo Gonz\\'alez-Brevis Dr", "authors": "Jacek Gondzio, Pablo Gonz\\'alez-Brevis and Pedro Munari", "title": "Large-scale optimization with the primal-dual column generation method", "comments": "28 pages, 1 figure, minor revision, scaled CPU times", "journal-ref": null, "doi": null, "report-no": "Technical Report ERGO 13-014", "categories": "math.OC cs.LG cs.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The primal-dual column generation method (PDCGM) is a general-purpose column\ngeneration technique that relies on the primal-dual interior point method to\nsolve the restricted master problems. The use of this interior point method\nvariant allows to obtain suboptimal and well-centered dual solutions which\nnaturally stabilizes the column generation. As recently presented in the\nliterature, reductions in the number of calls to the oracle and in the CPU\ntimes are typically observed when compared to the standard column generation,\nwhich relies on extreme optimal dual solutions. However, these results are\nbased on relatively small problems obtained from linear relaxations of\ncombinatorial applications. In this paper, we investigate the behaviour of the\nPDCGM in a broader context, namely when solving large-scale convex optimization\nproblems. We have selected applications that arise in important real-life\ncontexts such as data analysis (multiple kernel learning problem),\ndecision-making under uncertainty (two-stage stochastic programming problems)\nand telecommunication and transportation networks (multicommodity network flow\nproblem). In the numerical experiments, we use publicly available benchmark\ninstances to compare the performance of the PDCGM against recent results for\ndifferent methods presented in the literature, which were the best available\nresults to date. The analysis of these results suggests that the PDCGM offers\nan attractive alternative over specialized methods since it remains competitive\nin terms of number of iterations and CPU times even for large-scale\noptimization problems.\n", "versions": [{"version": "v1", "created": "Mon, 9 Sep 2013 14:19:10 GMT"}, {"version": "v2", "created": "Mon, 16 Feb 2015 17:40:28 GMT"}], "update_date": "2015-02-17", "authors_parsed": [["Gondzio", "Jacek", ""], ["Gonz\u00e1lez-Brevis", "Pablo", ""], ["Munari", "Pedro", ""]]}, {"id": "1309.2350", "submitter": "Shahin Shahrampour", "authors": "Shahin Shahrampour and Ali Jadbabaie", "title": "Exponentially Fast Parameter Estimation in Networks Using Distributed\n  Dual Averaging", "comments": "6 pages, To appear in Conference on Decision and Control 2013", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.SI math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we present an optimization-based view of distributed parameter\nestimation and observational social learning in networks. Agents receive a\nsequence of random, independent and identically distributed (i.i.d.) signals,\neach of which individually may not be informative about the underlying true\nstate, but the signals together are globally informative enough to make the\ntrue state identifiable. Using an optimization-based characterization of\nBayesian learning as proximal stochastic gradient descent (with\nKullback-Leibler divergence from a prior as a proximal function), we show how\nto efficiently use a distributed, online variant of Nesterov's dual averaging\nmethod to solve the estimation with purely local information. When the true\nstate is globally identifiable, and the network is connected, we prove that\nagents eventually learn the true parameter using a randomized gossip scheme. We\ndemonstrate that with high probability the convergence is exponentially fast\nwith a rate dependent on the KL divergence of observations under the true state\nfrom observations under the second likeliest state. Furthermore, our work also\nhighlights the possibility of learning under continuous adaptation of network\nwhich is a consequence of employing constant, unit stepsize for the algorithm.\n", "versions": [{"version": "v1", "created": "Tue, 10 Sep 2013 00:36:44 GMT"}], "update_date": "2013-09-11", "authors_parsed": [["Shahrampour", "Shahin", ""], ["Jadbabaie", "Ali", ""]]}, {"id": "1309.2375", "submitter": "Shai Shalev-Shwartz", "authors": "Shai Shalev-Shwartz and Tong Zhang", "title": "Accelerated Proximal Stochastic Dual Coordinate Ascent for Regularized\n  Loss Minimization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG cs.NA stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a proximal version of the stochastic dual coordinate ascent\nmethod and show how to accelerate the method using an inner-outer iteration\nprocedure. We analyze the runtime of the framework and obtain rates that\nimprove state-of-the-art results for various key machine learning optimization\nproblems including SVM, logistic regression, ridge regression, Lasso, and\nmulticlass SVM. Experiments validate our theoretical findings.\n", "versions": [{"version": "v1", "created": "Tue, 10 Sep 2013 05:39:25 GMT"}, {"version": "v2", "created": "Tue, 8 Oct 2013 06:06:09 GMT"}], "update_date": "2013-10-09", "authors_parsed": [["Shalev-Shwartz", "Shai", ""], ["Zhang", "Tong", ""]]}, {"id": "1309.2388", "submitter": "Mark Schmidt", "authors": "Mark Schmidt (SIERRA, LIENS), Nicolas Le Roux (SIERRA, LIENS), Francis\n  Bach (SIERRA, LIENS)", "title": "Minimizing Finite Sums with the Stochastic Average Gradient", "comments": "Revision from January 2015 submission. Major changes: updated\n  literature follow and discussion of subsequent work, additional Lemma showing\n  the validity of one of the formulas, somewhat simplified presentation of\n  Lyapunov bound, included code needed for checking proofs rather than the\n  polynomials generated by the code, added error regions to the numerical\n  experiments", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.LG stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose the stochastic average gradient (SAG) method for optimizing the\nsum of a finite number of smooth convex functions. Like stochastic gradient\n(SG) methods, the SAG method's iteration cost is independent of the number of\nterms in the sum. However, by incorporating a memory of previous gradient\nvalues the SAG method achieves a faster convergence rate than black-box SG\nmethods. The convergence rate is improved from O(1/k^{1/2}) to O(1/k) in\ngeneral, and when the sum is strongly-convex the convergence rate is improved\nfrom the sub-linear O(1/k) to a linear convergence rate of the form O(p^k) for\np \\textless{} 1. Further, in many cases the convergence rate of the new method\nis also faster than black-box deterministic gradient methods, in terms of the\nnumber of gradient evaluations. Numerical experiments indicate that the new\nalgorithm often dramatically outperforms existing SG and deterministic gradient\nmethods, and that the performance may be further improved through the use of\nnon-uniform sampling strategies.\n", "versions": [{"version": "v1", "created": "Tue, 10 Sep 2013 06:49:15 GMT"}, {"version": "v2", "created": "Wed, 11 May 2016 06:51:31 GMT"}], "update_date": "2016-05-12", "authors_parsed": [["Schmidt", "Mark", "", "SIERRA, LIENS"], ["Roux", "Nicolas Le", "", "SIERRA, LIENS"], ["Bach", "Francis", "", "SIERRA, LIENS"]]}, {"id": "1309.2593", "submitter": "K. S. Sesh Kumar", "authors": "K. S. Sesh Kumar (LIENS, INRIA Paris - Rocquencourt), Francis Bach\n  (LIENS, INRIA Paris - Rocquencourt)", "title": "Maximizing submodular functions using probabilistic graphical models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of maximizing submodular functions; while this\nproblem is known to be NP-hard, several numerically efficient local search\ntechniques with approximation guarantees are available. In this paper, we\npropose a novel convex relaxation which is based on the relationship between\nsubmodular functions, entropies and probabilistic graphical models. In a\ngraphical model, the entropy of the joint distribution decomposes as a sum of\nmarginal entropies of subsets of variables; moreover, for any distribution, the\nentropy of the closest distribution factorizing in the graphical model provides\nan bound on the entropy. For directed graphical models, this last property\nturns out to be a direct consequence of the submodularity of the entropy\nfunction, and allows the generalization of graphical-model-based upper bounds\nto any submodular functions. These upper bounds may then be jointly maximized\nwith respect to a set, while minimized with respect to the graph, leading to a\nconvex variational inference scheme for maximizing submodular functions, based\non outer approximations of the marginal polytope and maximum likelihood bounded\ntreewidth structures. By considering graphs of increasing treewidths, we may\nthen explore the trade-off between computational complexity and tightness of\nthe relaxation. We also present extensions to constrained problems and\nmaximizing the difference of submodular functions, which include all possible\nset functions.\n", "versions": [{"version": "v1", "created": "Tue, 10 Sep 2013 18:04:15 GMT"}], "update_date": "2013-09-11", "authors_parsed": [["Kumar", "K. S. Sesh", "", "LIENS, INRIA Paris - Rocquencourt"], ["Bach", "Francis", "", "LIENS, INRIA Paris - Rocquencourt"]]}, {"id": "1309.2765", "submitter": "Patoomsiri Songsiri Ms.", "authors": "Patoomsiri Songsiri, Thimaporn Phetkaew, Boonserm Kijsirikul", "title": "Enhancements of Multi-class Support Vector Machine Construction from\n  Binary Learners using Generalization Performance", "comments": "17 pages, 13 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  We propose several novel methods for enhancing the multi-class SVMs by\napplying the generalization performance of binary classifiers as the core idea.\nThis concept will be applied on the existing algorithms, i.e., the Decision\nDirected Acyclic Graph (DDAG), the Adaptive Directed Acyclic Graphs (ADAG), and\nMax Wins. Although in the previous approaches there have been many attempts to\nuse some information such as the margin size and the number of support vectors\nas performance estimators for binary SVMs, they may not accurately reflect the\nactual performance of the binary SVMs. We show that the generalization ability\nevaluated via a cross-validation mechanism is more suitable to directly extract\nthe actual performance of binary SVMs. Our methods are built around this\nperformance measure, and each of them is crafted to overcome the weakness of\nthe previous algorithm. The proposed methods include the Reordering Adaptive\nDirected Acyclic Graph (RADAG), Strong Elimination of the classifiers (SE),\nWeak Elimination of the classifiers (WE), and Voting based Candidate Filtering\n(VCF). Experimental results demonstrate that our methods give significantly\nhigher accuracy than all of the traditional ones. Especially, WE provides\nsignificantly superior results compared to Max Wins which is recognized as the\nstate of the art algorithm in terms of both accuracy and classification speed\nwith two times faster in average.\n", "versions": [{"version": "v1", "created": "Wed, 11 Sep 2013 08:59:07 GMT"}], "update_date": "2013-09-12", "authors_parsed": [["Songsiri", "Patoomsiri", ""], ["Phetkaew", "Thimaporn", ""], ["Kijsirikul", "Boonserm", ""]]}, {"id": "1309.2796", "submitter": "Ferdinando Cicalese", "authors": "Ferdinando Cicalese and Eduardo Laber and Aline Medeiros Saettler", "title": "Decision Trees for Function Evaluation - Simultaneous Optimization of\n  Worst and Expected Cost", "comments": "A preliminary version of this paper was accepted for presentation at\n  ICML 2014", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In several applications of automatic diagnosis and active learning a central\nproblem is the evaluation of a discrete function by adaptively querying the\nvalues of its variables until the values read uniquely determine the value of\nthe function. In general, the process of reading the value of a variable might\ninvolve some cost, computational or even a fee to be paid for the experiment\nrequired for obtaining the value. This cost should be taken into account when\ndeciding the next variable to read. The goal is to design a strategy for\nevaluating the function incurring little cost (in the worst case or in\nexpectation according to a prior distribution on the possible variables'\nassignments). Our algorithm builds a strategy (decision tree) which attains a\nlogarithmic approxima- tion simultaneously for the expected and worst cost\nspent. This is best possible under the assumption that $P \\neq NP.$\n", "versions": [{"version": "v1", "created": "Wed, 11 Sep 2013 11:50:44 GMT"}, {"version": "v2", "created": "Sat, 26 Jul 2014 15:42:05 GMT"}], "update_date": "2014-07-29", "authors_parsed": [["Cicalese", "Ferdinando", ""], ["Laber", "Eduardo", ""], ["Saettler", "Aline Medeiros", ""]]}, {"id": "1309.2848", "submitter": "Shabnam Kadir", "authors": "Shabnam N. Kadir, Dan F. M. Goodman, and Kenneth D. Harris", "title": "High-dimensional cluster analysis with the Masked EM Algorithm", "comments": "10 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.QM cs.LG q-bio.NC stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cluster analysis faces two problems in high dimensions: first, the `curse of\ndimensionality' that can lead to overfitting and poor generalization\nperformance; and second, the sheer time taken for conventional algorithms to\nprocess large amounts of high-dimensional data. In many applications, only a\nsmall subset of features provide information about the cluster membership of\nany one data point, however this informative feature subset may not be the same\nfor all data points. Here we introduce a `Masked EM' algorithm for fitting\nmixture of Gaussians models in such cases. We show that the algorithm performs\nclose to optimally on simulated Gaussian data, and in an application of `spike\nsorting' of high channel-count neuronal recordings.\n", "versions": [{"version": "v1", "created": "Wed, 11 Sep 2013 14:55:50 GMT"}], "update_date": "2013-09-12", "authors_parsed": [["Kadir", "Shabnam N.", ""], ["Goodman", "Dan F. M.", ""], ["Harris", "Kenneth D.", ""]]}, {"id": "1309.3103", "submitter": "Alex Susemihl", "authors": "Chris H\\\"ausler, Alex Susemihl, Martin P Nawrot, Manfred Opper", "title": "Temporal Autoencoding Improves Generative Models of Time Series", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Restricted Boltzmann Machines (RBMs) are generative models which can learn\nuseful representations from samples of a dataset in an unsupervised fashion.\nThey have been widely employed as an unsupervised pre-training method in\nmachine learning. RBMs have been modified to model time series in two main\nways: The Temporal RBM stacks a number of RBMs laterally and introduces\ntemporal dependencies between the hidden layer units; The Conditional RBM, on\nthe other hand, considers past samples of the dataset as a conditional bias and\nlearns a representation which takes these into account. Here we propose a new\ntraining method for both the TRBM and the CRBM, which enforces the dynamic\nstructure of temporal datasets. We do so by treating the temporal models as\ndenoising autoencoders, considering past frames of the dataset as corrupted\nversions of the present frame and minimizing the reconstruction error of the\npresent data by the model. We call this approach Temporal Autoencoding. This\nleads to a significant improvement in the performance of both models in a\nfilling-in-frames task across a number of datasets. The error reduction for\nmotion capture data is 56\\% for the CRBM and 80\\% for the TRBM. Taking the\nposterior mean prediction instead of single samples further improves the\nmodel's estimates, decreasing the error by as much as 91\\% for the CRBM on\nmotion capture data. We also trained the model to perform forecasting on a\nlarge number of datasets and have found TA pretraining to consistently improve\nthe performance of the forecasts. Furthermore, by looking at the prediction\nerror across time, we can see that this improvement reflects a better\nrepresentation of the dynamics of the data as opposed to a bias towards\nreconstructing the observed data on a short time scale.\n", "versions": [{"version": "v1", "created": "Thu, 12 Sep 2013 10:39:50 GMT"}], "update_date": "2013-09-13", "authors_parsed": [["H\u00e4usler", "Chris", ""], ["Susemihl", "Alex", ""], ["Nawrot", "Martin P", ""], ["Opper", "Manfred", ""]]}, {"id": "1309.3117", "submitter": "Francis Bach", "authors": "Francis Bach (INRIA Paris - Rocquencourt, LIENS)", "title": "Convex relaxations of structured matrix factorizations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the factorization of a rectangular matrix $X $ into a positive\nlinear combination of rank-one factors of the form $u v^\\top$, where $u$ and\n$v$ belongs to certain sets $\\mathcal{U}$ and $\\mathcal{V}$, that may encode\nspecific structures regarding the factors, such as positivity or sparsity. In\nthis paper, we show that computing the optimal decomposition is equivalent to\ncomputing a certain gauge function of $X$ and we provide a detailed analysis of\nthese gauge functions and their polars. Since these gauge functions are\ntypically hard to compute, we present semi-definite relaxations and several\nalgorithms that may recover approximate decompositions with approximation\nguarantees. We illustrate our results with simulations on finding\ndecompositions with elements in $\\{0,1\\}$. As side contributions, we present a\ndetailed analysis of variational quadratic representations of norms as well as\na new iterative basis pursuit algorithm that can deal with inexact first-order\noracles.\n", "versions": [{"version": "v1", "created": "Thu, 12 Sep 2013 11:28:12 GMT"}], "update_date": "2013-09-13", "authors_parsed": [["Bach", "Francis", "", "INRIA Paris - Rocquencourt, LIENS"]]}, {"id": "1309.3233", "submitter": "Franz J. Kir\\'aly", "authors": "Franz J. Kir\\'aly", "title": "Efficient Orthogonal Tensor Decomposition, with an Application to Latent\n  Variable Model Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Decomposing tensors into orthogonal factors is a well-known task in\nstatistics, machine learning, and signal processing. We study orthogonal outer\nproduct decompositions where the factors in the summands in the decomposition\nare required to be orthogonal across summands, by relating this orthogonal\ndecomposition to the singular value decompositions of the flattenings. We show\nthat it is a non-trivial assumption for a tensor to have such an orthogonal\ndecomposition, and we show that it is unique (up to natural symmetries) in case\nit exists, in which case we also demonstrate how it can be efficiently and\nreliably obtained by a sequence of singular value decompositions. We\ndemonstrate how the factoring algorithm can be applied for parameter\nidentification in latent variable and mixture models.\n", "versions": [{"version": "v1", "created": "Thu, 12 Sep 2013 18:23:33 GMT"}], "update_date": "2013-09-13", "authors_parsed": [["Kir\u00e1ly", "Franz J.", ""]]}, {"id": "1309.3256", "submitter": "Rachel Ward", "authors": "Abhinav Nellore and Rachel Ward", "title": "Recovery guarantees for exemplar-based clustering", "comments": "24 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For a certain class of distributions, we prove that the linear programming\nrelaxation of $k$-medoids clustering---a variant of $k$-means clustering where\nmeans are replaced by exemplars from within the dataset---distinguishes points\ndrawn from nonoverlapping balls with high probability once the number of points\ndrawn and the separation distance between any two balls are sufficiently large.\nOur results hold in the nontrivial regime where the separation distance is\nsmall enough that points drawn from different balls may be closer to each other\nthan points drawn from the same ball; in this case, clustering by thresholding\npairwise distances between points can fail. We also exhibit numerical evidence\nof high-probability recovery in a substantially more permissive regime.\n", "versions": [{"version": "v1", "created": "Thu, 12 Sep 2013 19:38:18 GMT"}, {"version": "v2", "created": "Mon, 3 Feb 2014 03:56:31 GMT"}], "update_date": "2014-02-04", "authors_parsed": [["Nellore", "Abhinav", ""], ["Ward", "Rachel", ""]]}, {"id": "1309.3533", "submitter": "Emily Fox", "authors": "Emily B. Fox and Michael I. Jordan", "title": "Mixed Membership Models for Time Series", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this article we discuss some of the consequences of the mixed membership\nperspective on time series analysis. In its most abstract form, a mixed\nmembership model aims to associate an individual entity with some set of\nattributes based on a collection of observed data. Although much of the\nliterature on mixed membership models considers the setting in which\nexchangeable collections of data are associated with each member of a set of\nentities, it is equally natural to consider problems in which an entire time\nseries is viewed as an entity and the goal is to characterize the time series\nin terms of a set of underlying dynamic attributes or \"dynamic regimes\".\nIndeed, this perspective is already present in the classical hidden Markov\nmodel, where the dynamic regimes are referred to as \"states\", and the\ncollection of states realized in a sample path of the underlying process can be\nviewed as a mixed membership characterization of the observed time series. Our\ngoal here is to review some of the richer modeling possibilities for time\nseries that are provided by recent developments in the mixed membership\nframework.\n", "versions": [{"version": "v1", "created": "Fri, 13 Sep 2013 18:31:02 GMT"}], "update_date": "2013-09-16", "authors_parsed": [["Fox", "Emily B.", ""], ["Jordan", "Michael I.", ""]]}, {"id": "1309.3676", "submitter": "Nicolae Cleju", "authors": "Nicolae Cleju", "title": "Optimized projections for compressed sensing via rank-constrained\n  nearest correlation matrix", "comments": "25 pages, 13 figures, to appear in Applied and Computational Harmonic\n  Analysis", "journal-ref": null, "doi": "10.1016/j.acha.2013.08.005", "report-no": null, "categories": "cs.IT cs.LG math.IT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Optimizing the acquisition matrix is useful for compressed sensing of signals\nthat are sparse in overcomplete dictionaries, because the acquisition matrix\ncan be adapted to the particular correlations of the dictionary atoms. In this\npaper a novel formulation of the optimization problem is proposed, in the form\nof a rank-constrained nearest correlation matrix problem. Furthermore,\nimprovements for three existing optimization algorithms are introduced, which\nare shown to be particular instances of the proposed formulation. Simulation\nresults show notable improvements and superior robustness in sparse signal\nrecovery.\n", "versions": [{"version": "v1", "created": "Sat, 14 Sep 2013 15:08:48 GMT"}], "update_date": "2013-09-17", "authors_parsed": [["Cleju", "Nicolae", ""]]}, {"id": "1309.3697", "submitter": "Yang Liu", "authors": "Yang Liu, Mingyan Liu", "title": "Group Learning and Opinion Diffusion in a Broadcast Network", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We analyze the following group learning problem in the context of opinion\ndiffusion: Consider a network with $M$ users, each facing $N$ options. In a\ndiscrete time setting, at each time step, each user chooses $K$ out of the $N$\noptions, and receive randomly generated rewards, whose statistics depend on the\noptions chosen as well as the user itself, and are unknown to the users. Each\nuser aims to maximize their expected total rewards over a certain time horizon\nthrough an online learning process, i.e., a sequence of exploration (sampling\nthe return of each option) and exploitation (selecting empirically good\noptions) steps.\n  Within this context we consider two group learning scenarios, (1) users with\nuniform preferences and (2) users with diverse preferences, and examine how a\nuser should construct its learning process to best extract information from\nother's decisions and experiences so as to maximize its own reward. Performance\nis measured in {\\em weak regret}, the difference between the user's total\nreward and the reward from a user-specific best single-action policy (i.e.,\nalways selecting the set of options generating the highest mean rewards for\nthis user). Within each scenario we also consider two cases: (i) when users\nexchange full information, meaning they share the actual rewards they obtained\nfrom their choices, and (ii) when users exchange limited information, e.g.,\nonly their choices but not rewards obtained from these choices.\n", "versions": [{"version": "v1", "created": "Sat, 14 Sep 2013 19:56:58 GMT"}], "update_date": "2013-09-17", "authors_parsed": [["Liu", "Yang", ""], ["Liu", "Mingyan", ""]]}, {"id": "1309.3699", "submitter": "Ravi Ganti", "authors": "Ravi Ganti and Alexander Gray", "title": "Local Support Vector Machines:Formulation and Analysis", "comments": "12 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We provide a formulation for Local Support Vector Machines (LSVMs) that\ngeneralizes previous formulations, and brings out the explicit connections to\nlocal polynomial learning used in nonparametric estimation literature. We\ninvestigate the simplest type of LSVMs called Local Linear Support Vector\nMachines (LLSVMs). For the first time we establish conditions under which\nLLSVMs make Bayes consistent predictions at each test point $x_0$. We also\nestablish rates at which the local risk of LLSVMs converges to the minimum\nvalue of expected local risk at each point $x_0$. Using stability arguments we\nestablish generalization error bounds for LLSVMs.\n", "versions": [{"version": "v1", "created": "Sat, 14 Sep 2013 21:06:22 GMT"}], "update_date": "2018-05-23", "authors_parsed": [["Ganti", "Ravi", ""], ["Gray", "Alexander", ""]]}, {"id": "1309.3809", "submitter": "Ishani Chakraborty", "authors": "Ishani Chakraborty and Ahmed Elgammal", "title": "Visual-Semantic Scene Understanding by Sharing Labels in a Context\n  Network", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of naming objects in complex, natural scenes\ncontaining widely varying object appearance and subtly different names.\nInformed by cognitive research, we propose an approach based on sharing context\nbased object hypotheses between visual and lexical spaces. To this end, we\npresent the Visual Semantic Integration Model (VSIM) that represents object\nlabels as entities shared between semantic and visual contexts and infers a new\nimage by updating labels through context switching. At the core of VSIM is a\nsemantic Pachinko Allocation Model and a visual nearest neighbor Latent\nDirichlet Allocation Model. For inference, we derive an iterative Data\nAugmentation algorithm that pools the label probabilities and maximizes the\njoint label posterior of an image. Our model surpasses the performance of\nstate-of-art methods in several visual tasks on the challenging SUN09 dataset.\n", "versions": [{"version": "v1", "created": "Mon, 16 Sep 2013 00:22:01 GMT"}], "update_date": "2013-09-17", "authors_parsed": [["Chakraborty", "Ishani", ""], ["Elgammal", "Ahmed", ""]]}, {"id": "1309.3877", "submitter": "Huyen  Do", "authors": "Huyen Do and Alexandros Kalousis", "title": "A Metric-learning based framework for Support Vector Machines and\n  Multiple Kernel Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most metric learning algorithms, as well as Fisher's Discriminant Analysis\n(FDA), optimize some cost function of different measures of within-and\nbetween-class distances. On the other hand, Support Vector Machines(SVMs) and\nseveral Multiple Kernel Learning (MKL) algorithms are based on the SVM large\nmargin theory. Recently, SVMs have been analyzed from SVM and metric learning,\nand to develop new algorithms that build on the strengths of each. Inspired by\nthe metric learning interpretation of SVM, we develop here a new\nmetric-learning based SVM framework in which we incorporate metric learning\nconcepts within SVM. We extend the optimization problem of SVM to include some\nmeasure of the within-class distance and along the way we develop a new\nwithin-class distance measure which is appropriate for SVM. In addition, we\nadopt the same approach for MKL and show that it can be also formulated as a\nMahalanobis metric learning problem. Our end result is a number of SVM/MKL\nalgorithms that incorporate metric learning concepts. We experiment with them\non a set of benchmark datasets and observe important predictive performance\nimprovements.\n", "versions": [{"version": "v1", "created": "Mon, 16 Sep 2013 09:39:25 GMT"}], "update_date": "2013-09-17", "authors_parsed": [["Do", "Huyen", ""], ["Kalousis", "Alexandros", ""]]}, {"id": "1309.3949", "submitter": "Anuj Sharma Dr", "authors": "Anuj sharma, Shubhamoy Dey", "title": "Performance Investigation of Feature Selection Methods", "comments": "6 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sentiment analysis or opinion mining has become an open research domain after\nproliferation of Internet and Web 2.0 social media. People express their\nattitudes and opinions on social media including blogs, discussion forums,\ntweets, etc. and, sentiment analysis concerns about detecting and extracting\nsentiment or opinion from online text. Sentiment based text classification is\ndifferent from topical text classification since it involves discrimination\nbased on expressed opinion on a topic. Feature selection is significant for\nsentiment analysis as the opinionated text may have high dimensions, which can\nadversely affect the performance of sentiment analysis classifier. This paper\nexplores applicability of feature selection methods for sentiment analysis and\ninvestigates their performance for classification in term of recall, precision\nand accuracy. Five feature selection methods (Document Frequency, Information\nGain, Gain Ratio, Chi Squared, and Relief-F) and three popular sentiment\nfeature lexicons (HM, GI and Opinion Lexicon) are investigated on movie reviews\ncorpus with a size of 2000 documents. The experimental results show that\nInformation Gain gave consistent results and Gain Ratio performs overall best\nfor sentimental feature selection while sentiment lexicons gave poor\nperformance. Furthermore, we found that performance of the classifier depends\non appropriate number of representative feature selected from text.\n", "versions": [{"version": "v1", "created": "Mon, 16 Sep 2013 13:27:04 GMT"}], "update_date": "2013-09-17", "authors_parsed": [["sharma", "Anuj", ""], ["Dey", "Shubhamoy", ""]]}, {"id": "1309.4024", "submitter": "Patrick C. McGuire", "authors": "P.C. McGuire, A. Bonnici, K.R. Bruner, C. Gross, J. Orm\\\"o, R.A.\n  Smosna, S. Walter, L. Wendt", "title": "The Cyborg Astrobiologist: Matching of Prior Textures by Image\n  Compression for Geological Mapping and Novelty Detection", "comments": "27 pages, 3 figures, 2 tables, accepted for publication in the\n  International Journal of Astrobiology", "journal-ref": "International Journal of Astrobiology, 13(03), pp. 191-202 (2014)", "doi": "10.1017/S1473550413000372", "report-no": null, "categories": "cs.CV astro-ph.EP astro-ph.IM cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  (abridged) We describe an image-comparison technique of Heidemann and Ritter\nthat uses image compression, and is capable of: (i) detecting novel textures in\na series of images, as well as of: (ii) alerting the user to the similarity of\na new image to a previously-observed texture. This image-comparison technique\nhas been implemented and tested using our Astrobiology Phone-cam system, which\nemploys Bluetooth communication to send images to a local laptop server in the\nfield for the image-compression analysis. We tested the system in a field site\ndisplaying a heterogeneous suite of sandstones, limestones, mudstones and\ncoalbeds. Some of the rocks are partly covered with lichen. The image-matching\nprocedure of this system performed very well with data obtained through our\nfield test, grouping all images of yellow lichens together and grouping all\nimages of a coal bed together, and giving a 91% accuracy for similarity\ndetection. Such similarity detection could be employed to make maps of\ndifferent geological units. The novelty-detection performance of our system was\nalso rather good (a 64% accuracy). Such novelty detection may become valuable\nin searching for new geological units, which could be of astrobiological\ninterest. The image-comparison technique is an unsupervised technique that is\nnot capable of directly classifying an image as containing a particular\ngeological feature; labeling of such geological features is done post facto by\nhuman geologists associated with this study, for the purpose of analyzing the\nsystem's performance. By providing more advanced capabilities for similarity\ndetection and novelty detection, this image-compression technique could be\nuseful in giving more scientific autonomy to robotic planetary rovers, and in\nassisting human astronauts in their geological exploration and assessment.\n", "versions": [{"version": "v1", "created": "Mon, 16 Sep 2013 16:32:35 GMT"}], "update_date": "2014-07-08", "authors_parsed": [["McGuire", "P. C.", ""], ["Bonnici", "A.", ""], ["Bruner", "K. R.", ""], ["Gross", "C.", ""], ["Orm\u00f6", "J.", ""], ["Smosna", "R. A.", ""], ["Walter", "S.", ""], ["Wendt", "L.", ""]]}, {"id": "1309.4035", "submitter": "Peter Turney", "authors": "Peter D. Turney", "title": "Domain and Function: A Dual-Space Model of Semantic Relations and\n  Compositions", "comments": null, "journal-ref": "Journal of Artificial Intelligence Research (JAIR), (2012), 44,\n  533-585", "doi": "10.1613/jair.3640", "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given appropriate representations of the semantic relations between carpenter\nand wood and between mason and stone (for example, vectors in a vector space\nmodel), a suitable algorithm should be able to recognize that these relations\nare highly similar (carpenter is to wood as mason is to stone; the relations\nare analogous). Likewise, with representations of dog, house, and kennel, an\nalgorithm should be able to recognize that the semantic composition of dog and\nhouse, dog house, is highly similar to kennel (dog house and kennel are\nsynonymous). It seems that these two tasks, recognizing relations and\ncompositions, are closely connected. However, up to now, the best models for\nrelations are significantly different from the best models for compositions. In\nthis paper, we introduce a dual-space model that unifies these two tasks. This\nmodel matches the performance of the best previous models for relations and\ncompositions. The dual-space model consists of a space for measuring domain\nsimilarity and a space for measuring function similarity. Carpenter and wood\nshare the same domain, the domain of carpentry. Mason and stone share the same\ndomain, the domain of masonry. Carpenter and mason share the same function, the\nfunction of artisans. Wood and stone share the same function, the function of\nmaterials. In the composition dog house, kennel has some domain overlap with\nboth dog and house (the domains of pets and buildings). The function of kennel\nis similar to the function of house (the function of shelters). By combining\ndomain and function similarities in various ways, we can model relations,\ncompositions, and other aspects of semantics.\n", "versions": [{"version": "v1", "created": "Mon, 16 Sep 2013 16:51:02 GMT"}], "update_date": "2013-09-17", "authors_parsed": [["Turney", "Peter D.", ""]]}, {"id": "1309.4061", "submitter": "Andreas Christian M\\\"uller", "authors": "Andreas Christian Mueller, Sven Behnke", "title": "Learning a Loopy Model For Semantic Segmentation Exactly", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning structured models using maximum margin techniques has become an\nindispensable tool for com- puter vision researchers, as many computer vision\napplications can be cast naturally as an image labeling problem. Pixel-based or\nsuperpixel-based conditional random fields are particularly popular examples.\nTyp- ically, neighborhood graphs, which contain a large number of cycles, are\nused. As exact inference in loopy graphs is NP-hard in general, learning these\nmodels without approximations is usually deemed infeasible. In this work we\nshow that, despite the theoretical hardness, it is possible to learn loopy\nmodels exactly in practical applications. To this end, we analyze the use of\nmultiple approximate inference techniques together with cutting plane training\nof structural SVMs. We show that our proposed method yields exact solutions\nwith an optimality guarantees in a computer vision application, for little\nadditional computational cost. We also propose a dynamic caching scheme to\naccelerate training further, yielding runtimes that are comparable with\napproximate methods. We hope that this insight can lead to a reconsideration of\nthe tractability of loopy models in computer vision.\n", "versions": [{"version": "v1", "created": "Mon, 16 Sep 2013 18:30:41 GMT"}], "update_date": "2013-09-17", "authors_parsed": [["Mueller", "Andreas Christian", ""], ["Behnke", "Sven", ""]]}, {"id": "1309.4111", "submitter": "Tai Qin", "authors": "Tai Qin, Karl Rohe", "title": "Regularized Spectral Clustering under the Degree-Corrected Stochastic\n  Blockmodel", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Spectral clustering is a fast and popular algorithm for finding clusters in\nnetworks. Recently, Chaudhuri et al. (2012) and Amini et al.(2012) proposed\ninspired variations on the algorithm that artificially inflate the node degrees\nfor improved statistical performance. The current paper extends the previous\nstatistical estimation results to the more canonical spectral clustering\nalgorithm in a way that removes any assumption on the minimum degree and\nprovides guidance on the choice of the tuning parameter. Moreover, our results\nshow how the \"star shape\" in the eigenvectors--a common feature of empirical\nnetworks--can be explained by the Degree-Corrected Stochastic Blockmodel and\nthe Extended Planted Partition model, two statistical models that allow for\nhighly heterogeneous degrees. Throughout, the paper characterizes and justifies\nseveral of the variations of the spectral clustering algorithm in terms of\nthese models.\n", "versions": [{"version": "v1", "created": "Mon, 16 Sep 2013 20:47:51 GMT"}], "update_date": "2013-09-18", "authors_parsed": [["Qin", "Tai", ""], ["Rohe", "Karl", ""]]}, {"id": "1309.4132", "submitter": "Varun Kanade", "authors": "Elaine Angelino, Varun Kanade", "title": "Attribute-Efficient Evolvability of Linear Functions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG q-bio.PE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In a seminal paper, Valiant (2006) introduced a computational model for\nevolution to address the question of complexity that can arise through\nDarwinian mechanisms. Valiant views evolution as a restricted form of\ncomputational learning, where the goal is to evolve a hypothesis that is close\nto the ideal function. Feldman (2008) showed that (correlational) statistical\nquery learning algorithms could be framed as evolutionary mechanisms in\nValiant's model. P. Valiant (2012) considered evolvability of real-valued\nfunctions and also showed that weak-optimization algorithms that use\nweak-evaluation oracles could be converted to evolutionary mechanisms.\n  In this work, we focus on the complexity of representations of evolutionary\nmechanisms. In general, the reductions of Feldman and P. Valiant may result in\nintermediate representations that are arbitrarily complex (polynomial-sized\ncircuits). We argue that biological constraints often dictate that the\nrepresentations have low complexity, such as constant depth and fan-in\ncircuits. We give mechanisms for evolving sparse linear functions under a large\nclass of smooth distributions. These evolutionary algorithms are\nattribute-efficient in the sense that the size of the representations and the\nnumber of generations required depend only on the sparsity of the target\nfunction and the accuracy parameter, but have no dependence on the total number\nof attributes.\n", "versions": [{"version": "v1", "created": "Mon, 16 Sep 2013 22:31:58 GMT"}, {"version": "v2", "created": "Thu, 3 Apr 2014 23:07:16 GMT"}], "update_date": "2014-04-07", "authors_parsed": [["Angelino", "Elaine", ""], ["Kanade", "Varun", ""]]}, {"id": "1309.4714", "submitter": "Patrick M. Pilarski", "authors": "Ann L. Edwards, Alexandra Kearney, Michael Rory Dawson, Richard S.\n  Sutton, Patrick M. Pilarski", "title": "Temporal-Difference Learning to Assist Human Decision Making during the\n  Control of an Artificial Limb", "comments": "5 pages, 4 figures, This version to appear at The 1st\n  Multidisciplinary Conference on Reinforcement Learning and Decision Making,\n  Princeton, NJ, USA, Oct. 25-27, 2013", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work we explore the use of reinforcement learning (RL) to help with\nhuman decision making, combining state-of-the-art RL algorithms with an\napplication to prosthetics. Managing human-machine interaction is a problem of\nconsiderable scope, and the simplification of human-robot interfaces is\nespecially important in the domains of biomedical technology and rehabilitation\nmedicine. For example, amputees who control artificial limbs are often required\nto quickly switch between a number of control actions or modes of operation in\norder to operate their devices. We suggest that by learning to anticipate\n(predict) a user's behaviour, artificial limbs could take on an active role in\na human's control decisions so as to reduce the burden on their users.\nRecently, we showed that RL in the form of general value functions (GVFs) could\nbe used to accurately detect a user's control intent prior to their explicit\ncontrol choices. In the present work, we explore the use of temporal-difference\nlearning and GVFs to predict when users will switch their control influence\nbetween the different motor functions of a robot arm. Experiments were\nperformed using a multi-function robot arm that was controlled by muscle\nsignals from a user's body (similar to conventional artificial limb control).\nOur approach was able to acquire and maintain forecasts about a user's\nswitching decisions in real time. It also provides an intuitive and reward-free\nway for users to correct or reinforce the decisions made by the machine\nlearning system. We expect that when a system is certain enough about its\npredictions, it can begin to take over switching decisions from the user to\nstreamline control and potentially decrease the time and effort needed to\ncomplete tasks. This preliminary study therefore suggests a way to naturally\nintegrate human- and machine-based decision making systems.\n", "versions": [{"version": "v1", "created": "Wed, 18 Sep 2013 17:29:03 GMT"}], "update_date": "2013-09-19", "authors_parsed": [["Edwards", "Ann L.", ""], ["Kearney", "Alexandra", ""], ["Dawson", "Michael Rory", ""], ["Sutton", "Richard S.", ""], ["Pilarski", "Patrick M.", ""]]}, {"id": "1309.4844", "submitter": "Jing Wang", "authors": "Jing Wang, Daniel Rossell, Christos G. Cassandras, and Ioannis Ch.\n  Paschalidis", "title": "Network Anomaly Detection: A Survey and Comparative Analysis of\n  Stochastic and Deterministic Methods", "comments": "7 pages. 1 more figure than final CDC 2013 version", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present five methods to the problem of network anomaly detection. These\nmethods cover most of the common techniques in the anomaly detection field,\nincluding Statistical Hypothesis Tests (SHT), Support Vector Machines (SVM) and\nclustering analysis. We evaluate all methods in a simulated network that\nconsists of nominal data, three flow-level anomalies and one packet-level\nattack. Through analyzing the results, we point out the advantages and\ndisadvantages of each method and conclude that combining the results of the\nindividual methods can yield improved anomaly detection results.\n", "versions": [{"version": "v1", "created": "Thu, 19 Sep 2013 03:09:33 GMT"}], "update_date": "2013-09-20", "authors_parsed": [["Wang", "Jing", ""], ["Rossell", "Daniel", ""], ["Cassandras", "Christos G.", ""], ["Paschalidis", "Ioannis Ch.", ""]]}, {"id": "1309.4962", "submitter": "Josef Urban", "authors": "Cezary Kaliszyk and Josef Urban", "title": "HOL(y)Hammer: Online ATP Service for HOL Light", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.DL cs.LG cs.LO cs.MS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  HOL(y)Hammer is an online AI/ATP service for formal (computer-understandable)\nmathematics encoded in the HOL Light system. The service allows its users to\nupload and automatically process an arbitrary formal development (project)\nbased on HOL Light, and to attack arbitrary conjectures that use the concepts\ndefined in some of the uploaded projects. For that, the service uses several\nautomated reasoning systems combined with several premise selection methods\ntrained on all the project proofs. The projects that are readily available on\nthe server for such query answering include the recent versions of the\nFlyspeck, Multivariate Analysis and Complex Analysis libraries. The service\nruns on a 48-CPU server, currently employing in parallel for each task 7 AI/ATP\ncombinations and 4 decision procedures that contribute to its overall\nperformance. The system is also available for local installation by interested\nusers, who can customize it for their own proof development. An Emacs interface\nallowing parallel asynchronous queries to the service is also provided. The\noverall structure of the service is outlined, problems that arise and their\nsolutions are discussed, and an initial account of using the system is given.\n", "versions": [{"version": "v1", "created": "Thu, 19 Sep 2013 13:22:31 GMT"}], "update_date": "2013-09-20", "authors_parsed": [["Kaliszyk", "Cezary", ""], ["Urban", "Josef", ""]]}, {"id": "1309.4999", "submitter": "Cyril Voyant", "authors": "Cyril Voyant (SPE), C. Darras (SPE), Marc Muselli (SPE), Christophe\n  Paoli (SPE), Marie Laure Nivet (SPE), Philippe Poggi (SPE)", "title": "Bayesian rules and stochastic models for high accuracy prediction of\n  solar radiation", "comments": "Applied Energy (2013)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is essential to find solar predictive methods to massively insert\nrenewable energies on the electrical distribution grid. The goal of this study\nis to find the best methodology allowing predicting with high accuracy the\nhourly global radiation. The knowledge of this quantity is essential for the\ngrid manager or the private PV producer in order to anticipate fluctuations\nrelated to clouds occurrences and to stabilize the injected PV power. In this\npaper, we test both methodologies: single and hybrid predictors. In the first\nclass, we include the multi-layer perceptron (MLP), auto-regressive and moving\naverage (ARMA), and persistence models. In the second class, we mix these\npredictors with Bayesian rules to obtain ad-hoc models selections, and Bayesian\naverages of outputs related to single models. If MLP and ARMA are equivalent\n(nRMSE close to 40.5% for the both), this hybridization allows a nRMSE gain\nupper than 14 percentage points compared to the persistence estimation\n(nRMSE=37% versus 51%).\n", "versions": [{"version": "v1", "created": "Wed, 18 Sep 2013 06:44:33 GMT"}], "update_date": "2013-09-20", "authors_parsed": [["Voyant", "Cyril", "", "SPE"], ["Darras", "C.", "", "SPE"], ["Muselli", "Marc", "", "SPE"], ["Paoli", "Christophe", "", "SPE"], ["Nivet", "Marie Laure", "", "SPE"], ["Poggi", "Philippe", "", "SPE"]]}, {"id": "1309.5047", "submitter": "Sean Whalen", "authors": "Sean Whalen and Gaurav Pandey", "title": "A Comparative Analysis of Ensemble Classifiers: Case Studies in Genomics", "comments": "10 pages, 3 figures, 8 tables, to appear in Proceedings of the 2013\n  International Conference on Data Mining", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG q-bio.GN stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The combination of multiple classifiers using ensemble methods is\nincreasingly important for making progress in a variety of difficult prediction\nproblems. We present a comparative analysis of several ensemble methods through\ntwo case studies in genomics, namely the prediction of genetic interactions and\nprotein functions, to demonstrate their efficacy on real-world datasets and\ndraw useful conclusions about their behavior. These methods include simple\naggregation, meta-learning, cluster-based meta-learning, and ensemble selection\nusing heterogeneous classifiers trained on resampled data to improve the\ndiversity of their predictions. We present a detailed analysis of these methods\nacross 4 genomics datasets and find the best of these methods offer\nstatistically significant improvements over the state of the art in their\nrespective domains. In addition, we establish a novel connection between\nensemble selection and meta-learning, demonstrating how both of these disparate\nmethods establish a balance between ensemble diversity and performance.\n", "versions": [{"version": "v1", "created": "Thu, 19 Sep 2013 16:45:18 GMT"}], "update_date": "2013-09-20", "authors_parsed": [["Whalen", "Sean", ""], ["Pandey", "Gaurav", ""]]}, {"id": "1309.5319", "submitter": "Clement Moulin-Frier", "authors": "Cl\\'ement Moulin-Frier (INRIA Bordeaux - Sud-Ouest, GIPSA-lab), M. A.\n  Arbib (USC)", "title": "Recognizing Speech in a Novel Accent: The Motor Theory of Speech\n  Perception Reframed", "comments": null, "journal-ref": "Biological Cybernetics 107, 4 (2013) 421-447", "doi": "10.1007/s00422-013-0557-3", "report-no": null, "categories": "cs.CL cs.LG q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The motor theory of speech perception holds that we perceive the speech of\nanother in terms of a motor representation of that speech. However, when we\nhave learned to recognize a foreign accent, it seems plausible that recognition\nof a word rarely involves reconstruction of the speech gestures of the speaker\nrather than the listener. To better assess the motor theory and this\nobservation, we proceed in three stages. Part 1 places the motor theory of\nspeech perception in a larger framework based on our earlier models of the\nadaptive formation of mirror neurons for grasping, and for viewing extensions\nof that mirror system as part of a larger system for neuro-linguistic\nprocessing, augmented by the present consideration of recognizing speech in a\nnovel accent. Part 2 then offers a novel computational model of how a listener\ncomes to understand the speech of someone speaking the listener's native\nlanguage with a foreign accent. The core tenet of the model is that the\nlistener uses hypotheses about the word the speaker is currently uttering to\nupdate probabilities linking the sound produced by the speaker to phonemes in\nthe native language repertoire of the listener. This, on average, improves the\nrecognition of later words. This model is neutral regarding the nature of the\nrepresentations it uses (motor vs. auditory). It serve as a reference point for\nthe discussion in Part 3, which proposes a dual-stream neuro-linguistic\narchitecture to revisits claims for and against the motor theory of speech\nperception and the relevance of mirror neurons, and extracts some implications\nfor the reframing of the motor theory.\n", "versions": [{"version": "v1", "created": "Fri, 20 Sep 2013 16:47:48 GMT"}], "update_date": "2013-09-23", "authors_parsed": [["Moulin-Frier", "Cl\u00e9ment", "", "INRIA Bordeaux - Sud-Ouest, GIPSA-lab"], ["Arbib", "M. A.", "", "USC"]]}, {"id": "1309.5427", "submitter": "Gang Chen", "authors": "Gang Chen", "title": "Latent Fisher Discriminant Analysis", "comments": "12 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  Linear Discriminant Analysis (LDA) is a well-known method for dimensionality\nreduction and classification. Previous studies have also extended the\nbinary-class case into multi-classes. However, many applications, such as\nobject detection and keyframe extraction cannot provide consistent\ninstance-label pairs, while LDA requires labels on instance level for training.\nThus it cannot be directly applied for semi-supervised classification problem.\nIn this paper, we overcome this limitation and propose a latent variable Fisher\ndiscriminant analysis model. We relax the instance-level labeling into\nbag-level, is a kind of semi-supervised (video-level labels of event type are\nrequired for semantic frame extraction) and incorporates a data-driven prior\nover the latent variables. Hence, our method combines the latent variable\ninference and dimension reduction in an unified bayesian framework. We test our\nmethod on MUSK and Corel data sets and yield competitive results compared to\nthe baseline approach. We also demonstrate its capacity on the challenging\nTRECVID MED11 dataset for semantic keyframe extraction and conduct a\nhuman-factors ranking-based experimental evaluation, which clearly demonstrates\nour proposed method consistently extracts more semantically meaningful\nkeyframes than challenging baselines.\n", "versions": [{"version": "v1", "created": "Sat, 21 Sep 2013 03:42:04 GMT"}], "update_date": "2013-09-24", "authors_parsed": [["Chen", "Gang", ""]]}, {"id": "1309.5605", "submitter": "Anna Choromanska", "authors": "Anna Choromanska and Tony Jebara", "title": "Stochastic Bound Majorization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently a majorization method for optimizing partition functions of\nlog-linear models was proposed alongside a novel quadratic variational\nupper-bound. In the batch setting, it outperformed state-of-the-art first- and\nsecond-order optimization methods on various learning tasks. We propose a\nstochastic version of this bound majorization method as well as a low-rank\nmodification for high-dimensional data-sets. The resulting stochastic\nsecond-order method outperforms stochastic gradient descent (across variations\nand various tunings) both in terms of the number of iterations and computation\ntime till convergence while finding a better quality parameter setting. The\nproposed method bridges first- and second-order stochastic optimization methods\nby maintaining a computational complexity that is linear in the data dimension\nand while exploiting second order information about the pseudo-global curvature\nof the objective function (as opposed to the local curvature in the Hessian).\n", "versions": [{"version": "v1", "created": "Sun, 22 Sep 2013 14:46:15 GMT"}], "update_date": "2013-09-24", "authors_parsed": [["Choromanska", "Anna", ""], ["Jebara", "Tony", ""]]}, {"id": "1309.5643", "submitter": "Veronika Cheplygina", "authors": "Veronika Cheplygina, David M. J. Tax, and Marco Loog", "title": "Multiple Instance Learning with Bag Dissimilarities", "comments": "Pattern Recognition, in press", "journal-ref": "Pattern Recognition 48.1 (2015): 264-275", "doi": "10.1016/j.patcog.2014.07.022", "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multiple instance learning (MIL) is concerned with learning from sets (bags)\nof objects (instances), where the individual instance labels are ambiguous. In\nthis setting, supervised learning cannot be applied directly. Often,\nspecialized MIL methods learn by making additional assumptions about the\nrelationship of the bag labels and instance labels. Such assumptions may fit a\nparticular dataset, but do not generalize to the whole range of MIL problems.\nOther MIL methods shift the focus of assumptions from the labels to the overall\n(dis)similarity of bags, and therefore learn from bags directly. We propose to\nrepresent each bag by a vector of its dissimilarities to other bags in the\ntraining set, and treat these dissimilarities as a feature representation. We\nshow several alternatives to define a dissimilarity between bags and discuss\nwhich definitions are more suitable for particular MIL problems. The\nexperimental results show that the proposed approach is computationally\ninexpensive, yet very competitive with state-of-the-art algorithms on a wide\nrange of MIL datasets.\n", "versions": [{"version": "v1", "created": "Sun, 22 Sep 2013 20:24:50 GMT"}, {"version": "v2", "created": "Thu, 6 Feb 2014 13:13:11 GMT"}, {"version": "v3", "created": "Tue, 12 Aug 2014 09:04:32 GMT"}], "update_date": "2014-12-04", "authors_parsed": [["Cheplygina", "Veronika", ""], ["Tax", "David M. J.", ""], ["Loog", "Marco", ""]]}, {"id": "1309.5655", "submitter": "Vladimir Kolmogorov", "authors": "Vladimir Kolmogorov", "title": "A new look at reweighted message passing", "comments": "TPAMI accepted version", "journal-ref": "TPAMI, 37(5):919-930 (May, 2015)", "doi": "10.1109/TPAMI.2014.2363465", "report-no": null, "categories": "cs.AI cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new family of message passing techniques for MAP estimation in\ngraphical models which we call {\\em Sequential Reweighted Message Passing}\n(SRMP). Special cases include well-known techniques such as {\\em Min-Sum\nDiffusion} (MSD) and a faster {\\em Sequential Tree-Reweighted Message Passing}\n(TRW-S). Importantly, our derivation is simpler than the original derivation of\nTRW-S, and does not involve a decomposition into trees. This allows easy\ngeneralizations. We present such a generalization for the case of higher-order\ngraphical models, and test it on several real-world problems with promising\nresults.\n", "versions": [{"version": "v1", "created": "Sun, 22 Sep 2013 21:19:36 GMT"}, {"version": "v2", "created": "Thu, 16 Jan 2014 11:57:31 GMT"}, {"version": "v3", "created": "Thu, 19 Jan 2017 17:45:24 GMT"}], "update_date": "2017-01-20", "authors_parsed": [["Kolmogorov", "Vladimir", ""]]}, {"id": "1309.5803", "submitter": "Henrik Ohlsson", "authors": "Henrik Ohlsson, Tianshi Chen, Sina Khoshfetrat Pakazad, Lennart Ljung\n  and S. Shankar Sastry", "title": "Scalable Anomaly Detection in Large Homogenous Populations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DC cs.SY math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Anomaly detection in large populations is a challenging but highly relevant\nproblem. The problem is essentially a multi-hypothesis problem, with a\nhypothesis for every division of the systems into normal and anomal systems.\nThe number of hypothesis grows rapidly with the number of systems and\napproximate solutions become a necessity for any problems of practical\ninterests. In the current paper we take an optimization approach to this\nmulti-hypothesis problem. We first observe that the problem is equivalent to a\nnon-convex combinatorial optimization problem. We then relax the problem to a\nconvex problem that can be solved distributively on the systems and that stays\ncomputationally tractable as the number of systems increase. An interesting\nproperty of the proposed method is that it can under certain conditions be\nshown to give exactly the same result as the combinatorial multi-hypothesis\nproblem and the relaxation is hence tight.\n", "versions": [{"version": "v1", "created": "Fri, 20 Sep 2013 14:38:01 GMT"}], "update_date": "2013-09-24", "authors_parsed": [["Ohlsson", "Henrik", ""], ["Chen", "Tianshi", ""], ["Pakazad", "Sina Khoshfetrat", ""], ["Ljung", "Lennart", ""], ["Sastry", "S. Shankar", ""]]}, {"id": "1309.5823", "submitter": "Wangmeng Zuo", "authors": "Faqiang Wang, Wangmeng Zuo, Lei Zhang, Deyu Meng and David Zhang", "title": "A Kernel Classification Framework for Metric Learning", "comments": "11 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning a distance metric from the given training samples plays a crucial\nrole in many machine learning tasks, and various models and optimization\nalgorithms have been proposed in the past decade. In this paper, we generalize\nseveral state-of-the-art metric learning methods, such as large margin nearest\nneighbor (LMNN) and information theoretic metric learning (ITML), into a kernel\nclassification framework. First, doublets and triplets are constructed from the\ntraining samples, and a family of degree-2 polynomial kernel functions are\nproposed for pairs of doublets or triplets. Then, a kernel classification\nframework is established, which can not only generalize many popular metric\nlearning methods such as LMNN and ITML, but also suggest new metric learning\nmethods, which can be efficiently implemented, interestingly, by using the\nstandard support vector machine (SVM) solvers. Two novel metric learning\nmethods, namely doublet-SVM and triplet-SVM, are then developed under the\nproposed framework. Experimental results show that doublet-SVM and triplet-SVM\nachieve competitive classification accuracies with state-of-the-art metric\nlearning methods such as ITML and LMNN but with significantly less training\ntime.\n", "versions": [{"version": "v1", "created": "Mon, 23 Sep 2013 14:39:48 GMT"}], "update_date": "2013-09-24", "authors_parsed": [["Wang", "Faqiang", ""], ["Zuo", "Wangmeng", ""], ["Zhang", "Lei", ""], ["Meng", "Deyu", ""], ["Zhang", "David", ""]]}, {"id": "1309.5854", "submitter": "Hossein Hosseini", "authors": "Seyed Hossein Hosseini, Mahrokh G. Shayesteh, Mehdi Chehel Amirani", "title": "Demodulation of Sparse PPM Signals with Low Samples Using Trained RIP\n  Matrix", "comments": "4 pages, 6 figures, conference paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.OH cs.IT cs.LG math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Compressed sensing (CS) theory considers the restricted isometry property\n(RIP) as a sufficient condition for measurement matrix which guarantees the\nrecovery of any sparse signal from its compressed measurements. The RIP\ncondition also preserves enough information for classification of sparse\nsymbols, even with fewer measurements. In this work, we utilize RIP bound as\nthe cost function for training a simple neural network in order to exploit the\nnear optimal measurements or equivalently near optimal features for\nclassification of a known set of sparse symbols. As an example, we consider\ndemodulation of pulse position modulation (PPM) signals. The results indicate\nthat the proposed method has much better performance than the random\nmeasurements and requires less samples than the optimum matched filter\ndemodulator, at the expense of some performance loss. Further, the proposed\napproach does not need equalizer for multipath channels in contrast to the\nconventional receiver.\n", "versions": [{"version": "v1", "created": "Sun, 1 Sep 2013 22:14:52 GMT"}], "update_date": "2013-09-24", "authors_parsed": [["Hosseini", "Seyed Hossein", ""], ["Shayesteh", "Mahrokh G.", ""], ["Amirani", "Mehdi Chehel", ""]]}, {"id": "1309.5904", "submitter": "Syamantak Das", "authors": "Suman K Bera, Anamitra R Choudhury, Syamantak Das, Sambuddha Roy and\n  Jayram S. Thatchachar", "title": "Fenchel Duals for Drifting Adversaries", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe a primal-dual framework for the design and analysis of online\nconvex optimization algorithms for {\\em drifting regret}. Existing literature\nshows (nearly) optimal drifting regret bounds only for the $\\ell_2$ and the\n$\\ell_1$-norms. Our work provides a connection between these algorithms and the\nOnline Mirror Descent ($\\omd$) updates; one key insight that results from our\nwork is that in order for these algorithms to succeed, it suffices to have the\ngradient of the regularizer to be bounded (in an appropriate norm). For\nsituations (like for the $\\ell_1$ norm) where the vanilla regularizer does not\nhave this property, we have to {\\em shift} the regularizer to ensure this.\nThus, this helps explain the various updates presented in \\cite{bansal10,\nbuchbinder12}. We also consider the online variant of the problem with\n1-lookahead, and with movement costs in the $\\ell_2$-norm. Our primal dual\napproach yields nearly optimal competitive ratios for this problem.\n", "versions": [{"version": "v1", "created": "Mon, 23 Sep 2013 18:14:02 GMT"}], "update_date": "2013-09-24", "authors_parsed": [["Bera", "Suman K", ""], ["Choudhury", "Anamitra R", ""], ["Das", "Syamantak", ""], ["Roy", "Sambuddha", ""], ["Thatchachar", "Jayram S.", ""]]}, {"id": "1309.6176", "submitter": "Xin Zheng", "authors": "Xin Zheng, Zhiyong Wu, Helen Meng, Weifeng Li, Lianhong Cai", "title": "Feature Learning with Gaussian Restricted Boltzmann Machine for Robust\n  Speech Recognition", "comments": "4 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG cs.SD", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  In this paper, we first present a new variant of Gaussian restricted\nBoltzmann machine (GRBM) called multivariate Gaussian restricted Boltzmann\nmachine (MGRBM), with its definition and learning algorithm. Then we propose\nusing a learned GRBM or MGRBM to extract better features for robust speech\nrecognition. Our experiments on Aurora2 show that both GRBM-extracted and\nMGRBM-extracted feature performs much better than Mel-frequency cepstral\ncoefficient (MFCC) with either HMM-GMM or hybrid HMM-deep neural network (DNN)\nacoustic model, and MGRBM-extracted feature is slightly better.\n", "versions": [{"version": "v1", "created": "Mon, 23 Sep 2013 13:51:28 GMT"}], "update_date": "2013-09-25", "authors_parsed": [["Zheng", "Xin", ""], ["Wu", "Zhiyong", ""], ["Meng", "Helen", ""], ["Li", "Weifeng", ""], ["Cai", "Lianhong", ""]]}, {"id": "1309.6301", "submitter": "Xiangrong Zeng", "authors": "Xiangrong Zeng and M\\'ario A. T. Figueiredo", "title": "Solving OSCAR regularization problems by proximal splitting algorithms", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The OSCAR (octagonal selection and clustering algorithm for regression)\nregularizer consists of a L_1 norm plus a pair-wise L_inf norm (responsible for\nits grouping behavior) and was proposed to encourage group sparsity in\nscenarios where the groups are a priori unknown. The OSCAR regularizer has a\nnon-trivial proximity operator, which limits its applicability. We reformulate\nthis regularizer as a weighted sorted L_1 norm, and propose its grouping\nproximity operator (GPO) and approximate proximity operator (APO), thus making\nstate-of-the-art proximal splitting algorithms (PSAs) available to solve\ninverse problems with OSCAR regularization. The GPO is in fact the APO followed\nby additional grouping and averaging operations, which are costly in time and\nstorage, explaining the reason why algorithms with APO are much faster than\nthat with GPO. The convergences of PSAs with GPO are guaranteed since GPO is an\nexact proximity operator. Although convergence of PSAs with APO is may not be\nguaranteed, we have experimentally found that APO behaves similarly to GPO when\nthe regularization parameter of the pair-wise L_inf norm is set to an\nappropriately small value. Experiments on recovery of group-sparse signals\n(with unknown groups) show that PSAs with APO are very fast and accurate.\n", "versions": [{"version": "v1", "created": "Tue, 24 Sep 2013 19:48:56 GMT"}, {"version": "v2", "created": "Fri, 27 Sep 2013 19:36:41 GMT"}], "update_date": "2013-09-30", "authors_parsed": [["Zeng", "Xiangrong", ""], ["Figueiredo", "M\u00e1rio A. T.", ""]]}, {"id": "1309.6487", "submitter": "Xi Peng", "authors": "Xi Peng, Huajin Tang, Lei Zhang, Zhang Yi, and Shijie Xiao", "title": "A Unified Framework for Representation-based Subspace Clustering of\n  Out-of-sample and Large-scale Data", "comments": "in IEEE Trans. on Neural Networks and Learning Systems, 2015", "journal-ref": "IEEE Trans. on Neural Networks and Learning Systems, vol. 27, no.\n  12, pp. 2499-2512, Dec. 2016", "doi": "10.1109/TNNLS.2015.2490080", "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Under the framework of spectral clustering, the key of subspace clustering is\nbuilding a similarity graph which describes the neighborhood relations among\ndata points. Some recent works build the graph using sparse, low-rank, and\n$\\ell_2$-norm-based representation, and have achieved state-of-the-art\nperformance. However, these methods have suffered from the following two\nlimitations. First, the time complexities of these methods are at least\nproportional to the cube of the data size, which make those methods inefficient\nfor solving large-scale problems. Second, they cannot cope with out-of-sample\ndata that are not used to construct the similarity graph. To cluster each\nout-of-sample datum, the methods have to recalculate the similarity graph and\nthe cluster membership of the whole data set. In this paper, we propose a\nunified framework which makes representation-based subspace clustering\nalgorithms feasible to cluster both out-of-sample and large-scale data. Under\nour framework, the large-scale problem is tackled by converting it as\nout-of-sample problem in the manner of \"sampling, clustering, coding, and\nclassifying\". Furthermore, we give an estimation for the error bounds by\ntreating each subspace as a point in a hyperspace. Extensive experimental\nresults on various benchmark data sets show that our methods outperform several\nrecently-proposed scalable methods in clustering large-scale data set.\n", "versions": [{"version": "v1", "created": "Wed, 25 Sep 2013 12:53:13 GMT"}, {"version": "v2", "created": "Fri, 30 Oct 2015 14:43:50 GMT"}], "update_date": "2017-05-17", "authors_parsed": [["Peng", "Xi", ""], ["Tang", "Huajin", ""], ["Zhang", "Lei", ""], ["Yi", "Zhang", ""], ["Xiao", "Shijie", ""]]}, {"id": "1309.6584", "submitter": "Liane Gabora", "authors": "Liane Gabora", "title": "Should I Stay or Should I Go: Coordinating Biological Needs with\n  Continuously-updated Assessments of the Environment", "comments": null, "journal-ref": "In S. Wilson, J. A. Meyer, & H. Roitblat (Eds.), Proceedings of\n  the Second International Conference on the Simulation of Adaptive Behavior\n  (pp. 156-162). Cambridge, MA: MIT Press. (1992)", "doi": null, "report-no": null, "categories": "cs.NE cs.LG q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents Wanderer, a model of how autonomous adaptive systems\ncoordinate internal biological needs with moment-by-moment assessments of the\nprobabilities of events in the external world. The extent to which Wanderer\nmoves about or explores its environment reflects the relative activations of\ntwo competing motivational sub-systems: one represents the need to acquire\nenergy and it excites exploration, and the other represents the need to avoid\npredators and it inhibits exploration. The environment contains food,\npredators, and neutral stimuli. Wanderer responds to these events in a way that\nis adaptive in the short turn, and reassesses the probabilities of these events\nso that it can modify its long term behaviour appropriately. When food appears,\nWanderer be-comes satiated and exploration temporarily decreases. When a\npredator appears, Wanderer both decreases exploration in the short term, and\nbecomes more \"cautious\" about exploring in the future. Wanderer also forms\nassociations between neutral features and salient ones (food and predators)\nwhen they are present at the same time, and uses these associations to guide\nits behaviour.\n", "versions": [{"version": "v1", "created": "Wed, 25 Sep 2013 17:32:24 GMT"}, {"version": "v2", "created": "Fri, 5 Jul 2019 20:07:12 GMT"}, {"version": "v3", "created": "Tue, 9 Jul 2019 19:38:38 GMT"}], "update_date": "2019-07-11", "authors_parsed": [["Gabora", "Liane", ""]]}, {"id": "1309.6707", "submitter": "Cem Tekin", "authors": "Cem Tekin, Simpson Zhang, Mihaela van der Schaar", "title": "Distributed Online Learning in Social Recommender Systems", "comments": null, "journal-ref": "Selected Topics in Signal Processing, IEEE Journal of , vol.8,\n  no.4, pp.638,652, Aug. 2014", "doi": "10.1109/JSTSP.2014.2299517", "report-no": null, "categories": "cs.SI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we consider decentralized sequential decision making in\ndistributed online recommender systems, where items are recommended to users\nbased on their search query as well as their specific background including\nhistory of bought items, gender and age, all of which comprise the context\ninformation of the user. In contrast to centralized recommender systems, in\nwhich there is a single centralized seller who has access to the complete\ninventory of items as well as the complete record of sales and user\ninformation, in decentralized recommender systems each seller/learner only has\naccess to the inventory of items and user information for its own products and\nnot the products and user information of other sellers, but can get commission\nif it sells an item of another seller. Therefore the sellers must distributedly\nfind out for an incoming user which items to recommend (from the set of own\nitems or items of another seller), in order to maximize the revenue from own\nsales and commissions. We formulate this problem as a cooperative contextual\nbandit problem, analytically bound the performance of the sellers compared to\nthe best recommendation strategy given the complete realization of user\narrivals and the inventory of items, as well as the context-dependent purchase\nprobabilities of each item, and verify our results via numerical examples on a\ndistributed data set adapted based on Amazon data. We evaluate the dependence\nof the performance of a seller on the inventory of items the seller has, the\nnumber of connections it has with the other sellers, and the commissions which\nthe seller gets by selling items of other sellers to its users.\n", "versions": [{"version": "v1", "created": "Thu, 26 Sep 2013 02:01:44 GMT"}, {"version": "v2", "created": "Wed, 22 Jan 2014 02:42:52 GMT"}], "update_date": "2015-03-25", "authors_parsed": [["Tekin", "Cem", ""], ["Zhang", "Simpson", ""], ["van der Schaar", "Mihaela", ""]]}, {"id": "1309.6786", "submitter": "Ulrich Paquet", "authors": "Ulrich Paquet, Noam Koenigstein", "title": "One-class Collaborative Filtering with Random Graphs: Annotated Version", "comments": "11 pages, 7 figures. Detailed, annotated and expanded version of\n  conference paper \"One-class Collaborative Filtering with Random Graphs\" (WWW\n  2013)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The bane of one-class collaborative filtering is interpreting and modelling\nthe latent signal from the missing class. In this paper we present a novel\nBayesian generative model for implicit collaborative filtering. It forms a core\ncomponent of the Xbox Live architecture, and unlike previous approaches,\ndelineates the odds of a user disliking an item from simply not considering it.\nThe latent signal is treated as an unobserved random graph connecting users\nwith items they might have encountered. We demonstrate how large-scale\ndistributed learning can be achieved through a combination of stochastic\ngradient descent and mean field variational inference over random graph\nsamples. A fine-grained comparison is done against a state of the art baseline\non real world data.\n", "versions": [{"version": "v1", "created": "Thu, 26 Sep 2013 10:32:43 GMT"}, {"version": "v2", "created": "Fri, 25 Oct 2013 13:45:46 GMT"}, {"version": "v3", "created": "Mon, 21 Jul 2014 08:50:30 GMT"}, {"version": "v4", "created": "Wed, 24 Sep 2014 09:25:09 GMT"}], "update_date": "2014-09-25", "authors_parsed": [["Paquet", "Ulrich", ""], ["Koenigstein", "Noam", ""]]}, {"id": "1309.6811", "submitter": "Tameem Adel", "authors": "Tameem Adel, Benn Smith, Ruth Urner, Daniel Stashuk, Daniel J. Lizotte", "title": "Generative Multiple-Instance Learning Models For Quantitative\n  Electromyography", "comments": "Appears in Proceedings of the Twenty-Ninth Conference on Uncertainty\n  in Artificial Intelligence (UAI2013)", "journal-ref": null, "doi": null, "report-no": "UAI-P-2013-PG-2-11", "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a comprehensive study of the use of generative modeling approaches\nfor Multiple-Instance Learning (MIL) problems. In MIL a learner receives\ntraining instances grouped together into bags with labels for the bags only\n(which might not be correct for the comprised instances). Our work was\nmotivated by the task of facilitating the diagnosis of neuromuscular disorders\nusing sets of motor unit potential trains (MUPTs) detected within a muscle\nwhich can be cast as a MIL problem. Our approach leads to a state-of-the-art\nsolution to the problem of muscle classification. By introducing and analyzing\ngenerative models for MIL in a general framework and examining a variety of\nmodel structures and components, our work also serves as a methodological guide\nto modelling MIL tasks. We evaluate our proposed methods both on MUPT datasets\nand on the MUSK1 dataset, one of the most widely used benchmarks for MIL.\n", "versions": [{"version": "v1", "created": "Thu, 26 Sep 2013 12:26:53 GMT"}], "update_date": "2013-09-27", "authors_parsed": [["Adel", "Tameem", ""], ["Smith", "Benn", ""], ["Urner", "Ruth", ""], ["Stashuk", "Daniel", ""], ["Lizotte", "Daniel J.", ""]]}, {"id": "1309.6812", "submitter": "Saeed Amizadeh", "authors": "Saeed Amizadeh, Bo Thiesson, Milos Hauskrecht", "title": "The Bregman Variational Dual-Tree Framework", "comments": "Appears in Proceedings of the Twenty-Ninth Conference on Uncertainty\n  in Artificial Intelligence (UAI2013)", "journal-ref": null, "doi": null, "report-no": "UAI-P-2013-PG-22-31", "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Graph-based methods provide a powerful tool set for many non-parametric\nframeworks in Machine Learning. In general, the memory and computational\ncomplexity of these methods is quadratic in the number of examples in the data\nwhich makes them quickly infeasible for moderate to large scale datasets. A\nsignificant effort to find more efficient solutions to the problem has been\nmade in the literature. One of the state-of-the-art methods that has been\nrecently introduced is the Variational Dual-Tree (VDT) framework. Despite some\nof its unique features, VDT is currently restricted only to Euclidean spaces\nwhere the Euclidean distance quantifies the similarity. In this paper, we\nextend the VDT framework beyond the Euclidean distance to more general Bregman\ndivergences that include the Euclidean distance as a special case. By\nexploiting the properties of the general Bregman divergence, we show how the\nnew framework can maintain all the pivotal features of the VDT framework and\nyet significantly improve its performance in non-Euclidean domains. We apply\nthe proposed framework to different text categorization problems and\ndemonstrate its benefits over the original VDT.\n", "versions": [{"version": "v1", "created": "Thu, 26 Sep 2013 12:28:35 GMT"}], "update_date": "2013-09-27", "authors_parsed": [["Amizadeh", "Saeed", ""], ["Thiesson", "Bo", ""], ["Hauskrecht", "Milos", ""]]}, {"id": "1309.6813", "submitter": "Stephen Bach", "authors": "Stephen Bach, Bert Huang, Ben London, Lise Getoor", "title": "Hinge-loss Markov Random Fields: Convex Inference for Structured\n  Prediction", "comments": "Appears in Proceedings of the Twenty-Ninth Conference on Uncertainty\n  in Artificial Intelligence (UAI2013)", "journal-ref": null, "doi": null, "report-no": "UAI-P-2013-PG-32-41", "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Graphical models for structured domains are powerful tools, but the\ncomputational complexities of combinatorial prediction spaces can force\nrestrictions on models, or require approximate inference in order to be\ntractable. Instead of working in a combinatorial space, we use hinge-loss\nMarkov random fields (HL-MRFs), an expressive class of graphical models with\nlog-concave density functions over continuous variables, which can represent\nconfidences in discrete predictions. This paper demonstrates that HL-MRFs are\ngeneral tools for fast and accurate structured prediction. We introduce the\nfirst inference algorithm that is both scalable and applicable to the full\nclass of HL-MRFs, and show how to train HL-MRFs with several learning\nalgorithms. Our experiments show that HL-MRFs match or surpass the predictive\nperformance of state-of-the-art methods, including discrete models, in four\napplication domains.\n", "versions": [{"version": "v1", "created": "Thu, 26 Sep 2013 12:28:52 GMT"}], "update_date": "2013-09-27", "authors_parsed": [["Bach", "Stephen", ""], ["Huang", "Bert", ""], ["London", "Ben", ""], ["Getoor", "Lise", ""]]}, {"id": "1309.6814", "submitter": "Krishnakumar Balasubramanian", "authors": "Krishnakumar Balasubramanian, Kai Yu, Tong Zhang", "title": "High-dimensional Joint Sparsity Random Effects Model for Multi-task\n  Learning", "comments": "Appears in Proceedings of the Twenty-Ninth Conference on Uncertainty\n  in Artificial Intelligence (UAI2013)", "journal-ref": null, "doi": null, "report-no": "UAI-P-2013-PG-42-51", "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Joint sparsity regularization in multi-task learning has attracted much\nattention in recent years. The traditional convex formulation employs the group\nLasso relaxation to achieve joint sparsity across tasks. Although this approach\nleads to a simple convex formulation, it suffers from several issues due to the\nlooseness of the relaxation. To remedy this problem, we view jointly sparse\nmulti-task learning as a specialized random effects model, and derive a convex\nrelaxation approach that involves two steps. The first step learns the\ncovariance matrix of the coefficients using a convex formulation which we refer\nto as sparse covariance coding; the second step solves a ridge regression\nproblem with a sparse quadratic regularizer based on the covariance matrix\nobtained in the first step. It is shown that this approach produces an\nasymptotically optimal quadratic regularizer in the multitask learning setting\nwhen the number of tasks approaches infinity. Experimental results demonstrate\nthat the convex formulation obtained via the proposed model significantly\noutperforms group Lasso (and related multi-stage formulations\n", "versions": [{"version": "v1", "created": "Thu, 26 Sep 2013 12:29:19 GMT"}], "update_date": "2013-09-27", "authors_parsed": [["Balasubramanian", "Krishnakumar", ""], ["Yu", "Kai", ""], ["Zhang", "Tong", ""]]}, {"id": "1309.6818", "submitter": "Jakramate Bootkrajang", "authors": "Jakramate Bootkrajang, Ata Kaban", "title": "Boosting in the presence of label noise", "comments": "Appears in Proceedings of the Twenty-Ninth Conference on Uncertainty\n  in Artificial Intelligence (UAI2013)", "journal-ref": null, "doi": null, "report-no": "UAI-P-2013-PG-82-91", "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Boosting is known to be sensitive to label noise. We studied two approaches\nto improve AdaBoost's robustness against labelling errors. One is to employ a\nlabel-noise robust classifier as a base learner, while the other is to modify\nthe AdaBoost algorithm to be more robust. Empirical evaluation shows that a\ncommittee of robust classifiers, although converges faster than non label-noise\naware AdaBoost, is still susceptible to label noise. However, pairing it with\nthe new robust Boosting algorithm we propose here results in a more resilient\nalgorithm under mislabelling.\n", "versions": [{"version": "v1", "created": "Thu, 26 Sep 2013 12:35:03 GMT"}], "update_date": "2013-09-27", "authors_parsed": [["Bootkrajang", "Jakramate", ""], ["Kaban", "Ata", ""]]}, {"id": "1309.6819", "submitter": "Byron Boots", "authors": "Byron Boots, Geoffrey Gordon, Arthur Gretton", "title": "Hilbert Space Embeddings of Predictive State Representations", "comments": "Appears in Proceedings of the Twenty-Ninth Conference on Uncertainty\n  in Artificial Intelligence (UAI2013)", "journal-ref": null, "doi": null, "report-no": "UAI-P-2013-PG-92-101", "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Predictive State Representations (PSRs) are an expressive class of models for\ncontrolled stochastic processes. PSRs represent state as a set of predictions\nof future observable events. Because PSRs are defined entirely in terms of\nobservable data, statistically consistent estimates of PSR parameters can be\nlearned efficiently by manipulating moments of observed training data. Most\nlearning algorithms for PSRs have assumed that actions and observations are\nfinite with low cardinality. In this paper, we generalize PSRs to infinite sets\nof observations and actions, using the recent concept of Hilbert space\nembeddings of distributions. The essence is to represent the state as a\nnonparametric conditional embedding operator in a Reproducing Kernel Hilbert\nSpace (RKHS) and leverage recent work in kernel methods to estimate, predict,\nand update the representation. We show that these Hilbert space embeddings of\nPSRs are able to gracefully handle continuous actions and observations, and\nthat our learned models outperform competing system identification algorithms\non several prediction benchmarks.\n", "versions": [{"version": "v1", "created": "Thu, 26 Sep 2013 12:35:19 GMT"}], "update_date": "2013-09-27", "authors_parsed": [["Boots", "Byron", ""], ["Gordon", "Geoffrey", ""], ["Gretton", "Arthur", ""]]}, {"id": "1309.6820", "submitter": "Eliot Brenner", "authors": "Eliot Brenner, David Sontag", "title": "SparsityBoost: A New Scoring Function for Learning Bayesian Network\n  Structure", "comments": "Appears in Proceedings of the Twenty-Ninth Conference on Uncertainty\n  in Artificial Intelligence (UAI2013)", "journal-ref": null, "doi": null, "report-no": "UAI-P-2013-PG-112-121", "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We give a new consistent scoring function for structure learning of Bayesian\nnetworks. In contrast to traditional approaches to scorebased structure\nlearning, such as BDeu or MDL, the complexity penalty that we propose is\ndata-dependent and is given by the probability that a conditional independence\ntest correctly shows that an edge cannot exist. What really distinguishes this\nnew scoring function from earlier work is that it has the property of becoming\ncomputationally easier to maximize as the amount of data increases. We prove a\npolynomial sample complexity result, showing that maximizing this score is\nguaranteed to correctly learn a structure with no false edges and a\ndistribution close to the generating distribution, whenever there exists a\nBayesian network which is a perfect map for the data generating distribution.\nAlthough the new score can be used with any search algorithm, we give empirical\nresults showing that it is particularly effective when used together with a\nlinear programming relaxation approach to Bayesian network structure learning.\n", "versions": [{"version": "v1", "created": "Thu, 26 Sep 2013 12:35:41 GMT"}], "update_date": "2013-09-27", "authors_parsed": [["Brenner", "Eliot", ""], ["Sontag", "David", ""]]}, {"id": "1309.6821", "submitter": "Emma Brunskill", "authors": "Emma Brunskill, Lihong Li", "title": "Sample Complexity of Multi-task Reinforcement Learning", "comments": "Appears in Proceedings of the Twenty-Ninth Conference on Uncertainty\n  in Artificial Intelligence (UAI2013)", "journal-ref": null, "doi": null, "report-no": "UAI-P-2013-PG-122-131", "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Transferring knowledge across a sequence of reinforcement-learning tasks is\nchallenging, and has a number of important applications. Though there is\nencouraging empirical evidence that transfer can improve performance in\nsubsequent reinforcement-learning tasks, there has been very little theoretical\nanalysis. In this paper, we introduce a new multi-task algorithm for a sequence\nof reinforcement-learning tasks when each task is sampled independently from\n(an unknown) distribution over a finite set of Markov decision processes whose\nparameters are initially unknown. For this setting, we prove under certain\nassumptions that the per-task sample complexity of exploration is reduced\nsignificantly due to transfer compared to standard single-task algorithms. Our\nmulti-task algorithm also has the desired characteristic that it is guaranteed\nnot to exhibit negative transfer: in the worst case its per-task sample\ncomplexity is comparable to the corresponding single-task algorithm.\n", "versions": [{"version": "v1", "created": "Thu, 26 Sep 2013 12:36:00 GMT"}], "update_date": "2013-09-27", "authors_parsed": [["Brunskill", "Emma", ""], ["Li", "Lihong", ""]]}, {"id": "1309.6823", "submitter": "Hao Cheng", "authors": "Hao Cheng, Xinhua Zhang, Dale Schuurmans", "title": "Convex Relaxations of Bregman Divergence Clustering", "comments": "Appears in Proceedings of the Twenty-Ninth Conference on Uncertainty\n  in Artificial Intelligence (UAI2013)", "journal-ref": null, "doi": null, "report-no": "UAI-P-2013-PG-162-171", "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Although many convex relaxations of clustering have been proposed in the past\ndecade, current formulations remain restricted to spherical Gaussian or\ndiscriminative models and are susceptible to imbalanced clusters. To address\nthese shortcomings, we propose a new class of convex relaxations that can be\nflexibly applied to more general forms of Bregman divergence clustering. By\nbasing these new formulations on normalized equivalence relations we retain\nadditional control on relaxation quality, which allows improvement in\nclustering quality. We furthermore develop optimization methods that improve\nscalability by exploiting recent implicit matrix norm methods. In practice, we\nfind that the new formulations are able to efficiently produce tighter\nclusterings that improve the accuracy of state of the art methods.\n", "versions": [{"version": "v1", "created": "Thu, 26 Sep 2013 12:36:30 GMT"}], "update_date": "2013-09-27", "authors_parsed": [["Cheng", "Hao", ""], ["Zhang", "Xinhua", ""], ["Schuurmans", "Dale", ""]]}, {"id": "1309.6829", "submitter": "Qiang Fu", "authors": "Qiang Fu, Huahua Wang, Arindam Banerjee", "title": "Bethe-ADMM for Tree Decomposition based Parallel MAP Inference", "comments": "Appears in Proceedings of the Twenty-Ninth Conference on Uncertainty\n  in Artificial Intelligence (UAI2013)", "journal-ref": null, "doi": null, "report-no": "UAI-P-2013-PG-222-231", "categories": "cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of maximum a posteriori (MAP) inference in discrete\ngraphical models. We present a parallel MAP inference algorithm called\nBethe-ADMM based on two ideas: tree-decomposition of the graph and the\nalternating direction method of multipliers (ADMM). However, unlike the\nstandard ADMM, we use an inexact ADMM augmented with a Bethe-divergence based\nproximal function, which makes each subproblem in ADMM easy to solve in\nparallel using the sum-product algorithm. We rigorously prove global\nconvergence of Bethe-ADMM. The proposed algorithm is extensively evaluated on\nboth synthetic and real datasets to illustrate its effectiveness. Further, the\nparallel Bethe-ADMM is shown to scale almost linearly with increasing number of\ncores.\n", "versions": [{"version": "v1", "created": "Thu, 26 Sep 2013 12:38:09 GMT"}], "update_date": "2013-09-27", "authors_parsed": [["Fu", "Qiang", ""], ["Wang", "Huahua", ""], ["Banerjee", "Arindam", ""]]}, {"id": "1309.6830", "submitter": "Ravi Ganti", "authors": "Ravi Ganti, Alexander G. Gray", "title": "Building Bridges: Viewing Active Learning from the Multi-Armed Bandit\n  Lens", "comments": "Appears in Proceedings of the Twenty-Ninth Conference on Uncertainty\n  in Artificial Intelligence (UAI2013)", "journal-ref": null, "doi": null, "report-no": "UAI-P-2013-PG-232-241", "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we propose a multi-armed bandit inspired, pool based active\nlearning algorithm for the problem of binary classification. By carefully\nconstructing an analogy between active learning and multi-armed bandits, we\nutilize ideas such as lower confidence bounds, and self-concordant\nregularization from the multi-armed bandit literature to design our proposed\nalgorithm. Our algorithm is a sequential algorithm, which in each round assigns\na sampling distribution on the pool, samples one point from this distribution,\nand queries the oracle for the label of this sampled point. The design of this\nsampling distribution is also inspired by the analogy between active learning\nand multi-armed bandits. We show how to derive lower confidence bounds required\nby our algorithm. Experimental comparisons to previously proposed active\nlearning algorithms show superior performance on some standard UCI datasets.\n", "versions": [{"version": "v1", "created": "Thu, 26 Sep 2013 12:39:01 GMT"}], "update_date": "2013-09-27", "authors_parsed": [["Ganti", "Ravi", ""], ["Gray", "Alexander G.", ""]]}, {"id": "1309.6831", "submitter": "Alborz Geramifard", "authors": "Alborz Geramifard, Thomas J. Walsh, Nicholas Roy, Jonathan How", "title": "Batch-iFDD for Representation Expansion in Large MDPs", "comments": "Appears in Proceedings of the Twenty-Ninth Conference on Uncertainty\n  in Artificial Intelligence (UAI2013)", "journal-ref": null, "doi": null, "report-no": "UAI-P-2013-PG-242-251", "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Matching pursuit (MP) methods are a promising class of feature construction\nalgorithms for value function approximation. Yet existing MP methods require\ncreating a pool of potential features, mandating expert knowledge or\nenumeration of a large feature pool, both of which hinder scalability. This\npaper introduces batch incremental feature dependency discovery (Batch-iFDD) as\nan MP method that inherits a provable convergence property. Additionally,\nBatch-iFDD does not require a large pool of features, leading to lower\ncomputational complexity. Empirical policy evaluation results across three\ndomains with up to one million states highlight the scalability of Batch-iFDD\nover the previous state of the art MP algorithm.\n", "versions": [{"version": "v1", "created": "Thu, 26 Sep 2013 12:39:19 GMT"}], "update_date": "2013-09-27", "authors_parsed": [["Geramifard", "Alborz", ""], ["Walsh", "Thomas J.", ""], ["Roy", "Nicholas", ""], ["How", "Jonathan", ""]]}, {"id": "1309.6833", "submitter": "Hossein Hajimirsadeghi", "authors": "Hossein Hajimirsadeghi, Jinling Li, Greg Mori, Mohammad Zaki, Tarek\n  Sayed", "title": "Multiple Instance Learning by Discriminative Training of Markov Networks", "comments": "Appears in Proceedings of the Twenty-Ninth Conference on Uncertainty\n  in Artificial Intelligence (UAI2013)", "journal-ref": null, "doi": null, "report-no": "UAI-P-2013-PG-262-271", "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a graphical framework for multiple instance learning (MIL) based\non Markov networks. This framework can be used to model the traditional MIL\ndefinition as well as more general MIL definitions. Different levels of\nambiguity -- the portion of positive instances in a bag -- can be explored in\nweakly supervised data. To train these models, we propose a discriminative\nmax-margin learning algorithm leveraging efficient inference for\ncardinality-based cliques. The efficacy of the proposed framework is evaluated\non a variety of data sets. Experimental results verify that encoding or\nlearning the degree of ambiguity can improve classification performance.\n", "versions": [{"version": "v1", "created": "Thu, 26 Sep 2013 12:40:19 GMT"}], "update_date": "2013-09-27", "authors_parsed": [["Hajimirsadeghi", "Hossein", ""], ["Li", "Jinling", ""], ["Mori", "Greg", ""], ["Zaki", "Mohammad", ""], ["Sayed", "Tarek", ""]]}, {"id": "1309.6834", "submitter": "Yonatan Halpern", "authors": "Yonatan Halpern, David Sontag", "title": "Unsupervised Learning of Noisy-Or Bayesian Networks", "comments": "Appears in Proceedings of the Twenty-Ninth Conference on Uncertainty\n  in Artificial Intelligence (UAI2013)", "journal-ref": null, "doi": null, "report-no": "UAI-P-2013-PG-272-281", "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper considers the problem of learning the parameters in Bayesian\nnetworks of discrete variables with known structure and hidden variables.\nPrevious approaches in these settings typically use expectation maximization;\nwhen the network has high treewidth, the required expectations might be\napproximated using Monte Carlo or variational methods. We show how to avoid\ninference altogether during learning by giving a polynomial-time algorithm\nbased on the method-of-moments, building upon recent work on learning\ndiscrete-valued mixture models. In particular, we show how to learn the\nparameters for a family of bipartite noisy-or Bayesian networks. In our\nexperimental results, we demonstrate an application of our algorithm to\nlearning QMR-DT, a large Bayesian network used for medical diagnosis. We show\nthat it is possible to fully learn the parameters of QMR-DT even when only the\nfindings are observed in the training data (ground truth diseases unknown).\n", "versions": [{"version": "v1", "created": "Thu, 26 Sep 2013 12:40:36 GMT"}], "update_date": "2013-09-27", "authors_parsed": [["Halpern", "Yonatan", ""], ["Sontag", "David", ""]]}, {"id": "1309.6835", "submitter": "James Hensman", "authors": "James Hensman, Nicolo Fusi, Neil D. Lawrence", "title": "Gaussian Processes for Big Data", "comments": "Appears in Proceedings of the Twenty-Ninth Conference on Uncertainty\n  in Artificial Intelligence (UAI2013)", "journal-ref": null, "doi": null, "report-no": "UAI-P-2013-PG-282-290", "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce stochastic variational inference for Gaussian process models.\nThis enables the application of Gaussian process (GP) models to data sets\ncontaining millions of data points. We show how GPs can be vari- ationally\ndecomposed to depend on a set of globally relevant inducing variables which\nfactorize the model in the necessary manner to perform variational inference.\nOur ap- proach is readily extended to models with non-Gaussian likelihoods and\nlatent variable models based around Gaussian processes. We demonstrate the\napproach on a simple toy problem and two real world data sets.\n", "versions": [{"version": "v1", "created": "Thu, 26 Sep 2013 12:41:06 GMT"}], "update_date": "2013-09-27", "authors_parsed": [["Hensman", "James", ""], ["Fusi", "Nicolo", ""], ["Lawrence", "Neil D.", ""]]}, {"id": "1309.6838", "submitter": "Jean Honorio", "authors": "Jean Honorio, Tommi S. Jaakkola", "title": "Inverse Covariance Estimation for High-Dimensional Data in Linear Time\n  and Space: Spectral Methods for Riccati and Sparse Models", "comments": "Appears in Proceedings of the Twenty-Ninth Conference on Uncertainty\n  in Artificial Intelligence (UAI2013)", "journal-ref": "Uncertainty in Artificial Intelligence (UAI), 2013", "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose maximum likelihood estimation for learning Gaussian graphical\nmodels with a Gaussian (ell_2^2) prior on the parameters. This is in contrast\nto the commonly used Laplace (ell_1) prior for encouraging sparseness. We show\nthat our optimization problem leads to a Riccati matrix equation, which has a\nclosed form solution. We propose an efficient algorithm that performs a\nsingular value decomposition of the training data. Our algorithm is\nO(NT^2)-time and O(NT)-space for N variables and T samples. Our method is\ntailored to high-dimensional problems (N gg T), in which sparseness promoting\nmethods become intractable. Furthermore, instead of obtaining a single solution\nfor a specific regularization parameter, our algorithm finds the whole solution\npath. We show that the method has logarithmic sample complexity under the\nspiked covariance model. We also propose sparsification of the dense solution\nwith provable performance guarantees. We provide techniques for using our\nlearnt models, such as removing unimportant variables, computing likelihoods\nand conditional distributions. Finally, we show promising results in several\ngene expressions datasets.\n", "versions": [{"version": "v1", "created": "Thu, 26 Sep 2013 12:41:38 GMT"}], "update_date": "2018-11-16", "authors_parsed": [["Honorio", "Jean", ""], ["Jaakkola", "Tommi S.", ""]]}, {"id": "1309.6840", "submitter": "Oluwasanmi Koyejo", "authors": "Oluwasanmi Koyejo, Joydeep Ghosh", "title": "Constrained Bayesian Inference for Low Rank Multitask Learning", "comments": "Appears in Proceedings of the Twenty-Ninth Conference on Uncertainty\n  in Artificial Intelligence (UAI2013)", "journal-ref": null, "doi": null, "report-no": "UAI-P-2013-PG-341-350", "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a novel approach for constrained Bayesian inference. Unlike\ncurrent methods, our approach does not require convexity of the constraint set.\nWe reduce the constrained variational inference to a parametric optimization\nover the feasible set of densities and propose a general recipe for such\nproblems. We apply the proposed constrained Bayesian inference approach to\nmultitask learning subject to rank constraints on the weight matrix. Further,\nconstrained parameter estimation is applied to recover the sparse conditional\nindependence structure encoded by prior precision matrices. Our approach is\nmotivated by reverse inference for high dimensional functional neuroimaging, a\ndomain where the high dimensionality and small number of examples requires the\nuse of constraints to ensure meaningful and effective models. For this\napplication, we propose a model that jointly learns a weight matrix and the\nprior inverse covariance structure between different tasks. We present\nexperimental validation showing that the proposed approach outperforms strong\nbaseline models in terms of predictive performance and structure recovery.\n", "versions": [{"version": "v1", "created": "Thu, 26 Sep 2013 12:42:25 GMT"}], "update_date": "2013-09-27", "authors_parsed": [["Koyejo", "Oluwasanmi", ""], ["Ghosh", "Joydeep", ""]]}, {"id": "1309.6847", "submitter": "Ofer Meshi", "authors": "Ofer Meshi, Elad Eban, Gal Elidan, Amir Globerson", "title": "Learning Max-Margin Tree Predictors", "comments": "Appears in Proceedings of the Twenty-Ninth Conference on Uncertainty\n  in Artificial Intelligence (UAI2013)", "journal-ref": null, "doi": null, "report-no": "UAI-P-2013-PG-411-420", "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Structured prediction is a powerful framework for coping with joint\nprediction of interacting outputs. A central difficulty in using this framework\nis that often the correct label dependence structure is unknown. At the same\ntime, we would like to avoid an overly complex structure that will lead to\nintractable prediction. In this work we address the challenge of learning tree\nstructured predictive models that achieve high accuracy while at the same time\nfacilitate efficient (linear time) inference. We start by proving that this\ntask is in general NP-hard, and then suggest an approximate alternative.\nBriefly, our CRANK approach relies on a novel Circuit-RANK regularizer that\npenalizes non-tree structures and that can be optimized using a CCCP procedure.\nWe demonstrate the effectiveness of our approach on several domains and show\nthat, despite the relative simplicity of the structure, prediction accuracy is\ncompetitive with a fully connected model that is computationally costly at\nprediction time.\n", "versions": [{"version": "v1", "created": "Thu, 26 Sep 2013 12:45:00 GMT"}], "update_date": "2013-09-27", "authors_parsed": [["Meshi", "Ofer", ""], ["Eban", "Elad", ""], ["Elidan", "Gal", ""], ["Globerson", "Amir", ""]]}, {"id": "1309.6849", "submitter": "Joris Mooij", "authors": "Joris Mooij, Tom Heskes", "title": "Cyclic Causal Discovery from Continuous Equilibrium Data", "comments": "Appears in Proceedings of the Twenty-Ninth Conference on Uncertainty\n  in Artificial Intelligence (UAI2013)", "journal-ref": null, "doi": null, "report-no": "UAI-P-2013-PG-431-439", "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a method for learning cyclic causal models from a combination of\nobservational and interventional equilibrium data. Novel aspects of the\nproposed method are its ability to work with continuous data (without assuming\nlinearity) and to deal with feedback loops. Within the context of biochemical\nreactions, we also propose a novel way of modeling interventions that modify\nthe activity of compounds instead of their abundance. For computational\nreasons, we approximate the nonlinear causal mechanisms by (coupled) local\nlinearizations, one for each experimental condition. We apply the method to\nreconstruct a cellular signaling network from the flow cytometry data measured\nby Sachs et al. (2005). We show that our method finds evidence in the data for\nfeedback loops and that it gives a more accurate quantitative description of\nthe data at comparable model complexity.\n", "versions": [{"version": "v1", "created": "Thu, 26 Sep 2013 12:45:43 GMT"}], "update_date": "2013-09-27", "authors_parsed": [["Mooij", "Joris", ""], ["Heskes", "Tom", ""]]}, {"id": "1309.6850", "submitter": "Kiyohito Nagano", "authors": "Kiyohito Nagano, Yoshinobu Kawahara", "title": "Structured Convex Optimization under Submodular Constraints", "comments": "Appears in Proceedings of the Twenty-Ninth Conference on Uncertainty\n  in Artificial Intelligence (UAI2013)", "journal-ref": null, "doi": null, "report-no": "UAI-P-2013-PG-459-468", "categories": "cs.LG cs.DS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A number of discrete and continuous optimization problems in machine learning\nare related to convex minimization problems under submodular constraints. In\nthis paper, we deal with a submodular function with a directed graph structure,\nand we show that a wide range of convex optimization problems under submodular\nconstraints can be solved much more efficiently than general submodular\noptimization methods by a reduction to a maximum flow problem. Furthermore, we\ngive some applications, including sparse optimization methods, in which the\nproposed methods are effective. Additionally, we evaluate the performance of\nthe proposed method through computational experiments.\n", "versions": [{"version": "v1", "created": "Thu, 26 Sep 2013 12:45:59 GMT"}], "update_date": "2013-09-27", "authors_parsed": [["Nagano", "Kiyohito", ""], ["Kawahara", "Yoshinobu", ""]]}, {"id": "1309.6851", "submitter": "Teppo Niinimaki", "authors": "Teppo Niinimaki, Mikko Koivisto", "title": "Treedy: A Heuristic for Counting and Sampling Subsets", "comments": "Appears in Proceedings of the Twenty-Ninth Conference on Uncertainty\n  in Artificial Intelligence (UAI2013)", "journal-ref": null, "doi": null, "report-no": "UAI-P-2013-PG-469-477", "categories": "cs.DS cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Consider a collection of weighted subsets of a ground set N. Given a query\nsubset Q of N, how fast can one (1) find the weighted sum over all subsets of\nQ, and (2) sample a subset of Q proportionally to the weights? We present a\ntree-based greedy heuristic, Treedy, that for a given positive tolerance d\nanswers such counting and sampling queries to within a guaranteed relative\nerror d and total variation distance d, respectively. Experimental results on\nartificial instances and in application to Bayesian structure discovery in\nBayesian networks show that approximations yield dramatic savings in running\ntime compared to exact computation, and that Treedy typically outperforms a\npreviously proposed sorting-based heuristic.\n", "versions": [{"version": "v1", "created": "Thu, 26 Sep 2013 12:46:13 GMT"}], "update_date": "2013-09-27", "authors_parsed": [["Niinimaki", "Teppo", ""], ["Koivisto", "Mikko", ""]]}, {"id": "1309.6852", "submitter": "Shuzi Niu", "authors": "Shuzi Niu, Yanyan Lan, Jiafeng Guo, Xueqi Cheng", "title": "Stochastic Rank Aggregation", "comments": "Appears in Proceedings of the Twenty-Ninth Conference on Uncertainty\n  in Artificial Intelligence (UAI2013)", "journal-ref": null, "doi": null, "report-no": "UAI-P-2013-PG-478-487", "categories": "cs.LG cs.IR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper addresses the problem of rank aggregation, which aims to find a\nconsensus ranking among multiple ranking inputs. Traditional rank aggregation\nmethods are deterministic, and can be categorized into explicit and implicit\nmethods depending on whether rank information is explicitly or implicitly\nutilized. Surprisingly, experimental results on real data sets show that\nexplicit rank aggregation methods would not work as well as implicit methods,\nalthough rank information is critical for the task. Our analysis indicates that\nthe major reason might be the unreliable rank information from incomplete\nranking inputs. To solve this problem, we propose to incorporate uncertainty\ninto rank aggregation and tackle the problem in both unsupervised and\nsupervised scenario. We call this novel framework {stochastic rank aggregation}\n(St.Agg for short). Specifically, we introduce a prior distribution on ranks,\nand transform the ranking functions or objectives in traditional explicit\nmethods to their expectations over this distribution. Our experiments on\nbenchmark data sets show that the proposed St.Agg outperforms the baselines in\nboth unsupervised and supervised scenarios.\n", "versions": [{"version": "v1", "created": "Thu, 26 Sep 2013 12:46:39 GMT"}], "update_date": "2013-09-27", "authors_parsed": [["Niu", "Shuzi", ""], ["Lan", "Yanyan", ""], ["Guo", "Jiafeng", ""], ["Cheng", "Xueqi", ""]]}, {"id": "1309.6858", "submitter": "Novi Quadrianto", "authors": "Novi Quadrianto, Viktoriia Sharmanska, David A. Knowles, Zoubin\n  Ghahramani", "title": "The Supervised IBP: Neighbourhood Preserving Infinite Latent Feature\n  Models", "comments": "Appears in Proceedings of the Twenty-Ninth Conference on Uncertainty\n  in Artificial Intelligence (UAI2013)", "journal-ref": null, "doi": null, "report-no": "UAI-P-2013-PG-527-536", "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a probabilistic model to infer supervised latent variables in the\nHamming space from observed data. Our model allows simultaneous inference of\nthe number of binary latent variables, and their values. The latent variables\npreserve neighbourhood structure of the data in a sense that objects in the\nsame semantic concept have similar latent values, and objects in different\nconcepts have dissimilar latent values. We formulate the supervised infinite\nlatent variable problem based on an intuitive principle of pulling objects\ntogether if they are of the same type, and pushing them apart if they are not.\nWe then combine this principle with a flexible Indian Buffet Process prior on\nthe latent variables. We show that the inferred supervised latent variables can\nbe directly used to perform a nearest neighbour search for the purpose of\nretrieval. We introduce a new application of dynamically extending hash codes,\nand show how to effectively couple the structure of the hash codes with\ncontinuously growing structure of the neighbourhood preserving infinite latent\nfeature space.\n", "versions": [{"version": "v1", "created": "Thu, 26 Sep 2013 12:49:02 GMT"}], "update_date": "2013-09-27", "authors_parsed": [["Quadrianto", "Novi", ""], ["Sharmanska", "Viktoriia", ""], ["Knowles", "David A.", ""], ["Ghahramani", "Zoubin", ""]]}, {"id": "1309.6860", "submitter": "Eleni Sgouritsa", "authors": "Eleni Sgouritsa, Dominik Janzing, Jonas Peters, Bernhard Schoelkopf", "title": "Identifying Finite Mixtures of Nonparametric Product Distributions and\n  Causal Inference of Confounders", "comments": "Appears in Proceedings of the Twenty-Ninth Conference on Uncertainty\n  in Artificial Intelligence (UAI2013)", "journal-ref": null, "doi": null, "report-no": "UAI-P-2013-PG-556-565", "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a kernel method to identify finite mixtures of nonparametric\nproduct distributions. It is based on a Hilbert space embedding of the joint\ndistribution. The rank of the constructed tensor is equal to the number of\nmixture components. We present an algorithm to recover the components by\npartitioning the data points into clusters such that the variables are jointly\nconditionally independent given the cluster. This method can be used to\nidentify finite confounders.\n", "versions": [{"version": "v1", "created": "Thu, 26 Sep 2013 12:49:46 GMT"}], "update_date": "2013-09-27", "authors_parsed": [["Sgouritsa", "Eleni", ""], ["Janzing", "Dominik", ""], ["Peters", "Jonas", ""], ["Schoelkopf", "Bernhard", ""]]}, {"id": "1309.6862", "submitter": "Amar Shah", "authors": "Amar Shah, Zoubin Ghahramani", "title": "Determinantal Clustering Processes - A Nonparametric Bayesian Approach\n  to Kernel Based Semi-Supervised Clustering", "comments": "Appears in Proceedings of the Twenty-Ninth Conference on Uncertainty\n  in Artificial Intelligence (UAI2013)", "journal-ref": null, "doi": null, "report-no": "UAI-P-2013-PG-566-575", "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Semi-supervised clustering is the task of clustering data points into\nclusters where only a fraction of the points are labelled. The true number of\nclusters in the data is often unknown and most models require this parameter as\nan input. Dirichlet process mixture models are appealing as they can infer the\nnumber of clusters from the data. However, these models do not deal with high\ndimensional data well and can encounter difficulties in inference. We present a\nnovel nonparameteric Bayesian kernel based method to cluster data points\nwithout the need to prespecify the number of clusters or to model complicated\ndensities from which data points are assumed to be generated from. The key\ninsight is to use determinants of submatrices of a kernel matrix as a measure\nof how close together a set of points are. We explore some theoretical\nproperties of the model and derive a natural Gibbs based algorithm with MCMC\nhyperparameter learning. The model is implemented on a variety of synthetic and\nreal world data sets.\n", "versions": [{"version": "v1", "created": "Thu, 26 Sep 2013 12:50:04 GMT"}], "update_date": "2013-09-27", "authors_parsed": [["Shah", "Amar", ""], ["Ghahramani", "Zoubin", ""]]}, {"id": "1309.6863", "submitter": "Ilya Shpitser", "authors": "Ilya Shpitser, Robin J. Evans, Thomas S. Richardson, James M. Robins", "title": "Sparse Nested Markov models with Log-linear Parameters", "comments": "Appears in Proceedings of the Twenty-Ninth Conference on Uncertainty\n  in Artificial Intelligence (UAI2013)", "journal-ref": null, "doi": null, "report-no": "UAI-P-2013-PG-576-585", "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hidden variables are ubiquitous in practical data analysis, and therefore\nmodeling marginal densities and doing inference with the resulting models is an\nimportant problem in statistics, machine learning, and causal inference.\nRecently, a new type of graphical model, called the nested Markov model, was\ndeveloped which captures equality constraints found in marginals of directed\nacyclic graph (DAG) models. Some of these constraints, such as the so called\n`Verma constraint', strictly generalize conditional independence. To make\nmodeling and inference with nested Markov models practical, it is necessary to\nlimit the number of parameters in the model, while still correctly capturing\nthe constraints in the marginal of a DAG model. Placing such limits is similar\nin spirit to sparsity methods for undirected graphical models, and regression\nmodels. In this paper, we give a log-linear parameterization which allows\nsparse modeling with nested Markov models. We illustrate the advantages of this\nparameterization with a simulation study.\n", "versions": [{"version": "v1", "created": "Thu, 26 Sep 2013 12:50:19 GMT"}], "update_date": "2013-09-27", "authors_parsed": [["Shpitser", "Ilya", ""], ["Evans", "Robin J.", ""], ["Richardson", "Thomas S.", ""], ["Robins", "James M.", ""]]}, {"id": "1309.6865", "submitter": "Nitish Srivastava", "authors": "Nitish Srivastava, Ruslan R Salakhutdinov, Geoffrey E. Hinton", "title": "Modeling Documents with Deep Boltzmann Machines", "comments": "Appears in Proceedings of the Twenty-Ninth Conference on Uncertainty\n  in Artificial Intelligence (UAI2013)", "journal-ref": null, "doi": null, "report-no": "UAI-P-2013-PG-616-624", "categories": "cs.LG cs.IR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a Deep Boltzmann Machine model suitable for modeling and\nextracting latent semantic representations from a large unstructured collection\nof documents. We overcome the apparent difficulty of training a DBM with\njudicious parameter tying. This parameter tying enables an efficient\npretraining algorithm and a state initialization scheme that aids inference.\nThe model can be trained just as efficiently as a standard Restricted Boltzmann\nMachine. Our experiments show that the model assigns better log probability to\nunseen data than the Replicated Softmax model. Features extracted from our\nmodel outperform LDA, Replicated Softmax, and DocNADE models on document\nretrieval and document classification tasks.\n", "versions": [{"version": "v1", "created": "Thu, 26 Sep 2013 12:50:54 GMT"}], "update_date": "2013-09-27", "authors_parsed": [["Srivastava", "Nitish", ""], ["Salakhutdinov", "Ruslan R", ""], ["Hinton", "Geoffrey E.", ""]]}, {"id": "1309.6867", "submitter": "Yaniv Tenzer", "authors": "Yaniv Tenzer, Gal Elidan", "title": "Speedy Model Selection (SMS) for Copula Models", "comments": "Appears in Proceedings of the Twenty-Ninth Conference on Uncertainty\n  in Artificial Intelligence (UAI2013)", "journal-ref": null, "doi": null, "report-no": "UAI-P-2013-PG-625-634", "categories": "cs.LG stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We tackle the challenge of efficiently learning the structure of expressive\nmultivariate real-valued densities of copula graphical models. We start by\ntheoretically substantiating the conjecture that for many copula families the\nmagnitude of Spearman's rank correlation coefficient is monotone in the\nexpected contribution of an edge in network, namely the negative copula\nentropy. We then build on this theory and suggest a novel Bayesian approach\nthat makes use of a prior over values of Spearman's rho for learning\ncopula-based models that involve a mix of copula families. We demonstrate the\ngeneralization effectiveness of our highly efficient approach on sizable and\nvaried real-life datasets.\n", "versions": [{"version": "v1", "created": "Thu, 26 Sep 2013 12:51:22 GMT"}], "update_date": "2013-09-27", "authors_parsed": [["Tenzer", "Yaniv", ""], ["Elidan", "Gal", ""]]}, {"id": "1309.6868", "submitter": "Charles Tripp", "authors": "Charles Tripp, Ross D. Shachter", "title": "Approximate Kalman Filter Q-Learning for Continuous State-Space MDPs", "comments": "Appears in Proceedings of the Twenty-Ninth Conference on Uncertainty\n  in Artificial Intelligence (UAI2013)", "journal-ref": null, "doi": null, "report-no": "UAI-P-2013-PG-644-653", "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We seek to learn an effective policy for a Markov Decision Process (MDP) with\ncontinuous states via Q-Learning. Given a set of basis functions over state\naction pairs we search for a corresponding set of linear weights that minimizes\nthe mean Bellman residual. Our algorithm uses a Kalman filter model to estimate\nthose weights and we have developed a simpler approximate Kalman filter model\nthat outperforms the current state of the art projected TD-Learning methods on\nseveral standard benchmark problems.\n", "versions": [{"version": "v1", "created": "Thu, 26 Sep 2013 12:51:47 GMT"}], "update_date": "2013-09-27", "authors_parsed": [["Tripp", "Charles", ""], ["Shachter", "Ross D.", ""]]}, {"id": "1309.6869", "submitter": "Michal Valko", "authors": "Michal Valko, Nathaniel Korda, Remi Munos, Ilias Flaounas, Nelo\n  Cristianini", "title": "Finite-Time Analysis of Kernelised Contextual Bandits", "comments": "Appears in Proceedings of the Twenty-Ninth Conference on Uncertainty\n  in Artificial Intelligence (UAI2013)", "journal-ref": null, "doi": null, "report-no": "UAI-P-2013-PG-654-663", "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We tackle the problem of online reward maximisation over a large finite set\nof actions described by their contexts. We focus on the case when the number of\nactions is too big to sample all of them even once. However we assume that we\nhave access to the similarities between actions' contexts and that the expected\nreward is an arbitrary linear function of the contexts' images in the related\nreproducing kernel Hilbert space (RKHS). We propose KernelUCB, a kernelised UCB\nalgorithm, and give a cumulative regret bound through a frequentist analysis.\nFor contextual bandits, the related algorithm GP-UCB turns out to be a special\ncase of our algorithm, and our finite-time analysis improves the regret bound\nof GP-UCB for the agnostic case, both in the terms of the kernel-dependent\nquantity and the RKHS norm of the reward function. Moreover, for the linear\nkernel, our regret bound matches the lower bound for contextual linear bandits.\n", "versions": [{"version": "v1", "created": "Thu, 26 Sep 2013 12:52:20 GMT"}], "update_date": "2013-09-27", "authors_parsed": [["Valko", "Michal", ""], ["Korda", "Nathaniel", ""], ["Munos", "Remi", ""], ["Flaounas", "Ilias", ""], ["Cristianini", "Nelo", ""]]}, {"id": "1309.6874", "submitter": "Pengtao Xie", "authors": "Pengtao Xie, Eric P. Xing", "title": "Integrating Document Clustering and Topic Modeling", "comments": "Appears in Proceedings of the Twenty-Ninth Conference on Uncertainty\n  in Artificial Intelligence (UAI2013)", "journal-ref": null, "doi": null, "report-no": "UAI-P-2013-PG-694-703", "categories": "cs.LG cs.CL cs.IR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Document clustering and topic modeling are two closely related tasks which\ncan mutually benefit each other. Topic modeling can project documents into a\ntopic space which facilitates effective document clustering. Cluster labels\ndiscovered by document clustering can be incorporated into topic models to\nextract local topics specific to each cluster and global topics shared by all\nclusters. In this paper, we propose a multi-grain clustering topic model\n(MGCTM) which integrates document clustering and topic modeling into a unified\nframework and jointly performs the two tasks to achieve the overall best\nperformance. Our model tightly couples two components: a mixture component used\nfor discovering latent groups in document collection and a topic model\ncomponent used for mining multi-grain topics including local topics specific to\neach cluster and global topics shared across clusters.We employ variational\ninference to approximate the posterior of hidden variables and learn model\nparameters. Experiments on two datasets demonstrate the effectiveness of our\nmodel.\n", "versions": [{"version": "v1", "created": "Thu, 26 Sep 2013 12:54:02 GMT"}], "update_date": "2013-09-27", "authors_parsed": [["Xie", "Pengtao", ""], ["Xing", "Eric P.", ""]]}, {"id": "1309.6875", "submitter": "Peilin Zhao", "authors": "Peilin Zhao, Steven Hoi, Jinfeng Zhuang", "title": "Active Learning with Expert Advice", "comments": "Appears in Proceedings of the Twenty-Ninth Conference on Uncertainty\n  in Artificial Intelligence (UAI2013)", "journal-ref": null, "doi": null, "report-no": "UAI-P-2013-PG-704-713", "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Conventional learning with expert advice methods assumes a learner is always\nreceiving the outcome (e.g., class labels) of every incoming training instance\nat the end of each trial. In real applications, acquiring the outcome from\noracle can be costly or time consuming. In this paper, we address a new problem\nof active learning with expert advice, where the outcome of an instance is\ndisclosed only when it is requested by the online learner. Our goal is to learn\nan accurate prediction model by asking the oracle the number of questions as\nsmall as possible. To address this challenge, we propose a framework of active\nforecasters for online active learning with expert advice, which attempts to\nextend two regular forecasters, i.e., Exponentially Weighted Average Forecaster\nand Greedy Forecaster, to tackle the task of active learning with expert\nadvice. We prove that the proposed algorithms satisfy the Hannan consistency\nunder some proper assumptions, and validate the efficacy of our technique by an\nextensive set of experiments.\n", "versions": [{"version": "v1", "created": "Thu, 26 Sep 2013 12:54:31 GMT"}], "update_date": "2013-09-27", "authors_parsed": [["Zhao", "Peilin", ""], ["Hoi", "Steven", ""], ["Zhuang", "Jinfeng", ""]]}, {"id": "1309.6876", "submitter": "Chao Zhang", "authors": "Chao Zhang", "title": "Bennett-type Generalization Bounds: Large-deviation Case and Faster Rate\n  of Convergence", "comments": "Appears in Proceedings of the Twenty-Ninth Conference on Uncertainty\n  in Artificial Intelligence (UAI2013)", "journal-ref": null, "doi": null, "report-no": "UAI-P-2013-PG-714-722", "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present the Bennett-type generalization bounds of the\nlearning process for i.i.d. samples, and then show that the generalization\nbounds have a faster rate of convergence than the traditional results. In\nparticular, we first develop two types of Bennett-type deviation inequality for\nthe i.i.d. learning process: one provides the generalization bounds based on\nthe uniform entropy number; the other leads to the bounds based on the\nRademacher complexity. We then adopt a new method to obtain the alternative\nexpressions of the Bennett-type generalization bounds, which imply that the\nbounds have a faster rate o(N^{-1/2}) of convergence than the traditional\nresults O(N^{-1/2}). Additionally, we find that the rate of the bounds will\nbecome faster in the large-deviation case, which refers to a situation where\nthe empirical risk is far away from (at least not close to) the expected risk.\nFinally, we analyze the asymptotical convergence of the learning process and\ncompare our analysis with the existing results.\n", "versions": [{"version": "v1", "created": "Thu, 26 Sep 2013 12:54:57 GMT"}], "update_date": "2013-09-27", "authors_parsed": [["Zhang", "Chao", ""]]}, {"id": "1309.6933", "submitter": "Larry Wasserman", "authors": "Larry Wasserman, Mladen Kolar and Alessandro Rinaldo", "title": "Estimating Undirected Graphs Under Weak Assumptions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.LG stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of providing nonparametric confidence guarantees for\nundirected graphs under weak assumptions. In particular, we do not assume\nsparsity, incoherence or Normality. We allow the dimension $D$ to increase with\nthe sample size $n$. First, we prove lower bounds that show that if we want\naccurate inferences with low assumptions then there are limitations on the\ndimension as a function of sample size. When the dimension increases slowly\nwith sample size, we show that methods based on Normal approximations and on\nthe bootstrap lead to valid inferences and we provide Berry-Esseen bounds on\nthe accuracy of the Normal approximation. When the dimension is large relative\nto sample size, accurate inferences for graphs under low assumptions are not\npossible. Instead we propose to estimate something less demanding than the\nentire partial correlation graph. In particular, we consider: cluster graphs,\nrestricted partial correlation graphs and correlation graphs.\n", "versions": [{"version": "v1", "created": "Thu, 26 Sep 2013 15:18:22 GMT"}], "update_date": "2013-09-27", "authors_parsed": [["Wasserman", "Larry", ""], ["Kolar", "Mladen", ""], ["Rinaldo", "Alessandro", ""]]}, {"id": "1309.7119", "submitter": "Yanshan Wang", "authors": "Yanshan Wang", "title": "Stock price direction prediction by directly using prices data: an\n  empirical study on the KOSPI and HSI", "comments": "in International Journal of Business Intelligence and Data Mining,\n  2014", "journal-ref": null, "doi": "10.1504/IJBIDM.2014.065091", "report-no": null, "categories": "cs.CE cs.LG q-fin.ST", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The prediction of a stock market direction may serve as an early\nrecommendation system for short-term investors and as an early financial\ndistress warning system for long-term shareholders. Many stock prediction\nstudies focus on using macroeconomic indicators, such as CPI and GDP, to train\nthe prediction model. However, daily data of the macroeconomic indicators are\nalmost impossible to obtain. Thus, those methods are difficult to be employed\nin practice. In this paper, we propose a method that directly uses prices data\nto predict market index direction and stock price direction. An extensive\nempirical study of the proposed method is presented on the Korean Composite\nStock Price Index (KOSPI) and Hang Seng Index (HSI), as well as the individual\nconstituents included in the indices. The experimental results show notably\nhigh hit ratios in predicting the movements of the individual constituents in\nthe KOSPI and HIS.\n", "versions": [{"version": "v1", "created": "Fri, 27 Sep 2013 05:35:50 GMT"}, {"version": "v2", "created": "Wed, 13 Jul 2016 15:12:18 GMT"}, {"version": "v3", "created": "Sat, 7 Jan 2017 00:01:32 GMT"}], "update_date": "2017-01-10", "authors_parsed": [["Wang", "Yanshan", ""]]}, {"id": "1309.7261", "submitter": "Ahmed Abbasi", "authors": "Ahmed Abbasi and Hsinchun Chen", "title": "Detecting Fake Escrow Websites using Rich Fraud Cues and Kernel Based\n  Methods", "comments": null, "journal-ref": "Abbasi, A. and Chen, H. \"Detecting Fake Escrow Websites using Rich\n  Fraud Cues and Kernel Based Methods,\" In Proceedings of the 17th Annual\n  Workshop on Information Technologies and Systems, Montreal, Canada, December\n  8-9, 2007, pp. 55-60", "doi": null, "report-no": null, "categories": "cs.CY cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The ability to automatically detect fraudulent escrow websites is important\nin order to alleviate online auction fraud. Despite research on related topics,\nfake escrow website categorization has received little attention. In this study\nwe evaluated the effectiveness of various features and techniques for detecting\nfake escrow websites. Our analysis included a rich set of features extracted\nfrom web page text, image, and link information. We also proposed a composite\nkernel tailored to represent the properties of fake websites, including content\nduplication and structural attributes. Experiments were conducted to assess the\nproposed features, techniques, and kernels on a test bed encompassing nearly\n90,000 web pages derived from 410 legitimate and fake escrow sites. The\ncombination of an extended feature set and the composite kernel attained over\n98% accuracy when differentiating fake sites from real ones, using the support\nvector machines algorithm. The results suggest that automated web-based\ninformation systems for detecting fake escrow sites could be feasible and may\nbe utilized as authentication mechanisms.\n", "versions": [{"version": "v1", "created": "Fri, 27 Sep 2013 15:04:05 GMT"}], "update_date": "2013-09-30", "authors_parsed": [["Abbasi", "Ahmed", ""], ["Chen", "Hsinchun", ""]]}, {"id": "1309.7266", "submitter": "Ahmed Abbasi", "authors": "Ahmed Abbasi, Siddharth Kaza and F. Mariam Zahedi", "title": "Evaluating Link-Based Techniques for Detecting Fake Pharmacy Websites", "comments": "Abbasi, A., Kaza, S., and Zahedi, F. M. \"Evaluating Link-Based\n  Techniques for Detecting Fake Pharmacy Websites,\" In Proceedings of the 19th\n  Annual Workshop on Information Technologies and Systems, Phoenix, Arizona,\n  December 14-15, 2009", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fake online pharmacies have become increasingly pervasive, constituting over\n90% of online pharmacy websites. There is a need for fake website detection\ntechniques capable of identifying fake online pharmacy websites with a high\ndegree of accuracy. In this study, we compared several well-known link-based\ndetection techniques on a large-scale test bed with the hyperlink graph\nencompassing over 80 million links between 15.5 million web pages, including\n1.2 million known legitimate and fake pharmacy pages. We found that the QoC and\nQoL class propagation algorithms achieved an accuracy of over 90% on our\ndataset. The results revealed that algorithms that incorporate dual class\npropagation as well as inlink and outlink information, on page-level or\nsite-level graphs, are better suited for detecting fake pharmacy websites. In\naddition, site-level analysis yielded significantly better results than\npage-level analysis for most algorithms evaluated.\n", "versions": [{"version": "v1", "created": "Fri, 27 Sep 2013 15:09:24 GMT"}], "update_date": "2013-09-30", "authors_parsed": [["Abbasi", "Ahmed", ""], ["Kaza", "Siddharth", ""], ["Zahedi", "F. Mariam", ""]]}, {"id": "1309.7311", "submitter": "Peter Orchard", "authors": "Peter Orchard, Felix Agakov, Amos Storkey", "title": "Bayesian Inference in Sparse Gaussian Graphical Models", "comments": null, "journal-ref": null, "doi": "10.1017/S0956796814000057", "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One of the fundamental tasks of science is to find explainable relationships\nbetween observed phenomena. One approach to this task that has received\nattention in recent years is based on probabilistic graphical modelling with\nsparsity constraints on model structures. In this paper, we describe two new\napproaches to Bayesian inference of sparse structures of Gaussian graphical\nmodels (GGMs). One is based on a simple modification of the cutting-edge block\nGibbs sampler for sparse GGMs, which results in significant computational gains\nin high dimensions. The other method is based on a specific construction of the\nHamiltonian Monte Carlo sampler, which results in further significant\nimprovements. We compare our fully Bayesian approaches with the popular\nregularisation-based graphical LASSO, and demonstrate significant advantages of\nthe Bayesian treatment under the same computing costs. We apply the methods to\na broad range of simulated data sets, and a real-life financial data set.\n", "versions": [{"version": "v1", "created": "Fri, 27 Sep 2013 17:53:57 GMT"}], "update_date": "2014-04-16", "authors_parsed": [["Orchard", "Peter", ""], ["Agakov", "Felix", ""], ["Storkey", "Amos", ""]]}, {"id": "1309.7367", "submitter": "M. Sadegh Talebi", "authors": "M. Sadegh Talebi, Zhenhua Zou, Richard Combes, Alexandre Proutiere,\n  Mikael Johansson", "title": "Stochastic Online Shortest Path Routing: The Value of Feedback", "comments": "18 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI cs.LG math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper studies online shortest path routing over multi-hop networks. Link\ncosts or delays are time-varying and modeled by independent and identically\ndistributed random processes, whose parameters are initially unknown. The\nparameters, and hence the optimal path, can only be estimated by routing\npackets through the network and observing the realized delays. Our aim is to\nfind a routing policy that minimizes the regret (the cumulative difference of\nexpected delay) between the path chosen by the policy and the unknown optimal\npath. We formulate the problem as a combinatorial bandit optimization problem\nand consider several scenarios that differ in where routing decisions are made\nand in the information available when making the decisions. For each scenario,\nwe derive a tight asymptotic lower bound on the regret that has to be satisfied\nby any online routing policy. These bounds help us to understand the\nperformance improvements we can expect when (i) taking routing decisions at\neach hop rather than at the source only, and (ii) observing per-link delays\nrather than end-to-end path delays. In particular, we show that (i) is of no\nuse while (ii) can have a spectacular impact. Three algorithms, with a\ntrade-off between computational complexity and performance, are proposed. The\nregret upper bounds of these algorithms improve over those of the existing\nalgorithms, and they significantly outperform state-of-the-art algorithms in\nnumerical experiments.\n", "versions": [{"version": "v1", "created": "Fri, 27 Sep 2013 20:56:41 GMT"}, {"version": "v2", "created": "Thu, 28 May 2015 11:41:51 GMT"}, {"version": "v3", "created": "Tue, 27 Oct 2015 16:32:53 GMT"}, {"version": "v4", "created": "Mon, 25 Jul 2016 16:30:00 GMT"}, {"version": "v5", "created": "Wed, 18 Jan 2017 10:47:41 GMT"}], "update_date": "2017-01-19", "authors_parsed": [["Talebi", "M. Sadegh", ""], ["Zou", "Zhenhua", ""], ["Combes", "Richard", ""], ["Proutiere", "Alexandre", ""], ["Johansson", "Mikael", ""]]}, {"id": "1309.7439", "submitter": "Viswanadh Konjeti", "authors": "K Viswanadh and Dr.G Rama Murthy", "title": "Optimal Hybrid Channel Allocation:Based On Machine Learning Algorithms", "comments": "5 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI cs.LG", "license": "http://creativecommons.org/licenses/publicdomain/", "abstract": "  Recent advances in cellular communication systems resulted in a huge increase\nin spectrum demand. To meet the requirements of the ever-growing need for\nspectrum, efficient utilization of the existing resources is of utmost\nimportance. Channel Allocation, has thus become an inevitable research topic in\nwireless communications. In this paper, we propose an optimal channel\nallocation scheme, Optimal Hybrid Channel Allocation (OHCA) for an effective\nallocation of channels. We improvise upon the existing Fixed Channel Allocation\n(FCA) technique by imparting intelligence to the existing system by employing\nthe multilayer perceptron technique.\n", "versions": [{"version": "v1", "created": "Sat, 28 Sep 2013 07:44:11 GMT"}], "update_date": "2013-10-01", "authors_parsed": [["Viswanadh", "K", ""], ["Murthy", "Dr. G Rama", ""]]}, {"id": "1309.7512", "submitter": "Alexander Fix", "authors": "Alexander Fix and Thorsten Joachims and Sam Park and Ramin Zabih", "title": "Structured learning of sum-of-submodular higher order energy functions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Submodular functions can be exactly minimized in polynomial time, and the\nspecial case that graph cuts solve with max flow \\cite{KZ:PAMI04} has had\nsignificant impact in computer vision\n\\cite{BVZ:PAMI01,Kwatra:SIGGRAPH03,Rother:GrabCut04}. In this paper we address\nthe important class of sum-of-submodular (SoS) functions\n\\cite{Arora:ECCV12,Kolmogorov:DAM12}, which can be efficiently minimized via a\nvariant of max flow called submodular flow \\cite{Edmonds:ADM77}. SoS functions\ncan naturally express higher order priors involving, e.g., local image patches;\nhowever, it is difficult to fully exploit their expressive power because they\nhave so many parameters. Rather than trying to formulate existing higher order\npriors as an SoS function, we take a discriminative learning approach,\neffectively searching the space of SoS functions for a higher order prior that\nperforms well on our training set. We adopt a structural SVM approach\n\\cite{Joachims/etal/09a,Tsochantaridis/etal/04} and formulate the training\nproblem in terms of quadratic programming; as a result we can efficiently\nsearch the space of SoS priors via an extended cutting-plane algorithm. We also\nshow how the state-of-the-art max flow method for vision problems\n\\cite{Goldberg:ESA11} can be modified to efficiently solve the submodular flow\nproblem. Experimental comparisons are made against the OpenCV implementation of\nthe GrabCut interactive segmentation technique \\cite{Rother:GrabCut04}, which\nuses hand-tuned parameters instead of machine learning. On a standard dataset\n\\cite{Gulshan:CVPR10} our method learns higher order priors with hundreds of\nparameter values, and produces significantly better segmentations. While our\nfocus is on binary labeling problems, we show that our techniques can be\nnaturally generalized to handle more than two labels.\n", "versions": [{"version": "v1", "created": "Sat, 28 Sep 2013 23:55:01 GMT"}, {"version": "v2", "created": "Tue, 1 Oct 2013 02:45:20 GMT"}], "update_date": "2013-10-02", "authors_parsed": [["Fix", "Alexander", ""], ["Joachims", "Thorsten", ""], ["Park", "Sam", ""], ["Zabih", "Ramin", ""]]}, {"id": "1309.7598", "submitter": "Tamir Hazan", "authors": "Tamir Hazan, Subhransu Maji and Tommi Jaakkola", "title": "On Sampling from the Gibbs Distribution with Random Maximum A-Posteriori\n  Perturbations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we describe how MAP inference can be used to sample efficiently\nfrom Gibbs distributions. Specifically, we provide means for drawing either\napproximate or unbiased samples from Gibbs' distributions by introducing low\ndimensional perturbations and solving the corresponding MAP assignments. Our\napproach also leads to new ways to derive lower bounds on partition functions.\nWe demonstrate empirically that our method excels in the typical \"high signal -\nhigh coupling\" regime. The setting results in ragged energy landscapes that are\nchallenging for alternative approaches to sampling and/or lower bounds.\n", "versions": [{"version": "v1", "created": "Sun, 29 Sep 2013 13:48:52 GMT"}], "update_date": "2013-10-01", "authors_parsed": [["Hazan", "Tamir", ""], ["Maji", "Subhransu", ""], ["Jaakkola", "Tommi", ""]]}, {"id": "1309.7611", "submitter": "Bal\\'azs Hidasi", "authors": "Bal\\'azs Hidasi, Domonkos Tikk", "title": "Context-aware recommendations from implicit data via scalable tensor\n  factorization", "comments": "Extended version of the ECML/PKDD 2012 paper of B. Hidasi & D. Tikk:\n  Fast ALS-based tensor factorization for context-aware recommendation from\n  implicit feedback [arXiv:1204.1259]", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Albeit the implicit feedback based recommendation problem - when only the\nuser history is available but there are no ratings - is the most typical\nsetting in real-world applications, it is much less researched than the\nexplicit feedback case. State-of-the-art algorithms that are efficient on the\nexplicit case cannot be automatically transformed to the implicit case if\nscalability should be maintained. There are few implicit feedback benchmark\ndata sets, therefore new ideas are usually experimented on explicit benchmarks.\nIn this paper, we propose a generic context-aware implicit feedback recommender\nalgorithm, coined iTALS. iTALS applies a fast, ALS-based tensor factorization\nlearning method that scales linearly with the number of non-zero elements in\nthe tensor. We also present two approximate and faster variants of iTALS using\ncoordinate descent and conjugate gradient methods at learning. The method also\nallows us to incorporate various contextual information into the model while\nmaintaining its computational efficiency. We present two context-aware variants\nof iTALS incorporating seasonality and item purchase sequentiality into the\nmodel to distinguish user behavior at different time intervals, and product\ntypes with different repetitiveness. Experiments run on six data sets shows\nthat iTALS clearly outperforms context-unaware models and context aware\nbaselines, while it is on par with factorization machines (beats 7 times out of\n12 cases) both in terms of recall and MAP.\n", "versions": [{"version": "v1", "created": "Sun, 29 Sep 2013 15:50:45 GMT"}], "update_date": "2013-10-01", "authors_parsed": [["Hidasi", "Bal\u00e1zs", ""], ["Tikk", "Domonkos", ""]]}, {"id": "1309.7676", "submitter": "Eric Christiansen", "authors": "Eric Christiansen", "title": "An upper bound on prototype set size for condensed nearest neighbor", "comments": "This was submitted to the journal Artificial Intelligence in 2009,\n  and while it was considered technically sound, it was also believed to be of\n  minor importance. My research has since moved on, so I'm unlikely to attempt\n  a resubmission", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  The condensed nearest neighbor (CNN) algorithm is a heuristic for reducing\nthe number of prototypical points stored by a nearest neighbor classifier,\nwhile keeping the classification rule given by the reduced prototypical set\nconsistent with the full set. I present an upper bound on the number of\nprototypical points accumulated by CNN. The bound originates in a bound on the\nnumber of times the decision rule is updated during training in the multiclass\nperceptron algorithm, and thus is independent of training set size.\n", "versions": [{"version": "v1", "created": "Sun, 29 Sep 2013 23:45:59 GMT"}], "update_date": "2013-10-01", "authors_parsed": [["Christiansen", "Eric", ""]]}, {"id": "1309.7750", "submitter": "Stefanos Ougiaroglou", "authors": "Stefanos Ougiaroglou, Georgios Evangelidis, Dimitris A. Dervos", "title": "An Extensive Experimental Study on the Cluster-based Reference Set\n  Reduction for speeding-up the k-NN Classifier", "comments": "Proceeding of International Conference on Integrated Information\n  (IC-InInfo 2011), pp. 12-15, Kos island, Greece, 2011", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  The k-Nearest Neighbor (k-NN) classification algorithm is one of the most\nwidely-used lazy classifiers because of its simplicity and ease of\nimplementation. It is considered to be an effective classifier and has many\napplications. However, its major drawback is that when sequential search is\nused to find the neighbors, it involves high computational cost. Speeding-up\nk-NN search is still an active research field. Hwang and Cho have recently\nproposed an adaptive cluster-based method for fast Nearest Neighbor searching.\nThe effectiveness of this method is based on the adjustment of three\nparameters. However, the authors evaluated their method by setting specific\nparameter values and using only one dataset. In this paper, an extensive\nexperimental study of this method is presented. The results, which are based on\nfive real life datasets, illustrate that if the parameters of the method are\ncarefully defined, one can achieve even better classification performance.\n", "versions": [{"version": "v1", "created": "Mon, 30 Sep 2013 08:24:14 GMT"}, {"version": "v2", "created": "Tue, 11 Feb 2014 22:46:36 GMT"}], "update_date": "2014-02-13", "authors_parsed": [["Ougiaroglou", "Stefanos", ""], ["Evangelidis", "Georgios", ""], ["Dervos", "Dimitris A.", ""]]}, {"id": "1309.7804", "submitter": "Michael I. Jordan", "authors": "Michael I. Jordan", "title": "On statistics, computation and scalability", "comments": "Published in at http://dx.doi.org/10.3150/12-BEJSP17 the Bernoulli\n  (http://isi.cbs.nl/bernoulli/) by the International Statistical\n  Institute/Bernoulli Society (http://isi.cbs.nl/BS/bshome.htm)", "journal-ref": "Bernoulli 2013, Vol. 19, No. 4, 1378-1390", "doi": "10.3150/12-BEJSP17", "report-no": "IMS-BEJ-BEJSP17", "categories": "stat.ML cs.LG math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  How should statistical procedures be designed so as to be scalable\ncomputationally to the massive datasets that are increasingly the norm? When\ncoupled with the requirement that an answer to an inferential question be\ndelivered within a certain time budget, this question has significant\nrepercussions for the field of statistics. With the goal of identifying\n\"time-data tradeoffs,\" we investigate some of the statistical consequences of\ncomputational perspectives on scability, in particular divide-and-conquer\nmethodology and hierarchies of convex relaxations.\n", "versions": [{"version": "v1", "created": "Mon, 30 Sep 2013 11:51:23 GMT"}], "update_date": "2013-10-01", "authors_parsed": [["Jordan", "Michael I.", ""]]}, {"id": "1309.7824", "submitter": "Patrick Loiseau", "authors": "Nicolas Gast, Stratis Ioannidis, Patrick Loiseau, and Benjamin\n  Roussillon", "title": "Linear Regression from Strategic Data Sources", "comments": "This version (v3) extends the results on the sub-optimality of GLS\n  (Section 6) and improves writing in multiple places compared to v2. Compared\n  to the initial version v1, it also fixes an error in Theorem 6 (now Theorem\n  5), and extended many of the results", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GT cs.LG math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Linear regression is a fundamental building block of statistical data\nanalysis. It amounts to estimating the parameters of a linear model that maps\ninput features to corresponding outputs. In the classical setting where the\nprecision of each data point is fixed, the famous Aitken/Gauss-Markov theorem\nin statistics states that generalized least squares (GLS) is a so-called \"Best\nLinear Unbiased Estimator\" (BLUE). In modern data science, however, one often\nfaces strategic data sources, namely, individuals who incur a cost for\nproviding high-precision data.\n  In this paper, we study a setting in which features are public but\nindividuals choose the precision of the outputs they reveal to an analyst. We\nassume that the analyst performs linear regression on this dataset, and\nindividuals benefit from the outcome of this estimation. We model this scenario\nas a game where individuals minimize a cost comprising two components: (a) an\n(agent-specific) disclosure cost for providing high-precision data; and (b) a\n(global) estimation cost representing the inaccuracy in the linear model\nestimate. In this game, the linear model estimate is a public good that\nbenefits all individuals. We establish that this game has a unique non-trivial\nNash equilibrium. We study the efficiency of this equilibrium and we prove\ntight bounds on the price of stability for a large class of disclosure and\nestimation costs. Finally, we study the estimator accuracy achieved at\nequilibrium. We show that, in general, Aitken's theorem does not hold under\nstrategic data sources, though it does hold if individuals have identical\ndisclosure costs (up to a multiplicative factor). When individuals have\nnon-identical costs, we derive a bound on the improvement of the equilibrium\nestimation cost that can be achieved by deviating from GLS, under mild\nassumptions on the disclosure cost functions.\n", "versions": [{"version": "v1", "created": "Mon, 30 Sep 2013 12:48:35 GMT"}, {"version": "v2", "created": "Thu, 4 Jul 2019 14:29:00 GMT"}, {"version": "v3", "created": "Thu, 12 Dec 2019 23:47:00 GMT"}], "update_date": "2019-12-16", "authors_parsed": [["Gast", "Nicolas", ""], ["Ioannidis", "Stratis", ""], ["Loiseau", "Patrick", ""], ["Roussillon", "Benjamin", ""]]}, {"id": "1309.7958", "submitter": "Ahmed Abbasi", "authors": "Ahmed Abbasi, Zhu Zhang and Hsinchun Chen", "title": "A Statistical Learning Based System for Fake Website Detection", "comments": null, "journal-ref": "Abbasi, A., Zhang, Z., and Chen, H. \"A Statistical Learning Based\n  System for Fake Website Detection,\" In Proceedings of the Workshop on Secure\n  Knowledge Management, Dallas, Texas, November 3-4 2008", "doi": null, "report-no": null, "categories": "cs.CY cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Existing fake website detection systems are unable to effectively detect fake\nwebsites. In this study, we advocate the development of fake website detection\nsystems that employ classification methods grounded in statistical learning\ntheory (SLT). Experimental results reveal that a prototype system developed\nusing SLT-based methods outperforms seven existing fake website detection\nsystems on a test bed encompassing 900 real and fake websites.\n", "versions": [{"version": "v1", "created": "Fri, 27 Sep 2013 15:05:21 GMT"}], "update_date": "2013-10-01", "authors_parsed": [["Abbasi", "Ahmed", ""], ["Zhang", "Zhu", ""], ["Chen", "Hsinchun", ""]]}, {"id": "1309.7959", "submitter": "Laurens Bliek", "authors": "Laurens Bliek", "title": "Exploration and Exploitation in Visuomotor Prediction of Autonomous\n  Agents", "comments": "Award-winning paper of the internal conference 'Almende research\n  workshop 2013'", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV math.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper discusses various techniques to let an agent learn how to predict\nthe effects of its own actions on its sensor data autonomously, and their\nusefulness to apply them to visual sensors. An Extreme Learning Machine is used\nfor visuomotor prediction, while various autonomous control techniques that can\naid the prediction process by balancing exploration and exploitation are\ndiscussed and tested in a simple system: a camera moving over a 2D greyscale\nimage.\n", "versions": [{"version": "v1", "created": "Thu, 19 Sep 2013 07:10:53 GMT"}], "update_date": "2013-10-01", "authors_parsed": [["Bliek", "Laurens", ""]]}, {"id": "1309.7982", "submitter": "Shou Chung Li scli", "authors": "Zhung-Xun Liao, Shou-Chung Li, Wen-Chih Peng, Philip S Yu", "title": "On the Feature Discovery for App Usage Prediction in Smartphones", "comments": "10 pages, 17 figures, ICDM 2013 short paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the increasing number of mobile Apps developed, they are now closely\nintegrated into daily life. In this paper, we develop a framework to predict\nmobile Apps that are most likely to be used regarding the current device status\nof a smartphone. Such an Apps usage prediction framework is a crucial\nprerequisite for fast App launching, intelligent user experience, and power\nmanagement of smartphones. By analyzing real App usage log data, we discover\ntwo kinds of features: The Explicit Feature (EF) from sensing readings of\nbuilt-in sensors, and the Implicit Feature (IF) from App usage relations. The\nIF feature is derived by constructing the proposed App Usage Graph (abbreviated\nas AUG) that models App usage transitions. In light of AUG, we are able to\ndiscover usage relations among Apps. Since users may have different usage\nbehaviors on their smartphones, we further propose one personalized feature\nselection algorithm. We explore minimum description length (MDL) from the\ntraining data and select those features which need less length to describe the\ntraining data. The personalized feature selection can successfully reduce the\nlog size and the prediction time. Finally, we adopt the kNN classification\nmodel to predict Apps usage. Note that through the features selected by the\nproposed personalized feature selection algorithm, we only need to keep these\nfeatures, which in turn reduces the prediction time and avoids the curse of\ndimensionality when using the kNN classifier. We conduct a comprehensive\nexperimental study based on a real mobile App usage dataset. The results\ndemonstrate the effectiveness of the proposed framework and show the predictive\ncapability for App usage prediction.\n", "versions": [{"version": "v1", "created": "Thu, 26 Sep 2013 14:44:10 GMT"}], "update_date": "2013-10-01", "authors_parsed": [["Liao", "Zhung-Xun", ""], ["Li", "Shou-Chung", ""], ["Peng", "Wen-Chih", ""], ["Yu", "Philip S", ""]]}]