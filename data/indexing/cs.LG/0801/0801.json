[{"id": "0801.0390", "submitter": "Richard Nock", "authors": "Richard Nock, Nicolas Sanz, Fred Celimene, Frank Nielsen", "title": "Staring at Economic Aggregators through Information Lenses", "comments": "18 pages, 2 tables, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.LG math.IT math.OC", "license": null, "abstract": "  It is hard to exaggerate the role of economic aggregators -- functions that\nsummarize numerous and / or heterogeneous data -- in economic models since the\nearly XX$^{th}$ century. In many cases, as witnessed by the pioneering works of\nCobb and Douglas, these functions were information quantities tailored to\neconomic theories, i.e. they were built to fit economic phenomena. In this\npaper, we look at these functions from the complementary side: information. We\nuse a recent toolbox built on top of a vast class of distortions coined by\nBregman, whose application field rivals metrics' in various subfields of\nmathematics. This toolbox makes it possible to find the quality of an\naggregator (for consumptions, prices, labor, capital, wages, etc.), from the\nstandpoint of the information it carries. We prove a rather striking result.\n  From the informational standpoint, well-known economic aggregators do belong\nto the \\textit{optimal} set. As common economic assumptions enter the analysis,\nthis large set shrinks, and it essentially ends up \\textit{exactly fitting}\neither CES, or Cobb-Douglas, or both. To summarize, in the relevant economic\ncontexts, one could not have crafted better some aggregator from the\ninformation standpoint. We also discuss global economic behaviors of optimal\ninformation aggregators in general, and present a brief panorama of the links\nbetween economic and information aggregators.\n  Keywords: Economic Aggregators, CES, Cobb-Douglas, Bregman divergences\n", "versions": [{"version": "v1", "created": "Wed, 2 Jan 2008 13:23:04 GMT"}], "update_date": "2008-01-03", "authors_parsed": [["Nock", "Richard", ""], ["Sanz", "Nicolas", ""], ["Celimene", "Fred", ""], ["Nielsen", "Frank", ""]]}, {"id": "0801.1988", "submitter": "Andras Lorincz", "authors": "Istvan Szita and Andras Lorincz", "title": "Online variants of the cross-entropy method", "comments": "8 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": null, "abstract": "  The cross-entropy method is a simple but efficient method for global\noptimization. In this paper we provide two online variants of the basic CEM,\ntogether with a proof of convergence.\n", "versions": [{"version": "v1", "created": "Mon, 14 Jan 2008 06:56:42 GMT"}], "update_date": "2008-01-15", "authors_parsed": [["Szita", "Istvan", ""], ["Lorincz", "Andras", ""]]}, {"id": "0801.2069", "submitter": "Istvan Szita", "authors": "Istvan Szita and Andras Lorincz", "title": "Factored Value Iteration Converges", "comments": "17 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we propose a novel algorithm, factored value iteration (FVI),\nfor the approximate solution of factored Markov decision processes (fMDPs). The\ntraditional approximate value iteration algorithm is modified in two ways. For\none, the least-squares projection operator is modified so that it does not\nincrease max-norm, and thus preserves convergence. The other modification is\nthat we uniformly sample polynomially many samples from the (exponentially\nlarge) state space. This way, the complexity of our algorithm becomes\npolynomial in the size of the fMDP description length. We prove that the\nalgorithm is convergent. We also derive an upper bound on the difference\nbetween our approximate solution and the optimal one, and also on the error\nintroduced by sampling. We analyze various projection operators with respect to\ntheir computation complexity and their convergence when combined with\napproximate value iteration.\n", "versions": [{"version": "v1", "created": "Mon, 14 Jan 2008 13:09:06 GMT"}, {"version": "v2", "created": "Wed, 13 Aug 2008 15:07:08 GMT"}], "update_date": "2008-08-13", "authors_parsed": [["Szita", "Istvan", ""], ["Lorincz", "Andras", ""]]}, {"id": "0801.4061", "submitter": "Jean-Philippe Vert", "authors": "Jean-Philippe Vert (CB)", "title": "The optimal assignment kernel is not positive definite", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": null, "abstract": "  We prove that the optimal assignment kernel, proposed recently as an attempt\nto embed labeled graphs and more generally tuples of basic data to a Hilbert\nspace, is in fact not always positive definite.\n", "versions": [{"version": "v1", "created": "Sat, 26 Jan 2008 07:32:48 GMT"}], "update_date": "2008-01-29", "authors_parsed": [["Vert", "Jean-Philippe", "", "CB"]]}, {"id": "0801.4790", "submitter": "Joel Ratsaby", "authors": "Joel Ratsaby", "title": "Information Width", "comments": "Typo error in eq. (13)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DM cs.IT cs.LG math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Kolmogorov argued that the concept of information exists also in problems\nwith no underlying stochastic model (as Shannon's information representation)\nfor instance, the information contained in an algorithm or in the genome. He\nintroduced a combinatorial notion of entropy and information $I(x:\\sy)$\nconveyed by a binary string $x$ about the unknown value of a variable $\\sy$.\nThe current paper poses the following questions: what is the relationship\nbetween the information conveyed by $x$ about $\\sy$ to the description\ncomplexity of $x$ ? is there a notion of cost of information ? are there limits\non how efficient $x$ conveys information ?\n  To answer these questions Kolmogorov's definition is extended and a new\nconcept termed {\\em information width} which is similar to $n$-widths in\napproximation theory is introduced. Information of any input source, e.g.,\nsample-based, general side-information or a hybrid of both can be evaluated by\na single common formula. An application to the space of binary functions is\nconsidered.\n", "versions": [{"version": "v1", "created": "Wed, 30 Jan 2008 22:49:57 GMT"}, {"version": "v2", "created": "Tue, 1 Jul 2008 09:46:33 GMT"}], "update_date": "2008-07-01", "authors_parsed": [["Ratsaby", "Joel", ""]]}, {"id": "0801.4794", "submitter": "Joel Ratsaby", "authors": "Joel Ratsaby", "title": "On the Complexity of Binary Samples", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DM cs.AI cs.LG", "license": null, "abstract": "  Consider a class $\\mH$ of binary functions $h: X\\to\\{-1, +1\\}$ on a finite\ninterval $X=[0, B]\\subset \\Real$. Define the {\\em sample width} of $h$ on a\nfinite subset (a sample) $S\\subset X$ as $\\w_S(h) \\equiv \\min_{x\\in S}\n|\\w_h(x)|$, where $\\w_h(x) = h(x) \\max\\{a\\geq 0: h(z)=h(x), x-a\\leq z\\leq\nx+a\\}$. Let $\\mathbb{S}_\\ell$ be the space of all samples in $X$ of cardinality\n$\\ell$ and consider sets of wide samples, i.e., {\\em hypersets} which are\ndefined as $A_{\\beta, h} = \\{S\\in \\mathbb{S}_\\ell: \\w_{S}(h) \\geq \\beta\\}$.\nThrough an application of the Sauer-Shelah result on the density of sets an\nupper estimate is obtained on the growth function (or trace) of the class\n$\\{A_{\\beta, h}: h\\in\\mH\\}$, $\\beta>0$, i.e., on the number of possible\ndichotomies obtained by intersecting all hypersets with a fixed collection of\nsamples $S\\in\\mathbb{S}_\\ell$ of cardinality $m$. The estimate is\n$2\\sum_{i=0}^{2\\lfloor B/(2\\beta)\\rfloor}{m-\\ell\\choose i}$.\n", "versions": [{"version": "v1", "created": "Wed, 30 Jan 2008 23:14:19 GMT"}], "update_date": "2008-02-01", "authors_parsed": [["Ratsaby", "Joel", ""]]}]