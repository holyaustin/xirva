[{"id": "1705.00033", "submitter": "Mohamed Abuella", "authors": "Mohamed Abuella and Badrul Chowdhury", "title": "Random Forest Ensemble of Support Vector Regression Models for Solar\n  Power Forecasting", "comments": "This is a preprint of the full paper that published in Innovative\n  Smart Grid Technologies, North America Conference, 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To mitigate the uncertainty of variable renewable resources, two\noff-the-shelf machine learning tools are deployed to forecast the solar power\noutput of a solar photovoltaic system. The support vector machines generate the\nforecasts and the random forest acts as an ensemble learning method to combine\nthe forecasts. The common ensemble technique in wind and solar power\nforecasting is the blending of meteorological data from several sources. In\nthis study though, the present and the past solar power forecasts from several\nmodels, as well as the associated meteorological data, are incorporated into\nthe random forest to combine and improve the accuracy of the day-ahead solar\npower forecasts. The performance of the combined model is evaluated over the\nentire year and compared with other combining techniques.\n", "versions": [{"version": "v1", "created": "Thu, 27 Apr 2017 04:27:00 GMT"}], "update_date": "2017-05-02", "authors_parsed": [["Abuella", "Mohamed", ""], ["Chowdhury", "Badrul", ""]]}, {"id": "1705.00034", "submitter": "Sara Bahaadini", "authors": "Sara Bahaadini, Neda Rohani, Scott Coughlin, Michael Zevin, Vicky\n  Kalogera, and Aggelos K Katsaggelos", "title": "Deep Multi-view Models for Glitch Classification", "comments": "Accepted to the 42nd IEEE International Conference on Acoustics,\n  Speech and Signal Processing (ICASSP'17)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Non-cosmic, non-Gaussian disturbances known as \"glitches\", show up in\ngravitational-wave data of the Advanced Laser Interferometer Gravitational-wave\nObservatory, or aLIGO. In this paper, we propose a deep multi-view\nconvolutional neural network to classify glitches automatically. The primary\npurpose of classifying glitches is to understand their characteristics and\norigin, which facilitates their removal from the data or from the detector\nentirely. We visualize glitches as spectrograms and leverage the\nstate-of-the-art image classification techniques in our model. The suggested\nclassifier is a multi-view deep neural network that exploits four different\nviews for classification. The experimental results demonstrate that the\nproposed model improves the overall accuracy of the classification compared to\ntraditional single view algorithms.\n", "versions": [{"version": "v1", "created": "Fri, 28 Apr 2017 18:45:57 GMT"}], "update_date": "2017-05-02", "authors_parsed": [["Bahaadini", "Sara", ""], ["Rohani", "Neda", ""], ["Coughlin", "Scott", ""], ["Zevin", "Michael", ""], ["Kalogera", "Vicky", ""], ["Katsaggelos", "Aggelos K", ""]]}, {"id": "1705.00125", "submitter": "Patrick Judd", "authors": "Patrick Judd, Alberto Delmas, Sayeh Sharify and Andreas Moshovos", "title": "Cnvlutin2: Ineffectual-Activation-and-Weight-Free Deep Neural Network\n  Computing", "comments": "6 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We discuss several modifications and extensions over the previous proposed\nCnvlutin (CNV) accelerator for convolutional and fully-connected layers of Deep\nLearning Network. We first describe different encodings of the activations that\nare deemed ineffectual. The encodings have different memory overhead and energy\ncharacteristics. We propose using a level of indirection when accessing\nactivations from memory to reduce their memory footprint by storing only the\neffectual activations. We also present a modified organization that detects the\nactivations that are deemed as ineffectual while fetching them from memory.\nThis is different than the original design that instead detected them at the\noutput of the preceding layer. Finally, we present an extended CNV that can\nalso skip ineffectual weights.\n", "versions": [{"version": "v1", "created": "Sat, 29 Apr 2017 03:49:34 GMT"}], "update_date": "2017-05-02", "authors_parsed": [["Judd", "Patrick", ""], ["Delmas", "Alberto", ""], ["Sharify", "Sayeh", ""], ["Moshovos", "Andreas", ""]]}, {"id": "1705.00132", "submitter": "Scott Yang", "authors": "Mehryar Mohri, Scott Yang", "title": "Online Learning with Automata-based Expert Sequences", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider a general framework of online learning with expert advice where\nregret is defined with respect to sequences of experts accepted by a weighted\nautomaton. Our framework covers several problems previously studied, including\ncompeting against k-shifting experts. We give a series of algorithms for this\nproblem, including an automata-based algorithm extending weighted-majority and\nmore efficient algorithms based on the notion of failure transitions. We\nfurther present efficient algorithms based on an approximation of the\ncompetitor automaton, in particular n-gram models obtained by minimizing the\n\\infty-R\\'{e}nyi divergence, and present an extensive study of the\napproximation properties of such models. Finally, we also extend our algorithms\nand results to the framework of sleeping experts.\n", "versions": [{"version": "v1", "created": "Sat, 29 Apr 2017 05:31:20 GMT"}, {"version": "v2", "created": "Sat, 6 May 2017 18:05:25 GMT"}, {"version": "v3", "created": "Sat, 13 May 2017 20:08:02 GMT"}, {"version": "v4", "created": "Sun, 22 Oct 2017 05:24:43 GMT"}], "update_date": "2017-10-24", "authors_parsed": [["Mohri", "Mehryar", ""], ["Yang", "Scott", ""]]}, {"id": "1705.00219", "submitter": "Amit Dhurandhar", "authors": "Amit Dhurandhar, Steve Hanneke and Liu Yang", "title": "Learning with Changing Features", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we study the setting where features are added or change\ninterpretation over time, which has applications in multiple domains such as\nretail, manufacturing, finance. In particular, we propose an approach to\nprovably determine the time instant from which the new/changed features start\nbecoming relevant with respect to an output variable in an agnostic\n(supervised) learning setting. We also suggest an efficient version of our\napproach which has the same asymptotic performance. Moreover, our theory also\napplies when we have more than one such change point. Independent post analysis\nof a change point identified by our method for a large retailer revealed that\nit corresponded in time with certain unflattering news stories about a brand\nthat resulted in the change in customer behavior. We also applied our method to\ndata from an advanced manufacturing plant identifying the time instant from\nwhich downstream features became relevant. To the best of our knowledge this is\nthe first work that formally studies change point detection in a distribution\nindependent agnostic setting, where the change point is based on the changing\nrelationship between input and output.\n", "versions": [{"version": "v1", "created": "Sat, 29 Apr 2017 18:11:10 GMT"}], "update_date": "2017-05-02", "authors_parsed": [["Dhurandhar", "Amit", ""], ["Hanneke", "Steve", ""], ["Yang", "Liu", ""]]}, {"id": "1705.00243", "submitter": "Ellen Vitercik", "authors": "Maria-Florina Balcan, Tuomas Sandholm, and Ellen Vitercik", "title": "Generalization Guarantees for Multi-item Profit Maximization: Pricing,\n  Auctions, and Randomized Mechanisms", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.GT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the design of multi-item mechanisms that maximize expected profit\nwith respect to a distribution over buyers' values. In practice, a full\ndescription of the distribution is typically unavailable. Therefore, we study\nthe setting where the designer only has samples from the distribution and the\ngoal is to find a high-profit mechanism within a class of mechanisms. If the\nclass is complex, a mechanism may have high average profit over the samples but\nlow expected profit. This raises the question: how many samples are sufficient\nto ensure that a mechanism's average profit is close to its expected profit? To\nanswer this question, we uncover structure shared by many pricing, auction, and\nlottery mechanisms: for any set of buyers' values, profit is piecewise linear\nin the mechanism's parameters. Using this structure, we prove new bounds for\nmechanism classes not yet studied in the sample-based mechanism design\nliterature and match or improve over the best known guarantees for many\nclasses. Finally, we provide tools for optimizing an important tradeoff: more\ncomplex mechanisms typically have higher average profit over the samples than\nsimpler mechanisms, but more samples are required to ensure that average profit\nnearly matches expected profit.\n", "versions": [{"version": "v1", "created": "Sat, 29 Apr 2017 22:02:14 GMT"}, {"version": "v2", "created": "Tue, 9 May 2017 11:52:01 GMT"}, {"version": "v3", "created": "Mon, 26 Feb 2018 17:33:50 GMT"}, {"version": "v4", "created": "Wed, 8 Aug 2018 14:43:21 GMT"}, {"version": "v5", "created": "Sat, 10 Apr 2021 15:25:51 GMT"}], "update_date": "2021-04-13", "authors_parsed": [["Balcan", "Maria-Florina", ""], ["Sandholm", "Tuomas", ""], ["Vitercik", "Ellen", ""]]}, {"id": "1705.00253", "submitter": "Yanan Sui", "authors": "Yanan Sui, Vincent Zhuang, Joel W. Burdick, Yisong Yue", "title": "Multi-dueling Bandits with Dependent Arms", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The dueling bandits problem is an online learning framework for learning from\npairwise preference feedback, and is particularly well-suited for modeling\nsettings that elicit subjective or implicit human feedback. In this paper, we\nstudy the problem of multi-dueling bandits with dependent arms, which extends\nthe original dueling bandits setting by simultaneously dueling multiple arms as\nwell as modeling dependencies between arms. These extensions capture key\ncharacteristics found in many real-world applications, and allow for the\nopportunity to develop significantly more efficient algorithms than were\npossible in the original setting. We propose the \\selfsparring algorithm, which\nreduces the multi-dueling bandits problem to a conventional bandit setting that\ncan be solved using a stochastic bandit algorithm such as Thompson Sampling,\nand can naturally model dependencies using a Gaussian process prior. We present\na no-regret analysis for multi-dueling setting, and demonstrate the\neffectiveness of our algorithm empirically on a wide range of simulation\nsettings.\n", "versions": [{"version": "v1", "created": "Sat, 29 Apr 2017 23:47:52 GMT"}], "update_date": "2017-05-02", "authors_parsed": [["Sui", "Yanan", ""], ["Zhuang", "Vincent", ""], ["Burdick", "Joel W.", ""], ["Yue", "Yisong", ""]]}, {"id": "1705.00321", "submitter": "Ganbin Zhou", "authors": "Ganbin Zhou, Ping Luo, Rongyu Cao, Yijun Xiao, Fen Lin, Bo Chen, Qing\n  He", "title": "Tree-Structured Neural Machine for Linguistics-Aware Sentence Generation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Different from other sequential data, sentences in natural language are\nstructured by linguistic grammars. Previous generative conversational models\nwith chain-structured decoder ignore this structure in human language and might\ngenerate plausible responses with less satisfactory relevance and fluency. In\nthis study, we aim to incorporate the results from linguistic analysis into the\nprocess of sentence generation for high-quality conversation generation.\nSpecifically, we use a dependency parser to transform each response sentence\ninto a dependency tree and construct a training corpus of sentence-tree pairs.\nA tree-structured decoder is developed to learn the mapping from a sentence to\nits tree, where different types of hidden states are used to depict the local\ndependencies from an internal tree node to its children. For training\nacceleration, we propose a tree canonicalization method, which transforms trees\ninto equivalent ternary trees. Then, with a proposed tree-structured search\nmethod, the model is able to generate the most probable responses in the form\nof dependency trees, which are finally flattened into sequences as the system\noutput. Experimental results demonstrate that the proposed X2Tree framework\noutperforms baseline methods over 11.15% increase of acceptance ratio.\n", "versions": [{"version": "v1", "created": "Sun, 30 Apr 2017 15:09:10 GMT"}, {"version": "v2", "created": "Sat, 6 May 2017 14:38:10 GMT"}, {"version": "v3", "created": "Tue, 21 Nov 2017 05:31:23 GMT"}, {"version": "v4", "created": "Wed, 3 Jan 2018 07:35:19 GMT"}], "update_date": "2018-01-04", "authors_parsed": [["Zhou", "Ganbin", ""], ["Luo", "Ping", ""], ["Cao", "Rongyu", ""], ["Xiao", "Yijun", ""], ["Lin", "Fen", ""], ["Chen", "Bo", ""], ["He", "Qing", ""]]}, {"id": "1705.00334", "submitter": "Sibi Venkatesan", "authors": "Sibi Venkatesan, James K. Miller, Jeff Schneider and Artur Dubrawski", "title": "Scaling Active Search using Linear Similarity Functions", "comments": "To be published as conference paper at IJCAI 2017, 11 pages, 2\n  figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Active Search has become an increasingly useful tool in information retrieval\nproblems where the goal is to discover as many target elements as possible\nusing only limited label queries. With the advent of big data, there is a\ngrowing emphasis on the scalability of such techniques to handle very large and\nvery complex datasets.\n  In this paper, we consider the problem of Active Search where we are given a\nsimilarity function between data points. We look at an algorithm introduced by\nWang et al. [2013] for Active Search over graphs and propose crucial\nmodifications which allow it to scale significantly. Their approach selects\npoints by minimizing an energy function over the graph induced by the\nsimilarity function on the data. Our modifications require the similarity\nfunction to be a dot-product between feature vectors of data points, equivalent\nto having a linear kernel for the adjacency matrix. With this, we are able to\nscale tremendously: for $n$ data points, the original algorithm runs in\n$O(n^2)$ time per iteration while ours runs in only $O(nr + r^2)$ given\n$r$-dimensional features.\n  We also describe a simple alternate approach using a weighted-neighbor\npredictor which also scales well. In our experiments, we show that our method\nis competitive with existing semi-supervised approaches. We also briefly\ndiscuss conditions under which our algorithm performs well.\n", "versions": [{"version": "v1", "created": "Sun, 30 Apr 2017 15:49:01 GMT"}, {"version": "v2", "created": "Tue, 22 Aug 2017 00:08:00 GMT"}], "update_date": "2017-08-23", "authors_parsed": [["Venkatesan", "Sibi", ""], ["Miller", "James K.", ""], ["Schneider", "Jeff", ""], ["Dubrawski", "Artur", ""]]}, {"id": "1705.00345", "submitter": "Andrea Rocchetto", "authors": "Andrea Rocchetto", "title": "Stabiliser states are efficiently PAC-learnable", "comments": "v2: 11 pages, typos corrected, introduction of a number of stylistic\n  changes and analysis of the computational cost of the learning algorithm", "journal-ref": "Quantum Information and Computation, Vol. 18, No. 7&8 (2018)\n  541-552", "doi": null, "report-no": null, "categories": "quant-ph cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The exponential scaling of the wave function is a fundamental property of\nquantum systems with far reaching implications in our ability to process\nquantum information. A problem where these are particularly relevant is quantum\nstate tomography. State tomography, whose objective is to obtain a full\ndescription of a quantum system, can be analysed in the framework of\ncomputational learning theory. In this model, quantum states have been shown to\nbe Probably Approximately Correct (PAC)-learnable with sample complexity linear\nin the number of qubits. However, it is conjectured that in general quantum\nstates require an exponential amount of computation to be learned. Here, using\nresults from the literature on the efficient classical simulation of quantum\nsystems, we show that stabiliser states are efficiently PAC-learnable. Our\nresults solve an open problem formulated by Aaronson [Proc. R. Soc. A, 2088,\n(2007)] and propose learning theory as a tool for exploring the power of\nquantum computation.\n", "versions": [{"version": "v1", "created": "Sun, 30 Apr 2017 17:16:10 GMT"}, {"version": "v2", "created": "Fri, 8 Jun 2018 17:41:51 GMT"}], "update_date": "2018-06-11", "authors_parsed": [["Rocchetto", "Andrea", ""]]}, {"id": "1705.00346", "submitter": "Andre Luckow", "authors": "Andre Luckow and Matthew Cook and Nathan Ashcraft and Edwin Weill and\n  Emil Djerekarov and Bennie Vorster", "title": "Deep Learning in the Automotive Industry: Applications and Tools", "comments": "10 pages", "journal-ref": null, "doi": "10.1109/BigData.2016.7841045", "report-no": null, "categories": "cs.LG cs.CV cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep Learning refers to a set of machine learning techniques that utilize\nneural networks with many hidden layers for tasks, such as image\nclassification, speech recognition, language understanding. Deep learning has\nbeen proven to be very effective in these domains and is pervasively used by\nmany Internet services. In this paper, we describe different automotive uses\ncases for deep learning in particular in the domain of computer vision. We\nsurveys the current state-of-the-art in libraries, tools and infrastructures\n(e.\\,g.\\ GPUs and clouds) for implementing, training and deploying deep neural\nnetworks. We particularly focus on convolutional neural networks and computer\nvision use cases, such as the visual inspection process in manufacturing plants\nand the analysis of social media data. To train neural networks, curated and\nlabeled datasets are essential. In particular, both the availability and scope\nof such datasets is typically very limited. A main contribution of this paper\nis the creation of an automotive dataset, that allows us to learn and\nautomatically recognize different vehicle properties. We describe an end-to-end\ndeep learning application utilizing a mobile app for data collection and\nprocess support, and an Amazon-based cloud backend for storage and training.\nFor training we evaluate the use of cloud and on-premises infrastructures\n(including multiple GPUs) in conjunction with different neural network\narchitectures and frameworks. We assess both the training times as well as the\naccuracy of the classifier. Finally, we demonstrate the effectiveness of the\ntrained classifier in a real world setting during manufacturing process.\n", "versions": [{"version": "v1", "created": "Sun, 30 Apr 2017 17:17:44 GMT"}], "update_date": "2017-05-02", "authors_parsed": [["Luckow", "Andre", ""], ["Cook", "Matthew", ""], ["Ashcraft", "Nathan", ""], ["Weill", "Edwin", ""], ["Djerekarov", "Emil", ""], ["Vorster", "Bennie", ""]]}, {"id": "1705.00347", "submitter": "Jayadeva", "authors": "Jayadeva, Himanshu Pant, Sumit Soman and Mayank Sharma", "title": "Scalable Twin Neural Networks for Classification of Unbalanced Data", "comments": "20 pages, 8 figures, 14 tables", "journal-ref": "Neurocomputing (Special Issue on Learning in the Presence of Class\n  Imbalance and Concept Drift), 2019", "doi": "10.1016/j.neucom.2018.07.089", "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Twin Support Vector Machines (TWSVMs) have emerged an efficient alternative\nto Support Vector Machines (SVM) for learning from imbalanced datasets. The\nTWSVM learns two non-parallel classifying hyperplanes by solving a couple of\nsmaller sized problems. However, it is unsuitable for large datasets, as it\ninvolves matrix operations. In this paper, we discuss a Twin Neural Network\n(Twin NN) architecture for learning from large unbalanced datasets. The Twin NN\nalso learns an optimal feature map, allowing for better discrimination between\nclasses. We also present an extension of this network architecture for\nmulticlass datasets. Results presented in the paper demonstrate that the Twin\nNN generalizes well and scales well on large unbalanced datasets.\n", "versions": [{"version": "v1", "created": "Sun, 30 Apr 2017 17:25:45 GMT"}, {"version": "v2", "created": "Sat, 27 Jan 2018 05:03:58 GMT"}], "update_date": "2019-02-12", "authors_parsed": [["Jayadeva", "", ""], ["Pant", "Himanshu", ""], ["Soman", "Sumit", ""], ["Sharma", "Mayank", ""]]}, {"id": "1705.00375", "submitter": "Natali Ruchansky", "authors": "Natali Ruchansky and Mark Crovella and Evimaria Terzi", "title": "Targeted matrix completion", "comments": "Proceedings of the 2017 SIAM International Conference on Data Mining\n  (SDM)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Matrix completion is a problem that arises in many data-analysis settings\nwhere the input consists of a partially-observed matrix (e.g., recommender\nsystems, traffic matrix analysis etc.). Classical approaches to matrix\ncompletion assume that the input partially-observed matrix is low rank. The\nsuccess of these methods depends on the number of observed entries and the rank\nof the matrix; the larger the rank, the more entries need to be observed in\norder to accurately complete the matrix. In this paper, we deal with matrices\nthat are not necessarily low rank themselves, but rather they contain low-rank\nsubmatrices. We propose Targeted, which is a general framework for completing\nsuch matrices. In this framework, we first extract the low-rank submatrices and\nthen apply a matrix-completion algorithm to these low-rank submatrices as well\nas the remainder matrix separately. Although for the completion itself we use\nstate-of-the-art completion methods, our results demonstrate that Targeted\nachieves significantly smaller reconstruction errors than other classical\nmatrix-completion methods. One of the key technical contributions of the paper\nlies in the identification of the low-rank submatrices from the input\npartially-observed matrices.\n", "versions": [{"version": "v1", "created": "Sun, 30 Apr 2017 21:30:05 GMT"}], "update_date": "2017-05-02", "authors_parsed": [["Ruchansky", "Natali", ""], ["Crovella", "Mark", ""], ["Terzi", "Evimaria", ""]]}, {"id": "1705.00399", "submitter": "Natali Ruchansky", "authors": "Natali Ruchansky and Mark Crovella and Evimaria Terzi", "title": "Matrix completion with queries", "comments": "Proceedings of the 21th ACM SIGKDD International Conference on\n  Knowledge Discovery and Data Mining", "journal-ref": null, "doi": "10.1145/2783258.2783259", "report-no": null, "categories": "cs.LG cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In many applications, e.g., recommender systems and traffic monitoring, the\ndata comes in the form of a matrix that is only partially observed and low\nrank. A fundamental data-analysis task for these datasets is matrix completion,\nwhere the goal is to accurately infer the entries missing from the matrix. Even\nwhen the data satisfies the low-rank assumption, classical matrix-completion\nmethods may output completions with significant error -- in that the\nreconstructed matrix differs significantly from the true underlying matrix.\nOften, this is due to the fact that the information contained in the observed\nentries is insufficient. In this work, we address this problem by proposing an\nactive version of matrix completion, where queries can be made to the true\nunderlying matrix. Subsequently, we design Order&Extend, which is the first\nalgorithm to unify a matrix-completion approach and a querying strategy into a\nsingle algorithm. Order&Extend is able identify and alleviate insufficient\ninformation by judiciously querying a small number of additional entries. In an\nextensive experimental evaluation on real-world datasets, we demonstrate that\nour algorithm is efficient and is able to accurately reconstruct the true\nmatrix while asking only a small number of queries.\n", "versions": [{"version": "v1", "created": "Mon, 1 May 2017 01:58:45 GMT"}], "update_date": "2017-05-02", "authors_parsed": [["Ruchansky", "Natali", ""], ["Crovella", "Mark", ""], ["Terzi", "Evimaria", ""]]}, {"id": "1705.00462", "submitter": "Francisco Paisana Francisco Paisana", "authors": "Ahmed Selim, Francisco Paisana, Jerome A. Arokkiam, Yi Zhang, Linda\n  Doyle, Luiz A. DaSilva", "title": "Spectrum Monitoring for Radar Bands using Deep Convolutional Neural\n  Networks", "comments": "7 pages, 10 figures, conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI cs.IT cs.LG math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present a spectrum monitoring framework for the detection\nof radar signals in spectrum sharing scenarios. The core of our framework is a\ndeep convolutional neural network (CNN) model that enables Measurement Capable\nDevices to identify the presence of radar signals in the radio spectrum, even\nwhen these signals are overlapped with other sources of interference, such as\ncommercial LTE and WLAN. We collected a large dataset of RF measurements, which\ninclude the transmissions of multiple radar pulse waveforms, downlink LTE,\nWLAN, and thermal noise. We propose a pre-processing data representation that\nleverages the amplitude and phase shifts of the collected samples. This\nrepresentation allows our CNN model to achieve a classification accuracy of\n99.6% on our testing dataset. The trained CNN model is then tested under\nvarious SNR values, outperforming other models, such as spectrogram-based CNN\nmodels.\n", "versions": [{"version": "v1", "created": "Mon, 1 May 2017 10:37:43 GMT"}], "update_date": "2017-05-02", "authors_parsed": [["Selim", "Ahmed", ""], ["Paisana", "Francisco", ""], ["Arokkiam", "Jerome A.", ""], ["Zhang", "Yi", ""], ["Doyle", "Linda", ""], ["DaSilva", "Luiz A.", ""]]}, {"id": "1705.00467", "submitter": "Bamdev Mishra", "authors": "Bamdev Mishra, Hiroyuki Kasai, Pratik Jawanpuria, and Atul Saroop", "title": "A Riemannian gossip approach to subspace learning on Grassmann manifold", "comments": "Title change, extension of the technical report arXiv:1605.06968", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we focus on subspace learning problems on the Grassmann\nmanifold. Interesting applications in this setting include low-rank matrix\ncompletion and low-dimensional multivariate regression, among others. Motivated\nby privacy concerns, we aim to solve such problems in a decentralized setting\nwhere multiple agents have access to (and solve) only a part of the whole\noptimization problem. The agents communicate with each other to arrive at a\nconsensus, i.e., agree on a common quantity, via the gossip protocol.\n  We propose a novel cost function for subspace learning on the Grassmann\nmanifold, which is a weighted sum of several sub-problems (each solved by an\nagent) and the communication cost among the agents. The cost function has a\nfinite sum structure. In the proposed modeling approach, different agents learn\nindividual local subspace but they achieve asymptotic consensus on the global\nlearned subspace. The approach is scalable and parallelizable. Numerical\nexperiments show the efficacy of the proposed decentralized algorithms on\nvarious matrix completion and multivariate regression benchmarks.\n", "versions": [{"version": "v1", "created": "Mon, 1 May 2017 11:02:17 GMT"}, {"version": "v2", "created": "Wed, 28 Feb 2018 18:46:33 GMT"}], "update_date": "2018-03-01", "authors_parsed": [["Mishra", "Bamdev", ""], ["Kasai", "Hiroyuki", ""], ["Jawanpuria", "Pratik", ""], ["Saroop", "Atul", ""]]}, {"id": "1705.00470", "submitter": "Thomas Moerland", "authors": "Thomas M. Moerland, Joost Broekens and Catholijn M. Jonker", "title": "Learning Multimodal Transition Dynamics for Model-Based Reinforcement\n  Learning", "comments": "Scaling Up Reinforcement Learning (SURL) Workshop @ European Machine\n  Learning Conference (ECML)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we study how to learn stochastic, multimodal transition\ndynamics in reinforcement learning (RL) tasks. We focus on evaluating\ntransition function estimation, while we defer planning over this model to\nfuture work. Stochasticity is a fundamental property of many task environments.\nHowever, discriminative function approximators have difficulty estimating\nmultimodal stochasticity. In contrast, deep generative models do capture\ncomplex high-dimensional outcome distributions. First we discuss why, amongst\nsuch models, conditional variational inference (VI) is theoretically most\nappealing for model-based RL. Subsequently, we compare different VI models on\ntheir ability to learn complex stochasticity on simulated functions, as well as\non a typical RL gridworld with multimodal dynamics. Results show VI\nsuccessfully predicts multimodal outcomes, but also robustly ignores these for\ndeterministic parts of the transition dynamics. In summary, we show a robust\nmethod to learn multimodal transitions using function approximation, which is a\nkey preliminary for model-based RL in stochastic domains.\n", "versions": [{"version": "v1", "created": "Mon, 1 May 2017 11:06:04 GMT"}, {"version": "v2", "created": "Tue, 8 Aug 2017 13:50:49 GMT"}], "update_date": "2017-08-09", "authors_parsed": [["Moerland", "Thomas M.", ""], ["Broekens", "Joost", ""], ["Jonker", "Catholijn M.", ""]]}, {"id": "1705.00522", "submitter": "Sohrab Ferdowsi", "authors": "Sohrab Ferdowsi, Slava Voloshynovskiy, Dimche Kostadinov", "title": "Regularized Residual Quantization: a multi-layer sparse dictionary\n  learning approach", "comments": "To be presented at SPARS 2017, Lisbon, Portugal", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Residual Quantization (RQ) framework is revisited where the quantization\ndistortion is being successively reduced in multi-layers. Inspired by the\nreverse-water-filling paradigm in rate-distortion theory, an efficient\nregularization on the variances of the codewords is introduced which allows to\nextend the RQ for very large numbers of layers and also for high dimensional\ndata, without getting over-trained. The proposed Regularized Residual\nQuantization (RRQ) results in multi-layer dictionaries which are additionally\nsparse, thanks to the soft-thresholding nature of the regularization when\napplied to variance-decaying data which can arise from de-correlating\ntransformations applied to correlated data. Furthermore, we also propose a\ngeneral-purpose pre-processing for natural images which makes them suitable for\nsuch quantization. The RRQ framework is first tested on synthetic\nvariance-decaying data to show its efficiency in quantization of\nhigh-dimensional data. Next, we use the RRQ in super-resolution of a database\nof facial images where it is shown that low-resolution facial images from the\ntest set quantized with codebooks trained on high-resolution images from the\ntraining set show relevant high-frequency content when reconstructed with those\ncodebooks.\n", "versions": [{"version": "v1", "created": "Mon, 1 May 2017 13:59:04 GMT"}], "update_date": "2017-05-02", "authors_parsed": [["Ferdowsi", "Sohrab", ""], ["Voloshynovskiy", "Slava", ""], ["Kostadinov", "Dimche", ""]]}, {"id": "1705.00534", "submitter": "Bo Li", "authors": "Bo Li, Yuchao Dai, Huahui Chen, Mingyi He", "title": "Single image depth estimation by dilated deep residual convolutional\n  neural network and soft-weight-sum inference", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a new residual convolutional neural network (CNN)\narchitecture for single image depth estimation. Compared with existing deep CNN\nbased methods, our method achieves much better results with fewer training\nexamples and model parameters. The advantages of our method come from the usage\nof dilated convolution, skip connection architecture and soft-weight-sum\ninference. Experimental evaluation on the NYU Depth V2 dataset shows that our\nmethod outperforms other state-of-the-art methods by a margin.\n", "versions": [{"version": "v1", "created": "Thu, 27 Apr 2017 06:07:05 GMT"}], "update_date": "2017-05-02", "authors_parsed": [["Li", "Bo", ""], ["Dai", "Yuchao", ""], ["Chen", "Huahui", ""], ["He", "Mingyi", ""]]}, {"id": "1705.00557", "submitter": "Yacine Jernite", "authors": "Yacine Jernite, Samuel R. Bowman and David Sontag", "title": "Discourse-Based Objectives for Fast Unsupervised Sentence Representation\n  Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work presents a novel objective function for the unsupervised training\nof neural network sentence encoders. It exploits signals from paragraph-level\ndiscourse coherence to train these models to understand text. Our objective is\npurely discriminative, allowing us to train models many times faster than was\npossible under prior methods, and it yields models which perform well in\nextrinsic evaluations.\n", "versions": [{"version": "v1", "created": "Sun, 23 Apr 2017 09:15:35 GMT"}], "update_date": "2017-05-02", "authors_parsed": [["Jernite", "Yacine", ""], ["Bowman", "Samuel R.", ""], ["Sontag", "David", ""]]}, {"id": "1705.00574", "submitter": "Alexey Romanov", "authors": "Alexey Romanov and Anna Rumshisky", "title": "Forced to Learn: Discovering Disentangled Representations Without\n  Exhaustive Labels", "comments": "Abstract accepted at ICLR 2017 Workshop:\n  https://openreview.net/pdf?id=SkCmfeSFg", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning a better representation with neural networks is a challenging\nproblem, which was tackled extensively from different prospectives in the past\nfew years. In this work, we focus on learning a representation that could be\nused for a clustering task and introduce two novel loss components that\nsubstantially improve the quality of produced clusters, are simple to apply to\nan arbitrary model and cost function, and do not require a complicated training\nprocedure. We evaluate them on two most common types of models, Recurrent\nNeural Networks and Convolutional Neural Networks, showing that the approach we\npropose consistently improves the quality of KMeans clustering in terms of\nAdjusted Mutual Information score and outperforms previously proposed methods.\n", "versions": [{"version": "v1", "created": "Mon, 1 May 2017 16:03:25 GMT"}], "update_date": "2017-05-02", "authors_parsed": [["Romanov", "Alexey", ""], ["Rumshisky", "Anna", ""]]}, {"id": "1705.00597", "submitter": "Xiaofeng Zhang", "authors": "Zhaocai Sun, William K. Cheung, Xiaofeng Zhang, Jun Yang", "title": "Towards well-specified semi-supervised model-based classifiers via\n  structural adaptation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Semi-supervised learning plays an important role in large-scale machine\nlearning. Properly using additional unlabeled data (largely available nowadays)\noften can improve the machine learning accuracy. However, if the machine\nlearning model is misspecified for the underlying true data distribution, the\nmodel performance could be seriously jeopardized. This issue is known as model\nmisspecification. To address this issue, we focus on generative models and\npropose a criterion to detect the onset of model misspecification by measuring\nthe performance difference between models obtained using supervised and\nsemi-supervised learning. Then, we propose to automatically modify the\ngenerative models during model training to achieve an unbiased generative\nmodel. Rigorous experiments were carried out to evaluate the proposed method\nusing two image classification data sets PASCAL VOC'07 and MIR Flickr. Our\nproposed method has been demonstrated to outperform a number of\nstate-of-the-art semi-supervised learning approaches for the classification\ntask.\n", "versions": [{"version": "v1", "created": "Mon, 1 May 2017 17:26:53 GMT"}], "update_date": "2017-05-02", "authors_parsed": [["Sun", "Zhaocai", ""], ["Cheung", "William K.", ""], ["Zhang", "Xiaofeng", ""], ["Yang", "Jun", ""]]}, {"id": "1705.00607", "submitter": "Cheng Zhang", "authors": "Cheng Zhang, Hedvig Kjellstrom, Stephan Mandt", "title": "Determinantal Point Processes for Mini-Batch Diversification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study a mini-batch diversification scheme for stochastic gradient descent\n(SGD). While classical SGD relies on uniformly sampling data points to form a\nmini-batch, we propose a non-uniform sampling scheme based on the Determinantal\nPoint Process (DPP). The DPP relies on a similarity measure between data points\nand gives low probabilities to mini-batches which contain redundant data, and\nhigher probabilities to mini-batches with more diverse data. This\nsimultaneously balances the data and leads to stochastic gradients with lower\nvariance. We term this approach Diversified Mini-Batch SGD (DM-SGD). We show\nthat regular SGD and a biased version of stratified sampling emerge as special\ncases. Furthermore, DM-SGD generalizes stratified sampling to cases where no\ndiscrete features exist to bin the data into groups. We show experimentally\nthat our method results more interpretable and diverse features in unsupervised\nsetups, and in better classification accuracies in supervised setups.\n", "versions": [{"version": "v1", "created": "Mon, 1 May 2017 17:53:51 GMT"}, {"version": "v2", "created": "Wed, 23 Aug 2017 23:34:28 GMT"}], "update_date": "2017-09-12", "authors_parsed": [["Zhang", "Cheng", ""], ["Kjellstrom", "Hedvig", ""], ["Mandt", "Stephan", ""]]}, {"id": "1705.00678", "submitter": "Zhao Kang", "authors": "Zhao Kang, Chong Peng, Qiang Cheng", "title": "Twin Learning for Similarity and Clustering: A Unified Kernel Approach", "comments": "Published in AAAI 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many similarity-based clustering methods work in two separate steps including\nsimilarity matrix computation and subsequent spectral clustering. However,\nsimilarity measurement is challenging because it is usually impacted by many\nfactors, e.g., the choice of similarity metric, neighborhood size, scale of\ndata, noise and outliers. Thus the learned similarity matrix is often not\nsuitable, let alone optimal, for the subsequent clustering. In addition,\nnonlinear similarity often exists in many real world data which, however, has\nnot been effectively considered by most existing methods. To tackle these two\nchallenges, we propose a model to simultaneously learn cluster indicator matrix\nand similarity information in kernel spaces in a principled way. We show\ntheoretical relationships to kernel k-means, k-means, and spectral clustering\nmethods. Then, to address the practical issue of how to select the most\nsuitable kernel for a particular clustering task, we further extend our model\nwith a multiple kernel learning ability. With this joint model, we can\nautomatically accomplish three subtasks of finding the best cluster indicator\nmatrix, the most accurate similarity relations and the optimal combination of\nmultiple kernels. By leveraging the interactions between these three subtasks\nin a joint framework, each subtask can be iteratively boosted by using the\nresults of the others towards an overall optimal solution. Extensive\nexperiments are performed to demonstrate the effectiveness of our method.\n", "versions": [{"version": "v1", "created": "Mon, 1 May 2017 19:33:27 GMT"}, {"version": "v2", "created": "Wed, 3 May 2017 00:29:13 GMT"}], "update_date": "2017-05-04", "authors_parsed": [["Kang", "Zhao", ""], ["Peng", "Chong", ""], ["Cheng", "Qiang", ""]]}, {"id": "1705.00687", "submitter": "Junming Yin", "authors": "Junming Yin and Yaoliang Yu", "title": "Convex-constrained Sparse Additive Modeling and Its Extensions", "comments": "17 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sparse additive modeling is a class of effective methods for performing\nhigh-dimensional nonparametric regression. In this work we show how shape\nconstraints such as convexity/concavity and their extensions, can be integrated\ninto additive models. The proposed sparse difference of convex additive models\n(SDCAM) can estimate most continuous functions without any a priori smoothness\nassumption. Motivated by a characterization of difference of convex functions,\nour method incorporates a natural regularization functional to avoid\noverfitting and to reduce model complexity. Computationally, we develop an\nefficient backfitting algorithm with linear per-iteration complexity.\nExperiments on both synthetic and real data verify that our method is\ncompetitive against state-of-the-art sparse additive models, with improved\nperformance in most scenarios.\n", "versions": [{"version": "v1", "created": "Mon, 1 May 2017 19:56:03 GMT"}], "update_date": "2017-05-03", "authors_parsed": [["Yin", "Junming", ""], ["Yu", "Yaoliang", ""]]}, {"id": "1705.00740", "submitter": "Cheng Li", "authors": "Bingyu Wang, Cheng Li, Virgil Pavlu, Javed Aslam", "title": "Regularizing Model Complexity and Label Structure for Multi-Label Text\n  Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multi-label text classification is a popular machine learning task where each\ndocument is assigned with multiple relevant labels. This task is challenging\ndue to high dimensional features and correlated labels. Multi-label text\nclassifiers need to be carefully regularized to prevent the severe over-fitting\nin the high dimensional space, and also need to take into account label\ndependencies in order to make accurate predictions under uncertainty. We\ndemonstrate significant and practical improvement by carefully regularizing the\nmodel complexity during training phase, and also regularizing the label search\nspace during prediction phase. Specifically, we regularize the classifier\ntraining using Elastic-net (L1+L2) penalty for reducing model complexity/size,\nand employ early stopping to prevent overfitting. At prediction time, we apply\nsupport inference to restrict the search space to label sets encountered in the\ntraining set, and F-optimizer GFM to make optimal predictions for the F1\nmetric. We show that although support inference only provides density\nestimations on existing label combinations, when combined with GFM predictor,\nthe algorithm can output unseen label combinations. Taken collectively, our\nexperiments show state of the art results on many benchmark datasets. Beyond\nperformance and practical contributions, we make some interesting observations.\nContrary to the prior belief, which deems support inference as purely an\napproximate inference procedure, we show that support inference acts as a\nstrong regularizer on the label prediction structure. It allows the classifier\nto take into account label dependencies during prediction even if the\nclassifiers had not modeled any label dependencies during training.\n", "versions": [{"version": "v1", "created": "Mon, 1 May 2017 23:30:13 GMT"}], "update_date": "2017-05-03", "authors_parsed": [["Wang", "Bingyu", ""], ["Li", "Cheng", ""], ["Pavlu", "Virgil", ""], ["Aslam", "Javed", ""]]}, {"id": "1705.00744", "submitter": "Ragav Venkatesan", "authors": "Ragav Venkatesan, Hemanth Venkateswara, Sethuraman Panchanathan,\n  Baoxin Li", "title": "A Strategy for an Uncompromising Incremental Learner", "comments": "Under review at IEEE Transactions of Neural Networks and Learning\n  Systems", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multi-class supervised learning systems require the knowledge of the entire\nrange of labels they predict. Often when learnt incrementally, they suffer from\ncatastrophic forgetting. To avoid this, generous leeways have to be made to the\nphilosophy of incremental learning that either forces a part of the machine to\nnot learn, or to retrain the machine again with a selection of the historic\ndata. While these hacks work to various degrees, they do not adhere to the\nspirit of incremental learning. In this article, we redefine incremental\nlearning with stringent conditions that do not allow for any undesirable\nrelaxations and assumptions. We design a strategy involving generative models\nand the distillation of dark knowledge as a means of hallucinating data along\nwith appropriate targets from past distributions. We call this technique,\nphantom sampling.We show that phantom sampling helps avoid catastrophic\nforgetting during incremental learning. Using an implementation based on deep\nneural networks, we demonstrate that phantom sampling dramatically avoids\ncatastrophic forgetting. We apply these strategies to competitive multi-class\nincremental learning of deep neural networks. Using various benchmark datasets\nand through our strategy, we demonstrate that strict incremental learning could\nbe achieved. We further put our strategy to test on challenging cases,\nincluding cross-domain increments and incrementing on a novel label space. We\nalso propose a trivial extension to unbounded-continual learning and identify\npotential for future development.\n", "versions": [{"version": "v1", "created": "Tue, 2 May 2017 00:17:54 GMT"}, {"version": "v2", "created": "Mon, 17 Jul 2017 07:30:18 GMT"}], "update_date": "2017-07-18", "authors_parsed": [["Venkatesan", "Ragav", ""], ["Venkateswara", "Hemanth", ""], ["Panchanathan", "Sethuraman", ""], ["Li", "Baoxin", ""]]}, {"id": "1705.00797", "submitter": "Konstantin Bauman", "authors": "Evgeny Bauman and Konstantin Bauman", "title": "One-Class Semi-Supervised Learning: Detecting Linearly Separable Class\n  by its Mean", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we presented a novel semi-supervised one-class classification\nalgorithm which assumes that class is linearly separable from other elements.\nWe proved theoretically that class is linearly separable if and only if it is\nmaximal by probability within the sets with the same mean. Furthermore, we\npresented an algorithm for identifying such linearly separable class utilizing\nlinear programming. We described three application cases including an\nassumption of linear separability, Gaussian distribution, and the case of\nlinear separability in transformed space of kernel functions. Finally, we\ndemonstrated the work of the proposed algorithm on the USPS dataset and\nanalyzed the relationship of the performance of the algorithm and the size of\nthe initially labeled sample.\n", "versions": [{"version": "v1", "created": "Tue, 2 May 2017 05:00:28 GMT"}], "update_date": "2017-05-03", "authors_parsed": [["Bauman", "Evgeny", ""], ["Bauman", "Konstantin", ""]]}, {"id": "1705.00813", "submitter": "YueChi Ma", "authors": "Yue-Chi Ma, Man-Hong Yung", "title": "Transforming Bell's Inequalities into State Classifiers with Machine\n  Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "quant-ph cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Quantum information science has profoundly changed the ways we understand,\nstore, and process information. A major challenge in this field is to look for\nan efficient means for classifying quantum state. For instance, one may want to\ndetermine if a given quantum state is entangled or not. However, the process of\na complete characterization of quantum states, known as quantum state\ntomography, is a resource-consuming operation in general. An attractive\nproposal would be the use of Bell's inequalities as an entanglement witness,\nwhere only partial information of the quantum state is needed. The problem is\nthat entanglement is necessary but not sufficient for violating Bell's\ninequalities, making it an unreliable state classifier. Here we aim at solving\nthis problem by the methods of machine learning. More precisely, given a family\nof quantum states, we randomly picked a subset of it to construct a\nquantum-state classifier, accepting only partial information of each quantum\nstate. Our results indicated that these transformed Bell-type inequalities can\nperform significantly better than the original Bell's inequalities in\nclassifying entangled states. We further extended our analysis to three-qubit\nand four-qubit systems, performing classification of quantum states into\nmultiple species. These results demonstrate how the tools in machine learning\ncan be applied to solving problems in quantum information science.\n", "versions": [{"version": "v1", "created": "Tue, 2 May 2017 06:30:56 GMT"}], "update_date": "2017-05-03", "authors_parsed": [["Ma", "Yue-Chi", ""], ["Yung", "Man-Hong", ""]]}, {"id": "1705.00825", "submitter": "Xiaokai Wei", "authors": "Xiaokai Wei, Bokai Cao and Philip S. Yu", "title": "Multi-view Unsupervised Feature Selection by Cross-diffused Matrix\n  Alignment", "comments": "8 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multi-view high-dimensional data become increasingly popular in the big data\nera. Feature selection is a useful technique for alleviating the curse of\ndimensionality in multi-view learning. In this paper, we study unsupervised\nfeature selection for multi-view data, as class labels are usually expensive to\nobtain. Traditional feature selection methods are mostly designed for\nsingle-view data and cannot fully exploit the rich information from multi-view\ndata. Existing multi-view feature selection methods are usually based on noisy\ncluster labels which might not preserve sufficient information from multi-view\ndata. To better utilize multi-view information, we propose a method, CDMA-FS,\nto select features for each view by performing alignment on a cross diffused\nmatrix. We formulate it as a constrained optimization problem and solve it\nusing Quasi-Newton based method. Experiments results on four real-world\ndatasets show that the proposed method is more effective than the\nstate-of-the-art methods in multi-view setting.\n", "versions": [{"version": "v1", "created": "Tue, 2 May 2017 07:12:59 GMT"}], "update_date": "2017-05-03", "authors_parsed": [["Wei", "Xiaokai", ""], ["Cao", "Bokai", ""], ["Yu", "Philip S.", ""]]}, {"id": "1705.00831", "submitter": "Brendan Avent", "authors": "Brendan Avent, Aleksandra Korolova, David Zeber, Torgeir Hovden,\n  Benjamin Livshits", "title": "BLENDER: Enabling Local Search with a Hybrid Differential Privacy Model", "comments": "Proceedings of the 26th USENIX Security Symposium (USENIX Security\n  17). August 16-18, 2017, Vancouver, BC. ISBN 978-1-931971-40-9", "journal-ref": "Journal of Privacy and Confidentiality, Vol. 9 (2) 2019", "doi": "10.29012/jpc.680", "report-no": null, "categories": "cs.CR cs.CY cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a hybrid model of differential privacy that considers a\ncombination of regular and opt-in users who desire the differential privacy\nguarantees of the local privacy model and the trusted curator model,\nrespectively. We demonstrate that within this model, it is possible to design a\nnew type of blended algorithm for the task of privately computing the head of a\nsearch log. This blended approach provides significant improvements in the\nutility of obtained data compared to related work while providing users with\ntheir desired privacy guarantees. Specifically, on two large search click data\nsets, comprising 1.75 and 16 GB respectively, our approach attains NDCG values\nexceeding 95% across a range of privacy budget values.\n", "versions": [{"version": "v1", "created": "Tue, 2 May 2017 07:36:35 GMT"}, {"version": "v2", "created": "Fri, 27 Oct 2017 17:28:55 GMT"}, {"version": "v3", "created": "Sun, 26 Aug 2018 10:42:39 GMT"}, {"version": "v4", "created": "Thu, 21 Nov 2019 23:11:29 GMT"}], "update_date": "2019-11-25", "authors_parsed": [["Avent", "Brendan", ""], ["Korolova", "Aleksandra", ""], ["Zeber", "David", ""], ["Hovden", "Torgeir", ""], ["Livshits", "Benjamin", ""]]}, {"id": "1705.00840", "submitter": "{\\L}ukasz Struski", "authors": "{\\L}ukasz Struski, Marek \\'Smieja, Jacek Tabor", "title": "Pointed subspace approach to incomplete data", "comments": "13 pages, 3 figures and 3 tables. arXiv admin note: text overlap with\n  arXiv:1612.01480", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Incomplete data are often represented as vectors with filled missing\nattributes joined with flag vectors indicating missing components. In this\npaper we generalize this approach and represent incomplete data as pointed\naffine subspaces. This allows to perform various affine transformations of\ndata, as whitening or dimensionality reduction. We embed such generalized\nmissing data into a vector space by mapping pointed affine subspace\n(generalized missing data point) to a vector containing imputed values joined\nwith a corresponding projection matrix. Such an operation preserves the scalar\nproduct of the embedding defined for flag vectors and allows to input\ntransformed incomplete data to typical classification methods.\n", "versions": [{"version": "v1", "created": "Tue, 2 May 2017 07:59:01 GMT"}], "update_date": "2017-05-03", "authors_parsed": [["Struski", "\u0141ukasz", ""], ["\u015amieja", "Marek", ""], ["Tabor", "Jacek", ""]]}, {"id": "1705.00850", "submitter": "Haiping Huang", "authors": "Haiping Huang, and Alireza Goudarzi", "title": "Random active path model of deep neural networks with diluted binary\n  synapses", "comments": "10 pages, 5 figures, with Supplemental Material (upon request)", "journal-ref": "Phys. Rev. E 98, 042311 (2018)", "doi": "10.1103/PhysRevE.98.042311", "report-no": null, "categories": "cs.LG cond-mat.stat-mech stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning has become a powerful and popular tool for a variety of machine\nlearning tasks. However, it is challenging to understand the mechanism of deep\nlearning from a theoretical perspective. In this work, we propose a random\nactive path model to study collective properties of deep neural networks with\nbinary synapses, under the removal perturbation of connections between layers.\nIn the model, the path from input to output is randomly activated, and the\ncorresponding input unit constrains the weights along the path into the form of\na $p$-weight interaction glass model. A critical value of the perturbation is\nobserved to separate a spin glass regime from a paramagnetic regime, with the\ntransition being of the first order. The paramagnetic phase is conjectured to\nhave a poor generalization performance.\n", "versions": [{"version": "v1", "created": "Tue, 2 May 2017 08:16:12 GMT"}, {"version": "v2", "created": "Fri, 18 May 2018 00:49:27 GMT"}, {"version": "v3", "created": "Wed, 26 Sep 2018 13:09:33 GMT"}], "update_date": "2018-10-31", "authors_parsed": [["Huang", "Haiping", ""], ["Goudarzi", "Alireza", ""]]}, {"id": "1705.00861", "submitter": "Mingxuan Wang", "authors": "Mingxuan Wang, Zhengdong Lu, Jie Zhou, Qun Liu", "title": "Deep Neural Machine Translation with Linear Associative Unit", "comments": "10 pages, ACL 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep Neural Networks (DNNs) have provably enhanced the state-of-the-art\nNeural Machine Translation (NMT) with their capability in modeling complex\nfunctions and capturing complex linguistic structures. However NMT systems with\ndeep architecture in their encoder or decoder RNNs often suffer from severe\ngradient diffusion due to the non-linear recurrent activations, which often\nmake the optimization much more difficult. To address this problem we propose\nnovel linear associative units (LAU) to reduce the gradient propagation length\ninside the recurrent unit. Different from conventional approaches (LSTM unit\nand GRU), LAUs utilizes linear associative connections between input and output\nof the recurrent unit, which allows unimpeded information flow through both\nspace and time direction. The model is quite simple, but it is surprisingly\neffective. Our empirical study on Chinese-English translation shows that our\nmodel with proper configuration can improve by 11.7 BLEU upon Groundhog and the\nbest reported results in the same setting. On WMT14 English-German task and a\nlarger WMT14 English-French task, our model achieves comparable results with\nthe state-of-the-art.\n", "versions": [{"version": "v1", "created": "Tue, 2 May 2017 08:58:17 GMT"}], "update_date": "2017-05-03", "authors_parsed": [["Wang", "Mingxuan", ""], ["Lu", "Zhengdong", ""], ["Zhou", "Jie", ""], ["Liu", "Qun", ""]]}, {"id": "1705.00930", "submitter": "Tseng-Hung Chen", "authors": "Tseng-Hung Chen, Yuan-Hong Liao, Ching-Yao Chuang, Wan-Ting Hsu,\n  Jianlong Fu, Min Sun", "title": "Show, Adapt and Tell: Adversarial Training of Cross-domain Image\n  Captioner", "comments": "ICCV 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Impressive image captioning results are achieved in domains with plenty of\ntraining image and sentence pairs (e.g., MSCOCO). However, transferring to a\ntarget domain with significant domain shifts but no paired training data\n(referred to as cross-domain image captioning) remains largely unexplored. We\npropose a novel adversarial training procedure to leverage unpaired data in the\ntarget domain. Two critic networks are introduced to guide the captioner,\nnamely domain critic and multi-modal critic. The domain critic assesses whether\nthe generated sentences are indistinguishable from sentences in the target\ndomain. The multi-modal critic assesses whether an image and its generated\nsentence are a valid pair. During training, the critics and captioner act as\nadversaries -- captioner aims to generate indistinguishable sentences, whereas\ncritics aim at distinguishing them. The assessment improves the captioner\nthrough policy gradient updates. During inference, we further propose a novel\ncritic-based planning method to select high-quality sentences without\nadditional supervision (e.g., tags). To evaluate, we use MSCOCO as the source\ndomain and four other datasets (CUB-200-2011, Oxford-102, TGIF, and Flickr30k)\nas the target domains. Our method consistently performs well on all datasets.\nIn particular, on CUB-200-2011, we achieve 21.8% CIDEr-D improvement after\nadaptation. Utilizing critics during inference further gives another 4.5%\nboost.\n", "versions": [{"version": "v1", "created": "Tue, 2 May 2017 12:06:54 GMT"}, {"version": "v2", "created": "Mon, 14 Aug 2017 15:54:32 GMT"}], "update_date": "2017-08-15", "authors_parsed": [["Chen", "Tseng-Hung", ""], ["Liao", "Yuan-Hong", ""], ["Chuang", "Ching-Yao", ""], ["Hsu", "Wan-Ting", ""], ["Fu", "Jianlong", ""], ["Sun", "Min", ""]]}, {"id": "1705.01015", "submitter": "Jens Behrmann", "authors": "Jens Behrmann, Christian Etmann, Tobias Boskamp, Rita Casadonte,\n  J\\\"org Kriegsmann, Peter Maass", "title": "Deep Learning for Tumor Classification in Imaging Mass Spectrometry", "comments": "10 pages, 5 figures", "journal-ref": "Bioinformatics, 2018, Volume 34, Issue 7, Pages 1215-1223", "doi": "10.1093/bioinformatics/btx724", "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motivation: Tumor classification using Imaging Mass Spectrometry (IMS) data\nhas a high potential for future applications in pathology. Due to the\ncomplexity and size of the data, automated feature extraction and\nclassification steps are required to fully process the data. Deep learning\noffers an approach to learn feature extraction and classification combined in a\nsingle model. Commonly these steps are handled separately in IMS data analysis,\nhence deep learning offers an alternative strategy worthwhile to explore.\nResults: Methodologically, we propose an adapted architecture based on deep\nconvolutional networks to handle the characteristics of mass spectrometry data,\nas well as a strategy to interpret the learned model in the spectral domain\nbased on a sensitivity analysis. The proposed methods are evaluated on two\nchallenging tumor classification tasks and compared to a baseline approach.\nCompetitiveness of the proposed methods are shown on both tasks by studying the\nperformance via cross-validation. Moreover, the learned models are analyzed by\nthe proposed sensitivity analysis revealing biologically plausible effects as\nwell as confounding factors of the considered task. Thus, this study may serve\nas a starting point for further development of deep learning approaches in IMS\nclassification tasks.\n", "versions": [{"version": "v1", "created": "Tue, 2 May 2017 15:15:19 GMT"}, {"version": "v2", "created": "Mon, 15 May 2017 14:34:05 GMT"}, {"version": "v3", "created": "Fri, 20 Oct 2017 10:03:22 GMT"}], "update_date": "2018-06-28", "authors_parsed": [["Behrmann", "Jens", ""], ["Etmann", "Christian", ""], ["Boskamp", "Tobias", ""], ["Casadonte", "Rita", ""], ["Kriegsmann", "J\u00f6rg", ""], ["Maass", "Peter", ""]]}, {"id": "1705.01040", "submitter": "Chih-Hong Cheng", "authors": "Chih-Hong Cheng, Georg N\\\"uhrenberg, Harald Ruess", "title": "Maximum Resilience of Artificial Neural Networks", "comments": "Timestamp research work conducted in the project. version 2: fix some\n  typos, rephrase the definition, and add some more existing work", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.LO cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The deployment of Artificial Neural Networks (ANNs) in safety-critical\napplications poses a number of new verification and certification challenges.\nIn particular, for ANN-enabled self-driving vehicles it is important to\nestablish properties about the resilience of ANNs to noisy or even maliciously\nmanipulated sensory input. We are addressing these challenges by defining\nresilience properties of ANN-based classifiers as the maximal amount of input\nor sensor perturbation which is still tolerated. This problem of computing\nmaximal perturbation bounds for ANNs is then reduced to solving mixed integer\noptimization problems (MIP). A number of MIP encoding heuristics are developed\nfor drastically reducing MIP-solver runtimes, and using parallelization of\nMIP-solvers results in an almost linear speed-up in the number (up to a certain\nlimit) of computing cores in our experiments. We demonstrate the effectiveness\nand scalability of our approach by means of computing maximal resilience bounds\nfor a number of ANN benchmark sets ranging from typical image recognition\nscenarios to the autonomous maneuvering of robots.\n", "versions": [{"version": "v1", "created": "Fri, 28 Apr 2017 12:04:02 GMT"}, {"version": "v2", "created": "Wed, 5 Jul 2017 11:27:46 GMT"}], "update_date": "2017-07-06", "authors_parsed": [["Cheng", "Chih-Hong", ""], ["N\u00fchrenberg", "Georg", ""], ["Ruess", "Harald", ""]]}, {"id": "1705.01091", "submitter": "Dmitry Rokhlin B.", "authors": "Dmitry B. Rokhlin", "title": "PDE approach to the problem of online prediction with expert advice: a\n  construction of potential-based strategies", "comments": "7 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider a sequence of repeated prediction games and formally pass to the\nlimit. The supersolutions of the resulting non-linear parabolic partial\ndifferential equation are closely related to the potential functions in the\nsense of N.\\,Cesa-Bianci, G.\\,Lugosi (2003). Any such supersolution gives an\nupper bound for forecaster's regret and suggests a potential-based prediction\nstrategy, satisfying the Blackwell condition. A conventional upper bound for\nthe worst-case regret is justified by a simple verification argument.\n", "versions": [{"version": "v1", "created": "Tue, 2 May 2017 17:56:54 GMT"}], "update_date": "2017-05-03", "authors_parsed": [["Rokhlin", "Dmitry B.", ""]]}, {"id": "1705.01143", "submitter": "Shih-Chieh Su", "authors": "Shih-Chieh Su", "title": "Summarized Network Behavior Prediction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work studies the entity-wise topical behavior from massive network logs.\nBoth the temporal and the spatial relationships of the behavior are explored\nwith the learning architectures combing the recurrent neural network (RNN) and\nthe convolutional neural network (CNN). To make the behavioral data appropriate\nfor the spatial learning in CNN, several reduction steps are taken to form the\ntopical metrics and place them homogeneously like pixels in the images. The\nexperimental result shows both the temporal- and the spatial- gains when\ncompared to a multilayer perceptron (MLP) network. A new learning framework\ncalled spatially connected convolutional networks (SCCN) is introduced to more\nefficiently predict the behavior.\n", "versions": [{"version": "v1", "created": "Tue, 2 May 2017 19:12:23 GMT"}], "update_date": "2017-05-04", "authors_parsed": [["Su", "Shih-Chieh", ""]]}, {"id": "1705.01197", "submitter": "Akansel Cosgun", "authors": "David Isele, Akansel Cosgun, Kikuo Fujimura", "title": "Analyzing Knowledge Transfer in Deep Q-Networks for Autonomously\n  Handling Multiple Intersections", "comments": "Submitted to IEEE International Conference on Intelligent\n  Transportation Systems (ITSC 2017)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We analyze how the knowledge to autonomously handle one type of intersection,\nrepresented as a Deep Q-Network, translates to other types of intersections\n(tasks). We view intersection handling as a deep reinforcement learning\nproblem, which approximates the state action Q function as a deep neural\nnetwork. Using a traffic simulator, we show that directly copying a network\ntrained for one type of intersection to another type of intersection decreases\nthe success rate. We also show that when a network that is pre-trained on Task\nA and then is fine-tuned on a Task B, the resulting network not only performs\nbetter on the Task B than an network exclusively trained on Task A, but also\nretained knowledge on the Task A. Finally, we examine a lifelong learning\nsetting, where we train a single network on five different types of\nintersections sequentially and show that the resulting network exhibited\ncatastrophic forgetting of knowledge on previous tasks. This result suggests a\nneed for a long-term memory component to preserve knowledge.\n", "versions": [{"version": "v1", "created": "Tue, 2 May 2017 23:05:56 GMT"}], "update_date": "2017-05-04", "authors_parsed": [["Isele", "David", ""], ["Cosgun", "Akansel", ""], ["Fujimura", "Kikuo", ""]]}, {"id": "1705.01206", "submitter": "Feiping Nie", "authors": "Zan Gao, Guotai Zhang, Feiping Nie, Hua Zhang", "title": "Local Shrunk Discriminant Analysis (LSDA)", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Dimensionality reduction is a crucial step for pattern recognition and data\nmining tasks to overcome the curse of dimensionality. Principal component\nanalysis (PCA) is a traditional technique for unsupervised dimensionality\nreduction, which is often employed to seek a projection to best represent the\ndata in a least-squares sense, but if the original data is nonlinear structure,\nthe performance of PCA will quickly drop. An supervised dimensionality\nreduction algorithm called Linear discriminant analysis (LDA) seeks for an\nembedding transformation, which can work well with Gaussian distribution data\nor single-modal data, but for non-Gaussian distribution data or multimodal\ndata, it gives undesired results. What is worse, the dimension of LDA cannot be\nmore than the number of classes. In order to solve these issues, Local shrunk\ndiscriminant analysis (LSDA) is proposed in this work to process the\nnon-Gaussian distribution data or multimodal data, which not only incorporate\nboth the linear and nonlinear structures of original data, but also learn the\npattern shrinking to make the data more flexible to fit the manifold structure.\nFurther, LSDA has more strong generalization performance, whose objective\nfunction will become local LDA and traditional LDA when different extreme\nparameters are utilized respectively. What is more, a new efficient\noptimization algorithm is introduced to solve the non-convex objective function\nwith low computational cost. Compared with other related approaches, such as\nPCA, LDA and local LDA, the proposed method can derive a subspace which is more\nsuitable for non-Gaussian distribution and real data. Promising experimental\nresults on different kinds of data sets demonstrate the effectiveness of the\nproposed approach.\n", "versions": [{"version": "v1", "created": "Wed, 3 May 2017 00:20:31 GMT"}], "update_date": "2017-05-04", "authors_parsed": [["Gao", "Zan", ""], ["Zhang", "Guotai", ""], ["Nie", "Feiping", ""], ["Zhang", "Hua", ""]]}, {"id": "1705.01209", "submitter": "Gan Sun", "authors": "Gan Sun, Yang Cong, Ji Liu and Xiaowei Xu", "title": "Lifelong Metric Learning", "comments": "10 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The state-of-the-art online learning approaches are only capable of learning\nthe metric for predefined tasks. In this paper, we consider lifelong learning\nproblem to mimic \"human learning\", i.e., endowing a new capability to the\nlearned metric for a new task from new online samples and incorporating\nprevious experiences and knowledge. Therefore, we propose a new metric learning\nframework: lifelong metric learning (LML), which only utilizes the data of the\nnew task to train the metric model while preserving the original capabilities.\nMore specifically, the proposed LML maintains a common subspace for all learned\nmetrics, named lifelong dictionary, transfers knowledge from the common\nsubspace to each new metric task with task-specific idiosyncrasy, and redefines\nthe common subspace over time to maximize performance across all metric tasks.\nFor model optimization, we apply online passive aggressive optimization\nalgorithm to solve the proposed LML framework, where the lifelong dictionary\nand task-specific partition are optimized alternatively and consecutively.\nFinally, we evaluate our approach by analyzing several multi-task metric\nlearning datasets. Extensive experimental results demonstrate effectiveness and\nefficiency of the proposed framework.\n", "versions": [{"version": "v1", "created": "Wed, 3 May 2017 00:31:55 GMT"}, {"version": "v2", "created": "Mon, 12 Jun 2017 15:09:20 GMT"}], "update_date": "2017-06-13", "authors_parsed": [["Sun", "Gan", ""], ["Cong", "Yang", ""], ["Liu", "Ji", ""], ["Xu", "Xiaowei", ""]]}, {"id": "1705.01320", "submitter": "R\\\"udiger Ehlers", "authors": "Ruediger Ehlers", "title": "Formal Verification of Piece-Wise Linear Feed-Forward Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an approach for the verification of feed-forward neural networks\nin which all nodes have a piece-wise linear activation function. Such networks\nare often used in deep learning and have been shown to be hard to verify for\nmodern satisfiability modulo theory (SMT) and integer linear programming (ILP)\nsolvers.\n  The starting point of our approach is the addition of a global linear\napproximation of the overall network behavior to the verification problem that\nhelps with SMT-like reasoning over the network behavior. We present a\nspecialized verification algorithm that employs this approximation in a search\nprocess in which it infers additional node phases for the non-linear nodes in\nthe network from partial node phase assignments, similar to unit propagation in\nclassical SAT solving. We also show how to infer additional conflict clauses\nand safe node fixtures from the results of the analysis steps performed during\nthe search. The resulting approach is evaluated on collision avoidance and\nhandwritten digit recognition case studies.\n", "versions": [{"version": "v1", "created": "Wed, 3 May 2017 09:13:10 GMT"}, {"version": "v2", "created": "Thu, 18 May 2017 07:32:33 GMT"}, {"version": "v3", "created": "Wed, 2 Aug 2017 09:21:18 GMT"}], "update_date": "2017-08-03", "authors_parsed": [["Ehlers", "Ruediger", ""]]}, {"id": "1705.01346", "submitter": "Danhao Zhu", "authors": "Danhao Zhu, Si Shen, Xin-Yu Dai and Jiajun Chen", "title": "Going Wider: Recurrent Neural Network With Parallel Cells", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recurrent Neural Network (RNN) has been widely applied for sequence modeling.\nIn RNN, the hidden states at current step are full connected to those at\nprevious step, thus the influence from less related features at previous step\nmay potentially decrease model's learning ability. We propose a simple\ntechnique called parallel cells (PCs) to enhance the learning ability of\nRecurrent Neural Network (RNN). In each layer, we run multiple small RNN cells\nrather than one single large cell. In this paper, we evaluate PCs on 2 tasks.\nOn language modeling task on PTB (Penn Tree Bank), our model outperforms state\nof art models by decreasing perplexity from 78.6 to 75.3. On Chinese-English\ntranslation task, our model increases BLEU score for 0.39 points than baseline\nmodel.\n", "versions": [{"version": "v1", "created": "Wed, 3 May 2017 10:22:22 GMT"}], "update_date": "2017-05-04", "authors_parsed": [["Zhu", "Danhao", ""], ["Shen", "Si", ""], ["Dai", "Xin-Yu", ""], ["Chen", "Jiajun", ""]]}, {"id": "1705.01425", "submitter": "Mengyu Chu", "authors": "Mengyu Chu and Nils Thuerey", "title": "Data-Driven Synthesis of Smoke Flows with CNN-based Feature Descriptors", "comments": "14 pages, 17 figures, to appear at SIGGRAPH 2017, v2 only fixes small\n  typos", "journal-ref": "ACM Trans. Graph.36, 4 (2017), 69:1-69:13", "doi": "10.1145/3072959.3073643", "report-no": null, "categories": "cs.GR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a novel data-driven algorithm to synthesize high-resolution flow\nsimulations with reusable repositories of space-time flow data. In our work, we\nemploy a descriptor learning approach to encode the similarity between fluid\nregions with differences in resolution and numerical viscosity. We use\nconvolutional neural networks to generate the descriptors from fluid data such\nas smoke density and flow velocity. At the same time, we present a deformation\nlimiting patch advection method which allows us to robustly track deformable\nfluid regions. With the help of this patch advection, we generate stable\nspace-time data sets from detailed fluids for our repositories. We can then use\nour learned descriptors to quickly localize a suitable data set when running a\nnew simulation. This makes our approach very efficient, and resolution\nindependent. We will demonstrate with several examples that our method yields\nvolumes with very high effective resolutions, and non-dissipative small scale\ndetails that naturally integrate into the motions of the underlying flow.\n", "versions": [{"version": "v1", "created": "Wed, 3 May 2017 13:41:49 GMT"}, {"version": "v2", "created": "Tue, 25 Jul 2017 14:35:49 GMT"}], "update_date": "2017-07-26", "authors_parsed": [["Chu", "Mengyu", ""], ["Thuerey", "Nils", ""]]}, {"id": "1705.01462", "submitter": "Naveen Mellempudi", "authors": "Naveen Mellempudi, Abhisek Kundu, Dheevatsa Mudigere, Dipankar Das,\n  Bharat Kaul, Pradeep Dubey", "title": "Ternary Neural Networks with Fine-Grained Quantization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel fine-grained quantization (FGQ) method to ternarize\npre-trained full precision models, while also constraining activations to 8 and\n4-bits. Using this method, we demonstrate a minimal loss in classification\naccuracy on state-of-the-art topologies without additional training. We provide\nan improved theoretical formulation that forms the basis for a higher quality\nsolution using FGQ. Our method involves ternarizing the original weight tensor\nin groups of $N$ weights. Using $N=4$, we achieve Top-1 accuracy within $3.7\\%$\nand $4.2\\%$ of the baseline full precision result for Resnet-101 and Resnet-50\nrespectively, while eliminating $75\\%$ of all multiplications. These results\nenable a full 8/4-bit inference pipeline, with best-reported accuracy using\nternary weights on ImageNet dataset, with a potential of $9\\times$ improvement\nin performance. Also, for smaller networks like AlexNet, FGQ achieves\nstate-of-the-art results. We further study the impact of group size on both\nperformance and accuracy. With a group size of $N=64$, we eliminate\n$\\approx99\\%$ of the multiplications; however, this introduces a noticeable\ndrop in accuracy, which necessitates fine tuning the parameters at lower\nprecision. We address this by fine-tuning Resnet-50 with 8-bit activations and\nternary weights at $N=64$, improving the Top-1 accuracy to within $4\\%$ of the\nfull precision result with $<30\\%$ additional training overhead. Our final\nquantized model can run on a full 8-bit compute pipeline using 2-bit weights\nand has the potential of up to $15\\times$ improvement in performance compared\nto baseline full-precision models.\n", "versions": [{"version": "v1", "created": "Tue, 2 May 2017 10:15:21 GMT"}, {"version": "v2", "created": "Thu, 11 May 2017 09:19:55 GMT"}, {"version": "v3", "created": "Tue, 30 May 2017 17:10:24 GMT"}], "update_date": "2017-05-31", "authors_parsed": [["Mellempudi", "Naveen", ""], ["Kundu", "Abhisek", ""], ["Mudigere", "Dheevatsa", ""], ["Das", "Dipankar", ""], ["Kaul", "Bharat", ""], ["Dubey", "Pradeep", ""]]}, {"id": "1705.01485", "submitter": "Marco Todescato", "authors": "Marco Todescato, Andrea Carron, Ruggero Carli, Gianluigi Pillonetto,\n  Luca Schenato", "title": "Efficient Spatio-Temporal Gaussian Regression via Kalman Filtering", "comments": "26 pages, 12 figures. Submitted to IEEE Transactions on Pattern\n  Analysis and Machine Intelligence", "journal-ref": null, "doi": "10.1016/j.automatica.2020.109032", "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work we study the non-parametric reconstruction of spatio-temporal\ndynamical Gaussian processes (GPs) via GP regression from sparse and noisy\ndata. GPs have been mainly applied to spatial regression where they represent\none of the most powerful estimation approaches also thanks to their universal\nrepresenting properties. Their extension to dynamical processes has been\ninstead elusive so far since classical implementations lead to unscalable\nalgorithms. We then propose a novel procedure to address this problem by\ncoupling GP regression and Kalman filtering. In particular, assuming space/time\nseparability of the covariance (kernel) of the process and rational time\nspectrum, we build a finite-dimensional discrete-time state-space process\nrepresentation amenable of Kalman filtering. With sampling over a finite set of\nfixed spatial locations, our major finding is that the Kalman filter state at\ninstant $t_k$ represents a sufficient statistic to compute the minimum variance\nestimate of the process at any $t \\geq t_k$ over the entire spatial domain.\nThis result can be interpreted as a novel Kalman representer theorem for\ndynamical GPs. We then extend the study to situations where the set of spatial\ninput locations can vary over time. The proposed algorithms are finally tested\non both synthetic and real field data, also providing comparisons with standard\nGP and truncated GP regression techniques.\n", "versions": [{"version": "v1", "created": "Wed, 3 May 2017 15:49:38 GMT"}], "update_date": "2020-10-06", "authors_parsed": [["Todescato", "Marco", ""], ["Carron", "Andrea", ""], ["Carli", "Ruggero", ""], ["Pillonetto", "Gianluigi", ""], ["Schenato", "Luca", ""]]}, {"id": "1705.01502", "submitter": "Ran Rubin", "authors": "Ran Rubin, L.F. Abbott and Haim Sompolinsky", "title": "Balanced Excitation and Inhibition are Required for High-Capacity,\n  Noise-Robust Neuronal Selectivity", "comments": "Article and supplementary information", "journal-ref": "Proceedings of the National Academy of Sciences of the United\n  States of America, 114(41), 2017", "doi": "10.1073/pnas.1705841114", "report-no": null, "categories": "q-bio.NC cond-mat.dis-nn cs.LG cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neurons and networks in the cerebral cortex must operate reliably despite\nmultiple sources of noise. To evaluate the impact of both input and output\nnoise, we determine the robustness of single-neuron stimulus selective\nresponses, as well as the robustness of attractor states of networks of neurons\nperforming memory tasks. We find that robustness to output noise requires\nsynaptic connections to be in a balanced regime in which excitation and\ninhibition are strong and largely cancel each other. We evaluate the conditions\nrequired for this regime to exist and determine the properties of networks\noperating within it. A plausible synaptic plasticity rule for learning that\nbalances weight configurations is presented. Our theory predicts an optimal\nratio of the number of excitatory and inhibitory synapses for maximizing the\nencoding capacity of balanced networks for a given statistics of afferent\nactivations. Previous work has shown that balanced networks amplify\nspatio-temporal variability and account for observed asynchronous irregular\nstates. Here we present a novel type of balanced network that amplifies small\nchanges in the impinging signals, and emerges automatically from learning to\nperform neuronal and network functions robustly.\n", "versions": [{"version": "v1", "created": "Wed, 3 May 2017 16:38:01 GMT"}], "update_date": "2018-01-24", "authors_parsed": [["Rubin", "Ran", ""], ["Abbott", "L. F.", ""], ["Sompolinsky", "Haim", ""]]}, {"id": "1705.01507", "submitter": "Joerg Evermann", "authors": "Joerg Evermann and Jana-Rebecca Rehse and Peter Fettke", "title": "XES Tensorflow - Process Prediction using the Tensorflow Deep-Learning\n  Framework", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Predicting the next activity of a running process is an important aspect of\nprocess management. Recently, artificial neural networks, so called\ndeep-learning approaches, have been proposed to address this challenge. This\ndemo paper describes a software application that applies the Tensorflow\ndeep-learning framework to process prediction. The software application reads\nindustry-standard XES files for training and presents the user with an\neasy-to-use graphical user interface for both training and prediction. The\nsystem provides several improvements over earlier work. This demo paper focuses\non the software implementation and describes the architecture and user\ninterface.\n", "versions": [{"version": "v1", "created": "Wed, 3 May 2017 16:48:51 GMT"}], "update_date": "2017-05-04", "authors_parsed": [["Evermann", "Joerg", ""], ["Rehse", "Jana-Rebecca", ""], ["Fettke", "Peter", ""]]}, {"id": "1705.01601", "submitter": "Marek Smieja", "authors": "Marek \\'Smieja and Bernhard C. Geiger", "title": "Semi-supervised cross-entropy clustering with information bottleneck\n  constraint", "comments": null, "journal-ref": "Information Sciences, vol. 421, Dec. 2017, pp. 254-271", "doi": "10.1016/j.ins.2017.07.016", "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a semi-supervised clustering method, CEC-IB, that\nmodels data with a set of Gaussian distributions and that retrieves clusters\nbased on a partial labeling provided by the user (partition-level side\ninformation). By combining the ideas from cross-entropy clustering (CEC) with\nthose from the information bottleneck method (IB), our method trades between\nthree conflicting goals: the accuracy with which the data set is modeled, the\nsimplicity of the model, and the consistency of the clustering with side\ninformation. Experiments demonstrate that CEC-IB has a performance comparable\nto Gaussian mixture models (GMM) in a classical semi-supervised scenario, but\nis faster, more robust to noisy labels, automatically determines the optimal\nnumber of clusters, and performs well when not all classes are present in the\nside information. Moreover, in contrast to other semi-supervised models, it can\nbe successfully applied in discovering natural subgroups if the partition-level\nside information is derived from the top levels of a hierarchical clustering.\n", "versions": [{"version": "v1", "created": "Wed, 3 May 2017 20:09:43 GMT"}], "update_date": "2017-11-15", "authors_parsed": [["\u015amieja", "Marek", ""], ["Geiger", "Bernhard C.", ""]]}, {"id": "1705.01626", "submitter": "Minsoo Rhu", "authors": "Minsoo Rhu, Mike O'Connor, Niladrish Chatterjee, Jeff Pool, Stephen W.\n  Keckler", "title": "Compressing DMA Engine: Leveraging Activation Sparsity for Training Deep\n  Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Popular deep learning frameworks require users to fine-tune their memory\nusage so that the training data of a deep neural network (DNN) fits within the\nGPU physical memory. Prior work tries to address this restriction by\nvirtualizing the memory usage of DNNs, enabling both CPU and GPU memory to be\nutilized for memory allocations. Despite its merits, virtualizing memory can\nincur significant performance overheads when the time needed to copy data back\nand forth from CPU memory is higher than the latency to perform the\ncomputations required for DNN forward and backward propagation. We introduce a\nhigh-performance virtualization strategy based on a \"compressing DMA engine\"\n(cDMA) that drastically reduces the size of the data structures that are\ntargeted for CPU-side allocations. The cDMA engine offers an average 2.6x\n(maximum 13.8x) compression ratio by exploiting the sparsity inherent in\noffloaded data, improving the performance of virtualized DNNs by an average 32%\n(maximum 61%).\n", "versions": [{"version": "v1", "created": "Wed, 3 May 2017 21:07:47 GMT"}], "update_date": "2017-05-05", "authors_parsed": [["Rhu", "Minsoo", ""], ["O'Connor", "Mike", ""], ["Chatterjee", "Niladrish", ""], ["Pool", "Jeff", ""], ["Keckler", "Stephen W.", ""]]}, {"id": "1705.01707", "submitter": "Jan Svoboda", "authors": "Jan Svoboda, Federico Monti, Michael M. Bronstein", "title": "Generative Convolutional Networks for Latent Fingerprint Reconstruction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Performance of fingerprint recognition depends heavily on the extraction of\nminutiae points. Enhancement of the fingerprint ridge pattern is thus an\nessential pre-processing step that noticeably reduces false positive and\nnegative detection rates. A particularly challenging setting is when the\nfingerprint images are corrupted or partially missing. In this work, we apply\ngenerative convolutional networks to denoise visible minutiae and predict the\nmissing parts of the ridge pattern. The proposed enhancement approach is tested\nas a pre-processing step in combination with several standard feature\nextraction methods such as MINDTCT, followed by biometric comparison using MCC\nand BOZORTH3. We evaluate our method on several publicly available latent\nfingerprint datasets captured using different sensors.\n", "versions": [{"version": "v1", "created": "Thu, 4 May 2017 05:29:23 GMT"}], "update_date": "2017-05-05", "authors_parsed": [["Svoboda", "Jan", ""], ["Monti", "Federico", ""], ["Bronstein", "Michael M.", ""]]}, {"id": "1705.01708", "submitter": "Tomoya Sakai", "authors": "Tomoya Sakai, Gang Niu, Masashi Sugiyama", "title": "Semi-Supervised AUC Optimization based on Positive-Unlabeled Learning", "comments": "To appear in Machine Learning", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Maximizing the area under the receiver operating characteristic curve (AUC)\nis a standard approach to imbalanced classification. So far, various supervised\nAUC optimization methods have been developed and they are also extended to\nsemi-supervised scenarios to cope with small sample problems. However, existing\nsemi-supervised AUC optimization methods rely on strong distributional\nassumptions, which are rarely satisfied in real-world problems. In this paper,\nwe propose a novel semi-supervised AUC optimization method that does not\nrequire such restrictive assumptions. We first develop an AUC optimization\nmethod based only on positive and unlabeled data (PU-AUC) and then extend it to\nsemi-supervised learning by combining it with a supervised AUC optimization\nmethod. We theoretically prove that, without the restrictive distributional\nassumptions, unlabeled data contribute to improving the generalization\nperformance in PU and semi-supervised AUC optimization methods. Finally, we\ndemonstrate the practical usefulness of the proposed methods through\nexperiments.\n", "versions": [{"version": "v1", "created": "Thu, 4 May 2017 05:46:32 GMT"}, {"version": "v2", "created": "Mon, 16 Oct 2017 15:25:32 GMT"}], "update_date": "2017-10-17", "authors_parsed": [["Sakai", "Tomoya", ""], ["Niu", "Gang", ""], ["Sugiyama", "Masashi", ""]]}, {"id": "1705.01714", "submitter": "Philipp Petersen", "authors": "Helmut B\\\"olcskei, Philipp Grohs, Gitta Kutyniok, Philipp Petersen", "title": "Optimal Approximation with Sparsely Connected Deep Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.IT math.FA math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We derive fundamental lower bounds on the connectivity and the memory\nrequirements of deep neural networks guaranteeing uniform approximation rates\nfor arbitrary function classes in $L^2(\\mathbb R^d)$. In other words, we\nestablish a connection between the complexity of a function class and the\ncomplexity of deep neural networks approximating functions from this class to\nwithin a prescribed accuracy. Additionally, we prove that our lower bounds are\nachievable for a broad family of function classes. Specifically, all function\nclasses that are optimally approximated by a general class of representation\nsystems---so-called \\emph{affine systems}---can be approximated by deep neural\nnetworks with minimal connectivity and memory requirements. Affine systems\nencompass a wealth of representation systems from applied harmonic analysis\nsuch as wavelets, ridgelets, curvelets, shearlets, $\\alpha$-shearlets, and more\ngenerally $\\alpha$-molecules. Our central result elucidates a remarkable\nuniversality property of neural networks and shows that they achieve the\noptimum approximation properties of all affine systems combined. As a specific\nexample, we consider the class of $\\alpha^{-1}$-cartoon-like functions, which\nis approximated optimally by $\\alpha$-shearlets. We also explain how our\nresults can be extended to the case of functions on low-dimensional immersed\nmanifolds. Finally, we present numerical experiments demonstrating that the\nstandard stochastic gradient descent algorithm generates deep neural networks\nproviding close-to-optimal approximation rates. Moreover, these results\nindicate that stochastic gradient descent can actually learn approximations\nthat are sparse in the representation systems optimally sparsifying the\nfunction class the network is trained on.\n", "versions": [{"version": "v1", "created": "Thu, 4 May 2017 06:45:06 GMT"}, {"version": "v2", "created": "Thu, 15 Feb 2018 08:25:48 GMT"}, {"version": "v3", "created": "Wed, 4 Apr 2018 09:56:10 GMT"}, {"version": "v4", "created": "Wed, 16 May 2018 07:44:38 GMT"}], "update_date": "2018-05-17", "authors_parsed": [["B\u00f6lcskei", "Helmut", ""], ["Grohs", "Philipp", ""], ["Kutyniok", "Gitta", ""], ["Petersen", "Philipp", ""]]}, {"id": "1705.01720", "submitter": "Shay Moran", "authors": "Daniel M. Kane and Shachar Lovett and Shay Moran", "title": "Near-optimal linear decision trees for k-SUM and related problems", "comments": "18 paged, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CG cs.CC cs.DM cs.LG math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We construct near optimal linear decision trees for a variety of decision\nproblems in combinatorics and discrete geometry. For example, for any constant\n$k$, we construct linear decision trees that solve the $k$-SUM problem on $n$\nelements using $O(n \\log^2 n)$ linear queries. Moreover, the queries we use are\ncomparison queries, which compare the sums of two $k$-subsets; when viewed as\nlinear queries, comparison queries are $2k$-sparse and have only $\\{-1,0,1\\}$\ncoefficients. We give similar constructions for sorting sumsets $A+B$ and for\nsolving the SUBSET-SUM problem, both with optimal number of queries, up to\npoly-logarithmic terms.\n  Our constructions are based on the notion of \"inference dimension\", recently\nintroduced by the authors in the context of active classification with\ncomparison queries. This can be viewed as another contribution to the fruitful\nlink between machine learning and discrete geometry, which goes back to the\ndiscovery of the VC dimension.\n", "versions": [{"version": "v1", "created": "Thu, 4 May 2017 07:11:47 GMT"}], "update_date": "2017-05-05", "authors_parsed": [["Kane", "Daniel M.", ""], ["Lovett", "Shachar", ""], ["Moran", "Shay", ""]]}, {"id": "1705.01813", "submitter": "Cheng-Hao Deng", "authors": "Cheng-Hao Deng and Wan-Lei Zhao", "title": "Fast k-means based on KNN Graph", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the era of big data, k-means clustering has been widely adopted as a basic\nprocessing tool in various contexts. However, its computational cost could be\nprohibitively high as the data size and the cluster number are large. It is\nwell known that the processing bottleneck of k-means lies in the operation of\nseeking closest centroid in each iteration. In this paper, a novel solution\ntowards the scalability issue of k-means is presented. In the proposal, k-means\nis supported by an approximate k-nearest neighbors graph. In the k-means\niteration, each data sample is only compared to clusters that its nearest\nneighbors reside. Since the number of nearest neighbors we consider is much\nless than k, the processing cost in this step becomes minor and irrelevant to\nk. The processing bottleneck is therefore overcome. The most interesting thing\nis that k-nearest neighbor graph is constructed by iteratively calling the fast\n$k$-means itself. Comparing with existing fast k-means variants, the proposed\nalgorithm achieves hundreds to thousands times speed-up while maintaining high\nclustering quality. As it is tested on 10 million 512-dimensional data, it\ntakes only 5.2 hours to produce 1 million clusters. In contrast, to fulfill the\nsame scale of clustering, it would take 3 years for traditional k-means.\n", "versions": [{"version": "v1", "created": "Thu, 4 May 2017 12:27:28 GMT"}], "update_date": "2017-05-05", "authors_parsed": [["Deng", "Cheng-Hao", ""], ["Zhao", "Wan-Lei", ""]]}, {"id": "1705.01877", "submitter": "Marek Smieja", "authors": "Marek \\'Smieja and {\\L}ukasz Struski and Jacek Tabor", "title": "Semi-supervised model-based clustering with controlled clusters leakage", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we focus on finding clusters in partially categorized data\nsets. We propose a semi-supervised version of Gaussian mixture model, called\nC3L, which retrieves natural subgroups of given categories. In contrast to\nother semi-supervised models, C3L is parametrized by user-defined leakage\nlevel, which controls maximal inconsistency between initial categorization and\nresulting clustering. Our method can be implemented as a module in practical\nexpert systems to detect clusters, which combine expert knowledge with true\ndistribution of data. Moreover, it can be used for improving the results of\nless flexible clustering techniques, such as projection pursuit clustering. The\npaper presents extensive theoretical analysis of the model and fast algorithm\nfor its efficient optimization. Experimental results show that C3L finds high\nquality clustering model, which can be applied in discovering meaningful groups\nin partially classified data.\n", "versions": [{"version": "v1", "created": "Thu, 4 May 2017 15:13:28 GMT"}], "update_date": "2017-05-05", "authors_parsed": [["\u015amieja", "Marek", ""], ["Struski", "\u0141ukasz", ""], ["Tabor", "Jacek", ""]]}, {"id": "1705.01936", "submitter": "Curtis Northcutt", "authors": "Curtis G. Northcutt, Tailin Wu, Isaac L. Chuang", "title": "Learning with Confident Examples: Rank Pruning for Robust Classification\n  with Noisy Labels", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Noisy PN learning is the problem of binary classification when training\nexamples may be mislabeled (flipped) uniformly with noise rate rho1 for\npositive examples and rho0 for negative examples. We propose Rank Pruning (RP)\nto solve noisy PN learning and the open problem of estimating the noise rates,\ni.e. the fraction of wrong positive and negative labels. Unlike prior\nsolutions, RP is time-efficient and general, requiring O(T) for any\nunrestricted choice of probabilistic classifier with T fitting time. We prove\nRP has consistent noise estimation and equivalent expected risk as learning\nwith uncorrupted labels in ideal conditions, and derive closed-form solutions\nwhen conditions are non-ideal. RP achieves state-of-the-art noise estimation\nand F1, error, and AUC-PR for both MNIST and CIFAR datasets, regardless of the\namount of noise and performs similarly impressively when a large portion of\ntraining examples are noise drawn from a third distribution. To highlight, RP\nwith a CNN classifier can predict if an MNIST digit is a \"one\"or \"not\" with\nonly 0.25% error, and 0.46 error across all digits, even when 50% of positive\nexamples are mislabeled and 50% of observed positive labels are mislabeled\nnegative examples.\n", "versions": [{"version": "v1", "created": "Thu, 4 May 2017 17:59:30 GMT"}, {"version": "v2", "created": "Thu, 11 May 2017 16:07:54 GMT"}, {"version": "v3", "created": "Wed, 9 Aug 2017 23:21:44 GMT"}], "update_date": "2017-08-11", "authors_parsed": [["Northcutt", "Curtis G.", ""], ["Wu", "Tailin", ""], ["Chuang", "Isaac L.", ""]]}, {"id": "1705.02009", "submitter": "Hien To", "authors": "Hien To, Sumeet Agrawal, Seon Ho Kim, Cyrus Shahabi", "title": "On Identifying Disaster-Related Tweets: Matching-based or\n  Learning-based?", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Social media such as tweets are emerging as platforms contributing to\nsituational awareness during disasters. Information shared on Twitter by both\naffected population (e.g., requesting assistance, warning) and those outside\nthe impact zone (e.g., providing assistance) would help first responders,\ndecision makers, and the public to understand the situation first-hand.\nEffective use of such information requires timely selection and analysis of\ntweets that are relevant to a particular disaster. Even though abundant tweets\nare promising as a data source, it is challenging to automatically identify\nrelevant messages since tweet are short and unstructured, resulting to\nunsatisfactory classification performance of conventional learning-based\napproaches. Thus, we propose a simple yet effective algorithm to identify\nrelevant messages based on matching keywords and hashtags, and provide a\ncomparison between matching-based and learning-based approaches. To evaluate\nthe two approaches, we put them into a framework specifically proposed for\nanalyzing disaster-related tweets. Analysis results on eleven datasets with\nvarious disaster types show that our technique provides relevant tweets of\nhigher quality and more interpretable results of sentiment analysis tasks when\ncompared to learning approach.\n", "versions": [{"version": "v1", "created": "Thu, 4 May 2017 20:42:23 GMT"}], "update_date": "2017-05-08", "authors_parsed": [["To", "Hien", ""], ["Agrawal", "Sumeet", ""], ["Kim", "Seon Ho", ""], ["Shahabi", "Cyrus", ""]]}, {"id": "1705.02033", "submitter": "Yu Chen", "authors": "Yu Chen and Mohammed J. Zaki", "title": "KATE: K-Competitive Autoencoder for Text", "comments": "10 pages, KDD'17", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Autoencoders have been successful in learning meaningful representations from\nimage datasets. However, their performance on text datasets has not been widely\nstudied. Traditional autoencoders tend to learn possibly trivial\nrepresentations of text documents due to their confounding properties such as\nhigh-dimensionality, sparsity and power-law word distributions. In this paper,\nwe propose a novel k-competitive autoencoder, called KATE, for text documents.\nDue to the competition between the neurons in the hidden layer, each neuron\nbecomes specialized in recognizing specific data patterns, and overall the\nmodel can learn meaningful representations of textual data. A comprehensive set\nof experiments show that KATE can learn better representations than traditional\nautoencoders including denoising, contractive, variational, and k-sparse\nautoencoders. Our model also outperforms deep generative models, probabilistic\ntopic models, and even word representation models (e.g., Word2Vec) in terms of\nseveral downstream tasks such as document classification, regression, and\nretrieval.\n", "versions": [{"version": "v1", "created": "Thu, 4 May 2017 22:04:17 GMT"}, {"version": "v2", "created": "Sun, 4 Jun 2017 16:50:03 GMT"}], "update_date": "2017-06-06", "authors_parsed": [["Chen", "Yu", ""], ["Zaki", "Mohammed J.", ""]]}, {"id": "1705.02047", "submitter": "Vatsal Shah", "authors": "Vatsal Shah, Nikhil Rao, Weicong Ding", "title": "Matrix Completion via Factorizing Polynomials", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Predicting unobserved entries of a partially observed matrix has found wide\napplicability in several areas, such as recommender systems, computational\nbiology, and computer vision. Many scalable methods with rigorous theoretical\nguarantees have been developed for algorithms where the matrix is factored into\nlow-rank components, and embeddings are learned for the row and column\nentities. While there has been recent research on incorporating explicit side\ninformation in the low-rank matrix factorization setting, often implicit\ninformation can be gleaned from the data, via higher-order interactions among\nentities. Such implicit information is especially useful in cases where the\ndata is very sparse, as is often the case in real-world datasets. In this\npaper, we design a method to learn embeddings in the context of recommendation\nsystems, using the observation that higher powers of a graph transition\nprobability matrix encode the probability that a random walker will hit that\nnode in a given number of steps. We develop a coordinate descent algorithm to\nsolve the resulting optimization, that makes explicit computation of the higher\norder powers of the matrix redundant, preserving sparsity and making\ncomputations efficient. Experiments on several datasets show that our method,\nthat can use higher order information, outperforms methods that only use\nexplicitly available side information, those that use only second-order\nimplicit information and in some cases, methods based on deep neural networks\nas well.\n", "versions": [{"version": "v1", "created": "Thu, 4 May 2017 23:47:12 GMT"}, {"version": "v2", "created": "Tue, 19 Sep 2017 20:28:36 GMT"}, {"version": "v3", "created": "Wed, 14 Feb 2018 00:38:17 GMT"}], "update_date": "2018-02-15", "authors_parsed": [["Shah", "Vatsal", ""], ["Rao", "Nikhil", ""], ["Ding", "Weicong", ""]]}, {"id": "1705.02210", "submitter": "Cheng-Hao Cai", "authors": "Cheng-Hao Cai", "title": "SLDR-DL: A Framework for SLD-Resolution with Deep Learning", "comments": "12 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces an SLD-resolution technique based on deep learning.\nThis technique enables neural networks to learn from old and successful\nresolution processes and to use learnt experiences to guide new resolution\nprocesses. An implementation of this technique is named SLDR-DL. It includes a\nProlog library of deep feedforward neural networks and some essential functions\nof resolution. In the SLDR-DL framework, users can define logical rules in the\nform of definite clauses and teach neural networks to use the rules in\nreasoning processes.\n", "versions": [{"version": "v1", "created": "Fri, 5 May 2017 13:32:54 GMT"}], "update_date": "2017-05-08", "authors_parsed": [["Cai", "Cheng-Hao", ""]]}, {"id": "1705.02212", "submitter": "Michel Besserve", "authors": "Michel Besserve, Naji Shajarisales, Bernhard Sch\\\"olkopf and Dominik\n  Janzing", "title": "Group invariance principles for causal generative models", "comments": "16 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.LG math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The postulate of independence of cause and mechanism (ICM) has recently led\nto several new causal discovery algorithms. The interpretation of independence\nand the way it is utilized, however, varies across these methods. Our aim in\nthis paper is to propose a group theoretic framework for ICM to unify and\ngeneralize these approaches. In our setting, the cause-mechanism relationship\nis assessed by comparing it against a null hypothesis through the application\nof random generic group transformations. We show that the group theoretic view\nprovides a very general tool to study the structure of data generating\nmechanisms with direct applications to machine learning.\n", "versions": [{"version": "v1", "created": "Fri, 5 May 2017 13:34:16 GMT"}], "update_date": "2017-05-08", "authors_parsed": [["Besserve", "Michel", ""], ["Shajarisales", "Naji", ""], ["Sch\u00f6lkopf", "Bernhard", ""], ["Janzing", "Dominik", ""]]}, {"id": "1705.02224", "submitter": "Lovedeep Gondara", "authors": "Lovedeep Gondara", "title": "Detecting Adversarial Samples Using Density Ratio Estimates", "comments": "Updated", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Machine learning models, especially based on deep architectures are used in\neveryday applications ranging from self driving cars to medical diagnostics. It\nhas been shown that such models are dangerously susceptible to adversarial\nsamples, indistinguishable from real samples to human eye, adversarial samples\nlead to incorrect classifications with high confidence. Impact of adversarial\nsamples is far-reaching and their efficient detection remains an open problem.\nWe propose to use direct density ratio estimation as an efficient model\nagnostic measure to detect adversarial samples. Our proposed method works\nequally well with single and multi-channel samples, and with different\nadversarial sample generation methods. We also propose a method to use density\nratio estimates for generating adversarial samples with an added constraint of\npreserving density ratio.\n", "versions": [{"version": "v1", "created": "Fri, 5 May 2017 15:28:59 GMT"}, {"version": "v2", "created": "Wed, 17 May 2017 15:23:57 GMT"}, {"version": "v3", "created": "Fri, 19 May 2017 21:22:00 GMT"}, {"version": "v4", "created": "Mon, 20 Nov 2017 16:17:18 GMT"}], "update_date": "2017-11-21", "authors_parsed": [["Gondara", "Lovedeep", ""]]}, {"id": "1705.02232", "submitter": "Marek Smieja", "authors": "Marek \\'Smieja and Jacek Tabor", "title": "Spherical Wards clustering and generalized Voronoi diagrams", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Gaussian mixture model is very useful in many practical problems.\nNevertheless, it cannot be directly generalized to non Euclidean spaces. To\novercome this problem we present a spherical Gaussian-based clustering approach\nfor partitioning data sets with respect to arbitrary dissimilarity measure. The\nproposed method is a combination of spherical Cross-Entropy Clustering with a\ngeneralized Wards approach. The algorithm finds the optimal number of clusters\nby automatically removing groups which carry no information. Moreover, it is\nscale invariant and allows for forming of spherically-shaped clusters of\narbitrary sizes. In order to graphically represent and interpret the results\nthe notion of Voronoi diagram was generalized to non Euclidean spaces and\napplied for introduced clustering method.\n", "versions": [{"version": "v1", "created": "Thu, 4 May 2017 16:27:28 GMT"}], "update_date": "2017-05-08", "authors_parsed": [["\u015amieja", "Marek", ""], ["Tabor", "Jacek", ""]]}, {"id": "1705.02245", "submitter": "Neil Lawrence", "authors": "Neil D. Lawrence", "title": "Data Readiness Levels", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.AI cs.CY cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Application of models to data is fraught. Data-generating collaborators often\nonly have a very basic understanding of the complications of collating,\nprocessing and curating data. Challenges include: poor data collection\npractices, missing values, inconvenient storage mechanisms, intellectual\nproperty, security and privacy. All these aspects obstruct the sharing and\ninterconnection of data, and the eventual interpretation of data through\nmachine learning or other approaches. In project reporting, a major challenge\nis in encapsulating these problems and enabling goals to be built around the\nprocessing of data. Project overruns can occur due to failure to account for\nthe amount of time required to curate and collate. But to understand these\nfailures we need to have a common language for assessing the readiness of a\nparticular data set. This position paper proposes the use of data readiness\nlevels: it gives a rough outline of three stages of data preparedness and\nspeculates on how formalisation of these levels into a common language for data\nreadiness could facilitate project management.\n", "versions": [{"version": "v1", "created": "Fri, 5 May 2017 14:53:56 GMT"}], "update_date": "2017-05-08", "authors_parsed": [["Lawrence", "Neil D.", ""]]}, {"id": "1705.02269", "submitter": "Sebastian Brarda", "authors": "Sebastian Brarda, Philip Yeres, Samuel R. Bowman", "title": "Sequential Attention: A Context-Aware Alignment Function for Machine\n  Reading", "comments": "To appear in ACL 2017 2nd Workshop on Representation Learning for\n  NLP. Contains additional experiments in section 4 and a revised Figure 1", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we propose a neural network model with a novel Sequential\nAttention layer that extends soft attention by assigning weights to words in an\ninput sequence in a way that takes into account not just how well that word\nmatches a query, but how well surrounding words match. We evaluate this\napproach on the task of reading comprehension (on the Who did What and CNN\ndatasets) and show that it dramatically improves a strong baseline--the\nStanford Reader--and is competitive with the state of the art.\n", "versions": [{"version": "v1", "created": "Fri, 5 May 2017 15:37:11 GMT"}, {"version": "v2", "created": "Mon, 26 Jun 2017 22:25:55 GMT"}], "update_date": "2017-06-28", "authors_parsed": [["Brarda", "Sebastian", ""], ["Yeres", "Philip", ""], ["Bowman", "Samuel R.", ""]]}, {"id": "1705.02302", "submitter": "Nadav Cohen", "authors": "Nadav Cohen, Or Sharir, Yoav Levine, Ronen Tamari, David Yakira, Amnon\n  Shashua", "title": "Analysis and Design of Convolutional Networks via Hierarchical Tensor\n  Decompositions", "comments": "Part of the Intel Collaborative Research Institute for Computational\n  Intelligence (ICRI-CI) Special Issue on Deep Learning Theory", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The driving force behind convolutional networks - the most successful deep\nlearning architecture to date, is their expressive power. Despite its wide\nacceptance and vast empirical evidence, formal analyses supporting this belief\nare scarce. The primary notions for formally reasoning about expressiveness are\nefficiency and inductive bias. Expressive efficiency refers to the ability of a\nnetwork architecture to realize functions that require an alternative\narchitecture to be much larger. Inductive bias refers to the prioritization of\nsome functions over others given prior knowledge regarding a task at hand. In\nthis paper we overview a series of works written by the authors, that through\nan equivalence to hierarchical tensor decompositions, analyze the expressive\nefficiency and inductive bias of various convolutional network architectural\nfeatures (depth, width, strides and more). The results presented shed light on\nthe demonstrated effectiveness of convolutional networks, and in addition,\nprovide new tools for network design.\n", "versions": [{"version": "v1", "created": "Fri, 5 May 2017 17:09:58 GMT"}, {"version": "v2", "created": "Mon, 8 May 2017 16:54:48 GMT"}, {"version": "v3", "created": "Tue, 9 May 2017 06:15:41 GMT"}, {"version": "v4", "created": "Sat, 10 Jun 2017 19:07:18 GMT"}, {"version": "v5", "created": "Mon, 11 Jun 2018 06:38:42 GMT"}], "update_date": "2018-06-12", "authors_parsed": [["Cohen", "Nadav", ""], ["Sharir", "Or", ""], ["Levine", "Yoav", ""], ["Tamari", "Ronen", ""], ["Yakira", "David", ""], ["Shashua", "Amnon", ""]]}, {"id": "1705.02307", "submitter": "Francesco Grassi", "authors": "Francesco Grassi, Andreas Loukas, Nathana\\\"el Perraudin, Benjamin\n  Ricaud", "title": "A Time-Vertex Signal Processing Framework", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An emerging way to deal with high-dimensional non-euclidean data is to assume\nthat the underlying structure can be captured by a graph. Recently, ideas have\nbegun to emerge related to the analysis of time-varying graph signals. This\nwork aims to elevate the notion of joint harmonic analysis to a full-fledged\nframework denoted as Time-Vertex Signal Processing, that links together the\ntime-domain signal processing techniques with the new tools of graph signal\nprocessing. This entails three main contributions: (a) We provide a formal\nmotivation for harmonic time-vertex analysis as an analysis tool for the state\nevolution of simple Partial Differential Equations on graphs. (b) We improve\nthe accuracy of joint filtering operators by up-to two orders of magnitude. (c)\nUsing our joint filters, we construct time-vertex dictionaries analyzing the\ndifferent scales and the local time-frequency content of a signal. The utility\nof our tools is illustrated in numerous applications and datasets, such as\ndynamic mesh denoising and classification, still-video inpainting, and source\nlocalization in seismic events. Our results suggest that joint analysis of\ntime-vertex signals can bring benefits to regression and learning.\n", "versions": [{"version": "v1", "created": "Fri, 5 May 2017 17:20:32 GMT"}], "update_date": "2017-05-08", "authors_parsed": [["Grassi", "Francesco", ""], ["Loukas", "Andreas", ""], ["Perraudin", "Nathana\u00ebl", ""], ["Ricaud", "Benjamin", ""]]}, {"id": "1705.02394", "submitter": "Stefan Scherer", "authors": "Jonathan Chang, Stefan Scherer", "title": "Learning Representations of Emotional Speech with Deep Convolutional\n  Generative Adversarial Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automatically assessing emotional valence in human speech has historically\nbeen a difficult task for machine learning algorithms. The subtle changes in\nthe voice of the speaker that are indicative of positive or negative emotional\nstates are often \"overshadowed\" by voice characteristics relating to emotional\nintensity or emotional activation. In this work we explore a representation\nlearning approach that automatically derives discriminative representations of\nemotional speech. In particular, we investigate two machine learning strategies\nto improve classifier performance: (1) utilization of unlabeled data using a\ndeep convolutional generative adversarial network (DCGAN), and (2) multitask\nlearning. Within our extensive experiments we leverage a multitask annotated\nemotional corpus as well as a large unlabeled meeting corpus (around 100\nhours). Our speaker-independent classification experiments show that in\nparticular the use of unlabeled data in our investigations improves performance\nof the classifiers and both fully supervised baseline approaches are\noutperformed considerably. We improve the classification of emotional valence\non a discrete 5-point scale to 43.88% and on a 3-point scale to 49.80%, which\nis competitive to state-of-the-art performance.\n", "versions": [{"version": "v1", "created": "Sat, 22 Apr 2017 18:28:25 GMT"}], "update_date": "2017-05-09", "authors_parsed": [["Chang", "Jonathan", ""], ["Scherer", "Stefan", ""]]}, {"id": "1705.02395", "submitter": "Markus Borg", "authors": "Markus Borg, Iben Lennerstad, Rasmus Ros, Elizabeth Bjarnason", "title": "On Using Active Learning and Self-Training when Mining Performance\n  Discussions on Stack Overflow", "comments": "Preprint of paper accepted for the Proc. of the 21st International\n  Conference on Evaluation and Assessment in Software Engineering, 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.HC cs.LG cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Abundant data is the key to successful machine learning. However, supervised\nlearning requires annotated data that are often hard to obtain. In a\nclassification task with limited resources, Active Learning (AL) promises to\nguide annotators to examples that bring the most value for a classifier. AL can\nbe successfully combined with self-training, i.e., extending a training set\nwith the unlabelled examples for which a classifier is the most certain. We\nreport our experiences on using AL in a systematic manner to train an SVM\nclassifier for Stack Overflow posts discussing performance of software\ncomponents. We show that the training examples deemed as the most valuable to\nthe classifier are also the most difficult for humans to annotate. Despite\ncarefully evolved annotation criteria, we report low inter-rater agreement, but\nwe also propose mitigation strategies. Finally, based on one annotator's work,\nwe show that self-training can improve the classification accuracy. We conclude\nthe paper by discussing implication for future text miners aspiring to use AL\nand self-training.\n", "versions": [{"version": "v1", "created": "Wed, 26 Apr 2017 20:47:36 GMT"}], "update_date": "2017-05-09", "authors_parsed": [["Borg", "Markus", ""], ["Lennerstad", "Iben", ""], ["Ros", "Rasmus", ""], ["Bjarnason", "Elizabeth", ""]]}, {"id": "1705.02411", "submitter": "Anirudh Raju", "authors": "Ming Sun, Anirudh Raju, George Tucker, Sankaran Panchapagesan,\n  Gengshen Fu, Arindam Mandal, Spyros Matsoukas, Nikko Strom, Shiv Vitaladevuni", "title": "Max-Pooling Loss Training of Long Short-Term Memory Networks for\n  Small-Footprint Keyword Spotting", "comments": null, "journal-ref": "Spoken Language Technology Workshop (SLT), 2016 IEEE (pp.\n  474-480). IEEE", "doi": "10.1109/SLT.2016.7846306", "report-no": null, "categories": "cs.CL cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a max-pooling based loss function for training Long Short-Term\nMemory (LSTM) networks for small-footprint keyword spotting (KWS), with low\nCPU, memory, and latency requirements. The max-pooling loss training can be\nfurther guided by initializing with a cross-entropy loss trained network. A\nposterior smoothing based evaluation approach is employed to measure keyword\nspotting performance. Our experimental results show that LSTM models trained\nusing cross-entropy loss or max-pooling loss outperform a cross-entropy loss\ntrained baseline feed-forward Deep Neural Network (DNN). In addition,\nmax-pooling loss trained LSTM with randomly initialized network performs better\ncompared to cross-entropy loss trained LSTM. Finally, the max-pooling loss\ntrained LSTM initialized with a cross-entropy pre-trained network shows the\nbest performance, which yields $67.6\\%$ relative reduction compared to baseline\nfeed-forward DNN in Area Under the Curve (AUC) measure.\n", "versions": [{"version": "v1", "created": "Fri, 5 May 2017 22:36:04 GMT"}], "update_date": "2017-05-09", "authors_parsed": [["Sun", "Ming", ""], ["Raju", "Anirudh", ""], ["Tucker", "George", ""], ["Panchapagesan", "Sankaran", ""], ["Fu", "Gengshen", ""], ["Mandal", "Arindam", ""], ["Matsoukas", "Spyros", ""], ["Strom", "Nikko", ""], ["Vitaladevuni", "Shiv", ""]]}, {"id": "1705.02414", "submitter": "Patrick Doetsch", "authors": "Patrick Doetsch, Pavel Golik, Hermann Ney", "title": "A comprehensive study of batch construction strategies for recurrent\n  neural networks in MXNet", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work we compare different batch construction methods for mini-batch\ntraining of recurrent neural networks. While popular implementations like\nTensorFlow and MXNet suggest a bucketing approach to improve the\nparallelization capabilities of the recurrent training process, we propose a\nsimple ordering strategy that arranges the training sequences in a stochastic\nalternatingly sorted way. We compare our method to sequence bucketing as well\nas various other batch construction strategies on the CHiME-4 noisy speech\nrecognition corpus. The experiments show that our alternated sorting approach\nis able to compete both in training time and recognition performance while\nbeing conceptually simpler to implement.\n", "versions": [{"version": "v1", "created": "Fri, 5 May 2017 22:45:31 GMT"}], "update_date": "2017-05-09", "authors_parsed": [["Doetsch", "Patrick", ""], ["Golik", "Pavel", ""], ["Ney", "Hermann", ""]]}, {"id": "1705.02426", "submitter": "Hanxiao Liu", "authors": "Hanxiao Liu, Yuexin Wu, Yiming Yang", "title": "Analogical Inference for Multi-Relational Embeddings", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Large-scale multi-relational embedding refers to the task of learning the\nlatent representations for entities and relations in large knowledge graphs. An\neffective and scalable solution for this problem is crucial for the true\nsuccess of knowledge-based inference in a broad range of applications. This\npaper proposes a novel framework for optimizing the latent representations with\nrespect to the \\textit{analogical} properties of the embedded entities and\nrelations. By formulating the learning objective in a differentiable fashion,\nour model enjoys both theoretical power and computational scalability, and\nsignificantly outperformed a large number of representative baseline methods on\nbenchmark datasets. Furthermore, the model offers an elegant unification of\nseveral well-known methods in multi-relational embedding, which can be proven\nto be special instantiations of our framework.\n", "versions": [{"version": "v1", "created": "Sat, 6 May 2017 01:40:28 GMT"}, {"version": "v2", "created": "Thu, 6 Jul 2017 16:58:24 GMT"}], "update_date": "2017-07-07", "authors_parsed": [["Liu", "Hanxiao", ""], ["Wu", "Yuexin", ""], ["Yang", "Yiming", ""]]}, {"id": "1705.02436", "submitter": "Artemy Kolchinsky", "authors": "Artemy Kolchinsky, Brendan D. Tracey, David H. Wolpert", "title": "Nonlinear Information Bottleneck", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.LG math.IT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Information bottleneck (IB) is a technique for extracting information in one\nrandom variable $X$ that is relevant for predicting another random variable\n$Y$. IB works by encoding $X$ in a compressed \"bottleneck\" random variable $M$\nfrom which $Y$ can be accurately decoded. However, finding the optimal\nbottleneck variable involves a difficult optimization problem, which until\nrecently has been considered for only two limited cases: discrete $X$ and $Y$\nwith small state spaces, and continuous $X$ and $Y$ with a Gaussian joint\ndistribution (in which case optimal encoding and decoding maps are linear). We\npropose a method for performing IB on arbitrarily-distributed discrete and/or\ncontinuous $X$ and $Y$, while allowing for nonlinear encoding and decoding\nmaps. Our approach relies on a novel non-parametric upper bound for mutual\ninformation. We describe how to implement our method using neural networks. We\nthen show that it achieves better performance than the recently-proposed\n\"variational IB\" method on several real-world datasets.\n", "versions": [{"version": "v1", "created": "Sat, 6 May 2017 03:13:21 GMT"}, {"version": "v2", "created": "Wed, 8 Nov 2017 20:57:07 GMT"}, {"version": "v3", "created": "Fri, 16 Feb 2018 20:37:51 GMT"}, {"version": "v4", "created": "Fri, 2 Mar 2018 00:15:09 GMT"}, {"version": "v5", "created": "Wed, 4 Apr 2018 20:55:02 GMT"}, {"version": "v6", "created": "Fri, 20 Jul 2018 21:24:38 GMT"}, {"version": "v7", "created": "Tue, 4 Sep 2018 22:39:50 GMT"}, {"version": "v8", "created": "Thu, 17 Oct 2019 18:01:46 GMT"}, {"version": "v9", "created": "Sat, 30 Nov 2019 19:03:11 GMT"}], "update_date": "2019-12-03", "authors_parsed": [["Kolchinsky", "Artemy", ""], ["Tracey", "Brendan D.", ""], ["Wolpert", "David H.", ""]]}, {"id": "1705.02438", "submitter": "Zhimin Chen", "authors": "Zhimin Chen, Yuguang Tong", "title": "Face Super-Resolution Through Wasserstein GANs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generative adversarial networks (GANs) have received a tremendous amount of\nattention in the past few years, and have inspired applications addressing a\nwide range of problems. Despite its great potential, GANs are difficult to\ntrain. Recently, a series of papers (Arjovsky & Bottou, 2017a; Arjovsky et al.\n2017b; and Gulrajani et al. 2017) proposed using Wasserstein distance as the\ntraining objective and promised easy, stable GAN training across architectures\nwith minimal hyperparameter tuning. In this paper, we compare the performance\nof Wasserstein distance with other training objectives on a variety of GAN\narchitectures in the context of single image super-resolution. Our results\nagree that Wasserstein GAN with gradient penalty (WGAN-GP) provides stable and\nconverging GAN training and that Wasserstein distance is an effective metric to\ngauge training progress.\n", "versions": [{"version": "v1", "created": "Sat, 6 May 2017 03:48:02 GMT"}], "update_date": "2017-05-09", "authors_parsed": [["Chen", "Zhimin", ""], ["Tong", "Yuguang", ""]]}, {"id": "1705.02553", "submitter": "Kamyar Azizzadenesheli Ph.D.", "authors": "Kamyar Azizzadenesheli, Alessandro Lazaric, Animashree Anandkumar", "title": "Experimental results : Reinforcement Learning of POMDPs using Spectral\n  Methods", "comments": "30th Conference on Neural Information Processing Systems (NIPS 2016),\n  Barcelona, Spain", "journal-ref": "NIPS-DeepRL-Workshop-2016Barcelona", "doi": null, "report-no": null, "categories": "cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new reinforcement learning algorithm for partially observable\nMarkov decision processes (POMDP) based on spectral decomposition methods.\nWhile spectral methods have been previously employed for consistent learning of\n(passive) latent variable models such as hidden Markov models, POMDPs are more\nchallenging since the learner interacts with the environment and possibly\nchanges the future observations in the process. We devise a learning algorithm\nrunning through epochs, in each epoch we employ spectral techniques to learn\nthe POMDP parameters from a trajectory generated by a fixed policy. At the end\nof the epoch, an optimization oracle returns the optimal memoryless planning\npolicy which maximizes the expected reward based on the estimated POMDP model.\nWe prove an order-optimal regret bound with respect to the optimal memoryless\npolicy and efficient scaling with respect to the dimensionality of observation\nand action spaces.\n", "versions": [{"version": "v1", "created": "Sun, 7 May 2017 02:49:10 GMT"}], "update_date": "2017-06-20", "authors_parsed": [["Azizzadenesheli", "Kamyar", ""], ["Lazaric", "Alessandro", ""], ["Anandkumar", "Animashree", ""]]}, {"id": "1705.02556", "submitter": "Ishan Jindal", "authors": "Ishan Jindal and Matthew Nokleby", "title": "Classification and Representation via Separable Subspaces: Performance\n  Limits and Algorithms", "comments": "This paper is submitted to IEEE JSTSP Special Issue on\n  Information-Theoretic Methods in Data Acquisition, Analysis, and Processing\n  2018", "journal-ref": "IEEE Journal of Selected Topics in Signal Processing ( Volume: 12\n  , Issue: 5 , Oct. 2018 )", "doi": "10.1109/JSTSP.2018.2838549", "report-no": null, "categories": "cs.IT cs.LG math.IT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the classification performance of Kronecker-structured models in two\nasymptotic regimes and developed an algorithm for separable, fast and compact\nK-S dictionary learning for better classification and representation of\nmultidimensional signals by exploiting the structure in the signal. First, we\nstudy the classification performance in terms of diversity order and pairwise\ngeometry of the subspaces. We derive an exact expression for the diversity\norder as a function of the signal and subspace dimensions of a K-S model. Next,\nwe study the classification capacity, the maximum rate at which the number of\nclasses can grow as the signal dimension goes to infinity. Then we describe a\nfast algorithm for Kronecker-Structured Learning of Discriminative Dictionaries\n(K-SLD2). Finally, we evaluate the empirical classification performance of K-S\nmodels for the synthetic data, showing that they agree with the diversity order\nanalysis. We also evaluate the performance of K-SLD2 on synthetic and\nreal-world datasets showing that the K-SLD2 balances compact signal\nrepresentation and good classification performance.\n", "versions": [{"version": "v1", "created": "Sun, 7 May 2017 03:36:05 GMT"}, {"version": "v2", "created": "Fri, 29 Dec 2017 18:13:49 GMT"}], "update_date": "2018-10-26", "authors_parsed": [["Jindal", "Ishan", ""], ["Nokleby", "Matthew", ""]]}, {"id": "1705.02562", "submitter": "Ajay Nagesh", "authors": "Naveen Nair, Ajay Nagesh, Ganesh Ramakrishnan", "title": "Learning Discriminative Relational Features for Sequence Labeling", "comments": "13 pages, technical report", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Discovering relational structure between input features in sequence labeling\nmodels has shown to improve their accuracy in several problem settings.\nHowever, the search space of relational features is exponential in the number\nof basic input features. Consequently, approaches that learn relational\nfeatures, tend to follow a greedy search strategy. In this paper, we study the\npossibility of optimally learning and applying discriminative relational\nfeatures for sequence labeling. For learning features derived from inputs at a\nparticular sequence position, we propose a Hierarchical Kernels-based approach\n(referred to as Hierarchical Kernel Learning for Structured Output Spaces -\nStructHKL). This approach optimally and efficiently explores the hierarchical\nstructure of the feature space for problems with structured output spaces such\nas sequence labeling. Since the StructHKL approach has limitations in learning\ncomplex relational features derived from inputs at relative positions, we\npropose two solutions to learn relational features namely, (i) enumerating\nsimple component features of complex relational features and discovering their\ncompositions using StructHKL and (ii) leveraging relational kernels, that\ncompute the similarity between instances implicitly, in the sequence labeling\nproblem. We perform extensive empirical evaluation on publicly available\ndatasets and record our observations on settings in which certain approaches\nare effective.\n", "versions": [{"version": "v1", "created": "Sun, 7 May 2017 04:37:53 GMT"}], "update_date": "2017-05-09", "authors_parsed": [["Nair", "Naveen", ""], ["Nagesh", "Ajay", ""], ["Ramakrishnan", "Ganesh", ""]]}, {"id": "1705.02583", "submitter": "Xinyu Zhang", "authors": "Xinyu Zhang, Srinjoy Das, Ojash Neopane and Ken Kreutz-Delgado", "title": "A Design Methodology for Efficient Implementation of Deconvolutional\n  Neural Networks on an FPGA", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years deep learning algorithms have shown extremely high\nperformance on machine learning tasks such as image classification and speech\nrecognition. In support of such applications, various FPGA accelerator\narchitectures have been proposed for convolutional neural networks (CNNs) that\nenable high performance for classification tasks at lower power than CPU and\nGPU processors. However, to date, there has been little research on the use of\nFPGA implementations of deconvolutional neural networks (DCNNs). DCNNs, also\nknown as generative CNNs, encode high-dimensional probability distributions and\nhave been widely used for computer vision applications such as scene\ncompletion, scene segmentation, image creation, image denoising, and\nsuper-resolution imaging. We propose an FPGA architecture for deconvolutional\nnetworks built around an accelerator which effectively handles the complex\nmemory access patterns needed to perform strided deconvolutions, and that\nsupports convolution as well. We also develop a three-step design optimization\nmethod that systematically exploits statistical analysis, design space\nexploration and VLSI optimization. To verify our FPGA deconvolutional\naccelerator design methodology we train DCNNs offline on two representative\ndatasets using the generative adversarial network method (GAN) run on\nTensorflow, and then map these DCNNs to an FPGA DCNN-plus-accelerator\nimplementation to perform generative inference on a Xilinx Zynq-7000 FPGA. Our\nDCNN implementation achieves a peak performance density of 0.012 GOPs/DSP.\n", "versions": [{"version": "v1", "created": "Sun, 7 May 2017 09:18:44 GMT"}], "update_date": "2017-05-09", "authors_parsed": [["Zhang", "Xinyu", ""], ["Das", "Srinjoy", ""], ["Neopane", "Ojash", ""], ["Kreutz-Delgado", "Ken", ""]]}, {"id": "1705.02627", "submitter": "Mostafa Tavassolipour", "authors": "Mostafa Tavassolipour, Seyed Abolfazl Motahari, Mohammad-Taghi Manzuri\n  Shalmani", "title": "Learning of Gaussian Processes in Distributed and Communication Limited\n  Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.IT cs.LG math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is of fundamental importance to find algorithms obtaining optimal\nperformance for learning of statistical models in distributed and communication\nlimited systems. Aiming at characterizing the optimal strategies, we consider\nlearning of Gaussian Processes (GPs) in distributed systems as a pivotal\nexample. We first address a very basic problem: how many bits are required to\nestimate the inner-products of Gaussian vectors across distributed machines?\nUsing information theoretic bounds, we obtain an optimal solution for the\nproblem which is based on vector quantization. Two suboptimal and more\npractical schemes are also presented as substitute for the vector quantization\nscheme. In particular, it is shown that the performance of one of the practical\nschemes which is called per-symbol quantization is very close to the optimal\none. Schemes provided for the inner-product calculations are incorporated into\nour proposed distributed learning methods for GPs. Experimental results show\nthat with spending few bits per symbol in our communication scheme, our\nproposed methods outperform previous zero rate distributed GP learning schemes\nsuch as Bayesian Committee Model (BCM) and Product of experts (PoE).\n", "versions": [{"version": "v1", "created": "Sun, 7 May 2017 14:15:57 GMT"}], "update_date": "2017-05-09", "authors_parsed": [["Tavassolipour", "Mostafa", ""], ["Motahari", "Seyed Abolfazl", ""], ["Shalmani", "Mohammad-Taghi Manzuri", ""]]}, {"id": "1705.02636", "submitter": "Xiang Jiang", "authors": "Xiang Jiang, Erico N de Souza, Ahmad Pesaranghader, Baifan Hu, Daniel\n  L. Silver and Stan Matwin", "title": "TrajectoryNet: An Embedded GPS Trajectory Representation for Point-based\n  Classification Using Recurrent Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Understanding and discovering knowledge from GPS (Global Positioning System)\ntraces of human activities is an essential topic in mobility-based urban\ncomputing. We propose TrajectoryNet-a neural network architecture for\npoint-based trajectory classification to infer real world human transportation\nmodes from GPS traces. To overcome the challenge of capturing the underlying\nlatent factors in the low-dimensional and heterogeneous feature space imposed\nby GPS data, we develop a novel representation that embeds the original feature\nspace into another space that can be understood as a form of basis expansion.\nWe also enrich the feature space via segment-based information and use Maxout\nactivations to improve the predictive power of Recurrent Neural Networks\n(RNNs). We achieve over 98% classification accuracy when detecting four types\nof transportation modes, outperforming existing models without additional\nsensory data or location-based prior knowledge.\n", "versions": [{"version": "v1", "created": "Sun, 7 May 2017 15:40:08 GMT"}, {"version": "v2", "created": "Wed, 30 Aug 2017 15:06:43 GMT"}], "update_date": "2017-08-31", "authors_parsed": [["Jiang", "Xiang", ""], ["de Souza", "Erico N", ""], ["Pesaranghader", "Ahmad", ""], ["Hu", "Baifan", ""], ["Silver", "Daniel L.", ""], ["Matwin", "Stan", ""]]}, {"id": "1705.02643", "submitter": "Davide Bacciu", "authors": "Davide Bacciu and Francesco Crecchi and Davide Morelli", "title": "DropIn: Making Reservoir Computing Neural Networks Robust to Missing\n  Inputs by Dropout", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The paper presents a novel, principled approach to train recurrent neural\nnetworks from the Reservoir Computing family that are robust to missing part of\nthe input features at prediction time. By building on the ensembling properties\nof Dropout regularization, we propose a methodology, named DropIn, which\nefficiently trains a neural model as a committee machine of subnetworks, each\ncapable of predicting with a subset of the original input features. We discuss\nthe application of the DropIn methodology in the context of Reservoir Computing\nmodels and targeting applications characterized by input sources that are\nunreliable or prone to be disconnected, such as in pervasive wireless sensor\nnetworks and ambient intelligence. We provide an experimental assessment using\nreal-world data from such application domains, showing how the Dropin\nmethodology allows to maintain predictive performances comparable to those of a\nmodel without missing features, even when 20\\%-50\\% of the inputs are not\navailable.\n", "versions": [{"version": "v1", "created": "Sun, 7 May 2017 16:03:06 GMT"}], "update_date": "2017-05-09", "authors_parsed": [["Bacciu", "Davide", ""], ["Crecchi", "Francesco", ""], ["Morelli", "Davide", ""]]}, {"id": "1705.02670", "submitter": "Jessica Hamrick", "authors": "Jessica B. Hamrick, Andrew J. Ballard, Razvan Pascanu, Oriol Vinyals,\n  Nicolas Heess, Peter W. Battaglia", "title": "Metacontrol for Adaptive Imagination-Based Optimization", "comments": "Published as a conference paper at ICLR 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many machine learning systems are built to solve the hardest examples of a\nparticular task, which often makes them large and expensive to run---especially\nwith respect to the easier examples, which might require much less computation.\nFor an agent with a limited computational budget, this \"one-size-fits-all\"\napproach may result in the agent wasting valuable computation on easy examples,\nwhile not spending enough on hard examples. Rather than learning a single,\nfixed policy for solving all instances of a task, we introduce a metacontroller\nwhich learns to optimize a sequence of \"imagined\" internal simulations over\npredictive models of the world in order to construct a more informed, and more\neconomical, solution. The metacontroller component is a model-free\nreinforcement learning agent, which decides both how many iterations of the\noptimization procedure to run, as well as which model to consult on each\niteration. The models (which we call \"experts\") can be state transition models,\naction-value functions, or any other mechanism that provides information useful\nfor solving the task, and can be learned on-policy or off-policy in parallel\nwith the metacontroller. When the metacontroller, controller, and experts were\ntrained with \"interaction networks\" (Battaglia et al., 2016) as expert models,\nour approach was able to solve a challenging decision-making problem under\ncomplex non-linear dynamics. The metacontroller learned to adapt the amount of\ncomputation it performed to the difficulty of the task, and learned how to\nchoose which experts to consult by factoring in both their reliability and\nindividual computational resource costs. This allowed the metacontroller to\nachieve a lower overall cost (task loss plus computational cost) than more\ntraditional fixed policy approaches. These results demonstrate that our\napproach is a powerful framework for using...\n", "versions": [{"version": "v1", "created": "Sun, 7 May 2017 17:48:14 GMT"}], "update_date": "2017-05-09", "authors_parsed": [["Hamrick", "Jessica B.", ""], ["Ballard", "Andrew J.", ""], ["Pascanu", "Razvan", ""], ["Vinyals", "Oriol", ""], ["Heess", "Nicolas", ""], ["Battaglia", "Peter W.", ""]]}, {"id": "1705.02687", "submitter": "Seyed Sajjadi", "authors": "Seyed Sajjadi, Bruce Shapiro, Christopher McKinlay, Allen Sarkisyan,\n  Carol Shubin, Efunwande Osoba", "title": "Finding Bottlenecks: Predicting Student Attrition with Unsupervised\n  Classifier", "comments": "7 pages, 10 figures, Finding Bottlenecks: Predicting Student\n  Attrition with Unsupervised Classifiers, IEEE, IntelliSys 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.CY cs.LG stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With pressure to increase graduation rates and reduce time to degree in\nhigher education, it is important to identify at-risk students early. Automated\nearly warning systems are therefore highly desirable. In this paper, we use\nunsupervised clustering techniques to predict the graduation status of declared\nmajors in five departments at California State University Northridge (CSUN),\nbased on a minimal number of lower division courses in each major. In addition,\nwe use the detected clusters to identify hidden bottleneck courses.\n", "versions": [{"version": "v1", "created": "Sun, 7 May 2017 19:45:49 GMT"}], "update_date": "2017-05-09", "authors_parsed": [["Sajjadi", "Seyed", ""], ["Shapiro", "Bruce", ""], ["McKinlay", "Christopher", ""], ["Sarkisyan", "Allen", ""], ["Shubin", "Carol", ""], ["Osoba", "Efunwande", ""]]}, {"id": "1705.02694", "submitter": "Amol Patwardhan", "authors": "Amol S Patwardhan and Gerald M Knapp", "title": "Multimodal Affect Analysis for Product Feedback Assessment", "comments": "10 pages, ISERC 2013, IIE Annual Conference. Proceedings. Institute\n  of Industrial Engineers", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.AI cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Consumers often react expressively to products such as food samples, perfume,\njewelry, sunglasses, and clothing accessories. This research discusses a\nmultimodal affect recognition system developed to classify whether a consumer\nlikes or dislikes a product tested at a counter or kiosk, by analyzing the\nconsumer's facial expression, body posture, hand gestures, and voice after\ntesting the product. A depth-capable camera and microphone system - Kinect for\nWindows - is utilized. An emotion identification engine has been developed to\nanalyze the images and voice to determine affective state of the customer. The\nimage is segmented using skin color and adaptive threshold. Face, body and\nhands are detected using the Haar cascade classifier. Canny edges are\nidentified and the lip, body and hand contours are extracted using spatial\nfiltering. Edge count and orientation around the mouth, cheeks, eyes,\nshoulders, fingers and the location of the edges are used as features.\nClassification is done by an emotion template mapping algorithm and training a\nclassifier using support vector machines. The real-time performance, accuracy\nand feasibility for multimodal affect recognition in feedback assessment are\nevaluated.\n", "versions": [{"version": "v1", "created": "Sun, 7 May 2017 20:39:35 GMT"}], "update_date": "2017-05-09", "authors_parsed": [["Patwardhan", "Amol S", ""], ["Knapp", "Gerald M", ""]]}, {"id": "1705.02699", "submitter": "Xiaolei Ma", "authors": "Haiyang Yu, Zhihai Wu, Shuqin Wang, Yunpeng Wang, Xiaolei Ma", "title": "Spatiotemporal Recurrent Convolutional Networks for Traffic Prediction\n  in Transportation Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Predicting large-scale transportation network traffic has become an important\nand challenging topic in recent decades. Inspired by the domain knowledge of\nmotion prediction, in which the future motion of an object can be predicted\nbased on previous scenes, we propose a network grid representation method that\ncan retain the fine-scale structure of a transportation network. Network-wide\ntraffic speeds are converted into a series of static images and input into a\nnovel deep architecture, namely, spatiotemporal recurrent convolutional\nnetworks (SRCNs), for traffic forecasting. The proposed SRCNs inherit the\nadvantages of deep convolutional neural networks (DCNNs) and long short-term\nmemory (LSTM) neural networks. The spatial dependencies of network-wide traffic\ncan be captured by DCNNs, and the temporal dynamics can be learned by LSTMs. An\nexperiment on a Beijing transportation network with 278 links demonstrates that\nSRCNs outperform other deep learning-based algorithms in both short-term and\nlong-term traffic prediction.\n", "versions": [{"version": "v1", "created": "Sun, 7 May 2017 21:14:02 GMT"}], "update_date": "2017-05-09", "authors_parsed": [["Yu", "Haiyang", ""], ["Wu", "Zhihai", ""], ["Wang", "Shuqin", ""], ["Wang", "Yunpeng", ""], ["Ma", "Xiaolei", ""]]}, {"id": "1705.02737", "submitter": "Lovedeep Gondara", "authors": "Lovedeep Gondara, Ke Wang", "title": "MIDA: Multiple Imputation using Denoising Autoencoders", "comments": "To appear in the proceedings of the 22nd Pacific-Asia Conference on\n  Knowledge Discovery and Data Mining (PAKDD 2018)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Missing data is a significant problem impacting all domains. State-of-the-art\nframework for minimizing missing data bias is multiple imputation, for which\nthe choice of an imputation model remains nontrivial. We propose a multiple\nimputation model based on overcomplete deep denoising autoencoders. Our\nproposed model is capable of handling different data types, missingness\npatterns, missingness proportions and distributions. Evaluation on several real\nlife datasets show our proposed model significantly outperforms current\nstate-of-the-art methods under varying conditions while simultaneously\nimproving end of the line analytics.\n", "versions": [{"version": "v1", "created": "Mon, 8 May 2017 04:00:25 GMT"}, {"version": "v2", "created": "Fri, 19 May 2017 21:15:44 GMT"}, {"version": "v3", "created": "Sat, 17 Feb 2018 16:05:32 GMT"}], "update_date": "2018-02-20", "authors_parsed": [["Gondara", "Lovedeep", ""], ["Wang", "Ke", ""]]}, {"id": "1705.02758", "submitter": "Xiu-Shen Wei", "authors": "Xiu-Shen Wei, Chen-Lin Zhang, Yao Li, Chen-Wei Xie, Jianxin Wu,\n  Chunhua Shen, Zhi-Hua Zhou", "title": "Deep Descriptor Transforming for Image Co-Localization", "comments": "Accepted by IJCAI 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Reusable model design becomes desirable with the rapid expansion of machine\nlearning applications. In this paper, we focus on the reusability of\npre-trained deep convolutional models. Specifically, different from treating\npre-trained models as feature extractors, we reveal more treasures beneath\nconvolutional layers, i.e., the convolutional activations could act as a\ndetector for the common object in the image co-localization problem. We propose\na simple but effective method, named Deep Descriptor Transforming (DDT), for\nevaluating the correlations of descriptors and then obtaining the\ncategory-consistent regions, which can accurately locate the common object in a\nset of images. Empirical studies validate the effectiveness of the proposed DDT\nmethod. On benchmark image co-localization datasets, DDT consistently\noutperforms existing state-of-the-art methods by a large margin. Moreover, DDT\nalso demonstrates good generalization ability for unseen categories and\nrobustness for dealing with noisy data.\n", "versions": [{"version": "v1", "created": "Mon, 8 May 2017 06:52:44 GMT"}], "update_date": "2017-05-30", "authors_parsed": [["Wei", "Xiu-Shen", ""], ["Zhang", "Chen-Lin", ""], ["Li", "Yao", ""], ["Xie", "Chen-Wei", ""], ["Wu", "Jianxin", ""], ["Shen", "Chunhua", ""], ["Zhou", "Zhi-Hua", ""]]}, {"id": "1705.02801", "submitter": "Palash Goyal", "authors": "Palash Goyal, Emilio Ferrara", "title": "Graph Embedding Techniques, Applications, and Performance: A Survey", "comments": "Submitted to Knowledge Based Systems for review", "journal-ref": "Knowledge Based Systems, Volume 151, 1 July 2018, Pages 78-94,\n  2018", "doi": "10.1016/j.knosys.2018.03.022", "report-no": null, "categories": "cs.SI cs.LG physics.data-an", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Graphs, such as social networks, word co-occurrence networks, and\ncommunication networks, occur naturally in various real-world applications.\nAnalyzing them yields insight into the structure of society, language, and\ndifferent patterns of communication. Many approaches have been proposed to\nperform the analysis. Recently, methods which use the representation of graph\nnodes in vector space have gained traction from the research community. In this\nsurvey, we provide a comprehensive and structured analysis of various graph\nembedding techniques proposed in the literature. We first introduce the\nembedding task and its challenges such as scalability, choice of\ndimensionality, and features to be preserved, and their possible solutions. We\nthen present three categories of approaches based on factorization methods,\nrandom walks, and deep learning, with examples of representative algorithms in\neach category and analysis of their performance on various tasks. We evaluate\nthese state-of-the-art methods on a few common datasets and compare their\nperformance against one another. Our analysis concludes by suggesting some\npotential applications and future directions. We finally present the\nopen-source Python library we developed, named GEM (Graph Embedding Methods,\navailable at https://github.com/palash1992/GEM), which provides all presented\nalgorithms within a unified interface to foster and facilitate research on the\ntopic.\n", "versions": [{"version": "v1", "created": "Mon, 8 May 2017 09:47:38 GMT"}, {"version": "v2", "created": "Tue, 9 May 2017 17:28:21 GMT"}, {"version": "v3", "created": "Sun, 10 Dec 2017 17:24:19 GMT"}, {"version": "v4", "created": "Fri, 22 Dec 2017 20:42:06 GMT"}], "update_date": "2019-08-22", "authors_parsed": [["Goyal", "Palash", ""], ["Ferrara", "Emilio", ""]]}, {"id": "1705.02891", "submitter": "Alessandro Barp", "authors": "Alessandro Barp, Francois-Xavier Briol, Anthony D. Kennedy, Mark\n  Girolami", "title": "Geometry and Dynamics for Markov Chain Monte Carlo", "comments": "Submitted to \"Annual Review of Statistics and Its Applications\"", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO cs.LG hep-lat math.NA stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Markov Chain Monte Carlo methods have revolutionised mathematical computation\nand enabled statistical inference within many previously intractable models. In\nthis context, Hamiltonian dynamics have been proposed as an efficient way of\nbuilding chains which can explore probability densities efficiently. The method\nemerges from physics and geometry and these links have been extensively studied\nby a series of authors through the last thirty years. However, there is\ncurrently a gap between the intuitions and knowledge of users of the\nmethodology and our deep understanding of these theoretical foundations. The\naim of this review is to provide a comprehensive introduction to the geometric\ntools used in Hamiltonian Monte Carlo at a level accessible to statisticians,\nmachine learners and other users of the methodology with only a basic\nunderstanding of Monte Carlo methods. This will be complemented with some\ndiscussion of the most recent advances in the field which we believe will\nbecome increasingly relevant to applied scientists.\n", "versions": [{"version": "v1", "created": "Mon, 8 May 2017 14:19:53 GMT"}], "update_date": "2017-05-09", "authors_parsed": [["Barp", "Alessandro", ""], ["Briol", "Francois-Xavier", ""], ["Kennedy", "Anthony D.", ""], ["Girolami", "Mark", ""]]}, {"id": "1705.02894", "submitter": "Jong Chul Ye", "authors": "Jae Hyun Lim and Jong Chul Ye", "title": "Geometric GAN", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cond-mat.dis-nn cs.AI cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generative Adversarial Nets (GANs) represent an important milestone for\neffective generative models, which has inspired numerous variants seemingly\ndifferent from each other. One of the main contributions of this paper is to\nreveal a unified geometric structure in GAN and its variants. Specifically, we\nshow that the adversarial generative model training can be decomposed into\nthree geometric steps: separating hyperplane search, discriminator parameter\nupdate away from the separating hyperplane, and the generator update along the\nnormal vector direction of the separating hyperplane. This geometric intuition\nreveals the limitations of the existing approaches and leads us to propose a\nnew formulation called geometric GAN using SVM separating hyperplane that\nmaximizes the margin. Our theoretical analysis shows that the geometric GAN\nconverges to a Nash equilibrium between the discriminator and generator. In\naddition, extensive numerical results show that the superior performance of\ngeometric GAN.\n", "versions": [{"version": "v1", "created": "Mon, 8 May 2017 14:32:33 GMT"}, {"version": "v2", "created": "Tue, 9 May 2017 01:12:28 GMT"}], "update_date": "2017-05-10", "authors_parsed": [["Lim", "Jae Hyun", ""], ["Ye", "Jong Chul", ""]]}, {"id": "1705.02908", "submitter": "Yangqiu Song", "authors": "Yangqiu Song and Dan Roth", "title": "Machine Learning with World Knowledge: The Position and Survey", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Machine learning has become pervasive in multiple domains, impacting a wide\nvariety of applications, such as knowledge discovery and data mining, natural\nlanguage processing, information retrieval, computer vision, social and health\ninformatics, ubiquitous computing, etc. Two essential problems of machine\nlearning are how to generate features and how to acquire labels for machines to\nlearn. Particularly, labeling large amount of data for each domain-specific\nproblem can be very time consuming and costly. It has become a key obstacle in\nmaking learning protocols realistic in applications. In this paper, we will\ndiscuss how to use the existing general-purpose world knowledge to enhance\nmachine learning processes, by enriching the features or reducing the labeling\nwork. We start from the comparison of world knowledge with domain-specific\nknowledge, and then introduce three key problems in using world knowledge in\nlearning processes, i.e., explicit and implicit feature representation,\ninference for knowledge linking and disambiguation, and learning with direct or\nindirect supervision. Finally we discuss the future directions of this research\ntopic.\n", "versions": [{"version": "v1", "created": "Mon, 8 May 2017 15:06:32 GMT"}], "update_date": "2017-05-09", "authors_parsed": [["Song", "Yangqiu", ""], ["Roth", "Dan", ""]]}, {"id": "1705.02928", "submitter": "Yuantao Gu", "authors": "Xiudong Wang and Yuantao Gu", "title": "Cross-label Suppression: A Discriminative and Fast Dictionary Learning\n  with Group Regularization", "comments": "36 pages, 12 figures, 11 tables", "journal-ref": null, "doi": "10.1109/TIP.2017.2703101", "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper addresses image classification through learning a compact and\ndiscriminative dictionary efficiently. Given a structured dictionary with each\natom (columns in the dictionary matrix) related to some label, we propose\ncross-label suppression constraint to enlarge the difference among\nrepresentations for different classes. Meanwhile, we introduce group\nregularization to enforce representations to preserve label properties of\noriginal samples, meaning the representations for the same class are encouraged\nto be similar. Upon the cross-label suppression, we don't resort to\nfrequently-used $\\ell_0$-norm or $\\ell_1$-norm for coding, and obtain\ncomputational efficiency without losing the discriminative power for\ncategorization. Moreover, two simple classification schemes are also developed\nto take full advantage of the learnt dictionary. Extensive experiments on six\ndata sets including face recognition, object categorization, scene\nclassification, texture recognition and sport action categorization are\nconducted, and the results show that the proposed approach can outperform lots\nof recently presented dictionary algorithms on both recognition accuracy and\ncomputational efficiency.\n", "versions": [{"version": "v1", "created": "Mon, 8 May 2017 15:49:43 GMT"}], "update_date": "2017-08-02", "authors_parsed": [["Wang", "Xiudong", ""], ["Gu", "Yuantao", ""]]}, {"id": "1705.02994", "submitter": "Andrea Montanari", "authors": "Hamid Javadi and Andrea Montanari", "title": "Non-negative Matrix Factorization via Archetypal Analysis", "comments": "39 pages; 11 pdf figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given a collection of data points, non-negative matrix factorization (NMF)\nsuggests to express them as convex combinations of a small set of `archetypes'\nwith non-negative entries. This decomposition is unique only if the true\narchetypes are non-negative and sufficiently sparse (or the weights are\nsufficiently sparse), a regime that is captured by the separability condition\nand its generalizations.\n  In this paper, we study an approach to NMF that can be traced back to the\nwork of Cutler and Breiman (1994) and does not require the data to be\nseparable, while providing a generally unique decomposition. We optimize the\ntrade-off between two objectives: we minimize the distance of the data points\nfrom the convex envelope of the archetypes (which can be interpreted as an\nempirical risk), while minimizing the distance of the archetypes from the\nconvex envelope of the data (which can be interpreted as a data-dependent\nregularization). The archetypal analysis method of (Cutler, Breiman, 1994) is\nrecovered as the limiting case in which the last term is given infinite weight.\n  We introduce a `uniqueness condition' on the data which is necessary for\nexactly recovering the archetypes from noiseless data. We prove that, under\nuniqueness (plus additional regularity conditions on the geometry of the\narchetypes), our estimator is robust. While our approach requires solving a\nnon-convex optimization problem, we find that standard optimization methods\nsucceed in finding good solutions both for real and synthetic data.\n", "versions": [{"version": "v1", "created": "Mon, 8 May 2017 17:53:36 GMT"}], "update_date": "2017-05-09", "authors_parsed": [["Javadi", "Hamid", ""], ["Montanari", "Andrea", ""]]}, {"id": "1705.03071", "submitter": "Behnam Neyshabur", "authors": "Behnam Neyshabur, Ryota Tomioka, Ruslan Salakhutdinov, Nathan Srebro", "title": "Geometry of Optimization and Implicit Regularization in Deep Learning", "comments": "This survey chapter was done as a part of Intel Collaborative\n  Research institute for Computational Intelligence (ICRI-CI) \"Why & When Deep\n  Learning works -- looking inside Deep Learning\" compendium with the generous\n  support of ICRI-CI. arXiv admin note: substantial text overlap with\n  arXiv:1506.02617", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We argue that the optimization plays a crucial role in generalization of deep\nlearning models through implicit regularization. We do this by demonstrating\nthat generalization ability is not controlled by network size but rather by\nsome other implicit control. We then demonstrate how changing the empirical\noptimization procedure can improve generalization, even if actual optimization\nquality is not affected. We do so by studying the geometry of the parameter\nspace of deep networks, and devising an optimization algorithm attuned to this\ngeometry.\n", "versions": [{"version": "v1", "created": "Mon, 8 May 2017 20:12:08 GMT"}], "update_date": "2017-05-10", "authors_parsed": [["Neyshabur", "Behnam", ""], ["Tomioka", "Ryota", ""], ["Salakhutdinov", "Ruslan", ""], ["Srebro", "Nathan", ""]]}, {"id": "1705.03151", "submitter": "Zhiyuan Tang", "authors": "Zhiyuan Tang, Dong Wang, Yixiang Chen, Lantian Li and Andrew Abel", "title": "Phonetic Temporal Neural Model for Language Identification", "comments": "Submitted to TASLP", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural models, particularly the LSTM-RNN model, have shown great\npotential for language identification (LID). However, the use of phonetic\ninformation has been largely overlooked by most existing neural LID methods,\nalthough this information has been used very successfully in conventional\nphonetic LID systems. We present a phonetic temporal neural model for LID,\nwhich is an LSTM-RNN LID system that accepts phonetic features produced by a\nphone-discriminative DNN as the input, rather than raw acoustic features. This\nnew model is similar to traditional phonetic LID methods, but the phonetic\nknowledge here is much richer: it is at the frame level and involves compacted\ninformation of all phones. Our experiments conducted on the Babel database and\nthe AP16-OLR database demonstrate that the temporal phonetic neural approach is\nvery effective, and significantly outperforms existing acoustic neural models.\nIt also outperforms the conventional i-vector approach on short utterances and\nin noisy conditions.\n", "versions": [{"version": "v1", "created": "Tue, 9 May 2017 02:46:21 GMT"}, {"version": "v2", "created": "Mon, 22 May 2017 11:23:34 GMT"}, {"version": "v3", "created": "Fri, 25 Aug 2017 05:23:26 GMT"}], "update_date": "2017-08-28", "authors_parsed": [["Tang", "Zhiyuan", ""], ["Wang", "Dong", ""], ["Chen", "Yixiang", ""], ["Li", "Lantian", ""], ["Abel", "Andrew", ""]]}, {"id": "1705.03152", "submitter": "Zhiyuan Tang", "authors": "Zhiyuan Tang, Dong Wang, Yixiang Chen, Ying Shi, Lantian Li", "title": "Phone-aware Neural Language Identification", "comments": "arXiv admin note: text overlap with arXiv:1705.03151", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Pure acoustic neural models, particularly the LSTM-RNN model, have shown\ngreat potential in language identification (LID). However, the phonetic\ninformation has been largely overlooked by most of existing neural LID models,\nalthough this information has been used in the conventional phonetic LID\nsystems with a great success. We present a phone-aware neural LID architecture,\nwhich is a deep LSTM-RNN LID system but accepts output from an RNN-based ASR\nsystem. By utilizing the phonetic knowledge, the LID performance can be\nsignificantly improved. Interestingly, even if the test language is not\ninvolved in the ASR training, the phonetic knowledge still presents a large\ncontribution. Our experiments conducted on four languages within the Babel\ncorpus demonstrated that the phone-aware approach is highly effective.\n", "versions": [{"version": "v1", "created": "Tue, 9 May 2017 02:47:22 GMT"}, {"version": "v2", "created": "Mon, 22 May 2017 11:43:40 GMT"}], "update_date": "2017-05-24", "authors_parsed": [["Tang", "Zhiyuan", ""], ["Wang", "Dong", ""], ["Chen", "Yixiang", ""], ["Shi", "Ying", ""], ["Li", "Lantian", ""]]}, {"id": "1705.03290", "submitter": "Iiris Sundin", "authors": "Iiris Sundin, Tomi Peltola, Muntasir Mamun Majumder, Pedram Daee,\n  Marta Soare, Homayun Afrabandpey, Caroline Heckman, Samuel Kaski and Pekka\n  Marttinen", "title": "Improving drug sensitivity predictions in precision medicine through\n  active expert knowledge elicitation", "comments": "24 pages, 3 figures", "journal-ref": null, "doi": "10.1093/bioinformatics/bty257", "report-no": null, "categories": "cs.AI cs.HC cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Predicting the efficacy of a drug for a given individual, using\nhigh-dimensional genomic measurements, is at the core of precision medicine.\nHowever, identifying features on which to base the predictions remains a\nchallenge, especially when the sample size is small. Incorporating expert\nknowledge offers a promising alternative to improve a prediction model, but\ncollecting such knowledge is laborious to the expert if the number of candidate\nfeatures is very large. We introduce a probabilistic model that can incorporate\nexpert feedback about the impact of genomic measurements on the sensitivity of\na cancer cell for a given drug. We also present two methods to intelligently\ncollect this feedback from the expert, using experimental design and\nmulti-armed bandit models. In a multiple myeloma blood cancer data set (n=51),\nexpert knowledge decreased the prediction error by 8%. Furthermore, the\nintelligent approaches can be used to reduce the workload of feedback\ncollection to less than 30% on average compared to a naive approach.\n", "versions": [{"version": "v1", "created": "Tue, 9 May 2017 12:04:33 GMT"}], "update_date": "2019-01-08", "authors_parsed": [["Sundin", "Iiris", ""], ["Peltola", "Tomi", ""], ["Majumder", "Muntasir Mamun", ""], ["Daee", "Pedram", ""], ["Soare", "Marta", ""], ["Afrabandpey", "Homayun", ""], ["Heckman", "Caroline", ""], ["Kaski", "Samuel", ""], ["Marttinen", "Pekka", ""]]}, {"id": "1705.03321", "submitter": "Hamid Reza Hassanzadeh", "authors": "Hamid Reza Hassanzadeh, Pushkar Kolhe, Charles L. Isbell, May D. Wang", "title": "MotifMark: Finding Regulatory Motifs in DNA Sequences", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.QM cs.LG q-bio.GN", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The interaction between proteins and DNA is a key driving force in a\nsignificant number of biological processes such as transcriptional regulation,\nrepair, recombination, splicing, and DNA modification. The identification of\nDNA-binding sites and the specificity of target proteins in binding to these\nregions are two important steps in understanding the mechanisms of these\nbiological activities. A number of high-throughput technologies have recently\nemerged that try to quantify the affinity between proteins and DNA motifs.\nDespite their success, these technologies have their own limitations and fall\nshort in precise characterization of motifs, and as a result, require further\ndownstream analysis to extract useful and interpretable information from a\nhaystack of noisy and inaccurate data. Here we propose MotifMark, a new\nalgorithm based on graph theory and machine learning, that can find binding\nsites on candidate probes and rank their specificity in regard to the\nunderlying transcription factor. We developed a pipeline to analyze\nexperimental data derived from compact universal protein binding microarrays\nand benchmarked it against two of the most accurate motif search methods. Our\nresults indicate that MotifMark can be a viable alternative technique for\nprediction of motif from protein binding microarrays and possibly other related\nhigh-throughput techniques.\n", "versions": [{"version": "v1", "created": "Thu, 4 May 2017 14:50:12 GMT"}], "update_date": "2017-05-10", "authors_parsed": [["Hassanzadeh", "Hamid Reza", ""], ["Kolhe", "Pushkar", ""], ["Isbell", "Charles L.", ""], ["Wang", "May D.", ""]]}, {"id": "1705.03341", "submitter": "Lars Ruthotto", "authors": "Eldad Haber and Lars Ruthotto", "title": "Stable Architectures for Deep Neural Networks", "comments": "23 pages, 7 figures", "journal-ref": "Inverse Problems, Volume 34, Number 1 Inverse Problems, Volume 34,\n  Number 1, 2017", "doi": "10.1088/1361-6420/aa9a90", "report-no": null, "categories": "cs.LG cs.NA math.NA math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural networks have become invaluable tools for supervised machine\nlearning, e.g., classification of text or images. While often offering superior\nresults over traditional techniques and successfully expressing complicated\npatterns in data, deep architectures are known to be challenging to design and\ntrain such that they generalize well to new data. Important issues with deep\narchitectures are numerical instabilities in derivative-based learning\nalgorithms commonly called exploding or vanishing gradients. In this paper we\npropose new forward propagation techniques inspired by systems of Ordinary\nDifferential Equations (ODE) that overcome this challenge and lead to\nwell-posed learning problems for arbitrarily deep networks.\n  The backbone of our approach is our interpretation of deep learning as a\nparameter estimation problem of nonlinear dynamical systems. Given this\nformulation, we analyze stability and well-posedness of deep learning and use\nthis new understanding to develop new network architectures. We relate the\nexploding and vanishing gradient phenomenon to the stability of the discrete\nODE and present several strategies for stabilizing deep learning for very deep\nnetworks. While our new architectures restrict the solution space, several\nnumerical experiments show their competitiveness with state-of-the-art\nnetworks.\n", "versions": [{"version": "v1", "created": "Tue, 9 May 2017 14:13:18 GMT"}, {"version": "v2", "created": "Fri, 30 Jun 2017 08:47:13 GMT"}, {"version": "v3", "created": "Sat, 16 Feb 2019 16:13:21 GMT"}], "update_date": "2019-02-19", "authors_parsed": [["Haber", "Eldad", ""], ["Ruthotto", "Lars", ""]]}, {"id": "1705.03387", "submitter": "Hyeungill Lee", "authors": "Hyeungill Lee, Sungyeob Han, Jungwoo Lee", "title": "Generative Adversarial Trainer: Defense to Adversarial Perturbations\n  with GAN", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel technique to make neural network robust to adversarial\nexamples using a generative adversarial network. We alternately train both\nclassifier and generator networks. The generator network generates an\nadversarial perturbation that can easily fool the classifier network by using a\ngradient of each image. Simultaneously, the classifier network is trained to\nclassify correctly both original and adversarial images generated by the\ngenerator. These procedures help the classifier network to become more robust\nto adversarial perturbations. Furthermore, our adversarial training framework\nefficiently reduces overfitting and outperforms other regularization methods\nsuch as Dropout. We applied our method to supervised learning for CIFAR\ndatasets, and experimantal results show that our method significantly lowers\nthe generalization error of the network. To the best of our knowledge, this is\nthe first method which uses GAN to improve supervised learning.\n", "versions": [{"version": "v1", "created": "Tue, 9 May 2017 15:30:58 GMT"}, {"version": "v2", "created": "Fri, 26 May 2017 21:44:32 GMT"}], "update_date": "2017-05-30", "authors_parsed": [["Lee", "Hyeungill", ""], ["Han", "Sungyeob", ""], ["Lee", "Jungwoo", ""]]}, {"id": "1705.03414", "submitter": "Nisheeth Vishnoi", "authors": "L. Elisa Celis, Peter M. Krafft, Nisheeth K. Vishnoi", "title": "A Distributed Learning Dynamics in Social Groups", "comments": "To appear in PODC 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CY cs.DC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study a distributed learning process observed in human groups and other\nsocial animals. This learning process appears in settings in which each\nindividual in a group is trying to decide over time, in a distributed manner,\nwhich option to select among a shared set of options. Specifically, we consider\na stochastic dynamics in a group in which every individual selects an option in\nthe following two-step process: (1) select a random individual and observe the\noption that individual chose in the previous time step, and (2) adopt that\noption if its stochastic quality was good at that time step. Various\ninstantiations of such distributed learning appear in nature, and have also\nbeen studied in the social science literature. From the perspective of an\nindividual, an attractive feature of this learning process is that it is a\nsimple heuristic that requires extremely limited computational capacities. But\nwhat does it mean for the group -- could such a simple, distributed and\nessentially memoryless process lead the group as a whole to perform optimally?\nWe show that the answer to this question is yes -- this distributed learning is\nhighly effective at identifying the best option and is close to optimal for the\ngroup overall. Our analysis also gives quantitative bounds that show fast\nconvergence of these stochastic dynamics. Prior to our work the only\ntheoretical work related to such learning dynamics has been either in\ndeterministic special cases or in the asymptotic setting. Finally, we observe\nthat our infinite population dynamics is a stochastic variant of the classic\nmultiplicative weights update (MWU) method. Consequently, we arrive at the\nfollowing interesting converse: the learning dynamics on a finite population\nconsidered here can be viewed as a novel distributed and low-memory\nimplementation of the classic MWU method.\n", "versions": [{"version": "v1", "created": "Mon, 8 May 2017 15:15:18 GMT"}], "update_date": "2017-05-10", "authors_parsed": [["Celis", "L. Elisa", ""], ["Krafft", "Peter M.", ""], ["Vishnoi", "Nisheeth K.", ""]]}, {"id": "1705.03419", "submitter": "Ishan Jindal", "authors": "Ishan Jindal, Matthew Nokleby and Xuewen Chen", "title": "Learning Deep Networks from Noisy Labels with Dropout Regularization", "comments": "Published at 2016 IEEE 16th International Conference on Data Mining", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Large datasets often have unreliable labels-such as those obtained from\nAmazon's Mechanical Turk or social media platforms-and classifiers trained on\nmislabeled datasets often exhibit poor performance. We present a simple,\neffective technique for accounting for label noise when training deep neural\nnetworks. We augment a standard deep network with a softmax layer that models\nthe label noise statistics. Then, we train the deep network and noise model\njointly via end-to-end stochastic gradient descent on the (perhaps mislabeled)\ndataset. The augmented model is overdetermined, so in order to encourage the\nlearning of a non-trivial noise model, we apply dropout regularization to the\nweights of the noise model during training. Numerical experiments on noisy\nversions of the CIFAR-10 and MNIST datasets show that the proposed dropout\ntechnique outperforms state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Tue, 9 May 2017 16:42:32 GMT"}], "update_date": "2017-05-10", "authors_parsed": [["Jindal", "Ishan", ""], ["Nokleby", "Matthew", ""], ["Chen", "Xuewen", ""]]}, {"id": "1705.03439", "submitter": "Yixin Wang", "authors": "Yixin Wang, David M. Blei", "title": "Frequentist Consistency of Variational Bayes", "comments": null, "journal-ref": "Journal of the American Statistical Association 114.527 (2019):\n  1147-1161", "doi": "10.1080/01621459.2018.1473776", "report-no": null, "categories": "stat.ML cs.LG math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A key challenge for modern Bayesian statistics is how to perform scalable\ninference of posterior distributions. To address this challenge, variational\nBayes (VB) methods have emerged as a popular alternative to the classical\nMarkov chain Monte Carlo (MCMC) methods. VB methods tend to be faster while\nachieving comparable predictive performance. However, there are few theoretical\nresults around VB. In this paper, we establish frequentist consistency and\nasymptotic normality of VB methods. Specifically, we connect VB methods to\npoint estimates based on variational approximations, called frequentist\nvariational approximations, and we use the connection to prove a variational\nBernstein-von Mises theorem. The theorem leverages the theoretical\ncharacterizations of frequentist variational approximations to understand\nasymptotic properties of VB. In summary, we prove that (1) the VB posterior\nconverges to the Kullback-Leibler (KL) minimizer of a normal distribution,\ncentered at the truth and (2) the corresponding variational expectation of the\nparameter is consistent and asymptotically normal. As applications of the\ntheorem, we derive asymptotic properties of VB posteriors in Bayesian mixture\nmodels, Bayesian generalized linear mixed models, and Bayesian stochastic block\nmodels. We conduct a simulation study to illustrate these theoretical results.\n", "versions": [{"version": "v1", "created": "Tue, 9 May 2017 17:30:16 GMT"}, {"version": "v2", "created": "Thu, 24 May 2018 14:41:19 GMT"}, {"version": "v3", "created": "Thu, 8 Jul 2021 01:16:04 GMT"}], "update_date": "2021-07-09", "authors_parsed": [["Wang", "Yixin", ""], ["Blei", "David M.", ""]]}, {"id": "1705.03455", "submitter": "Ankur Bapna", "authors": "Ankur Bapna, Gokhan Tur, Dilek Hakkani-Tur, Larry Heck", "title": "Sequential Dialogue Context Modeling for Spoken Language Understanding", "comments": "8 + 2 pages, Updated 10/17: Updated typos in abstract, Updated 07/07:\n  Updated Title, abstract and few minor changes", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Spoken Language Understanding (SLU) is a key component of goal oriented\ndialogue systems that would parse user utterances into semantic frame\nrepresentations. Traditionally SLU does not utilize the dialogue history beyond\nthe previous system turn and contextual ambiguities are resolved by the\ndownstream components. In this paper, we explore novel approaches for modeling\ndialogue context in a recurrent neural network (RNN) based language\nunderstanding system. We propose the Sequential Dialogue Encoder Network, that\nallows encoding context from the dialogue history in chronological order. We\ncompare the performance of our proposed architecture with two context models,\none that uses just the previous turn context and another that encodes dialogue\ncontext in a memory network, but loses the order of utterances in the dialogue\nhistory. Experiments with a multi-domain dialogue dataset demonstrate that the\nproposed architecture results in reduced semantic frame error rates.\n", "versions": [{"version": "v1", "created": "Mon, 8 May 2017 20:57:30 GMT"}, {"version": "v2", "created": "Thu, 11 May 2017 00:21:13 GMT"}, {"version": "v3", "created": "Fri, 7 Jul 2017 21:35:02 GMT"}], "update_date": "2017-07-11", "authors_parsed": [["Bapna", "Ankur", ""], ["Tur", "Gokhan", ""], ["Hakkani-Tur", "Dilek", ""], ["Heck", "Larry", ""]]}, {"id": "1705.03497", "submitter": "Honglun Zhang", "authors": "Honglun Zhang, Haiyang Wang, Xiaming Chen, Yongkun Wang, Yaohui Jin", "title": "OMNIRank: Risk Quantification for P2P Platforms with Deep Learning", "comments": "9 pages, in Chinese, 7 figures, CCFBD2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  P2P lending presents as an innovative and flexible alternative for\nconventional lending institutions like banks, where lenders and borrowers\ndirectly make transactions and benefit each other without complicated\nverifications. However, due to lack of specialized laws, delegated monitoring\nand effective managements, P2P platforms may spawn potential risks, such as\nwithdraw failures, investigation involvements and even runaway bosses, which\ncause great losses to lenders and are especially serious and notorious in\nChina. Although there are abundant public information and data available on the\nInternet related to P2P platforms, challenges of multi-sourcing and\nheterogeneity matter. In this paper, we promote a novel deep learning model,\nOMNIRank, which comprehends multi-dimensional features of P2P platforms for\nrisk quantification and produces scores for ranking. We first construct a\nlarge-scale flexible crawling framework and obtain great amounts of\nmulti-source heterogeneous data of domestic P2P platforms since 2007 from the\nInternet. Purifications like duplication and noise removal, null handing,\nformat unification and fusion are applied to improve data qualities. Then we\nextract deep features of P2P platforms via text comprehension, topic modeling,\nknowledge graph and sentiment analysis, which are delivered as inputs to\nOMNIRank, a deep learning model for risk quantification of P2P platforms.\nFinally, according to rankings generated by OMNIRank, we conduct flourish data\nvisualizations and interactions, providing lenders with comprehensive\ninformation supports, decision suggestions and safety guarantees.\n", "versions": [{"version": "v1", "created": "Thu, 27 Apr 2017 03:15:38 GMT"}], "update_date": "2017-05-11", "authors_parsed": [["Zhang", "Honglun", ""], ["Wang", "Haiyang", ""], ["Chen", "Xiaming", ""], ["Wang", "Yongkun", ""], ["Jin", "Yaohui", ""]]}, {"id": "1705.03508", "submitter": "Hamid Reza Hassanzadeh", "authors": "Hamid Reza Hassanzadeh, Ying Sha, May D. Wang", "title": "DeepDeath: Learning to Predict the Underlying Cause of Death with Big\n  Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multiple cause-of-death data provides a valuable source of information that\ncan be used to enhance health standards by predicting health related\ntrajectories in societies with large populations. These data are often\navailable in large quantities across U.S. states and require Big Data\ntechniques to uncover complex hidden patterns. We design two different classes\nof models suitable for large-scale analysis of mortality data, a Hadoop-based\nensemble of random forests trained over N-grams, and the DeepDeath, a deep\nclassifier based on the recurrent neural network (RNN). We apply both classes\nto the mortality data provided by the National Center for Health Statistics and\nshow that while both perform significantly better than the random classifier,\nthe deep model that utilizes long short-term memory networks (LSTMs), surpasses\nthe N-gram based models and is capable of learning the temporal aspect of the\ndata without a need for building ad-hoc, expert-driven features.\n", "versions": [{"version": "v1", "created": "Sat, 6 May 2017 17:01:57 GMT"}], "update_date": "2017-05-11", "authors_parsed": [["Hassanzadeh", "Hamid Reza", ""], ["Sha", "Ying", ""], ["Wang", "May D.", ""]]}, {"id": "1705.03520", "submitter": "Jaeyoung Lee", "authors": "Jaeyoung Lee and Richard S. Sutton", "title": "Policy Iterations for Reinforcement Learning Problems in Continuous Time\n  and Space -- Fundamental Theory and Methods", "comments": "To appear in Automatica. All the Appendices are provided", "journal-ref": "Automatica vol. 126, 109421 (2021)", "doi": "10.1016/j.automatica.2020.109421", "report-no": null, "categories": "cs.AI cs.LG cs.SY eess.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Policy iteration (PI) is a recursive process of policy evaluation and\nimprovement for solving an optimal decision-making/control problem, or in other\nwords, a reinforcement learning (RL) problem. PI has also served as the\nfundamental for developing RL methods. In this paper, we propose two PI\nmethods, called differential PI (DPI) and integral PI (IPI), and their\nvariants, for a general RL framework in continuous time and space (CTS), where\nthe environment is modeled by a system of ordinary differential equations\n(ODEs). The proposed methods inherit the current ideas of PI in classical RL\nand optimal control and theoretically support the existing RL algorithms in\nCTS: TD-learning and value-gradient-based (VGB) greedy policy update. We also\nprovide case studies including 1) discounted RL and 2) optimal control tasks.\nFundamental mathematical properties -- admissibility, uniqueness of the\nsolution to the Bellman equation (BE), monotone improvement, convergence, and\noptimality of the solution to the Hamilton-Jacobi-Bellman equation (HJBE) --\nare all investigated in-depth and improved from the existing theory, along with\nthe general and case studies. Finally, the proposed ones are simulated with an\ninverted-pendulum model and their model-based and partially model-free\nimplementations to support the theory and further investigate them beyond.\n", "versions": [{"version": "v1", "created": "Tue, 9 May 2017 20:01:34 GMT"}, {"version": "v2", "created": "Sat, 31 Oct 2020 16:19:02 GMT"}], "update_date": "2021-04-06", "authors_parsed": [["Lee", "Jaeyoung", ""], ["Sutton", "Richard S.", ""]]}, {"id": "1705.03540", "submitter": "Michael Lash", "authors": "Michael T. Lash, Jason Slater, Philip M. Polgreen, and Alberto M.\n  Segre", "title": "A Large-Scale Exploration of Factors Affecting Hand Hygiene Compliance\n  Using Linear Predictive Models", "comments": "Accepted to ICHI 2017. 8 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.LG stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This large-scale study, consisting of 24.5 million hand hygiene opportunities\nspanning 19 distinct facilities in 10 different states, uses linear predictive\nmodels to expose factors that may affect hand hygiene compliance. We examine\nthe use of features such as temperature, relative humidity, influenza severity,\nday/night shift, federal holidays and the presence of new residents in\npredicting daily hand hygiene compliance. The results suggest that colder\ntemperatures and federal holidays have an adverse effect on hand hygiene\ncompliance rates, and that individual cultures and attitudes regarding hand\nhygiene seem to exist among facilities.\n", "versions": [{"version": "v1", "created": "Sat, 6 May 2017 15:57:30 GMT"}, {"version": "v2", "created": "Fri, 7 Jul 2017 18:15:26 GMT"}], "update_date": "2017-07-11", "authors_parsed": [["Lash", "Michael T.", ""], ["Slater", "Jason", ""], ["Polgreen", "Philip M.", ""], ["Segre", "Alberto M.", ""]]}, {"id": "1705.03550", "submitter": "Vincenzo Lomonaco", "authors": "Vincenzo Lomonaco and Davide Maltoni", "title": "CORe50: a New Dataset and Benchmark for Continuous Object Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Continuous/Lifelong learning of high-dimensional data streams is a\nchallenging research problem. In fact, fully retraining models each time new\ndata become available is infeasible, due to computational and storage issues,\nwhile na\\\"ive incremental strategies have been shown to suffer from\ncatastrophic forgetting. In the context of real-world object recognition\napplications (e.g., robotic vision), where continuous learning is crucial, very\nfew datasets and benchmarks are available to evaluate and compare emerging\ntechniques. In this work we propose a new dataset and benchmark CORe50,\nspecifically designed for continuous object recognition, and introduce baseline\napproaches for different continuous learning scenarios.\n", "versions": [{"version": "v1", "created": "Tue, 9 May 2017 21:32:19 GMT"}], "update_date": "2017-05-11", "authors_parsed": [["Lomonaco", "Vincenzo", ""], ["Maltoni", "Davide", ""]]}, {"id": "1705.03556", "submitter": "Hamed Zamani", "authors": "Hamed Zamani, W. Bruce Croft", "title": "Relevance-based Word Embedding", "comments": "to appear in the proceedings of The 40th International ACM SIGIR\n  Conference on Research and Development in Information Retrieval (SIGIR '17)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning a high-dimensional dense representation for vocabulary terms, also\nknown as a word embedding, has recently attracted much attention in natural\nlanguage processing and information retrieval tasks. The embedding vectors are\ntypically learned based on term proximity in a large corpus. This means that\nthe objective in well-known word embedding algorithms, e.g., word2vec, is to\naccurately predict adjacent word(s) for a given word or context. However, this\nobjective is not necessarily equivalent to the goal of many information\nretrieval (IR) tasks. The primary objective in various IR tasks is to capture\nrelevance instead of term proximity, syntactic, or even semantic similarity.\nThis is the motivation for developing unsupervised relevance-based word\nembedding models that learn word representations based on query-document\nrelevance information. In this paper, we propose two learning models with\ndifferent objective functions; one learns a relevance distribution over the\nvocabulary set for each query, and the other classifies each term as belonging\nto the relevant or non-relevant class for each query. To train our models, we\nused over six million unique queries and the top ranked documents retrieved in\nresponse to each query, which are assumed to be relevant to the query. We\nextrinsically evaluate our learned word representation models using two IR\ntasks: query expansion and query classification. Both query expansion\nexperiments on four TREC collections and query classification experiments on\nthe KDD Cup 2005 dataset suggest that the relevance-based word embedding models\nsignificantly outperform state-of-the-art proximity-based embedding models,\nsuch as word2vec and GloVe.\n", "versions": [{"version": "v1", "created": "Tue, 9 May 2017 22:09:01 GMT"}, {"version": "v2", "created": "Sun, 16 Jul 2017 22:11:57 GMT"}], "update_date": "2017-07-18", "authors_parsed": [["Zamani", "Hamed", ""], ["Croft", "W. Bruce", ""]]}, {"id": "1705.03557", "submitter": "Ahmed Khalifa", "authors": "Ahmed Khalifa, Gabriella A. B. Barros, Julian Togelius", "title": "DeepTingle", "comments": "8 pages, 7 figures and 1 table. Published at ICCC 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  DeepTingle is a text prediction and classification system trained on the\ncollected works of the renowned fantastic gay erotica author Chuck Tingle.\nWhereas the writing assistance tools you use everyday (in the form of\npredictive text, translation, grammar checking and so on) are trained on\ngeneric, purportedly \"neutral\" datasets, DeepTingle is trained on a very\nspecific, internally consistent but externally arguably eccentric dataset. This\nallows us to foreground and confront the norms embedded in data-driven\ncreativity and productivity assistance tools. As such tools effectively\nfunction as extensions of our cognition into technology, it is important to\nidentify the norms they embed within themselves and, by extension, us.\nDeepTingle is realized as a web application based on LSTM networks and the\nGloVe word embedding, implemented in JavaScript with Keras-JS.\n", "versions": [{"version": "v1", "created": "Tue, 9 May 2017 22:12:19 GMT"}, {"version": "v2", "created": "Wed, 27 Mar 2019 20:14:32 GMT"}], "update_date": "2019-03-29", "authors_parsed": [["Khalifa", "Ahmed", ""], ["Barros", "Gabriella A. B.", ""], ["Togelius", "Julian", ""]]}, {"id": "1705.03562", "submitter": "Steven Hansen", "authors": "Steven Stenberg Hansen", "title": "Deep Episodic Value Iteration for Model-based Meta-Reinforcement\n  Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a new deep meta reinforcement learner, which we call Deep Episodic\nValue Iteration (DEVI). DEVI uses a deep neural network to learn a similarity\nmetric for a non-parametric model-based reinforcement learning algorithm. Our\nmodel is trained end-to-end via back-propagation. Despite being trained using\nthe model-free Q-learning objective, we show that DEVI's model-based internal\nstructure provides `one-shot' transfer to changes in reward and transition\nstructure, even for tasks with very high-dimensional state spaces.\n", "versions": [{"version": "v1", "created": "Tue, 9 May 2017 22:59:18 GMT"}], "update_date": "2017-05-11", "authors_parsed": [["Hansen", "Steven Stenberg", ""]]}, {"id": "1705.03566", "submitter": "Mostafa Rahmani", "authors": "Mostafa Rahmani, George Atia", "title": "Spatial Random Sampling: A Structure-Preserving Data Sketching Tool", "comments": null, "journal-ref": null, "doi": "10.1109/LSP.2017.2723472", "report-no": null, "categories": "cs.LG stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Random column sampling is not guaranteed to yield data sketches that preserve\nthe underlying structures of the data and may not sample sufficiently from\nless-populated data clusters. Also, adaptive sampling can often provide\naccurate low rank approximations, yet may fall short of producing descriptive\ndata sketches, especially when the cluster centers are linearly dependent.\nMotivated by that, this paper introduces a novel randomized column sampling\ntool dubbed Spatial Random Sampling (SRS), in which data points are sampled\nbased on their proximity to randomly sampled points on the unit sphere. The\nmost compelling feature of SRS is that the corresponding probability of\nsampling from a given data cluster is proportional to the surface area the\ncluster occupies on the unit sphere, independently from the size of the cluster\npopulation. Although it is fully randomized, SRS is shown to provide\ndescriptive and balanced data representations. The proposed idea addresses a\npressing need in data science and holds potential to inspire many novel\napproaches for analysis of big data.\n", "versions": [{"version": "v1", "created": "Tue, 9 May 2017 23:31:15 GMT"}, {"version": "v2", "created": "Wed, 12 Jul 2017 23:19:02 GMT"}], "update_date": "2017-10-11", "authors_parsed": [["Rahmani", "Mostafa", ""], ["Atia", "George", ""]]}, {"id": "1705.03572", "submitter": "Alexander Wong", "authors": "Mohammad Javad Shafiee, Audrey G. Chung, Farzad Khalvati, Masoom A.\n  Haider, and Alexander Wong", "title": "Discovery Radiomics via Evolutionary Deep Radiomic Sequencer Discovery\n  for Pathologically-Proven Lung Cancer Detection", "comments": "26 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While lung cancer is the second most diagnosed form of cancer in men and\nwomen, a sufficiently early diagnosis can be pivotal in patient survival rates.\nImaging-based, or radiomics-driven, detection methods have been developed to\naid diagnosticians, but largely rely on hand-crafted features which may not\nfully encapsulate the differences between cancerous and healthy tissue.\nRecently, the concept of discovery radiomics was introduced, where custom\nabstract features are discovered from readily available imaging data. We\npropose a novel evolutionary deep radiomic sequencer discovery approach based\non evolutionary deep intelligence. Motivated by patient privacy concerns and\nthe idea of operational artificial intelligence, the evolutionary deep radiomic\nsequencer discovery approach organically evolves increasingly more efficient\ndeep radiomic sequencers that produce significantly more compact yet similarly\ndescriptive radiomic sequences over multiple generations. As a result, this\nframework improves operational efficiency and enables diagnosis to be run\nlocally at the radiologist's computer while maintaining detection accuracy. We\nevaluated the evolved deep radiomic sequencer (EDRS) discovered via the\nproposed evolutionary deep radiomic sequencer discovery framework against\nstate-of-the-art radiomics-driven and discovery radiomics methods using\nclinical lung CT data with pathologically-proven diagnostic data from the\nLIDC-IDRI dataset. The evolved deep radiomic sequencer shows improved\nsensitivity (93.42%), specificity (82.39%), and diagnostic accuracy (88.78%)\nrelative to previous radiomics approaches.\n", "versions": [{"version": "v1", "created": "Wed, 10 May 2017 00:20:23 GMT"}, {"version": "v2", "created": "Fri, 20 Oct 2017 00:01:27 GMT"}], "update_date": "2017-10-23", "authors_parsed": [["Shafiee", "Mohammad Javad", ""], ["Chung", "Audrey G.", ""], ["Khalvati", "Farzad", ""], ["Haider", "Masoom A.", ""], ["Wong", "Alexander", ""]]}, {"id": "1705.03595", "submitter": "Hirokatsu Kataoka", "authors": "Hirokatsu Kataoka, Kaori Abe, Akio Nakamura, Yutaka Satoh", "title": "Collaborative Descriptors: Convolutional Maps for Preprocessing", "comments": "CVPR 2017 Workshop Submission", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The paper presents a novel concept for collaborative descriptors between\ndeeply learned and hand-crafted features. To achieve this concept, we apply\nconvolutional maps for pre-processing, namely the convovlutional maps are used\nas input of hand-crafted features. We recorded an increase in the performance\nrate of +17.06 % (multi-class object recognition) and +24.71 % (car detection)\nfrom grayscale input to convolutional maps. Although the framework is\nstraight-forward, the concept should be inherited for an improved\nrepresentation.\n", "versions": [{"version": "v1", "created": "Wed, 10 May 2017 03:04:48 GMT"}], "update_date": "2017-05-11", "authors_parsed": [["Kataoka", "Hirokatsu", ""], ["Abe", "Kaori", ""], ["Nakamura", "Akio", ""], ["Satoh", "Yutaka", ""]]}, {"id": "1705.03613", "submitter": "Hassan Ismkhan", "authors": "Hassan Ismkhan", "title": "An initialization method for the k-means using the concept of useful\n  nearest centers", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The aim of the k-means is to minimize squared sum of Euclidean distance from\nthe mean (SSEDM) of each cluster. The k-means can effectively optimize this\nfunction, but it is too sensitive for initial centers (seeds). This paper\nproposed a method for initialization of the k-means using the concept of useful\nnearest center for each data point.\n", "versions": [{"version": "v1", "created": "Wed, 10 May 2017 06:19:04 GMT"}], "update_date": "2017-05-11", "authors_parsed": [["Ismkhan", "Hassan", ""]]}, {"id": "1705.03633", "submitter": "Justin Johnson", "authors": "Justin Johnson, Bharath Hariharan, Laurens van der Maaten, Judy\n  Hoffman, Li Fei-Fei, C. Lawrence Zitnick, Ross Girshick", "title": "Inferring and Executing Programs for Visual Reasoning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Existing methods for visual reasoning attempt to directly map inputs to\noutputs using black-box architectures without explicitly modeling the\nunderlying reasoning processes. As a result, these black-box models often learn\nto exploit biases in the data rather than learning to perform visual reasoning.\nInspired by module networks, this paper proposes a model for visual reasoning\nthat consists of a program generator that constructs an explicit representation\nof the reasoning process to be performed, and an execution engine that executes\nthe resulting program to produce an answer. Both the program generator and the\nexecution engine are implemented by neural networks, and are trained using a\ncombination of backpropagation and REINFORCE. Using the CLEVR benchmark for\nvisual reasoning, we show that our model significantly outperforms strong\nbaselines and generalizes better in a variety of settings.\n", "versions": [{"version": "v1", "created": "Wed, 10 May 2017 07:08:23 GMT"}], "update_date": "2017-05-11", "authors_parsed": [["Johnson", "Justin", ""], ["Hariharan", "Bharath", ""], ["van der Maaten", "Laurens", ""], ["Hoffman", "Judy", ""], ["Fei-Fei", "Li", ""], ["Zitnick", "C. Lawrence", ""], ["Girshick", "Ross", ""]]}, {"id": "1705.03670", "submitter": "Lantian Li Mr.", "authors": "Lantian Li, Yixiang Chen, Ying Shi, Zhiyuan Tang, Dong Wang", "title": "Deep Speaker Feature Learning for Text-independent Speaker Verification", "comments": "deep neural networks, speaker verification, speaker feature", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently deep neural networks (DNNs) have been used to learn speaker\nfeatures. However, the quality of the learned features is not sufficiently\ngood, so a complex back-end model, either neural or probabilistic, has to be\nused to address the residual uncertainty when applied to speaker verification,\njust as with raw features. This paper presents a convolutional time-delay deep\nneural network structure (CT-DNN) for speaker feature learning. Our\nexperimental results on the Fisher database demonstrated that this CT-DNN can\nproduce high-quality speaker features: even with a single feature (0.3 seconds\nincluding the context), the EER can be as low as 7.68%. This effectively\nconfirmed that the speaker trait is largely a deterministic short-time property\nrather than a long-time distributional pattern, and therefore can be extracted\nfrom just dozens of frames.\n", "versions": [{"version": "v1", "created": "Wed, 10 May 2017 09:30:42 GMT"}], "update_date": "2017-05-11", "authors_parsed": [["Li", "Lantian", ""], ["Chen", "Yixiang", ""], ["Shi", "Ying", ""], ["Tang", "Zhiyuan", ""], ["Wang", "Dong", ""]]}, {"id": "1705.03771", "submitter": "Feng Nan", "authors": "Feng Nan and Venkatesh Saligrama", "title": "Comments on the proof of adaptive submodular function minimization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We point out an issue with Theorem 5 appearing in \"Group-based active query\nselection for rapid diagnosis in time-critical situations\". Theorem 5 bounds\nthe expected number of queries for a greedy algorithm to identify the class of\nan item within a constant factor of optimal. The Theorem is based on\ncorrectness of a result on minimization of adaptive submodular functions. We\npresent an example that shows that a critical step in Theorem A.11 of \"Adaptive\nSubmodularity: Theory and Applications in Active Learning and Stochastic\nOptimization\" is incorrect.\n", "versions": [{"version": "v1", "created": "Wed, 10 May 2017 13:52:31 GMT"}], "update_date": "2017-05-11", "authors_parsed": [["Nan", "Feng", ""], ["Saligrama", "Venkatesh", ""]]}, {"id": "1705.03800", "submitter": "Pierre-Francois Marteau", "authors": "Pierre-Fran\\c{c}ois Marteau, Saeid Soheily-Khah, Nicolas B\\'echet", "title": "Hybrid Isolation Forest - Application to Intrusion Detection", "comments": "24 pages, working paper", "journal-ref": null, "doi": null, "report-no": "IRISA/EXPRESSION/2017.1", "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  From the identification of a drawback in the Isolation Forest (IF) algorithm\nthat limits its use in the scope of anomaly detection, we propose two\nextensions that allow to firstly overcome the previously mention limitation and\nsecondly to provide it with some supervised learning capability. The resulting\nHybrid Isolation Forest (HIF) that we propose is first evaluated on a synthetic\ndataset to analyze the effect of the new meta-parameters that are introduced\nand verify that the addressed limitation of the IF algorithm is effectively\novercame. We hen compare the two algorithms on the ISCX benchmark dataset, in\nthe context of a network intrusion detection application. Our experiments show\nthat HIF outperforms IF, but also challenges the 1-class and 2-classes SVM\nbaselines with computational efficiency.\n", "versions": [{"version": "v1", "created": "Wed, 10 May 2017 14:42:30 GMT"}], "update_date": "2018-10-30", "authors_parsed": [["Marteau", "Pierre-Fran\u00e7ois", ""], ["Soheily-Khah", "Saeid", ""], ["B\u00e9chet", "Nicolas", ""]]}, {"id": "1705.03821", "submitter": "Djallel Bouneffouf", "authors": "Djallel Bouneffouf, Irina Rish, Guillermo A. Cecchi, Raphael Feraud", "title": "Context Attentive Bandits: Contextual Bandit with Restricted Context", "comments": "IJCAI 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider a novel formulation of the multi-armed bandit model, which we\ncall the contextual bandit with restricted context, where only a limited number\nof features can be accessed by the learner at every iteration. This novel\nformulation is motivated by different online problems arising in clinical\ntrials, recommender systems and attention modeling. Herein, we adapt the\nstandard multi-armed bandit algorithm known as Thompson Sampling to take\nadvantage of our restricted context setting, and propose two novel algorithms,\ncalled the Thompson Sampling with Restricted Context(TSRC) and the Windows\nThompson Sampling with Restricted Context(WTSRC), for handling stationary and\nnonstationary environments, respectively. Our empirical results demonstrate\nadvantages of the proposed approaches on several real-life datasets\n", "versions": [{"version": "v1", "created": "Wed, 10 May 2017 15:32:36 GMT"}, {"version": "v2", "created": "Wed, 7 Jun 2017 18:40:28 GMT"}], "update_date": "2017-06-09", "authors_parsed": [["Bouneffouf", "Djallel", ""], ["Rish", "Irina", ""], ["Cecchi", "Guillermo A.", ""], ["Feraud", "Raphael", ""]]}, {"id": "1705.03822", "submitter": "Sabrina Klos (N\\'ee M\\\"uller)", "authors": "Sabrina Klos (n\\'ee M\\\"uller), Cem Tekin, Mihaela van der Schaar, Anja\n  Klein", "title": "Context-Aware Hierarchical Online Learning for Performance Maximization\n  in Mobile Crowdsourcing", "comments": "18 pages, 10 figures", "journal-ref": "IEEE/ACM Transactions on Networking, vol. 26, no. 3, pp.\n  1334-1347, June 2018", "doi": "10.1109/TNET.2018.2828415", "report-no": null, "categories": "cs.LG cs.HC cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In mobile crowdsourcing (MCS), mobile users accomplish outsourced human\nintelligence tasks. MCS requires an appropriate task assignment strategy, since\ndifferent workers may have different performance in terms of acceptance rate\nand quality. Task assignment is challenging, since a worker's performance (i)\nmay fluctuate, depending on both the worker's current personal context and the\ntask context, (ii) is not known a priori, but has to be learned over time.\nMoreover, learning context-specific worker performance requires access to\ncontext information, which may not be available at a central entity due to\ncommunication overhead or privacy concerns. Additionally, evaluating worker\nperformance might require costly quality assessments. In this paper, we propose\na context-aware hierarchical online learning algorithm addressing the problem\nof performance maximization in MCS. In our algorithm, a local controller (LC)\nin the mobile device of a worker regularly observes the worker's context,\nher/his decisions to accept or decline tasks and the quality in completing\ntasks. Based on these observations, the LC regularly estimates the worker's\ncontext-specific performance. The mobile crowdsourcing platform (MCSP) then\nselects workers based on performance estimates received from the LCs. This\nhierarchical approach enables the LCs to learn context-specific worker\nperformance and it enables the MCSP to select suitable workers. In addition,\nour algorithm preserves worker context locally, and it keeps the number of\nrequired quality assessments low. We prove that our algorithm converges to the\noptimal task assignment strategy. Moreover, the algorithm outperforms simpler\ntask assignment strategies in experiments based on synthetic and real data.\n", "versions": [{"version": "v1", "created": "Wed, 10 May 2017 15:34:54 GMT"}, {"version": "v2", "created": "Thu, 8 Nov 2018 09:33:39 GMT"}], "update_date": "2018-11-09", "authors_parsed": [["Klos", "Sabrina", "", "n\u00e9e M\u00fcller"], ["Tekin", "Cem", ""], ["van der Schaar", "Mihaela", ""], ["Klein", "Anja", ""]]}, {"id": "1705.03881", "submitter": "Roberto Gonzalez", "authors": "Roberto Gonzalez, Filipe Manco, Alberto Garcia-Duran, Jose Mendes,\n  Felipe Huici, Saverio Niccolini, Mathias Niepert", "title": "Net2Vec: Deep Learning for the Network", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present Net2Vec, a flexible high-performance platform that allows the\nexecution of deep learning algorithms in the communication network. Net2Vec is\nable to capture data from the network at more than 60Gbps, transform it into\nmeaningful tuples and apply predictions over the tuples in real time. This\nplatform can be used for different purposes ranging from traffic classification\nto network performance analysis.\n  Finally, we showcase the use of Net2Vec by implementing and testing a\nsolution able to profile network users at line rate using traces coming from a\nreal network. We show that the use of deep learning for this case outperforms\nthe baseline method both in terms of accuracy and performance.\n", "versions": [{"version": "v1", "created": "Wed, 10 May 2017 13:28:19 GMT"}], "update_date": "2017-05-12", "authors_parsed": [["Gonzalez", "Roberto", ""], ["Manco", "Filipe", ""], ["Garcia-Duran", "Alberto", ""], ["Mendes", "Jose", ""], ["Huici", "Felipe", ""], ["Niccolini", "Saverio", ""], ["Niepert", "Mathias", ""]]}, {"id": "1705.03921", "submitter": "Ronny Ronen", "authors": "Ronny Ronen", "title": "Why & When Deep Learning Works: Looking Inside Deep Learnings", "comments": "This paper is the preface part of the \"Why & When Deep Learning works\n  looking inside Deep Learning\" ICRI-CI paper bundle", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Intel Collaborative Research Institute for Computational Intelligence\n(ICRI-CI) has been heavily supporting Machine Learning and Deep Learning\nresearch from its foundation in 2012. We have asked six leading ICRI-CI Deep\nLearning researchers to address the challenge of \"Why & When Deep Learning\nworks\", with the goal of looking inside Deep Learning, providing insights on\nhow deep networks function, and uncovering key observations on their\nexpressiveness, limitations, and potential. The output of this challenge\nresulted in five papers that address different facets of deep learning. These\ndifferent facets include a high-level understating of why and when deep\nnetworks work (and do not work), the impact of geometry on the expressiveness\nof deep networks, and making deep networks interpretable.\n", "versions": [{"version": "v1", "created": "Wed, 10 May 2017 18:52:26 GMT"}], "update_date": "2017-05-12", "authors_parsed": [["Ronen", "Ronny", ""]]}, {"id": "1705.03967", "submitter": "Richard Sutton", "authors": "Adam White and Richard S. Sutton", "title": "GQ($\\lambda$) Quick Reference and Implementation Guide", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This document should serve as a quick reference for and guide to the\nimplementation of linear GQ($\\lambda$), a gradient-based off-policy\ntemporal-difference learning algorithm. Explanation of the intuition and theory\nbehind the algorithm are provided elsewhere (e.g., Maei & Sutton 2010, Maei\n2011). If you questions or concerns about the content in this document or the\nattached java code please email Adam White (adam.white@ualberta.ca).\n  The code is provided as part of the source files in the arXiv submission.\n", "versions": [{"version": "v1", "created": "Wed, 10 May 2017 22:43:11 GMT"}], "update_date": "2017-05-12", "authors_parsed": [["White", "Adam", ""], ["Sutton", "Richard S.", ""]]}, {"id": "1705.03998", "submitter": "Jiahui Liu", "authors": "YaoGong Zhang, YingJie Xu, Xin Fan, YuXiang Hong, Jiahui Liu, ZhiCheng\n  He, YaLou Huang and MaoQiang Xie", "title": "Mining Functional Modules by Multiview-NMF of Phenome-Genome Association", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG q-bio.QM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Background: Mining gene modules from genomic data is an important step to\ndetect gene members of pathways or other relations such as protein-protein\ninteractions. In this work, we explore the plausibility of detecting gene\nmodules by factorizing gene-phenotype associations from a phenotype ontology\nrather than the conventionally used gene expression data. In particular, the\nhierarchical structure of ontology has not been sufficiently utilized in\nclustering genes while functionally related genes are consistently associated\nwith phenotypes on the same path in the phenotype ontology. Results: We propose\na hierarchal Nonnegative Matrix Factorization (NMF)-based method, called\nConsistent Multiple Nonnegative Matrix Factorization (CMNMF), to factorize\ngenome-phenome association matrix at two levels of the hierarchical structure\nin phenotype ontology for mining gene functional modules. CMNMF constrains the\ngene clusters from the association matrices at two consecutive levels to be\nconsistent since the genes are annotated with both the child phenotype and the\nparent phenotype in the consecutive levels. CMNMF also restricts the identified\nphenotype clusters to be densely connected in the phenotype ontology hierarchy.\nIn the experiments on mining functionally related genes from mouse phenotype\nontology and human phenotype ontology, CMNMF effectively improved clustering\nperformance over the baseline methods. Gene ontology enrichment analysis was\nalso conducted to reveal interesting gene modules. Conclusions: Utilizing the\ninformation in the hierarchical structure of phenotype ontology, CMNMF can\nidentify functional gene modules with more biological significance than the\nconventional methods. CMNMF could also be a better tool for predicting members\nof gene pathways and protein-protein interactions. Availability:\nhttps://github.com/nkiip/CMNMF\n", "versions": [{"version": "v1", "created": "Thu, 11 May 2017 03:33:26 GMT"}], "update_date": "2017-05-12", "authors_parsed": [["Zhang", "YaoGong", ""], ["Xu", "YingJie", ""], ["Fan", "Xin", ""], ["Hong", "YuXiang", ""], ["Liu", "Jiahui", ""], ["He", "ZhiCheng", ""], ["Huang", "YaLou", ""], ["Xie", "MaoQiang", ""]]}, {"id": "1705.04138", "submitter": "Yue Yu", "authors": "Yue Yu, Longbo Huang", "title": "Fast Stochastic Variance Reduced ADMM for Stochastic Composition\n  Optimization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the stochastic composition optimization problem proposed in\n\\cite{wang2017stochastic}, which has applications ranging from estimation to\nstatistical and machine learning. We propose the first ADMM-based algorithm\nnamed com-SVR-ADMM, and show that com-SVR-ADMM converges linearly for strongly\nconvex and Lipschitz smooth objectives, and has a convergence rate of $O( \\log\nS/S)$, which improves upon the $O(S^{-4/9})$ rate in\n\\cite{wang2016accelerating} when the objective is convex and Lipschitz smooth.\nMoreover, com-SVR-ADMM possesses a rate of $O(1/\\sqrt{S})$ when the objective\nis convex but without Lipschitz smoothness. We also conduct experiments and\nshow that it outperforms existing algorithms.\n", "versions": [{"version": "v1", "created": "Thu, 11 May 2017 12:50:23 GMT"}, {"version": "v2", "created": "Mon, 22 May 2017 02:28:15 GMT"}], "update_date": "2017-05-23", "authors_parsed": [["Yu", "Yue", ""], ["Huang", "Longbo", ""]]}, {"id": "1705.04146", "submitter": "Wang Ling", "authors": "Wang Ling, Dani Yogatama, Chris Dyer, Phil Blunsom", "title": "Program Induction by Rationale Generation : Learning to Solve and\n  Explain Algebraic Word Problems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Solving algebraic word problems requires executing a series of arithmetic\noperations---a program---to obtain a final answer. However, since programs can\nbe arbitrarily complicated, inducing them directly from question-answer pairs\nis a formidable challenge. To make this task more feasible, we solve these\nproblems by generating answer rationales, sequences of natural language and\nhuman-readable mathematical expressions that derive the final answer through a\nseries of small steps. Although rationales do not explicitly specify programs,\nthey provide a scaffolding for their structure via intermediate milestones. To\nevaluate our approach, we have created a new 100,000-sample dataset of\nquestions, answers and rationales. Experimental results show that indirect\nsupervision of program learning via answer rationales is a promising strategy\nfor inducing arithmetic programs.\n", "versions": [{"version": "v1", "created": "Thu, 11 May 2017 13:04:47 GMT"}, {"version": "v2", "created": "Tue, 10 Oct 2017 14:57:26 GMT"}, {"version": "v3", "created": "Mon, 23 Oct 2017 16:45:03 GMT"}], "update_date": "2017-10-24", "authors_parsed": [["Ling", "Wang", ""], ["Yogatama", "Dani", ""], ["Dyer", "Chris", ""], ["Blunsom", "Phil", ""]]}, {"id": "1705.04185", "submitter": "Sina Ghiassian", "authors": "Sina Ghiassian, Banafsheh Rafiee, Richard S. Sutton", "title": "A First Empirical Study of Emphatic Temporal Difference Learning", "comments": "5 pages, Accepted to NIPS Continual Learning and Deep Networks\n  workshop, 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we present the first empirical study of the emphatic\ntemporal-difference learning algorithm (ETD), comparing it with conventional\ntemporal-difference learning, in particular, with linear TD(0), on on-policy\nand off-policy variations of the Mountain Car problem. The initial motivation\nfor developing ETD was that it has good convergence properties under off-policy\ntraining (Sutton, Mahmood and White 2016), but it is also a new algorithm for\nthe on-policy case. In both our on-policy and off-policy experiments, we found\nthat each method converged to a characteristic asymptotic level of error, with\nETD better than TD(0). TD(0) achieved a still lower error level temporarily\nbefore falling back to its higher asymptote, whereas ETD never showed this kind\nof \"bounce\". In the off-policy case (in which TD(0) is not guaranteed to\nconverge), ETD was significantly slower.\n", "versions": [{"version": "v1", "created": "Thu, 11 May 2017 13:52:52 GMT"}, {"version": "v2", "created": "Fri, 12 May 2017 16:49:38 GMT"}], "update_date": "2017-05-15", "authors_parsed": [["Ghiassian", "Sina", ""], ["Rafiee", "Banafsheh", ""], ["Sutton", "Richard S.", ""]]}, {"id": "1705.04193", "submitter": "Dylan Fagot", "authors": "Dylan Fagot, C\\'edric F\\'evotte and Herwig Wendt", "title": "Nonnegative Matrix Factorization with Transform Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Traditional NMF-based signal decomposition relies on the factorization of\nspectral data, which is typically computed by means of short-time frequency\ntransform. In this paper we propose to relax the choice of a pre-fixed\ntransform and learn a short-time orthogonal transform together with the\nfactorization. To this end, we formulate a regularized optimization problem\nreminiscent of conventional NMF, yet with the transform as additional unknown\nparameters, and design a novel block-descent algorithm enabling to find\nstationary points of this objective function. The proposed joint transform\nlearning and factorization approach is tested for two audio signal processing\nexperiments, illustrating its conceptual and practical benefits.\n", "versions": [{"version": "v1", "created": "Thu, 11 May 2017 14:12:23 GMT"}, {"version": "v2", "created": "Fri, 15 Dec 2017 12:54:29 GMT"}], "update_date": "2017-12-18", "authors_parsed": [["Fagot", "Dylan", ""], ["F\u00e9votte", "C\u00e9dric", ""], ["Wendt", "Herwig", ""]]}, {"id": "1705.04228", "submitter": "Amir Rosenfeld", "authors": "Amir Rosenfeld, John K. Tsotsos", "title": "Incremental Learning Through Deep Adaptation", "comments": "Extended version", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given an existing trained neural network, it is often desirable to learn new\ncapabilities without hindering performance of those already learned. Existing\napproaches either learn sub-optimal solutions, require joint training, or incur\na substantial increment in the number of parameters for each added domain,\ntypically as many as the original network. We propose a method called\n\\emph{Deep Adaptation Networks} (DAN) that constrains newly learned filters to\nbe linear combinations of existing ones. DANs precisely preserve performance on\nthe original domain, require a fraction (typically 13\\%, dependent on network\narchitecture) of the number of parameters compared to standard fine-tuning\nprocedures and converge in less cycles of training to a comparable or better\nlevel of performance. When coupled with standard network quantization\ntechniques, we further reduce the parameter cost to around 3\\% of the original\nwith negligible or no loss in accuracy. The learned architecture can be\ncontrolled to switch between various learned representations, enabling a single\nnetwork to solve a task from multiple different domains. We conduct extensive\nexperiments showing the effectiveness of our method on a range of image\nclassification tasks and explore different aspects of its behavior.\n", "versions": [{"version": "v1", "created": "Thu, 11 May 2017 15:04:10 GMT"}, {"version": "v2", "created": "Tue, 13 Feb 2018 19:41:40 GMT"}], "update_date": "2018-02-15", "authors_parsed": [["Rosenfeld", "Amir", ""], ["Tsotsos", "John K.", ""]]}, {"id": "1705.04249", "submitter": "Li Heng Liou", "authors": "Cheng-Shang Chang, Chia-Tai Chang, Duan-Shin Lee and Li-Heng Liou", "title": "K-sets+: a Linear-time Clustering Algorithm for Data Points with a\n  Sparse Similarity Measure", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we first propose a new iterative algorithm, called the K-sets+\nalgorithm for clustering data points in a semi-metric space, where the distance\nmeasure does not necessarily satisfy the triangular inequality. We show that\nthe K-sets+ algorithm converges in a finite number of iterations and it retains\nthe same performance guarantee as the K-sets algorithm for clustering data\npoints in a metric space. We then extend the applicability of the K-sets+\nalgorithm from data points in a semi-metric space to data points that only have\na symmetric similarity measure. Such an extension leads to great reduction of\ncomputational complexity. In particular, for an n * n similarity matrix with m\nnonzero elements in the matrix, the computational complexity of the K-sets+\nalgorithm is O((Kn + m)I), where I is the number of iterations. The memory\ncomplexity to achieve that computational complexity is O(Kn + m). As such, both\nthe computational complexity and the memory complexity are linear in n when the\nn * n similarity matrix is sparse, i.e., m = O(n). We also conduct various\nexperiments to show the effectiveness of the K-sets+ algorithm by using a\nsynthetic dataset from the stochastic block model and a real network from the\nWonderNetwork website.\n", "versions": [{"version": "v1", "created": "Thu, 11 May 2017 15:39:48 GMT"}], "update_date": "2017-05-12", "authors_parsed": [["Chang", "Cheng-Shang", ""], ["Chang", "Chia-Tai", ""], ["Lee", "Duan-Shin", ""], ["Liou", "Li-Heng", ""]]}, {"id": "1705.04282", "submitter": "Amanda Song", "authors": "Amanda Song, Linjie Li, Chad Atalla, Garrison Cottrell", "title": "Learning to see people like people", "comments": "10 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Humans make complex inferences on faces, ranging from objective properties\n(gender, ethnicity, expression, age, identity, etc) to subjective judgments\n(facial attractiveness, trustworthiness, sociability, friendliness, etc). While\nthe objective aspects of face perception have been extensively studied,\nrelatively fewer computational models have been developed for the social\nimpressions of faces. Bridging this gap, we develop a method to predict human\nimpressions of faces in 40 subjective social dimensions, using deep\nrepresentations from state-of-the-art neural networks. We find that model\nperformance grows as the human consensus on a face trait increases, and that\nmodel predictions outperform human groups in correlation with human averages.\nThis illustrates the learnability of subjective social perception of faces,\nespecially when there is high human consensus. Our system can be used to decide\nwhich photographs from a personal collection will make the best impression. The\nresults are significant for the field of social robotics, demonstrating that\nrobots can learn the subjective judgments defining the underlying fabric of\nhuman interaction.\n", "versions": [{"version": "v1", "created": "Fri, 5 May 2017 05:47:15 GMT"}], "update_date": "2017-05-12", "authors_parsed": [["Song", "Amanda", ""], ["Li", "Linjie", ""], ["Atalla", "Chad", ""], ["Cottrell", "Garrison", ""]]}, {"id": "1705.04286", "submitter": "Aydogan Ozcan", "authors": "Yair Rivenson, Yibo Zhang, Harun Gunaydin, Da Teng, Aydogan Ozcan", "title": "Phase recovery and holographic image reconstruction using deep learning\n  in neural networks", "comments": null, "journal-ref": "Light: Science and Applications, 7, e17141 (2018)", "doi": "10.1038/lsa.2017.141", "report-no": null, "categories": "cs.CV cs.IR cs.LG physics.app-ph physics.optics", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Phase recovery from intensity-only measurements forms the heart of coherent\nimaging techniques and holography. Here we demonstrate that a neural network\ncan learn to perform phase recovery and holographic image reconstruction after\nappropriate training. This deep learning-based approach provides an entirely\nnew framework to conduct holographic imaging by rapidly eliminating twin-image\nand self-interference related spatial artifacts. Compared to existing\napproaches, this neural network based method is significantly faster to\ncompute, and reconstructs improved phase and amplitude images of the objects\nusing only one hologram, i.e., requires less number of measurements in addition\nto being computationally faster. We validated this method by reconstructing\nphase and amplitude images of various samples, including blood and Pap smears,\nand tissue sections. These results are broadly applicable to any phase recovery\nproblem, and highlight that through machine learning challenging problems in\nimaging science can be overcome, providing new avenues to design powerful\ncomputational imaging systems.\n", "versions": [{"version": "v1", "created": "Wed, 10 May 2017 03:26:30 GMT"}], "update_date": "2017-12-13", "authors_parsed": [["Rivenson", "Yair", ""], ["Zhang", "Yibo", ""], ["Gunaydin", "Harun", ""], ["Teng", "Da", ""], ["Ozcan", "Aydogan", ""]]}, {"id": "1705.04293", "submitter": "Danica J. Sutherland", "authors": "Ho Chung Leon Law, Danica J. Sutherland, Dino Sejdinovic, Seth Flaxman", "title": "Bayesian Approaches to Distribution Regression", "comments": null, "journal-ref": "Proceedings of the Twenty-First International Conference on\n  Artificial Intelligence and Statistics (AISTATS 2018), PMLR 84:1167-1176", "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Distribution regression has recently attracted much interest as a generic\nsolution to the problem of supervised learning where labels are available at\nthe group level, rather than at the individual level. Current approaches,\nhowever, do not propagate the uncertainty in observations due to sampling\nvariability in the groups. This effectively assumes that small and large groups\nare estimated equally well, and should have equal weight in the final\nregression. We account for this uncertainty with a Bayesian distribution\nregression formalism, improving the robustness and performance of the model\nwhen group sizes vary. We frame our models in a neural network style, allowing\nfor simple MAP inference using backpropagation to learn the parameters, as well\nas MCMC-based inference which can fully propagate uncertainty. We demonstrate\nour approach on illustrative toy datasets, as well as on a challenging problem\nof predicting age from images.\n", "versions": [{"version": "v1", "created": "Thu, 11 May 2017 17:20:07 GMT"}, {"version": "v2", "created": "Mon, 16 Oct 2017 10:18:41 GMT"}, {"version": "v3", "created": "Thu, 22 Feb 2018 14:08:11 GMT"}, {"version": "v4", "created": "Thu, 14 Jan 2021 19:08:00 GMT"}], "update_date": "2021-01-18", "authors_parsed": [["Law", "Ho Chung Leon", ""], ["Sutherland", "Danica J.", ""], ["Sejdinovic", "Dino", ""], ["Flaxman", "Seth", ""]]}, {"id": "1705.04379", "submitter": "Alexander Jung", "authors": "Alexander Jung, Madelon Hulsebos", "title": "The Network Nullspace Property for Compressed Sensing of Big Data over\n  Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a novel condition, which we term the net- work nullspace property,\nwhich ensures accurate recovery of graph signals representing massive\nnetwork-structured datasets from few signal values. The network nullspace\nproperty couples the cluster structure of the underlying network-structure with\nthe geometry of the sampling set. Our results can be used to design efficient\nsampling strategies based on the network topology.\n", "versions": [{"version": "v1", "created": "Thu, 11 May 2017 21:21:08 GMT"}, {"version": "v2", "created": "Thu, 25 May 2017 06:54:34 GMT"}, {"version": "v3", "created": "Wed, 6 Sep 2017 13:59:19 GMT"}, {"version": "v4", "created": "Tue, 13 Mar 2018 08:16:25 GMT"}], "update_date": "2018-03-14", "authors_parsed": [["Jung", "Alexander", ""], ["Hulsebos", "Madelon", ""]]}, {"id": "1705.04524", "submitter": "Peng Su", "authors": "Peng Su, Xiao-Rong Ding, Yuan-Ting Zhang, Jing Liu, Fen Miao, Ni Zhao", "title": "Long-term Blood Pressure Prediction with Deep Recurrent Neural Networks", "comments": "To appear in IEEE BHI 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI math.DS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Existing methods for arterial blood pressure (BP) estimation directly map the\ninput physiological signals to output BP values without explicitly modeling the\nunderlying temporal dependencies in BP dynamics. As a result, these models\nsuffer from accuracy decay over a long time and thus require frequent\ncalibration. In this work, we address this issue by formulating BP estimation\nas a sequence prediction problem in which both the input and target are\ntemporal sequences. We propose a novel deep recurrent neural network (RNN)\nconsisting of multilayered Long Short-Term Memory (LSTM) networks, which are\nincorporated with (1) a bidirectional structure to access larger-scale context\ninformation of input sequence, and (2) residual connections to allow gradients\nin deep RNN to propagate more effectively. The proposed deep RNN model was\ntested on a static BP dataset, and it achieved root mean square error (RMSE) of\n3.90 and 2.66 mmHg for systolic BP (SBP) and diastolic BP (DBP) prediction\nrespectively, surpassing the accuracy of traditional BP prediction models. On a\nmulti-day BP dataset, the deep RNN achieved RMSE of 3.84, 5.25, 5.80 and 5.81\nmmHg for the 1st day, 2nd day, 4th day and 6th month after the 1st day SBP\nprediction, and 1.80, 4.78, 5.0, 5.21 mmHg for corresponding DBP prediction,\nrespectively, which outperforms all previous models with notable improvement.\nThe experimental results suggest that modeling the temporal dependencies in BP\ndynamics significantly improves the long-term BP prediction accuracy.\n", "versions": [{"version": "v1", "created": "Fri, 12 May 2017 11:53:26 GMT"}, {"version": "v2", "created": "Tue, 27 Jun 2017 15:45:23 GMT"}, {"version": "v3", "created": "Sun, 14 Jan 2018 13:56:46 GMT"}], "update_date": "2018-01-16", "authors_parsed": [["Su", "Peng", ""], ["Ding", "Xiao-Rong", ""], ["Zhang", "Yuan-Ting", ""], ["Liu", "Jing", ""], ["Miao", "Fen", ""], ["Zhao", "Ni", ""]]}, {"id": "1705.04591", "submitter": "Mahdi Soltanolkotabi", "authors": "Mahdi Soltanolkotabi", "title": "Learning ReLUs via Gradient Descent", "comments": "arXiv admin note: text overlap with arXiv:1702.06175", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.IT math.IT math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we study the problem of learning Rectified Linear Units (ReLUs)\nwhich are functions of the form $max(0,<w,x>)$ with $w$ denoting the weight\nvector. We study this problem in the high-dimensional regime where the number\nof observations are fewer than the dimension of the weight vector. We assume\nthat the weight vector belongs to some closed set (convex or nonconvex) which\ncaptures known side-information about its structure. We focus on the realizable\nmodel where the inputs are chosen i.i.d.~from a Gaussian distribution and the\nlabels are generated according to a planted weight vector. We show that\nprojected gradient descent, when initialization at 0, converges at a linear\nrate to the planted model with a number of samples that is optimal up to\nnumerical constants. Our results on the dynamics of convergence of these very\nshallow neural nets may provide some insights towards understanding the\ndynamics of deeper architectures.\n", "versions": [{"version": "v1", "created": "Wed, 10 May 2017 22:06:46 GMT"}, {"version": "v2", "created": "Tue, 1 Aug 2017 23:29:56 GMT"}], "update_date": "2017-08-03", "authors_parsed": [["Soltanolkotabi", "Mahdi", ""]]}, {"id": "1705.04612", "submitter": "Esben Jannik Bjerrum", "authors": "Esben Jannik Bjerrum, Richard Threlfall", "title": "Molecular Generation with Recurrent Neural Networks (RNNs)", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG q-bio.BM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The potential number of drug like small molecules is estimated to be between\n10^23 and 10^60 while current databases of known compounds are orders of\nmagnitude smaller with approximately 10^8 compounds. This discrepancy has led\nto an interest in generating virtual libraries using hand crafted chemical\nrules and fragment based methods to cover a larger area of chemical space and\ngenerate chemical libraries for use in in silico drug discovery endeavors. Here\nit is explored to what extent a recurrent neural network with long short term\nmemory cells can figure out sensible chemical rules and generate synthesizable\nmolecules by being trained on existing compounds encoded as SMILES. The\nnetworks can to a high extent generate novel, but chemically sensible\nmolecules. The properties of the molecules are tuned by training on two\ndifferent datasets consisting of fragment like molecules and drug like\nmolecules. The produced molecules and the training databases have very similar\ndistributions of molar weight, predicted logP, number of hydrogen bond\nacceptors and donors, number of rotatable bonds and topological polar surface\narea when compared to their respective training sets. The compounds are for the\nmost cases synthesizable as assessed with SA score and Wiley ChemPlanner.\n", "versions": [{"version": "v1", "created": "Fri, 12 May 2017 14:56:09 GMT"}, {"version": "v2", "created": "Wed, 17 May 2017 10:55:22 GMT"}], "update_date": "2017-05-18", "authors_parsed": [["Bjerrum", "Esben Jannik", ""], ["Threlfall", "Richard", ""]]}, {"id": "1705.04630", "submitter": "Vanessa Kosoy", "authors": "Vanessa Kosoy", "title": "Forecasting using incomplete models", "comments": "29 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the task of forecasting an infinite sequence of future\nobservations based on some number of past observations, where the probability\nmeasure generating the observations is \"suspected\" to satisfy one or more of a\nset of incomplete models, i.e. convex sets in the space of probability\nmeasures. This setting is in some sense intermediate between the realizable\nsetting where the probability measure comes from some known set of probability\nmeasures (which can be addressed using e.g. Bayesian inference) and the\nunrealizable setting where the probability measure is completely arbitrary. We\ndemonstrate a method of forecasting which guarantees that, whenever the true\nprobability measure satisfies an incomplete model in a given countable set, the\nforecast converges to the same incomplete model in the (appropriately\nnormalized) Kantorovich-Rubinstein metric. This is analogous to merging of\nopinions for Bayesian inference, except that convergence in the\nKantorovich-Rubinstein metric is weaker than convergence in total variation.\n", "versions": [{"version": "v1", "created": "Fri, 12 May 2017 15:38:57 GMT"}, {"version": "v2", "created": "Fri, 28 Jul 2017 12:33:39 GMT"}, {"version": "v3", "created": "Tue, 21 Nov 2017 10:49:09 GMT"}, {"version": "v4", "created": "Sat, 14 Apr 2018 13:14:50 GMT"}, {"version": "v5", "created": "Thu, 29 Nov 2018 12:02:09 GMT"}, {"version": "v6", "created": "Thu, 16 May 2019 12:54:56 GMT"}], "update_date": "2019-05-17", "authors_parsed": [["Kosoy", "Vanessa", ""]]}, {"id": "1705.04651", "submitter": "Hien Nguyen", "authors": "Hien D. Nguyen and Geoffrey J. McLachlan", "title": "Iteratively-Reweighted Least-Squares Fitting of Support Vector Machines:\n  A Majorization--Minimization Algorithm Approach", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Support vector machines (SVMs) are an important tool in modern data analysis.\nTraditionally, support vector machines have been fitted via quadratic\nprogramming, either using purpose-built or off-the-shelf algorithms. We present\nan alternative approach to SVM fitting via the majorization--minimization (MM)\nparadigm. Algorithms that are derived via MM algorithm constructions can be\nshown to monotonically decrease their objectives at each iteration, as well as\nbe globally convergent to stationary points. We demonstrate the construction of\niteratively-reweighted least-squares (IRLS) algorithms, via the MM paradigm,\nfor SVM risk minimization problems involving the hinge, least-square,\nsquared-hinge, and logistic losses, and 1-norm, 2-norm, and elastic net\npenalizations. Successful implementations of our algorithms are presented via\nsome numerical examples.\n", "versions": [{"version": "v1", "created": "Fri, 12 May 2017 16:44:03 GMT"}], "update_date": "2017-05-15", "authors_parsed": [["Nguyen", "Hien D.", ""], ["McLachlan", "Geoffrey J.", ""]]}, {"id": "1705.04662", "submitter": "Karl Ni", "authors": "Cory Stephenson, Patrick Callier, Abhinav Ganesh, Karl Ni", "title": "Monaural Audio Speaker Separation with Source Contrastive Estimation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.AI cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We propose an algorithm to separate simultaneously speaking persons from each\nother, the \"cocktail party problem\", using a single microphone. Our approach\ninvolves a deep recurrent neural networks regression to a vector space that is\ndescriptive of independent speakers. Such a vector space can embed empirically\ndetermined speaker characteristics and is optimized by distinguishing between\nspeaker masks. We call this technique source-contrastive estimation. The\nmethodology is inspired by negative sampling, which has seen success in natural\nlanguage processing, where an embedding is learned by correlating and\nde-correlating a given input vector with output weights. Although the matrix\ndetermined by the output weights is dependent on a set of known speakers, we\nonly use the input vectors during inference. Doing so will ensure that source\nseparation is explicitly speaker-independent. Our approach is similar to recent\ndeep neural network clustering and permutation-invariant training research; we\nuse weighted spectral features and masks to augment individual speaker\nfrequencies while filtering out other speakers. We avoid, however, the severe\ncomputational burden of other approaches with our technique. Furthermore, by\ntraining a vector space rather than combinations of different speakers or\ndifferences thereof, we avoid the so-called permutation problem during\ntraining. Our algorithm offers an intuitive, computationally efficient response\nto the cocktail party problem, and most importantly boasts better empirical\nperformance than other current techniques.\n", "versions": [{"version": "v1", "created": "Fri, 12 May 2017 17:23:02 GMT"}], "update_date": "2017-05-22", "authors_parsed": [["Stephenson", "Cory", ""], ["Callier", "Patrick", ""], ["Ganesh", "Abhinav", ""], ["Ni", "Karl", ""]]}, {"id": "1705.04709", "submitter": "Aydogan Ozcan", "authors": "Yair Rivenson, Zoltan Gorocs, Harun Gunaydin, Yibo Zhang, Hongda Wang,\n  Aydogan Ozcan", "title": "Deep Learning Microscopy", "comments": null, "journal-ref": "Optica, Vol. 4, Issue 11, pp. 1437-1443 (2017)", "doi": "10.1364/OPTICA.4.001437", "report-no": null, "categories": "cs.LG cs.CV physics.optics", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We demonstrate that a deep neural network can significantly improve optical\nmicroscopy, enhancing its spatial resolution over a large field-of-view and\ndepth-of-field. After its training, the only input to this network is an image\nacquired using a regular optical microscope, without any changes to its design.\nWe blindly tested this deep learning approach using various tissue samples that\nare imaged with low-resolution and wide-field systems, where the network\nrapidly outputs an image with remarkably better resolution, matching the\nperformance of higher numerical aperture lenses, also significantly surpassing\ntheir limited field-of-view and depth-of-field. These results are\ntransformative for various fields that use microscopy tools, including e.g.,\nlife sciences, where optical microscopy is considered as one of the most widely\nused and deployed techniques. Beyond such applications, our presented approach\nis broadly applicable to other imaging modalities, also spanning different\nparts of the electromagnetic spectrum, and can be used to design computational\nimagers that get better and better as they continue to image specimen and\nestablish new transformations among different modes of imaging.\n", "versions": [{"version": "v1", "created": "Fri, 12 May 2017 18:22:54 GMT"}], "update_date": "2017-11-21", "authors_parsed": [["Rivenson", "Yair", ""], ["Gorocs", "Zoltan", ""], ["Gunaydin", "Harun", ""], ["Zhang", "Yibo", ""], ["Wang", "Hongda", ""], ["Ozcan", "Aydogan", ""]]}, {"id": "1705.04770", "submitter": "M. Amin Rahimian", "authors": "Jan H\\k{a}z{\\l}a, Ali Jadbabaie, Elchanan Mossel, M. Amin Rahimian", "title": "Bayesian Decision Making in Groups is Hard", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.CC cs.LG cs.MA cs.SI stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the computations that Bayesian agents undertake when exchanging\nopinions over a network. The agents act repeatedly on their private information\nand take myopic actions that maximize their expected utility according to a\nfully rational posterior belief. We show that such computations are NP-hard for\ntwo natural utility functions: one with binary actions, and another where\nagents reveal their posterior beliefs. In fact, we show that distinguishing\nbetween posteriors that are concentrated on different states of the world is\nNP-hard. Therefore, even approximating the Bayesian posterior beliefs is hard.\nWe also describe a natural search algorithm to compute agents' actions, which\nwe call elimination of impossible signals, and show that if the network is\ntransitive, the algorithm can be modified to run in polynomial time.\n", "versions": [{"version": "v1", "created": "Fri, 12 May 2017 23:38:35 GMT"}, {"version": "v2", "created": "Thu, 19 Oct 2017 20:16:31 GMT"}, {"version": "v3", "created": "Sat, 14 Jul 2018 05:35:09 GMT"}, {"version": "v4", "created": "Sat, 27 Jul 2019 17:27:23 GMT"}], "update_date": "2019-07-30", "authors_parsed": [["H\u0105z\u0142a", "Jan", ""], ["Jadbabaie", "Ali", ""], ["Mossel", "Elchanan", ""], ["Rahimian", "M. Amin", ""]]}, {"id": "1705.04804", "submitter": "Shuchu Han", "authors": "Shuchu Han, Hao Huang, Hong Qin", "title": "Automatically Redundant Features Removal for Unsupervised Feature\n  Selection via Sparse Feature Graph", "comments": "correct several typo and format issues", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The redundant features existing in high dimensional datasets always affect\nthe performance of learning and mining algorithms. How to detect and remove\nthem is an important research topic in machine learning and data mining\nresearch. In this paper, we propose a graph based approach to find and remove\nthose redundant features automatically for high dimensional data. Based on the\nsparse learning based unsupervised feature selection framework, Sparse Feature\nGraph (SFG) is introduced not only to model the redundancy between two\nfeatures, but also to disclose the group redundancy between two groups of\nfeatures. With SFG, we can divide the whole features into different groups, and\nimprove the intrinsic structure of data by removing detected redundant\nfeatures. With accurate data structure, quality indicator vectors can be\nobtained to improve the learning performance of existing unsupervised feature\nselection algorithms such as multi-cluster feature selection (MCFS). Our\nexperimental results on benchmark datasets show that the proposed SFG and\nfeature redundancy remove algorithm can improve the performance of unsupervised\nfeature selection algorithms consistently.\n", "versions": [{"version": "v1", "created": "Sat, 13 May 2017 09:34:17 GMT"}, {"version": "v2", "created": "Fri, 30 Jun 2017 18:33:48 GMT"}], "update_date": "2017-07-04", "authors_parsed": [["Han", "Shuchu", ""], ["Huang", "Hao", ""], ["Qin", "Hong", ""]]}, {"id": "1705.04862", "submitter": "Alfredo V. Clemente Mr", "authors": "Alfredo V. Clemente, Humberto N. Castej\\'on, Arjun Chandra", "title": "Efficient Parallel Methods for Deep Reinforcement Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel framework for efficient parallelization of deep\nreinforcement learning algorithms, enabling these algorithms to learn from\nmultiple actors on a single machine. The framework is algorithm agnostic and\ncan be applied to on-policy, off-policy, value based and policy gradient based\nalgorithms. Given its inherent parallelism, the framework can be efficiently\nimplemented on a GPU, allowing the usage of powerful models while significantly\nreducing training time. We demonstrate the effectiveness of our framework by\nimplementing an advantage actor-critic algorithm on a GPU, using on-policy\nexperiences and employing synchronous updates. Our algorithm achieves\nstate-of-the-art performance on the Atari domain after only a few hours of\ntraining. Our framework thus opens the door for much faster experimentation on\ndemanding problem domains. Our implementation is open-source and is made public\nat https://github.com/alfredvc/paac\n", "versions": [{"version": "v1", "created": "Sat, 13 May 2017 17:39:54 GMT"}, {"version": "v2", "created": "Tue, 16 May 2017 14:30:14 GMT"}], "update_date": "2017-05-17", "authors_parsed": [["Clemente", "Alfredo V.", ""], ["Castej\u00f3n", "Humberto N.", ""], ["Chandra", "Arjun", ""]]}, {"id": "1705.04925", "submitter": "Qunwei Li", "authors": "Qunwei Li, Yi Zhou, Yingbin Liang, Pramod K. Varshney", "title": "Convergence Analysis of Proximal Gradient with Momentum for Nonconvex\n  Optimization", "comments": "Accepted in ICML 2017, 9 papes, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In many modern machine learning applications, structures of underlying\nmathematical models often yield nonconvex optimization problems. Due to the\nintractability of nonconvexity, there is a rising need to develop efficient\nmethods for solving general nonconvex problems with certain performance\nguarantee. In this work, we investigate the accelerated proximal gradient\nmethod for nonconvex programming (APGnc). The method compares between a usual\nproximal gradient step and a linear extrapolation step, and accepts the one\nthat has a lower function value to achieve a monotonic decrease. In specific,\nunder a general nonsmooth and nonconvex setting, we provide a rigorous argument\nto show that the limit points of the sequence generated by APGnc are critical\npoints of the objective function. Then, by exploiting the\nKurdyka-{\\L}ojasiewicz (\\KL) property for a broad class of functions, we\nestablish the linear and sub-linear convergence rates of the function value\nsequence generated by APGnc. We further propose a stochastic variance reduced\nAPGnc (SVRG-APGnc), and establish its linear convergence under a special case\nof the \\KL property. We also extend the analysis to the inexact version of\nthese methods and develop an adaptive momentum strategy that improves the\nnumerical performance.\n", "versions": [{"version": "v1", "created": "Sun, 14 May 2017 07:22:20 GMT"}], "update_date": "2017-05-16", "authors_parsed": [["Li", "Qunwei", ""], ["Zhou", "Yi", ""], ["Liang", "Yingbin", ""], ["Varshney", "Pramod K.", ""]]}, {"id": "1705.04977", "submitter": "Michael Tsang", "authors": "Michael Tsang, Dehua Cheng, Yan Liu", "title": "Detecting Statistical Interactions from Neural Network Weights", "comments": "Published in ICLR 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Interpreting neural networks is a crucial and challenging task in machine\nlearning. In this paper, we develop a novel framework for detecting statistical\ninteractions captured by a feedforward multilayer neural network by directly\ninterpreting its learned weights. Depending on the desired interactions, our\nmethod can achieve significantly better or similar interaction detection\nperformance compared to the state-of-the-art without searching an exponential\nsolution space of possible interactions. We obtain this accuracy and efficiency\nby observing that interactions between input features are created by the\nnon-additive effect of nonlinear activation functions, and that interacting\npaths are encoded in weight matrices. We demonstrate the performance of our\nmethod and the importance of discovered interactions via experimental results\non both synthetic datasets and real-world application datasets.\n", "versions": [{"version": "v1", "created": "Sun, 14 May 2017 15:35:29 GMT"}, {"version": "v2", "created": "Sun, 11 Jun 2017 22:27:48 GMT"}, {"version": "v3", "created": "Sun, 25 Feb 2018 02:09:25 GMT"}, {"version": "v4", "created": "Tue, 27 Feb 2018 18:58:21 GMT"}], "update_date": "2018-02-28", "authors_parsed": [["Tsang", "Michael", ""], ["Cheng", "Dehua", ""], ["Liu", "Yan", ""]]}, {"id": "1705.05020", "submitter": "Emanuel Laude", "authors": "Emanuel Laude, Jan-Hendrik Lange, Jonas Sch\\\"upfer, Csaba Domokos,\n  Laura Leal-Taix\\'e, Frank R. Schmidt, Bjoern Andres, Daniel Cremers", "title": "Discrete-Continuous ADMM for Transductive Inference in Higher-Order MRFs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces a novel algorithm for transductive inference in\nhigher-order MRFs, where the unary energies are parameterized by a variable\nclassifier. The considered task is posed as a joint optimization problem in the\ncontinuous classifier parameters and the discrete label variables. In contrast\nto prior approaches such as convex relaxations, we propose an advantageous\ndecoupling of the objective function into discrete and continuous subproblems\nand a novel, efficient optimization method related to ADMM. This approach\npreserves integrality of the discrete label variables and guarantees global\nconvergence to a critical point. We demonstrate the advantages of our approach\nin several experiments including video object segmentation on the DAVIS data\nset and interactive image segmentation.\n", "versions": [{"version": "v1", "created": "Sun, 14 May 2017 19:32:50 GMT"}, {"version": "v2", "created": "Mon, 19 Jun 2017 15:51:08 GMT"}, {"version": "v3", "created": "Wed, 16 Aug 2017 15:18:11 GMT"}, {"version": "v4", "created": "Mon, 27 Nov 2017 15:59:10 GMT"}, {"version": "v5", "created": "Sat, 28 Apr 2018 12:55:18 GMT"}], "update_date": "2018-05-01", "authors_parsed": [["Laude", "Emanuel", ""], ["Lange", "Jan-Hendrik", ""], ["Sch\u00fcpfer", "Jonas", ""], ["Domokos", "Csaba", ""], ["Leal-Taix\u00e9", "Laura", ""], ["Schmidt", "Frank R.", ""], ["Andres", "Bjoern", ""], ["Cremers", "Daniel", ""]]}, {"id": "1705.05035", "submitter": "Luke Metz", "authors": "Luke Metz, Julian Ibarz, Navdeep Jaitly, James Davidson", "title": "Discrete Sequential Prediction of Continuous Actions for Deep RL", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It has long been assumed that high dimensional continuous control problems\ncannot be solved effectively by discretizing individual dimensions of the\naction space due to the exponentially large number of bins over which policies\nwould have to be learned. In this paper, we draw inspiration from the recent\nsuccess of sequence-to-sequence models for structured prediction problems to\ndevelop policies over discretized spaces. Central to this method is the\nrealization that complex functions over high dimensional spaces can be modeled\nby neural networks that predict one dimension at a time. Specifically, we show\nhow Q-values and policies over continuous spaces can be modeled using a next\nstep prediction model over discretized dimensions. With this parameterization,\nit is possible to both leverage the compositional structure of action spaces\nduring learning, as well as compute maxima over action spaces (approximately).\nOn a simple example task we demonstrate empirically that our method can perform\nglobal search, which effectively gets around the local optimization issues that\nplague DDPG. We apply the technique to off-policy (Q-learning) methods and show\nthat our method can achieve the state-of-the-art for off-policy methods on\nseveral continuous control tasks.\n", "versions": [{"version": "v1", "created": "Sun, 14 May 2017 22:53:13 GMT"}, {"version": "v2", "created": "Sat, 16 Jun 2018 03:11:46 GMT"}, {"version": "v3", "created": "Sat, 8 Jun 2019 00:56:16 GMT"}], "update_date": "2019-06-11", "authors_parsed": [["Metz", "Luke", ""], ["Ibarz", "Julian", ""], ["Jaitly", "Navdeep", ""], ["Davidson", "James", ""]]}, {"id": "1705.05067", "submitter": "Luo Luo", "authors": "Luo Luo, Cheng Chen, Zhihua Zhang, Wu-Jun Li, Tong Zhang", "title": "Robust Frequent Directions with Application in Online Learning", "comments": "Journal of Machine Learning Research 2019, Accepted", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The frequent directions (FD) technique is a deterministic approach for online\nsketching that has many applications in machine learning. The conventional FD\nis a heuristic procedure that often outputs rank deficient matrices. To\novercome the rank deficiency problem, we propose a new sketching strategy\ncalled robust frequent directions (RFD) by introducing a regularization term.\nRFD can be derived from an optimization problem. It updates the sketch matrix\nand the regularization term adaptively and jointly. RFD reduces the\napproximation error of FD without increasing the computational cost. We also\napply RFD to online learning and propose an effective hyperparameter-free\nonline Newton algorithm. We derive a regret bound for our online Newton\nalgorithm based on RFD, which guarantees the robustness of the algorithm. The\nexperimental studies demonstrate that the proposed method outperforms\nstate-of-the-art second order online learning algorithms.\n", "versions": [{"version": "v1", "created": "Mon, 15 May 2017 04:18:29 GMT"}, {"version": "v2", "created": "Mon, 25 Dec 2017 06:19:12 GMT"}, {"version": "v3", "created": "Sun, 24 Feb 2019 03:44:44 GMT"}], "update_date": "2019-02-26", "authors_parsed": [["Luo", "Luo", ""], ["Chen", "Cheng", ""], ["Zhang", "Zhihua", ""], ["Li", "Wu-Jun", ""], ["Zhang", "Tong", ""]]}, {"id": "1705.05085", "submitter": "Vincent Zheng", "authors": "Hongyun Cai, Vincent W. Zheng, Kevin Chen-Chuan Chang", "title": "Active Learning for Graph Embedding", "comments": "Technical Report", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Graph embedding provides an efficient solution for graph analysis by\nconverting the graph into a low-dimensional space which preserves the structure\ninformation. In contrast to the graph structure data, the i.i.d. node embedding\ncan be processed efficiently in terms of both time and space. Current\nsemi-supervised graph embedding algorithms assume the labelled nodes are given,\nwhich may not be always true in the real world. While manually label all\ntraining data is inapplicable, how to select the subset of training data to\nlabel so as to maximize the graph analysis task performance is of great\nimportance. This motivates our proposed active graph embedding (AGE) framework,\nin which we design a general active learning query strategy for any\nsemi-supervised graph embedding algorithm. AGE selects the most informative\nnodes as the training labelled nodes based on the graphical information (i.e.,\nnode centrality) as well as the learnt node embedding (i.e., node\nclassification uncertainty and node embedding representativeness). Different\nquery criteria are combined with the time-sensitive parameters which shift the\nfocus from graph based query criteria to embedding based criteria as the\nlearning progresses. Experiments have been conducted on three public data sets\nand the results verified the effectiveness of each component of our query\nstrategy and the power of combining them using time-sensitive parameters. Our\ncode is available online at: https://github.com/vwz/AGE.\n", "versions": [{"version": "v1", "created": "Mon, 15 May 2017 06:49:04 GMT"}], "update_date": "2017-05-16", "authors_parsed": [["Cai", "Hongyun", ""], ["Zheng", "Vincent W.", ""], ["Chang", "Kevin Chen-Chuan", ""]]}, {"id": "1705.05091", "submitter": "Ohad Shamir", "authors": "Nicol\\`o Cesa-Bianchi and Ohad Shamir", "title": "Bandit Regret Scaling with the Effective Loss Range", "comments": "The results in section 4 are incorrect as stated -- we have added an\n  erratum at the beginning of the document. The results in the other sections\n  are still valid. We thank \\'{E}tienne de Montbrun for locating the error", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study how the regret guarantees of nonstochastic multi-armed bandits can\nbe improved, if the effective range of the losses in each round is small (e.g.\nthe maximal difference between two losses in a given round). Despite a recent\nimpossibility result, we show how this can be made possible under certain mild\nadditional assumptions, such as availability of rough estimates of the losses,\nor advance knowledge of the loss of a single, possibly unspecified arm. Along\nthe way, we develop a novel technique which might be of independent interest,\nto convert any multi-armed bandit algorithm with regret depending on the loss\nrange, to an algorithm with regret depending only on the effective range, while\navoiding predictably bad arms altogether.\n", "versions": [{"version": "v1", "created": "Mon, 15 May 2017 07:25:00 GMT"}, {"version": "v2", "created": "Thu, 18 May 2017 17:59:39 GMT"}, {"version": "v3", "created": "Thu, 2 Jan 2020 15:24:03 GMT"}], "update_date": "2020-01-03", "authors_parsed": [["Cesa-Bianchi", "Nicol\u00f2", ""], ["Shamir", "Ohad", ""]]}, {"id": "1705.05116", "submitter": "Fangyi Zhang", "authors": "Fangyi Zhang, J\\\"urgen Leitner, Michael Milford, Peter I. Corke", "title": "Tuning Modular Networks with Weighted Losses for Hand-Eye Coordination", "comments": "2 pages, to appear in the Deep Learning for Robotic Vision (DLRV)\n  Workshop in CVPR 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AI cs.CV cs.LG cs.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces an end-to-end fine-tuning method to improve hand-eye\ncoordination in modular deep visuo-motor policies (modular networks) where each\nmodule is trained independently. Benefiting from weighted losses, the\nfine-tuning method significantly improves the performance of the policies for a\nrobotic planar reaching task.\n", "versions": [{"version": "v1", "created": "Mon, 15 May 2017 08:57:27 GMT"}], "update_date": "2017-05-16", "authors_parsed": [["Zhang", "Fangyi", ""], ["Leitner", "J\u00fcrgen", ""], ["Milford", "Michael", ""], ["Corke", "Peter I.", ""]]}, {"id": "1705.05154", "submitter": "Heng Guo", "authors": "Heng Guo and Kaan Kara and Ce Zhang", "title": "Layerwise Systematic Scan: Deep Boltzmann Machines and Beyond", "comments": "v2: typo fixes and improved presentation", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For Markov chain Monte Carlo methods, one of the greatest discrepancies\nbetween theory and system is the scan order - while most theoretical\ndevelopment on the mixing time analysis deals with random updates, real-world\nsystems are implemented with systematic scans. We bridge this gap for models\nthat exhibit a bipartite structure, including, most notably, the\nRestricted/Deep Boltzmann Machine. The de facto implementation for these models\nscans variables in a layerwise fashion. We show that the Gibbs sampler with a\nlayerwise alternating scan order has its relaxation time (in terms of epochs)\nno larger than that of a random-update Gibbs sampler (in terms of variable\nupdates). We also construct examples to show that this bound is asymptotically\ntight. Through standard inequalities, our result also implies a comparison on\nthe mixing times.\n", "versions": [{"version": "v1", "created": "Mon, 15 May 2017 11:00:25 GMT"}, {"version": "v2", "created": "Mon, 9 Oct 2017 11:03:50 GMT"}], "update_date": "2017-10-10", "authors_parsed": [["Guo", "Heng", ""], ["Kara", "Kaan", ""], ["Zhang", "Ce", ""]]}, {"id": "1705.05172", "submitter": "Thomas Moerland", "authors": "Thomas M. Moerland, Joost Broekens, Catholijn M. Jonker", "title": "Emotion in Reinforcement Learning Agents and Robots: A Survey", "comments": "To be published in Machine Learning Journal", "journal-ref": "Machine Learning 2017", "doi": "10.1007/s10994-017-5666-0", "report-no": null, "categories": "cs.LG cs.AI cs.HC cs.RO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article provides the first survey of computational models of emotion in\nreinforcement learning (RL) agents. The survey focuses on agent/robot emotions,\nand mostly ignores human user emotions. Emotions are recognized as functional\nin decision-making by influencing motivation and action selection. Therefore,\ncomputational emotion models are usually grounded in the agent's decision\nmaking architecture, of which RL is an important subclass. Studying emotions in\nRL-based agents is useful for three research fields. For machine learning (ML)\nresearchers, emotion models may improve learning efficiency. For the\ninteractive ML and human-robot interaction (HRI) community, emotions can\ncommunicate state and enhance user investment. Lastly, it allows affective\nmodelling (AM) researchers to investigate their emotion theories in a\nsuccessful AI agent class. This survey provides background on emotion theory\nand RL. It systematically addresses 1) from what underlying dimensions (e.g.,\nhomeostasis, appraisal) emotions can be derived and how these can be modelled\nin RL-agents, 2) what types of emotions have been derived from these\ndimensions, and 3) how these emotions may either influence the learning\nefficiency of the agent or be useful as social signals. We also systematically\ncompare evaluation criteria, and draw connections to important RL sub-domains\nlike (intrinsic) motivation and model-based RL. In short, this survey provides\nboth a practical overview for engineers wanting to implement emotions in their\nRL agents, and identifies challenges and directions for future emotion-RL\nresearch.\n", "versions": [{"version": "v1", "created": "Mon, 15 May 2017 11:49:56 GMT"}], "update_date": "2017-11-30", "authors_parsed": [["Moerland", "Thomas M.", ""], ["Broekens", "Joost", ""], ["Jonker", "Catholijn M.", ""]]}, {"id": "1705.05229", "submitter": "Zhou  Xing", "authors": "Zhou Xing, Eddy Baik, Yan Jiao, Nilesh Kulkarni, Chris Li, Gautam\n  Muralidhar, Marzieh Parandehgheibi, Erik Reed, Abhishek Singhal, Fei Xiao and\n  Chris Pouliot", "title": "Modeling of the Latent Embedding of Music using Deep Neural Network", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While both the data volume and heterogeneity of the digital music content is\nhuge, it has become increasingly important and convenient to build a\nrecommendation or search system to facilitate surfacing these content to the\nuser or consumer community. Most of the recommendation models fall into two\nprimary species, collaborative filtering based and content based approaches.\nVariants of instantiations of collaborative filtering approach suffer from the\ncommon issues of so called \"cold start\" and \"long tail\" problems where there is\nnot much user interaction data to reveal user opinions or affinities on the\ncontent and also the distortion towards the popular content. Content-based\napproaches are sometimes limited by the richness of the available content data\nresulting in a heavily biased and coarse recommendation result. In recent\nyears, the deep neural network has enjoyed a great success in large-scale image\nand video recognitions. In this paper, we propose and experiment using deep\nconvolutional neural network to imitate how human brain processes hierarchical\nstructures in the auditory signals, such as music, speech, etc., at various\ntimescales. This approach can be used to discover the latent factor models of\nthe music based upon acoustic hyper-images that are extracted from the raw\naudio waves of music. These latent embeddings can be used either as features to\nfeed to subsequent models, such as collaborative filtering, or to build\nsimilarity metrics between songs, or to classify music based on the labels for\ntraining such as genre, mood, sentiment, etc.\n", "versions": [{"version": "v1", "created": "Fri, 12 May 2017 01:08:31 GMT"}], "update_date": "2017-05-16", "authors_parsed": [["Xing", "Zhou", ""], ["Baik", "Eddy", ""], ["Jiao", "Yan", ""], ["Kulkarni", "Nilesh", ""], ["Li", "Chris", ""], ["Muralidhar", "Gautam", ""], ["Parandehgheibi", "Marzieh", ""], ["Reed", "Erik", ""], ["Singhal", "Abhishek", ""], ["Xiao", "Fei", ""], ["Pouliot", "Chris", ""]]}, {"id": "1705.05263", "submitter": "Ivo Danihelka", "authors": "Ivo Danihelka, Balaji Lakshminarayanan, Benigno Uria, Daan Wierstra,\n  Peter Dayan", "title": "Comparison of Maximum Likelihood and GAN-based training of Real NVPs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We train a generator by maximum likelihood and we also train the same\ngenerator architecture by Wasserstein GAN. We then compare the generated\nsamples, exact log-probability densities and approximate Wasserstein distances.\nWe show that an independent critic trained to approximate Wasserstein distance\nbetween the validation set and the generator distribution helps detect\noverfitting. Finally, we use ideas from the one-shot learning literature to\ndevelop a novel fast learning critic.\n", "versions": [{"version": "v1", "created": "Mon, 15 May 2017 14:24:01 GMT"}], "update_date": "2017-05-16", "authors_parsed": [["Danihelka", "Ivo", ""], ["Lakshminarayanan", "Balaji", ""], ["Uria", "Benigno", ""], ["Wierstra", "Daan", ""], ["Dayan", "Peter", ""]]}, {"id": "1705.05264", "submitter": "Nicolas Papernot", "authors": "Nicolas Papernot and Patrick McDaniel", "title": "Extending Defensive Distillation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Machine learning is vulnerable to adversarial examples: inputs carefully\nmodified to force misclassification. Designing defenses against such inputs\nremains largely an open problem. In this work, we revisit defensive\ndistillation---which is one of the mechanisms proposed to mitigate adversarial\nexamples---to address its limitations. We view our results not only as an\neffective way of addressing some of the recently discovered attacks but also as\nreinforcing the importance of improved training techniques.\n", "versions": [{"version": "v1", "created": "Mon, 15 May 2017 14:25:15 GMT"}], "update_date": "2017-05-16", "authors_parsed": [["Papernot", "Nicolas", ""], ["McDaniel", "Patrick", ""]]}, {"id": "1705.05267", "submitter": "Ahmed Alaa", "authors": "Ahmed M. Alaa, Scott Hu, Mihaela van der Schaar", "title": "Learning from Clinical Judgments: Semi-Markov-Modulated Marked Hawkes\n  Processes for Risk Prognosis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Critically ill patients in regular wards are vulnerable to unanticipated\nadverse events which require prompt transfer to the intensive care unit (ICU).\nTo allow for accurate prognosis of deteriorating patients, we develop a novel\ncontinuous-time probabilistic model for a monitored patient's temporal sequence\nof physiological data. Our model captures \"informatively sampled\" patient\nepisodes: the clinicians' decisions on when to observe a hospitalized patient's\nvital signs and lab tests over time are represented by a marked Hawkes process,\nwith intensity parameters that are modulated by the patient's latent clinical\nstates, and with observable physiological data (mark process) modeled as a\nswitching multi-task Gaussian process. In addition, our model captures\n\"informatively censored\" patient episodes by representing the patient's latent\nclinical states as an absorbing semi-Markov jump process. The model parameters\nare learned from offline patient episodes in the electronic health records via\nan EM-based algorithm. Experiments conducted on a cohort of patients admitted\nto a major medical center over a 3-year period show that risk prognosis based\non our model significantly outperforms the currently deployed medical risk\nscores and other baseline machine learning algorithms.\n", "versions": [{"version": "v1", "created": "Mon, 15 May 2017 14:29:51 GMT"}], "update_date": "2017-05-16", "authors_parsed": [["Alaa", "Ahmed M.", ""], ["Hu", "Scott", ""], ["van der Schaar", "Mihaela", ""]]}, {"id": "1705.05363", "submitter": "Deepak Pathak", "authors": "Deepak Pathak, Pulkit Agrawal, Alexei A. Efros, Trevor Darrell", "title": "Curiosity-driven Exploration by Self-supervised Prediction", "comments": "In ICML 2017. Website at https://pathak22.github.io/noreward-rl/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV cs.RO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In many real-world scenarios, rewards extrinsic to the agent are extremely\nsparse, or absent altogether. In such cases, curiosity can serve as an\nintrinsic reward signal to enable the agent to explore its environment and\nlearn skills that might be useful later in its life. We formulate curiosity as\nthe error in an agent's ability to predict the consequence of its own actions\nin a visual feature space learned by a self-supervised inverse dynamics model.\nOur formulation scales to high-dimensional continuous state spaces like images,\nbypasses the difficulties of directly predicting pixels, and, critically,\nignores the aspects of the environment that cannot affect the agent. The\nproposed approach is evaluated in two environments: VizDoom and Super Mario\nBros. Three broad settings are investigated: 1) sparse extrinsic reward, where\ncuriosity allows for far fewer interactions with the environment to reach the\ngoal; 2) exploration with no extrinsic reward, where curiosity pushes the agent\nto explore more efficiently; and 3) generalization to unseen scenarios (e.g.\nnew levels of the same game) where the knowledge gained from earlier experience\nhelps the agent explore new places much faster than starting from scratch. Demo\nvideo and code available at https://pathak22.github.io/noreward-rl/\n", "versions": [{"version": "v1", "created": "Mon, 15 May 2017 17:56:22 GMT"}], "update_date": "2017-05-16", "authors_parsed": [["Pathak", "Deepak", ""], ["Agrawal", "Pulkit", ""], ["Efros", "Alexei A.", ""], ["Darrell", "Trevor", ""]]}, {"id": "1705.05366", "submitter": "Venkatadheeraj Pichapati", "authors": "Moein Falahatgar and Alon Orlitsky and Venkatadheeraj Pichapati and\n  Ananda Theertha Suresh", "title": "Maximum Selection and Ranking under Noisy Comparisons", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider $(\\epsilon,\\delta)$-PAC maximum-selection and ranking for general\nprobabilistic models whose comparisons probabilities satisfy strong stochastic\ntransitivity and stochastic triangle inequality. Modifying the popular knockout\ntournament, we propose a maximum-selection algorithm that uses\n$\\mathcal{O}\\left(\\frac{n}{\\epsilon^2}\\log \\frac{1}{\\delta}\\right)$\ncomparisons, a number tight up to a constant factor. We then derive a general\nframework that improves the performance of many ranking algorithms, and combine\nit with merge sort and binary search to obtain a ranking algorithm that uses\n$\\mathcal{O}\\left(\\frac{n\\log n (\\log \\log n)^3}{\\epsilon^2}\\right)$\ncomparisons for any $\\delta\\ge\\frac1n$, a number optimal up to a $(\\log \\log\nn)^3$ factor.\n", "versions": [{"version": "v1", "created": "Mon, 15 May 2017 17:59:17 GMT"}], "update_date": "2017-05-16", "authors_parsed": [["Falahatgar", "Moein", ""], ["Orlitsky", "Alon", ""], ["Pichapati", "Venkatadheeraj", ""], ["Suresh", "Ananda Theertha", ""]]}, {"id": "1705.05394", "submitter": "David Held", "authors": "David Held, Zoe McCarthy, Michael Zhang, Fred Shentu, Pieter Abbeel", "title": "Probabilistically Safe Policy Transfer", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Although learning-based methods have great potential for robotics, one\nconcern is that a robot that updates its parameters might cause large amounts\nof damage before it learns the optimal policy. We formalize the idea of safe\nlearning in a probabilistic sense by defining an optimization problem: we\ndesire to maximize the expected return while keeping the expected damage below\na given safety limit. We study this optimization for the case of a robot\nmanipulator with safety-based torque limits. We would like to ensure that the\ndamage constraint is maintained at every step of the optimization and not just\nat convergence. To achieve this aim, we introduce a novel method which predicts\nhow modifying the torque limit, as well as how updating the policy parameters,\nmight affect the robot's safety. We show through a number of experiments that\nour approach allows the robot to improve its performance while ensuring that\nthe expected damage constraint is not violated during the learning process.\n", "versions": [{"version": "v1", "created": "Mon, 15 May 2017 18:02:17 GMT"}], "update_date": "2017-05-17", "authors_parsed": [["Held", "David", ""], ["McCarthy", "Zoe", ""], ["Zhang", "Michael", ""], ["Shentu", "Fred", ""], ["Abbeel", "Pieter", ""]]}, {"id": "1705.05396", "submitter": "Avi Pfeffer", "authors": "Avi Pfeffer", "title": "Learning Probabilistic Programs Using Backpropagation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Probabilistic modeling enables combining domain knowledge with learning from\ndata, thereby supporting learning from fewer training instances than purely\ndata-driven methods. However, learning probabilistic models is difficult and\nhas not achieved the level of performance of methods such as deep neural\nnetworks on many tasks. In this paper, we attempt to address this issue by\npresenting a method for learning the parameters of a probabilistic program\nusing backpropagation. Our approach opens the possibility to building deep\nprobabilistic programming models that are trained in a similar way to neural\nnetworks.\n", "versions": [{"version": "v1", "created": "Mon, 15 May 2017 18:07:31 GMT"}], "update_date": "2017-05-17", "authors_parsed": [["Pfeffer", "Avi", ""]]}, {"id": "1705.05427", "submitter": "Nan Jiang", "authors": "Kareem Amin, Nan Jiang, Satinder Singh", "title": "Repeated Inverse Reinforcement Learning", "comments": "The first two authors contributed equally to this work. The paper\n  appears in NIPS 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a novel repeated Inverse Reinforcement Learning problem: the\nagent has to act on behalf of a human in a sequence of tasks and wishes to\nminimize the number of tasks that it surprises the human by acting suboptimally\nwith respect to how the human would have acted. Each time the human is\nsurprised, the agent is provided a demonstration of the desired behavior by the\nhuman. We formalize this problem, including how the sequence of tasks is\nchosen, in a few different ways and provide some foundational results.\n", "versions": [{"version": "v1", "created": "Mon, 15 May 2017 20:06:35 GMT"}, {"version": "v2", "created": "Thu, 18 May 2017 19:32:27 GMT"}, {"version": "v3", "created": "Sat, 4 Nov 2017 00:38:19 GMT"}], "update_date": "2017-11-07", "authors_parsed": [["Amin", "Kareem", ""], ["Jiang", "Nan", ""], ["Singh", "Satinder", ""]]}, {"id": "1705.05475", "submitter": "Tsung-Han Lin", "authors": "Ping Tak Peter Tang, Tsung-Han Lin, Mike Davies", "title": "Sparse Coding by Spiking Neural Networks: Convergence Theory and\n  Computational Results", "comments": "13 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NA cs.NE q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In a spiking neural network (SNN), individual neurons operate autonomously\nand only communicate with other neurons sparingly and asynchronously via spike\nsignals. These characteristics render a massively parallel hardware\nimplementation of SNN a potentially powerful computer, albeit a non von Neumann\none. But can one guarantee that a SNN computer solves some important problems\nreliably? In this paper, we formulate a mathematical model of one SNN that can\nbe configured for a sparse coding problem for feature extraction. With a\nmoderate but well-defined assumption, we prove that the SNN indeed solves\nsparse coding. To the best of our knowledge, this is the first rigorous result\nof this kind.\n", "versions": [{"version": "v1", "created": "Mon, 15 May 2017 23:06:34 GMT"}], "update_date": "2017-05-17", "authors_parsed": [["Tang", "Ping Tak Peter", ""], ["Lin", "Tsung-Han", ""], ["Davies", "Mike", ""]]}, {"id": "1705.05491", "submitter": "Lili Su", "authors": "Yudong Chen, Lili Su, Jiaming Xu", "title": "Distributed Statistical Machine Learning in Adversarial Settings:\n  Byzantine Gradient Descent", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.CR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of distributed statistical machine learning in\nadversarial settings, where some unknown and time-varying subset of working\nmachines may be compromised and behave arbitrarily to prevent an accurate model\nfrom being learned. This setting captures the potential adversarial attacks\nfaced by Federated Learning -- a modern machine learning paradigm that is\nproposed by Google researchers and has been intensively studied for ensuring\nuser privacy. Formally, we focus on a distributed system consisting of a\nparameter server and $m$ working machines. Each working machine keeps $N/m$\ndata samples, where $N$ is the total number of samples. The goal is to\ncollectively learn the underlying true model parameter of dimension $d$.\n  In classical batch gradient descent methods, the gradients reported to the\nserver by the working machines are aggregated via simple averaging, which is\nvulnerable to a single Byzantine failure. In this paper, we propose a Byzantine\ngradient descent method based on the geometric median of means of the\ngradients. We show that our method can tolerate $q \\le (m-1)/2$ Byzantine\nfailures, and the parameter estimate converges in $O(\\log N)$ rounds with an\nestimation error of $\\sqrt{d(2q+1)/N}$, hence approaching the optimal error\nrate $\\sqrt{d/N}$ in the centralized and failure-free setting. The total\ncomputational complexity of our algorithm is of $O((Nd/m) \\log N)$ at each\nworking machine and $O(md + kd \\log^3 N)$ at the central server, and the total\ncommunication cost is of $O(m d \\log N)$. We further provide an application of\nour general results to the linear regression problem.\n  A key challenge arises in the above problem is that Byzantine failures create\narbitrary and unspecified dependency among the iterations and the aggregated\ngradients. We prove that the aggregated gradient converges uniformly to the\ntrue gradient function.\n", "versions": [{"version": "v1", "created": "Tue, 16 May 2017 00:20:49 GMT"}, {"version": "v2", "created": "Mon, 23 Oct 2017 01:16:25 GMT"}], "update_date": "2017-10-24", "authors_parsed": [["Chen", "Yudong", ""], ["Su", "Lili", ""], ["Xu", "Jiaming", ""]]}, {"id": "1705.05494", "submitter": "Paulo Roberto Urio", "authors": "Paulo Roberto Urio, Zhao Liang", "title": "Data clustering with edge domination in complex networks", "comments": "13 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.LG physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a model for a dynamical system where particles dominate\nedges in a complex network. The proposed dynamical system is then extended to\nan application on the problem of community detection and data clustering. In\nthe case of the data clustering problem, 6 different techniques were simulated\non 10 different datasets in order to compare with the proposed technique. The\nresults show that the proposed algorithm performs well when prior knowledge of\nthe number of clusters is known to the algorithm.\n", "versions": [{"version": "v1", "created": "Tue, 16 May 2017 00:44:31 GMT"}], "update_date": "2017-05-17", "authors_parsed": [["Urio", "Paulo Roberto", ""], ["Liang", "Zhao", ""]]}, {"id": "1705.05502", "submitter": "David Rolnick", "authors": "David Rolnick (MIT), Max Tegmark (MIT)", "title": "The power of deeper networks for expressing natural functions", "comments": "Replaced to match version published at ICLR 2018. 14 pages, 2 figs", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is well-known that neural networks are universal approximators, but that\ndeeper networks tend in practice to be more powerful than shallower ones. We\nshed light on this by proving that the total number of neurons $m$ required to\napproximate natural classes of multivariate polynomials of $n$ variables grows\nonly linearly with $n$ for deep neural networks, but grows exponentially when\nmerely a single hidden layer is allowed. We also provide evidence that when the\nnumber of hidden layers is increased from $1$ to $k$, the neuron requirement\ngrows exponentially not with $n$ but with $n^{1/k}$, suggesting that the\nminimum number of layers required for practical expressibility grows only\nlogarithmically with $n$.\n", "versions": [{"version": "v1", "created": "Tue, 16 May 2017 02:02:24 GMT"}, {"version": "v2", "created": "Fri, 27 Apr 2018 17:52:03 GMT"}], "update_date": "2018-04-30", "authors_parsed": [["Rolnick", "David", "", "MIT"], ["Tegmark", "Max", "", "MIT"]]}, {"id": "1705.05524", "submitter": "Dieterich Lawson", "authors": "Dieterich Lawson, Chung-Cheng Chiu, George Tucker, Colin Raffel, Kevin\n  Swersky, Navdeep Jaitly", "title": "Learning Hard Alignments with Variational Inference", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There has recently been significant interest in hard attention models for\ntasks such as object recognition, visual captioning and speech recognition.\nHard attention can offer benefits over soft attention such as decreased\ncomputational cost, but training hard attention models can be difficult because\nof the discrete latent variables they introduce. Previous work used REINFORCE\nand Q-learning to approach these issues, but those methods can provide\nhigh-variance gradient estimates and be slow to train. In this paper, we tackle\nthe problem of learning hard attention for a sequential task using variational\ninference methods, specifically the recently introduced VIMCO and NVIL.\nFurthermore, we propose a novel baseline that adapts VIMCO to this setting. We\ndemonstrate our method on a phoneme recognition task in clean and noisy\nenvironments and show that our method outperforms REINFORCE, with the\ndifference being greater for a more complicated task.\n", "versions": [{"version": "v1", "created": "Tue, 16 May 2017 04:30:56 GMT"}, {"version": "v2", "created": "Wed, 1 Nov 2017 19:08:18 GMT"}], "update_date": "2017-11-03", "authors_parsed": [["Lawson", "Dieterich", ""], ["Chiu", "Chung-Cheng", ""], ["Tucker", "George", ""], ["Raffel", "Colin", ""], ["Swersky", "Kevin", ""], ["Jaitly", "Navdeep", ""]]}, {"id": "1705.05584", "submitter": "Varun Ojha", "authors": "Varun Kumar Ojha, Ajith Abraham, V\\'aclav Sn\\'a\\v{s}el", "title": "Metaheuristic Design of Feedforward Neural Networks: A Review of Two\n  Decades of Research", "comments": null, "journal-ref": "Engineering Applications of Artificial Intelligence Volume 60,\n  April 2017, Pages 97 to 116", "doi": "10.1016/j.engappai.2017.01.013", "report-no": null, "categories": "cs.NE cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Over the past two decades, the feedforward neural network (FNN) optimization\nhas been a key interest among the researchers and practitioners of multiple\ndisciplines. The FNN optimization is often viewed from the various\nperspectives: the optimization of weights, network architecture, activation\nnodes, learning parameters, learning environment, etc. Researchers adopted such\ndifferent viewpoints mainly to improve the FNN's generalization ability. The\ngradient-descent algorithm such as backpropagation has been widely applied to\noptimize the FNNs. Its success is evident from the FNN's application to\nnumerous real-world problems. However, due to the limitations of the\ngradient-based optimization methods, the metaheuristic algorithms including the\nevolutionary algorithms, swarm intelligence, etc., are still being widely\nexplored by the researchers aiming to obtain generalized FNN for a given\nproblem. This article attempts to summarize a broad spectrum of FNN\noptimization methodologies including conventional and metaheuristic approaches.\nThis article also tries to connect various research directions emerged out of\nthe FNN optimization practices, such as evolving neural network (NN),\ncooperative coevolution NN, complex-valued NN, deep learning, extreme learning\nmachine, quantum NN, etc. Additionally, it provides interesting research\nchallenges for future research to cope-up with the present information\nprocessing era.\n", "versions": [{"version": "v1", "created": "Tue, 16 May 2017 08:29:00 GMT"}], "update_date": "2017-05-17", "authors_parsed": [["Ojha", "Varun Kumar", ""], ["Abraham", "Ajith", ""], ["Sn\u00e1\u0161el", "V\u00e1clav", ""]]}, {"id": "1705.05591", "submitter": "Ha Nguyen", "authors": "Ha Q. Nguyen and Emrah Bostan and Michael Unser", "title": "Learning Convex Regularizers for Optimal Bayesian Denoising", "comments": null, "journal-ref": null, "doi": "10.1109/TSP.2017.2777407", "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a data-driven algorithm for the maximum a posteriori (MAP)\nestimation of stochastic processes from noisy observations. The primary\nstatistical properties of the sought signal is specified by the penalty\nfunction (i.e., negative logarithm of the prior probability density function).\nOur alternating direction method of multipliers (ADMM)-based approach\ntranslates the estimation task into successive applications of the proximal\nmapping of the penalty function. Capitalizing on this direct link, we define\nthe proximal operator as a parametric spline curve and optimize the spline\ncoefficients by minimizing the average reconstruction error for a given\ntraining set. The key aspects of our learning method are that the associated\npenalty function is constrained to be convex and the convergence of the ADMM\niterations is proven. As a result of these theoretical guarantees, adaptation\nof the proposed framework to different levels of measurement noise is extremely\nsimple and does not require any retraining. We apply our method to estimation\nof both sparse and non-sparse models of L\\'{e}vy processes for which the\nminimum mean square error (MMSE) estimators are available. We carry out a\nsingle training session and perform comparisons at various signal-to-noise\nratio (SNR) values. Simulations illustrate that the performance of our\nalgorithm is practically identical to the one of the MMSE estimator\nirrespective of the noise power.\n", "versions": [{"version": "v1", "created": "Tue, 16 May 2017 08:39:46 GMT"}], "update_date": "2018-02-14", "authors_parsed": [["Nguyen", "Ha Q.", ""], ["Bostan", "Emrah", ""], ["Unser", "Michael", ""]]}, {"id": "1705.05598", "submitter": "Pieter-Jan Kindermans", "authors": "Pieter-Jan Kindermans, Kristof T. Sch\\\"utt, Maximilian Alber,\n  Klaus-Robert M\\\"uller, Dumitru Erhan, Been Kim, Sven D\\\"ahne", "title": "Learning how to explain neural networks: PatternNet and\n  PatternAttribution", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  DeConvNet, Guided BackProp, LRP, were invented to better understand deep\nneural networks. We show that these methods do not produce the theoretically\ncorrect explanation for a linear model. Yet they are used on multi-layer\nnetworks with millions of parameters. This is a cause for concern since linear\nmodels are simple neural networks. We argue that explanation methods for neural\nnets should work reliably in the limit of simplicity, the linear models. Based\non our analysis of linear models we propose a generalization that yields two\nexplanation techniques (PatternNet and PatternAttribution) that are\ntheoretically sound for linear models and produce improved explanations for\ndeep networks.\n", "versions": [{"version": "v1", "created": "Tue, 16 May 2017 08:58:25 GMT"}, {"version": "v2", "created": "Tue, 24 Oct 2017 23:10:33 GMT"}], "update_date": "2017-10-26", "authors_parsed": [["Kindermans", "Pieter-Jan", ""], ["Sch\u00fctt", "Kristof T.", ""], ["Alber", "Maximilian", ""], ["M\u00fcller", "Klaus-Robert", ""], ["Erhan", "Dumitru", ""], ["Kim", "Been", ""], ["D\u00e4hne", "Sven", ""]]}, {"id": "1705.05615", "submitter": "Sami Abu-El-Haija", "authors": "Sami Abu-El-Haija, Bryan Perozzi, Rami Al-Rfou", "title": "Learning Edge Representations via Low-Rank Asymmetric Projections", "comments": null, "journal-ref": "ACM International Conference on Information and Knowledge\n  Management, 2017", "doi": "10.1145/3132847.3132959", "report-no": null, "categories": "cs.LG cs.SI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new method for embedding graphs while preserving directed edge\ninformation. Learning such continuous-space vector representations (or\nembeddings) of nodes in a graph is an important first step for using network\ninformation (from social networks, user-item graphs, knowledge bases, etc.) in\nmany machine learning tasks.\n  Unlike previous work, we (1) explicitly model an edge as a function of node\nembeddings, and we (2) propose a novel objective, the \"graph likelihood\", which\ncontrasts information from sampled random walks with non-existent edges.\nIndividually, both of these contributions improve the learned representations,\nespecially when there are memory constraints on the total size of the\nembeddings. When combined, our contributions enable us to significantly improve\nthe state-of-the-art by learning more concise representations that better\npreserve the graph structure.\n  We evaluate our method on a variety of link-prediction task including social\nnetworks, collaboration networks, and protein interactions, showing that our\nproposed method learn representations with error reductions of up to 76% and\n55%, on directed and undirected graphs. In addition, we show that the\nrepresentations learned by our method are quite space efficient, producing\nembeddings which have higher structure-preserving accuracy but are 10 times\nsmaller.\n", "versions": [{"version": "v1", "created": "Tue, 16 May 2017 09:44:28 GMT"}, {"version": "v2", "created": "Wed, 17 May 2017 23:15:39 GMT"}, {"version": "v3", "created": "Mon, 29 May 2017 12:00:41 GMT"}, {"version": "v4", "created": "Wed, 13 Sep 2017 18:21:14 GMT"}], "update_date": "2017-09-15", "authors_parsed": [["Abu-El-Haija", "Sami", ""], ["Perozzi", "Bryan", ""], ["Al-Rfou", "Rami", ""]]}, {"id": "1705.05633", "submitter": "Tao Ding", "authors": "Tao Ding, Warren K. Bickel, Shimei Pan", "title": "Social Media-based Substance Use Prediction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we demonstrate how the state-of-the-art machine learning and\ntext mining techniques can be used to build effective social media-based\nsubstance use detection systems. Since a substance use ground truth is\ndifficult to obtain on a large scale, to maximize system performance, we\nexplore different feature learning methods to take advantage of a large amount\nof unsupervised social media data. We also demonstrate the benefit of using\nmulti-view unsupervised feature learning to combine heterogeneous user\ninformation such as Facebook `\"likes\" and \"status updates\" to enhance system\nperformance. Based on our evaluation, our best models achieved 86% AUC for\npredicting tobacco use, 81% for alcohol use and 84% for drug use, all of which\nsignificantly outperformed existing methods. Our investigation has also\nuncovered interesting relations between a user's social media behavior (e.g.,\nword usage) and substance use.\n", "versions": [{"version": "v1", "created": "Tue, 16 May 2017 10:37:52 GMT"}, {"version": "v2", "created": "Wed, 31 May 2017 19:38:14 GMT"}], "update_date": "2017-06-02", "authors_parsed": [["Ding", "Tao", ""], ["Bickel", "Warren K.", ""], ["Pan", "Shimei", ""]]}, {"id": "1705.05654", "submitter": "Philipp Probst", "authors": "Philipp Probst, Anne-Laure Boulesteix", "title": "To tune or not to tune the number of trees in random forest?", "comments": "20 pages, 4 figures", "journal-ref": "Journal of Machine Learning Research 18 (2018) 1-18", "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The number of trees T in the random forest (RF) algorithm for supervised\nlearning has to be set by the user. It is controversial whether T should simply\nbe set to the largest computationally manageable value or whether a smaller T\nmay in some cases be better. While the principle underlying bagging is that\n\"more trees are better\", in practice the classification error rate sometimes\nreaches a minimum before increasing again for increasing number of trees. The\ngoal of this paper is four-fold: (i) providing theoretical results showing that\nthe expected error rate may be a non-monotonous function of the number of trees\nand explaining under which circumstances this happens; (ii) providing\ntheoretical results showing that such non-monotonous patterns cannot be\nobserved for other performance measures such as the Brier score and the\nlogarithmic loss (for classification) and the mean squared error (for\nregression); (iii) illustrating the extent of the problem through an\napplication to a large number (n = 306) of datasets from the public database\nOpenML; (iv) finally arguing in favor of setting it to a computationally\nfeasible large number, depending on convergence properties of the desired\nperformance measure.\n", "versions": [{"version": "v1", "created": "Tue, 16 May 2017 11:38:12 GMT"}], "update_date": "2019-03-11", "authors_parsed": [["Probst", "Philipp", ""], ["Boulesteix", "Anne-Laure", ""]]}, {"id": "1705.05665", "submitter": "Yao Lu", "authors": "Yao Lu, Zhirong Yang, Juho Kannala, Samuel Kaski", "title": "Learning Image Relations with Contrast Association Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Inferring the relations between two images is an important class of tasks in\ncomputer vision. Examples of such tasks include computing optical flow and\nstereo disparity. We treat the relation inference tasks as a machine learning\nproblem and tackle it with neural networks. A key to the problem is learning a\nrepresentation of relations. We propose a new neural network module, contrast\nassociation unit (CAU), which explicitly models the relations between two sets\nof input variables. Due to the non-negativity of the weights in CAU, we adopt a\nmultiplicative update algorithm for learning these weights. Experiments show\nthat neural networks with CAUs are more effective in learning five fundamental\nimage transformations than conventional neural networks.\n", "versions": [{"version": "v1", "created": "Tue, 16 May 2017 12:09:44 GMT"}, {"version": "v2", "created": "Mon, 11 Mar 2019 21:44:39 GMT"}], "update_date": "2019-03-13", "authors_parsed": [["Lu", "Yao", ""], ["Yang", "Zhirong", ""], ["Kannala", "Juho", ""], ["Kaski", "Samuel", ""]]}, {"id": "1705.05681", "submitter": "Brijnesh Jain", "authors": "Brijnesh J. Jain and David Schultz", "title": "Optimal Warping Paths are unique for almost every Pair of Time Series", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Update rules for learning in dynamic time warping spaces are based on optimal\nwarping paths between parameter and input time series. In general, optimal\nwarping paths are not unique resulting in adverse effects in theory and\npractice. Under the assumption of squared error local costs, we show that no\ntwo warping paths have identical costs almost everywhere in a measure-theoretic\nsense. Two direct consequences of this result are: (i) optimal warping paths\nare unique almost everywhere, and (ii) the set of all pairs of time series with\nmultiple equal-cost warping paths coincides with the union of exponentially\nmany zero sets of quadratic forms. One implication of the proposed results is\nthat typical distance-based cost functions such as the k-means objective are\ndifferentiable almost everywhere and can be minimized by subgradient methods.\n", "versions": [{"version": "v1", "created": "Tue, 16 May 2017 12:41:25 GMT"}, {"version": "v2", "created": "Fri, 2 Mar 2018 11:18:36 GMT"}], "update_date": "2018-03-05", "authors_parsed": [["Jain", "Brijnesh J.", ""], ["Schultz", "David", ""]]}, {"id": "1705.05690", "submitter": "Abdelhadi Azzouni", "authors": "Abdelhadi Azzouni, Guy Pujolle", "title": "A Long Short-Term Memory Recurrent Neural Network Framework for Network\n  Traffic Matrix Prediction", "comments": "Submitted for peer review. arXiv admin note: text overlap with\n  arXiv:1402.1128 by other authors", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Network Traffic Matrix (TM) prediction is defined as the problem of\nestimating future network traffic from the previous and achieved network\ntraffic data. It is widely used in network planning, resource management and\nnetwork security. Long Short-Term Memory (LSTM) is a specific recurrent neural\nnetwork (RNN) architecture that is well-suited to learn from experience to\nclassify, process and predict time series with time lags of unknown size. LSTMs\nhave been shown to model temporal sequences and their long-range dependencies\nmore accurately than conventional RNNs. In this paper, we propose a LSTM RNN\nframework for predicting short and long term Traffic Matrix (TM) in large\nnetworks. By validating our framework on real-world data from GEANT network, we\nshow that our LSTM models converge quickly and give state of the art TM\nprediction performance for relatively small sized models.\n", "versions": [{"version": "v1", "created": "Tue, 16 May 2017 12:57:24 GMT"}, {"version": "v2", "created": "Sat, 20 May 2017 12:33:10 GMT"}, {"version": "v3", "created": "Thu, 8 Jun 2017 12:31:20 GMT"}], "update_date": "2017-06-12", "authors_parsed": [["Azzouni", "Abdelhadi", ""], ["Pujolle", "Guy", ""]]}, {"id": "1705.05742", "submitter": "Rakshit Trivedi", "authors": "Rakshit Trivedi, Hanjun Dai, Yichen Wang, Le Song", "title": "Know-Evolve: Deep Temporal Reasoning for Dynamic Knowledge Graphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The availability of large scale event data with time stamps has given rise to\ndynamically evolving knowledge graphs that contain temporal information for\neach edge. Reasoning over time in such dynamic knowledge graphs is not yet well\nunderstood. To this end, we present Know-Evolve, a novel deep evolutionary\nknowledge network that learns non-linearly evolving entity representations over\ntime. The occurrence of a fact (edge) is modeled as a multivariate point\nprocess whose intensity function is modulated by the score for that fact\ncomputed based on the learned entity embeddings. We demonstrate significantly\nimproved performance over various relational learning approaches on two large\nscale real-world datasets. Further, our method effectively predicts occurrence\nor recurrence time of a fact which is novel compared to prior reasoning\napproaches in multi-relational setting.\n", "versions": [{"version": "v1", "created": "Tue, 16 May 2017 14:53:02 GMT"}, {"version": "v2", "created": "Wed, 17 May 2017 04:54:07 GMT"}, {"version": "v3", "created": "Wed, 21 Jun 2017 05:21:46 GMT"}], "update_date": "2017-06-22", "authors_parsed": [["Trivedi", "Rakshit", ""], ["Dai", "Hanjun", ""], ["Wang", "Yichen", ""], ["Song", "Le", ""]]}, {"id": "1705.05782", "submitter": "Claudio Gallicchio", "authors": "Claudio Gallicchio, Alessio Micheli, Luca Pedrelli", "title": "Hierarchical Temporal Representation in Linear Reservoir Computing", "comments": "This is a pre-print of the paper submitted to the 27th Italian\n  Workshop on Neural Networks, WIRN 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, studies on deep Reservoir Computing (RC) highlighted the role of\nlayering in deep recurrent neural networks (RNNs). In this paper, the use of\nlinear recurrent units allows us to bring more evidence on the intrinsic\nhierarchical temporal representation in deep RNNs through frequency analysis\napplied to the state signals. The potentiality of our approach is assessed on\nthe class of Multiple Superimposed Oscillator tasks. Furthermore, our\ninvestigation provides useful insights to open a discussion on the main aspects\nthat characterize the deep learning framework in the temporal domain.\n", "versions": [{"version": "v1", "created": "Tue, 16 May 2017 16:03:00 GMT"}, {"version": "v2", "created": "Wed, 17 May 2017 10:30:52 GMT"}, {"version": "v3", "created": "Mon, 5 Jun 2017 12:03:06 GMT"}, {"version": "v4", "created": "Thu, 8 Jun 2017 16:00:15 GMT"}, {"version": "v5", "created": "Mon, 10 Jul 2017 10:51:24 GMT"}], "update_date": "2017-07-11", "authors_parsed": [["Gallicchio", "Claudio", ""], ["Micheli", "Alessio", ""], ["Pedrelli", "Luca", ""]]}, {"id": "1705.05785", "submitter": "Sebastijan Dumancic", "authors": "Sebastijan Duman\\v{c}i\\'c and Hendrik Blockeel", "title": "Demystifying Relational Latent Representations", "comments": "12 pages, 8 figures; accepted to ILP 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Latent features learned by deep learning approaches have proven to be a\npowerful tool for machine learning. They serve as a data abstraction that makes\nlearning easier by capturing regularities in data explicitly. Their benefits\nmotivated their adaptation to relational learning context. In our previous\nwork, we introduce an approach that learns relational latent features by means\nof clustering instances and their relations. The major drawback of latent\nrepresentations is that they are often black-box and difficult to interpret.\nThis work addresses these issues and shows that (1) latent features created by\nclustering are interpretable and capture interesting properties of data; (2)\nthey identify local regions of instances that match well with the label, which\npartially explains their benefit; and (3) although the number of latent\nfeatures generated by this approach is large, often many of them are highly\nredundant and can be removed without hurting performance much.\n", "versions": [{"version": "v1", "created": "Tue, 16 May 2017 16:06:59 GMT"}, {"version": "v2", "created": "Thu, 28 Sep 2017 13:46:17 GMT"}, {"version": "v3", "created": "Fri, 29 Sep 2017 07:36:32 GMT"}], "update_date": "2017-10-02", "authors_parsed": [["Duman\u010di\u0107", "Sebastijan", ""], ["Blockeel", "Hendrik", ""]]}, {"id": "1705.05823", "submitter": "Oren Rippel", "authors": "Oren Rippel, Lubomir Bourdev", "title": "Real-Time Adaptive Image Compression", "comments": "Published at ICML 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a machine learning-based approach to lossy image compression which\noutperforms all existing codecs, while running in real-time.\n  Our algorithm typically produces files 2.5 times smaller than JPEG and JPEG\n2000, 2 times smaller than WebP, and 1.7 times smaller than BPG on datasets of\ngeneric images across all quality levels. At the same time, our codec is\ndesigned to be lightweight and deployable: for example, it can encode or decode\nthe Kodak dataset in around 10ms per image on GPU.\n  Our architecture is an autoencoder featuring pyramidal analysis, an adaptive\ncoding module, and regularization of the expected codelength. We also\nsupplement our approach with adversarial training specialized towards use in a\ncompression setting: this enables us to produce visually pleasing\nreconstructions for very low bitrates.\n", "versions": [{"version": "v1", "created": "Tue, 16 May 2017 17:51:07 GMT"}], "update_date": "2017-05-17", "authors_parsed": [["Rippel", "Oren", ""], ["Bourdev", "Lubomir", ""]]}, {"id": "1705.05919", "submitter": "Maxat Kulmanov", "authors": "Maxat Kulmanov, Mohammed Asif Khan and Robert Hoehndorf", "title": "DeepGO: Predicting protein functions from sequence and interactions\n  using a deep ontology-aware classifier", "comments": null, "journal-ref": null, "doi": "10.1093/bioinformatics/btx624", "report-no": null, "categories": "q-bio.GN cs.LG q-bio.QM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A large number of protein sequences are becoming available through the\napplication of novel high-throughput sequencing technologies. Experimental\nfunctional characterization of these proteins is time-consuming and expensive,\nand is often only done rigorously for few selected model organisms.\nComputational function prediction approaches have been suggested to fill this\ngap. The functions of proteins are classified using the Gene Ontology (GO),\nwhich contains over 40,000 classes. Additionally, proteins have multiple\nfunctions, making function prediction a large-scale, multi-class, multi-label\nproblem.\n  We have developed a novel method to predict protein function from sequence.\nWe use deep learning to learn features from protein sequences as well as a\ncross-species protein-protein interaction network. Our approach specifically\noutputs information in the structure of the GO and utilizes the dependencies\nbetween GO classes as background information to construct a deep learning\nmodel. We evaluate our method using the standards established by the\nComputational Assessment of Function Annotation (CAFA) and demonstrate a\nsignificant improvement over baseline methods such as BLAST, with significant\nimprovement for predicting cellular locations.\n", "versions": [{"version": "v1", "created": "Mon, 15 May 2017 06:04:08 GMT"}], "update_date": "2017-09-28", "authors_parsed": [["Kulmanov", "Maxat", ""], ["Khan", "Mohammed Asif", ""], ["Hoehndorf", "Robert", ""]]}, {"id": "1705.05933", "submitter": "Aurelien Lucchi", "authors": "Jonas Moritz Kohler and Aurelien Lucchi", "title": "Sub-sampled Cubic Regularization for Non-convex Optimization", "comments": "Proceedings of the 34th International Conference on Machine Learning", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the minimization of non-convex functions that typically arise in\nmachine learning. Specifically, we focus our attention on a variant of trust\nregion methods known as cubic regularization. This approach is particularly\nattractive because it escapes strict saddle points and it provides stronger\nconvergence guarantees than first- and second-order as well as classical trust\nregion methods. However, it suffers from a high computational complexity that\nmakes it impractical for large-scale learning. Here, we propose a novel method\nthat uses sub-sampling to lower this computational cost. By the use of\nconcentration inequalities we provide a sampling scheme that gives sufficiently\naccurate gradient and Hessian approximations to retain the strong global and\nlocal convergence guarantees of cubically regularized methods. To the best of\nour knowledge this is the first work that gives global convergence guarantees\nfor a sub-sampled variant of cubic regularization on non-convex functions.\nFurthermore, we provide experimental results supporting our theory.\n", "versions": [{"version": "v1", "created": "Tue, 16 May 2017 21:44:44 GMT"}, {"version": "v2", "created": "Fri, 23 Jun 2017 11:49:57 GMT"}, {"version": "v3", "created": "Sat, 1 Jul 2017 11:17:59 GMT"}], "update_date": "2017-07-04", "authors_parsed": [["Kohler", "Jonas Moritz", ""], ["Lucchi", "Aurelien", ""]]}, {"id": "1705.06000", "submitter": "Abhishek Sharma", "authors": "Abhishek Sharma", "title": "One Shot Joint Colocalization and Cosegmentation", "comments": "8 pages, Under Review", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a novel framework in which image cosegmentation and\ncolocalization are cast into a single optimization problem that integrates\ninformation from low level appearance cues with that of high level localization\ncues in a very weakly supervised manner. In contrast to multi-task learning\nparadigm that learns similar tasks using a shared representation, the proposed\nframework leverages two representations at different levels and simultaneously\ndiscriminates between foreground and background at the bounding box and\nsuperpixel level using discriminative clustering. We show empirically that\nconstraining the two problems at different scales enables the transfer of\nsemantic localization cues to improve cosegmentation output whereas local\nappearance based segmentation cues help colocalization. The unified framework\noutperforms strong baseline approaches, of learning the two problems\nseparately, by a large margin on four benchmark datasets. Furthermore, it\nobtains competitive results compared to the state of the art for cosegmentation\non two benchmark datasets and second best result for colocalization on Pascal\nVOC 2007.\n", "versions": [{"version": "v1", "created": "Wed, 17 May 2017 04:18:19 GMT"}], "update_date": "2017-05-18", "authors_parsed": [["Sharma", "Abhishek", ""]]}, {"id": "1705.06211", "submitter": "Albert Berahas", "authors": "Albert S. Berahas, Raghu Bollapragada and Jorge Nocedal", "title": "An Investigation of Newton-Sketch and Subsampled Newton Methods", "comments": "36 pages, 22 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sketching, a dimensionality reduction technique, has received much attention\nin the statistics community. In this paper, we study sketching in the context\nof Newton's method for solving finite-sum optimization problems in which the\nnumber of variables and data points are both large. We study two forms of\nsketching that perform dimensionality reduction in data space: Hessian\nsubsampling and randomized Hadamard transformations. Each has its own\nadvantages, and their relative tradeoffs have not been investigated in the\noptimization literature. Our study focuses on practical versions of the two\nmethods in which the resulting linear systems of equations are solved\napproximately, at every iteration, using an iterative solver. The advantages of\nusing the conjugate gradient method vs. a stochastic gradient iteration are\nrevealed through a set of numerical experiments, and a complexity analysis of\nthe Hessian subsampling method is presented.\n", "versions": [{"version": "v1", "created": "Wed, 17 May 2017 15:31:37 GMT"}, {"version": "v2", "created": "Sat, 20 May 2017 01:07:15 GMT"}, {"version": "v3", "created": "Tue, 24 Jul 2018 21:12:26 GMT"}, {"version": "v4", "created": "Thu, 30 May 2019 23:01:20 GMT"}], "update_date": "2019-06-03", "authors_parsed": [["Berahas", "Albert S.", ""], ["Bollapragada", "Raghu", ""], ["Nocedal", "Jorge", ""]]}, {"id": "1705.06224", "submitter": "Kleomenis Katevas", "authors": "Kleomenis Katevas, Ilias Leontiadis, Martin Pielot, Joan Serr\\`a", "title": "Practical Processing of Mobile Sensor Data for Continual Deep Learning\n  Predictions", "comments": "6 pages, 3 figures, 3 tables", "journal-ref": "DeepMobile Workshop, MobileHCI 2017", "doi": "10.1145/3089801.3089802", "report-no": null, "categories": "cs.LG cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a practical approach for processing mobile sensor time series data\nfor continual deep learning predictions. The approach comprises data cleaning,\nnormalization, capping, time-based compression, and finally classification with\na recurrent neural network. We demonstrate the effectiveness of the approach in\na case study with 279 participants. On the basis of sparse sensor events, the\nnetwork continually predicts whether the participants would attend to a\nnotification within 10 minutes. Compared to a random baseline, the classifier\nachieves a 40% performance increase (AUC of 0.702) on a withheld test set. This\napproach allows to forgo resource-intensive, domain-specific, error-prone\nfeature engineering, which may drastically increase the applicability of\nmachine learning to mobile phone sensor data.\n", "versions": [{"version": "v1", "created": "Wed, 17 May 2017 15:55:53 GMT"}], "update_date": "2017-05-22", "authors_parsed": [["Katevas", "Kleomenis", ""], ["Leontiadis", "Ilias", ""], ["Pielot", "Martin", ""], ["Serr\u00e0", "Joan", ""]]}, {"id": "1705.06243", "submitter": "Jaeyong Sung", "authors": "Jaeyong Sung, J. Kenneth Salisbury, Ashutosh Saxena", "title": "Learning to Represent Haptic Feedback for Partially-Observable Tasks", "comments": "IEEE International Conference on Robotics and Automation (ICRA), 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The sense of touch, being the earliest sensory system to develop in a human\nbody [1], plays a critical part of our daily interaction with the environment.\nIn order to successfully complete a task, many manipulation interactions\nrequire incorporating haptic feedback. However, manually designing a feedback\nmechanism can be extremely challenging. In this work, we consider manipulation\ntasks that need to incorporate tactile sensor feedback in order to modify a\nprovided nominal plan. To incorporate partial observation, we present a new\nframework that models the task as a partially observable Markov decision\nprocess (POMDP) and learns an appropriate representation of haptic feedback\nwhich can serve as the state for a POMDP model. The model, that is parametrized\nby deep recurrent neural networks, utilizes variational Bayes methods to\noptimize the approximate posterior. Finally, we build on deep Q-learning to be\nable to select the optimal action in each state without access to a simulator.\nWe test our model on a PR2 robot for multiple tasks of turning a knob until it\nclicks.\n", "versions": [{"version": "v1", "created": "Wed, 17 May 2017 16:21:56 GMT"}], "update_date": "2017-05-18", "authors_parsed": [["Sung", "Jaeyong", ""], ["Salisbury", "J. Kenneth", ""], ["Saxena", "Ashutosh", ""]]}, {"id": "1705.06299", "submitter": "Mohammad Bari", "authors": "Mohammad Bari, Hussain Taher, Syed Saad Sherazi, Milos Doroslovacki", "title": "Supervised Machine Learning for Signals Having RRC Shaped Pulses", "comments": "5 pages", "journal-ref": "2016 50th Asilomar Conference on Signals, Systems, and Computers", "doi": "10.1109/ACSSC.2016.7869124", "report-no": null, "categories": "cs.IT cs.LG math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Classification performances of the supervised machine learning techniques\nsuch as support vector machines, neural networks and logistic regression are\ncompared for modulation recognition purposes. The simple and robust features\nare used to distinguish continuous-phase FSK from QAM-PSK signals. Signals\nhaving root-raised-cosine shaped pulses are simulated in extreme noisy\nconditions having joint impurities of block fading, lack of symbol and sampling\nsynchronization, carrier offset, and additive white Gaussian noise. The\nfeatures are based on sample mean and sample variance of the imaginary part of\nthe product of two consecutive complex signal values.\n", "versions": [{"version": "v1", "created": "Wed, 17 May 2017 18:17:00 GMT"}], "update_date": "2017-05-19", "authors_parsed": [["Bari", "Mohammad", ""], ["Taher", "Hussain", ""], ["Sherazi", "Syed Saad", ""], ["Doroslovacki", "Milos", ""]]}, {"id": "1705.06366", "submitter": "Carlos Florensa", "authors": "Carlos Florensa, David Held, Xinyang Geng, Pieter Abbeel", "title": "Automatic Goal Generation for Reinforcement Learning Agents", "comments": "Accepted at ICML 2018, Proceedings of the 35th International\n  Conference on Machine Learning, Stockholm, Sweden, PMLR 80, 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Reinforcement learning is a powerful technique to train an agent to perform a\ntask. However, an agent that is trained using reinforcement learning is only\ncapable of achieving the single task that is specified via its reward function.\nSuch an approach does not scale well to settings in which an agent needs to\nperform a diverse set of tasks, such as navigating to varying positions in a\nroom or moving objects to varying locations. Instead, we propose a method that\nallows an agent to automatically discover the range of tasks that it is capable\nof performing. We use a generator network to propose tasks for the agent to try\nto achieve, specified as goal states. The generator network is optimized using\nadversarial training to produce tasks that are always at the appropriate level\nof difficulty for the agent. Our method thus automatically produces a\ncurriculum of tasks for the agent to learn. We show that, by using this\nframework, an agent can efficiently and automatically learn to perform a wide\nset of tasks without requiring any prior knowledge of its environment. Our\nmethod can also learn to achieve tasks with sparse rewards, which traditionally\npose significant challenges.\n", "versions": [{"version": "v1", "created": "Wed, 17 May 2017 23:05:46 GMT"}, {"version": "v2", "created": "Wed, 31 May 2017 17:46:36 GMT"}, {"version": "v3", "created": "Mon, 4 Jun 2018 07:43:29 GMT"}, {"version": "v4", "created": "Tue, 17 Jul 2018 16:25:52 GMT"}, {"version": "v5", "created": "Mon, 23 Jul 2018 09:25:37 GMT"}], "update_date": "2018-07-24", "authors_parsed": [["Florensa", "Carlos", ""], ["Held", "David", ""], ["Geng", "Xinyang", ""], ["Abbeel", "Pieter", ""]]}, {"id": "1705.06371", "submitter": "Robert Durrant", "authors": "Xianghui Luo and Robert J. Durrant", "title": "Maximum Margin Principal Components", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Principal Component Analysis (PCA) is a very successful dimensionality\nreduction technique, widely used in predictive modeling. A key factor in its\nwidespread use in this domain is the fact that the projection of a dataset onto\nits first $K$ principal components minimizes the sum of squared errors between\nthe original data and the projected data over all possible rank $K$\nprojections. Thus, PCA provides optimal low-rank representations of data for\nleast-squares linear regression under standard modeling assumptions. On the\nother hand, when the loss function for a prediction problem is not the\nleast-squares error, PCA is typically a heuristic choice of dimensionality\nreduction -- in particular for classification problems under the zero-one loss.\nIn this paper we target classification problems by proposing a straightforward\nalternative to PCA that aims to minimize the difference in margin distribution\nbetween the original and the projected data. Extensive experiments show that\nour simple approach typically outperforms PCA on any particular dataset, in\nterms of classification error, though this difference is not always\nstatistically significant, and despite being a filter method is frequently\ncompetitive with Partial Least Squares (PLS) and Lasso on a wide range of\ndatasets.\n", "versions": [{"version": "v1", "created": "Wed, 17 May 2017 23:45:11 GMT"}], "update_date": "2017-05-19", "authors_parsed": [["Luo", "Xianghui", ""], ["Durrant", "Robert J.", ""]]}, {"id": "1705.06400", "submitter": "Matthias Plappert", "authors": "Matthias Plappert, Christian Mandery, Tamim Asfour", "title": "Learning a bidirectional mapping between human whole-body motion and\n  natural language using deep recurrent neural networks", "comments": null, "journal-ref": null, "doi": "10.1016/j.robot.2018.07.006", "report-no": null, "categories": "cs.LG cs.CL cs.RO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Linking human whole-body motion and natural language is of great interest for\nthe generation of semantic representations of observed human behaviors as well\nas for the generation of robot behaviors based on natural language input. While\nthere has been a large body of research in this area, most approaches that\nexist today require a symbolic representation of motions (e.g. in the form of\nmotion primitives), which have to be defined a-priori or require complex\nsegmentation algorithms. In contrast, recent advances in the field of neural\nnetworks and especially deep learning have demonstrated that sub-symbolic\nrepresentations that can be learned end-to-end usually outperform more\ntraditional approaches, for applications such as machine translation. In this\npaper we propose a generative model that learns a bidirectional mapping between\nhuman whole-body motion and natural language using deep recurrent neural\nnetworks (RNNs) and sequence-to-sequence learning. Our approach does not\nrequire any segmentation or manual feature engineering and learns a distributed\nrepresentation, which is shared for all motions and descriptions. We evaluate\nour approach on 2,846 human whole-body motions and 6,187 natural language\ndescriptions thereof from the KIT Motion-Language Dataset. Our results clearly\ndemonstrate the effectiveness of the proposed model: We show that our model\ngenerates a wide variety of realistic motions only from descriptions thereof in\nform of a single sentence. Conversely, our model is also capable of generating\ncorrect and detailed natural language descriptions from human motions.\n", "versions": [{"version": "v1", "created": "Thu, 18 May 2017 02:50:40 GMT"}, {"version": "v2", "created": "Thu, 2 Aug 2018 10:07:57 GMT"}], "update_date": "2018-08-06", "authors_parsed": [["Plappert", "Matthias", ""], ["Mandery", "Christian", ""], ["Asfour", "Tamim", ""]]}, {"id": "1705.06412", "submitter": "Gauri Jagatap", "authors": "Gauri Jagatap and Chinmay Hegde", "title": "Sample-Efficient Algorithms for Recovering Structured Signals from\n  Magnitude-Only Measurements", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of recovering a signal $\\mathbf{x}^* \\in\n\\mathbf{R}^n$, from magnitude-only measurements $y_i =\n|\\left\\langle\\mathbf{a}_i,\\mathbf{x}^*\\right\\rangle|$ for $i=[m]$. Also called\nthe phase retrieval, this is a fundamental challenge in bio-,astronomical\nimaging and speech processing. The problem above is ill-posed; additional\nassumptions on the signal and/or the measurements are necessary. In this paper\nwe first study the case where the signal $\\mathbf{x}^*$ is $s$-sparse. We\ndevelop a novel algorithm that we call Compressive Phase Retrieval with\nAlternating Minimization, or CoPRAM. Our algorithm is simple; it combines the\nclassical alternating minimization approach for phase retrieval with the CoSaMP\nalgorithm for sparse recovery. Despite its simplicity, we prove that CoPRAM\nachieves a sample complexity of $O(s^2\\log n)$ with Gaussian measurements\n$\\mathbf{a}_i$, matching the best known existing results; moreover, it\ndemonstrates linear convergence in theory and practice. Additionally, it\nrequires no extra tuning parameters other than signal sparsity $s$ and is\nrobust to noise. When the sorted coefficients of the sparse signal exhibit a\npower law decay, we show that CoPRAM achieves a sample complexity of $O(s\\log\nn)$, which is close to the information-theoretic limit. We also consider the\ncase where the signal $\\mathbf{x}^*$ arises from structured sparsity models. We\nspecifically examine the case of block-sparse signals with uniform block size\nof $b$ and block sparsity $k=s/b$. For this problem, we design a recovery\nalgorithm Block CoPRAM that further reduces the sample complexity to $O(ks\\log\nn)$. For sufficiently large block lengths of $b=\\Theta(s)$, this bound equates\nto $O(s\\log n)$. To our knowledge, this constitutes the first end-to-end\nalgorithm for phase retrieval where the Gaussian sample complexity has a\nsub-quadratic dependence on the signal sparsity level.\n", "versions": [{"version": "v1", "created": "Thu, 18 May 2017 04:26:01 GMT"}, {"version": "v2", "created": "Sun, 26 Nov 2017 21:14:14 GMT"}], "update_date": "2017-11-28", "authors_parsed": [["Jagatap", "Gauri", ""], ["Hegde", "Chinmay", ""]]}, {"id": "1705.06452", "submitter": "Jernej Kos", "authors": "Jernej Kos, Dawn Song", "title": "Delving into adversarial attacks on deep policies", "comments": "ICLR 2017 Workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Adversarial examples have been shown to exist for a variety of deep learning\narchitectures. Deep reinforcement learning has shown promising results on\ntraining agent policies directly on raw inputs such as image pixels. In this\npaper we present a novel study into adversarial attacks on deep reinforcement\nlearning polices. We compare the effectiveness of the attacks using adversarial\nexamples vs. random noise. We present a novel method for reducing the number of\ntimes adversarial examples need to be injected for a successful attack, based\non the value function. We further explore how re-training on random noise and\nFGSM perturbations affects the resilience against adversarial examples.\n", "versions": [{"version": "v1", "created": "Thu, 18 May 2017 08:01:53 GMT"}], "update_date": "2017-05-19", "authors_parsed": [["Kos", "Jernej", ""], ["Song", "Dawn", ""]]}, {"id": "1705.06460", "submitter": "Mahardhika Pratama Dr", "authors": "Mahardhika Pratama, Witold Pedrycz, Edwin Lughofer", "title": "Evolving Ensemble Fuzzy Classifier", "comments": "this paper has been published by IEEE Transactions on Fuzzy Systems", "journal-ref": "IEEE Transactions on Fuzzy Systems, 2018", "doi": "10.1109/TFUZZ.2018.2796099", "report-no": null, "categories": "cs.LG cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The concept of ensemble learning offers a promising avenue in learning from\ndata streams under complex environments because it addresses the bias and\nvariance dilemma better than its single model counterpart and features a\nreconfigurable structure, which is well suited to the given context. While\nvarious extensions of ensemble learning for mining non-stationary data streams\ncan be found in the literature, most of them are crafted under a static base\nclassifier and revisits preceding samples in the sliding window for a\nretraining step. This feature causes computationally prohibitive complexity and\nis not flexible enough to cope with rapidly changing environments. Their\ncomplexities are often demanding because it involves a large collection of\noffline classifiers due to the absence of structural complexities reduction\nmechanisms and lack of an online feature selection mechanism. A novel evolving\nensemble classifier, namely Parsimonious Ensemble pENsemble, is proposed in\nthis paper. pENsemble differs from existing architectures in the fact that it\nis built upon an evolving classifier from data streams, termed Parsimonious\nClassifier pClass. pENsemble is equipped by an ensemble pruning mechanism,\nwhich estimates a localized generalization error of a base classifier. A\ndynamic online feature selection scenario is integrated into the pENsemble.\nThis method allows for dynamic selection and deselection of input features on\nthe fly. pENsemble adopts a dynamic ensemble structure to output a final\nclassification decision where it features a novel drift detection scenario to\ngrow the ensemble structure. The efficacy of the pENsemble has been numerically\ndemonstrated through rigorous numerical studies with dynamic and evolving data\nstreams where it delivers the most encouraging performance in attaining a\ntradeoff between accuracy and complexity.\n", "versions": [{"version": "v1", "created": "Thu, 18 May 2017 08:19:41 GMT"}, {"version": "v2", "created": "Sat, 7 Dec 2019 20:26:52 GMT"}], "update_date": "2019-12-10", "authors_parsed": [["Pratama", "Mahardhika", ""], ["Pedrycz", "Witold", ""], ["Lughofer", "Edwin", ""]]}, {"id": "1705.06573", "submitter": "Pontus Svenson", "authors": "Magnus J\\\"andel, Pontus Svenson, Niclas Wadstr\\\"omer", "title": "Online learnability of Statistical Relational Learning in anomaly\n  detection", "comments": "8 pages. Author contact xpontus@gmail.com", "journal-ref": "Proc 15th Int Conf Information Fusion (2012)", "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Statistical Relational Learning (SRL) methods for anomaly detection are\nintroduced via a security-related application. Operational requirements for\nonline learning stability are outlined and compared to mathematical definitions\nas applied to the learning process of a representative SRL method - Bayesian\nLogic Programs (BLP). Since a formal proof of online stability appears to be\nimpossible, tentative common sense requirements are formulated and tested by\ntheoretical and experimental analysis of a simple and analytically tractable\nBLP model. It is found that learning algorithms in initial stages of online\nlearning can lock on unstable false predictors that nevertheless comply with\nour tentative stability requirements and thus masquerade as bona fide\nsolutions. The very expressiveness of SRL seems to cause significant stability\nissues in settings with many variables and scarce data. We conclude that\nreliable anomaly detection with SRL-methods requires monitoring by an\noverarching framework that may involve a comprehensive context knowledge base\nor human supervision.\n", "versions": [{"version": "v1", "created": "Thu, 18 May 2017 13:14:43 GMT"}], "update_date": "2017-05-19", "authors_parsed": [["J\u00e4ndel", "Magnus", ""], ["Svenson", "Pontus", ""], ["Wadstr\u00f6mer", "Niclas", ""]]}, {"id": "1705.06640", "submitter": "Kexin Pei", "authors": "Kexin Pei, Yinzhi Cao, Junfeng Yang, Suman Jana", "title": "DeepXplore: Automated Whitebox Testing of Deep Learning Systems", "comments": "To be published in SOSP'17", "journal-ref": null, "doi": "10.1145/3132747.3132785", "report-no": null, "categories": "cs.LG cs.CR cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning (DL) systems are increasingly deployed in safety- and\nsecurity-critical domains including self-driving cars and malware detection,\nwhere the correctness and predictability of a system's behavior for corner case\ninputs are of great importance. Existing DL testing depends heavily on manually\nlabeled data and therefore often fails to expose erroneous behaviors for rare\ninputs.\n  We design, implement, and evaluate DeepXplore, the first whitebox framework\nfor systematically testing real-world DL systems. First, we introduce neuron\ncoverage for systematically measuring the parts of a DL system exercised by\ntest inputs. Next, we leverage multiple DL systems with similar functionality\nas cross-referencing oracles to avoid manual checking. Finally, we demonstrate\nhow finding inputs for DL systems that both trigger many differential behaviors\nand achieve high neuron coverage can be represented as a joint optimization\nproblem and solved efficiently using gradient-based search techniques.\n  DeepXplore efficiently finds thousands of incorrect corner case behaviors\n(e.g., self-driving cars crashing into guard rails and malware masquerading as\nbenign software) in state-of-the-art DL models with thousands of neurons\ntrained on five popular datasets including ImageNet and Udacity self-driving\nchallenge data. For all tested DL models, on average, DeepXplore generated one\ntest input demonstrating incorrect behavior within one second while running\nonly on a commodity laptop. We further show that the test inputs generated by\nDeepXplore can also be used to retrain the corresponding DL model to improve\nthe model's accuracy by up to 3%.\n", "versions": [{"version": "v1", "created": "Thu, 18 May 2017 15:09:39 GMT"}, {"version": "v2", "created": "Sun, 4 Jun 2017 23:40:43 GMT"}, {"version": "v3", "created": "Mon, 10 Jul 2017 17:05:40 GMT"}, {"version": "v4", "created": "Sun, 24 Sep 2017 15:55:11 GMT"}], "update_date": "2017-09-26", "authors_parsed": [["Pei", "Kexin", ""], ["Cao", "Yinzhi", ""], ["Yang", "Junfeng", ""], ["Jana", "Suman", ""]]}, {"id": "1705.06693", "submitter": "Ilya Loshchilov", "authors": "Ilya Loshchilov, Tobias Glasmachers, Hans-Georg Beyer", "title": "Limited-Memory Matrix Adaptation for Large Scale Black-box Optimization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.LG math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Covariance Matrix Adaptation Evolution Strategy (CMA-ES) is a popular\nmethod to deal with nonconvex and/or stochastic optimization problems when the\ngradient information is not available. Being based on the CMA-ES, the recently\nproposed Matrix Adaptation Evolution Strategy (MA-ES) provides a rather\nsurprising result that the covariance matrix and all associated operations\n(e.g., potentially unstable eigendecomposition) can be replaced in the CMA-ES\nby a updated transformation matrix without any loss of performance. In order to\nfurther simplify MA-ES and reduce its $\\mathcal{O}\\big(n^2\\big)$ time and\nstorage complexity to $\\mathcal{O}\\big(n\\log(n)\\big)$, we present the\nLimited-Memory Matrix Adaptation Evolution Strategy (LM-MA-ES) for efficient\nzeroth order large-scale optimization. The algorithm demonstrates\nstate-of-the-art performance on a set of established large-scale benchmarks. We\nexplore the algorithm on the problem of generating adversarial inputs for a\n(non-smooth) random forest classifier, demonstrating a surprising vulnerability\nof the classifier.\n", "versions": [{"version": "v1", "created": "Thu, 18 May 2017 16:53:36 GMT"}], "update_date": "2017-05-19", "authors_parsed": [["Loshchilov", "Ilya", ""], ["Glasmachers", "Tobias", ""], ["Beyer", "Hans-Georg", ""]]}, {"id": "1705.06709", "submitter": "Zhuolin Jiang", "authors": "Zhuolin Jiang, Viktor Rozgic, Sancar Adali", "title": "Learning Spatiotemporal Features for Infrared Action Recognition with 3D\n  Convolutional Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Infrared (IR) imaging has the potential to enable more robust action\nrecognition systems compared to visible spectrum cameras due to lower\nsensitivity to lighting conditions and appearance variability. While the action\nrecognition task on videos collected from visible spectrum imaging has received\nmuch attention, action recognition in IR videos is significantly less explored.\nOur objective is to exploit imaging data in this modality for the action\nrecognition task. In this work, we propose a novel two-stream 3D convolutional\nneural network (CNN) architecture by introducing the discriminative code layer\nand the corresponding discriminative code loss function. The proposed network\nprocesses IR image and the IR-based optical flow field sequences. We pretrain\nthe 3D CNN model on the visible spectrum Sports-1M action dataset and finetune\nit on the Infrared Action Recognition (InfAR) dataset. To our best knowledge,\nthis is the first application of the 3D CNN to action recognition in the IR\ndomain. We conduct an elaborate analysis of different fusion schemes (weighted\naverage, single and double-layer neural nets) applied to different 3D CNN\noutputs. Experimental results demonstrate that our approach can achieve\nstate-of-the-art average precision (AP) performances on the InfAR dataset: (1)\nthe proposed two-stream 3D CNN achieves the best reported 77.5% AP, and (2) our\n3D CNN model applied to the optical flow fields achieves the best reported\nsingle stream 75.42% AP.\n", "versions": [{"version": "v1", "created": "Thu, 18 May 2017 17:26:34 GMT"}], "update_date": "2017-05-19", "authors_parsed": [["Jiang", "Zhuolin", ""], ["Rozgic", "Viktor", ""], ["Adali", "Sancar", ""]]}, {"id": "1705.06753", "submitter": "Konstantin Bauman", "authors": "Evgeny Bauman, Konstantin Bauman", "title": "Discovering the Graph Structure in the Clustering Results", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In a standard cluster analysis, such as k-means, in addition to clusters\nlocations and distances between them, it's important to know if they are\nconnected or well separated from each other. The main focus of this paper is\ndiscovering the relations between the resulting clusters. We propose a new\nmethod which is based on pairwise overlapping k-means clustering, that in\naddition to means of clusters provides the graph structure of their relations.\nThe proposed method has a set of parameters that can be tuned in order to\ncontrol the sensitivity of the model and the desired relative size of the\npairwise overlapping interval between means of two adjacent clusters, i.e.,\nlevel of overlapping. We present the exact formula for calculating that\nparameter. The empirical study presented in the paper demonstrates that our\napproach works well not only on toy data but also compliments standard\nclustering results with a reasonable graph structure on real datasets, such as\nfinancial indices and restaurants.\n", "versions": [{"version": "v1", "created": "Thu, 18 May 2017 18:01:50 GMT"}], "update_date": "2017-05-22", "authors_parsed": [["Bauman", "Evgeny", ""], ["Bauman", "Konstantin", ""]]}, {"id": "1705.06769", "submitter": "Nat Dilokthanakul", "authors": "Nat Dilokthanakul, Christos Kaplanis, Nick Pawlowski, Murray Shanahan", "title": "Feature Control as Intrinsic Motivation for Hierarchical Reinforcement\n  Learning", "comments": null, "journal-ref": null, "doi": "10.1109/TNNLS.2019.2891792", "report-no": null, "categories": "cs.LG cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The problem of sparse rewards is one of the hardest challenges in\ncontemporary reinforcement learning. Hierarchical reinforcement learning (HRL)\ntackles this problem by using a set of temporally-extended actions, or options,\neach of which has its own subgoal. These subgoals are normally handcrafted for\nspecific tasks. Here, though, we introduce a generic class of subgoals with\nbroad applicability in the visual domain. Underlying our approach (in common\nwith work using \"auxiliary tasks\") is the hypothesis that the ability to\ncontrol aspects of the environment is an inherently useful skill to have. We\nincorporate such subgoals in an end-to-end hierarchical reinforcement learning\nsystem and test two variants of our algorithm on a number of games from the\nAtari suite. We highlight the advantage of our approach in one of the hardest\ngames -- Montezuma's revenge -- for which the ability to handle sparse rewards\nis key. Our agent learns several times faster than the current state-of-the-art\nHRL agent in this game, reaching a similar level of performance. UPDATE\n22/11/17: We found that a standard A3C agent with a simple shaped reward, i.e.\nextrinsic reward + feature control intrinsic reward, has comparable performance\nto our agent in Montezuma Revenge. In light of the new experiments performed,\nthe advantage of our HRL approach can be attributed more to its ability to\nlearn useful features from intrinsic rewards rather than its ability to explore\nand reuse abstracted skills with hierarchical components. This has led us to a\nnew conclusion about the result.\n", "versions": [{"version": "v1", "created": "Thu, 18 May 2017 19:00:43 GMT"}, {"version": "v2", "created": "Wed, 22 Nov 2017 15:56:19 GMT"}], "update_date": "2020-01-14", "authors_parsed": [["Dilokthanakul", "Nat", ""], ["Kaplanis", "Christos", ""], ["Pawlowski", "Nick", ""], ["Shanahan", "Murray", ""]]}, {"id": "1705.06820", "submitter": "Hongyang Gao", "authors": "Hongyang Gao and Hao Yuan and Zhengyang Wang and Shuiwang Ji", "title": "Pixel Deconvolutional Networks", "comments": "11 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deconvolutional layers have been widely used in a variety of deep models for\nup-sampling, including encoder-decoder networks for semantic segmentation and\ndeep generative models for unsupervised learning. One of the key limitations of\ndeconvolutional operations is that they result in the so-called checkerboard\nproblem. This is caused by the fact that no direct relationship exists among\nadjacent pixels on the output feature map. To address this problem, we propose\nthe pixel deconvolutional layer (PixelDCL) to establish direct relationships\namong adjacent pixels on the up-sampled feature map. Our method is based on a\nfresh interpretation of the regular deconvolution operation. The resulting\nPixelDCL can be used to replace any deconvolutional layer in a plug-and-play\nmanner without compromising the fully trainable capabilities of original\nmodels. The proposed PixelDCL may result in slight decrease in efficiency, but\nthis can be overcome by an implementation trick. Experimental results on\nsemantic segmentation demonstrate that PixelDCL can consider spatial features\nsuch as edges and shapes and yields more accurate segmentation outputs than\ndeconvolutional layers. When used in image generation tasks, our PixelDCL can\nlargely overcome the checkerboard problem suffered by regular deconvolution\noperations.\n", "versions": [{"version": "v1", "created": "Thu, 18 May 2017 22:31:26 GMT"}, {"version": "v2", "created": "Wed, 7 Jun 2017 17:17:07 GMT"}, {"version": "v3", "created": "Fri, 24 Nov 2017 18:17:31 GMT"}, {"version": "v4", "created": "Mon, 27 Nov 2017 02:53:39 GMT"}], "update_date": "2017-11-28", "authors_parsed": [["Gao", "Hongyang", ""], ["Yuan", "Hao", ""], ["Wang", "Zhengyang", ""], ["Ji", "Shuiwang", ""]]}, {"id": "1705.06821", "submitter": "Zhengyang Wang", "authors": "Zhengyang Wang, Hao Yuan, Shuiwang Ji", "title": "Spatial Variational Auto-Encoding via Matrix-Variate Normal\n  Distributions", "comments": "Accepted by SDM2019. Code is publicly available at\n  https://github.com/divelab/svae", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The key idea of variational auto-encoders (VAEs) resembles that of\ntraditional auto-encoder models in which spatial information is supposed to be\nexplicitly encoded in the latent space. However, the latent variables in VAEs\nare vectors, which can be interpreted as multiple feature maps of size 1x1.\nSuch representations can only convey spatial information implicitly when\ncoupled with powerful decoders. In this work, we propose spatial VAEs that use\nfeature maps of larger size as latent variables to explicitly capture spatial\ninformation. This is achieved by allowing the latent variables to be sampled\nfrom matrix-variate normal (MVN) distributions whose parameters are computed\nfrom the encoder network. To increase dependencies among locations on latent\nfeature maps and reduce the number of parameters, we further propose spatial\nVAEs via low-rank MVN distributions. Experimental results show that the\nproposed spatial VAEs outperform original VAEs in capturing rich structural and\nspatial information.\n", "versions": [{"version": "v1", "created": "Thu, 18 May 2017 22:32:57 GMT"}, {"version": "v2", "created": "Tue, 22 Jan 2019 17:48:22 GMT"}], "update_date": "2019-01-23", "authors_parsed": [["Wang", "Zhengyang", ""], ["Yuan", "Hao", ""], ["Ji", "Shuiwang", ""]]}, {"id": "1705.06824", "submitter": "Zhengyang Wang", "authors": "Zhengyang Wang, Shuiwang Ji", "title": "Learning Convolutional Text Representations for Visual Question\n  Answering", "comments": "Conference paper at SDM 2018. https://github.com/divelab/svae", "journal-ref": "In proceedings of the 2018 SIAM International Conference on Data\n  Mining (pp. 594-602). 2018", "doi": "10.1137/1.9781611975321.67", "report-no": null, "categories": "cs.LG cs.CL cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Visual question answering is a recently proposed artificial intelligence task\nthat requires a deep understanding of both images and texts. In deep learning,\nimages are typically modeled through convolutional neural networks, and texts\nare typically modeled through recurrent neural networks. While the requirement\nfor modeling images is similar to traditional computer vision tasks, such as\nobject recognition and image classification, visual question answering raises a\ndifferent need for textual representation as compared to other natural language\nprocessing tasks. In this work, we perform a detailed analysis on natural\nlanguage questions in visual question answering. Based on the analysis, we\npropose to rely on convolutional neural networks for learning textual\nrepresentations. By exploring the various properties of convolutional neural\nnetworks specialized for text data, such as width and depth, we present our\n\"CNN Inception + Gate\" model. We show that our model improves question\nrepresentations and thus the overall accuracy of visual question answering\nmodels. We also show that the text representation requirement in visual\nquestion answering is more complicated and comprehensive than that in\nconventional natural language processing tasks, making it a better task to\nevaluate textual representation methods. Shallow models like fastText, which\ncan obtain comparable results with deep learning models in tasks like text\nclassification, are not suitable in visual question answering.\n", "versions": [{"version": "v1", "created": "Thu, 18 May 2017 22:51:44 GMT"}, {"version": "v2", "created": "Wed, 18 Apr 2018 17:38:50 GMT"}], "update_date": "2018-09-05", "authors_parsed": [["Wang", "Zhengyang", ""], ["Ji", "Shuiwang", ""]]}, {"id": "1705.06884", "submitter": "Renbo Zhao", "authors": "Renbo Zhao, William B. Haskell, Jiashi Feng", "title": "A Unified Framework for Stochastic Matrix Factorization via Variance\n  Reduction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a unified framework to speed up the existing stochastic matrix\nfactorization (SMF) algorithms via variance reduction. Our framework is general\nand it subsumes several well-known SMF formulations in the literature. We\nperform a non-asymptotic convergence analysis of our framework and derive\ncomputational and sample complexities for our algorithm to converge to an\n$\\epsilon$-stationary point in expectation. In addition, extensive experiments\nfor a wide class of SMF formulations demonstrate that our framework\nconsistently yields faster convergence and a more accurate output dictionary\nvis-\\`a-vis state-of-the-art frameworks.\n", "versions": [{"version": "v1", "created": "Fri, 19 May 2017 08:05:10 GMT"}, {"version": "v2", "created": "Mon, 22 May 2017 02:35:21 GMT"}], "update_date": "2017-05-23", "authors_parsed": [["Zhao", "Renbo", ""], ["Haskell", "William B.", ""], ["Feng", "Jiashi", ""]]}, {"id": "1705.06894", "submitter": "Mingda Qiao", "authors": "Haotian Jiang, Jian Li, Mingda Qiao", "title": "Practical Algorithms for Best-K Identification in Multi-Armed Bandits", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the Best-$K$ identification problem (Best-$K$-Arm), we are given $N$\nstochastic bandit arms with unknown reward distributions. Our goal is to\nidentify the $K$ arms with the largest means with high confidence, by drawing\nsamples from the arms adaptively. This problem is motivated by various\npractical applications and has attracted considerable attention in the past\ndecade. In this paper, we propose new practical algorithms for the Best-$K$-Arm\nproblem, which have nearly optimal sample complexity bounds (matching the lower\nbound up to logarithmic factors) and outperform the state-of-the-art algorithms\nfor the Best-$K$-Arm problem (even for $K=1$) in practice.\n", "versions": [{"version": "v1", "created": "Fri, 19 May 2017 08:49:29 GMT"}], "update_date": "2017-05-22", "authors_parsed": [["Jiang", "Haotian", ""], ["Li", "Jian", ""], ["Qiao", "Mingda", ""]]}, {"id": "1705.06899", "submitter": "Zhongmin Luo", "authors": "Raymond Brummelhuis and Zhongmin Luo", "title": "CDS Rate Construction Methods by Machine Learning Techniques", "comments": "51 pages; 21 Figures; 15 Tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.ST cs.LG q-fin.RM stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Regulators require financial institutions to estimate counterparty default\nrisks from liquid CDS quotes for the valuation and risk management of OTC\nderivatives. However, the vast majority of counterparties do not have liquid\nCDS quotes and need proxy CDS rates. Existing methods cannot account for\ncounterparty-specific default risks; we propose to construct proxy CDS rates by\nassociating to illiquid counterparty liquid CDS Proxy based on Machine Learning\nTechniques. After testing 156 classifiers from 8 most popular classifier\nfamilies, we found that some classifiers achieve highly satisfactory accuracy\nrates. Furthermore, we have rank-ordered the performances and investigated\nperformance variations amongst and within the 8 classifier families. This paper\nis, to the best of our knowledge, the first systematic study of CDS Proxy\nconstruction by Machine Learning techniques, and the first systematic\nclassifier comparison study based entirely on financial market data. Its\nfindings both confirm and contrast existing classifier performance literature.\nGiven the typically highly correlated nature of financial data, we investigated\nthe impact of correlation on classifier performance. The techniques used in\nthis paper should be of interest for financial institutions seeking a CDS Proxy\nmethod, and can serve for proxy construction for other financial variables.\nSome directions for future research are indicated.\n", "versions": [{"version": "v1", "created": "Fri, 19 May 2017 09:20:30 GMT"}], "update_date": "2017-05-22", "authors_parsed": [["Brummelhuis", "Raymond", ""], ["Luo", "Zhongmin", ""]]}, {"id": "1705.06908", "submitter": "Micha{\\l} Derezi\\'nski", "authors": "Micha{\\l} Derezi\\'nski and Manfred K. Warmuth", "title": "Unbiased estimates for linear regression via volume sampling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given a full rank matrix $X$ with more columns than rows, consider the task\nof estimating the pseudo inverse $X^+$ based on the pseudo inverse of a sampled\nsubset of columns (of size at least the number of rows). We show that this is\npossible if the subset of columns is chosen proportional to the squared volume\nspanned by the rows of the chosen submatrix (ie, volume sampling). The\nresulting estimator is unbiased and surprisingly the covariance of the\nestimator also has a closed form: It equals a specific factor times\n$X^{+\\top}X^+$. Pseudo inverse plays an important part in solving the linear\nleast squares problem, where we try to predict a label for each column of $X$.\nWe assume labels are expensive and we are only given the labels for the small\nsubset of columns we sample from $X$. Using our methods we show that the weight\nvector of the solution for the sub problem is an unbiased estimator of the\noptimal solution for the whole problem based on all column labels. We believe\nthat these new formulas establish a fundamental connection between linear least\nsquares and volume sampling. We use our methods to obtain an algorithm for\nvolume sampling that is faster than state-of-the-art and for obtaining bounds\nfor the total loss of the estimated least-squares solution on all labeled\ncolumns.\n", "versions": [{"version": "v1", "created": "Fri, 19 May 2017 09:43:41 GMT"}, {"version": "v2", "created": "Wed, 7 Jun 2017 22:42:47 GMT"}, {"version": "v3", "created": "Tue, 13 Jun 2017 22:36:50 GMT"}, {"version": "v4", "created": "Wed, 3 Jan 2018 00:31:28 GMT"}, {"version": "v5", "created": "Tue, 5 Jun 2018 22:46:03 GMT"}], "update_date": "2018-06-07", "authors_parsed": [["Derezi\u0144ski", "Micha\u0142", ""], ["Warmuth", "Manfred K.", ""]]}, {"id": "1705.06922", "submitter": "Zhenfang Hu", "authors": "Zhenfang Hu, Gang Pan, and Zhaohui Wu", "title": "Spectral-graph Based Classifications: Linear Regression for\n  Classification and Normalized Radial Basis Function Network", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Spectral graph theory has been widely applied in unsupervised and\nsemi-supervised learning. In this paper, we find for the first time, to our\nknowledge, that it also plays a concrete role in supervised classification. It\nturns out that two classifiers are inherently related to the theory: linear\nregression for classification (LRC) and normalized radial basis function\nnetwork (nRBFN), corresponding to linear and nonlinear kernel respectively. The\nspectral graph theory provides us with a new insight into a fundamental aspect\nof classification: the tradeoff between fitting error and overfitting risk.\nWith the theory, ideal working conditions for LRC and nRBFN are presented,\nwhich ensure not only zero fitting error but also low overfitting risk. For\nquantitative analysis, two concepts, the fitting error and the spectral risk\n(indicating overfitting), have been defined. Their bounds for nRBFN and LRC are\nderived. A special result shows that the spectral risk of nRBFN is lower\nbounded by the number of classes and upper bounded by the size of radial basis.\nWhen the conditions are not met exactly, the classifiers will pursue the\nminimum fitting error, running into the risk of overfitting. It turns out that\n$\\ell_2$-norm regularization can be applied to control overfitting. Its effect\nis explored under the spectral context. It is found that the two terms in the\n$\\ell_2$-regularized objective are one-one correspondent to the fitting error\nand the spectral risk, revealing a tradeoff between the two quantities.\nConcerning practical performance, we devise a basis selection strategy to\naddress the main problem hindering the applications of (n)RBFN. With the\nstrategy, nRBFN is easy to implement yet flexible. Experiments on 14 benchmark\ndata sets show the performance of nRBFN is comparable to that of SVM, whereas\nthe parameter tuning of nRBFN is much easier, leading to reduction of model\nselection time.\n", "versions": [{"version": "v1", "created": "Fri, 19 May 2017 10:35:37 GMT"}, {"version": "v2", "created": "Tue, 13 Jun 2017 15:18:08 GMT"}], "update_date": "2017-06-14", "authors_parsed": [["Hu", "Zhenfang", ""], ["Pan", "Gang", ""], ["Wu", "Zhaohui", ""]]}, {"id": "1705.06936", "submitter": "Tomasz Grel", "authors": "Robert Adamski, Tomasz Grel, Maciej Klimek and Henryk Michalewski", "title": "Atari games and Intel processors", "comments": null, "journal-ref": null, "doi": "10.1007/978-3-319-75931-9_1", "report-no": null, "categories": "cs.DC cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The asynchronous nature of the state-of-the-art reinforcement learning\nalgorithms such as the Asynchronous Advantage Actor-Critic algorithm, makes\nthem exceptionally suitable for CPU computations. However, given the fact that\ndeep reinforcement learning often deals with interpreting visual information, a\nlarge part of the train and inference time is spent performing convolutions. In\nthis work we present our results on learning strategies in Atari games using a\nConvolutional Neural Network, the Math Kernel Library and TensorFlow 0.11rc0\nmachine learning framework. We also analyze effects of asynchronous\ncomputations on the convergence of reinforcement learning algorithms.\n", "versions": [{"version": "v1", "created": "Fri, 19 May 2017 11:19:45 GMT"}], "update_date": "2018-04-17", "authors_parsed": [["Adamski", "Robert", ""], ["Grel", "Tomasz", ""], ["Klimek", "Maciej", ""], ["Michalewski", "Henryk", ""]]}, {"id": "1705.06995", "submitter": "Yang Cao", "authors": "Yang Cao, Liyan Xie, Yao Xie, and Huan Xu", "title": "Nearly second-order asymptotic optimality of sequential change-point\n  detection with one-sample updates", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.LG stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sequential change-point detection when the distribution parameters are\nunknown is a fundamental problem in statistics and machine learning. When the\npost-change parameters are unknown, we consider a set of detection procedures\nbased on sequential likelihood ratios with non-anticipating estimators\nconstructed using online convex optimization algorithms such as online mirror\ndescent, which provides a more versatile approach to tackle complex situations\nwhere recursive maximum likelihood estimators cannot be found. When the\nunderlying distributions belong to a exponential family and the estimators\nsatisfy the logarithm regret property, we show that this approach is nearly\nsecond-order asymptotically optimal. This means that the upper bound for the\nfalse alarm rate of the algorithm (measured by the average-run-length) meets\nthe lower bound asymptotically up to a log-log factor when the threshold tends\nto infinity. Our proof is achieved by making a connection between sequential\nchange-point and online convex optimization and leveraging the logarithmic\nregret bound property of online mirror descent algorithm. Numerical and real\ndata examples validate our theory.\n", "versions": [{"version": "v1", "created": "Fri, 19 May 2017 13:53:14 GMT"}, {"version": "v2", "created": "Fri, 1 Sep 2017 00:43:32 GMT"}, {"version": "v3", "created": "Thu, 16 Nov 2017 15:48:02 GMT"}, {"version": "v4", "created": "Mon, 4 Dec 2017 23:31:58 GMT"}], "update_date": "2017-12-06", "authors_parsed": [["Cao", "Yang", ""], ["Xie", "Liyan", ""], ["Xie", "Yao", ""], ["Xu", "Huan", ""]]}, {"id": "1705.07025", "submitter": "David Kale", "authors": "Sebastien Dubois, Nathanael Romano, David C. Kale, Nigam Shah, and\n  Kenneth Jung", "title": "Effective Representations of Clinical Notes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Clinical notes are a rich source of information about patient state. However,\nusing them to predict clinical events with machine learning models is\nchallenging. They are very high dimensional, sparse and have complex structure.\nFurthermore, training data is often scarce because it is expensive to obtain\nreliable labels for many clinical events. These difficulties have traditionally\nbeen addressed by manual feature engineering encoding task specific domain\nknowledge. We explored the use of neural networks and transfer learning to\nlearn representations of clinical notes that are useful for predicting future\nclinical events of interest, such as all causes mortality, inpatient\nadmissions, and emergency room visits. Our data comprised 2.7 million notes and\n115 thousand patients at Stanford Hospital. We used the learned\nrepresentations, along with commonly used bag of words and topic model\nrepresentations, as features for predictive models of clinical events. We\nevaluated the effectiveness of these representations with respect to the\nperformance of the models trained on small datasets. Models using the neural\nnetwork derived representations performed significantly better than models\nusing the baseline representations with small ($N < 1000$) training datasets.\nThe learned representations offer significant performance gains over commonly\nused baseline representations for a range of predictive modeling tasks and\ncohort sizes, offering an effective alternative to task specific feature\nengineering when plentiful labeled training data is not available.\n", "versions": [{"version": "v1", "created": "Fri, 19 May 2017 14:42:48 GMT"}, {"version": "v2", "created": "Thu, 15 Jun 2017 03:31:52 GMT"}, {"version": "v3", "created": "Thu, 16 Aug 2018 22:56:41 GMT"}], "update_date": "2018-08-20", "authors_parsed": [["Dubois", "Sebastien", ""], ["Romano", "Nathanael", ""], ["Kale", "David C.", ""], ["Shah", "Nigam", ""], ["Jung", "Kenneth", ""]]}, {"id": "1705.07038", "submitter": "Pan Zhou", "authors": "Pan Zhou, Jiashi Feng", "title": "The Landscape of Deep Learning Algorithms", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper studies the landscape of empirical risk of deep neural networks by\ntheoretically analyzing its convergence behavior to the population risk as well\nas its stationary points and properties. For an $l$-layer linear neural\nnetwork, we prove its empirical risk uniformly converges to its population risk\nat the rate of $\\mathcal{O}(r^{2l}\\sqrt{d\\log(l)}/\\sqrt{n})$ with training\nsample size of $n$, the total weight dimension of $d$ and the magnitude bound\n$r$ of weight of each layer. We then derive the stability and generalization\nbounds for the empirical risk based on this result. Besides, we establish the\nuniform convergence of gradient of the empirical risk to its population\ncounterpart. We prove the one-to-one correspondence of the non-degenerate\nstationary points between the empirical and population risks with convergence\nguarantees, which describes the landscape of deep neural networks. In addition,\nwe analyze these properties for deep nonlinear neural networks with sigmoid\nactivation functions. We prove similar results for convergence behavior of\ntheir empirical risks as well as the gradients and analyze properties of their\nnon-degenerate stationary points.\n  To our best knowledge, this work is the first one theoretically\ncharacterizing landscapes of deep learning algorithms. Besides, our results\nprovide the sample complexity of training a good deep neural network. We also\nprovide theoretical understanding on how the neural network depth $l$, the\nlayer width, the network size $d$ and parameter magnitude determine the neural\nnetwork landscapes.\n", "versions": [{"version": "v1", "created": "Fri, 19 May 2017 15:07:07 GMT"}, {"version": "v2", "created": "Sat, 5 Aug 2017 12:30:25 GMT"}], "update_date": "2017-08-08", "authors_parsed": [["Zhou", "Pan", ""], ["Feng", "Jiashi", ""]]}, {"id": "1705.07041", "submitter": "Randy Jia", "authors": "Shipra Agrawal and Randy Jia", "title": "Posterior sampling for reinforcement learning: worst-case regret bounds", "comments": "This revision fixes an error due to use of some incorrect results\n  (Lemma C.1 and Lemma C.2) in the earlier version. The regret bounds in this\n  version are worse by a factor of sqrt(S) as compared to the previous version", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an algorithm based on posterior sampling (aka Thompson sampling)\nthat achieves near-optimal worst-case regret bounds when the underlying Markov\nDecision Process (MDP) is communicating with a finite, though unknown,\ndiameter. Our main result is a high probability regret upper bound of\n$\\tilde{O}(DS\\sqrt{AT})$ for any communicating MDP with $S$ states, $A$ actions\nand diameter $D$. Here, regret compares the total reward achieved by the\nalgorithm to the total expected reward of an optimal infinite-horizon\nundiscounted average reward policy, in time horizon $T$. This result closely\nmatches the known lower bound of $\\Omega(\\sqrt{DSAT})$. Our techniques involve\nproving some novel results about the anti-concentration of Dirichlet\ndistribution, which may be of independent interest.\n", "versions": [{"version": "v1", "created": "Fri, 19 May 2017 15:10:21 GMT"}, {"version": "v2", "created": "Mon, 4 Feb 2019 14:26:06 GMT"}, {"version": "v3", "created": "Mon, 30 Mar 2020 23:58:02 GMT"}], "update_date": "2020-04-01", "authors_parsed": [["Agrawal", "Shipra", ""], ["Jia", "Randy", ""]]}, {"id": "1705.07048", "submitter": "Daniel Hsu", "authors": "Daniel Hsu, Kevin Shi, Xiaorui Sun", "title": "Linear regression without correspondence", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article considers algorithmic and statistical aspects of linear\nregression when the correspondence between the covariates and the responses is\nunknown. First, a fully polynomial-time approximation scheme is given for the\nnatural least squares optimization problem in any constant dimension. Next, in\nan average-case and noise-free setting where the responses exactly correspond\nto a linear function of i.i.d. draws from a standard multivariate normal\ndistribution, an efficient algorithm based on lattice basis reduction is shown\nto exactly recover the unknown linear function in arbitrary dimension. Finally,\nlower bounds on the signal-to-noise ratio are established for approximate\nrecovery of the unknown linear function by any estimator.\n", "versions": [{"version": "v1", "created": "Fri, 19 May 2017 15:22:38 GMT"}, {"version": "v2", "created": "Tue, 7 Nov 2017 23:16:39 GMT"}], "update_date": "2017-11-09", "authors_parsed": [["Hsu", "Daniel", ""], ["Shi", "Kevin", ""], ["Sun", "Xiaorui", ""]]}, {"id": "1705.07057", "submitter": "George Papamakarios", "authors": "George Papamakarios, Theo Pavlakou, Iain Murray", "title": "Masked Autoregressive Flow for Density Estimation", "comments": "section 4.3 is corrected since the previous version", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Autoregressive models are among the best performing neural density\nestimators. We describe an approach for increasing the flexibility of an\nautoregressive model, based on modelling the random numbers that the model uses\ninternally when generating data. By constructing a stack of autoregressive\nmodels, each modelling the random numbers of the next model in the stack, we\nobtain a type of normalizing flow suitable for density estimation, which we\ncall Masked Autoregressive Flow. This type of flow is closely related to\nInverse Autoregressive Flow and is a generalization of Real NVP. Masked\nAutoregressive Flow achieves state-of-the-art performance in a range of\ngeneral-purpose density estimation tasks.\n", "versions": [{"version": "v1", "created": "Fri, 19 May 2017 15:42:54 GMT"}, {"version": "v2", "created": "Fri, 27 Oct 2017 19:51:14 GMT"}, {"version": "v3", "created": "Thu, 11 Jan 2018 15:27:21 GMT"}, {"version": "v4", "created": "Thu, 14 Jun 2018 10:28:12 GMT"}], "update_date": "2018-06-15", "authors_parsed": [["Papamakarios", "George", ""], ["Pavlakou", "Theo", ""], ["Murray", "Iain", ""]]}, {"id": "1705.07070", "submitter": "Mehmet Donmez", "authors": "Mehmet A. Donmez and Maxim Raginsky and Andrew C. Singer", "title": "EE-Grad: Exploration and Exploitation for Cost-Efficient Mini-Batch SGD", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a generic framework for trading off fidelity and cost in computing\nstochastic gradients when the costs of acquiring stochastic gradients of\ndifferent quality are not known a priori. We consider a mini-batch oracle that\ndistributes a limited query budget over a number of stochastic gradients and\naggregates them to estimate the true gradient. Since the optimal mini-batch\nsize depends on the unknown cost-fidelity function, we propose an algorithm,\n{\\it EE-Grad}, that sequentially explores the performance of mini-batch oracles\nand exploits the accumulated knowledge to estimate the one achieving the best\nperformance in terms of cost-efficiency. We provide performance guarantees for\nEE-Grad with respect to the optimal mini-batch oracle, and illustrate these\nresults in the case of strongly convex objectives. We also provide a simple\nnumerical example that corroborates our theoretical findings.\n", "versions": [{"version": "v1", "created": "Fri, 19 May 2017 16:16:42 GMT"}], "update_date": "2017-05-22", "authors_parsed": [["Donmez", "Mehmet A.", ""], ["Raginsky", "Maxim", ""], ["Singer", "Andrew C.", ""]]}, {"id": "1705.07077", "submitter": "Haifeng Li", "authors": "Haifeng Li, Jian Peng, Chao Tao, Jie Chen, Min Deng", "title": "What do We Learn by Semantic Scene Understanding for Remote Sensing\n  imagery in CNN framework?", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, deep convolutional neural network (DCNN) achieved increasingly\nremarkable success and rapidly developed in the field of natural image\nrecognition. Compared with the natural image, the scale of remote sensing image\nis larger and the scene and the object it represents are more macroscopic. This\nstudy inquires whether remote sensing scene and natural scene recognitions\ndiffer and raises the following questions: What are the key factors in remote\nsensing scene recognition? Is the DCNN recognition mechanism centered on object\nrecognition still applicable to the scenarios of remote sensing scene\nunderstanding? We performed several experiments to explore the influence of the\nDCNN structure and the scale of remote sensing scene understanding from the\nperspective of scene complexity. Our experiment shows that understanding a\ncomplex scene depends on an in-depth network and multiple-scale perception.\nUsing a visualization method, we qualitatively and quantitatively analyze the\nrecognition mechanism in a complex remote sensing scene and demonstrate the\nimportance of multi-objective joint semantic support.\n", "versions": [{"version": "v1", "created": "Fri, 19 May 2017 16:23:53 GMT"}], "update_date": "2017-05-22", "authors_parsed": [["Li", "Haifeng", ""], ["Peng", "Jian", ""], ["Tao", "Chao", ""], ["Chen", "Jie", ""], ["Deng", "Min", ""]]}, {"id": "1705.07086", "submitter": "Emmanouil Antonios Platanios", "authors": "Emmanouil A. Platanios, Hoifung Poon, Tom M. Mitchell, Eric Horvitz", "title": "Estimating Accuracy from Unlabeled Data: A Probabilistic Logic Approach", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose an efficient method to estimate the accuracy of classifiers using\nonly unlabeled data. We consider a setting with multiple classification\nproblems where the target classes may be tied together through logical\nconstraints. For example, a set of classes may be mutually exclusive, meaning\nthat a data instance can belong to at most one of them. The proposed method is\nbased on the intuition that: (i) when classifiers agree, they are more likely\nto be correct, and (ii) when the classifiers make a prediction that violates\nthe constraints, at least one classifier must be making an error. Experiments\non four real-world data sets produce accuracy estimates within a few percent of\nthe true accuracy, using solely unlabeled data. Our models also outperform\nexisting state-of-the-art solutions in both estimating accuracies, and\ncombining multiple classifier outputs. The results emphasize the utility of\nlogical constraints in estimating accuracy, thus validating our intuition.\n", "versions": [{"version": "v1", "created": "Fri, 19 May 2017 16:52:52 GMT"}], "update_date": "2017-05-22", "authors_parsed": [["Platanios", "Emmanouil A.", ""], ["Poon", "Hoifung", ""], ["Mitchell", "Tom M.", ""], ["Horvitz", "Eric", ""]]}, {"id": "1705.07099", "submitter": "Balazs Kegl", "authors": "Laetitia Le, Camille Marini, Alexandre Gramfort, David Nguyen, Mehdi\n  Cherti, Sana Tfaili, Ali Tfayli, Arlette Baillet-Guffroy, Patrice Prognon,\n  Pierre Chaminade, Eric Caudron, Bal\\'azs K\\'egl", "title": "Machine learning for classification and quantification of monoclonal\n  antibody preparations for cancer therapy", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.QM cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Monoclonal antibodies constitute one of the most important strategies to\ntreat patients suffering from cancers such as hematological malignancies and\nsolid tumors. In order to guarantee the quality of those preparations prepared\nat hospital, quality control has to be developed. The aim of this study was to\nexplore a noninvasive, nondestructive, and rapid analytical method to ensure\nthe quality of the final preparation without causing any delay in the process.\nWe analyzed four mAbs (Inlfiximab, Bevacizumab, Ramucirumab and Rituximab)\ndiluted at therapeutic concentration in chloride sodium 0.9% using Raman\nspectroscopy. To reduce the prediction errors obtained with traditional\nchemometric data analysis, we explored a data-driven approach using statistical\nmachine learning methods where preprocessing and predictive models are jointly\noptimized. We prepared a data analytics workflow and submitted the problem to a\ncollaborative data challenge platform called Rapid Analytics and Model\nPrototyping (RAMP). This allowed to use solutions from about 300 data\nscientists during five days of collaborative work. The prediction of the four\nmAbs samples was considerably improved with a misclassification rate and the\nmean error rate of 0.8% and 4%, respectively.\n", "versions": [{"version": "v1", "created": "Fri, 19 May 2017 17:23:38 GMT"}, {"version": "v2", "created": "Wed, 31 May 2017 12:42:21 GMT"}], "update_date": "2017-06-01", "authors_parsed": [["Le", "Laetitia", ""], ["Marini", "Camille", ""], ["Gramfort", "Alexandre", ""], ["Nguyen", "David", ""], ["Cherti", "Mehdi", ""], ["Tfaili", "Sana", ""], ["Tfayli", "Ali", ""], ["Baillet-Guffroy", "Arlette", ""], ["Prognon", "Patrice", ""], ["Chaminade", "Pierre", ""], ["Caudron", "Eric", ""], ["K\u00e9gl", "Bal\u00e1zs", ""]]}, {"id": "1705.07107", "submitter": "Yingzhen Li", "authors": "Yingzhen Li, Richard E. Turner", "title": "Gradient Estimators for Implicit Models", "comments": "v5 fixed a typo in Figure 3 of v4 (the version at ICLR 2018 main\n  conference)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Implicit models, which allow for the generation of samples but not for\npoint-wise evaluation of probabilities, are omnipresent in real-world problems\ntackled by machine learning and a hot topic of current research. Some examples\ninclude data simulators that are widely used in engineering and scientific\nresearch, generative adversarial networks (GANs) for image synthesis, and\nhot-off-the-press approximate inference techniques relying on implicit\ndistributions. The majority of existing approaches to learning implicit models\nrely on approximating the intractable distribution or optimisation objective\nfor gradient-based optimisation, which is liable to produce inaccurate updates\nand thus poor models. This paper alleviates the need for such approximations by\nproposing the Stein gradient estimator, which directly estimates the score\nfunction of the implicitly defined distribution. The efficacy of the proposed\nestimator is empirically demonstrated by examples that include meta-learning\nfor approximate inference, and entropy regularised GANs that provide improved\nsample diversity.\n", "versions": [{"version": "v1", "created": "Fri, 19 May 2017 17:35:04 GMT"}, {"version": "v2", "created": "Fri, 27 Oct 2017 09:02:34 GMT"}, {"version": "v3", "created": "Mon, 19 Feb 2018 15:19:59 GMT"}, {"version": "v4", "created": "Fri, 23 Feb 2018 16:45:57 GMT"}, {"version": "v5", "created": "Thu, 26 Apr 2018 08:43:09 GMT"}], "update_date": "2018-04-27", "authors_parsed": [["Li", "Yingzhen", ""], ["Turner", "Richard E.", ""]]}, {"id": "1705.07109", "submitter": "Ya\\u{g}mur G\\\"u\\c{c}l\\\"ut\\\"urk", "authors": "Ya\\u{g}mur G\\\"u\\c{c}l\\\"ut\\\"urk, Umut G\\\"u\\c{c}l\\\"u, Katja Seeliger,\n  Sander Bosch, Rob van Lier, Marcel van Gerven", "title": "Deep adversarial neural decoding", "comments": "Added appendix and updated figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Here, we present a novel approach to solve the problem of reconstructing\nperceived stimuli from brain responses by combining probabilistic inference\nwith deep learning. Our approach first inverts the linear transformation from\nlatent features to brain responses with maximum a posteriori estimation and\nthen inverts the nonlinear transformation from perceived stimuli to latent\nfeatures with adversarial training of convolutional neural networks. We test\nour approach with a functional magnetic resonance imaging experiment and show\nthat it can generate state-of-the-art reconstructions of perceived faces from\nbrain activations.\n", "versions": [{"version": "v1", "created": "Fri, 19 May 2017 17:43:01 GMT"}, {"version": "v2", "created": "Thu, 1 Jun 2017 13:15:25 GMT"}, {"version": "v3", "created": "Thu, 15 Jun 2017 16:56:34 GMT"}], "update_date": "2017-06-16", "authors_parsed": [["G\u00fc\u00e7l\u00fct\u00fcrk", "Ya\u011fmur", ""], ["G\u00fc\u00e7l\u00fc", "Umut", ""], ["Seeliger", "Katja", ""], ["Bosch", "Sander", ""], ["van Lier", "Rob", ""], ["van Gerven", "Marcel", ""]]}, {"id": "1705.07112", "submitter": "Masaki Onuki", "authors": "Masaki Onuki, Shunsuke Ono, Keiichiro Shirai, Yuichi Tanaka", "title": "Fast Singular Value Shrinkage with Chebyshev Polynomial Approximation\n  Based on Signal Sparsity", "comments": "This is a journal paper", "journal-ref": null, "doi": "10.1109/TSP.2017.2745444", "report-no": null, "categories": "cs.NA cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose an approximation method for thresholding of singular values using\nChebyshev polynomial approximation (CPA). Many signal processing problems\nrequire iterative application of singular value decomposition (SVD) for\nminimizing the rank of a given data matrix with other cost functions and/or\nconstraints, which is called matrix rank minimization. In matrix rank\nminimization, singular values of a matrix are shrunk by hard-thresholding,\nsoft-thresholding, or weighted soft-thresholding. However, the computational\ncost of SVD is generally too expensive to handle high dimensional signals such\nas images; hence, in this case, matrix rank minimization requires enormous\ncomputation time. In this paper, we leverage CPA to (approximately) manipulate\nsingular values without computing singular values and vectors. The thresholding\nof singular values is expressed by a multiplication of certain matrices, which\nis derived from a characteristic of CPA. The multiplication is also efficiently\ncomputed using the sparsity of signals. As a result, the computational cost is\nsignificantly reduced. Experimental results suggest the effectiveness of our\nmethod through several image processing applications based on matrix rank\nminimization with nuclear norm relaxation in terms of computation time and\napproximation precision.\n", "versions": [{"version": "v1", "created": "Fri, 19 May 2017 17:55:58 GMT"}], "update_date": "2017-11-22", "authors_parsed": [["Onuki", "Masaki", ""], ["Ono", "Shunsuke", ""], ["Shirai", "Keiichiro", ""], ["Tanaka", "Yuichi", ""]]}, {"id": "1705.07117", "submitter": "Melvin Robinson", "authors": "Mohammadmehdi Ezzatabadipour, Parth Singh, Melvin D. Robinson, Pablo\n  Guillen-Rondon, Carlos Torres", "title": "Deep Learning as a Tool to Predict Flow Patterns in Two-Phase Flow", "comments": "Part of DM4OG 2017 proceedings (arXiv:1705.03451)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In order to better model complex real-world data such as multiphase flow, one\napproach is to develop pattern recognition techniques and robust features that\ncapture the relevant information. In this paper, we use deep learning methods,\nand in particular employ the multilayer perceptron, to build an algorithm that\ncan predict flow pattern in twophase flow from fluid properties and pipe\nconditions. The preliminary results show excellent performance when compared\nwith classical methods of flow pattern prediction.\n", "versions": [{"version": "v1", "created": "Fri, 19 May 2017 03:11:30 GMT"}], "update_date": "2017-05-23", "authors_parsed": [["Ezzatabadipour", "Mohammadmehdi", ""], ["Singh", "Parth", ""], ["Robinson", "Melvin D.", ""], ["Guillen-Rondon", "Pablo", ""], ["Torres", "Carlos", ""]]}, {"id": "1705.07120", "submitter": "Jakub Tomczak Ph.D.", "authors": "Jakub M. Tomczak and Max Welling", "title": "VAE with a VampPrior", "comments": "16 pages, final version, AISTATS 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many different methods to train deep generative models have been introduced\nin the past. In this paper, we propose to extend the variational auto-encoder\n(VAE) framework with a new type of prior which we call \"Variational Mixture of\nPosteriors\" prior, or VampPrior for short. The VampPrior consists of a mixture\ndistribution (e.g., a mixture of Gaussians) with components given by\nvariational posteriors conditioned on learnable pseudo-inputs. We further\nextend this prior to a two layer hierarchical model and show that this\narchitecture with a coupled prior and posterior, learns significantly better\nmodels. The model also avoids the usual local optima issues related to useless\nlatent dimensions that plague VAEs. We provide empirical studies on six\ndatasets, namely, static and binary MNIST, OMNIGLOT, Caltech 101 Silhouettes,\nFrey Faces and Histopathology patches, and show that applying the hierarchical\nVampPrior delivers state-of-the-art results on all datasets in the unsupervised\npermutation invariant setting and the best results or comparable to SOTA\nmethods for the approach with convolutional networks.\n", "versions": [{"version": "v1", "created": "Fri, 19 May 2017 10:07:00 GMT"}, {"version": "v2", "created": "Mon, 24 Jul 2017 14:14:08 GMT"}, {"version": "v3", "created": "Mon, 21 Aug 2017 12:21:47 GMT"}, {"version": "v4", "created": "Fri, 13 Oct 2017 17:54:28 GMT"}, {"version": "v5", "created": "Mon, 26 Feb 2018 15:23:53 GMT"}], "update_date": "2018-02-27", "authors_parsed": [["Tomczak", "Jakub M.", ""], ["Welling", "Max", ""]]}, {"id": "1705.07136", "submitter": "Xuezhe Ma", "authors": "Xuezhe Ma, Pengcheng Yin, Jingzhou Liu, Graham Neubig, Eduard Hovy", "title": "Softmax Q-Distribution Estimation for Structured Prediction: A\n  Theoretical Interpretation for RAML", "comments": "Under Review of ICLR 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Reward augmented maximum likelihood (RAML), a simple and effective learning\nframework to directly optimize towards the reward function in structured\nprediction tasks, has led to a number of impressive empirical successes. RAML\nincorporates task-specific reward by performing maximum-likelihood updates on\ncandidate outputs sampled according to an exponentiated payoff distribution,\nwhich gives higher probabilities to candidates that are close to the reference\noutput. While RAML is notable for its simplicity, efficiency, and its\nimpressive empirical successes, the theoretical properties of RAML, especially\nthe behavior of the exponentiated payoff distribution, has not been examined\nthoroughly. In this work, we introduce softmax Q-distribution estimation, a\nnovel theoretical interpretation of RAML, which reveals the relation between\nRAML and Bayesian decision theory. The softmax Q-distribution can be regarded\nas a smooth approximation of the Bayes decision boundary, and the Bayes\ndecision rule is achieved by decoding with this Q-distribution. We further show\nthat RAML is equivalent to approximately estimating the softmax Q-distribution,\nwith the temperature $\\tau$ controlling approximation error. We perform two\nexperiments, one on synthetic data of multi-class classification and one on\nreal data of image captioning, to demonstrate the relationship between RAML and\nthe proposed softmax Q-distribution estimation method, verifying our\ntheoretical analysis. Additional experiments on three structured prediction\ntasks with rewards defined on sequential (named entity recognition), tree-based\n(dependency parsing) and irregular (machine translation) structures show\nnotable improvements over maximum likelihood baselines.\n", "versions": [{"version": "v1", "created": "Fri, 19 May 2017 18:17:00 GMT"}, {"version": "v2", "created": "Thu, 25 May 2017 04:17:49 GMT"}, {"version": "v3", "created": "Fri, 27 Oct 2017 19:50:50 GMT"}], "update_date": "2017-10-31", "authors_parsed": [["Ma", "Xuezhe", ""], ["Yin", "Pengcheng", ""], ["Liu", "Jingzhou", ""], ["Neubig", "Graham", ""], ["Hovy", "Eduard", ""]]}, {"id": "1705.07149", "submitter": "Tsung-Han Lin", "authors": "Tsung-Han Lin", "title": "Local Information with Feedback Perturbation Suffices for Dictionary\n  Learning in Neural Circuits", "comments": "10 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While the sparse coding principle can successfully model information\nprocessing in sensory neural systems, it remains unclear how learning can be\naccomplished under neural architectural constraints. Feasible learning rules\nmust rely solely on synaptically local information in order to be implemented\non spatially distributed neurons. We describe a neural network with spiking\nneurons that can address the aforementioned fundamental challenge and solve the\nL1-minimizing dictionary learning problem, representing the first model able to\ndo so. Our major innovation is to introduce feedback synapses to create a\npathway to turn the seemingly non-local information into local ones. The\nresulting network encodes the error signal needed for learning as the change of\nnetwork steady states caused by feedback, and operates akin to the classical\nstochastic gradient descent method.\n", "versions": [{"version": "v1", "created": "Fri, 19 May 2017 19:06:27 GMT"}], "update_date": "2017-05-23", "authors_parsed": [["Lin", "Tsung-Han", ""]]}, {"id": "1705.07157", "submitter": "Colin White", "authors": "Maria-Florina Balcan, Colin White", "title": "Clustering under Local Stability: Bridging the Gap between Worst-Case\n  and Beyond Worst-Case Analysis", "comments": "arXiv admin note: substantial text overlap with arXiv:1505.03924", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, there has been substantial interest in clustering research that\ntakes a beyond worst-case approach to the analysis of algorithms. The typical\nidea is to design a clustering algorithm that outputs a near-optimal solution,\nprovided the data satisfy a natural stability notion. For example, Bilu and\nLinial (2010) and Awasthi et al. (2012) presented algorithms that output\nnear-optimal solutions, assuming the optimal solution is preserved under small\nperturbations to the input distances. A drawback to this approach is that the\nalgorithms are often explicitly built according to the stability assumption and\ngive no guarantees in the worst case; indeed, several recent algorithms output\narbitrarily bad solutions even when just a small section of the data does not\nsatisfy the given stability notion.\n  In this work, we address this concern in two ways. First, we provide\nalgorithms that inherit the worst-case guarantees of clustering approximation\nalgorithms, while simultaneously guaranteeing near-optimal solutions when the\ndata is stable. Our algorithms are natural modifications to existing\nstate-of-the-art approximation algorithms. Second, we initiate the study of\nlocal stability, which is a property of a single optimal cluster rather than an\nentire optimal solution. We show our algorithms output all optimal clusters\nwhich satisfy stability locally. Specifically, we achieve strong positive\nresults in our local framework under recent stability notions including metric\nperturbation resilience (Angelidakis et al. 2017) and robust perturbation\nresilience (Balcan and Liang 2012) for the $k$-median, $k$-means, and\nsymmetric/asymmetric $k$-center objectives.\n", "versions": [{"version": "v1", "created": "Fri, 19 May 2017 19:30:33 GMT"}], "update_date": "2018-12-31", "authors_parsed": [["Balcan", "Maria-Florina", ""], ["White", "Colin", ""]]}, {"id": "1705.07164", "submitter": "Tianyi Lin", "authors": "Xin Guo, Johnny Hong, Tianyi Lin and Nan Yang", "title": "Relaxed Wasserstein with Applications to GANs", "comments": "Accepted by ICASSP 2021; add the references", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Wasserstein Generative Adversarial Networks (WGANs) provide a versatile class\nof models, which have attracted great attention in various applications.\nHowever, this framework has two main drawbacks: (i) Wasserstein-1 (or\nEarth-Mover) distance is restrictive such that WGANs cannot always fit data\ngeometry well; (ii) It is difficult to achieve fast training of WGANs. In this\npaper, we propose a new class of \\textit{Relaxed Wasserstein} (RW) distances by\ngeneralizing Wasserstein-1 distance with Bregman cost functions. We show that\nRW distances achieve nice statistical properties while not sacrificing the\ncomputational tractability. Combined with the GANs framework, we develop\nRelaxed WGANs (RWGANs) which are not only statistically flexible but can be\napproximated efficiently using heuristic approaches. Experiments on real images\ndemonstrate that the RWGAN with Kullback-Leibler (KL) cost function outperforms\nother competing approaches, e.g., WGANs, even with gradient penalty.\n", "versions": [{"version": "v1", "created": "Fri, 19 May 2017 19:51:34 GMT"}, {"version": "v2", "created": "Tue, 31 Oct 2017 08:39:34 GMT"}, {"version": "v3", "created": "Fri, 30 Mar 2018 01:54:01 GMT"}, {"version": "v4", "created": "Sun, 16 Sep 2018 20:50:00 GMT"}, {"version": "v5", "created": "Sat, 4 May 2019 08:49:44 GMT"}, {"version": "v6", "created": "Thu, 22 Oct 2020 08:18:42 GMT"}, {"version": "v7", "created": "Sat, 6 Feb 2021 09:33:22 GMT"}, {"version": "v8", "created": "Sat, 17 Jul 2021 06:03:54 GMT"}], "update_date": "2021-07-20", "authors_parsed": [["Guo", "Xin", ""], ["Hong", "Johnny", ""], ["Lin", "Tianyi", ""], ["Yang", "Nan", ""]]}, {"id": "1705.07171", "submitter": "Haishan Ye", "authors": "Haishan Ye and Zhihua Zhang", "title": "Nestrov's Acceleration For Second Order Method", "comments": "Have an important typo in title. Superseded by arXiv:1710.08496", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Optimization plays a key role in machine learning. Recently, stochastic\nsecond-order methods have attracted much attention due to their low\ncomputational cost in each iteration. However, these algorithms might perform\npoorly especially if it is hard to approximate the Hessian well and\nefficiently. As far as we know, there is no effective way to handle this\nproblem. In this paper, we resort to Nestrov's acceleration technique to\nimprove the convergence performance of a class of second-order methods called\napproximate Newton. We give a theoretical analysis that Nestrov's acceleration\ntechnique can improve the convergence performance for approximate Newton just\nlike for first-order methods. We accordingly propose an accelerated regularized\nsub-sampled Newton. Our accelerated algorithm performs much better than the\noriginal regularized sub-sampled Newton in experiments, which validates our\ntheory empirically. Besides, the accelerated regularized sub-sampled Newton has\ngood performance comparable to or even better than state-of-art algorithms.\n", "versions": [{"version": "v1", "created": "Fri, 19 May 2017 20:17:56 GMT"}, {"version": "v2", "created": "Tue, 17 Oct 2017 06:13:24 GMT"}], "update_date": "2017-10-25", "authors_parsed": [["Ye", "Haishan", ""], ["Zhang", "Zhihua", ""]]}, {"id": "1705.07175", "submitter": "Fabrizio Pedersoli", "authors": "Fabrizio Pedersoli and George Tzanetakis and Andrea Tagliasacchi", "title": "Espresso: Efficient Forward Propagation for BCNNs", "comments": "10 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.CV cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There are many applications scenarios for which the computational performance\nand memory footprint of the prediction phase of Deep Neural Networks (DNNs)\nneeds to be optimized. Binary Neural Networks (BDNNs) have been shown to be an\neffective way of achieving this objective. In this paper, we show how\nConvolutional Neural Networks (CNNs) can be implemented using binary\nrepresentations. Espresso is a compact, yet powerful library written in C/CUDA\nthat features all the functionalities required for the forward propagation of\nCNNs, in a binary file less than 400KB, without any external dependencies.\nAlthough it is mainly designed to take advantage of massive GPU parallelism,\nEspresso also provides an equivalent CPU implementation for CNNs. Espresso\nprovides special convolutional and dense layers for BCNNs, leveraging\nbit-packing and bit-wise computations for efficient execution. These techniques\nprovide a speed-up of matrix-multiplication routines, and at the same time,\nreduce memory usage when storing parameters and activations. We experimentally\nshow that Espresso is significantly faster than existing implementations of\noptimized binary neural networks ($\\approx$ 2 orders of magnitude). Espresso is\nreleased under the Apache 2.0 license and is available at\nhttp://github.com/fpeder/espresso.\n", "versions": [{"version": "v1", "created": "Fri, 19 May 2017 20:29:42 GMT"}, {"version": "v2", "created": "Wed, 7 Mar 2018 17:59:16 GMT"}], "update_date": "2018-03-08", "authors_parsed": [["Pedersoli", "Fabrizio", ""], ["Tzanetakis", "George", ""], ["Tagliasacchi", "Andrea", ""]]}, {"id": "1705.07199", "submitter": "Alexander G.  Anderson", "authors": "Alexander G. Anderson, Cory P. Berg", "title": "The High-Dimensional Geometry of Binary Neural Networks", "comments": "12 pages, 4 Figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent research has shown that one can train a neural network with binary\nweights and activations at train time by augmenting the weights with a\nhigh-precision continuous latent variable that accumulates small changes from\nstochastic gradient descent. However, there is a dearth of theoretical analysis\nto explain why we can effectively capture the features in our data with binary\nweights and activations. Our main result is that the neural networks with\nbinary weights and activations trained using the method of Courbariaux, Hubara\net al. (2016) work because of the high-dimensional geometry of binary vectors.\nIn particular, the ideal continuous vectors that extract out features in the\nintermediate representations of these BNNs are well-approximated by binary\nvectors in the sense that dot products are approximately preserved. Compared to\nprevious research that demonstrated the viability of such BNNs, our work\nexplains why these BNNs work in terms of the HD geometry. Our theory serves as\na foundation for understanding not only BNNs but a variety of methods that seek\nto compress traditional neural networks. Furthermore, a better understanding of\nmultilayer binary neural networks serves as a starting point for generalizing\nBNNs to other neural network architectures such as recurrent neural networks.\n", "versions": [{"version": "v1", "created": "Fri, 19 May 2017 21:33:00 GMT"}], "update_date": "2017-05-23", "authors_parsed": [["Anderson", "Alexander G.", ""], ["Berg", "Cory P.", ""]]}, {"id": "1705.07202", "submitter": "Lei Cai", "authors": "Lei Cai and Hongyang Gao and Shuiwang Ji", "title": "Multi-Stage Variational Auto-Encoders for Coarse-to-Fine Image\n  Generation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Variational auto-encoder (VAE) is a powerful unsupervised learning framework\nfor image generation. One drawback of VAE is that it generates blurry images\ndue to its Gaussianity assumption and thus L2 loss. To allow the generation of\nhigh quality images by VAE, we increase the capacity of decoder network by\nemploying residual blocks and skip connections, which also enable efficient\noptimization. To overcome the limitation of L2 loss, we propose to generate\nimages in a multi-stage manner from coarse to fine. In the simplest case, the\nproposed multi-stage VAE divides the decoder into two components in which the\nsecond component generates refined images based on the course images generated\nby the first component. Since the second component is independent of the VAE\nmodel, it can employ other loss functions beyond the L2 loss and different\nmodel architectures. The proposed framework can be easily generalized to\ncontain more than two components. Experiment results on the MNIST and CelebA\ndatasets demonstrate that the proposed multi-stage VAE can generate sharper\nimages as compared to those from the original VAE.\n", "versions": [{"version": "v1", "created": "Fri, 19 May 2017 21:51:30 GMT"}], "update_date": "2017-05-23", "authors_parsed": [["Cai", "Lei", ""], ["Gao", "Hongyang", ""], ["Ji", "Shuiwang", ""]]}, {"id": "1705.07204", "submitter": "Florian Tram\\`er", "authors": "Florian Tram\\`er, Alexey Kurakin, Nicolas Papernot, Ian Goodfellow,\n  Dan Boneh, Patrick McDaniel", "title": "Ensemble Adversarial Training: Attacks and Defenses", "comments": "22 pages, 5 figures, International Conference on Learning\n  Representations (ICLR) 2018 (amended in April 2020 to include subsequent\n  attacks that significantly reduced the robustness of our models)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Adversarial examples are perturbed inputs designed to fool machine learning\nmodels. Adversarial training injects such examples into training data to\nincrease robustness. To scale this technique to large datasets, perturbations\nare crafted using fast single-step methods that maximize a linear approximation\nof the model's loss. We show that this form of adversarial training converges\nto a degenerate global minimum, wherein small curvature artifacts near the data\npoints obfuscate a linear approximation of the loss. The model thus learns to\ngenerate weak perturbations, rather than defend against strong ones. As a\nresult, we find that adversarial training remains vulnerable to black-box\nattacks, where we transfer perturbations computed on undefended models, as well\nas to a powerful novel single-step attack that escapes the non-smooth vicinity\nof the input data via a small random step. We further introduce Ensemble\nAdversarial Training, a technique that augments training data with\nperturbations transferred from other models. On ImageNet, Ensemble Adversarial\nTraining yields models with strong robustness to black-box attacks. In\nparticular, our most robust model won the first round of the NIPS 2017\ncompetition on Defenses against Adversarial Attacks. However, subsequent work\nfound that more elaborate black-box attacks could significantly enhance\ntransferability and reduce the accuracy of our models.\n", "versions": [{"version": "v1", "created": "Fri, 19 May 2017 21:56:43 GMT"}, {"version": "v2", "created": "Tue, 30 May 2017 17:00:00 GMT"}, {"version": "v3", "created": "Tue, 30 Jan 2018 18:47:39 GMT"}, {"version": "v4", "created": "Sun, 22 Jul 2018 23:43:46 GMT"}, {"version": "v5", "created": "Sun, 26 Apr 2020 22:20:25 GMT"}], "update_date": "2020-04-28", "authors_parsed": [["Tram\u00e8r", "Florian", ""], ["Kurakin", "Alexey", ""], ["Papernot", "Nicolas", ""], ["Goodfellow", "Ian", ""], ["Boneh", "Dan", ""], ["McDaniel", "Patrick", ""]]}, {"id": "1705.07205", "submitter": "Jun Lu", "authors": "Jun Lu", "title": "Machine learning modeling for time series problem: Predicting flight\n  ticket prices", "comments": "7 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Machine learning has been used in all kinds of fields. In this article, we\nintroduce how machine learning can be applied into time series problem.\nEspecially, we use the airline ticket prediction problem as our specific\nproblem. Airline companies use many different variables to determine the flight\nticket prices: indicator whether the travel is during the holidays, the number\nof free seats in the plane etc. Some of the variables are observed, but some of\nthem are hidden. Based on the data over a 103 day period, we trained our\nmodels, getting the best model - which is AdaBoost-Decision Tree\nClassification. This algorithm has best performance over the observed 8 routes\nwhich has 61.35$\\%$ better performance than the random purchase strategy, and\nrelatively small variance over these routes. And we also considered the\nsituation that we cannot get too much historical datas for some routes (for\nexample the route is new and does not have historical data) or we do not want\nto train historical data to predict to buy or wait quickly, in which problem,\nwe used HMM Sequence Classification based AdaBoost-Decision Tree Classification\nto perform our prediction on 12 new routes. Finally, we got 31.71$\\%$ better\nperformance than the random purchase strategy.\n", "versions": [{"version": "v1", "created": "Fri, 19 May 2017 21:59:02 GMT"}, {"version": "v2", "created": "Sun, 4 Feb 2018 03:59:31 GMT"}], "update_date": "2018-02-06", "authors_parsed": [["Lu", "Jun", ""]]}, {"id": "1705.07208", "submitter": "Ryan Dahl", "authors": "Sergio Guadarrama, Ryan Dahl, David Bieber, Mohammad Norouzi, Jonathon\n  Shlens, Kevin Murphy", "title": "PixColor: Pixel Recursive Colorization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel approach to automatically produce multiple colorized\nversions of a grayscale image. Our method results from the observation that the\ntask of automated colorization is relatively easy given a low-resolution\nversion of the color image. We first train a conditional PixelCNN to generate a\nlow resolution color for a given grayscale image. Then, given the generated\nlow-resolution color image and the original grayscale image as inputs, we train\na second CNN to generate a high-resolution colorization of an image. We\ndemonstrate that our approach produces more diverse and plausible colorizations\nthan existing methods, as judged by human raters in a \"Visual Turing Test\".\n", "versions": [{"version": "v1", "created": "Fri, 19 May 2017 22:10:51 GMT"}, {"version": "v2", "created": "Mon, 5 Jun 2017 18:38:01 GMT"}], "update_date": "2017-06-07", "authors_parsed": [["Guadarrama", "Sergio", ""], ["Dahl", "Ryan", ""], ["Bieber", "David", ""], ["Norouzi", "Mohammad", ""], ["Shlens", "Jonathon", ""], ["Murphy", "Kevin", ""]]}, {"id": "1705.07210", "submitter": "Ehsan Amid", "authors": "Ehsan Amid, Manfred K. Warmuth, Sriram Srinivasan", "title": "Two-temperature logistic regression based on the Tsallis divergence", "comments": null, "journal-ref": "In The 22nd International Conference on Artificial Intelligence\n  and Statistics, pp. 2388-2396. 2019", "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop a variant of multiclass logistic regression that is significantly\nmore robust to noise. The algorithm has one weight vector per class and the\nsurrogate loss is a function of the linear activations (one per class). The\nsurrogate loss of an example with linear activation vector $\\mathbf{a}$ and\nclass $c$ has the form $-\\log_{t_1} \\exp_{t_2} (a_c - G_{t_2}(\\mathbf{a}))$\nwhere the two temperatures $t_1$ and $t_2$ ''temper'' the $\\log$ and $\\exp$,\nrespectively, and $G_{t_2}(\\mathbf{a})$ is a scalar value that generalizes the\nlog-partition function. We motivate this loss using the Tsallis divergence. Our\nmethod allows transitioning between non-convex and convex losses by the choice\nof the temperature parameters. As the temperature $t_1$ of the logarithm\nbecomes smaller than the temperature $t_2$ of the exponential, the surrogate\nloss becomes ''quasi convex''. Various tunings of the temperatures recover\nprevious methods and tuning the degree of non-convexity is crucial in the\nexperiments. In particular, quasi-convexity and boundedness of the loss provide\nsignificant robustness to the outliers. We explain this by showing that $t_1 <\n1$ caps the surrogate loss and $t_2 >1$ makes the predictive distribution have\na heavy tail.\n  We show that the surrogate loss is Bayes-consistent, even in the non-convex\ncase. Additionally, we provide efficient iterative algorithms for calculating\nthe log-partition value only in a few number of iterations. Our compelling\nexperimental results on large real-world datasets show the advantage of using\nthe two-temperature variant in the noisy as well as the noise free case.\n", "versions": [{"version": "v1", "created": "Fri, 19 May 2017 22:18:25 GMT"}, {"version": "v2", "created": "Sat, 3 Aug 2019 01:40:56 GMT"}], "update_date": "2019-08-06", "authors_parsed": [["Amid", "Ehsan", ""], ["Warmuth", "Manfred K.", ""], ["Srinivasan", "Sriram", ""]]}, {"id": "1705.07213", "submitter": "Sailik Sengupta", "authors": "Sailik Sengupta, Tathagata Chakraborti, Subbarao Kambhampati", "title": "MTDeep: Boosting the Security of Deep Neural Nets Against Adversarial\n  Attacks with Moving Target Defense", "comments": "Accepted to the Conference on Decision and Game Theory for Security\n  (GameSec), 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR cs.GT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Present attack methods can make state-of-the-art classification systems based\non deep neural networks misclassify every adversarially modified test example.\nThe design of general defense strategies against a wide range of such attacks\nstill remains a challenging problem. In this paper, we draw inspiration from\nthe fields of cybersecurity and multi-agent systems and propose to leverage the\nconcept of Moving Target Defense (MTD) in designing a meta-defense for\n'boosting' the robustness of an ensemble of deep neural networks (DNNs) for\nvisual classification tasks against such adversarial attacks. To classify an\ninput image, a trained network is picked randomly from this set of networks by\nformulating the interaction between a Defender (who hosts the classification\nnetworks) and their (Legitimate and Malicious) users as a Bayesian Stackelberg\nGame (BSG). We empirically show that this approach, MTDeep, reduces\nmisclassification on perturbed images in various datasets such as MNIST,\nFashionMNIST, and ImageNet while maintaining high classification accuracy on\nlegitimate test images. We then demonstrate that our framework, being the first\nmeta-defense technique, can be used in conjunction with any existing defense\nmechanism to provide more resilience against adversarial attacks that can be\nafforded by these defense mechanisms. Lastly, to quantify the increase in\nrobustness of an ensemble-based classification system when we use MTDeep, we\nanalyze the properties of a set of DNNs and introduce the concept of\ndifferential immunity that formalizes the notion of attack transferability.\n", "versions": [{"version": "v1", "created": "Fri, 19 May 2017 22:36:55 GMT"}, {"version": "v2", "created": "Fri, 22 Sep 2017 20:32:46 GMT"}, {"version": "v3", "created": "Sat, 3 Aug 2019 19:43:16 GMT"}], "update_date": "2019-08-06", "authors_parsed": [["Sengupta", "Sailik", ""], ["Chakraborti", "Tathagata", ""], ["Kambhampati", "Subbarao", ""]]}, {"id": "1705.07215", "submitter": "Naveen Kodali", "authors": "Naveen Kodali, Jacob Abernethy, James Hays, Zsolt Kira", "title": "On Convergence and Stability of GANs", "comments": "Analysis of convergence and mode collapse by studying GAN training\n  process as regret minimization. Some new results", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CV cs.GT cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose studying GAN training dynamics as regret minimization, which is in\ncontrast to the popular view that there is consistent minimization of a\ndivergence between real and generated distributions. We analyze the convergence\nof GAN training from this new point of view to understand why mode collapse\nhappens. We hypothesize the existence of undesirable local equilibria in this\nnon-convex game to be responsible for mode collapse. We observe that these\nlocal equilibria often exhibit sharp gradients of the discriminator function\naround some real data points. We demonstrate that these degenerate local\nequilibria can be avoided with a gradient penalty scheme called DRAGAN. We show\nthat DRAGAN enables faster training, achieves improved stability with fewer\nmode collapses, and leads to generator networks with better modeling\nperformance across a variety of architectures and objective functions.\n", "versions": [{"version": "v1", "created": "Fri, 19 May 2017 22:41:56 GMT"}, {"version": "v2", "created": "Wed, 24 May 2017 15:13:01 GMT"}, {"version": "v3", "created": "Thu, 25 May 2017 00:51:40 GMT"}, {"version": "v4", "created": "Fri, 27 Oct 2017 21:47:51 GMT"}, {"version": "v5", "created": "Sun, 10 Dec 2017 15:24:13 GMT"}], "update_date": "2017-12-12", "authors_parsed": [["Kodali", "Naveen", ""], ["Abernethy", "Jacob", ""], ["Hays", "James", ""], ["Kira", "Zsolt", ""]]}, {"id": "1705.07219", "submitter": "Ozsel Kilinc", "authors": "Ozsel Kilinc, Ismail Uysal", "title": "GAR: An efficient and scalable Graph-based Activity Regularization for\n  semi-supervised learning", "comments": null, "journal-ref": null, "doi": "10.1016/j.neucom.2018.03.028", "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a novel graph-based approach for semi-supervised\nlearning problems, which considers an adaptive adjacency of the examples\nthroughout the unsupervised portion of the training. Adjacency of the examples\nis inferred using the predictions of a neural network model which is first\ninitialized by a supervised pretraining. These predictions are then updated\naccording to a novel unsupervised objective which regularizes another\nadjacency, now linking the output nodes. Regularizing the adjacency of the\noutput nodes, inferred from the predictions of the network, creates an easier\noptimization problem and ultimately provides that the predictions of the\nnetwork turn into the optimal embedding. Ultimately, the proposed framework\nprovides an effective and scalable graph-based solution which is natural to the\noperational mechanism of deep neural networks. Our results show comparable\nperformance with state-of-the-art generative approaches for semi-supervised\nlearning on an easier-to-train, low-cost framework.\n", "versions": [{"version": "v1", "created": "Fri, 19 May 2017 23:02:58 GMT"}, {"version": "v2", "created": "Thu, 8 Feb 2018 22:35:43 GMT"}, {"version": "v3", "created": "Wed, 5 Aug 2020 12:29:49 GMT"}], "update_date": "2020-08-06", "authors_parsed": [["Kilinc", "Ozsel", ""], ["Uysal", "Ismail", ""]]}, {"id": "1705.07224", "submitter": "Marco Cusumano-Towner", "authors": "Marco F. Cusumano-Towner, Vikash K. Mansinghka", "title": "AIDE: An algorithm for measuring the accuracy of probabilistic inference\n  algorithms", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Approximate probabilistic inference algorithms are central to many fields.\nExamples include sequential Monte Carlo inference in robotics, variational\ninference in machine learning, and Markov chain Monte Carlo inference in\nstatistics. A key problem faced by practitioners is measuring the accuracy of\nan approximate inference algorithm on a specific data set. This paper\nintroduces the auxiliary inference divergence estimator (AIDE), an algorithm\nfor measuring the accuracy of approximate inference algorithms. AIDE is based\non the observation that inference algorithms can be treated as probabilistic\nmodels and the random variables used within the inference algorithm can be\nviewed as auxiliary variables. This view leads to a new estimator for the\nsymmetric KL divergence between the approximating distributions of two\ninference algorithms. The paper illustrates application of AIDE to algorithms\nfor inference in regression, hidden Markov, and Dirichlet process mixture\nmodels. The experiments show that AIDE captures the qualitative behavior of a\nbroad class of inference algorithms and can detect failure modes of inference\nalgorithms that are missed by standard heuristics.\n", "versions": [{"version": "v1", "created": "Fri, 19 May 2017 23:48:11 GMT"}, {"version": "v2", "created": "Sat, 4 Nov 2017 16:09:58 GMT"}], "update_date": "2017-11-07", "authors_parsed": [["Cusumano-Towner", "Marco F.", ""], ["Mansinghka", "Vikash K.", ""]]}, {"id": "1705.07250", "submitter": "Michael Zimmer", "authors": "Michael F. Zimmer", "title": "Speedup from a different parametrization within the Neural Network\n  algorithm", "comments": "8 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A different parametrization of the hyperplanes is used in the neural network\nalgorithm. As demonstrated on several autoencoder examples it significantly\noutperforms the usual parametrization, reaching lower training error values\nwith only a fraction of the number of epochs. It's argued that it makes it\neasier to understand and initialize the parameters.\n", "versions": [{"version": "v1", "created": "Sat, 20 May 2017 02:54:52 GMT"}, {"version": "v2", "created": "Thu, 1 Jun 2017 13:50:46 GMT"}, {"version": "v3", "created": "Fri, 2 Jun 2017 14:58:53 GMT"}], "update_date": "2017-06-05", "authors_parsed": [["Zimmer", "Michael F.", ""]]}, {"id": "1705.07252", "submitter": "Yifei Jin", "authors": "Yifei Jin and Lingxiao Huang and Jian Li", "title": "SVM via Saddle Point Optimization: New Bounds and Distributed Algorithms", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study two important SVM variants: hard-margin SVM (for linearly separable\ncases) and $\\nu$-SVM (for linearly non-separable cases). We propose new\nalgorithms from the perspective of saddle point optimization. Our algorithms\nachieve $(1-\\epsilon)$-approximations with running time $\\tilde{O}(nd+n\\sqrt{d\n/ \\epsilon})$ for both variants, where $n$ is the number of points and $d$ is\nthe dimensionality. To the best of our knowledge, the current best algorithm\nfor $\\nu$-SVM is based on quadratic programming approach which requires\n$\\Omega(n^2 d)$ time in worst case~\\cite{joachims1998making,platt199912}. In\nthe paper, we provide the first nearly linear time algorithm for $\\nu$-SVM. The\ncurrent best algorithm for hard margin SVM achieved by Gilbert\nalgorithm~\\cite{gartner2009coresets} requires $O(nd / \\epsilon )$ time. Our\nalgorithm improves the running time by a factor of $\\sqrt{d}/\\sqrt{\\epsilon}$.\nMoreover, our algorithms can be implemented in the distributed settings\nnaturally. We prove that our algorithms require $\\tilde{O}(k(d\n+\\sqrt{d/\\epsilon}))$ communication cost, where $k$ is the number of clients,\nwhich almost matches the theoretical lower bound. Numerical experiments support\nour theory and show that our algorithms converge faster on high dimensional,\nlarge and dense data sets, as compared to previous methods.\n", "versions": [{"version": "v1", "created": "Sat, 20 May 2017 03:06:13 GMT"}, {"version": "v2", "created": "Tue, 23 May 2017 07:09:30 GMT"}, {"version": "v3", "created": "Wed, 24 May 2017 05:40:16 GMT"}, {"version": "v4", "created": "Sun, 28 Jan 2018 12:51:58 GMT"}], "update_date": "2018-01-30", "authors_parsed": [["Jin", "Yifei", ""], ["Huang", "Lingxiao", ""], ["Li", "Jian", ""]]}, {"id": "1705.07256", "submitter": "Samet Oymak", "authors": "Samet Oymak, Mehrdad Mahdavi, Jiasi Chen", "title": "Learning Feature Nonlinearities with Non-Convex Regularized Binned\n  Regression", "comments": "22 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.IT math.IT math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For various applications, the relations between the dependent and independent\nvariables are highly nonlinear. Consequently, for large scale complex problems,\nneural networks and regression trees are commonly preferred over linear models\nsuch as Lasso. This work proposes learning the feature nonlinearities by\nbinning feature values and finding the best fit in each quantile using\nnon-convex regularized linear regression. The algorithm first captures the\ndependence between neighboring quantiles by enforcing smoothness via\npiecewise-constant/linear approximation and then selects a sparse subset of\ngood features. We prove that the proposed algorithm is statistically and\ncomputationally efficient. In particular, it achieves linear rate of\nconvergence while requiring near-minimal number of samples. Evaluations on\nsynthetic and real datasets demonstrate that algorithm is competitive with\ncurrent state-of-the-art and accurately learns feature nonlinearities. Finally,\nwe explore an interesting connection between the binning stage of our algorithm\nand sparse Johnson-Lindenstrauss matrices.\n", "versions": [{"version": "v1", "created": "Sat, 20 May 2017 03:46:32 GMT"}], "update_date": "2017-05-23", "authors_parsed": [["Oymak", "Samet", ""], ["Mahdavi", "Mehrdad", ""], ["Chen", "Jiasi", ""]]}, {"id": "1705.07261", "submitter": "Lam Nguyen", "authors": "Lam M. Nguyen, Jie Liu, Katya Scheinberg, Martin Tak\\'a\\v{c}", "title": "Stochastic Recursive Gradient Algorithm for Nonconvex Optimization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we study and analyze the mini-batch version of StochAstic\nRecursive grAdient algoritHm (SARAH), a method employing the stochastic\nrecursive gradient, for solving empirical loss minimization for the case of\nnonconvex losses. We provide a sublinear convergence rate (to stationary\npoints) for general nonconvex functions and a linear convergence rate for\ngradient dominated functions, both of which have some advantages compared to\nother modern stochastic gradient algorithms for nonconvex losses.\n", "versions": [{"version": "v1", "created": "Sat, 20 May 2017 05:09:33 GMT"}], "update_date": "2017-05-23", "authors_parsed": [["Nguyen", "Lam M.", ""], ["Liu", "Jie", ""], ["Scheinberg", "Katya", ""], ["Tak\u00e1\u010d", "Martin", ""]]}, {"id": "1705.07262", "submitter": "Daniel Hein", "authors": "Daniel Hein, Steffen Udluft, Michel Tokic, Alexander Hentschel, Thomas\n  A. Runkler, Volkmar Sterzing", "title": "Batch Reinforcement Learning on the Industrial Benchmark: First\n  Experiences", "comments": null, "journal-ref": "2017 International Joint Conference on Neural Networks (IJCNN),\n  Anchorage, AK, 2017, pp. 4214-4221", "doi": "10.1109/IJCNN.2017.7966389", "report-no": null, "categories": "cs.LG cs.AI cs.NE cs.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Particle Swarm Optimization Policy (PSO-P) has been recently introduced\nand proven to produce remarkable results on interacting with academic\nreinforcement learning benchmarks in an off-policy, batch-based setting. To\nfurther investigate the properties and feasibility on real-world applications,\nthis paper investigates PSO-P on the so-called Industrial Benchmark (IB), a\nnovel reinforcement learning (RL) benchmark that aims at being realistic by\nincluding a variety of aspects found in industrial applications, like\ncontinuous state and action spaces, a high dimensional, partially observable\nstate space, delayed effects, and complex stochasticity. The experimental\nresults of PSO-P on IB are compared to results of closed-form control policies\nderived from the model-based Recurrent Control Neural Network (RCNN) and the\nmodel-free Neural Fitted Q-Iteration (NFQ). Experiments show that PSO-P is not\nonly of interest for academic benchmarks, but also for real-world industrial\napplications, since it also yielded the best performing policy in our IB\nsetting. Compared to other well established RL techniques, PSO-P produced\noutstanding results in performance and robustness, requiring only a relatively\nlow amount of effort in finding adequate parameters or making complex design\ndecisions.\n", "versions": [{"version": "v1", "created": "Sat, 20 May 2017 05:31:52 GMT"}, {"version": "v2", "created": "Thu, 27 Jul 2017 15:34:21 GMT"}], "update_date": "2018-01-26", "authors_parsed": [["Hein", "Daniel", ""], ["Udluft", "Steffen", ""], ["Tokic", "Michel", ""], ["Hentschel", "Alexander", ""], ["Runkler", "Thomas A.", ""], ["Sterzing", "Volkmar", ""]]}, {"id": "1705.07263", "submitter": "Nicholas Carlini", "authors": "Nicholas Carlini, David Wagner", "title": "Adversarial Examples Are Not Easily Detected: Bypassing Ten Detection\n  Methods", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural networks are known to be vulnerable to adversarial examples: inputs\nthat are close to natural inputs but classified incorrectly. In order to better\nunderstand the space of adversarial examples, we survey ten recent proposals\nthat are designed for detection and compare their efficacy. We show that all\ncan be defeated by constructing new loss functions. We conclude that\nadversarial examples are significantly harder to detect than previously\nappreciated, and the properties believed to be intrinsic to adversarial\nexamples are in fact not. Finally, we propose several simple guidelines for\nevaluating future proposed defenses.\n", "versions": [{"version": "v1", "created": "Sat, 20 May 2017 05:59:23 GMT"}, {"version": "v2", "created": "Wed, 1 Nov 2017 04:07:05 GMT"}], "update_date": "2017-11-02", "authors_parsed": [["Carlini", "Nicholas", ""], ["Wagner", "David", ""]]}, {"id": "1705.07267", "submitter": "Jiatao Gu", "authors": "Jiatao Gu, Yong Wang, Kyunghyun Cho and Victor O.K. Li", "title": "Search Engine Guided Non-Parametric Neural Machine Translation", "comments": "Accepted by AAAI 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we extend an attention-based neural machine translation (NMT)\nmodel by allowing it to access an entire training set of parallel sentence\npairs even after training. The proposed approach consists of two stages. In the\nfirst stage--retrieval stage--, an off-the-shelf, black-box search engine is\nused to retrieve a small subset of sentence pairs from a training set given a\nsource sentence. These pairs are further filtered based on a fuzzy matching\nscore based on edit distance. In the second stage--translation stage--, a novel\ntranslation model, called translation memory enhanced NMT (TM-NMT), seamlessly\nuses both the source sentence and a set of retrieved sentence pairs to perform\nthe translation. Empirical evaluation on three language pairs (En-Fr, En-De,\nand En-Es) shows that the proposed approach significantly outperforms the\nbaseline approach and the improvement is more significant when more relevant\nsentence pairs were retrieved.\n", "versions": [{"version": "v1", "created": "Sat, 20 May 2017 06:53:09 GMT"}, {"version": "v2", "created": "Thu, 8 Mar 2018 08:15:24 GMT"}], "update_date": "2018-03-09", "authors_parsed": [["Gu", "Jiatao", ""], ["Wang", "Yong", ""], ["Cho", "Kyunghyun", ""], ["Li", "Victor O. K.", ""]]}, {"id": "1705.07269", "submitter": "Sahil Sharma", "authors": "Sahil Sharma, Aravind Suresh, Rahul Ramesh, Balaraman Ravindran", "title": "Learning to Factor Policies and Action-Value Functions: Factored Action\n  Space Representations for Deep Reinforcement learning", "comments": "11 pages + 7 pages appendix", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep Reinforcement Learning (DRL) methods have performed well in an\nincreasing numbering of high-dimensional visual decision making domains. Among\nall such visual decision making problems, those with discrete action spaces\noften tend to have underlying compositional structure in the said action space.\nSuch action spaces often contain actions such as go left, go up as well as go\ndiagonally up and left (which is a composition of the former two actions). The\nrepresentations of control policies in such domains have traditionally been\nmodeled without exploiting this inherent compositional structure in the action\nspaces. We propose a new learning paradigm, Factored Action space\nRepresentations (FAR) wherein we decompose a control policy learned using a\nDeep Reinforcement Learning Algorithm into independent components, analogous to\ndecomposing a vector in terms of some orthogonal basis vectors. This\narchitectural modification of the control policy representation allows the\nagent to learn about multiple actions simultaneously, while executing only one\nof them. We demonstrate that FAR yields considerable improvements on top of two\nDRL algorithms in Atari 2600: FARA3C outperforms A3C (Asynchronous Advantage\nActor Critic) in 9 out of 14 tasks and FARAQL outperforms AQL (Asynchronous\nn-step Q-Learning) in 9 out of 13 tasks.\n", "versions": [{"version": "v1", "created": "Sat, 20 May 2017 07:18:40 GMT"}], "update_date": "2017-05-23", "authors_parsed": [["Sharma", "Sahil", ""], ["Suresh", "Aravind", ""], ["Ramesh", "Rahul", ""], ["Ravindran", "Balaraman", ""]]}, {"id": "1705.07290", "submitter": "Chandra Sekhar Seelamantula", "authors": "Debabrata Mahapatra, Subhadip Mukherjee, and Chandra Sekhar\n  Seelamantula", "title": "Deep Sparse Coding Using Optimized Linear Expansion of Thresholds", "comments": "Submission date: November 11, 2016. 19 pages; 9 figures", "journal-ref": null, "doi": null, "report-no": "IEEE Transactions on Pattern Analysis and Machine Intelligence\n  Manuscript ID: TPAMI-2016-11-0861;", "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We address the problem of reconstructing sparse signals from noisy and\ncompressive measurements using a feed-forward deep neural network (DNN) with an\narchitecture motivated by the iterative shrinkage-thresholding algorithm\n(ISTA). We maintain the weights and biases of the network links as prescribed\nby ISTA and model the nonlinear activation function using a linear expansion of\nthresholds (LET), which has been very successful in image denoising and\ndeconvolution. The optimal set of coefficients of the parametrized activation\nis learned over a training dataset containing measurement-sparse signal pairs,\ncorresponding to a fixed sensing matrix. For training, we develop an efficient\nsecond-order algorithm, which requires only matrix-vector product computations\nin every training epoch (Hessian-free optimization) and offers superior\nconvergence performance than gradient-descent optimization. Subsequently, we\nderive an improved network architecture inspired by FISTA, a faster version of\nISTA, to achieve similar signal estimation performance with about 50% of the\nnumber of layers. The resulting architecture turns out to be a deep residual\nnetwork, which has recently been shown to exhibit superior performance in\nseveral visual recognition tasks. Numerical experiments demonstrate that the\nproposed DNN architectures lead to 3 to 4 dB improvement in the reconstruction\nsignal-to-noise ratio (SNR), compared with the state-of-the-art sparse coding\nalgorithms.\n", "versions": [{"version": "v1", "created": "Sat, 20 May 2017 11:14:39 GMT"}], "update_date": "2017-05-23", "authors_parsed": [["Mahapatra", "Debabrata", ""], ["Mukherjee", "Subhadip", ""], ["Seelamantula", "Chandra Sekhar", ""]]}, {"id": "1705.07312", "submitter": "Yichen Chen", "authors": "Yichen Chen and Mengdi Wang", "title": "Lower Bound On the Computational Complexity of Discounted Markov\n  Decision Problems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the computational complexity of the infinite-horizon\ndiscounted-reward Markov Decision Problem (MDP) with a finite state space\n$|\\mathcal{S}|$ and a finite action space $|\\mathcal{A}|$. We show that any\nrandomized algorithm needs a running time at least\n$\\Omega(|\\mathcal{S}|^2|\\mathcal{A}|)$ to compute an $\\epsilon$-optimal policy\nwith high probability. We consider two variants of the MDP where the input is\ngiven in specific data structures, including arrays of cumulative probabilities\nand binary trees of transition probabilities. For these cases, we show that the\ncomplexity lower bound reduces to $\\Omega\\left( \\frac{|\\mathcal{S}|\n|\\mathcal{A}|}{\\epsilon} \\right)$. These results reveal a surprising\nobservation that the computational complexity of the MDP depends on the data\nstructure of input.\n", "versions": [{"version": "v1", "created": "Sat, 20 May 2017 14:21:30 GMT"}], "update_date": "2017-05-24", "authors_parsed": [["Chen", "Yichen", ""], ["Wang", "Mengdi", ""]]}, {"id": "1705.07347", "submitter": "Xiuyuan Lu", "authors": "Xiuyuan Lu, Benjamin Van Roy", "title": "Ensemble Sampling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Thompson sampling has emerged as an effective heuristic for a broad range of\nonline decision problems. In its basic form, the algorithm requires computing\nand sampling from a posterior distribution over models, which is tractable only\nfor simple special cases. This paper develops ensemble sampling, which aims to\napproximate Thompson sampling while maintaining tractability even in the face\nof complex models such as neural networks. Ensemble sampling dramatically\nexpands on the range of applications for which Thompson sampling is viable. We\nestablish a theoretical basis that supports the approach and present\ncomputational results that offer further insight.\n", "versions": [{"version": "v1", "created": "Sat, 20 May 2017 19:36:36 GMT"}, {"version": "v2", "created": "Wed, 5 Jul 2017 21:11:12 GMT"}, {"version": "v3", "created": "Wed, 22 Nov 2017 21:48:42 GMT"}], "update_date": "2017-11-27", "authors_parsed": [["Lu", "Xiuyuan", ""], ["Van Roy", "Benjamin", ""]]}, {"id": "1705.07349", "submitter": "Ning Xu", "authors": "Ning Xu, Jian Hong, Timothy C.G. Fisher", "title": "$\\left( \\beta, \\varpi \\right)$-stability for cross-validation and the\n  choice of the number of folds", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we introduce a new concept of stability for cross-validation,\ncalled the $\\left( \\beta, \\varpi \\right)$-stability, and use it as a new\nperspective to build the general theory for cross-validation. The $\\left(\n\\beta, \\varpi \\right)$-stability mathematically connects the generalization\nability and the stability of the cross-validated model via the Rademacher\ncomplexity. Our result reveals mathematically the effect of cross-validation\nfrom two sides: on one hand, cross-validation picks the model with the best\nempirical generalization ability by validating all the alternatives on test\nsets; on the other hand, cross-validation may compromise the stability of the\nmodel selection by causing subsampling error. Moreover, the difference between\ntraining and test errors in q\\textsuperscript{th} round, sometimes referred to\nas the generalization error, might be autocorrelated on q. Guided by the ideas\nabove, the $\\left( \\beta, \\varpi \\right)$-stability help us derivd a new class\nof Rademacher bounds, referred to as the one-round/convoluted Rademacher\nbounds, for the stability of cross-validation in both the i.i.d.\\ and\nnon-i.i.d.\\ cases. For both light-tail and heavy-tail losses, the new bounds\nquantify the stability of the one-round/average test error of the\ncross-validated model in terms of its one-round/average training error, the\nsample sizes $n$, number of folds $K$, the tail property of the loss (encoded\nas Orlicz-$\\Psi_\\nu$ norms) and the Rademacher complexity of the model class\n$\\Lambda$. The new class of bounds not only quantitatively reveals the\nstability of the generalization ability of the cross-validated model, it also\nshows empirically the optimal choice for number of folds $K$, at which the\nupper bound of the one-round/average test error is lowest, or, to put it in\nanother way, where the test error is most stable.\n", "versions": [{"version": "v1", "created": "Sat, 20 May 2017 19:46:01 GMT"}, {"version": "v2", "created": "Sat, 27 May 2017 22:53:42 GMT"}, {"version": "v3", "created": "Tue, 30 May 2017 01:44:13 GMT"}, {"version": "v4", "created": "Mon, 19 Jun 2017 10:54:21 GMT"}, {"version": "v5", "created": "Thu, 6 Jul 2017 00:21:03 GMT"}], "update_date": "2017-07-07", "authors_parsed": [["Xu", "Ning", ""], ["Hong", "Jian", ""], ["Fisher", "Timothy C. G.", ""]]}, {"id": "1705.07364", "submitter": "Sohil Shah", "authors": "Abhay Yadav, Sohil Shah, Zheng Xu, David Jacobs and Tom Goldstein", "title": "Stabilizing Adversarial Nets With Prediction Methods", "comments": "Accepted at ICLR 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Adversarial neural networks solve many important problems in data science,\nbut are notoriously difficult to train. These difficulties come from the fact\nthat optimal weights for adversarial nets correspond to saddle points, and not\nminimizers, of the loss function. The alternating stochastic gradient methods\ntypically used for such problems do not reliably converge to saddle points, and\nwhen convergence does happen it is often highly sensitive to learning rates. We\npropose a simple modification of stochastic gradient descent that stabilizes\nadversarial networks. We show, both in theory and practice, that the proposed\nmethod reliably converges to saddle points, and is stable with a wider range of\ntraining parameters than a non-prediction method. This makes adversarial\nnetworks less likely to \"collapse,\" and enables faster training with larger\nlearning rates.\n", "versions": [{"version": "v1", "created": "Sat, 20 May 2017 22:27:19 GMT"}, {"version": "v2", "created": "Fri, 9 Jun 2017 04:22:35 GMT"}, {"version": "v3", "created": "Thu, 8 Feb 2018 21:57:54 GMT"}], "update_date": "2018-02-12", "authors_parsed": [["Yadav", "Abhay", ""], ["Shah", "Sohil", ""], ["Xu", "Zheng", ""], ["Jacobs", "David", ""], ["Goldstein", "Tom", ""]]}, {"id": "1705.07366", "submitter": "Jeffrey Humpherys", "authors": "Kevin Miller, Chris Hettinger, Jeffrey Humpherys, Tyler Jarvis, and\n  David Kartchner", "title": "Forward Thinking: Building Deep Random Forests", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The success of deep neural networks has inspired many to wonder whether other\nlearners could benefit from deep, layered architectures. We present a general\nframework called forward thinking for deep learning that generalizes the\narchitectural flexibility and sophistication of deep neural networks while also\nallowing for (i) different types of learning functions in the network, other\nthan neurons, and (ii) the ability to adaptively deepen the network as needed\nto improve results. This is done by training one layer at a time, and once a\nlayer is trained, the input data are mapped forward through the layer to create\na new learning problem. The process is then repeated, transforming the data\nthrough multiple layers, one at a time, rendering a new dataset, which is\nexpected to be better behaved, and on which a final output layer can achieve\ngood performance. In the case where the neurons of deep neural nets are\nreplaced with decision trees, we call the result a Forward Thinking Deep Random\nForest (FTDRF). We demonstrate a proof of concept by applying FTDRF on the\nMNIST dataset. We also provide a general mathematical formulation that allows\nfor other types of deep learning problems to be considered.\n", "versions": [{"version": "v1", "created": "Sat, 20 May 2017 22:39:51 GMT"}], "update_date": "2017-05-23", "authors_parsed": [["Miller", "Kevin", ""], ["Hettinger", "Chris", ""], ["Humpherys", "Jeffrey", ""], ["Jarvis", "Tyler", ""], ["Kartchner", "David", ""]]}, {"id": "1705.07368", "submitter": "James Foulds", "authors": "James Foulds", "title": "Mixed Membership Word Embeddings for Computational Social Science", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Word embeddings improve the performance of NLP systems by revealing the\nhidden structural relationships between words. Despite their success in many\napplications, word embeddings have seen very little use in computational social\nscience NLP tasks, presumably due to their reliance on big data, and to a lack\nof interpretability. I propose a probabilistic model-based word embedding\nmethod which can recover interpretable embeddings, without big data. The key\ninsight is to leverage mixed membership modeling, in which global\nrepresentations are shared, but individual entities (i.e. dictionary words) are\nfree to use these representations to uniquely differing degrees. I show how to\ntrain the model using a combination of state-of-the-art training techniques for\nword embeddings and topic models. The experimental results show an improvement\nin predictive language modeling of up to 63% in MRR over the skip-gram, and\ndemonstrate that the representations are beneficial for supervised learning. I\nillustrate the interpretability of the models with computational social science\ncase studies on State of the Union addresses and NIPS articles.\n", "versions": [{"version": "v1", "created": "Sat, 20 May 2017 23:45:54 GMT"}, {"version": "v2", "created": "Thu, 25 May 2017 03:12:35 GMT"}, {"version": "v3", "created": "Tue, 20 Feb 2018 00:34:49 GMT"}], "update_date": "2018-02-21", "authors_parsed": [["Foulds", "James", ""]]}, {"id": "1705.07377", "submitter": "Nathan Kallus", "authors": "Nathan Kallus", "title": "Instrument-Armed Bandits", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We extend the classic multi-armed bandit (MAB) model to the setting of\nnoncompliance, where the arm pull is a mere instrument and the treatment\napplied may differ from it, which gives rise to the instrument-armed bandit\n(IAB) problem. The IAB setting is relevant whenever the experimental units are\nhuman since free will, ethics, and the law may prohibit unrestricted or forced\napplication of treatment. In particular, the setting is relevant in bandit\nmodels of dynamic clinical trials and other controlled trials on human\ninterventions. Nonetheless, the setting has not been fully investigate in the\nbandit literature. We show that there are various and divergent notions of\nregret in this setting, all of which coincide only in the classic MAB setting.\nWe characterize the behavior of these regrets and analyze standard MAB\nalgorithms. We argue for a particular kind of regret that captures the causal\neffect of treatments but show that standard MAB algorithms cannot achieve\nsublinear control on this regret. Instead, we develop new algorithms for the\nIAB problem, prove new regret bounds for them, and compare them to standard MAB\nalgorithms in numerical examples.\n", "versions": [{"version": "v1", "created": "Sun, 21 May 2017 02:23:36 GMT"}], "update_date": "2017-05-23", "authors_parsed": [["Kallus", "Nathan", ""]]}, {"id": "1705.07384", "submitter": "Nathan Kallus", "authors": "Nathan Kallus", "title": "Balanced Policy Evaluation and Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a new approach to the problems of evaluating and learning\npersonalized decision policies from observational data of past contexts,\ndecisions, and outcomes. Only the outcome of the enacted decision is available\nand the historical policy is unknown. These problems arise in personalized\nmedicine using electronic health records and in internet advertising. Existing\napproaches use inverse propensity weighting (or, doubly robust versions) to\nmake historical outcome (or, residual) data look like it were generated by a\nnew policy being evaluated or learned. But this relies on a plug-in approach\nthat rejects data points with a decision that disagrees with the new policy,\nleading to high variance estimates and ineffective learning. We propose a new,\nbalance-based approach that too makes the data look like the new policy but\ndoes so directly by finding weights that optimize for balance between the\nweighted data and the target policy in the given, finite sample, which is\nequivalent to minimizing worst-case or posterior conditional mean square error.\nOur policy learner proceeds as a two-level optimization problem over policies\nand weights. We demonstrate that this approach markedly outperforms existing\nones both in evaluation and learning, which is unsurprising given the wider\nsupport of balance-based weights. We establish extensive theoretical\nconsistency guarantees and regret bounds that support this empirical success.\n", "versions": [{"version": "v1", "created": "Sun, 21 May 2017 03:03:27 GMT"}, {"version": "v2", "created": "Mon, 3 Jun 2019 13:54:48 GMT"}], "update_date": "2019-06-04", "authors_parsed": [["Kallus", "Nathan", ""]]}, {"id": "1705.07386", "submitter": "Philip Bontrager", "authors": "Philip Bontrager, Aditi Roy, Julian Togelius, Nasir Memon, Arun Ross", "title": "DeepMasterPrints: Generating MasterPrints for Dictionary Attacks via\n  Latent Variable Evolution", "comments": "8 pages; added new verification systems and diagrams. Accepted to\n  conference Biometrics: Theory, Applications, and Systems 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent research has demonstrated the vulnerability of fingerprint recognition\nsystems to dictionary attacks based on MasterPrints. MasterPrints are real or\nsynthetic fingerprints that can fortuitously match with a large number of\nfingerprints thereby undermining the security afforded by fingerprint systems.\nPrevious work by Roy et al. generated synthetic MasterPrints at the\nfeature-level. In this work we generate complete image-level MasterPrints known\nas DeepMasterPrints, whose attack accuracy is found to be much superior than\nthat of previous methods. The proposed method, referred to as Latent Variable\nEvolution, is based on training a Generative Adversarial Network on a set of\nreal fingerprint images. Stochastic search in the form of the Covariance Matrix\nAdaptation Evolution Strategy is then used to search for latent input variables\nto the generator network that can maximize the number of impostor matches as\nassessed by a fingerprint recognizer. Experiments convey the efficacy of the\nproposed method in generating DeepMasterPrints. The underlying method is likely\nto have broad applications in fingerprint security as well as fingerprint\nsynthesis.\n", "versions": [{"version": "v1", "created": "Sun, 21 May 2017 03:43:46 GMT"}, {"version": "v2", "created": "Tue, 3 Oct 2017 21:51:38 GMT"}, {"version": "v3", "created": "Fri, 16 Feb 2018 22:42:13 GMT"}, {"version": "v4", "created": "Thu, 18 Oct 2018 21:29:47 GMT"}], "update_date": "2018-10-22", "authors_parsed": [["Bontrager", "Philip", ""], ["Roy", "Aditi", ""], ["Togelius", "Julian", ""], ["Memon", "Nasir", ""], ["Ross", "Arun", ""]]}, {"id": "1705.07404", "submitter": "Joe Klobusicky", "authors": "Chirag Agarwal, Joe Klobusicky, and Dan Schonfeld", "title": "Convergence of backpropagation with momentum for network architectures\n  with skip connections", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study a class of deep neural networks with networks that form a directed\nacyclic graph (DAG). For backpropagation defined by gradient descent with\nadaptive momentum, we show weights converge for a large class of nonlinear\nactivation functions. The proof generalizes the results of Wu et al. (2008) who\nshowed convergence for a feed forward network with one hidden layer. For an\nexample of the effectiveness of DAG architectures, we describe an example of\ncompression through an autoencoder, and compare against sequential feed forward\nnetworks under several metrics.\n", "versions": [{"version": "v1", "created": "Sun, 21 May 2017 06:50:49 GMT"}, {"version": "v2", "created": "Sun, 15 Jul 2018 23:57:53 GMT"}, {"version": "v3", "created": "Mon, 17 Dec 2018 01:42:03 GMT"}, {"version": "v4", "created": "Sun, 19 Jan 2020 04:59:25 GMT"}], "update_date": "2020-01-22", "authors_parsed": [["Agarwal", "Chirag", ""], ["Klobusicky", "Joe", ""], ["Schonfeld", "Dan", ""]]}, {"id": "1705.07414", "submitter": "Ming Chen", "authors": "Jing Zhang and Ming Chen", "title": "Unfolding Hidden Barriers by Active Enhanced Sampling", "comments": "5 pages, 3 figures", "journal-ref": "Phys. Rev. Lett. 121, 010601 (2018)", "doi": "10.1103/PhysRevLett.121.010601", "report-no": null, "categories": "physics.chem-ph cond-mat.stat-mech cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Collective variable (CV) or order parameter based enhanced sampling\nalgorithms have achieved great success due to their ability to efficiently\nexplore the rough potential energy landscapes of complex systems. However, the\ndegeneracy of microscopic configurations, originating from the orthogonal space\nperpendicular to the CVs, is likely to shadow \"hidden barriers\" and greatly\nreduce the efficiency of CV-based sampling. Here we demonstrate that systematic\nmachine learning CV, through enhanced sampling, can iteratively lift such\ndegeneracies on the fly. We introduce an active learning scheme that consists\nof a parametric CV learner based on deep neural network and a CV-based enhanced\nsampler. Our active enhanced sampling (AES) algorithm is capable of identifying\nthe least informative regions based on a historical sample, forming a positive\nfeedback loop between the CV learner and sampler. This approach is able to\nglobally preserve kinetic characteristics by incrementally enhancing both\nsample completeness and CV quality.\n", "versions": [{"version": "v1", "created": "Sun, 21 May 2017 08:39:23 GMT"}, {"version": "v2", "created": "Fri, 10 Nov 2017 07:44:35 GMT"}], "update_date": "2018-07-11", "authors_parsed": [["Zhang", "Jing", ""], ["Chen", "Ming", ""]]}, {"id": "1705.07425", "submitter": "Thomas Niebler", "authors": "Thomas Niebler, Martin Becker, Christian P\\\"olitz, Andreas Hotho", "title": "Learning Semantic Relatedness From Human Feedback Using Metric Learning", "comments": "Under review at ISWC 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Assessing the degree of semantic relatedness between words is an important\ntask with a variety of semantic applications, such as ontology learning for the\nSemantic Web, semantic search or query expansion. To accomplish this in an\nautomated fashion, many relatedness measures have been proposed. However, most\nof these metrics only encode information contained in the underlying corpus and\nthus do not directly model human intuition. To solve this, we propose to\nutilize a metric learning approach to improve existing semantic relatedness\nmeasures by learning from additional information, such as explicit human\nfeedback. For this, we argue to use word embeddings instead of traditional\nhigh-dimensional vector representations in order to leverage their semantic\ndensity and to reduce computational cost. We rigorously test our approach on\nseveral domains including tagging data as well as publicly available embeddings\nbased on Wikipedia texts and navigation. Human feedback about semantic\nrelatedness for learning and evaluation is extracted from publicly available\ndatasets such as MEN or WS-353. We find that our method can significantly\nimprove semantic relatedness measures by learning from additional information,\nsuch as explicit human feedback. For tagging data, we are the first to generate\nand study embeddings. Our results are of special interest for ontology and\nrecommendation engineers, but also for any other researchers and practitioners\nof Semantic Web techniques.\n", "versions": [{"version": "v1", "created": "Sun, 21 May 2017 10:16:49 GMT"}, {"version": "v2", "created": "Wed, 24 May 2017 13:07:07 GMT"}], "update_date": "2017-05-25", "authors_parsed": [["Niebler", "Thomas", ""], ["Becker", "Martin", ""], ["P\u00f6litz", "Christian", ""], ["Hotho", "Andreas", ""]]}, {"id": "1705.07443", "submitter": "Matthew Staib", "authors": "Matthew Staib, Sebastian Claici, Justin Solomon, Stefanie Jegelka", "title": "Parallel Streaming Wasserstein Barycenters", "comments": "NIPS 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.OC stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Efficiently aggregating data from different sources is a challenging problem,\nparticularly when samples from each source are distributed differently. These\ndifferences can be inherent to the inference task or present for other reasons:\nsensors in a sensor network may be placed far apart, affecting their individual\nmeasurements. Conversely, it is computationally advantageous to split Bayesian\ninference tasks across subsets of data, but data need not be identically\ndistributed across subsets. One principled way to fuse probability\ndistributions is via the lens of optimal transport: the Wasserstein barycenter\nis a single distribution that summarizes a collection of input measures while\nrespecting their geometry. However, computing the barycenter scales poorly and\nrequires discretization of all input distributions and the barycenter itself.\nImproving on this situation, we present a scalable, communication-efficient,\nparallel algorithm for computing the Wasserstein barycenter of arbitrary\ndistributions. Our algorithm can operate directly on continuous input\ndistributions and is optimized for streaming data. Our method is even robust to\nnonstationary input distributions and produces a barycenter estimate that\ntracks the input measures over time. The algorithm is semi-discrete, needing to\ndiscretize only the barycenter estimate. To the best of our knowledge, we also\nprovide the first bounds on the quality of the approximate barycenter as the\ndiscretization becomes finer. Finally, we demonstrate the practical\neffectiveness of our method, both in tracking moving distributions on a sphere,\nas well as in a large-scale Bayesian inference task.\n", "versions": [{"version": "v1", "created": "Sun, 21 May 2017 12:24:27 GMT"}, {"version": "v2", "created": "Tue, 14 Nov 2017 04:19:11 GMT"}], "update_date": "2017-11-15", "authors_parsed": [["Staib", "Matthew", ""], ["Claici", "Sebastian", ""], ["Solomon", "Justin", ""], ["Jegelka", "Stefanie", ""]]}, {"id": "1705.07445", "submitter": "Sahil Sharma", "authors": "Sahil Sharma, Girish Raguvir J, Srivatsan Ramesh, Balaraman Ravindran", "title": "Learning to Mix n-Step Returns: Generalizing lambda-Returns for Deep\n  Reinforcement Learning", "comments": "10 pages + 9 page appendix", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Reinforcement Learning (RL) can model complex behavior policies for\ngoal-directed sequential decision making tasks. A hallmark of RL algorithms is\nTemporal Difference (TD) learning: value function for the current state is\nmoved towards a bootstrapped target that is estimated using next state's value\nfunction. $\\lambda$-returns generalize beyond 1-step returns and strike a\nbalance between Monte Carlo and TD learning methods. While lambda-returns have\nbeen extensively studied in RL, they haven't been explored a lot in Deep RL.\nThis paper's first contribution is an exhaustive benchmarking of\nlambda-returns. Although mathematically tractable, the use of exponentially\ndecaying weighting of n-step returns based targets in lambda-returns is a\nrather ad-hoc design choice. Our second major contribution is that we propose a\ngeneralization of lambda-returns called Confidence-based Autodidactic Returns\n(CAR), wherein the RL agent learns the weighting of the n-step returns in an\nend-to-end manner. This allows the agent to learn to decide how much it wants\nto weigh the n-step returns based targets. In contrast, lambda-returns restrict\nRL agents to use an exponentially decaying weighting scheme. Autodidactic\nreturns can be used for improving any RL algorithm which uses TD learning. We\nempirically demonstrate that using sophisticated weighted mixtures of\nmulti-step returns (like CAR and lambda-returns) considerably outperforms the\nuse of n-step returns. We perform our experiments on the Asynchronous Advantage\nActor Critic (A3C) algorithm in the Atari 2600 domain.\n", "versions": [{"version": "v1", "created": "Sun, 21 May 2017 12:47:37 GMT"}, {"version": "v2", "created": "Sun, 5 Nov 2017 06:36:53 GMT"}], "update_date": "2017-11-07", "authors_parsed": [["Sharma", "Sahil", ""], ["J", "Girish Raguvir", ""], ["Ramesh", "Srivatsan", ""], ["Ravindran", "Balaraman", ""]]}, {"id": "1705.07461", "submitter": "Nir Levine", "authors": "Nir Levine, Tom Zahavy, Daniel J. Mankowitz, Aviv Tamar, Shie Mannor", "title": "Shallow Updates for Deep Reinforcement Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep reinforcement learning (DRL) methods such as the Deep Q-Network (DQN)\nhave achieved state-of-the-art results in a variety of challenging,\nhigh-dimensional domains. This success is mainly attributed to the power of\ndeep neural networks to learn rich domain representations for approximating the\nvalue function or policy. Batch reinforcement learning methods with linear\nrepresentations, on the other hand, are more stable and require less hyper\nparameter tuning. Yet, substantial feature engineering is necessary to achieve\ngood results. In this work we propose a hybrid approach -- the Least Squares\nDeep Q-Network (LS-DQN), which combines rich feature representations learned by\na DRL algorithm with the stability of a linear least squares method. We do this\nby periodically re-training the last hidden layer of a DRL network with a batch\nleast squares update. Key to our approach is a Bayesian regularization term for\nthe least squares update, which prevents over-fitting to the more recent data.\nWe tested LS-DQN on five Atari games and demonstrate significant improvement\nover vanilla DQN and Double-DQN. We also investigated the reasons for the\nsuperior performance of our method. Interestingly, we found that the\nperformance improvement can be attributed to the large batch size used by the\nLS method when optimizing the last layer.\n", "versions": [{"version": "v1", "created": "Sun, 21 May 2017 15:20:15 GMT"}, {"version": "v2", "created": "Thu, 2 Nov 2017 19:00:40 GMT"}], "update_date": "2017-11-06", "authors_parsed": [["Levine", "Nir", ""], ["Zahavy", "Tom", ""], ["Mankowitz", "Daniel J.", ""], ["Tamar", "Aviv", ""], ["Mannor", "Shie", ""]]}, {"id": "1705.07474", "submitter": "Alex Townsend", "authors": "Madeleine Udell and Alex Townsend", "title": "Why are Big Data Matrices Approximately Low Rank?", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Matrices of (approximate) low rank are pervasive in data science, appearing\nin recommender systems, movie preferences, topic models, medical records, and\ngenomics. While there is a vast literature on how to exploit low rank structure\nin these datasets, there is less attention on explaining why the low rank\nstructure appears in the first place. Here, we explain the effectiveness of low\nrank models in data science by considering a simple generative model for these\nmatrices: we suppose that each row or column is associated to a (possibly high\ndimensional) bounded latent variable, and entries of the matrix are generated\nby applying a piecewise analytic function to these latent variables. These\nmatrices are in general full rank. However, we show that we can approximate\nevery entry of an $m \\times n$ matrix drawn from this model to within a fixed\nabsolute error by a low rank matrix whose rank grows as $\\mathcal O(\\log(m +\nn))$. Hence any sufficiently large matrix from such a latent variable model can\nbe approximated, up to a small entrywise error, by a low rank matrix.\n", "versions": [{"version": "v1", "created": "Sun, 21 May 2017 16:49:36 GMT"}, {"version": "v2", "created": "Tue, 29 May 2018 18:23:30 GMT"}], "update_date": "2018-05-31", "authors_parsed": [["Udell", "Madeleine", ""], ["Townsend", "Alex", ""]]}, {"id": "1705.07477", "submitter": "Tianyang Li", "authors": "Tianyang Li, Liu Liu, Anastasios Kyrillidis, Constantine Caramanis", "title": "Statistical inference using SGD", "comments": "To appear in AAAI 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI math.OC math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a novel method for frequentist statistical inference in\n$M$-estimation problems, based on stochastic gradient descent (SGD) with a\nfixed step size: we demonstrate that the average of such SGD sequences can be\nused for statistical inference, after proper scaling. An intuitive analysis\nusing the Ornstein-Uhlenbeck process suggests that such averages are\nasymptotically normal. From a practical perspective, our SGD-based inference\nprocedure is a first order method, and is well-suited for large scale problems.\nTo show its merits, we apply it to both synthetic and real datasets, and\ndemonstrate that its accuracy is comparable to classical statistical methods,\nwhile requiring potentially far less computation.\n", "versions": [{"version": "v1", "created": "Sun, 21 May 2017 17:01:51 GMT"}, {"version": "v2", "created": "Sun, 19 Nov 2017 20:59:52 GMT"}], "update_date": "2017-11-21", "authors_parsed": [["Li", "Tianyang", ""], ["Liu", "Liu", ""], ["Kyrillidis", "Anastasios", ""], ["Caramanis", "Constantine", ""]]}, {"id": "1705.07485", "submitter": "Xavier Gastaldi", "authors": "Xavier Gastaldi", "title": "Shake-Shake regularization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The method introduced in this paper aims at helping deep learning\npractitioners faced with an overfit problem. The idea is to replace, in a\nmulti-branch network, the standard summation of parallel branches with a\nstochastic affine combination. Applied to 3-branch residual networks,\nshake-shake regularization improves on the best single shot published results\non CIFAR-10 and CIFAR-100 by reaching test errors of 2.86% and 15.85%.\nExperiments on architectures without skip connections or Batch Normalization\nshow encouraging results and open the door to a large set of applications. Code\nis available at https://github.com/xgastaldi/shake-shake\n", "versions": [{"version": "v1", "created": "Sun, 21 May 2017 18:51:27 GMT"}, {"version": "v2", "created": "Tue, 23 May 2017 13:36:46 GMT"}], "update_date": "2017-05-24", "authors_parsed": [["Gastaldi", "Xavier", ""]]}, {"id": "1705.07505", "submitter": "Arash Mehrjou", "authors": "Arash Mehrjou, Bernhard Sch\\\"olkopf, Saeed Saremi", "title": "Annealed Generative Adversarial Networks", "comments": "9 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a novel framework for adversarial training where the target\ndistribution is annealed between the uniform distribution and the data\ndistribution. We posited a conjecture that learning under continuous annealing\nin the nonparametric regime is stable irrespective of the divergence measures\nin the objective function and proposed an algorithm, dubbed {\\ss}-GAN, in\ncorollary. In this framework, the fact that the initial support of the\ngenerative network is the whole ambient space combined with annealing are key\nto balancing the minimax game. In our experiments on synthetic data, MNIST, and\nCelebA, {\\ss}-GAN with a fixed annealing schedule was stable and did not suffer\nfrom mode collapse.\n", "versions": [{"version": "v1", "created": "Sun, 21 May 2017 20:05:59 GMT"}], "update_date": "2017-05-23", "authors_parsed": [["Mehrjou", "Arash", ""], ["Sch\u00f6lkopf", "Bernhard", ""], ["Saremi", "Saeed", ""]]}, {"id": "1705.07538", "submitter": "Peter Bailis", "authors": "Peter Bailis, Kunle Olukotun, Christopher Re, Matei Zaharia", "title": "Infrastructure for Usable Machine Learning: The Stanford DAWN Project", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DB stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Despite incredible recent advances in machine learning, building machine\nlearning applications remains prohibitively time-consuming and expensive for\nall but the best-trained, best-funded engineering organizations. This expense\ncomes not from a need for new and improved statistical models but instead from\na lack of systems and tools for supporting end-to-end machine learning\napplication development, from data preparation and labeling to\nproductionization and monitoring. In this document, we outline opportunities\nfor infrastructure supporting usable, end-to-end machine learning applications\nin the context of the nascent DAWN (Data Analytics for What's Next) project at\nStanford.\n", "versions": [{"version": "v1", "created": "Mon, 22 May 2017 02:28:19 GMT"}, {"version": "v2", "created": "Fri, 9 Jun 2017 02:13:09 GMT"}], "update_date": "2017-06-12", "authors_parsed": [["Bailis", "Peter", ""], ["Olukotun", "Kunle", ""], ["Re", "Christopher", ""], ["Zaharia", "Matei", ""]]}, {"id": "1705.07541", "submitter": "Takashi Ishida", "authors": "Takashi Ishida, Gang Niu, Weihua Hu, Masashi Sugiyama", "title": "Learning from Complementary Labels", "comments": "NIPS 2017 camera-ready version", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Collecting labeled data is costly and thus a critical bottleneck in\nreal-world classification tasks. To mitigate this problem, we propose a novel\nsetting, namely learning from complementary labels for multi-class\nclassification. A complementary label specifies a class that a pattern does not\nbelong to. Collecting complementary labels would be less laborious than\ncollecting ordinary labels, since users do not have to carefully choose the\ncorrect class from a long list of candidate classes. However, complementary\nlabels are less informative than ordinary labels and thus a suitable approach\nis needed to better learn from them. In this paper, we show that an unbiased\nestimator to the classification risk can be obtained only from complementarily\nlabeled data, if a loss function satisfies a particular symmetric condition. We\nderive estimation error bounds for the proposed method and prove that the\noptimal parametric convergence rate is achieved. We further show that learning\nfrom complementary labels can be easily combined with learning from ordinary\nlabels (i.e., ordinary supervised learning), providing a highly practical\nimplementation of the proposed method. Finally, we experimentally demonstrate\nthe usefulness of the proposed methods.\n", "versions": [{"version": "v1", "created": "Mon, 22 May 2017 02:40:47 GMT"}, {"version": "v2", "created": "Sun, 12 Nov 2017 13:38:55 GMT"}], "update_date": "2017-11-15", "authors_parsed": [["Ishida", "Takashi", ""], ["Niu", "Gang", ""], ["Hu", "Weihua", ""], ["Sugiyama", "Masashi", ""]]}, {"id": "1705.07562", "submitter": "Lei Li", "authors": "Wenqing Hu, Chris Junchi Li, Lei Li, Jian-Guo Liu", "title": "On the diffusion approximation of nonconvex stochastic gradient descent", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the Stochastic Gradient Descent (SGD) method in nonconvex\noptimization problems from the point of view of approximating diffusion\nprocesses. We prove rigorously that the diffusion process can approximate the\nSGD algorithm weakly using the weak form of master equation for probability\nevolution. In the small step size regime and the presence of omnidirectional\nnoise, our weak approximating diffusion process suggests the following dynamics\nfor the SGD iteration starting from a local minimizer (resp.~saddle point): it\nescapes in a number of iterations exponentially (resp.~almost linearly)\ndependent on the inverse stepsize. The results are obtained using the theory\nfor random perturbations of dynamical systems (theory of large deviations for\nlocal minimizers and theory of exiting for unstable stationary points). In\naddition, we discuss the effects of batch size for the deep neural networks,\nand we find that small batch size is helpful for SGD algorithms to escape\nunstable stationary points and sharp minimizers. Our theory indicates that one\nshould increase the batch size at later stage for the SGD to be trapped in flat\nminimizers for better generalization.\n", "versions": [{"version": "v1", "created": "Mon, 22 May 2017 05:34:00 GMT"}, {"version": "v2", "created": "Sat, 3 Mar 2018 15:31:03 GMT"}], "update_date": "2018-03-06", "authors_parsed": [["Hu", "Wenqing", ""], ["Li", "Chris Junchi", ""], ["Li", "Lei", ""], ["Liu", "Jian-Guo", ""]]}, {"id": "1705.07565", "submitter": "Xin Dong", "authors": "Xin Dong, Shangyu Chen, Sinno Jialin Pan", "title": "Learning to Prune Deep Neural Networks via Layer-wise Optimal Brain\n  Surgeon", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  How to develop slim and accurate deep neural networks has become crucial for\nreal- world applications, especially for those employed in embedded systems.\nThough previous work along this research line has shown some promising results,\nmost existing methods either fail to significantly compress a well-trained deep\nnetwork or require a heavy retraining process for the pruned deep network to\nre-boost its prediction performance. In this paper, we propose a new layer-wise\npruning method for deep neural networks. In our proposed method, parameters of\neach individual layer are pruned independently based on second order\nderivatives of a layer-wise error function with respect to the corresponding\nparameters. We prove that the final prediction performance drop after pruning\nis bounded by a linear combination of the reconstructed errors caused at each\nlayer. Therefore, there is a guarantee that one only needs to perform a light\nretraining process on the pruned network to resume its original prediction\nperformance. We conduct extensive experiments on benchmark datasets to\ndemonstrate the effectiveness of our pruning method compared with several\nstate-of-the-art baseline methods.\n", "versions": [{"version": "v1", "created": "Mon, 22 May 2017 05:54:37 GMT"}, {"version": "v2", "created": "Thu, 9 Nov 2017 23:50:55 GMT"}], "update_date": "2019-10-02", "authors_parsed": [["Dong", "Xin", ""], ["Chen", "Shangyu", ""], ["Pan", "Sinno Jialin", ""]]}, {"id": "1705.07576", "submitter": "Paul Hand", "authors": "Paul Hand, Vladislav Voroninski", "title": "Global Guarantees for Enforcing Deep Generative Priors by Empirical Risk", "comments": "Accepted for presentation at Conference on Learning Theory (COLT)\n  2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.LG math.IT math.OC math.PR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We examine the theoretical properties of enforcing priors provided by\ngenerative deep neural networks via empirical risk minimization. In particular\nwe consider two models, one in which the task is to invert a generative neural\nnetwork given access to its last layer and another in which the task is to\ninvert a generative neural network given only compressive linear observations\nof its last layer. We establish that in both cases, in suitable regimes of\nnetwork layer sizes and a randomness assumption on the network weights, that\nthe non-convex objective function given by empirical risk minimization does not\nhave any spurious stationary points. That is, we establish that with high\nprobability, at any point away from small neighborhoods around two scalar\nmultiples of the desired solution, there is a descent direction. Hence, there\nare no local minima, saddle points, or other stationary points outside these\nneighborhoods. These results constitute the first theoretical guarantees which\nestablish the favorable global geometry of these non-convex optimization\nproblems, and they bridge the gap between the empirical success of enforcing\ndeep generative priors and a rigorous understanding of non-linear inverse\nproblems.\n", "versions": [{"version": "v1", "created": "Mon, 22 May 2017 06:40:31 GMT"}, {"version": "v2", "created": "Tue, 13 Feb 2018 08:54:21 GMT"}, {"version": "v3", "created": "Wed, 26 Dec 2018 12:49:16 GMT"}], "update_date": "2018-12-27", "authors_parsed": [["Hand", "Paul", ""], ["Voroninski", "Vladislav", ""]]}, {"id": "1705.07600", "submitter": "Art\\\"ur Manukyan", "authors": "Art\\\"ur Manukyan and Elvan Ceyhan", "title": "Classification Using Proximity Catch Digraphs (Technical Report)", "comments": "41 pages, 16 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We employ random geometric digraphs to construct semi-parametric classifiers.\nThese data-random digraphs are from parametrized random digraph families called\nproximity catch digraphs (PCDs). A related geometric digraph family, class\ncover catch digraph (CCCD), has been used to solve the class cover problem by\nusing its approximate minimum dominating set. CCCDs showed relatively good\nperformance in the classification of imbalanced data sets, and although CCCDs\nhave a convenient construction in $\\mathbb{R}^d$, finding minimum dominating\nsets is NP-hard and its probabilistic behaviour is not mathematically tractable\nexcept for $d=1$. On the other hand, a particular family of PCDs, called\n\\emph{proportional-edge} PCDs (PE-PCDs), has mathematical tractable minimum\ndominating sets in $\\mathbb{R}^d$; however their construction in higher\ndimensions may be computationally demanding. More specifically, we show that\nthe classifiers based on PE-PCDs are prototype-based classifiers such that the\nexact minimum number of prototypes (equivalent to minimum dominating sets) are\nfound in polynomial time on the number of observations. We construct two types\nof classifiers based on PE-PCDs. One is a family of hybrid classifiers depend\non the location of the points of the training data set, and another type is a\nfamily of classifiers solely based on class covers. We assess the\nclassification performance of our PE-PCD based classifiers by extensive Monte\nCarlo simulations, and compare them with that of other commonly used\nclassifiers. We also show that, similar to CCCD classifiers, our classifiers\nare relatively better in classification in the presence of class imbalance.\n", "versions": [{"version": "v1", "created": "Mon, 22 May 2017 08:09:29 GMT"}], "update_date": "2017-05-23", "authors_parsed": [["Manukyan", "Art\u00fcr", ""], ["Ceyhan", "Elvan", ""]]}, {"id": "1705.07603", "submitter": "Vlad Niculae", "authors": "Mathieu Blondel, Vlad Niculae, Takuma Otsuka and Naonori Ueda", "title": "Multi-output Polynomial Networks and Factorization Machines", "comments": "Published at NIPS 2017. 17 pages, including appendix", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Factorization machines and polynomial networks are supervised polynomial\nmodels based on an efficient low-rank decomposition. We extend these models to\nthe multi-output setting, i.e., for learning vector-valued functions, with\napplication to multi-class or multi-task problems. We cast this as the problem\nof learning a 3-way tensor whose slices share a common basis and propose a\nconvex formulation of that problem. We then develop an efficient conditional\ngradient algorithm and prove its global convergence, despite the fact that it\ninvolves a non-convex basis selection step. On classification tasks, we show\nthat our algorithm achieves excellent accuracy with much sparser models than\nexisting methods. On recommendation system tasks, we show how to combine our\nalgorithm with a reduction from ordinal regression to multi-output\nclassification and show that the resulting algorithm outperforms simple\nbaselines in terms of ranking accuracy.\n", "versions": [{"version": "v1", "created": "Mon, 22 May 2017 08:20:31 GMT"}, {"version": "v2", "created": "Sat, 4 Nov 2017 18:28:53 GMT"}], "update_date": "2017-11-07", "authors_parsed": [["Blondel", "Mathieu", ""], ["Niculae", "Vlad", ""], ["Otsuka", "Takuma", ""], ["Ueda", "Naonori", ""]]}, {"id": "1705.07661", "submitter": "Anne Morvan", "authors": "Anne Morvan and Antoine Souloumiac and C\\'edric Gouy-Pailler and Jamal\n  Atif", "title": "Streaming Binary Sketching based on Subspace Tracking and Diagonal\n  Uniformization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we address the problem of learning compact\nsimilarity-preserving embeddings for massive high-dimensional streams of data\nin order to perform efficient similarity search. We present a new online method\nfor computing binary compressed representations -sketches- of high-dimensional\nreal feature vectors. Given an expected code length $c$ and high-dimensional\ninput data points, our algorithm provides a $c$-bits binary code for preserving\nthe distance between the points from the original high-dimensional space. Our\nalgorithm does not require neither the storage of the whole dataset nor a\nchunk, thus it is fully adaptable to the streaming setting. It also provides\nlow time complexity and convergence guarantees. We demonstrate the quality of\nour binary sketches through experiments on real data for the nearest neighbors\nsearch task in the online setting.\n", "versions": [{"version": "v1", "created": "Mon, 22 May 2017 10:54:20 GMT"}, {"version": "v2", "created": "Thu, 8 Feb 2018 20:47:43 GMT"}], "update_date": "2018-02-12", "authors_parsed": [["Morvan", "Anne", ""], ["Souloumiac", "Antoine", ""], ["Gouy-Pailler", "C\u00e9dric", ""], ["Atif", "Jamal", ""]]}, {"id": "1705.07663", "submitter": "Emiliano De Cristofaro", "authors": "Jamie Hayes, Luca Melis, George Danezis, Emiliano De Cristofaro", "title": "LOGAN: Membership Inference Attacks Against Generative Models", "comments": null, "journal-ref": "Proceedings on Privacy Enhancing Technologies (PoPETs), Vol. 2019,\n  Issue 1", "doi": null, "report-no": null, "categories": "cs.CR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generative models estimate the underlying distribution of a dataset to\ngenerate realistic samples according to that distribution. In this paper, we\npresent the first membership inference attacks against generative models: given\na data point, the adversary determines whether or not it was used to train the\nmodel. Our attacks leverage Generative Adversarial Networks (GANs), which\ncombine a discriminative and a generative model, to detect overfitting and\nrecognize inputs that were part of training datasets, using the discriminator's\ncapacity to learn statistical differences in distributions.\n  We present attacks based on both white-box and black-box access to the target\nmodel, against several state-of-the-art generative models, over datasets of\ncomplex representations of faces (LFW), objects (CIFAR-10), and medical images\n(Diabetic Retinopathy). We also discuss the sensitivity of the attacks to\ndifferent training parameters, and their robustness against mitigation\nstrategies, finding that defenses are either ineffective or lead to\nsignificantly worse performances of the generative models in terms of training\nstability and/or sample quality.\n", "versions": [{"version": "v1", "created": "Mon, 22 May 2017 11:05:06 GMT"}, {"version": "v2", "created": "Sat, 12 Aug 2017 10:39:48 GMT"}, {"version": "v3", "created": "Sat, 2 Dec 2017 12:50:23 GMT"}, {"version": "v4", "created": "Tue, 21 Aug 2018 13:24:17 GMT"}], "update_date": "2018-08-22", "authors_parsed": [["Hayes", "Jamie", ""], ["Melis", "Luca", ""], ["Danezis", "George", ""], ["De Cristofaro", "Emiliano", ""]]}, {"id": "1705.07664", "submitter": "Ron Levie", "authors": "Ron Levie, Federico Monti, Xavier Bresson, Michael M. Bronstein", "title": "CayleyNets: Graph Convolutional Neural Networks with Complex Rational\n  Spectral Filters", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The rise of graph-structured data such as social networks, regulatory\nnetworks, citation graphs, and functional brain networks, in combination with\nresounding success of deep learning in various applications, has brought the\ninterest in generalizing deep learning models to non-Euclidean domains. In this\npaper, we introduce a new spectral domain convolutional architecture for deep\nlearning on graphs. The core ingredient of our model is a new class of\nparametric rational complex functions (Cayley polynomials) allowing to\nefficiently compute spectral filters on graphs that specialize on frequency\nbands of interest. Our model generates rich spectral filters that are localized\nin space, scales linearly with the size of the input data for\nsparsely-connected graphs, and can handle different constructions of Laplacian\noperators. Extensive experimental results show the superior performance of our\napproach, in comparison to other spectral domain convolutional architectures,\non spectral image classification, community detection, vertex classification\nand matrix completion tasks.\n", "versions": [{"version": "v1", "created": "Mon, 22 May 2017 11:05:37 GMT"}, {"version": "v2", "created": "Wed, 31 Oct 2018 13:19:40 GMT"}], "update_date": "2018-11-01", "authors_parsed": [["Levie", "Ron", ""], ["Monti", "Federico", ""], ["Bresson", "Xavier", ""], ["Bronstein", "Michael M.", ""]]}, {"id": "1705.07673", "submitter": "Wittawat Jitkrittum", "authors": "Wittawat Jitkrittum, Wenkai Xu, Zoltan Szabo, Kenji Fukumizu, Arthur\n  Gretton", "title": "A Linear-Time Kernel Goodness-of-Fit Test", "comments": "Accepted to NIPS 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel adaptive test of goodness-of-fit, with computational cost\nlinear in the number of samples. We learn the test features that best indicate\nthe differences between observed samples and a reference model, by minimizing\nthe false negative rate. These features are constructed via Stein's method,\nmeaning that it is not necessary to compute the normalising constant of the\nmodel. We analyse the asymptotic Bahadur efficiency of the new test, and prove\nthat under a mean-shift alternative, our test always has greater relative\nefficiency than a previous linear-time kernel test, regardless of the choice of\nparameters for that test. In experiments, the performance of our method exceeds\nthat of the earlier linear-time test, and matches or exceeds the power of a\nquadratic-time kernel test. In high dimensions and where model structure may be\nexploited, our goodness of fit test performs far better than a quadratic-time\ntwo-sample test based on the Maximum Mean Discrepancy, with samples drawn from\nthe model.\n", "versions": [{"version": "v1", "created": "Mon, 22 May 2017 11:26:46 GMT"}, {"version": "v2", "created": "Tue, 24 Oct 2017 12:45:00 GMT"}], "update_date": "2017-10-25", "authors_parsed": [["Jitkrittum", "Wittawat", ""], ["Xu", "Wenkai", ""], ["Szabo", "Zoltan", ""], ["Fukumizu", "Kenji", ""], ["Gretton", "Arthur", ""]]}, {"id": "1705.07674", "submitter": "Ahmed Alaa", "authors": "Ahmed M. Alaa, Jinsung Yoon, Scott Hu, and Mihaela van der Schaar", "title": "Individualized Risk Prognosis for Critical Care Patients: A Multi-task\n  Gaussian Process Model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We report the development and validation of a data-driven real-time risk\nscore that provides timely assessments for the clinical acuity of ward patients\nbased on their temporal lab tests and vital signs, which allows for timely\nintensive care unit (ICU) admissions. Unlike the existing risk scoring\ntechnologies, the proposed score is individualized; it uses the electronic\nhealth record (EHR) data to cluster the patients based on their static\ncovariates into subcohorts of similar patients, and then learns a separate\ntemporal, non-stationary multi-task Gaussian Process (GP) model that captures\nthe physiology of every subcohort. Experiments conducted on data from a\nheterogeneous cohort of 6,094 patients admitted to the Ronald Reagan UCLA\nmedical center show that our risk score significantly outperforms the\nstate-of-the-art risk scoring technologies, such as the Rothman index and MEWS,\nin terms of timeliness, true positive rate (TPR), and positive predictive value\n(PPV). In particular, the proposed score increases the AUC with 20% and 38% as\ncompared to Rothman index and MEWS respectively, and can predict ICU admissions\n8 hours before clinicians at a PPV of 35% and a TPR of 50%. Moreover, we show\nthat the proposed risk score allows for better decisions on when to discharge\nclinically stable patients from the ward, thereby improving the efficiency of\nhospital resource utilization.\n", "versions": [{"version": "v1", "created": "Mon, 22 May 2017 11:27:58 GMT"}], "update_date": "2017-05-23", "authors_parsed": [["Alaa", "Ahmed M.", ""], ["Yoon", "Jinsung", ""], ["Hu", "Scott", ""], ["van der Schaar", "Mihaela", ""]]}, {"id": "1705.07704", "submitter": "Mathieu Blondel", "authors": "Vlad Niculae and Mathieu Blondel", "title": "A Regularized Framework for Sparse and Structured Neural Attention", "comments": "In proceedings of NeurIPS 2017; added errata", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modern neural networks are often augmented with an attention mechanism, which\ntells the network where to focus within the input. We propose in this paper a\nnew framework for sparse and structured attention, building upon a smoothed max\noperator. We show that the gradient of this operator defines a mapping from\nreal values to probabilities, suitable as an attention mechanism. Our framework\nincludes softmax and a slight generalization of the recently-proposed sparsemax\nas special cases. However, we also show how our framework can incorporate\nmodern structured penalties, resulting in more interpretable attention\nmechanisms, that focus on entire segments or groups of an input. We derive\nefficient algorithms to compute the forward and backward passes of our\nattention mechanisms, enabling their use in a neural network trained with\nbackpropagation. To showcase their potential as a drop-in replacement for\nexisting ones, we evaluate our attention mechanisms on three large-scale tasks:\ntextual entailment, machine translation, and sentence summarization. Our\nattention mechanisms improve interpretability without sacrificing performance;\nnotably, on textual entailment and summarization, we outperform the standard\nattention mechanisms based on softmax and sparsemax.\n", "versions": [{"version": "v1", "created": "Mon, 22 May 2017 13:11:24 GMT"}, {"version": "v2", "created": "Fri, 3 Nov 2017 23:58:20 GMT"}, {"version": "v3", "created": "Sat, 23 Feb 2019 01:00:47 GMT"}], "update_date": "2019-02-26", "authors_parsed": [["Niculae", "Vlad", ""], ["Blondel", "Mathieu", ""]]}, {"id": "1705.07706", "submitter": "Armand Vilalta", "authors": "Dario Garcia-Gasulla, Armand Vilalta, Ferran Par\\'es, Jonatan Moreno,\n  Eduard Ayguad\\'e, Jesus Labarta, Ulises Cort\\'es and Toyotaro Suzumura", "title": "An Out-of-the-box Full-network Embedding for Convolutional Neural\n  Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Transfer learning for feature extraction can be used to exploit deep\nrepresentations in contexts where there is very few training data, where there\nare limited computational resources, or when tuning the hyper-parameters needed\nfor training is not an option. While previous contributions to feature\nextraction propose embeddings based on a single layer of the network, in this\npaper we propose a full-network embedding which successfully integrates\nconvolutional and fully connected features, coming from all layers of a deep\nconvolutional neural network. To do so, the embedding normalizes features in\nthe context of the problem, and discretizes their values to reduce noise and\nregularize the embedding space. Significantly, this also reduces the\ncomputational cost of processing the resultant representations. The proposed\nmethod is shown to outperform single layer embeddings on several image\nclassification tasks, while also being more robust to the choice of the\npre-trained model used for obtaining the initial features. The performance gap\nin classification accuracy between thoroughly tuned solutions and the\nfull-network embedding is also reduced, which makes of the proposed approach a\ncompetitive solution for a large set of applications.\n", "versions": [{"version": "v1", "created": "Mon, 22 May 2017 13:14:11 GMT"}], "update_date": "2017-05-23", "authors_parsed": [["Garcia-Gasulla", "Dario", ""], ["Vilalta", "Armand", ""], ["Par\u00e9s", "Ferran", ""], ["Moreno", "Jonatan", ""], ["Ayguad\u00e9", "Eduard", ""], ["Labarta", "Jesus", ""], ["Cort\u00e9s", "Ulises", ""], ["Suzumura", "Toyotaro", ""]]}, {"id": "1705.07750", "submitter": "Joao Carreira", "authors": "Joao Carreira and Andrew Zisserman", "title": "Quo Vadis, Action Recognition? A New Model and the Kinetics Dataset", "comments": "Removed references to mini-kinetics dataset that was never made\n  publicly available and repeated all experiments on the full Kinetics dataset", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The paucity of videos in current action classification datasets (UCF-101 and\nHMDB-51) has made it difficult to identify good video architectures, as most\nmethods obtain similar performance on existing small-scale benchmarks. This\npaper re-evaluates state-of-the-art architectures in light of the new Kinetics\nHuman Action Video dataset. Kinetics has two orders of magnitude more data,\nwith 400 human action classes and over 400 clips per class, and is collected\nfrom realistic, challenging YouTube videos. We provide an analysis on how\ncurrent architectures fare on the task of action classification on this dataset\nand how much performance improves on the smaller benchmark datasets after\npre-training on Kinetics.\n  We also introduce a new Two-Stream Inflated 3D ConvNet (I3D) that is based on\n2D ConvNet inflation: filters and pooling kernels of very deep image\nclassification ConvNets are expanded into 3D, making it possible to learn\nseamless spatio-temporal feature extractors from video while leveraging\nsuccessful ImageNet architecture designs and even their parameters. We show\nthat, after pre-training on Kinetics, I3D models considerably improve upon the\nstate-of-the-art in action classification, reaching 80.9% on HMDB-51 and 98.0%\non UCF-101.\n", "versions": [{"version": "v1", "created": "Mon, 22 May 2017 13:57:53 GMT"}, {"version": "v2", "created": "Thu, 20 Jul 2017 15:24:03 GMT"}, {"version": "v3", "created": "Mon, 12 Feb 2018 17:10:11 GMT"}], "update_date": "2018-02-13", "authors_parsed": [["Carreira", "Joao", ""], ["Zisserman", "Andrew", ""]]}, {"id": "1705.07774", "submitter": "Lukas Balles", "authors": "Lukas Balles and Philipp Hennig", "title": "Dissecting Adam: The Sign, Magnitude and Variance of Stochastic\n  Gradients", "comments": "Presented at the 35th International Conference on Machine Learning\n  (ICML), 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The ADAM optimizer is exceedingly popular in the deep learning community.\nOften it works very well, sometimes it doesn't. Why? We interpret ADAM as a\ncombination of two aspects: for each weight, the update direction is determined\nby the sign of stochastic gradients, whereas the update magnitude is determined\nby an estimate of their relative variance. We disentangle these two aspects and\nanalyze them in isolation, gaining insight into the mechanisms underlying ADAM.\nThis analysis also extends recent results on adverse effects of ADAM on\ngeneralization, isolating the sign aspect as the problematic one. Transferring\nthe variance adaptation to SGD gives rise to a novel method, completing the\npractitioner's toolbox for problems where ADAM fails.\n", "versions": [{"version": "v1", "created": "Mon, 22 May 2017 14:38:16 GMT"}, {"version": "v2", "created": "Mon, 12 Feb 2018 15:43:10 GMT"}, {"version": "v3", "created": "Wed, 20 Jun 2018 15:00:28 GMT"}, {"version": "v4", "created": "Sun, 13 Dec 2020 14:41:08 GMT"}], "update_date": "2020-12-15", "authors_parsed": [["Balles", "Lukas", ""], ["Hennig", "Philipp", ""]]}, {"id": "1705.07795", "submitter": "Francesco Orabona", "authors": "Francesco Orabona and Tatiana Tommasi", "title": "Training Deep Networks without Learning Rates Through Coin Betting", "comments": "Camera-ready version for NIPS 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning methods achieve state-of-the-art performance in many\napplication scenarios. Yet, these methods require a significant amount of\nhyperparameters tuning in order to achieve the best results. In particular,\ntuning the learning rates in the stochastic optimization process is still one\nof the main bottlenecks. In this paper, we propose a new stochastic gradient\ndescent procedure for deep networks that does not require any learning rate\nsetting. Contrary to previous methods, we do not adapt the learning rates nor\nwe make use of the assumed curvature of the objective function. Instead, we\nreduce the optimization process to a game of betting on a coin and propose a\nlearning-rate-free optimal algorithm for this scenario. Theoretical convergence\nis proven for convex and quasi-convex functions and empirical evidence shows\nthe advantage of our algorithm over popular stochastic gradient algorithms.\n", "versions": [{"version": "v1", "created": "Mon, 22 May 2017 15:04:05 GMT"}, {"version": "v2", "created": "Tue, 30 May 2017 17:21:27 GMT"}, {"version": "v3", "created": "Sat, 4 Nov 2017 21:19:04 GMT"}], "update_date": "2017-11-07", "authors_parsed": [["Orabona", "Francesco", ""], ["Tommasi", "Tatiana", ""]]}, {"id": "1705.07798", "submitter": "Gergely Neu", "authors": "Gergely Neu and Anders Jonsson and Vicen\\c{c} G\\'omez", "title": "A unified view of entropy-regularized Markov decision processes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a general framework for entropy-regularized average-reward\nreinforcement learning in Markov decision processes (MDPs). Our approach is\nbased on extending the linear-programming formulation of policy optimization in\nMDPs to accommodate convex regularization functions. Our key result is showing\nthat using the conditional entropy of the joint state-action distributions as\nregularization yields a dual optimization problem closely resembling the\nBellman optimality equations. This result enables us to formalize a number of\nstate-of-the-art entropy-regularized reinforcement learning algorithms as\napproximate variants of Mirror Descent or Dual Averaging, and thus to argue\nabout the convergence properties of these methods. In particular, we show that\nthe exact version of the TRPO algorithm of Schulman et al. (2015) actually\nconverges to the optimal policy, while the entropy-regularized policy gradient\nmethods of Mnih et al. (2016) may fail to converge to a fixed point. Finally,\nwe illustrate empirically the effects of using various regularization\ntechniques on learning performance in a simple reinforcement learning setup.\n", "versions": [{"version": "v1", "created": "Mon, 22 May 2017 15:06:25 GMT"}], "update_date": "2017-05-23", "authors_parsed": [["Neu", "Gergely", ""], ["Jonsson", "Anders", ""], ["G\u00f3mez", "Vicen\u00e7", ""]]}, {"id": "1705.07807", "submitter": "Piotr Mardziel", "authors": "Anupam Datta, Matthew Fredrikson, Gihyuk Ko, Piotr Mardziel, Shayak\n  Sen", "title": "Use Privacy in Data-Driven Systems: Theory and Experiments with Machine\n  Learnt Programs", "comments": "extended CCS 2017 camera-ready: several new discussions, and\n  complexity results added to appendix", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents an approach to formalizing and enforcing a class of use\nprivacy properties in data-driven systems. In contrast to prior work, we focus\non use restrictions on proxies (i.e. strong predictors) of protected\ninformation types. Our definition relates proxy use to intermediate\ncomputations that occur in a program, and identify two essential properties\nthat characterize this behavior: 1) its result is strongly associated with the\nprotected information type in question, and 2) it is likely to causally affect\nthe final output of the program. For a specific instantiation of this\ndefinition, we present a program analysis technique that detects instances of\nproxy use in a model, and provides a witness that identifies which parts of the\ncorresponding program exhibit the behavior. Recognizing that not all instances\nof proxy use of a protected information type are inappropriate, we make use of\na normative judgment oracle that makes this inappropriateness determination for\na given witness. Our repair algorithm uses the witness of an inappropriate\nproxy use to transform the model into one that provably does not exhibit proxy\nuse, while avoiding changes that unduly affect classification accuracy. Using a\ncorpus of social datasets, our evaluation shows that these algorithms are able\nto detect proxy use instances that would be difficult to find using existing\ntechniques, and subsequently remove them while maintaining acceptable\nclassification performance.\n", "versions": [{"version": "v1", "created": "Mon, 22 May 2017 15:28:43 GMT"}, {"version": "v2", "created": "Wed, 24 May 2017 03:46:13 GMT"}, {"version": "v3", "created": "Thu, 7 Sep 2017 06:36:33 GMT"}], "update_date": "2017-09-08", "authors_parsed": [["Datta", "Anupam", ""], ["Fredrikson", "Matthew", ""], ["Ko", "Gihyuk", ""], ["Mardziel", "Piotr", ""], ["Sen", "Shayak", ""]]}, {"id": "1705.07809", "submitter": "Maxim Raginsky", "authors": "Aolin Xu and Maxim Raginsky", "title": "Information-theoretic analysis of generalization capability of learning\n  algorithms", "comments": "Final version, accepted to NIPS 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.IT math.IT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We derive upper bounds on the generalization error of a learning algorithm in\nterms of the mutual information between its input and output. The bounds\nprovide an information-theoretic understanding of generalization in learning\nproblems, and give theoretical guidelines for striking the right balance\nbetween data fit and generalization by controlling the input-output mutual\ninformation. We propose a number of methods for this purpose, among which are\nalgorithms that regularize the ERM algorithm with relative entropy or with\nrandom noise. Our work extends and leads to nontrivial improvements on the\nrecent results of Russo and Zou.\n", "versions": [{"version": "v1", "created": "Mon, 22 May 2017 15:38:22 GMT"}, {"version": "v2", "created": "Mon, 6 Nov 2017 18:58:37 GMT"}], "update_date": "2017-11-07", "authors_parsed": [["Xu", "Aolin", ""], ["Raginsky", "Maxim", ""]]}, {"id": "1705.07815", "submitter": "Jaeho Lee", "authors": "Jaeho Lee and Maxim Raginsky", "title": "Minimax Statistical Learning with Wasserstein Distances", "comments": "published as a conference paper at NIPS 2018, change in title", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As opposed to standard empirical risk minimization (ERM), distributionally\nrobust optimization aims to minimize the worst-case risk over a larger\nambiguity set containing the original empirical distribution of the training\ndata. In this work, we describe a minimax framework for statistical learning\nwith ambiguity sets given by balls in Wasserstein space. In particular, we\nprove generalization bounds that involve the covering number properties of the\noriginal ERM problem. As an illustrative example, we provide generalization\nguarantees for transport-based domain adaptation problems where the Wasserstein\ndistance between the source and target domain distributions can be reliably\nestimated from unlabeled samples.\n", "versions": [{"version": "v1", "created": "Mon, 22 May 2017 15:50:47 GMT"}, {"version": "v2", "created": "Fri, 26 Oct 2018 21:31:53 GMT"}], "update_date": "2021-01-06", "authors_parsed": [["Lee", "Jaeho", ""], ["Raginsky", "Maxim", ""]]}, {"id": "1705.07817", "submitter": "Mingyuan Jiu", "authors": "Mingyuan Jiu, Nelly Pustelnik, Stefan Janaqi, M\\'eriam Chebre, Lin Qi,\n  Philippe Ricoux", "title": "Sparse hierarchical interaction learning with epigraphical projection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work focuses on learning optimization problems with quadratical\ninteractions between variables, which go beyond the additive models of\ntraditional linear learning. We investigate more specifically two different\nmethods encountered in the literature to deal with this problem: \"hierNet\" and\nstructured-sparsity regularization, and study their connections. We propose a\nprimal-dual proximal algorithm based on an epigraphical projection to optimize\na general formulation of these learning problems. The experimental setting\nfirst highlights the improvement of the proposed procedure compared to\nstate-of-the-art methods based on fast iterative shrinkage-thresholding\nalgorithm (i.e. FISTA) or alternating direction method of multipliers (i.e.\nADMM), and then, using the proposed flexible optimization framework, we provide\nfair comparisons between the different hierarchical penalizations and their\nimprovement over the standard $\\ell_1$-norm penalization. The experiments are\nconducted both on synthetic and real data, and they clearly show that the\nproposed primal-dual proximal algorithm based on epigraphical projection is\nefficient and effective to solve and investigate the problem of hierarchical\ninteraction learning.\n", "versions": [{"version": "v1", "created": "Mon, 22 May 2017 15:53:22 GMT"}, {"version": "v2", "created": "Thu, 3 Aug 2017 15:05:53 GMT"}, {"version": "v3", "created": "Mon, 18 Dec 2017 03:52:48 GMT"}, {"version": "v4", "created": "Wed, 28 Aug 2019 03:05:33 GMT"}, {"version": "v5", "created": "Tue, 9 Feb 2021 06:31:31 GMT"}], "update_date": "2021-02-10", "authors_parsed": [["Jiu", "Mingyuan", ""], ["Pustelnik", "Nelly", ""], ["Janaqi", "Stefan", ""], ["Chebre", "M\u00e9riam", ""], ["Qi", "Lin", ""], ["Ricoux", "Philippe", ""]]}, {"id": "1705.07819", "submitter": "Swami Sankaranarayanan", "authors": "Swami Sankaranarayanan, Arpit Jain, Rama Chellappa and Ser Nam Lim", "title": "Regularizing deep networks using efficient layerwise adversarial\n  training", "comments": "Published at the Thirty-Second AAAI Conference on Artificial\n  Intelligence (AAAI-18). Official link:\n  https://www.aaai.org/ocs/index.php/AAAI/AAAI18/paper/view/16634", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Adversarial training has been shown to regularize deep neural networks in\naddition to increasing their robustness to adversarial examples. However, its\nimpact on very deep state of the art networks has not been fully investigated.\nIn this paper, we present an efficient approach to perform adversarial training\nby perturbing intermediate layer activations and study the use of such\nperturbations as a regularizer during training. We use these perturbations to\ntrain very deep models such as ResNets and show improvement in performance both\non adversarial and original test data. Our experiments highlight the benefits\nof perturbing intermediate layer activations compared to perturbing only the\ninputs. The results on CIFAR-10 and CIFAR-100 datasets show the merits of the\nproposed adversarial training approach. Additional results on WideResNets show\nthat our approach provides significant improvement in classification accuracy\nfor a given base model, outperforming dropout and other base models of larger\nsize.\n", "versions": [{"version": "v1", "created": "Mon, 22 May 2017 15:55:42 GMT"}, {"version": "v2", "created": "Tue, 29 May 2018 02:27:51 GMT"}], "update_date": "2018-05-30", "authors_parsed": [["Sankaranarayanan", "Swami", ""], ["Jain", "Arpit", ""], ["Chellappa", "Rama", ""], ["Lim", "Ser Nam", ""]]}, {"id": "1705.07831", "submitter": "Ayan Chakrabarti", "authors": "Behnam Neyshabur, Srinadh Bhojanapalli, Ayan Chakrabarti", "title": "Stabilizing GAN Training with Multiple Random Projections", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Training generative adversarial networks is unstable in high-dimensions as\nthe true data distribution tends to be concentrated in a small fraction of the\nambient space. The discriminator is then quickly able to classify nearly all\ngenerated samples as fake, leaving the generator without meaningful gradients\nand causing it to deteriorate after a point in training. In this work, we\npropose training a single generator simultaneously against an array of\ndiscriminators, each of which looks at a different random low-dimensional\nprojection of the data. Individual discriminators, now provided with restricted\nviews of the input, are unable to reject generated samples perfectly and\ncontinue to provide meaningful gradients to the generator throughout training.\nMeanwhile, the generator learns to produce samples consistent with the full\ndata distribution to satisfy all discriminators simultaneously. We demonstrate\nthe practical utility of this approach experimentally, and show that it is able\nto produce image samples with higher quality than traditional training with a\nsingle discriminator.\n", "versions": [{"version": "v1", "created": "Mon, 22 May 2017 16:23:26 GMT"}, {"version": "v2", "created": "Sat, 23 Jun 2018 00:53:46 GMT"}], "update_date": "2018-06-26", "authors_parsed": [["Neyshabur", "Behnam", ""], ["Bhojanapalli", "Srinadh", ""], ["Chakrabarti", "Ayan", ""]]}, {"id": "1705.07853", "submitter": "Ilja Kuzborskij", "authors": "Ilja Kuzborskij, Nicol\\`o Cesa-Bianchi", "title": "Nonparametric Online Regression while Learning the Metric", "comments": "To appear in NIPS 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study algorithms for online nonparametric regression that learn the\ndirections along which the regression function is smoother. Our algorithm\nlearns the Mahalanobis metric based on the gradient outer product matrix\n$\\boldsymbol{G}$ of the regression function (automatically adapting to the\neffective rank of this matrix), while simultaneously bounding the regret ---on\nthe same data sequence--- in terms of the spectrum of $\\boldsymbol{G}$. As a\npreliminary step in our analysis, we extend a nonparametric online learning\nalgorithm by Hazan and Megiddo enabling it to compete against functions whose\nLipschitzness is measured with respect to an arbitrary Mahalanobis metric.\n", "versions": [{"version": "v1", "created": "Mon, 22 May 2017 16:58:13 GMT"}, {"version": "v2", "created": "Mon, 23 Oct 2017 15:51:33 GMT"}], "update_date": "2017-10-24", "authors_parsed": [["Kuzborskij", "Ilja", ""], ["Cesa-Bianchi", "Nicol\u00f2", ""]]}, {"id": "1705.07860", "submitter": "Graham Neubig", "authors": "Graham Neubig and Yoav Goldberg and Chris Dyer", "title": "On-the-fly Operation Batching in Dynamic Computation Graphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Dynamic neural network toolkits such as PyTorch, DyNet, and Chainer offer\nmore flexibility for implementing models that cope with data of varying\ndimensions and structure, relative to toolkits that operate on statically\ndeclared computations (e.g., TensorFlow, CNTK, and Theano). However, existing\ntoolkits - both static and dynamic - require that the developer organize the\ncomputations into the batches necessary for exploiting high-performance\nalgorithms and hardware. This batching task is generally difficult, but it\nbecomes a major hurdle as architectures become complex. In this paper, we\npresent an algorithm, and its implementation in the DyNet toolkit, for\nautomatically batching operations. Developers simply write minibatch\ncomputations as aggregations of single instance computations, and the batching\nalgorithm seamlessly executes them, on the fly, using computationally efficient\nbatched operations. On a variety of tasks, we obtain throughput similar to that\nobtained with manual batches, as well as comparable speedups over\nsingle-instance learning on architectures that are impractical to batch\nmanually.\n", "versions": [{"version": "v1", "created": "Mon, 22 May 2017 17:04:56 GMT"}], "update_date": "2017-05-23", "authors_parsed": [["Neubig", "Graham", ""], ["Goldberg", "Yoav", ""], ["Dyer", "Chris", ""]]}, {"id": "1705.07867", "submitter": "Miltiadis Allamanis", "authors": "Miltiadis Allamanis and Marc Brockschmidt", "title": "SmartPaste: Learning to Adapt Source Code", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep Neural Networks have been shown to succeed at a range of natural\nlanguage tasks such as machine translation and text summarization. While tasks\non source code (ie, formal languages) have been considered recently, most work\nin this area does not attempt to capitalize on the unique opportunities offered\nby its known syntax and structure. In this work, we introduce SmartPaste, a\nfirst task that requires to use such information. The task is a variant of the\nprogram repair problem that requires to adapt a given (pasted) snippet of code\nto surrounding, existing source code. As first solutions, we design a set of\ndeep neural models that learn to represent the context of each variable\nlocation and variable usage in a data flow-sensitive way. Our evaluation\nsuggests that our models can learn to solve the SmartPaste task in many cases,\nachieving 58.6% accuracy, while learning meaningful representation of variable\nusages.\n", "versions": [{"version": "v1", "created": "Mon, 22 May 2017 17:16:06 GMT"}], "update_date": "2017-05-23", "authors_parsed": [["Allamanis", "Miltiadis", ""], ["Brockschmidt", "Marc", ""]]}, {"id": "1705.07874", "submitter": "Scott Lundberg", "authors": "Scott Lundberg and Su-In Lee", "title": "A Unified Approach to Interpreting Model Predictions", "comments": "To appear in NIPS 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Understanding why a model makes a certain prediction can be as crucial as the\nprediction's accuracy in many applications. However, the highest accuracy for\nlarge modern datasets is often achieved by complex models that even experts\nstruggle to interpret, such as ensemble or deep learning models, creating a\ntension between accuracy and interpretability. In response, various methods\nhave recently been proposed to help users interpret the predictions of complex\nmodels, but it is often unclear how these methods are related and when one\nmethod is preferable over another. To address this problem, we present a\nunified framework for interpreting predictions, SHAP (SHapley Additive\nexPlanations). SHAP assigns each feature an importance value for a particular\nprediction. Its novel components include: (1) the identification of a new class\nof additive feature importance measures, and (2) theoretical results showing\nthere is a unique solution in this class with a set of desirable properties.\nThe new class unifies six existing methods, notable because several recent\nmethods in the class lack the proposed desirable properties. Based on insights\nfrom this unification, we present new methods that show improved computational\nperformance and/or better consistency with human intuition than previous\napproaches.\n", "versions": [{"version": "v1", "created": "Mon, 22 May 2017 17:38:10 GMT"}, {"version": "v2", "created": "Sat, 25 Nov 2017 03:53:32 GMT"}], "update_date": "2017-11-28", "authors_parsed": [["Lundberg", "Scott", ""], ["Lee", "Su-In", ""]]}, {"id": "1705.07878", "submitter": "Wei Wen", "authors": "Wei Wen, Cong Xu, Feng Yan, Chunpeng Wu, Yandan Wang, Yiran Chen, Hai\n  Li", "title": "TernGrad: Ternary Gradients to Reduce Communication in Distributed Deep\n  Learning", "comments": "NIPS 2017 Oral", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DC cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  High network communication cost for synchronizing gradients and parameters is\nthe well-known bottleneck of distributed training. In this work, we propose\nTernGrad that uses ternary gradients to accelerate distributed deep learning in\ndata parallelism. Our approach requires only three numerical levels {-1,0,1},\nwhich can aggressively reduce the communication time. We mathematically prove\nthe convergence of TernGrad under the assumption of a bound on gradients.\nGuided by the bound, we propose layer-wise ternarizing and gradient clipping to\nimprove its convergence. Our experiments show that applying TernGrad on AlexNet\ndoes not incur any accuracy loss and can even improve accuracy. The accuracy\nloss of GoogLeNet induced by TernGrad is less than 2% on average. Finally, a\nperformance model is proposed to study the scalability of TernGrad. Experiments\nshow significant speed gains for various deep neural networks. Our source code\nis available.\n", "versions": [{"version": "v1", "created": "Mon, 22 May 2017 17:42:15 GMT"}, {"version": "v2", "created": "Wed, 24 May 2017 06:41:05 GMT"}, {"version": "v3", "created": "Mon, 4 Sep 2017 23:49:08 GMT"}, {"version": "v4", "created": "Mon, 18 Sep 2017 16:21:51 GMT"}, {"version": "v5", "created": "Tue, 31 Oct 2017 16:36:41 GMT"}, {"version": "v6", "created": "Fri, 29 Dec 2017 02:51:48 GMT"}], "update_date": "2018-01-01", "authors_parsed": [["Wen", "Wei", ""], ["Xu", "Cong", ""], ["Yan", "Feng", ""], ["Wu", "Chunpeng", ""], ["Wang", "Yandan", ""], ["Chen", "Yiran", ""], ["Li", "Hai", ""]]}, {"id": "1705.07881", "submitter": "Lin Yang", "authors": "Lin F. Yang, Vladimir Braverman, Tuo Zhao, Mengdi Wang", "title": "Online Factorization and Partition of Complex Networks From Random Walks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Finding the reduced-dimensional structure is critical to understanding\ncomplex networks. Existing approaches such as spectral clustering are\napplicable only when the full network is explicitly observed. In this paper, we\nfocus on the online factorization and partition of implicit large-scale\nnetworks based on observations from an associated random walk. We formulate\nthis into a nonconvex stochastic factorization problem and propose an efficient\nand scalable stochastic generalized Hebbian algorithm. The algorithm is able to\nprocess dependent state-transition data dynamically generated by the underlying\nnetwork and learn a low-dimensional representation for each vertex. By applying\na diffusion approximation analysis, we show that the continuous-time limiting\nprocess of the stochastic algorithm converges globally to the \"principal\ncomponents\" of the Markov chain and achieves a nearly optimal sample\ncomplexity. Once given the learned low-dimensional representations, we further\napply clustering techniques to recover the network partition. We show that when\nthe associated Markov process is lumpable, one can recover the partition\nexactly with high probability. We apply the proposed approach to model the\ntraffic flow of Manhattan as city-wide random walks. By using our algorithm to\nanalyze the taxi trip data, we discover a latent partition of the Manhattan\ncity that closely matches the traffic dynamics.\n", "versions": [{"version": "v1", "created": "Mon, 22 May 2017 17:51:33 GMT"}, {"version": "v2", "created": "Tue, 23 May 2017 16:23:29 GMT"}, {"version": "v3", "created": "Sun, 19 Nov 2017 21:29:08 GMT"}, {"version": "v4", "created": "Mon, 11 Dec 2017 20:47:50 GMT"}], "update_date": "2017-12-13", "authors_parsed": [["Yang", "Lin F.", ""], ["Braverman", "Vladimir", ""], ["Zhao", "Tuo", ""], ["Wang", "Mengdi", ""]]}, {"id": "1705.07904", "submitter": "Chris Donahue", "authors": "Chris Donahue, Zachary C. Lipton, Akshay Balsubramani, Julian McAuley", "title": "Semantically Decomposing the Latent Spaces of Generative Adversarial\n  Networks", "comments": "Published as a conference paper at ICLR 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new algorithm for training generative adversarial networks that\njointly learns latent codes for both identities (e.g. individual humans) and\nobservations (e.g. specific photographs). By fixing the identity portion of the\nlatent codes, we can generate diverse images of the same subject, and by fixing\nthe observation portion, we can traverse the manifold of subjects while\nmaintaining contingent aspects such as lighting and pose. Our algorithm\nfeatures a pairwise training scheme in which each sample from the generator\nconsists of two images with a common identity code. Corresponding samples from\nthe real dataset consist of two distinct photographs of the same subject. In\norder to fool the discriminator, the generator must produce pairs that are\nphotorealistic, distinct, and appear to depict the same individual. We augment\nboth the DCGAN and BEGAN approaches with Siamese discriminators to facilitate\npairwise training. Experiments with human judges and an off-the-shelf face\nverification system demonstrate our algorithm's ability to generate convincing,\nidentity-matched photographs.\n", "versions": [{"version": "v1", "created": "Mon, 22 May 2017 18:00:02 GMT"}, {"version": "v2", "created": "Tue, 31 Oct 2017 18:00:05 GMT"}, {"version": "v3", "created": "Thu, 22 Feb 2018 19:36:33 GMT"}], "update_date": "2018-02-26", "authors_parsed": [["Donahue", "Chris", ""], ["Lipton", "Zachary C.", ""], ["Balsubramani", "Akshay", ""], ["McAuley", "Julian", ""]]}, {"id": "1705.07957", "submitter": "Mark Eisen", "authors": "Mark Eisen, Aryan Mokhtari, Alejandro Ribeiro", "title": "Large Scale Empirical Risk Minimization via Truncated Adaptive Newton\n  Method", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider large scale empirical risk minimization (ERM) problems, where\nboth the problem dimension and variable size is large. In these cases, most\nsecond order methods are infeasible due to the high cost in both computing the\nHessian over all samples and computing its inverse in high dimensions. In this\npaper, we propose a novel adaptive sample size second-order method, which\nreduces the cost of computing the Hessian by solving a sequence of ERM problems\ncorresponding to a subset of samples and lowers the cost of computing the\nHessian inverse using a truncated eigenvalue decomposition. We show that while\nwe geometrically increase the size of the training set at each stage, a single\niteration of the truncated Newton method is sufficient to solve the new ERM\nwithin its statistical accuracy. Moreover, for a large number of samples we are\nallowed to double the size of the training set at each stage, and the proposed\nmethod subsequently reaches the statistical accuracy of the full training set\napproximately after two effective passes. In addition to this theoretical\nresult, we show empirically on a number of well known data sets that the\nproposed truncated adaptive sample size algorithm outperforms stochastic\nalternatives for solving ERM problems.\n", "versions": [{"version": "v1", "created": "Mon, 22 May 2017 19:23:02 GMT"}], "update_date": "2017-05-24", "authors_parsed": [["Eisen", "Mark", ""], ["Mokhtari", "Aryan", ""], ["Ribeiro", "Alejandro", ""]]}, {"id": "1705.07962", "submitter": "Tony Beltramelli", "authors": "Tony Beltramelli", "title": "pix2code: Generating Code from a Graphical User Interface Screenshot", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CL cs.CV cs.NE", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Transforming a graphical user interface screenshot created by a designer into\ncomputer code is a typical task conducted by a developer in order to build\ncustomized software, websites, and mobile applications. In this paper, we show\nthat deep learning methods can be leveraged to train a model end-to-end to\nautomatically generate code from a single input image with over 77% of accuracy\nfor three different platforms (i.e. iOS, Android and web-based technologies).\n", "versions": [{"version": "v1", "created": "Mon, 22 May 2017 19:32:20 GMT"}, {"version": "v2", "created": "Tue, 19 Sep 2017 11:27:47 GMT"}], "update_date": "2017-09-20", "authors_parsed": [["Beltramelli", "Tony", ""]]}, {"id": "1705.08011", "submitter": "Yintai Ma", "authors": "Yintai Ma and Diego Klabjan", "title": "Diminishing Batch Normalization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a generalization of the Batch Normalization (BN)\nalgorithm, diminishing batch normalization (DBN), where we update the BN\nparameters in a diminishing moving average way. BN is very effective in\naccelerating the convergence of a neural network training phase that it has\nbecome a common practice. Our proposed DBN algorithm remains the overall\nstructure of the original BN algorithm while introduces a weighted averaging\nupdate to some trainable parameters. We provide an analysis of the convergence\nof the DBN algorithm that converges to a stationary point with respect to\ntrainable parameters. Our analysis can be easily generalized for original BN\nalgorithm by setting some parameters to constant. To the best knowledge of\nauthors, this analysis is the first of its kind for convergence with Batch\nNormalization introduced. We analyze a two-layer model with arbitrary\nactivation function. The primary challenge of the analysis is the fact that\nsome parameters are updated by gradient while others are not. The convergence\nanalysis applies to any activation function that satisfies our common\nassumptions. In the numerical experiments, we test the proposed algorithm on\ncomplex modern CNN models with stochastic gradients and ReLU activation. We\nobserve that DBN outperforms the original BN algorithm on MNIST, NI and\nCIFAR-10 datasets with reasonable complex FNN and CNN models.\n", "versions": [{"version": "v1", "created": "Mon, 22 May 2017 21:31:10 GMT"}, {"version": "v2", "created": "Tue, 19 Feb 2019 17:19:55 GMT"}], "update_date": "2019-02-20", "authors_parsed": [["Ma", "Yintai", ""], ["Klabjan", "Diego", ""]]}, {"id": "1705.08014", "submitter": "Tayfun Gokmen", "authors": "Tayfun Gokmen, O. Murat Onen, Wilfried Haensch", "title": "Training Deep Convolutional Neural Networks with Resistive Cross-Point\n  Devices", "comments": "22 pages, 6 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In a previous work we have detailed the requirements to obtain a maximal\nperformance benefit by implementing fully connected deep neural networks (DNN)\nin form of arrays of resistive devices for deep learning. This concept of\nResistive Processing Unit (RPU) devices we extend here towards convolutional\nneural networks (CNNs). We show how to map the convolutional layers to RPU\narrays such that the parallelism of the hardware can be fully utilized in all\nthree cycles of the backpropagation algorithm. We find that the noise and bound\nlimitations imposed due to analog nature of the computations performed on the\narrays effect the training accuracy of the CNNs. Noise and bound management\ntechniques are presented that mitigate these problems without introducing any\nadditional complexity in the analog circuits and can be addressed by the\ndigital circuits. In addition, we discuss digitally programmable update\nmanagement and device variability reduction techniques that can be used\nselectively for some of the layers in a CNN. We show that combination of all\nthose techniques enables a successful application of the RPU concept for\ntraining CNNs. The techniques discussed here are more general and can be\napplied beyond CNN architectures and therefore enables applicability of RPU\napproach for large class of neural network architectures.\n", "versions": [{"version": "v1", "created": "Mon, 22 May 2017 21:40:56 GMT"}], "update_date": "2017-05-24", "authors_parsed": [["Gokmen", "Tayfun", ""], ["Onen", "O. Murat", ""], ["Haensch", "Wilfried", ""]]}, {"id": "1705.08030", "submitter": "Saeed Maleki", "authors": "Saeed Maleki, Madanlal Musuvathi, Todd Mytkowicz", "title": "Parallel Stochastic Gradient Descent with Sound Combiners", "comments": "16 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Stochastic gradient descent (SGD) is a well known method for regression and\nclassification tasks. However, it is an inherently sequential algorithm at each\nstep, the processing of the current example depends on the parameters learned\nfrom the previous examples. Prior approaches to parallelizing linear learners\nusing SGD, such as HOGWILD! and ALLREDUCE, do not honor these dependencies\nacross threads and thus can potentially suffer poor convergence rates and/or\npoor scalability. This paper proposes SYMSGD, a parallel SGD algorithm that, to\na first-order approximation, retains the sequential semantics of SGD. Each\nthread learns a local model in addition to a model combiner, which allows local\nmodels to be combined to produce the same result as what a sequential SGD would\nhave produced. This paper evaluates SYMSGD's accuracy and performance on 6\ndatasets on a shared-memory machine shows upto 11x speedup over our heavily\noptimized sequential baseline on 16 cores and 2.2x, on average, faster than\nHOGWILD!.\n", "versions": [{"version": "v1", "created": "Mon, 22 May 2017 22:32:28 GMT"}], "update_date": "2017-05-24", "authors_parsed": [["Maleki", "Saeed", ""], ["Musuvathi", "Madanlal", ""], ["Mytkowicz", "Todd", ""]]}, {"id": "1705.08039", "submitter": "Maximilian Nickel", "authors": "Maximilian Nickel, Douwe Kiela", "title": "Poincar\\'e Embeddings for Learning Hierarchical Representations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Representation learning has become an invaluable approach for learning from\nsymbolic data such as text and graphs. However, while complex symbolic datasets\noften exhibit a latent hierarchical structure, state-of-the-art methods\ntypically learn embeddings in Euclidean vector spaces, which do not account for\nthis property. For this purpose, we introduce a new approach for learning\nhierarchical representations of symbolic data by embedding them into hyperbolic\nspace -- or more precisely into an n-dimensional Poincar\\'e ball. Due to the\nunderlying hyperbolic geometry, this allows us to learn parsimonious\nrepresentations of symbolic data by simultaneously capturing hierarchy and\nsimilarity. We introduce an efficient algorithm to learn the embeddings based\non Riemannian optimization and show experimentally that Poincar\\'e embeddings\noutperform Euclidean embeddings significantly on data with latent hierarchies,\nboth in terms of representation capacity and in terms of generalization\nability.\n", "versions": [{"version": "v1", "created": "Mon, 22 May 2017 23:14:36 GMT"}, {"version": "v2", "created": "Fri, 26 May 2017 17:40:55 GMT"}], "update_date": "2017-05-29", "authors_parsed": [["Nickel", "Maximilian", ""], ["Kiela", "Douwe", ""]]}, {"id": "1705.08044", "submitter": "Nariman Farsad", "authors": "Nariman Farsad and Andrea Goldsmith", "title": "Detection Algorithms for Communication Systems Using Deep Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.ET", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The design and analysis of communication systems typically rely on the\ndevelopment of mathematical models that describe the underlying communication\nchannel, which dictates the relationship between the transmitted and the\nreceived signals. However, in some systems, such as molecular communication\nsystems where chemical signals are used for transfer of information, it is not\npossible to accurately model this relationship. In these scenarios, because of\nthe lack of mathematical channel models, a completely new approach to design\nand analysis is required. In this work, we focus on one important aspect of\ncommunication systems, the detection algorithms, and demonstrate that by\nborrowing tools from deep learning, it is possible to train detectors that\nperform well, without any knowledge of the underlying channel models. We\nevaluate these algorithms using experimental data that is collected by a\nchemical communication platform, where the channel model is unknown and\ndifficult to model analytically. We show that deep learning algorithms perform\nsignificantly better than a simple detector that was used in previous works,\nwhich also did not assume any knowledge of the channel.\n", "versions": [{"version": "v1", "created": "Mon, 22 May 2017 23:47:47 GMT"}, {"version": "v2", "created": "Mon, 31 Jul 2017 02:43:27 GMT"}], "update_date": "2017-08-01", "authors_parsed": [["Farsad", "Nariman", ""], ["Goldsmith", "Andrea", ""]]}, {"id": "1705.08049", "submitter": "Steven Chen", "authors": "Steven W Chen, Nikolay Atanasov, Arbaaz Khan, Konstantinos Karydis,\n  Daniel D. Lee, and Vijay Kumar", "title": "Neural Network Memory Architectures for Autonomous Robot Navigation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper highlights the significance of including memory structures in\nneural networks when the latter are used to learn perception-action loops for\nautonomous robot navigation. Traditional navigation approaches rely on global\nmaps of the environment to overcome cul-de-sacs and plan feasible motions. Yet,\nmaintaining an accurate global map may be challenging in real-world settings. A\npossible way to mitigate this limitation is to use learning techniques that\nforgo hand-engineered map representations and infer appropriate control\nresponses directly from sensed information. An important but unexplored aspect\nof such approaches is the effect of memory on their performance. This work is a\nfirst thorough study of memory structures for deep-neural-network-based robot\nnavigation, and offers novel tools to train such networks from supervision and\nquantify their ability to generalize to unseen scenarios. We analyze the\nseparation and generalization abilities of feedforward, long short-term memory,\nand differentiable neural computer networks. We introduce a new method to\nevaluate the generalization ability by estimating the VC-dimension of networks\nwith a final linear readout layer. We validate that the VC estimates are good\npredictors of actual test performance. The reported method can be applied to\ndeep learning problems beyond robotics.\n", "versions": [{"version": "v1", "created": "Tue, 23 May 2017 00:58:20 GMT"}], "update_date": "2017-05-24", "authors_parsed": [["Chen", "Steven W", ""], ["Atanasov", "Nikolay", ""], ["Khan", "Arbaaz", ""], ["Karydis", "Konstantinos", ""], ["Lee", "Daniel D.", ""], ["Kumar", "Vijay", ""]]}, {"id": "1705.08051", "submitter": "Shuai Xiao", "authors": "Shuai Xiao, Mehrdad Farajtabar, Xiaojing Ye, Junchi Yan, Le Song,\n  Hongyuan Zha", "title": "Wasserstein Learning of Deep Generative Point Process Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Point processes are becoming very popular in modeling asynchronous sequential\ndata due to their sound mathematical foundation and strength in modeling a\nvariety of real-world phenomena. Currently, they are often characterized via\nintensity function which limits model's expressiveness due to unrealistic\nassumptions on its parametric form used in practice. Furthermore, they are\nlearned via maximum likelihood approach which is prone to failure in\nmulti-modal distributions of sequences. In this paper, we propose an\nintensity-free approach for point processes modeling that transforms nuisance\nprocesses to a target one. Furthermore, we train the model using a\nlikelihood-free leveraging Wasserstein distance between point processes.\nExperiments on various synthetic and real-world data substantiate the\nsuperiority of the proposed point process model over conventional ones.\n", "versions": [{"version": "v1", "created": "Tue, 23 May 2017 01:08:38 GMT"}], "update_date": "2017-05-24", "authors_parsed": [["Xiao", "Shuai", ""], ["Farajtabar", "Mehrdad", ""], ["Ye", "Xiaojing", ""], ["Yan", "Junchi", ""], ["Song", "Le", ""], ["Zha", "Hongyuan", ""]]}, {"id": "1705.08052", "submitter": "Andros Tjandra", "authors": "Andros Tjandra, Sakriani Sakti, Satoshi Nakamura", "title": "Compressing Recurrent Neural Network with Tensor Train", "comments": "Accepted at IJCNN 2017", "journal-ref": null, "doi": "10.1109/IJCNN.2017.7966420", "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recurrent Neural Network (RNN) are a popular choice for modeling temporal and\nsequential tasks and achieve many state-of-the-art performance on various\ncomplex problems. However, most of the state-of-the-art RNNs have millions of\nparameters and require many computational resources for training and predicting\nnew data. This paper proposes an alternative RNN model to reduce the number of\nparameters significantly by representing the weight parameters based on Tensor\nTrain (TT) format. In this paper, we implement the TT-format representation for\nseveral RNN architectures such as simple RNN and Gated Recurrent Unit (GRU). We\ncompare and evaluate our proposed RNN model with uncompressed RNN model on\nsequence classification and sequence prediction tasks. Our proposed RNNs with\nTT-format are able to preserve the performance while reducing the number of RNN\nparameters significantly up to 40 times smaller.\n", "versions": [{"version": "v1", "created": "Tue, 23 May 2017 01:14:22 GMT"}], "update_date": "2017-10-31", "authors_parsed": [["Tjandra", "Andros", ""], ["Sakti", "Sakriani", ""], ["Nakamura", "Satoshi", ""]]}, {"id": "1705.08056", "submitter": "Nan Yang", "authors": "Xin Guo, Johnny Hong, Nan Yang", "title": "Ambiguity set and learning via Bregman and Wasserstein", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Construction of ambiguity set in robust optimization relies on the choice of\ndivergences between probability distributions. In distribution learning,\nchoosing appropriate probability distributions based on observed data is\ncritical for approximating the true distribution. To improve the performance of\nmachine learning models, there has recently been interest in designing\nobjective functions based on Lp-Wasserstein distance rather than the classical\nKullback-Leibler (KL) divergence. In this paper, we derive concentration and\nasymptotic results using Bregman divergence. We propose a novel asymmetric\nstatistical divergence called Wasserstein-Bregman divergence as a\ngeneralization of L2-Wasserstein distance. We discuss how these results can be\napplied to the construction of ambiguity set in robust optimization.\n", "versions": [{"version": "v1", "created": "Tue, 23 May 2017 01:58:02 GMT"}], "update_date": "2017-05-24", "authors_parsed": [["Guo", "Xin", ""], ["Hong", "Johnny", ""], ["Yang", "Nan", ""]]}, {"id": "1705.08076", "submitter": "Michael Luby", "authors": "Sanjoy Dasgupta and Michael Luby", "title": "Learning from partial correction", "comments": "13 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a new model of interactive learning in which an expert examines\nthe predictions of a learner and partially fixes them if they are wrong.\nAlthough this kind of feedback is not i.i.d., we show statistical\ngeneralization bounds on the quality of the learned model.\n", "versions": [{"version": "v1", "created": "Tue, 23 May 2017 05:07:52 GMT"}, {"version": "v2", "created": "Wed, 24 May 2017 16:39:52 GMT"}, {"version": "v3", "created": "Thu, 25 May 2017 16:50:52 GMT"}, {"version": "v4", "created": "Mon, 9 Apr 2018 20:33:39 GMT"}], "update_date": "2018-04-11", "authors_parsed": [["Dasgupta", "Sanjoy", ""], ["Luby", "Michael", ""]]}, {"id": "1705.08080", "submitter": "Yuke Zhu", "authors": "Yuke Zhu, Daniel Gordon, Eric Kolve, Dieter Fox, Li Fei-Fei, Abhinav\n  Gupta, Roozbeh Mottaghi, Ali Farhadi", "title": "Visual Semantic Planning using Deep Successor Representations", "comments": "ICCV 2017 camera ready", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A crucial capability of real-world intelligent agents is their ability to\nplan a sequence of actions to achieve their goals in the visual world. In this\nwork, we address the problem of visual semantic planning: the task of\npredicting a sequence of actions from visual observations that transform a\ndynamic environment from an initial state to a goal state. Doing so entails\nknowledge about objects and their affordances, as well as actions and their\npreconditions and effects. We propose learning these through interacting with a\nvisual and dynamic environment. Our proposed solution involves bootstrapping\nreinforcement learning with imitation learning. To ensure cross task\ngeneralization, we develop a deep predictive model based on successor\nrepresentations. Our experimental results show near optimal results across a\nwide range of tasks in the challenging THOR environment.\n", "versions": [{"version": "v1", "created": "Tue, 23 May 2017 05:22:47 GMT"}, {"version": "v2", "created": "Tue, 15 Aug 2017 21:13:49 GMT"}], "update_date": "2017-08-17", "authors_parsed": [["Zhu", "Yuke", ""], ["Gordon", "Daniel", ""], ["Kolve", "Eric", ""], ["Fox", "Dieter", ""], ["Fei-Fei", "Li", ""], ["Gupta", "Abhinav", ""], ["Mottaghi", "Roozbeh", ""], ["Farhadi", "Ali", ""]]}, {"id": "1705.08110", "submitter": "Karthik Abinav Sankararaman", "authors": "Karthik Abinav Sankararaman, Aleksandrs Slivkins", "title": "Combinatorial Semi-Bandits with Knapsacks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We unify two prominent lines of work on multi-armed bandits: bandits with\nknapsacks (BwK) and combinatorial semi-bandits. The former concerns limited\n\"resources\" consumed by the algorithm, e.g., limited supply in dynamic pricing.\nThe latter allows a huge number of actions but assumes combinatorial structure\nand additional feedback to make the problem tractable. We define a common\ngeneralization, support it with several motivating examples, and design an\nalgorithm for it. Our regret bounds are comparable with those for BwK and\ncombinatorial semi- bandits.\n", "versions": [{"version": "v1", "created": "Tue, 23 May 2017 07:46:32 GMT"}, {"version": "v2", "created": "Mon, 16 Oct 2017 01:53:46 GMT"}, {"version": "v3", "created": "Tue, 20 Feb 2018 23:29:41 GMT"}], "update_date": "2018-02-22", "authors_parsed": [["Sankararaman", "Karthik Abinav", ""], ["Slivkins", "Aleksandrs", ""]]}, {"id": "1705.08118", "submitter": "Carlo Ciliberto", "authors": "Carlo Ciliberto, Alessandro Rudi, Lorenzo Rosasco, Massimiliano Pontil", "title": "Consistent Multitask Learning with Nonlinear Output Relations", "comments": "25 pages, 1 figure, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Key to multitask learning is exploiting relationships between different tasks\nto improve prediction performance. If the relations are linear, regularization\napproaches can be used successfully. However, in practice assuming the tasks to\nbe linearly related might be restrictive, and allowing for nonlinear structures\nis a challenge. In this paper, we tackle this issue by casting the problem\nwithin the framework of structured prediction. Our main contribution is a novel\nalgorithm for learning multiple tasks which are related by a system of\nnonlinear equations that their joint outputs need to satisfy. We show that the\nalgorithm is consistent and can be efficiently implemented. Experimental\nresults show the potential of the proposed method.\n", "versions": [{"version": "v1", "created": "Tue, 23 May 2017 08:24:06 GMT"}, {"version": "v2", "created": "Thu, 10 Aug 2017 13:47:31 GMT"}], "update_date": "2017-08-11", "authors_parsed": [["Ciliberto", "Carlo", ""], ["Rudi", "Alessandro", ""], ["Rosasco", "Lorenzo", ""], ["Pontil", "Massimiliano", ""]]}, {"id": "1705.08131", "submitter": "Weiwei Hu", "authors": "Weiwei Hu and Ying Tan", "title": "Black-Box Attacks against RNN based Malware Detection Algorithms", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent researches have shown that machine learning based malware detection\nalgorithms are very vulnerable under the attacks of adversarial examples. These\nworks mainly focused on the detection algorithms which use features with fixed\ndimension, while some researchers have begun to use recurrent neural networks\n(RNN) to detect malware based on sequential API features. This paper proposes a\nnovel algorithm to generate sequential adversarial examples, which are used to\nattack a RNN based malware detection system. It is usually hard for malicious\nattackers to know the exact structures and weights of the victim RNN. A\nsubstitute RNN is trained to approximate the victim RNN. Then we propose a\ngenerative RNN to output sequential adversarial examples from the original\nsequential malware inputs. Experimental results showed that RNN based malware\ndetection algorithms fail to detect most of the generated malicious adversarial\nexamples, which means the proposed model is able to effectively bypass the\ndetection algorithms.\n", "versions": [{"version": "v1", "created": "Tue, 23 May 2017 08:51:37 GMT"}], "update_date": "2017-05-24", "authors_parsed": [["Hu", "Weiwei", ""], ["Tan", "Ying", ""]]}, {"id": "1705.08142", "submitter": "Sebastian Ruder", "authors": "Sebastian Ruder, Joachim Bingel, Isabelle Augenstein, Anders\n  S{\\o}gaard", "title": "Latent Multi-task Architecture Learning", "comments": "To appear in Proceedings of AAAI 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.CL cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multi-task learning (MTL) allows deep neural networks to learn from related\ntasks by sharing parameters with other networks. In practice, however, MTL\ninvolves searching an enormous space of possible parameter sharing\narchitectures to find (a) the layers or subspaces that benefit from sharing,\n(b) the appropriate amount of sharing, and (c) the appropriate relative weights\nof the different task losses. Recent work has addressed each of the above\nproblems in isolation. In this work we present an approach that learns a latent\nmulti-task architecture that jointly addresses (a)--(c). We present experiments\non synthetic data and data from OntoNotes 5.0, including four different tasks\nand seven different domains. Our extension consistently outperforms previous\napproaches to learning latent architectures for multi-task problems and\nachieves up to 15% average error reductions over common approaches to MTL.\n", "versions": [{"version": "v1", "created": "Tue, 23 May 2017 08:58:09 GMT"}, {"version": "v2", "created": "Tue, 16 Jan 2018 14:05:37 GMT"}, {"version": "v3", "created": "Mon, 19 Nov 2018 10:30:52 GMT"}], "update_date": "2018-11-20", "authors_parsed": [["Ruder", "Sebastian", ""], ["Bingel", "Joachim", ""], ["Augenstein", "Isabelle", ""], ["S\u00f8gaard", "Anders", ""]]}, {"id": "1705.08153", "submitter": "Jos van der Westhuizen", "authors": "Jos van der Westhuizen and Joan Lasenby", "title": "Techniques for visualizing LSTMs applied to electrocardiograms", "comments": "presented at 2018 ICML Workshop on Human Interpretability in Machine\n  Learning (WHI 2018), Stockholm, Sweden", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper explores four different visualization techniques for long\nshort-term memory (LSTM) networks applied to continuous-valued time series. On\nthe datasets analysed, we find that the best visualization technique is to\nlearn an input deletion mask that optimally reduces the true class score. With\na specific focus on single-lead electrocardiograms from the MIT-BIH arrhythmia\ndataset, we show that salient input features for the LSTM classifier align well\nwith medical theory.\n", "versions": [{"version": "v1", "created": "Tue, 23 May 2017 09:35:39 GMT"}, {"version": "v2", "created": "Wed, 8 Nov 2017 16:03:15 GMT"}, {"version": "v3", "created": "Fri, 15 Jun 2018 13:45:59 GMT"}], "update_date": "2018-06-18", "authors_parsed": [["van der Westhuizen", "Jos", ""], ["Lasenby", "Joan", ""]]}, {"id": "1705.08168", "submitter": "Relja Arandjelovi\\'c", "authors": "Relja Arandjelovi\\'c, Andrew Zisserman", "title": "Look, Listen and Learn", "comments": "Appears in: IEEE International Conference on Computer Vision (ICCV)\n  2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the question: what can be learnt by looking at and listening to a\nlarge number of unlabelled videos? There is a valuable, but so far untapped,\nsource of information contained in the video itself -- the correspondence\nbetween the visual and the audio streams, and we introduce a novel\n\"Audio-Visual Correspondence\" learning task that makes use of this. Training\nvisual and audio networks from scratch, without any additional supervision\nother than the raw unconstrained videos themselves, is shown to successfully\nsolve this task, and, more interestingly, result in good visual and audio\nrepresentations. These features set the new state-of-the-art on two sound\nclassification benchmarks, and perform on par with the state-of-the-art\nself-supervised approaches on ImageNet classification. We also demonstrate that\nthe network is able to localize objects in both modalities, as well as perform\nfine-grained recognition tasks.\n", "versions": [{"version": "v1", "created": "Tue, 23 May 2017 10:37:54 GMT"}, {"version": "v2", "created": "Tue, 1 Aug 2017 12:04:50 GMT"}], "update_date": "2017-08-02", "authors_parsed": [["Arandjelovi\u0107", "Relja", ""], ["Zisserman", "Andrew", ""]]}, {"id": "1705.08184", "submitter": "Roi Weiss", "authors": "Aryeh Kontorovich, Sivan Sabato, Roi Weiss", "title": "Nearest-Neighbor Sample Compression: Efficiency, Consistency, Infinite\n  Dimensions", "comments": null, "journal-ref": "Advances in Neural Information Processing Systems 30 (NIPS),\n  1573--1583, 2017", "doi": null, "report-no": null, "categories": "cs.LG math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We examine the Bayes-consistency of a recently proposed\n1-nearest-neighbor-based multiclass learning algorithm. This algorithm is\nderived from sample compression bounds and enjoys the statistical advantages of\ntight, fully empirical generalization bounds, as well as the algorithmic\nadvantages of a faster runtime and memory savings. We prove that this algorithm\nis strongly Bayes-consistent in metric spaces with finite doubling dimension\n--- the first consistency result for an efficient nearest-neighbor sample\ncompression scheme. Rather surprisingly, we discover that this algorithm\ncontinues to be Bayes-consistent even in a certain infinite-dimensional\nsetting, in which the basic measure-theoretic conditions on which classic\nconsistency proofs hinge are violated. This is all the more surprising, since\nit is known that $k$-NN is not Bayes-consistent in this setting. We pose\nseveral challenging open problems for future research.\n", "versions": [{"version": "v1", "created": "Tue, 23 May 2017 11:21:22 GMT"}, {"version": "v2", "created": "Wed, 25 Oct 2017 10:55:59 GMT"}, {"version": "v3", "created": "Wed, 26 Jun 2019 06:47:20 GMT"}], "update_date": "2019-06-27", "authors_parsed": [["Kontorovich", "Aryeh", ""], ["Sabato", "Sivan", ""], ["Weiss", "Roi", ""]]}, {"id": "1705.08197", "submitter": "Jure Sokolic", "authors": "Jure Sokolic, Qiang Qiu, Miguel R. D. Rodrigues, Guillermo Sapiro", "title": "Learning to Succeed while Teaching to Fail: Privacy in Closed Machine\n  Learning Systems", "comments": "14 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Security, privacy, and fairness have become critical in the era of data\nscience and machine learning. More and more we see that achieving universally\nsecure, private, and fair systems is practically impossible. We have seen for\nexample how generative adversarial networks can be used to learn about the\nexpected private training data; how the exploitation of additional data can\nreveal private information in the original one; and how what looks like\nunrelated features can teach us about each other. Confronted with this\nchallenge, in this paper we open a new line of research, where the security,\nprivacy, and fairness is learned and used in a closed environment. The goal is\nto ensure that a given entity (e.g., the company or the government), trusted to\ninfer certain information with our data, is blocked from inferring protected\ninformation from it. For example, a hospital might be allowed to produce\ndiagnosis on the patient (the positive task), without being able to infer the\ngender of the subject (negative task). Similarly, a company can guarantee that\ninternally it is not using the provided data for any undesired task, an\nimportant goal that is not contradicting the virtually impossible challenge of\nblocking everybody from the undesired task. We design a system that learns to\nsucceed on the positive task while simultaneously fail at the negative one, and\nillustrate this with challenging cases where the positive task is actually\nharder than the negative one being blocked. Fairness, to the information in the\nnegative task, is often automatically obtained as a result of this proposed\napproach. The particular framework and examples open the door to security,\nprivacy, and fairness in very important closed scenarios, ranging from private\ndata accumulation companies like social networks to law-enforcement and\nhospitals.\n", "versions": [{"version": "v1", "created": "Tue, 23 May 2017 11:53:02 GMT"}], "update_date": "2017-05-24", "authors_parsed": [["Sokolic", "Jure", ""], ["Qiu", "Qiang", ""], ["Rodrigues", "Miguel R. D.", ""], ["Sapiro", "Guillermo", ""]]}, {"id": "1705.08209", "submitter": "Corentin Tallec", "authors": "Corentin Tallec and Yann Ollivier", "title": "Unbiasing Truncated Backpropagation Through Time", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Truncated Backpropagation Through Time (truncated BPTT) is a widespread\nmethod for learning recurrent computational graphs. Truncated BPTT keeps the\ncomputational benefits of Backpropagation Through Time (BPTT) while relieving\nthe need for a complete backtrack through the whole data sequence at every\nstep. However, truncation favors short-term dependencies: the gradient estimate\nof truncated BPTT is biased, so that it does not benefit from the convergence\nguarantees from stochastic gradient theory. We introduce Anticipated Reweighted\nTruncated Backpropagation (ARTBP), an algorithm that keeps the computational\nbenefits of truncated BPTT, while providing unbiasedness. ARTBP works by using\nvariable truncation lengths together with carefully chosen compensation factors\nin the backpropagation equation. We check the viability of ARTBP on two tasks.\nFirst, a simple synthetic task where careful balancing of temporal dependencies\nat different scales is needed: truncated BPTT displays unreliable performance,\nand in worst case scenarios, divergence, while ARTBP converges reliably.\nSecond, on Penn Treebank character-level language modelling, ARTBP slightly\noutperforms truncated BPTT.\n", "versions": [{"version": "v1", "created": "Tue, 23 May 2017 12:32:48 GMT"}], "update_date": "2017-05-24", "authors_parsed": [["Tallec", "Corentin", ""], ["Ollivier", "Yann", ""]]}, {"id": "1705.08272", "submitter": "Nikolay Savinov", "authors": "Nikolay Savinov, Lubor Ladicky and Marc Pollefeys", "title": "Matching neural paths: transfer from recognition to correspondence\n  search", "comments": "Accepted at NIPS 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many machine learning tasks require finding per-part correspondences between\nobjects. In this work we focus on low-level correspondences - a highly\nambiguous matching problem. We propose to use a hierarchical semantic\nrepresentation of the objects, coming from a convolutional neural network, to\nsolve this ambiguity. Training it for low-level correspondence prediction\ndirectly might not be an option in some domains where the ground-truth\ncorrespondences are hard to obtain. We show how transfer from recognition can\nbe used to avoid such training. Our idea is to mark parts as \"matching\" if\ntheir features are close to each other at all the levels of convolutional\nfeature hierarchy (neural paths). Although the overall number of such paths is\nexponential in the number of layers, we propose a polynomial algorithm for\naggregating all of them in a single backward pass. The empirical validation is\ndone on the task of stereo correspondence and demonstrates that we achieve\ncompetitive results among the methods which do not use labeled target domain\ndata.\n", "versions": [{"version": "v1", "created": "Fri, 19 May 2017 16:40:35 GMT"}, {"version": "v2", "created": "Wed, 24 May 2017 08:04:30 GMT"}, {"version": "v3", "created": "Sun, 5 Nov 2017 22:33:22 GMT"}], "update_date": "2017-11-07", "authors_parsed": [["Savinov", "Nikolay", ""], ["Ladicky", "Lubor", ""], ["Pollefeys", "Marc", ""]]}, {"id": "1705.08292", "submitter": "Rebecca Roelofs", "authors": "Ashia C. Wilson and Rebecca Roelofs and Mitchell Stern and Nathan\n  Srebro and Benjamin Recht", "title": "The Marginal Value of Adaptive Gradient Methods in Machine Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Adaptive optimization methods, which perform local optimization with a metric\nconstructed from the history of iterates, are becoming increasingly popular for\ntraining deep neural networks. Examples include AdaGrad, RMSProp, and Adam. We\nshow that for simple overparameterized problems, adaptive methods often find\ndrastically different solutions than gradient descent (GD) or stochastic\ngradient descent (SGD). We construct an illustrative binary classification\nproblem where the data is linearly separable, GD and SGD achieve zero test\nerror, and AdaGrad, Adam, and RMSProp attain test errors arbitrarily close to\nhalf. We additionally study the empirical generalization capability of adaptive\nmethods on several state-of-the-art deep learning models. We observe that the\nsolutions found by adaptive methods generalize worse (often significantly\nworse) than SGD, even when these solutions have better training performance.\nThese results suggest that practitioners should reconsider the use of adaptive\nmethods to train neural networks.\n", "versions": [{"version": "v1", "created": "Tue, 23 May 2017 14:11:34 GMT"}, {"version": "v2", "created": "Tue, 22 May 2018 00:10:53 GMT"}], "update_date": "2018-05-23", "authors_parsed": [["Wilson", "Ashia C.", ""], ["Roelofs", "Rebecca", ""], ["Stern", "Mitchell", ""], ["Srebro", "Nathan", ""], ["Recht", "Benjamin", ""]]}, {"id": "1705.08360", "submitter": "Danica J. Sutherland", "authors": "Danica J. Sutherland, Heiko Strathmann, Michael Arbel, Arthur Gretton", "title": "Efficient and principled score estimation with Nystr\\\"om kernel\n  exponential families", "comments": null, "journal-ref": "Proceedings of the Twenty-First International Conference on\n  Artificial Intelligence and Statistics (AISTATS 2018), PMLR 84:652-660", "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We propose a fast method with statistical guarantees for learning an\nexponential family density model where the natural parameter is in a\nreproducing kernel Hilbert space, and may be infinite-dimensional. The model is\nlearned by fitting the derivative of the log density, the score, thus avoiding\nthe need to compute a normalization constant. Our approach improves the\ncomputational efficiency of an earlier solution by using a low-rank,\nNystr\\\"om-like solution. The new solution retains the consistency and\nconvergence rates of the full-rank solution (exactly in Fisher distance, and\nnearly in other distances), with guarantees on the degree of cost and storage\nreduction. We evaluate the method in experiments on density estimation and in\nthe construction of an adaptive Hamiltonian Monte Carlo sampler. Compared to an\nexisting score learning approach using a denoising autoencoder, our estimator\nis empirically more data-efficient when estimating the score, runs faster, and\nhas fewer parameters (which can be tuned in a principled and interpretable\nway), in addition to providing statistical guarantees.\n", "versions": [{"version": "v1", "created": "Tue, 23 May 2017 15:29:00 GMT"}, {"version": "v2", "created": "Sun, 4 Jun 2017 22:38:44 GMT"}, {"version": "v3", "created": "Mon, 19 Jun 2017 16:19:42 GMT"}, {"version": "v4", "created": "Fri, 13 Oct 2017 21:43:03 GMT"}, {"version": "v5", "created": "Tue, 13 Mar 2018 19:06:52 GMT"}, {"version": "v6", "created": "Thu, 14 Jan 2021 05:43:23 GMT"}], "update_date": "2021-01-15", "authors_parsed": [["Sutherland", "Danica J.", ""], ["Strathmann", "Heiko", ""], ["Arbel", "Michael", ""], ["Gretton", "Arthur", ""]]}, {"id": "1705.08378", "submitter": "Hongcheng Li", "authors": "Bin Liang, Hongcheng Li, Miaoqiang Su, Xirong Li, Wenchang Shi and\n  Xiaofeng Wang", "title": "Detecting Adversarial Image Examples in Deep Networks with Adaptive\n  Noise Reduction", "comments": "14 pages,\n  http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8482346&isnumber=4358699", "journal-ref": null, "doi": "10.1109/TDSC.2018.2874243", "report-no": null, "categories": "cs.CR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, many studies have demonstrated deep neural network (DNN)\nclassifiers can be fooled by the adversarial example, which is crafted via\nintroducing some perturbations into an original sample. Accordingly, some\npowerful defense techniques were proposed. However, existing defense techniques\noften require modifying the target model or depend on the prior knowledge of\nattacks. In this paper, we propose a straightforward method for detecting\nadversarial image examples, which can be directly deployed into unmodified\noff-the-shelf DNN models. We consider the perturbation to images as a kind of\nnoise and introduce two classic image processing techniques, scalar\nquantization and smoothing spatial filter, to reduce its effect. The image\nentropy is employed as a metric to implement an adaptive noise reduction for\ndifferent kinds of images. Consequently, the adversarial example can be\neffectively detected by comparing the classification results of a given sample\nand its denoised version, without referring to any prior knowledge of attacks.\nMore than 20,000 adversarial examples against some state-of-the-art DNN models\nare used to evaluate the proposed method, which are crafted with different\nattack techniques. The experiments show that our detection method can achieve a\nhigh overall F1 score of 96.39% and certainly raises the bar for defense-aware\nattacks.\n", "versions": [{"version": "v1", "created": "Tue, 23 May 2017 15:50:32 GMT"}, {"version": "v2", "created": "Mon, 19 Jun 2017 02:28:53 GMT"}, {"version": "v3", "created": "Tue, 20 Jun 2017 01:15:17 GMT"}, {"version": "v4", "created": "Sun, 3 Jun 2018 09:10:16 GMT"}, {"version": "v5", "created": "Wed, 9 Jan 2019 01:54:39 GMT"}], "update_date": "2019-01-10", "authors_parsed": [["Liang", "Bin", ""], ["Li", "Hongcheng", ""], ["Su", "Miaoqiang", ""], ["Li", "Xirong", ""], ["Shi", "Wenchang", ""], ["Wang", "Xiaofeng", ""]]}, {"id": "1705.08386", "submitter": "Karol Kurach", "authors": "Karol Kurach, Sylvain Gelly, Michal Jastrzebski, Philip Haeusser,\n  Olivier Teytaud, Damien Vincent, Olivier Bousquet", "title": "Better Text Understanding Through Image-To-Text Transfer", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generic text embeddings are successfully used in a variety of tasks. However,\nthey are often learnt by capturing the co-occurrence structure from pure text\ncorpora, resulting in limitations of their ability to generalize. In this\npaper, we explore models that incorporate visual information into the text\nrepresentation. Based on comprehensive ablation studies, we propose a\nconceptually simple, yet well performing architecture. It outperforms previous\nmultimodal approaches on a set of well established benchmarks. We also improve\nthe state-of-the-art results for image-related text datasets, using orders of\nmagnitude less data.\n", "versions": [{"version": "v1", "created": "Tue, 23 May 2017 16:06:32 GMT"}, {"version": "v2", "created": "Fri, 26 May 2017 08:08:20 GMT"}], "update_date": "2017-06-02", "authors_parsed": [["Kurach", "Karol", ""], ["Gelly", "Sylvain", ""], ["Jastrzebski", "Michal", ""], ["Haeusser", "Philip", ""], ["Teytaud", "Olivier", ""], ["Vincent", "Damien", ""], ["Bousquet", "Olivier", ""]]}, {"id": "1705.08395", "submitter": "Ari Seff", "authors": "Ari Seff, Alex Beatson, Daniel Suo, Han Liu", "title": "Continual Learning in Generative Adversarial Nets", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Developments in deep generative models have allowed for tractable learning of\nhigh-dimensional data distributions. While the employed learning procedures\ntypically assume that training data is drawn i.i.d. from the distribution of\ninterest, it may be desirable to model distinct distributions which are\nobserved sequentially, such as when different classes are encountered over\ntime. Although conditional variations of deep generative models permit multiple\ndistributions to be modeled by a single network in a disentangled fashion, they\nare susceptible to catastrophic forgetting when the distributions are\nencountered sequentially. In this paper, we adapt recent work in reducing\ncatastrophic forgetting to the task of training generative adversarial networks\non a sequence of distinct distributions, enabling continual generative\nmodeling.\n", "versions": [{"version": "v1", "created": "Tue, 23 May 2017 16:27:19 GMT"}], "update_date": "2017-05-24", "authors_parsed": [["Seff", "Ari", ""], ["Beatson", "Alex", ""], ["Suo", "Daniel", ""], ["Liu", "Han", ""]]}, {"id": "1705.08409", "submitter": "Leye Wang", "authors": "Leye Wang, Xu Geng, Jintao Ke, Chen Peng, Xiaojuan Ma, Daqing Zhang,\n  Qiang Yang", "title": "Ridesourcing Car Detection by Transfer Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Ridesourcing platforms like Uber and Didi are getting more and more popular\naround the world. However, unauthorized ridesourcing activities taking\nadvantages of the sharing economy can greatly impair the healthy development of\nthis emerging industry. As the first step to regulate on-demand ride services\nand eliminate black market, we design a method to detect ridesourcing cars from\na pool of cars based on their trajectories. Since licensed ridesourcing car\ntraces are not openly available and may be completely missing in some cities\ndue to legal issues, we turn to transferring knowledge from public transport\nopen data, i.e, taxis and buses, to ridesourcing detection among ordinary\nvehicles. We propose a two-stage transfer learning framework. In Stage 1, we\ntake taxi and bus data as input to learn a random forest (RF) classifier using\ntrajectory features shared by taxis/buses and ridesourcing/other cars. Then, we\nuse the RF to label all the candidate cars. In Stage 2, leveraging the subset\nof high confident labels from the previous stage as input, we further learn a\nconvolutional neural network (CNN) classifier for ridesourcing detection, and\niteratively refine RF and CNN, as well as the feature set, via a co-training\nprocess. Finally, we use the resulting ensemble of RF and CNN to identify the\nridesourcing cars in the candidate pool. Experiments on real car, taxi and bus\ntraces show that our transfer learning framework, with no need of a pre-labeled\nridesourcing dataset, can achieve similar accuracy as the supervised learning\nmethods.\n", "versions": [{"version": "v1", "created": "Tue, 23 May 2017 16:59:29 GMT"}], "update_date": "2017-05-24", "authors_parsed": [["Wang", "Leye", ""], ["Geng", "Xu", ""], ["Ke", "Jintao", ""], ["Peng", "Chen", ""], ["Ma", "Xiaojuan", ""], ["Zhang", "Daqing", ""], ["Yang", "Qiang", ""]]}, {"id": "1705.08417", "submitter": "Viktoriya Krakovna", "authors": "Tom Everitt, Victoria Krakovna, Laurent Orseau, Marcus Hutter, Shane\n  Legg", "title": "Reinforcement Learning with a Corrupted Reward Channel", "comments": "A shorter version of this report was accepted to IJCAI 2017 AI and\n  Autonomy track", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  No real-world reward function is perfect. Sensory errors and software bugs\nmay result in RL agents observing higher (or lower) rewards than they should.\nFor example, a reinforcement learning agent may prefer states where a sensory\nerror gives it the maximum reward, but where the true reward is actually small.\nWe formalise this problem as a generalised Markov Decision Problem called\nCorrupt Reward MDP. Traditional RL methods fare poorly in CRMDPs, even under\nstrong simplifying assumptions and when trying to compensate for the possibly\ncorrupt rewards. Two ways around the problem are investigated. First, by giving\nthe agent richer data, such as in inverse reinforcement learning and\nsemi-supervised reinforcement learning, reward corruption stemming from\nsystematic sensory errors may sometimes be completely managed. Second, by using\nrandomisation to blunt the agent's optimisation, reward corruption can be\npartially managed under some assumptions.\n", "versions": [{"version": "v1", "created": "Tue, 23 May 2017 17:06:56 GMT"}, {"version": "v2", "created": "Sat, 19 Aug 2017 05:01:16 GMT"}], "update_date": "2017-08-22", "authors_parsed": [["Everitt", "Tom", ""], ["Krakovna", "Victoria", ""], ["Orseau", "Laurent", ""], ["Hutter", "Marcus", ""], ["Legg", "Shane", ""]]}, {"id": "1705.08422", "submitter": "Marzyeh Ghassemi", "authors": "Aniruddh Raghu, Matthieu Komorowski, Leo Anthony Celi, Peter Szolovits\n  and Marzyeh Ghassemi", "title": "Continuous State-Space Models for Optimal Sepsis Treatment - a Deep\n  Reinforcement Learning Approach", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sepsis is a leading cause of mortality in intensive care units (ICUs) and\ncosts hospitals billions annually. Treating a septic patient is highly\nchallenging, because individual patients respond very differently to medical\ninterventions and there is no universally agreed-upon treatment for sepsis.\nUnderstanding more about a patient's physiological state at a given time could\nhold the key to effective treatment policies. In this work, we propose a new\napproach to deduce optimal treatment policies for septic patients by using\ncontinuous state-space models and deep reinforcement learning. Learning\ntreatment policies over continuous spaces is important, because we retain more\nof the patient's physiological information. Our model is able to learn\nclinically interpretable treatment policies, similar in important aspects to\nthe treatment policies of physicians. Evaluating our algorithm on past ICU\npatient data, we find that our model could reduce patient mortality in the\nhospital by up to 3.6% over observed clinical policies, from a baseline\nmortality of 13.7%. The learned treatment policies could be used to aid\nintensive care clinicians in medical decision making and improve the likelihood\nof patient survival.\n", "versions": [{"version": "v1", "created": "Tue, 23 May 2017 17:13:11 GMT"}], "update_date": "2017-05-24", "authors_parsed": [["Raghu", "Aniruddh", ""], ["Komorowski", "Matthieu", ""], ["Celi", "Leo Anthony", ""], ["Szolovits", "Peter", ""], ["Ghassemi", "Marzyeh", ""]]}, {"id": "1705.08430", "submitter": "Yannai A. Gonczarowski", "authors": "Noga Alon, Moshe Babaioff, Yannai A. Gonczarowski, Yishay Mansour,\n  Shay Moran, Amir Yehudayoff", "title": "Submultiplicative Glivenko-Cantelli and Uniform Convergence of Revenues", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.GT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work we derive a variant of the classic Glivenko-Cantelli Theorem,\nwhich asserts uniform convergence of the empirical Cumulative Distribution\nFunction (CDF) to the CDF of the underlying distribution. Our variant allows\nfor tighter convergence bounds for extreme values of the CDF.\n  We apply our bound in the context of revenue learning, which is a\nwell-studied problem in economics and algorithmic game theory. We derive\nsample-complexity bounds on the uniform convergence rate of the empirical\nrevenues to the true revenues, assuming a bound on the $k$th moment of the\nvaluations, for any (possibly fractional) $k>1$.\n  For uniform convergence in the limit, we give a complete characterization and\na zero-one law: if the first moment of the valuations is finite, then uniform\nconvergence almost surely occurs; conversely, if the first moment is infinite,\nthen uniform convergence almost never occurs.\n", "versions": [{"version": "v1", "created": "Tue, 23 May 2017 17:37:33 GMT"}, {"version": "v2", "created": "Fri, 4 Aug 2017 12:40:36 GMT"}, {"version": "v3", "created": "Mon, 6 Nov 2017 08:05:11 GMT"}], "update_date": "2017-11-07", "authors_parsed": [["Alon", "Noga", ""], ["Babaioff", "Moshe", ""], ["Gonczarowski", "Yannai A.", ""], ["Mansour", "Yishay", ""], ["Moran", "Shay", ""], ["Yehudayoff", "Amir", ""]]}, {"id": "1705.08435", "submitter": "Aur\\'elien Bellet", "authors": "Aur\\'elien Bellet, Rachid Guerraoui, Mahsa Taziki, Marc Tommasi", "title": "Personalized and Private Peer-to-Peer Machine Learning", "comments": "20 pages, to appear in the Proceedings of the 21st International\n  Conference on Artificial Intelligence and Statistics (AISTATS 2018)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR cs.DC cs.SY stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The rise of connected personal devices together with privacy concerns call\nfor machine learning algorithms capable of leveraging the data of a large\nnumber of agents to learn personalized models under strong privacy\nrequirements. In this paper, we introduce an efficient algorithm to address the\nabove problem in a fully decentralized (peer-to-peer) and asynchronous fashion,\nwith provable convergence rate. We show how to make the algorithm\ndifferentially private to protect against the disclosure of information about\nthe personal datasets, and formally analyze the trade-off between utility and\nprivacy. Our experiments show that our approach dramatically outperforms\nprevious work in the non-private case, and that under privacy constraints, we\ncan significantly improve over models learned in isolation.\n", "versions": [{"version": "v1", "created": "Tue, 23 May 2017 17:43:18 GMT"}, {"version": "v2", "created": "Mon, 19 Feb 2018 09:43:47 GMT"}], "update_date": "2018-02-20", "authors_parsed": [["Bellet", "Aur\u00e9lien", ""], ["Guerraoui", "Rachid", ""], ["Taziki", "Mahsa", ""], ["Tommasi", "Marc", ""]]}, {"id": "1705.08475", "submitter": "Matthias Hein", "authors": "Matthias Hein, Maksym Andriushchenko", "title": "Formal Guarantees on the Robustness of a Classifier against Adversarial\n  Manipulation", "comments": "final version accepted at NIPS 2017, fixed bug in implementation of\n  Cross-Lipschitz regularization and lower bound computation, now results are\n  better", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent work has shown that state-of-the-art classifiers are quite brittle, in\nthe sense that a small adversarial change of an originally with high confidence\ncorrectly classified input leads to a wrong classification again with high\nconfidence. This raises concerns that such classifiers are vulnerable to\nattacks and calls into question their usage in safety-critical systems. We show\nin this paper for the first time formal guarantees on the robustness of a\nclassifier by giving instance-specific lower bounds on the norm of the input\nmanipulation required to change the classifier decision. Based on this analysis\nwe propose the Cross-Lipschitz regularization functional. We show that using\nthis form of regularization in kernel methods resp. neural networks improves\nthe robustness of the classifier without any loss in prediction performance.\n", "versions": [{"version": "v1", "created": "Tue, 23 May 2017 18:48:20 GMT"}, {"version": "v2", "created": "Sun, 5 Nov 2017 20:58:09 GMT"}], "update_date": "2017-11-07", "authors_parsed": [["Hein", "Matthias", ""], ["Andriushchenko", "Maksym", ""]]}, {"id": "1705.08480", "submitter": "Brendan Maginnis", "authors": "Brendan Maginnis, Pierre H. Richemond", "title": "Efficiently applying attention to sequential data with the Recurrent\n  Discounted Attention unit", "comments": "Updated results of RDA-exp-tanh unit for the wikipedia char\n  prediction task", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recurrent Neural Networks architectures excel at processing sequences by\nmodelling dependencies over different timescales. The recently introduced\nRecurrent Weighted Average (RWA) unit captures long term dependencies far\nbetter than an LSTM on several challenging tasks. The RWA achieves this by\napplying attention to each input and computing a weighted average over the full\nhistory of its computations. Unfortunately, the RWA cannot change the attention\nit has assigned to previous timesteps, and so struggles with carrying out\nconsecutive tasks or tasks with changing requirements. We present the Recurrent\nDiscounted Attention (RDA) unit that builds on the RWA by additionally allowing\nthe discounting of the past.\n  We empirically compare our model to RWA, LSTM and GRU units on several\nchallenging tasks. On tasks with a single output the RWA, RDA and GRU units\nlearn much quicker than the LSTM and with better performance. On the multiple\nsequence copy task our RDA unit learns the task three times as quickly as the\nLSTM or GRU units while the RWA fails to learn at all. On the Wikipedia\ncharacter prediction task the LSTM performs best but it followed closely by our\nRDA unit. Overall our RDA unit performs well and is sample efficient on a large\nvariety of sequence tasks.\n", "versions": [{"version": "v1", "created": "Tue, 23 May 2017 18:57:50 GMT"}, {"version": "v2", "created": "Mon, 19 Jun 2017 08:53:04 GMT"}], "update_date": "2017-06-20", "authors_parsed": [["Maginnis", "Brendan", ""], ["Richemond", "Pierre H.", ""]]}, {"id": "1705.08481", "submitter": "Cuong Nguyen", "authors": "Cuong V. Nguyen, Lam Si Tung Ho, Huan Xu, Vu Dinh, Binh Nguyen", "title": "Bayesian Pool-based Active Learning With Abstention Feedbacks", "comments": "There is a new version at arXiv:1906.02179", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study pool-based active learning with abstention feedbacks, where a\nlabeler can abstain from labeling a queried example with some unknown\nabstention rate. This is an important problem with many useful applications. We\ntake a Bayesian approach to the problem and develop two new greedy algorithms\nthat learn both the classification problem and the unknown abstention rate at\nthe same time. These are achieved by simply incorporating the estimated\nabstention rate into the greedy criteria. We prove that both of our algorithms\nhave near-optimality guarantees: they respectively achieve a\n${(1-\\frac{1}{e})}$ constant factor approximation of the optimal expected or\nworst-case value of a useful utility function. Our experiments show the\nalgorithms perform well in various practical scenarios.\n", "versions": [{"version": "v1", "created": "Tue, 23 May 2017 18:58:51 GMT"}, {"version": "v2", "created": "Wed, 27 Sep 2017 13:57:16 GMT"}, {"version": "v3", "created": "Sun, 3 Jan 2021 01:03:43 GMT"}], "update_date": "2021-01-05", "authors_parsed": [["Nguyen", "Cuong V.", ""], ["Ho", "Lam Si Tung", ""], ["Xu", "Huan", ""], ["Dinh", "Vu", ""], ["Nguyen", "Binh", ""]]}, {"id": "1705.08498", "submitter": "Marzyeh Ghassemi", "authors": "Harini Suresh, Nathan Hunt, Alistair Johnson, Leo Anthony Celi, Peter\n  Szolovits and Marzyeh Ghassemi", "title": "Clinical Intervention Prediction and Understanding using Deep Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Real-time prediction of clinical interventions remains a challenge within\nintensive care units (ICUs). This task is complicated by data sources that are\nnoisy, sparse, heterogeneous and outcomes that are imbalanced. In this paper,\nwe integrate data from all available ICU sources (vitals, labs, notes,\ndemographics) and focus on learning rich representations of this data to\npredict onset and weaning of multiple invasive interventions. In particular, we\ncompare both long short-term memory networks (LSTM) and convolutional neural\nnetworks (CNN) for prediction of five intervention tasks: invasive ventilation,\nnon-invasive ventilation, vasopressors, colloid boluses, and crystalloid\nboluses. Our predictions are done in a forward-facing manner to enable\n\"real-time\" performance, and predictions are made with a six hour gap time to\nsupport clinically actionable planning. We achieve state-of-the-art results on\nour predictive tasks using deep architectures. We explore the use of feature\nocclusion to interpret LSTM models, and compare this to the interpretability\ngained from examining inputs that maximally activate CNN outputs. We show that\nour models are able to significantly outperform baselines in intervention\nprediction, and provide insight into model learning, which is crucial for the\nadoption of such models in practice.\n", "versions": [{"version": "v1", "created": "Tue, 23 May 2017 19:42:20 GMT"}], "update_date": "2017-05-25", "authors_parsed": [["Suresh", "Harini", ""], ["Hunt", "Nathan", ""], ["Johnson", "Alistair", ""], ["Celi", "Leo Anthony", ""], ["Szolovits", "Peter", ""], ["Ghassemi", "Marzyeh", ""]]}, {"id": "1705.08499", "submitter": "Yonatan Geifman", "authors": "Ran El-Yaniv, Yonatan Geifman, Yair Wiener", "title": "The Prediction Advantage: A Universally Meaningful Performance Measure\n  for Classification and Regression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce the Prediction Advantage (PA), a novel performance measure for\nprediction functions under any loss function (e.g., classification or\nregression). The PA is defined as the performance advantage relative to the\nBayesian risk restricted to knowing only the distribution of the labels. We\nderive the PA for well-known loss functions, including 0/1 loss, cross-entropy\nloss, absolute loss, and squared loss. In the latter case, the PA is identical\nto the well-known R-squared measure, widely used in statistics. The use of the\nPA ensures meaningful quantification of prediction performance, which is not\nguaranteed, for example, when dealing with noisy imbalanced classification\nproblems. We argue that among several known alternative performance measures,\nPA is the best (and only) quantity ensuring meaningfulness for all noise and\nimbalance levels.\n", "versions": [{"version": "v1", "created": "Tue, 23 May 2017 19:43:37 GMT"}, {"version": "v2", "created": "Fri, 26 May 2017 20:45:54 GMT"}], "update_date": "2017-05-30", "authors_parsed": [["El-Yaniv", "Ran", ""], ["Geifman", "Yonatan", ""], ["Wiener", "Yair", ""]]}, {"id": "1705.08500", "submitter": "Yonatan Geifman", "authors": "Yonatan Geifman, Ran El-Yaniv", "title": "Selective Classification for Deep Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Selective classification techniques (also known as reject option) have not\nyet been considered in the context of deep neural networks (DNNs). These\ntechniques can potentially significantly improve DNNs prediction performance by\ntrading-off coverage. In this paper we propose a method to construct a\nselective classifier given a trained neural network. Our method allows a user\nto set a desired risk level. At test time, the classifier rejects instances as\nneeded, to grant the desired risk (with high probability). Empirical results\nover CIFAR and ImageNet convincingly demonstrate the viability of our method,\nwhich opens up possibilities to operate DNNs in mission-critical applications.\nFor example, using our method an unprecedented 2% error in top-5 ImageNet\nclassification can be guaranteed with probability 99.9%, and almost 60% test\ncoverage.\n", "versions": [{"version": "v1", "created": "Tue, 23 May 2017 19:43:56 GMT"}, {"version": "v2", "created": "Thu, 1 Jun 2017 14:10:34 GMT"}], "update_date": "2017-06-02", "authors_parsed": [["Geifman", "Yonatan", ""], ["El-Yaniv", "Ran", ""]]}, {"id": "1705.08504", "submitter": "Osbert Bastani", "authors": "Osbert Bastani, Carolyn Kim, Hamsa Bastani", "title": "Interpreting Blackbox Models via Model Extraction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Interpretability has become incredibly important as machine learning is\nincreasingly used to inform consequential decisions. We propose to construct\nglobal explanations of complex, blackbox models in the form of a decision tree\napproximating the original model---as long as the decision tree is a good\napproximation, then it mirrors the computation performed by the blackbox model.\nWe devise a novel algorithm for extracting decision tree explanations that\nactively samples new training points to avoid overfitting. We evaluate our\nalgorithm on a random forest to predict diabetes risk and a learned controller\nfor cart-pole. Compared to several baselines, our decision trees are both\nsubstantially more accurate and equally or more interpretable based on a user\nstudy. Finally, we describe several insights provided by our interpretations,\nincluding a causal issue validated by a physician.\n", "versions": [{"version": "v1", "created": "Tue, 23 May 2017 19:47:52 GMT"}, {"version": "v2", "created": "Mon, 12 Feb 2018 06:46:11 GMT"}, {"version": "v3", "created": "Tue, 13 Mar 2018 01:02:30 GMT"}, {"version": "v4", "created": "Sun, 18 Mar 2018 02:58:20 GMT"}, {"version": "v5", "created": "Tue, 22 May 2018 00:36:39 GMT"}, {"version": "v6", "created": "Thu, 24 Jan 2019 19:01:56 GMT"}], "update_date": "2019-01-28", "authors_parsed": [["Bastani", "Osbert", ""], ["Kim", "Carolyn", ""], ["Bastani", "Hamsa", ""]]}, {"id": "1705.08520", "submitter": "Horst Samulowitz", "authors": "Gonzalo Diaz, Achille Fokoue, Giacomo Nannicini, Horst Samulowitz", "title": "An effective algorithm for hyperparameter optimization of neural\n  networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A major challenge in designing neural network (NN) systems is to determine\nthe best structure and parameters for the network given the data for the\nmachine learning problem at hand. Examples of parameters are the number of\nlayers and nodes, the learning rates, and the dropout rates. Typically, these\nparameters are chosen based on heuristic rules and manually fine-tuned, which\nmay be very time-consuming, because evaluating the performance of a single\nparametrization of the NN may require several hours. This paper addresses the\nproblem of choosing appropriate parameters for the NN by formulating it as a\nbox-constrained mathematical optimization problem, and applying a\nderivative-free optimization tool that automatically and effectively searches\nthe parameter space. The optimization tool employs a radial basis function\nmodel of the objective function (the prediction accuracy of the NN) to\naccelerate the discovery of configurations yielding high accuracy. Candidate\nconfigurations explored by the algorithm are trained to a small number of\nepochs, and only the most promising candidates receive full training. The\nperformance of the proposed methodology is assessed on benchmark sets and in\nthe context of predicting drug-drug interactions, showing promising results.\nThe optimization tool used in this paper is open-source.\n", "versions": [{"version": "v1", "created": "Tue, 23 May 2017 20:17:44 GMT"}], "update_date": "2017-05-25", "authors_parsed": [["Diaz", "Gonzalo", ""], ["Fokoue", "Achille", ""], ["Nannicini", "Giacomo", ""], ["Samulowitz", "Horst", ""]]}, {"id": "1705.08525", "submitter": "Wei-Cheng Chang", "authors": "Wei-Cheng Chang, Chun-Liang Li, Yiming Yang, Barnabas Poczos", "title": "Data-driven Random Fourier Features using Stein Effect", "comments": "To appear in International Joint Conference on Artificial\n  Intelligence (IJCAI), 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Large-scale kernel approximation is an important problem in machine learning\nresearch. Approaches using random Fourier features have become increasingly\npopular [Rahimi and Recht, 2007], where kernel approximation is treated as\nempirical mean estimation via Monte Carlo (MC) or Quasi-Monte Carlo (QMC)\nintegration [Yang et al., 2014]. A limitation of the current approaches is that\nall the features receive an equal weight summing to 1. In this paper, we\npropose a novel shrinkage estimator from \"Stein effect\", which provides a\ndata-driven weighting strategy for random features and enjoys theoretical\njustifications in terms of lowering the empirical risk. We further present an\nefficient randomized algorithm for large-scale applications of the proposed\nmethod. Our empirical results on six benchmark data sets demonstrate the\nadvantageous performance of this approach over representative baselines in both\nkernel approximation and supervised learning tasks.\n", "versions": [{"version": "v1", "created": "Tue, 23 May 2017 20:27:19 GMT"}], "update_date": "2017-05-25", "authors_parsed": [["Chang", "Wei-Cheng", ""], ["Li", "Chun-Liang", ""], ["Yang", "Yiming", ""], ["Poczos", "Barnabas", ""]]}, {"id": "1705.08530", "submitter": "Bowei Yan", "authors": "Bowei Yan, Mingzhang Yin and Purnamrita Sarkar", "title": "Convergence Analysis of Gradient EM for Multi-component Gaussian Mixture", "comments": "36 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.LG stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we study convergence properties of the gradient\nExpectation-Maximization algorithm \\cite{lange1995gradient} for Gaussian\nMixture Models for general number of clusters and mixing coefficients. We\nderive the convergence rate depending on the mixing coefficients, minimum and\nmaximum pairwise distances between the true centers and dimensionality and\nnumber of components; and obtain a near-optimal local contraction radius. While\nthere have been some recent notable works that derive local convergence rates\nfor EM in the two equal mixture symmetric GMM, in the more general case, the\nderivations need structurally different and non-trivial arguments. We use\nrecent tools from learning theory and empirical processes to achieve our\ntheoretical results.\n", "versions": [{"version": "v1", "created": "Tue, 23 May 2017 20:47:17 GMT"}, {"version": "v2", "created": "Mon, 4 Dec 2017 18:16:50 GMT"}], "update_date": "2017-12-05", "authors_parsed": [["Yan", "Bowei", ""], ["Yin", "Mingzhang", ""], ["Sarkar", "Purnamrita", ""]]}, {"id": "1705.08550", "submitter": "Wentao Zhu", "authors": "Wentao Zhu, Qi Lou, Yeeleng Scott Vang, and Xiaohui Xie", "title": "Deep Multi-instance Networks with Sparse Label Assignment for Whole\n  Mammogram Classification", "comments": "MICCAI 2017 Camera Ready", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.NE", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Mammogram classification is directly related to computer-aided diagnosis of\nbreast cancer. Traditional methods rely on regions of interest (ROIs) which\nrequire great efforts to annotate. Inspired by the success of using deep\nconvolutional features for natural image analysis and multi-instance learning\n(MIL) for labeling a set of instances/patches, we propose end-to-end trained\ndeep multi-instance networks for mass classification based on whole mammogram\nwithout the aforementioned ROIs. We explore three different schemes to\nconstruct deep multi-instance networks for whole mammogram classification.\nExperimental results on the INbreast dataset demonstrate the robustness of\nproposed networks compared to previous work using segmentation and detection\nannotations.\n", "versions": [{"version": "v1", "created": "Tue, 23 May 2017 22:16:20 GMT"}], "update_date": "2017-05-25", "authors_parsed": [["Zhu", "Wentao", ""], ["Lou", "Qi", ""], ["Vang", "Yeeleng Scott", ""], ["Xie", "Xiaohui", ""]]}, {"id": "1705.08551", "submitter": "Felix Berkenkamp", "authors": "Felix Berkenkamp, Matteo Turchetta, Angela P. Schoellig, Andreas\n  Krause", "title": "Safe Model-based Reinforcement Learning with Stability Guarantees", "comments": "Proc. of Neural Information Processing Systems (NIPS), 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.LG cs.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Reinforcement learning is a powerful paradigm for learning optimal policies\nfrom experimental data. However, to find optimal policies, most reinforcement\nlearning algorithms explore all possible actions, which may be harmful for\nreal-world systems. As a consequence, learning algorithms are rarely applied on\nsafety-critical systems in the real world. In this paper, we present a learning\nalgorithm that explicitly considers safety, defined in terms of stability\nguarantees. Specifically, we extend control-theoretic results on Lyapunov\nstability verification and show how to use statistical models of the dynamics\nto obtain high-performance control policies with provable stability\ncertificates. Moreover, under additional regularity assumptions in terms of a\nGaussian process prior, we prove that one can effectively and safely collect\ndata in order to learn about the dynamics and thus both improve control\nperformance and expand the safe region of the state space. In our experiments,\nwe show how the resulting algorithm can safely optimize a neural network policy\non a simulated inverted pendulum, without the pendulum ever falling down.\n", "versions": [{"version": "v1", "created": "Tue, 23 May 2017 22:20:08 GMT"}, {"version": "v2", "created": "Tue, 7 Nov 2017 10:14:13 GMT"}, {"version": "v3", "created": "Mon, 13 Nov 2017 18:49:54 GMT"}], "update_date": "2017-11-15", "authors_parsed": [["Berkenkamp", "Felix", ""], ["Turchetta", "Matteo", ""], ["Schoellig", "Angela P.", ""], ["Krause", "Andreas", ""]]}, {"id": "1705.08557", "submitter": "Ankit Vani", "authors": "Ankit Vani, Yacine Jernite, David Sontag", "title": "Grounded Recurrent Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CL cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we present the Grounded Recurrent Neural Network (GRNN), a\nrecurrent neural network architecture for multi-label prediction which\nexplicitly ties labels to specific dimensions of the recurrent hidden state (we\ncall this process \"grounding\"). The approach is particularly well-suited for\nextracting large numbers of concepts from text. We apply the new model to\naddress an important problem in healthcare of understanding what medical\nconcepts are discussed in clinical text. Using a publicly available dataset\nderived from Intensive Care Units, we learn to label a patient's diagnoses and\nprocedures from their discharge summary. Our evaluation shows a clear advantage\nto using our proposed architecture over a variety of strong baselines.\n", "versions": [{"version": "v1", "created": "Tue, 23 May 2017 23:17:49 GMT"}], "update_date": "2017-05-25", "authors_parsed": [["Vani", "Ankit", ""], ["Jernite", "Yacine", ""], ["Sontag", "David", ""]]}, {"id": "1705.08562", "submitter": "Kun He", "authors": "Kun He, Fatih Cakir, Sarah Adel Bargal, Stan Sclaroff", "title": "Hashing as Tie-Aware Learning to Rank", "comments": "15 pages, 3 figures. IEEE Conference on Computer Vision and Pattern\n  Recognition (CVPR), 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hashing, or learning binary embeddings of data, is frequently used in nearest\nneighbor retrieval. In this paper, we develop learning to rank formulations for\nhashing, aimed at directly optimizing ranking-based evaluation metrics such as\nAverage Precision (AP) and Normalized Discounted Cumulative Gain (NDCG). We\nfirst observe that the integer-valued Hamming distance often leads to tied\nrankings, and propose to use tie-aware versions of AP and NDCG to evaluate\nhashing for retrieval. Then, to optimize tie-aware ranking metrics, we derive\ntheir continuous relaxations, and perform gradient-based optimization with deep\nneural networks. Our results establish the new state-of-the-art for image\nretrieval by Hamming ranking in common benchmarks.\n", "versions": [{"version": "v1", "created": "Tue, 23 May 2017 23:42:46 GMT"}, {"version": "v2", "created": "Thu, 23 Nov 2017 02:07:11 GMT"}, {"version": "v3", "created": "Wed, 28 Mar 2018 04:42:56 GMT"}, {"version": "v4", "created": "Tue, 9 Oct 2018 20:37:00 GMT"}], "update_date": "2018-10-11", "authors_parsed": [["He", "Kun", ""], ["Cakir", "Fatih", ""], ["Bargal", "Sarah Adel", ""], ["Sclaroff", "Stan", ""]]}, {"id": "1705.08564", "submitter": "Wenbo Guo", "authors": "Wenbo Guo, Kaixuan Zhang, Lin Lin, Sui Huang, Xinyu Xing", "title": "Towards Interrogating Discriminative Machine Learning Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is oftentimes impossible to understand how machine learning models reach a\ndecision. While recent research has proposed various technical approaches to\nprovide some clues as to how a learning model makes individual decisions, they\ncannot provide users with ability to inspect a learning model as a complete\nentity. In this work, we propose a new technical approach that augments a\nBayesian regression mixture model with multiple elastic nets. Using the\nenhanced mixture model, we extract explanations for a target model through\nglobal approximation. To demonstrate the utility of our approach, we evaluate\nit on different learning models covering the tasks of text mining and image\nrecognition. Our results indicate that the proposed approach not only\noutperforms the state-of-the-art technique in explaining individual decisions\nbut also provides users with an ability to discover the vulnerabilities of a\nlearning model.\n", "versions": [{"version": "v1", "created": "Tue, 23 May 2017 23:51:37 GMT"}], "update_date": "2017-05-25", "authors_parsed": [["Guo", "Wenbo", ""], ["Zhang", "Kaixuan", ""], ["Lin", "Lin", ""], ["Huang", "Sui", ""], ["Xing", "Xinyu", ""]]}, {"id": "1705.08584", "submitter": "Wei-Cheng Chang", "authors": "Chun-Liang Li, Wei-Cheng Chang, Yu Cheng, Yiming Yang, Barnab\\'as\n  P\\'oczos", "title": "MMD GAN: Towards Deeper Understanding of Moment Matching Network", "comments": "In the Proceedings of Thirty-first Annual Conference on Neural\n  Information Processing Systems (NIPS 2017)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generative moment matching network (GMMN) is a deep generative model that\ndiffers from Generative Adversarial Network (GAN) by replacing the\ndiscriminator in GAN with a two-sample test based on kernel maximum mean\ndiscrepancy (MMD). Although some theoretical guarantees of MMD have been\nstudied, the empirical performance of GMMN is still not as competitive as that\nof GAN on challenging and large benchmark datasets. The computational\nefficiency of GMMN is also less desirable in comparison with GAN, partially due\nto its requirement for a rather large batch size during the training. In this\npaper, we propose to improve both the model expressiveness of GMMN and its\ncomputational efficiency by introducing adversarial kernel learning techniques,\nas the replacement of a fixed Gaussian kernel in the original GMMN. The new\napproach combines the key ideas in both GMMN and GAN, hence we name it MMD GAN.\nThe new distance measure in MMD GAN is a meaningful loss that enjoys the\nadvantage of weak topology and can be optimized via gradient descent with\nrelatively small batch sizes. In our evaluation on multiple benchmark datasets,\nincluding MNIST, CIFAR- 10, CelebA and LSUN, the performance of MMD-GAN\nsignificantly outperforms GMMN, and is competitive with other representative\nGAN works.\n", "versions": [{"version": "v1", "created": "Wed, 24 May 2017 02:20:29 GMT"}, {"version": "v2", "created": "Mon, 6 Nov 2017 17:05:42 GMT"}, {"version": "v3", "created": "Mon, 27 Nov 2017 14:04:35 GMT"}], "update_date": "2017-11-28", "authors_parsed": [["Li", "Chun-Liang", ""], ["Chang", "Wei-Cheng", ""], ["Cheng", "Yu", ""], ["Yang", "Yiming", ""], ["P\u00f3czos", "Barnab\u00e1s", ""]]}, {"id": "1705.08618", "submitter": "Aniket Anand Deshmukh", "authors": "Aniket Anand Deshmukh, Urun Dogan, Clayton Scott", "title": "Multi-Task Learning for Contextual Bandits", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Contextual bandits are a form of multi-armed bandit in which the agent has\naccess to predictive side information (known as the context) for each arm at\neach time step, and have been used to model personalized news recommendation,\nad placement, and other applications. In this work, we propose a multi-task\nlearning framework for contextual bandit problems. Like multi-task learning in\nthe batch setting, the goal is to leverage similarities in contexts for\ndifferent arms so as to improve the agent's ability to predict rewards from\ncontexts. We propose an upper confidence bound-based multi-task learning\nalgorithm for contextual bandits, establish a corresponding regret bound, and\ninterpret this bound to quantify the advantages of learning in the presence of\nhigh task (arm) similarity. We also describe an effective scheme for estimating\ntask similarity from data, and demonstrate our algorithm's performance on\nseveral data sets.\n", "versions": [{"version": "v1", "created": "Wed, 24 May 2017 05:47:52 GMT"}], "update_date": "2017-05-25", "authors_parsed": [["Deshmukh", "Aniket Anand", ""], ["Dogan", "Urun", ""], ["Scott", "Clayton", ""]]}, {"id": "1705.08619", "submitter": "Sandeep Chandra Bollepalli", "authors": "Bollepalli S. Chandra, Challa S. Sastry, Laxminarayana Anumandla and\n  Soumya Jana", "title": "Dictionary-based Monitoring of Premature Ventricular Contractions: An\n  Ultra-Low-Cost Point-of-Care Service", "comments": "19 pages, 9 figures and 5 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  While cardiovascular diseases (CVDs) are prevalent across economic strata,\nthe economically disadvantaged population is disproportionately affected due to\nthe high cost of traditional CVD management. Accordingly, developing an\nultra-low-cost alternative, affordable even to groups at the bottom of the\neconomic pyramid, has emerged as a societal imperative. Against this backdrop,\nwe propose an inexpensive yet accurate home-based electrocardiogram(ECG)\nmonitoring service. Specifically, we seek to provide point-of-care monitoring\nof premature ventricular contractions (PVCs), high frequency of which could\nindicate the onset of potentially fatal arrhythmia. Note that a traditional\ntelecardiology system acquires the ECG, transmits it to a professional\ndiagnostic centre without processing, and nearly achieves the diagnostic\naccuracy of a bedside setup, albeit at high bandwidth cost. In this context, we\naim at reducing cost without significantly sacrificing reliability. To this\nend, we develop a dictionary-based algorithm that detects with high sensitivity\nthe anomalous beats only which are then transmitted. We further compress those\ntransmitted beats using class-specific dictionaries subject to suitable\nreconstruction/diagnostic fidelity. Such a scheme would not only reduce the\noverall bandwidth requirement, but also localising anomalous beats, thereby\nreducing physicians' burden. Finally, using Monte Carlo cross validation on\nMIT/BIH arrhythmia database, we evaluate the performance of the proposed\nsystem. In particular, with a sensitivity target of at most one undetected PVC\nin one hundred beats, and a percentage root mean squared difference less than\n9% (a clinically acceptable level of fidelity), we achieved about 99.15%\nreduction in bandwidth cost, equivalent to 118-fold savings over traditional\ntelecardiology.\n", "versions": [{"version": "v1", "created": "Wed, 24 May 2017 06:00:57 GMT"}], "update_date": "2017-05-25", "authors_parsed": [["Chandra", "Bollepalli S.", ""], ["Sastry", "Challa S.", ""], ["Anumandla", "Laxminarayana", ""], ["Jana", "Soumya", ""]]}, {"id": "1705.08621", "submitter": "Julian Katz-Samuels", "authors": "Julian Katz-Samuels and Clayton Scott", "title": "Nonparametric Preference Completion", "comments": "AISTATS 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the task of collaborative preference completion: given a pool of\nitems, a pool of users and a partially observed item-user rating matrix, the\ngoal is to recover the \\emph{personalized ranking} of each user over all of the\nitems. Our approach is nonparametric: we assume that each item $i$ and each\nuser $u$ have unobserved features $x_i$ and $y_u$, and that the associated\nrating is given by $g_u(f(x_i,y_u))$ where $f$ is Lipschitz and $g_u$ is a\nmonotonic transformation that depends on the user. We propose a $k$-nearest\nneighbors-like algorithm and prove that it is consistent. To the best of our\nknowledge, this is the first consistency result for the collaborative\npreference completion problem in a nonparametric setting. Finally, we\ndemonstrate the performance of our algorithm with experiments on the Netflix\nand Movielens datasets.\n", "versions": [{"version": "v1", "created": "Wed, 24 May 2017 06:04:58 GMT"}, {"version": "v2", "created": "Tue, 10 Apr 2018 17:03:04 GMT"}], "update_date": "2018-04-11", "authors_parsed": [["Katz-Samuels", "Julian", ""], ["Scott", "Clayton", ""]]}, {"id": "1705.08664", "submitter": "Yuting Zhang", "authors": "Anna C. Gilbert, Yi Zhang, Kibok Lee, Yuting Zhang, Honglak Lee", "title": "Towards Understanding the Invertibility of Convolutional Neural Networks", "comments": null, "journal-ref": "IJCAI 2017", "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Several recent works have empirically observed that Convolutional Neural Nets\n(CNNs) are (approximately) invertible. To understand this approximate\ninvertibility phenomenon and how to leverage it more effectively, we focus on a\ntheoretical explanation and develop a mathematical model of sparse signal\nrecovery that is consistent with CNNs with random weights. We give an exact\nconnection to a particular model of model-based compressive sensing (and its\nrecovery algorithms) and random-weight CNNs. We show empirically that several\nlearned networks are consistent with our mathematical analysis and then\ndemonstrate that with such a simple theoretical framework, we can obtain\nreasonable re- construction results on real images. We also discuss gaps\nbetween our model assumptions and the CNN trained for classification in\npractical scenarios.\n", "versions": [{"version": "v1", "created": "Wed, 24 May 2017 09:02:52 GMT"}], "update_date": "2017-05-25", "authors_parsed": [["Gilbert", "Anna C.", ""], ["Zhang", "Yi", ""], ["Lee", "Kibok", ""], ["Zhang", "Yuting", ""], ["Lee", "Honglak", ""]]}, {"id": "1705.08665", "submitter": "Christos Louizos", "authors": "Christos Louizos, Karen Ullrich and Max Welling", "title": "Bayesian Compression for Deep Learning", "comments": "Published as a conference paper at NIPS 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Compression and computational efficiency in deep learning have become a\nproblem of great significance. In this work, we argue that the most principled\nand effective way to attack this problem is by adopting a Bayesian point of\nview, where through sparsity inducing priors we prune large parts of the\nnetwork. We introduce two novelties in this paper: 1) we use hierarchical\npriors to prune nodes instead of individual weights, and 2) we use the\nposterior uncertainties to determine the optimal fixed point precision to\nencode the weights. Both factors significantly contribute to achieving the\nstate of the art in terms of compression rates, while still staying competitive\nwith methods designed to optimize for speed or energy efficiency.\n", "versions": [{"version": "v1", "created": "Wed, 24 May 2017 09:07:01 GMT"}, {"version": "v2", "created": "Mon, 29 May 2017 04:59:44 GMT"}, {"version": "v3", "created": "Thu, 10 Aug 2017 04:03:01 GMT"}, {"version": "v4", "created": "Mon, 6 Nov 2017 12:46:40 GMT"}], "update_date": "2017-11-07", "authors_parsed": [["Louizos", "Christos", ""], ["Ullrich", "Karen", ""], ["Welling", "Max", ""]]}, {"id": "1705.08690", "submitter": "Hanul Shin", "authors": "Hanul Shin, Jung Kwon Lee, Jaehong Kim, Jiwon Kim", "title": "Continual Learning with Deep Generative Replay", "comments": "NIPS 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Attempts to train a comprehensive artificial intelligence capable of solving\nmultiple tasks have been impeded by a chronic problem called catastrophic\nforgetting. Although simply replaying all previous data alleviates the problem,\nit requires large memory and even worse, often infeasible in real world\napplications where the access to past data is limited. Inspired by the\ngenerative nature of hippocampus as a short-term memory system in primate\nbrain, we propose the Deep Generative Replay, a novel framework with a\ncooperative dual model architecture consisting of a deep generative model\n(\"generator\") and a task solving model (\"solver\"). With only these two models,\ntraining data for previous tasks can easily be sampled and interleaved with\nthose for a new task. We test our methods in several sequential learning\nsettings involving image classification tasks.\n", "versions": [{"version": "v1", "created": "Wed, 24 May 2017 10:37:38 GMT"}, {"version": "v2", "created": "Sat, 9 Sep 2017 15:31:38 GMT"}, {"version": "v3", "created": "Tue, 12 Dec 2017 02:14:21 GMT"}], "update_date": "2017-12-13", "authors_parsed": [["Shin", "Hanul", ""], ["Lee", "Jung Kwon", ""], ["Kim", "Jaehong", ""], ["Kim", "Jiwon", ""]]}, {"id": "1705.08695", "submitter": "Zenglin Xu", "authors": "Hao Liu and Haoli Bai and Lirong He and Zenglin Xu", "title": "Stochastic Sequential Neural Networks with Structured Inference", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Unsupervised structure learning in high-dimensional time series data has\nattracted a lot of research interests. For example, segmenting and labelling\nhigh dimensional time series can be helpful in behavior understanding and\nmedical diagnosis. Recent advances in generative sequential modeling have\nsuggested to combine recurrent neural networks with state space models (e.g.,\nHidden Markov Models). This combination can model not only the long term\ndependency in sequential data, but also the uncertainty included in the hidden\nstates. Inheriting these advantages of stochastic neural sequential models, we\npropose a structured and stochastic sequential neural network, which models\nboth the long-term dependencies via recurrent neural networks and the\nuncertainty in the segmentation and labels via discrete random variables. For\naccurate and efficient inference, we present a bi-directional inference network\nby reparamterizing the categorical segmentation and labels with the recent\nproposed Gumbel-Softmax approximation and resort to the Stochastic Gradient\nVariational Bayes. We evaluate the proposed model in a number of tasks,\nincluding speech modeling, automatic segmentation and labeling in behavior\nunderstanding, and sequential multi-objects recognition. Experimental results\nhave demonstrated that our proposed model can achieve significant improvement\nover the state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Wed, 24 May 2017 10:52:19 GMT"}], "update_date": "2017-05-25", "authors_parsed": [["Liu", "Hao", ""], ["Bai", "Haoli", ""], ["He", "Lirong", ""], ["Xu", "Zenglin", ""]]}, {"id": "1705.08722", "submitter": "Yang Yu", "authors": "Yang Yu, Wei-Yang Qu, Nan Li, Zimin Guo", "title": "Open-Category Classification by Adversarial Sample Generation", "comments": "Published in IJCAI 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In real-world classification tasks, it is difficult to collect training\nsamples from all possible categories of the environment. Therefore, when an\ninstance of an unseen class appears in the prediction stage, a robust\nclassifier should be able to tell that it is from an unseen class, instead of\nclassifying it to be any known category. In this paper, adopting the idea of\nadversarial learning, we propose the ASG framework for open-category\nclassification. ASG generates positive and negative samples of seen categories\nin the unsupervised manner via an adversarial learning strategy. With the\ngenerated samples, ASG then learns to tell seen from unseen in the supervised\nmanner. Experiments performed on several datasets show the effectiveness of\nASG.\n", "versions": [{"version": "v1", "created": "Wed, 24 May 2017 12:27:06 GMT"}, {"version": "v2", "created": "Sat, 17 Jun 2017 09:08:34 GMT"}], "update_date": "2017-06-20", "authors_parsed": [["Yu", "Yang", ""], ["Qu", "Wei-Yang", ""], ["Li", "Nan", ""], ["Guo", "Zimin", ""]]}, {"id": "1705.08736", "submitter": "Sami Remes", "authors": "Sami Remes, Markus Heinonen, Samuel Kaski", "title": "Non-Stationary Spectral Kernels", "comments": "16 pages, 5 figures", "journal-ref": "Advances in Neural Information Processing Systems 30 (2017),\n  4642-4651", "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose non-stationary spectral kernels for Gaussian process regression.\nWe propose to model the spectral density of a non-stationary kernel function as\na mixture of input-dependent Gaussian process frequency density surfaces. We\nsolve the generalised Fourier transform with such a model, and present a family\nof non-stationary and non-monotonic kernels that can learn input-dependent and\npotentially long-range, non-monotonic covariances between inputs. We derive\nefficient inference using model whitening and marginalized posterior, and show\nwith case studies that these kernels are necessary when modelling even rather\nsimple time series, image or geospatial data with non-stationary\ncharacteristics.\n", "versions": [{"version": "v1", "created": "Wed, 24 May 2017 12:58:58 GMT"}], "update_date": "2019-09-25", "authors_parsed": [["Remes", "Sami", ""], ["Heinonen", "Markus", ""], ["Kaski", "Samuel", ""]]}, {"id": "1705.08741", "submitter": "Elad Hoffer", "authors": "Elad Hoffer, Itay Hubara, Daniel Soudry", "title": "Train longer, generalize better: closing the generalization gap in large\n  batch training of neural networks", "comments": null, "journal-ref": "Advances in Neural Information Processing Systems 30 2017; pages\n  1729-1739;\n  http://papers.nips.cc/paper/6770-train-longer-generalize-better-closing-the-generalization-gap-in-large-batch-training-of-neural-networks", "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Background: Deep learning models are typically trained using stochastic\ngradient descent or one of its variants. These methods update the weights using\ntheir gradient, estimated from a small fraction of the training data. It has\nbeen observed that when using large batch sizes there is a persistent\ndegradation in generalization performance - known as the \"generalization gap\"\nphenomena. Identifying the origin of this gap and closing it had remained an\nopen problem.\n  Contributions: We examine the initial high learning rate training phase. We\nfind that the weight distance from its initialization grows logarithmically\nwith the number of weight updates. We therefore propose a \"random walk on\nrandom landscape\" statistical model which is known to exhibit similar\n\"ultra-slow\" diffusion behavior. Following this hypothesis we conducted\nexperiments to show empirically that the \"generalization gap\" stems from the\nrelatively small number of updates rather than the batch size, and can be\ncompletely eliminated by adapting the training regime used. We further\ninvestigate different techniques to train models in the large-batch regime and\npresent a novel algorithm named \"Ghost Batch Normalization\" which enables\nsignificant decrease in the generalization gap without increasing the number of\nupdates. To validate our findings we conduct several additional experiments on\nMNIST, CIFAR-10, CIFAR-100 and ImageNet. Finally, we reassess common practices\nand beliefs concerning training of deep models and suggest they may not be\noptimal to achieve good generalization.\n", "versions": [{"version": "v1", "created": "Wed, 24 May 2017 13:17:27 GMT"}, {"version": "v2", "created": "Mon, 1 Jan 2018 08:49:43 GMT"}], "update_date": "2018-01-03", "authors_parsed": [["Hoffer", "Elad", ""], ["Hubara", "Itay", ""], ["Soudry", "Daniel", ""]]}, {"id": "1705.08804", "submitter": "Sirui Yao", "authors": "Sirui Yao, Bert Huang", "title": "Beyond Parity: Fairness Objectives for Collaborative Filtering", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study fairness in collaborative-filtering recommender systems, which are\nsensitive to discrimination that exists in historical data. Biased data can\nlead collaborative-filtering methods to make unfair predictions for users from\nminority groups. We identify the insufficiency of existing fairness metrics and\npropose four new metrics that address different forms of unfairness. These\nfairness metrics can be optimized by adding fairness terms to the learning\nobjective. Experiments on synthetic and real data show that our new metrics can\nbetter measure fairness than the baseline, and that the fairness objectives\neffectively help reduce unfairness.\n", "versions": [{"version": "v1", "created": "Wed, 24 May 2017 14:52:06 GMT"}, {"version": "v2", "created": "Thu, 30 Nov 2017 21:11:25 GMT"}], "update_date": "2017-12-04", "authors_parsed": [["Yao", "Sirui", ""], ["Huang", "Bert", ""]]}, {"id": "1705.08821", "submitter": "Christos Louizos", "authors": "Christos Louizos, Uri Shalit, Joris Mooij, David Sontag, Richard Zemel\n  and Max Welling", "title": "Causal Effect Inference with Deep Latent-Variable Models", "comments": "Published as a conference paper at NIPS 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning individual-level causal effects from observational data, such as\ninferring the most effective medication for a specific patient, is a problem of\ngrowing importance for policy makers. The most important aspect of inferring\ncausal effects from observational data is the handling of confounders, factors\nthat affect both an intervention and its outcome. A carefully designed\nobservational study attempts to measure all important confounders. However,\neven if one does not have direct access to all confounders, there may exist\nnoisy and uncertain measurement of proxies for confounders. We build on recent\nadvances in latent variable modeling to simultaneously estimate the unknown\nlatent space summarizing the confounders and the causal effect. Our method is\nbased on Variational Autoencoders (VAE) which follow the causal structure of\ninference with proxies. We show our method is significantly more robust than\nexisting methods, and matches the state-of-the-art on previous benchmarks\nfocused on individual treatment effects.\n", "versions": [{"version": "v1", "created": "Wed, 24 May 2017 15:33:48 GMT"}, {"version": "v2", "created": "Mon, 6 Nov 2017 13:09:24 GMT"}], "update_date": "2017-11-07", "authors_parsed": [["Louizos", "Christos", ""], ["Shalit", "Uri", ""], ["Mooij", "Joris", ""], ["Sontag", "David", ""], ["Zemel", "Richard", ""], ["Welling", "Max", ""]]}, {"id": "1705.08826", "submitter": "Yanbo Fan", "authors": "Yanbo Fan, Siwei Lyu, Yiming Ying, Bao-Gang Hu", "title": "Learning with Average Top-k Loss", "comments": "18 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we introduce the {\\em average top-$k$} (\\atk) loss as a new\naggregate loss for supervised learning, which is the average over the $k$\nlargest individual losses over a training dataset. We show that the \\atk loss\nis a natural generalization of the two widely used aggregate losses, namely the\naverage loss and the maximum loss, but can combine their advantages and\nmitigate their drawbacks to better adapt to different data distributions.\nFurthermore, it remains a convex function over all individual losses, which can\nlead to convex optimization problems that can be solved effectively with\nconventional gradient-based methods. We provide an intuitive interpretation of\nthe \\atk loss based on its equivalent effect on the continuous individual loss\nfunctions, suggesting that it can reduce the penalty on correctly classified\ndata. We further give a learning theory analysis of \\matk learning on the\nclassification calibration of the \\atk loss and the error bounds of \\atk-SVM.\nWe demonstrate the applicability of minimum average top-$k$ learning for binary\nclassification and regression using synthetic and real datasets.\n", "versions": [{"version": "v1", "created": "Wed, 24 May 2017 15:49:55 GMT"}, {"version": "v2", "created": "Wed, 20 Dec 2017 07:16:13 GMT"}], "update_date": "2017-12-21", "authors_parsed": [["Fan", "Yanbo", ""], ["Lyu", "Siwei", ""], ["Ying", "Yiming", ""], ["Hu", "Bao-Gang", ""]]}, {"id": "1705.08841", "submitter": "Diane Bouchacourt", "authors": "Diane Bouchacourt, Ryota Tomioka, Sebastian Nowozin", "title": "Multi-Level Variational Autoencoder: Learning Disentangled\n  Representations from Grouped Observations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We would like to learn a representation of the data which decomposes an\nobservation into factors of variation which we can independently control.\nSpecifically, we want to use minimal supervision to learn a latent\nrepresentation that reflects the semantics behind a specific grouping of the\ndata, where within a group the samples share a common factor of variation. For\nexample, consider a collection of face images grouped by identity. We wish to\nanchor the semantics of the grouping into a relevant and disentangled\nrepresentation that we can easily exploit. However, existing deep probabilistic\nmodels often assume that the observations are independent and identically\ndistributed. We present the Multi-Level Variational Autoencoder (ML-VAE), a new\ndeep probabilistic model for learning a disentangled representation of a set of\ngrouped observations. The ML-VAE separates the latent representation into\nsemantically meaningful parts by working both at the group level and the\nobservation level, while retaining efficient test-time inference. Quantitative\nand qualitative evaluations show that the ML-VAE model (i) learns a\nsemantically meaningful disentanglement of grouped data, (ii) enables\nmanipulation of the latent representation, and (iii) generalises to unseen\ngroups.\n", "versions": [{"version": "v1", "created": "Wed, 24 May 2017 16:14:54 GMT"}], "update_date": "2017-05-25", "authors_parsed": [["Bouchacourt", "Diane", ""], ["Tomioka", "Ryota", ""], ["Nowozin", "Sebastian", ""]]}, {"id": "1705.08848", "submitter": "Nicolas Courty", "authors": "Nicolas Courty, R\\'emi Flamary, Amaury Habrard and Alain Rakotomamonjy", "title": "Joint Distribution Optimal Transportation for Domain Adaptation", "comments": "Accepted for publication at NIPS 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper deals with the unsupervised domain adaptation problem, where one\nwants to estimate a prediction function $f$ in a given target domain without\nany labeled sample by exploiting the knowledge available from a source domain\nwhere labels are known. Our work makes the following assumption: there exists a\nnon-linear transformation between the joint feature/label space distributions\nof the two domain $\\mathcal{P}_s$ and $\\mathcal{P}_t$. We propose a solution of\nthis problem with optimal transport, that allows to recover an estimated target\n$\\mathcal{P}^f_t=(X,f(X))$ by optimizing simultaneously the optimal coupling\nand $f$. We show that our method corresponds to the minimization of a bound on\nthe target error, and provide an efficient algorithmic solution, for which\nconvergence is proved. The versatility of our approach, both in terms of class\nof hypothesis or loss functions is demonstrated with real world classification\nand regression problems, for which we reach or surpass state-of-the-art\nresults.\n", "versions": [{"version": "v1", "created": "Wed, 24 May 2017 16:34:41 GMT"}, {"version": "v2", "created": "Sun, 22 Oct 2017 12:16:35 GMT"}], "update_date": "2017-10-24", "authors_parsed": [["Courty", "Nicolas", ""], ["Flamary", "R\u00e9mi", ""], ["Habrard", "Amaury", ""], ["Rakotomamonjy", "Alain", ""]]}, {"id": "1705.08850", "submitter": "Abhishek Kumar", "authors": "Abhishek Kumar, Prasanna Sattigeri, P. Thomas Fletcher", "title": "Semi-supervised Learning with GANs: Manifold Invariance with Improved\n  Inference", "comments": "NIPS 2017 accepted version, including appendix", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Semi-supervised learning methods using Generative Adversarial Networks (GANs)\nhave shown promising empirical success recently. Most of these methods use a\nshared discriminator/classifier which discriminates real examples from fake\nwhile also predicting the class label. Motivated by the ability of the GANs\ngenerator to capture the data manifold well, we propose to estimate the tangent\nspace to the data manifold using GANs and employ it to inject invariances into\nthe classifier. In the process, we propose enhancements over existing methods\nfor learning the inverse mapping (i.e., the encoder) which greatly improves in\nterms of semantic similarity of the reconstructed sample with the input sample.\nWe observe considerable empirical gains in semi-supervised learning over\nbaselines, particularly in the cases when the number of labeled examples is\nlow. We also provide insights into how fake examples influence the\nsemi-supervised learning procedure.\n", "versions": [{"version": "v1", "created": "Wed, 24 May 2017 16:35:37 GMT"}, {"version": "v2", "created": "Tue, 5 Dec 2017 18:34:17 GMT"}], "update_date": "2018-06-13", "authors_parsed": [["Kumar", "Abhishek", ""], ["Sattigeri", "Prasanna", ""], ["Fletcher", "P. Thomas", ""]]}, {"id": "1705.08858", "submitter": "Galina Lavrentyeva", "authors": "Galina Lavrentyeva, Sergey Novoselov, Egor Malykh, Alexander Kozlov,\n  Oleg Kudashev and Vadim Shchemelinin", "title": "Audio-replay attack detection countermeasures", "comments": "11 pages, 3 figures, accepted for Specom 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents the Speech Technology Center (STC) replay attack\ndetection systems proposed for Automatic Speaker Verification Spoofing and\nCountermeasures Challenge 2017. In this study we focused on comparison of\ndifferent spoofing detection approaches. These were GMM based methods, high\nlevel features extraction with simple classifier and deep learning frameworks.\nExperiments performed on the development and evaluation parts of the challenge\ndataset demonstrated stable efficiency of deep learning approaches in case of\nchanging acoustic conditions. At the same time SVM classifier with high level\nfeatures provided a substantial input in the efficiency of the resulting STC\nsystems according to the fusion systems results.\n", "versions": [{"version": "v1", "created": "Wed, 24 May 2017 16:48:03 GMT"}], "update_date": "2017-05-25", "authors_parsed": [["Lavrentyeva", "Galina", ""], ["Novoselov", "Sergey", ""], ["Malykh", "Egor", ""], ["Kozlov", "Alexander", ""], ["Kudashev", "Oleg", ""], ["Shchemelinin", "Vadim", ""]]}, {"id": "1705.08865", "submitter": "Galina Lavrentyeva", "authors": "Galina Lavrentyeva, Sergey Novoselov and Konstantin Simonchik", "title": "Anti-spoofing Methods for Automatic SpeakerVerification System", "comments": "12 pages, 0 figures, published in Springer Communications in Computer\n  and Information Science (CCIS) vol. 661", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Growing interest in automatic speaker verification (ASV)systems has lead to\nsignificant quality improvement of spoofing attackson them. Many research works\nconfirm that despite the low equal er-ror rate (EER) ASV systems are still\nvulnerable to spoofing attacks. Inthis work we overview different acoustic\nfeature spaces and classifiersto determine reliable and robust countermeasures\nagainst spoofing at-tacks. We compared several spoofing detection systems,\npresented so far,on the development and evaluation datasets of the Automatic\nSpeakerVerification Spoofing and Countermeasures (ASVspoof) Challenge\n2015.Experimental results presented in this paper demonstrate that the useof\nmagnitude and phase information combination provides a substantialinput into\nthe efficiency of the spoofing detection systems. Also wavelet-based features\nshow impressive results in terms of equal error rate. Inour overview we compare\nspoofing performance for systems based on dif-ferent classifiers. Comparison\nresults demonstrate that the linear SVMclassifier outperforms the conventional\nGMM approach. However, manyresearchers inspired by the great success of deep\nneural networks (DNN)approaches in the automatic speech recognition, applied\nDNN in thespoofing detection task and obtained quite low EER for known and\nun-known type of spoofing attacks.\n", "versions": [{"version": "v1", "created": "Wed, 24 May 2017 16:58:03 GMT"}], "update_date": "2017-05-25", "authors_parsed": [["Lavrentyeva", "Galina", ""], ["Novoselov", "Sergey", ""], ["Simonchik", "Konstantin", ""]]}, {"id": "1705.08868", "submitter": "Aditya Grover", "authors": "Aditya Grover, Manik Dhar, Stefano Ermon", "title": "Flow-GAN: Combining Maximum Likelihood and Adversarial Learning in\n  Generative Models", "comments": "AAAI 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Adversarial learning of probabilistic models has recently emerged as a\npromising alternative to maximum likelihood. Implicit models such as generative\nadversarial networks (GAN) often generate better samples compared to explicit\nmodels trained by maximum likelihood. Yet, GANs sidestep the characterization\nof an explicit density which makes quantitative evaluations challenging. To\nbridge this gap, we propose Flow-GANs, a generative adversarial network for\nwhich we can perform exact likelihood evaluation, thus supporting both\nadversarial and maximum likelihood training. When trained adversarially,\nFlow-GANs generate high-quality samples but attain extremely poor\nlog-likelihood scores, inferior even to a mixture model memorizing the training\ndata; the opposite is true when trained by maximum likelihood. Results on MNIST\nand CIFAR-10 demonstrate that hybrid training can attain high held-out\nlikelihoods while retaining visual fidelity in the generated samples.\n", "versions": [{"version": "v1", "created": "Wed, 24 May 2017 17:11:25 GMT"}, {"version": "v2", "created": "Wed, 3 Jan 2018 21:47:01 GMT"}], "update_date": "2018-01-08", "authors_parsed": [["Grover", "Aditya", ""], ["Dhar", "Manik", ""], ["Ermon", "Stefano", ""]]}, {"id": "1705.08881", "submitter": "Jun Li", "authors": "Jun Li, Yongjun Chen, Lei Cai, Ian Davidson, Shuiwang Ji", "title": "Dense Transformer Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The key idea of current deep learning methods for dense prediction is to\napply a model on a regular patch centered on each pixel to make pixel-wise\npredictions. These methods are limited in the sense that the patches are\ndetermined by network architecture instead of learned from data. In this work,\nwe propose the dense transformer networks, which can learn the shapes and sizes\nof patches from data. The dense transformer networks employ an encoder-decoder\narchitecture, and a pair of dense transformer modules are inserted into each of\nthe encoder and decoder paths. The novelty of this work is that we provide\ntechnical solutions for learning the shapes and sizes of patches from data and\nefficiently restoring the spatial correspondence required for dense prediction.\nThe proposed dense transformer modules are differentiable, thus the entire\nnetwork can be trained. We apply the proposed networks on natural and\nbiological image segmentation tasks and show superior performance is achieved\nin comparison to baseline methods.\n", "versions": [{"version": "v1", "created": "Wed, 24 May 2017 17:50:32 GMT"}, {"version": "v2", "created": "Thu, 8 Jun 2017 00:10:04 GMT"}], "update_date": "2017-06-09", "authors_parsed": [["Li", "Jun", ""], ["Chen", "Yongjun", ""], ["Cai", "Lei", ""], ["Davidson", "Ian", ""], ["Ji", "Shuiwang", ""]]}, {"id": "1705.08918", "submitter": "Liang Zhao", "authors": "Liang Zhao, Yang Wang, Yi Yang, Wei Xu", "title": "Unsupervised Learning Layers for Video Analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents two unsupervised learning layers (UL layers) for\nlabel-free video analysis: one for fully connected layers, and the other for\nconvolutional ones. The proposed UL layers can play two roles: they can be the\ncost function layer for providing global training signal; meanwhile they can be\nadded to any regular neural network layers for providing local training signals\nand combined with the training signals backpropagated from upper layers for\nextracting both slow and fast changing features at layers of different depths.\nTherefore, the UL layers can be used in either pure unsupervised or\nsemi-supervised settings. Both a closed-form solution and an online learning\nalgorithm for two UL layers are provided. Experiments with unlabeled synthetic\nand real-world videos demonstrated that the neural networks equipped with UL\nlayers and trained with the proposed online learning algorithm can extract\nshape and motion information from video sequences of moving objects. The\nexperiments demonstrated the potential applications of UL layers and online\nlearning algorithm to head orientation estimation and moving object\nlocalization.\n", "versions": [{"version": "v1", "created": "Wed, 24 May 2017 18:22:41 GMT"}], "update_date": "2017-05-26", "authors_parsed": [["Zhao", "Liang", ""], ["Wang", "Yang", ""], ["Yang", "Yi", ""], ["Xu", "Wei", ""]]}, {"id": "1705.08921", "submitter": "Efr\\'en Cruz Cort\\'es", "authors": "Efr\\'en Cruz Cort\\'es, Clayton Scott", "title": "Consistent Kernel Density Estimation with Non-Vanishing Bandwidth", "comments": "17 pages, updated abstract", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Consistency of the kernel density estimator requires that the kernel\nbandwidth tends to zero as the sample size grows. In this paper we investigate\nthe question of whether consistency is possible when the bandwidth is fixed, if\nwe consider a more general class of weighted KDEs. To answer this question in\nthe affirmative, we introduce the fixed-bandwidth KDE (fbKDE), obtained by\nsolving a quadratic program, and prove that it consistently estimates any\ncontinuous square-integrable density. We also establish rates of convergence\nfor the fbKDE with radial kernels and the box kernel under appropriate\nsmoothness assumptions. Furthermore, in an experimental study we demonstrate\nthat the fbKDE compares favorably to the standard KDE and the previously\nproposed variable bandwidth KDE.\n", "versions": [{"version": "v1", "created": "Wed, 24 May 2017 18:30:45 GMT"}, {"version": "v2", "created": "Mon, 29 May 2017 16:53:22 GMT"}], "update_date": "2017-05-30", "authors_parsed": [["Cort\u00e9s", "Efr\u00e9n Cruz", ""], ["Scott", "Clayton", ""]]}, {"id": "1705.08922", "submitter": "Huizi Mao", "authors": "Huizi Mao, Song Han, Jeff Pool, Wenshuo Li, Xingyu Liu, Yu Wang,\n  William J. Dally", "title": "Exploring the Regularity of Sparse Structure in Convolutional Neural\n  Networks", "comments": "submitted to NIPS 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Sparsity helps reduce the computational complexity of deep neural networks by\nskipping zeros. Taking advantage of sparsity is listed as a high priority in\nnext generation DNN accelerators such as TPU. The structure of sparsity, i.e.,\nthe granularity of pruning, affects the efficiency of hardware accelerator\ndesign as well as the prediction accuracy. Coarse-grained pruning creates\nregular sparsity patterns, making it more amenable for hardware acceleration\nbut more challenging to maintain the same accuracy. In this paper we\nquantitatively measure the trade-off between sparsity regularity and prediction\naccuracy, providing insights in how to maintain accuracy while having more a\nmore structured sparsity pattern. Our experimental results show that\ncoarse-grained pruning can achieve a sparsity ratio similar to unstructured\npruning without loss of accuracy. Moreover, due to the index saving effect,\ncoarse-grained pruning is able to obtain a better compression ratio than\nfine-grained sparsity at the same accuracy threshold. Based on the recent\nsparse convolutional neural network accelerator (SCNN), our experiments further\ndemonstrate that coarse-grained sparsity saves about 2x the memory references\ncompared to fine-grained sparsity. Since memory reference is more than two\norders of magnitude more expensive than arithmetic operations, the regularity\nof sparse structure leads to more efficient hardware design.\n", "versions": [{"version": "v1", "created": "Wed, 24 May 2017 18:35:41 GMT"}, {"version": "v2", "created": "Thu, 1 Jun 2017 20:34:06 GMT"}, {"version": "v3", "created": "Mon, 5 Jun 2017 00:22:25 GMT"}], "update_date": "2017-06-06", "authors_parsed": [["Mao", "Huizi", ""], ["Han", "Song", ""], ["Pool", "Jeff", ""], ["Li", "Wenshuo", ""], ["Liu", "Xingyu", ""], ["Wang", "Yu", ""], ["Dally", "William J.", ""]]}, {"id": "1705.08931", "submitter": "Jaan Altosaar", "authors": "Jaan Altosaar, Rajesh Ranganath, David M. Blei", "title": "Proximity Variational Inference", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Variational inference is a powerful approach for approximate posterior\ninference. However, it is sensitive to initialization and can be subject to\npoor local optima. In this paper, we develop proximity variational inference\n(PVI). PVI is a new method for optimizing the variational objective that\nconstrains subsequent iterates of the variational parameters to robustify the\noptimization path. Consequently, PVI is less sensitive to initialization and\noptimization quirks and finds better local optima. We demonstrate our method on\nthree proximity statistics. We study PVI on a Bernoulli factor model and\nsigmoid belief network with both real and synthetic data and compare to\ndeterministic annealing (Katahira et al., 2008). We highlight the flexibility\nof PVI by designing a proximity statistic for Bayesian deep learning models\nsuch as the variational autoencoder (Kingma and Welling, 2014; Rezende et al.,\n2014). Empirically, we show that PVI consistently finds better local optima and\ngives better predictive performance.\n", "versions": [{"version": "v1", "created": "Wed, 24 May 2017 19:06:14 GMT"}], "update_date": "2017-05-26", "authors_parsed": [["Altosaar", "Jaan", ""], ["Ranganath", "Rajesh", ""], ["Blei", "David M.", ""]]}, {"id": "1705.08971", "submitter": "Scott Cheng-Hsin Yang", "authors": "Scott Cheng-Hsin Yang, Yue Yu, Arash Givchi, Pei Wang, Wai Keen Vong,\n  and Patrick Shafto", "title": "Optimal Cooperative Inference", "comments": "16 pages (5 pages of Supplementary Material), 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cooperative transmission of data fosters rapid accumulation of knowledge by\nefficiently combining experiences across learners. Although well studied in\nhuman learning and increasingly in machine learning, we lack formal frameworks\nthrough which we may reason about the benefits and limitations of cooperative\ninference. We present such a framework. We introduce novel indices for\nmeasuring the effectiveness of probabilistic and cooperative information\ntransmission. We relate our indices to the well-known Teaching Dimension in\ndeterministic settings. We prove conditions under which optimal cooperative\ninference can be achieved, including a representation theorem that constrains\nthe form of inductive biases for learners optimized for cooperative inference.\nWe conclude by demonstrating how these principles may inform the design of\nmachine learning algorithms and discuss implications for human and machine\nlearning.\n", "versions": [{"version": "v1", "created": "Wed, 24 May 2017 21:42:00 GMT"}, {"version": "v2", "created": "Thu, 25 Jan 2018 19:51:57 GMT"}], "update_date": "2018-01-29", "authors_parsed": [["Yang", "Scott Cheng-Hsin", ""], ["Yu", "Yue", ""], ["Givchi", "Arash", ""], ["Wang", "Pei", ""], ["Vong", "Wai Keen", ""], ["Shafto", "Patrick", ""]]}, {"id": "1705.08982", "submitter": "Shuai Xiao", "authors": "Shuai Xiao, Junchi Yan, Stephen M. Chu, Xiaokang Yang, Hongyuan Zha", "title": "Modeling The Intensity Function Of Point Process Via Recurrent Neural\n  Networks", "comments": "Accepted at Thirty-First AAAI Conference on Artificial Intelligence\n  (AAAI17)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Event sequence, asynchronously generated with random timestamp, is ubiquitous\namong applications. The precise and arbitrary timestamp can carry important\nclues about the underlying dynamics, and has lent the event data fundamentally\ndifferent from the time-series whereby series is indexed with fixed and equal\ntime interval. One expressive mathematical tool for modeling event is point\nprocess. The intensity functions of many point processes involve two\ncomponents: the background and the effect by the history. Due to its inherent\nspontaneousness, the background can be treated as a time series while the other\nneed to handle the history events. In this paper, we model the background by a\nRecurrent Neural Network (RNN) with its units aligned with time series indexes\nwhile the history effect is modeled by another RNN whose units are aligned with\nasynchronous events to capture the long-range dynamics. The whole model with\nevent type and timestamp prediction output layers can be trained end-to-end.\nOur approach takes an RNN perspective to point process, and models its\nbackground and history effect. For utility, our method allows a black-box\ntreatment for modeling the intensity which is often a pre-defined parametric\nform in point processes. Meanwhile end-to-end training opens the venue for\nreusing existing rich techniques in deep network for point process modeling. We\napply our model to the predictive maintenance problem using a log dataset by\nmore than 1000 ATMs from a global bank headquartered in North America.\n", "versions": [{"version": "v1", "created": "Wed, 24 May 2017 22:23:14 GMT"}], "update_date": "2017-05-26", "authors_parsed": [["Xiao", "Shuai", ""], ["Yan", "Junchi", ""], ["Chu", "Stephen M.", ""], ["Yang", "Xiaokang", ""], ["Zha", "Hongyuan", ""]]}, {"id": "1705.08991", "submitter": "Shuang Liu", "authors": "Shuang Liu, Olivier Bousquet, Kamalika Chaudhuri", "title": "Approximation and Convergence Properties of Generative Adversarial\n  Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generative adversarial networks (GAN) approximate a target data distribution\nby jointly optimizing an objective function through a \"two-player game\" between\na generator and a discriminator. Despite their empirical success, however, two\nvery basic questions on how well they can approximate the target distribution\nremain unanswered. First, it is not known how restricting the discriminator\nfamily affects the approximation quality. Second, while a number of different\nobjective functions have been proposed, we do not understand when convergence\nto the global minima of the objective function leads to convergence to the\ntarget distribution under various notions of distributional convergence.\n  In this paper, we address these questions in a broad and unified setting by\ndefining a notion of adversarial divergences that includes a number of recently\nproposed objective functions. We show that if the objective function is an\nadversarial divergence with some additional conditions, then using a restricted\ndiscriminator family has a moment-matching effect. Additionally, we show that\nfor objective functions that are strict adversarial divergences, convergence in\nthe objective function implies weak convergence, thus generalizing previous\nresults.\n", "versions": [{"version": "v1", "created": "Wed, 24 May 2017 22:53:22 GMT"}], "update_date": "2017-05-30", "authors_parsed": [["Liu", "Shuang", ""], ["Bousquet", "Olivier", ""], ["Chaudhuri", "Kamalika", ""]]}, {"id": "1705.08997", "submitter": "Farhan Tejani", "authors": "Himanshu Sahni, Saurabh Kumar, Farhan Tejani, Yannick Schroecker,\n  Charles Isbell", "title": "State Space Decomposition and Subgoal Creation for Transfer in Deep\n  Reinforcement Learning", "comments": "5 pages, 6 figures; 3rd Multidisciplinary Conference on Reinforcement\n  Learning and Decision Making (RLDM 2017), Ann Arbor, Michigan", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Typical reinforcement learning (RL) agents learn to complete tasks specified\nby reward functions tailored to their domain. As such, the policies they learn\ndo not generalize even to similar domains. To address this issue, we develop a\nframework through which a deep RL agent learns to generalize policies from\nsmaller, simpler domains to more complex ones using a recurrent attention\nmechanism. The task is presented to the agent as an image and an instruction\nspecifying the goal. This meta-controller guides the agent towards its goal by\ndesigning a sequence of smaller subtasks on the part of the state space within\nthe attention, effectively decomposing it. As a baseline, we consider a setup\nwithout attention as well. Our experiments show that the meta-controller learns\nto create subgoals within the attention.\n", "versions": [{"version": "v1", "created": "Wed, 24 May 2017 23:19:44 GMT"}], "update_date": "2017-05-26", "authors_parsed": [["Sahni", "Himanshu", ""], ["Kumar", "Saurabh", ""], ["Tejani", "Farhan", ""], ["Schroecker", "Yannick", ""], ["Isbell", "Charles", ""]]}, {"id": "1705.09011", "submitter": "Han Zhao", "authors": "Han Zhao, Zhenyao Zhu, Junjie Hu, Adam Coates, Geoff Gordon", "title": "Principled Hybrids of Generative and Discriminative Domain Adaptation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a probabilistic framework for domain adaptation that blends both\ngenerative and discriminative modeling in a principled way. Under this\nframework, generative and discriminative models correspond to specific choices\nof the prior over parameters. This provides us a very general way to\ninterpolate between generative and discriminative extremes through different\nchoices of priors. By maximizing both the marginal and the conditional\nlog-likelihoods, models derived from this framework can use both labeled\ninstances from the source domain as well as unlabeled instances from both\nsource and target domains. Under this framework, we show that the popular\nreconstruction loss of autoencoder corresponds to an upper bound of the\nnegative marginal log-likelihoods of unlabeled instances, where marginal\ndistributions are given by proper kernel density estimations. This provides a\nway to interpret the empirical success of autoencoders in domain adaptation and\nsemi-supervised learning. We instantiate our framework using neural networks,\nand build a concrete model, DAuto. Empirically, we demonstrate the\neffectiveness of DAuto on text, image and speech datasets, showing that it\noutperforms related competitors when domain adaptation is possible.\n", "versions": [{"version": "v1", "created": "Thu, 25 May 2017 01:02:16 GMT"}, {"version": "v2", "created": "Fri, 27 Oct 2017 19:29:31 GMT"}], "update_date": "2017-10-31", "authors_parsed": [["Zhao", "Han", ""], ["Zhu", "Zhenyao", ""], ["Hu", "Junjie", ""], ["Coates", "Adam", ""], ["Gordon", "Geoff", ""]]}, {"id": "1705.09021", "submitter": "Yu Sun", "authors": "Yongqiang Huang and Yu Sun", "title": "Learning to Pour", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Pouring is a simple task people perform daily. It is the second most\nfrequently executed motion in cooking scenarios, after pick-and-place. We\npresent a pouring trajectory generation approach, which uses force feedback\nfrom the cup to determine the future velocity of pouring. The approach uses\nrecurrent neural networks as its building blocks. We collected the pouring\ndemonstrations which we used for training. To test our approach in simulation,\nwe also created and trained a force estimation system. The simulated\nexperiments show that the system is able to generalize to single unseen element\nof the pouring characteristics.\n", "versions": [{"version": "v1", "created": "Thu, 25 May 2017 01:54:58 GMT"}], "update_date": "2017-05-26", "authors_parsed": [["Huang", "Yongqiang", ""], ["Sun", "Yu", ""]]}, {"id": "1705.09026", "submitter": "Bert Huang", "authors": "Walid Chaabene and Bert Huang", "title": "Best-Choice Edge Grafting for Efficient Structure Learning of Markov\n  Random Fields", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Incremental methods for structure learning of pairwise Markov random fields\n(MRFs), such as grafting, improve scalability by avoiding inference over the\nentire feature space in each optimization step. Instead, inference is performed\nover an incrementally grown active set of features. In this paper, we address\nkey computational bottlenecks that current incremental techniques still suffer\nby introducing best-choice edge grafting, an incremental, structured method\nthat activates edges as groups of features in a streaming setting. The method\nuses a reservoir of edges that satisfy an activation condition, approximating\nthe search for the optimal edge to activate. It also reorganizes the search\nspace using search-history and structure heuristics. Experiments show a\nsignificant speedup for structure learning and a controllable trade-off between\nthe speed and quality of learning.\n", "versions": [{"version": "v1", "created": "Thu, 25 May 2017 02:28:53 GMT"}, {"version": "v2", "created": "Sun, 20 May 2018 01:28:19 GMT"}], "update_date": "2018-05-22", "authors_parsed": [["Chaabene", "Walid", ""], ["Huang", "Bert", ""]]}, {"id": "1705.09037", "submitter": "Tao Lei", "authors": "Tao Lei, Wengong Jin, Regina Barzilay and Tommi Jaakkola", "title": "Deriving Neural Architectures from Sequence and Graph Kernels", "comments": "extended version of ICML 2017 camera ready", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The design of neural architectures for structured objects is typically guided\nby experimental insights rather than a formal process. In this work, we appeal\nto kernels over combinatorial structures, such as sequences and graphs, to\nderive appropriate neural operations. We introduce a class of deep recurrent\nneural operations and formally characterize their associated kernel spaces. Our\nrecurrent modules compare the input to virtual reference objects (cf. filters\nin CNN) via the kernels. Similar to traditional neural operations, these\nreference objects are parameterized and directly optimized in end-to-end\ntraining. We empirically evaluate the proposed class of neural architectures on\nstandard applications such as language modeling and molecular graph regression,\nachieving state-of-the-art results across these applications.\n", "versions": [{"version": "v1", "created": "Thu, 25 May 2017 03:58:10 GMT"}, {"version": "v2", "created": "Wed, 14 Jun 2017 14:34:24 GMT"}, {"version": "v3", "created": "Mon, 30 Oct 2017 13:56:23 GMT"}], "update_date": "2017-10-31", "authors_parsed": [["Lei", "Tao", ""], ["Jin", "Wengong", ""], ["Barzilay", "Regina", ""], ["Jaakkola", "Tommi", ""]]}, {"id": "1705.09050", "submitter": "Mohamed Aslan A.", "authors": "Mohamed Aslan, Ashraf Matrawy", "title": "A Clustering-based Consistency Adaptation Strategy for Distributed SDN\n  Controllers", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Distributed controllers are oftentimes used in large-scale SDN deployments\nwhere they run a myriad of network applications simultaneously. Such\napplications could have different consistency and availability preferences.\nThese controllers need to communicate via east/west interfaces in order to\nsynchronize their state information. The consistency and the availability of\nthe distributed state information are governed by an underlying consistency\nmodel. Earlier, we suggested the use of adaptively-consistent controllers that\ncan autonomously tune their consistency parameters in order to meet the\nperformance requirements of a certain application. In this paper, we examine\nthe feasibility of employing adaptive controllers that are built on-top of\ntunable consistency models similar to that of Apache Cassandra. We present an\nadaptation strategy that uses clustering techniques (sequential k-means and\nincremental k-means) in order to map a given application performance indicator\ninto a feasible consistency level that can be used with the underlying tunable\nconsistency model. In the cases that we modeled and tested, our results show\nthat in the case of sequential k-means, with a reasonable number of clusters\n(>= 50), a plausible mapping (low RMSE) could be estimated between the\napplication performance indicators and the consistency level indicator. In the\ncase of incremental k-means, the results also showed that a plausible mapping\n(low RMSE) could be estimated using a similar number of clusters (>= 50) by\nusing a small threshold (~$ 0.01).\n", "versions": [{"version": "v1", "created": "Thu, 25 May 2017 05:28:57 GMT"}], "update_date": "2017-05-26", "authors_parsed": [["Aslan", "Mohamed", ""], ["Matrawy", "Ashraf", ""]]}, {"id": "1705.09055", "submitter": "Aditya Menon", "authors": "Aditya Krishna Menon and Robert C. Williamson", "title": "The cost of fairness in classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of learning classifiers with a fairness constraint, with\nthree main contributions towards the goal of quantifying the problem's inherent\ntradeoffs. First, we relate two existing fairness measures to cost-sensitive\nrisks. Second, we show that for cost-sensitive classification and fairness\nmeasures, the optimal classifier is an instance-dependent thresholding of the\nclass-probability function. Third, we show how the tradeoff between accuracy\nand fairness is determined by the alignment between the class-probabilities for\nthe target and sensitive features. Underpinning our analysis is a general\nframework that casts the problem of learning with a fairness requirement as one\nof minimising the difference of two statistical risks.\n", "versions": [{"version": "v1", "created": "Thu, 25 May 2017 05:56:08 GMT"}], "update_date": "2017-05-26", "authors_parsed": [["Menon", "Aditya Krishna", ""], ["Williamson", "Robert C.", ""]]}, {"id": "1705.09056", "submitter": "Xiangru Lian", "authors": "Xiangru Lian, Ce Zhang, Huan Zhang, Cho-Jui Hsieh, Wei Zhang, Ji Liu", "title": "Can Decentralized Algorithms Outperform Centralized Algorithms? A Case\n  Study for Decentralized Parallel Stochastic Gradient Descent", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.DC cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most distributed machine learning systems nowadays, including TensorFlow and\nCNTK, are built in a centralized fashion. One bottleneck of centralized\nalgorithms lies on high communication cost on the central node. Motivated by\nthis, we ask, can decentralized algorithms be faster than its centralized\ncounterpart?\n  Although decentralized PSGD (D-PSGD) algorithms have been studied by the\ncontrol community, existing analysis and theory do not show any advantage over\ncentralized PSGD (C-PSGD) algorithms, simply assuming the application scenario\nwhere only the decentralized network is available. In this paper, we study a\nD-PSGD algorithm and provide the first theoretical analysis that indicates a\nregime in which decentralized algorithms might outperform centralized\nalgorithms for distributed stochastic gradient descent. This is because D-PSGD\nhas comparable total computational complexities to C-PSGD but requires much\nless communication cost on the busiest node. We further conduct an empirical\nstudy to validate our theoretical analysis across multiple frameworks (CNTK and\nTorch), different network configurations, and computation platforms up to 112\nGPUs. On network configurations with low bandwidth or high latency, D-PSGD can\nbe up to one order of magnitude faster than its well-optimized centralized\ncounterparts.\n", "versions": [{"version": "v1", "created": "Thu, 25 May 2017 05:58:17 GMT"}, {"version": "v2", "created": "Fri, 26 May 2017 00:22:51 GMT"}, {"version": "v3", "created": "Sat, 10 Jun 2017 05:08:21 GMT"}, {"version": "v4", "created": "Fri, 21 Jul 2017 13:50:26 GMT"}, {"version": "v5", "created": "Mon, 11 Sep 2017 04:21:43 GMT"}], "update_date": "2017-09-12", "authors_parsed": [["Lian", "Xiangru", ""], ["Zhang", "Ce", ""], ["Zhang", "Huan", ""], ["Hsieh", "Cho-Jui", ""], ["Zhang", "Wei", ""], ["Liu", "Ji", ""]]}, {"id": "1705.09064", "submitter": "Dongyu Meng", "authors": "Dongyu Meng, Hao Chen", "title": "MagNet: a Two-Pronged Defense against Adversarial Examples", "comments": "Accepted at the ACM Conference on Computer and Communications\n  Security (CCS), 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning has shown promising results on hard perceptual problems in\nrecent years. However, deep learning systems are found to be vulnerable to\nsmall adversarial perturbations that are nearly imperceptible to human. Such\nspecially crafted perturbations cause deep learning systems to output incorrect\ndecisions, with potentially disastrous consequences. These vulnerabilities\nhinder the deployment of deep learning systems where safety or security is\nimportant. Attempts to secure deep learning systems either target specific\nattacks or have been shown to be ineffective.\n  In this paper, we propose MagNet, a framework for defending neural network\nclassifiers against adversarial examples. MagNet does not modify the protected\nclassifier or know the process for generating adversarial examples. MagNet\nincludes one or more separate detector networks and a reformer network.\nDifferent from previous work, MagNet learns to differentiate between normal and\nadversarial examples by approximating the manifold of normal examples. Since it\ndoes not rely on any process for generating adversarial examples, it has\nsubstantial generalization power. Moreover, MagNet reconstructs adversarial\nexamples by moving them towards the manifold, which is effective for helping\nclassify adversarial examples with small perturbation correctly. We discuss the\nintrinsic difficulty in defending against whitebox attack and propose a\nmechanism to defend against graybox attack. Inspired by the use of randomness\nin cryptography, we propose to use diversity to strengthen MagNet. We show\nempirically that MagNet is effective against most advanced state-of-the-art\nattacks in blackbox and graybox scenarios while keeping false positive rate on\nnormal examples very low.\n", "versions": [{"version": "v1", "created": "Thu, 25 May 2017 06:49:57 GMT"}, {"version": "v2", "created": "Mon, 11 Sep 2017 02:41:15 GMT"}], "update_date": "2017-09-12", "authors_parsed": [["Meng", "Dongyu", ""], ["Chen", "Hao", ""]]}, {"id": "1705.09185", "submitter": "Timur Pekhovsky", "authors": "Timur Pekhovsky, Maxim Korenevsky", "title": "Investigation of Using VAE for i-Vector Speaker Verification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  New system for i-vector speaker recognition based on variational autoencoder\n(VAE) is investigated. VAE is a promising approach for developing accurate deep\nnonlinear generative models of complex data. Experiments show that VAE provides\nspeaker embedding and can be effectively trained in an unsupervised manner. LLR\nestimate for VAE is developed. Experiments on NIST SRE 2010 data demonstrate\nits correctness. Additionally, we show that the performance of VAE-based system\nin the i-vectors space is close to that of the diagonal PLDA. Several\ninteresting results are also observed in the experiments with $\\beta$-VAE. In\nparticular, we found that for $\\beta\\ll 1$, VAE can be trained to capture the\nfeatures of complex input data distributions in an effective way, which is hard\nto obtain in the standard VAE ($\\beta=1$).\n", "versions": [{"version": "v1", "created": "Thu, 25 May 2017 13:59:18 GMT"}], "update_date": "2017-05-26", "authors_parsed": [["Pekhovsky", "Timur", ""], ["Korenevsky", "Maxim", ""]]}, {"id": "1705.09193", "submitter": "Sultan Imangaliyev", "authors": "Sultan Imangaliyev, Monique H. van der Veen, Catherine M. C.\n  Volgenant, Bruno G. Loos, Bart J. F. Keijser, Wim Crielaard, Evgeni Levin", "title": "Classification of Quantitative Light-Induced Fluorescence Images Using\n  Convolutional Neural Network", "comments": "Full version of ICANN 2017 submission", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Images are an important data source for diagnosis and treatment of oral\ndiseases. The manual classification of images may lead to misdiagnosis or\nmistreatment due to subjective errors. In this paper an image classification\nmodel based on Convolutional Neural Network is applied to Quantitative\nLight-induced Fluorescence images. The deep neural network outperforms other\nstate of the art shallow classification models in predicting labels derived\nfrom three different dental plaque assessment scores. The model directly\nbenefits from multi-channel representation of the images resulting in improved\nperformance when, besides the Red colour channel, additional Green and Blue\ncolour channels are used.\n", "versions": [{"version": "v1", "created": "Thu, 25 May 2017 14:21:40 GMT"}], "update_date": "2017-05-26", "authors_parsed": [["Imangaliyev", "Sultan", ""], ["van der Veen", "Monique H.", ""], ["Volgenant", "Catherine M. C.", ""], ["Loos", "Bruno G.", ""], ["Keijser", "Bart J. F.", ""], ["Crielaard", "Wim", ""], ["Levin", "Evgeni", ""]]}, {"id": "1705.09236", "submitter": "Kirthevasan Kandasamy", "authors": "Kirthevasan Kandasamy, Akshay Krishnamurthy, Jeff Schneider and\n  Barnabas Poczos", "title": "Asynchronous Parallel Bayesian Optimisation via Thompson Sampling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We design and analyse variations of the classical Thompson sampling (TS)\nprocedure for Bayesian optimisation (BO) in settings where function evaluations\nare expensive, but can be performed in parallel. Our theoretical analysis shows\nthat a direct application of the sequential Thompson sampling algorithm in\neither synchronous or asynchronous parallel settings yields a surprisingly\npowerful result: making $n$ evaluations distributed among $M$ workers is\nessentially equivalent to performing $n$ evaluations in sequence. Further, by\nmodeling the time taken to complete a function evaluation, we show that, under\na time constraint, asynchronously parallel TS achieves asymptotically lower\nregret than both the synchronous and sequential versions. These results are\ncomplemented by an experimental analysis, showing that asynchronous TS\noutperforms a suite of existing parallel BO algorithms in simulations and in a\nhyper-parameter tuning application in convolutional neural networks. In\naddition to these, the proposed procedure is conceptually and computationally\nmuch simpler than existing work for parallel BO.\n", "versions": [{"version": "v1", "created": "Thu, 25 May 2017 15:46:23 GMT"}], "update_date": "2017-05-26", "authors_parsed": [["Kandasamy", "Kirthevasan", ""], ["Krishnamurthy", "Akshay", ""], ["Schneider", "Jeff", ""], ["Poczos", "Barnabas", ""]]}, {"id": "1705.09269", "submitter": "Joseph Anderson", "authors": "Joseph Anderson", "title": "Geometric Methods for Robust Data Analysis in High Dimension", "comments": "180 Pages, 7 Figures, PhD thesis, Ohio State (2017)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Machine learning and data analysis now finds both scientific and industrial\napplication in biology, chemistry, geology, medicine, and physics. These\napplications rely on large quantities of data gathered from automated sensors\nand user input. Furthermore, the dimensionality of many datasets is extreme:\nmore details are being gathered about single user interactions or sensor\nreadings. All of these applications encounter problems with a common theme: use\nobserved data to make inferences about the world. Our work obtains the first\nprovably efficient algorithms for Independent Component Analysis (ICA) in the\npresence of heavy-tailed data. The main tool in this result is the centroid\nbody (a well-known topic in convex geometry), along with optimization and\nrandom walks for sampling from a convex body. This is the first algorithmic use\nof the centroid body and it is of independent theoretical interest, since it\neffectively replaces the estimation of covariance from samples, and is more\ngenerally accessible.\n  This reduction relies on a non-linear transformation of samples from such an\nintersection of halfspaces (i.e. a simplex) to samples which are approximately\nfrom a linearly transformed product distribution. Through this transformation\nof samples, which can be done efficiently, one can then use an ICA algorithm to\nrecover the vertices of the intersection of halfspaces.\n  Finally, we again use ICA as an algorithmic primitive to construct an\nefficient solution to the widely-studied problem of learning the parameters of\na Gaussian mixture model. Our algorithm again transforms samples from a\nGaussian mixture model into samples which fit into the ICA model and, when\nprocessed by an ICA algorithm, result in recovery of the mixture parameters.\nOur algorithm is effective even when the number of Gaussians in the mixture\ngrows polynomially with the ambient dimension\n", "versions": [{"version": "v1", "created": "Thu, 25 May 2017 17:25:04 GMT"}], "update_date": "2017-05-26", "authors_parsed": [["Anderson", "Joseph", ""]]}, {"id": "1705.09279", "submitter": "Chris J. Maddison", "authors": "Chris J. Maddison, Dieterich Lawson, George Tucker, Nicolas Heess,\n  Mohammad Norouzi, Andriy Mnih, Arnaud Doucet, Yee Whye Teh", "title": "Filtering Variational Objectives", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When used as a surrogate objective for maximum likelihood estimation in\nlatent variable models, the evidence lower bound (ELBO) produces\nstate-of-the-art results. Inspired by this, we consider the extension of the\nELBO to a family of lower bounds defined by a particle filter's estimator of\nthe marginal likelihood, the filtering variational objectives (FIVOs). FIVOs\ntake the same arguments as the ELBO, but can exploit a model's sequential\nstructure to form tighter bounds. We present results that relate the tightness\nof FIVO's bound to the variance of the particle filter's estimator by\nconsidering the generic case of bounds defined as log-transformed likelihood\nestimators. Experimentally, we show that training with FIVO results in\nsubstantial improvements over training the same model architecture with the\nELBO on sequential data.\n", "versions": [{"version": "v1", "created": "Thu, 25 May 2017 17:52:41 GMT"}, {"version": "v2", "created": "Mon, 28 Aug 2017 17:58:51 GMT"}, {"version": "v3", "created": "Sun, 12 Nov 2017 20:38:13 GMT"}], "update_date": "2017-11-15", "authors_parsed": [["Maddison", "Chris J.", ""], ["Lawson", "Dieterich", ""], ["Tucker", "George", ""], ["Heess", "Nicolas", ""], ["Norouzi", "Mohammad", ""], ["Mnih", "Andriy", ""], ["Doucet", "Arnaud", ""], ["Teh", "Yee Whye", ""]]}, {"id": "1705.09280", "submitter": "Suriya Gunasekar", "authors": "Suriya Gunasekar, Blake Woodworth, Srinadh Bhojanapalli, Behnam\n  Neyshabur, Nathan Srebro", "title": "Implicit Regularization in Matrix Factorization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study implicit regularization when optimizing an underdetermined quadratic\nobjective over a matrix $X$ with gradient descent on a factorization of $X$. We\nconjecture and provide empirical and theoretical evidence that with small\nenough step sizes and initialization close enough to the origin, gradient\ndescent on a full dimensional factorization converges to the minimum nuclear\nnorm solution.\n", "versions": [{"version": "v1", "created": "Thu, 25 May 2017 17:55:24 GMT"}], "update_date": "2017-05-26", "authors_parsed": [["Gunasekar", "Suriya", ""], ["Woodworth", "Blake", ""], ["Bhojanapalli", "Srinadh", ""], ["Neyshabur", "Behnam", ""], ["Srebro", "Nathan", ""]]}, {"id": "1705.09283", "submitter": "Peng Jiao", "authors": "Lei Deng, Peng Jiao, Jing Pei, Zhenzhi Wu and Guoqi Li", "title": "GXNOR-Net: Training deep neural networks with ternary weights and\n  activations without full-precision memory under a unified discretization\n  framework", "comments": "11 pages, 13 figures", "journal-ref": "Neural Networks(Volume 100,April 2018)", "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There is a pressing need to build an architecture that could subsume these\nnetworks under a unified framework that achieves both higher performance and\nless overhead. To this end, two fundamental issues are yet to be addressed. The\nfirst one is how to implement the back propagation when neuronal activations\nare discrete. The second one is how to remove the full-precision hidden weights\nin the training phase to break the bottlenecks of memory/computation\nconsumption. To address the first issue, we present a multi-step neuronal\nactivation discretization method and a derivative approximation technique that\nenable the implementing the back propagation algorithm on discrete DNNs. While\nfor the second issue, we propose a discrete state transition (DST) methodology\nto constrain the weights in a discrete space without saving the hidden weights.\nThrough this way, we build a unified framework that subsumes the binary or\nternary networks as its special cases, and under which a heuristic algorithm is\nprovided at the website https://github.com/AcrossV/Gated-XNOR. More\nparticularly, we find that when both the weights and activations become ternary\nvalues, the DNNs can be reduced to sparse binary networks, termed as gated XNOR\nnetworks (GXNOR-Nets) since only the event of non-zero weight and non-zero\nactivation enables the control gate to start the XNOR logic operations in the\noriginal binary networks. This promises the event-driven hardware design for\nefficient mobile intelligence. We achieve advanced performance compared with\nstate-of-the-art algorithms. Furthermore, the computational sparsity and the\nnumber of states in the discrete space can be flexibly modified to make it\nsuitable for various hardware platforms.\n", "versions": [{"version": "v1", "created": "Thu, 25 May 2017 17:59:41 GMT"}, {"version": "v2", "created": "Mon, 17 Jul 2017 03:01:18 GMT"}, {"version": "v3", "created": "Fri, 18 Aug 2017 07:43:44 GMT"}, {"version": "v4", "created": "Mon, 9 Apr 2018 05:25:13 GMT"}, {"version": "v5", "created": "Wed, 2 May 2018 17:30:40 GMT"}], "update_date": "2018-05-03", "authors_parsed": [["Deng", "Lei", ""], ["Jiao", "Peng", ""], ["Pei", "Jing", ""], ["Wu", "Zhenzhi", ""], ["Li", "Guoqi", ""]]}, {"id": "1705.09303", "submitter": "Matt Feiszli", "authors": "Matt Feiszli", "title": "Latent Geometry and Memorization in Generative Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It can be difficult to tell whether a trained generative model has learned to\ngenerate novel examples or has simply memorized a specific set of outputs. In\npublished work, it is common to attempt to address this visually, for example\nby displaying a generated example and its nearest neighbor(s) in the training\nset (in, for example, the L2 metric). As any generative model induces a\nprobability density on its output domain, we propose studying this density\ndirectly. We first study the geometry of the latent representation and\ngenerator, relate this to the output density, and then develop techniques to\ncompute and inspect the output density. As an application, we demonstrate that\n\"memorization\" tends to a density made of delta functions concentrated on the\nmemorized examples. We note that without first understanding the geometry, the\nmeasurement would be essentially impossible to make.\n", "versions": [{"version": "v1", "created": "Thu, 25 May 2017 18:00:19 GMT"}], "update_date": "2017-05-29", "authors_parsed": [["Feiszli", "Matt", ""]]}, {"id": "1705.09319", "submitter": "L\\'eon Bottou", "authors": "Jean Lafond, Nicolas Vasilache, L\\'eon Bottou", "title": "Diagonal Rescaling For Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We define a second-order neural network stochastic gradient training\nalgorithm whose block-diagonal structure effectively amounts to normalizing the\nunit activations. Investigating why this algorithm lacks in robustness then\nreveals two interesting insights. The first insight suggests a new way to scale\nthe stepsizes, clarifying popular algorithms such as RMSProp as well as old\nneural network tricks such as fanin stepsize scaling. The second insight\nstresses the practical importance of dealing with fast changes of the curvature\nof the cost.\n", "versions": [{"version": "v1", "created": "Thu, 25 May 2017 18:33:24 GMT"}], "update_date": "2017-05-29", "authors_parsed": [["Lafond", "Jean", ""], ["Vasilache", "Nicolas", ""], ["Bottou", "L\u00e9on", ""]]}, {"id": "1705.09322", "submitter": "Ahmed Touati", "authors": "Ahmed Touati, Pierre-Luc Bacon, Doina Precup, Pascal Vincent", "title": "Convergent Tree Backup and Retrace with Function Approximation", "comments": null, "journal-ref": "ICML 2018, Proceedings of the 35th International Conference on\n  Machine Learning, PMLR 80:4955-4964, 2018", "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Off-policy learning is key to scaling up reinforcement learning as it allows\nto learn about a target policy from the experience generated by a different\nbehavior policy. Unfortunately, it has been challenging to combine off-policy\nlearning with function approximation and multi-step bootstrapping in a way that\nleads to both stable and efficient algorithms. In this work, we show that the\n\\textsc{Tree Backup} and \\textsc{Retrace} algorithms are unstable with linear\nfunction approximation, both in theory and in practice with specific examples.\nBased on our analysis, we then derive stable and efficient gradient-based\nalgorithms using a quadratic convex-concave saddle-point formulation. By\nexploiting the problem structure proper to these algorithms, we are able to\nprovide convergence guarantees and finite-sample bounds. The applicability of\nour new analysis also goes beyond \\textsc{Tree Backup} and \\textsc{Retrace} and\nallows us to provide new convergence rates for the GTD and GTD2 algorithms\nwithout having recourse to projections or Polyak averaging.\n", "versions": [{"version": "v1", "created": "Thu, 25 May 2017 18:37:55 GMT"}, {"version": "v2", "created": "Fri, 17 Nov 2017 20:44:47 GMT"}, {"version": "v3", "created": "Fri, 23 Feb 2018 15:25:32 GMT"}, {"version": "v4", "created": "Mon, 22 Oct 2018 21:34:58 GMT"}], "update_date": "2019-01-04", "authors_parsed": [["Touati", "Ahmed", ""], ["Bacon", "Pierre-Luc", ""], ["Precup", "Doina", ""], ["Vincent", "Pascal", ""]]}, {"id": "1705.09359", "submitter": "Niek Tax", "authors": "Niek Tax, Emin Alasgarov, Natalia Sidorova, Wil M.P. van der Aalst,\n  Reinder Haakma", "title": "Generating Time-Based Label Refinements to Discover More Precise Process\n  Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Process mining is a research field focused on the analysis of event data with\nthe aim of extracting insights related to dynamic behavior. Applying process\nmining techniques on data from smart home environments has the potential to\nprovide valuable insights in (un)healthy habits and to contribute to ambient\nassisted living solutions. Finding the right event labels to enable the\napplication of process mining techniques is however far from trivial, as simply\nusing the triggering sensor as the label for sensor events results in\nuninformative models that allow for too much behavior (overgeneralizing).\nRefinements of sensor level event labels suggested by domain experts have been\nshown to enable discovery of more precise and insightful process models.\nHowever, there exists no automated approach to generate refinements of event\nlabels in the context of process mining. In this paper we propose a framework\nfor the automated generation of label refinements based on the time attribute\nof events, allowing us to distinguish behaviourally different instances of the\nsame event type based on their time attribute. We show on a case study with\nreal life smart home event data that using automatically generated refined\nlabels in process discovery, we can find more specific, and therefore more\ninsightful, process models. We observe that one label refinement could have an\neffect on the usefulness of other label refinements when used together.\nTherefore, we explore four strategies to generate useful combinations of\nmultiple label refinements and evaluate those on three real life smart home\nevent logs.\n", "versions": [{"version": "v1", "created": "Thu, 25 May 2017 21:01:20 GMT"}, {"version": "v2", "created": "Tue, 31 Oct 2017 16:22:43 GMT"}], "update_date": "2017-11-01", "authors_parsed": [["Tax", "Niek", ""], ["Alasgarov", "Emin", ""], ["Sidorova", "Natalia", ""], ["van der Aalst", "Wil M. P.", ""], ["Haakma", "Reinder", ""]]}, {"id": "1705.09367", "submitter": "Kevin Roth", "authors": "Kevin Roth, Aurelien Lucchi, Sebastian Nowozin, Thomas Hofmann", "title": "Stabilizing Training of Generative Adversarial Networks through\n  Regularization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep generative models based on Generative Adversarial Networks (GANs) have\ndemonstrated impressive sample quality but in order to work they require a\ncareful choice of architecture, parameter initialization, and selection of\nhyper-parameters. This fragility is in part due to a dimensional mismatch or\nnon-overlapping support between the model distribution and the data\ndistribution, causing their density ratio and the associated f-divergence to be\nundefined. We overcome this fundamental limitation and propose a new\nregularization approach with low computational cost that yields a stable GAN\ntraining procedure. We demonstrate the effectiveness of this regularizer across\nseveral architectures trained on common benchmark image generation tasks. Our\nregularization turns GAN models into reliable building blocks for deep\nlearning.\n", "versions": [{"version": "v1", "created": "Thu, 25 May 2017 21:17:50 GMT"}, {"version": "v2", "created": "Tue, 7 Nov 2017 11:17:04 GMT"}], "update_date": "2017-11-08", "authors_parsed": [["Roth", "Kevin", ""], ["Lucchi", "Aurelien", ""], ["Nowozin", "Sebastian", ""], ["Hofmann", "Thomas", ""]]}, {"id": "1705.09396", "submitter": "Nan Ye", "authors": "Nan Ye and Peter Bartlett", "title": "Approximate and Stochastic Greedy Optimization", "comments": "15 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider two greedy algorithms for minimizing a convex function in a\nbounded convex set: an algorithm by Jones [1992] and the Frank-Wolfe (FW)\nalgorithm. We first consider approximate versions of these algorithms. For\nsmooth convex functions, we give sufficient conditions for convergence, a\nunified analysis for the well-known convergence rate of O(1/k) together with a\nresult showing that this rate is the best obtainable from the proof technique,\nand an equivalence result for the two algorithms. We also consider approximate\nstochastic greedy algorithms for minimizing expectations. We show that\nreplacing the full gradient by a single stochastic gradient can fail even on\nsmooth convex functions. We give a convergent approximate stochastic Jones\nalgorithm and a convergent approximate stochastic FW algorithm for smooth\nconvex functions. In addition, we give a convergent approximate stochastic FW\nalgorithm for nonsmooth convex functions. Convergence rates for these\nalgorithms are given and proved.\n", "versions": [{"version": "v1", "created": "Thu, 25 May 2017 23:24:21 GMT"}], "update_date": "2018-11-19", "authors_parsed": [["Ye", "Nan", ""], ["Bartlett", "Peter", ""]]}, {"id": "1705.09406", "submitter": "Tadas Baltrusaitis", "authors": "Tadas Baltru\\v{s}aitis, Chaitanya Ahuja, Louis-Philippe Morency", "title": "Multimodal Machine Learning: A Survey and Taxonomy", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Our experience of the world is multimodal - we see objects, hear sounds, feel\ntexture, smell odors, and taste flavors. Modality refers to the way in which\nsomething happens or is experienced and a research problem is characterized as\nmultimodal when it includes multiple such modalities. In order for Artificial\nIntelligence to make progress in understanding the world around us, it needs to\nbe able to interpret such multimodal signals together. Multimodal machine\nlearning aims to build models that can process and relate information from\nmultiple modalities. It is a vibrant multi-disciplinary field of increasing\nimportance and with extraordinary potential. Instead of focusing on specific\nmultimodal applications, this paper surveys the recent advances in multimodal\nmachine learning itself and presents them in a common taxonomy. We go beyond\nthe typical early and late fusion categorization and identify broader\nchallenges that are faced by multimodal machine learning, namely:\nrepresentation, translation, alignment, fusion, and co-learning. This new\ntaxonomy will enable researchers to better understand the state of the field\nand identify directions for future research.\n", "versions": [{"version": "v1", "created": "Fri, 26 May 2017 01:35:31 GMT"}, {"version": "v2", "created": "Tue, 1 Aug 2017 17:39:39 GMT"}], "update_date": "2017-08-02", "authors_parsed": [["Baltru\u0161aitis", "Tadas", ""], ["Ahuja", "Chaitanya", ""], ["Morency", "Louis-Philippe", ""]]}, {"id": "1705.09407", "submitter": "Giuseppe Nuti", "authors": "Giuseppe Nuti", "title": "An Efficient Algorithm for Bayesian Nearest Neighbours", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  K-Nearest Neighbours (k-NN) is a popular classification and regression\nalgorithm, yet one of its main limitations is the difficulty in choosing the\nnumber of neighbours. We present a Bayesian algorithm to compute the posterior\nprobability distribution for k given a target point within a data-set,\nefficiently and without the use of Markov Chain Monte Carlo (MCMC) methods or\nsimulation - alongside an exact solution for distributions within the\nexponential family. The central idea is that data points around our target are\ngenerated by the same probability distribution, extending outwards over the\nappropriate, though unknown, number of neighbours. Once the data is projected\nonto a distance metric of choice, we can transform the choice of k into a\nchange-point detection problem, for which there is an efficient solution: we\nrecursively compute the probability of the last change-point as we move towards\nour target, and thus de facto compute the posterior probability distribution\nover k. Applying this approach to both a classification and a regression UCI\ndata-sets, we compare favourably and, most importantly, by removing the need\nfor simulation, we are able to compute the posterior probability of k exactly\nand rapidly. As an example, the computational time for the Ripley data-set is a\nfew milliseconds compared to a few hours when using a MCMC approach.\n", "versions": [{"version": "v1", "created": "Fri, 26 May 2017 01:36:15 GMT"}, {"version": "v2", "created": "Fri, 2 Jun 2017 02:29:13 GMT"}], "update_date": "2017-06-05", "authors_parsed": [["Nuti", "Giuseppe", ""]]}, {"id": "1705.09436", "submitter": "Daksh Varshneya", "authors": "Daksh Varshneya, G. Srinivasaraghavan", "title": "Human Trajectory Prediction using Spatially aware Deep Attention Models", "comments": "10 pages, 5 figures, Submitted to 31st Conference on Neural\n  Information Processing Systems (NIPS 2017)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Trajectory Prediction of dynamic objects is a widely studied topic in the\nfield of artificial intelligence. Thanks to a large number of applications like\npredicting abnormal events, navigation system for the blind, etc. there have\nbeen many approaches to attempt learning patterns of motion directly from data\nusing a wide variety of techniques ranging from hand-crafted features to\nsophisticated deep learning models for unsupervised feature learning. All these\napproaches have been limited by problems like inefficient features in the case\nof hand crafted features, large error propagation across the predicted\ntrajectory and no information of static artefacts around the dynamic moving\nobjects. We propose an end to end deep learning model to learn the motion\npatterns of humans using different navigational modes directly from data using\nthe much popular sequence to sequence model coupled with a soft attention\nmechanism. We also propose a novel approach to model the static artefacts in a\nscene and using these to predict the dynamic trajectories. The proposed method,\ntested on trajectories of pedestrians, consistently outperforms previously\nproposed state of the art approaches on a variety of large scale data sets. We\nalso show how our architecture can be naturally extended to handle multiple\nmodes of movement (say pedestrians, skaters, bikers and buses) simultaneously.\n", "versions": [{"version": "v1", "created": "Fri, 26 May 2017 05:37:36 GMT"}], "update_date": "2017-05-29", "authors_parsed": [["Varshneya", "Daksh", ""], ["Srinivasaraghavan", "G.", ""]]}, {"id": "1705.09439", "submitter": "Kosetsu Tsukuda", "authors": "Kosetsu Tsukuda, Masataka Goto", "title": "Taste or Addiction?: Using Play Logs to Infer Song Selection Motivation", "comments": "Accepted by The 21st Pacific-Asia Conference on Knowledge Discovery\n  and Data Mining (PAKDD 2017)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Online music services are increasing in popularity. They enable us to analyze\npeople's music listening behavior based on play logs. Although it is known that\npeople listen to music based on topic (e.g., rock or jazz), we assume that when\na user is addicted to an artist, s/he chooses the artist's songs regardless of\ntopic. Based on this assumption, in this paper, we propose a probabilistic\nmodel to analyze people's music listening behavior. Our main contributions are\nthree-fold. First, to the best of our knowledge, this is the first study\nmodeling music listening behavior by taking into account the influence of\naddiction to artists. Second, by using real-world datasets of play logs, we\nshowed the effectiveness of our proposed model. Third, we carried out\nqualitative experiments and showed that taking addiction into account enables\nus to analyze music listening behavior from a new viewpoint in terms of how\npeople listen to music according to the time of day, how an artist's songs are\nlistened to by people, etc. We also discuss the possibility of applying the\nanalysis results to applications such as artist similarity computation and song\nrecommendation.\n", "versions": [{"version": "v1", "created": "Fri, 26 May 2017 05:54:20 GMT"}], "update_date": "2017-05-29", "authors_parsed": [["Tsukuda", "Kosetsu", ""], ["Goto", "Masataka", ""]]}, {"id": "1705.09476", "submitter": "Donghui Wang", "authors": "Yanan Li, Donghui Wang", "title": "Learning Robust Features with Incremental Auto-Encoders", "comments": "This work was completed in Feb, 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automatically learning features, especially robust features, has attracted\nmuch attention in the machine learning community. In this paper, we propose a\nnew method to learn non-linear robust features by taking advantage of the data\nmanifold structure. We first follow the commonly used trick of the trade, that\nis learning robust features with artificially corrupted data, which are\ntraining samples with manually injected noise. Following the idea of the\nauto-encoder, we first assume features should contain much information to well\nreconstruct the input from its corrupted copies. However, merely reconstructing\nclean input from its noisy copies could make data manifold in the feature space\nnoisy. To address this problem, we propose a new method, called Incremental\nAuto-Encoders, to iteratively denoise the extracted features. We assume the\nnoisy manifold structure is caused by a diffusion process. Consequently, we\nreverse this specific diffusion process to further contract this noisy\nmanifold, which results in an incremental optimization of model parameters .\nFurthermore, we show these learned non-linear features can be stacked into a\nhierarchy of features. Experimental results on real-world datasets demonstrate\nthe proposed method can achieve better classification performances.\n", "versions": [{"version": "v1", "created": "Fri, 26 May 2017 08:30:41 GMT"}], "update_date": "2017-05-29", "authors_parsed": [["Li", "Yanan", ""], ["Wang", "Donghui", ""]]}, {"id": "1705.09518", "submitter": "Aamir Anis", "authors": "Aamir Anis and Aly El Gamal and Salman Avestimehr and Antonio Ortega", "title": "A Sampling Theory Perspective of Graph-based Semi-supervised Learning", "comments": null, "journal-ref": "in IEEE Transactions on Information Theory, vol. 65, no. 4, pp.\n  2322-2342, April 2019", "doi": "10.1109/TIT.2018.2879897", "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Graph-based methods have been quite successful in solving unsupervised and\nsemi-supervised learning problems, as they provide a means to capture the\nunderlying geometry of the dataset. It is often desirable for the constructed\ngraph to satisfy two properties: first, data points that are similar in the\nfeature space should be strongly connected on the graph, and second, the class\nlabel information should vary smoothly with respect to the graph, where\nsmoothness is measured using the spectral properties of the graph Laplacian\nmatrix. Recent works have justified some of these smoothness conditions by\nshowing that they are strongly linked to the semi-supervised smoothness\nassumption and its variants. In this work, we reinforce this connection by\nviewing the problem from a graph sampling theoretic perspective, where class\nindicator functions are treated as bandlimited graph signals (in the\neigenvector basis of the graph Laplacian) and label prediction as a bandlimited\nreconstruction problem. Our approach involves analyzing the bandwidth of class\nindicator signals generated from statistical data models with separable and\nnonseparable classes. These models are quite general and mimic the nature of\nmost real-world datasets. Our results show that in the asymptotic limit, the\nbandwidth of any class indicator is also closely related to the geometry of the\ndataset. This allows one to theoretically justify the assumption of\nbandlimitedness of class indicator signals, thereby providing a sampling\ntheoretic interpretation of graph-based semi-supervised classification.\n", "versions": [{"version": "v1", "created": "Fri, 26 May 2017 10:39:08 GMT"}, {"version": "v2", "created": "Sat, 13 Apr 2019 06:03:29 GMT"}], "update_date": "2019-04-16", "authors_parsed": [["Anis", "Aamir", ""], ["Gamal", "Aly El", ""], ["Avestimehr", "Salman", ""], ["Ortega", "Antonio", ""]]}, {"id": "1705.09524", "submitter": "Marco Cristoforetti", "authors": "Marco Cristoforetti, Giuseppe Jurman, Andrea I. Nardelli, Cesare\n  Furlanello", "title": "Towards meaningful physics from generative models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "hep-lat cond-mat.stat-mech cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In several physical systems, important properties characterizing the system\nitself are theoretically related with specific degrees of freedom. Although\nstandard Monte Carlo simulations provide an effective tool to accurately\nreconstruct the physical configurations of the system, they are unable to\nisolate the different contributions corresponding to different degrees of\nfreedom. Here we show that unsupervised deep learning can become a valid\nsupport to MC simulation, coupling useful insights in the phases detection task\nwith good reconstruction performance. As a testbed we consider the 2D XY model,\nshowing that a deep neural network based on variational autoencoders can detect\nthe continuous Kosterlitz-Thouless (KT) transitions, and that, if endowed with\nthe appropriate constrains, they generate configurations with meaningful\nphysical content.\n", "versions": [{"version": "v1", "created": "Fri, 26 May 2017 10:45:59 GMT"}], "update_date": "2017-05-29", "authors_parsed": [["Cristoforetti", "Marco", ""], ["Jurman", "Giuseppe", ""], ["Nardelli", "Andrea I.", ""], ["Furlanello", "Cesare", ""]]}, {"id": "1705.09552", "submitter": "Seyed-Mohsen Moosavi-Dezfooli", "authors": "Alhussein Fawzi, Seyed-Mohsen Moosavi-Dezfooli, Pascal Frossard,\n  Stefano Soatto", "title": "Classification regions of deep neural networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The goal of this paper is to analyze the geometric properties of deep neural\nnetwork classifiers in the input space. We specifically study the topology of\nclassification regions created by deep networks, as well as their associated\ndecision boundary. Through a systematic empirical investigation, we show that\nstate-of-the-art deep nets learn connected classification regions, and that the\ndecision boundary in the vicinity of datapoints is flat along most directions.\nWe further draw an essential connection between two seemingly unrelated\nproperties of deep networks: their sensitivity to additive perturbations in the\ninputs, and the curvature of their decision boundary. The directions where the\ndecision boundary is curved in fact remarkably characterize the directions to\nwhich the classifier is the most vulnerable. We finally leverage a fundamental\nasymmetry in the curvature of the decision boundary of deep nets, and propose a\nmethod to discriminate between original images, and images perturbed with small\nadversarial examples. We show the effectiveness of this purely geometric\napproach for detecting small adversarial perturbations in images, and for\nrecovering the labels of perturbed images.\n", "versions": [{"version": "v1", "created": "Fri, 26 May 2017 12:38:48 GMT"}], "update_date": "2017-05-29", "authors_parsed": [["Fawzi", "Alhussein", ""], ["Moosavi-Dezfooli", "Seyed-Mohsen", ""], ["Frossard", "Pascal", ""], ["Soatto", "Stefano", ""]]}, {"id": "1705.09554", "submitter": "Seyed-Mohsen Moosavi-Dezfooli", "authors": "Seyed-Mohsen Moosavi-Dezfooli, Alhussein Fawzi, Omar Fawzi, Pascal\n  Frossard, Stefano Soatto", "title": "Robustness of classifiers to universal perturbations: a geometric\n  perspective", "comments": "Published at ICLR 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep networks have recently been shown to be vulnerable to universal\nperturbations: there exist very small image-agnostic perturbations that cause\nmost natural images to be misclassified by such classifiers. In this paper, we\npropose the first quantitative analysis of the robustness of classifiers to\nuniversal perturbations, and draw a formal link between the robustness to\nuniversal perturbations, and the geometry of the decision boundary.\nSpecifically, we establish theoretical bounds on the robustness of classifiers\nunder two decision boundary models (flat and curved models). We show in\nparticular that the robustness of deep networks to universal perturbations is\ndriven by a key property of their curvature: there exists shared directions\nalong which the decision boundary of deep networks is systematically positively\ncurved. Under such conditions, we prove the existence of small universal\nperturbations. Our analysis further provides a novel geometric method for\ncomputing universal perturbations, in addition to explaining their properties.\n", "versions": [{"version": "v1", "created": "Fri, 26 May 2017 12:42:55 GMT"}, {"version": "v2", "created": "Mon, 1 Mar 2021 20:45:00 GMT"}], "update_date": "2021-03-03", "authors_parsed": [["Moosavi-Dezfooli", "Seyed-Mohsen", ""], ["Fawzi", "Alhussein", ""], ["Fawzi", "Omar", ""], ["Frossard", "Pascal", ""], ["Soatto", "Stefano", ""]]}, {"id": "1705.09558", "submitter": "Andrew Wilson", "authors": "Yunus Saatchi, Andrew Gordon Wilson", "title": "Bayesian GAN", "comments": "Updated to the version that appears at Advances in Neural Information\n  Processing Systems 30 (NIPS), 2017", "journal-ref": "Advances in Neural Information Processing Systems 30 (NIPS), 2017", "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generative adversarial networks (GANs) can implicitly learn rich\ndistributions over images, audio, and data which are hard to model with an\nexplicit likelihood. We present a practical Bayesian formulation for\nunsupervised and semi-supervised learning with GANs. Within this framework, we\nuse stochastic gradient Hamiltonian Monte Carlo to marginalize the weights of\nthe generator and discriminator networks. The resulting approach is\nstraightforward and obtains good performance without any standard interventions\nsuch as feature matching, or mini-batch discrimination. By exploring an\nexpressive posterior over the parameters of the generator, the Bayesian GAN\navoids mode-collapse, produces interpretable and diverse candidate samples, and\nprovides state-of-the-art quantitative results for semi-supervised learning on\nbenchmarks including SVHN, CelebA, and CIFAR-10, outperforming DCGAN,\nWasserstein GANs, and DCGAN ensembles.\n", "versions": [{"version": "v1", "created": "Fri, 26 May 2017 12:47:56 GMT"}, {"version": "v2", "created": "Mon, 29 May 2017 07:54:47 GMT"}, {"version": "v3", "created": "Wed, 8 Nov 2017 17:52:21 GMT"}], "update_date": "2017-11-09", "authors_parsed": [["Saatchi", "Yunus", ""], ["Wilson", "Andrew Gordon", ""]]}, {"id": "1705.09605", "submitter": "James Grant", "authors": "James A. Grant, David S. Leslie, Kevin Glazebrook, Roberto Szechtman", "title": "Combinatorial Multi-Armed Bandits with Filtered Feedback", "comments": "16 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motivated by problems in search and detection we present a solution to a\nCombinatorial Multi-Armed Bandit (CMAB) problem with both heavy-tailed reward\ndistributions and a new class of feedback, filtered semibandit feedback. In a\nCMAB problem an agent pulls a combination of arms from a set $\\{1,...,k\\}$ in\neach round, generating random outcomes from probability distributions\nassociated with these arms and receiving an overall reward. Under semibandit\nfeedback it is assumed that the random outcomes generated are all observed.\nFiltered semibandit feedback allows the outcomes that are observed to be\nsampled from a second distribution conditioned on the initial random outcomes.\nThis feedback mechanism is valuable as it allows CMAB methods to be applied to\nsequential search and detection problems where combinatorial actions are made,\nbut the true rewards (number of objects of interest appearing in the round) are\nnot observed, rather a filtered reward (the number of objects the searcher\nsuccessfully finds, which must by definition be less than the number that\nappear). We present an upper confidence bound type algorithm, Robust-F-CUCB,\nand associated regret bound of order $\\mathcal{O}(\\ln(n))$ to balance\nexploration and exploitation in the face of both filtering of reward and heavy\ntailed reward distributions.\n", "versions": [{"version": "v1", "created": "Fri, 26 May 2017 14:53:46 GMT"}], "update_date": "2017-05-29", "authors_parsed": [["Grant", "James A.", ""], ["Leslie", "David S.", ""], ["Glazebrook", "Kevin", ""], ["Szechtman", "Roberto", ""]]}, {"id": "1705.09620", "submitter": "Lev Utkin", "authors": "Lev V. Utkin and Mikhail A. Ryabinin", "title": "Discriminative Metric Learning with Deep Forest", "comments": "arXiv admin note: substantial text overlap with arXiv:1704.08715", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A Discriminative Deep Forest (DisDF) as a metric learning algorithm is\nproposed in the paper. It is based on the Deep Forest or gcForest proposed by\nZhou and Feng and can be viewed as a gcForest modification. The case of the\nfully supervised learning is studied when the class labels of individual\ntraining examples are known. The main idea underlying the algorithm is to\nassign weights to decision trees in random forest in order to reduce distances\nbetween objects from the same class and to increase them between objects from\ndifferent classes. The weights are training parameters. A specific objective\nfunction which combines Euclidean and Manhattan distances and simplifies the\noptimization problem for training the DisDF is proposed. The numerical\nexperiments illustrate the proposed distance metric algorithm.\n", "versions": [{"version": "v1", "created": "Thu, 25 May 2017 12:24:11 GMT"}], "update_date": "2017-05-29", "authors_parsed": [["Utkin", "Lev V.", ""], ["Ryabinin", "Mikhail A.", ""]]}, {"id": "1705.09644", "submitter": "AmirEmad Ghassami", "authors": "AmirEmad Ghassami, Saber Salehkaleybar, Negar Kiyavash, Kun Zhang", "title": "Learning Causal Structures Using Regression Invariance", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study causal inference in a multi-environment setting, in which the\nfunctional relations for producing the variables from their direct causes\nremain the same across environments, while the distribution of exogenous noises\nmay vary. We introduce the idea of using the invariance of the functional\nrelations of the variables to their causes across a set of environments. We\ndefine a notion of completeness for a causal inference algorithm in this\nsetting and prove the existence of such algorithm by proposing the baseline\nalgorithm. Additionally, we present an alternate algorithm that has\nsignificantly improved computational and sample complexity compared to the\nbaseline algorithm. The experiment results show that the proposed algorithm\noutperforms the other existing algorithms.\n", "versions": [{"version": "v1", "created": "Fri, 26 May 2017 16:34:13 GMT"}], "update_date": "2017-05-29", "authors_parsed": [["Ghassami", "AmirEmad", ""], ["Salehkaleybar", "Saber", ""], ["Kiyavash", "Negar", ""], ["Zhang", "Kun", ""]]}, {"id": "1705.09650", "submitter": "Qin Lin", "authors": "Xiaoran Liu and Qin Lin and Sicco Verwer and Dmitri Jarnikov", "title": "Anomaly Detection in a Digital Video Broadcasting System Using Timed\n  Automata", "comments": "This paper has been accepted by the Thirty-Second Annual ACM/IEEE\n  Symposium on Logic in Computer Science (LICS) Workshop on Learning and\n  Automata (LearnAut)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.FL cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper focuses on detecting anomalies in a digital video broadcasting\n(DVB) system from providers' perspective. We learn a probabilistic\ndeterministic real timed automaton profiling benign behavior of encryption\ncontrol in the DVB control access system. This profile is used as a one-class\nclassifier. Anomalous items in a testing sequence are detected when the\nsequence is not accepted by the learned model.\n", "versions": [{"version": "v1", "created": "Wed, 24 May 2017 09:26:49 GMT"}], "update_date": "2017-05-29", "authors_parsed": [["Liu", "Xiaoran", ""], ["Lin", "Qin", ""], ["Verwer", "Sicco", ""], ["Jarnikov", "Dmitri", ""]]}, {"id": "1705.09655", "submitter": "Tianxiao Shen", "authors": "Tianxiao Shen, Tao Lei, Regina Barzilay, Tommi Jaakkola", "title": "Style Transfer from Non-Parallel Text by Cross-Alignment", "comments": "NIPS 2017 camera-ready. Added human evaluation on sentiment transfer", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper focuses on style transfer on the basis of non-parallel text. This\nis an instance of a broad family of problems including machine translation,\ndecipherment, and sentiment modification. The key challenge is to separate the\ncontent from other aspects such as style. We assume a shared latent content\ndistribution across different text corpora, and propose a method that leverages\nrefined alignment of latent representations to perform style transfer. The\ntransferred sentences from one style should match example sentences from the\nother style as a population. We demonstrate the effectiveness of this\ncross-alignment method on three tasks: sentiment modification, decipherment of\nword substitution ciphers, and recovery of word order.\n", "versions": [{"version": "v1", "created": "Fri, 26 May 2017 17:40:12 GMT"}, {"version": "v2", "created": "Mon, 6 Nov 2017 15:07:03 GMT"}], "update_date": "2017-11-07", "authors_parsed": [["Shen", "Tianxiao", ""], ["Lei", "Tao", ""], ["Barzilay", "Regina", ""], ["Jaakkola", "Tommi", ""]]}, {"id": "1705.09675", "submitter": "Tom Sercu", "authors": "Youssef Mroueh, Tom Sercu", "title": "Fisher GAN", "comments": "Published at NIPS 2017. v2: added inception score table & plot\n  update, relation to f-gan, illustration (Figure 1). v3: added strong SSL\n  results for critic without batch normalization", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generative Adversarial Networks (GANs) are powerful models for learning\ncomplex distributions. Stable training of GANs has been addressed in many\nrecent works which explore different metrics between distributions. In this\npaper we introduce Fisher GAN which fits within the Integral Probability\nMetrics (IPM) framework for training GANs. Fisher GAN defines a critic with a\ndata dependent constraint on its second order moments. We show in this paper\nthat Fisher GAN allows for stable and time efficient training that does not\ncompromise the capacity of the critic, and does not need data independent\nconstraints such as weight clipping. We analyze our Fisher IPM theoretically\nand provide an algorithm based on Augmented Lagrangian for Fisher GAN. We\nvalidate our claims on both image sample generation and semi-supervised\nclassification using Fisher GAN.\n", "versions": [{"version": "v1", "created": "Fri, 26 May 2017 18:22:24 GMT"}, {"version": "v2", "created": "Tue, 1 Aug 2017 16:42:53 GMT"}, {"version": "v3", "created": "Fri, 3 Nov 2017 18:26:22 GMT"}], "update_date": "2017-11-07", "authors_parsed": [["Mroueh", "Youssef", ""], ["Sercu", "Tom", ""]]}, {"id": "1705.09684", "submitter": "Han Zhao", "authors": "Han Zhao, Shanghang Zhang, Guanhang Wu, Jo\\~ao P. Costeira, Jos\\'e M.\n  F. Moura, Geoffrey J. Gordon", "title": "Multiple Source Domain Adaptation with Adversarial Training of Neural\n  Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While domain adaptation has been actively researched in recent years, most\ntheoretical results and algorithms focus on the single-source-single-target\nadaptation setting. Naive application of such algorithms on multiple source\ndomain adaptation problem may lead to suboptimal solutions. As a step toward\nbridging the gap, we propose a new generalization bound for domain adaptation\nwhen there are multiple source domains with labeled instances and one target\ndomain with unlabeled instances. Compared with existing bounds, the new bound\ndoes not require expert knowledge about the target distribution, nor the\noptimal combination rule for multisource domains. Interestingly, our theory\nalso leads to an efficient learning strategy using adversarial neural networks:\nwe show how to interpret it as learning feature representations that are\ninvariant to the multiple domain shifts while still being discriminative for\nthe learning task. To this end, we propose two models, both of which we call\nmultisource domain adversarial networks (MDANs): the first model optimizes\ndirectly our bound, while the second model is a smoothed approximation of the\nfirst one, leading to a more data-efficient and task-adaptive model. The\noptimization tasks of both models are minimax saddle point problems that can be\noptimized by adversarial training. To demonstrate the effectiveness of MDANs,\nwe conduct extensive experiments showing superior adaptation performance on\nthree real-world datasets: sentiment analysis, digit classification, and\nvehicle counting.\n", "versions": [{"version": "v1", "created": "Fri, 26 May 2017 19:10:56 GMT"}, {"version": "v2", "created": "Fri, 27 Oct 2017 22:10:46 GMT"}], "update_date": "2017-10-31", "authors_parsed": [["Zhao", "Han", ""], ["Zhang", "Shanghang", ""], ["Wu", "Guanhang", ""], ["Costeira", "Jo\u00e3o P.", ""], ["Moura", "Jos\u00e9 M. F.", ""], ["Gordon", "Geoffrey J.", ""]]}, {"id": "1705.09700", "submitter": "Rad Niazadeh", "authors": "S\\'ebastien Bubeck, Nikhil R. Devanur, Zhiyi Huang, Rad Niazadeh", "title": "Multi-scale Online Learning and its Applications to Online Auctions", "comments": "Preliminary conference version In the Proc. of 18th ACM conference on\n  Economics and Computation (EC 2017)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GT cs.DS cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider revenue maximization in online auction/pricing problems. A seller\nsells an identical item in each period to a new buyer, or a new set of buyers.\nFor the online posted pricing problem, we show regret bounds that scale with\nthe best fixed price, rather than the range of the values. We also show regret\nbounds that are almost scale free, and match the offline sample complexity,\nwhen comparing to a benchmark that requires a lower bound on the market share.\nThese results are obtained by generalizing the classical learning from experts\nand multi-armed bandit problems to their multi-scale versions. In this version,\nthe reward of each action is in a different range, and the regret w.r.t. a\ngiven action scales with its own range, rather than the maximum range.\n", "versions": [{"version": "v1", "created": "Fri, 26 May 2017 19:59:50 GMT"}, {"version": "v2", "created": "Tue, 11 Sep 2018 18:59:34 GMT"}], "update_date": "2018-09-13", "authors_parsed": [["Bubeck", "S\u00e9bastien", ""], ["Devanur", "Nikhil R.", ""], ["Huang", "Zhiyi", ""], ["Niazadeh", "Rad", ""]]}, {"id": "1705.09761", "submitter": "Mohammadhussein Rafieisakhaei", "authors": "Dan Yu, Mohammadhussein Rafieisakhaei and Suman Chakravorty", "title": "Stochastic Feedback Control of Systems with Unknown Nonlinear Dynamics", "comments": "7 pages, 7 figures, submitted to 56th IEEE Conference on Decision and\n  Control (CDC), 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SY cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper studies the stochastic optimal control problem for systems with\nunknown dynamics. First, an open-loop deterministic trajectory optimization\nproblem is solved without knowing the explicit form of the dynamical system.\nNext, a Linear Quadratic Gaussian (LQG) controller is designed for the nominal\ntrajectory-dependent linearized system, such that under a small noise\nassumption, the actual states remain close to the optimal trajectory. The\ntrajectory-dependent linearized system is identified using input-output\nexperimental data consisting of the impulse responses of the nominal system. A\ncomputational example is given to illustrate the performance of the proposed\napproach.\n", "versions": [{"version": "v1", "created": "Sat, 27 May 2017 03:57:41 GMT"}], "update_date": "2017-05-30", "authors_parsed": [["Yu", "Dan", ""], ["Rafieisakhaei", "Mohammadhussein", ""], ["Chakravorty", "Suman", ""]]}, {"id": "1705.09764", "submitter": "Chang Song", "authors": "Chang Song, Hsin-Pai Cheng, Huanrui Yang, Sicheng Li, Chunpeng Wu,\n  Qing Wu, Hai Li, Yiran Chen", "title": "MAT: A Multi-strength Adversarial Training Method to Mitigate\n  Adversarial Attacks", "comments": "6 pages, 4 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Some recent works revealed that deep neural networks (DNNs) are vulnerable to\nso-called adversarial attacks where input examples are intentionally perturbed\nto fool DNNs. In this work, we revisit the DNN training process that includes\nadversarial examples into the training dataset so as to improve DNN's\nresilience to adversarial attacks, namely, adversarial training. Our\nexperiments show that different adversarial strengths, i.e., perturbation\nlevels of adversarial examples, have different working zones to resist the\nattack. Based on the observation, we propose a multi-strength adversarial\ntraining method (MAT) that combines the adversarial training examples with\ndifferent adversarial strengths to defend adversarial attacks. Two training\nstructures - mixed MAT and parallel MAT - are developed to facilitate the\ntradeoffs between training time and memory occupation. Our results show that\nMAT can substantially minimize the accuracy degradation of deep learning\nsystems to adversarial attacks on MNIST, CIFAR-10, CIFAR-100, and SVHN.\n", "versions": [{"version": "v1", "created": "Sat, 27 May 2017 04:29:04 GMT"}, {"version": "v2", "created": "Fri, 11 May 2018 18:29:36 GMT"}], "update_date": "2018-05-15", "authors_parsed": [["Song", "Chang", ""], ["Cheng", "Hsin-Pai", ""], ["Yang", "Huanrui", ""], ["Li", "Sicheng", ""], ["Wu", "Chunpeng", ""], ["Wu", "Qing", ""], ["Li", "Hai", ""], ["Chen", "Yiran", ""]]}, {"id": "1705.09783", "submitter": "Zihang Dai", "authors": "Zihang Dai, Zhilin Yang, Fan Yang, William W. Cohen, Ruslan\n  Salakhutdinov", "title": "Good Semi-supervised Learning that Requires a Bad GAN", "comments": "NIPS 2017 camera ready", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Semi-supervised learning methods based on generative adversarial networks\n(GANs) obtained strong empirical results, but it is not clear 1) how the\ndiscriminator benefits from joint training with a generator, and 2) why good\nsemi-supervised classification performance and a good generator cannot be\nobtained at the same time. Theoretically, we show that given the discriminator\nobjective, good semisupervised learning indeed requires a bad generator, and\npropose the definition of a preferred generator. Empirically, we derive a novel\nformulation based on our analysis that substantially improves over feature\nmatching GANs, obtaining state-of-the-art results on multiple benchmark\ndatasets.\n", "versions": [{"version": "v1", "created": "Sat, 27 May 2017 07:53:53 GMT"}, {"version": "v2", "created": "Wed, 21 Jun 2017 07:25:43 GMT"}, {"version": "v3", "created": "Fri, 3 Nov 2017 17:18:43 GMT"}], "update_date": "2017-11-06", "authors_parsed": [["Dai", "Zihang", ""], ["Yang", "Zhilin", ""], ["Yang", "Fan", ""], ["Cohen", "William W.", ""], ["Salakhutdinov", "Ruslan", ""]]}, {"id": "1705.09786", "submitter": "Ryota Tomioka", "authors": "Alexander L. Gaunt, Matthew A. Johnson, Maik Riechert, Daniel Tarlow,\n  Ryota Tomioka, Dimitrios Vytiniotis, Sam Webster", "title": "AMPNet: Asynchronous Model-Parallel Training for Dynamic Neural Networks", "comments": "17 pages, 13 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.DC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  New types of machine learning hardware in development and entering the market\nhold the promise of revolutionizing deep learning in a manner as profound as\nGPUs. However, existing software frameworks and training algorithms for deep\nlearning have yet to evolve to fully leverage the capability of the new wave of\nsilicon. We already see the limitations of existing algorithms for models that\nexploit structured input via complex and instance-dependent control flow, which\nprohibits minibatching. We present an asynchronous model-parallel (AMP)\ntraining algorithm that is specifically motivated by training on networks of\ninterconnected devices. Through an implementation on multi-core CPUs, we show\nthat AMP training converges to the same accuracy as conventional synchronous\ntraining algorithms in a similar number of epochs, but utilizes the available\nhardware more efficiently even for small minibatch sizes, resulting in\nsignificantly shorter overall training times. Our framework opens the door for\nscaling up a new class of deep learning models that cannot be efficiently\ntrained today.\n", "versions": [{"version": "v1", "created": "Sat, 27 May 2017 08:10:40 GMT"}, {"version": "v2", "created": "Sun, 11 Jun 2017 19:51:03 GMT"}, {"version": "v3", "created": "Thu, 22 Jun 2017 17:34:30 GMT"}], "update_date": "2017-06-23", "authors_parsed": [["Gaunt", "Alexander L.", ""], ["Johnson", "Matthew A.", ""], ["Riechert", "Maik", ""], ["Tarlow", "Daniel", ""], ["Tomioka", "Ryota", ""], ["Vytiniotis", "Dimitrios", ""], ["Webster", "Sam", ""]]}, {"id": "1705.09792", "submitter": "Chiheb Trabelsi", "authors": "Chiheb Trabelsi, Olexa Bilaniuk, Ying Zhang, Dmitriy Serdyuk, Sandeep\n  Subramanian, Jo\\~ao Felipe Santos, Soroush Mehri, Negar Rostamzadeh, Yoshua\n  Bengio, Christopher J Pal", "title": "Deep Complex Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  At present, the vast majority of building blocks, techniques, and\narchitectures for deep learning are based on real-valued operations and\nrepresentations. However, recent work on recurrent neural networks and older\nfundamental theoretical analysis suggests that complex numbers could have a\nricher representational capacity and could also facilitate noise-robust memory\nretrieval mechanisms. Despite their attractive properties and potential for\nopening up entirely new neural architectures, complex-valued deep neural\nnetworks have been marginalized due to the absence of the building blocks\nrequired to design such models. In this work, we provide the key atomic\ncomponents for complex-valued deep neural networks and apply them to\nconvolutional feed-forward networks and convolutional LSTMs. More precisely, we\nrely on complex convolutions and present algorithms for complex\nbatch-normalization, complex weight initialization strategies for\ncomplex-valued neural nets and we use them in experiments with end-to-end\ntraining schemes. We demonstrate that such complex-valued models are\ncompetitive with their real-valued counterparts. We test deep complex models on\nseveral computer vision tasks, on music transcription using the MusicNet\ndataset and on Speech Spectrum Prediction using the TIMIT dataset. We achieve\nstate-of-the-art performance on these audio-related tasks.\n", "versions": [{"version": "v1", "created": "Sat, 27 May 2017 09:04:55 GMT"}, {"version": "v2", "created": "Tue, 30 May 2017 08:38:38 GMT"}, {"version": "v3", "created": "Fri, 17 Nov 2017 22:42:47 GMT"}, {"version": "v4", "created": "Sun, 25 Feb 2018 23:42:06 GMT"}], "update_date": "2018-02-27", "authors_parsed": [["Trabelsi", "Chiheb", ""], ["Bilaniuk", "Olexa", ""], ["Zhang", "Ying", ""], ["Serdyuk", "Dmitriy", ""], ["Subramanian", "Sandeep", ""], ["Santos", "Jo\u00e3o Felipe", ""], ["Mehri", "Soroush", ""], ["Rostamzadeh", "Negar", ""], ["Bengio", "Yoshua", ""], ["Pal", "Christopher J", ""]]}, {"id": "1705.09800", "submitter": "Guy Uziel", "authors": "Guy Uziel and Ran El-Yaniv", "title": "Growth-Optimal Portfolio Selection under CVaR Constraints", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.MF cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Online portfolio selection research has so far focused mainly on minimizing\nregret defined in terms of wealth growth. Practical financial decision making,\nhowever, is deeply concerned with both wealth and risk. We consider online\nlearning of portfolios of stocks whose prices are governed by arbitrary\n(unknown) stationary and ergodic processes, where the goal is to maximize\nwealth while keeping the conditional value at risk (CVaR) below a desired\nthreshold. We characterize the asymptomatically optimal risk-adjusted\nperformance and present an investment strategy whose portfolios are guaranteed\nto achieve the asymptotic optimal solution while fulfilling the desired risk\nconstraint. We also numerically demonstrate and validate the viability of our\nmethod on standard datasets.\n", "versions": [{"version": "v1", "created": "Sat, 27 May 2017 10:27:03 GMT"}], "update_date": "2017-05-30", "authors_parsed": [["Uziel", "Guy", ""], ["El-Yaniv", "Ran", ""]]}, {"id": "1705.09805", "submitter": "Rico Jonschkowski", "authors": "Rico Jonschkowski, Roland Hafner, Jonathan Scholz, and Martin\n  Riedmiller", "title": "PVEs: Position-Velocity Encoders for Unsupervised Learning of Structured\n  State Representations", "comments": "Accepted at Robotics: Science and Systems (RSS 2017) Workshop -- New\n  Frontiers for Deep Learning in Robotics\n  http://juxi.net/workshop/deep-learning-rss-2017/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose position-velocity encoders (PVEs) which learn---without\nsupervision---to encode images to positions and velocities of task-relevant\nobjects. PVEs encode a single image into a low-dimensional position state and\ncompute the velocity state from finite differences in position. In contrast to\nautoencoders, position-velocity encoders are not trained by image\nreconstruction, but by making the position-velocity representation consistent\nwith priors about interacting with the physical world. We applied PVEs to\nseveral simulated control tasks from pixels and achieved promising preliminary\nresults.\n", "versions": [{"version": "v1", "created": "Sat, 27 May 2017 11:17:49 GMT"}, {"version": "v2", "created": "Mon, 19 Jun 2017 16:07:17 GMT"}, {"version": "v3", "created": "Mon, 24 Jul 2017 12:15:40 GMT"}], "update_date": "2017-07-25", "authors_parsed": [["Jonschkowski", "Rico", ""], ["Hafner", "Roland", ""], ["Scholz", "Jonathan", ""], ["Riedmiller", "Martin", ""]]}, {"id": "1705.09816", "submitter": "Reza Borhani", "authors": "Reza Borhani, Jeremy Watt, Aggelos Katsaggelos", "title": "Global hard thresholding algorithms for joint sparse image\n  representation and denoising", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sparse coding of images is traditionally done by cutting them into small\npatches and representing each patch individually over some dictionary given a\npre-determined number of nonzero coefficients to use for each patch. In lack of\na way to effectively distribute a total number (or global budget) of nonzero\ncoefficients across all patches, current sparse recovery algorithms distribute\nthe global budget equally across all patches despite the wide range of\ndifferences in structural complexity among them. In this work we propose a new\nframework for joint sparse representation and recovery of all image patches\nsimultaneously. We also present two novel global hard thresholding algorithms,\nbased on the notion of variable splitting, for solving the joint sparse model.\nExperimentation using both synthetic and real data shows effectiveness of the\nproposed framework for sparse image representation and denoising tasks.\nAdditionally, time complexity analysis of the proposed algorithms indicate high\nscalability of both algorithms, making them favorable to use on large megapixel\nimages.\n", "versions": [{"version": "v1", "created": "Sat, 27 May 2017 12:40:24 GMT"}], "update_date": "2017-05-30", "authors_parsed": [["Borhani", "Reza", ""], ["Watt", "Jeremy", ""], ["Katsaggelos", "Aggelos", ""]]}, {"id": "1705.09847", "submitter": "Jason Ramapuram", "authors": "Jason Ramapuram, Magda Gregorova, Alexandros Kalousis", "title": "Lifelong Generative Modeling", "comments": "32 pages", "journal-ref": "Neurocomputing 2020, Volume 404, Pages 381-400", "doi": "10.1016/j.neucom.2020.02.115", "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Lifelong learning is the problem of learning multiple consecutive tasks in a\nsequential manner, where knowledge gained from previous tasks is retained and\nused to aid future learning over the lifetime of the learner. It is essential\ntowards the development of intelligent machines that can adapt to their\nsurroundings. In this work we focus on a lifelong learning approach to\nunsupervised generative modeling, where we continuously incorporate newly\nobserved distributions into a learned model. We do so through a student-teacher\nVariational Autoencoder architecture which allows us to learn and preserve all\nthe distributions seen so far, without the need to retain the past data nor the\npast models. Through the introduction of a novel cross-model regularizer,\ninspired by a Bayesian update rule, the student model leverages the information\nlearned by the teacher, which acts as a probabilistic knowledge store. The\nregularizer reduces the effect of catastrophic interference that appears when\nwe learn over sequences of distributions. We validate our model's performance\non sequential variants of MNIST, FashionMNIST, PermutedMNIST, SVHN and Celeb-A\nand demonstrate that our model mitigates the effects of catastrophic\ninterference faced by neural networks in sequential learning scenarios.\n", "versions": [{"version": "v1", "created": "Sat, 27 May 2017 17:34:15 GMT"}, {"version": "v2", "created": "Wed, 1 Nov 2017 10:35:15 GMT"}, {"version": "v3", "created": "Mon, 28 May 2018 23:01:23 GMT"}, {"version": "v4", "created": "Tue, 18 Sep 2018 14:29:40 GMT"}, {"version": "v5", "created": "Fri, 10 May 2019 09:17:09 GMT"}, {"version": "v6", "created": "Thu, 14 Nov 2019 17:04:58 GMT"}, {"version": "v7", "created": "Tue, 8 Sep 2020 14:50:12 GMT"}], "update_date": "2020-09-09", "authors_parsed": [["Ramapuram", "Jason", ""], ["Gregorova", "Magda", ""], ["Kalousis", "Alexandros", ""]]}, {"id": "1705.09862", "submitter": "Zhenwen Dai", "authors": "Zhenwen Dai, Mauricio A. \\'Alvarez, Neil D. Lawrence", "title": "Efficient Modeling of Latent Information in Supervised Learning using\n  Gaussian Processes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Often in machine learning, data are collected as a combination of multiple\nconditions, e.g., the voice recordings of multiple persons, each labeled with\nan ID. How could we build a model that captures the latent information related\nto these conditions and generalize to a new one with few data? We present a new\nmodel called Latent Variable Multiple Output Gaussian Processes (LVMOGP) and\nthat allows to jointly model multiple conditions for regression and generalize\nto a new condition with a few data points at test time. LVMOGP infers the\nposteriors of Gaussian processes together with a latent space representing the\ninformation about different conditions. We derive an efficient variational\ninference method for LVMOGP, of which the computational complexity is as low as\nsparse Gaussian processes. We show that LVMOGP significantly outperforms\nrelated Gaussian process methods on various tasks with both synthetic and real\ndata.\n", "versions": [{"version": "v1", "created": "Sat, 27 May 2017 20:41:15 GMT"}], "update_date": "2017-05-30", "authors_parsed": [["Dai", "Zhenwen", ""], ["\u00c1lvarez", "Mauricio A.", ""], ["Lawrence", "Neil D.", ""]]}, {"id": "1705.09864", "submitter": "Haojin Yang", "authors": "Haojin Yang, Martin Fritzsche, Christian Bartz, Christoph Meinel", "title": "BMXNet: An Open-Source Binary Neural Network Implementation Based on\n  MXNet", "comments": "4 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Binary Neural Networks (BNNs) can drastically reduce memory size and accesses\nby applying bit-wise operations instead of standard arithmetic operations.\nTherefore it could significantly improve the efficiency and lower the energy\nconsumption at runtime, which enables the application of state-of-the-art deep\nlearning models on low power devices. BMXNet is an open-source BNN library\nbased on MXNet, which supports both XNOR-Networks and Quantized Neural\nNetworks. The developed BNN layers can be seamlessly applied with other\nstandard library components and work in both GPU and CPU mode. BMXNet is\nmaintained and developed by the multimedia research group at Hasso Plattner\nInstitute and released under Apache license. Extensive experiments validate the\nefficiency and effectiveness of our implementation. The BMXNet library, several\nsample projects, and a collection of pre-trained binary deep models are\navailable for download at https://github.com/hpi-xnor\n", "versions": [{"version": "v1", "created": "Sat, 27 May 2017 20:52:10 GMT"}], "update_date": "2017-05-31", "authors_parsed": [["Yang", "Haojin", ""], ["Fritzsche", "Martin", ""], ["Bartz", "Christian", ""], ["Meinel", "Christoph", ""]]}, {"id": "1705.09869", "submitter": "Allon G. Percus", "authors": "Justin Sunu, Allon G. Percus", "title": "Dimensionality reduction for acoustic vehicle classification with\n  spectral embedding", "comments": "Proceedings of the 15th IEEE International Conference on Networking,\n  Sensing and Control (2018)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG physics.data-an", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a method for recognizing moving vehicles, using data from roadside\naudio sensors. This problem has applications ranging widely, from traffic\nanalysis to surveillance. We extract a frequency signature from the audio\nsignal using a short-time Fourier transform, and treat each time window as an\nindividual data point to be classified. By applying a spectral embedding, we\ndecrease the dimensionality of the data sufficiently for K-nearest neighbors to\nprovide accurate vehicle identification.\n", "versions": [{"version": "v1", "created": "Sat, 27 May 2017 21:39:47 GMT"}, {"version": "v2", "created": "Sat, 17 Feb 2018 19:42:29 GMT"}], "update_date": "2018-02-20", "authors_parsed": [["Sunu", "Justin", ""], ["Percus", "Allon G.", ""]]}, {"id": "1705.09886", "submitter": "Yang Yuan", "authors": "Yuanzhi Li, Yang Yuan", "title": "Convergence Analysis of Two-layer Neural Networks with ReLU Activation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, stochastic gradient descent (SGD) based techniques has\nbecome the standard tools for training neural networks. However, formal\ntheoretical understanding of why SGD can train neural networks in practice is\nlargely missing.\n  In this paper, we make progress on understanding this mystery by providing a\nconvergence analysis for SGD on a rich subset of two-layer feedforward networks\nwith ReLU activations. This subset is characterized by a special structure\ncalled \"identity mapping\". We prove that, if input follows from Gaussian\ndistribution, with standard $O(1/\\sqrt{d})$ initialization of the weights, SGD\nconverges to the global minimum in polynomial number of steps. Unlike normal\nvanilla networks, the \"identity mapping\" makes our network asymmetric and thus\nthe global minimum is unique. To complement our theory, we are also able to\nshow experimentally that multi-layer networks with this mapping have better\nperformance compared with normal vanilla networks.\n  Our convergence theorem differs from traditional non-convex optimization\ntechniques. We show that SGD converges to optimal in \"two phases\": In phase I,\nthe gradient points to the wrong direction, however, a potential function $g$\ngradually decreases. Then in phase II, SGD enters a nice one point convex\nregion and converges. We also show that the identity mapping is necessary for\nconvergence, as it moves the initial point to a better place for optimization.\nExperiment verifies our claims.\n", "versions": [{"version": "v1", "created": "Sun, 28 May 2017 02:11:10 GMT"}, {"version": "v2", "created": "Wed, 1 Nov 2017 21:42:23 GMT"}], "update_date": "2017-11-03", "authors_parsed": [["Li", "Yuanzhi", ""], ["Yuan", "Yang", ""]]}, {"id": "1705.09922", "submitter": "Ole-Christoffer Granmo", "authors": "Ole-Christoffer Granmo", "title": "Bayesian Unification of Gradient and Bandit-based Learning for\n  Accelerated Global Optimisation", "comments": "15th IEEE International Conference on Machine Learning and\n  Applications (ICMLA 2016)", "journal-ref": null, "doi": "10.1109/ICMLA.2016.0044", "report-no": null, "categories": "cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bandit based optimisation has a remarkable advantage over gradient based\napproaches due to their global perspective, which eliminates the danger of\ngetting stuck at local optima. However, for continuous optimisation problems or\nproblems with a large number of actions, bandit based approaches can be\nhindered by slow learning. Gradient based approaches, on the other hand,\nnavigate quickly in high-dimensional continuous spaces through local\noptimisation, following the gradient in fine grained steps. Yet, apart from\nbeing susceptible to local optima, these schemes are less suited for online\nlearning due to their reliance on extensive trial-and-error before the optimum\ncan be identified. In this paper, we propose a Bayesian approach that unifies\nthe above two paradigms in one single framework, with the aim of combining\ntheir advantages. At the heart of our approach we find a stochastic linear\napproximation of the function to be optimised, where both the gradient and\nvalues of the function are explicitly captured. This allows us to learn from\nboth noisy function and gradient observations, and predict these properties\nacross the action space to support optimisation. We further propose an\naccompanying bandit driven exploration scheme that uses Bayesian credible\nbounds to trade off exploration against exploitation. Our empirical results\ndemonstrate that by unifying bandit and gradient based learning, one obtains\nconsistently improved performance across a wide spectrum of problem\nenvironments. Furthermore, even when gradient feedback is unavailable, the\nflexibility of our model, including gradient prediction, still allows us\noutperform competing approaches, although with a smaller margin. Due to the\npervasiveness of bandit based optimisation, our scheme opens up for improved\nperformance both in meta-optimisation and in applications where gradient\nrelated information is readily available.\n", "versions": [{"version": "v1", "created": "Sun, 28 May 2017 09:55:11 GMT"}], "update_date": "2017-05-30", "authors_parsed": [["Granmo", "Ole-Christoffer", ""]]}, {"id": "1705.09944", "submitter": "SueYeon Chung", "authors": "SueYeon Chung, Uri Cohen, Haim Sompolinsky, Daniel D. Lee", "title": "Learning Data Manifolds with a Cutting Plane Method", "comments": null, "journal-ref": "Neural Computation. Volume:30, Issue:10, (2018) pp.2593-2615", "doi": "10.1162/neco_a_01119", "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of classifying data manifolds where each manifold\nrepresents invariances that are parameterized by continuous degrees of freedom.\nConventional data augmentation methods rely upon sampling large numbers of\ntraining examples from these manifolds; instead, we propose an iterative\nalgorithm called M_{CP} based upon a cutting-plane approach that efficiently\nsolves a quadratic semi-infinite programming problem to find the maximum margin\nsolution. We provide a proof of convergence as well as a polynomial bound on\nthe number of iterations required for a desired tolerance in the objective\nfunction. The efficiency and performance of M_{CP} are demonstrated in\nhigh-dimensional simulations and on image manifolds generated from the ImageNet\ndataset. Our results indicate that M_{CP} is able to rapidly learn good\nclassifiers and shows superior generalization performance compared with\nconventional maximum margin methods using data augmentation methods.\n", "versions": [{"version": "v1", "created": "Sun, 28 May 2017 14:51:11 GMT"}], "update_date": "2018-10-01", "authors_parsed": [["Chung", "SueYeon", ""], ["Cohen", "Uri", ""], ["Sompolinsky", "Haim", ""], ["Lee", "Daniel D.", ""]]}, {"id": "1705.09966", "submitter": "Yongyi Lu", "authors": "Yongyi Lu, Yu-Wing Tai, Chi-Keung Tang", "title": "Attribute-Guided Face Generation Using Conditional CycleGAN", "comments": "ECCV 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We are interested in attribute-guided face generation: given a low-res face\ninput image, an attribute vector that can be extracted from a high-res image\n(attribute image), our new method generates a high-res face image for the\nlow-res input that satisfies the given attributes. To address this problem, we\ncondition the CycleGAN and propose conditional CycleGAN, which is designed to\n1) handle unpaired training data because the training low/high-res and high-res\nattribute images may not necessarily align with each other, and to 2) allow\neasy control of the appearance of the generated face via the input attributes.\nWe demonstrate impressive results on the attribute-guided conditional CycleGAN,\nwhich can synthesize realistic face images with appearance easily controlled by\nuser-supplied attributes (e.g., gender, makeup, hair color, eyeglasses). Using\nthe attribute image as identity to produce the corresponding conditional vector\nand by incorporating a face verification network, the attribute-guided network\nbecomes the identity-guided conditional CycleGAN which produces impressive and\ninteresting results on identity transfer. We demonstrate three applications on\nidentity-guided conditional CycleGAN: identity-preserving face superresolution,\nface swapping, and frontal face generation, which consistently show the\nadvantage of our new method.\n", "versions": [{"version": "v1", "created": "Sun, 28 May 2017 17:37:23 GMT"}, {"version": "v2", "created": "Wed, 14 Nov 2018 13:35:49 GMT"}], "update_date": "2018-11-15", "authors_parsed": [["Lu", "Yongyi", ""], ["Tai", "Yu-Wing", ""], ["Tang", "Chi-Keung", ""]]}, {"id": "1705.09993", "submitter": "John Pavlopoulos", "authors": "John Pavlopoulos and Prodromos Malakasiotis and Ion Androutsopoulos", "title": "Deep Learning for User Comment Moderation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Experimenting with a new dataset of 1.6M user comments from a Greek news\nportal and existing datasets of English Wikipedia comments, we show that an RNN\noutperforms the previous state of the art in moderation. A deep,\nclassification-specific attention mechanism improves further the overall\nperformance of the RNN. We also compare against a CNN and a word-list baseline,\nconsidering both fully automatic and semi-automatic moderation.\n", "versions": [{"version": "v1", "created": "Sun, 28 May 2017 21:12:56 GMT"}, {"version": "v2", "created": "Mon, 17 Jul 2017 15:25:56 GMT"}], "update_date": "2017-07-18", "authors_parsed": [["Pavlopoulos", "John", ""], ["Malakasiotis", "Prodromos", ""], ["Androutsopoulos", "Ion", ""]]}, {"id": "1705.10033", "submitter": "Chao Qin", "authors": "Chao Qin, Diego Klabjan, and Daniel Russo", "title": "Improving the Expected Improvement Algorithm", "comments": "Submitted to NIPS 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The expected improvement (EI) algorithm is a popular strategy for information\ncollection in optimization under uncertainty. The algorithm is widely known to\nbe too greedy, but nevertheless enjoys wide use due to its simplicity and\nability to handle uncertainty and noise in a coherent decision theoretic\nframework. To provide rigorous insight into EI, we study its properties in a\nsimple setting of Bayesian optimization where the domain consists of a finite\ngrid of points. This is the so-called best-arm identification problem, where\nthe goal is to allocate measurement effort wisely to confidently identify the\nbest arm using a small number of measurements. In this framework, one can show\nformally that EI is far from optimal. To overcome this shortcoming, we\nintroduce a simple modification of the expected improvement algorithm.\nSurprisingly, this simple change results in an algorithm that is asymptotically\noptimal for Gaussian best-arm identification problems, and provably outperforms\nstandard EI by an order of magnitude.\n", "versions": [{"version": "v1", "created": "Mon, 29 May 2017 04:02:27 GMT"}], "update_date": "2017-05-30", "authors_parsed": [["Qin", "Chao", ""], ["Klabjan", "Diego", ""], ["Russo", "Daniel", ""]]}, {"id": "1705.10051", "submitter": "Jittat Fakcharoenphol", "authors": "Adisak Supeesun (Kasetsart University, Bangkok, Thailand) and Jittat\n  Fakcharoenphol (Kasetsart University, Bangkok, Thailand)", "title": "Learning Network Structures from Contagion", "comments": null, "journal-ref": "Information Processing Letters, Volume 121, May 2017, Pages 11-16", "doi": "10.1016/j.ipl.2017.01.005", "report-no": null, "categories": "cs.LG cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In 2014, Amin, Heidari, and Kearns proved that tree networks can be learned\nby observing only the infected set of vertices of the contagion process under\nthe independent cascade model, in both the active and passive query models.\nThey also showed empirically that simple extensions of their algorithms work on\nsparse networks. In this work, we focus on the active model. We prove that a\nsimple modification of Amin et al.'s algorithm works on more general classes of\nnetworks, namely (i) networks with large girth and low path growth rate, and\n(ii) networks with bounded degree. This also provides partial theoretical\nexplanation for Amin et al.'s experiments on sparse networks.\n", "versions": [{"version": "v1", "created": "Mon, 29 May 2017 06:53:13 GMT"}], "update_date": "2017-05-30", "authors_parsed": [["Supeesun", "Adisak", "", "Kasetsart University, Bangkok, Thailand"], ["Fakcharoenphol", "Jittat", "", "Kasetsart University, Bangkok, Thailand"]]}, {"id": "1705.10085", "submitter": "Sivan Sabato", "authors": "Eyal Gutflaish, Aryeh Kontorovich, Sivan Sabato, Ofer Biller, Oded\n  Sofer", "title": "Temporal anomaly detection: calibrating the surprise", "comments": "AAAI-19", "journal-ref": "Proceedings of the AAAI Conference on Artificial Intelligence,\n  33(01), 3755-3762 (2019)", "doi": null, "report-no": null, "categories": "cs.CR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a hybrid approach to temporal anomaly detection in access data of\nusers to databases --- or more generally, any kind of subject-object\nco-occurrence data. We consider a high-dimensional setting that also requires\nfast computation at test time. Our methodology identifies anomalies based on a\nsingle stationary model, instead of requiring a full temporal one, which would\nbe prohibitive in this setting. We learn a low-rank stationary model from the\ntraining data, and then fit a regression model for predicting the expected\nlikelihood score of normal access patterns in the future. The disparity between\nthe predicted likelihood score and the observed one is used to assess the\n`surprise' at test time. This approach enables calibration of the anomaly\nscore, so that time-varying normal behavior patterns are not considered\nanomalous. We provide a detailed description of the algorithm, including a\nconvergence analysis, and report encouraging empirical results. One of the data\nsets that we tested, TDA, is new for the public domain. It consists of two\nmonths' worth of database access records from a live system. Our code is\npublicly available at https://github.com/eyalgut/TLR_anomaly_detection.git. The\nTDA data set is available at\nhttps://www.kaggle.com/eyalgut/binary-traffic-matrices.\n", "versions": [{"version": "v1", "created": "Mon, 29 May 2017 09:16:34 GMT"}, {"version": "v2", "created": "Tue, 20 Nov 2018 07:25:54 GMT"}], "update_date": "2019-08-13", "authors_parsed": [["Gutflaish", "Eyal", ""], ["Kontorovich", "Aryeh", ""], ["Sabato", "Sivan", ""], ["Biller", "Ofer", ""], ["Sofer", "Oded", ""]]}, {"id": "1705.10087", "submitter": "Thomas Moreau", "authors": "Thomas Moreau, Laurent Oudre, Nicolas Vayatis", "title": "DICOD: Distributed Convolutional Sparse Coding", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we introduce DICOD, a convolutional sparse coding algorithm\nwhich builds shift invariant representations for long signals. This algorithm\nis designed to run in a distributed setting, with local message passing, making\nit communication efficient. It is based on coordinate descent and uses locally\ngreedy updates which accelerate the resolution compared to greedy coordinate\nselection. We prove the convergence of this algorithm and highlight its\ncomputational speed-up which is super-linear in the number of cores used. We\nalso provide empirical evidence for the acceleration properties of our\nalgorithm compared to state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Mon, 29 May 2017 09:21:30 GMT"}, {"version": "v2", "created": "Sun, 13 May 2018 14:04:25 GMT"}], "update_date": "2018-05-15", "authors_parsed": [["Moreau", "Thomas", ""], ["Oudre", "Laurent", ""], ["Vayatis", "Nicolas", ""]]}, {"id": "1705.10102", "submitter": "Jiasen Yang", "authors": "Agniva Chowdhury, Jiasen Yang, Petros Drineas", "title": "Structural Conditions for Projection-Cost Preservation via Randomized\n  Matrix Multiplication", "comments": "16 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Projection-cost preservation is a low-rank approximation guarantee which\nensures that the cost of any rank-$k$ projection can be preserved using a\nsmaller sketch of the original data matrix. We present a general structural\nresult outlining four sufficient conditions to achieve projection-cost\npreservation. These conditions can be satisfied using tools from the Randomized\nLinear Algebra literature.\n", "versions": [{"version": "v1", "created": "Mon, 29 May 2017 10:29:23 GMT"}, {"version": "v2", "created": "Fri, 17 Aug 2018 21:38:55 GMT"}], "update_date": "2018-08-21", "authors_parsed": [["Chowdhury", "Agniva", ""], ["Yang", "Jiasen", ""], ["Drineas", "Petros", ""]]}, {"id": "1705.10119", "submitter": "Jiaxin Shi", "authors": "Jiaxin Shi, Shengyang Sun, Jun Zhu", "title": "Kernel Implicit Variational Inference", "comments": "Published as a conference paper at ICLR 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent progress in variational inference has paid much attention to the\nflexibility of variational posteriors. One promising direction is to use\nimplicit distributions, i.e., distributions without tractable densities as the\nvariational posterior. However, existing methods on implicit posteriors still\nface challenges of noisy estimation and computational infeasibility when\napplied to models with high-dimensional latent variables. In this paper, we\npresent a new approach named Kernel Implicit Variational Inference that\naddresses these challenges. As far as we know, for the first time implicit\nvariational inference is successfully applied to Bayesian neural networks,\nwhich shows promising results on both regression and classification tasks.\n", "versions": [{"version": "v1", "created": "Mon, 29 May 2017 11:11:35 GMT"}, {"version": "v2", "created": "Thu, 22 Feb 2018 13:49:00 GMT"}, {"version": "v3", "created": "Fri, 23 Feb 2018 15:45:59 GMT"}], "update_date": "2018-02-26", "authors_parsed": [["Shi", "Jiaxin", ""], ["Sun", "Shengyang", ""], ["Zhu", "Jun", ""]]}, {"id": "1705.10134", "submitter": "Egor Malykh", "authors": "Egor Malykh, Sergey Novoselov, Oleg Kudashev", "title": "On Residual CNN in text-dependent speaker verification task", "comments": "Accepted for Specom 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning approaches are still not very common in the speaker\nverification field. We investigate the possibility of using deep residual\nconvolutional neural network with spectrograms as an input features in the\ntext-dependent speaker verification task. Despite the fact that we were not\nable to surpass the baseline system in quality, we achieved a quite good\nresults for such a new approach getting an 5.23% ERR on the RSR2015 evaluation\npart. Fusion of the baseline and proposed systems outperformed the best\nindividual system by 18% relatively.\n", "versions": [{"version": "v1", "created": "Mon, 29 May 2017 11:50:57 GMT"}, {"version": "v2", "created": "Tue, 30 May 2017 13:17:47 GMT"}], "update_date": "2017-05-31", "authors_parsed": [["Malykh", "Egor", ""], ["Novoselov", "Sergey", ""], ["Kudashev", "Oleg", ""]]}, {"id": "1705.10142", "submitter": "Cijo Jose", "authors": "Cijo Jose, Moustpaha Cisse and Francois Fleuret", "title": "Kronecker Recurrent Units", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Our work addresses two important issues with recurrent neural networks: (1)\nthey are over-parameterized, and (2) the recurrence matrix is ill-conditioned.\nThe former increases the sample complexity of learning and the training time.\nThe latter causes the vanishing and exploding gradient problem. We present a\nflexible recurrent neural network model called Kronecker Recurrent Units (KRU).\nKRU achieves parameter efficiency in RNNs through a Kronecker factored\nrecurrent matrix. It overcomes the ill-conditioning of the recurrent matrix by\nenforcing soft unitary constraints on the factors. Thanks to the small\ndimensionality of the factors, maintaining these constraints is computationally\nefficient. Our experimental results on seven standard data-sets reveal that KRU\ncan reduce the number of parameters by three orders of magnitude in the\nrecurrent weight matrix compared to the existing recurrent models, without\ntrading the statistical performance. These results in particular show that\nwhile there are advantages in having a high dimensional recurrent space, the\ncapacity of the recurrent part of the model can be dramatically reduced.\n", "versions": [{"version": "v1", "created": "Mon, 29 May 2017 12:14:45 GMT"}, {"version": "v2", "created": "Tue, 30 May 2017 08:32:37 GMT"}, {"version": "v3", "created": "Wed, 31 May 2017 08:55:38 GMT"}, {"version": "v4", "created": "Wed, 26 Jul 2017 11:05:47 GMT"}, {"version": "v5", "created": "Fri, 27 Oct 2017 09:33:54 GMT"}, {"version": "v6", "created": "Fri, 10 Nov 2017 10:49:54 GMT"}, {"version": "v7", "created": "Sun, 31 Dec 2017 12:07:27 GMT"}], "update_date": "2018-01-03", "authors_parsed": [["Jose", "Cijo", ""], ["Cisse", "Moustpaha", ""], ["Fleuret", "Francois", ""]]}, {"id": "1705.10152", "submitter": "Benjamin Kutschan", "authors": "Benjamin Kutschan", "title": "Tangent Cones to TT Varieties", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.LG math.AG math.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As already done for the matrix case for example in [Joe Harris, Algebraic\nGeometry - A first course, p.256] we give a parametrization of the Bouligand\ntangent cone of the variety of tensors of bounded TT rank. We discuss how the\nproof generalizes to any binary hierarchical format. The parametrization can be\nrewritten as an orthogonal sum of TT tensors. Its retraction onto the variety\nis particularly easy to compose. We also give an implicit description of the\ntangent cone as the solution of a system of polynomial equations.\n", "versions": [{"version": "v1", "created": "Mon, 29 May 2017 12:46:32 GMT"}], "update_date": "2017-05-30", "authors_parsed": [["Kutschan", "Benjamin", ""]]}, {"id": "1705.10182", "submitter": "Taiji Suzuki", "authors": "Taiji Suzuki", "title": "Fast learning rate of deep learning via a kernel perspective", "comments": "36 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.LG stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop a new theoretical framework to analyze the generalization error of\ndeep learning, and derive a new fast learning rate for two representative\nalgorithms: empirical risk minimization and Bayesian deep learning. The series\nof theoretical analyses of deep learning has revealed its high expressive power\nand universal approximation capability. Although these analyses are highly\nnonparametric, existing generalization error analyses have been developed\nmainly in a fixed dimensional parametric model. To compensate this gap, we\ndevelop an infinite dimensional model that is based on an integral form as\nperformed in the analysis of the universal approximation capability. This\nallows us to define a reproducing kernel Hilbert space corresponding to each\nlayer. Our point of view is to deal with the ordinary finite dimensional deep\nneural network as a finite approximation of the infinite dimensional one. The\napproximation error is evaluated by the degree of freedom of the reproducing\nkernel Hilbert space in each layer. To estimate a good finite dimensional\nmodel, we consider both of empirical risk minimization and Bayesian deep\nlearning. We derive its generalization error bound and it is shown that there\nappears bias-variance trade-off in terms of the number of parameters of the\nfinite dimensional approximation. We show that the optimal width of the\ninternal layers can be determined through the degree of freedom and the\nconvergence rate can be faster than $O(1/\\sqrt{n})$ rate which has been shown\nin the existing studies.\n", "versions": [{"version": "v1", "created": "Mon, 29 May 2017 13:47:44 GMT"}], "update_date": "2017-05-31", "authors_parsed": [["Suzuki", "Taiji", ""]]}, {"id": "1705.10194", "submitter": "Feng Nan", "authors": "Feng Nan, Venkatesh Saligrama", "title": "Adaptive Classification for Prediction Under a Budget", "comments": "arXiv admin note: substantial text overlap with arXiv:1704.07505", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel adaptive approximation approach for test-time\nresource-constrained prediction. Given an input instance at test-time, a gating\nfunction identifies a prediction model for the input among a collection of\nmodels. Our objective is to minimize overall average cost without sacrificing\naccuracy. We learn gating and prediction models on fully labeled training data\nby means of a bottom-up strategy. Our novel bottom-up method first trains a\nhigh-accuracy complex model. Then a low-complexity gating and prediction model\nare subsequently learned to adaptively approximate the high-accuracy model in\nregions where low-cost models are capable of making highly accurate\npredictions. We pose an empirical loss minimization problem with cost\nconstraints to jointly train gating and prediction models. On a number of\nbenchmark datasets our method outperforms state-of-the-art achieving higher\naccuracy for the same cost.\n", "versions": [{"version": "v1", "created": "Fri, 26 May 2017 12:28:42 GMT"}], "update_date": "2017-05-30", "authors_parsed": [["Nan", "Feng", ""], ["Saligrama", "Venkatesh", ""]]}, {"id": "1705.10202", "submitter": "Niek Tax", "authors": "Niek Tax, Natalia Sidorova, Reinder Haakma, Wil M.P. van der Aalst", "title": "Mining Process Model Descriptions of Daily Life through Event\n  Abstraction", "comments": "arXiv admin note: substantial text overlap with arXiv:1606.07283", "journal-ref": "Studies in Computational Intelligence, 751 (2017) 83-104", "doi": "10.1007/978-3-319-69266-1_5", "report-no": null, "categories": "cs.LG cs.AI cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Process mining techniques focus on extracting insight in processes from event\nlogs. Process mining has the potential to provide valuable insights in\n(un)healthy habits and to contribute to ambient assisted living solutions when\napplied on data from smart home environments. However, events recorded in smart\nhome environments are on the level of sensor triggers, at which process\ndiscovery algorithms produce overgeneralizing process models that allow for too\nmuch behavior and that are difficult to interpret for human experts. We show\nthat abstracting the events to a higher-level interpretation can enable\ndiscovery of more precise and more comprehensible models. We present a\nframework for the extraction of features that can be used for abstraction with\nsupervised learning methods that is based on the XES IEEE standard for event\nlogs. This framework can automatically abstract sensor-level events to their\ninterpretation at the human activity level, after training it on training data\nfor which both the sensor and human activity events are known. We demonstrate\nour abstraction framework on three real-life smart home event logs and show\nthat the process models that can be discovered after abstraction are more\nprecise indeed.\n", "versions": [{"version": "v1", "created": "Thu, 25 May 2017 20:32:56 GMT"}], "update_date": "2018-01-11", "authors_parsed": [["Tax", "Niek", ""], ["Sidorova", "Natalia", ""], ["Haakma", "Reinder", ""], ["van der Aalst", "Wil M. P.", ""]]}, {"id": "1705.10209", "submitter": "Micha{\\l} Zapotoczny", "authors": "Micha{\\l} Zapotoczny, Pawe{\\l} Rychlikowski, and Jan Chorowski", "title": "On Multilingual Training of Neural Dependency Parsers", "comments": "preprint accepted into the TSD2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We show that a recently proposed neural dependency parser can be improved by\njoint training on multiple languages from the same family. The parser is\nimplemented as a deep neural network whose only input is orthographic\nrepresentations of words. In order to successfully parse, the network has to\ndiscover how linguistically relevant concepts can be inferred from word\nspellings. We analyze the representations of characters and words that are\nlearned by the network to establish which properties of languages were\naccounted for. In particular we show that the parser has approximately learned\nto associate Latin characters with their Cyrillic counterparts and that it can\ngroup Polish and Russian words that have a similar grammatical function.\nFinally, we evaluate the parser on selected languages from the Universal\nDependencies dataset and show that it is competitive with other recently\nproposed state-of-the art methods, while having a simple structure.\n", "versions": [{"version": "v1", "created": "Mon, 29 May 2017 14:24:08 GMT"}], "update_date": "2017-05-30", "authors_parsed": [["Zapotoczny", "Micha\u0142", ""], ["Rychlikowski", "Pawe\u0142", ""], ["Chorowski", "Jan", ""]]}, {"id": "1705.10229", "submitter": "Tsung-Hsien Wen", "authors": "Tsung-Hsien Wen, Yishu Miao, Phil Blunsom, Steve Young", "title": "Latent Intention Dialogue Models", "comments": "Accepted at ICML 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Developing a dialogue agent that is capable of making autonomous decisions\nand communicating by natural language is one of the long-term goals of machine\nlearning research. Traditional approaches either rely on hand-crafting a small\nstate-action set for applying reinforcement learning that is not scalable or\nconstructing deterministic models for learning dialogue sentences that fail to\ncapture natural conversational variability. In this paper, we propose a Latent\nIntention Dialogue Model (LIDM) that employs a discrete latent variable to\nlearn underlying dialogue intentions in the framework of neural variational\ninference. In a goal-oriented dialogue scenario, these latent intentions can be\ninterpreted as actions guiding the generation of machine responses, which can\nbe further refined autonomously by reinforcement learning. The experimental\nevaluation of LIDM shows that the model out-performs published benchmarks for\nboth corpus-based and human evaluation, demonstrating the effectiveness of\ndiscrete latent variable models for learning goal-oriented dialogues.\n", "versions": [{"version": "v1", "created": "Mon, 29 May 2017 15:01:44 GMT"}], "update_date": "2017-05-30", "authors_parsed": [["Wen", "Tsung-Hsien", ""], ["Miao", "Yishu", ""], ["Blunsom", "Phil", ""], ["Young", "Steve", ""]]}, {"id": "1705.10245", "submitter": "Tristan Sylvain", "authors": "Margaux Luck, Tristan Sylvain, H\\'elo\\\"ise Cardinal, Andrea Lodi,\n  Yoshua Bengio", "title": "Deep Learning for Patient-Specific Kidney Graft Survival Analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An accurate model of patient-specific kidney graft survival distributions can\nhelp to improve shared-decision making in the treatment and care of patients.\nIn this paper, we propose a deep learning method that directly models the\nsurvival function instead of estimating the hazard function to predict survival\ntimes for graft patients based on the principle of multi-task learning. By\nlearning to jointly predict the time of the event, and its rank in the cox\npartial log likelihood framework, our deep learning approach outperforms, in\nterms of survival time prediction quality and concordance index, other common\nmethods for survival analysis, including the Cox Proportional Hazards model and\na network trained on the cox partial log-likelihood.\n", "versions": [{"version": "v1", "created": "Mon, 29 May 2017 15:17:14 GMT"}], "update_date": "2017-05-30", "authors_parsed": [["Luck", "Margaux", ""], ["Sylvain", "Tristan", ""], ["Cardinal", "H\u00e9lo\u00efse", ""], ["Lodi", "Andrea", ""], ["Bengio", "Yoshua", ""]]}, {"id": "1705.10246", "submitter": "Gil Keren", "authors": "Gil Keren, Sivan Sabato, Bj\\\"orn Schuller", "title": "Fast Single-Class Classification and the Principle of Logit Separation", "comments": "Published as a conference paper in ICDM 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider neural network training, in applications in which there are many\npossible classes, but at test-time, the task is a binary classification task of\ndetermining whether the given example belongs to a specific class, where the\nclass of interest can be different each time the classifier is applied. For\ninstance, this is the case for real-time image search. We define the Single\nLogit Classification (SLC) task: training the network so that at test-time, it\nwould be possible to accurately identify whether the example belongs to a given\nclass in a computationally efficient manner, based only on the output logit for\nthis class. We propose a natural principle, the Principle of Logit Separation,\nas a guideline for choosing and designing losses suitable for the SLC. We show\nthat the cross-entropy loss function is not aligned with the Principle of Logit\nSeparation. In contrast, there are known loss functions, as well as novel batch\nloss functions that we propose, which are aligned with this principle. In\ntotal, we study seven loss functions. Our experiments show that indeed in\nalmost all cases, losses that are aligned with the Principle of Logit\nSeparation obtain at least 20% relative accuracy improvement in the SLC task\ncompared to losses that are not aligned with it, and sometimes considerably\nmore. Furthermore, we show that fast SLC does not cause any drop in binary\nclassification accuracy, compared to standard classification in which all\nlogits are computed, and yields a speedup which grows with the number of\nclasses. For instance, we demonstrate a 10x speedup when the number of classes\nis 400,000. Tensorflow code for optimizing the new batch losses is publicly\navailable at https://github.com/cruvadom/Logit Separation.\n", "versions": [{"version": "v1", "created": "Mon, 29 May 2017 15:18:59 GMT"}, {"version": "v2", "created": "Thu, 20 Jul 2017 13:44:15 GMT"}, {"version": "v3", "created": "Thu, 26 Oct 2017 08:35:28 GMT"}, {"version": "v4", "created": "Mon, 17 Sep 2018 15:05:39 GMT"}], "update_date": "2018-09-18", "authors_parsed": [["Keren", "Gil", ""], ["Sabato", "Sivan", ""], ["Schuller", "Bj\u00f6rn", ""]]}, {"id": "1705.10257", "submitter": "Gergely Neu", "authors": "Nicol\\`o Cesa-Bianchi and Claudio Gentile and G\\'abor Lugosi and\n  Gergely Neu", "title": "Boltzmann Exploration Done Right", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Boltzmann exploration is a classic strategy for sequential decision-making\nunder uncertainty, and is one of the most standard tools in Reinforcement\nLearning (RL). Despite its widespread use, there is virtually no theoretical\nunderstanding about the limitations or the actual benefits of this exploration\nscheme. Does it drive exploration in a meaningful way? Is it prone to\nmisidentifying the optimal actions or spending too much time exploring the\nsuboptimal ones? What is the right tuning for the learning rate? In this paper,\nwe address several of these questions in the classic setup of stochastic\nmulti-armed bandits. One of our main results is showing that the Boltzmann\nexploration strategy with any monotone learning-rate sequence will induce\nsuboptimal behavior. As a remedy, we offer a simple non-monotone schedule that\nguarantees near-optimal performance, albeit only when given prior access to key\nproblem parameters that are typically not available in practical situations\n(like the time horizon $T$ and the suboptimality gap $\\Delta$). More\nimportantly, we propose a novel variant that uses different learning rates for\ndifferent arms, and achieves a distribution-dependent regret bound of order\n$\\frac{K\\log^2 T}{\\Delta}$ and a distribution-independent bound of order\n$\\sqrt{KT}\\log K$ without requiring such prior knowledge. To demonstrate the\nflexibility of our technique, we also propose a variant that guarantees the\nsame performance bounds even if the rewards are heavy-tailed.\n", "versions": [{"version": "v1", "created": "Mon, 29 May 2017 15:33:29 GMT"}, {"version": "v2", "created": "Tue, 7 Nov 2017 18:08:29 GMT"}], "update_date": "2017-11-08", "authors_parsed": [["Cesa-Bianchi", "Nicol\u00f2", ""], ["Gentile", "Claudio", ""], ["Lugosi", "G\u00e1bor", ""], ["Neu", "Gergely", ""]]}, {"id": "1705.10301", "submitter": "Maruan Al-Shedivat", "authors": "Maruan Al-Shedivat, Avinava Dubey, Eric P. Xing", "title": "Contextual Explanation Networks", "comments": "48 pages, 18 figures, to appear in JMLR", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modern learning algorithms excel at producing accurate but complex models of\nthe data. However, deploying such models in the real-world requires extra care:\nwe must ensure their reliability, robustness, and absence of undesired biases.\nThis motivates the development of models that are equally accurate but can be\nalso easily inspected and assessed beyond their predictive performance. To this\nend, we introduce contextual explanation networks (CEN)---a class of\narchitectures that learn to predict by generating and utilizing intermediate,\nsimplified probabilistic models. Specifically, CENs generate parameters for\nintermediate graphical models which are further used for prediction and play\nthe role of explanations. Contrary to the existing post-hoc model-explanation\ntools, CENs learn to predict and to explain simultaneously. Our approach offers\ntwo major advantages: (i) for each prediction valid, instance-specific\nexplanation is generated with no computational overhead and (ii) prediction via\nexplanation acts as a regularizer and boosts performance in data-scarce\nsettings. We analyze the proposed framework theoretically and experimentally.\nOur results on image and text classification and survival analysis tasks\ndemonstrate that CENs are not only competitive with the state-of-the-art\nmethods but also offer additional insights behind each prediction, that can be\nvaluable for decision support. We also show that while post-hoc methods may\nproduce misleading explanations in certain cases, CENs are consistent and allow\nto detect such cases systematically.\n", "versions": [{"version": "v1", "created": "Mon, 29 May 2017 17:39:51 GMT"}, {"version": "v2", "created": "Tue, 30 Jan 2018 00:06:02 GMT"}, {"version": "v3", "created": "Tue, 18 Dec 2018 22:33:40 GMT"}, {"version": "v4", "created": "Wed, 9 Sep 2020 14:20:44 GMT"}], "update_date": "2020-09-10", "authors_parsed": [["Al-Shedivat", "Maruan", ""], ["Dubey", "Avinava", ""], ["Xing", "Eric P.", ""]]}, {"id": "1705.10312", "submitter": "Dajiang Zhu", "authors": "Dajiang Zhu, Brandalyn C. Riedel, Neda Jahanshad, Nynke A. Groenewold,\n  Dan J. Stein, Ian H. Gotlib, Matthew D. Sacchet, Danai Dima, James H. Cole,\n  Cynthia H.Y. Fu, Henrik Walter, Ilya M. Veer, Thomas Frodl, Lianne Schmaal,\n  Dick J. Veltman, Paul M. Thompson", "title": "Classification of Major Depressive Disorder via Multi-Site Weighted\n  LASSO Model", "comments": "Accepted by MICCAI 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CE stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Large-scale collaborative analysis of brain imaging data, in psychiatry and\nneu-rology, offers a new source of statistical power to discover features that\nboost ac-curacy in disease classification, differential diagnosis, and outcome\nprediction. However, due to data privacy regulations or limited accessibility\nto large datasets across the world, it is challenging to efficiently integrate\ndistributed information. Here we propose a novel classification framework\nthrough multi-site weighted LASSO: each site performs an iterative weighted\nLASSO for feature selection separately. Within each iteration, the\nclassification result and the selected features are collected to update the\nweighting parameters for each feature. This new weight is used to guide the\nLASSO process at the next iteration. Only the fea-tures that help to improve\nthe classification accuracy are preserved. In tests on da-ta from five sites\n(299 patients with major depressive disorder (MDD) and 258 normal controls),\nour method boosted classification accuracy for MDD by 4.9% on average. This\nresult shows the potential of the proposed new strategy as an ef-fective and\npractical collaborative platform for machine learning on large scale\ndistributed imaging and biobank data.\n", "versions": [{"version": "v1", "created": "Fri, 26 May 2017 21:19:22 GMT"}, {"version": "v2", "created": "Sat, 3 Jun 2017 18:54:04 GMT"}], "update_date": "2017-06-06", "authors_parsed": [["Zhu", "Dajiang", ""], ["Riedel", "Brandalyn C.", ""], ["Jahanshad", "Neda", ""], ["Groenewold", "Nynke A.", ""], ["Stein", "Dan J.", ""], ["Gotlib", "Ian H.", ""], ["Sacchet", "Matthew D.", ""], ["Dima", "Danai", ""], ["Cole", "James H.", ""], ["Fu", "Cynthia H. Y.", ""], ["Walter", "Henrik", ""], ["Veer", "Ilya M.", ""], ["Frodl", "Thomas", ""], ["Schmaal", "Lianne", ""], ["Veltman", "Dick J.", ""], ["Thompson", "Paul M.", ""]]}, {"id": "1705.10342", "submitter": "Thomas Lukasiewicz", "authors": "Patrick Hohenecker and Thomas Lukasiewicz", "title": "Deep Learning for Ontology Reasoning", "comments": "9 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we present a novel approach to ontology reasoning that is based\non deep learning rather than logic-based formal reasoning. To this end, we\nintroduce a new model for statistical relational learning that is built upon\ndeep recursive neural networks, and give experimental evidence that it can\neasily compete with, or even outperform, existing logic-based reasoners on the\ntask of ontology reasoning. More precisely, we compared our implemented system\nwith one of the best logic-based ontology reasoners at present, RDFox, on a\nnumber of large standard benchmark datasets, and found that our system attained\nhigh reasoning quality, while being up to two orders of magnitude faster.\n", "versions": [{"version": "v1", "created": "Mon, 29 May 2017 18:17:52 GMT"}], "update_date": "2017-05-31", "authors_parsed": [["Hohenecker", "Patrick", ""], ["Lukasiewicz", "Thomas", ""]]}, {"id": "1705.10359", "submitter": "Benjamin Chamberlain", "authors": "Benjamin Paul Chamberlain, James Clough, Marc Peter Deisenroth", "title": "Neural Embeddings of Graphs in Hyperbolic Space", "comments": "7 pages, 5 figures", "journal-ref": "13th international workshop on mining and learning from graphs\n  held in conjunction with KDD, 2017", "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural embeddings have been used with great success in Natural Language\nProcessing (NLP). They provide compact representations that encapsulate word\nsimilarity and attain state-of-the-art performance in a range of linguistic\ntasks. The success of neural embeddings has prompted significant amounts of\nresearch into applications in domains other than language. One such domain is\ngraph-structured data, where embeddings of vertices can be learned that\nencapsulate vertex similarity and improve performance on tasks including edge\nprediction and vertex labelling. For both NLP and graph based tasks, embeddings\nhave been learned in high-dimensional Euclidean spaces. However, recent work\nhas shown that the appropriate isometric space for embedding complex networks\nis not the flat Euclidean space, but negatively curved, hyperbolic space. We\npresent a new concept that exploits these recent insights and propose learning\nneural embeddings of graphs in hyperbolic space. We provide experimental\nevidence that embedding graphs in their natural geometry significantly improves\nperformance on downstream tasks for several real-world public datasets.\n", "versions": [{"version": "v1", "created": "Mon, 29 May 2017 18:47:30 GMT"}], "update_date": "2018-09-20", "authors_parsed": [["Chamberlain", "Benjamin Paul", ""], ["Clough", "James", ""], ["Deisenroth", "Marc Peter", ""]]}, {"id": "1705.10369", "submitter": "Andrew Drozdov", "authors": "Katrina Evtimova, Andrew Drozdov, Douwe Kiela, Kyunghyun Cho", "title": "Emergent Communication in a Multi-Modal, Multi-Step Referential Game", "comments": "Published as a conference paper at ICLR 2018. 12 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL cs.CV cs.IT cs.MA math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Inspired by previous work on emergent communication in referential games, we\npropose a novel multi-modal, multi-step referential game, where the sender and\nreceiver have access to distinct modalities of an object, and their information\nexchange is bidirectional and of arbitrary duration. The multi-modal multi-step\nsetting allows agents to develop an internal communication significantly closer\nto natural language, in that they share a single set of messages, and that the\nlength of the conversation may vary according to the difficulty of the task. We\nexamine these properties empirically using a dataset consisting of images and\ntextual descriptions of mammals, where the agents are tasked with identifying\nthe correct object. Our experiments indicate that a robust and efficient\ncommunication protocol emerges, where gradual information exchange informs\nbetter predictions and higher communication bandwidth improves generalization.\n", "versions": [{"version": "v1", "created": "Mon, 29 May 2017 19:25:49 GMT"}, {"version": "v2", "created": "Mon, 30 Oct 2017 17:09:14 GMT"}, {"version": "v3", "created": "Mon, 5 Mar 2018 04:07:30 GMT"}, {"version": "v4", "created": "Mon, 16 Apr 2018 19:22:22 GMT"}], "update_date": "2018-04-18", "authors_parsed": [["Evtimova", "Katrina", ""], ["Drozdov", "Andrew", ""], ["Kiela", "Douwe", ""], ["Cho", "Kyunghyun", ""]]}, {"id": "1705.10385", "submitter": "Minje Kim", "authors": "Minje Kim", "title": "Collaborative Deep Learning for Speech Enhancement: A Run-Time Model\n  Selection Method Using Autoencoders", "comments": null, "journal-ref": "Proc. of the IEEE International Conference on Acoustics, Speech\n  and Signal Processing (ICASSP), pp 76-80, March 2017", "doi": null, "report-no": null, "categories": "cs.SD cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We show that a Modular Neural Network (MNN) can combine various speech\nenhancement modules, each of which is a Deep Neural Network (DNN) specialized\non a particular enhancement job. Differently from an ordinary ensemble\ntechnique that averages variations in models, the propose MNN selects the best\nmodule for the unseen test signal to produce a greedy ensemble. We see this as\nCollaborative Deep Learning (CDL), because it can reuse various already-trained\nDNN models without any further refining. In the proposed MNN selecting the best\nmodule during run time is challenging. To this end, we employ a speech\nAutoEncoder (AE) as an arbitrator, whose input and output are trained to be as\nsimilar as possible if its input is clean speech. Therefore, the AE can gauge\nthe quality of the module-specific denoised result by seeing its AE\nreconstruction error, e.g. low error means that the module output is similar to\nclean speech. We propose an MNN structure with various modules that are\nspecialized on dealing with a specific noise type, gender, and input\nSignal-to-Noise Ratio (SNR) value, and empirically prove that it almost always\nworks better than an arbitrarily chosen DNN module and sometimes as good as an\noracle result.\n", "versions": [{"version": "v1", "created": "Mon, 29 May 2017 20:30:24 GMT"}], "update_date": "2017-05-31", "authors_parsed": [["Kim", "Minje", ""]]}, {"id": "1705.10405", "submitter": "Nicolas Le Roux", "authors": "Cl\\'ement Calauz\\`enes and Nicolas Le Roux", "title": "Distributed SAGA: Maintaining linear convergence rate with limited\n  communication", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, variance-reducing stochastic methods have shown great\npractical performance, exhibiting linear convergence rate when other stochastic\nmethods offered a sub-linear rate. However, as datasets grow ever bigger and\nclusters become widespread, the need for fast distribution methods is pressing.\nWe propose here a distribution scheme for SAGA which maintains a linear\nconvergence rate, even when communication between nodes is limited.\n", "versions": [{"version": "v1", "created": "Mon, 29 May 2017 21:53:01 GMT"}], "update_date": "2017-05-31", "authors_parsed": [["Calauz\u00e8nes", "Cl\u00e9ment", ""], ["Roux", "Nicolas Le", ""]]}, {"id": "1705.10412", "submitter": "Simon Du", "authors": "Simon S. Du, Chi Jin, Jason D. Lee, Michael I. Jordan, Barnabas\n  Poczos, Aarti Singh", "title": "Gradient Descent Can Take Exponential Time to Escape Saddle Points", "comments": "Accepted by NIPS 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Although gradient descent (GD) almost always escapes saddle points\nasymptotically [Lee et al., 2016], this paper shows that even with fairly\nnatural random initialization schemes and non-pathological functions, GD can be\nsignificantly slowed down by saddle points, taking exponential time to escape.\nOn the other hand, gradient descent with perturbations [Ge et al., 2015, Jin et\nal., 2017] is not slowed down by saddle points - it can find an approximate\nlocal minimizer in polynomial time. This result implies that GD is inherently\nslower than perturbed GD, and justifies the importance of adding perturbations\nfor efficient non-convex optimization. While our focus is theoretical, we also\npresent experiments that illustrate our theoretical findings.\n", "versions": [{"version": "v1", "created": "Mon, 29 May 2017 23:03:01 GMT"}, {"version": "v2", "created": "Sun, 5 Nov 2017 16:35:27 GMT"}], "update_date": "2017-11-07", "authors_parsed": [["Du", "Simon S.", ""], ["Jin", "Chi", ""], ["Lee", "Jason D.", ""], ["Jordan", "Michael I.", ""], ["Poczos", "Barnabas", ""], ["Singh", "Aarti", ""]]}, {"id": "1705.10417", "submitter": "Jonathan Gryak", "authors": "Jonathan Gryak, Robert M. Haralick, Delaram Kahrobaei", "title": "Solving the Conjugacy Decision Problem via Machine Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.GR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Machine learning and pattern recognition techniques have been successfully\napplied to algorithmic problems in free groups. In this paper, we seek to\nextend these techniques to finitely presented non-free groups, with a\nparticular emphasis on polycyclic and metabelian groups that are of interest to\nnon-commutative cryptography.\n  As a prototypical example, we utilize supervised learning methods to\nconstruct classifiers that can solve the conjugacy decision problem, i.e.,\ndetermine whether or not a pair of elements from a specified group are\nconjugate. The accuracies of classifiers created using decision trees, random\nforests, and N-tuple neural network models are evaluated for several non-free\ngroups. The very high accuracy of these classifiers suggests an underlying\nmathematical relationship with respect to conjugacy in the tested groups.\n", "versions": [{"version": "v1", "created": "Tue, 30 May 2017 00:01:07 GMT"}, {"version": "v2", "created": "Wed, 21 Feb 2018 01:14:57 GMT"}], "update_date": "2018-02-22", "authors_parsed": [["Gryak", "Jonathan", ""], ["Haralick", "Robert M.", ""], ["Kahrobaei", "Delaram", ""]]}, {"id": "1705.10422", "submitter": "Guan-Horng Liu", "authors": "Guan-Horng Liu, Avinash Siravuru, Sai Prabhakar, Manuela Veloso,\n  George Kantor", "title": "Learning End-to-end Multimodal Sensor Policies for Autonomous Navigation", "comments": "to be published in Conference on Robot Learning (CoRL), 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multisensory polices are known to enhance both state estimation and target\ntracking. However, in the space of end-to-end sensorimotor control, this\nmulti-sensor outlook has received limited attention. Moreover, systematic ways\nto make policies robust to partial sensor failure are not well explored. In\nthis work, we propose a specific customization of Dropout, called\n\\textit{Sensor Dropout}, to improve multisensory policy robustness and handle\npartial failure in the sensor-set. We also introduce an additional auxiliary\nloss on the policy network in order to reduce variance in the band of potential\nmulti- and uni-sensory policies to reduce jerks during policy switching\ntriggered by an abrupt sensor failure or deactivation/activation. Finally,\nthrough the visualization of gradients, we show that the learned policies are\nconditioned on the same latent states representation despite having diverse\nobservations spaces - a hallmark of true sensor-fusion. Simulation results of\nthe multisensory policy, as visualized in TORCS racing game, can be seen here:\nhttps://youtu.be/QAK2lcXjNZc.\n", "versions": [{"version": "v1", "created": "Tue, 30 May 2017 00:52:24 GMT"}, {"version": "v2", "created": "Wed, 1 Nov 2017 02:30:51 GMT"}], "update_date": "2017-11-02", "authors_parsed": [["Liu", "Guan-Horng", ""], ["Siravuru", "Avinash", ""], ["Prabhakar", "Sai", ""], ["Veloso", "Manuela", ""], ["Kantor", "George", ""]]}, {"id": "1705.10461", "submitter": "Lars Mescheder", "authors": "Lars Mescheder, Sebastian Nowozin, Andreas Geiger", "title": "The Numerics of GANs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we analyze the numerics of common algorithms for training\nGenerative Adversarial Networks (GANs). Using the formalism of smooth\ntwo-player games we analyze the associated gradient vector field of GAN\ntraining objectives. Our findings suggest that the convergence of current\nalgorithms suffers due to two factors: i) presence of eigenvalues of the\nJacobian of the gradient vector field with zero real-part, and ii) eigenvalues\nwith big imaginary part. Using these findings, we design a new algorithm that\novercomes some of these limitations and has better convergence properties.\nExperimentally, we demonstrate its superiority on training common GAN\narchitectures and show convergence on GAN architectures that are known to be\nnotoriously hard to train.\n", "versions": [{"version": "v1", "created": "Tue, 30 May 2017 05:54:59 GMT"}, {"version": "v2", "created": "Mon, 27 Nov 2017 18:45:00 GMT"}, {"version": "v3", "created": "Mon, 11 Jun 2018 13:12:41 GMT"}], "update_date": "2018-06-12", "authors_parsed": [["Mescheder", "Lars", ""], ["Nowozin", "Sebastian", ""], ["Geiger", "Andreas", ""]]}, {"id": "1705.10467", "submitter": "Virginia Smith", "authors": "Virginia Smith, Chao-Kai Chiang, Maziar Sanjabi, Ameet Talwalkar", "title": "Federated Multi-Task Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Federated learning poses new statistical and systems challenges in training\nmachine learning models over distributed networks of devices. In this work, we\nshow that multi-task learning is naturally suited to handle the statistical\nchallenges of this setting, and propose a novel systems-aware optimization\nmethod, MOCHA, that is robust to practical systems issues. Our method and\ntheory for the first time consider issues of high communication cost,\nstragglers, and fault tolerance for distributed multi-task learning. The\nresulting method achieves significant speedups compared to alternatives in the\nfederated setting, as we demonstrate through simulations on real-world\nfederated datasets.\n", "versions": [{"version": "v1", "created": "Tue, 30 May 2017 06:20:31 GMT"}, {"version": "v2", "created": "Tue, 27 Feb 2018 07:29:26 GMT"}], "update_date": "2018-02-28", "authors_parsed": [["Smith", "Virginia", ""], ["Chiang", "Chao-Kai", ""], ["Sanjabi", "Maziar", ""], ["Talwalkar", "Ameet", ""]]}, {"id": "1705.10470", "submitter": "Weiyang Liu", "authors": "Weiyang Liu, Bo Dai, Ahmad Humayun, Charlene Tay, Chen Yu, Linda B.\n  Smith, James M. Rehg, Le Song", "title": "Iterative Machine Teaching", "comments": "Published in ICML 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we consider the problem of machine teaching, the inverse\nproblem of machine learning. Different from traditional machine teaching which\nviews the learners as batch algorithms, we study a new paradigm where the\nlearner uses an iterative algorithm and a teacher can feed examples\nsequentially and intelligently based on the current performance of the learner.\nWe show that the teaching complexity in the iterative case is very different\nfrom that in the batch case. Instead of constructing a minimal training set for\nlearners, our iterative machine teaching focuses on achieving fast convergence\nin the learner model. Depending on the level of information the teacher has\nfrom the learner model, we design teaching algorithms which can provably reduce\nthe number of teaching examples and achieve faster convergence than learning\nwithout teachers. We also validate our theoretical findings with extensive\nexperiments on different data distribution and real image datasets.\n", "versions": [{"version": "v1", "created": "Tue, 30 May 2017 06:35:29 GMT"}, {"version": "v2", "created": "Tue, 13 Jun 2017 05:42:32 GMT"}, {"version": "v3", "created": "Fri, 17 Nov 2017 22:19:21 GMT"}], "update_date": "2017-11-21", "authors_parsed": [["Liu", "Weiyang", ""], ["Dai", "Bo", ""], ["Humayun", "Ahmad", ""], ["Tay", "Charlene", ""], ["Yu", "Chen", ""], ["Smith", "Linda B.", ""], ["Rehg", "James M.", ""], ["Song", "Le", ""]]}, {"id": "1705.10479", "submitter": "Karol Hausman", "authors": "Karol Hausman, Yevgen Chebotar, Stefan Schaal, Gaurav Sukhatme, Joseph\n  Lim", "title": "Multi-Modal Imitation Learning from Unstructured Demonstrations using\n  Generative Adversarial Nets", "comments": "Paper accepted to NIPS 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Imitation learning has traditionally been applied to learn a single task from\ndemonstrations thereof. The requirement of structured and isolated\ndemonstrations limits the scalability of imitation learning approaches as they\nare difficult to apply to real-world scenarios, where robots have to be able to\nexecute a multitude of tasks. In this paper, we propose a multi-modal imitation\nlearning framework that is able to segment and imitate skills from unlabelled\nand unstructured demonstrations by learning skill segmentation and imitation\nlearning jointly. The extensive simulation results indicate that our method can\nefficiently separate the demonstrations into individual skills and learn to\nimitate them using a single multi-modal policy. The video of our experiments is\navailable at http://sites.google.com/view/nips17intentiongan\n", "versions": [{"version": "v1", "created": "Tue, 30 May 2017 07:15:11 GMT"}, {"version": "v2", "created": "Thu, 23 Nov 2017 05:32:56 GMT"}], "update_date": "2017-11-27", "authors_parsed": [["Hausman", "Karol", ""], ["Chebotar", "Yevgen", ""], ["Schaal", "Stefan", ""], ["Sukhatme", "Gaurav", ""], ["Lim", "Joseph", ""]]}, {"id": "1705.10494", "submitter": "Baruch Epstein", "authors": "Baruch Epstein. Ron Meir, Tomer Michaeli", "title": "Joint auto-encoders: a flexible multi-task learning framework", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The incorporation of prior knowledge into learning is essential in achieving\ngood performance based on small noisy samples. Such knowledge is often\nincorporated through the availability of related data arising from domains and\ntasks similar to the one of current interest. Ideally one would like to allow\nboth the data for the current task and for previous related tasks to\nself-organize the learning system in such a way that commonalities and\ndifferences between the tasks are learned in a data-driven fashion. We develop\na framework for learning multiple tasks simultaneously, based on sharing\nfeatures that are common to all tasks, achieved through the use of a modular\ndeep feedforward neural network consisting of shared branches, dealing with the\ncommon features of all tasks, and private branches, learning the specific\nunique aspects of each task. Once an appropriate weight sharing architecture\nhas been established, learning takes place through standard algorithms for\nfeedforward networks, e.g., stochastic gradient descent and its variations. The\nmethod deals with domain adaptation and multi-task learning in a unified\nfashion, and can easily deal with data arising from different types of sources.\nNumerical experiments demonstrate the effectiveness of learning in domain\nadaptation and transfer learning setups, and provide evidence for the flexible\nand task-oriented representations arising in the network.\n", "versions": [{"version": "v1", "created": "Tue, 30 May 2017 07:51:42 GMT"}], "update_date": "2017-05-31", "authors_parsed": [["Meir", "Baruch Epstein. Ron", ""], ["Michaeli", "Tomer", ""]]}, {"id": "1705.10498", "submitter": "Guillaume Gautier", "authors": "Guillaume Gautier, R\\'emi Bardenet, Michal Valko", "title": "Zonotope hit-and-run for efficient sampling from projection DPPs", "comments": "12 pages, 12 figures, 2 columns, accepted to ICML 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Determinantal point processes (DPPs) are distributions over sets of items\nthat model diversity using kernels. Their applications in machine learning\ninclude summary extraction and recommendation systems. Yet, the cost of\nsampling from a DPP is prohibitive in large-scale applications, which has\ntriggered an effort towards efficient approximate samplers. We build a novel\nMCMC sampler that combines ideas from combinatorial geometry, linear\nprogramming, and Monte Carlo methods to sample from DPPs with a fixed sample\ncardinality, also called projection DPPs. Our sampler leverages the ability of\nthe hit-and-run MCMC kernel to efficiently move across convex bodies. Previous\ntheoretical results yield a fast mixing time of our chain when targeting a\ndistribution that is close to a projection DPP, but not a DPP in general. Our\nempirical results demonstrate that this extends to sampling projection DPPs,\ni.e., our sampler is more sample-efficient than previous approaches which in\nturn translates to faster convergence when dealing with costly-to-evaluate\nfunctions, such as summary extraction in our experiments.\n", "versions": [{"version": "v1", "created": "Tue, 30 May 2017 08:07:19 GMT"}, {"version": "v2", "created": "Thu, 15 Jun 2017 08:36:00 GMT"}], "update_date": "2018-06-06", "authors_parsed": [["Gautier", "Guillaume", ""], ["Bardenet", "R\u00e9mi", ""], ["Valko", "Michal", ""]]}, {"id": "1705.10499", "submitter": "Kfir Levy Yehuda", "authors": "Kfir Y. Levy", "title": "Online to Offline Conversions, Universality and Adaptive Minibatch Sizes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an approach towards convex optimization that relies on a novel\nscheme which converts online adaptive algorithms into offline methods. In the\noffline optimization setting, our derived methods are shown to obtain\nfavourable adaptive guarantees which depend on the harmonic sum of the queried\ngradients. We further show that our methods implicitly adapt to the objective's\nstructure: in the smooth case fast convergence rates are ensured without any\nprior knowledge of the smoothness parameter, while still maintaining guarantees\nin the non-smooth setting. Our approach has a natural extension to the\nstochastic setting, resulting in a lazy version of SGD (stochastic GD), where\nminibathces are chosen \\emph{adaptively} depending on the magnitude of the\ngradients. Thus providing a principled approach towards choosing minibatch\nsizes.\n", "versions": [{"version": "v1", "created": "Tue, 30 May 2017 08:08:36 GMT"}, {"version": "v2", "created": "Wed, 31 May 2017 07:22:55 GMT"}], "update_date": "2017-06-01", "authors_parsed": [["Levy", "Kfir Y.", ""]]}, {"id": "1705.10500", "submitter": "Luisa Polania", "authors": "Luisa F. Polania, Kenneth E. Barner", "title": "Exploiting Restricted Boltzmann Machines and Deep Belief Networks in\n  Compressed Sensing", "comments": "Accepted for publication at IEEE Transactions on Signal Processing", "journal-ref": null, "doi": "10.1109/TSP.2017.2712128", "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a CS scheme that exploits the representational power of\nrestricted Boltzmann machines and deep learning architectures to model the\nprior distribution of the sparsity pattern of signals belonging to the same\nclass. The determined probability distribution is then used in a maximum a\nposteriori (MAP) approach for the reconstruction. The parameters of the prior\ndistribution are learned from training data. The motivation behind this\napproach is to model the higher-order statistical dependencies between the\ncoefficients of the sparse representation, with the final goal of improving the\nreconstruction. The performance of the proposed method is validated on the\nBerkeley Segmentation Dataset and the MNIST Database of handwritten digits.\n", "versions": [{"version": "v1", "created": "Tue, 30 May 2017 08:11:05 GMT"}], "update_date": "2017-08-02", "authors_parsed": [["Polania", "Luisa F.", ""], ["Barner", "Kenneth E.", ""]]}, {"id": "1705.10503", "submitter": "Marko Jankovic", "authors": "Marko V. Jankovic", "title": "Quantum Low Entropy based Associative Reasoning or QLEAR Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.IT math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose the classification method based on a learning\nparadigm we are going to call Quantum Low Entropy based Associative Reasoning\nor QLEAR learning. The approach is based on the idea that classification can be\nunderstood as supervised clustering, where a quantum entropy in the context of\nthe quantum probabilistic model, will be used as a \"capturer\" (measure, or\nexternal index), of the \"natural structure\" of the data. By using quantum\nentropy we do not make any assumption about linear separability of the data\nthat are going to be classified. The basic idea is to find close neighbors to a\nquery sample and then use relative change in the quantum entropy as a measure\nof similarity of the newly arrived sample with the representatives of interest.\nIn other words, method is based on calculation of quantum entropy of the\nreferent system and its relative change with the addition of the newly arrived\nsample. Referent system consists of vectors that represent individual classes\nand that are the most similar, in Euclidean distance sense, to the vector that\nis analyzed. Here, we analyze the classification problem in the context of\nmeasuring similarities to prototype examples of categories. While nearest\nneighbor classifiers are natural in this setting, they suffer from the problem\nof high variance (in bias-variance decomposition) in the case of limited\nsampling. Alternatively, one could use machine learning techniques (like\nsupport vector machines) but they involve time-consuming optimization. Here we\npropose a hybrid of nearest neighbor and machine learning technique which deals\nnaturally with the multi-class setting, has reasonable computational complexity\nboth in training and at run time, and yields excellent results in practice.\n", "versions": [{"version": "v1", "created": "Tue, 30 May 2017 08:16:09 GMT"}], "update_date": "2017-05-31", "authors_parsed": [["Jankovic", "Marko V.", ""]]}, {"id": "1705.10508", "submitter": "Francesc Wilhelmi", "authors": "Francesc Wilhelmi, Boris Bellalta, Cristina Cano, Anders Jonsson", "title": "Implications of Decentralized Q-learning Resource Allocation in Wireless\n  Networks", "comments": "Conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Reinforcement Learning is gaining attention by the wireless networking\ncommunity due to its potential to learn good-performing configurations only\nfrom the observed results. In this work we propose a stateless variation of\nQ-learning, which we apply to exploit spatial reuse in a wireless network. In\nparticular, we allow networks to modify both their transmission power and the\nchannel used solely based on the experienced throughput. We concentrate in a\ncompletely decentralized scenario in which no information about neighbouring\nnodes is available to the learners. Our results show that although the\nalgorithm is able to find the best-performing actions to enhance aggregate\nthroughput, there is high variability in the throughput experienced by the\nindividual networks. We identify the cause of this variability as the\nadversarial setting of our setup, in which the most played actions provide\nintermittent good/poor performance depending on the neighbouring decisions. We\nalso evaluate the effect of the intrinsic learning parameters of the algorithm\non this variability.\n", "versions": [{"version": "v1", "created": "Tue, 30 May 2017 08:41:53 GMT"}, {"version": "v2", "created": "Tue, 29 Aug 2017 09:13:51 GMT"}], "update_date": "2017-08-30", "authors_parsed": [["Wilhelmi", "Francesc", ""], ["Bellalta", "Boris", ""], ["Cano", "Cristina", ""], ["Jonsson", "Anders", ""]]}, {"id": "1705.10513", "submitter": "Weinan Zhang", "authors": "Jun Wang, Lantao Yu, Weinan Zhang, Yu Gong, Yinghui Xu, Benyou Wang,\n  Peng Zhang, Dell Zhang", "title": "IRGAN: A Minimax Game for Unifying Generative and Discriminative\n  Information Retrieval Models", "comments": "12 pages; appendix added", "journal-ref": null, "doi": "10.1145/3077136.3080786", "report-no": null, "categories": "cs.IR cs.LG", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  This paper provides a unified account of two schools of thinking in\ninformation retrieval modelling: the generative retrieval focusing on\npredicting relevant documents given a query, and the discriminative retrieval\nfocusing on predicting relevancy given a query-document pair. We propose a game\ntheoretical minimax game to iteratively optimise both models. On one hand, the\ndiscriminative model, aiming to mine signals from labelled and unlabelled data,\nprovides guidance to train the generative model towards fitting the underlying\nrelevance distribution over documents given the query. On the other hand, the\ngenerative model, acting as an attacker to the current discriminative model,\ngenerates difficult examples for the discriminative model in an adversarial way\nby minimising its discrimination objective. With the competition between these\ntwo models, we show that the unified framework takes advantage of both schools\nof thinking: (i) the generative model learns to fit the relevance distribution\nover documents via the signals from the discriminative model, and (ii) the\ndiscriminative model is able to exploit the unlabelled data selected by the\ngenerative model to achieve a better estimation for document ranking. Our\nexperimental results have demonstrated significant performance gains as much as\n23.96% on Precision@5 and 15.50% on MAP over strong baselines in a variety of\napplications including web search, item recommendation, and question answering.\n", "versions": [{"version": "v1", "created": "Tue, 30 May 2017 09:03:31 GMT"}, {"version": "v2", "created": "Thu, 22 Feb 2018 09:00:19 GMT"}], "update_date": "2018-02-23", "authors_parsed": [["Wang", "Jun", ""], ["Yu", "Lantao", ""], ["Zhang", "Weinan", ""], ["Gong", "Yu", ""], ["Xu", "Yinghui", ""], ["Wang", "Benyou", ""], ["Zhang", "Peng", ""], ["Zhang", "Dell", ""]]}, {"id": "1705.10528", "submitter": "Joshua Achiam", "authors": "Joshua Achiam, David Held, Aviv Tamar, Pieter Abbeel", "title": "Constrained Policy Optimization", "comments": "Accepted to ICML 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For many applications of reinforcement learning it can be more convenient to\nspecify both a reward function and constraints, rather than trying to design\nbehavior through the reward function. For example, systems that physically\ninteract with or around humans should satisfy safety constraints. Recent\nadvances in policy search algorithms (Mnih et al., 2016, Schulman et al., 2015,\nLillicrap et al., 2016, Levine et al., 2016) have enabled new capabilities in\nhigh-dimensional control, but do not consider the constrained setting.\n  We propose Constrained Policy Optimization (CPO), the first general-purpose\npolicy search algorithm for constrained reinforcement learning with guarantees\nfor near-constraint satisfaction at each iteration. Our method allows us to\ntrain neural network policies for high-dimensional control while making\nguarantees about policy behavior all throughout training. Our guarantees are\nbased on a new theoretical result, which is of independent interest: we prove a\nbound relating the expected returns of two policies to an average divergence\nbetween them. We demonstrate the effectiveness of our approach on simulated\nrobot locomotion tasks where the agent must satisfy constraints motivated by\nsafety.\n", "versions": [{"version": "v1", "created": "Tue, 30 May 2017 10:07:31 GMT"}], "update_date": "2017-05-31", "authors_parsed": [["Achiam", "Joshua", ""], ["Held", "David", ""], ["Tamar", "Aviv", ""], ["Abbeel", "Pieter", ""]]}, {"id": "1705.10574", "submitter": "Sergiy Vorobyov A.", "authors": "Farshad G. Veshki and Sergiy A. Vorobyov", "title": "Multi-Focus Image Fusion Using Sparse Representation and Coupled\n  Dictionary Learning", "comments": "25 pages, 15 figures, 2 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We address the multi-focus image fusion problem, where multiple images\ncaptured with different focal settings are to be fused into an all-in-focus\nimage of higher quality. Algorithms for this problem necessarily admit the\nsource image characteristics along with focused and blurred features. However,\nmost sparsity-based approaches use a single dictionary in focused feature space\nto describe multi-focus images, and ignore the representations in blurred\nfeature space. We propose a multi-focus image fusion approach based on sparse\nrepresentation using a coupled dictionary. It exploits the observations that\nthe patches from a given training set can be sparsely represented by a couple\nof overcomplete dictionaries related to the focused and blurred categories of\nimages and that a sparse approximation based on such coupled dictionary leads\nto a more flexible and therefore better fusion strategy than the one based on\njust selecting the sparsest representation in the original image estimate. In\naddition, to improve the fusion performance, we employ a coupled dictionary\nlearning approach that enforces pairwise correlation between atoms of\ndictionaries learned to represent the focused and blurred feature spaces. We\nalso discuss the advantages of the fusion approach based on coupled dictionary\nlearning, and present efficient algorithms for fusion based on coupled\ndictionary learning. Extensive experimental comparisons with state-of-the-art\nmulti-focus image fusion algorithms validate the effectiveness of the proposed\napproach.\n", "versions": [{"version": "v1", "created": "Tue, 30 May 2017 12:20:26 GMT"}, {"version": "v2", "created": "Fri, 26 Apr 2019 05:54:06 GMT"}, {"version": "v3", "created": "Fri, 3 May 2019 08:05:44 GMT"}], "update_date": "2019-05-06", "authors_parsed": [["Veshki", "Farshad G.", ""], ["Vorobyov", "Sergiy A.", ""]]}, {"id": "1705.10586", "submitter": "Zhenzhou Wu", "authors": "Zhenzhou Wu and Xin Zheng and Daniel Dahlmeier", "title": "Character-Based Text Classification using Top Down Semantic Model for\n  Sentence Representation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Despite the success of deep learning on many fronts especially image and\nspeech, its application in text classification often is still not as good as a\nsimple linear SVM on n-gram TF-IDF representation especially for smaller\ndatasets. Deep learning tends to emphasize on sentence level semantics when\nlearning a representation with models like recurrent neural network or\nrecursive neural network, however from the success of TF-IDF representation, it\nseems a bag-of-words type of representation has its strength. Taking advantage\nof both representions, we present a model known as TDSM (Top Down Semantic\nModel) for extracting a sentence representation that considers both the\nword-level semantics by linearly combining the words with attention weights and\nthe sentence-level semantics with BiLSTM and use it on text classification. We\napply the model on characters and our results show that our model is better\nthan all the other character-based and word-based convolutional neural network\nmodels by \\cite{zhang15} across seven different datasets with only 1\\% of their\nparameters. We also demonstrate that this model beats traditional linear models\non TF-IDF vectors on small and polished datasets like news article in which\ntypically deep learning models surrender.\n", "versions": [{"version": "v1", "created": "Mon, 29 May 2017 15:53:00 GMT"}], "update_date": "2017-05-31", "authors_parsed": [["Wu", "Zhenzhou", ""], ["Zheng", "Xin", ""], ["Dahlmeier", "Daniel", ""]]}, {"id": "1705.10591", "submitter": "X. Sharon Hu", "authors": "Xiaoming Chen, Jianxu Chen, Danny Z. Chen, and Xiaobo Sharon Hu", "title": "Optimizing Memory Efficiency for Convolution Kernels on Kepler GPUs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolution is a fundamental operation in many applications, such as computer\nvision, natural language processing, image processing, etc. Recent successes of\nconvolutional neural networks in various deep learning applications put even\nhigher demand on fast convolution. The high computation throughput and memory\nbandwidth of graphics processing units (GPUs) make GPUs a natural choice for\naccelerating convolution operations. However, maximally exploiting the\navailable memory bandwidth of GPUs for convolution is a challenging task. This\npaper introduces a general model to address the mismatch between the memory\nbank width of GPUs and computation data width of threads. Based on this model,\nwe develop two convolution kernels, one for the general case and the other for\na special case with one input channel. By carefully optimizing memory access\npatterns and computation patterns, we design a communication-optimized kernel\nfor the special case and a communication-reduced kernel for the general case.\nExperimental data based on implementations on Kepler GPUs show that our kernels\nachieve 5.16X and 35.5% average performance improvement over the latest cuDNN\nlibrary, for the special case and the general case, respectively.\n", "versions": [{"version": "v1", "created": "Mon, 29 May 2017 14:52:42 GMT"}], "update_date": "2017-05-31", "authors_parsed": [["Chen", "Xiaoming", ""], ["Chen", "Jianxu", ""], ["Chen", "Danny Z.", ""], ["Hu", "Xiaobo Sharon", ""]]}, {"id": "1705.10596", "submitter": "Zhulin Liu", "authors": "Zhulin Liu and C. L. Philip Chen", "title": "Approximation learning methods of Harmonic Mappings in relation to Hardy\n  Spaces", "comments": "2016 3rd International Conference on Informative and Cybernetics for\n  Computational Social Systems (ICCSS)", "journal-ref": null, "doi": "10.1109/ICCSS.2016.7586421", "report-no": null, "categories": "math.NA cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A new Hardy space Hardy space approach of Dirichlet type problem based on\nTikhonov regularization and Reproducing Hilbert kernel space is discussed in\nthis paper, which turns out to be a typical extremal problem located on the\nupper upper-high complex plane. If considering this in the Hardy space, the\noptimization operator of this problem will be highly simplified and an\nefficient algorithm is possible. This is mainly realized by the help of\nreproducing properties of the functions in the Hardy space of upper-high\ncomplex plane, and the detail algorithm is proposed. Moreover, harmonic\nmappings, which is a significant geometric transformation, are commonly used in\nmany applications such as image processing, since it describes the energy\nminimization mappings between individual manifolds. Particularly, when we focus\non the planer mappings between two Euclid planer regions, the harmonic mappings\nare exist and unique, which is guaranteed solidly by the existence of harmonic\nfunction. This property is attractive and simulation results are shown in this\npaper to ensure the capability of applications such as planer shape distortion\nand surface registration.\n", "versions": [{"version": "v1", "created": "Wed, 24 May 2017 06:58:01 GMT"}], "update_date": "2017-05-31", "authors_parsed": [["Liu", "Zhulin", ""], ["Chen", "C. L. Philip", ""]]}, {"id": "1705.10639", "submitter": "Rick Smetsers", "authors": "Rick Smetsers", "title": "Grammatical Inference as a Satisfiability Modulo Theories Problem", "comments": "Submitted and selected for oral presentation at the LearnAut workshop\n  at LICS 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.FL cs.LG cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The problem of learning a minimal consistent model from a set of labeled\nsequences of symbols is addressed from a satisfiability modulo theories\nperspective. We present two encodings for deterministic finite automata and\nextend one of these for Moore and Mealy machines. Our experimental results show\nthat these encodings improve upon the state-of-the-art, and are useful in\npractice for learning small models.\n", "versions": [{"version": "v1", "created": "Tue, 30 May 2017 13:48:15 GMT"}], "update_date": "2017-05-31", "authors_parsed": [["Smetsers", "Rick", ""]]}, {"id": "1705.10667", "submitter": "Mingsheng Long", "authors": "Mingsheng Long, Zhangjie Cao, Jianmin Wang, Michael I. Jordan", "title": "Conditional Adversarial Domain Adaptation", "comments": "32nd Conference on Neural Information Processing Systems (NeurIPS\n  2018), Montreal, Canada", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Adversarial learning has been embedded into deep networks to learn\ndisentangled and transferable representations for domain adaptation. Existing\nadversarial domain adaptation methods may not effectively align different\ndomains of multimodal distributions native in classification problems. In this\npaper, we present conditional adversarial domain adaptation, a principled\nframework that conditions the adversarial adaptation models on discriminative\ninformation conveyed in the classifier predictions. Conditional domain\nadversarial networks (CDANs) are designed with two novel conditioning\nstrategies: multilinear conditioning that captures the cross-covariance between\nfeature representations and classifier predictions to improve the\ndiscriminability, and entropy conditioning that controls the uncertainty of\nclassifier predictions to guarantee the transferability. With theoretical\nguarantees and a few lines of codes, the approach has exceeded state-of-the-art\nresults on five datasets.\n", "versions": [{"version": "v1", "created": "Fri, 26 May 2017 00:50:36 GMT"}, {"version": "v2", "created": "Sat, 10 Feb 2018 00:39:20 GMT"}, {"version": "v3", "created": "Fri, 14 Dec 2018 03:57:28 GMT"}, {"version": "v4", "created": "Sat, 29 Dec 2018 16:43:57 GMT"}], "update_date": "2019-01-01", "authors_parsed": [["Long", "Mingsheng", ""], ["Cao", "Zhangjie", ""], ["Wang", "Jianmin", ""], ["Jordan", "Michael I.", ""]]}, {"id": "1705.10686", "submitter": "Weilin Xu", "authors": "Weilin Xu, David Evans, Yanjun Qi", "title": "Feature Squeezing Mitigates and Detects Carlini/Wagner Adversarial\n  Examples", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Feature squeezing is a recently-introduced framework for mitigating and\ndetecting adversarial examples. In previous work, we showed that it is\neffective against several earlier methods for generating adversarial examples.\nIn this short note, we report on recent results showing that simple feature\nsqueezing techniques also make deep learning models significantly more robust\nagainst the Carlini/Wagner attacks, which are the best known adversarial\nmethods discovered to date.\n", "versions": [{"version": "v1", "created": "Tue, 30 May 2017 15:00:55 GMT"}], "update_date": "2017-05-31", "authors_parsed": [["Xu", "Weilin", ""], ["Evans", "David", ""], ["Qi", "Yanjun", ""]]}, {"id": "1705.10694", "submitter": "Andreas Veit", "authors": "David Rolnick, Andreas Veit, Serge Belongie, Nir Shavit", "title": "Deep Learning is Robust to Massive Label Noise", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural networks trained on large supervised datasets have led to\nimpressive results in image classification and other tasks. However,\nwell-annotated datasets can be time-consuming and expensive to collect, lending\nincreased interest to larger but noisy datasets that are more easily obtained.\nIn this paper, we show that deep neural networks are capable of generalizing\nfrom training data for which true labels are massively outnumbered by incorrect\nlabels. We demonstrate remarkably high test performance after training on\ncorrupted data from MNIST, CIFAR, and ImageNet. For example, on MNIST we obtain\ntest accuracy above 90 percent even after each clean training example has been\ndiluted with 100 randomly-labeled examples. Such behavior holds across multiple\npatterns of label noise, even when erroneous labels are biased towards\nconfusing classes. We show that training in this regime requires a significant\nbut manageable increase in dataset size that is related to the factor by which\ncorrect labels have been diluted. Finally, we provide an analysis of our\nresults that shows how increasing noise decreases the effective batch size.\n", "versions": [{"version": "v1", "created": "Tue, 30 May 2017 15:10:51 GMT"}, {"version": "v2", "created": "Wed, 31 May 2017 02:02:56 GMT"}, {"version": "v3", "created": "Mon, 26 Feb 2018 16:51:57 GMT"}], "update_date": "2018-02-27", "authors_parsed": [["Rolnick", "David", ""], ["Veit", "Andreas", ""], ["Belongie", "Serge", ""], ["Shavit", "Nir", ""]]}, {"id": "1705.10701", "submitter": "I-Chen Wu", "authors": "Ti-Rong Wu, I-Chen Wu, Guan-Wun Chen, Ting-han Wei, Tung-Yi Lai,\n  Hung-Chun Wu and Li-Cheng Lan", "title": "Multi-Labelled Value Networks for Computer Go", "comments": "This version was also submitted to IEEE TCIAIG on May 30, 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a new approach to a novel value network architecture for\nthe game Go, called a multi-labelled (ML) value network. In the ML value\nnetwork, different values (win rates) are trained simultaneously for different\nsettings of komi, a compensation given to balance the initiative of playing\nfirst. The ML value network has three advantages, (a) it outputs values for\ndifferent komi, (b) it supports dynamic komi, and (c) it lowers the mean\nsquared error (MSE). This paper also proposes a new dynamic komi method to\nimprove game-playing strength. This paper also performs experiments to\ndemonstrate the merits of the architecture. First, the MSE of the ML value\nnetwork is generally lower than the value network alone. Second, the program\nbased on the ML value network wins by a rate of 67.6% against the program based\non the value network alone. Third, the program with the proposed dynamic komi\nmethod significantly improves the playing strength over the baseline that does\nnot use dynamic komi, especially for handicap games. To our knowledge, up to\ndate, no handicap games have been played openly by programs using value\nnetworks. This paper provides these programs with a useful approach to playing\nhandicap games.\n", "versions": [{"version": "v1", "created": "Tue, 30 May 2017 15:23:32 GMT"}], "update_date": "2017-05-31", "authors_parsed": [["Wu", "Ti-Rong", ""], ["Wu", "I-Chen", ""], ["Chen", "Guan-Wun", ""], ["Wei", "Ting-han", ""], ["Lai", "Tung-Yi", ""], ["Wu", "Hung-Chun", ""], ["Lan", "Li-Cheng", ""]]}, {"id": "1705.10723", "submitter": "Zhao Song", "authors": "Eric Price, Zhao Song, David P. Woodruff", "title": "Fast Regression with an $\\ell_\\infty$ Guarantee", "comments": "ICALP 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Sketching has emerged as a powerful technique for speeding up problems in\nnumerical linear algebra, such as regression. In the overconstrained regression\nproblem, one is given an $n \\times d$ matrix $A$, with $n \\gg d$, as well as an\n$n \\times 1$ vector $b$, and one wants to find a vector $\\hat{x}$ so as to\nminimize the residual error $\\|Ax-b\\|_2$. Using the sketch and solve paradigm,\none first computes $S \\cdot A$ and $S \\cdot b$ for a randomly chosen matrix\n$S$, then outputs $x' = (SA)^{\\dagger} Sb$ so as to minimize $\\|SAx' - Sb\\|_2$.\n  The sketch-and-solve paradigm gives a bound on $\\|x'-x^*\\|_2$ when $A$ is\nwell-conditioned. Our main result is that, when $S$ is the subsampled\nrandomized Fourier/Hadamard transform, the error $x' - x^*$ behaves as if it\nlies in a \"random\" direction within this bound: for any fixed direction $a\\in\n\\mathbb{R}^d$, we have with $1 - d^{-c}$ probability that\n  \\[\n  \\langle a, x'-x^*\\rangle \\lesssim\n\\frac{\\|a\\|_2\\|x'-x^*\\|_2}{d^{\\frac{1}{2}-\\gamma}}, \\quad (1)\n  \\]\n  where $c, \\gamma > 0$ are arbitrary constants.\n  This implies $\\|x'-x^*\\|_{\\infty}$ is a factor $d^{\\frac{1}{2}-\\gamma}$\nsmaller than $\\|x'-x^*\\|_2$. It also gives a better bound on the generalization\nof $x'$ to new examples: if rows of $A$ correspond to examples and columns to\nfeatures, then our result gives a better bound for the error introduced by\nsketch-and-solve when classifying fresh examples. We show that not all\noblivious subspace embeddings $S$ satisfy these properties. In particular, we\ngive counterexamples showing that matrices based on Count-Sketch or leverage\nscore sampling do not satisfy these properties.\n  We also provide lower bounds, both on how small $\\|x'-x^*\\|_2$ can be, and\nfor our new guarantee (1), showing that the subsampled randomized\nFourier/Hadamard transform is nearly optimal.\n", "versions": [{"version": "v1", "created": "Tue, 30 May 2017 16:20:34 GMT"}], "update_date": "2017-05-31", "authors_parsed": [["Price", "Eric", ""], ["Song", "Zhao", ""], ["Woodruff", "David P.", ""]]}, {"id": "1705.10743", "submitter": "Marc G. Bellemare", "authors": "Marc G. Bellemare, Ivo Danihelka, Will Dabney, Shakir Mohamed, Balaji\n  Lakshminarayanan, Stephan Hoyer, R\\'emi Munos", "title": "The Cramer Distance as a Solution to Biased Wasserstein Gradients", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Wasserstein probability metric has received much attention from the\nmachine learning community. Unlike the Kullback-Leibler divergence, which\nstrictly measures change in probability, the Wasserstein metric reflects the\nunderlying geometry between outcomes. The value of being sensitive to this\ngeometry has been demonstrated, among others, in ordinal regression and\ngenerative modelling. In this paper we describe three natural properties of\nprobability divergences that reflect requirements from machine learning: sum\ninvariance, scale sensitivity, and unbiased sample gradients. The Wasserstein\nmetric possesses the first two properties but, unlike the Kullback-Leibler\ndivergence, does not possess the third. We provide empirical evidence\nsuggesting that this is a serious issue in practice. Leveraging insights from\nprobabilistic forecasting we propose an alternative to the Wasserstein metric,\nthe Cram\\'er distance. We show that the Cram\\'er distance possesses all three\ndesired properties, combining the best of the Wasserstein and Kullback-Leibler\ndivergences. To illustrate the relevance of the Cram\\'er distance in practice\nwe design a new algorithm, the Cram\\'er Generative Adversarial Network (GAN),\nand show that it performs significantly better than the related Wasserstein\nGAN.\n", "versions": [{"version": "v1", "created": "Tue, 30 May 2017 16:53:12 GMT"}], "update_date": "2017-06-01", "authors_parsed": [["Bellemare", "Marc G.", ""], ["Danihelka", "Ivo", ""], ["Dabney", "Will", ""], ["Mohamed", "Shakir", ""], ["Lakshminarayanan", "Balaji", ""], ["Hoyer", "Stephan", ""], ["Munos", "R\u00e9mi", ""]]}, {"id": "1705.10744", "submitter": "Ondrej Bajgar", "authors": "Rudolf Kadlec, Ondrej Bajgar and Jan Kleindienst", "title": "Knowledge Base Completion: Baselines Strike Back", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many papers have been published on the knowledge base completion task in the\npast few years. Most of these introduce novel architectures for relation\nlearning that are evaluated on standard datasets such as FB15k and WN18. This\npaper shows that the accuracy of almost all models published on the FB15k can\nbe outperformed by an appropriately tuned baseline - our reimplementation of\nthe DistMult model. Our findings cast doubt on the claim that the performance\nimprovements of recent models are due to architectural changes as opposed to\nhyper-parameter tuning or different training objectives. This should prompt\nfuture research to re-consider how the performance of models is evaluated and\nreported.\n", "versions": [{"version": "v1", "created": "Tue, 30 May 2017 16:54:19 GMT"}], "update_date": "2017-05-31", "authors_parsed": [["Kadlec", "Rudolf", ""], ["Bajgar", "Ondrej", ""], ["Kleindienst", "Jan", ""]]}, {"id": "1705.10750", "submitter": "Junier Oliva", "authors": "Junier B. Oliva, Kumar Avinava Dubey, Barnabas Poczos, Eric Xing, Jeff\n  Schneider", "title": "Recurrent Estimation of Distributions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents the recurrent estimation of distributions (RED) for\nmodeling real-valued data in a semiparametric fashion. RED models make two\nnovel uses of recurrent neural networks (RNNs) for density estimation of\ngeneral real-valued data. First, RNNs are used to transform input covariates\ninto a latent space to better capture conditional dependencies in inputs.\nAfter, an RNN is used to compute the conditional distributions of the latent\ncovariates. The resulting model is efficient to train, compute, and sample\nfrom, whilst producing normalized pdfs. The effectiveness of RED is shown via\nseveral real-world data experiments. Our results show that RED models achieve a\nlower held-out negative log-likelihood than other neural network approaches\nacross multiple dataset sizes and dimensionalities. Further context of the\nefficacy of RED is provided by considering anomaly detection tasks, where we\nalso observe better performance over alternative models.\n", "versions": [{"version": "v1", "created": "Tue, 30 May 2017 17:00:59 GMT"}], "update_date": "2017-05-31", "authors_parsed": [["Oliva", "Junier B.", ""], ["Dubey", "Kumar Avinava", ""], ["Poczos", "Barnabas", ""], ["Xing", "Eric", ""], ["Schneider", "Jeff", ""]]}, {"id": "1705.10762", "submitter": "Ramakrishna Vedantam", "authors": "Ramakrishna Vedantam, Ian Fischer, Jonathan Huang, Kevin Murphy", "title": "Generative Models of Visually Grounded Imagination", "comments": "International Conference on Learning Representations (ICLR), 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is easy for people to imagine what a man with pink hair looks like, even\nif they have never seen such a person before. We call the ability to create\nimages of novel semantic concepts visually grounded imagination. In this paper,\nwe show how we can modify variational auto-encoders to perform this task. Our\nmethod uses a novel training objective, and a novel product-of-experts\ninference network, which can handle partially specified (abstract) concepts in\na principled and efficient way. We also propose a set of easy-to-compute\nevaluation metrics that capture our intuitive notions of what it means to have\ngood visual imagination, namely correctness, coverage, and compositionality\n(the 3 C's). Finally, we perform a detailed comparison of our method with two\nexisting joint image-attribute VAE methods (the JMVAE method of Suzuki et.al.\nand the BiVCCA method of Wang et.al.) by applying them to two datasets: the\nMNIST-with-attributes dataset (which we introduce here), and the CelebA\ndataset.\n", "versions": [{"version": "v1", "created": "Tue, 30 May 2017 17:32:26 GMT"}, {"version": "v2", "created": "Tue, 18 Jul 2017 22:38:31 GMT"}, {"version": "v3", "created": "Sat, 28 Oct 2017 21:10:46 GMT"}, {"version": "v4", "created": "Thu, 2 Nov 2017 18:38:45 GMT"}, {"version": "v5", "created": "Wed, 15 Nov 2017 04:58:10 GMT"}, {"version": "v6", "created": "Mon, 12 Feb 2018 05:04:18 GMT"}, {"version": "v7", "created": "Sun, 25 Feb 2018 18:14:07 GMT"}, {"version": "v8", "created": "Fri, 9 Nov 2018 08:16:22 GMT"}], "update_date": "2018-11-12", "authors_parsed": [["Vedantam", "Ramakrishna", ""], ["Fischer", "Ian", ""], ["Huang", "Jonathan", ""], ["Murphy", "Kevin", ""]]}, {"id": "1705.10770", "submitter": "Giorgos Borboudakis", "authors": "Giorgos Borboudakis and Ioannis Tsamardinos", "title": "Forward-Backward Selection with Early Dropping", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Forward-backward selection is one of the most basic and commonly-used feature\nselection algorithms available. It is also general and conceptually applicable\nto many different types of data. In this paper, we propose a heuristic that\nsignificantly improves its running time, while preserving predictive accuracy.\nThe idea is to temporarily discard the variables that are conditionally\nindependent with the outcome given the selected variable set. Depending on how\nthose variables are reconsidered and reintroduced, this heuristic gives rise to\na family of algorithms with increasingly stronger theoretical guarantees. In\ndistributions that can be faithfully represented by Bayesian networks or\nmaximal ancestral graphs, members of this algorithmic family are able to\ncorrectly identify the Markov blanket in the sample limit. In experiments we\nshow that the proposed heuristic increases computational efficiency by about\ntwo orders of magnitude in high-dimensional problems, while selecting fewer\nvariables and retaining predictive performance. Furthermore, we show that the\nproposed algorithm and feature selection with LASSO perform similarly when\nrestricted to select the same number of variables, making the proposed\nalgorithm an attractive alternative for problems where no (efficient) algorithm\nfor LASSO exists.\n", "versions": [{"version": "v1", "created": "Tue, 30 May 2017 17:47:56 GMT"}], "update_date": "2017-05-31", "authors_parsed": [["Borboudakis", "Giorgos", ""], ["Tsamardinos", "Ioannis", ""]]}, {"id": "1705.10786", "submitter": "Hamidreza Alvari", "authors": "Hamidreza Alvari, Paulo Shakarian, J.E. Kelly Snyder", "title": "Semi-Supervised Learning for Detecting Human Trafficking", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Human trafficking is one of the most atrocious crimes and among the\nchallenging problems facing law enforcement which demands attention of global\nmagnitude. In this study, we leverage textual data from the website \"Backpage\"-\nused for classified advertisement- to discern potential patterns of human\ntrafficking activities which manifest online and identify advertisements of\nhigh interest to law enforcement. Due to the lack of ground truth, we rely on a\nhuman analyst from law enforcement, for hand-labeling a small portion of the\ncrawled data. We extend the existing Laplacian SVM and present S3VM-R, by\nadding a regularization term to exploit exogenous information embedded in our\nfeature space in favor of the task at hand. We train the proposed method using\nlabeled and unlabeled data and evaluate it on a fraction of the unlabeled data,\nherein referred to as unseen data, with our expert's further verification.\nResults from comparisons between our method and other semi-supervised and\nsupervised approaches on the labeled data demonstrate that our learner is\neffective in identifying advertisements of high interest to law enforcement\n", "versions": [{"version": "v1", "created": "Tue, 30 May 2017 05:51:53 GMT"}], "update_date": "2017-06-01", "authors_parsed": [["Alvari", "Hamidreza", ""], ["Shakarian", "Paulo", ""], ["Snyder", "J. E. Kelly", ""]]}, {"id": "1705.10819", "submitter": "Joan Bruna", "authors": "Ilya Kostrikov, Zhongshi Jiang, Daniele Panozzo, Denis Zorin, Joan\n  Bruna", "title": "Surface Networks", "comments": null, "journal-ref": "CVPR 2018", "doi": null, "report-no": null, "categories": "stat.ML cs.GR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study data-driven representations for three-dimensional triangle meshes,\nwhich are one of the prevalent objects used to represent 3D geometry. Recent\nworks have developed models that exploit the intrinsic geometry of manifolds\nand graphs, namely the Graph Neural Networks (GNNs) and its spectral variants,\nwhich learn from the local metric tensor via the Laplacian operator. Despite\noffering excellent sample complexity and built-in invariances, intrinsic\ngeometry alone is invariant to isometric deformations, making it unsuitable for\nmany applications. To overcome this limitation, we propose several upgrades to\nGNNs to leverage extrinsic differential geometry properties of\nthree-dimensional surfaces, increasing its modeling power.\n  In particular, we propose to exploit the Dirac operator, whose spectrum\ndetects principal curvature directions --- this is in stark contrast with the\nclassical Laplace operator, which directly measures mean curvature. We coin the\nresulting models \\emph{Surface Networks (SN)}. We prove that these models\ndefine shape representations that are stable to deformation and to\ndiscretization, and we demonstrate the efficiency and versatility of SNs on two\nchallenging tasks: temporal prediction of mesh deformations under non-linear\ndynamics and generative models using a variational autoencoder framework with\nencoders/decoders given by SNs.\n", "versions": [{"version": "v1", "created": "Tue, 30 May 2017 18:40:47 GMT"}, {"version": "v2", "created": "Mon, 18 Jun 2018 14:42:28 GMT"}], "update_date": "2018-06-19", "authors_parsed": [["Kostrikov", "Ilya", ""], ["Jiang", "Zhongshi", ""], ["Panozzo", "Daniele", ""], ["Zorin", "Denis", ""], ["Bruna", "Joan", ""]]}, {"id": "1705.10823", "submitter": "Otkrist Gupta", "authors": "Bowen Baker, Otkrist Gupta, Ramesh Raskar and Nikhil Naik", "title": "Accelerating Neural Architecture Search using Performance Prediction", "comments": "Submitted to International Conference on Learning Representations,\n  (2018)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Methods for neural network hyperparameter optimization and meta-modeling are\ncomputationally expensive due to the need to train a large number of model\nconfigurations. In this paper, we show that standard frequentist regression\nmodels can predict the final performance of partially trained model\nconfigurations using features based on network architectures, hyperparameters,\nand time-series validation performance data. We empirically show that our\nperformance prediction models are much more effective than prominent Bayesian\ncounterparts, are simpler to implement, and are faster to train. Our models can\npredict final performance in both visual classification and language modeling\ndomains, are effective for predicting performance of drastically varying model\narchitectures, and can even generalize between model classes. Using these\nprediction models, we also propose an early stopping method for hyperparameter\noptimization and meta-modeling, which obtains a speedup of a factor up to 6x in\nboth hyperparameter optimization and meta-modeling. Finally, we empirically\nshow that our early stopping method can be seamlessly incorporated into both\nreinforcement learning-based architecture selection algorithms and bandit based\nsearch methods. Through extensive experimentation, we empirically show our\nperformance prediction models and early stopping algorithm are state-of-the-art\nin terms of prediction accuracy and speedup achieved while still identifying\nthe optimal model configurations.\n", "versions": [{"version": "v1", "created": "Tue, 30 May 2017 19:00:53 GMT"}, {"version": "v2", "created": "Wed, 8 Nov 2017 16:09:35 GMT"}], "update_date": "2017-11-09", "authors_parsed": [["Baker", "Bowen", ""], ["Gupta", "Otkrist", ""], ["Raskar", "Ramesh", ""], ["Naik", "Nikhil", ""]]}, {"id": "1705.10829", "submitter": "Bo Waggoner", "authors": "Katrina Ligett, Seth Neel, Aaron Roth, Bo Waggoner, Z. Steven Wu", "title": "Accuracy First: Selecting a Differential Privacy Level for\n  Accuracy-Constrained ERM", "comments": "24 pages single-column", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Traditional approaches to differential privacy assume a fixed privacy\nrequirement $\\epsilon$ for a computation, and attempt to maximize the accuracy\nof the computation subject to the privacy constraint. As differential privacy\nis increasingly deployed in practical settings, it may often be that there is\ninstead a fixed accuracy requirement for a given computation and the data\nanalyst would like to maximize the privacy of the computation subject to the\naccuracy constraint. This raises the question of how to find and run a\nmaximally private empirical risk minimizer subject to a given accuracy\nrequirement. We propose a general \"noise reduction\" framework that can apply to\na variety of private empirical risk minimization (ERM) algorithms, using them\nto \"search\" the space of privacy levels to find the empirically strongest one\nthat meets the accuracy constraint, incurring only logarithmic overhead in the\nnumber of privacy levels searched. The privacy analysis of our algorithm leads\nnaturally to a version of differential privacy where the privacy parameters are\ndependent on the data, which we term ex-post privacy, and which is related to\nthe recently introduced notion of privacy odometers. We also give an ex-post\nprivacy analysis of the classical AboveThreshold privacy tool, modifying it to\nallow for queries chosen depending on the database. Finally, we apply our\napproach to two common objectives, regularized linear and logistic regression,\nand empirically compare our noise reduction methods to (i) inverting the\ntheoretical utility guarantees of standard private ERM algorithms and (ii) a\nstronger, empirical baseline based on binary search.\n", "versions": [{"version": "v1", "created": "Tue, 30 May 2017 19:20:28 GMT"}], "update_date": "2017-06-01", "authors_parsed": [["Ligett", "Katrina", ""], ["Neel", "Seth", ""], ["Roth", "Aaron", ""], ["Waggoner", "Bo", ""], ["Wu", "Z. Steven", ""]]}, {"id": "1705.10843", "submitter": "Benjamin Sanchez-Lengeling", "authors": "Gabriel Lima Guimaraes, Benjamin Sanchez-Lengeling, Carlos Outeiral,\n  Pedro Luis Cunha Farias, Al\\'an Aspuru-Guzik", "title": "Objective-Reinforced Generative Adversarial Networks (ORGAN) for\n  Sequence Generation Models", "comments": "10 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In unsupervised data generation tasks, besides the generation of a sample\nbased on previous observations, one would often like to give hints to the model\nin order to bias the generation towards desirable metrics. We propose a method\nthat combines Generative Adversarial Networks (GANs) and reinforcement learning\n(RL) in order to accomplish exactly that. While RL biases the data generation\nprocess towards arbitrary metrics, the GAN component of the reward function\nensures that the model still remembers information learned from data. We build\nupon previous results that incorporated GANs and RL in order to generate\nsequence data and test this model in several settings for the generation of\nmolecules encoded as text sequences (SMILES) and in the context of music\ngeneration, showing for each case that we can effectively bias the generation\nprocess towards desired metrics.\n", "versions": [{"version": "v1", "created": "Tue, 30 May 2017 19:58:08 GMT"}, {"version": "v2", "created": "Wed, 29 Nov 2017 18:41:15 GMT"}, {"version": "v3", "created": "Wed, 7 Feb 2018 04:58:57 GMT"}], "update_date": "2018-02-08", "authors_parsed": [["Guimaraes", "Gabriel Lima", ""], ["Sanchez-Lengeling", "Benjamin", ""], ["Outeiral", "Carlos", ""], ["Farias", "Pedro Luis Cunha", ""], ["Aspuru-Guzik", "Al\u00e1n", ""]]}, {"id": "1705.10874", "submitter": "Zixing Zhang", "authors": "Zixing Zhang, J\\\"urgen Geiger, Jouni Pohjalainen, Amr El-Desoky Mousa,\n  Wenyu Jin, Bj\\\"orn Schuller", "title": "Deep Learning for Environmentally Robust Speech Recognition: An Overview\n  of Recent Developments", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Eliminating the negative effect of non-stationary environmental noise is a\nlong-standing research topic for automatic speech recognition that stills\nremains an important challenge. Data-driven supervised approaches, including\nones based on deep neural networks, have recently emerged as potential\nalternatives to traditional unsupervised approaches and with sufficient\ntraining, can alleviate the shortcomings of the unsupervised methods in various\nreal-life acoustic environments. In this light, we review recently developed,\nrepresentative deep learning approaches for tackling non-stationary additive\nand convolutional degradation of speech with the aim of providing guidelines\nfor those involved in the development of environmentally robust speech\nrecognition systems. We separately discuss single- and multi-channel techniques\ndeveloped for the front-end and back-end of speech recognition systems, as well\nas joint front-end and back-end training frameworks.\n", "versions": [{"version": "v1", "created": "Tue, 30 May 2017 21:31:25 GMT"}, {"version": "v2", "created": "Sat, 8 Jul 2017 09:44:32 GMT"}, {"version": "v3", "created": "Fri, 21 Sep 2018 09:05:57 GMT"}], "update_date": "2018-09-24", "authors_parsed": [["Zhang", "Zixing", ""], ["Geiger", "J\u00fcrgen", ""], ["Pohjalainen", "Jouni", ""], ["Mousa", "Amr El-Desoky", ""], ["Jin", "Wenyu", ""], ["Schuller", "Bj\u00f6rn", ""]]}, {"id": "1705.10883", "submitter": "Velibor Mi\\v{s}i\\'c", "authors": "Velibor V. Mi\\v{s}i\\'c", "title": "Optimization of Tree Ensembles", "comments": "49 pages, 10 figures; to appear in Operations Research", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Tree ensemble models such as random forests and boosted trees are among the\nmost widely used and practically successful predictive models in applied\nmachine learning and business analytics. Although such models have been used to\nmake predictions based on exogenous, uncontrollable independent variables, they\nare increasingly being used to make predictions where the independent variables\nare controllable and are also decision variables. In this paper, we study the\nproblem of tree ensemble optimization: given a tree ensemble that predicts some\ndependent variable using controllable independent variables, how should we set\nthese variables so as to maximize the predicted value? We formulate the problem\nas a mixed-integer optimization problem. We theoretically examine the strength\nof our formulation, provide a hierarchy of approximate formulations with bounds\non approximation quality and exploit the structure of the problem to develop\ntwo large-scale solution methods, one based on Benders decomposition and one\nbased on iteratively generating tree split constraints. We test our methodology\non real data sets, including two case studies in drug design and customized\npricing, and show that our methodology can efficiently solve large-scale\ninstances to near or full optimality, and outperforms solutions obtained by\nheuristic approaches. In our drug design case, we show how our approach can\nidentify compounds that efficiently trade-off predicted performance and novelty\nwith respect to existing, known compounds. In our customized pricing case, we\nshow how our approach can efficiently determine optimal store-level prices\nunder a random forest model that delivers excellent predictive accuracy.\n", "versions": [{"version": "v1", "created": "Tue, 30 May 2017 22:37:22 GMT"}, {"version": "v2", "created": "Wed, 9 Oct 2019 21:03:32 GMT"}], "update_date": "2019-10-11", "authors_parsed": [["Mi\u0161i\u0107", "Velibor V.", ""]]}, {"id": "1705.10886", "submitter": "Qilong Gu", "authors": "Qilong Gu and Arindam Banerjee", "title": "High Dimensional Structured Superposition Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  High dimensional superposition models characterize observations using\nparameters which can be written as a sum of multiple component parameters, each\nwith its own structure, e.g., sum of low rank and sparse matrices, sum of\nsparse and rotated sparse vectors, etc. In this paper, we consider general\nsuperposition models which allow sum of any number of component parameters, and\neach component structure can be characterized by any norm. We present a simple\nestimator for such models, give a geometric condition under which the\ncomponents can be accurately estimated, characterize sample complexity of the\nestimator, and give high probability non-asymptotic bounds on the componentwise\nestimation error. We use tools from empirical processes and generic chaining\nfor the statistical analysis, and our results, which substantially generalize\nprior work on superposition models, are in terms of Gaussian widths of suitable\nsets.\n", "versions": [{"version": "v1", "created": "Tue, 30 May 2017 23:00:34 GMT"}], "update_date": "2017-06-01", "authors_parsed": [["Gu", "Qilong", ""], ["Banerjee", "Arindam", ""]]}, {"id": "1705.10887", "submitter": "Javier Turek", "authors": "Javier S. Turek, Alexander Huth", "title": "Efficient, sparse representation of manifold distance matrices for\n  classical scaling", "comments": "Conference CVPR 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CV cs.LG cs.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Geodesic distance matrices can reveal shape properties that are largely\ninvariant to non-rigid deformations, and thus are often used to analyze and\nrepresent 3-D shapes. However, these matrices grow quadratically with the\nnumber of points. Thus for large point sets it is common to use a low-rank\napproximation to the distance matrix, which fits in memory and can be\nefficiently analyzed using methods such as multidimensional scaling (MDS). In\nthis paper we present a novel sparse method for efficiently representing\ngeodesic distance matrices using biharmonic interpolation. This method exploits\nknowledge of the data manifold to learn a sparse interpolation operator that\napproximates distances using a subset of points. We show that our method is 2x\nfaster and uses 20x less memory than current leading methods for solving MDS on\nlarge point sets, with similar quality. This enables analyses of large point\nsets that were previously infeasible.\n", "versions": [{"version": "v1", "created": "Tue, 30 May 2017 23:18:18 GMT"}, {"version": "v2", "created": "Thu, 29 Mar 2018 17:35:03 GMT"}], "update_date": "2018-04-02", "authors_parsed": [["Turek", "Javier S.", ""], ["Huth", "Alexander", ""]]}, {"id": "1705.10915", "submitter": "Emily Denton", "authors": "Emily Denton, Vighnesh Birodkar", "title": "Unsupervised Learning of Disentangled Representations from Video", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a new model DrNET that learns disentangled image representations\nfrom video. Our approach leverages the temporal coherence of video and a novel\nadversarial loss to learn a representation that factorizes each frame into a\nstationary part and a temporally varying component. The disentangled\nrepresentation can be used for a range of tasks. For example, applying a\nstandard LSTM to the time-vary components enables prediction of future frames.\nWe evaluate our approach on a range of synthetic and real videos, demonstrating\nthe ability to coherently generate hundreds of steps into the future.\n", "versions": [{"version": "v1", "created": "Wed, 31 May 2017 02:12:19 GMT"}], "update_date": "2017-06-01", "authors_parsed": [["Denton", "Emily", ""], ["Birodkar", "Vighnesh", ""]]}, {"id": "1705.10918", "submitter": "Nikolaos Sahinidis", "authors": "Zachary T. Wilson and Nikolaos V. Sahinidis", "title": "The ALAMO approach to machine learning", "comments": null, "journal-ref": null, "doi": "10.1016/j.compchemeng.2017.02.010", "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  ALAMO is a computational methodology for leaning algebraic functions from\ndata. Given a data set, the approach begins by building a low-complexity,\nlinear model composed of explicit non-linear transformations of the independent\nvariables. Linear combinations of these non-linear transformations allow a\nlinear model to better approximate complex behavior observed in real processes.\nThe model is refined, as additional data are obtained in an adaptive fashion\nthrough error maximization sampling using derivative-free optimization. Models\nbuilt using ALAMO can enforce constraints on the response variables to\nincorporate first-principles knowledge. The ability of ALAMO to generate simple\nand accurate models for a number of reaction problems is demonstrated. The\nerror maximization sampling is compared with Latin hypercube designs to\ndemonstrate its sampling efficiency. ALAMO's constrained regression methodology\nis used to further refine concentration models, resulting in models that\nperform better on validation data and satisfy upper and lower bounds placed on\nmodel outputs.\n", "versions": [{"version": "v1", "created": "Wed, 31 May 2017 02:28:04 GMT"}], "update_date": "2017-06-01", "authors_parsed": [["Wilson", "Zachary T.", ""], ["Sahinidis", "Nikolaos V.", ""]]}, {"id": "1705.10924", "submitter": "Feng Nan", "authors": "Henghui Zhu, Feng Nan, Ioannis Paschalidis, Venkatesh Saligrama", "title": "Sequential Dynamic Decision Making with Deep Neural Nets on a Test-Time\n  Budget", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural network (DNN) based approaches hold significant potential for\nreinforcement learning (RL) and have already shown remarkable gains over\nstate-of-art methods in a number of applications. The effectiveness of DNN\nmethods can be attributed to leveraging the abundance of supervised data to\nlearn value functions, Q-functions, and policy function approximations without\nthe need for feature engineering. Nevertheless, the deployment of DNN-based\npredictors with very deep architectures can pose an issue due to computational\nand other resource constraints at test-time in a number of applications. We\npropose a novel approach for reducing the average latency by learning a\ncomputationally efficient gating function that is capable of recognizing states\nin a sequential decision process for which policy prescriptions of a shallow\nnetwork suffices and deeper layers of the DNN have little marginal utility. The\noverall system is adaptive in that it dynamically switches control actions\nbased on state-estimates in order to reduce average latency without sacrificing\nterminal performance. We experiment with a number of alternative loss-functions\nto train gating functions and shallow policies and show that in a number of\napplications a speed-up of up to almost 5X can be obtained with little loss in\nperformance.\n", "versions": [{"version": "v1", "created": "Wed, 31 May 2017 02:45:55 GMT"}], "update_date": "2017-06-01", "authors_parsed": [["Zhu", "Henghui", ""], ["Nan", "Feng", ""], ["Paschalidis", "Ioannis", ""], ["Saligrama", "Venkatesh", ""]]}, {"id": "1705.10941", "submitter": "Yuichi Yoshida", "authors": "Yuichi Yoshida and Takeru Miyato", "title": "Spectral Norm Regularization for Improving the Generalizability of Deep\n  Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate the generalizability of deep learning based on the sensitivity\nto input perturbation. We hypothesize that the high sensitivity to the\nperturbation of data degrades the performance on it. To reduce the sensitivity\nto perturbation, we propose a simple and effective regularization method,\nreferred to as spectral norm regularization, which penalizes the high spectral\nnorm of weight matrices in neural networks. We provide supportive evidence for\nthe abovementioned hypothesis by experimentally confirming that the models\ntrained using spectral norm regularization exhibit better generalizability than\nother baseline methods.\n", "versions": [{"version": "v1", "created": "Wed, 31 May 2017 04:56:25 GMT"}], "update_date": "2017-06-01", "authors_parsed": [["Yoshida", "Yuichi", ""], ["Miyato", "Takeru", ""]]}, {"id": "1705.10958", "submitter": "Alessandro Rudi", "authors": "Alessandro Rudi, Luigi Carratino and Lorenzo Rosasco", "title": "FALKON: An Optimal Large Scale Kernel Method", "comments": "NIPS 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Kernel methods provide a principled way to perform non linear, nonparametric\nlearning. They rely on solid functional analytic foundations and enjoy optimal\nstatistical properties. However, at least in their basic form, they have\nlimited applicability in large scale scenarios because of stringent\ncomputational requirements in terms of time and especially memory. In this\npaper, we take a substantial step in scaling up kernel methods, proposing\nFALKON, a novel algorithm that allows to efficiently process millions of\npoints. FALKON is derived combining several algorithmic principles, namely\nstochastic subsampling, iterative solvers and preconditioning. Our theoretical\nanalysis shows that optimal statistical accuracy is achieved requiring\nessentially $O(n)$ memory and $O(n\\sqrt{n})$ time. An extensive experimental\nanalysis on large scale datasets shows that, even with a single machine, FALKON\noutperforms previous state of the art solutions, which exploit\nparallel/distributed architectures.\n", "versions": [{"version": "v1", "created": "Wed, 31 May 2017 06:56:53 GMT"}, {"version": "v2", "created": "Tue, 31 Oct 2017 10:43:36 GMT"}, {"version": "v3", "created": "Wed, 31 Jan 2018 18:35:40 GMT"}], "update_date": "2018-02-01", "authors_parsed": [["Rudi", "Alessandro", ""], ["Carratino", "Luigi", ""], ["Rosasco", "Lorenzo", ""]]}, {"id": "1705.10993", "submitter": "Julien Perez", "authors": "Julien Perez and Tomi Silander", "title": "Non-Markovian Control with Gated End-to-End Memory Policy Networks", "comments": "11 pages, 1 figure, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Partially observable environments present an important open challenge in the\ndomain of sequential control learning with delayed rewards. Despite numerous\nattempts during the two last decades, the majority of reinforcement learning\nalgorithms and associated approximate models, applied to this context, still\nassume Markovian state transitions. In this paper, we explore the use of a\nrecently proposed attention-based model, the Gated End-to-End Memory Network,\nfor sequential control. We call the resulting model the Gated End-to-End Memory\nPolicy Network. More precisely, we use a model-free value-based algorithm to\nlearn policies for partially observed domains using this memory-enhanced neural\nnetwork. This model is end-to-end learnable and it features unbounded memory.\nIndeed, because of its attention mechanism and associated non-parametric\nmemory, the proposed model allows us to define an attention mechanism over the\nobservation stream unlike recurrent models. We show encouraging results that\nillustrate the capability of our attention-based model in the context of the\ncontinuous-state non-stationary control problem of stock trading. We also\npresent an OpenAI Gym environment for simulated stock exchange and explain its\nrelevance as a benchmark for the field of non-Markovian decision process\nlearning.\n", "versions": [{"version": "v1", "created": "Wed, 31 May 2017 09:00:44 GMT"}], "update_date": "2017-06-01", "authors_parsed": [["Perez", "Julien", ""], ["Silander", "Tomi", ""]]}, {"id": "1705.11001", "submitter": "Kevin Lin", "authors": "Kevin Lin, Dianqi Li, Xiaodong He, Zhengyou Zhang, Ming-Ting Sun", "title": "Adversarial Ranking for Language Generation", "comments": "NIPS2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generative adversarial networks (GANs) have great successes on synthesizing\ndata. However, the existing GANs restrict the discriminator to be a binary\nclassifier, and thus limit their learning capacity for tasks that need to\nsynthesize output with rich structures such as natural language descriptions.\nIn this paper, we propose a novel generative adversarial network, RankGAN, for\ngenerating high-quality language descriptions. Rather than training the\ndiscriminator to learn and assign absolute binary predicate for individual data\nsample, the proposed RankGAN is able to analyze and rank a collection of\nhuman-written and machine-written sentences by giving a reference group. By\nviewing a set of data samples collectively and evaluating their quality through\nrelative ranking scores, the discriminator is able to make better assessment\nwhich in turn helps to learn a better generator. The proposed RankGAN is\noptimized through the policy gradient technique. Experimental results on\nmultiple public datasets clearly demonstrate the effectiveness of the proposed\napproach.\n", "versions": [{"version": "v1", "created": "Wed, 31 May 2017 09:21:04 GMT"}, {"version": "v2", "created": "Sun, 3 Dec 2017 01:58:50 GMT"}, {"version": "v3", "created": "Mon, 16 Apr 2018 05:43:33 GMT"}], "update_date": "2018-04-17", "authors_parsed": [["Lin", "Kevin", ""], ["Li", "Dianqi", ""], ["He", "Xiaodong", ""], ["Zhang", "Zhengyou", ""], ["Sun", "Ming-Ting", ""]]}, {"id": "1705.11023", "submitter": "Dan Oprisa", "authors": "Dan Oprisa, Peter Toth", "title": "Criticality & Deep Learning II: Momentum Renormalisation Group", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cond-mat.stat-mech cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Guided by critical systems found in nature we develop a novel mechanism\nconsisting of inhomogeneous polynomial regularisation via which we can induce\nscale invariance in deep learning systems. Technically, we map our deep\nlearning (DL) setup to a genuine field theory, on which we act with the\nRenormalisation Group (RG) in momentum space and produce the flow equations of\nthe couplings; those are translated to constraints and consequently interpreted\nas \"critical regularisation\" conditions in the optimiser; the resulting\nequations hence prove to be sufficient conditions for - and serve as an elegant\nand simple mechanism to induce scale invariance in any deep learning setup.\n", "versions": [{"version": "v1", "created": "Wed, 31 May 2017 10:28:39 GMT"}], "update_date": "2017-06-01", "authors_parsed": [["Oprisa", "Dan", ""], ["Toth", "Peter", ""]]}, {"id": "1705.11040", "submitter": "Tim Rockt\\\"aschel", "authors": "Tim Rockt\\\"aschel and Sebastian Riedel", "title": "End-to-End Differentiable Proving", "comments": "NIPS 2017 camera-ready, NIPS 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.AI cs.LG cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce neural networks for end-to-end differentiable proving of queries\nto knowledge bases by operating on dense vector representations of symbols.\nThese neural networks are constructed recursively by taking inspiration from\nthe backward chaining algorithm as used in Prolog. Specifically, we replace\nsymbolic unification with a differentiable computation on vector\nrepresentations of symbols using a radial basis function kernel, thereby\ncombining symbolic reasoning with learning subsymbolic vector representations.\nBy using gradient descent, the resulting neural network can be trained to infer\nfacts from a given incomplete knowledge base. It learns to (i) place\nrepresentations of similar symbols in close proximity in a vector space, (ii)\nmake use of such similarities to prove queries, (iii) induce logical rules, and\n(iv) use provided and induced logical rules for multi-hop reasoning. We\ndemonstrate that this architecture outperforms ComplEx, a state-of-the-art\nneural link prediction model, on three out of four benchmark knowledge bases\nwhile at the same time inducing interpretable function-free first-order logic\nrules.\n", "versions": [{"version": "v1", "created": "Wed, 31 May 2017 11:40:57 GMT"}, {"version": "v2", "created": "Mon, 4 Dec 2017 00:24:04 GMT"}], "update_date": "2017-12-05", "authors_parsed": [["Rockt\u00e4schel", "Tim", ""], ["Riedel", "Sebastian", ""]]}, {"id": "1705.11041", "submitter": "Francesco Locatello", "authors": "Francesco Locatello, Michael Tschannen, Gunnar R\\\"atsch, Martin Jaggi", "title": "Greedy Algorithms for Cone Constrained Optimization with Convergence\n  Guarantees", "comments": "NIPS 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Greedy optimization methods such as Matching Pursuit (MP) and Frank-Wolfe\n(FW) algorithms regained popularity in recent years due to their simplicity,\neffectiveness and theoretical guarantees. MP and FW address optimization over\nthe linear span and the convex hull of a set of atoms, respectively. In this\npaper, we consider the intermediate case of optimization over the convex cone,\nparametrized as the conic hull of a generic atom set, leading to the first\nprincipled definitions of non-negative MP algorithms for which we give explicit\nconvergence rates and demonstrate excellent empirical performance. In\nparticular, we derive sublinear ($\\mathcal{O}(1/t)$) convergence on general\nsmooth and convex objectives, and linear convergence ($\\mathcal{O}(e^{-t})$) on\nstrongly convex objectives, in both cases for general sets of atoms.\nFurthermore, we establish a clear correspondence of our algorithms to known\nalgorithms from the MP and FW literature. Our novel algorithms and analyses\ntarget general atom sets and general objective functions, and hence are\ndirectly applicable to a large variety of learning settings.\n", "versions": [{"version": "v1", "created": "Wed, 31 May 2017 11:47:55 GMT"}, {"version": "v2", "created": "Mon, 7 Aug 2017 07:57:03 GMT"}, {"version": "v3", "created": "Sun, 19 Nov 2017 16:17:07 GMT"}], "update_date": "2017-11-21", "authors_parsed": [["Locatello", "Francesco", ""], ["Tschannen", "Michael", ""], ["R\u00e4tsch", "Gunnar", ""], ["Jaggi", "Martin", ""]]}, {"id": "1705.11105", "submitter": "Zhenzhou Wu", "authors": "Zhenzhou Wu, Sean Saito", "title": "HiNet: Hierarchical Classification with Neural Network", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Traditionally, classifying large hierarchical labels with more than 10000\ndistinct traces can only be achieved with flatten labels. Although flatten\nlabels is feasible, it misses the hierarchical information in the labels.\nHierarchical models like HSVM by \\cite{vural2004hierarchical} becomes\nimpossible to train because of the sheer number of SVMs in the whole\narchitecture. We developed a hierarchical architecture based on neural networks\nthat is simple to train. Also, we derived an inference algorithm that can\nefficiently infer the MAP (maximum a posteriori) trace guaranteed by our\ntheorems. Furthermore, the complexity of the model is only $O(n^2)$ compared to\n$O(n^h)$ in a flatten model, where $h$ is the height of the hierarchy.\n", "versions": [{"version": "v1", "created": "Wed, 31 May 2017 14:08:03 GMT"}, {"version": "v2", "created": "Fri, 12 Jan 2018 14:55:09 GMT"}], "update_date": "2018-01-15", "authors_parsed": [["Wu", "Zhenzhou", ""], ["Saito", "Sean", ""]]}, {"id": "1705.11107", "submitter": "Ankur Moitra", "authors": "Linus Hamilton, Frederic Koehler, Ankur Moitra", "title": "Information Theoretic Properties of Markov Random Fields, and their\n  Algorithmic Applications", "comments": "25 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS cs.IT math.IT math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Markov random fields area popular model for high-dimensional probability\ndistributions. Over the years, many mathematical, statistical and algorithmic\nproblems on them have been studied. Until recently, the only known algorithms\nfor provably learning them relied on exhaustive search, correlation decay or\nvarious incoherence assumptions. Bresler gave an algorithm for learning general\nIsing models on bounded degree graphs. His approach was based on a structural\nresult about mutual information in Ising models.\n  Here we take a more conceptual approach to proving lower bounds on the mutual\ninformation through setting up an appropriate zero-sum game. Our proof\ngeneralizes well beyond Ising models, to arbitrary Markov random fields with\nhigher order interactions. As an application, we obtain algorithms for learning\nMarkov random fields on bounded degree graphs on $n$ nodes with $r$-order\ninteractions in $n^r$ time and $\\log n$ sample complexity. The sample\ncomplexity is information theoretically optimal up to the dependence on the\nmaximum degree. The running time is nearly optimal under standard conjectures\nabout the hardness of learning parity with noise.\n", "versions": [{"version": "v1", "created": "Wed, 31 May 2017 14:18:20 GMT"}], "update_date": "2017-06-01", "authors_parsed": [["Hamilton", "Linus", ""], ["Koehler", "Frederic", ""], ["Moitra", "Ankur", ""]]}, {"id": "1705.11122", "submitter": "Qizhe Xie", "authors": "Qizhe Xie, Zihang Dai, Yulun Du, Eduard Hovy, Graham Neubig", "title": "Controllable Invariance through Adversarial Feature Learning", "comments": "NIPS 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning meaningful representations that maintain the content necessary for a\nparticular task while filtering away detrimental variations is a problem of\ngreat interest in machine learning. In this paper, we tackle the problem of\nlearning representations invariant to a specific factor or trait of data. The\nrepresentation learning process is formulated as an adversarial minimax game.\nWe analyze the optimal equilibrium of such a game and find that it amounts to\nmaximizing the uncertainty of inferring the detrimental factor given the\nrepresentation while maximizing the certainty of making task-specific\npredictions. On three benchmark tasks, namely fair and bias-free\nclassification, language-independent generation, and lighting-independent image\nclassification, we show that the proposed framework induces an invariant\nrepresentation, and leads to better generalization evidenced by the improved\nperformance.\n", "versions": [{"version": "v1", "created": "Wed, 31 May 2017 14:57:33 GMT"}, {"version": "v2", "created": "Mon, 6 Nov 2017 17:47:33 GMT"}, {"version": "v3", "created": "Mon, 29 Jan 2018 00:59:58 GMT"}], "update_date": "2018-01-30", "authors_parsed": [["Xie", "Qizhe", ""], ["Dai", "Zihang", ""], ["Du", "Yulun", ""], ["Hovy", "Eduard", ""], ["Neubig", "Graham", ""]]}, {"id": "1705.11146", "submitter": "Friedemann Zenke", "authors": "Friedemann Zenke and Surya Ganguli", "title": "SuperSpike: Supervised learning in multi-layer spiking neural networks", "comments": null, "journal-ref": null, "doi": "10.1162/neco_a_01086", "report-no": null, "categories": "q-bio.NC cs.LG cs.NE stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  A vast majority of computation in the brain is performed by spiking neural\nnetworks. Despite the ubiquity of such spiking, we currently lack an\nunderstanding of how biological spiking neural circuits learn and compute\nin-vivo, as well as how we can instantiate such capabilities in artificial\nspiking circuits in-silico. Here we revisit the problem of supervised learning\nin temporally coding multi-layer spiking neural networks. First, by using a\nsurrogate gradient approach, we derive SuperSpike, a nonlinear voltage-based\nthree factor learning rule capable of training multi-layer networks of\ndeterministic integrate-and-fire neurons to perform nonlinear computations on\nspatiotemporal spike patterns. Second, inspired by recent results on feedback\nalignment, we compare the performance of our learning rule under different\ncredit assignment strategies for propagating output errors to hidden units.\nSpecifically, we test uniform, symmetric and random feedback, finding that\nsimpler tasks can be solved with any type of feedback, while more complex tasks\nrequire symmetric feedback. In summary, our results open the door to obtaining\na better scientific understanding of learning and computation in spiking neural\nnetworks by advancing our ability to train them to solve nonlinear problems\ninvolving transformations between different spatiotemporal spike-time patterns.\n", "versions": [{"version": "v1", "created": "Wed, 31 May 2017 15:31:26 GMT"}, {"version": "v2", "created": "Sat, 14 Oct 2017 15:08:04 GMT"}], "update_date": "2018-05-31", "authors_parsed": [["Zenke", "Friedemann", ""], ["Ganguli", "Surya", ""]]}, {"id": "1705.11159", "submitter": "Chang Xu", "authors": "Chang Xu, Tao Qin, Gang Wang and Tie-Yan Liu", "title": "Reinforcement Learning for Learning Rate Control", "comments": "7 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Stochastic gradient descent (SGD), which updates the model parameters by\nadding a local gradient times a learning rate at each step, is widely used in\nmodel training of machine learning algorithms such as neural networks. It is\nobserved that the models trained by SGD are sensitive to learning rates and\ngood learning rates are problem specific. We propose an algorithm to\nautomatically learn learning rates using neural network based actor-critic\nmethods from deep reinforcement learning (RL).In particular, we train a policy\nnetwork called actor to decide the learning rate at each step during training,\nand a value network called critic to give feedback about quality of the\ndecision (e.g., the goodness of the learning rate outputted by the actor) that\nthe actor made. The introduction of auxiliary actor and critic networks helps\nthe main network achieve better performance. Experiments on different datasets\nand network architectures show that our approach leads to better convergence of\nSGD than human-designed competitors.\n", "versions": [{"version": "v1", "created": "Wed, 31 May 2017 15:58:35 GMT"}], "update_date": "2017-06-01", "authors_parsed": [["Xu", "Chang", ""], ["Qin", "Tao", ""], ["Wang", "Gang", ""], ["Liu", "Tie-Yan", ""]]}, {"id": "1705.11192", "submitter": "Serhii Havrylov", "authors": "Serhii Havrylov, Ivan Titov", "title": "Emergence of Language with Multi-agent Games: Learning to Communicate\n  with Sequences of Symbols", "comments": "The paper was accepted at NIPS 2017. The extended abstract was\n  presented at ICLR 2017 workshop track", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL cs.CV cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning to communicate through interaction, rather than relying on explicit\nsupervision, is often considered a prerequisite for developing a general AI. We\nstudy a setting where two agents engage in playing a referential game and, from\nscratch, develop a communication protocol necessary to succeed in this game.\nUnlike previous work, we require that messages they exchange, both at train and\ntest time, are in the form of a language (i.e. sequences of discrete symbols).\nWe compare a reinforcement learning approach and one using a differentiable\nrelaxation (straight-through Gumbel-softmax estimator) and observe that the\nlatter is much faster to converge and it results in more effective protocols.\nInterestingly, we also observe that the protocol we induce by optimizing the\ncommunication success exhibits a degree of compositionality and variability\n(i.e. the same information can be phrased in different ways), both properties\ncharacteristic of natural languages. As the ultimate goal is to ensure that\ncommunication is accomplished in natural language, we also perform experiments\nwhere we inject prior information about natural language into our model and\nstudy properties of the resulting protocol.\n", "versions": [{"version": "v1", "created": "Wed, 31 May 2017 17:47:55 GMT"}, {"version": "v2", "created": "Sat, 4 Nov 2017 15:04:51 GMT"}], "update_date": "2017-11-07", "authors_parsed": [["Havrylov", "Serhii", ""], ["Titov", "Ivan", ""]]}]