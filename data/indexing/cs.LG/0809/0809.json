[{"id": "0809.0032", "submitter": "Darryl Lin", "authors": "D. D. Lin and T. J. Lim", "title": "A Variational Inference Framework for Soft-In-Soft-Out Detection in\n  Multiple Access Channels", "comments": "Submitted to Transactions on Information Theory", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.LG math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a unified framework for deriving and studying soft-in-soft-out\n(SISO) detection in interference channels using the concept of variational\ninference. The proposed framework may be used in multiple-access interference\n(MAI), inter-symbol interference (ISI), and multiple-input multiple-outpu\n(MIMO) channels. Without loss of generality, we will focus our attention on\nturbo multiuser detection, to facilitate a more concrete discussion. It is\nshown that, with some loss of optimality, variational inference avoids the\nexponential complexity of a posteriori probability (APP) detection by\noptimizing a closely-related, but much more manageable, objective function\ncalled variational free energy. In addition to its systematic appeal, there are\nseveral other advantages to this viewpoint. First of all, it provides unified\nand rigorous justifications for numerous detectors that were proposed on\nradically different grounds, and facilitates convenient joint detection and\ndecoding (utilizing the turbo principle) when error-control codes are\nincorporated. Secondly, efficient joint parameter estimation and data detection\nis possible via the variational expectation maximization (EM) algorithm, such\nthat the detrimental effect of inaccurate channel knowledge at the receiver may\nbe dealt with systematically. We are also able to extend BPSK-based SISO\ndetection schemes to arbitrary square QAM constellations in a rigorous manner\nusing a variational argument.\n", "versions": [{"version": "v1", "created": "Sat, 30 Aug 2008 01:05:29 GMT"}], "update_date": "2009-11-23", "authors_parsed": [["Lin", "D. D.", ""], ["Lim", "T. J.", ""]]}, {"id": "0809.0124", "submitter": "Peter Turney", "authors": "Peter D. Turney (National Research Council of Canada)", "title": "A Uniform Approach to Analogies, Synonyms, Antonyms, and Associations", "comments": "related work available at http://purl.org/peter.turney/", "journal-ref": "Proceedings of the 22nd International Conference on Computational\n  Linguistics (Coling 2008), August 2008, Manchester, UK, Pages 905-912", "doi": null, "report-no": "NRC 50398", "categories": "cs.CL cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recognizing analogies, synonyms, antonyms, and associations appear to be four\ndistinct tasks, requiring distinct NLP algorithms. In the past, the four tasks\nhave been treated independently, using a wide variety of algorithms. These four\nsemantic classes, however, are a tiny sample of the full range of semantic\nphenomena, and we cannot afford to create ad hoc algorithms for each semantic\nphenomenon; we need to seek a unified approach. We propose to subsume a broad\nrange of phenomena under analogies. To limit the scope of this paper, we\nrestrict our attention to the subsumption of synonyms, antonyms, and\nassociations. We introduce a supervised corpus-based machine learning algorithm\nfor classifying analogous word pairs, and we show that it can solve\nmultiple-choice SAT analogy questions, TOEFL synonym questions, ESL\nsynonym-antonym questions, and similar-associated-both questions from cognitive\npsychology.\n", "versions": [{"version": "v1", "created": "Sun, 31 Aug 2008 14:00:26 GMT"}], "update_date": "2008-09-02", "authors_parsed": [["Turney", "Peter D.", "", "National Research Council of Canada"]]}, {"id": "0809.0444", "submitter": "Gambs S\\'ebastien", "authors": "S\\'ebastien Gambs", "title": "Quantum classification", "comments": "Preliminary version, comments are welcome", "journal-ref": null, "doi": null, "report-no": null, "categories": "quant-ph cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Quantum classification is defined as the task of predicting the associated\nclass of an unknown quantum state drawn from an ensemble of pure states given a\nfinite number of copies of this state. By recasting the state discrimination\nproblem within the framework of Machine Learning (ML), we can use the notion of\nlearning reduction coming from classical ML to solve different variants of the\nclassification task, such as the weighted binary and the multiclass versions.\n", "versions": [{"version": "v1", "created": "Tue, 2 Sep 2008 19:56:54 GMT"}, {"version": "v2", "created": "Tue, 2 Sep 2008 20:02:34 GMT"}], "update_date": "2016-08-14", "authors_parsed": [["Gambs", "S\u00e9bastien", ""]]}, {"id": "0809.0490", "submitter": "Alexander Gorban", "authors": "A. N. Gorban, A. Y. Zinovyev", "title": "Principal Graphs and Manifolds", "comments": "36 pages, 6 figures, minor corrections", "journal-ref": "Handbook of Research on Machine Learning Applications and Trends:\n  Algorithms, Methods and Techniques, Ch. 2, Information Science Reference,\n  2009. 28-59", "doi": "10.4018/978-1-60566-766-9", "report-no": null, "categories": "cs.LG cs.NE stat.ML", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  In many physical, statistical, biological and other investigations it is\ndesirable to approximate a system of points by objects of lower dimension\nand/or complexity. For this purpose, Karl Pearson invented principal component\nanalysis in 1901 and found 'lines and planes of closest fit to system of\npoints'. The famous k-means algorithm solves the approximation problem too, but\nby finite sets instead of lines and planes. This chapter gives a brief\npractical introduction into the methods of construction of general principal\nobjects, i.e. objects embedded in the 'middle' of the multidimensional data\nset. As a basis, the unifying framework of mean squared distance approximation\nof finite datasets is selected. Principal graphs and manifolds are constructed\nas generalisations of principal components and k-means principal points. For\nthis purpose, the family of expectation/maximisation algorithms with nearest\ngeneralisations is presented. Construction of principal graphs with controlled\ncomplexity is based on the graph grammar approach.\n", "versions": [{"version": "v1", "created": "Tue, 2 Sep 2008 18:04:53 GMT"}, {"version": "v2", "created": "Mon, 9 May 2011 13:23:08 GMT"}], "update_date": "2011-05-10", "authors_parsed": [["Gorban", "A. N.", ""], ["Zinovyev", "A. Y.", ""]]}, {"id": "0809.1017", "submitter": "Peter Grunwald", "authors": "Peter Grunwald", "title": "Entropy Concentration and the Empirical Coding Game", "comments": "A somewhat modified version of this paper was published in Statistica\n  Neerlandica 62(3), pages 374-392, 2008", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.LG math.IT math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We give a characterization of Maximum Entropy/Minimum Relative Entropy\ninference by providing two `strong entropy concentration' theorems. These\ntheorems unify and generalize Jaynes' `concentration phenomenon' and Van\nCampenhout and Cover's `conditional limit theorem'. The theorems characterize\nexactly in what sense a prior distribution Q conditioned on a given constraint,\nand the distribution P, minimizing the relative entropy D(P ||Q) over all\ndistributions satisfying the constraint, are `close' to each other. We then\napply our theorems to establish the relationship between entropy concentration\nand a game-theoretic characterization of Maximum Entropy Inference due to\nTopsoe and others.\n", "versions": [{"version": "v1", "created": "Fri, 5 Sep 2008 12:18:15 GMT"}], "update_date": "2008-09-17", "authors_parsed": [["Grunwald", "Peter", ""]]}, {"id": "0809.1241", "submitter": "Xinjia Chen", "authors": "Xinjia Chen", "title": "A New Framework of Multistage Estimation", "comments": "254 pages, no figure; added more references; main results appeared in\n  Proceedings of SPIE, Orlando, Florida, USA, April 2010 and 2011", "journal-ref": null, "doi": "10.1103/PhysRevE.79.026307", "report-no": null, "categories": "math.ST cs.LG math.PR stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we have established a unified framework of multistage\nparameter estimation. We demonstrate that a wide variety of statistical\nproblems such as fixed-sample-size interval estimation, point estimation with\nerror control, bounded-width confidence intervals, interval estimation\nfollowing hypothesis testing, construction of confidence sequences, can be cast\ninto the general framework of constructing sequential random intervals with\nprescribed coverage probabilities. We have developed exact methods for the\nconstruction of such sequential random intervals in the context of multistage\nsampling. In particular, we have established inclusion principle and coverage\ntuning techniques to control and adjust the coverage probabilities of\nsequential random intervals. We have obtained concrete sampling schemes which\nare unprecedentedly efficient in terms of sampling effort as compared to\nexisting procedures.\n", "versions": [{"version": "v1", "created": "Mon, 8 Sep 2008 14:03:24 GMT"}, {"version": "v10", "created": "Tue, 7 Apr 2009 16:48:35 GMT"}, {"version": "v11", "created": "Fri, 24 Apr 2009 17:17:49 GMT"}, {"version": "v12", "created": "Mon, 27 Apr 2009 18:33:01 GMT"}, {"version": "v13", "created": "Mon, 29 Jun 2009 01:01:34 GMT"}, {"version": "v14", "created": "Wed, 23 Sep 2009 16:53:22 GMT"}, {"version": "v15", "created": "Wed, 23 Sep 2009 20:01:39 GMT"}, {"version": "v16", "created": "Fri, 20 Nov 2009 22:10:54 GMT"}, {"version": "v17", "created": "Wed, 2 Dec 2009 22:01:56 GMT"}, {"version": "v18", "created": "Mon, 21 Dec 2009 22:43:19 GMT"}, {"version": "v19", "created": "Sun, 7 Mar 2010 04:49:46 GMT"}, {"version": "v2", "created": "Thu, 16 Oct 2008 16:14:26 GMT"}, {"version": "v20", "created": "Sat, 7 Aug 2010 00:17:28 GMT"}, {"version": "v21", "created": "Mon, 15 Nov 2010 02:38:02 GMT"}, {"version": "v22", "created": "Thu, 25 Nov 2010 00:30:46 GMT"}, {"version": "v23", "created": "Tue, 1 Feb 2011 00:08:06 GMT"}, {"version": "v24", "created": "Mon, 21 Feb 2011 00:58:11 GMT"}, {"version": "v25", "created": "Mon, 6 Jun 2011 01:11:59 GMT"}, {"version": "v26", "created": "Wed, 13 Jul 2011 18:13:33 GMT"}, {"version": "v27", "created": "Sun, 31 Jul 2011 19:29:34 GMT"}, {"version": "v28", "created": "Sun, 4 Sep 2011 18:46:04 GMT"}, {"version": "v29", "created": "Tue, 29 Nov 2011 19:21:24 GMT"}, {"version": "v3", "created": "Sun, 2 Nov 2008 23:25:28 GMT"}, {"version": "v30", "created": "Thu, 29 Dec 2011 17:10:00 GMT"}, {"version": "v31", "created": "Thu, 9 Feb 2012 19:54:11 GMT"}, {"version": "v32", "created": "Tue, 15 May 2012 16:10:57 GMT"}, {"version": "v33", "created": "Sat, 1 Sep 2012 21:53:39 GMT"}, {"version": "v34", "created": "Sun, 30 Sep 2012 20:46:09 GMT"}, {"version": "v35", "created": "Wed, 5 Dec 2012 00:39:40 GMT"}, {"version": "v4", "created": "Tue, 2 Dec 2008 21:40:37 GMT"}, {"version": "v5", "created": "Fri, 30 Jan 2009 02:04:54 GMT"}, {"version": "v6", "created": "Mon, 2 Mar 2009 18:51:46 GMT"}, {"version": "v7", "created": "Mon, 2 Mar 2009 21:35:54 GMT"}, {"version": "v8", "created": "Sun, 29 Mar 2009 20:07:10 GMT"}, {"version": "v9", "created": "Sun, 5 Apr 2009 21:07:25 GMT"}], "update_date": "2013-11-05", "authors_parsed": [["Chen", "Xinjia", ""]]}, {"id": "0809.1270", "submitter": "Marcus Hutter", "authors": "Marcus Hutter", "title": "Predictive Hypothesis Identification", "comments": "16 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While statistics focusses on hypothesis testing and on estimating (properties\nof) the true sampling distribution, in machine learning the performance of\nlearning algorithms on future data is the primary issue. In this paper we\nbridge the gap with a general principle (PHI) that identifies hypotheses with\nbest predictive performance. This includes predictive point and interval\nestimation, simple and composite hypothesis testing, (mixture) model selection,\nand others as special cases. For concrete instantiations we will recover\nwell-known methods, variations thereof, and new ones. PHI nicely justifies,\nreconciles, and blends (a reparametrization invariant variation of) MAP, ML,\nMDL, and moment estimation. One particular feature of PHI is that it can\ngenuinely deal with nested hypotheses.\n", "versions": [{"version": "v1", "created": "Mon, 8 Sep 2008 04:18:17 GMT"}], "update_date": "2009-12-30", "authors_parsed": [["Hutter", "Marcus", ""]]}, {"id": "0809.1493", "submitter": "Francis Bach", "authors": "Francis Bach (INRIA Rocquencourt)", "title": "Exploring Large Feature Spaces with Hierarchical Multiple Kernel\n  Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For supervised and unsupervised learning, positive definite kernels allow to\nuse large and potentially infinite dimensional feature spaces with a\ncomputational cost that only depends on the number of observations. This is\nusually done through the penalization of predictor functions by Euclidean or\nHilbertian norms. In this paper, we explore penalizing by sparsity-inducing\nnorms such as the l1-norm or the block l1-norm. We assume that the kernel\ndecomposes into a large sum of individual basis kernels which can be embedded\nin a directed acyclic graph; we show that it is then possible to perform kernel\nselection through a hierarchical multiple kernel learning framework, in\npolynomial time in the number of selected kernels. This framework is naturally\napplied to non linear variable selection; our extensive simulations on\nsynthetic datasets and datasets from the UCI repository show that efficiently\nexploring the large feature space through sparsity-inducing norms leads to\nstate-of-the-art predictive performance.\n", "versions": [{"version": "v1", "created": "Tue, 9 Sep 2008 06:48:10 GMT"}], "update_date": "2008-09-10", "authors_parsed": [["Bach", "Francis", "", "INRIA Rocquencourt"]]}, {"id": "0809.1590", "submitter": "Massimiliano Pontil", "authors": "Andreas Argyriou, Charles Micchelli and Massimiliano Pontil", "title": "When is there a representer theorem? Vector versus matrix regularizers", "comments": "22 pages 2 figures", "journal-ref": "Journal of Machine Learning Research, 10:2507-2529, 2009", "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider a general class of regularization methods which learn a vector of\nparameters on the basis of linear measurements. It is well known that if the\nregularizer is a nondecreasing function of the inner product then the learned\nvector is a linear combination of the input data. This result, known as the\n{\\em representer theorem}, is at the basis of kernel-based methods in machine\nlearning. In this paper, we prove the necessity of the above condition, thereby\ncompleting the characterization of kernel methods based on regularization. We\nfurther extend our analysis to regularization methods which learn a matrix, a\nproblem which is motivated by the application to multi-task learning. In this\ncontext, we study a more general representer theorem, which holds for a larger\nclass of regularizers. We provide a necessary and sufficient condition for\nthese class of matrix regularizers and highlight them with some concrete\nexamples of practical importance. Our analysis uses basic principles from\nmatrix theory, especially the useful notion of matrix nondecreasing function.\n", "versions": [{"version": "v1", "created": "Tue, 9 Sep 2008 16:11:12 GMT"}], "update_date": "2012-08-21", "authors_parsed": [["Argyriou", "Andreas", ""], ["Micchelli", "Charles", ""], ["Pontil", "Massimiliano", ""]]}, {"id": "0809.2075", "submitter": "Jittat Fakcharoenphol", "authors": "Jittat Fakcharoenphol, Boonserm Kijsirikul", "title": "Low congestion online routing and an improved mistake bound for online\n  prediction of graph labeling", "comments": "5 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DM cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we show a connection between a certain online low-congestion\nrouting problem and an online prediction of graph labeling. More specifically,\nwe prove that if there exists a routing scheme that guarantees a congestion of\n$\\alpha$ on any edge, there exists an online prediction algorithm with mistake\nbound $\\alpha$ times the cut size, which is the size of the cut induced by the\nlabel partitioning of graph vertices. With previous known bound of $O(\\log n)$\nfor $\\alpha$ for the routing problem on trees with $n$ vertices, we obtain an\nimproved prediction algorithm for graphs with high effective resistance.\n  In contrast to previous approaches that move the graph problem into problems\nin vector space using graph Laplacian and rely on the analysis of the\nperceptron algorithm, our proof are purely combinatorial. Further more, our\napproach directly generalizes to the case where labels are not binary.\n", "versions": [{"version": "v1", "created": "Thu, 11 Sep 2008 19:32:49 GMT"}, {"version": "v2", "created": "Fri, 12 Sep 2008 07:02:37 GMT"}], "update_date": "2008-09-12", "authors_parsed": [["Fakcharoenphol", "Jittat", ""], ["Kijsirikul", "Boonserm", ""]]}, {"id": "0809.2085", "submitter": "Laurent Jacob", "authors": "Laurent Jacob, Francis Bach (INRIA Rocquencourt), Jean-Philippe Vert", "title": "Clustered Multi-Task Learning: A Convex Formulation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In multi-task learning several related tasks are considered simultaneously,\nwith the hope that by an appropriate sharing of information across tasks, each\ntask may benefit from the others. In the context of learning linear functions\nfor supervised classification or regression, this can be achieved by including\na priori information about the weight vectors associated with the tasks, and\nhow they are expected to be related to each other. In this paper, we assume\nthat tasks are clustered into groups, which are unknown beforehand, and that\ntasks within a group have similar weight vectors. We design a new spectral norm\nthat encodes this a priori assumption, without the prior knowledge of the\npartition of tasks into groups, resulting in a new convex optimization\nformulation for multi-task learning. We show in simulations on synthetic\nexamples and on the IEDB MHC-I binding dataset, that our approach outperforms\nwell-known convex methods for multi-task learning, as well as related non\nconvex methods dedicated to the same problem.\n", "versions": [{"version": "v1", "created": "Thu, 11 Sep 2008 19:01:39 GMT"}], "update_date": "2008-09-12", "authors_parsed": [["Jacob", "Laurent", "", "INRIA Rocquencourt"], ["Bach", "Francis", "", "INRIA Rocquencourt"], ["Vert", "Jean-Philippe", ""]]}, {"id": "0809.2754", "submitter": "Paul Vitanyi", "authors": "Peter D. Grunwald (CWI) and Paul M.B. Vitanyi (CWI and Univ.\n  Amsterdam)", "title": "Algorithmic information theory", "comments": "37 pages, 2 figures, pdf, in: Philosophy of Information, P. Adriaans\n  and J. van Benthem, Eds., A volume in Handbook of the philosophy of science,\n  D. Gabbay, P. Thagard, and J. Woods, Eds., Elsevier, 2008. In version 1 of\n  September 16 the refs are missing. Corrected in version 2 of September 17", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.LG math.IT math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce algorithmic information theory, also known as the theory of\nKolmogorov complexity. We explain the main concepts of this quantitative\napproach to defining `information'. We discuss the extent to which Kolmogorov's\nand Shannon's information theory have a common purpose, and where they are\nfundamentally different. We indicate how recent developments within the theory\nallow one to formally distinguish between `structural' (meaningful) and\n`random' information as measured by the Kolmogorov structure function, which\nleads to a mathematical formalization of Occam's razor in inductive inference.\nWe end by discussing some of the philosophical implications of the theory.\n", "versions": [{"version": "v1", "created": "Tue, 16 Sep 2008 16:38:18 GMT"}, {"version": "v2", "created": "Wed, 17 Sep 2008 17:25:44 GMT"}], "update_date": "2008-09-17", "authors_parsed": [["Grunwald", "Peter D.", "", "CWI"], ["Vitanyi", "Paul M. B.", "", "CWI and Univ.\n  Amsterdam"]]}, {"id": "0809.2792", "submitter": "Ronny Luss", "authors": "Ronny Luss, Alexandre d'Aspremont", "title": "Predicting Abnormal Returns From News Using Text Classification", "comments": "Larger data sets, results on time of day effect, and use of delta\n  hedged covered call options to trade on daily predictions", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We show how text from news articles can be used to predict intraday price\nmovements of financial assets using support vector machines. Multiple kernel\nlearning is used to combine equity returns with text as predictive features to\nincrease classification performance and we develop an analytic center cutting\nplane method to solve the kernel learning problem efficiently. We observe that\nwhile the direction of returns is not predictable using either text or returns,\ntheir size is, with text features producing significantly better performance\nthan historical returns alone.\n", "versions": [{"version": "v1", "created": "Tue, 16 Sep 2008 20:05:00 GMT"}, {"version": "v2", "created": "Sat, 15 Nov 2008 23:31:38 GMT"}, {"version": "v3", "created": "Wed, 24 Jun 2009 17:45:11 GMT"}], "update_date": "2009-06-24", "authors_parsed": [["Luss", "Ronny", ""], ["d'Aspremont", "Alexandre", ""]]}, {"id": "0809.3170", "submitter": "Xinjia Chen", "authors": "Xinjia Chen", "title": "A New Framework of Multistage Hypothesis Tests", "comments": "77 pages, no figure; added more references; in Proceedings of SPIE\n  Conferences, Orlando, Florida, April 5-10, 2010 and April 25-29, 2011", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.LG math.PR stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we have established a general framework of multistage\nhypothesis tests which applies to arbitrarily many mutually exclusive and\nexhaustive composite hypotheses. Within the new framework, we have constructed\nspecific multistage tests which rigorously control the risk of committing\ndecision errors and are more efficient than previous tests in terms of average\nsample number and the number of sampling operations. Without truncation, the\nsample numbers of our testing plans are absolutely bounded.\n", "versions": [{"version": "v1", "created": "Thu, 18 Sep 2008 14:25:06 GMT"}, {"version": "v10", "created": "Fri, 20 Nov 2009 22:17:43 GMT"}, {"version": "v11", "created": "Mon, 21 Dec 2009 22:56:13 GMT"}, {"version": "v12", "created": "Tue, 16 Feb 2010 00:27:27 GMT"}, {"version": "v13", "created": "Wed, 17 Feb 2010 03:04:41 GMT"}, {"version": "v14", "created": "Mon, 28 Jun 2010 22:26:57 GMT"}, {"version": "v15", "created": "Fri, 6 Aug 2010 20:16:51 GMT"}, {"version": "v16", "created": "Sat, 13 Nov 2010 16:41:18 GMT"}, {"version": "v17", "created": "Mon, 21 Mar 2011 01:28:32 GMT"}, {"version": "v18", "created": "Mon, 6 Jun 2011 01:16:15 GMT"}, {"version": "v19", "created": "Sat, 18 Jun 2011 00:46:04 GMT"}, {"version": "v2", "created": "Thu, 16 Oct 2008 16:19:07 GMT"}, {"version": "v20", "created": "Wed, 3 Aug 2011 16:02:49 GMT"}, {"version": "v21", "created": "Wed, 30 Nov 2011 19:11:21 GMT"}, {"version": "v22", "created": "Sat, 31 Dec 2011 02:28:00 GMT"}, {"version": "v23", "created": "Mon, 13 Feb 2012 17:11:54 GMT"}, {"version": "v24", "created": "Tue, 3 Apr 2012 16:36:09 GMT"}, {"version": "v25", "created": "Wed, 5 Dec 2012 00:35:38 GMT"}, {"version": "v3", "created": "Sun, 2 Nov 2008 23:37:14 GMT"}, {"version": "v4", "created": "Sun, 22 Mar 2009 21:23:57 GMT"}, {"version": "v5", "created": "Sun, 29 Mar 2009 20:20:01 GMT"}, {"version": "v6", "created": "Mon, 27 Apr 2009 18:37:54 GMT"}, {"version": "v7", "created": "Wed, 23 Sep 2009 17:00:30 GMT"}, {"version": "v8", "created": "Wed, 23 Sep 2009 20:12:53 GMT"}, {"version": "v9", "created": "Sun, 4 Oct 2009 19:06:44 GMT"}], "update_date": "2013-11-05", "authors_parsed": [["Chen", "Xinjia", ""]]}, {"id": "0809.3352", "submitter": "Steffen Kuehn", "authors": "Steffen Kuehn", "title": "Generalized Prediction Intervals for Arbitrary Distributed\n  High-Dimensional Data", "comments": "13 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper generalizes the traditional statistical concept of prediction\nintervals for arbitrary probability density functions in high-dimensional\nfeature spaces by introducing significance level distributions, which provides\ninterval-independent probabilities for continuous random variables. The\nadvantage of the transformation of a probability density function into a\nsignificance level distribution is that it enables one-class classification or\noutlier detection in a direct manner.\n", "versions": [{"version": "v1", "created": "Fri, 19 Sep 2008 11:02:39 GMT"}], "update_date": "2008-09-22", "authors_parsed": [["Kuehn", "Steffen", ""]]}, {"id": "0809.3618", "submitter": "Julian McAuley", "authors": "Julian J. McAuley, Tiberio S. Caetano, Alexander J. Smola", "title": "Robust Near-Isometric Matching via Structured Learning of Graphical\n  Models", "comments": "11 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Models for near-rigid shape matching are typically based on distance-related\nfeatures, in order to infer matches that are consistent with the isometric\nassumption. However, real shapes from image datasets, even when expected to be\nrelated by \"almost isometric\" transformations, are actually subject not only to\nnoise but also, to some limited degree, to variations in appearance and scale.\nIn this paper, we introduce a graphical model that parameterises appearance,\ndistance, and angle features and we learn all of the involved parameters via\nstructured prediction. The outcome is a model for near-rigid shape matching\nwhich is robust in the sense that it is able to capture the possibly limited\nbut still important scale and appearance variations. Our experimental results\nreveal substantial improvements upon recent successful models, while\nmaintaining similar running times.\n", "versions": [{"version": "v1", "created": "Sun, 21 Sep 2008 23:23:26 GMT"}], "update_date": "2008-09-23", "authors_parsed": [["McAuley", "Julian J.", ""], ["Caetano", "Tiberio S.", ""], ["Smola", "Alexander J.", ""]]}, {"id": "0809.4086", "submitter": "Valentino Crespi", "authors": "George Cybenko and Valentino Crespi", "title": "Learning Hidden Markov Models using Non-Negative Matrix Factorization", "comments": "Submitted to IEEE Transactions on Information Theory in September\n  2008", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.IT math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Baum-Welsh algorithm together with its derivatives and variations has\nbeen the main technique for learning Hidden Markov Models (HMM) from\nobservational data. We present an HMM learning algorithm based on the\nnon-negative matrix factorization (NMF) of higher order Markovian statistics\nthat is structurally different from the Baum-Welsh and its associated\napproaches. The described algorithm supports estimation of the number of\nrecurrent states of an HMM and iterates the non-negative matrix factorization\n(NMF) algorithm to improve the learned HMM parameters. Numerical examples are\nprovided as well.\n", "versions": [{"version": "v1", "created": "Wed, 24 Sep 2008 05:34:56 GMT"}, {"version": "v2", "created": "Sat, 8 Jan 2011 03:16:39 GMT"}], "update_date": "2011-01-11", "authors_parsed": [["Cybenko", "George", ""], ["Crespi", "Valentino", ""]]}, {"id": "0809.4632", "submitter": "Sriharsha Veeramachaneni", "authors": "Sriharsha Veeramachaneni and Ravikumar Kondadadi", "title": "Surrogate Learning - An Approach for Semi-Supervised Classification", "comments": "8 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the task of learning a classifier from the feature space\n$\\mathcal{X}$ to the set of classes $\\mathcal{Y} = \\{0, 1\\}$, when the features\ncan be partitioned into class-conditionally independent feature sets\n$\\mathcal{X}_1$ and $\\mathcal{X}_2$. We show the surprising fact that the\nclass-conditional independence can be used to represent the original learning\ntask in terms of 1) learning a classifier from $\\mathcal{X}_2$ to\n$\\mathcal{X}_1$ and 2) learning the class-conditional distribution of the\nfeature set $\\mathcal{X}_1$. This fact can be exploited for semi-supervised\nlearning because the former task can be accomplished purely from unlabeled\nsamples. We present experimental evaluation of the idea in two real world\napplications.\n", "versions": [{"version": "v1", "created": "Fri, 26 Sep 2008 13:47:36 GMT"}], "update_date": "2008-09-29", "authors_parsed": [["Veeramachaneni", "Sriharsha", ""], ["Kondadadi", "Ravikumar", ""]]}, {"id": "0809.4882", "submitter": "Aleksandrs Slivkins", "authors": "Robert Kleinberg, Aleksandrs Slivkins and Eli Upfal", "title": "Multi-Armed Bandits in Metric Spaces", "comments": "16 pages, 0 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In a multi-armed bandit problem, an online algorithm chooses from a set of\nstrategies in a sequence of trials so as to maximize the total payoff of the\nchosen strategies. While the performance of bandit algorithms with a small\nfinite strategy set is quite well understood, bandit problems with large\nstrategy sets are still a topic of very active investigation, motivated by\npractical applications such as online auctions and web advertisement. The goal\nof such research is to identify broad and natural classes of strategy sets and\npayoff functions which enable the design of efficient solutions. In this work\nwe study a very general setting for the multi-armed bandit problem in which the\nstrategies form a metric space, and the payoff function satisfies a Lipschitz\ncondition with respect to the metric. We refer to this problem as the\n\"Lipschitz MAB problem\". We present a complete solution for the multi-armed\nproblem in this setting. That is, for every metric space (L,X) we define an\nisometry invariant which bounds from below the performance of Lipschitz MAB\nalgorithms for X, and we present an algorithm which comes arbitrarily close to\nmeeting this bound. Furthermore, our technique gives even better results for\nbenign payoff functions.\n", "versions": [{"version": "v1", "created": "Mon, 29 Sep 2008 01:58:13 GMT"}], "update_date": "2008-09-30", "authors_parsed": [["Kleinberg", "Robert", ""], ["Slivkins", "Aleksandrs", ""], ["Upfal", "Eli", ""]]}, {"id": "0809.4883", "submitter": "Venkatesh Saligrama", "authors": "V. Saligrama, M. Zhao", "title": "Thresholded Basis Pursuit: An LP Algorithm for Achieving Optimal Support\n  Recovery for Sparse and Approximately Sparse Signals from Noisy Random\n  Measurements", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.LG math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we present a linear programming solution for sign pattern\nrecovery of a sparse signal from noisy random projections of the signal. We\nconsider two types of noise models, input noise, where noise enters before the\nrandom projection; and output noise, where noise enters after the random\nprojection. Sign pattern recovery involves the estimation of sign pattern of a\nsparse signal. Our idea is to pretend that no noise exists and solve the\nnoiseless $\\ell_1$ problem, namely, $\\min \\|\\beta\\|_1 ~ s.t. ~ y=G \\beta$ and\nquantizing the resulting solution. We show that the quantized solution\nperfectly reconstructs the sign pattern of a sufficiently sparse signal.\nSpecifically, we show that the sign pattern of an arbitrary k-sparse,\nn-dimensional signal $x$ can be recovered with $SNR=\\Omega(\\log n)$ and\nmeasurements scaling as $m= \\Omega(k \\log{n/k})$ for all sparsity levels $k$\nsatisfying $0< k \\leq \\alpha n$, where $\\alpha$ is a sufficiently small\npositive constant. Surprisingly, this bound matches the optimal\n\\emph{Max-Likelihood} performance bounds in terms of $SNR$, required number of\nmeasurements, and admissible sparsity level in an order-wise sense. In contrast\nto our results, previous results based on LASSO and Max-Correlation techniques\neither assume significantly larger $SNR$, sublinear sparsity levels or\nrestrictive assumptions on signal sets. Our proof technique is based on noisy\nperturbation of the noiseless $\\ell_1$ problem, in that, we estimate the\nmaximum admissible noise level before sign pattern recovery fails.\n", "versions": [{"version": "v1", "created": "Mon, 29 Sep 2008 14:01:13 GMT"}, {"version": "v2", "created": "Sun, 22 Mar 2009 01:49:48 GMT"}, {"version": "v3", "created": "Sat, 8 May 2010 11:34:25 GMT"}], "update_date": "2015-03-13", "authors_parsed": [["Saligrama", "V.", ""], ["Zhao", "M.", ""]]}]